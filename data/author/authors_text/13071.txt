Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1133?1141,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Real-time decision detection in multi-party dialogue
Matthew Frampton, Jia Huang, Trung Huu Bui and Stanley Peters
Center for the Study of Language and Information
Stanford University
Stanford, CA, 94305, USA
{frampton|jiahuang|thbui|peters}@stanford.edu
Abstract
We describe a process for automatically
detecting decision-making sub-dialogues
in multi-party, human-human meetings in
real-time. Our basic approach to decision
detection involves distinguishing between
different utterance types based on the roles
that they play in the formulation of a de-
cision. In this paper, we describe how this
approach can be implemented in real-time,
and show that the resulting system?s per-
formance compares well with other detec-
tors, including an off-line version.
1 Introduction
In collaborative and organized work environ-
ments, people share information and make de-
cisions through multi-party conversations, com-
monly referred to as meetings. The demand for
automatic methods that process, understand and
summarize information contained in audio and
video recordings of meetings is growing rapidly,
as evidenced by on-going projects which are fo-
cused on this goal, (Waibel et al, 2003; Janin et
al., 2004). Our research is part of a general effort
to develop a system that can automatically extract
and summarize information such as conversational
topics, action items, and decisions.
This paper concerns the development of a real-
time decision detector ? a system which can de-
tect and summarize decisions as they are made
during a meeting. Such a system could provide
a summary of all of the decisions which have been
made up until the current point in the meeting,
and this is something which we expect will help
users to enjoy more productive meetings. Cer-
tainly, good decision-making relies on access to
relevant information, and decisions made earlier
in a meeting often have a bearing on the current
topic of discussion, and so form part of this rele-
vant information. However, in a long and winding
meeting, participants might not have these earlier
decisions at the forefront of their minds, and so
an accurate and succinct reminder, as provided by
a real-time decision detector, could potentially be
very useful. A record of earlier decisions could
also help users to identify outstanding issues for
discussion, and to therefore make better use of the
remainder of the meeting.
Our approach to decision detection uses an an-
notation scheme which distinguishes between dif-
ferent types of utterance based on the roles which
they play in the decision-making process. Such a
scheme facilitates the detection of decision discus-
sions (Fern?andez et al, 2008), and by indicating
which utterances contain particular types of infor-
mation, it also aids their summarization. To auto-
matically detect decision discussions, we use what
we refer to as hierarchical classification. Here, in-
dependent binary sub-classifiers detect the differ-
ent decision dialogue acts, and then based on the
sub-classifier hypotheses, a super-classifier deter-
mines which dialogue regions are decision discus-
sions. In this paper then, we address the chal-
lenges for applying this approach in real-time, and
produce a system which is able to detect decisions
soon after they are made, (for example within a
minute). We conduct tests and compare this sys-
tem?s performance with other detectors, including
an off-line equivalent.
The remainder of the paper proceeds as follows.
Section 2 describes related work, and Section 3 de-
scribes our annotation scheme for decision discus-
sions, and our experimental data. Next, Section
4 explains the hierarchical classification approach
in more detail, and Section 5 considers how it can
be applied in real-time. Section 6 describes the
experiments in which we test the real-time detec-
tor, and finally, Section 7 presents conclusions and
ideas for future work.
1133
2 Related Work
Decisions are one of the most important meet-
ing outputs. User studies (Lisowska et al, 2004;
Banerjee et al, 2005) have confirmed that meeting
participants consider this to be the case, and Whit-
taker et al (2006) found that the development of
an automatic decision detection component is crit-
ical to the re-use of meeting archives. As a result,
with the new availability of substantial meeting
corpora such as the ISL (Burger et al, 2002), ICSI
(Janin et al, 2004) and AMI (McCowan et al,
2005) Meeting Corpora, recent years have seen an
increasing amount of research on decision-making
dialogue.
This recent research has tackled issues such
as the automatic detection of agreement and dis-
agreement (Hillard et al, 2003; Galley et al,
2004), and of the level of involvement of conver-
sational participants (Wrede and Shriberg, 2003;
Gatica-Perez et al, 2005). In addition, Verbree
et al (2006) created an argumentation scheme in-
tended to support automatic production of argu-
ment structure diagrams from decision-oriented
meeting transcripts. Only very recent research has
specifically investigated the automatic detection of
decisions, namely (Hsueh and Moore, 2007) and
(Fern?andez et al, 2008).
Hsueh and Moore (2007) used the AMI Meeting
Corpus, and attempted to automatically identify
dialogue acts (DAs) in meeting transcripts which
are ?decision-related?. Within any meeting, the
authors decided which DAs were decision-related
based on two different kinds of manually created
summary: the first was an extractive summary of
the whole meeting, and the second, an abstrac-
tive summary of the decisions which were made.
Those DAs in the extractive summary which sup-
port any of the decisions in the abstractive sum-
mary were manually tagged as decision-related.
Hsueh and Moore (2007) then trained a Maxi-
mum Entropy classifier to recognize this single
DA class, using a variety of lexical, prosodic, dia-
logue act and conversational topic features. They
achieved an F-score of 0.35, which gives an indi-
cation of the difficulty of this task.
Unlike Hsueh and Moore (2007), Fern?andez
et al (2008) made an attempt at modelling the
structure of decision-making dialogue. They de-
signed an annotation scheme that takes account of
the different roles which different utterances play
in the decision-making process ? for example,
their scheme distinguishes between decision DAs
(DDAs) which initiate a discussion by raising a
topic/issue, those which propose a resolution, and
those which express agreement for a proposed res-
olution and cause it to be accepted as a decision.
The authors applied the annotation scheme to a
portion of the AMI corpus, and then took what
they refer to as a hierarchical classification ap-
proach in order to automatically identify decision
discussions and their component DAs. Here, one
binary Support Vector Machine (SVM) per DDA
class hypothesized occurrences of that DDA class,
and then based on the hypotheses of these so-
called sub-classifiers, a super-classifier, (a further
SVM), determined which regions of dialogue rep-
resented decision discussions. This approach pro-
duced better results than the kind of ?flat classi-
fication? approach pursued by Hsueh and Moore
(2007) where a single classifier looks for exam-
ples of a single decision-related DA class. Using
manual transcripts, and a variety of lexical, utter-
ance, speaker, DA and prosodic features for the
sub-classifiers, the super-classifier?s F1-score was
0.58 according to a lenient match metric. Note that
(Purver et al, 2007) had previously pursued the
same basic approach as Fern?andez et al (2008) in
order to detect action items.
While both Hsueh and Moore (2007), and
Fern?andez et al (2008) attempted off-line decision
detection, in this paper, we attempt real-time deci-
sion detection. We take the same basic approach
as Fern?andez et al (2008), and make changes to
its implementation so that it can work effectively
in real-time.
3 Data
The AMI corpus (McCowan et al, 2005), is a
freely available corpus of multi-party meetings
containing both audio and video recordings, as
well as a wide range of annotated information
including dialogue acts and topic segmentation.
Conversations are all in English, but participants
can include non-native English speakers. All of
the meetings in our sub-corpus last around 30 min-
utes, and are scenario-driven, wherein four partic-
ipants play different roles in a company?s design
team: project manager, marketing expert, inter-
face designer and industrial designer. The discus-
sions concern how to design a remote control.
We used the off-line version of the Decipher
speech recognition engine (Stolcke et al, 2008) in
1134
order to obtain off-line ASR transcripts for these
17 meetings, and the real-time version, to ob-
tain real-time ASR transcripts. Decipher gener-
ates the transcripts by first producing Word Con-
fusion Networks (WCNs) and then extracting their
best paths. The real-time recognizer generates
?live? transcripts with 5 to 15 seconds of latency
for immediate display. In processing completed
meetings, the off-line system makes seven recog-
nition passes, including acoustic adaptation and
language model rescoring, in about 4.2 times real-
time (on a 4-score 2.6 GHz Opteron server). In
general usage with multi-party dialogue, the word
error rate (WER) for the off-line version of De-
cipher is approximately 23%, and for the real-
time version, approximately 35%
1
. Stolcke et al
(2008) report a WER of 26.9% for the off-line ver-
sion on AMI meetings.
The real-time ASR transcripts for the 17 meet-
ings contain a total of 8440 utterances/dialogue
acts, (around 496 per meeting), and the off-line
ASR transcripts, 7495 utterances/dialogue acts,
(around 441 per meeting).
3.1 Modelling Decision Discussions
We use the same annotation scheme as
(Fern?andez et al, 2008) in order to model
decision-making dialogue. As stated in Section 2,
this scheme distinguishes between a small number
of dialogue act types based on the role which they
perform in the formulation of a decision. Recall
that using this scheme in conjunction with hierar-
chical classification produced better decision de-
tection than a ?flat classification? approach with a
single ?decision-related? DA class. Since it indi-
cates which utterances contain particular types of
information, such a scheme also aids the summa-
rization of decision discussions.
The annotation scheme (see Table 1 for a sum-
mary) is based on the observation that a decision
discussion contains the following main structural
components: (a) a topic or issue requiring resolu-
tion is raised, (b) one or more possible resolutions
are considered, (c) a particular resolution is agreed
upon, that is, it becomes the decision. Hence the
scheme distinguishes between three correspond-
ing decision dialogue act (DDA) classes: Issue (I),
Resolution (R), and Agreement (A). Class R is fur-
ther subdivided into Resolution Proposal (RP) and
1
This information was obtained through personal commu-
nication.
Resolution Restatement (RR). Note that an utter-
ance can be assigned to more than one of these
DDA classes, and that within a decision discus-
sion, more than one utterance may correspond to a
particular DDA class.
Here we use the sample decision discussion
below in 1 in order to provide examples of the
different DDA types. I utterances introduce the
topic of the decision discussion, examples be-
ing ?Are we going to have a backup?? and ?But
would a backup really be necessary?? On the
other hand, R utterances specify the resolution
which is ultimately adopted as the decision. RP
utterances propose this resolution (e.g. ?I think
maybe we could just go for the kinetic energy. . . ?),
while RR utterances close the discussion by con-
firming/summarizing the decision (e.g. ?Okay,
fully kinetic energy?). Finally, A utterances agree
with the proposed resolution, so causing it to be
adopted as the decision, (e.g. ?Yeah?, ?Good?
and ?Okay?.
(1) A: Are we going to have a backup?
Or we do just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.
2
3.2 Experimental data for real-time decision
detection
Originally, two individuals used the annotation
scheme described above in order to annotate the
manual transcripts of 9 and 10 meetings respec-
tively. The annotators overlapped on two meet-
ings, and their kappa inter-annotator agreement
ranged from 0.63 to 0.73 for the four DDA classes.
The highest agreement was obtained for class RP,
and the lowest for class A. Although these kappa
values are not extremely high, if we used a single,
less homogeneous ?decision-related? DA class
like Hsueh and Moore (2007), then its kappa score
2
This example was extracted from the AMI dialogue
ES2015c and has been modified slightly for presentation pur-
poses.
1135
key DDA class description
I issue utterances introducing the issue or topic under discussion
R resolution utterances containing the resolution adopted as the decision
RP ? proposal ? utterances where the decision is originally proposed
RR ? restatement ? utterances where the decision is confirmed or restated
A agreement utterances explicitly signalling agreement with the decision
Table 1: Set of decision dialogue act (DDA) classes
would probably be significantly lower. The de-
cision discussion annotations used by Hsueh and
Moore (2007) are part of the AMI corpus, and are
for the manual transcriptions. The reader can find
a comparison between these annotations and our
own manual transcript annotations in (Fern?andez
et al, 2008).
After obtaining the new off-line and real-time
ASR transcripts, we transferred the DDA annota-
tions from the manual transcripts. In both sets of
ASR transcripts, each meeting contains on aver-
age around 26 DAs tagged with one or more of the
DDA sub-classes in Table 1. DDAs are thus very
sparse, corresponding to only 5.3% of utterances
in the real-time transcripts, and 6.0% in the off-
line. In the real-time transcripts, Issue utterances
make up less than 1.2% of the total number of ut-
terances in a meeting, while Resolution utterances
are around 1.6%: 1.2% are RP and less than 0.4%
are RR on average. Almost half of DDA utterances
(slightly over 2.6% of all utterances on average)
are tagged as belonging to class Agreement. In the
off-line transcripts, the percentages are fairly sim-
ilar: 1.6% of utterances are Issue DDAs, 2.0% are
RP, 0.5% are RR, and 2.4% are A.
We now move on to describe the hierarchical
classification approach which we use to try to au-
tomatically detect decision sub-dialogues and their
component DDAs.
4 Hierarchical Classification
Hierarchical classification is designed to exploit
the fact that within decision discussions, our
DDAs can be expected to co-occur in particular
types of patterns. It involves two different types of
classifier:
1. Sub-classifier: One independent binary sub-
classifier per DDA class classifies each utter-
ance.
2. Super-classifier: A sliding window shifts
through the meeting one utterance at a time,
and following each shift, a binary super-
classifier determines whether the region of
dialogue within the window is part of a de-
cision discussion.
In our decision detectors, the sub-classifiers run
in parallel in order to reduce processing time.
For each utterance, the sub-classifiers use fea-
tures which are derived from the properties of
that utterance in context. On the other hand,
the super-classifier?s features are the hypothesized
class labels and confidence scores for the utter-
ances within the window. In various experiments,
we have found that a suitable size for the window,
is the average length of a decision discussion in
our data in utterances. The super-classifier also
?corrects? the sub-classifiers. This means that if a
DA is classified as positive by a sub-classifier, but
does not fall within a region classified as part of
a decision discussion by the super-classifier, then
the sub-classifier?s hypothesis is changed to nega-
tive.
We now move on to consider how this basic ap-
proach to decision detection can be implemented
in a real-time system.
5 Design considerations for our real-time
system
A real-time decision detector should detect deci-
sions as soon after they are made as possible. It is
for this reason that we have set our real-time de-
tector to automatically run at frequent and regular
intervals during a meeting. An alternative would
be to give the user (a meeting participant) respon-
sibility for instructing the detector when to run.
However, a user may sometimes leave substantial
gaps between giving run commands. When this
happens, the detector will have to process a large
number of utterances in a single run, and so the
user may wait some time before being presented
with any results. In addition, giving the user re-
sponsibility for instructing the detector when to
1136
Figure 1: Decision discussion regions hypothesized by
consecutive runs overlap (D
1
to D
3
and D
2
to D
4
) and so
are merged.
run means burdening the user with an extra task to
perform during the meeting, and this goes against
the general philosophy behind the system?s devel-
opment. The system is intended to be as unobtru-
sive as possible during the meeting, and to relieve
users of tasks which distract their attention away
from the current discussion (e.g. note-taking), not
to create new tasks, however small.
Obviously, on the first occasion that the detector
runs during a meeting, it can only process ?new?
(previously unprocessed) utterances, but on sub-
sequent runs, it has the option to reprocess some
number of ?old? utterances (utterances which it
has already processed in a previous run). Cer-
tainly, the detector should reprocess some of the
most recent old utterances because it is possible
that a decision discussion straddles these utter-
ances and new utterances. However, the number of
old utterances that are reprocessed should be lim-
ited. If the meeting has lasted a while already, then
the processing of a large portion of the earlier old
utterances is likely to be redundant ? it will sim-
ply produce the same results for these utterances
as the previous run.
The fact that the real-time detector processes re-
cent old utterances means that consecutive runs
can produce hypotheses for decision discussion re-
gions which overlap, or which are duplicates. Fig-
ure 1 gives an example of the former. We deal with
overlapping hypotheses by merging them into one,
so forming a larger single decision discussion re-
gion. Figure 2 gives an example of duplicate hy-
potheses. Here, on run n, the detector hypothe-
sizes decision discussion D
1
to D
2
, and then on
run n+1, since the bounds of this original hypoth-
esis are now wholly contained within the region of
Figure 2: Consecutive runs hypothesize the same decision
discussion region D
1
to D
2
, and so one of the duplicates is
discarded.
old reprocessed utterances, the detector hypothe-
sizes a duplicate. We deal with such cases by dis-
carding the duplicate.
6 Experiments
We conducted various experiments related to real-
time decision detection, our goal being to produce
a system which:
? relative to alternative versions, detects deci-
sion discussions accurately,
? generates results for any portion of dialogue
very soon after that portion of dialogue has
ended.
The current version of our real-time detector is set
to process the same number of old and new utter-
ances on each run. Here, we refer to this value as i,
and hence on each run the system processes a total
of 2i utterances (i old and i new). Another of the
system?s characteristics is that runs take place ev-
ery i utterances, meaning that as we decrease i, the
system provides new results more frequently and
is hence ?more real-time?. One of the things we
investigate here then, is what to set i to in order
to best satisfy the two design goals given above.
Having found this value, we compare the hierar-
chical real-time detector?s performance with alter-
native detectors, these being:
? an off-line detector applied to off-line ASR
transcripts,
? a flat real-time detector,
? an off-line detector applied to the real-time
ASR transcripts.
1137
Lexical unigrams after text normalization
Utterance length in words, duration in
word rate
Speaker speaker ID & AMI speaker role
Context features as above for utterances
u +/- 1. . .u +/- 5
Table 2: Features for decision DA detection
Note that the off-line detectors use hierarchical
classification, and that the flat real-time detec-
tor uses a single binary classifier which treats all
DDAs as members of a single merged DDA class.
6.1 Classifiers and features
All classifiers (sub and super-classifiers) in all de-
tectors are linear-kernel Support Vector Machines
(SVMs), produced using SVMlight (Joachims,
1999). For the sub-classifiers, we are obviously re-
stricted to using features which can be computed
in a very short period of time, and in the experi-
ments here, we use lexical, utterance and speaker
features. These are summarized in Table 2. An
utterance?s lexical features are the words in its
transcription, its utterance features are its dura-
tion, number of words, and word rate (number of
words divided by duration), and its speaker fea-
tures are the speaker?s role (see Section 3) and ID.
We also use lexical features for the previous and
where available, next utterances: the I, RP and RR
sub-classifiers use the lexical features for the pre-
vious/next utterance and the A sub-classifier, those
from the previous/next 5 utterances. These set-
tings produced the best results in preliminary ex-
periments. We do not use DA features because
we lack an automatic DA tagger, nor do we use
prosodic features because (Fern?andez et al, 2008)
was unable to derive any value from them with
SVMs.
6.2 Evaluation
We evaluate each of our decision detectors in 17-
fold cross validations, where in each fold, the de-
tector trains on 16 meetings and then tests on the
remaining one. Evaluation can be made at three
levels:
1. The sub-classifiers? detection of each of the
DDA classes.
2. The sub-classifiers? detection of each of the
DDA classes after correction by the super-
classifier.
Figure 3: The relationship between the number of old/new
utterances processed in a single run, and the super-classifier?s
F1-score. Here the sub-classifiers use only lexical features.
3. The super-classifier?s detection of decision
discussion regions.
For 1 and 2, we use the same lenient-match met-
ric as (Fern?andez et al, 2008; Hsueh and Moore,
2007), which allows a margin of 20 seconds pre-
ceding and following a hypothesized DDA. Note
that here we only give credit for hypotheses based
on a 1-1 mapping with the gold-standard labels.
For 3, we follow (Fern?andez et al, 2008; Purver et
al., 2007) and use a windowed metric that divides
the dialogue into 30-second windows and evalu-
ates on a per window basis.
6.3 Results and analysis
Here, Section 6.3.1 will present results for differ-
ent values of i, the number of old/new utterances
processed in a single run. Section 6.3.2 then com-
pares the performance of the real-time and off-line
systems, (and also real-time systems which use hi-
erarchical vs. flat classification), and Section 6.3.3
presents some feature analysis.
6.3.1 Varying the number of old/new
utterances processed in a run
Figure 3 shows the relationship between i, the set-
ting for the number of old/new utterances pro-
cessed in a single run, and the super-classifier?s
F1-score. Here, the sub-classifiers are using only
lexical features. We can see from the graph that
as i increases to 15, the super-classifier?s F1-score
also increases, but thereafter, it plateaus. Hence
15 is apparently the value which best satisfies the
two design goals given at the start of Section 6.
It should also be noted that 15 is the mean length
of a decision discussion in our data, and so per-
1138
sub-classifiers super
I RP RR A classifier
Re .73 .73 .84 .71 .82
Pr .08 .09 .03 .15 .40
F1 .15 .16 .06 .25 .54
Table 3: Results for the hierarchical real-time
decision detector, using lexical, utterance and
speaker features.
sub-classifiers super
I RP RR A classifier
Re .51 .51 .10 .63 .83
Pr .12 .11 .04 .15 .41
F1 .19 .19 .05 .24 .55
Table 4: Results for the hierarchical off-line de-
cision detector on off-line ASR transcripts, using
lexical, utterance and speaker features.
haps this is a transferable finding. The mean du-
ration of a run when i = 15 is approximately 4
seconds, while the mean duration of 15 utterances
in our data-set is approximately 60 seconds, mean-
ing that for the average case, the detector returns
the results for the current run, long before it is
due to make the next. Significant lee-way is per-
haps necessary here, because the final version of
the real-time detector will include a summariza-
tion component which extracts key phrases from
Issue/Resolution utterances, and its processing can
last some time, even for a single decision.
We should say then, that the system is not
strictly real-time because in general, it detects de-
cisions soon after they are made (for example
within a minute), rather than immediately after. In
the future we intend to modify the system so that
it can run more frequently than once every i ut-
terances. However it is important that runs do not
occur too frequently ? for example, if i = 15 and
the system runs after every utterance, then the ex-
tra processing will cause it to gradually fall further
and further behind the meeting.
6.3.2 Real-time vs. off-line results
Table 3 shows the results achieved by a hierarchi-
cal real-time decision detector whose run settings
are as described above, and whose sub-classifiers
3
use lexical, utterance and speaker features. These
results compare well with those of an equivalent
3
In Tables 3 to 6, sub-classifier results are post-correction
(see Section 6.2).
sub-classifiers super
I RP RR A classifier
Re .50 .51 .09 .63 .83
Pr .11 .11 .03 .14 .41
F1 .19 .18 .05 .23 .55
Table 5: Results for the hierarchical off-line de-
tector on real-time ASR transcripts, using lexical,
utterance and speaker features.
sub-classifiers super
I RP RR A classifier
Re .67 .74 .84 .66 .85
Pr .07 .08 .03 .14 .41
F1 .13 .15 .05 .24 .55
Table 6: Results for the hierarchical real-time de-
cision detector, using lexical features only.
off-line detector, which are shown in Table 4. The
F1-scores for the real-time and off-line decision
super-classifiers are .54 and .55 respectively, and
the difference is not statistically significant. This
may indicate that the hierarchical classification ap-
proach is fairly robust to increasing ASR Word
Error Rates (WERs). Combining the output from
each of the independent sub-classifiers might com-
pensate somewhat for any decreases in their indi-
vidual accuracy, as there was here for the I and RP
sub-classifiers.
The hierarchical real-time detector?s F1-score is
also 10 points higher than a flat classifier (.54 vs.
.44). Hence, while Fern?andez et al (2008) demon-
strated that the hierarchical classification approach
could improve off-line decision detection, we have
demonstrated here that it can also improve real-
time decision detection.
Table 5 shows the results when an off-line
detector is applied to real-time ASR transcripts.
Here, the super-classifier obtains an F1-score of
.55, one point higher than the real-time detector,
but again, the difference is not statistically signifi-
cant.
6.3.3 Feature analysis
We also investigated the contribution of the ut-
terance and speaker features. Table 6 shows the
results for the hierarchical real-time decision de-
tector when its sub-classifiers use only lexical fea-
tures. The sub-classifier F1-scores are all slightly
lower than when utterance and speaker features
are used (see Table 3), and the super-classifier
1139
score is only 1 point different. None of these dif-
ferences are statistically significant.
Since lexical features are important, we used in-
formation gain in order to investigate which words
are predictive of each DDA type. Due to differ-
ences in the transcripts, the predictive words for
the off-line and real-time systems are not the same,
but we can find commonalities, and these com-
monalities make sense given the DDA definitions.
Firstly in Resolution and particularly Issue DAs,
some of the most predictive words could be used
to define discussion topics, and so we might ex-
pect to find them in the meeting agenda. Exam-
ples are ?energy?, and ?color?. Predictive words
for Resolutions also include semantically-related
words which are key in defining the decision (?ki-
netic?,?green?). Additional predictive words for
RPs are the personal pronouns ?I? and ?we?,
and the verbs, ?think? and ?like?, and for RRs,
words which we would associate with summing
up (?consensus?, ?definitely?, and ?okay?). Un-
surprisingly, for Agreements, ?yeah? and ?okay?
both score very highly.
7 Conclusion
(Fern?andez et al, 2008) described an approach
to decision detection in multi-party meetings and
demonstrated how it could work relatively well in
an off-line system. The approach has two defining
characteristics. The first is its use of an annota-
tion scheme which distinguishes between differ-
ent utterance types based on the roles which they
play in the decision-making process. The second
is its use of hierarchical classification, whereby
binary sub-classifiers detect instances of each of
the decision DAs (DDAs), and then based on the
sub-classifier hypotheses, a super-classifier deter-
mines which regions of dialogue are decision dis-
cussions.
In this paper then, we have taken the same ba-
sic approach to decision detection as Fern?andez et
al. (2008), but changed the way in which it is im-
plemented so that it can work effectively in real-
time. Our implementation changes include run-
ning the detector at regular and frequent intervals
during the meeting, and reprocessing recent utter-
ances in case a decision discussion straddles these
and brand new utterances. The fact that the de-
tector reprocesses utterances means that on con-
secutive runs, overlapping and duplicate hypothe-
sized decision discussions are possible. We have
therefore added facilities to merge overlapping hy-
potheses and to remove duplicates.
In general, the resulting system is able to detect
decisions soon after they are made (for example
within a minute), rather than immediately after. It
has performed well in testing, achieving an F1-
score of .54, which is only one point lower than
an equivalent off-line system, and in any case, the
difference was not statistically significant. A flat
real-time detector achieved .44.
In future work, we plan to extend the decision
discussion annotation scheme and try to extract
supporting arguments for decisions. We will also
experiment with using sequential models in order
to try to exploit any sequential ordering patterns in
the occurrence of the DDAs.
Acknowledgements This material is based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. FA8750-07-D-0185/0004, and by the
Department of the Navy Office of Naval Research
(ONR) under Grants No. N00014-05-1-0187 and
N00014-09-1-0106. Any opinions, findings and
conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of DARPA or ONR.
We are grateful to the three anonymous EMNLP
reviewers for their helpful comments and sugges-
tions, and to our partners at SRI International who
provided us with off-line and real-time transcripts
for our meeting data.
References
Satanjeev Banerjee, Carolyn Ros?e, and Alex Rudnicky.
2005. The necessity of a meeting recording and
playback system, and the benefit of topic-level anno-
tations to meeting browsing. In Proceedings of the
10th International Conference on Human-Computer
Interaction.
Susanne Burger, Victoria MacLaren, and Hua Yu.
2002. The ISL Meeting Corpus: The impact of
meeting type on speech style. In Proceedings of the
7th International Conference on Spoken Language
Processing (INTERSPEECH - ICSLP), Denver, Col-
orado.
Raquel Fern?andez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008. Mod-
elling and detecting decisions in multi-party dia-
logue. In Proc. of the 9th SIGdial Workshop on Dis-
course and Dialogue.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
1140
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Daniel Gatica-Perez, Ian McCowan, Dong Zhang, and
Samy Bengio. 2005. Detecting group interest level
in meetings. In IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
Dustin Hillard, Mari Ostendorf, and Elisabeth
Shriberg. 2003. Detection of agreement vs. dis-
agreement in meetings: Training with unlabeled
data. In Companion Volume of the Proceedings of
HLT-NAACL 2003 - Short Papers, Edmonton, Al-
berta, May.
Pey-Yun Hsueh and Johanna Moore. 2007. Automatic
decision detection in meeting speech. In Proceed-
ings of MLMI 2007, Lecture Notes in Computer Sci-
ence. Springer-Verlag.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Marc??as-Guarasa,
Nelson Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI meeting project: Resources and re-
search. In Proceedings of the 2004 ICASSP NIST
Meeting Recognition Workshop.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Sch?olkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods ?
Support Vector Learning. MIT Press.
Agnes Lisowska, Andrei Popescu-Belis, and Susan
Armstrong. 2004. User query analysis for the spec-
ification and evaluation of a dialogue processing and
retrieval system. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Matthew Purver, John Dowding, John Niekrasz,
Patrick Ehlen, Sharareh Noorbaloochi, and Stanley
Peters. 2007. Detecting and summarizing action
items in multi-party dialogue. In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
Andreas Stolcke, Xavier Anguera, Kofi Boakye,
?
Ozg?ur
C?etin, Adam Janin, Matthew Magimai-Doss, Chuck
Wooters, and Jing Zheng. 2008. The ICSI-SRI
Spring 2007 meeting and lecture recognition system.
In Proc. of CLEAR 2007 and RT2007.
Daan Verbree, Rutger Rienks, and Dirk Heylen. 2006.
First steps towards the automatic construction of
argument-diagrams from real discussions. In Pro-
ceedings of the 1st International Conference on
Computational Models of Argument, volume 144,
pages 183?194. IOS press.
A. Waibel, T. Schultz, M. Bett, M. Denecke, R. Malkin,
I. Rogina, R. Stiefelhagen, and J. Yang. 2003.
SMaRT: The smart meeting room task at ISL. In
ICASSP.
Steve Whittaker, Rachel Laban, and Simon Tucker.
2006. Analysing meeting records: An ethnographic
study and technological implications. In S. Renals
and S. Bengio, editors, Machine Learning for Multi-
modal Interaction: Second International Workshop,
MLMI 2005, Revised Selected Papers, volume 3869
of Lecture Notes in Computer Science, pages 101?
113. Springer.
Britta Wrede and Elizabeth Shriberg. 2003. Spotting
?hot spots? in meetings: Human judgements and
prosodic cues. In Proceedings of the 9th European
Conference on Speech Communication and Technol-
ogy, Geneva, Switzerland.
1141
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 235?243,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Extracting decisions from multi-party dialogue using directed graphical
models and semantic similarity
Trung H. Bui1, Matthew Frampton1, John Dowding2, and Stanley Peters1
1Center for the Study of Language and Information, Stanford University
{thbui|frampton|peters}@stanford.edu
2University of California/Santa Cruz
jdowding@ucsc.edu
Abstract
We use directed graphical models (DGMs)
to automatically detect decision discus-
sions in multi-party dialogue. Our ap-
proach distinguishes between different di-
alogue act (DA) types based on their role
in the formulation of a decision. DGMs
enable us to model dependencies, includ-
ing sequential ones. We summarize deci-
sions by extracting suitable phrases from
DAs that concern the issue under discus-
sion and its resolution. Here we use a
semantic-similarity metric to improve re-
sults on both manual and ASR transcripts.
1 Introduction
In work environments, people share information
and make decisions in multi-party conversations
known as meetings. The demand for systems that
can automatically process, understand and sum-
marize information contained in audio and video
recordings of meetings is growing rapidly. Our
own research, and that of other contemporary
projects (Janin et al, 2004), aim at meeting this
demand.
At present, we are focusing on the automatic
detection and summarization of decision discus-
sions. Our approach for detecting decision dis-
cussions involves distinguishing between differ-
ent dialogue act (DA) types based on their role
in the decision-making process. Two of these
types are DAs which describe the Issue under dis-
cussion, and DAs which describe its Resolution.
To summarize a decision discussion, we identify
words and phrases in the Issue and Resolution
DAs, which can be used to produce a concise, de-
scriptive summary.
This paper describes new experiments in both
detecting and summarizing decision discussions.
In the detection stage, we investigate the use of
Directed Graphical Models (DGMs). DGMs are
attractive because they can be used to model se-
quence and dependencies between predictor vari-
ables. In the summarization stage, we attempt to
improve phrase selection with a new feature that
measures the level of semantic similarity between
candidate Issue phrases and Resolution utterances,
and vice-versa. The feature is generated by a
semantic-similarity metric which uses WordNet as
a knowledge source. The motivation is that ordi-
narily, the Issue and Resolution components in a
decision summary should be semantically similar.
The paper proceeds as follows. Firstly, Sec-
tion 2 describes related work, and Section 3, our
data-set and annotation scheme for decision dis-
cussions. Section 4 then reports our decision de-
tection experiments using DGMs, and Section 5,
the summarization experiments. Finally, Section
6 draws conclusions and proposes ideas for future
work.
2 Related Work
User studies (Banerjee et al, 2005) have con-
firmed that meeting participants consider deci-
sions to be one of the most important meeting
outputs, and (Whittaker et al, 2006) found that
the development of an automatic decision detec-
tion component is critical to the re-use of meet-
ing archives. With the new availability of substan-
tial meeting corpora such as the AMI corpus (Mc-
Cowan et al, 2005), recent years have therefore
seen an increasing amount of research on decision-
making dialog. This research has tackled issues
such as the automatic detection of agreement and
disagreement (Galley et al, 2004), and of the
235
level of involvement of conversational participants
(Gatica-Perez et al, 2005). In addition, (Verbree
et al, 2006) created an argumentation scheme in-
tended to support automatic production of argu-
ment structure diagrams from decision-oriented
meeting transcripts. As yet, there has been rela-
tively little work which specifically addresses the
automatic detection and summarization of deci-
sions.
Decision discussion detection: (Hsueh and
Moore, 2007) used the AMI Meeting Corpus, and
attempted to automatically identify DAs in meet-
ing transcripts which are ?decision-related?. For
each meeting, two manually created summaries
were used to judge which DAs were decision-
related: an extractive summary of the whole meet-
ing, and an abstractive summary of its decisions.
Those DAs in the extractive summary which sup-
port any of the decisions in the abstractive sum-
mary were manually tagged as decision-related.
(Hsueh and Moore, 2007) then trained a Maxi-
mum Entropy classifier to recognize this single
DA class, using a variety of lexical, prosodic, DA
and conversational topic features. They achieved
an F-score of 0.35.
Unlike (Hsueh and Moore, 2007), (Ferna?ndez et
al., 2008b) made an attempt at modelling the struc-
ture of decision-making dialogue. The authors de-
signed an annotation scheme that takes account of
the different roles which utterances can play in the
decision-making process?for example it distin-
guishes between DDAs (decision DAs) which ini-
tiate a discussion by raising an issue, those which
propose a resolution, and those which express
agreement for a proposed resolution. The authors
annotated a portion of the AMI corpus, and then
applied what they refer to as ?hierarchical classi-
fication?. Here, one sub-classifier per DDA class
hypothesizes occurrences of that DDA class, and
then based on these hypotheses, a super-classifier
determines which regions of dialogue are deci-
sion discussions. All of the classifiers, (sub and
super), were linear kernel binary Support Vec-
tor Machines (SVMs). Results were better than
those obtained with (Hsueh and Moore, 2007)?s
approach?the F1-score for detecting decision dis-
cussions in manual transcripts was .58 vs. .50.
Note that (Purver et al, 2007) had previously pur-
sued the same basic approach as (Ferna?ndez et al,
2008b) in order to detect action items.
In this paper, we build on the promising results
of (Ferna?ndez et al, 2008b), by using Directed
Graphical Models (DGMs) in place of SVMs.
DGMs are attractive because they provide a natu-
ral framework for modelling sequence and depen-
dencies between variables including the DDAs.
We are especially interested in whether DGMs
better exploit non-lexical features. (Ferna?ndez et
al., 2008b) obtained much more value from lexi-
cal than non-lexical features (and indeed no value
at all from prosodic features), but lexical features
have disadvantages. In particular, they can be do-
main specific, increase the size of the feature space
dramatically, and deteriorate more than other fea-
tures in quality when ASR is poor.
Decision summarization: Recent years have
seen research on spoken dialogue summarization
(e.g. (Zechner, 2002)). Most has attempted to gen-
erate summaries of full dialogues, but some very
recent research has focused on specific dialogue
events, namely action items (Purver et al, 2007),
and decisions (Ferna?ndez et al, 2008a).
(Ferna?ndez et al, 2008a) used the DDA an-
notation scheme mentioned above, and began by
extracting the DDAs which raise issues or pro-
vide accepted resolutions. Only manual tran-
scripts were used and the DDAs were extracted
by hand rather than automatically. The next step
was to parse each DDA with a general rule-based
parser (Dowding et al, 1993), producing multi-
ple short fragments rather than one full utterance
parse. Then, for each DDA, an SVM regression
model used various features (including parse, se-
mantic and lexical features) to select the fragment
which was most likely to appear in a gold-standard
extractive decision summary. The entire manual
utterance transcriptions were used as the baseline,
and although the SVM?s precision was high, it was
not enough to offset the baseline?s perfect recall,
and so its F-score was lower. The ?Oracle?, which
always chooses the fragment with the highest F1-
score produced very good results. This motivates
deeper investigation into how to improve the frag-
ment/parse selection phase, and so we assess the
usefulness of a semantic-similarity feature for the
SVM. We conduct experiments with ASR as well
as manual transcripts.
3 Data
For the experiments reported in this study, we used
17 meetings from the AMI Meeting Corpus (Mc-
Cowan et al, 2005), a freely available corpus of
236
multi-party meetings with both audio and video
recordings, and a wide range of annotated in-
formation including DAs and topic segmentation.
Conversations are in English, but some partici-
pants are non-native English speakers. The meet-
ings last around 30 minutes each, and are scenario-
driven, wherein four participants play different
roles in a company?s design team: project man-
ager, marketing expert, interface designer and in-
dustrial designer.
3.1 Modelling Decision Discussions
We use the same annotation scheme as (Ferna?ndez
et al, 2008b) to model decision-making dialogue.
As stated in Section 2, this scheme distinguishes
between a small number of DA types based on the
role which they perform in the formulation of a de-
cision. Apart from improving the initial detection
of decision discussions (Ferna?ndez et al, 2008b),
such a scheme also aids their subsequent summa-
rization, because it indicates which utterances con-
tain particular types of information.
The annotation scheme is based on the observa-
tion that a decision discussion contains the follow-
ing main structural components: (a) a topic or is-
sue requiring resolution is raised, (b) one or more
possible resolutions are considered, (c) a particular
resolution is agreed upon and so becomes the de-
cision. Hence the scheme distinguishes between
three main decision dialogue act (DDA) classes:
issue (I), resolution (R), and agreement (A). Class
R is further subdivided into resolution proposal
(RP) and resolution restatement (RR). I utterances
introduce the topic of the decision discussion, ex-
amples being ?Are we going to have a backup??
and ?But would a backup really be necessary??
in Dialogue 1. On the other hand, R utterances
specify the resolution which is ultimately adopted
as the decision. RP utterances propose this reso-
lution (e.g. ?I think maybe we could just go for
the kinetic energy. . . ?), while RR utterances close
the discussion by confirming/summarizing the de-
cision (e.g. ?Okay, fully kinetic energy?) . Finally,
A utterances agree with the proposed resolution,
signalling that it is adopted as the decision, (e.g.
?Yeah?, ?Good? and ?Okay?). Note that an utter-
ance can be assigned to more than one DDA class,
and within a decision discussion, more than one
utterance can be assigned to the same DDA class.
We use both manual and ASR one-best tran-
scripts1 in the experiments described here. DDA
annotations were first made on the manual tran-
scripts, and then transferred onto the ASR tran-
scripts. Inter-annotator agreement was satisfac-
tory, with kappa values ranging from .63 to .73 for
the four DDA classes. Due to different segmen-
tation, the manual and ASR transcripts contain a
total of 15,680 and 8,357 utterances respectively,
and on average, 40 and 33 DDAs per meeting.
Hence DDAs are slightly less sparse in the ASR
transcripts: for all DDAs, 6.7% vs. 4.3% of the to-
tal number of utterances, for I, 1.6% vs. 0.9%, for
RP, 2% vs. 1%, for RR, 0.5% vs. 0.4%, and for A,
2.6% vs. 2%.
(1) A: Are we going to have a backup? Or we do
just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.2
4 Decision Discussion Detection using
Directed Graphical Models
A directed graphical model (DGM) M, (see Mur-
phy (2002)), is a directed acyclic graph consisting
of nodes which represent random variables, arcs
which represent dependencies among these vari-
ables, and a probability distribution P over the
variables. Let X = {X1, X2, ..., Xn} be a set of
random variables that are associated with nodes in
a DGM and Pa(Xi) be parents of Xi. The proba-
bility distribution of the model M satisfies:
P (X1, X2, ..., Xn) =
n?
i=1
(P (Xi)|Pa(Xi))
When a DGM is used as a classifier, the goal is to
correctly infer the value of the class node Xc ? X
given a vector of values for the observed node(s)
1We used SRI?s Decipher for which (Stolcke et al, 2008)
reports a word error rate of 26.9% on AMI meetings.
2This example was extracted from the AMI dialogue
ES2015c and has been modified slightly for presentation pur-
poses.
237
Xo ? X\Xc. This is done by using M to find the
value of Xc which gives the highest conditional
probability P (Xc|Xo).
To detect each individual DDA class, we ex-
amined the four simple DGMs in Figure 1 (see
Appendix). The DDA node is binary where
value 1 indicates the presence of a DDA and 0
its absence. The evidence node (E) is a multi-
dimensional vector of observed values of non-
lexical features. These include utterance features
(UTT) such as length in words, duration in mil-
liseconds, position within the meeting (as percent-
age of elapsed time), manually annotated dialogue
act (DA) features3 such as inform, assess, suggest,
and prosodic features (PROS) such as energy and
pitch. These features are the same as the non-
lexical features used by Ferna?ndez et al (2008b).
The hidden component node (C) represents the
distribution of observable evidence E as a single
Gaussian in the -sim models, and a mixture in the
-mix models. For the -mix models, the number
of Gaussian components is hand-tuned during the
training phase.
More complex models are constructed from the
four simple models in Figure 1 to allow for depen-
dencies between different DDAs. For example, the
model in Figure 2 (see Appendix) generalizes Fig-
ure 1c with arcs connecting the DDA classes based
on analysis of the annotated AMI data.
4.1 Experiments
The DGM classifiers in Figures 1 and 2 were im-
plemented in Matlab using the BNT software4.
Since the current BNT version does not sup-
port multiple time series training for fully observ-
able Dynamic Bayesian Networks (DBNs), we ex-
tended the software for training models using this
structure (e.g., Figure 1c and Figure 2).
A DGM classifier is considered to have hy-
pothesized a DDA if the marginal probability of
its DDA node is above a hand-tuned threshold.
We tested the DGMs on manual and ASR tran-
scripts in a 17-fold cross-validation, and evaluated
their performance on both a per-utterance basis,
and also with the same lenient-match metric as
Ferna?ndez et al (2008b). This allows a margin
of 20 seconds preceding and following a hypoth-
esized DDA, and so we refer to it as the 40 sec-
ond metric. In addition, we hypothesized decision
3We use the AMI DA annotations. These are only avail-
able for manual transcripts.
4http://www.cs.ubc.ca/?murphyk/Software/BNT/bnt.html
discussion regions using the DGM output and the
following two simple rules:
? A decision discussion region begins with an
Issue DDA.
? A decision discussion region contains at least
one Issue DDA and one Resolution DDA.
To evaluate the accuracy of these hypothesized re-
gions, like Ferna?ndez et al (2008b), we divided
the dialogue into 30-second windows and evalu-
ated on a per window basis.
4.2 Results
Tables 1 and 2 show the F1-scores for each
DGM when using the best feature sets (I:
UTT+DA+PROS, RP: UTT+DA, RR: UTT, A:
UTT+DA). The BN-mix model gives the highest
F1-score for A on both evaluation metrics, and the
DBN-mix model, the highest for I, RP, and RR,
but there are no statistically significant differences
between any of the alternative DGMs.
Classifier I RP RR A
BN-mix .09 .09 .04 .19
DBN-mix .16 .14 .05 .17
BN-sim .12 .09 .04 .17
DBN-sim .15 .11 .04 .16
Table 1: F1-score (per utterance) of the DGMs us-
ing the best combination of non-lexical features.
Classifier I RP RR A
BN-mix .19 .24 .07 .38
DBN-mix .27 .24 .07 .32
BN-sim .23 .22 .06 .36
DBN-sim .25 .22 .06 .31
Table 2: F1-score (40 seconds) of the DGMs using
the best combination of non-lexical features.
To determine whether modeling dependencies
between DDAs improves performance, we exper-
imented with the DGMs that are generalized from
the DBN-sim (Figure 2) and DBN-mix models.
The F1-scores did not improve for I, RP, and RR,
while for A, the DGM generalized from DBN-sim
gave a .03 improvement according to the 40 sec-
onds metric, but this was not statistically signifi-
cant.
For each DDA, Table 3 compares the results of
the best DGM and the hierarchical SVM classi-
fication method of Ferna?ndez et al (2008b) (see
238
Section 2). The DGM performs better for all
DDAs on both evaluation metrics (p < 0.005).
Note that while prosodic features proved useless
to SVM classifiers (Ferna?ndez et al (2008b)), with
DGMs, they have some predictive power.
Per utterance 40 seconds
Classifier DDA Pr Re F1 Pr Re F1
SVM I .03 .62 .05 .04 .89 .08
DGM .11 .28 .16 .20 .44 .27
SVM RP .03 .60 .07 .05 .90 .10
DGM .09 .35 .14 .16 .57 .24
SVM RR .01 .49 .02 .01 .80 .03
DGM .02 .42 .05 .04 .58 .07
SVM A .05 .70 .10 .07 .90 .13
DGM .13 .31 .19 .29 .55 .38
Table 3: Performance of the DGM classifier vs.
the SVM classifier. Both use the best combination
of non-lexical features.
We also generated results without DA features.
Here, the best F1-scores for I, RP, and A degrade
between .07 and .09 (p < 0.05), but they are still
higher than the equivalent SVM results with DA
features. Since (Ferna?ndez et al, 2008b) report
that lexical features are the most useful for the
SVM classifiers, it will be interesting to see how
well the DGMs perform when they use lexical as
well as non-lexical features.
Detecting DDAs in ASR transcripts: Table 4
compares the DGM F1-scores when using ASR
one-best and manual transcripts. The DGMs per-
form well on ASR output. For I and RP, the results
on ASR are actually higher, perhaps because the
DDAs are less sparse. In the absence of DA fea-
tures, prosodic features improve the performance
for A in both sources.
UTT UTT+PROS
I RP RR A I RP RR A
ASR .20 .21 .06 .24 .16 .24 .07 .28
Man .18 .17 .07 .27 .16 .15 .05 .30
Table 4: F1-scores (40 seconds) computed using
ASR one-best vs. manual transcriptions.
Detecting decision discussion regions: Table 5
shows that according to the 30-second window
metric, rule-based classification with DGM output
compares well with hierarchical SVM classifica-
tion (Ferna?ndez et al, 2008b). In fact, even when
the latter uses lexical as well as non-lexical fea-
tures, its F1-score is still about the same as the
DGM-based classifier. Our future work will in-
volve dispensing with the rule-based approach and
designing a DGM which can detect decision dis-
cussion regions.
Classifier Pr Re F1
SVM .35 .88 .50
DGM .39 .93 .55
Table 5: Results in detecting decision discussion
regions for the SVM super-classifier and rule-
based DGM classifier, both using the best com-
bination of non-lexical features.
5 Decision Summarization
We now turn to the task of extracting useful
phrases for summarization. Since a summary of a
decision discussion should minimally contain the
issue under discussion, and its resolution, we leave
Agreement (A) utterances aside, and concentrate
on extracting phrases from Issues (I) and Resolu-
tions (R).
Our basic approach is the same taken in
(Ferna?ndez et al, 2008a): The WCN5 of each I
and R utterance is parsed by the Gemini parser
(Dowding et al, 1993) to produce multiple short
fragments, and then an SVM regression model
uses certain features in order to select the parse
that is most likely to match a gold-standard extrac-
tive summary. Our work is new in two respects:
summarizing from ASR output in addition to man-
ual transcriptions, and using a semantic-similarity
feature in the SVM. This new feature is generated
using Ted Pedersen?s semantic-similarity package
(Pedersen, 2002), and is motivated by the fact that
ordinarily the Issue summary should be semanti-
cally similar to the Resolution and vice versa.
The next section describes the lexical resources
used by Gemini, and Section 5.2, the metric for
calculating semantic similarity.
5.1 Open-Domain Semantic Parser
Since human-human spoken dialogue, especially
after being processed by an imperfect recognizer,
is likely to be highly ungrammatical, we have de-
veloped a semantic parser that only attempts to
find basic predicate-argument structures of the ma-
jor phrase types (S, VP, NP, and PP) and has access
to a broad-coverage lexicon. To build a broad-
coverage lexicon, we used publicly available lex-
ical resources for English, including COMLEX,
5When using manual transcripts, we create ?dummy
WCNs?: WCNs with a single path.
239
VerbNet, WordNet, and NOMLEX.
COMLEX provides detailed syntactic informa-
tion for the 40k most common words of En-
glish, and VerbNet, detailed semantic information
for verbs, including verb class, verb frames, the-
matic roles, mappings of syntactic position to the-
matic roles, and selection restrictions on thematic
role fillers. From WordNet we extracted another
15K nouns and the semantic class information for
all nouns. These semantic classes were hand-
aligned to the selectional classes used in Verb-
Net, based on the upper ontology of EuroWord-
Net. NOMLEX provides syntactic information for
event nominalizations, and information for map-
ping the noun arguments to the corresponding verb
syntactic positions.
These resources were combined and converted
to the Prolog-based format used in the Gemini
framework, which includes a fast bottom-up ro-
bust parser in which syntactic and semantic in-
formation is applied interleaved. Gemini can
compute parse probabilities on the context-free
skeleton of the grammar. In the experiments de-
scribed here these parse probabilities are trained
on Switchboard tree-bank data.
5.2 Semantic Similarity Metric: Normalized
Path Length
Ted Pedersen?s semantic similarity package (Ped-
ersen, 2002) can be used to apply a number of
different metrics that use WordNet as a knowl-
edge base. The metric used here, Normalized Path
Length (Leacock and Chodorow, 1998), defines
the semantic similarity sim between words w1 and
w2 as:
simc1,c2 = ? log
len(c1, c2)
2?D (1)
where c1 and c2 are concepts corresponding to w1
and w2, len(c1, c2) is the length of the shortest
path between them, and D is the maximum depth
of the taxonomy.
5.3 Experiments
Data: For the manual transcripts in our sub-
corpus, the average length in words of I and R ut-
terances is 12.2 and 11.9 respectively, and for the
ASR, 22.4 and 18.1. To provide a gold-standard,
phrases from I and R utterances in the man-
ual transcriptions were annotated as summary-
worthy. The aim was to select those phrases
which should appear in an extractive summary, or
could be the basis of a generated abstractive sum-
mary. As a general guideline, we tried to select
the phrase(s) which describe the issue/resolution
as succinctly as possible. This does not include
phrases which express the speaker?s attitude to-
wards the issue/resolution. Dialogue 2 is an exam-
ple where square brackets indicate which phrases
were selected as summary-worthy.
(2) A:(I) So we we?re looking at [sliders for both
volume and channel change]
B:(R)I was thinking kind of [just for the
volume]
Regression models: We use SVMlight
(Joachims, 1999) to learn separate SVM re-
gression models for Issues and Resolutions.
These rank the Gemini parses for each utterance
according to their likelihood of matching the
gold-standard summary. The top-ranked parse
is then entered into the automatically-generated
decision summary.
Features: We train the regression models with
various types of feature (see Table 6), including
properties of the WCN paths, parse, semantic and
lexical features. As lexical features are likely to be
more domain-specific, and they dramatically in-
crease size of the feature space, we prefer to avoid
them if possible.
To generate the semantic-similarity feature for
an I/R parse, we compute its semantic similarity
with the full transcripts of each of the R/I utter-
ances within the same decision discussion. The
feature?s value is then equal to the greatest of the
resulting semantic-similarity scores. Since Ted
Pedersen?s package operates on the noun portion
of WordNet, we must first extract all of the nouns
in the parse/utterance transcription. Next, we form
all of the possible pairs containing one noun from
the parse, and one from the utterance transcrip-
tion. Then we compute the semantic similarity
for each pair, and take their sum to be the level
of semantic similarity between the parse and the
utterance transcription. We experimented with av-
eraging rather than summing these scores, but the
resulting semantic-similarity feature was less pre-
dictive.
Evaluation: The models are evaluated in 10-
fold cross-validations using the same metric as
(Ferna?ndez et al, 2008a): Recall is the total pro-
portion of the gold-standard extractive summary
240
WCN phrase length (WCN arcs)
start/end point (absolute & percentage)
Parse parse probability
phrase type (S/VP/NP/PP)
Semantic main verb VerbNet class
head noun WordNet synset
Sem-sim Normalized Path Length
Lexical main verb, head noun
Table 6: Features for parse fragment ranking
Issue Resolution
Re Pr F1 Re Pr F1
Baseline 1.0 .50 .67 1.0 .60 .75
Oracle .77 .96 .85 .74 .99 .84
WCN,parse,sem .63 .69 .66 .61 .66 .64
+ sem-sim .65 .71 .68 .64 .69 .67
+ lexical .65 .67 .66 .65 .70 .67
Table 7: Parse ranking results for I & R Utterances
using manual transcriptions.
covered by the selected parse; precision is the to-
tal proportion of the chosen parse which overlaps
with the gold-standard summary. The baseline is
the entire transcription, and we also compare to
an ?oracle? that always chooses a parse with the
highest F1-score. Note that we use the extractive
summaries from the manual transcriptions as the
gold-standard for the evaluation of the results ob-
tained with ASR.
Results and analysis: Results with manual tran-
scriptions are shown in Table 7, and those with
ASR, in Table 8. In all cases, when starting with
a feature set containing WCN, parse and seman-
tic features, the F1-score is improved by adding
the semantic-similarity feature. For Issues, the F1-
score improves from .66 to .68 with manual tran-
scripts, and from .30 to .32 with ASR. The im-
provements for Resolutions are highly significant:
with manual transcripts, the F1 score increases
from .64 to .67 (p < 0.005), and with ASR, from
.33 to .37 (p < 0.005). Note that the further addi-
tion of lexical features only produces a significant
improvement in the case of I summarization with
ASR.
Compared to the full transcript baseline, we
achieve higher F1-scores for Issues?.68 vs. .67
with manual transcriptions, and .35 vs. .31 with
ASR?but slightly lower for Resolutions. There
remains a fairly large gap between our best scores
and their corresponding oracles (especially with
ASR), and so there may still be potential for sub-
stantial improvement.
Issue Resolution
Re Pr F1 Re Pr F1
Baseline .77 .20 .31 .80 .27 .40
Oracle .61 .87 .72 .59 .91 .72
WCN,parse,sem .28 .33 .30 .31 .35 .33
+ sem-sim .30 .34 .32 .35 .38 .36
+ lexical .35 .35 .35 .34 .39 .37
Table 8: Parse ranking results for I & R Utterances
using ASR.
6 Conclusions and Future Work
This paper has presented work on the detec-
tion and summarization of decision discussions
in multi-party dialogue. In the detection experi-
ments, we investigated the use of directed graph-
ical models (DGMs), and found that when us-
ing non-lexical features, the DGMs outperform
the hierarchical SVM classification method of
Ferna?ndez et al (2008b). The F1-score for the
four DDA classes increased between .04 and .19
(p < .005), and for identifying decision discus-
sion regions, by .05. This is encouraging because
lexical features have disadvantages?for example
they can be domain specific and greatly increase
the feature space. In addition, modelling the de-
pendencies between the DDA classes increased
performance for Agreement utterances, and the
DGMs were robust to ASR.
In the summarization experiments, we sum-
marized decision discussions by extracting key
words/phrases from their Issue (I) and Resolu-
tion (R) utterances. Each utterance?s Word Confu-
sion Network was parsed with an open-domain se-
mantic parser, thus producing multiple candidate
phrases, and then an SVM regression model se-
lected one of these phrases to enter into the sum-
mary. The experiments here investigated the use-
fulness of a new SVM feature which measures the
level of semantic similarity between candidate I
parses and R utterances, and vice-versa. This fea-
ture was generated with a semantic-similarity met-
ric which uses WordNet as a knowledge source.
It was found to improve performance with both
manual transcripts and ASR, and for R summa-
rization, the improvements were highly significant
(p < .005).
In future work, we plan to integrate lexical fea-
tures into our DGMs by using a switching Dy-
namic Bayesian Network similar to that reported
in (Ji and Bilmes, 2005). We also plan to extend
the decision discussion annotation scheme so that
we can try to automatically extract supporting ar-
241
guments for decisions.
Acknowledgements This material is based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. FA8750-07-D-0185/0004, and by the
Department of the Navy Office of Naval Research
(ONR) under Grants No. N00014-05-1-0187 and
N00014-09-1-0106. Any opinions, findings and
conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of DARPA or ONR.
References
Satanjeev Banerjee, Carolyn Rose?, and Alex Rudnicky.
2005. The necessity of a meeting recording and
playback system, and the benefit of topic-level anno-
tations to meeting browsing. In Proceedings of the
10th International Conference on Human-Computer
Interaction.
John Dowding, Jean Mark Gawron, Doug Appelt, John
Bear, Lynn Cherny, Robert Moore, and Douglas
Moran. 1993. GEMINI: a natural language system
for spoken-language understanding. In Proceedings
of the 31st Annual Meeting of the Association for
Computational Linguistics (ACL).
Raquel Ferna?ndez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Pe-
ters. 2008a. Identifying relevant phrases to summa-
rize decisions in spoken meetings. In Proceedings
of Interspeech.
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008b. Mod-
elling and detecting decisions in multi-party dia-
logue. In Proceedings of the 9th SIGdial Workshop
on Discourse and Dialogue.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Daniel Gatica-Perez, Ian McCowan, Dong Zhang, and
Samy Bengio. 2005. Detecting group interest level
in meetings. In Proceedings of ICASSP.
Pey-Yun Hsueh and Johanna Moore. 2007. Automatic
decision detection in meeting speech. In Proceed-
ings of MLMI 2007, Lecture Notes in Computer Sci-
ence. Springer-Verlag.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Marc??as-Guarasa,
Nelson Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI meeting project: Resources and re-
search. In Proceedings of the 2004 ICASSP NIST
Meeting Recognition Workshop.
Gang Ji and Jeff Bilmes. 2005. Dialog act tagging
using graphical models. In Proceedings of ICASSP.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods ?
Support Vector Learning. MIT Press.
Claudia Leacock and Martin Chodorow, 1998. Word-
Net: An Electronic Lexical Database, chapter Com-
bining local context and WordNet similarity for
word sense identification. University of Chicago
Press.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Kevin Murphy. 2002. Dynamic Bayesian Networks:
Representation, Inference and Learning. Ph.D. the-
sis, University of California Berkeley.
Ted Pedersen. 2002. Semantic similarity package.
http:/www.d.umn.edu/ tpederse/similarity.
Matthew Purver, John Dowding, John Niekrasz,
Patrick Ehlen, Sharareh Noorbaloochi, and Stanley
Peters. 2007. Detecting and summarizing action
items in multi-party dialogue. In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
Andreas Stolcke, Xavier Anguera, Kofi Boakye, O?zgu?r
C?etin, Adam Janin, Matthew Magimai-Doss, Chuck
Wooters, and Jing Zheng. 2008. The ICSI-SRI
spring 2007 meeting and lecture recognition system.
In Proceedings of CLEAR 2007 and RT2007.
Daan Verbree, Rutger Rienks, and Dirk Heylen. 2006.
First steps towards the automatic construction of
argument-diagrams from real discussions. In Pro-
ceedings of the 1st International Conference on
Computational Models of Argument, volume 144,
pages 183?194. IOS press.
Steve Whittaker, Rachel Laban, and Simon Tucker.
2006. Analysing meeting records: An ethnographic
study and technological implications. In S. Renals
and S. Bengio, editors, Machine Learning for Multi-
modal Interaction: Second International Workshop,
MLMI 2005, Revised Selected Papers, volume 3869
of Lecture Notes in Computer Science, pages 101?
113. Springer.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Computational Linguistics, 28(4):447?485.
242
Appendix
DDA
E
a) BN-sim
DDA
E
b) BN-mix
C
DDA
time t-1 time t
E
DDA
E
c) DBN-sim
DDA
time t-1 time t
E
DDA
E
d) DBN-mix
CC
Figure 1: Simple DGMs for individual decision
detection. During training, the shaded nodes are
hidden, and the clear nodes are observable.
A
time t-1 time t
E
A
E
I I
RP RP
RR RR
Figure 2: A DGM that takes the dependencies be-
tween decisions into account.
243
Proceedings of the ACL 2010 Conference Short Papers, pages 307?312,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Decision detection using hierarchical graphical models
Trung H. Bui
CSLI
Stanford University
Stanford, CA 94305, USA
thbui@stanford.edu
Stanley Peters
CSLI
Stanford University
Stanford, CA 94305, USA
peters@csli.stanford.edu
Abstract
We investigate hierarchical graphical
models (HGMs) for automatically detect-
ing decisions in multi-party discussions.
Several types of dialogue act (DA) are
distinguished on the basis of their roles in
formulating decisions. HGMs enable us
to model dependencies between observed
features of discussions, decision DAs, and
subdialogues that result in a decision. For
the task of detecting decision regions, an
HGM classifier was found to outperform
non-hierarchical graphical models and
support vector machines, raising the
F1-score to 0.80 from 0.55.
1 Introduction
In work environments, people share information
and make decisions in multi-party conversations
known as meetings. The demand for systems that
can automatically process information contained
in audio and video recordings of meetings is grow-
ing rapidly. Our own research, and that of other
contemporary projects (Janin et al, 2004) aim at
meeting this demand.
We are currently investigating the automatic de-
tection of decision discussions. Our approach in-
volves distinguishing between different dialogue
act (DA) types based on their role in the decision-
making process. These DA types are called De-
cision Dialogue Acts (DDAs). Groups of DDAs
combine to form a decision region.
Recent work (Bui et al, 2009) showed that
Directed Graphical Models (DGMs) outperform
other machine learning techniques such as Sup-
port Vector Machines (SVMs) for detecting in-
dividual DDAs. However, the proposed mod-
els, which were non-hierarchical, did not signifi-
cantly improve identification of decision regions.
This paper tests whether giving DGMs hierarchi-
cal structure (making them HGMs) can improve
their performance at this task compared with non-
hierarchical DGMs.
We proceed as follows. Section 2 discusses re-
lated work, and section 3 our data set and anno-
tation scheme for decision discussions. Section
4 summarizes previous decision detection exper-
iments using DGMs. Section 5 presents the HGM
approach, and section 6 describes our HGM exper-
iments. Finally, section 7 draws conclusions and
presents ideas for future work.
2 Related work
User studies (Banerjee et al, 2005) have con-
firmed that meeting participants consider deci-
sions to be one of the most important meeting
outputs, and Whittaker et al (2006) found that
the development of an automatic decision de-
tection component is critical for re-using meet-
ing archives. With the new availability of sub-
stantial meeting corpora such as the AMI cor-
pus (McCowan et al, 2005), recent years have
seen an increasing amount of research on decision-
making dialogue. This research has tackled is-
sues such as the automatic detection of agreement
and disagreement (Galley et al, 2004), and of
the level of involvement of conversational partic-
ipants (Gatica-Perez et al, 2005). Recent work
on automatic detection of decisions has been con-
ducted by Hsueh and Moore (2007), Ferna?ndez et
al. (2008), and Bui et al (2009).
Ferna?ndez et al (2008) proposed an approach
to modeling the structure of decision-making di-
alogue. These authors designed an annotation
scheme that takes account of the different roles
that utterances can play in the decision-making
process?for example it distinguishes between
DDAs that initiate a decision discussion by rais-
ing an issue, those that propose a resolution of the
issue, and those that express agreement to a pro-
posed resolution. The authors annotated a por-
tion of the AMI corpus, and then applied what
307
they refer to as ?hierarchical classification.? Here,
one sub-classifier per DDA class hypothesizes oc-
currences of that type of DDA and then, based
on these hypotheses, a super-classifier determines
which regions of dialogue are decision discus-
sions. All of the classifiers, (sub and super), were
linear kernel binary SVMs. Results were bet-
ter than those obtained with (Hsueh and Moore,
2007)?s approach?the F1-score for detecting de-
cision discussions in manual transcripts was 0.58
vs. 0.50. Purver et al (2007) had earlier detected
action items with the approach Ferna?ndez et al
(2008) extended to decisions.
Bui et al (2009) built on the promising results
of (Ferna?ndez et al, 2008), by employing DGMs
in place of SVMs. DGMs are attractive because
they provide a natural framework for modeling se-
quence and dependencies between variables, in-
cluding the DDAs. Bui et al (2009) were espe-
cially interested in whether DGMs better exploit
non-lexical features. Ferna?ndez et al (2008) ob-
tained much more value from lexical than non-
lexical features (and indeed no value at all from
prosodic features), but lexical features have limi-
tations. In particular, they can be domain specific,
increase the size of the feature space dramatically,
and deteriorate more in quality than other features
when automatic speech recognition (ASR) is poor.
More detail about decision detection using DGMs
will be presented in section 4.
Beyond decision detection, DGMs are used for
labeling and segmenting sequences of observa-
tions in many different fields?including bioin-
formatics, ASR, Natural Language Processing
(NLP), and information extraction. In particular,
Dynamic Bayesian Networks (DBNs) are a pop-
ular model for probabilistic sequence modeling
because they exploit structure in the problem to
compactly represent distributions over multi-state
and observation variables. Hidden Markov Mod-
els (HMMs), a special case of DBNs, are a classi-
cal method for important NLP applications such
as unsupervised part-of-speech tagging (Gael et
al., 2009) and grammar induction (Johnson et al,
2007) as well as for ASR. More complex DBNs
have been used for applications such as DA recog-
nition (Crook et al, 2009) and activity recogni-
tion (Bui et al, 2002).
Undirected graphical models (UGMs) are also
valuable for building probabilistic models for seg-
menting and labeling sequence data. Conditional
Random Fields (CRFs), a simple UGM case, can
avoid the label bias problem (Lafferty et al, 2001)
and outperform maximum entropy Markov mod-
els and HMMs.
However, the graphical models used in these
applications are mainly non-hierarchical, includ-
ing those in Bui et al (2009). Only Sutton et al
(2007) proposed a three-level HGM (in the form of
a dynamic CRF) for the joint noun phrase chunk-
ing and part of speech labeling problem; they
showed that this model performs better than a non-
hierarchical counterpart.
3 Data
For the experiments reported in this study, we
used 17 meetings from the AMI Meeting Corpus1,
a freely available corpus of multi-party meetings
with both audio and video recordings, and a wide
range of annotated information including DAs and
topic segmentation. The meetings last around 30
minutes each, and are scenario-driven, wherein
four participants play different roles in a com-
pany?s design team: project manager, marketing
expert, interface designer and industrial designer.
We use the same annotation scheme as
Ferna?ndez et al (2008) to model decision-making
dialogue. As stated in section 2, this scheme dis-
tinguishes between a small number of DA types
based on the role which they perform in the for-
mulation of a decision. Besides improving the de-
tection of decision discussions (Ferna?ndez et al,
2008), such a scheme also aids in summarization
of them, because it indicates which utterances pro-
vide particular types of information.
The annotation scheme is based on the observa-
tion that a decision discussion typically contains
the following main structural components: (a) A
topic or issue requiring resolution is raised; (b)
One or more possible resolutions are considered;
(c) A particular resolution is agreed upon, and so
adopted as the decision. Hence the scheme dis-
tinguishes between three main DDA classes: issue
(I), resolution (R), and agreement (A). Class R is
further subdivided into resolution proposal (RP)
and resolution restatement (RR). I utterances in-
troduce the topic of the decision discussion, ex-
amples being ?Are we going to have a backup??
and ?But would a backup really be necessary?? in
Table 1. In comparison, R utterances specify the
resolution which is ultimately adopted as the deci-
1http://corpus.amiproject.org/
308
(1) A: Are we going to have a backup? Or we do
just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.
Table 1: An excerpt from the AMI dialogue
ES2015c. It has been modified slightly for pre-
sentation purposes.
sion. RP utterances propose this resolution (e.g. ?I
think maybe we could just go for the kinetic energy
. . . ?), while RR utterances close the discussion by
confirming/summarizing the decision (e.g. ?Okay,
fully kinetic energy?). Finally, A utterances agree
with the proposed resolution, signaling that it is
adopted as the decision, (e.g. ?Yeah?, ?Good? and
?Okay?). Unsurprisingly, an utterance may be as-
signed to more than one DDA class; and within a
decision discussion, more than one utterance can
be assigned to the same DDA class.
We use manual transcripts in the experiments
described here. Inter-annotator agreement was sat-
isfactory, with kappa values ranging from .63 to
.73 for the four DDA classes. The manual tran-
scripts contain a total of 15,680 utterances, and on
average 40 DDAs per meeting. DDAs are sparse
in the transcripts: for all DDAs, 6.7% of the total-
ity of utterances; for I,1.6%; for RP, 2%; for RR,
0.5%; and for A, 2.6%. In all, 3753 utterances (i.e.,
23.9%) are tagged as decision-related utterances,
and on average there are 221 decision-related ut-
terances per meeting.
4 Prior Work on Decision Detection
using Graphical Models
To detect each individual DDA class, Bui et al
(2009) examined the four simple DGMs shown
in Fig. 1. The DDA node is binary valued, with
value 1 indicating the presence of a DDA and 0
its absence. The evidence node (E) is a multi-
dimensional vector of observed values of non-
lexical features. These include utterance features
(UTT) such as length in words2, duration in mil-
liseconds, position within the meeting (as percent-
age of elapsed time), manually annotated dialogue
act (DA) features3 such as inform, assess, suggest,
and prosodic features (PROS) such as energy and
pitch. These features are the same as the non-
lexical features used by Ferna?ndez et al (2008).
The hidden component node (C) in the -mix mod-
els represents the distribution of observable evi-
dence E as a mixture of Gaussian distributions.
The number of Gaussian components was hand-
tuned during the training phase.
DDA
E
a) BN-sim
DDA
E
b) BN-mix
C
DDA
time t-1 time t
E
DDA
E
c) DBN-sim
DDA
time t-1 time t
E
DDA
E
d) DBN-mix
CC
Figure 1: Simple DGMs for individual decision
dialogue act detection. The clear nodes are hidden,
and the shaded nodes are observable.
More complex models were constructed from
the four simple models in Fig. 1 to allow for de-
pendencies between different DDAs. For exam-
ple, the model in Fig. 2 generalizes Fig. 1c with
arcs connecting the DDA classes based on analy-
sis of the annotated AMI data.
Atime t-1 time t
E E
I RP RR AI RP RR
Figure 2: A DGM that takes the dependencies be-
tween decision dialogue acts into account.
Decision discussion regions were identified us-
ing the DGM output and the following two simple
rules: (1) A decision discussion region begins with
an Issue DDA; (2) A decision discussion region
contains at least one Issue DDA and one Resolu-
tion DDA.
2This feature is a manual count of lexical tokens; but word
count was extracted automatically from ASR output by Bui
et al (2009). We plan experiments to determine how much
using ASR output degrades detection of decision regions.
3The authors used the AMI DA annotations.
309
The authors conducted experiments using the
AMI corpus and found that when using non-
lexical features, the DGMs outperform the hierar-
chical SVM classification method of (Ferna?ndez et
al., 2008). The F1-score for the four DDA classes
increased between 0.04 and 0.19 (p < 0.005),
and for identifying decision discussion regions, by
0.05 (p > 0.05).
5 Hierarchical graphical models
Although the results just discussed showed graph-
ical models are better than SVMs for detecting de-
cision dialogue acts (Bui et al, 2009), two-level
graphical models like those shown in Figs. 1 and 2
cannot exploit dependencies between high-level
discourse items such as decision discussions and
DDAs; and the ?superclassifier? rule (Bui et al,
2009) used for detecting decision regions did not
significantly improve the F1-score for decisions.
We thus investigate whether HGMs (structured
as three or more levels) are superior for discov-
ering the structure and learning the parameters
of decision recognition. Our approach composes
graphical models to increase hierarchy with an ad-
ditional level above or below previous ones, or in-
serts a new level such as for discourse topics into
the interior of a given model.
Fig. 3 shows a simple structure for three-level
HGMs. The top level corresponds to high-level
discourse regions such as decision discussions.
The segmentation into these regions is represented
in terms of a random variable (at each DR node)
that takes on discrete values: {positive, negative}
(the utterance belongs to a decision region or not)
or {begin, middle, end, outside} (indicating the
position of the utterance relative to a decision dis-
cussion region). The middle level corresponds to
mid-level discourse items such as issues, resolu-
tion proposals, resolution restatements, and agree-
ments. These classes (C1, C2, ..., Cn nodes) are
represented as a collection of random variables,
each corresponding to an individual mid-level ut-
terance class. For example, the middle level of the
three-level HGM Fig. 3 could be the top-level of
the two-level DGM in Fig. 2, each middle level
node containing random variables for the DDA
classes I, RP, RR, and A. The bottom level cor-
responds to vectors of observed features as before,
e.g. lexical, utterance, and prosodic features.
CnC
Cn
C
DR DR
C1
E ELevel 1
Level 2
Level 3
current utterance next utterance
C1
Figure 3: A simple structure of a three-level
HGM: DRs are high-level discourse regions;
C1, C2, ..., Cn are mid-level utterance classes; and
Es are vectors of observed features.
6 Experiments
The HGM classifier in Figure 3 was implemented
in Matlab using the BNT software4. The classifier
hypothesizes that an utterance belongs to a deci-
sion region if the marginal probability of the ut-
terance?s DR node is above a hand-tuned thresh-
old. The threshold is selected using the ROC curve
analysis5 to obtain the highest F1-score. To evalu-
ate the accuracy of hypothesized decision regions,
we divided the dialogue into 30-second windows
and evaluated on a per window basis.
The best model structure was selected by com-
paring the performance of various handcrafted
structures. For example, the model in Fig. 4b out-
performs the one in Fig. 4a. Fig. 4b explicitly
models the dependency between the decision re-
gions and the observed features.
I RP RR A
DR
E
I RP RR A
DR
E
a) b)
Figure 4: Three-level HGMs for recognition of de-
cisions. This illustrates the choice of the structure
for each time slice of the HGM sequence models.
Table 2 shows the results of 17-fold cross-
validation for the hierarchical SVM classifica-
tion (Ferna?ndez et al, 2008), rule-based classifi-
cation with DGM output (Bui et al, 2009), and
our HGM classification using the best combina-
tion of non-lexical features. All three methods
4http://www.cs.ubc.ca/?murphyk/Software/BNT/bnt.html
5http://en.wikipedia.org/wiki/Receiver operating characteristic
310
were implemented by us using exactly the same
data and 17-fold cross-validation. The features
were selected based on the best combination of
non-lexical features for each method. The HGM
classifier outperforms both its SVM and DGM
counterparts (p < 0.0001)6. In fact, even when the
SVM uses lexical as well as non-lexical features,
its F1-score is still lower than the HGM classifier.
Classifier Pr Re F1
SVM 0.35 0.88 0.50
DGM 0.39 0.93 0.55
HGM 0.69 0.96 0.80
Table 2: Results for detection of decision dis-
cussion regions by the SVM super-classifier,
rule-based DGM classifier, and HGM clas-
sifier, each using its best combination of
non-lexical features: SVM (UTT+DA), DGM
(UTT+DA+PROS), HGM (UTT+DA).
In contrast with the hierarchical SVM and rule-
based DGM methods, the HGM method identifies
decision-related utterances by exploiting not just
DDAs but also direct dependencies between deci-
sion regions and UTT, DA, and PROS features. As
mentioned in the second paragraph of this section,
explicitly modeling the dependency between deci-
sion regions and observable features helps to im-
prove detection of decision regions. Furthermore,
a three-level HGM can straightforwardly model
the composition of each high-level decision region
as a sequence of mid-level DDA utterances. While
the hierarchical SVM method can also take depen-
dency between successive utterances into account,
it has no principled way to associate this depen-
dency with more extended decision regions. In
addition, this dependency is only meaningful for
lexical features (Ferna?ndez et al, 2008).
The HGM result presented in Table 2 was
computed using the three-level DBN model (see
Fig. 4b) using the combination of UTT and DA
features. Without DA features, the F1-score de-
grades from 0.8 to 0.78. However, this difference
is not statistically significant (i.e., p > 0.5).
7 Conclusions and Future Work
To detect decision discussions in multi-party dia-
logue, we investigated HGMs as an extension of
6We used the paired t test for computing statistical signif-
icance. http://www.graphpad.com/quickcalcs/ttest1.cfm
the DGMs studied in (Bui et al, 2009). When
using non-lexical features, HGMs outperform the
non-hierarchical DGMs of (Bui et al, 2009) and
also the hierarchical SVM classification method
of Ferna?ndez et al (2008). The F1-score for
identifying decision discussion regions increased
to 0.80 from 0.55 and 0.50 respectively (p <
0.0001).
In future work, we plan to (a) investigate cas-
caded learning methods (Sutton et al, 2007) to
improve the detection of DDAs further by using
detected decision regions and (b) extend HGMs
beyond three levels in order to integrate useful se-
mantic information such as topic structure.
Acknowledgments
The research reported in this paper was spon-
sored by the Department of the Navy, Office of
Naval Research, under grants number N00014-
09-1-0106 and N00014-09-1-0122. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the Office of
Naval Research.
References
Satanjeev Banerjee, Carolyn Rose?, and Alex Rudnicky.
2005. The necessity of a meeting recording and
playback system, and the benefit of topic-level anno-
tations to meeting browsing. In Proceedings of the
10th International Conference on Human-Computer
Interaction.
H. H. Bui, S. Venkatesh, and G. West. 2002. Pol-
icy recognition in the abstract hidden markov model.
Journal of Artificial Intelligence Research, 17:451?
499.
Trung Huu Bui, Matthew Frampton, John Dowding,
and Stanley Peters. 2009. Extracting decisions from
multi-party dialogue using directed graphical mod-
els and semantic similarity. In Proceedings of the
10th Annual SIGDIAL Meeting on Discourse and
Dialogue (SIGdial09).
Nigel Crook, Ramon Granell, and Stephen Pulman.
2009. Unsupervised classification of dialogue acts
using a dirichlet process mixture model. In Pro-
ceedings of SIGDIAL 2009: the 10th Annual Meet-
ing of the Special Interest Group in Discourse and
Dialogue, pages 341?348.
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008. Mod-
elling and detecting decisions in multi-party dia-
logue. In Proceedings of the 9th SIGdial Workshop
on Discourse and Dialogue.
311
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 678?687.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Daniel Gatica-Perez, Ian McCowan, Dong Zhang, and
Samy Bengio. 2005. Detecting group interest level
in meetings. In Proceedings of ICASSP.
Pey-Yun Hsueh and Johanna Moore. 2007. Automatic
decision detection in meeting speech. In Proceed-
ings of MLMI 2007, Lecture Notes in Computer Sci-
ence. Springer-Verlag.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Marc??as-Guarasa,
Nelson Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI meeting project: Resources and re-
search. In Proceedings of the 2004 ICASSP NIST
Meeting Recognition Workshop.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proceedings of
Human Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 139?146,
Rochester, New York, April. Association for Com-
putational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282?
289. Morgan Kaufmann.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Matthew Purver, John Dowding, John Niekrasz,
Patrick Ehlen, Sharareh Noorbaloochi, and Stanley
Peters. 2007. Detecting and summarizing action
items in multi-party dialogue. In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. Journal of Machine
Learning Research, 8:693?723.
Steve Whittaker, Rachel Laban, and Simon Tucker.
2006. Analysing meeting records: An ethnographic
study and technological implications. In S. Renals
and S. Bengio, editors, Machine Learning for Multi-
modal Interaction: Second International Workshop,
MLMI 2005, Revised Selected Papers, volume 3869
of Lecture Notes in Computer Science, pages 101?
113. Springer.
312
