A C lass i f i ca t ion  Approach  to  Word  Pred ic t ion*  
Ya i r  Even-Zohar  Dan Roth  
Depar tment  of Computer  Science 
University of Illinois at Urbana-Champaign  
{evenzoha, danr}~uiuc, edu 
Abst rac t  
The eventual goal of a language model is to accu- 
rately predict he value of a missing word given its 
context. We present an approach to word prediction 
that is based on learning a representation for each 
word as a function of words and linguistics pred- 
icates in its context. This approach raises a few 
new questions that we address. First, in order to 
learn good word representations it is necessary to 
use an expressive representation f the context. We 
present away that uses external knowledge to gener- 
ate expressive context representations, along with a 
learning method capable of handling the large num- 
ber of features generated this way that can, poten- 
tially, contribute to each prediction. Second, since 
the number of words "competing" for each predic- 
tion is large, there is a need to "focus the attention" 
on a smaller subset of these. We exhibit he contri- 
bution of a "focus of attention" mechanism to the 
performance of the word predictor. Finally, we de- 
scribe a large scale experimental study in which the 
approach presented is shown to yield significant im- 
provements in word prediction tasks. 
1 In t roduct ion  
The task of predicting the most likely word based on 
properties of its surrounding context is the archetyp- 
ical prediction problem in natural language process- 
ing (NLP). In many NLP tasks it is necessary to de- 
termine the most likely word, part-of-speech (POS) 
tag or any other token, given its history or context. 
Examples include part-of speech tagging, word-sense 
disambiguation, speech recognition, accent restora- 
tion, word choice selection in machine translation, 
context-sensitive spelling correction and identifying 
discourse markers. Most approaches to these prob- 
lems are based on n-gram-like modeling. Namely, 
the learning methods make use of features which are 
conjunctions of typically (up to) three consecutive 
words or POS tags in order to derive the predictor. 
In this paper we show that incorporating addi- 
tional information into the learning process is very 
* This research is supported by NSF grants IIS-9801638 and 
SBR-987345. 
beneficial. In particular, we provide the learner with 
a rich set of features that combine the information 
available in the local context along with shallow 
parsing information. At the same time, we study 
a learning approach that is specifically tailored for 
problems in which the potential number of features 
is very large but only a fairly small number of them 
actually participates in the decision. Word predic- 
tion experiments hat we perform show significant 
improvements in error rate relative to the use of the 
traditional, restricted, set of features. 
Background 
The most influential problem in motivating statis- 
tical learning application in NLP tasks is that of 
word selection in speech recognition (Jelinek, 1998). 
There, word classifiers are derived from a probabilis- 
tic language model which estimates the probability 
of a sentence s using Bayes rule as the product of 
conditional probabilities, 
Pr(s) - = 
-- H~=lPr(wi \ ]wl , . . .Wi_ l  ) 
-- H?=lPr(wi\[hi ) 
where hi is the relevant history when predicting wi. 
Thus, in order to predict he most likely word in a 
given context, a global estimation of the sentence 
probability is derived which, in turn, is computed 
by estimating the probability of each word given its 
local context or history. Estimating terms of the 
form Pr (wlh  ) is done by assuming some generative 
probabilistic model, typically using Markov or other 
independence assumptions, which gives rise to es- 
timating conditional probabilities of n-grams type 
features (in the word or POS space). Machine learn- 
ing based classifiers and maximum entropy models 
which, in principle, are not restricted to features of 
these forms have used them nevertheless, perhaps 
under the influence of probabilistic methods (Brill, 
1995; Yarowsky, 1994; Ratnaparkhi et al, 1994). 
It has been argued that the information available 
in the local context of each word should be aug- 
mented by global sentence information and even in- 
formation external to the sentence in order to learn 
124 
better classifiers and language models. Efforts in 
this directions consists of (1) directly adding syn- 
tactic information, as in (Chelba and Jelinek, 1998; 
Rosenfeld, 1996), and (2) indirectly adding syntac- 
tic and semantic information, via similarity models; 
in this case n-gram type features are used when- 
ever possible, and when they cannot be used (due 
to data sparsity), additional information compiled 
into a similarity measure is used (Dagan et al, 
1999). Nevertheless, the efforts in this direction so 
far have shown very insignificant improvements, if 
any (Chelba and Jelinek, 1998; Rosenfeld, 1996). 
We believe that the main reason for that is that in- 
corporating information sources in NLP needs to be 
coupled with a learning approach that is suitable for 
it. 
Studies have shown that both machine learning 
and probabilistic learning methods used in NLP 
make decisions using a linear decision surface over 
the feature space (Roth, 1998; Roth, 1999). In this 
view, the feature space consists of simple functions 
(e.g., n-grams) over the the original data so as to 
allow for expressive nough representations u ing a 
simple functional form (e.g., a linear function). This 
implies that the number of potential features that 
the learning stage needs to consider may be very 
large, and may grow rapidly when increasing the ex- 
pressivity of the features. Therefore a feasible com- 
putational approach needs to be feature-efficient. I  
needs to tolerate a large number of potential features 
in the sense that the number of examples required 
for it to converge should depend mostly on the num- 
ber features relevant o the decision, rather than on 
the number of potential features. 
This paper addresses the two issues mentioned 
above. It presents a rich set of features that is con- 
structed using information readily available in the 
sentence along with shallow parsing and dependency 
information. It then presents a learning approach 
that can use this expressive (and potentially large) 
intermediate r presentation a d shows that it yields 
a significant improvement in word error rate for the 
task of word prediction. 
The rest of the paper is organized as follows. In 
section 2 we formalize the problem, discuss the in- 
formation sources available to the learning system 
and how we use those to construct features. In sec- 
tion 3 we present he learning approach, based on 
the SNoW learning architecture. Section 4 presents 
our experimental study and results. In section 4.4 
we discuss the issue of deciding on a set of candi- 
date words for each decision. Section 5 concludes 
and discusses future work. 
2 In fo rmat ion  Sources  and  Features  
Our goal is to learn a representation for each word 
in terms of features which characterize the syntactic 
and semantic ontext in which the word tends to 
appear. Our features are defined as simple relations 
over a collection of predicates that capture (some of) 
the information available in a sentence. 
2.1 In fo rmat ion  Sources  
Def in i t ion  1 Let s =< wl,w2,. . . ,wn > be a sen- 
tence in which wi is the i-th word. Let :? be a col- 
lection of predicates over a sentence s. IS(s)) 1, the 
In fo rmat ion  source(s)  available for the sentence 
s is a representation ors as a list of predicates I E :r, 
XS(S) = {II(Wll  , ...Wl,),-.., /~g(W~l , ..-Wk,)}. 
Ji is the arity of the predicate I j .  
Example  2 Let s be the sentence 
< John, X, at,  the, clock, to, see, what, time, i t ,  i s  > 
Let ~={word, pos, subj-verb}, with the interpreta- 
tion that word is a unary predicate that returns the 
value of the word in its domain; pos is a unary 
predicate that returns the value of the pos of the 
word in its domain, in the context of the sentence; 
sub j -  verb is a binary predicate that returns the 
value of the two words in its domain if the second is 
a verb in the sentence and the first is its subject; it 
returns ? otherwise. Then, 
IS(s)  = {word(wl) = John, ..., word(w3) = at,... ,  
word(wn) = is,  pos(w4) = DET,. . . ,  
s bj - verb(w , w2) = {John, x}...}. 
The IS representation f s consists only of the pred- 
icates with non-empty values. E.g., pos(w6) = 
modal is not part of the IS for the sentence above. 
subj - verb might not exist at all in the IS even if the 
predicate is available, e.g., in The ba l l  was given 
to Mary. 
Clearly the I S  representation f s does not contain 
all the information available to a human reading s; 
it captures, however, all the input that is available 
to the computational process discussed in the rest 
of this paper. The predicates could be generated by 
any external mechanism, even a learned one. This 
issue is orthogonal to the current discussion. 
2.2  Generat ing  Features  
Our goal is to learn a representation for each word 
of interest. Most efficient learning methods known 
today and, in particular, those used in NLP, make 
use of a linear decision surface over their feature 
space (Roth, 1998; Roth, 1999). Therefore, in or- 
der to learn expressive representations one needs to 
compose complex features as a function of the in- 
formation sources available. A linear function ex- 
pressed irectly in terms of those will not be expres- 
sive enough. We now define a language that allows 
1We denote IS(s) as IS wherever it is obvious what the 
referred sentence we is, or whenever we want to indicate In- 
formation Source in general. 
125 
one to define "types" of features 2 in terms of the 
information sources available to it. 
Def in i t ion 3 (Basic Features )  Let I E Z be a 
k-ary predicate with range R. Denote w k = 
(Wjl , . . .  , wjk). We define two basic binary relations 
as follows. For a e R we define: 
1 iffI(w k)=a 
f ( I (wk) ,  a) = 0 otherwise (1) 
An existential version of the relation is defined by: 
l i f f3aERs . t I (w  k)=a 
f ( I (wk) ,x )  = 0 otherwise (2) 
Features, which are defined as binary relations, can 
be composed to yield more complex relations in 
terms of the original predicates available in IS. 
Def in i t ion 4 (Compos ing  features)  Let f l ,  f2 
be feature definitions. Then fand(fl,  f2)  for(f1, f2)  
fnot(fl) are defined and given the usual semantic: 
liff:----h=l 
fand(fl, f2) = 0 otherwise 
l i f f := l  o r f2=l  
fob(f1, f2)  = 0 otherwise 
{ l~f f l=O 
fnot(fx) = 0 otherwise 
In order to learn with features generated using these 
definitions as input, it is important hat features 
generated when applying the definitions on different 
ISs are given the same identification. In this pre- 
sentation we assume that the composition operator 
along with the appropriate IS element (e.g., Ex. 2, 
Ex. 9) are written explicitly as the identification of 
the features. Some of the subtleties in defining the 
output representation are addressed in (Cumby and 
Roth, 2000). 
2.3 S t ructured  Features  
So far we have presented features as relations over 
IS(s)  and allowed for Boolean composition opera- 
tors. In most cases more information than just a list 
of active predicates is available. We abstract his 
using the notion of a structural information source 
(SIS(s))  defined below. This allows richer class of 
feature types to be defined. 
2We note that we do not define the features will be used in 
the learning process. These are going to be defined in a data 
driven way given the definitions discussed here and the input 
ISs. The importance of formally defining the "types" is due 
to the fact that some of these are quantified. Evaluating them 
on a given sentence might be computationally intractable and 
a formal definition would help to flesh out the difficulties and 
aid in designing the language (Cumby and Roth, 2000). 
2.4 S t ructured  Instances  
Def in i t ion 5 (S t ructura l  In fo rmat ion  Source)  
Let s =< wl,w2, ...,Wn >. SIS(s)), the St ructura l  
In fo rmat ion  source(s)  available for the sentence 
s, is a tuple (s, E1, . . .  ,Ek) of directed acyclic 
graphs with s as the set of vertices and Ei 's, a set 
of edges in s. 
Example  6 (L inear  S t ructure)  The simplest 
SIS is the one corresponding to the linear structure 
of the sentence. That is, S IS(s )  = (s ,E)  where 
(wi, wj) E E iff the word wi occurs immediately 
before wj in the sentence (Figure 1 bottom left 
part). 
In a linear structure (s =< Wl,W2,...,Wn >,E) ,  
where E = {(wi,wi+l); i  = 1, . . .n -  1}, we define 
the chain 
c(w j ,  \[l, r\]) = {w,_ , , . . . ,  w j , . . ,  n s. 
We can now define a new set of features that 
makes use of the structural information. Structural 
features are defined using the SIS. When defining a 
feature, the naming of nodes in s is done relative to 
a distinguished node, denoted wp, which we call the 
focus word of the feature. Regardless of the arity 
of the features we sometimes denote the feature f
defined with respect o wp as f(wp). 
Def in i t ion  7 (P rox imi ty )  Let S IS (s )  = (s, E) be 
the linear structure and let I E Z be a k-ary predicate 
with range R. Let Wp be a focus word and C = 
C(wp, \[l, r\]) the chain around it. Then, the proximity 
features for I with respect o the chain C are defined 
as: 
fc(l(w), a) = { 1 i f I (w)  = a ,a  E R ,w E C 0 otherwise 
(3) 
The second type of feature composition defined 
using the structure is a collocation operator. 
Def in i t ion 8 (Col locat ion)  Let f l , . . . f k  be fea- 
ture definitions, col locc ( f l , f 2, . . . f k ) is a restricted 
conjunctive operator that is evaluated on a chain 
C of length k in a graph. Specifically, let C = 
{wj,, wj=, .. . , wjk } be a chain of length k in S IS(s) .  
Then, the collocation feature for f l , . . ,  fk with re- 
spect to the chain C is defined as 
collocc(fl, . . . , fk) = { 
1 ifVi = 1 , . . . k ,  f i(wj,) = 1 
0 otherwise 
(4) 
The following example defines features that are 
used in the experiments described in Sec. 4. 
126 
Example  9 Let s be the sentence in Example 2. We 
define some of the features with respect o the linear 
structure of the sentence. The word X is used as 
the focus word and a chain \[-10, 10\] is defined with 
respect o it. The proximity features are defined with 
respect o the predicate word. We get, for example: 
fc(word) ---- John; fc(word) = at; fc(word) = clock. 
Collocation features are defined with respect o a 
chain \[-2, 2\] centered at the focus word X.  They are 
defined with respect o two basic features f l ,  f2 each 
of which can be either f(word, a) or f(pos, a). The 
resulting features include, for example: 
collocc(word, word)= { John-  X}; 
collocc(word, word) = {X - at}; 
collocc(word, pos) = {at -  DET}.  
2.5 Non-L inear  S t ructure  
So far we have described feature definitions which 
make use of the linear structure of the sentence and 
yield features which are not too different from stan- 
dard features used in the literature e.g., n-grams 
with respect o pos or word can be defined as colloc 
for the appropriate chain. Consider now that we are 
given a general directed acyclic graph G = (s, E) 
on the the sentence s as its nodes. Given a distin- 
guished focus word wp 6 s we can define a chain in 
the graph as we did above for the linear structure 
of the sentence. Since the definitions given above, 
Def. 7 and Def. 8, were given for chains they would 
apply for any chain in any graph. This generaliza- 
tion becomes interesting if we are given a graph that 
represents a more involved structure of the sentence. 
Consider, for example the graph DG(s) in Fig- 
ure 1. DG(s) described the dependency graph of 
the sentence s. An edge (wi,wj) in DG(s) repre- 
sent a dependency between the two words. In our 
feature generation language we separate the infor- 
mation provided by the dependency grammar 3 to 
two parts. The structural information, provided in 
the left side of Figure 1, is used to generate SIS(s) .  
The labels on the edges are used as predicates and 
are part of IS(s).  Notice that some authors (Yuret, 
1998; Berger and Printz, 1998) have used the struc- 
tural information, but have not used the information 
given by the labels on the edges as we do. 
The following example defines features that are 
used in the experiments described in Sec. 4. 
Example  10 Let s be the sentence in Figure 1 
along with its IS that is defined using the predicates 
word, pos, sub j ,  obj ,  aux_vrb. A sub j -verb  
3This information can be produced by a functional de- 
pendency grammar (FDG), which assigns each word a spe- 
cific function, and then structures the sentence hierarchical ly 
based on it, as we do here (Tapanainen and Jrvinen, 1997), 
but can also be generated by an external  rule-based parser or 
a learned one. 
feature, fsubj-verb, can be defined as a collocation 
over chains constructed with respect to the focus 
word jo in .  Moreover, we can define fsubj-verb to 
be active also when there is an aux_vrb between 
the subj and verb, by defining it as a disjunction 
of two collocation features, the sub j -verb  and the 
subj-aux_vrb-verb.  Other features that we use are 
conjunctions of words that occur before the focus 
verb (here: j o in )  along all the chains it occurs in 
(here: wi l l ,  board,  as) and collocations of obj 
and verb. 
As a final comment on feature generation, we note 
that the language presented is used to define "types" 
of features. These are instantiated in a data driven 
way given input sentences. A large number of fea- 
tures is created in this way, most of which might not 
be relevant o the decision at hand; thus, this pro- 
cess needs to be followed by a learning process that 
can learn in the presence of these many features. 
3 The  Learn ing  Approach  
Our experimental investigation is done using the 
SNo W learning system (Roth, 1998). Earlier ver- 
sions of SNoW (Roth, 1998; Golding and Roth, 
1999; Roth and Zelenko, 1998; Munoz et al, 1999) 
have been applied successfully to several natural an- 
guage related tasks. Here we use SNo W for the task 
of word prediction; a representation is learned for 
each word of interest, and these compete at evalua- 
tion time to determine the prediction. 
3.1 The  SNOW Arch i tec ture  
The SNo W architecture is a sparse network of linear 
units over a common pre-defined or incrementally 
learned feature space. It is specifically tailored for 
learning in domains in which the potential number of 
features might be very large but only a small subset 
of them is actually relevant o the decision made. 
Nodes in the input layer of the network represent 
simple relations on the input sentence and are being 
used as the input features. Target nodes represent 
words that are of interest; in the case studied here, 
each of the word candidates for prediction is repre- 
sented as a target node. An input sentence, along 
with a designated word of interest in it, is mapped 
into a set of features which are active in it; this rep- 
resentation is presented to the input layer of SNoW 
and propagates to the target nodes. Target nodes 
are linked via weighted edges to (some of) the input 
features. Let At = {Q, . . .  , i,~} be the set of features 
that are active in an example and are linked to the 
target node t. Then the linear unit corresponding to
t is active iff 
t E w i > Ot, 
iEAt 
where w~ is the weight on the edge connecting the ith 
feature to the target node t, and Ot is the threshold 
127 
~ , Nov.29. 
will- board -~? 
Vi ~ ,~ t!e d i re~a 
Pierr/e Ye~ars old l 
-~ non-executive 
61 
Pierre -*Vinken-.-, -~ 61 -*-years -'*old ~ -" will - -  \] 
?join ---the -,-board -.- as--- a +non-executive I 
? 
director-.- 29. -=-Nov. 
~.  ,, Nov. 29. old 
aux_vrb obj 
~sub?" will J  b?~rd corped~%~ 
det pcomp det.. a-.. Vir~.n 
attr "moc l  tl~e director, 
Pier're years attr 
* non-eXecutive qnt 
6t 
Pierre Vinken, 61 years old, will join the board as a 
nonexecutive director Nov. 29. 
Figure 1: A sentence  w i th  a l inear and a dependency  grammar  s t ructure  
for the target node t. In this way, SNo W provides 
a collection of word representations rather than just 
discriminators. 
A given example is treated autonomously by each 
target subnetwork; an example labeled t may be 
treated as a positive example by the subnetwork 
for t and as a negative xample by the rest of the 
target nodes. The learning policy is on-line and 
mistake-driven; several update rules can be used 
within SNOW. The most successful update rule is 
a variant of Littlestone's Winnow update rule (Lit- 
tlestone, 1988), a multiplicative update rule that is 
tailored to the situation in which the set of input 
features is not known a priori, as in the infinite 
attribute model (Blum, 1992). This mechanism is 
implemented via the sparse architecture of SNOW. 
That is, (1) input features are allocated in a data 
driven way - an input node for the feature i is al- 
located only if the feature i was active in any input 
sentence and (2) a link (i.e., a non-zero weight) ex- 
ists between a target node t and a feature i if and 
only if i was active in an example labeled t. 
One of the important properties of the sparse ar- 
chitecture is that the complexity of processing an 
example depends only on the number of features ac- 
tive in it, na, and is independent of the total num- 
ber of features, nt, observed over the life time of the 
system. This is important in domains in which the 
total number of features is very large, but only a 
small number of them is active in each example. 
4 Exper imenta l  S tudy  
4.1 Task def in i t ion 
The experiments were conducted with four goals in 
mind: 
1. To compare mistake driven algorithms with 
naive Bayes, trigram with backoff and a simple 
maximum likelihood estimation (MLE) base- 
line. 
2. To create a set of experiments which is compa- 
rable with similar experiments hat were previ- 
ously conducted by other researchers. 
3. To build a baseline for two types of extensions of 
the simple use of linear features: (i) Non-Linear 
features (ii) Automatic focus of attention. 
4. To evaluate word prediction as a simple lan- 
guage model. 
We chose the verb prediction task which is sim- 
ilar to other word prediction tasks (e.g.,(Golding 
and Roth, 1999)) and, in particular, follows the 
paradigm in (Lee and Pereira, 1999; Dagan et al, 
1999; Lee, 1999). There, a list of the confusion sets is 
constructed first, each consists of two different verbs. 
The verb vl is coupled with v2 provided that they 
occur equally likely in the corpus. In the test set, 
every occurrence of vl or v2 was replaced by a set 
{vl, v2} and the classification task was to predict he 
correct verb. For example, if a confusion set is cre- 
ated for the verbs "make" and "sell", then the data 
is altered as follows: 
Once target subnetworks have been learned and 
the network is being evaluated, a decision sup- 
port mechanism is employed, which selects the 
dominant active target node in the SNoW unit 
via a winner-take-all mechanism to produce a fi- 
nal prediction. SNoW is available publicly at 
http ://L2R. cs. uiuc. edu/- cogcomp, html. 
make the paper --+ {make,sell} the paper 
sell sensitive data --~ {make,sell} sensitive data 
The evaluated predictor chooses which of the two 
verbs is more likely to occur in the current sentence. 
In choosing the prediction task in this way, we 
make sure the task in difficult by choosing between 
128 
competing words that have the same prior proba- 
bilities and have the same part of speech. A fur- 
ther advantage of this paradigm is that in future 
experiments we may choose the candidate verbs so 
that they have the same sub-categorization, pho- 
netic transcription, etc. in order to imitate the first 
phase of language modeling used in creating can- 
didates for the prediction task. Moreover, the pre- 
transformed data provides the correct answer so that 
(i) it is easy to generate training data; no supervi- 
sion is required, and (ii) it is easy to evaluate the 
results assuming that the most appropriate word is 
provided in the original text. 
Results are evaluated using word-error rate 
(WER). Namely, every time we predict the wrong 
word it is counted as a mistake. 
4.2 Data 
We used the Wall Street Journal (WSJ) of the years 
88-89. The size of our corpus is about 1,000,000 
words. The corpus was divided into 80% training 
and 20% test. The training and the test data were 
processed by the FDG parser (Tapanainen and Jrvi- 
nen, 1997). Only verbs that occur at least 50 times 
in the corpus were chosen. This resulted in 278 verbs 
that we split into 139 confusion sets as above. Af- 
ter filtering the examples of verbs which were not in 
any of the sets we use 73, 184 training examples and 
19,852 test examples. 
4.3 Results 
4.3.1 Features 
In order to test the advantages of different feature 
sets we conducted experiments using the following 
features ets: 
1. Linear features: proximity of window size 4-10 
words, conjunction of size 2 using window size 
4-2. The conjunction combines words and parts 
of speech. 
2. Linear + Non linear features: using the lin- 
ear features defined in (1) along with non 
linear features that use the predicates ub j ,  
obj ,  word, pos, the collocations ubj -verb,  
verb-obj  linked to the focus verb via the graph 
structure and conjunction of 2 linked words. 
The over all number of features we have generated 
for all 278 target verbs was around 400,000. In all 
tables below the NB columns represent results of the 
naive Bayes algorithm as implemented within SNoW 
and the SNoW column represents the results of the 
sparse Winnow algorithm within SNOW. 
Table 1 summarizes the results of the experiments 
with the features ets (1), (2) above. The baseline 
experiment uses MLE, the majority predictor. In 
addition, we conducted the same experiment using 
trigram with backoff and the WER is 29.3%. From 
Linear 
Non Linear 
Bline NB SNoW 
49.6 13.54 11.56 
49.6 12.25 9.84 
Table 1: Word Error Rate results for linear 
and non-linear features 
these results we conclude that using more expressive 
features helps significantly in reducing the WER. 
However, one can use those types of features only 
if the learning method handles large number of pos- 
sible features. This emphasizes the importance of 
the new learning method. 
Similarity NB SNoW 
54.6% 59.1% WSJ data 
AP news 47.6% 
Table 2: Compar i son  o f  the improvement 
achieved using similarity methods (Dagan et 
al., 1999) and using the methods presented in 
this paper. Results are shown in percentage 
of improvement in accuracy over the baseline. 
Table 2 compares our method to methods that use 
similarity measures (Dagan et al, 1999; Lee, 1999). 
Since we could not use the same corpus as in those 
experiments, we compare the ratio of improvement 
and not the WER. The baseline in this studies is 
different, but other than that the experiments are 
identical. We show an improvement over the best 
similarity method. Furthermore, we train using only 
73,184 examples while (Dagan et al, 1999) train 
using 587, 833 examples. Given our experience with 
our approach on other data sets we conjecture that 
we could have improved the results further had we 
used that many training examples. 
4.4 Focus of  attention 
SNoW is used in our experiments as a multi-class 
predictor - a representation is learned for each word 
in a given set and, at evaluation time, one of these 
is selected as the prediction. The set of candidate 
words is called the confusion set (Golding and Roth, 
1999). Let C be the set of all target words. In previ- 
ous experiments we generated artificially subsets of 
size 2 of C in order to evaluate the performance of 
our methods. In general, however, the question of 
determining a good set of candidates i interesting in
it own right. In the absence, of a good method, one 
might end up choosing a verb from among a larger 
set of candidates. We would like to study the effects 
this issue has on the performance of our method. 
In principle, instead of working with a single large 
confusion set C, it might be possible to,split C into 
subsets of smaller size. This process, which we call 
the focus of attention (FOA) would be beneficial 
only if we can guarantee that, with high probability, 
129 
given a prediction task, we know which confusion 
set to use, so that the true target belongs to it. In 
fact, the FOA problem can be discussed separately 
for the training and test stages. 
1. Training: Given our training policy (Sec. 3) ev- 
ery positive xample serves as a negative xam- 
ple to all other targets in its confusion set. For 
a large set C training might become computa- 
tionally infeasible. 
2. Testing: considering only a small set of words 
as candidates at evaluation time increases the 
baseline and might be significant from the point 
of view of accuracy and efficiency. 
To evaluate the advantage of reducing the size of 
the confusion set in the training and test phases, we 
conducted the following experiments u ing the same 
features et (linear features as in Table 1). 
Bline NB SNoW 
Tra in  All Test All 87.44 65.22 65.05 
Train All Test 2 49.6 13.54 13.15 
Train 2 Test 2 49.6 13.54 11.55 
Table 3: Evaluat ing Focus of  Attent ion:  Word 
Er ror  Rate  for Training and testing using 
all the words together  against using pairs of  
words. 
"Train All" means training on all 278 targets to- 
gether. "Test all" means that the confusion set is 
of size 278 and includes all the targets. The results 
shown in Table 3 suggest that, in terms of accuracy, 
the significant factor is the confusion set size in the 
test stage. The effect of the confusion set size on 
training is minimal (although it does affect raining 
time). We note that for the naive Bayes algorithm 
the notion of negative xamples does not exist, and 
therefore regardless of the size of confusion set in 
training, it learns exactly the same representations. 
Thus, in the NB column, the confusion set size in 
training makes no difference. 
The application in which a word predictor is used 
might give a partial solution to the FOA problem. 
For example, given a prediction task in the context 
of speech recognition the phonemes that constitute 
the word might be known and thus suggest a way 
to generate a small confusion set to be used when 
evaluating the predictors. 
Tables 4,5 present he results of using artificially 
simulated speech recognizer using a method of gen- 
eral phonetic lasses. That is, instead of transcrib- 
ing a word by the phoneme, the word is transcribed 
by the phoneme classes(Jurafsky and Martin, 200). 
Specifically, these experiments deviate from the task 
definition given above. The confusion sets used are 
of different sizes and they consist of verbs with dif- 
ferent prior probabilities in the corpus. Two sets of 
experiments were conducted that use the phonetic 
transcription ofthe words to generate confusion sets. 
Bl ine NB SNoW 
Train All Test PC  19.84 11.6 12.3 
Train PC  Test PC  19.84 11.6 11.3 
Table 4: Simulating Speech Recognizer:  Word  
Er ror  Rate  for Training and testing with 
confusion sets determined based on phonet ic  
classes (PC) f rom a s imulated speech recog- 
nizer. 
In the first experiment (Table 4), the transcription 
of each word is given by the broad phonetic groups 
to which the phonemes belong i.e., nasals, fricative, 
etc. 4. For example, the word "b_u_y" is transcribed 
using phonemes as "b_Y" and here we transcribe it
as "P_VI" which stands for "Plosive_Vowell". This 
partition results in a partition of the set of verbs 
into several confusions sets. A few of these confusion 
sets consist of a single word and therefore have 100% 
baseline, which explains the high baseline. 
Bline NB SNoW 
Train All Test PC  45.63 26.36 27.54 
Train PC  Test PC  45.63 26.36 25.55 
Table 5: Simulat ing Speech Recognizer:  Word  
Er ror  Rate  for Training and testing with 
confusion sets determined based on phonet ic  
classes (PC) f rom a s imulated speech recog- 
nizer. In this case only confusion sets that 
have less than 98% baseline are used, which 
explains the overall  lower baseline. 
Table 5 presents the results of a similar exper- 
iment in which only confusion sets with multiple 
words were used, resulting in a lower baseline. 
As before, Train All means that training is done 
with all 278 targets together while Train PC means 
that the PC confusion sets were used also in train- 
ing. We note that for the case of SNOW, used here 
with the sparse Winnow algorithm, that size of the 
confusion set in training has some, although small, 
effect. The reason is that when the training is done 
with all the target words, each target word repre- 
sentation with all the examples in which it does not 
occur are used as negative xamples. When a smaller 
confusion set is used the negative xamples are more 
likely to be "true" negative. 
5 Conc lus ion  
This paper presents a new approach to word predic- 
tion tasks. For each word of interest, a word repre- 
sentation is learned as a function of a common, but 
4In this experiment, he vowels phonemes were divided 
into two different groups to account for different sounds. 
130 
potentially very large set of expressive (relational) 
features. Given a prediction task (a sentence with 
a missing word) the word representations are evalu- 
ated on it and compete for the most likely word to 
complete the sentence. 
We have described a language that allows one to 
define expressive feature types and have exhibited 
experimentally the advantage ofusing those on word 
prediction task. We have argued that the success of 
this approach inges on the combination of using a 
large set of expressive f atures along with a learning 
approach that can tolerate it and converges quickly 
despite the large dimensionality of the data. We 
believe that this approach would be useful for other 
disambiguation tasks in NLP. 
We have also presented a preliminary study of a 
reduction in the confusion set size and its effects 
on the prediction performance. In future work we 
intend to study ways that determine the appropriate 
confusion set in a way to makes use of the current 
task properties. 
Acknowledgments 
We gratefully acknowledge helpful comments and 
programming help from Chad Cumby. 
References 
A. Berger and H. Printz. 1998. Recognition perfor- 
mance of a large-scale dependency-grammar lan- 
guage model. In Int'l Conference on Spoken Lan- 
guage Processing (ICSLP'98), Sydney, Australia. 
A. Blum. 1992. Learning boolean functions in 
an infinite attribute space. Machine Learning, 
9(4):373-386. 
E. Brill. 1995. Transformation-based error-driven 
learning and natural language processing: A case 
study in part of speech tagging. Computational 
Linguistics, 21(4):543-565. 
C. Chelba and F. Jelinek. 1998. Exploiting syntac- 
tic structure for language modeling. In COLING- 
A CL '98. 
C. Cumby and D. Roth. 2000. Relational repre- 
sentations that facilitate learning. In Proc. of 
the International Conference on the Principles of 
Knowledge Representation a d Reasoning. To ap- 
pear. 
I. Dagan, L. Lee, and F. Pereira. 1999. Similarity- 
based models of word cooccurrence probabilities. 
Machine Learning, 34(1-3):43-69. 
A. R. Golding and D. Roth. 1999. A Winnow based 
approach to context-sensitive spelling correction. 
Machine Learning, 34(1-3):107-130. Special Issue 
on Machine Learning and Natural Language. 
F. Jelinek. 1998. Statistical Methods for Speech 
Recognition. MIT Press. 
D. Jurafsky and J. H. Martin. 200. Speech and Lan- 
guage Processing. Prentice Hall. 
L. Lee and F. Pereira. 1999. Distributional similar- 
ity models: Clustering vs. nearest neighbors. In 
A CL 99, pages 33-40. 
L. Lee. 1999. Measure of distributional similarity. 
In A CL 99, pages 25-32. 
N. Littlestone. 1988. Learning quickly when irrel- 
evant attributes abound: A new linear-threshold 
algorithm. Machine Learning, 2:285-318. 
M. Munoz, V. Punyakanok, D. Roth, and D. Zimak. 
1999. A learning approach to shallow parsing. In 
EMNLP-VLC'99, the Joint SIGDAT Conference 
on Empirical Methods in Natural Language Pro- 
cessing and Very Large Corpora, June. 
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A 
maximum entropy model for prepositional phrase 
attachment. In ARPA, Plainsboro, N J, March. 
R. Rosenfeld. 1996. A maximum entropy approach 
to adaptive statistical language modeling. Com- 
puter, Speech and Language, 10. 
D. Roth and D. Zelenko. 1998. Part of speech 
tagging using a network of linear separators. 
In COLING-ACL 98, The 17th International 
Conference on Computational Linguistics, pages 
1136-1142. 
D. Roth. 1998. Learning to resolve natural language 
ambiguities: A unified approach. In Proc. Na- 
tional Conference on Artificial Intelligence, pages 
806-813. 
D. Roth. 1999. Learning in natural anguage. In 
Proc. of the International Joint Conference of Ar- 
tificial Intelligence, pages 898-904. 
P. Tapanainen and T. Jrvinen. 1997. A non- 
projective dependency parser. In In Proceedings 
of the 5th Conference on Applied Natural Lan- 
guage Processing, Washington DC. 
D. Yarowsky. 1994. Decision lists for lexical ambi- 
guity resolution: application to accent restoration 
in Spanish and French. In Proc. of the Annual 
Meeting of the A CL, pages 88-95. 
D. Yuret. 1998. Discovery of Linguistic Relations 
Using Lexical Attraction. Ph.D. thesis, MIT. 
131 
Applying System Combinat ion to Base Noun Phrase Identif ication 
Er ik  F. T jong  K im Sang",  Wal ter  Dae lemans  '~, Herv6  D6 jean  ~, 
Rob  Koel ingT,  Yuva l  Krymolowski /~,  Vas in  Punyakanok  '~, Dan Roth" 
~University of Antwert) 
Uifiversiteitsplohl 1 
13.-261.0 Wilri jk, Belgium 
{erikt,daelem}@uia.ua.ac.be 
r Unive.rsitiil; Tii l) ingen 
Kleine Wilhehnstrat./e 113 
I)-72074 T/il)ingen, Germany 
(lejean((~sl:q, ni)hil.ulfi-l;uebingen.de, 
7S1{,I Cambridge 
23 Millers Yard,Mil l  Lane 
Cambridge, CB2 ll{Q, UK 
koeling@caln.sri.coIn 
;~Bal'-Ilan University 
lbunat  Gan, 52900, Israel 
yuwdk(c~)macs. 1)iu.ac.il 
"University of Illinois 
1304: W. Sl)ringfield Ave. 
Url)ana, IL 61801, USA 
{lmnyakan,(lanr} ((~cs.uiuc.edu 
A1)s t rac t  
We us('. seven machine h;arning algorithms tbr 
one task: idenl;it~ying l)ase holm phrases. The 
results have 1)een t)rocessed by ditt'erent system 
combination methods and all of these (mtt)er- 
formed the t)est individual result. We have ap- 
t)lied the seven learners with the best (:omt)ina- 
tot, a majority vote of the top tive systenls, to a 
standard (lata set and lllallage(1 I;O ilnl)rov(', 1;11(' 
t)est pul)lished result %r this (lata set. 
1 In t roduct ion  
Van Haltor(m eta\ ] .  (1998) and Brill and Wu 
(1998) show that part-ofst)ee(:h tagger l)erfor- 
mance can 1)e iml)roved 1)y (:oml)ining ditl'erent 
tatters. By using te(:hni(tues su(:h as majority 
voting, errors made l)y 1;11(; minority of the tag- 
gers can 1)e r(;moved. Van Ilaltere, n et al (1998) 
rel)ort that the results of such a ('oml)ined al)- 
proach can improve ll\])Oll the aCcllracy error of 
the best individual system with as much as 19%. 
Tim positive (;tl'e(:t of system combination tbr 
non-language t)ro(:essing tasks has t)een shown 
in a large l)o(ly of mac\]fine l arning work. 
In this 1)aper we will use system (:omt)ination 
for identifying base noun 1)hrases (1)aseNt)s). 
W(; will at)l)ly seven machine learning algo- 
rithms to the same 1)aseNP task. At two l)oints 
we will al)ply confl)ination methods. We will 
start with making the systems process five out- 
trot representations and combine the l'esults t)y 
(:hoosing the majority of the outtmt tL'atures. 
Three of the seven systems use this al)l)roaeh. 
Afl, er this w(; will make an overall eoml)ination 
of the results of the seven systems. There we 
will evaluate several system combination meth- 
()(Is. The 1)est l)erforming method will 1)e at)- 
t)lied to a standard ata set tbr baseNP identi- 
tication. 
2 Methods  and exper iments  
in this se(:tion we will describe our lem:ning task: 
recognizing 1)ase noun phrases. After this we 
will (tes(:ril)e the data representations we used 
and the ma('hine learning algorithms that we 
will at)l)ly to the task. We will con(:ludc with 
an overview of the (:ombination metllo(ls that 
we will test. 
2.1 Task descript ion 
Base noun \])hrases (1)aseNPs) are n(mn phrases 
whi(:h do not (:ontain another noun l)hrase. \]?or 
cxamt)le , the sentence 
In \[early trading\] in \[ IIong Kong\] 
\[ Mo,l,tay \], \[ g,,la \] was q, loted at 
\[ $ 366. 0 \] \ [a .  o1,,,.(; \] .  
contains six baseN1)s (marked as phrases be- 
tween square 1)rackets). The phrase $ 266.50  
an  ounce  ix a holm phrase as well. However, it 
is not a baseNP since it contains two other noun 
phrases. Two baseNP data sets haw.' been put 
forward by Ramshaw and Marcus (1995). The 
main data set consist of tbur sections of the Wall 
Street Journal (WSJ) part of the Penn Tree- 
bank (Marcus et al, 1.993) as training mate- 
rial (sections 15-18, 211727 tokens) and one sec- 
tion aS test material (section 20, 47377 tokens)5. 
The data contains words, their part-of-speech 
1This Ramshaw and Marcus (1995) bascNP data set 
is availal)le via ffp://fti).cis.upe,m.edu/pub/chunker/ 
857 
(POS) tags as computed by the Brill tagger and 
their baseNP segmentation asderived from the 
%'eebank (with some modifications). 
In the baseNP identitication task, perfor- 
mance is measured with three rates. First, 
with the percentage of detected noun phrases 
that are correct (precision). Second, with the 
1)ercentage of noun phrases in the data that 
were found by the classifier (recall). And third, 
with the F#=~ rate which is equal to (2*preci- 
sion*recall)/(precision+recall). The latter rate 
has been used as the target for optimization. 
2.2 Data representat ion 
In our example sentence in section 2.1, noun 
phrases are represented by bracket structures. 
It has been shown by Mufioz et al (1999) 
that for baseNP recognition, the representa- 
tion with brackets outperforms other data rep- 
resentations. One classifier can be trained to 
recognize open brackets (O) and another can 
handle close brackets (C). Their results can be 
combined by making pairs of open and close 
brackets with large probability scores. We have 
used this bracket representation (O+C) as well. 
However, we have not used the combination 
strategy from Mufioz et al (1999) trot in- 
stead used the strategy outlined in Tjong Kim 
Sang (2000): regard only the shortest possi- 
ble phrases between candidate open and close 
brackets as base noun phrases. 
An alternative representation for baseNPs 
has been put tbrward by Ramshaw and Mar- 
cus (1995). They have defined baseNP recog- 
nition as a tagging task: words can be inside a 
baseNP (I) or outside a baseNP (O). In the case 
that one baseNP immediately follows another 
baseNP, the first word in the second baseNP 
receives tag B. Example: 
Ino early1 trading1 ino Hongi Kongi 
MondayB ,o gold1 waso quotedo ato 
$I 366.501 anu ounce1 .o 
This set of three tags is sufficient for encod- 
ing baseNP structures since these structures are 
nonrecursive and nonoverlapping. 
Tjong Kiln Sang (2000) outlines alternative 
versions of this tagging representation. First, 
the B tag can be used for tile first word of ev- 
ery baseNP (IOB2 representation). Second, in- 
stead of the B tag an E tag can be used to 
nlark the last word of a baseNP immediately 
before another baseNP (IOE1). And third, the 
E tag call be used for every noun phrase final 
word (IOE2). He used the Ramshaw and Mar- 
cus (1995) representation as well (IOB1). We 
will use these tbur tagging representations and 
the O+C representation for the system-internal 
combination experiments. 
2.a Machine learning algorithms 
This section contains a brief description of tile 
seven machine learning algorithms that we will 
apply to the baseNP identification task: AL- 
LiS, c5.0, IO~?ee, MaxEnt, MBL, MBSL and 
SNOW. 
ALLiS 2 (Architecture for Learning Linguistic 
Structures) is a learning system which uses the- 
ory refinement in order to learn non-recursive 
NP and VP structures (Ddjean, 2000). ALLiS 
generates a regular expression grammar which 
describes the phrase structure (NP or VP). This 
grammar is then used by the CASS parser (Ab- 
hey, 1996). Following the principle of theory re- 
finement, tile learning task is composed of two 
steps. The first step is the generation of an 
initial wa, mmar. The generation of this grmn- 
mar uses the notion of default values and some 
background knowledge which provides general 
expectations concerning the immr structure of 
NPs and VPs. This initial grammar provides 
an incomplete and/or incorrect analysis of tile 
data. The second step is the refinement of this 
grammar. During this step, the validity of the 
rules of the initial grammar is checked and the 
rules are improved (refined) if necessary. This 
refinement relies on the use of two operations: 
the contextualization (i which contexts uch a 
tag always belongs to the phrase) and lexical- 
ization (use of information about the words and 
not only about POS). 
05.0 a, a commercial version of 04.5 (Quin- 
lan, 1993), performs top-do,vn induction of de- 
cision trees (TDIDT). O,1 the basis of an in- 
stance base of examples, 05.0 constructs a deci- 
sion tree which compresses the classification i - 
formation in the instance base by exploiting dif- 
tbrences in relative importance of different fea- 
tures. Instances are stored in the tree as paths 
2A demo f the NP and VP ctmnker is available at 
ht;t:p: / /www.sfb441.unituebingen.de/~ dej an/chunker.h 
tml 
aAvailable fl'om http://www.rulequest.com 
858 
of commcted nodes ending in leaves which con- 
tain classification information. Nodes are con- 
nected via arcs denoting feature wflues. Feature 
inff)rmation gain (nmt;ual inforniation 1)etween 
features and class) is used to determine the or- 
der in which features are mnt)loyed as tests at all 
levels of the tree (Quinlan, 1993), With the full 
inlmt representation (words and POS tags)~ we 
were not able to run comt)lete xperiments. We 
therefore xperimented only with the POS tags 
(with a context of two left; and right). We have 
used the default parameter setting with decision 
trees coml)ined with wflue groul)ing. 
We have used a nearest neighbor algoritlml 
(IBI.-1G, here listed as MBL) and a decision tree 
algoritlmi (llG\[lh:ee) from the TiMBL learning 
package (Da(flmnans et al, 19991)). Both algo- 
rithms store the training data and ('lassi(y new 
it;eros by choosing the most frequent (:lassiti(:a- 
lion among training items which are closest to 
this new item. l)ata it(uns rare rel)resented as 
sets of thature-vahu; 1)airs. Each ti;ature recc'ives 
a weight which is t)ased on the amount of in- 
formation whi(:h it t/rovides fi)r comtmting the 
classification of t;t1(; items in the training data. 
IBI-IG uses these weights tbr comt)uting the dis- 
lance l)etween a t)air of data items and IGTree 
uses them fi)r deciding which feature-value de- 
cisions shouM t)e made in the top nod(;s of the 
decision tree (l)a(;lenJans et al, 19991)). We 
will use their det, mlt pm:amet('a:s excel)t for the 
IBI-IG t)arameter for the numl)er of exmnine(t 
m',arest n(,ighl)ors (k) whi('h we h~ve s(,t to 3 
(Daelemans et al, 1999a). The classifiers use a 
left and right context of four words and part- 
ofsl)eech tags. t~i)r |;lie four IO representations 
we have used a second i)rocessing stage which 
used a smaller context lint which included in- 
formation at)out the IO tags 1)redicted by the 
first processing phase (Tjong Kim Sang, 2000). 
When /)uilding a classifier, one must gather 
evidence ti)r predicting the correct class of an 
item from its context. The Maxinmm Entropy 
(MaxEnt) fl:mnework is especially suited tbr 
integrating evidence tiom various inti)rmal;ion 
sources. Frequencies of evidence/class combi~ 
nations (called features) are extracted fl'om a 
sample corlms and considere(t to be t)roperties 
of the classification process. Attention is con- 
strained to models with these l)roperties. The 
MaxEnt t)rinciph; now demands that among all 
1;11(; 1)robability distributions that obey these 
constraints, the most mfiform is chosen, l)ur- 
ing training, features are assigned weights in 
such a way that, given the MaxEnt principle, 
the training data is matched as well as possible. 
During evaluation it is tested which features are 
active (i.e. a feature is active when the context 
meets the requirements given by t;11(', feature). 
For every class the weights of the active fea- 
tures are combined and the best scoring class 
is chosen (Berger et al, 1996). D)r the classi- 
tier built here the surromlding words, their POS 
tags and lmseNP tags predicted for the previous 
words are used its evidence. A mixture of simple 
features (consisting of one of the mentioned in- 
formation sources) and complex features (com- 
binations thereof) were used. The left context 
never exceeded 3 words, the right context was 
maximally 2 words. The model wits (:ah:ulated 
using existing software (l)ehaspe, 1997). 
MBSL (Argalnon et al, 1999) uses POS data 
in order to identit~y t/aseNPs, hfferenee re- 
lies on a memory which contains all the o(:- 
cm:rences of P()S sequences which apt)ear in 
the t)egimfing, or the end, of a 1)aseNl? (in- 
(:hiding complete t)hrases). These sequences 
may include a thw context tags, up to a 1)re- 
st)ecifi('d max_(:ont<~:t. \])uring inti',rence, MBSL 
tries to 'tile' each POS string with parts of 
noun-l)hrases from l;he memory. If the string 
coul(1 l)e fully covered t)y the tiles, il; becomes 
l)art of a (:andidate list, anfl)iguities 1)etween 
candidates are resolved by a constraint )ropa- 
gation algorithm. Adding a (:ontext extends the 
possil)ilities for tiling, thereby giving more op- 
portunities to 1)etter candidates. The at)t)roaeh 
of MBSL to the i)rot)lem of identifying 1)aseNPs 
is sequence-1)ased rather than word-based, that 
is, decisions are taken per POS sequence, or per 
candidate, trot not for a single word. In addi- 
tion, the tiling l)rocess gives no preference to 
any (tirection in the sentence. The tiles may 1)e 
of any length, up to the maximal ength of a 
1)hrase in the training (ILl;L, which gives MBSL 
a generalization power that compensates for the 
setup of using only POS tags. The results t)re- 
seated here were obtained by optimizing MBSL 
parameters based on 5-fold CV on the training 
data. 
SNoW uses the Open/Close model, described 
in Mufioz et al (1999). As is shown there, this 
859 
section 21 
IOB1 
IOB2 
IOE1 
IOE2 
O+C 
0 
97.81% 
97.63% 
97.80% 
97.72% 
97.72% 
MBL 
Majority 98.04% 98.20% 
C Ffl=l 
97.97% 91.68 
97.96% 91.79 
97.92% 91.54 
97.94% 92.06 
98.04% 92.03 
92.82 
MaxEnt 
O C 
97.90% 98.11% 
97.81% 98.14% 
97.88% 98.12% 
97.84% 98.12% 
97.82% 98.15% 
97.94% 98.24% 
Ffl=l 
92.43 
92.14 
92.37 
92.13 
92.26 
92.60 
IGTree 
O C 
96.62% 96.89% 
97.27% 97.30% 
95.88% 96.01% 
97.19% 97.62% 
96.89% 97.49% 
97.70% 97.99% 
F\[~=1 
87.88 
90.03 
82.80 
89.98 
89.37 
91.92 
Table 1: The effects of system-internal combination by using different output representations. A 
straight-forward majority vote of the output yields better bracket accuracies and Ffl=l rates than 
any included individual classifier. The bracket accuracies in the cohmms O and C show what 
percentage of words was correctly classified as baseNP start, baseNP end or neither. 
model produced better results than the other 
paradigm evaluated there, the Inside/Outside 
paradigm. The Open/Close model consists of 
two SNoW predictors, one of which predicts the 
beginning of baseNPs (Open predictor), and the 
other predicts the end of the ptlrase (Close pre- 
dictor). The Open predictor is learned using 
SNoW (Carlson el; al., 1999; Roth, 1998) as a 
flmction of features that utilize words and POS 
tags in the sentence and, given a new sentence, 
will predict for each word whether it is the first 
word in the phrase or not. For each Open, the 
Close predictor is learned using SNoW as a func- 
tion of features that utilize the words ill the sen- 
tence, the POS tags and the open prediction. It 
will predict, tbr each word, whether it Call be 
the end of" the I)hrase, given the previously pre- 
dicted Open. Each pair of predicted Open mid 
Close forms a candidate of a baseNP. These can- 
didates may conflict due to overlapping; at this 
stage, a graph-based constraint satisfaction al- 
gorithm that uses the confidence values SNoW 
associates with its predictions i elnployed. This 
algorithln ("the combinator') produces tile list 
of" the final baseNPs fbr each sentence. Details 
of SNOW, its application in shallow parsing and 
the combinator% Mgorithm are in Mufioz et al 
(1999). 
2.4 Combinat ion techniques 
At two points in our noun phrase recognition 
process we will use system combination. We will 
start with system-internal combination: apply 
the same learning algorithm to variants of the 
task and combine the results. The approach 
we have chosen here is the same as in Tjong 
Kim Sang (2000): generate different variants 
of the task by using different representations 
of the output (IOB1, IOB2, IOE1, IOE2 and 
O+C). The five outputs will converted to the 
open bracket representation (O) and the close 
bracket; representation (C) and M'ter this, tile 
most frequent of the five analyses of each word 
will chosen (inajority voting, see below). We 
expect the systems which use this combination 
phase to perform better than their individuM 
members (Tjong Kim Sang, 2000). 
Our seven learners will generate different clas- 
sifications of tile training data and we need to 
find out which combination techniques are most 
appropriate. For the system-external combi- 
nation experiment, we have evaluated itfi;rent 
voting lllechanisms~ effectively the voting meth- 
ods as described in Van Halteren et al (1998). 
In the first method each classification receives 
the same weight and the most frequent classifi- 
cation is chosen (Majority). The second nmthod 
regards as tile weight of each individual clas- 
sification algorithm its accuracy on solne part 
of the data, tile tuning data (TotPrecision). 
The third voting method computes the preci- 
sion of each assigned tag per classifer and uses 
this value as a weight for tile classifier in those 
cases that it chooses the tag (TagPrecision). 
The fourth method uses both the precision of 
each assigned tag and tile recall of the com- 
peting tags (Precision-Recall). Finally, tile fifth 
lnethod uses not only a weight for tile current 
classification but it also computes weights tbr 
other possible classifications. The other classi- 
fications are deternfined by exalnining the tun- 
860 
ing data and registering the correct wflues for 
(;very pair of classitier esults (pair-wise voting, 
see Van Halteren et al (1998) tbr an elaborate 
explanation). 
Apart from these five voting methods we have 
also processed the output streams with two clas- 
sifters: MBL and IG%'ee. This approach is 
called classifier stacking. Like Van Halteren et 
al. (1998), we have used diff'erent intmt ver- 
sions: olle containing only the classitier Otltl)ut 
and another containing both classifier outlmt 
and a compressed representation of the data 
item tamer consideration. \]?or the latter lmr- 
pose we have used the part-of-speech tag of the 
carrent word. 
3 Resul ts  4 
We want to find out whether system combi- 
nation could improve performmlce of baseNP 
recognition and, if this is the fact, we want to 
seJect the best confl)ination technique. For this 
lmrpose we have pertbrmed an experiment with 
sections 15-18 of the WSJ part of the Prom %'ee- 
bank as training data (211727 tokens) and sec- 
tion 21 as test data (40039 tokens). Like the 
data used by Ramshaw and Marcus (1995), this 
data was retagged by the Brill tagger in order 
to obtain realistic part-of  speech (POS) tags 5. 
The data was seglnente.d into baseNP parts and 
non-lmseNP t)arts ill a similar fitshion as the 
data used 1)y Ramshaw and Marcus (1995). Of 
the training data, only 90% was used for train- 
ing. The remaining 10% was used as laming 
data for determining the weights of the combi- 
nation techniques. 
D)r three classifiers (MBL, MaxEnt and 
IGTree) we haw; used system-internal coral)i- 
nation. These learning algorithms have pro- 
cessed five dittbrent representations of the out- 
put (IOB1, IOB2, IOE1, IOE2 and O-t-C) and 
the results have been combined with majority 
voting. The test data results can 1)e fimnd in 
Table 1. In all cases, the combined results were 
better than that of the best included system. 
Tile results of ALLiS, 05.0, MB SL and SNoW 
have tmen converted to the O and the C repre- 
4Detailed results of our experiments me available on 
http: / /lcg-www.uia.ae.be/-erikt /np('oml,i / 
SThe retagging was necessary to assure that the per- 
formance rates obtained here would be similar to rates 
obtained for texts for which no Treebank POS tags are 
available. 
section 21 
Classifier 
ALLiS 
05.0 
IGTree 
MaxEnt 
MBL 
MBSL 
SNoW 
Simple Voting 
Majority 
TotPrecision 
TagPrecision 
Precision-Recall 
0 
97.87% 
97.05% 
97.70% 
97.94% 
98.04% 
97.27% 
97.78% 
98.08% 
98.08% 
98.08% 
98.08% 
C FS=j 
98.08% 92.15 
97.76% 89.97 
97.99% 91.92 
98.24% 92.60 
98.20% 92.82 
97.66% 90.71 
97.68% 91.87 
98.21% 92.95 
98.21% 92.95 
98.21% 92.95 
98.21% 92.95 
Pairwise Voting 
TagPair 98.13% 98.23% 
Memory-Based 
Tags 98.24% 98.35% 
Tags 4- P()S 98.14% 98.33% 
Deeision Trees 
Tags 98.24% 98.35% 
Tags + POS 98.13% 98.32% 
93.07 
93.39 
93.24 
93.39 
93.21 
Table 2: Bracket accuracies and Ff~=l scores 
for section WSJ 21 of the Penn ~15'eebank with 
seve, n individual classifiers and combinations of 
them. Each combination t)erforms t)etter than 
its best individual me, tuber. The stacked classi- 
tiers without COllte, xt intbrmation perform best. 
sentation. Together with the bracket; ret)resen- 
tations of the other three techniques, this gave 
us a total of seven O results and seven C results. 
These two data streams have been combined 
with the combination techniques described in 
section 2.4. After this, we built baseNPs from 
the, O and C results of each combinatkm tech- 
nique, like, described in section 2.2. The bracket 
accuracies and tile F~=I scores tbr test data can 
be found in Table 2. 
All combinations iml)rove the results of the 
best individual classifier. The best results were 
obtained with a memory-based stacked classi- 
ter. This is different from the combination re- 
sults presented in Van Ilalteren et al (1998), 
in which pairwise voting pertbrmed best. How- 
eves, in their later work stacked classifiers out- 
perIbrm voting methods as well (Van Halteren 
et al, to appear). 
861 
section 20 accuracy precision recall 
Best-five combination 0:98.32% C:98.41% 94.18% 93.55% 
Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89% 
Mufioz et al (1999) O:98.1% C:98.2% 92.4% 93.1% 
Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27% 
Argamon et al (1999) - 91.6% 91.6% 
F/3=1 
93.86 
93.26 
92.8 
92.03 
91.6 
Table 3: The overall pertbrmance of the majority voting combination of our best five systems 
(selected on tinting data perfbrnmnce) applied to the standard data set pnt tbrward by Ramshaw 
and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how 
often a word was classified correctly with the representation used (O, C or IOB1). The combined 
system outperforms all earlier reported results tbr this data set. 
Based on an earlier combination study 
(Tjong Kim Sang, 2000) we had expected the 
voting methods to do better. We suspect hat 
their pertbrmance is below that of the stacked 
classifiers because the diflhrence between tile 
best and the worst individual system is larger 
than in our earlier study. We assume that the 
voting methods might perform better if they 
were only applied to the classifiers that per- 
form well on this task. In order to test this 
hypothesis, we have repeated the combination 
experiments with the best n classitiers, where 
n took vahms from 3 to 6 and the classifiers 
were ranked based on their performance on the 
tnning data. The t)est pertbrmances were ob- 
tained with five classifiers: F/~=1=93.44 for all 
five voting methods with tile best stacked classi- 
tier reaching 93.24. With the top five classifiers, 
tile voting methods outpertbrm the best; combi- 
nation with seven systems G. Adding extra clas- 
sification results to a good combination system 
should not make overall performance worse so 
it is clear that there is some room left for im- 
provement of our combination algorithms. 
We conclude that the best results ill this 
task can be obtained with tile simplest voting 
method, majority voting, applied to the best 
five of our classifiers. Our next task was to 
apply the combination apt)roach to a standard 
data set so that we could compare our results 
with other work. For this purpose we have used 
6V~re are unaware of a good method for determining 
the significance of F~=I differences but we assume that 
this F~=I difference is not significant. However, we be- 
lieve that the fact that more colnbination methods per- 
tbrm well, shows that it easier to get a good pertbrmmlce 
out of the best; five systems than with all seven. 
tile data put tbrward by ll,amshaw and Marcus 
(1995). Again, only 90% of the training data 
was used tbr training while the remaining 11)% 
was reserved tbr ranking the classifiers. The 
seven learners were trained with the same pa- 
rameters as in the previous experiment. Three 
of the classifiers (MBL, MaxEnt and iG%'ee) 
used system-internal combination by processing 
different output representations. 
The classifier output was converted to the 
O and the C representation. Based on the 
tuning data performance, the classifiers ALLiS, 
IGTREE, MaxEnt, MBL and SNoW were se- 
lected for being combined with majority vot- 
ing. After this, the resulting O and C repre- 
sentations were combined to baseNPs by using 
the method described in section 2.2. The re- 
sults can be found in Table 3. Our combined 
system obtains an F/~=I score of 93.86 which 
corresponds to an 8% error reduction compared 
with tile best published result tbr this data set 
(93.26). 
4 Conc lud ing  remarks  
In this paper we have examined two methods for 
combining the results of machine learuing algo- 
rithms tbr identii}cing base noun phrases. Ill the 
first Inethod, the learner processed ifferent out- 
put data representations and tile results were 
combined by majority voting. This approach 
yielded better results than the best included 
classifier. Ill the second combination approach 
we have combined the results of seven learning 
systems (ALLiS, c5.0, IGTree, MaxEnt, MBL, 
MBSL and SNOW). Here we have tested d i f  
ferent confl)ination methods. Each coilfl)ination 
862 
nmthod outt)erformed the best individual learn- 
ing algorithm and a majority vote of the tol) 
five systems peribrmed best. We, have at}i}lie, d 
this approach of system-internal nd system- 
external coral}|nation to a standard ata set for 
base noun phrase identification and the 1}ertbr- 
mance of our system was 1)etter than any other 
tmblished result tbr this data set. 
Our study shows that the c, omt)ination meth- 
(}{Is that we have tested are sensitive for the in- 
clusion of classifier esults of poor quality. This 
leaves room for imt)rovement of our results t}y 
evaluating other coml}inators. Another interest- 
ing apl)roach which might lead to a l}etter t)er- 
f{}rmance is taking into a{-com~t more context 
inibrmation, for example by coral)in|rig com- 
plete 1}hrases instead of indet}endent t}ra{:kets. 
It would also be worthwhile to evaluate using 
more elaborate me, thods lbr building baseNPs 
out of ot}en and close t}ra{:ket (:an{ti{tates. 
Acknowledgements  
l)djean, Koeling and 'l?jong Kim Sang are 
funded by the TMII. 1\]etwork Learning (Jompu- 
tational Grammars r. 1}unyakanok and Roth are 
SUl)t}orted by NFS grants IIS-98{}1638 an{t SBR- 
9873450. 
Re ferences  
Steven Alm{',y. 1996. Partial t)a\]'sing via finite- 
state cascades. In l'n, l}~wce, di'ngs of the /~,gS- 
LLI '95 l?,obust 1)arsi'n9 Worlcsh, op. 
SMomo Argam(m, Ido l)agan, an(l YllV~t\] Kry- 
molowsld. 1999. A memory-1}ased at}proach 
to learning shalh}w natural anguage patterns. 
Journal of E:rperimental and Th, eovetical AL 
11(3). 
Adam L. Berge, r, SteI}hen A. l)ellaPietra, and 
Vincent J. DellaPietra. 1996. A inaximum 
entrol)y apI)roach to natural language pro- 
cessing. Computational Linguistics, 22(1). 
Eric Bri\]l and ,lun Wu. 1998. Classifier com- 
bination tbr improved lexical disaml)iguation. 
In P~vccedings o.f COLING-A 6'15 '98. Associ- 
ation for Computational Linguistics. 
A. Carlson, C. Cunfl)y, J. Rosen, and 
D. l/,oth. 1.999. The SNoW learning archi- 
tecture. Technical Report UIUCDCS-11,-99- 
2101, UIUC Computer Science Department, 
May. 
r httl): / /lcg-www.ui',,.ac.be~/ 
Walter Daelemans, A.ntal van den Bosch, and 
Jakub Zavrel. 1999a. \])brgetting exceptions 
is harmflll in language learning. Machine 
Learning, 34(1). 
Walter Daelemans, Jakub Zavrel, Ko wmder  
Sloot, and Antal van den Bosch. 1999b. 
TiMBL: Tilb'arg Memory Bused Learner, ver- 
sion 2.G Rqfi;rence Guide. ILK Te(:hnical 
th',port 99-01. http:// i lk.kub.nl/.  
Luc Dehaspe. 1997. Maximum entropy model- 
ing with clausal constraints, in PTvcecdings oJ' 
th, c 7th, 1}l, ternational Workshop on ind'uctivc 
Logic Programming. 
Hervd Ddjean. 200(I. Theory refinement and 
natural language processing. In Proceedings 
of the ColingEO00. Association for Computa- 
tional Linguistics. 
Mitchell 17 }. Marcus, Beatrice Santorini, and 
Mary Aim Marcinkiewicz. 1993. Building a 
large mmotated corpus of english: the penn 
treebank. Computational Linguistics, 19(2). 
Marcia Munoz,  Vasin Punyakanok, l)an l l,oth, 
and Day Zimak. 1999. A learning ap- 
t}roa(:h to shallow t)arsing. In P~vceedings of 
EMNLP-WVLC'99.  Asso('iation for Coml)u- 
tational Linguisti(:s. 
J. Ross Quinlan. 1993. c/t.5: Programs for Ma- 
th,|he Learning. Morgan Kauflnann. 
Lance A. Ramshaw and Mitchell P. Marcus. 
1995. Text chunking using transformation- 
l)ase{t learn|Jig. In 1}roceeding s o\[ the Th, i'rd 
A CL Worksh, op on Ve, r~.l LacTic Corpora. As- 
sociation for Comlmtational Linguistics. 
D. Roth. 1.9!t8. Learning to resolve natural an- 
guage aml}iguities: A unified approach. In 
AAAL98.  
Erik F. Tjong Kim Sang. 2000. N{mn phrase 
recognition by system {:ombination. In Pro- 
ceedings of th, e ANLP-NAA CL-2000. Seattle, 
Washington, USA. Morgan Kauflnan Pub- 
lishers. 
Hans van Halteren, Jakub Zavrel, and Wal- 
ter Daelemans. 1998. Iml)roving data driven 
wordclass tagging by system corot}|nation. In
P~veeedings of COLING-ACL '98. Associa- 
tion tbr Computational Linguistics. 
Hans van Halteren, Jakub Zavrel, and Walter 
Daelemans. to appear, hnproving accuracy 
ill nlp through coati)|nation ofmachine learn- 
ing systems. 
863 
Learning Question Classifiers
Xin Li Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
fxli1,danrg@uiuc.edu
Abstract
In order to respond correctly to a free form factual ques-
tion given a large collection of texts, one needs to un-
derstand the question to a level that allows determining
some of the constraints the question imposes on a pos-
sible answer. These constraints may include a semantic
classification of the sought after answer and may even
suggest using different strategies when looking for and
verifying a candidate answer.
This paper presents a machine learning approach to
question classification. We learn a hierarchical classi-
fier that is guided by a layered semantic hierarchy of an-
swer types, and eventually classifies questions into fine-
grained classes. We show accurate results on a large col-
lection of free-form questions used in TREC 10.
1 Introduction
Open-domain question answering (Lehnert, 1986;
Harabagiu et al, 2001; Light et al, 2001) and story
comprehension (Hirschman et al, 1999) have be-
come important directions in natural language pro-
cessing. Question answering is a retrieval task more
challenging than common search engine tasks be-
cause its purpose is to find an accurate and concise
answer to a question rather than a relevant docu-
ment. The difficulty is more acute in tasks such as
story comprehension in which the target text is less
likely to overlap with the text in the questions. For
this reason, advanced natural language techniques
rather than simple key term extraction are needed.
One of the important stages in this process is analyz-
ing the question to a degree that allows determining
the ?type? of the sought after answer. In the TREC
competition (Voorhees, 2000), participants are re-
quested to build a system which, given a set of En-
glish questions, can automatically extract answers
(a short phrase) of no more than 50 bytes from a
5-gigabyte document library. Participants have re-
 Research supported by NSF grants IIS-9801638 and ITR IIS-
0085836 and an ONR MURI Award.
alized that locating an answer accurately hinges on
first filtering out a wide range of candidates (Hovy
et al, 2001; Ittycheriah et al, 2001) based on some
categorization of answer types.
This work develops a machine learning approach
to question classification (QC) (Harabagiu et al,
2001; Hermjakob, 2001). Our goal is to categorize
questions into different semantic classes that impose
constraints on potential answers, so that they can
be utilized in later stages of the question answering
process. For example, when considering the ques-
tion Q: What Canadian city has the largest popula-
tion?, the hope is to classify this question as having
answer type city, implying that only candidate an-
swers that are cities need consideration.
Based on the SNoW learning architecture, we de-
velop a hierarchical classifier that is guided by a lay-
ered semantic hierarchy of answer types and is able
to classify questions into fine-grained classes. We
suggest that it is useful to consider this classifica-
tion task as a multi-label classification and find that
it is possible to achieve good classification results
(over 90%) despite the fact that the number of dif-
ferent labels used is fairly large, 50. We observe that
local features are not sufficient to support this accu-
racy, and that inducing semantic features is crucial
for good performance.
The paper is organized as follows: Sec. 2 presents
the question classification problem; Sec. 3 discusses
the learning issues involved in QC and presents our
learning approach; Sec. 4 describes our experimen-
tal study.
2 Question Classification
We define Question Classification(QC) here to be
the task that, given a question, maps it to one of
k classes, which provide a semantic constraint on
the sought-after answer1. The intension is that this
1We do not address questions like ?Do you have a light??,
which calls for an action, but rather only factual Wh-questions.
classification, potentially with other constraints on
the answer, will be used by a downstream process
which selects a correct answer from among several
candidates.
A question classification module in a question an-
swering system has two main requirements. First, it
provides constraints on the answer types that allow
further processing to precisely locate and verify the
answer. Second, it provides information that down-
stream processes may use in determining answer se-
lection strategies that may be answer type specific,
rather than uniform. For example, given the ques-
tion ?Who was the first woman killed in the Vietnam
War?? we do not want to test every noun phrase
in a document to see whether it provides an answer.
At the very least, we would like to know that the
target of this question is a person, thereby reducing
the space of possible answers significantly. The fol-
lowing examples, taken from the TREC 10 question
collection, exhibit several aspects of this point.
Q: What is a prism? Identifying that the target of this
question is a definition, strategies that are specific for
definitions (e.g., using predefined templates) may be use-
ful. Similarly, in:
Q: Why is the sun yellow? Identifying that this question
asks for a reason, may lead to using a specific strategy
for reasons.
The above examples indicate that, given that dif-
ferent answer types may be searched using different
strategies, a good classification module may help
the question answering task. Moreover, determin-
ing the specific semantic type of the answer could
also be beneficial in locating the answer and veri-
fying it. For example, in the next two questions,
knowing that the targets are a city or country will
be more useful than just knowing that they are loca-
tions.
Q: What Canadian city has the largest population?
Q: Which country gave New York the Statue of Liberty?
However, confined by the huge amount of man-
ual work needed for constructing a classifier for a
complicated taxonomy of questions, most question
answering systems can only perform a coarse clas-
sification for no more than 20 classes. As a result,
existing approaches, as in (Singhal et al, 2000),
have adopted a small set of simple answer entity
types, which consisted of the classes: Person, Loca-
tion, Organization, Date, Quantity, Duration, Lin-
ear Measure. The rules used in the classification
were of the following forms:
? If a query starts with Who or Whom: type Person.
? If a query starts with Where: type Location.
? If a query contains Which or What, the head noun
phrase determines the class, as for What X questions.
While the rules used have large coverage and rea-
sonable accuracy, they are not sufficient to support
fine-grained classification. One difficulty in sup-
porting fine-grained classification is the need to ex-
tract from the questions finer features that require
syntactic and semantic analysis of questions, and
possibly, many of them. The approach we adopted
is a multi-level learning approach: some of our fea-
tures rely on finer analysis of the questions that are
outcomes of learned classifiers; the QC module then
applies learning with these as input features.
2.1 Classification Standard
Earlier works have suggested various standards of
classifying questions. Wendy Lehnert?s conceptual
taxonomy (Lehnert, 1986), for example, proposes
about 13 conceptual classes including causal an-
tecedent, goal orientation, enablement, causal con-
sequent, verification, disjunctive, and so on. How-
ever, in the context of factual questions that are
of interest to us here, conceptual categories do not
seem to be helpful; instead, our goal is to se-
mantically classify questions, as in earlier work on
TREC (Singhal et al, 2000; Hovy et al, 2001;
Harabagiu et al, 2001; Ittycheriah et al, 2001).
The key difference, though, is that we attempt to
do that with a significantly finer taxonomy of an-
swer types; the hope is that with the semantic an-
swer types as input, one can easily locate answer
candidates, given a reasonably accurate named en-
tity recognizer for documents.
2.2 Question Hierarchy
We define a two-layered taxonomy, which repre-
sents a natural semantic classification for typical
answers in the TREC task. The hierarchy con-
tains 6 coarse classes (ABBREVIATION, ENTITY,
DESCRIPTION, HUMAN, LOCATION and NU-
MERIC VALUE) and 50 fine classes, Table 1 shows
the distribution of these classes in the 500 ques-
tions of TREC 10. Each coarse class contains a
non-overlapping set of fine classes. The motiva-
tion behind adding a level of coarse classes is that of
compatibility with previous work?s definitions, and
comprehensibility. We also hoped that a hierarchi-
cal classifier would have a performance advantage
over a multi-class classifier; this point, however is
not fully supported by our experiments.
Class # Class #
ABBREV. 9 description 7
abb 1 manner 2
exp 8 reason 6
ENTITY 94 HUMAN 65
animal 16 group 6
body 2 individual 55
color 10 title 1
creative 0 description 3
currency 6 LOCATION 81
dis.med. 2 city 18
event 2 country 3
food 4 mountain 3
instrument 1 other 50
lang 2 state 7
letter 0 NUMERIC 113
other 12 code 0
plant 5 count 9
product 4 date 47
religion 0 distance 16
sport 1 money 3
substance 15 order 0
symbol 0 other 12
technique 1 period 8
term 7 percent 3
vehicle 4 speed 6
word 0 temp 5
DESCRIPTION 138 size 0
definition 123 weight 4
Table 1: The distribution of 500 TREC 10 questions
over the question hierarchy. Coarse classes (in bold) are
followed by their fine class refinements.
2.3 The Ambiguity Problem
One difficulty in the question classification task is
that there is no completely clear boundary between
classes. Therefore, the classification of a specific
question can be quite ambiguous. Consider
1. What is bipolar disorder?
2. What do bats eat?
3. What is the PH scale?
Question 1 could belong to definition or dis-
ease medicine; Question 2 could belong to food,
plant or animal; And Question 3 could be a nu-
meric value or a definition. It is hard to catego-
rize those questions into one single class and it is
likely that mistakes will be introduced in the down-
stream process if we do so. To avoid this problem,
we allow our classifiers to assign multiple class la-
bels for a single question. This method is better than
only allowing one label because we can apply all the
classes in the later precessing steps without any loss.
3 Learning a Question Classifier
Using machine learning methods for question clas-
sification is advantageous over manual methods for
several reasons. The construction of a manual clas-
sifier for questions is a tedious task that requires
the analysis of a large number of questions. More-
over, mapping questions into fine classes requires
the use of lexical items (specific words) and there-
fore an explicit representation of the mapping may
be very large. On the other hand, in our learning
approach one can define only a small number of
?types? of features, which are then expanded in a
data-driven way to a potentially large number of fea-
tures (Cumby and Roth, 2000), relying on the abil-
ity of the learning process to handle it. It is hard to
imagine writing explicitly a classifier that depends
on thousands or more features. Finally, a learned
classifier is more flexible to reconstruct than a man-
ual one because it can be trained on a new taxonomy
in a very short time.
One way to exhibit the difficulty in manually con-
structing a classifier is to consider reformulations of
a question:
What tourist attractions are there in Reims?
What are the names of the tourist attractions in Reims?
What do most tourists visit in Reims?
What attracts tourists to Reims?
What is worth seeing in Reims?
All these reformulations target the same answer
type Location. However, different words and syn-
tactic structures make it difficult for a manual clas-
sifier based on a small set of rules to generalize well
and map all these to the same answer type. Good
learning methods with appropriate features, on the
other hand, may not suffer from the fact that the
number of potential features (derived from words
and syntactic structures) is so large and would gen-
eralize and classify these cases correctly.
3.1 A Hierarchical Classifier
Question classification is a multi-class classifica-
tion. A question can be mapped to one of 50 pos-
sible classes (We call the set of all possible class
labels for a given question a confusion set (Golding
and Roth, 1999)). Our learned classifier is based
on the SNoW learning architecture (Carlson et al,
1999; Roth, 1998)2 where, in order to allow the
classifier to output more than one class label, we
map the classifier?s output activation into a condi-
tional probability of the class labels and threshold
it.
The question classifier makes use of a sequence
of two simple classifiers (Even-Zohar and Roth,
2001), each utilizing the Winnow algorithm within
SNoW. The first classifies questions into coarse
classes (Coarse Classifier) and the second into fine
classes (Fine Classifier). A feature extractor auto-
matically extracts the same features for each clas-
sifier. The second classifier depends on the first in
2Freely available at http://L2R.cs.uiuc.edu/cogcomp/cc-
software.html
ABBR, ENTITY,DESC,HUMAN,LOC,NUM
ABBR, 
ENTITY
ENTITY,
HUMAN
ENTITY, 
LOC,NUM DESC
Coarse Classifier
Fine Classifier
abb,exp ind, plant date
abb, animal, 
food, plant?
food,plant, 
ind,group? 
food, plant, 
city, state?
definition,
reason,?
Map coarse classes 
to fine classes
C0
C1
C2
C3 abb,def animal,food
all possible subsets 
of C0 wih size <= 5
all possible subsets 
of C2 with size <=5
Figure 1: The hierarchical classifier
that its candidate labels are generated by expanding
the set of retained coarse classes from the first into
a set of fine classes; this set is then treated as the
confusion set for the second classifier.
Figure 1 shows the basic structure of the hierar-
chical classifier. During either the training or the
testing stage, a question is processed along one path
top-down to get classified.
The initial confusion set of any question is C
0
=
fc
1
; c
2
; : : : ; c
n
g, the set of all the coarse classes.
The coarse classifier determines a set of preferred
labels, C
1
= Coarse Classifier(C
0
), C
1
 C
0
so that jC
1
j  5. Then each coarse class label
in C
1
is expanded to a fixed set of fine classes
determined by the class hierarchy. That is, sup-
pose the coarse class c
i
is mapped into the set
c
i
= ff
i1
; f
i2
; : : : ; f
im
g of fine classes, then C
2
=
S
c
i
2C
1
c
i
. The fine classifier determines a set of
preferred labels, C
3
= Fine Classifier(C
2
) so
that C
3
 C
2
and jC
3
j  5. C
1
and C
3
are the ul-
timate outputs from the whole classifier which are
used in our evaluation.
3.2 Feature Space
Each question is analyzed and represented as a list
of features to be treated as a training or test exam-
ple for learning. We use several types of features
and investigate below their contribution to the QC
accuracy.
The primitive feature types extracted for each
question include words, pos tags, chunks (non-
overlapping phrases) (Abney, 1991), named entities,
head chunks (e.g., the first noun chunk in a sen-
tence) and semantically related words (words that
often occur with a specific question class).
Over these primitive features (which we call
?sensors?) we use a set of operators to compose
more complex features, such as conjunctive (n-
grams) and relational features, as in (Cumby and
Roth, 2000; Roth and Yih, 2001). A simple script
that describes the ?types? of features used, (e.g.,
conjunction of two consecutive words and their pos
tags) is written and the features themselves are ex-
tracted in a data driven way. Only ?active? features
are listed in our representation so that despite the
large number of potential features, the size of each
example is small.
Among the 6 primitive feature types, pos tags,
chunks and head chunks are syntactic features while
named entities and semantically related words are
semantic features. Pos tags are extracted using
a SNoW-based pos tagger (Even-Zohar and Roth,
2001). Chunks are extracted using a previously
learned classifier (Punyakanok and Roth, 2001; Li
and Roth, 2001). The named entity classifier is
also learned and makes use of the same technol-
ogy developed for the chunker (Roth et al, 2002).
The ?related word? sensors were constructed semi-
automatically.
Most question classes have a semantically related
word list. Features will be extracted for this class if
a word in a question belongs to the list. For exam-
ple, when ?away?, which belongs to a list of words
semantically related to the class distance, occurs in
the sentence, the sensor Rel(distance) will be ac-
tive. We note that the features from these sensors are
different from those achieved using named entity
since they support more general ?semantic catego-
rization? and include nouns, verbs, adjectives rather
than just named entities.
For the sake of the experimental comparison, we
define six feature sets, each of which is an incre-
mental combination of the primitive feature types.
That is, Feature set 1 (denoted by Word) contains
word features; Feature set 2 (Pos) contains features
composed of words and pos tags and so on; The fi-
nal feature set, Feature set 6 (RelWord) contains all
the feature types and is the only one that contains
the related words lists. The classifiers will be exper-
imented with different feature sets to test the influ-
ence of different features. Overall, there are about
200; 000 features in the feature space of RelWord
due to the generation of complex features over sim-
ple feature types. For each question, up to a couple
of hundreds of them are active.
3.3 Decision Model
For both the coarse and fine classifiers, the same
decision model is used to choose class labels for
a question. Given a confusion set and a question,
SNoW outputs a density over the classes derived
from the activation of each class. After ranking the
classes in the decreasing order of density values, we
have the possible class labels C = fc
1
; c
2
; : : : ; c
n
g,
with their densities P = fp
1
; p
2
; : : : ; p
n
g (where,
P
n
1
p
i
= 1, 0  p
i
 1, 1  i  n). As dis-
cussed earlier, for each question we output the first
k classes (1  k  5), c
1
; c
2
; : : : c
k
where k satis-
fies,
k = min(argmin
t
(
t
X
1
p
i
 T ); 5) (1)
T is a threshold value in [0,1]. If we treat p
i
as
the probability that a question belongs to Class i,
the decision model yields a reasonable probabilistic
interpretation. We use T = 0:95 in the experiments.
4 Experimental Study
We designed two experiments to test the accuracy of
our classifier on TREC questions. The first experi-
ment evaluates the contribution of different feature
types to the quality of the classification. Our hi-
erarchical classifier is trained and tested using one
of the six feature sets defined in Sect. 3.2 (we re-
peated the experiments on several different training
and test sets). In the second experiment, we evalu-
ate the advantage we get from the hierarchical clas-
sifier. We construct a multi-class classifier only for
fine classes. This flat classifier takes all fine classes
as its initial confusion set and classifies a question
into fine classes directly. Its parameters and deci-
sion model are the same as those of the hierarchical
one. By comparing this flat classifier with our hi-
erarchical classifier in classifying fine classes, we
hope to know whether the hierarchical classifier has
any advantage in performance, in addition to the ad-
vantages it might have in downstream processing
and comprehensibility.
4.1 Data
Data are collected from four sources: 4,500 English
questions published by USC (Hovy et al, 2001),
about 500 manually constructed questions for a few
rare classes, 894 TREC 8 and TREC 9 questions,
and also 500 questions from TREC 10 which serves
as our test set3.
These questions were manually labeled accord-
ing to our question hierarchy. Although we allow
multiple labels for one question in our classifiers,
in our labeling, for simplicity, we assigned exactly
3The annotated data and experimental results are available
from http://L2R.cs.uiuc.edu/cogcomp/
one label to each question. Our annotators were re-
quested to choose the most suitable class accord-
ing to their own understanding. This methodology
might cause slight problems in training, when the
labels are ambiguous, since some questions are not
treated as positive examples for possible classes as
they should be. In training, we divide the 5,500
questions from the first three sources randomly into
5 training sets of 1,000, 2,000, 3,000, 4,000 and
5,500 questions. All 500 TREC 10 questions are
used as the test set.
4.2 Evaluation
In this paper, we count the number of correctly clas-
sified questions by two different precision standards
P
1
and P
5
. Suppose k
i
labels are output for the i-
th question (k
i
 5) and are ranked in a decreasing
order according to their density values. We define
I
ij
= f
1; if the correct label of the ith
question is output in rank j;
0; otherwise:
(2)
Then, P
1
=
P
m
i=1
I
i1
=m and P
5
=
P
m
i=1
P
k
i
j=1
I
ij
=m where m is the total number of
test examples. P
1
corresponds to the usual defini-
tion of precision which allows only one label for
each question, while P
5
allows multiple labels.
P
5
reflects the accuracy of our classifier with re-
spect to later stages in a question answering sys-
tem. As the results below show, although question
classes are still ambiguous, few mistakes are intro-
duced by our classifier in this step.
4.3 Experimental Results
Performance of the hierarchical classifier
Table 2 shows the P
5
precision of the hierarchi-
cal classifier when trained on 5,500 examples and
tested on the 500 TREC 10 questions. The re-
sults are quite encouraging; question classification
is shown to be solved effectively using machine
learning techniques. It also shows the contribution
of the feature sets we defined. Overall, we get a
98.80% precision for coarse classes with all the fea-
tures and 95% for the fine classes.
P
<=5
Word Pos Chunk NE Head RelWord
Coarse 92.00 96.60 97.00 97.00 97.80 98.80
Fine 86.00 86.60 87.60 88.60 89.40 95.00
Table 2: Classification results of the hierarchical clas-
sifier on 500 TREC 10 questions. Training is done on
5,500 questions. Columns show the performance for
difference feature sets and rows show the precision for
coarse and fine classes, resp. All the results are evalu-
ated using P
5
.
Inspecting the data carefully, we can observe the
significant contribution of the features constructed
based on semantically related words sensors. It is
interesting to observe that this improvement is even
more significant for fine classes.
No. Train Test P
1
P
<=5
1 1000 500 83.80 95.60
2 2000 500 84.80 96.40
3 3000 500 91.00 98.00
4 4000 500 90.80 98.00
5 5500 500 91.00 98.80
Table 3: Classification accuracy for coarse classes on
different training sets using the feature set RelWord. Re-
sults are evaluated using P
1
and P
5
.
No. Train Test P
1
P
<=5
1 1000 500 71.00 83.80
2 2000 500 77.80 88.20
3 3000 500 79.80 90.60
4 4000 500 80.00 91.20
5 5500 500 84.20 95.00
Table 4: Classification accuracy for fine classes on dif-
ferent training sets using the feature set RelWord. Re-
sults are evaluated using P
1
and P
5
.
Tables 3 and 4 show the P
1
and P
5
accuracy
of the hierarchical classifier on training sets of dif-
ferent sizes and exhibit the learning curve for this
problem.
We note that the average numbers of labels out-
put by the coarse and fine classifiers are 1.54 and
2.05 resp., (using the feature set RelWord and 5,500
training examples), which shows the decision model
is accurate as well as efficient.
Comparison of the hierarchical and the flat
classifier
The flat classifier consists of one classifier which is
almost the same as the fine classifier in the hierar-
chical case, except that its initial confusion set is
the whole set of fine classes. Our original hope was
that the hierarchical classifier would have a better
performance, given that its fine classifier only needs
to deal with a smaller confusion set. However, it
turns out that there is a tradeoff between this factor
and the inaccuracy, albeit small, of the coarse level
prediction. As the results show, there is no perfor-
mance advantage for using a level of coarse classes,
and the semantically appealing coarse classes do not
contribute to better performance.
Figure 2 give some more intuition on the flat vs.
hierarchical issue. We define the tendency of Class
i to be confused with Class j as follows:
D
ij
= Err
ij
 2=(N
i
+ N
j
); (3)
where (when using P
1
), Err
ij
is the number of
questions in Class i that are misclassified as belong-
P
1
Word Pos Chunk NE Head RelWord
h 77.60 78.20 77.40 78.80 78.80 84.20
f 52.40 77.20 77.00 78.40 76.80 84.00
P
<=5
Word Pos Chunk NE Head RelWord
h 86.00 86.60 87.60 88.60 89.40 95.00
f 83.20 86.80 86.60 88.40 89.80 95.60
Table 5: Comparing accuracy of the hierarchical (h) and
flat (f) classifiers on 500 TREC 10 question; training is
done on 5,500 questions. Results are shown for different
feature sets using P
1
and P
5
.
Fine Classes 1?50
Fi
ne
 C
la
ss
es
 1
?5
0
2 24 28 32 37 50
2
24
28
32
37
50
Figure 2: The gray?scale map of the matrix D[n,n]. The
color of the small box in position (i,j) denotes D
ij
. The
larger D
ij
is, the darker the color is. The dotted lines
separate the 6 coarse classes.
ing to Class j, and N
i
; N
j
are the numbers of ques-
tions in Class i and j resp.
Figure 2 is a gray-scale map of the matrix D[n,n].
D[n,n] is so sparse that most parts of the graph are
blank. We can see that there is no good cluster-
ing of fine classes mistakes within a coarse class,
which explains intuitively why the hierarchical clas-
sifier with an additional level coarse classes does not
work much better.
4.4 Discussion and Examples
We have shown that the overall accuracy of our clas-
sifier is satisfactory. Indeed, all the reformulation
questions that we exemplified in Sec. 3 have been
correctly classified. Nevertheless, it is constructive
to consider some cases in which the classifier fails.
Below are some examples misclassified by the hier-
archical classifier.
What French ruler was defeated at the battle of Water-
loo?
The correct label is individual, but the classifier,
failing to relate the word ?ruler? to a person, since
it was not in any semantic list, outputs event.
What is the speed hummingbirds fly ?
The correct label is speed, but the classifier outputs
animal. Our feature sensors fail to determine that
the focus of the question is ?speed?. This example
illustrates the necessity of identifying the question
focus by analyzing syntactic structures.
What do you call a professional map drawer ?
The classifier returns other entities instead of
equivalent term. In this case, both classes are ac-
ceptable. The ambiguity causes the classifier not to
output equivalent term as the first choice.
5 Conclusion
This paper presents a machine learning approach to
question classification. We developed a hierarchical
classifier that is guided by a layered semantic hier-
archy of answers types, and used it to classify ques-
tions into fine-grained classes. Our experimental re-
sults prove that the question classification problem
can be solved quite accurately using a learning ap-
proach, and exhibit the benefits of features based on
semantic analysis.
In future work we plan to investigate further the
application of deeper semantic analysis (including
better named entity and semantic categorization) to
feature extraction, automate the generation of the
semantic features and develop a better understand-
ing to some of the learning issues involved in the
difference between a flat and a hierarchical classi-
fier.
References
S. P. Abney. 1991. Parsing by chunks. In S. P. Abney
R. C. Berwick and C. Tenny, editors, Principle-based
parsing: Computation and Psycholinguistics, pages
257?278. Kluwer, Dordrecht.
A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer Science De-
partment, May.
C. Cumby and D. Roth. 2000. Relational representations
that facilitate learning. In Proc. of the International
Conference on the Principles of Knowledge Represen-
tation and Reasoning, pages 425?434.
Y. Even-Zohar and D. Roth. 2001. A sequential model
for multi class classification. In EMNLP-2001, the
SIGDAT Conference on Empirical Methods in Natu-
ral Language Processing, pages 10?19.
A. R. Golding and D. Roth. 1999. A Winnow based ap-
proach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and
P. Morarescu. 2001. Falcon: Boosting knowledge for
answer engines. In Proceedings of the 9th Text Re-
trieval Conference, NIST.
U. Hermjakob. 2001. Parsing and question classification
for question answering. In ACL-2001 Workshop on
Open-Domain Question Answering.
L. Hirschman, M. Light, E. Breck, and J. Burger. 1999.
Deep read: A reading comprehension system. In Pro-
ceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics.
E. Hovy, L. Gerber, U. Hermjakob, C. Lin, and
D. Ravichandran. 2001. Toward semantics-based an-
swer pinpointing. In Proceedings of the DARPA Hu-
man Language Technology conference (HLT). San
Diego, CA.
A. Ittycheriah, M. Franz, W-J Zhu, A. Ratnaparkhi, and
R.J. Mammone. 2001. IBM?s statistical question an-
swering system. In Proceedings of the 9th Text Re-
trieval Conference, NIST.
W. G. Lehnert. 1986. A conceptual theory of question
answering. In B. J. Grosz, K. Sparck Jones, and B. L.
Webber, editors, Natural Language Processing, pages
651?657. Kaufmann, Los Altos, CA.
X. Li and D. Roth. 2001. Exploring evidence for shal-
low parsing. In Proc. of the Annual Conference on
Computational Natural Language Learning.
M. Light, G. Mann, E. Riloff, and E. Breck. 2001.
Analyses for Elucidating Current Question Answering
Technology. Journal for Natural Language Engineer-
ing. forthcoming.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS-13; The 2000
Conference on Advances in Neural Information Pro-
cessing Systems, pages 995?1001. MIT Press.
D. Roth and W. Yih. 2001. Relational learning via
propositional algorithms: An information extraction
case study. In Proc. of the International Joint Confer-
ence on Artificial Intelligence, pages 1257?1263.
D. Roth, G. Kao, X. Li, R. Nagarajan, V. Punyakanok,
N. Rizzolo, W. Yih, C. O. Alm, and L. G. Moran.
2002. Learning components for a question answering
system. In TREC-2001.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proc. of the Ameri-
can Association of Artificial Intelligence, pages 806?
813.
A. Singhal, S. Abney, M. Bacchiani, M. Collins, D. Hin-
dle, and F. Pereira. 2000. AT&T at TREC-8. In Pro-
ceedings of the 8th Text Retrieval Conference, NIST.
E. Voorhees. 2000. Overview of the TREC-9 question
answering track. In The Ninth Text Retrieval Confer-
ence (TREC-9), pages 71?80. NIST SP 500-249.
Probabilistic Reasoning for Entity & Relation Recognition?
Dan Roth Wen-tau Yih
Department of Computer Science
University of Illinois at Urbana-Champaign
{danr, yih}@uiuc.edu
Abstract
This paper develops a method for recognizing rela-
tions and entities in sentences, while taking mutual
dependencies among them into account. E.g., the kill
(Johns, Oswald) relation in: ?J. V. Oswald was
murdered at JFK after his assassin,
K. F. Johns...? depends on identifying Oswald
and Johns as people, JFK being identified as a location,
and the kill relation between Oswald and Johns; this, in
turn, enforces that Oswald and Johns are people.
In our framework, classifiers that identify entities and
relations among them are first learned from local infor-
mation in the sentence; this information, along with con-
straints induced among entity types and relations, is used
to perform global inference that accounts for the mutual
dependencies among the entities.
Our preliminary experimental results are promising
and show that our global inference approach improves
over learning relations and entities separately.
1 Introduction
Recognizing and classifying entities and relations in text
data is a key task in many NLP problems such as in-
formation extraction (IE) (Califf and Mooney, 1999;
Freitag, 2000; Roth and Yih, 2001), question an-
swering (QA) (Voorhees, 2000) and story comprehen-
sion (Hirschman et al, 1999). In a typical IE application
of constructing a jobs database from unstructured text,
the system has to extract meaningful entities like title and
salary and, ideally, to determine whether the entities are
associated with the same position. In a QA system, many
questions ask for specific entities involved in some rela-
tions. For example, the question ?Where was Poe born??
in TREC-9 asks for the location entity in which Poe was
born. The question ?Who killed Lee Harvey Oswald??
seeks a person entity that has the relation kill with the
person Lee Harvey Oswald.
In all earlier works we know of, the tasks of identify-
ing entities and relations were treated as separate prob-
lems. The common procedure is to first identify and clas-
sify entities using a named entity recognizer and only
? Research supported by NSF grants CAREER IIS-9984168 and ITR
IIS-0085836 and an ONR MURI Award.
then determine the relations between the entities. How-
ever, this approach has several problems. First, errors
made by the named entity recognizer propagate to the
relation classifier and may degrade its performance sig-
nificantly. For example, if ?Boston? is mislabeled as a
person, it will never be classified as the location of Poe?s
birthplace. Second, relation information is sometimes
crucial to resolving ambiguous named entity recognition.
For instance, if the entity ?JFK? is identified as the vic-
tim of the assassination, the named entity recognizer is
unlikely to misclassify it as a location (e.g. JFK airport).
This paper develops a novel approach for this prob-
lem ? a probabilistic framework for recognizing entities
and relations together. In this framework, separate clas-
sifiers are first trained for entities and relations. Their
output is used to represent a conditional distribution for
each entity and relation, given the observed data. This
information, along with constraints induced among rela-
tions and entities (e.g. the first argument of kill is likely
to be a person; the second argument of born in is a lo-
cation) are used to make global inferences for the most
probable assignment for all entities and relations of in-
terest. Our global inference approach accepts as input
conditional probabilities which are the outcomes of ?lo-
cal? classifiers. Note that each of the local classifiers
could depend on a large number of features, but these
are not viewed as relevant to the inference process and
are abstracted away in this process of ?inference with
classifiers?. In this sense, this work extends previous
works in this paradigm, such as (Punyakanok and Roth,
2001), in which inference with classifiers was studied
when the outcomes of the classifiers were sequentially
constrained; here the constraints are more general, which
necessitates a different inference approach.
The rest of the paper is organized as follows. Sec-
tion 2 defines the problem in a formal way. Section 3
describes our approach to this problem. It first intro-
duces how we learn the classifiers, and then introduces
the belief network we use to reason for global predic-
tions. Section 4 records preliminary experiments we ran
and exhibits some promising results. Finally, section 5
discusses some of the open problems and future work in
this framework.
E1
R31
SpellingPOS
...Label
Label-1Label-2
...Label-n
E2 E3
R32R32
R23R12
R13
Figure 1: Conceptual view of entities and relations
2 Global Inference of Entities/Relations
The problem at hand is that of producing a coherent la-
beling of entities and relations in a given sentence. Con-
ceptually, the entities and relations can be viewed, tak-
ing into account the mutual dependencies, as the labeled
graph in Figure 1, where the nodes represent entities
(e.g. phrases) and the links denote the binary relations
between the entities. Each entity and relation has sev-
eral properties ? denoted as labels of nodes and edges in
the graph. Some of the properties, such as words inside
the entities, can be read directly from the input; others,
like pos tags of words in the context of the sentence, are
easy to acquire via learned classifiers. However, proper-
ties like semantic types of phrases (i.e., class labels, such
as ?people?, ?locations?) and relations among them are
more difficult to acquire. Identifying the labels of entities
and relations is treated here as the target of our learning
problem. In particular, we learn these target properties
as functions of all other ?simple to acquire? properties of
the sentence.
To describe the problem in a formal way, we first de-
fine sentences and entities as follows.
Definition 2.1 (Sentence & Entity) A sentence S is a
linked list which consists of words w and entities E. An
entity can be a single word or a set of consecutive words
with a predefined boundary. Entities in a sentence are
labeled as E1, E2, ? ? ? according to their order, and they
take values that range over a set of entity types CE .
Notice that determining the entity boundaries is also
a difficult problem ? the segmentation (or phrase de-
tection) problem (Abney, 1991; Punyakanok and Roth,
2001). Here we assume it is solved and given to us as
input; thus we only concentrate on classification.
Example 2.1 The sentence in Figure 2 has three enti-
ties: E1 = ?Dole?, E2 = ?Elizabeth?, and E3 = ?Sal-
isbury, N.C.?
Dole ?s wife , Elizabeth , is a native of Salisbury , N.C.
 E1         E2                E3
Figure 2: A sentence that has three entities
A relation is defined by the entities that are involved in
it (its arguments). In this paper, we only discuss binary
relations.
Definition 2.2 (Relation) A (binary) relation Rij =
(Ei, Ej) represents the relation between Ei and Ej ,
where Ei is the first argument and Ej is the second. In
addition, Rij can range over a set of entity types CR.
Example 2.2 In the sentence given in Figure 2, there are
six relations between the entities: R12 = (?Dole?, ?Eliz-
abeth?), R21 = (?Elizabeth?, ?Dole?), R13 = (?Dole?,
?Salisbury, N.C.?), R31 = (?Salisbury, N.C.?, ?Dole?),
R23 = (?Elizabeth?, ?Salisbury, N.C.?), and R32 =
(?Salisbury, N.C.?, ?Elizabeth?)
We define the types (i.e. classes) of relations and enti-
ties as follows.
Definition 2.3 (Classes) We denote the set of predefined
entity classes and relation classes as CE and CR respec-
tively. CE has one special element other ent, which rep-
resents any unlisted entity class. Similarly, CR also has
one special element other rel, which means the involved
entities are irrelevant or the relation class is undefined.
When clear from the context, we use Ei and Rij to refer
to the entity and relation, as well as their types (class
labels).
Example 2.3 Suppose CE = { other ent, person, lo-
cation } and CR = { other rel, born in, spouse of }.
For the entities in Figure 2, E1 and E2 belong to person
and E3 belongs to location. In addition, relation R23 is
born in, R12 and R21 are spouse of. Other relations are
other rel.
The class label of a single entity or relation depends
not only on its local properties, but also on properties
of other entities and relations. The classification task is
somewhat difficult since the predictions of entity labels
and relation labels are mutually dependent. For instance,
the class label of E1 depends on the class label of R12
and the class label of R12 also depends on the class la-
bel of E1 and E2. While we can assume that all the
data is annotated for training purposes, this cannot be
assumed at evaluation time. We may presume that some
local properties such as the word, pos, etc. are given, but
none of the class labels for entities or relations is.
To simplify the complexity of the interaction within
the graph but still preserve the characteristic of mutual
dependency, we abstract this classification problem in the
following probabilistic framework. First, the classifiers
are trained independently and used to estimate the proba-
bilities of assigning different labels given the observation
(that is, the easily classified properties in it). Then, the
output of the classifiers is used as a conditional distribu-
tion for each entity and relation, given the observation.
This information, along with the constraints among the
relations and entities, is used to make global inferences
for the most probable assignment of types to the entities
and relations involved.
The class labels of entities and relations in a sentence
must satisfy some constraints. For example, if E1, the
first argument of R12, is a location, then R12 cannot be
born in because the first argument of relation born in has
to be a person. We define constraints as follows.
Definition 2.4 (Constraint) A constraint C is a 3-tuple
(R, E1, E2), where R ? CR and E1, E2 ? CE . If the
class label of a relation is R, then the legitimate class
labels of its two entity arguments are E1 and E2 respec-
tively.
Example 2.4 Some examples of constraints are:
(born in, person, location), (spouse of, person, person),
and (murder, person, person)
The constraints described above could be modeled us-
ing a joint probability distribution over the space of val-
ues of the relevant entities and relations. In the context of
this work, for algorithmic reasons, we model only some
of the conditional probabilities. In particular, the proba-
bility P (Rij |Ei, Ej) has the following properties.
Property 1 The probability of the label of relation Rij
given the labels of its arguments Ei and Ej has the fol-
lowing properties.
? P (Rij = other rel|Ei = e1, Ej = e2) = 1, if there
exists no r, such that (r, e1, e2) is a constraint.
? P (Rij = r|Ei = e1, Ej = e2) = 0, if there exists
no constraint c, such that c = (r, e1, e2).
Note that the conditional probabilities do not need to
be specified manually. In fact, they can be easily learned
from an annotated training dataset.
Under this framework, finding the most suitable
coherent labels becomes the problem of searching
the most probable assignment to all the E and R
variables. In other words, the global prediction
e1, e2, ..., en, r12, r21, ..., rn(n?1) satisfies the following
equation.
(e1, ..., en, r12, r21, ..., rn(n?1)) =
argmaxei,rjkProb(E1, ..., En, R12, R21, ..., Rn(n?1)).
3 Computational Approach
Each nontrivial property of the entities and relations,
such as the class label, depends on a very large number
of variables. In order to predict the most suitable co-
herent labels, we would like to make inferences on sev-
eral variables. However, when modeling the interaction
between the target properties, it is crucial to avoid ac-
counting for dependencies among the huge set of vari-
ables on which these properties depend. Incorporating
these dependencies into our inference is unnecessary and
will make the inference intractable. Instead, we can ab-
stract these dependencies away by learning the proba-
bility of each property conditioned upon an observation.
The number of features on which this learning problem
depends could be huge, and they can be of different gran-
ularity and based on previous learned predicates (e.g.
pos), as caricatured using the ?network-like? structure in
Figure 1. Inference is then made based on the probabili-
ties. This approach is similar to (Punyakanok and Roth,
2001; Lafferty et al, 2001) only that there it is restricted
to sequential inference, and done for syntactic structures.
The following subsections describe the details of these
two stages. Section 3.1 explains the feature extraction
method and learning algorithm we used. Section 3.2 in-
troduces the idea of using a belief network in search of
the best global class labeling and the applied inference
algorithm.
3.1 Learning Basic Classifiers
Although the labels of entities and relations from a sen-
tence mutually depend on each other, two basic classi-
fiers for entities and relations are first learned, in which
a multi-class classifier for E(or R) is learned as a func-
tion of all other ?known? properties of the observation.
The classifier for entities is a named entity classifier, in
which the boundary of an entity is predefined (Collins
and Singer, 1999). On the other hand, the relation clas-
sifier is given a pair of entities, which denote the two
arguments of the target relation. Accurate predictions of
these two classifiers seem to rely on complicated syntax
analysis and semantics related information of the whole
sentence. However, we derive weak classifiers by treat-
ing these two learning tasks as shallow text processing
problems. This strategy has been successfully applied on
several NLP tasks, such as information extraction (Califf
and Mooney, 1999; Freitag, 2000; Roth and Yih, 2001)
and chunking (i.e. shallow paring) (Munoz et al, 1999).
It assumes that the class labels can be decided by lo-
cal properties, such as the information provided by the
words around or inside the target. Examples include
the spelling of a word, part-of-speech, and semantic re-
lated attributes acquired from external resources such as
WordNet.
The propositional learner we use is SNoW (Roth,
1998; Carleson et al, 1999) 1 SNoW is a multi-class clas-
sifier that is specifically tailored for large scale learning
tasks. The learning architecture makes use of a network
of linear functions, in which the targets (entity classes
or relation classes, in this case) are represented as linear
1available at http://L2R.cs.uiuc.edu/?cogcomp/cc-software.html
functions over a common feature space. Within SNoW,
we use here a learning algorithm which is a variation of
Winnow (Littlestone, 1988), a feature efficient algorithm
that is suitable for learning in NLP-like domains, where
the number of potential features is very large, but only a
few of them are active in each example, and only a small
fraction of them are relevant to the target concept.
While typically SNoW is used as a classifier, and pre-
dicts using a winner-take-all mechanism over the activa-
tion value of the target classes, here we rely directly on
the raw activation value it outputs, which is the weighted
linear sum of the features, to estimate the posteriors.
It can be verified that the resulting values are mono-
tonic with the confidence in the prediction, therefore is
a good source of probability estimation. We use softmax
(Bishop, 1995) over the raw activation values as proba-
bilities. Specifically, suppose the number of classes is n,
and the raw activation values of class i is acti. The pos-
terior estimation for class i is derived by the following
equation.
pi = e
acti
?
1?j?n eactj
3.2 Bayesian Inference Model
Broadly used in the AI community, belief network
is a graphical representation of a probability distri-
bution (Pearl, 1988). It is a directed acyclic graph
(DAG), where the nodes are random variables and
each node is associated with a conditional probabil-
ity table which defines the probability given its par-
ents. We construct a belief network that represents
the constraints existing among R?s and E?s. Then,
for each sentence, we use the classifiers from sec-
tion 3.1 to compute the Prob(E|observations) and
Prob(R|observations), and use the belief network to
compute the most probable global predictions of the class
labels.
The structure of our belief network, which represents
the constraints is a bipartite graph. In particular, the vari-
able E?s and R?s are the nodes in the network, where the
E nodes are in one layer, and the R nodes are in the other.
Since the label of a relation is dependent on the entity
classes of its arguments, the links in the network connect
the entity nodes, and the relation nodes that have these
entities as arguments. For instance, node Rij has two
incoming links from nodes Ei and Ej . The conditional
probabilities P (Rij |Ei, Ej) encodes the constraints as in
Property 1. As an illustration, Figure 3 shows a be-
lief network that consists of 3 entity nodes and 6 relation
nodes.
Finding a most probable class assignment to the en-
tities and relations is equivalent to finding the assign-
ment of all the variables in the belief network that
maximizes the joint probability. However, this most-
probable-explanation (MPE) inference problem is in-
tractable (Roth, 1996) if the network contains loops
E2
E1
E3
R12
R21
R13
R31
R23
R32
P(R12|X)
P(R21|X)
P(R13|X)
P(R31|X)
P(R23|X)
P(R32|X)
P(E1|X)
P(E2|X)
P(E3|X)
Figure 3: Belief network of 3 entity nodes and 6 relation
nodes
(undirected cycles), which is exactly the case in our net-
work. Therefore, we resort to the following approxima-
tion method instead.
Recently, researchers have achieved great success in
solving the problem of decoding messages through a
noisy channel with the help of belief networks (Gal-
lager, 1962; MacKay, 1999). The network structure used
in their problem is similar to the network used here,
namely a loopy bipartite DAG. The inference algorithm
they used is Pearl?s belief propagation algorithm (Pearl,
1988), which outputs exact posteriors in linear time if the
network is singly connected (i.e. without loops) but does
not guarantee to converge for loopy networks. However,
researchers have empirically demonstrate that by iterat-
ing the belief propagation algorithm several times, the
outputted values often converge to the right posteriors
(Murphy et al, 1999). Due to the existence of loops, we
also apply belief propagation algorithm iteratively as our
inference procedure.
4 Experiments
The following subsections describe the data preparation
process, the approaches tested in the experiments, and
the experimental results.
4.1 Data Preparation
In order to build different datasets, we first collected sen-
tences from TREC documents, which are mostly daily
news such as Wall Street Journal, Associated Press, and
San Jose Mercury News. Among the collected sentences,
245 sentences contain relation kill (i.e. two entities that
have the murder-victim relation). 179 sentences contain
relation born in (i.e. a pair of entities where the second
is the birthplace of the first). In addition to the above
sentences, we also collected 502 sentences that contain
no relations.2
2available at http://l2r.cs.uiuc.edu/?cogcomp/Data/ER/
Entities in these sentences are segmented by the sim-
ple rule: consecutive proper nouns and commas are com-
bined and treated as an entity. Predefined entity class la-
bels include other ent, person, and location. Moreover,
relations are defined by every pair of entities in a sen-
tence, and the relation class labels defined are other rel,
kill, and birthplace.
Three datasets are constructed using the collected sen-
tences. Dataset ?kill? has all the 245 sentences of re-
lation kill. Dataset ?born in? has all the 179 sentences
of relation born in. The third dataset ?all? mixes all the
sentences.
4.2 Tested Approaches
We compare three approaches in the experiments: basic,
omniscient, and BN. The first approach, basic, tests our
baseline ? the performance of the basic classifiers. As
described in Section 3.1, these classifiers are learned in-
dependently using local features and make predictions on
entities and relations separately. Without taking global
interactions into account, the features extracted are de-
scribed as follows. For the entity classifier, features from
the words around each entity are: words, tags, conjunc-
tions of words and tags, bigram and trigram of words and
tags. Features from the entity itself include the number
of words it contains, bigrams of words in it, and some
attributes of the words inside such as the prefix and suf-
fix. In addition, whether the entity has some strings that
match the names of famous people and places is also
used as a feature. For the relation classifier, features are
extracted from words around and between the two en-
tity arguments. The types of features include bigrams,
trigrams, words, tags, and words related to ?kill? and
?birth? retrieved from WordNet.
The second approach, omniscient, is similar to basic.
The only difference here is the labels of entities are re-
vealed to the R classifier and vice versa. It is certainly
impossible to know the true entity and relation labels in
advance. However, this experiment may give us some
ideas about how much the performance of the entity clas-
sifier can be enhanced by knowing whether the target is
involved in some relations, and also how much the rela-
tion classifier can be benefited from knowing the entity
labels of its arguments. In addition, it also provides a
comparison to see how well the belief network inference
model can improve the results.
The third approach, BN, tests the ability of making
global inferences in our framework. We use the Bayes
Net Toolbox for Matlab by Murphy 3 to implement the
network and set the maximum number of the iteration of
belief propagation algorithm as 20. Given the probabili-
ties estimated by basic classifiers, the network infers the
labels of the entities and relations globally in a sentence.
Compared to the first two approaches, where some pre-
dictions may violate the constraints, the belief network
model incorporates the constraints between entities and
3available at http://www.cs.berkeley.edu/?murphyk/Bayes/bnt.html
relations, thus all the predictions it makes will be coher-
ent.
All the experiments of these approaches are done in
5-fold validation. In other words, these datasets are ran-
domly separated into 5 disjoint subsets, and experiments
are done 5 times by iteratively using 4 of them as training
data and the rest as testing.
4.3 Results
The experimental results in terms of recall, precision,and
F?=1 for datasets ?kill?, ?born in?, and ?all? are given
in Table 1, Table 2, and Table 3 respectively. We discuss
two interesting facts of the results as follows.
First, the belief network approach tends to decrease re-
call in a small degree but increase precision significantly.
This phenomenon is especially clear on the classification
results of some relations. As a result, the F1 value of
the relation classification results is still enhanced to the
extent that is near or even higher than the results of the
Omniscient approach. This may be explained by the fact
that if the label of a relation is predicted as positive (i.e.
not other rel), the types of its entity arguments must sat-
isfy the constraints. This inference process reduces the
number of false positive, thus enhance the precision.
Second, knowing the class labels of relations does not
seem to help the entity classifier much. In all three
datasets, the difference of Basic and Omniscient ap-
proaches is usually less than 3% in terms of F1, which
is not very significant given the size of our datasets. This
phenomenon may be due to the fact that only a few of en-
tities in a sentence are involved in some relations. There-
fore, it is unlikely that the entity classifier can use the
relation information to correct its prediction.
Approach person location
Rec Prec F1 Rec Prec F1
Basic 96.6 92.3 94.4 76.3 91.9 83.1
BN 89.0 96.1 92.4 78.8 86.3 82.1
Omniscient 96.4 92.6 94.5 75.4 90.2 81.9
Approach kill
Rec Prec F1
Basic 61.8 57.2 58.6
BN 49.8 85.4 62.2
Omniscient 67.7 63.6 64.8
Table 1: Results for dataset ?kill?
5 Discussion
The promising results of our preliminary experiments
demonstrate the feasibility of our probabilistic frame-
work. For the future work, we plan to extend this re-
search in the following directions.
The first direction we would like to explore is to apply
our framework in a boot-strapping manner. The main dif-
ficulty in applying learning on NLP problems is not lack
of text corpus, but lack of labeled data. Boot-strapping,
applying the classifiers to autonomously annotate the
Approach person location
Rec Prec F1 Rec Prec F1
Basic 85.5 90.7 87.8 89.5 93.2 91.1
BN 87.0 90.9 88.8 87.5 93.4 90.3
Omniscient 90.6 93.4 91.7 90.7 96.5 93.4
Approach born in
Rec Prec F1
Basic 81.4 63.4 70.9
BN 87.6 70.7 78.0
Omniscient 86.9 71.8 78.0
Table 2: Results for dataset ?born in?
Approach person location
Rec Prec F1 Rec Prec F1
Basic 92.1 87.0 89.4 83.2 81.1 82.0
BN 78.8 94.7 86.0 83.0 81.3 82.1
Omniscient 93.4 87.3 90.2 83.5 83.1 83.2
Approach kill born in
Rec Prec F1 Rec Prec F1
Basic 43.8 78.6 55.0 69.0 72.9 70.5
BN 47.2 86.8 60.7 68.4 87.5 76.6
Omniscient 52.8 79.5 62.1 76.1 71.3 73.2
Table 3: Results for dataset ?all?
data and using the new data to train and improve exist-
ing classifiers, is a promising approach. Since the pre-
cision of our framework is pretty high, it seems possible
to use the global inference to annotate new data. Based
on this property, we can derive an EM-like approach for
labelling and inferring the types of entities and relations
simultaneously. The basic idea is to use the global infer-
ence output as a means to annotate entities and relations.
The new annotated data can then be used to train classi-
fiers, and the whole process is repeated again.
The second direction is to improve our probabilistic
inference model in several ways. First, since the results
of the inference procedure we use, the loopy belief prop-
agation algorithm, produces approximate values, some
of the results can be wrong. Although the computational
time of the exact inference algorithm for loopy network
is exponential, we may still be able to run it given the
small number of variables that are of interest each time
in our case. Therefore, we can further check if the perfor-
mance suffers from the approximation. Second, the be-
lief network model may not be expressive enough since
it allows no cycles. To fully model the problem, cycles
may be needed. For example, the class labels of R12
and R21 actually depend on each other. (e.g. If R12 is
born in, then R21 will not be born in or kill.) Similarly,
the class labels of E1 and E2 can depend on the labels
of R12. To fully represent the mutual dependencies, we
would like to explore other probabilistic models that are
more expressive than the belief network.
References
S. P. Abney. 1991. Parsing by chunks. In S. P. Abney
R. C. Berwick and C. Tenny, editors, Principle-based
parsing: Computation and Psycholinguistics, pages
257?278. Kluwer, Dordrecht.
C. Bishop, 1995. Neural Networks for Pattern Recogni-
tion, chapter 6.4: Modelling conditional distributions,
page 215. Oxford University Press.
M. Califf and R. Mooney. 1999. Relational learning of
pattern-match rules for information extraction. In Na-
tional Conference on Artificial Intelligence.
A. Carleson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer Science De-
partment, May.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for name entity classification. In EMNLP-VLC?99,
the Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, June.
D. Freitag. 2000. Machine learning for information
extraction in informal domains. Machine Learning,
39(2/3):169?202.
R. Gallager. 1962. Low density parity check codes. IRE
Trans. Info. Theory, IT-8:21?28, Jan.
L. Hirschman, M. Light, E. Breck, and J. Burger. 1999.
Deep read: A reading comprehension system. In Pro-
ceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the
International Conference on Machine Learning.
N. Littlestone. 1988. Learning quickly when irrelevant
attributes abound: A new linear-threshold algorithm.
Machine Learning, 2:285?318.
D. MacKay. 1999. Good error-correcting codes based
on very sparse matrices. IEEE Transactions on Infor-
mation Theory, 45.
M. Munoz, V. Punyakanok, D. Roth, and D. Zimak.
1999. A learning approach to shallow parsing. In
EMNLP-VLC?99, the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora, June.
K. Murphy, Y. Weiss, and M. Jordan. 1999. Loopy belief
propagation for approximate inference: An empirical
study. In Proc. of Uncertainty in AI, pages 467?475.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems. Morgan Kaufmann.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS-13; The 2000
Conference on Advances in Neural Information Pro-
cessing Systems.
D. Roth and W. Yih. 2001. Relational learning via
propositional algorithms: An information extraction
case study. In Proc. of the International Joint Con-
ference on Artificial Intelligence, pages 1257?1263.
D. Roth. 1996. On the hardness of approximate reason-
ing. Artificial Inteligence, 82(1-2):273?302, April.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proc. National Con-
ference on Artificial Intelligence, pages 806?813.
E. Voorhees. 2000. Overview of the trec-9 question an-
swering track. In The Ninth Text Retrieval Conference
(TREC-9), pages 71?80. NIST SP 500-249.
Semantic Role Labeling via Integer Linear Programming Inference
Vasin Punyakanok Dan Roth Wen-tau Yih Dav Zimak
Department of Computer Science
University of Illinois at Urbana-Champaign
{punyakan,danr,yih,davzimak}@uiuc.edu
Abstract
We present a system for the semantic role la-
beling task. The system combines a machine
learning technique with an inference procedure
based on integer linear programming that sup-
ports the incorporation of linguistic and struc-
tural constraints into the decision process. The
system is tested on the data provided in CoNLL-
2004 shared task on semantic role labeling and
achieves very competitive results.
1 Introduction
Semantic parsing of sentences is believed to be an
important task toward natural language understand-
ing, and has immediate applications in tasks such
information extraction and question answering. We
study semantic role labeling(SRL). For each verb in
a sentence, the goal is to identify all constituents
that fill a semantic role, and to determine their roles,
such as Agent, Patient or Instrument, and their ad-
juncts, such as Locative, Temporal or Manner.
The PropBank project (Kingsbury and Palmer,
2002) provides a large human-annotated corpus
of semantic verb-argument relations. Specifically,
we use the data provided in the CoNLL-2004
shared task of semantic-role labeling (Carreras and
Ma`rquez, 2003) which consists of a portion of the
PropBank corpus, allowing us to compare the per-
formance of our approach with other systems.
Previous approaches to the SRL task have made
use of a full syntactic parse of the sentence in or-
der to define argument boundaries and to determine
the role labels (Gildea and Palmer, 2002; Chen and
Rambow, 2003; Gildea and Hockenmaier, 2003;
Pradhan et al, 2003; Pradhan et al, 2004; Sur-
deanu et al, 2003). In this work, following the
CoNLL-2004 shared task definition, we assume that
the SRL system takes as input only partial syn-
tactic information, and no external lexico-semantic
knowledge bases. Specifically, we assume as input
resources a part-of-speech tagger, a shallow parser
that can process the input to the level of based
chunks and clauses (Tjong Kim Sang and Buch-
holz, 2000; Tjong Kim Sang and De?jean, 2001),
and a named-entity recognizer (Tjong Kim Sang
and De Meulder, 2003). We do not assume a full
parse as input.
SRL is a difficult task, and one cannot expect
high levels of performance from either purely man-
ual classifiers or purely learned classifiers. Rather,
supplemental linguistic information must be used
to support and correct a learning system. So far,
machine learning approaches to SRL have incorpo-
rated linguistic information only implicitly, via the
classifiers? features. The key innovation in our ap-
proach is the development of a principled method to
combine machine learning techniques with linguis-
tic and structural constraints by explicitly incorpo-
rating inference into the decision process.
In the machine learning part, the system we
present here is composed of two phases. First, a
set of argument candidates is produced using two
learned classifiers?one to discover beginning po-
sitions and one to discover end positions of each
argument type. Hopefully, this phase discovers a
small superset of all arguments in the sentence (for
each verb). In a second learning phase, the candi-
date arguments from the first phase are re-scored
using a classifier designed to determine argument
type, given a candidate argument.
Unfortunately, it is difficult to utilize global prop-
erties of the sentence into the learning phases.
However, the inference level it is possible to in-
corporate the fact that the set of possible role-
labelings is restricted by both structural and lin-
guistic constraints?for example, arguments cannot
structurally overlap, or, given a predicate, some ar-
gument structures are illegal. The overall decision
problem must produce an outcome that consistent
with these constraints. We encode the constraints as
linear inequalities, and use integer linear program-
ming(ILP) as an inference procedure to make a fi-
nal decision that is both consistent with the con-
straints and most likely according to the learning
system. Although ILP is generally a computation-
ally hard problem, there are efficient implementa-
tions that can run on thousands of variables and con-
straints. In our experiments, we used the commer-
cial ILP package (Xpress-MP, 2003), and were able
to process roughly twenty sentences per second.
2 Task Description
The goal of the semantic-role labeling task is to dis-
cover the verb-argument structure for a given input
sentence. For example, given a sentence ? I left my
pearls to my daughter-in-law in my will?, the goal is
to identify different arguments of the verb left which
yields the output:
[A0 I] [V left ] [A1 my pearls] [A2 to my daughter-
in-law] [AM-LOC in my will].
Here A0 represents the leaver, A1 represents the
thing left, A2 represents the benefactor, AM-LOC
is an adjunct indicating the location of the action,
and V determines the verb.
Following the definition of the PropBank, and
CoNLL-2004 shared task, there are six different
types of arguments labelled as A0-A5 and AA.
These labels have different semantics for each verb
as specified in the PropBank Frame files. In addi-
tion, there are also 13 types of adjuncts labelled as
AM-XXX where XXX specifies the adjunct type.
In some cases, an argument may span over differ-
ent parts of a sentence, the label C-XXX is used to
specify the continuity of the arguments, as shown in
the example below.
[A1 The pearls] , [A0 I] [V said] , [C-A1 were left
to my daughter-in-law].
Moreover in some cases, an argument might be a
relative pronoun that in fact refers to the actual agent
outside the clause. In this case, the actual agent is la-
beled as the appropriate argument type, XXX, while
the relative pronoun is instead labeled as R-XXX.
For example,
[A1 The pearls] [R-A1 which] [A0 I] [V left] , [A2
to my daughter-in-law] are fake.
See the details of the definition in Kingsbury and
Palmer (2002) and Carreras and Ma`rquez (2003).
3 System Architecture
Our semantic role labeling system consists of two
phases. The first phase finds a subset of arguments
from all possible candidates. The goal here is to
filter out as many as possible false argument candi-
dates, while still maintaining high recall. The sec-
ond phase focuses on identifying the types of those
argument candidates. Since the number of candi-
dates is much fewer, the second phase is able to use
slightly complicated features to facilitate learning
a better classifier. This section first introduces the
learning system we use and then describes how we
learn the classifiers in these two phases.
3.1 SNoW Learning Architecture
The learning algorithm used is a variation of the
Winnow update rule incorporated in SNoW (Roth,
1998; Roth and Yih, 2002), a multi-class classifier
that is specifically tailored for large scale learning
tasks. SNoW learns a sparse network of linear func-
tions, in which the targets (argument border predic-
tions or argument type predictions, in this case) are
represented as linear functions over a common fea-
ture space. It incorporates several improvements
over the basic Winnow multiplicative update rule.
In particular, a regularization term is added, which
has the effect of trying to separate the data with a
thick separator (Grove and Roth, 2001; Hang et al,
2002). In the work presented here we use this regu-
larization with a fixed parameter.
Experimental evidence has shown that SNoW
activations are monotonic with the confidence in
the prediction. Therefore, it can provide a good
source of probability estimation. We use soft-
max (Bishop, 1995) over the raw activation values
as conditional probabilities, and also the score of the
target. Specifically, suppose the number of classes
is n, and the raw activation values of class i is acti.
The posterior estimation for class i is derived by the
following equation.
score(i) = pi = e
acti
?
1?j?n eactj
The score plays an important role in different
places. For example, the first phase uses the scores
to decide which argument candidates should be fil-
tered out. Also, the scores output by the second-
phase classifier are used in the inference procedure
to reason for the best global labeling.
3.2 First Phase: Find Argument Candidates
The first phase is to predict the argument candidates
of a given sentence that correspond to the active
verb. Unfortunately, it turns out that it is difficult to
predict the exact arguments accurately. Therefore,
the goal here is to output a superset of the correct
arguments by filtering out unlikely candidates.
Specifically, we learn two classifiers, one to de-
tect beginning argument locations and the other
to detect end argument locations. Each multi-
class classifier makes predictions over forty-three
classes?thirty-two argument types, ten continuous
argument types, and one class to detect not begin-
ning/not end. Features used for these classifiers are:
? Word feature includes the current word, two
words before and two words after.
? Part-of-speech tag (POS) feature includes the
POS tags of all words in a window of size two.
? Chunk feature includes the BIO tags for
chunks of all words in a window of size two.
? Predicate lemma & POS tag show the lemma
form and POS tag of the active predicate.
? Voice feature is the voice (active/passive) of
the current predicate. This is extracted with a
simple rule: a verb is identified as passive if it
follows a to-be verb in the same phrase chunk
and its POS tag is VBN(past participle) or it
immediately follows a noun phrase.
? Position feature describes if the current word
is before or after the predicate.
? Chunk pattern encodes the sequence of
chunks from the current words to the predicate.
? Clause tag indicates the boundary of clauses.
? Clause path feature is a path formed from a
semi-parsed tree containing only clauses and
chunks. Each clause is named with the chunk
preceding it. The clause path is the path from
predicate to target word in the semi-parse tree.
? Clause position feature is the position of the
target word relative to the predicate in the
semi-parse tree containing only clauses. There
are four configurations ? target word and pred-
icate share the same parent, target word parent
is an ancestor of predicate, predicate parent is
an ancestor of target word, or otherwise.
Because each argument consists of a single be-
ginning and a single ending, these classifiers can be
used to construct a set of potential arguments (by
combining each predicted begin with each predicted
end after it of the same type).
Although this phase identifies typed arguments
(i.e. labeled with argument types), the second phase
will re-score each phrase using phrase-based classi-
fiers ? therefore, the goal of the first phase is sim-
ply to identify non-typed phrase candidates. In this
task, we achieves 98.96% and 88.65% recall (over-
all, without verb) on the training and the develop-
ment set, respectively. Because these are the only
candidates passed to the second phase, the final sys-
tem performance is upper-bounded by 88.65%.
3.3 Second Phase: Argument Classification
The second phase of our system assigns the final ar-
gument classes to (a subset) of the argument can-
didates supplied from the first phase. Again, the
SNoW learning architecture is used to train a multi-
class classifier to label each argument to one of the
argument types, plus a special class?no argument
(null). Training examples are created from the argu-
ment candidates supplied from the first phase using
the following features:
? Predicate lemma & POS tag, voice, position,
clause Path, clause position, chunk pattern
Same features as those in the first phase.
? Word & POS tag from the argument, includ-
ing the first,last,and head1 word and tag.
? Named entity feature tells if the target argu-
ment is, embeds, overlaps, or is embedded in a
named entity with its type.
? Chunk tells if the target argument is, embeds,
overlaps, or is embedded in a chunk with its
type.
? Lengths of the target argument, in the numbers
of words and chunks separately.
? Verb class feature is the class of the active
predicate described in PropBank Frames.
? Phrase type uses simple heuristics to identify
the target argument as VP, PP, or NP.
? Sub-categorization describes the phrase
structure around the predicate. We separate
the clause where the predicate is in into three
parts?the predicate chunk, segments before
and after the predicate, and use the sequence
of phrase types of these three segments.
? Baseline features identified not in the main
verb chunk as AM-NEG and modal verb in the
main verb chunk as AM-MOD.
? Clause coverage describes how much of the
local clause (from the predicate) is covered by
the target argument.
? Chunk pattern length feature counts the num-
ber of patterns in the argument.
? Conjunctions join every pair of the above fea-
tures as new features.
? Boundary words & POS tag include two
words/tags before and after the target argu-
ment.
? Bigrams are pairs of words/tags in the window
from two words before the target to the first
word of the target, and also from the last word
to two words after the argument.
1We use simple rules to first decide if a candidate phrase
type is VP, NP, or PP. The headword of an NP phrase is the
right-most noun. Similarly, the left-most verb/proposition of a
VP/PP phrase is extracted as the headword
? Sparse collocation picks one word/tag from
the two words before the argument, the first
word/tag, the last word/tag of the argument,
and one word/tag from the two words after the
argument to join as features.
Although the predictions of the second-phase
classifier can be used directly, the labels of argu-
ments in a sentence often violate some constraints.
Therefore, we rely on the inference procedure to
make the final predictions.
4 Inference via ILP
Ideally, if the learned classifiers are perfect, argu-
ments can be labeled correctly according to the clas-
sifiers? predictions. In reality, labels assigned to ar-
guments in a sentence often contradict each other,
and violate the constraints arising from the struc-
tural and linguistic information. In order to resolve
the conflicts, we design an inference procedure that
takes the confidence scores of each individual ar-
gument given by the second-phase classifier as in-
put, and outputs the best global assignment that
also satisfies the constraints. In this section we first
introduce the constraints and the inference prob-
lem in the semantic role labeling task. Then, we
demonstrate how we apply integer linear program-
ming(ILP) to reason for the global label assignment.
4.1 Constraints over Argument Labeling
Formally, the argument classifier attempts to assign
labels to a set of arguments, S1:M , indexed from 1
to M . Each argument Si can take any label from a
set of argument labels, P , and the indexed set of
arguments can take a set of labels, c1:M ? PM .
If we assume that the classifier returns a score,
score(Si = ci), corresponding to the likelihood of
seeing label ci for argument Si, then, given a sen-
tence, the unaltered inference task is solved by max-
imizing the overall score of the arguments,
c?1:M = argmax
c1:M?PM
score(S1:M = c1:M )
= argmax
c1:M?PM
M?
i=1
score(Si = ci).
(1)
In the presence of global constraints derived from
linguistic information and structural considerations,
our system seeks for a legitimate labeling that max-
imizes the score. Specifically, it can be viewed as
the solution space is limited through the use of a fil-
ter function, F , that eliminates many argument la-
belings from consideration. It is interesting to con-
trast this with previous work that filters individual
phrases (see (Carreras and Ma`rquez, 2003)). Here,
we are concerned with global constraints as well as
constraints on the arguments. Therefore, the final
labeling becomes
c?1:M = argmax
c1:M?F(PM )
M?
i=1
score(Si = ci) (2)
The filter function used considers the following con-
straints:
1. Arguments cannot cover the predicate except
those that contain only the verb or the verb and
the following word.
2. Arguments cannot overlap with the clauses
(they can be embedded in one another).
3. If a predicate is outside a clause, its arguments
cannot be embedded in that clause.
4. No overlapping or embedding arguments.
5. No duplicate argument classes for A0?A5,V.
6. Exactly one V argument per verb.
7. If there is C-V, then there should be a sequence
of consecutive V, A1, and C-V pattern. For ex-
ample, when split is the verb in ?split it up?,
the A1 argument is ?it? and C-V argument is
?up?.
8. If there is an R-XXX argument, then there has
to be an XXX argument. That is, if an ar-
gument is a reference to some other argument
XXX, then this referenced argument must exist
in the sentence.
9. If there is a C-XXX argument, then there has
to be an XXX argument; in addition, the C-
XXX argument must occur after XXX. This is
stricter than the previous rule because the order
of appearance also needs to be considered.
10. Given the predicate, some argument classes
are illegal (e.g. predicate ?stalk? can take only
A0 or A1). This linguistic information can be
found in PropBank Frames.
We reformulate the constraints as linear
(in)equalities by introducing indicator variables.
The optimization problem (Eq. 2) is solved using
ILP.
4.2 Using Integer Linear Programming
As discussed previously, a collection of potential ar-
guments is not necessarily a valid semantic label-
ing since it must satisfy all of the constraints. In
this context, inference is the process of finding the
best (according to Equation 1) valid semantic labels
that satisfy all of the specified constraints. We take
a similar approach that has been previously used
for entity/relation recognition (Roth and Yih, 2004),
and model this inference procedure as solving an
ILP.
An integer linear program(ILP) is basically the
same as a linear program. The cost function and the
(in)equality constraints are all linear in terms of the
variables. The only difference in an ILP is the vari-
ables can only take integers as their values. In our
inference problem, the variables are in fact binary.
A general binary integer programming problem can
be stated as follows.
Given a cost vector p ? <d, a set of variables,
z = (z1, . . . , zd) and cost matrices C1 ? <t1 ?
<d,C2 ? <t2?<d , where t1 and t2 are the numbers
of inequality and equality constraints and d is the
number of binary variables. The ILP solution z? is
the vector that maximizes the cost function,
z? = argmax
z?{0,1}d
p ? z,
subject to C1z ? b1, and C2z = b2,
where b1,b2 ? <d, and for all z ? z, z ? {0, 1}.
To solve the problem of Equation 2 in this set-
ting, we first reformulate the original cost function?M
i=1 score(Si = ci) as a linear function over sev-
eral binary variables, and then represent the filter
function F using linear inequalities and equalities.
We set up a bijection from the semantic labeling
to the variable set z. This is done by setting z to a set
of indicator variables. Specifically, let zic = [Si =
c] be the indicator variable that represents whether
or not the argument type c is assigned to Si, and
let pic = score(Si = c). Equation 1 can then be
written as an ILP cost function as
argmax
z?{0,1}d
M?
i=1
|P|?
c=1
piczic,
subject to
|P|?
c=1
zic = 1 ?zic ? z,
which means that each argument can take only one
type. Note that this new constraint comes from the
variable transformation, and is not one of the con-
straints used in the filter function F .
Constraints 1 through 3 can be evaluated on a per-
argument basis ? the sake of efficiency, arguments
that violate these constraints are eliminated even
before given the second-phase classifier. Next, we
show how to transform the constraints in the filter
function into the form of linear (in)equalities over
z, and use them in this ILP setting.
Constraint 4: No overlapping or embedding If
arguments Sj1 , . . . , Sjk occupy the same word in a
sentence, then this constraint restricts only one ar-
guments to be assigned to an argument type. In
other words, k ? 1 arguments will be the special
class null, which means the argument candidate is
not a legitimate argument. If the special class null
is represented by the symbol ?, then for every set of
such arguments, the following linear equality repre-
sents this constraint.
k?
i=1
zji? = k ? 1
Constraint 5: No duplicate argument classes
Within the same sentence, several types of argu-
ments cannot appear more than once. For example,
a predicate can only take one A0. This constraint
can be represented using the following inequality.
M?
i=1
ziA0 ? 1
Constraint 6: Exactly one V argument For each
verb, there is one and has to be one V argument,
which represents the active verb. Similarly, this con-
straint can be represented by the following equality.
M?
i=1
ziV = 1
Constraint 7: V?A1?C-V pattern This con-
straint is only useful when there are three consec-
utive candidate arguments in a sentence. Suppose
arguments Sj1 , Sj2 , Sj3 are consecutive. If Sj3 is
C-V, then Sj1 and Sj2 have to be V and A1, respec-
tively. This if-then constraint can be represented by
the following two linear inequalities.
zj3C-V ? zj1V, and zj3C-V ? zj2A1
Constraint 8: R-XXX arguments Suppose the
referenced argument type is A0 and the reference
type is R-A0. The linear inequalities that represent
this constraint are:
?m ? {1, . . . ,M} :
M?
i=1
ziA0 ? zmR-A0
If there are ? reference argument pairs, then the
total number of inequalities needed is ?M .
Constraint 9: C-XXX arguments This con-
straint is similar to the reference argument con-
straints. The difference is that the continued argu-
ment XXX has to occur before C-XXX. Assume
that the argument pair is A0 and C-A0, and argu-
ment Sji appears before Sjk if i ? k. The linear
inequalities that represent this constraint are:
?m ? {2, . . . ,M} :
j?1?
i=1
zjiA0 ? zmR-A0
Constraint 10: Illegal argument types Given a
specific verb, some argument types should never oc-
cur. For example, most verbs don?t have arguments
A5. This constraint is represented by summing all
the corresponding indicator variables to be 0.
M?
i=1
ziA5 = 0
Using ILP to solve this inference problem en-
joys several advantages. Linear constraints are
very general, and are able to represent many types
of constraints. Previous approaches usually rely
on dynamic programming to resolve non over-
lapping/embedding constraints (i.e., Constraint 4)
when the data is sequential, but are unable to han-
dle other constraints. The ILP approach is flexible
enough to handle constraints regardless of the struc-
ture of the data. Although solving an ILP prob-
lem is NP-hard, with the help of todays commer-
cial numerical packages, this problem can usually
be solved very fast in practice. For instance, it only
takes about 10 minutes to solve the inference prob-
lem for 4305 sentences on a Pentium-III 800 MHz
machine in our experiments. Note that ordinary
search methods (e.g., beam search) are not neces-
sarily faster than solving an ILP problem and do not
guarantee the optimal solution.
5 Experimental Results
The system is evaluated on the data provided in
the CoNLL-2004 semantic-role labeling shared task
which consists of a portion of PropBank corpus.
The training set is extracted from TreeBank (Mar-
cus et al, 1993) section 15?18, the development set,
used in tuning parameters of the system, from sec-
tion 20, and the test set from section 21.
We first compare this system with the basic tagger
that we have, the CSCL shallow parser from (Pun-
yakanok and Roth, 2001), which is equivalent to us-
ing the scoring function from the first phase with
only the non-overlapping/embedding constraints. In
Prec. Rec. F?=1
1st-phase, non-overlap 70.54 61.50 65.71
1st-phase, All Const. 70.97 60.74 65.46
2nd-phase, non-overlap 69.69 64.75 67.13
2nd-phase, All Const. 71.96 64.93 68.26
Table 1: Summary of experiments on the development
set. All results are for overall performance.
Precision Recall F?=1
Without Inference 86.95 87.24 87.10
With Inference 88.03 88.23 88.13
Table 2: Results of second phase phrase prediction
and inference assuming perfect boundary detection in
the first phase. Inference improves performance by re-
stricting label sequences rather than restricting structural
properties since the correct boundaries are given. All re-
sults are for overall performance on the development set.
addition, we evaluate the effectiveness of using only
this constraint versus all constraints, as in Sec. 4.
Table 1 shows how additional constraints over the
standard non-overlapping constraints improve per-
formance on the development set. The argument
scoring is chosen from either the first phase or the
second phase and each is evaluated by considering
simply the non-overlapping/embedding constraint
or the full set of linguistic constraints. To make
a fair comparison, parameters were set separately
to optimize performance when using the first phase
results. In general, using all constraints increases
F?=1 by about 1% in this system, but slightly de-
creases the performance when only the first phase
classifier is used. Also, using the two-phase archi-
tecture improves both precision and recall, and the
enhancement reflected in F?=1 is about 2.5%.
It is interesting to find out how well the second
phase classifier can perform given perfectly seg-
mented arguments. This evaluates the quality of the
argument classifier, and also provides a conceptual
upper bound. Table 2 first shows the results without
using inference (i.e. F(PM ) = PM ). The second
row shows adding inference to the phrase classifica-
tion can further improve F?=1 by 1%.
Finally, the overall result on the official test set
is given in Table 3. Note that the result here is not
comparable with the best in this domain (Pradhan et
al., 2004) where the full parse tree is assumed given.
For a fair comparison, our system was among the
best at CoNLL-04, where the best system (Hacioglu
et al, 2004) achieve a 69.49 F1 score.
6 Conclusion
We show that linguistic information is useful for se-
mantic role labeling, both in extracting features and
Dist. Prec. Rec. F?=1
Overall 100.00 70.07 63.07 66.39
A0 26.87 81.13 77.70 79.38
A1 35.73 74.21 63.02 68.16
A2 7.44 54.16 41.04 46.69
A3 1.56 47.06 26.67 34.04
A4 0.52 71.43 60.00 65.22
AM-ADV 3.20 39.36 36.16 37.69
AM-CAU 0.51 45.95 34.69 39.53
AM-DIR 0.52 42.50 34.00 37.78
AM-DIS 2.22 52.00 67.14 58.61
AM-EXT 0.15 46.67 50.00 48.28
AM-LOC 2.38 33.47 34.65 34.05
AM-MNR 2.66 45.19 36.86 40.60
AM-MOD 3.51 92.49 94.96 93.70
AM-NEG 1.32 85.92 96.06 90.71
AM-PNC 0.89 32.79 23.53 27.40
AM-TMP 7.78 59.77 56.89 58.30
R-A0 1.66 81.33 76.73 78.96
R-A1 0.73 58.82 57.14 57.97
R-A2 0.09 100.00 22.22 36.36
R-AM-TMP 0.15 54.55 42.86 48.00
Table 3: Results on the test set.
deriving hard constraints on the output. We also
demonstrate that it is possible to use integer linear
programming to perform inference that incorporates
a wide variety of hard constraints, which would be
difficult to incorporate using existing methods. In
addition, we provide further evidence supporting
the use of scoring arguments over scoring argument
boundaries for complex tasks. In the future, we plan
to use the full PropBank corpus to see the improve-
ment when more training data is provided. In addi-
tion, we would like to explore the possibility of in-
teger linear programming approach using soft con-
straints. As more constraints are considered, we ex-
pect the overall performance to improve.
7 Acknowledgments
We thank Xavier Carreras and Llu??s Ma`rquez for the
data and scripts, Martha Palmer and the anonymous
referees for their useful comments, AMD for their
equipment donation, and Dash Optimization for the
free academic use of their Xpress-MP software.
This research is supported by NSF grants ITR-IIS-
0085836, ITR-IIS-0085980 and IIS-9984168, EIA-
0224453 and an ONR MURI Award.
References
C. Bishop, 1995. Neural Networks for Pattern Recogni-
tion, chapter 6.4: Modelling conditional distributions,
page 215. Oxford University Press.
X. Carreras and L. Ma`rquez. 2003. Phrase recognition
by filtering and ranking with perceptrons. In Proc. of
RANLP-2003.
J. Chen and O. Rambow. 2003. Use of deep linguistic
features for the recognition and labeling of semantic
arguments. In Proc. of EMNLP-2003, Sapporo, Japan.
D. Gildea and J. Hockenmaier. 2003. Identifying se-
mantic roles using combinatory categorial grammar.
In Proc. of the EMNLP-2003, Sapporo, Japan.
D. Gildea and M. Palmer. 2002. The necessity of parsing
for predicate argument recognition. In Proc. of ACL
2002, pages 239?246, Philadelphia, PA.
A. Grove and D. Roth. 2001. Linear concepts and hid-
den variables. Machine Learning, 42(1/2):123?141.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2004. Semantic role labeling by tagging
syntactic chunks. In Proc. of CoNLL-04.
T. Hang, F. Damerau, and D. Johnson. 2002. Text
chunking based on a generalization of winnow. J. of
Machine Learning Research, 2:615?637.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proc. of LREC-2002, Spain.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330, June.
S. Pradhan, K. Hacioglu, W. ward, J. Martin, and D. Ju-
rafsky. 2003. Semantic role parsing adding semantic
structure to unstructured text. In Proc. of ICDM-2003,
Melbourne, FL.
S. Pradhan, W. Ward, K. Hacioglu, J. H. Martin, and
D. Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In Proc. of NAACL-HLT
2004.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS-13; The 2000
Conference on Advances in Neural Information Pro-
cessing Systems, pages 995?1001. MIT Press.
D. Roth and W. Yih. 2002. Probabilistic reasoning for
entity & relation recognition. In Proc. of COLING-
2002, pages 835?841.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proc. of CoNLL-2004.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proc. of AAAI, pages
806?813.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proc. of ACL 2003.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In
Proc. of the CoNLL-2000 and LLL-2000.
E. F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proc. of
CoNLL-2003.
E. F. Tjong Kim Sang and H. De?jean. 2001. Introduction
to the CoNLL-2001 shared task: Clause identification.
In Proc. of the CoNLL-2001.
Xpress-MP. 2003. Dash Optimization. Xpress-MP.
http://www.dashoptimization.com/products.html.
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 294?303,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Understanding the Value of Features for Coreference Resolution
Eric Bengtson Dan Roth
Department of Computer Science
University of Illinois
Urbana, IL 61801
{ebengt2,danr}@illinois.edu
Abstract
In recent years there has been substantial work
on the important problem of coreference res-
olution, most of which has concentrated on
the development of new models and algo-
rithmic techniques. These works often show
that complex models improve over a weak
pairwise baseline. However, less attention
has been given to the importance of selecting
strong features to support learning a corefer-
ence model.
This paper describes a rather simple pair-
wise classification model for coreference res-
olution, developed with a well-designed set
of features. We show that this produces a
state-of-the-art system that outperforms sys-
tems built with complex models. We suggest
that our system can be used as a baseline for
the development of more complex models ?
which may have less impact when a more ro-
bust set of features is used. The paper also
presents an ablation study and discusses the
relative contributions of various features.
1 Introduction
Coreference resolution is the task of grouping all the
mentions of entities1 in a document into equivalence
classes so that all the mentions in a given class refer
to the same discourse entity. For example, given the
sentence (where the head noun of each mention is
subscripted)
1We follow the ACE (NIST, 2004) terminology: A noun
phrase referring to a discourse entity is called a mention, and
an equivalence class is called an entity.
An American1 official2 announced that
American1 President3 Bill Clinton3 met
his3 Russian4 counterpart5, Vladimir
Putin5, today.
the task is to group the mentions so that those refer-
ring to the same entity are placed together into an
equivalence class.
Many NLP tasks detect attributes, actions, and
relations between discourse entities. In order to
discover all information about a given entity, tex-
tual mentions of that entity must be grouped to-
gether. Thus coreference is an important prerequi-
site to such tasks as textual entailment and informa-
tion extraction, among others.
Although coreference resolution has received
much attention, that attention has not focused on the
relative impact of high-quality features. Thus, while
many structural innovations in the modeling ap-
proach have been made, those innovations have gen-
erally been tested on systems with features whose
strength has not been established, and compared to
weak pairwise baselines. As a result, it is possible
that some modeling innovations may have less im-
pact or applicability when applied to a stronger base-
line system.
This paper introduces a rather simple but state-
of-the-art system, which we intend to be used as a
strong baseline to evaluate the impact of structural
innovations. To this end, we combine an effective
coreference classification model with a strong set of
features, and present an ablation study to show the
relative impact of a variety of features.
As we show, this combination of a pairwise
model and strong features produces a 1.5 percent-
294
age point increase in B-Cubed F-Score over a com-
plex model in the state-of-the-art system by Culotta
et al (2007), although their system uses a complex,
non-pairwise model, computing features over partial
clusters of mentions.
2 A Pairwise Coreference Model
Given a document and a set of mentions, corefer-
ence resolution is the task of grouping the mentions
into equivalence classes, so that each equivalence
class contains exactly those mentions that refer to
the same discourse entity. The number of equiv-
alence classes is not specified in advance, but is
bounded by the number of mentions.
In this paper, we view coreference resolution as
a graph problem: Given a set of mentions and their
context as nodes, generate a set of edges such that
any two mentions that belong in the same equiva-
lence class are connected by some path in the graph.
We construct this entity-mention graph by learning
to decide for each mention which preceding men-
tion, if any, belongs in the same equivalence class;
this approach is commonly called the pairwise coref-
erence model (Soon et al, 2001). To decide whether
two mentions should be linked in the graph, we learn
a pairwise coreference function pc that produces a
value indicating the probability that the two men-
tions should be placed in the same equivalence class.
The remainder of this section first discusses how
this function is used as part of a document-level
coreference decision model and then describes how
we learn the pc function.
2.1 Document-Level Decision Model
Given a document d and a pairwise coreference scor-
ing function pc that maps an ordered pair of men-
tions to a value indicating the probability that they
are coreferential (see Section 2.2), we generate a
coreference graph Gd according to the Best-Link de-
cision model (Ng and Cardie, 2002b) as follows:
For each mention m in document d, let Bm be the
set of mentions appearing before m in d. Let a be
the highest scoring antecedent:
a = argmax
b?Bm
(pc(b,m)).
If pc(a,m) is above a threshold chosen as described
in Section 4.4, we add the edge (a,m) to the coref-
erence graph Gd.
The resulting graph contains connected compo-
nents, each representing one equivalence class, with
all the mentions in the component referring to the
same entity. This technique permits us to learn to
detect some links between mentions while being ag-
nostic about whether other mentions are linked, and
yet via the transitive closure of all links we can still
determine the equivalence classes.
We also require that no non-pronoun can refer
back to a pronoun: If m is not a pronoun, we do
not consider pronouns as candidate antecedents.
2.1.1 Related Models
For pairwise models, it is common to choose the
best antecedent for a given mention (thereby impos-
ing the constraint that each mention has at most one
antecedent); however, the method of deciding which
is the best antecedent varies.
Soon et al (2001) use the Closest-Link method:
They select as an antecedent the closest preced-
ing mention that is predicted coreferential by a
pairwise coreference module; this is equivalent to
choosing the closest mention whose pc value is
above a threshold. Best-Link was shown to out-
perform Closest-Link in an experiment by Ng and
Cardie (2002b). Our model differs from that of Ng
and Cardie in that we impose the constraint that
non-pronouns cannot refer back to pronouns, and in
that we use as training examples all ordered pairs of
mentions, subject to the constraint above.
Culotta et al (2007) introduced a model that pre-
dicts whether a pair of equivalence classes should be
merged, using features computed over all the men-
tions in both classes. Since the number of possi-
ble classes is exponential in the number of mentions,
they use heuristics to select training examples. Our
method does not require determining which equiva-
lence classes should be considered as examples.
2.2 Pairwise Coreference Function
Learning the pairwise scoring function pc is a cru-
cial issue for the pairwise coreference model. We
apply machine learning techniques to learn from ex-
amples a function pc that takes as input an ordered
pair of mentions (a,m) such that a precedes m in
the document, and produces as output a value that is
295
interpreted as the conditional probability that m and
a belong in the same equivalence class.
2.2.1 Training Example Selection
The ACE training data provides the equivalence
classes for mentions. However, for some pairs of
mentions from an equivalence class, there is little or
no direct evidence in the text that the mentions are
coreferential. Therefore, training pc on all pairs of
mentions within an equivalence class may not lead
to a good predictor. Thus, for each mention m we
select from m?s equivalence class the closest pre-
ceding mention a and present the pair (a,m) as a
positive training example, under the assumption that
there is more direct evidence in the text for the ex-
istence of this edge than for other edges. This is
similar to the technique of Ng and Cardie (2002b).
For each m, we generate negative examples (a,m)
for all mentions a that precede m and are not in the
same equivalence class. Note that in doing so we
generate more negative examples than positive ones.
Since we never apply pc to a pair where the first
mention is a pronoun and the second is not a pro-
noun, we do not train on examples of this form.
2.2.2 Learning Pairwise Coreference Scoring
Model
We learn the pairwise coreference function using
an averaged perceptron learning algorithm (Freund
and Schapire, 1998) ? we use the regularized version
in Learning Based Java2 (Rizzolo and Roth, 2007).
3 Features
The performance of the document-level coreference
model depends on the quality of the pairwise coref-
erence function pc. Beyond the training paradigm
described earlier, the quality of pc depends on the
features used.
We divide the features into categories, based on
their function. A full list of features and their cat-
egories is given in Table 2. In addition to these
boolean features, we also use the conjunctions of all
pairs of features.3
2LBJ code is available at http://L2R.cs.uiuc.edu/
?cogcomp/asoftware.php?skey=LBJ
3The package of all features used is available at
http://L2R.cs.uiuc.edu/?cogcomp/asoftware.
php?skey=LBJ#features.
In the following description, the term head means
the head noun phrase of a mention; the extent is the
largest noun phrase headed by the head noun phrase.
3.1 Mention Types
The type of a mention indicates whether it is a proper
noun, a common noun, or a pronoun. This feature,
when conjoined with others, allows us to give dif-
ferent weight to a feature depending on whether it is
being applied to a proper name or a pronoun. For
our experiments in Section 5, we use gold mention
types as is done by Culotta et al (2007) and Luo and
Zitouni (2005).
Note that in the experiments described in Sec-
tion 6 we predict the mention types as described
there and do not use any gold data. The mention
type feature is used in all experiments.
3.2 String Relation Features
String relation features indicate whether two strings
share some property, such as one being the substring
of another or both sharing a modifier word. Features
are listed in Table 1. Modifiers are limited to those
occurring before the head.
Feature Definition
Head match headi == headj
Extent match extenti == extentj
Substring headi substring of headj
Modifiers Match modi == (headj or modj)
Alias acronym(headi) == headj
or lastnamei == lastnamej
Table 1: String Relation Features
3.3 Semantic Features
Another class of features captures the semantic re-
lation between two words. Specifically, we check
whether gender or number match, or whether the
mentions are synonyms, antonyms, or hypernyms.
We also check the relationship of modifiers that
share a hypernym. Descriptions of the methods for
computing these features are described next.
Gender Match We determine the gender (male,
female, or neuter) of the two phrases, and report
whether they match (true, false, or unknown). For
296
Category Feature Source
Mention Types Mention Type Pair Annotation and tokens
String Relations Head Match Tokens
Extent Match Tokens
Substring Tokens
Modifiers Match Tokens
Alias Tokens and lists
Semantic Gender Match WordNet and lists
Number Match WordNet and lists
Synonyms WordNet
Antonyms WordNet
Hypernyms WordNet
Both Speak Context
Relative Location Apposition Positions and context
Relative Pronoun Positions and tokens
Distances Positions
Learned Anaphoricity Learned
Name Modifiers Predicted Match Learned
Aligned Modifiers Aligned Modifiers Relation WordNet and lists
Memorization Last Words Tokens
Predicted Entity Types Entity Types Match Annotation and tokens
Entity Type Pair WordNet and tokens
Table 2: Features by Category
a proper name, gender is determined by the exis-
tence of mr, ms, mrs, or the gender of the first name.
If only a last name is found, the phrase is consid-
ered to refer to a person. If the name is found in
a comprehensive list of cities or countries, or ends
with an organization ending such as inc, then the
gender is neuter. In the case of a common noun
phrase, the phrase is looked up in WordNet (Fell-
baum, 1998), and it is assigned a gender according to
whether male, female, person, artifact, location, or
group (the last three correspond to neuter) is found
in the hypernym tree. The gender of a pronoun is
looked up in a table.
Number Match Number is determined as fol-
lows: Phrases starting with the words a, an, or this
are singular; those, these, or some indicate plural.
Names not containing and are singular. Common
nouns are checked against extensive lists of singular
and plural nouns ? words found in neither or both
lists have unknown number. Finally, if the num-
ber is unknown yet the two mentions have the same
spelling, they are assumed to have the same number.
WordNet Features We check whether any sense
of one head noun phrase is a synonym, antonym, or
hypernym of any sense of the other. We also check
whether any sense of the phrases share a hypernym,
after dropping entity, abstraction, physical entity,
object, whole, artifact, and group from the senses,
since they are close to the root of the hypernym tree.
Modifiers Match Determines whether the text be-
fore the head of a mention matches the head or the
text before the head of the other mention.
Both Mentions Speak True if both mentions ap-
pear within two words of a verb meaning to say. Be-
ing in a window of size two is an approximation to
being a syntactic subject of such a verb. This feature
is a proxy for having similar semantic types.
3.4 Relative Location Features
Additional evidence is derived from the relative lo-
cation of the two mentions. We thus measure dis-
tance (quantized as multiple boolean features of the
297
form [distance ? i]) for all i up to the distance and
less than some maximum, using units of compatible
mentions, and whether the mentions are in the same
sentence. We also detect apposition (mentions sepa-
rated by a comma). For details, see Table 3.
Feature Definition
Distance In same sentence
# compatible mentions
Apposition m1 ,m2 found
Relative Pronoun Apposition and m2 is PRO
Table 3: Location Features. Compatible mentions are
those having the same gender and number.
3.5 Learned Features
Modifier Names If the mentions are both mod-
ified by other proper names, use a basic corefer-
ence classifier to determine whether the modifiers
are coreferential. This basic classifier is trained
using Mention Types, String Relations, Semantic
Features, Apposition, Relative Pronoun, and Both
Speak. For each mention m, examples are generated
with the closest antecedent a to form a positive ex-
ample, and every mention between a and m to form
negative examples.
Anaphoricity Ng and Cardie (2002a) and Denis
and Baldridge (2007) show that when used effec-
tively, explicitly predicting anaphoricity can be help-
ful. Thus, we learn a separate classifier to detect
whether a mention is anaphoric (that is, whether it
is not the first mention in its equivalence class), and
use that classifier?s output as a feature for the coref-
erence model. Features for the anaphoricity classi-
fier include the mention type, whether the mention
appears in a quotation, the text of the first word of
the extent, the text of the first word after the head
(if that word is part of the extent), whether there is
a longer mention preceding this mention and having
the same head text, whether any preceding mention
has the same extent text, and whether any preceding
mention has the same text from beginning of the ex-
tent to end of the head. Conjunctions of all pairs of
these features are also used. This classifier predicts
anaphoricity with about 82% accuracy.
3.6 Aligned Modifiers
We determine the relationship of any pair of modi-
fiers that share a hypernym. Each aligned pair may
have one of the following relations: match, sub-
string, synonyms, hypernyms, antonyms, or mis-
match. Mismatch is defined as none of the above.
We restrict modifiers to single nouns and adjectives
occurring before the head noun phrase.
3.7 Memorization Features
We allow our system to learn which pairs of nouns
tend to be used to mention the same entity. For ex-
ample, President and he often refer to Bush but she
and Prime Minister rarely do, if ever. To enable the
system to learn such patterns, we treat the presence
or absence of each pair of final head nouns, one from
each mention of an example, as a feature.
3.8 Predicted Entity Type
We predict the entity type (person, organization,
geo-political entity, location, facility, weapon, or ve-
hicle) as follows: If a proper name, we check a list of
personal first names, and a short list of honorary ti-
tles (e.g. mr) to determine if the mention is a person.
Otherwise we look in lists of personal last names
drawn from US census data, and in lists of cities,
states, countries, organizations, corporations, sports
teams, universities, political parties, and organiza-
tion endings (e.g. inc or corp). If found in exactly
one list, we return the appropriate type. We return
unknown if found in multiple lists because the lists
are quite comprehensive and may have significant
overlap.
For common nouns, we look at the hypernym tree
for one of the following: person, political unit, loca-
tion, organization, weapon, vehicle, industrial plant,
and facility. If any is found, we return the appropri-
ate type. If multiple are found, we sort as in the
above list.
For personal pronouns, we recognize the entity as
a person; otherwise we specify unknown.
This computation is used as part of the following
two features.
Entity Type Match This feature checks to see
whether the predicted entity types match. The result
is true if the types are identical, false if they are dif-
ferent, and unknown if at least one type is unknown.
298
Entity Type Conjunctions This feature indicates
the presence of the pair of predicted entity types for
the two mentions, except that if either word is a pro-
noun, the word token replaces the type in the pair.
Since we do this replacement for entity types, we
also add a similar feature for mention types here.
These features are boolean: For any given pair, a
feature is active if that pair describes the example.
3.9 Related Work
Many of our features are similar to those described
in Culotta et al (2007). This includes Mention
Types, String Relation Features, Gender and Num-
ber Match, WordNet Features, Alias, Apposition,
Relative Pronoun, and Both Mentions Speak. The
implementations of those features may vary from
those of other systems. Anaphoricity has been pro-
posed as a part of the model in several systems, in-
cluding Ng and Cardie (2002a), but we are not aware
of it being used as a feature for a learning algorithm.
Distances have been used in e.g. Luo et al (2004).
However, we are not aware of any system using the
number of compatible mentions as a distance.
4 Experimental Study
4.1 Corpus
We use the official ACE 2004 English training
data (NIST, 2004). Much work has been done on
coreference in several languages, but for this work
we focus on English text. We split the corpus into
three sets: Train, Dev, and Test. Our test set contains
the same 107 documents as Culotta et al (2007).
Our training set is a random 80% of the 336 doc-
uments in their training set and our Dev set is the
remaining 20%.
For our ablation study, we further randomly split
our development set into two evenly sized parts,
Dev-Tune and Dev-Eval. For each experiment, we
set the parameters of our algorithm to optimize B-
Cubed F-Score using Dev-Tune, and use those pa-
rameters to evaluate on the Dev-Eval data.
4.2 Preprocessing
For the experiments in Section 5, following Culotta
et al (2007), to make experiments more compara-
ble across systems, we assume that perfect mention
boundaries and mention type labels are given. We
do not use any other gold annotated input at evalu-
ation time. In Section 6 experiments we do not use
any gold annotated input and do not assume mention
types or boundaries are given. In all experiments we
automatically split words and sentences using our
preprocessing tools.4
4.3 Evaluation Scores
B-Cubed F-Score We evaluate over the com-
monly used B-Cubed F-Score (Bagga and Baldwin,
1998), which is a measure of the overlap of predicted
clusters and true clusters. It is computed as the har-
monic mean of precision (P ),
P =
1
N
?
d?D
?
?
?
m?d
(
cm
pm
)
?
? , (1)
and recall (R),
R =
1
N
?
d?D
?
?
?
m?d
(
cm
tm
)
?
? , (2)
where cm is the number of mentions appearing
both in m?s predicted cluster and in m?s true clus-
ter, pm is the size of the predicted cluster containing
m, and tm is the size of m?s true cluster. Finally, d
represents a document from the set D, and N is the
total number of mentions in D.
B-Cubed F-Score has the advantage of being able
to measure the impact of singleton entities, and of
giving more weight to the splitting or merging of
larger entities. It also gives equal weight to all types
of entities and mentions. For these reasons, we re-
port our results using B-Cubed F-Score.
MUC F-Score We also provide results using the
official MUC scoring algorithm (Vilain et al, 1995).
The MUC F-score is also the harmonic mean of
precision and recall. However, the MUC precision
counts precision errors by computing the minimum
number of links that must be added to ensure that all
mentions referring to a given entity are connected
in the graph. Recall errors are the number of links
that must be removed to ensure that no two men-
tions referring to different entities are connected in
the graph.
4The code is available at http://L2R.cs.uiuc.edu/
?cogcomp/tools.php
299
4.4 Learning Algorithm Details
We train a regularized average perceptron using ex-
amples selected as described in Section 2.2.1. The
learning rate is 0.1 and the regularization parameter
(separator thickness) is 3.5. At training time, we use
a threshold of 0.0, but when evaluating, we select pa-
rameters to optimize B-Cubed F-Score on a held-out
development set. We sample all even integer thresh-
olds from -16 to 8. We choose the number of rounds
of training similarly, allowing any number from one
to twenty.
5 Results
Precision Recall B3 F
Culotta et al 86.7 73.2 79.3
Current Work 88.3 74.5 80.8
Table 4: Evaluation on unseen Test Data using B3 score.
Shows that our system outperforms the advanced system
of Culotta et al The improvement is statistically signifi-
cant at the p = 0.05 level according to a non-parametric
bootstrapping percentile test.
In Table 4, we compare our performance against
a system that is comparable to ours: Both use gold
mention boundaries and types, evaluate using B-
Cubed F-Score, and have the same training and test
data split. Culotta et al (2007) is the best compara-
ble system of which we are aware.
Our results show that a pairwise model with
strong features outperforms a state-of-the-art system
with a more complex model.
MUC Score We evaluate the performance of our
system using the official MUC score in Table 5.
MUC Precision MUC Recall MUC F
82.7 69.9 75.8
Table 5: Evaluation of our system on unseen Test Data
using MUC score.
5.1 Analysis of Feature Contributions
In Table 6 we show the relative impact of various
features. We report data on Dev-Eval, to avoid the
possibility of overfitting by feature selection. The
parameters of the algorithm are chosen to maximize
the BCubed F-Score on the Dev-Tune data. Note
that since we report results on Dev-Eval, the results
in Table 6 are not directly comparable with Culotta
et al (2007). For comparable results, see Table 4
and the discussion above.
Our ablation study shows the impact of various
classes of features, indicating that almost all the fea-
tures help, although some more than others. It also
illustrates that some features contribute more to pre-
cision, others more to recall. For example, aligned
modifiers contribute primarily to precision, whereas
our learned features and our apposition features con-
tribute to recall. This information can be useful
when designing a coreference system in an applica-
tion where recall is more important than precision,
or vice versa.
We examine the effect of some important features,
selecting those that provide a substantial improve-
ment in precision, recall, or both. For each such
feature we examine the rate of coreference amongst
mention pairs for which the feature is active, com-
pared with the overall rate of coreference. We also
show examples on which the coreference systems
differ depending on the presence or absence of a fea-
ture.
Apposition This feature checks whether two men-
tions are separated by only a comma, and it in-
creases B-Cubed F-Score by about one percentage
point. We hypothesize that proper names and com-
mon noun phrases link primarily through apposition,
and that apposition is thus a significant feature for
good coreference resolution.
When this feature is active 36% of the examples
are coreferential, whereas only 6% of all examples
are coreferential. Looking at some examples our
system begins to get right when apposition is added,
we find the phrase
Israel?s Deputy Defense Minister,
Ephraim Sneh.
Upon adding apposition, our system begins to cor-
rectly associate Israel?s Deputy Defense Minister
with Ephraim Sneh. Likewise in the phrase
The court president, Ronald Sutherland,
the system correctly associates The court president
with Ronald Sutherland when they appear in an ap-
positive relation in the text. In addition, our system
300
Precision Recall B-Cubed F
String Similarity 86.88 67.17 75.76
+ Semantic Features 85.34 69.30 76.49
+ Apposition 89.77 67.53 77.07
+ Relative Pronoun 88.76 68.97 77.62
+ Distances 89.62 71.93 79.81
+ Learned Features 87.37 74.51 80.43
+ Aligned Modifiers 88.70 74.30 80.86
+ Memorization 86.57 75.59 80.71
+ Predicted Entity Types 87.92 76.46 81.79
Table 6: Contribution of Features as evaluated on a development set. Bold results are significantly better than the
previous line at the p = 0.05 level according to a paired non-parametric bootstrapping percentile test. These results
show the importance of Distance, Entity Type, and Apposition features.
begins correctly associating relative pronouns such
as who with their referents in phrases like
Sheikh Abbad, who died 500 years ago.
although an explicit relative pronoun feature is
added only later.
Although this feature may lead the system to link
comma separated lists of entities due to misinter-
pretation of the comma, for example Wyoming and
western South Dakota in a list of locations, we be-
lieve this can be avoided by refining the apposition
feature to ignore lists.
Relative Pronoun Next we investigate the relative
pronoun feature. With this feature active, 93% of
examples were positive, indicating the precision of
this feature. Looking to examples, we find who in
the official, who wished to remain anony-
mous
is properly linked, as is that in
nuclear warheads that can be fitted to mis-
siles.
Distances Our distance features measure separa-
tion of two mentions in number of compatible men-
tions (quantized), and whether the mentions are in
the same sentence. Distance features are important
for a system that makes links based on the best pair-
wise coreference value rather than implicitly incor-
porating distance by linking only the closest pair
whose score is above a threshold, as done by e.g.
Soon et al (2001).
Looking at examples, we find that adding dis-
tances allows the system to associate the pronoun
it with this missile not separated by any mentions,
rather than Tehran, which is separated from it by
many mentions.
Predicted Entity Types Since no two mentions
can have different entity types (person, organization,
geo-political entity, etc.) and be coreferential, this
feature has strong discriminative power. When the
entity types match, 13% of examples are positive
compared to only 6% of examples in general. Qual-
itatively, the entity type prediction correctly recog-
nizes the Gulf region as a geo-political entity, and
He as a person, and thus prevents linking the two.
Likewise, the system discerns Baghdad from am-
bassador due to the entity type. However, in some
cases an identity type match can cause the system to
be overly confident in a bad match, as in the case of
a palestinian state identified with holy Jerusalem on
the basis of proximity and shared entity type. This
type of example may require some additional world
knowledge or deeper comprehension of the docu-
ment.
6 End-to-End Coreference
The ultimate goal for a coreference system is to
process unannotated text. We use the term end-to-
end coreference for a system capable of determin-
ing coreference on plain text. We describe the chal-
lenges associated with an end-to-end system, de-
scribe our approach, and report results below.
301
6.1 Challenges
Developing an end-to-end system requires detecting
and classifying mentions, which may degrade coref-
erence results. One challenge in detecting mentions
is that they are often heavily nested. Additionally,
there are issues with evaluating an end-to-end sys-
tem against a gold standard corpus, resulting from
the possibility of mismatches in mention boundaries,
missing mentions, and additional mentions detected,
along with the need to align detected mentions to
their counterparts in the annotated data.
6.2 Approach
We resolve coreference on unannotated text as fol-
lows: First we detect mention heads following a
state of the art chunking approach (Punyakanok and
Roth, 2001) using standard features. This results in
a 90% F1 head detector. Next, we detect the extent
boundaries for each head using a learned classifier.
This is followed by determining whether a mention
is a proper name, common noun phrase, prenominal
modifier, or pronoun using a learned mention type
classifier that. Finally, we apply our coreference al-
gorithm described above.
6.3 Evaluation and Results
To evaluate, we align the heads of the detected men-
tions to the gold standard heads greedily based on
number of overlapping words. We choose not to
impute errors to the coreference system for men-
tions that were not detected or for spuriously de-
tected mentions (following Ji et al (2005) and oth-
ers). Although this evaluation is lenient, given that
the mention detection component performs at over
90% F1, we believe it provides a realistic measure
for the performance of the end-to-end system and fo-
cuses the evaluation on the coreference component.
The results of our end-to-end coreference system are
shown in Table 7.
Precision Recall B3 F
End-to-End System 84.91 72.53 78.24
Table 7: Coreference results using detected mentions on
unseen Test Data.
7 Conclusion
We described and evaluated a state-of-the-art coref-
erence system based on a pairwise model and strong
features. While previous work showed the impact
of complex models on a weak pairwise baseline, the
applicability and impact of such models on a strong
baseline system such as ours remains uncertain. We
also studied and demonstrated the relative value of
various types of features, showing in particular the
importance of distance and apposition features, and
showing which features impact precision or recall
more. Finally, we showed an end-to-end system ca-
pable of determining coreference in a plain text doc-
ument.
Acknowledgments
We would like to thank Ming-Wei Chang, Michael
Connor, Alexandre Klementiev, Nick Rizzolo,
Kevin Small, and the anonymous reviewers for their
insightful comments. This work is partly supported
by NSF grant SoD-HCER-0613885 and a grant from
Boeing.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In MUC7.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In HLT/NAACL, pages 81?88.
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In HLT/NAACL, pages 236?243,
Rochester, New York.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Y. Freund and R. E. Schapire. 1998. Large margin clas-
sification using the Perceptron algorithm. In COLT,
pages 209?217.
H. Ji, D. Westbrook, and R. Grishman. 2005. Us-
ing semantic relations to refine coreference decisions.
In EMNLP/HLT, pages 17?24, Vancouver, British
Columbia, Canada.
X. Luo and I. Zitouni. 2005. Multi-lingual coreference
resolution with syntactic features. In HLT/EMNLP,
pages 660?667, Vancouver, British Columbia, Canada.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
ACL, page 135, Morristown, NJ, USA.
302
V. Ng and C. Cardie. 2002a. Identifying anaphoric and
non-anaphoric noun phrases to improve coreference
resolution. In COLING-2002.
V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In ACL.
NIST. 2004. The ace evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 995?1001. MIT Press.
N. Rizzolo and D. Roth. 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Inter-
national Conference on Semantic Computing (ICSC),
pages 597?604, Irvine, California.
W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In MUC6, pages 45?52.
303
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 353?362,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Transliteration as Constrained Optimization
Dan Goldwasser Dan Roth
Department of Computer Science
University of Illinois
Urbana, IL 61801
{goldwas1,danr}@uiuc.edu
Abstract
This paper introduces a new method for iden-
tifying named-entity (NE) transliterations in
bilingual corpora. Recent works have shown
the advantage of discriminative approaches to
transliteration: given two strings (ws, wt) in
the source and target language, a classifier is
trained to determine if wt is the translitera-
tion of ws. This paper shows that the translit-
eration problem can be formulated as a con-
strained optimization problem and thus take
into account contextual dependencies and con-
straints among character bi-grams in the two
strings. We further explore several methods
for learning the objective function of the opti-
mization problem and show the advantage of
learning it discriminately. Our experiments
show that the new framework results in over
50% improvement in translating English NEs
to Hebrew.
1 Introduction
Named entity (NE) transliteration is the process of
transcribing a NE from a source language to some
target language based on phonetic similarity be-
tween the entities. Identifying transliteration pairs
is an important component in many linguistic appli-
cations which require identifying out-of-vocabulary
words, such as machine translation and multilingual
information retrieval (Klementiev and Roth, 2006b;
Hermjakob et al, 2008).
It may appear at first glance that identifying the
phonetic correlation between names based on an
orthographic analysis is a simple, straight-forward
Figure 1: Named entities transliteration pairs in English
and Hebrew and the character level mapping between the
two names. The Hebrew names can be romanized as ee-
ta-l-ya and a-ya
task; however in many cases a consistent deter-
ministic mapping between characters does not ex-
ist; rather, the mapping depends on the context the
characters appear in and on transliteration conven-
tions which may change across domains. Figure 1
exhibits two examples of NE transliterations in En-
glish and Hebrew, with the correct mapping across
the two scripts. Although the two Hebrew names
share a common prefix1, this prefix can be mapped
into a single English character or into two differ-
ent characters depending on the context it appears
in. Similarly, depending on the context it appears in,
the English character a can be mapped into different
characters or to an ?empty? character.
1In all our example the Hebrew script is shown left-to-right
to simplify the visualization of the transliteration mapping.
353
In recent years, as it became clear that solutions
that are based on linguistics rules are not satisfac-
tory, machine learning approaches have been de-
veloped to address this problem. The common ap-
proach adopted is therefore to view this problem
as a classification problem (Klementiev and Roth,
2006a; Tao et al, 2006) and train a discriminative
classifier. That is, given two strings, one in the
source and the other in the target language, extract
pairwise features, and train a classifier that deter-
mines if one is a transliteration of the other. Sev-
eral papers have followed up on this basic approach
and focused on semi-supervised approaches to this
problem or on extracting better features for the dis-
criminative classifier (Klementiev and Roth, 2006b;
Bergsma and Kondrak, 2007; Goldwasser and Roth,
2008). While it has been clear that the relevancy of
pairwise features is context sensitive and that there
are contextual constraints among them, the hope was
that a discriminative approach will be sufficient to
account for those by weighing features appropri-
ately. This has been shown to be difficult for lan-
guage pairs which are very different, such as English
and Hebrew (Goldwasser and Roth, 2008).
In this paper, we address these difficulties by
proposing to view the transliteration decision as a
globally phrased constrained optimization problem.
We formalize it as an optimization problem over
a set of local pairwise features ? character n-gram
matches across the two string ? and subject to legit-
imacy constraints.
We use a discriminatively trained classifier as a
way to learn the objective function for the global
constrained optimization problem. Our technical
approach follows a large body of work developed
over the last few years, following (Roth and Yih,
2004) that has formalized global decisions problems
in NLP as constrained optimization problems and
solved these optimization problems using Integer
Linear Programming (ILP) or other methods (Pun-
yakanok et al, 2005; Barzilay and Lapata, 2006;
Clarke and Lapata, ; Marciniak and Strube, 2005).
We investigate several ways to train our objective
function, which is represented as a dot product be-
tween a set of features chosen to represent a pair
(ws, wt), and a vector of initial weights. Our first
baseline makes use of all features extracted from a
pair, along with a simple counting method to deter-
mine initial weights. We then use a method simi-
lar to (Klementiev and Roth, 2006a; Goldwasser and
Roth, 2008) in order to discriminatively train a better
weight vector for the objective function.
Our key contribution is that we use a constrained
optimization approach also to determine a better fea-
ture representation for a given pair. (Bergsma and
Kondrak, 2007) attempted a related approach to re-
stricting the set of features representing a transliter-
ation candidate. However, rather than directly align-
ing the two strings as done there, we exploit the ex-
pressiveness of the ILP formulation and constraints
to generate a better representation of a pair. This
is the representation we then use to discriminatively
learn a better weight vector for the objective func-
tion used in our final model.
Our experiments focus on Hebrew-English
transliteration, which were shown to be very dif-
ficult in a previous work (Goldwasser and Roth,
2008). We show very significant improvements over
existing work with the same data set, proving the
advantage of viewing the transliteration decision as
a global inference problem. Furthermore, we show
the importance of using a discriminatively trained
objective function.
The rest of the paper is organized as follows. The
main algorithmic contribution of this paper is de-
scribed in Sec. 2. Our experimental study is de-
scribes in Sec. 3 and Sec. 4 concludes.
2 Using inference for transliteration
In this section we present our transliteration decision
framework, which is based on solving a constrained
optimization problem with an objective function that
is discriminatively learned. Our framework consists
of three key elements:
1. Decision Model When presented with a NE
in the source language ws and a set of candi-
dates {wt}k1 in the target language, the decision
model ranks the candidate pairs (ws, wt) and
selects the ?best? candidate pair. This is framed
as an optimization problem
w?t = argmaxi{w ? F (ws, wit)}, (1)
where F is a feature vector representation of
the pair (ws, wit) and w is a vector of weights
assigned to each feature.
354
2. Representation A pair s = (ws, wt) of source
and target NEs is represented as a vector of fea-
tures, each of which is a pair of character n-
grams, from ws and wt, resp. Starting with a
baseline representation introduced in (Klemen-
tiev and Roth, 2006a), denoted here AF (s),
we refine this representation to take into ac-
count dependencies among the individual n-
gram pairs. This refinement process is framed
as a constrained optimization problem:
F (s)? = argmaxF?AF {w ?AF (s)}, (2)
subject to a set C of linear constraints. Here
AF is the initial representation (All?Features),
w is a vector of weights assigned to each fea-
ture and C is a set of constraints accounting for
interdependencies among features.
3. Weight Vector Each pairwise n-gram feature is
associated with a weight; this weigh vector is
used in both optimization formulations above.
The weight vector is determined by considering
the whole training corpus. The initial weight
vector is obtained generatively, by counting the
relative occurrence of substring pairs in posi-
tive examples. The representation is refined by
discriminatively training a classifier to maxi-
mize transliteration performance on the train-
ing data. In doing that, each example is rep-
resented using the feature vector representation
described above.
The three key operations described above are be-
ing used in several stages, with different parameters
(weight vectors and representations) as described
in Alg. 1. In each stage a different element is re-
fined. The input to this process is a training corpus
Tr=(DS ,DT ) consisting of NE transliteration pairs
s = (ws, wt), where ws, wt are NEs in the source
and target language, respectively. Each such sam-
ple point is initially represented as a feature vector
AF (s) (for All?Features), where features are pairs
of substrings from the two words (following (Kle-
mentiev and Roth, 2006a)).
Given the set of feature vectors generated by ap-
plying AF to Tr, we assign initial weights W to
the features ((1) in Alg. 1). These weights form
the initial objective function used to construct a new
feature based representation, Informative?Features,
IFW (s) ((2) in Alg. 1). Specifically, for an instance
s, IFW (s) is the solution of the optimization prob-
lem in Eq. 2, with W as the weight vector, AF (s)
as the representation, and a set of constraints ensur-
ing the ?legitimacy? of the selected set of features
(Sec. 2.2.1).
Input: Training Corpora Tr=(DS ,DT )
Output: Transliteration model M
1. Initial Representation and Weights
For each sample s ? Tr, use AF to generate a
feature vector
{(fs, ft)1, (fs, ft)2, . . . , (fs, ft)n} ? {0, 1}n.
Define W :f ?R s.t. foreach feature f =(fs, ft)
W (f) = #(fs,ft)#(fs) ?
#(fs,ft)
#(ft)
2. Inferring Informative Representation (W )
Modify the initial representation by solving the
following constrained optimization problem:
IFW (s)? = argmaxIF (s)?(AF (s))W ?AF (s),
subject to constraints C.
3. Discriminative Training
Train a discriminative model on Tr, using
{IF (s)}s?Tr.
Let WD be the new weight vector obtained by
discriminative training.
4. Inferring Informative Representation (WD)
Modify the initial representation by solving the
following constrained optimization problem. This
time, the objective function is determined by the
discriminatively trained weight vector WD.
IFWD (s)? = argmaxIF (s)?(AF (s))WD ?AF (s),
subject to constraints C.
5. Decision Model
Given a word ws and a list of candidates
w1t , w2t , . . . wkt , the chosen transliteration is wt? ,
determined by:
t? = argmaxi{WD ? IFWD ((ws, wit))}
Algorithm 1: Transliteration Framework.
The new feature extraction operator IFW (s) is
now used to construct a new representation of the
training corpus. With this representation, we train
discriminately a new weight vector WD. This
weight vector, now defines a new objective function
for the optimization problem in Eq. 2; WD is the
weight vector and AF (s) the representation. We de-
355
note by IFWD(s) the solution of this optimization
problem for an instance s.
Given a representation and a weight vector, the
optimization problem in Eq. 1 is used to find the
transliteration of ws. Our best decision model makes
use of Eq. 1 using WD as the feature vector and
IFWD(s) as the feature representation of s.
The rest of this section provides details on the op-
erations and how we use them in different stages.
2.1 Initial Representation and Weights
The feature space we consider consists of n po-
tential features, each feature f = (fs, ft) repre-
sents a pairing of character level n-grams, where
fs ? {Source-Language ? empty-string } and ft ?
{Target-Language ? empty-string}. A given sample
(ws, wt) consisting of a pair of NEs is represented
as a features vector s ? {0, 1}n. We say that a fea-
ture f i is active if f i = 1 and that s1 ? s2, ??
{f i}{f i= 1 in s1} ? {f i}{f i=1 in s2}. We represent
the active features corresponding to a pair as a bipar-
tite graph G = (V,E), in which each vertex v ? V
either represents the empty string, a single character
or a bi-gram. V S , V T denote the vertices represent-
ing source and target language n-grams respectively.
Each of these sets is composed of two disjoint sub-
sets: VS = V SU ? V SB , VT = V TU ? V TB consisting
of vertices representing the uni-gram and bi-gram
strings. Given a vertex v, degree(v, V ?)denotes the
degree of v in a subgraph of G, consisting only of
V ? ? V ; index(v) is the index of the substring rep-
resented by v in the original string.
Edges in the bipartite graph represent active fea-
tures. The only deviation is that the vertex represent-
ing the empty string can be connected to any other
(non-empty) vertex.
Our initial feature extraction method follows the
one presented in (Klementiev and Roth, 2006a),
in which the feature space consists of n-gram pairs
from the two languages. Given a pair, each word
is decomposed into a set of character substrings of
up to a given length (including the empty string).
Features are generated by pairing substrings from
the two sets whose relative positions in the original
words differ by k or less places, or formally:
E = {e = (vi, vj) | (vi ? VS ? vj ? VT ) ?
(index(vj) + k ? index(vi) ? index(vj)? k) ?
Figure 2: All possible unigram and bigram pairs gener-
ated by the AF operator. The Hebrew name can be ro-
manized as lo-n-do-n
(vi 6= vempty?string ? vj 6= vempty?string)}.
In our experiments we used k=1 which tested em-
pirically, achieved the best performance.
Figure 2 exhibits the active features in the exam-
ple using the graph representation. We refer to this
feature extraction method as All-Features (AF ),
and define it formally as an operator AF : s ?
{(fs, ft)i} that maps a sample point s = (ws, wt)
to a set of active features.
The initial sample representation generates fea-
tures by coupling substrings from the two terms
without considering the dependencies between the
possible consistent combinations. Ideally, given
a positive sample, it is desirable that paired sub-
strings would encode phonetic similarity or a dis-
tinctive context in which the two substrings corre-
late. However, AF simply pairs substrings from the
two words, resulting in a noisy representation of the
sample point. Given enough positive samples, we
assume that features appearing with distinctive fre-
quency will encode the desired relation. We use this
observation, and construct a weight vector, associ-
ating each feature with a positive number indicating
its relative occurrence frequency in the training data
representation formed by AF . This weight is com-
puted as follows:
Definition 1 (Initial Feature Weights Vector) Let
W :f ?R s.t. for each feature f={fs, ft},
W (f) = #(fs, ft)#(fs) ?
#(fs, ft)
#(ft) ,
where #(fs, ft) is the number of occurrences of that
feature in the positive sample set, and #(fL), L =
{s, t} is the number of occurrences of an individual
substring, in any of the features extracted from pos-
itive samples in the training set.
356
These weights transform every example into a
weighted graph, where each edge is associated by W
with the weight assigned to the feature it represents.
As we empirically tested, this initialization assigns
high weights to features that preserve the phonetic
correlation between the two languages. The top part
of figure 5 presents several examples of weights as-
signed by W to features composed of different En-
glish and Hebrew substrings combinations. It can be
observed that combination which are phonetically
similar are associated with a higher weight. How-
ever, as it turns out, transliteration mappings do not
consist of ?clean? and consistent mappings of pho-
netically similar substrings. In the following section
we explain how to use these weights to generate a
more compact representation of samples.
2.2 Inferring Informative Representations
In this section we suggest a new feature extraction
method for determining the representation of a given
word pair. We use the strength of the active features
computed above, along with legitimacy constraints
on mappings between source and target strings to
find an optimal set of consistent active features that
represents a pair. This problem can be naturally en-
coded as a linear optimization problem, which seeks
to maximize a linear objective function determined
by W , over a set of variables representing the ac-
tive features selection, subject to a set of linear con-
straints representing the dependencies between se-
lections. We follow the formulation given by (Roth
and Yih, 2004), and define it as an Integer Linear
Programming (ILP) optimization problem, in which
each integer variable a(j,k), defined over {0, 1}, rep-
resents whether a feature pairing an n-gram j ? S
with an n-gram k ? T , is active. Although using ILP
is in general NP-hard, it has been used efficiently in
many natural language (see section 1). Our experi-
ence as well has been that this process is very effi-
cient due to the sparsity of the constraints used.
2.2.1 Constraining Feature Dependencies
To limit the selection of active features in each
sample we require that each element in the decom-
position of ws into bi-grams should be paired with
an element in wt, and the vice-versa. We restrict
the possible pairs by allowing only a single n-gram
to be matched to any other n-gram, with one excep-
tion - we allow every bi-gram to be mapped into an
empty string. Viewed as a bipartite graph, we allow
each node (with the exception of the empty string)
to have only one connected edge. These constraints,
given the right objective function, should enforce an
alignment of bi-grams according to phonetic simi-
larity; for example, the word pairs described in Fig-
ure 1, depicts a character level alignment between
the words, where in some cases a bi-gram is mapped
into a single character and in other cases single char-
acters are mapped to each other, based on phonetic
similarity encoded by the two scripts. However, im-
posing these constraints over the entire set of candi-
date features would be too restrictive; it is unlikely
that one can consistently represent a single ?correct?
phonetic mapping. We wish to represent both the
character level and bi-gram mapping between names
as both represent informative features on the corre-
spondence between the names over the two scripts.
To allow this, we decompose the problem into two
disjoint sets of constraints imposing 1-1 mappings,
one over the set of single character substrings and
the other over the bi-gram substrings. Given the bi-
partite graph generated by AF, we impose the fol-
lowing constraints:
Definition 2 (Transliteration Constraints) Let C
be the set of constraints, consisting of the following
predicates:
?v ? V S , degree(v,V S?V TU )?1 ?
?v ? V S , degree(v,V S?V TB )?1 ?
?v ? V T , degree(v,V T?V SU )?1 ?
?v ? V T , degree(v,V T?V SB )?1
For example, Figure 2 shows the graph of all pos-
sible candidates produced by AF . In Figure 3, the
graph is decomposed into two graphs, each depict-
ing possible matches between the character level
uni-gram or bi-gram substrings. the ILP constraints
ensure that in each graph, every node (with the ex-
ception of the empty string) has a degree of one .
Figure 4 gives the results of the ILP process ? a
unified graph in which every node has only a single
edge associated with it.
Definition 3 (Informative Feature Extraction (IF))
We define the Informative-Features(IF ) feature
extraction operator, IF : s ? {(fs, ft)i} as the
solution to the ILP problem in Eq. 2. Namely,
357
Figure 3: Find informative features by solving an ILP
problem. Dependencies between matching decisions are
modeled by allowing every node to be connected to a sin-
gle edge (except the node representing the empty-string).
Figure 4: The result of applying the IF operator by solv-
ing an ILP problem, represented as a pruned graph.
IF (s)? = argmaxIF (s)?(AF (s))w ?AF (s),
subject to constraints C.
We will use this operator with w = W , defined
above, and denote it IFW , and also use it with a
different weight vector, trained discriminatively, as
described next.
2.3 Discriminative Training
Using the IFW operator, we generate a better rep-
resentation of the training data, which is now used
to train a discriminative model. We use a linear
classifier trained with a regularized average percep-
tron update rule (Grove and Roth, 2001) as imple-
mented in SNoW, (Roth, 1998). This learning al-
gorithm provides a simple and general linear clas-
sifier that has been demonstrated to work well in
other NLP classification tasks, e.g. (Punyakanok
et al, 2005), and allows us to incorporate extensions
such as strength of features naturally into the train-
ing algorithm. We augment each sample in the train-
Figure 5: Several examples of weights assigned to fea-
tures generated by coupling English and Hebrew sub-
strings. Top figure: initial weights. Bottom figure: Dis-
criminatively learned weights. The Hebrew characters,
ordered left to right, can be romanized as y,z,t,sh
ing data with feature weights; given a sample, the
learner is presented with a real-valued feature vec-
tor instead of a binary vector. This can be viewed
as providing a better starting point for the learner,
which improves the learning rate (Golding and Roth,
1999; Ng and Jordan, 2001).
The weight vector learned by the discriminative
training is denoted WD. Given the new weight vec-
tor, we can define a new feature extraction opera-
tor, that we get by applying the objective function in
Eq. 2 with WD instead of W . Given a sample s, the
feature representation generated by this new infor-
mation extraction operator is denoted IFWD(s). The
key difference between W and WD is that the latter
was trained over a corpora containing both negative
and positive examples, and as a result WD contains
negative weights. To increase the impact of training
we multiplied the negative weights by 2.
Figure 5 presents some examples of the benefit
of discriminately learning the objective function; the
weighted edges in the top figure show the values as-
signed to features by W , while the bottom figure
shows the weights assigned by WD. In all cases,
phonetically similar characters were assigned higher
scores by WD, and character pairs not phonetically
similar were typically assigned negative weights. It
is also interesting to note a special phenomena oc-
curring in English-Hebrew transliterations. The En-
glish vowels will be paired to almost any Hebrew
character when generating pairs using AF , since
vowels in most cases are omitted in Hebrew, there
is no distinctive context in which English vowels
appear. We can see for example, in the top graph
358
presented in Figure 5 an edge matching a vowel to
a Hebrew character with a high weight, the bottom
graph showing the results of the discriminative train-
ing process show that this edge is associated with a
zero weight score.
2.4 Decision Models
This section defines several transliteration decision
models given a word ws and a list of candidates
w1t , w2t , . . . wkt . The models are used to identify the
correct transliteration pair from the set of candidates
{si = (ws, wit)}i=1...k.
In all cases, the decision is formulated as in Eq. 1,
where different models differ by the representations
and weight vectors used.
Decision Model 1 Ranking the transliteration can-
didates is done by evaluating
s? = argmaxi W ?AF (si),
which selects the transliteration pair which maxi-
mizes the objective function based on the genera-
tively computed weight vector.
Decision Model 2 Ranking the transliteration can-
didates is done by evaluating:
s? = argmaxi WD ?AF (si)).
This decision model is essentially equivalent to the
transliteration models used in (Klementiev and
Roth, 2006a; Goldwasser and Roth, 2008), in which
a linear transliteration model was trained using a fea-
ture extraction method equivalent to AF.
Decision Model 3 Ranking the transliteration can-
didates is done by evaluating:
s? = argmaxi W ? IFW (si),
which maximizes the objective function with the
generatively computed weight vector and the infor-
mative feature representation derived based on it.
Decision Model 4 Ranking the transliteration can-
didates is done by evaluating:
s? = argmaxi WD ? IFW (si)),
which conceptually resembles the transliteration
model presented in (Bergsma and Kondrak, 2007),
in that a discriminative classifier was trained and
used over a pruned feature set.
Decision Model 5 Ranking the transliteration can-
didates is done by evaluating:
s? = argmaxi WD ? IFWD(si),
which maximize the objective function with the dis-
criminately derived weight vector and the informa-
tive features inferred based on it. This decision
model is the only model that incorporates discrim-
inative weights as part of the feature extraction pro-
cess; WD is used as the objective function used
when inferring IFWD .
3 Evaluation
We evaluated our approach over a corpus of 300
English-Hebrew transliteration pairs, and used an-
other 250 different samples for training the models.
We constructed the test set by pairing each English
name with all Hebrew names in the corpus. The sys-
tem was evaluated on its ability to correctly iden-
tify the 300 transliteration pairs out of all the pos-
sible transliteration candidates. We measured per-
formance using the Mean Reciprocal Rank (MRR)
measure. This measure, originally introduced in
the field of information retrieval, is used to evaluate
systems that rank several options according to their
probability of correctness. MRR is a natural mea-
sure in our settings and has been used previously
for evaluating transliteration systems, for example
by (Tao et al, 2006).
Given a set Q of queries and their respective
responses ranked according to the system?s confi-
dence, we denote the rank of the correct response
to a query qi ? Q as rank(qi). MRR is then de-
fined as the average of the multiplicative inverse of
the rank of the correct answer, that is:
MRR = 1|Q|
?
i=1...|Q|
1
rank(qi) .
In our experiments we solved an ILP problem for
every transliteration candidate pairs, and computed
MRR with respect to the confidence of our decision
model across the candidates. Although this required
solving thousands of ILP instances, it posed no com-
putational burden as these instances typically con-
tained a small number of variables and constraints.
The entire test set is solved in less than 20 minutes
359
using the publicly available GLPK package (http:
//www.gnu.org/software/glpk/ ).
The performance of the different models is sum-
marized in table 1, these results are based on a train-
ing set of 250 samples used to train the discrimi-
native transliteration models and also to construct
the initial weight vector W . Figure 6 shows perfor-
mance over different number of training examples.
Our evaluation is concerns with the core transliter-
ation and decision models presented here and does
not consider any data set optimizations that were in-
troduced in previous works, which we view as or-
thogonal additions, hence the difference with the re-
sults published in (Goldwasser and Roth, 2008).
The results clearly show that our final model,
model 5, outperform other models. Interestingly,
model 1, a simplistic model, significantly outper-
forms the discriminative model presented in (Kle-
mentiev and Roth, 2006b). We believe that this is
due to two reasons. It shows that discriminative
training over the representation obtained using AF
is not efficient; moreover, this phenomenon is ac-
centuated given that we train over a very small data
set, which favors generative estimation of weights.
This is also clear when comparing the performance
of model 1 to model 4, which shows that learning
over the representation obtained using constrained
optimization (IF) results in a very significant perfor-
mance improvement.
The improvement of using IFW is not automatic.
Model 3, which uses IFW , and model 1, which uses
AF, converge to nearly the same result. Both these
models use generative weights to make the translit-
eration decision, and this highlights the importance
of discriminative training. Both model 4 and model
5 use discriminatively trained weights and signifi-
cantly outperform model 3. These results indicate
that using constraint optimization to generate the ex-
amples? representation in itself may not help; the ob-
jective function used in this inference has a signifi-
cant role in improved performance.
The benefit of discriminatively training the objec-
tive function becomes even clearer when compar-
ing the performance of model 5 to that of model 4,
which uses the original weight vector when inferring
the sample representation.
It can be assumed that this algorithm can bene-
fit from further iterations ? generating a new feature
Decision Model MRR
Baseline model, used in (KR?06,GR?08)
Model 2 0.51
Models presented in this paper
Model 1 0.713
Model 3 0.715
Model 4 0.832
Model 5 0.848
Table 1: Results of the different transliteration models,
trained using 250 samples. To facilitate readability (Kle-
mentiev and Roth, 2006b; Goldwasser and Roth, 2008)
are referenced as KR?06 and GR?08 respectively.
Figure 6: Results of the different constraint optimization
transliteration models. Performance is compared relative
to the number of samples used for training.
representations, training a model on it, and using the
resulting model as a new objective function. How-
ever, it turns out that after a single round, improved
weights due to additional training do not change the
feature representation; the inference process does
not yield a different outcome.
3.1 Normalized Objective Function
Formulating the transliteration decision as an op-
timization problem also allows us to naturally en-
code other considerations into our objective func-
tion. in this case we give preference to matching
short words. We encode this preference as a normal-
ization factor for the objective function. When eval-
uating on pair (ws, wt), we divide the weight vector
length of the shorter word; our decision model now
becomes:
Decision Model 6 (Model 5 - LengthNormalization)
360
Decision Model MRR
Model 5 0.848
Model 5 - LN 0.894
Table 2: Results of using model 5 with and without a
normalized objective function. Both models were trained
using 250 samples. The LN suffix in the model?s name
indicate that the objective function used length normal-
ization.
Figure 7: Results of using model 5 with and without a
normalized objective function. Performance is compared
relative to the number of samples used for training.
Ranking the transliteration candidates is done by
evaluating:
s? = argmaxi WD ? IFWD(si)/min(|ws|, |wt|)
As described in table 2 and figure 7, using
length normalization significantly improves the re-
sults. This can be attributed to the fact that typically
Hebrew names are shorter and therefore every pair
(ws, wt) considered by our model will be effected
differently by this normalization factor.
4 Discussion
We introduced a new approach for identifying NE
transliteration, viewing the transliteration decision
as a global inference problem. We explored sev-
eral methods for combining discriminative learning
in a global constraint optimization framework and
showed that discriminatively learning the objective
function improves performance significantly.
From an algorithmic perspective, our key contri-
bution is the introduction of a new method, in which
learning and inference are used in an integrated way.
We use learning to generate an objective function for
the inference process; use the inference process to
generate a better representation for the learning pro-
cess, and iterate these stages.
From the transliteration perspective, our key con-
tribution is in deriving and showing the significance
of a good representation for a pair of NEs. Our
representation captures both phonetic similarity and
distinctive occurrence patterns across character level
matchings of the two input strings, while enforcing
the constraints induced by the interdependencies of
the individual matchings. As we show, this represen-
tation serves to improve the ability of a discrimina-
tive learning algorithm to weigh features appropri-
ately and results in significantly better transliteration
models. This representation can be viewed as a com-
promise between models that do not consider depen-
dencies between local decisions and those that try to
align the two strings. Achieving this compromise is
one of the advantages of the flexibility allowed by
the constrained optimization framework we use. We
plan to investigate using more constraints within this
framework, such as soft constraints which can pe-
nalize unlikely local decisions while not completely
eliminating the entire solution.
Acknowledgments
We wish to thank Alex Klementiev and the anony-
mous reviewers for their insightful comments. This
work is partly supported by NSF grant SoD-HCER-
0613885 and DARPA funding under the Bootstrap
Learning Program.
References
R. Barzilay and M. Lapata. 2006. Aggregation via Set
Partitioning for Natural Language Generation. In Pro-
ceedings of HLT/NAACL, pages 359?366, New York
City, USA, June. Association for Computational Lin-
guistics.
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 656?663, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
J. Clarke and M. Lapata. Modeling compression with
discourse constraints. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
361
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1?11.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
D. Goldwasser and D. Roth. 2008. Active sample selec-
tion for named entity transliteration. In Proceedings
of ACL-08: HLT, Short Papers, Columbus, OH, USA,
Jun. Association for Computational Linguistics.
A. Grove and D. Roth. 2001. Linear concepts and hidden
variables. Machine Learning, 42(1/2):123?141.
Ulf Hermjakob, Kevin Knight, and Hal Daume? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proceedings of ACL-
08: HLT, pages 389?397, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
A. Klementiev and D. Roth. 2006a. Named entity
transliteration and discovery from multilingual com-
parable corpora. In Proc. of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL), pages 82?88, June.
A. Klementiev and D. Roth. 2006b. Weakly supervised
named entity transliteration and discovery from mul-
tilingual comparable corpora. In Proc. of the Annual
Meeting of the ACL, July.
T. Marciniak and M. Strube. 2005. Beyond the Pipeline:
Discrete Optimization in NLP. In Proceedings of the
Ninth CoNLL, pages 136?143, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
A. Y. Ng and M. I. Jordan. 2001. On discriminative vs.
generative classifiers: A comparison of logistic regres-
sion and na??ve bayes. In Neural Information Process-
ing Systems, pages 841?848.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling. In
Proc. of the International Joint Conference on Artifi-
cial Intelligence (IJCAI), pages 1117?1123.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 1?8. Association for
Computational Linguistics.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 806?813.
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat,
and ChengXiang Zhai. 2006. Unsupervised named
entity transliteration using temporal and phonetic cor-
relation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 250?257, Sydney, Australia, July. Association
for Computational Linguistics.
362
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 958?967,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Reading to Learn: Constructing Features from Semantic Abstracts
Jacob Eisenstein
?
James Clarke
?
Dan Goldwasser
?
Dan Roth
??
?
Beckman Institute for Advanced Science and Technology,
?
Department of Computer Science
University of Illinois
Urbana, IL 61801
{jacobe,clarkeje,goldwas1,danr}@illinois.edu
Abstract
Machine learning offers a range of tools
for training systems from data, but these
methods are only as good as the underly-
ing representation. This paper proposes to
acquire representations for machine learn-
ing by reading text written to accommo-
date human learning. We propose a novel
form of semantic analysis called read-
ing to learn, where the goal is to obtain
a high-level semantic abstract of multi-
ple documents in a representation that fa-
cilitates learning. We obtain this abstract
through a generative model that requires
no labeled data, instead leveraging repe-
tition across multiple documents. The se-
mantic abstract is converted into a trans-
formed feature space for learning, result-
ing in improved generalization on a rela-
tional learning task.
1 Introduction
Machine learning offers a range of powerful tools
for training systems to act in complex environ-
ments, but these methods depend on a well-chosen
representation for features. For learning to suc-
ceed the representation often must be crafted with
knowledge about the application domain. This
poses a bottleneck, requiring expertise in both ma-
chine learning and the application domain. How-
ever, domain experts often express their knowl-
edge through text; one direct expression is through
text designed to aid human learning. In this paper
we exploit text written by domain experts in or-
der to build a more expressive representation for
learning. We term this approach reading to learn.
The following scenario demonstrates the moti-
vation for reading to learn. Imagine an agent given
a task within its world/environment. The agent has
no prior knowledge of the task but can perceive the
world through low-level sensors. Learning directly
from the sensors may be difficult, as interesting
tasks typically require a complex combination of
sensors. Our goal is to acquire domain knowledge
through the semantic analysis of text, so as to pro-
duce higher-level relations through combinations
of sensors.
As a concrete example consider the problem of
learning how to make legal moves in Freecell soli-
taire. Relevant sensors may indicate if an object
is a card or a freecell, whether a card is a certain
value, and whether two values are in sequence.
Although it is possible to express the rules with
a combination of sensors, learning this combina-
tion is difficult. Text can facilitate learning by pro-
viding relations at the appropriate level of gen-
eralization. For example, the sentence: ?You can
place a card on an empty freecell,? suggests not
only which sensors are useful together but also
how these sensors should be linked. Assuming the
sensors are represented as predicates, one possi-
ble relation this sentence suggests is: r(x, y) =
card(x) ? freecell(y) ? empty(y). Armed
with this new relation the agent?s learning task
may be simpler. Throughout the paper we refer to
low-level sensory input as sensor or predicate, and
to a higher level concept as a logical formula or re-
lation.
Our approach to semantic analysis does not re-
quire a complete semantic representation of the
text. We merely wish to acquire a semantic ab-
stract of a document or document collection, and
use the discovered relations to facilitate data-
driven learning. This will allow us to directly eval-
uate the contribution of the extracted relations for
learning.
We develop an approach to recover semantic ab-
stracts that uses minimal supervision: we assume
only a very small set of lexical glosses, which map
from words to sensors. This marks a substantial
departure from previous work on semantic pars-
ing, which requires either annotations of the mean-
ings of each individual sentence (Zettlemoyer and
Collins, 2005; Liang et al, 2009), or alignments
of sentences to grounded representations of the
958
world (Chen and Mooney, 2008). For the purpose
of learning, this approach may be inapplicable, as
such text is often written at a high level of abstrac-
tion that permits no grounded representation.
There are two properties of our setting that
make unsupervised learning feasible. First, it is
not necessary to extract a semantic representation
of each individual sentence, but rather a summary
of the semantics of the document collection. Er-
rors in the semantic abstract are not fatal, as long
it guides the learning component towards a more
useful representation. Second, we can exploit rep-
etition across documents, which should generally
express the same underlying meaning. Logical for-
mulae that are well-supported by multiple docu-
ments are especially likely to be useful.
The rest of this paper describes our approach
for recovering semantic abstracts and outlines how
we apply and evaluate this approach on the Free-
cell domain. The paper contributes the following
key ideas: (1) Interpreting abstract ?instructional?
text, written at a level that does not correspond
to concrete sensory inputs in the world, so that
no grounded representation is possible, (2) read-
ing to learn, a new setting in which extracted se-
mantic representations are evaluated by whether
they facilitate learning; (3) abstractive semantic
summarization, aimed at capturing broad seman-
tic properties of a multi-document dataset, rather
than a semantic parse of individual sentences; (4) a
novel, minimally-supervised generative model for
semantic analysis which leverages both lexical and
syntactic properties of text.
2 Approach Overview
We describe our approach to text analysis as mul-
tidocument semantic abstraction, with the goal of
discovering a compact set of logical formulae to
explain the text in a document collection. To this
end, we develop a novel generative model in which
natural language sentences (e.g., ?You can always
place cards in empty freecells?) are stochastically
generated from logical formulae (e.g., card(x)?
freecell(y) ? empty(y)). We formally define
a generative process that reflects our intuitions
about the relationship between formulae and sen-
tences (Section 3), and perform sampling-based
inference to recover the formulae most likely to
have generated the observed data (Section 4). The
top N such formulae can then be added as addi-
tional predicates for relational learning.
Our semantic representation consists of con-
junctions of literals, each of which includes a sin-
gle predicate (e.g., empty) and one or more vari-
ables (e.g., x). Predicates describe atomic seman-
tic concepts, while variables construct networks
of relationships between them. While the impor-
tance of the predicates is obvious, the variable
assignments also exert a crucial influence on the
semantics of the conjunction: modifying a sin-
gle variable in the formula above from empty(y)
to empty(x) yields a formula that is trivially
false for all groundings (since cards can never be
empty).
Thus, our generative model must account for the
influence of both predicates and variables on the
sentences in the documents. A natural choice is to
use the predicates to influence the lexical items,
while letting the variables determine the syntac-
tic structure. For example, the formula card(x)?
freecell(y) ? empty(y) contains three pred-
icates and two variables. The predicates influence
the lexical items in a direct way: we expect that
sentences generated from this formula will include
a member of the gloss set for each predicate ?
the sentence ?Put the cards on the empty free-
cells? should be more likely than ?Columns are
constructed by playing cards in alternating colors.?
The impact of the variables on the generative
process is more subtle. The sharing of the variable
y suggests a relationship between the predicates
freecell and empty. This should be realized
in the syntactic structure of the sentence. Model-
ing syntax using a dependency tree, we expect that
the glosses for predicates that share terms will ap-
pear in compact sub-trees, while predicates that do
not share terms should be more distant. One pos-
sible surface realization of this logical formula is
the sentence, ?Put the card on the empty freecell,?
whose dependency parse is shown in the left tree
of Figure 1. The glosses empty and freecell are im-
mediately adjacent, while card is more remote.
We develop two metrics that quantify the com-
pactness of a set of variable assignments with
respect to a dependency tree: excess terms, and
shared terms. The number of excess terms in a
subtree is the number of unique terms assigned
to words in the subtree, minus the maximum arity
of any predicate in the subtree. Shared terms arise
whenever a node has multiple subtrees which each
contain the same variable. We will use the alterna-
tive alignments in Figure 1 to provide a more de-
tailed explanation. In each tree, the variables are
written in the nodes belonging to the associated
lexical items; variables are written over arrows to
indicate membership in some node in the subtree.
Excess Terms Alignment A of Fig-
ure 1, corresponding to the formula
959
Put
card on
freecell
the empty
the
X
X
X
XXX
X XX
XX
X
X
X
Y
XXY
X XY
XY
Y
X
Y
Y
XYY
X YY
YY
Y
X
Y
Z
XYZ
X YZ
YZ
Z
Dependency tree Alignment A Alignment B Alignment C Alignment D
Figure 1: A dependency parse and four different variable assignments. Each literal is aligned to a word (a
node in the graph), and the associated variables are written in the box. Variables belonging to descendant
nodes are written over the arrows.
card(x)?freecell(x)?empty(x), has zero
excess terms in every subtree; there is a total of one
variable, and all the predicates are unary. In Align-
ment B, card(x) ? freecell(x) ? empty(y),
there are excess terms at the root, and in the top
two subtrees on the right-hand side. Alignment C
contains an excess term at only the root node.
Even though it contains the same number of
unique variables as Alignment B, it is not penal-
ized as harshly because the alignment of variables
better corresponds to the syntactic structure.
Alignment D contains the greatest number of
excess terms: two at the root of the tree, and one
in each of the top two subtrees on the right side.
Shared Terms According to the excess term
metric, the best choice is simply to introduce as
few variables as possible. For this reason, we also
penalize shared terms which occur when a node
has subtree children that share a variable. In Fig-
ure 1, Alignments A and B each contain a shared
term at the top node; Alignments C and D contain
no shared terms.
Overall, we note that Alignment B is penalized
on both metrics, as it contains both excess terms
and shared terms; the syntactic structure of the
sentence makes such a variable assignment rela-
tively improbable.
card(x) & freecell(y) & empty(y)
f(y)e(y)c(x)
f(y)e(y)c(x)
Put the card on the empty freecell
(a)
(b)
(c)
(d)
(e)
Figure 2: A graphical depiction of the generative
process by which sentences are produced from for-
mulae
3 Generative Model
These intuitions are formalized in a generative
account of how sentences are stochastically pro-
duced from a set of logical formulae. This gener-
ative story guides an inference procedure for re-
covering logical formulae that are likely to have
generated any observed set of texts, which is de-
scribed in Section 4.
The outline of the generative process is depicted
in Figure 2. For each sentence, we begin in step (a)
by drawing a formula f from a Dirichlet pro-
cess (Ferguson, 1973). The Dirichlet process de-
960
fines a non-parametric mixture model, and has the
effect of adaptively selecting the appropriate num-
ber of formulae to explain the observed sentences
in the corpus.
1
We then draw the sentence length
from some distribution over positive integers; as
the sentence length is always observed, we need
not define the distribution (step (b)). In step (c), a
dependency tree is drawn from a uniform distribu-
tion over spanning trees with a number of nodes
equal to the length of the sentence. In step (d) we
draw an alignment of the literals in f to nodes in
the dependency tree, written a
t
(f). The distribu-
tion over alignments is described in Section 3.1.
Finally, the aligned literals are used to generate the
words at each slot in the dependency tree. A more
formal definition of this process is as follows:
? Draw ?, the expected number of literals per
formula, from a Gamma distribution G(u, v).
? Draw an infinite set of formulae f . For each
formula f
i
,
? Draw the formula length #|f
i
| from a
Poisson distribution, n
i
? Poisson(?).
? Draw n
i
literals from a uniform distri-
bution.
? Draw pi, an infinite multinomial distribution
over formulae: pi ? GEM(pi
0
), where GEM
refers to the stick-breaking prior (Sethura-
man, 1994) and pi
0
= 1 is the concentra-
tion parameter. By attaching the multinomial
pi to the infinite set of formulae f , we cre-
ate a Dirichlet process. This is conventionally
writtenDP (pi
0
, G
0
), where the base distribu-
tionG
0
encodes only the distribution over the
number of literals, Poisson(?).
? For each of D documents, draw the number
of sentences T ? Poisson. For each of the T
sentences in the document,
? Draw a formula f ? DP (pi
0
, G
0
) from
the Dirichlet Process described above.
? Draw a sentence length #|s| ? Poisson.
? Draw a dependency graph t (a spanning
tree of size #|s|) from a uniform distri-
bution.
? Draw an alignment a
t
(f), an injective
mapping from literals in f to nodes in
the dependency structure t. The distribu-
tion over alignments is described in Sec-
tion 3.1.
1
There are many recent applications of Dirichlet pro-
cesses in natural language processing, e.g. Goldwater et al
(2006).
? Draw the sentence s from the formula
f and the alignment a(f). For each
word token w
i
? s is drawn from
p(w
i
|a
t
(f, i)), where a
t
(f, i) indicates
the (possibly empty) literal assigned
to slot i in the alignment a
t
(f) (Sec-
tion 3.2).
3.1 Distribution over Alignments
The distribution over alignments reflects our intu-
ition that when literals share variables, they will
be aligned to word slots that are nearby in the de-
pendency structure; literals that do not share vari-
ables should be more distant. This is formalized by
applying the concepts of excess terms and shared
terms defined in Section 2. After computing the
number of excess and shared terms in each sub-
tree t
i
, we can compute a local score (LS ) for that
subtree:
LS (a
t
(f); t
i
) = ? ?NShared(a
t
(f), t
i
)
+ ? ?NExcess(a
t
(f), t
i
) ? height(t
i
).
This scoring function can be applied recursively to
each subtree in t; the overall score of the tree is the
recursive sum,
score(a
t
(f); t) = LS (a
t
(f); t)+
n
?
i
score(a
t
(f); t
i
),
(1)
where t
i
indicates the i
th
subtree of t. We hypoth-
esize a generative process that produces all possi-
ble alignments, scores them using score(a
t
(f); t),
and selects an alignment with probability,
p(a
t
(f)) ? exp{?score(a
t
(f); t)}. (2)
In our experiments, we define the parameters ? =
1, ? = 1.
3.2 Generation of Lexical Items
Once the logical formula is aligned to the parse
structure, the generation of the lexical items in
the sentence is straightforward. For word slots to
which no literals are aligned, the lexical item is
drawn from a language model ?, estimated from
the entire document collection. For slots to which
at least one literal is aligned, we construct a lan-
guage model ? in which the probability mass is
divided equally among all glosses of aligned pred-
icates. The language model ? is used as a backoff,
so that there is a strong bias in favor of generating
glosses, but some probability mass is reserved for
the other lexical items.
961
4 Inference
This section describes a sampling-based inference
procedure for obtaining a set of formulae f that
explain the observed text s and dependency struc-
tures t. We perform Gibbs sampling over the
formulae assigned to each sentence. Using the
Chinese Restaurant Process interpretation of the
Dirichlet Process (Aldous, 1985), we marginalize
pi, the infinite multinomial over all possible for-
mulae: at each sampling step we select either an
existing formula, or stochastically generate a new
formula. After each full round of Gibbs sampling,
a set of Metropolis-Hastings moves are applied to
explore modifications of the formulae. This proce-
dure converges on a stationary Markov chain cen-
tered on a set of formulae that cohere well with the
lexical and syntactic properties of the text.
4.1 Assigning Sentences to Formulae
For each sentence s
i
and dependency tree t
i
, a hid-
den variable y
i
indicates the index of the formula
that generates the text. We can resample y
i
using
Gibbs sampling. In the non-parametric setting, y
i
ranges over all non-negative integers; the Chinese
Restaurant Process formulation marginalizes the
infinite-dimensional parameter pi, yielding a prior
based on the counts for each ?active? formula (to
which at least one other sentence is assigned), and
a pseudo-count representing all non-active formu-
lae. Given K formulae, the prior on selecting for-
mula j is:
p(y
i
= j|y
?i
, pi
0
) ?
{
n
?i
(j) j < K
pi
0
j = K,
(3)
where y
?i
refers to the assignments of all y other
than y
i
and n
?i
refers to the counts over these as-
signments. Each j < K identifies an existing for-
mula in f , to which at least one other sentence is
assigned. When j = K, this means a new formula
f
?
must be generated.
To perform Gibbs sampling, we draw from the
posterior distribution over y
i
,
p(y
i
|s
i
, t
i
f , f
?
,y
?i
, pi
0
) ?
p(y
i
|y
?i
, pi
0
)p(s
i
, t
i
|y
i
, f , f
?
),
where the first term is the prior defined in Equa-
tion 3 and the latter term is the likelihood of gener-
ating the parsed sentence ?s
i
, t
i
? from the formula
indexed by y
i
.
To compute the probability of a parsed sentence
given a formula, we sum over alignments,
p(s, t|f) =
?
a
t
(f)
p(s, t,a
t
(f)|f)
=
?
a
t
(f)
p(s|a
t
(f))p(t,a
t
(f)|f)
=
?
a
t
(f)
p(s|a
t
(f))p(a
t
(f)|t, f)p(t|f),
(4)
applying the chain rule and independence assump-
tions from the generative model. The result is a
product of three terms: the likelihood of the lexi-
cal items given the aligned predicates (defined in
Section 3.2; the likelihood of the alignment given
the dependency tree and formula (defined in equa-
tion 2), and the probability of the dependency tree
given the formula, which is uniform.
Equation 4 takes a sum across alignments, but
most of the probability mass of p(s|a
t
(f)) will
be concentrated on alignments in which predicates
cover words that gloss them. Thus, we can apply
an approximation,
p(s, t|f) ?
N
?
a
t
(f)
p(s|a
t
(f))p(a
t
(f)|t, f)p(t|f),
(5)
in which we draw N samples in which predicates
are aligned to their glosses whenever possible.
Similarly, Equation 2 quantifies the likelihood
of an alignment only to a constant of proportional-
ity; again, a sum over possible alignments is nec-
essary. We do not expect the prior on alignments
to be strongly peaked like the sentence likelihood,
so we approximate the normalization term by sam-
pling M alignments at random and extrapolating:
p(a
t
(f)|t, f) ? q(a
t
(f); t)
=
q(a
t
(f); t)
?
a
?
t
(f)
q(a
?
t
(f); t)
?
#|a
?
t
(f)|
M
q(a
t
(f); t)
?
M
a
?
t
(f)
q(a
?
t
(f); t)
,
where q(a
t
(f); t) = exp{?score(a
t
(f); t)}, de-
fined in Equation 2. In our experiments, we set N
to at most 10, and M = 20. Drawing larger num-
bers of samples had no discernible effect on sys-
tem output.
962
4.1.1 Generating new formulae
Chinese Restaurant Process sampling requires the
generation of new candidate formulae at each re-
sampling stage. To generate a new formula, we
first sample the number of literals. As described
in the generative story (Section 3), the number
of literals is drawn from a Poisson distribution
with parameter ?. We treat ? as unknown and
marginalize, using the Gamma hyperprior G(u, v).
Due to Poisson-Gamma conjugacy, this marginal-
ization can be performed analytically, yielding
a Negative-Binomial distribution with parameters
?u+
?
i
#|f
i
|, (1+K+v)
?1
?, where
?
i
#|f
i
| is
the sum of the number of literals in each formula,
and K is the number of formulae which generate
at least one sentence. In this sense, the hyperpriors
u and v act as pseudo counts. We set u = 3, v = 1,
reflecting a weak prior expectation of three literals
per predicate.
After drawing the size of the formula, the predi-
cates are selected from a uniform random distribu-
tion. Finally, the terms are assigned: at each slot,
we reuse a previous term with probability 0.5, un-
less none is available; otherwise a new term is gen-
erated.
4.2 Proposing changes to formulae
The assignment resampling procedure has the
ability to generate new formulae, thus exploring
the space of relational features. However, to ex-
plore this space more rapidly, we introduce four
Metropolis-Hastings moves that modify existing
formulae (Gilks, 1995): adding a literal, deleting
a literal, substituting a literal, and rearranging the
terms of the formula. For each proposed move, we
recompute the joint likelihood of the formula and
all aligned sentences. The move is stochastically
accepted based on the ratio of the joint likelihoods
of the new and old configurations, multiplied by a
Hastings correction.
The joint likelihood with respect to formula f
is computed as p(s, t, f) = p(f)
?
i
p(s
i
, t
i
|f).
The prior on f considers only the number of liter-
als, using a Negative-Binomial distribution as de-
scribed in section 4.1.1. The likelihood p(s
i
, t
i
|f)
is given in equation 4. The Hastings correction is
p?(f
?
? f)/p?(f ? f
?
), with p?(f ? f
?
) indicat-
ing the probability of proposing a move from f
to f
?
,and p?(f
?
? f) indicating the probability of
proposing the reverse move. The Hastings correc-
tions depend on the arity of the predicates being
added and removed; the derivation is straightfor-
ward but tedious. We plan to release a technical
report with complete details.
4.3 Summary of inference
The final inference procedure iterates between
Gibbs sampling of assignments of formulae to
sentences, and manipulating the formulae through
Metropolis-Hastings moves. A full iteration com-
prises proposing a move to each formula, and then
using Gibbs sampling to reconsider all assign-
ments. If a formula no longer has any sentences
assigned to it, then it is dropped from the active
set, and can no longer be selected in Gibbs sam-
pling ? this is standard in the Chinese Restaurant
Process.
Five separate Markov chains are maintained in
parallel. To allow the sampling procedure to con-
verge to a stationary distribution, each chain be-
gins with 100 iterations of ?burn-in? sampling,
without storing the output. At this point, we per-
form another 100 iterations, storing the state at the
end of each iteration.
2
All formulae are ranked ac-
cording to the cumulative number of sentences to
which they are assigned (across all five Markov
chains), aggregating the counts for multiple in-
stances of identical formulae. This yields a ranked
list of formulae which will be used in our frame-
work as features for relational learning.
5 Evaluation
Our experimental setup is designed to evaluate the
quality of the semantic abstraction performed by
our model. The logical formulae obtained by our
system are applied as features for relational learn-
ing of the rules of the game of Freecell solitaire.
We investigate whether these features enable bet-
ter generalization given varying number of train-
ing examples of Freecell game states. We also
quantify the specific role of syntax, lexical choice,
and feature expressivity in learning performance.
This section describes the details of this evalua-
tion.
5.1 Relational Learning
We perform relational learning using Inductive
Logic Programming (ILP), which constructs gen-
eralized rules by assembling smaller logical for-
mulae to explain observed propositional exam-
ples (Muggleton, 1995). The lowest level formu-
lae consist of basic sensors that describe the en-
vironment. ILP?s expressivity enables it to build
complex conjunctions of these building blocks,
but at the cost of tractability. Our evaluation asks
whether the logical formulae abstracted from text
2
Sampling for more iterations was not found to affect per-
formance on development data, and the model likelihood ap-
peared stationary after 100 iterations.
963
Predicate Glosses
card(x) card
tableau(x) column, tableau
freecell(x) freecell, cell
homecell(x) foundation, cell, homecell
value(x,y) ace, king, rank, 8, 3, 7, lowest,
highest
successor(x,y) higher, sequence, sequential
color(x,y) black, red, color
suit(x,y) suit, club, diamond, spade,
heart
on(x,y) onto
top(x,y) bottom, available, top
empty(x) empty
Table 1: Predicates in the Freecell world model,
with natural language glosses obtained from the
development set text.
can transform the representation to facilitate learn-
ing. We compare against both the sensor-level rep-
resentation as well as richer representations that do
not benefit from the full power of our model?s se-
mantic analysis.
The ALEPH
3
ILP system, which is primarily
based on PROGOL (Muggleton, 1995), was used
to induce the rules of game. The search parame-
ters remained constant for all experiments.
5.2 Resources
There are four types of resources required to work
in the reading-to-learn setting: a world model, in-
structional text, a small set of glosses that map
from text to elements of the world model, and la-
beled examples of correct and incorrect actions
in the world. In our experiments, we consider
the domain of Freecell solitaire, a popular card
game (Morehead and Mott-Smith, 1983) in which
cards are moved between various types of loca-
tions, depending on their suit and rank. We now
describe the resources for the Freecell domain in
more detail.
WorldModel Freecell solitaire can be described
formally using first order logic; we consider a
slightly modified version of the representation
from the Planning Domain Definition Language
(PDDL), which is used in automatic game-playing
competitions. Specifically, there are 87 constants:
52 cards, 16 locations, 13 values, four suits, and
two colors. These constants are combined with a
fixed set of 11 predicates, listed in Table 1.
Instructional Text Our approach relies on text
that describes how to operate in the Freecell soli-
taire domain. A total of five instruction sets were
3
Freely available from http://www.comlab.ox.
ac.uk/activities/machinelearning/Aleph/
obtained from the Internet. Due to the popular-
ity of the Microsoft implementation of Freecell,
instructions often contain information specific to
playing Freecell on a computer. We manually re-
moved sentences which did not focus on the card
aspects of Freecell (e.g., how to set up the board
and information regarding where to click to move
cards). In order to use our semantic abstraction
model, the instructions were part-of-speech tagged
with the Stanford POS Tagger (Toutanova and
Manning, 2000) and dependency parses were ob-
tained using Malt (Nivre, 2006).
Glosses Our reading to learn setting requires a
small set of glosses, which are surface forms com-
monly used to represent predicates from the world
model. We envision an application scenario in
which a designer manually specifies a few glosses
for each predicate. However, for the purposes of
evaluation, it would be unprincipled for the exper-
imenters to handcraft the ideal set of glosses. In-
stead, we gathered a development set of text and
annotated the lexical mentions of the world model
predicates in text. This annotation is used to ob-
tain glosses to apply to the evaluation text. This
approximates a scenario in which the designer has
a reasonable idea of how the domain will be de-
scribed in text, but no prior knowledge of the spe-
cific details of the text instructions. Our exper-
iments used glosses that occurred two or more
times in the instructions: this yields a total of 32
glosses for 11 predicates, as shown in Table 1.
Evaluation game data Ultimately, the seman-
tic abstraction obtained from the text is applied
to learning on labeled examples of correct and
incorrect actions in the world model. For evalu-
ation, we automatically generated a set of move
scenarios: game states with one positive example
(a legal move) and one negative example (an ille-
gal move). To avoid bias in the data we generate
an equal number of move scenarios from each of
three types: moves to the freecells, homecells, and
tableaux. For our experiments we vary the number
of move scenarios in the training set; the develop-
ment and test sets consist of 900 and 1500 move
scenarios respectively.
5.3 Evaluation Settings
We compare four different feature sets, which
will be provided to the ALEPH ILP learner. All
feature sets include the sensor-level predicates
shown in Table 1. The FULL-MODEL feature
set alo includes the top logical formulae ob-
tained in our model?s semantic abstract (see Sec-
964
tion 4.3). The NO-SYNTAX feature set is obtained
from a variant of our model in which the in-
fluence of syntax is removed by setting parame-
ters ?, ? = 0. The SENSORS-ONLY feature set
uses only the sensor-level predicates. Finally, the
RELATIONAL-RANDOM feature set is constructed
by replacing each feature in the FULL-MODEL set
with a randomly generated relational feature of
identical expressivity (each predicate is replaced
by a randomly chosen alternative with identical
arity; terms are also assigned randomly). This en-
sures that any performance gains obtained by our
model were not due merely to the greater expres-
sivity of its relational features. The number of fea-
tures included in each scenario is tuned on a de-
velopment set of test examples.
The performance metric assesses the ability
of the ILP learner to classify proposed Freecell
moves as legal or illegal. As the evaluation set
contains an equal number of positive and negative
examples, accuracy is the appropriate metric. The
training scenarios are randomly generated; we re-
peat each run 50 times and average our results. For
the RELATIONAL-RANDOM feature set ? in which
predicates and terms are chosen randomly ? we
also regenerate the formulae per run.
6 Results
Table 2 shows a comparison of the results
using the setup described above. Our FULL-
MODEL achieves the best performance at ev-
ery training set size, consistently outperforming
the SENSORS-ONLY representation by an abso-
lute difference of three to four percent. This
demonstrates the semantic abstract obtained by
our model does indeed facilitate machine learning
in this domain.
RELATIONAL-RANDOM provides a baseline of
relational features with equal expressivity to those
chosen by our model, but with the predicates and
terms selected randomly. We consistently outper-
form this baseline, demonstrate that the improve-
ment obtained over the sensors only representation
is not due merely to the added expressivity of our
features.
The third row compares against NO-SYNTAX,
a crippled version of our model that incorpo-
rates lexical features but not the syntactic struc-
ture. The results are stronger than the SENSORS-
ONLY and RELATIONAL-RANDOM baselines, but
still weaker than our full system. This demon-
strates the syntactic features incorporated by our
model result in better semantic representations of
the underlying text.
Features Number of training scenarios
15 30 60 120
SENSORS-ONLY 79.12 88.07 92.77 93.73
RELATIONAL-RANDOM 82.72 89.14 93.08 94.17
NO-SYNTAX 80.98 89.79 94.11 97.04
FULL-MODEL 82.89 91.00 95.23 97.45
Table 2: Results as number of training examples
varied. Each value represents the accuracy of the
induced rules obtained with the given feature set.
card(x
1
) ? tableau(x
2
)
card(x
1
) ? freecell(x
2
)
homecell(x
1
) ? value(x
2
,x
3
)
empty(x
1
) ? freecell(x
1
)
card(x
1
) ? top(x
1
,x
2
)
card(x
1
) ? homecell(x
2
)
freecell(x
1
) ? homecell(x
2
)
card(x
1
) ? tableau(x
1
)
card(x
1
) ? top(x
2
,x
1
)
homecell(x
1
)
card(x
1
) ? homecell(x
1
)
color(x
1
,x
2
) ? value(x
3
,x
4
)
suit(x
1
,x
2
) ? value(x
3
,x
4
)
value(x
1
,x
2
) ? value(x
3
,x
4
)
homecell(x
1
) ? successor(x
2
,x
3
)
Figure 3: The top 15 features recovered by the se-
mantic abstraction of our full model.
Figure 3 shows the top 15 formulae recovered
by the full model running on the evaluation text.
Features such as empty(x
1
) ? freecell(x
1
)
are useful because they reuse variables to ensure
that objects have key properties ? in this case, en-
suring that a freecell is empty. Other features, such
as homecell(x
1
) ? value(x
2
, x
3
), help to fo-
cus the search on useful conjunctions of predicates
(in Freecell, the legality of playing a card on a
homecell depends on the value of the card). Note
that three of these 15 formulae are trivially use-
less, in that they are always false: e.g., card(x
1
)
? tableau(x
1
). This illustrates the importance
of term assignment in obtaining useful features
for learning. In the NO-SYNTAX system, which
ignores the relationship between term assignment
and syntactic structure, eight of the top 15 formu-
lae were trivially useless due to term incompatibil-
ity.
7 Related Work
This paper draws on recent literature on extract-
ing logical forms from surface text (Zettlemoyer
and Collins, 2005; Ge and Mooney, 2005; Downey
et al, 2005; Liang et al, 2009), interpreting lan-
guage in the context of a domain (Chen and
Mooney, 2008), and using an actionable domain
to guide text interpretation (Branavan et al, 2009).
We differentiate our research in several dimen-
sions:
965
Language Interpretation Instructional text de-
scribes generalized statements about entities in
the domain and the way they interact, thus the
text does not correspond directly to concrete sen-
sory inputs in the world (i.e., a specific world
state). Our interpretation captures these general-
izations as first-order logic statements that can be
evaluated given a specific state. This contrasts to
previous work which interprets ?directions? and
thus assumes a direct correspondence between text
and world state (Branavan et al, 2009; Chen and
Mooney, 2008).
Supervision Our work avoids supervision in the
form of labeled examples, using only a minimal
set of natural language glosses per predicate. Pre-
vious work also considered the supervision signal
obtained by interpreting natural language in the
context of a formal domain. Branavan et al (2009)
use feedback from a world model as a supervi-
sion signal. Chen and Mooney (2008) use tempo-
ral alignment of text and grounded descriptions of
the world state. In these approaches, concrete do-
main entities are grounded in language interpreta-
tion, and therefore require only a propositional se-
mantic representation. Previous approaches for in-
terpreting generalized natural language statements
are trained from labeled examples (Zettlemoyer
and Collins, 2005; Lu et al, 2008).
Level of analysis We aim for an abstractive
semantic summary across multiple documents,
whereas other approaches attempt to produce log-
ical forms for individual sentences (Zettlemoyer
and Collins, 2005; Ge and Mooney, 2005). We
avoid the requirement that each sentence have a
meaningful interpretation within the domain, al-
lowing us to handle relatively unstructured text.
Evaluation We do not evaluate the representa-
tions obtained by our model; rather we assess
whether these representations improve learning
performance. This is similar to work on Geo-
Query (Wong and Mooney, 2007; Ge and Mooney,
2005), and also to recent work on following step-
by-step directions (Branavan et al, 2009). While
these evaluations are performed on the basis of in-
dividual sentences, actions, or system responses,
we evaluate the holistic semantic analysis obtained
by our system.
Model We treat surface text as generated from a
latent semantic description. Lu et al (2008) ap-
ply a generative model, but require a complete
derivation from semantics to the lexical represen-
tation, while we favor a more flexible semantic
analysis that can be learned without annotation
and applied to noisy text. More similar is the work
of Liang et al (2009), which models the gener-
ation of semantically-relevant fields using lexical
and discourse features. Our approach differs by
accounting for syntax, which enables a more ex-
pressive semantic representation that includes un-
grounded variables.
Relational learning The output of our semantic
analysis is applied to learning in a structured rela-
tional space, using ILP. A key difficulty with ILP
is that the increased expressivity dramatically ex-
pands the hypothesis space, and it is widely agreed
that some learning bias is required for ILP to be
tractable (N?edellec et al, 1996; Cumby and Roth,
2003). Our work can be viewed as a new method
for acquiring such bias from text; moreover, our
approach is not specialized for ILP and may be
used to transform the feature space in other forms
of relational learning as well (Roth and Yih, 2001;
Cumby and Roth, 2003; Richardson and Domin-
gos, 2006).
8 Conclusion
This paper demonstrates a new setting for seman-
tic analysis, which we term reading to learn. We
handle text which describes the world in gen-
eral terms rather than refereing to concrete enti-
ties in the domain. We obtain a semantic abstract
of multiple documents, using a novel, minimally-
supervised generative model that accounts for both
syntax and lexical choice. The semantic abstract
is represented as a set of predicate logic formu-
lae, which are applied as higher-order features for
learning. We demonstrate that these features im-
prove learning performance, and that both the lex-
ical and syntactic aspects of our model yield sub-
stantial contributions.
In the current setup, we produce an ?overgener-
ated? semantic representation comprised of useful
features for learning but also some false positives.
Learning in our system can be seen as the process
of pruning this representation by selecting useful
formulae based on interaction with the training
data. In the future we hope to explore ways to in-
terleave semantic analysis with exploration of the
learning domain, by using the environment as a
supervision signal for linguistic analysis.
Acknowledgments We thank Gerald DeJong,
Julia Hockenmaier, Alex Klementiev and the
anonymous reviewers for their helpful feedback.
This work is supported by DARPA funding under
the Bootstrap Learning Program and the Beckman
Institute Postdoctoral Fellowship.
966
References
Aldous, David J. 1985. Exchangeability and re-
lated topics. Lecture Notes in Math 1117:1?198.
Branavan, S. R. K., Harr Chen, Luke Zettle-
moyer, and Regina Barzilay. 2009. Reinforce-
ment learning for mapping instructions to ac-
tions. In Proceedings of the Joint Conference
of the Association for Computational Linguis-
tics and International Joint Conference on Nat-
ural Language Processing Processing (ACL-
IJCNLP 2009). Singapore.
Chen, David L. and Raymond J. Mooney. 2008.
Learning to sportscast: A test of grounded lan-
guage acquisition. In Proceedings of 25th In-
ternational Conference on Machine Learning
(ICML 2008). Helsinki, Finland, pages 128?
135.
Cumby, Chad and Dan Roth. 2003. On kernel
methods for relational learning. In Proceed-
ings of the Twentieth International Conference
(ICML 2003). Washington, DC, pages 107?114.
Downey, Doug, Oren Etzioni, and Stephen Soder-
land. 2005. A probabilistic model of redun-
dancy in information extraction. In Proceedings
of the International Joint Conference on Arti-
ficial Intelligence (IJCAI 2005). pages 1034?
1041.
Ferguson, Thomas S. 1973. A bayesian analysis
of some nonparametric problems. The Annals
of Statistics 1(2):209?230.
Ge, Ruifang and Raymond J. Mooney. 2005. A
statistical semantic parser that integrates syn-
tax and semantics. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL-2005). Ann Arbor,
MI, pages 128?135.
Gilks, Walter R. 1995. Markov Chain Monte
Carlo in Practice. Chapman & Hall/CRC.
Goldwater, Sharon, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics
(COLING-ACL 2006). Sydney, Australia, pages
673?680.
Liang, Percy, Michael Jordan, and Dan Klein.
2009. Learning semantic correspondences with
less supervision. In Proceedings of the Joint
Conference of the Association for Computa-
tional Linguistics and International Joint Con-
ference on Natural Language Processing Pro-
cessing (ACL-IJCNLP 2009). Singapore.
Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for
parsing natural language to meaning representa-
tions. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP 2008).
Honolulu, Hawaii, pages 783?792.
Morehead, Albert H. and Geoffrey Mott-Smith.
1983. The Complete Book of Solitaire and Pa-
tience Games. Bantam.
Muggleton, Stephen. 1995. Inverse entailment and
progol. New Generation Computing Journal
13:245?286.
N?edellec, C., C. Rouveirol, H. Ad?e, F. Bergadano,
and B. Tausend. 1996. Declarative bias in ILP.
In L. De Raedt, editor, Advances in Inductive
Logic Programming, IOS Press, pages 82?103.
Nivre, Joakim. 2006. Inductive dependency pars-
ing. Springer.
Richardson, Matthew and Pedro Domingos. 2006.
Markov logic networks. Machine Learning
62:107?136.
Roth, Dan and Wen-tau Yih. 2001. Relational
learning via propositional algorithms: An infor-
mation extraction case study. In Proceedings of
the International Joint Conference on Artificial
Intelligence (IJCAI 2001). pages 1257?1263.
Sethuraman, Jayaram. 1994. A constructive def-
inition of dirichlet priors. Statistica Sinica
4(2):639?650.
Toutanova, Kristina and Christopher D. Manning.
2000. Enriching the knowledge sources used
in a maximum entropy part-of-speech tagger.
In Proceedings of the Joint SIGDAT Confer-
ence on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora
(EMNLP/VLC-2000). pages 63?70.
Wong, Yuk Wah and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic
parsing with lambda calculus. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics (ACL 2007). Prague,
Czech Republic, pages 128?135.
Zettlemoyer, Luke S. and Michael Collins. 2005.
Learning to map sentences to logical form:
Structured classification with probabilistic cat-
egorial grammars. In Proceedings of the 21st
Conference on Uncertainty in Artificial Intelli-
gence (UAI 2005). pages 658?666.
967
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 579?586, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Emotions from text: machine learning for text-based emotion prediction
Cecilia Ovesdotter Alm?
Dept. of Linguistics
UIUC
Illinois, USA
ebbaalm@uiuc.edu
Dan Roth
Dept. of Computer Science
UIUC
Illinois, USA
danr@uiuc.edu
Richard Sproat
Dept. of Linguistics
Dept. of Electrical Eng.
UIUC
Illinois, USA
rws@uiuc.edu
Abstract
In addition to information, text con-
tains attitudinal, and more specifically,
emotional content. This paper explores
the text-based emotion prediction prob-
lem empirically, using supervised machine
learning with the SNoW learning archi-
tecture. The goal is to classify the emo-
tional affinity of sentences in the narra-
tive domain of children?s fairy tales, for
subsequent usage in appropriate expres-
sive rendering of text-to-speech synthe-
sis. Initial experiments on a preliminary
data set of 22 fairy tales show encourag-
ing results over a na??ve baseline and BOW
approach for classification of emotional
versus non-emotional contents, with some
dependency on parameter tuning. We
also discuss results for a tripartite model
which covers emotional valence, as well
as feature set alernations. In addition, we
present plans for a more cognitively sound
sequential model, taking into considera-
tion a larger set of basic emotions.
1 Introduction
Text does not only communicate informative con-
tents, but also attitudinal information, including
emotional states. The following reports on an em-
pirical study of text-based emotion prediction.
Section 2 gives a brief overview of the intended
application area, whereas section 3 summarizes re-
lated work. Next, section 4 explains the empirical
study, including the machine learning model, the
corpus, the feature set, parameter tuning, etc. Sec-
tion 5 presents experimental results from two classi-
fication tasks and feature set modifications. Section
6 describes the agenda for refining the model, before
presenting concluding remarks in 7.
2 Application area: Text-to-speech
Narrative text is often especially prone to having
emotional contents. In the literary genre of fairy
tales, emotions such as HAPPINESS and ANGER and
related cognitive states, e.g. LOVE or HATE, become
integral parts of the story plot, and thus are of par-
ticular importance. Moreover, the story teller read-
ing the story interprets emotions in order to orally
convey the story in a fashion which makes the story
come alive and catches the listeners? attention.
In speech, speakers effectively express emotions
by modifying prosody, including pitch, intensity,
and durational cues in the speech signal. Thus, in
order to make text-to-speech synthesis sound as nat-
ural and engaging as possible, it is important to con-
vey the emotional stance in the text. However, this
implies first having identified the appropriate emo-
tional meaning of the corresponding text passage.
Thus, an application for emotional text-to-speech
synthesis has to solve two basic problems. First,
what emotion or emotions most appropriately de-
scribe a certain text passage, and second, given a text
passage and a specified emotional mark-up, how to
render the prosodic contour in order to convey the
emotional content, (Cahn, 1990). The text-based
emotion prediction task (TEP) addresses the first of
these two problems.
579
3 Previous work
For a complete general overview of the field of af-
fective computing, see (Picard, 1997). (Liu, Lieber-
man and Selker, 2003) is a rare study in text-
based inference of sentence-level emotional affin-
ity. The authors adopt the notion of basic emotions,
cf. (Ekman, 1993), and use six emotion categories:
ANGER, DISGUST, FEAR, HAPPINESS, SADNESS,
SURPRISE. They critique statistical NLP for being
unsuccessful at the small sentence level, and instead
use a database of common-sense knowledge and cre-
ate affect models which are combined to form a rep-
resentation of the emotional affinity of a sentence.
At its core, the approach remains dependent on an
emotion lexicon and hand-crafted rules for concep-
tual polarity. In order to be effective, emotion recog-
nition must go beyond such resources; the authors
note themselves that lexical affinity is fragile. The
method was tested on 20 users? preferences for an
email-client, based on user-composed text emails
describing short but colorful events. While the users
preferred the emotional client, this evaluation does
not reveal emotion classification accuracy, nor how
well the model generalizes on a large data set.
Whereas work on emotion classification from
the point of view of natural speech and human-
computer dialogues is fairly extensive, e.g. (Scherer,
2003), (Litman and Forbes-Riley, 2004), this ap-
pears not to be the case for text-to-speech synthe-
sis (TTS). A short study by (Sugimoto et al, 2004)
addresses sentence-level emotion recognition for
Japanese TTS. Their model uses a composition as-
sumption: the emotion of a sentence is a function of
the emotional affinity of the words in the sentence.
They obtain emotional judgements of 73 adjectives
and a set of sentences from 15 human subjects and
compute words? emotional strength based on the ra-
tio of times a word or a sentence was judged to fall
into a particular emotion bucket, given the number
of human subjects. Additionally, they conducted an
interactive experiment concerning the acoustic ren-
dering of emotion, using manual tuning of prosodic
parameters for Japanese sentences. While the au-
thors actually address the two fundamental problems
of emotional TTS, their approach is impractical and
most likely cannot scale up for a real corpus. Again,
while lexical items with clear emotional meaning,
such as happy or sad, matter, emotion classifica-
tion probably needs to consider additional inference
mechanisms. Moreover, a na??ve compositional ap-
proach to emotion recognition is risky due to simple
linguistic facts, such as context-dependent seman-
tics, domination of words with multiple meanings,
and emotional negation.
Many NLP problems address attitudinal mean-
ing distinctions in text, e.g. detecting subjective
opinion documents or expressions, e.g. (Wiebe et
al, 2004), measuring strength of subjective clauses
(Wilson, Wiebe and Hwa, 2004), determining word
polarity (Hatzivassiloglou and McKeown, 1997) or
texts? attitudinal valence, e.g. (Turney, 2002), (Bai,
Padman and Airoldi, 2004), (Beineke, Hastie and
Vaithyanathan, 2003), (Mullen and Collier, 2003),
(Pang and Lee, 2003). Here, it suffices to say that
the targets, the domain, and the intended application
differ; our goal is to classify emotional text passages
in children?s stories, and eventually use this infor-
mation for rendering expressive child-directed sto-
rytelling in a text-to-speech application. This can be
useful, e.g. in therapeutic education of children with
communication disorders (van Santen et al, 2003).
4 Empirical study
This part covers the experimental study with a for-
mal problem definition, computational implementa-
tion, data, features, and a note on parameter tuning.
4.1 Machine learning model
Determining emotion of a linguistic unit can be
cast as a multi-class classification problem. For
the flat case, let T denote the text, and s an em-
bedded linguistic unit, such as a sentence, where
s ? T . Let k be the number of emotion classes E =
{em1, em2, .., emk}, where em1 denotes the special
case of neutrality, or absence of emotion. The goal
is to determine a mapping function f : s ? emi,
such that we obtain an ordered labeled pair (s, emi).
The mapping is based on F = {f1, f2, .., fn}, where
F contains the features derived from the text.
Furthermore, if multiple emotion classes can
characterize s, then given E? ? E, the target of the
mapping function becomes the ordered pair (s,E?).
Finally, as further discussed in section 6, the hier-
archical case of label assignment requires a sequen-
580
tial model that further defines levels of coarse ver-
sus fine-grained classifiers, as done by (Li and Roth,
2002) for the question classification problem.
4.2 Implementation
Whereas our goal is to predict finer emotional mean-
ing distinctions according to emotional categories in
speech; in this study, we focus on the basic task of
recognizing emotional passages and on determining
their valence (i.e. positive versus negative) because
we currently do not have enough training data to ex-
plore finer-grained distinctions. The goal here is to
get a good understanding of the nature of the TEP
problem and explore features which may be useful.
We explore two cases of flat classification, us-
ing a variation of the Winnow update rule imple-
mented in the SNoW learning architecture (Carl-
son et al, 1999),1 which learns a linear classifier
in feature space, and has been successful in sev-
eral NLP applications, e.g. semantic role labeling
(Koomen, Punyakanok, Roth and Yih, 2005). In
the first case, the set of emotion classes E consists
of EMOTIONAL versus non-emotional or NEUTRAL,
i.e. E = {N,E}. In the second case, E has been
incremented with emotional distinctions according
to the valence, i.e. E = {N,PE,NE}. Experi-
ments used 10-fold cross-validation, with 90% train
and 10% test data.2
4.3 Data
The goal of our current data annotation project is
to annotate a corpus of approximately 185 children
stories, including Grimms?, H.C. Andersen?s and B.
Potter?s stories. So far, the annotation process pro-
ceeds as follows: annotators work in pairs on the
same stories. They have been trained separately and
work independently in order to avoid any annota-
tion bias and get a true understanding of the task
difficulty. Each annotator marks the sentence level
with one of eight primary emotions, see table 1, re-
flecting an extended set of basic emotions (Ekman,
1993). In order to make the annotation process more
focused, emotion is annotated from the point of view
of the text, i.e. the feeler in the sentence. While the
primary emotions are targets, the sentences are also
1Available from http://l2r.cs.uiuc.edu/?cogcomp/
2Experiments were also run for Perceptron, however the re-
sults are not included. Overall, Perceptron performed worse.
marked for other affective contents, i.e. background
mood, secondary emotions via intensity, feeler, and
textual cues. Disagreements in annotations are re-
solved by a second pass of tie-breaking by the first
author, who chooses one of the competing labels.
Eventually, the completed annotations will be made
available.
Table 1: Basic emotions used in annotation
Abbreviation Emotion class
A ANGRY
D DISGUSTED
F FEARFUL
H HAPPY
Sa SAD
Su+ POSITIVELY SURPRISED
Su- NEGATIVELY SURPRISED
Emotion annotation is hard; interannotator agree-
ment currently range at ? = .24 ? .51, with the ra-
tio of observed annotation overlap ranging between
45-64%, depending on annotator pair and stories as-
signed. This is expected, given the subjective nature
of the annotation task. The lack of a clear defini-
tion for emotion vs. non-emotion is acknowledged
across the emotion literature, and contributes to dy-
namic and shifting annotation targets. Indeed, a
common source of confusion is NEUTRAL, i.e. de-
ciding whether or not a sentence is emotional or
non-emotional. Emotion perception also depends on
which character?s point-of-view the annotator takes,
and on extratextual factors such as annotator?s per-
sonality or mood. It is possible that by focusing
more on the training of annotator pairs, particularly
on joint training, agreement might improve. How-
ever, that would also result in a bias, which is prob-
ably not preferable to actual perception. Moreover,
what agreement levels are needed for successful ex-
pressive TTS remains an empirical question.
The current data set consisted of a preliminary an-
notated and tie-broken data set of 1580 sentence, or
22 Grimms? tales. The label distribution is in table
2. NEUTRAL was most frequent with 59.94%.
Table 2: Percent of annotated labels
A D F H
12.34% 0.89% 7.03% 6.77%
N SA SU+ SU.-
59.94% 7.34% 2.59% 3.10%
581
Table 3: % EMOTIONAL vs. NEUTRAL examples
E N
40.06% 59.94%
Table 4: % POSITIVE vs. NEGATIVE vs. NEUTRAL
PE NE N
9.87% 30.19% 59.94%
Next, for the purpose of this study, all emotional
classes, i.e. A, D, F, H, SA, SU+, SU-, were com-
bined into one emotional superclass E for the first
experiment, as shown in table 3. For the second ex-
periment, we used two emotional classes, i.e. pos-
itive versus negative emotions; PE={H, SU+} and
NE={A, D, F, SA, SU-}, as seen in table 4.
4.4 Feature set
The feature extraction was written in python. SNoW
only requires active features as input, which resulted
in a typical feature vector size of around 30 features.
The features are listed below. They were imple-
mented as boolean values, with continuous values
represented by ranges. The ranges generally over-
lapped, in order to get more generalization coverage.
1. First sentence in story
2. Conjunctions of selected features (see below)
3. Direct speech (i.e. whole quote) in sentence
4. Thematic story type (3 top and 15 sub-types)
5. Special punctuation (! and ?)
6. Complete upper-case word
7. Sentence length in words (0-1, 2-3, 4-8, 9-15,
16-25, 26-35, >35)
8. Ranges of story progress (5-100%, 15-100%,
80-100%, 90-100%)
9. Percent of JJ, N, V, RB (0%, 1-100%, 50-
100%, 80-100%)
10. V count in sentence, excluding participles (0-1,
0-3, 0-5, 0-7, 0-9, > 9)
11. Positive and negative word counts ( ? 1, ? 2,
? 3, ? 4, ? 5, ? 6)
12. WordNet emotion words
13. Interjections and affective words
14. Content BOW: N, V, JJ, RB words by POS
Feature conjunctions covered pairings of counts of
positive and negative words with range of story
progress or interjections, respectively.
Feature groups 1, 3, 5, 6, 7, 8, 9, 10 and 14 are ex-
tracted automatically from the sentences in the sto-
ries; with the SNoW POS-tagger used for features
9, 10, and 14. Group 10 reflects how many verbs
are active in a sentence. Together with the quotation
and punctuation, verb domination intends to capture
the assumption that emotion is often accompanied
by increased action and interaction. Feature group
4 is based on Finish scholar Antti Aarne?s classes
of folk-tale types according to their informative the-
matic contents (Aarne, 1964). The current tales
have 3 top story types (ANIMAL TALES, ORDINARY
FOLK-TALES, and JOKES AND ANECDOTES), and
15 subtypes (e.g. supernatural helpers is a subtype
of the ORDINARY FOLK-TALE). This feature intends
to provide an idea about the story?s general affective
personality (Picard, 1997), whereas the feature re-
flecting the story progress is hoped to capture that
some emotions may be more prevalent in certain
sections of the story (e.g. the happy end).
For semantic tasks, words are obviously impor-
tant. In addition to considering ?content words?, we
also explored specific word lists. Group 11 uses
2 lists of 1636 positive and 2008 negative words,
obtained from (Di Cicco et al, online). Group 12
uses lexical lists extracted from WordNet (Fellbaum,
1998), on the basis of the primary emotion words
in their adjectival and nominal forms. For the ad-
jectives, Py-WordNet?s (Steele et al, 2004) SIMI-
LAR feature was used to retrieve similar items of
the primary emotion adjectives, exploring one addi-
tional level in the hierarchy (i.e. similar items of all
senses of all words in the synset). For the nouns and
any identical verbal homonyms, synonyms and hy-
ponyms were extracted manually.3 Feature group 13
used a short list of 22 interjections collected manu-
ally by browsing educational ESL sites, whereas the
affective word list of 771 words consisted of a com-
bination of the non-neutral words from (Johnson-
Laird and Oatley, 1989) and (Siegle, online). Only a
subset of these lexical lists actually occurred.4
3Multi-words were transformed to hyphenated form.
4At this point, neither stems and bigrams nor a list of ono-
matopoeic words contribute to accuracy. Intermediate resource
processing inserted some feature noise.
582
The above feature set is henceforth referred to as
all features, whereas content BOW is just group 14.
The content BOW is a more interesting baseline than
the na??ve one, P(Neutral), i.e. always assigning the
most likely NEUTRAL category. Lastly, emotions
blend and transform (Liu, Lieberman and Selker,
2003). Thus, emotion and background mood of im-
mediately adjacent sentences, i.e. the sequencing,
seems important. At this point, it is not implemented
automatically. Instead, it was extracted from the
manual emotion and mood annotations. If sequenc-
ing seemed important, an automatic method using
sequential target activation could be added next.
4.5 Parameter tuning
The Winnow parameters that were tuned included
promotional ?, demotional ?, activation threshold
?, initial weights ?, and the regularization parame-
ter, S, which implements a margin between positive
and negative examples. Given the currently fairly
limited data, results from 2 alternative tuning meth-
ods, applied to all features, are reported.
? For the condition called sep-tune-eval, 50%
of the sentences were randomly selected and
set aside to be used for the parameter tuning
process only. Of this subset, 10% were subse-
quently randomly chosen as test set with the re-
maining 90% used for training during the auto-
matic tuning process, which covered 4356 dif-
ferent parameter combinations. Resulting pa-
rameters were: ? = 1.1, ? = 0.5, ? = 5,
? = 1.0, S = 0.5. The remaining half of
the data was used for training and testing in the
10-fold cross-validation evaluation. (Also, note
the slight change for P(Neutral) in table 5, due
to randomly splitting the data.)
? Given that the data set is currently small, for the
condition named same-tune-eval, tuning was
performed automatically on all data using a
slightly smaller set of combinations, and then
manually adjusted against the 10-fold cross-
validation process. Resulting parameters were:
? = 1.2, ? = 0.9, ? = 4, ? = 1, S = 0.5. All
data was used for evaluation.
Emotion classification was sensitive to the selected
tuning data. Generally, a smaller tuning set resulted
in pejorative parameter settings. The random selec-
tion could make a difference, but was not explored.
5 Results and discussion
This section first presents the results from exper-
iments with the two different confusion sets de-
scribed above, as well as feature experimentation.
5.1 Classification results
Average accuracy from 10-fold cross validation for
the first experiment, i.e. classifying sentences as ei-
ther NEUTRAL or EMOTIONAL, are included in ta-
ble 5 and figure 1 for the two tuning conditions on
the main feature sets and baselines. As expected,
Table 5: Mean classification accuracy: N vs. E, 2 conditions
same-tune-eval sep-tune-eval
P(Neutral) 59.94 60.05
Content BOW 61.01 58.30
All features except BOW 64.68 63.45
All features 68.99 63.31
All features + sequencing 69.37 62.94
degree of success reflects parameter settings, both
for content BOW and all features. Nevertheless, un-
der these circumstances, performance above a na??ve
baseline and a BOW approach is obtained. More-
over, sequencing shows potential for contributing
in one case. However, observations also point to
three issues: first, the current data set appears to
be too small. Second, the data is not easily separa-
ble. This comes as no surprise, given the subjective
nature of the task, and the rather low interannota-
tor agreement, reported above. Moreover, despite
the schematic narrative plots of children?s stories,
tales still differ in their overall affective orientation,
which increases data complexity. Third and finally,
the EMOTION class is combined by basic emotion
labels, rather than an original annotated label.
More detailed averaged results from 10-fold
cross-validation are included in table 6 using all
features and the separated tuning and evaluation
data condition sep-tune-eval. With these parame-
ters, approximately 3% improvement in accuracy
over the na??ve baseline P(Neutral) was recorded,
and 5% over the content BOW, which obviously did
poorly with these parameters. Moreover, precision is
583
0 10 20 30 40 50 60 70
same-tune-eval
sep-tune-eval
Tuning sets
% Accuracy
P(Neutral) Content BOWAll features except BOW All featuresAll features + sequencing
Figure 1: Accuracy under different conditions (in %)
Table 6: Classifying N vs. E (all features, sep-tune-eval)
Measure N E
Averaged accuracy 0.63 0.63
Averaged error 0.37 0.37
Averaged precision 0.66 0.56
Averaged recall 0.75 0.42
Averaged F-score 0.70 0.47
higher than recall for the combined EMOTION class.
In comparison, with the same-tune-eval procedure,
the accuracy improved by approximately 9% over
P(Neutral) and by 8% over content BOW.
In the second experiment, the emotion category
was split into two classes: emotions with positive
versus negative valence. The results in terms of pre-
cision, recall, and F-score are included in table 7, us-
ing all features and the sep-tune-eval condition. The
decrease in performance for the emotion classes mir-
rors the smaller amounts of data available for each
class. As noted in section 4.3, only 9.87% of the
sentences were annotated with a positive emotion,
and the results for this class are worse. Thus, perfor-
mance seems likely to improve as more annotated
story data becomes available; at this point, we are
experimenting with merely around 12% of the total
texts targeted by the data annotation project.
5.2 Feature experiments
Emotions are poorly understood, and it is espe-
cially unclear which features may be important for
their recognition from text. Thus, we experimented
Table 7: N, PE, and NE (all features, sep-tune-eval)
N NE PE
Averaged precision 0.64 0.45 0.13
Averaged recall 0.75 0.27 0.19
Averaged F-score 0.69 0.32 0.13
Table 8: Feature group members
Word lists interj., WordNet, affective lists, pos/neg
Syntactic length ranges, % POS, V-count ranges
Story-related % story-progress, 1st sent., story type
Orthographic punctuation, upper-case words, quote
Conjunctions Conjunctions with pos/neg
Content BOW Words (N,V,Adj, Adv)
with different feature configurations. Starting with
all features, again using 10-fold cross-validation for
the separated tuning-evaluation condition sep-tune-
eval, one additional feature group was removed un-
til none remained. The feature groups are listed in
table 8. Figure 2 on the next page shows the accu-
racy at each step of the cumulative subtraction pro-
cess. While some feature groups, e.g. syntactic, ap-
peared less important, the removal order mattered;
e.g. if syntactic features were removed first, accu-
racy decreased. This fact also illustrated that fea-
tures work together; removing any group degraded
performance because features interact and there is
no true independence. It was observed that fea-
tures? contributions were sensitive to parameter tun-
ing. Clearly, further work on developing features
which fit the TEP problem is needed.
6 Refining the model
This was a ?first pass? of addressing TEP for TTS.
At this point, the annotation project is still on-going,
and we only had a fairly small data set to draw on.
Nevertheless, results indicate that our learning ap-
proach benefits emotion recognition. For example,
the following instances, also labeled with the same
valence by both annotators, were correctly classified
both in the binary (N vs. E) and the tripartite polar-
ity task (N, NE, PE), given the separated tuning and
evaluation data condition, and using all features:
(1a) E/NE: Then he offered the dwarfs money, and prayed and
besought them to let him take her away; but they said, ?We will
not part with her for all the gold in the world.?
584
Cumulative removal of feature groups
61.81
63.31
62.57
57.95
58.30
58.93
59.56
55
60
65
All features
- Word lists
- Syntactic
- Story-related
- Orthographic
- Conjunctions
- Content words
% A
ccur
acy
All features P(Neutral) BOW
Figure 2: Averaged effect of feature group removal, using sep-tune-eval
(1b) N: And so the little girl really did grow up; her skin was as
white as snow, her cheeks as rosy as the blood, and her hair as
black as ebony; and she was called Snowdrop.
(2a) E/NE: ?Ah,? she answered, ?have I not reason to weep?
(2b) N: Nevertheless, he wished to try him first, and took a stone
in his hand and squeezed it together so that water dropped out
of it.
Cases (1a) and (1b) are from the well-known FOLK
TALE Snowdrop, also called Snow White. (1a)
and (1b) are also correctly classified by the sim-
ple content BOW approach, although our approach
has higher prediction confidence for E/NE (1a); it
also considers, e.g. direct speech, a fairly high verb
count, advanced story progress, connotative words
and conjunctions thereof with story progress fea-
tures, all of which the BOW misses. In addition, the
simple content BOW approach makes incorrect pre-
dictions at both the bipartite and tripartite levels for
examples (2a) and (2b) from the JOKES AND ANEC-
DOTES stories Clever Hans and The Valiant Little
Tailor, while our classifier captures the affective dif-
ferences by considering, e.g. distinctions in verb
count, interjection, POS, sentence length, connota-
tions, story subtype, and conjunctions.
Next, we intend to use a larger data set to conduct
a more complete study to establish mature findings.
We also plan to explore finer emotional meaning dis-
tinctions, by using a hierarchical sequential model
which better corresponds to different levels of cog-
nitive difficulty in emotional categorization by hu-
mans, and to classify the full set of basic level emo-
tional categories discussed in section 4.3. Sequential
modeling of simple classifiers has been successfully
employed to question classification, for example by
(Li and Roth, 2002). In addition, we are working
on refining and improving the feature set, and given
more data, tuning can be improved on a sufficiently
large development set. The three subcorpora in the
annotation project can reveal how authorship affects
emotion perception and classification.
Moreover, arousal appears to be an important
dimension for emotional prosody (Scherer, 2003),
especially in storytelling (Alm and Sproat, 2005).
Thus, we are planning on exploring degrees of emo-
tional intensity in a learning scenario, i.e. a prob-
lem similar to measuring strength of opinion clauses
(Wilson, Wiebe and Hwa, 2004).
Finally, emotions are not discrete objects; rather
they have transitional nature, and blend and overlap
along the temporal dimension. For example, (Liu,
Lieberman and Selker, 2003) include parallel esti-
mations of emotional activity, and include smooth-
585
ing techniques such as interpolation and decay to
capture sequential and interactive emotional activity.
Observations from tales indicate that some emotions
are more likely to be prolonged than others.
7 Conclusion
This paper has discussed an empirical study of the
text-based emotion prediction problem in the do-
main of children?s fairy tales, with child-directed ex-
pressive text-to-speech synthesis as goal. Besides
reporting on encouraging results in a first set of com-
putational experiments using supervised machine
learning, we have set forth a research agenda for
tackling the TEP problem more comprehensively.
8 Acknowledgments
We are grateful to the annotators, in particular A.
Rasmussen and S. Siddiqui. We also thank two
anonymous reviewers for comments. This work was
funded by NSF under award ITR-#0205731, and NS
ITR IIS-0428472. The annotation is supported by
UIUC?s Research Board. The authors take sole re-
sponsibility for the work.
References
Antti Aarne. 1964. The Types of the Folk-Tale: a Classification
and Bibliography. Helsinki: Suomalainen Tiedeakatemia.
Cecilia O. Alm, and Richard Sproat. 2005. Perceptions of emo-
tions in expressive storytelling. INTERSPEECH 2005.
Xue Bai, Rema Padman, and Edoardo Airoldi. 2004. Sen-
timent extraction from unstructured text using tabu search-
enhanced Markov blankets. In MSW2004, Seattle.
Philip Beineke, Trevor Hastie, and Shivakumar Vaithyanathan.
2004. The sentimental factor: improving review classifi-
cation via human-provided information. In Proceedings of
ACL, 263?270.
Janet Cahn. 1990. The generation of affect in synthesized
Speech. Journal of the American Voice I/O Society, 8:1?19.
Andrew Carlson, Chad Cumby, Nicholas Rizzolo, Jeff Rosen,
and Dan Roth. 1999. The SNoW Learning Architecture.
Technical Report UIUCDCS-R-99-2101, UIUC Comp. Sci.
Stacey Di Cicco et al General Inquirer Pos./Neg. lists
http://www.webuse.umd.edu:9090/
Paul Ekman. 1993. Facial expression and emotion. American
Psychologist, 48(4), 384?392.
Christiane Fellbaum, Ed. 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press, Cambridge, Mass.
Vasileios Hatzivassiloglou, and Kathleen McKeown. 1997.
Predicting the semantic orientation of adjectives. In Pro-
ceedings of ACL, 174?181.
Philip Johnson-Laird, and Keith Oatley. 1989. The language
of emotions: an analysis of a semantic field. Cognition and
Emotion, 3:81?123.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2005. Generalized inference with multiple semantic role la-
beling systems. In Proceedings of the Annual Conference on
Computational Language Learning (CoNLL), 181?184.
Diane Litman, and Kate Forbes-Riley. 2004. Predicting stu-
dent emotions in computer-human tutoring dialogues. In
Proceedings of ACL, 351?358.
Xin Li, and Dan Roth. 2002. Learning question classifiers: the
role of semantic information. In Proc. International Confer-
ence on Computational Linguistics (COLING), 556?562.
Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A model of
textual affect sensing using real-world knowledge. In ACM
Conference on Intelligent User Interfaces, 125?132.
Tony Mullen, and Nigel Collier. 2004. Sentiment analy-
sis using support vector machines with diverse information
sources. In Proceedings of EMNLP, 412?418.
Bo Pang, and Lillian Lee. 2004. A sentimental education: sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL, 271?278.
Rosalind Picard. 1997. Affective computing. MIT Press, Cam-
bridge, Mass.
Dan Roth. 1998. Learning to resolve natural language ambigu-
ities: a unified approach. In AAAI, 806?813.
Klaus Scherer. 2003. Vocal communication of emotion: a
review of research paradigms. Speech Commununication,
40(1-2):227?256.
Greg Siegle. The Balanced Affective Word List
http://www.sci.sdsu.edu/CAL/wordlist/words.prn
Oliver Steele et al Py-WordNet
http://osteele.com/projects/pywordnet/
Futoshi Sugimoto et al 2004. A method to classify emotional
expressions of text and synthesize speech. In IEEE, 611?
614.
Peter Turney. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
In Proceedings of ACL, 417?424.
Jan van Santen et al 2003. Applications of computer gen-
erated expressive speech for communication disorders. In
EUROSPEECH 2003, 1657?1660.
Janyce Wiebe et al 2004. Learning subjective language. Jour-
nal of Computational Linguistics, 30(3):277?308.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004. Just
how mad are you? Finding strong and weak opinion clauses.
In Proceedings of the Nineteenth National Conference on Ar-
tificial Intelligence (AAAI), 761?769.
586
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 6?7,
Vancouver, October 2005.
Demonstrating an Interactive Semantic Role Labeling System
Vasin Punyakanok Dan Roth Mark Sammons
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{punyakan,danr,mssammon}@uiuc.edu
Wen-tau Yih
Microsoft Research
Redmond, WA 98052, USA
scottyih@microsoft.com
Abstract
Semantic Role Labeling (SRL) is the task
of performing a shallow semantic analy-
sis of text (i.e., Who did What to Whom,
When, Where, How). This is a cru-
cial step toward deeper understanding of
text and has many immediate applications.
Preprocessed information on text, mostly
syntactic, has been shown to be impor-
tant for SRL. Current research focuses on
improving the performance assuming that
this lower level information is given with-
out any attention to the overall efficiency
of the final system, although minimizing
execution time is a necessity in order to
support real world applications. The goal
of our demonstration is to present an inter-
active SRL system that can be used both
as a research and an educational tool. Its
architecture is based on the state-of-the-
art system (the top system in the 2005
CoNLL shared task), modified to process
raw text through the addition of lower
level processors, while achieving effective
real time performance.
1 Introduction
Semantic parsing of sentences is believed to be an
important subtask toward natural language under-
standing, and has immediate applications in tasks
such information extraction and question answering.
We study semantic role labeling (SRL), defined as
follows: for each verb in a sentence, the goal is to
identify all constituents that fill a semantic role, and
to determine their roles (such as Agent, Patient or In-
strument) and their adjuncts (such as Locative, Tem-
poral or Manner). The PropBank project (Kingsbury
and Palmer, 2002), which provides a large human-
annotated corpus of semantic verb-argument rela-
tions, has opened doors for researchers to apply ma-
chine learning techniques to this task.
The focus of the research has been on improving
the performance of the SRL system by using, in ad-
dition to raw text, various syntactic and semantic in-
formation, e.g. Part of Speech (POS) tags, chunks,
clauses, syntactic parse tree, and named entities,
which is found crucial to the SRL system (Pun-
yakanok et al, 2005).
In order to support a real world application such
as an interactive question-answering system, the
ability of an SRL system to analyze text in real time
is a necessity. However, in previous research, the
overall efficiency of the SRL system has not been
considered. At best, the efficiency of an SRL sys-
tem may be reported in an experiment assuming that
all the necessary information has already been pro-
vided, which is not realistic. A real world scenario
requires the SRL system to perform all necessary
preprocessing steps in real time. The overall effi-
ciency of SRL systems that include the preproces-
sors is not known.
Our demonstration aims to address this issue. We
present an interactive system that performs the SRL
task from raw text in real time. Its architecture is
based on the top system in the 2005 CoNLL shared
task (Koomen et al, 2005), modified to process raw
text using lower level processors but maintaining
6
good real time performance.
2 The SRL System Architecture
Our system begins preprocessing raw text by
using sentence segmentation tools (available at
http://l2r.cs.uiuc.edu/?cogcomp/tools.php). Next,
sentences are analyzed by a state-of-the-art syntac-
tic parser (Charniak, 2000) the output of which pro-
vides useful information for the main SRL module.
The main SRL module consists of four stages:
pruning, argument identification, argument classifi-
cation, and inference. The following is the overview
of these four stages. Details of them can be found
in (Koomen et al, 2005).
Pruning The goal of pruning is to filter out un-
likely argument candidates using simple heuristic
rules. Only the constituents in the parse tree are
considered as argument candidates. In addition, our
system exploits a heuristic modified from that intro-
duced by (Xue and Palmer, 2004) to filter out very
unlikely constituents.
Argument Identification The argument identifi-
cation stage uses binary classification to identify
whether a candidate is an argument or not. We train
and apply the binary classifiers on the constituents
supplied by the pruning stage.
Argument Classification This stage assigns the
final argument labels to the argument candidates
supplied from the previous stage. A multi-class clas-
sifier is trained to classify the types of the arguments
supplied by the argument identification stage.
Inference The purpose of this stage is to incor-
porate some prior linguistic and structural knowl-
edge, such as ?arguments do not overlap? and ?each
verb takes at most one argument of each type.? This
knowledge is used to resolve any inconsistencies in
argument classification in order to generate legiti-
mate final predictions. The process is formulated as
an integer linear programming problem that takes as
input confidence values for each argument type sup-
plied by the argument classifier for each constituent,
and outputs the optimal solution subject to the con-
straints that encode the domain knowledge.
The system in this demonstration, however, dif-
fers from its original version in several aspects.
First, all syntactic information is extracted from the
output of the full parser, where the original version
used different information obtained from different
processors. Second, the named-entity information is
discarded. Finally, no combination of different parse
tree outputs is performed. These alterations aim to
enhance the efficiency of the system while maintain-
ing strong performance.
Currently the system runs at the average speed of
1.25 seconds/predicate. Its performance is 77.88 and
65.87 F1-score on WSJ and Brown test sets (Car-
reras and Ma`rquez, 2005) while the original system
achieves 77.11 and 65.6 on the same test sets with-
out the combination of multiple parser outputs and
79.44 and 67.75 with the combination.
3 Goal of Demonstration
The goal of the demonstration is to present the sys-
tem?s ability to perform the SRL task on raw text in
real time. An interactive interface allows users to in-
put free form text and to receive the SRL analysis
from our system. This demonstration can be found
at http://l2r.cs.uiuc.edu/?cogcomp/srl-demo.php.
Acknowledgments
We are grateful to Dash Optimization for the free
academic use of Xpress-MP. This research is sup-
ported by ARDA?s AQUAINT Program, DOI?s Re-
flex program, and an ONR MURI Award.
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
conll-2005 shared tasks: Semantic role labeling. In
Proc. of CoNLL-2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL 2000.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proc. of LREC-2002, Spain.
P. Koomen, V. Punyakanok, D. Roth, and W. Yih. 2005.
Generalized Inference with Multiple Semantic Role
Labeling Systems. In Proceedings of CoNLL-2005.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling. In
Proc. of IJCAI-2005.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proc. of the EMNLP-2004.
7
The Importance of Syntactic Parsing and
Inference in Semantic Role Labeling
Vasin Punyakanok??
BBN Technologies
Dan Roth??
University of Illinois at
Urbana-Champaign
Wen-tau Yih??
Microsoft Research
We present a general framework for semantic role labeling. The framework combines a machine-
learning technique with an integer linear programming?based inference procedure, which in-
corporates linguistic and structural constraints into a global decision process. Within this
framework, we study the role of syntactic parsing information in semantic role labeling. We
show that full syntactic parsing information is, by far, most relevant in identifying the argument,
especially, in the very first stage?the pruning stage. Surprisingly, the quality of the pruning
stage cannot be solely determined based on its recall and precision. Instead, it depends on the
characteristics of the output candidates that determine the difficulty of the downstream prob-
lems. Motivated by this observation, we propose an effective and simple approach of combining
different semantic role labeling systems through joint inference, which significantly improves its
performance.
Our system has been evaluated in the CoNLL-2005 shared task on semantic role labeling,
and achieves the highest F1 score among 19 participants.
1. Introduction
Semantic parsing of sentences is believed to be an important task on the road to natural
language understanding, and has immediate applications in tasks such as informa-
tion extraction and question answering. Semantic Role Labeling (SRL) is a shallow
semantic parsing task, in which for each predicate in a sentence, the goal is to identify
all constituents that fill a semantic role, and to determine their roles (Agent, Patient,
Instrument, etc.) and their adjuncts (Locative, Temporal, Manner, etc.).
? 10 Moulton St., Cambridge, MA 02138, USA. E-mail: vpunyaka@bbn.com.
?? Department of Computer Science, University of Illinois at Urbana-Champaign, 201 N. Goodwin Ave.,
Urbana, IL 61801, USA. E-mail: danr@uiuc.edu.
? One Microsoft Way, Redmond, WA 98052, USA. E-mail: scottyih@microsoft.com.
? Most of the work was done when these authors were at the University of Illinois at Urbana-Champaign.
Submission received: 15 July 2006; revised submission received: 3 May 2007; accepted for publication:
19 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
The PropBank project (Kingsbury and Palmer 2002; Palmer, Gildea, and Kingsbury
2005), which provides a large human-annotated corpus of verb predicates and their ar-
guments, has enabled researchers to apply machine learning techniques to develop SRL
systems (Gildea and Palmer 2002; Chen and Rambow 2003; Gildea and Hockenmaier
2003; Pradhan et al 2003; Surdeanu et al 2003; Pradhan et al 2004; Xue and Palmer 2004;
Koomen et al 2005). However, most systems rely heavily on full syntactic parse trees.
Therefore, the overall performance of the system is largely determined by the quality
of the automatic syntactic parsers of which the state of the art (Collins 1999; Charniak
2001) is still far from perfect.
Alternatively, shallow syntactic parsers (i.e., chunkers and clausers), although they
do not provide as much information as a full syntactic parser, have been shown to
be more robust in their specific tasks (Li and Roth 2001). This raises the very natural
and interesting question of quantifying the importance of full parsing information to
semantic parsing and whether it is possible to use only shallow syntactic information to
build an outstanding SRL system.
Although PropBank is built by adding semantic annotations to the constituents in
the Penn Treebank syntactic parse trees, it is not clear how important syntactic parsing
is for an SRL system. To the best of our knowledge, this problem was first addressed
by Gildea and Palmer (2002). In their attempt to use limited syntactic information, the
parser they used was very shallow?clauses were not available and only chunks were
used. Moreover, the pruning stage there was very strict?only chunks were considered
as argument candidates. This results in over 60% of the actual arguments being ignored.
Consequently, the overall recall in their approach was very low.
The use of only shallow parsing information in an SRL system has largely been
ignored until the recent CoNLL-2004 shared task competition (Carreras and Ma`rquez
2004). In that competition, participants were restricted to using only shallow parsing
information, which included part-of-speech tags, chunks, and clauses (the definitions of
chunks and clauses can be found in Tjong Kim Sang and Buchholz [2000] and Carreras
et al [2002], respectively). As a result, the performance of the best shallow parsing?
based system (Hacioglu et al 2004) in the competition is about 10 points in F1 below the
best system that uses full parsing information (Koomen et al 2005). However, this is not
the outcome of a true and fair quantitative comparison. The CoNLL-2004 shared task
used only a subset of the data for training, which potentially makes the problem harder.
Furthermore, an SRL system is usually complicated and consists of several stages. It
was still unclear howmuch syntactic information helps and precisely where it helps the
most.
The goal of this paper is threefold. First, we describe an architecture for an SRL
system that incorporates a level of global inference on top of the relatively common
processing steps. This inference step allows us to incorporate structural and linguistic
constraints over the possible outcomes of the argument classifier in an easy way. The
inference procedure is formalized via an Integer Linear Programming framework and
is shown to yield state-of-the-art results on this task. Second, we provide a fair com-
parison between SRL systems that use full parse trees and systems that only use shal-
low syntactic information. As with our full syntactic parse?based SRL system (Koomen
et al 2005), our shallow parsing?based SRL system is based on the system that achieves
very competitive results and was one of the top systems in the CoNLL-2004 shared
task competition (Carreras and Ma`rquez 2004). This comparison brings forward a care-
ful analysis of the significance of full parsing information in the SRL task, and provides
an understanding of the stages in the process in which this information makes the most
difference. Finally, to relieve the dependency of the SRL system on the quality of
258
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
automatic parsers, we suggest a way to improve semantic role labeling significantly by
developing a global inference algorithm, which is used to combine several SRL systems
based on different state-of-the-art full parsers. The combination process is done through
a joint inference stage, which takes the output of each individual system as input and
generates the best predictions, subject to various structural and linguistic constraints.
The underlying system architecture can largely affect the outcome of our study.
Therefore, to make the conclusions of our experimental study as applicable as possible
to general SRL systems, the architecture of our SRL system follows the most widely
used two-step design. In the first step, the system is trained to identify argument candi-
dates for a given verb predicate. In the second step, the system classifies the argument
candidates into their types. In addition, it is also a simple procedure to prune obvious
non-candidates before the first step, and to use post-processing inference to fix incon-
sistent predictions after the second step. These two additional steps are also employed
by our system.
Our study of shallow and full syntactic information?based SRL systems was done
by comparing their impact at each stage of the process. Specifically, our goal is to investi-
gate at what stage full parsing information is most helpful relative to a shallow parsing?
based system. Therefore, our experiments were designed so that the compared systems
are as similar as possible, and the addition of the full parse tree?based features is the
only difference. The most interesting result of this comparison is that although each
step of the shallow parsing information?based system exhibits very good performance,
the overall performance is significantly inferior to the system that uses full parsing
information. Our explanation is that chaining multiple processing stages to produce
the final SRL analysis is crucial to understanding this analysis. Specifically, the quality
of the information passed from one stage to the other is a decisive issue, and it is
not necessarily judged simply by considering the F-measure. We conclude that, for
the system architecture used in our study, the significance of full parsing information
comes into play mostly at the pruning stage, where the candidates to be processed later
are determined. In addition, we produce a state-of-the-art SRL system by combining
different SRL systems based on two automatic full parsers (Collins 1999; Charniak 2001),
which achieves the best result in the CoNLL-2005 shared task (Carreras and Ma`rquez
2005).
The rest of this paper is organized as follows. Section 2 introduces the task of
semantic role labeling in more detail. Section 3 describes the four-stage architecture of
our SRL system, which includes pruning, argument identification, argument classifi-
cation, and inference. The features used for building the classifiers and the learning
algorithm applied are also explained there. Section 4 explains why and where full
parsing information contributes to SRL by conducting a series of carefully designed
experiments. Inspired by the result, we examine the effect of inference in a single system
and propose an approach that combines different SRL systems based on joint inference
in Section 5. Section 6 presents the empirical evaluation of our system in the CoNLL-
2005 shared task competition. After that, we discuss the related work in Section 7 and
conclude this paper in Section 8.
2. The Semantic Role Labeling (SRL) Task
The goal of the semantic role labeling task is to discover the predicate?argument struc-
ture of each predicate in a given input sentence. In this work, we focus only on the verb
predicate. For example, given a sentence I left my pearls to my daughter-in-law in my will,
259
Computational Linguistics Volume 34, Number 2
the goal is to identify the different arguments of the verb predicate left and produce the
output:
[A0 I] [V left ] [A1 my pearls] [A2 to my daughter-in-law] [AM-LOC in my will].
Here A0 represents the leaver, A1 represents the thing left, A2 represents the beneficiary,
AM-LOC is an adjunct indicating the location of the action, and V determines the
boundaries of the predicate, which is important when a predicate contains many words,
for example, a phrasal verb. In addition, each argument can be mapped to a constituent
in its corresponding full syntactic parse tree.
Following the definition of the PropBank and CoNLL-2004 and 2005 shared tasks,
there are six different types of arguments labeled as A0?A5 and AA. These labels have
different semantics for each verb and each of its senses as specified in the PropBank
Frame files. In addition, there are also 13 types of adjuncts labeled as AM-adj where adj
specifies the adjunct type. For simplicity in our presentation, we will also refer to these
adjuncts as arguments. In some cases, an argument may span over different parts of
a sentence; the label C-arg is then used to specify the continuity of the arguments, as
shown in this example:
[A1 The pearls] , [A0 I] [V said] , [C-A1 were left to my daughter-in-law].
In some other cases, an argumentmight be a relative pronoun that in fact refers to the ac-
tual agent outside the clause. In this case, the actual agent is labeled as the appropriate
argument type, arg, while the relative pronoun is instead labeled as R-arg. For example,
[A1 The pearls] [R-A1 which] [A0 I] [V left] [A2 to my daughter-in-law] are fake.
Because each verb may have different senses producing different semantic roles
for the same labels, the task of discovering the complete set of semantic roles should
involve not only identifying these labels, but also the underlying sense for a given
verb. However, as in all current SRL work, this article focuses only on identifying the
boundaries and the labels of the arguments, and ignores the verb sense disambiguation
problem.
The distribution of these argument labels is fairly unbalanced. In the official release
of PropBank I, core arguments (A0?A5 and AA) occupy 71.26% of the arguments, where
the largest parts are A0 (25.39%) and A1 (35.19%). The rest mostly consists of adjunct
arguments (24.90%). The continued (C-arg) and referential (R-arg) arguments are rela-
tively few, occupying 1.22% and 2.63%, respectively. For more information on PropBank
and the semantic role labeling task, readers can refer to Kingsbury and Palmer (2002)
and Carreras and Ma`rquez (2004, 2005).
Note that the semantic arguments of the same verb do not overlap. We define over-
lapping arguments to be those that share some of their parts. An argument is considered
embedded in another argument if the second argument completely covers the first one.
Arguments are exclusively overlapping if they are overlapping but are not embedded.
3. SRL System Architecture
Adhering to the most common architecture for SRL systems, our SRL system consists of
four stages: pruning, argument identification, argument classification, and inference.
In particular, the goal of pruning and argument identification is to identify argument
candidates for a given verb predicate. In the first three stages, however, decisions
are independently made for each argument, and information across arguments is not
260
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
incorporated. The final inference stage allows us to use this type of information along
with linguistic and structural constraints in order to make consistent global predictions.
This system architecture remains unchanged when used for studying the impor-
tance of syntactic parsing in SRL, although different information and features are used.
Throughout this article, when full parsing information is available, we assume that
the system is presented with the full phrase-structure parse tree as defined in the Penn
Treebank (Marcus, Marcinkiewicz, and Santorini 1993) but without trace and functional
tags. On the other hand, when only shallow parsing information is available, the full
parse tree is reduced to only the chunks and the clause constituents.
A chunk is a phrase containing syntactically related words. Roughly speaking,
chunks are obtained by projecting the full parse tree onto a flat tree; hence, they are
closely related to the base phrases. Chunks were not directly defined as part of the
standard annotation of the treebank, but, rather, their definition was introduced in the
CoNLL-2000 shared task on text chunking (Tjong Kim Sang and Buchholz 2000), which
aimed to discover such phrases in order to facilitate full parsing. A clause, on the other
hand, is the clausal constituent as defined by the treebank standard. An example of
chunks and clauses is shown in Figure 1.
3.1 Pruning
When the full parse tree of a sentence is available, only the constituents in the parse
tree are considered as argument candidates. Our system exploits the heuristic rules
introduced by Xue and Palmer (2004) to filter out simple constituents that are very
unlikely to be arguments. This pruning method is a recursive process starting from the
target verb. It first returns the siblings of the verb as candidates; then it moves to the
parent of the verb, and collects the siblings again. The process goes on until it reaches
the root. In addition, if a constituent is a PP (prepositional phrase), its children are also
collected. For example, in Figure 1, if the predicate (target verb) is assume, the pruning
heuristic will output: [PP by John Smith who has been elected deputy chairman], [NP John
Smith who has been elected deputy chairman], [VB be], [MD will], and [NP His duties].
3.2 Argument Identification
The argument identification stage utilizes binary classification to identify whether a
candidate is an argument or not. When full parsing is available, we train and apply
the binary classifiers on the constituents supplied by the pruning stage. When only
shallow parsing is available, the system does not have a pruning stage, and also does
not have constituents to begin with. Therefore, conceptually, the system has to consider
all possible subsequences (i.e., consecutive words) in a sentence as potential argument
candidates. We avoid this by using a learning scheme that utilizes two classifiers, one to
predict the beginnings of possible arguments, and the other the ends. The predictions
are combined to form argument candidates. However, we can employ a simple heuristic
to filter out some candidates that are obviously not arguments. The final predication
includes those that do not violate the following constraints.
1. Arguments cannot overlap with the predicate.
2. If a predicate is outside a clause, its arguments cannot be embedded in
that clause.
3. Arguments cannot exclusively overlap with the clauses.
261
Computational Linguistics Volume 34, Number 2
Figure 1
An example of a parse tree and its predicate?argument structure.
The first constraint comes from the definition of this task that the predicate simply
cannot take itself or any constituents that contain itself as arguments. The other two
constraints are due to the fact that a clause can be treated as a unit that has its own
verb?argument structure. If a verb predicate is outside a clause, then its argument can
only be the whole clause, but may not be embedded in or exclusively overlap with the
clause.
For the argument identification classifier, the features used in full parsing and
shallow parsing settings are all binary features, which are described subsequently.
3.2.1 Features Used When Full Parsing is Available. Most of the features used in our
system are common features for the SRL task. The creation of PropBank was inspired
by the works of Levin (1993) and Levin and Hovav (1996), which discuss the relation
between syntactic and semantic information. Following this philosophy, the features
aim to indicate the properties of the predicate, the constituent which is an argument
candidate, and the relationship between them through the available syntactic infor-
mation. We explain these features herein. For further discussion of these features, we
262
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
refer the readers to the article by Gildea and Jurafsky (2002), which introduced these
features.
 Predicate and POS tag of predicate: indicate the lemma of the predicate
verb and its POS tag.
 Voice: indicates passive/active voice of the predicate.
 Phrase type: provides the phrase type of the constituent, which is the tag
of the corresponding constituent in the parse tree.
 Head word and POS tag of the head word: provides the head word of the
constituent and its POS tag. We use the rules introduced by Collins (1999)
to extract this feature.
 Position: describes if the constituent is before or after the predicate,
relative to the position in the sentence.
 Path: records the tags of parse tree nodes in the traversal path from the
constituent to the predicate. For example, in Figure 1, if the predicate is
assume and the constituent is [S who has been elected deputy chairman], the
path is S?NP?PP?VP?VBN, where ? and ? indicate the traversal direction
in the path.
 Subcategorization: describes the phrase structure around the predicate?s
parent. It records the immediate structure in the parse tree that expands to
its parent. As an example, if the predicate is elect in Figure 1, its
subcategorization is VP?(VBN)-NP while the subcategorization of the
predicate assume is VP?(VBN)-PP. Parentheses indicate the position of the
predicate.
Generally speaking, we consider only the arguments that correspond to some con-
stituents in parse trees. However, in some cases, we need to consider an argument that
does not exactly correspond to a constituent, for example, in our experiment in Sec-
tion 4.2 where the gold-standard boundaries are used with the parse trees generated by
an automatic parse. In such cases, if the information on the constituent, such as phrase
type, needs to be extracted, the deepest constituent that covers the whole argument will
be used. For example, in Figure 1, the phrase type for by John Smith is PP, and its path
feature to the predicate assume is PP?VP?VBN.
We also use the following additional features. These features have been shown
to be useful for the systems by exploiting other information in the absence of the
full parse tree information (Punyakanok et al 2004), and, hence, can be helpful in
conjunction with the features extracted from a full parse tree. They also aim to encode
the properties of the predicate, the constituent to be classified, and their relationship in
the sentence.
 Context words and POS tags of the context words: the feature
includes the two words before and after the constituent, and their
POS tags.
 Verb class: the feature is the VerbNet (Kipper, Palmer, and Rambow 2002)
class of the predicate as described in PropBank Frames. Note that a
263
Computational Linguistics Volume 34, Number 2
verb may inhabit many classes and we collect all of these classes as
features, regardless of the context-specific sense which we do not attempt
to resolve.
 Lengths: of the constituent, in the numbers of words and chunks
separately.
 Chunk: tells if the constituent ?is,? ?embeds,? ?exclusively overlaps,? or
?is embedded in? a chunk with its type. For instance, in Figure 1, if the
constituents are [NP His duties], [PP by John Smith], and [VBN elected], then
their chunk features are ?is-NP,? ?embed-PP & embed-NP,? and
?embedded-in-VP,? respectively.
 Chunk pattern: encodes the sequence of chunks from the constituent to
the predicate. For example, in Figure 1 the chunk sequence from [NP His
duties] to the predicate elect is VP-PP-NP-NP-VP.
 Chunk pattern length: the feature counts the number of chunks in the
chunk pattern feature.
 Clause relative position: encodes the position of the constituent relative
to the predicate in the pseudo-parse tree constructed only from clause
constituents, chunks, and part-of-speech tags. In addition, we label the
clause with the type of chunk that immediately precedes the clause.
This is a simple rule to distinguish the type of clause based on
the intuition that a subordinate clause often modifies the part of the
sentence immediately before it. Figure 2 shows the pseudo-parse
tree of the parse tree in Figure 1. By disregarding the chunks, there
are four configurations??target constituent and predicate are
siblings,? ?target constituent?s parent is an ancestor of predicate,?
?predicate?s parent is an ancestor of target word,? or ?otherwise.?
This feature can be viewed as a generalization of the Path feature
described earlier.
 Clause coverage: describes how much of the local clause from the
predicate is covered by the target argument.
 NEG: the feature is active if the target verb chunk has not or n?t.
 MOD: the feature is active when there is a modal verb in the verb chunk.
The rules of the NEG and MOD features are used in a baseline SRL system
developed by Erik Tjong Kim Sang (Carreras and Ma`rquez 2004).
Figure 2
The pseudo-parse tree generated from the parse tree in Figure 1.
264
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
In addition, we also use the conjunctions of features which conjoin any two features
into a new feature. For example, the conjunction of the predicate and path features
for the predicate assume and the constituent [S who has been elected deputy chairman] in
Figure 1 is (S?NP?PP?VP?VBN, assume).
3.2.2 Features Used When Only Shallow Parsing is Available. Most features used here are
similar to those used by the systemwith full parsing information. However, for features
that need full parse trees in their extraction procedures, we either try to mimic them
with some heuristic rules or discard them. The details of these features are as follows.
 Phrase type: uses a simple heuristic to identify the type of the argument
candidate as VP, PP, or NP.
 Head word and POS tag of the head word: are the rightmost word for
NP, and the leftmost word for VP and PP.
 Shallow-Path: records the traversal path in the pseudo-parse tree.
This aims to approximate the Path features extracted from the full
parse tree.
 Shallow-Subcategorization: describes the chunk and clause structure
around the predicate?s parent in the pseudo-parse tree. This aims to
approximate the Subcategorization feature extracted from the full parse
tree.
3.3 Argument Classification
This stage assigns labels to the argument candidates identified in the previous stage.
A multi-class classifier is trained to predict the types of the argument candidates. In
addition, to reduce the excessive candidates mistakenly output by the previous stage,
the classifier can also label an argument as ?null? (meaning ?not an argument?) to dis-
card it.
The features used here are the same as those used in the argument identification
stage. However, when full parsing is available, an additional feature introduced by Xue
and Palmer (2004) is used.
 Syntactic frame: describes the sequential pattern of the noun phrases and
the predicate in the sentence which aims to complement the Path and
Subcategorization features.
The learning algorithm used for training the argument classifier and argument iden-
tifier is a variation of the Winnow update rule incorporated in SNoW (Roth 1998;
Carlson et al 1999), a multi-class classifier that is tailored for large scale learning tasks.
SNoW learns a sparse network of linear functions, in which the targets (argument
border predictions or argument type predictions, in this case) are represented as linear
functions over a common feature space; multi-class decisions are done via a winner-
take-all mechanism. It improves the basic Winnow multiplicative update rule with a
regularization term, which has the effect of separating the data with a large margin
separator (Dagan, Karov, and Roth 1997; Grove and Roth 2001; Zhang, Damerau, and
Johnson 2002) and voted (averaged) weight vector (Freund and Schapire 1999; Golding
and Roth 1999).
265
Computational Linguistics Volume 34, Number 2
The softmax function (Bishop 1995) is used to convert raw activation to conditional
probabilities. If there are n classes and the raw activation of class i is acti, the posterior
estimation for class i is
Prob(i) = e
acti
?
1?j?n e
actj
Note that in training this classifier, unless specified otherwise, the argument can-
didates used to generate the training examples are obtained from the output of the
argument identifier, not directly from the gold-standard corpus. In this case, we au-
tomatically obtain the necessary examples to learn for class ?null.?
3.4 Inference
In the previous stages, decisions were always made for each argument independently,
ignoring the global information across arguments in the final output. The purpose
of the inference stage is to incorporate such information, including both linguistic
and structural knowledge, such as ?arguments do not overlap? or ?each verb takes
at most one argument of each type.? This knowledge is useful to resolve any incon-
sistencies of argument classification in order to generate final legitimate predictions.
We design an inference procedure that is formalized as a constrained optimization
problem, represented as an integer linear program (Roth and Yih 2004). It takes as
input the argument classifiers? confidence scores for each type of argument, along
with a list of constraints. The output is the optimal solution that maximizes the lin-
ear sum of the confidence scores, subject to the constraints that encode the domain
knowledge.
The inference stage can be naturally extended to combine the output of several
different SRL systems, as we will show in Section 5. In this section we first introduce
the constraints and formalize the inference problem for the semantic role labeling task.
We then demonstrate how we apply integer linear programming (ILP) to generate the
global label assignment.
3.4.1 Constraints over Argument Labeling. Formally, the argument classifiers attempt to
assign labels to a set of arguments, S1:M, indexed from 1 toM. Each argument Si can take
any label from a set of argument labels, P , and the indexed set of arguments can take a
set of labels, c1:M ? PM. If we assume that the classifiers return a score score(Si = ci) that
corresponds to the likelihood of argument Si being labeled ci then, given a sentence, the
unaltered inference task is solved by maximizing the overall score of the arguments,
c?1:M = argmax
c1:M?PM
score(S1:M = c1:M) = argmax
c1:M?PM
M
?
i=1
score(Si = ci) (1)
In the presence of global constraints derived from linguistic information and struc-
tural considerations, our system seeks to output a legitimate labeling that maximizes this
score. Specifically, it can be thought of as if the solution space is limited through the use
of a filter function, F , which eliminates many argument labelings from consideration.
266
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Here, we are concerned with global constraints as well as constraints on the arguments.
Therefore, the final labeling becomes
c?1:M = argmax
c1:M?F (PM )
M
?
i=1
score(Si = ci) (2)
When the confidence scores correspond to the conditional probabilities estimated by
the argument classifiers, the value of the objective function represents the expected
number of correct argument predictions. Hence, the solution of Equation (2) is the one
that maximizes this expected value among all legitimate outputs.
The filter function used considers the following constraints:1
1. Arguments cannot overlap with the predicate.
2. Arguments cannot exclusively overlap with the clauses.
3. If a predicate is outside a clause, its arguments cannot be embedded in
that clause.
4. No overlapping or embedding arguments.
This constraint holds because semantic arguments are labeled on
non-embedding constituents in the syntactic parse tree. In addition, as
defined in the CoNLL-2004 and 2005 shared tasks, the legitimate output of
an SRL system must satisfy this constraint.
5. No duplicate argument classes for core arguments, such as A0?A5 and AA.
The only exception is when there is a conjunction in the sentence. For
example,
[A0 I] [V left ] [A1 my pearls] [A2 to my daughter] and [A1 my gold] [A2 to
my son].
Despite this exception, we treat it as a hard constraint because it almost
always holds.
6. If there is an R-arg argument, then there has to be an arg argument. That is,
if an argument is a reference to some other argument arg, then this
referenced argument must exist in the sentence. This constraint is directly
derived from the definition of R-arg arguments.
7. If there is a C-arg argument, then there has to be an arg argument; in
addition, the C-arg argument must occur after arg. This is stricter than
the previous rule because the order of appearance also needs to be
considered. Similarly, this constraint is directly derived from the definition
of C-arg arguments.
8. Given the predicate, some argument classes are illegal (e.g., predicate
stalk can take only A0 or A1). This information can be found in
PropBank Frames.
1 There are other constraints such as ?exactly one V argument per class,? or ?V?A1?C-V pattern? as
introduced by Punyakanok et al (2004). However, we did not find them particularly helpful in our
experiments. Therefore, we exclude those constraints in the presentation here.
267
Computational Linguistics Volume 34, Number 2
This constraint comes from the fact that different predicates take
different types and numbers of arguments. By checking the
PropBank Frame file of the target verb, we can exclude some core
argument labels.
Note that constraints 1, 2, and 3 are actually implemented in the argument identifi-
cation stage (see Section 3.2). In addition, they need to be explicitly enforced only when
full parsing information is not available because the output of the pruning heuristics
never violates these constraints.
The optimization problem (Equation (2)) can be solved using an ILP solver by
reformulating the constraints as linear (in)equalities over the indicator variables that
represent the truth value of statements of the form [argument i takes label j], as described
in detail next.
3.4.2 Using Integer Linear Programming. As discussed previously, a collection of po-
tential arguments is not necessarily a valid semantic labeling because it may not
satisfy all of the constraints. We enforce a legitimate solution using the following
inference algorithm. In our context, inference is the process of finding the best (ac-
cording to Equation (1)) valid semantic labels that satisfy all of the specified con-
straints. We take a similar approach to the one previously used for entity/relation
recognition (Roth and Yih 2004), and model this inference procedure as solving an ILP
problem.
An integer linear program is a linear program with integral variables. That is,
the cost function and the (in)equality constraints are all linear in terms of the variables.
The only difference in an integer linear program is that the variables can only take
integers as their values. In our inference problem, the variables are in fact binary. A
general binary integer linear programming problem can be stated as follows.
Given a cost vector p ? 
d, a collection of variables u = (u1, . . . ,ud) and cost ma-
trices C1 ? 
c1 ?
d,C2 ? 
c2 ?
d , where c1 and c2 are the numbers of inequality and
equality constraints and d is the number of binary variables, the ILP solution u? is the
vector that maximizes the cost function,
u? = argmax
u?{0,1}d
p ? u
subject to
C1u ? b1, and C2u = b2
where b1 ? 
c1 ,b2 ? 
c2 , and for all u ? {0, 1}d.
To solve the problem of Equation (2) in this setting, we first reformulate the
original cost function
?M
i=1 score(S
i = ci) as a linear function over several binary vari-
ables, and then represent the filter function F using linear inequalities and equalities.
We set up a bijection from the semantic labeling to the variable set u. This is done
by setting u to be a set of indicator variables that correspond to the labels assigned to ar-
guments. Specifically, let uic = [S
i = c] be the indicator variable that represents whether
268
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
or not the argument type c is assigned to Si, and let pic = score(S
i = c). Equation (1) can
then be written as an ILP cost function as
argmax
uic?{0,1}:?i?[1,M],c?P
M
?
i=1
?
c?P
picuic
subject to
?
c?P
uic = 1 ?i ? [1,M]
which means that each argument can take only one type. Note that this new constraint
comes from the variable transformation, and is not one of the constraints used in the
filter function F .
Of the constraints listed earlier, constraints 1 through 3 can be evaluated on a per-
argument basis and, for the sake of efficiency, arguments that violate these constraints
are eliminated even before being given to the argument classifier. Next, we show how to
transform the constraints in the filter function into the form of linear (in)equalities over
u and use them in this ILP setting. For a more complete example of this ILP formulation,
please see Appendix A.
Constraint 4: No overlapping or embedding. If arguments Sj1 , . . . ,Sjk cover the same word
in a sentence, then this constraint ensures that at most one of the arguments is assigned
to an argument type. In other words, at least k? 1 arguments will be the special class
null. If the special class null is represented by the symbol ?, then for every set of such
arguments, the following linear equality represents this constraint.
k
?
i=1
uji? ? k? 1
Constraint 5: No duplicate argument classes. Within the same clause, several types of
arguments cannot appear more than once. For example, a predicate can only take one
A0. This constraint can be represented using the following inequality.
M
?
i=1
uiA0 ? 1
Constraint 6: R-arg arguments. Suppose the referenced argument type is A0 and the
referential type is R-A0. The linear inequalities that represent this constraint are:
?m ? {1, . . . ,M} :
M
?
i=1
uiA0 ? umR-A0
If there are ? referential types, then the total number of inequalities needed is ?M.
Constraint 7: C-arg arguments. This constraint is similar to the reference argument con-
straints. The difference is that the continued argument arg has to occur before C-arg.
269
Computational Linguistics Volume 34, Number 2
Assume that the argument pair is A0 and C-A0, and arguments are sorted by their
beginning positions, i.e., if i < k, the position of the beginning of Sjk is not before that of
the beginning of Sji . The linear inequalities that represent this constraint are:
?m ? {2, . . . ,M} :
m?1
?
i=1
ujiA0 ? ujmC-A0
Constraint 8: Illegal argument types. Given a specific verb, some argument types should
never occur. For example, most verbs do not have arguments A5. This constraint is
represented by summing all the corresponding indicator variables to be 0.
M
?
i=1
uiA5 = 0
Using ILP to solve this inference problem enjoys several advantages. Linear con-
straints are very general, and are able to represent any Boolean constraint (Gue?ret, Prins,
and Sevaux 2002). Table 1 summarizes the transformations of common constraints (most
are Boolean), which are revised from Gue?ret, Prins, and Sevaux (2002), and can be used
for constructing complicated rules.
Previous approaches usually rely on dynamic programming to resolve non-
overlapping/embedding constraints (i.e., Constraint 4) when the constraint structure
is sequential. However, they are not able to handle more expressive constraints
such as those that take long-distance dependencies and counting dependencies into
account (Roth and Yih 2005). The ILP approach, on the other hand, is flexible enough
to handle more expressive and general constraints. Although solving an ILP problem is
NP-hard in the worst case, with the help of today?s numerical packages, this problem
can usually be solved very quickly in practice. For instance, in our experiments it
only took about 10 minutes to solve the inference problem for 4,305 sentences, using
Table 1
Rules of mapping constraints to linear (in)equalities for Boolean variables.
Original constraint Linear form
exactly k of x1, x2, ? ? ? , xn x1 + x2 + ? ? ?+ xn = k
at most k of x1, x2, ? ? ? , xn x1 + x2 + ? ? ?+ xn ? k
at least k of x1, x2, ? ? ? , xn x1 + x2 + ? ? ?+ xn ? k
a ? b a ? b
a = b? a = 1? b
a ? b? a+ b ? 1
a? ? b a+ b ? 1
a ? b a = b
a ? b ? c a ? b and a ? c
a ? b ? c a ? b+ c
b ? c ? a a ? b+ c? 1
b ? c ? a a ? (b+ c)/2
a? at least k of x1, x2, ? ? ? , xn a ? (x1 + x2 + ? ? ?+ xn)/k
At least k of x1, x2, ? ? ? , xn ? a a ? (x1 + x2 + ? ? ?+ xn ? (k? 1))/(n? (k? 1))
270
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Xpress-MP (2004) running on a Pentium-III 800 MHz machine. Note that ordinary
search methods (e.g., beam search) are not necessarily faster than solving an ILP
problem and do not guarantee the optimal solution.
4. The Importance of Syntactic Parsing
We experimentally study the significance of syntactic parsing by observing the effects
of using full parsing and shallow parsing information at each stage of an SRL system.
We first describe, in Section 4.1, how we prepare the data. The comparison of full
parsing and shallow parsing on the first three stages of the process is presented in the
reverse order (Sections 4.2, 4.3, 4.4). Note that in the following sections, in addition
to the performance comparison at various stages, we present also the overall system
performance for the different scenarios. In all cases, the overall system performance is
derived after the inference stage.
4.1 Experimental Setting
We use PropBank Sections 02 through 21 as training data, Section 23 as testing, and
Section 24 as a validation set when necessary. In order to apply the standard CoNLL
shared task evaluation script, our system conforms to both the input and output format
defined in the shared task.
The goal of the experiments in this section is to understand the effective contribu-
tion of full parsing information versus shallow parsing information (i.e., using only the
part-of-speech tags, chunks, and clauses). In addition, we also compare performance
when using the correct (gold-standard) data versus using automatic parse data. The
performance is measured in terms of precision, recall, and the F1 measure. Note that
all the numbers reported here do not take into account the V arguments as it is quite
trivial to predict V and, hence, this gives overoptimistic overall performance if included.
When doing the comparison, we also compute the 95% confidence interval of F1 us-
ing the bootstrap resampling method (Noreen 1989), and the difference is considered
significant if the compared F1 lies outside this interval. The automatic full parse trees
are derived using Charniak?s parser (2001) (version 0.4). In automatic shallow parsing,
the information is generated by different state-of-the-art components, including a POS
tagger (Even-Zohar and Roth 2001), a chunker (Punyakanok and Roth 2001), and a
clauser (Carreras, Ma`rquez, and Castro 2005).
4.2 Argument Classification
To evaluate the performance gap between full parsing and shallow parsing in argument
classification, we assume the argument boundaries are known, and only train classifiers
to classify the labels of these arguments. In this stage, the only difference between the
uses of full parsing and shallow parsing information is the construction of phrase type,
head word, POS tag of the head word, path, subcategorization, and syntactic frame features.
As described in Section 3.2.2, most of these features can be approximated using chunks
and clauses, with the exception of the syntactic frame feature. It is unclear how this
feature can be mimicked because it relies on the internal structure of a full parse tree.
Therefore, it does not have a corresponding feature in the shallow parsing case.
Table 2 reports the experimental results of argument classification when argument
boundaries are known. In this case, because the argument classifier of our SRL system
does not overpredict or miss any arguments, we do not need to train with a null class,
271
Computational Linguistics Volume 34, Number 2
Table 2
The accuracy of argument classification when argument boundaries are known.
Full Parsing Shallow Parsing
Gold 91.50 ? 0.48 90.75 ? 0.45
Auto 90.32 ? 0.48 89.71 ? 0.50
and we can simply measure the performance using accuracy instead of F1. The training
examples include 90,352 propositions with a total of 332,381 arguments. The test data
contain 5,246 propositions and 19,511 arguments. As shown in the table, although the
full-parsing features are more helpful than the shallow-parsing features, the perfor-
mance gap is quite small (0.75% on gold-standard data and 0.61% with the automatic
parsers).
The rather small difference in the performance between argument classifiers using
full parsing and shallow parsing information almost disappears when their output is
processed by the inference stage. Table 3 shows the final results in recall, precision, and
F1, when the argument boundaries are known. In all cases, the differences in F1 between
the full parsing?based and the shallow parsing?based systems are not statistically
significant.
Conclusion. When the argument boundaries are known, the performance of the full
parsing?based SRL system is about the same as the shallow parsing?based SRL system.
4.3 Argument Identification
Argument identification is an important stage that effectively reduces the number of
argument candidates after the pruning stage. Given an argument candidate, an argu-
ment identifier is a binary classifier that decides whether or not the candidate should be
considered as an argument. To evaluate the influence of full parsing information in this
stage, the candidate list used here is the outputs of the pruning heuristic applied on the
gold-standard parse trees. The heuristic results in a total number of 323,155 positive and
686,887 negative examples in the training set, and 18,988 positive and 39,585 negative
examples in the test set.
Similar to the argument classification stage, the only difference between full
parsing? and shallow parsing?based systems is in the construction of some features.
Specifically, phrase type, head word, POS tag of the head word, path, and subcategorization
features are approximated using chunks and clauses when the binary classifier is trained
using shallow parsing information.
Table 4 reports the performance of the argument identifier on the test set using
the direct predictions of the trained binary classifier. The recall and precision of the
Table 3
The overall system performance when argument boundaries are known.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 91.58 91.90 91.74 ? 0.51 91.14 91.48 91.31 ? 0.51
Auto 90.71 91.14 90.93 ? 0.53 90.50 90.88 90.69 ? 0.53
272
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 4
The performance of argument identification after pruning (based on the gold standard full parse
trees).
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 96.53 93.57 95.03 ? 0.32 93.66 91.72 92.68 ? 0.38
Auto 94.68 90.60 92.59 ? 0.39 92.31 88.36 90.29 ? 0.43
full parsing?based system are around 2 to 3 percentage points higher than the shallow
parsing?based system on the gold-standard data. As a result, the F1 score is 2.5 percent-
age points higher. The performance on automatic parse data is unsurprisingly lower
but the difference between the full parsing? and the shallow parsing?based systems is
as observed previously. In terms of filtering efficiency, around 25% of the examples are
predicted as positive. In other words, both argument identifiers filter out around 75%
of the argument candidates after pruning.
Because the recall in the argument identification stage sets the upper-bound the
recall in argument classification, the threshold that determines when examples are
predicted to be positive is usually lowered to allow more positive predictions. That
is, a candidate is predicted as positive when its probability estimation is larger than
the threshold. Table 5 shows the performance of the argument identifiers when the
threshold is 0.1.2
Because argument identification is just an intermediate step in a complete system,
a more realistic evaluation method is to see how each final system performs. Using an
argument identifier with threshold = 0.1 (i.e., Table 5), Table 6 reports the final results
in recall, precision, and F1. The F1 difference is 1.5 points when using the gold-standard
data. However, when automatic parsers are used, the shallow parsing?based system is,
in fact, slightly better; although the difference is not statistically significant. This may be
due to the fact that chunk and clause predictions are very important here, and shallow
parsers are more accurate in chunk or clause predictions than a full parser (Li and Roth
2001).
Conclusion. Full parsing information helps in argument identification. However, when
the automatic parsers are used, using the full parsing information may not have better
overall results compared to using shallow parsing.
4.4 Pruning
As shown in the previous two sections, the overall performance gaps of full parsing and
shallow parsing are small. When automatic parsers are used, the difference is less than 1
point in F1 or accuracy. Therefore, we conclude that themain contribution of full parsing
is in the pruning stage. Because the shallow parsing system does not have enough in-
formation for the pruning heuristics, we train two word-based classifiers to replace the
pruning stage. One classifier is trained to predict whether a given word is the start (S) of
2 The value was determined by experimenting with the complete system using automatic full parse trees,
on the development set. In our tests, lowering the threshold in argument identification always leads to
higher overall recall and lower overall precision. As a result, the gain in F1 is limited.
273
Computational Linguistics Volume 34, Number 2
Table 5
The performance of argument identification after pruning (based on the gold-standard full parse
trees) and with threshold = 0.1.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 92.13 95.62 93.84 ? 0.37 88.54 94.81 91.57 ? 0.42
Auto 89.48 94.14 91.75 ? 0.41 86.14 93.21 89.54 ? 0.47
Table 6
The overall system performance using the output from the pruning heuristics, applied on the
gold-standard full parse trees.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 86.22 87.40 86.81 ? 0.59 84.14 85.31 84.72 ? 0.63
Auto 84.21 85.04 84.63 ? 0.63 86.17 84.02 85.08 ? 0.63
an argument; the other classifier is to predict the end (E) of an argument. If the product
of probabilities of a pair of S and E predictions is larger than a predefined threshold,
then this pair is considered as an argument candidate. The threshold used here was
obtained by using the validation set. Both classifiers use very similar features to those
used by the argument identifier as explained in Section 3.2, treating the target word as
a constituent. Particularly, the features are predicate, POS tag of the predicate, voice,
context words, POS tags of the context words, chunk pattern, clause relative position,
and shallow-path. The headword and its POS tag are replaced by the target word and its
POS tag. The comparison of using the classifiers and the heuristics is shown in Table 7.
Even without the knowledge of the constituent boundaries, the classifiers seem
surprisingly better than the pruning heuristics. Using either the gold-standard data set
or the output of automatic parsers, the classifiers achieve higher F1 scores. One possible
reason for this phenomenon is that the accuracy of the pruning strategy is limited by
the number of agreements between the correct arguments and the constituents of the
parse trees. Table 8 summarizes the statistics of the examples seen by both strategies.
The pruning strategy needs to decide which are the potential arguments among all con-
stituents. This strategy is upper-bounded by the number of correct arguments that agree
with some constituent. On the other hand, the classifiers do not have this limitation. The
number of examples they observe is the total number of words to be processed, and the
positive examples are those arguments that are annotated as such in the data set.
Table 7
The performance of pruning using heuristics and classifiers.
Full Parsing Classifier Threshold = 0.04
Prec Rec F1 Prec Rec F1
Gold 25.94 97.27 40.96 ? 0.51 29.58 97.18 45.35 ? 0.83
Auto 22.79 86.08 36.04 ? 0.52 24.68 94.80 39.17 ? 0.79
274
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 8
Statistics of the training and test examples for the pruning stage.
Words Arguments Constituents Agreements
Gold Auto Gold Auto
Train 2,575,665 332,381 4,664,954 4,263,831 327,603 319,768
Test 147,981 19,511 268,678 268,482 19,266 18,301
The Agreements column shows the number of arguments that match the boundaries of some
constituents.
Note that because each verb is processed independently, a sentence is processed
once for each verb in the sentence. Therefore, the words and constituents in each
sentence are counted as many times as the number of verbs to be processed.
As before, in order to compare the systems that use full parsing and shallow
parsing information, we need to see the impact on the overall performance. There-
fore, we built two semantic role systems based on full parsing and shallow parsing
information. The full parsing?based system follows the pruning, argument identifica-
tion, argument classification, and inference stages, as described earlier. For the shallow
parsing system, the pruning heuristic is replaced by the word-based pruning classi-
fiers, and the remaining stages are designed to use only shallow parsing as described in
previous sections. Table 9 shows the overall performance of the two evaluation systems.
As indicated in the tables, the gap in F1 between full parsing and shallow parsing?
based systems enlarges tomore than 11 points on the gold-standard data. At first glance,
this result seems to contradict our conclusion in Section 4.3. After all, if the pruning
stage of shallow parsing SRL system performs equally well or even better, the overall
performance gap in F1 should be small.
After we carefully examined the output of the word-based classifier, we realized
that it filters out easy candidates, and leaves examples that are difficult to the later
stages. Specifically, these argument candidates often overlap and differ only in one or
twowords. On the other hand, the pruning heuristic based on full parsing never outputs
overlapping candidates and consequently provides input that is easier for the next stage
to handle. Indeed, the following argument identification stage turns out to be good in
discriminating these non-overlapping candidates.
Conclusion. The most crucial contribution of full parsing is in the pruning stage. The
internal tree structure significantly helps in discriminating argument candidates, which
makes the work done by the following stages easier.
Table 9
The overall system performance.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 86.22 87.40 86.81 ? 0.59 75.34 75.28 75.31 ? 0.76
Auto 77.09 75.51 76.29 ? 0.76 75.48 67.13 71.06 ? 0.80
275
Computational Linguistics Volume 34, Number 2
5. The Effect of Inference
Our inference procedure plays an important role in improving accuracy when the local
predictions violate the constraints among argument labels. In this section, we first
present the overall system performance when most constraints are not used. We then
demonstrate how the inference procedure can be used to combine the output of several
systems to yield better performance.
5.1 Inference with Limited Constraints
The inference stage in our system architecture provides a principled way to resolve
conflicting local predictions. It is interesting to see whether this procedure improves the
performance differently for the full parsing? vs. the shallow parsing?based system, as
well as gold-standard vs. automatic parsing input.
Table 10 shows the results of using only constraints 1, 2, 3, and 4. As mentioned
previously, the first three constraints are handled before the argument classification
stage. Constraint 4, which forbids overlapping or embedding arguments, is required
in order to use the official CoNLL-2005 evaluation script and is therefore kept.
By comparing Table 9 with Table 10, we can see that the effect of adding more
constraints is quite consistent over the four settings. Precision is improved by 1 to 2 per-
centage points but recall is decreased a little. As a result, the gain in F1 is about 0.5 to 1
point. It is not surprising to see this lower recall and higher precision phenomenon after
the constraints described in Section 3.4.1 are examined. Most constraints punish false
non-null output, but do not regulate false null predictions. For example, an assignment
that has two A1 arguments clearly violates the non-duplication constraint. However, if
an assignment has no predicted arguments at all, it still satisfies all the constraints.
5.2 Joint Inference
The empirical study in Section 4 indicates that the performance of an SRL system
primarily depends on the very first stage?pruning, which is directly derived from
the full parse trees. This also means that in practice the quality of the syntactic parser
is decisive to the quality of the SRL system. To improve semantic role labeling, one
possible way is to combine different SRL systems through a joint inference stage, given
that the systems are derived using different full parse trees.
To test this idea, we first build two SRL systems that use Collins?s parser (Collins
1999)3 and Charniak?s parser (Charniak 2001), respectively. In fact, these two parsers
have noticeably different outputs. Applying the pruning heuristics on the output of
Collins?s parser produces a list of candidates with 81.05% recall. Although this number
is significantly lower than the 86.08% recall produced by Charniak?s parser, the union
of the two candidate lists still significantly improves recall to 91.37%. We construct the
two systems by implementing the first three stages, namely, pruning, argument identifi-
cation, and argument classification. When a test sentence is given, a joint inference stage
is used to resolve the inconsistency of the output of argument classification in these two
systems.
We first briefly review the objective function used in the inference procedure in-
troduced in Section 3.4. Formally speaking, the argument classifiers attempt to assign
3 We use the Collins parser implemented by Bikel (2004).
276
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 10
The impact of removing most constraints in overall system performance.
Full Parsing Shallow Parsing
Prec Rec F1 Prec Rec F1
Gold 85.07 87.50 86.27 ? 0.58 73.19 75.63 74.39 ? 0.75
Auto 75.88 75.81 75.84 ? 0.75 73.56 67.45 70.37 ? 0.80
labels to a set of arguments, S1:M, indexed from 1 toM. Each argument Si can take any
label from a set of argument labels, P , and the indexed set of arguments can take a
set of labels, c1:M ? PM. If we assume that the argument classifier returns an estimated
conditional probability distribution, Prob(Si = ci), then, given a sentence, the inference
procedure seeks a global assignment that maximizes the objective function denoted by
Equation (2), which can be rewritten as follows,
c?1:M = argmax
c1:M?F (PM )
M
?
i=1
Prob(Si = ci) (3)
where the linguistic and structural constraints are represented by the filter F . In other
words, this objective function reflects the expected number of correct argument predic-
tions, subject to the constraints.
When there are two or more argument classifiers from different SRL systems, a joint
inference procedure can take the output estimated probabilities for all these candidates
as input, although some candidates may refer to the same phrases in the sentence. For
example, Figure 3 shows the two candidate sets for a fragment of a sentence, ..., traders
say, unable to cool the selling panic in both stocks and futures. In this example, system A has
two argument candidates, a1 = traders and a4 = the selling panic in both stocks and futures;
system B has three argument candidates, b1 = traders, b2 = the selling panic, and b3 = in
both stocks and futures.
A straightforward solution to the combination is to treat each argument produced
by a system as a possible output. Each possible labeling of the argument is associated
with a variable which is then used to set up the inference procedure. However, the final
predictionwill be likely dominated by the system that producesmore candidates, which
is system B in this example. The reason is that our objective function is the sum of the
probabilities of all the candidate assignments.
This bias can be corrected by the following observation. Although system A only
has two candidates, a1 and a4, it can be treated as if it also has two additional phantom
candidates, a2 and a3, where a2 and b2 refer to the same phrase, and so do a3 and b3.
Similarly, system B has a phantom candidate b4 that corresponds to a4. Because systemA
does not really generate a2 and a3, we can assume that these two phantom candidates are
predicted by it as ?null? (i.e., not an argument). We assign the same prior distribution to
each phantom candidate. In particular, the probability of the ?null? class is set to be 0.55
based on empirical tests, and the probabilities of the remaining classes are set based on
their occurrence frequencies in the training data.
Then, we treat each possible final argument output as a single unit. Each probability
estimation by a system can be viewed as evidence in the final probability estimation and,
therefore, we can simply average their estimation. Formally, let Si be the argument set
277
Computational Linguistics Volume 34, Number 2
Figure 3
The output of two SRL systems: system A has two candidates, a1 = traders and a4 = the selling
panic in both stocks and futures; system B has three argument candidates, b1 = traders, b2 = the
selling panic, and b3 = in both stocks and futures. In addition, we create two phantom candidates a2
and a3 for system A that correspond to b2 and b3 respectively, and b4 for system B that
corresponds to a4.
output by system i, and S =
?k
i=1 Si be the set of all arguments where k is the number
of systems; let N be the cardinality of S. Our augmented objective function is then:
c?1:N = argmax
c1:N?F (PN )
N
?
i=1
Prob(Si = ci) (4)
where Si ? S, and
Prob(Si = ci) = 1
k
k
?
j=1
Probj(S
i = ci) (5)
where Probj is the probability output by system j.
Note that we may also treat the individual systems differently by applying different
priors (i.e., weights) on the estimated probabilities of the argument candidates. For
example, if the performance of system A is much better than system B, then we may
want to trust system A?s output more by multiplying the output probabilities by a
larger weight.
Table 11 reports the performance of two individual systems based on Collins?s
parser and Charniak?s parser, as well as the joint system, where the two individual
systems are equally weighted. The joint system based on this straightforward strategy
significantly improves the performance compared to the two original SRL systems in
both recall and precision, and thus achieves a much higher F1.
6. Empirical Evaluation?CoNLL Shared Task 2005
In this section, we present the detailed evaluation of our SRL system, in the competi-
tion on semantic role labeling?the CoNLL-2005 shared task (Carreras and Ma`rquez
278
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Table 11
The performance of individual and combined SRL systems.
Prec Rec F1
Collins? parser 75.92 71.45 73.62 ? 0.79
Charniak?s parser 77.09 75.51 76.29 ? 0.76
Combined result 80.53 76.94 78.69 ? 0.71
2005). The setting of this shared task is basically the same as it was in 2004, with
some extensions. First, it allows much richer syntactic information. In particular, full
parse trees generated using Collins?s parser (Collins 1999) and Charniak?s parser
(Charniak 2001) were provided. Second, the full parsing standard partition was used?
the training set was enlarged and covered Sections 02?21, the development set was
Section 24, and the test set was Section 23. Finally, in addition to the Wall Street Journal
(WSJ) data, three sections of the Brown corpus were used to provide cross-corpora
evaluation.
The system we used to participate in the CoNLL-2005 shared task is an enhanced
version of the system described in Sections 3 and 5. The main difference was that
the joint-inference stage was extended to combine six basic SRL systems instead of
two. Specifically for this implementation, we first trained two SRL systems that use
Collins?s parser and Charniak?s parser, respectively, because of their noticeably dif-
ferent outputs. In evaluation, we ran the system that was trained with Charniak?s
parser five times, with the top-5 parse trees output by Charniak?s parser. Together we
have six different outputs per predicate. For each parse tree output, we ran the first
three stages, namely, pruning, argument identification, and argument classification.
Then, a joint-inference stage, where each individual system is weighted equally, was
used to resolve the inconsistency of the output of argument classification in these
systems.
Table 12 shows the overall results on the development set and different test sets; the
detailed results on WSJ section 23 are shown in Table 13. Table 14 shows the results of
individual systems and the improvement gained by the joint inference procedure on the
development set.
Our system reached the highest F1 scores on all the test sets and was the best system
among the 19 participating teams. After the competition, we improved the system
slightly by tuning the weights of the individual systems in the joint inference procedure,
where the F1 scores onWSJ test section and the Brown test set are 79.59 points and 67.98
points, respectively.
Table 12
Overall CoNLL-2005 shared task results.
Prec. Rec. F1
Development 80.05 74.83 77.35
Test WSJ 82.28 76.78 79.44
Test Brown 73.38 62.93 67.75
Test WSJ+Brown 81.18 74.92 77.92
279
Computational Linguistics Volume 34, Number 2
Table 13
Detailed CoNLL-2005 shared task results on the WSJ test set.
Test WSJ Prec. Rec. F1
Overall 82.28 76.78 79.44
A0 88.22 87.88 88.05
A1 82.25 77.69 79.91
A2 78.27 60.36 68.16
A3 82.73 52.60 64.31
A4 83.91 71.57 77.25
AM-ADV 63.82 56.13 59.73
AM-CAU 64.15 46.58 53.97
AM-DIR 57.89 38.82 46.48
AM-DIS 75.44 80.62 77.95
AM-EXT 68.18 46.88 55.56
AM-LOC 66.67 55.10 60.33
AM-MNR 66.79 53.20 59.22
AM-MOD 96.11 98.73 97.40
AM-NEG 97.40 97.83 97.61
AM-PNC 60.00 36.52 45.41
AM-TMP 78.16 76.72 77.44
R-A0 89.72 85.71 87.67
R-A1 70.00 76.28 73.01
R-A2 85.71 37.50 52.17
R-AM-LOC 85.71 57.14 68.57
R-AM-TMP 72.34 65.38 68.69
In terms of the computation time, for both the argument identifier and the argument
classifier, the training of each model, excluding feature extraction, takes 50?70 minutes
using less than 1GB memory on a 2.6GHz AMD machine. On the same machine, the
average test time for each stage, excluding feature extraction, is around 2 minutes.
7. Related Work
The pioneering work on building an automatic semantic role labeler was proposed
by Gildea and Jurafsky (2002). In their setting, semantic role labeling was treated as a
tagging problem on each constituent in a parse tree, solved by a two-stage architecture
consisting of an argument identifier and an argument classifier. This is similar to our
Table 14
The results of individual systems and the result with joint inference on the development set.
Prec. Rec. F1
Charniak-1 75.40 74.13 74.76
Charniak-2 74.21 73.06 73.63
Charniak-3 73.52 72.31 72.91
Charniak-4 74.29 72.92 73.60
Charniak-5 72.57 71.40 71.98
Collins 73.89 70.11 71.95
Joint inference 80.05 74.83 77.35
280
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
main architecture with the exclusion of the pruning and inference stages. There are
two additional key differences between their system and ours. First, their system
used a back-off probabilistic model as its main engine. Second, it was trained on
FrameNet (Baker, Fillmore, and Lowe 1998)?another large corpus, besides PropBank,
that contains selected examples of semantically labeled sentences.
Later that year, the same approach was applied on PropBank by Gildea and Palmer
(2002). Their system achieved 57.7% precision and 50.0% recall with automatic parse
trees, and 71.1% precision and 64.4% recall with gold-standard parse trees. It is worth
noticing that at that time the PropBank project was not finished and the data set
available was only a fraction in size of what it is today. Since these pioneering works, the
task has gained increasing popularity and created a new line of research. The two-step
constituent-by-constituent architecture became a common blueprint for many systems
that followed.
Partly due to the expansion of the PropBank dataset, researchers have gradually
made improvement on the performance of automatic SRL systems by using new tech-
niques and new features. Some of the early systems are described in Chen and Rambow
(2003), Gildea and Hockenmaier (2003), and Surdeanu et al (2003). All are based on a
two-stage architecture similar to the one proposed by Gildea and Palmer (2002) with
the differences in the machine-learning techniques and the features used. The first
breakthrough in terms of performance was due to Pradhan et al (2003), who first
viewed the task as a massive classification problem and applied multiple SVMs to it.
Their final result (after a few more improvements) reported in Pradhan et al (2004)
achieved 84% and 75% in precision and recall, respectively.
A second significant contribution beyond the two-stage architecture is due to Xue
and Palmer (2004), who introduced the pruning heuristics to the two-stage architecture,
and remarkably reduced the number of candidate arguments a system needs to con-
sider; this approach was adopted by many systems. Another significant advancement
was in the realization that global information can be exploited and benefits the results
significantly. Inference based on an integer linear programming technique, which was
originally introduced by Roth and Yih (2004) on a relation extraction problem, was
first applied to the SRL problem by Punyakanok et al (2004). It showed that domain
knowledge can be easily encoded and contributes significantly through inference over
the output of classifiers. The idea of exploiting global information, which is detailed in
this paper, was pursued later by other researchers, in different forms.
Besides the constituent-by-constituent based architecture, others have also been
explored. The alternative frameworks include representing semantic role labeling as
a sequence-tagging problem (Ma`rquez, Pere Comas, and Catala` 2005) and tagging the
edges in the corresponding dependency trees (Hacioglu 2004). However, the most pop-
ular architecture by far is the constituent-by-constituent based multi-stage architecture,
perhaps due to its conceptual simplicity and its success. In the CoNLL-2005 shared
task competition (Carreras and Ma`rquez 2005), the majority of the systems followed
the constituent-by-constituent based two-stage architecture, and the use of the pruning
heuristics was also fairly common.
The CoNLL-2005 shared task also highlighted the importance of system combina-
tion, such as our ILP technique when used in joint inference, in order to achieve superior
performance. The top four systems, which produced significantly better results than the
rest, all used some schemes to combine the output of several SRL systems, ranging from
using a fixed combination function (Haghighi, Toutanova, and Manning 2005; Koomen
et al 2005) to using a machine-learned combination strategy (Ma`rquez, Pere Comas,
and Catala` 2005; Pradhan, Hacioglu, Ward et al 2005).
281
Computational Linguistics Volume 34, Number 2
The work of Gildea and Palmer (2002) pioneered not only the fundamental archi-
tecture of SRL, but was also the first to investigate the interesting question regarding
the significance of using full parsing for high quality SRL. They compared their full
system with another system that only used chunking, and found that the chunk-based
system performed much worse. The precision and recall dropped from 57.7% and
50.0% to 27.6% and 22.0%, respectively. That led to the conclusion that full parsing
information is necessary to solving the SRL problem, especially at the stage of argu-
ment identification?a finding that is quite similar to ours in this article. However,
their chunk-based approach was very weak?only chunks were considered as possible
candidates; hence, it is not very surprising that the boundaries of the arguments could
not be reliably found. In contrast, our shallow parse?based system does not have these
restrictions on the argument boundaries and therefore performs much better at this
stage, providing a more fair comparison.
A related comparison can be found also in the work by Pradhan, Hacioglu, Krugler
et al (2005) (their earlier version appeared in Pradhan et al [2003]), which reported
the performance on several systems using different information sources and system
architectures. Their shallow parse?based system is modeled as a sequence tagging prob-
lem while the full system is a constituent-by-constituent based two-stage system. Due
to technical difficulties, though, they reported the results of the chunk-based systems
only on a subset of the full data set. Their shallow parse?based system achieved 60.4%
precision and 51.4% recall and their full system achieved 80.6% precision and 67.1%
recall on the same data set (but 84% precision and 75% recall with the full data set).
Therefore, due to the use of different architectures and data set sizes, the questions
of ?how much one can gain from full parsing over shallow parsing when using the
full PropBank data set? and ?what are the sources of the performance gain? were left
open.
Similarly, in the CoNLL-2004 shared task (Carreras andMa`rquez 2004), participants
were asked to develop SRL systems with the restriction that only shallow parsing infor-
mation (i.e., chunks and clauses) were allowed. The performance of the best systemwas
at 72.43% precision and 66.77% recall, which was about 10 points in F1 lower than the
best system based on full parsing in the literature. However, the training examples were
derived from only 5 sections and not all the 19 sections usually used in the standard
setting. Hence, the question was not yet fully answered.
Our experimental study, on the other hand, is done with a consistent architecture,
by considering each stage in a controlled manner, and using the full data set, allowing
one to draw direct conclusions regarding the impact of this information source.
8. Conclusion
This paper studies the important task of semantic role labeling. We presented an ap-
proach to SRL and a principled and general approach to incorporating global informa-
tion in natural language decisions. Beyond presenting this approach which leads to a
state-of-the-art SRL system, we focused on investigating the significance of using full
parse tree information as input to an SRL system adhering to the most common system
architecture, and the stages in the process where this information has the most impact.
We performed a detailed and fair experimental comparison between shallow and full
parsing information and concluded that, indeed, full syntactic information can improve
the performance of an SRL system. In particular, we have shown that this information
is most crucial in the pruning stage of the system, and relatively less important in the
following stages.
282
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
In addition, we showed the importance of global inference to good performance in
this task, characterized by rich structural and linguistic constraints among the predicted
labels of the arguments. Our integer linear programming?based inference procedure
is a powerful and flexible optimization strategy that finds the best solution subject to
these constraints. As we have shown, it can be used to resolve conflicting argument
predictions in an individual system but can also serve as an effective and simple
approach to combining different SRL systems, resulting in a significant improvement
in performance.
In the future, we plan to extend our work in several directions. By adding more
constraints to the inference procedure, an SRL system may be further improved.
Currently, the constraints are provided by human experts in advance. Learning both
hard and statistical constraints from the data will be our top priority. Some work on
combining statistical and declarative constraints has already started and is reported
in Roth and Yih (2005). Another issue we want to address is domain adaptation.
It has been clearly shown in the CoNLL-2005 shared task that the performance of
current SRL systems degrades significantly when tested on a corpus different from
the one used in training. This may be due to the underlying components, especially
the syntactic parsers which are very sensitive to changes in data genre. Developing
a better model that more robustly combines these components could be a promising
direction. In addition, although the shallow parsing?based system was shown here to
be inferior, shallow parsers were shown to be more robust than full parsers (Li and
Roth 2001). Therefore, combining these two systems may bring forward both of their
advantages.
Appendix A: An ILP Formulation for SRL
In this section, we show a complete example of the ILP formulation formulated to solve
the inference problem as described in Section 3.4.
Example. Assume the sentence is four words long with the following argument
candidates, and the following illegal argument types for the predicate of interest.
Sentence: w1 w2 w3 w4
Candidates: [ S1 ] [ S2 ] [ S3 ] [ S5 ]
[ S4 ]
Illegal argument types: A3, A4, A5
Indicator Variables and Their Costs. The followings are the indicator variables and their
associated costs set up for the example.
Indicator Variables:
u1A0,u1A1, . . . ,u1AM-LOC, . . . ,u1C-A0, . . . ,u1R-A0, . . . ,u1?
u2A0,u2A1, . . . ,u2AM-LOC, . . . ,u2C-A0, . . . ,u2R-A0, . . . ,u2?
...
u5A0,u5A1, . . . ,u5AM-LOC, . . . ,u5C-A0, . . . ,u5R-A0, . . . ,u5?
Costs:
p1A0, p1A1, . . . , p1AM-LOC, . . . , p1C-A0, . . . , p1R-A0, . . . , p1?
p2A0, p2A1, . . . , p2AM-LOC, . . . , p2C-A0, . . . , p2R-A0, . . . , p2?
...
p5A0, p5A1, . . . , p5AM-LOC, . . . , p5C-A0, . . . , p5R-A0, . . . , p5?
283
Computational Linguistics Volume 34, Number 2
Objective Function. The objective function can be written as the following.
argmaxuic?{0,1}:?i?[1,5],c?P
?5
i=1
?
c?P picuic
where
P = {A0,A1, . . . , AM-LOC, . . . , C-A0, . . . , R-A0, . . . ,?}
subject to
u1A0 + u1A1 + . . .+ u1AM-LOC + . . .+ u1C-A0 + . . .+ u1R-A0 + . . .+ u1? = 1
u2A0 + u2A1 + . . .+ u2AM-LOC + . . .+ u2C-A0 + . . .+ u2R-A0 + . . .+ u2? = 1
...
u5A0 + u5A1 + . . .+ u5AM-LOC + . . .+ u2C-A0 + . . .+ u5R-A0 + . . .+ u5? = 1
Additional Constraints. The rest of the constraints can be formulated as the following.
Constraint 4: No overlapping or embedding
u3? + u4? ? 1
u4? + u5? ? 1
Constraint 5: No duplicate argument classes
u1A0 + u2A0 + . . .+ u5A0 ? 1
u1A1 + u2A1 + . . .+ u5A1 ? 1
u1A2 + u2A2 + . . .+ u5A2 ? 1
Constraint 6: R-arg arguments
u1A0 + u2A0 + . . .+ u5A0 ? u1R-A0
u1A0 + u2A0 + . . .+ u5A0 ? u2R-A0
...
u1A0 + u2A0 + . . .+ u5A0 ? u5R-A0
u1A1 + u2A1 + . . .+ u5A1 ? u1R-A1
...
u1AM-LOC + u2AM-LOC + . . .+ u5AM-LOC ? u1R-AM-LOC
...
Constraint 7: C-arg arguments
u1A0 ? u2C-A0
u1A0 + u2A0 ? u3C-A0
...
u1A0 + u2A0 + . . .+ u4A0 ? u5C-A0
u1A1 ? u2C-A1
...
u1AM-LOC ? u2C-AM-LOC
...
284
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Constraint 8: Illegal argument types
u1A3 + u2A3 + . . .+ u5A3 = 0
u1A4 + u2A4 + . . .+ u5A4 = 0
u1A5 + u2A5 + . . .+ u5A5 = 0
Acknowledgments
We thank Xavier Carreras and Llu??s Ma`rquez
for the data and scripts, Szu-ting Yi for her
help in improving our joint inference
procedure, and Nick Rizzolo as well as the
anonymous reviewers for their comments
and suggestions. We are also grateful to Dash
Optimization for the free academic use of
Xpress-MP and AMD for their equipment
donation. This research is supported by the
Advanced Research and Development
Activity (ARDA)?s Advanced Question
Answering for Intelligence (AQUAINT)
Program, a DOI grant under the Reflex
program, NSF grants ITR-IIS-0085836,
ITR-IIS-0085980, and IIS-9984168,
EIA-0224453, and an ONR MURI Award.
References
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley Framenet
project. In Proceedings of COLING-ACL,
pages 86?90, Montreal, Canada.
Bikel, Daniel M. 2004. Intricacies of Collins?
parsing model. Computational Linguistics,
30(4):479?511.
Bishop, Christopher M., 1995. Neural
Networks for Pattern Recognition, chapter
6.4: Modelling conditional distributions,
page 215. Oxford University Press,
Oxford, UK.
Carlson, Andrew J., Chad M. Cumby, Jeff L.
Rosen, and Dan Roth. 1999. The SNoW
learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer
Science Department.
Carreras, Xavier and Llu?is Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
tasks: Semantic role labeling. In Proceedings
of CoNLL-2004, pages 89?97, Boston, MA.
Carreras, Xavier and Llu?is Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 152?164, Ann Arbor, MI.
Carreras, Xavier, Llu?is Ma`rquez, and Jorge
Castro. 2005. Filtering?ranking perceptron
learning for partial parsing.Machine
Learning, 60:41?71.
Carreras, Xavier, Llu?is Ma`rquez, Vasin
Punyakanok, and Dan Roth. 2002.
Learning and inference for clause
identification. In Proceedings of the 13th
European Conference on Machine Learning
(ECML-2002), pages 35?47, Helsinki,
Finland.
Charniak, Eugene. 2001. Immediate-head
parsing for language models. In
Proceedings of the 39th Annual Meeting of the
Association of Computational Linguistics
(ACL-2001), pages 116?123, Toulouse,
France.
Chen, John and Owen Rambow. 2003. Use of
deep linguistic features for the recognition
and labeling of semantic arguments. In
Proceedings of the 2003 Conference on
Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 41?48,
Sapporo, Japan.
Collins, Michael. 1999. Head-driven
Statistical Models for Natural Language
Parsing. Ph.D. thesis, Computer Science
Department, University of Pennsylvania,
Philadelphia, PA.
Dagan, Ido, Yael Karov, and Dan Roth.
1997. Mistake-driven learning in text
categorization. In Proceedings of the
Second Conference on Empirical Methods
in Natural Language Processing
(EMNLP-1997), pages 55?63,
Providence, RI.
Even-Zohar, Yair and Dan Roth. 2001. A
sequential model for multi-class
classification. In Proceedings of the 2001
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2001),
pages 10?19, Pittsburgh, PA.
Freund, Yoav and Robert E. Schapire. 1999.
Large margin classification using the
Perceptron algorithm.Machine Learning,
37(3):277?296.
Gildea, Daniel and Julia Hockenmaier. 2003.
Identifying semantic roles using
combinatory categorial grammar. In
Proceedings of the 2003 Conference on
Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 57?64,
Sapporo, Japan.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
285
Computational Linguistics Volume 34, Number 2
Gildea, Daniel and Martha Palmer. 2002.
The necessity of parsing for predicate
argument recognition. In Proceedings
of the 40th Annual Meeting of the
Association of Computational Linguistics
(ACL-2002), pages 239?246,
Philadelphia, PA.
Golding, Andrew R. and Dan Roth. 1999.
A Winnow based approach to
context-sensitive spelling correction.
Machine Learning, 34(1-3):107?130.
Grove, Adam J. and Dan Roth. 2001. Linear
concepts and hidden variables.Machine
Learning, 42(1?2):123?141.
Gue?ret, Christelle, Christian Prins, and Marc
Sevaux. 2002. Applications of Optimization
with Xpress-MP. Dash Optimization.
Translated and revised by Susanne
Heipcke. http://www.dashoptimization.
com/home/downloads/book/booka4.pdf.
Hacioglu, Kadri. 2004. Semantic role labeling
using dependency trees. In Proceedings of
the 20th International Conference on
Computational Linguistics (COLING),
Geneva, Switzerland.
Hacioglu, Kadri, Sameer Pradhan, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2004. Semantic role labeling by
tagging syntactic chunks. In Proceedings of
CoNLL-2004, pages 110?113, Boston, MA.
Haghighi, Aria, Kristina Toutanova, and
Christopher D. Manning. 2005. A joint
model for semantic role labeling. In
Proceedings of the Ninth Conference on
Computational Natural Language
Learning (CoNLL-2005), pages 173?176,
Ann Arbor, MI.
Kingsbury, Paul and Martha Palmer. 2002.
From Treebank to PropBank. In Proceedings
of LREC-2002, Las Palmas, Canary Islands,
Spain.
Kipper, Karin, Martha Palmer, and Owen
Rambow. 2002. Extending PropBank with
VerbNet semantic predicates. In
Proceedings of Workshop on Applied
Interlinguas, Tiburon, CA.
Koomen, Peter, Vasin Punyakanok, Dan
Roth, and Wen-tau Yih. 2005. Generalized
inference with multiple semantic role
labeling systems. In Proceedings of the Ninth
Conference on Computational Natural
Language Learning (CoNLL-2005),
pages 181?184, Ann Arbor, MI.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Levin, Beth and Malka R. Hovav. 1996. From
lexical semantics to argument realization.
Unpublished manuscript.
Li, Xin and Dan Roth. 2001. Exploring
evidence for shallow parsing. In
Proceedings of CoNLL-2001, pages 107?110,
Toulouse, France.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ma`rquez, Llu?is, Jesus Gime?nez Pere Comas,
and Neus Catala`. 2005. Semantic role
labeling as sequential tagging. In
Proceedings of the Ninth Conference on
Computational Natural Language
Learning (CoNLL-2005), pages 193?196,
Ann Arbor, MI.
Noreen, Eric W. 1989. Computer-Intensive
Methods for Testing Hypotheses. New York:
John Wiley & Sons.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The proposition bank: An
annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James H. Martin,
and Daniel Jurafsky. 2005. Support vector
learning for semantic argument
classification.Machine Learning, 60:11?39.
Pradhan, Sameer, Kadri Hacioglu, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2003. Semantic role parsing
adding semantic structure to
unstructured text. In Proceedings of the
3rd IEEE International Conference on Data
Mining (ICDM 2003), pages 629?632,
Melbourne, FL.
Pradhan, Sameer, Kadri Hacioglu, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2005. Semantic role chunking
combining complementary syntactic
views. In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 217?220,
Ann Arbor, MI.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James H. Martin, and Daniel
Jurafsky. 2004. Shallow semantic parsing
using support vector machines. In
Proceedings of NAACL-HLT 2004,
pages 233?240, Boston, MA.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
and Dav Zimak. 2004. Semantic role
labeling via integer linear programming
inference. In Proceedings the 20th
International Conference on Computational
Linguistics (COLING), pages 1346?1352,
Geneva, Switzerland.
Punyakanok, Vasin and Dan Roth. 2001. The
use of classifiers in sequential inference. In
286
Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL
Todd K. Leen, Thomas G. Dietterich, and
Volker Tresp, editors, Advances in Neural
Information Processing Systems 13,
pages 995?1001. MIT Press.
Roth, Dan. 1998. Learning to resolve
natural language ambiguities: A unified
approach. In Proceedings of the Fifteenth
National Conference on Artificial
Intelligence (AAAI-98), pages 806?813,
Madison, WI.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks. In
Proceedings of CoNLL-2004, pages 1?8,
Boston, MA.
Roth, Dan and Wen-tau Yih. 2005. Integer
linear programming inference for
conditional random fields. In Proceedings of
the 22nd International Conference on Machine
Learning (ICML-2005), pages 737?744,
Bonn, Germany.
Surdeanu, Mihai, Sanda Harabagiu, John
Williams, and Paul Aarseth. 2003. Using
predicate-argument structures for
information extraction. In Proceedings of the
41st Annual Meeting on Association for
Computational Linguistics, pages 8?15,
Sapporo, Japan.
Tjong Kim Sang, Erik F. and Sabine
Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In
Proceedings of CoNLL-2000 and LLL-2000,
pages 127?132, Lisbon, Portugal.
Xpress-MP. 2004. Dash Optimization.
Xpress-MP. http://www.
dashoptimization.com.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of the 2004
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2004),
pages 88?94, Barcelona, Spain.
Zhang, Tong, Fred Damerau, and David
Johnson. 2002. Text chunking based on a
generalization of Winnow. Journal of
Machine Learning Research, 2:615?637.
287

Identifying Semitic Roots: Machine Learning
with Linguistic Constraints
Ezra Daya?
University of Haifa
Dan Roth??
University of Illinois
Shuly Wintner?
University of Haifa
Words in Semitic languages are formed by combining two morphemes: a root and a pattern. The
root consists of consonants only, by default three, and the pattern is a combination of vowels
and consonants, with non-consecutive ?slots? into which the root consonants are inserted.
Identifying the root of a given word is an important task, considered to be an essential part
of the morphological analysis of Semitic languages, and information on roots is important for
linguistics research as well as for practical applications. We present a machine learning approach,
augmented by limited linguistic knowledge, to the problem of identifying the roots of Semitic
words. Although programs exist which can extract the root of words in Arabic and Hebrew, they
are all dependent on labor-intensive construction of large-scale lexicons which are components of
full-scale morphological analyzers. The advantage of our method is an automation of this process,
avoiding the bottleneck of having to laboriously list the root and pattern of each lexeme in the
language. To the best of our knowledge, this is the first application of machine learning to this
problem, and one of the few attempts to directly address non-concatenative morphology using
machine learning. More generally, our results shed light on the problem of combining classifiers
under (linguistically motivated) constraints.
1. Introduction
The standard account of word-formation processes in Semitic languages describes
words as combinations of two morphemes: a root and a pattern.1 The root consists of
consonants only, by default three (although longer roots are known), called radicals.
The pattern is a combination of vowels and, possibly, consonants too, with ?slots? into
which the root consonants can be inserted. Words are created by interdigitating roots
? Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: edaya@cs.haifa.ac.il.
?? Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801. E-mail:
danr@cs.uiuc.edu.
? Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: shuly@cs.haifa.ac.il
1 An additional morpheme, vocalization, is used to abstract the pattern further; for the present purposes,
this distinction is irrelevant.
Submission received: 19 June 2006; revised submission received: 30 May 2007; accepted for publication:
12 October 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
into patterns: The first radical is inserted into the first consonantal slot of the pattern,
the second fills the second slot, and the third fills the last slot. See Shimron (2003) for a
survey.
We present a machine learning approach, augmented by limited linguistic knowl-
edge, to the problem of identifying the roots of Semitic words. To the best of our
knowledge, this is the first application of machine learning to this problem, and one
of the few attempts to directly address the non-concatenative morphology of Semitic
languages using machine learning. Although there exist programs which can extract the
roots of words in Arabic (Beesley 1998a, 1998b) andHebrew (Choueka 1990), they are all
dependent on labor-intensive construction of large-scale lexicons which are components
of full-scale morphological analyzers. Note that the Arabic morphological analyzer of
Buckwalter (2002, software documentation) only uses ?word stems?rather than root
and pattern morphemes?to identify lexical items.? Buckwalter further notes that ?The
information on root and pattern morphemes could be added to each stem entry if
this were desired.? The challenge of our work is to automate this process, avoiding
the bottleneck of having to laboriously list the root and pattern of each lexeme in the
language.
Identifying the root of a given word is a non-trivial problem, due to the complex
nature of Semitic derivational and inflectional morphology and the peculiarities of the
orthography. It is also an important task. Although existingmorphological analyzers for
Hebrew only provide a lexeme (which is a combination of a root and a pattern), for other
Semitic languages, notably Arabic, the root is an essential part of any morphological
analysis simply because traditional dictionaries are organized by root, rather than by
lexeme (Owens 1997). Information on roots is important for linguistic research, because
roots can shed light on etymological processes, both within a single language and across
languages. Furthermore, roots are known to carry some meaning, albeit vague. This
information can be useful for computational applications: For example, several studies
show that indexing Arabic documents by root improves the performance of information
retrieval systems (Al-Kharashi and Evens 1994; Abu-Salem, Al-Omari, and Evens 1999;
Larkey, Ballesteros, and Connell 2002).2
The contributions of this article are manifold. First and foremost, we report on
a practical system which can be used to extract roots in Hebrew and Arabic (the
system is freely available; an on-line demo is provided at http://cl.haifa.ac.il/
projects/roots/index.shtml). The system can be used for practical applications or
for scientific (linguistic) research, and constitutes an important addition to the grow-
ing set of resources dedicated to Semitic languages. It is one of the few attempts to
directly address the non-concatenative morphology of Semitic languages and extract
non-contiguousmorphemes from surface forms. As amachine learning application, this
work describes a set of experiments in combination of classifiers under constraints. The
resulting insights can be used for other applications of the same techniques for similar
problems (see, e.g., Habash and Rambow 2005). Furthermore, this work demonstrates
that providing a data-driven classifier with limited linguistic knowledge significantly
improves the classification results.
We focus on Hebrew in the first part of this article. After sketching the linguistic
data in Section 2 and our methodology in Section 3, we discuss in Section 4 a simple,
baseline, learning approach.We then propose several methods for combining the results
2 These results are challenged by Darwish and Oard (2002), who conclude that roots are inferior to
character n-grams for this task.
430
Daya, Roth, and Wintner Identifying Semitic Roots
of interdependent classifiers in Section 5 and demonstrate the benefits of using limited
linguistic knowledge in the inference procedure. Then, the same technique is applied
to Arabic in Section 6 and we demonstrate comparable improvements. In Section 7
we discuss the influence of global constraints on local classifiers. We conclude with
suggestions for future research.
2. Linguistic Background
Root and pattern morphology is the major word formation device of Semitic languages.
As an example, consider the Hebrew roots g.d.l, k.t.b, and r.$.m and the patterns
haCCaCa, hitCaCCut, and miCCaC, where the ?C?s indicate the slots.3 When the
roots combine with these patterns the resulting lexemes are hagdala, hitgadlut, migdal,
haktaba, hitkatbut, miktab, har$ama, hitra$mut, and mir$am, respectively. After the
root combines with the pattern, some morpho-phonological alternations take place,
which may be non-trivial: For example, the hitCaCCut pattern triggers assimilation
when the first consonant of the root is t or d : thus, d.r.$+hitCaCCut yields hiddar$ut.
The same pattern triggers metathesis when the first radical is s or $ : s.d.r+hitCaCCut
yields histadrut rather than the expected *hitsadrut. Semi-vowels such as w or y in the
root are frequently combined with the vowels of the pattern, so that q.w.m+haCCaCa
yields haqama, and so on. Frequently, root consonants such as w or y are altogether
missing from the resulting form.
These matters are complicated further due to two sources: First, the standard
Hebrew orthography leaves most of the vowels unspecified.4 It does not explicate a and
e vowels, does not distinguish between o and u vowels and leaves many of the i vowels
unspecified. Furthermore, the single letter w is used both for the vowels o and u and for
the consonant v, whereas i is similarly used both for the vowel i and for the consonant y.
On top of that, the script dictates thatmany particles, including four of themost frequent
prepositions, the definite article, the coordinating conjunction, and some subordinating
conjunctions, all attach to the words which immediately follow them. Thus, a form such
as mhgr can be read as a lexeme (?immigrant?), as m-hgr (?from Hagar?), or even as
m-h-gr (?from the foreigner?). Note that there is no deterministic way to tell whether the
first m of the form is part of the pattern, the root, or a prefixing particle (the preposition
m ?from?).
The Hebrew script has 22 letters, all of which can be considered consonants. The
number of tri-consonantal roots is thus theoretically bounded by 223, although several
phonological constraints limit this number to a much smaller value. For example,
although roots whose second and third radicals are identical abound in Semitic lan-
guages, roots whose first and second radicals are identical are extremely rare (see
McCarthy 1981 for a theoretical explanation). To estimate the number of roots in Hebrew
we compiled a list of roots from two sources: a dictionary (Even-Shoshan 1993) and the
verb paradigm tables of Zdaqa (1974). The union of these yields a list of 2,152 roots.5
3 To facilitate readability we use a straight-forward transliteration of Hebrew using ASCII characters,
where the characters (in Hebrew alphabetic order) are: ?bgdhwzxviklmnsypcqr$t.
4 In this work we consider only texts in undotted, or unvocalized script. This is the standard script of both
Hebrew and Arabic.
5 Only tri-consonantal roots are counted. Ornan (2003) mentions 3,407 roots, whereas the number of roots
in Arabic is estimated to be 10,000 (Darwish 2002). We do not know why Arabic should have so many
more roots than Hebrew.
431
Computational Linguistics Volume 34, Number 3
Whereas most Hebrew roots are regular, many belong to weak paradigms, which
means that root consonants undergo changes in some patterns. Examples include i or n
as the first root consonant, w or i as the second, i as the third, and roots whose second
and third consonants are identical. For example, consider the pattern hCCCh. Regular
roots such as p.s.q yield forms such as hpsqh. However, the irregular roots n.p.l, i.c.g,
q.w.m, and g.n.n in this pattern yield the seemingly similar forms hplh, hcgh, hqmh,
and hgnh, respectively. Note that in the first and second examples, the first radical (n or
i ) is missing, in the third the second radical (w) is omitted, and in the last example one
of the two identical radicals is omitted. Consequently, a form such as hC1C2h can have
any of the roots n.C1.C2, C1.w.C2, C1.i.C2, C1.C2.C2 and even, in some cases, i.C1.C2.
Although root and pattern morphology is the major word formation device of Se-
mitic languages, both Hebrew and Arabic have words which are not generated through
this mechanism, and therefore have no root. These are either loan words (which are
oftentimes longer than originally Semitic words, in particular in the case of proper
names) or short functional or frequent wordswhose origin is more ancient. For example,
the most frequent token in Hebrew texts is the accusative preposition ?t, which is not
formed through root and pattern processes. Of course, theremay be surface formswhich
are ambiguous: one reading based on root and pattern morphology, the other a loan
word, for example, npl (either ?Nepal? or ?fall? (past, 3rd person masculine singular)).
Although the Hebrew script is highly ambiguous, ambiguity is somewhat reduced
for the task we consider here, as many of the possible lexemes of a given form share
the same root. Still, in order to correctly identify the root of a given word, context
must be taken into consideration. For example, the form $mnh has more than a dozen
readings, including the adjective ?fat? (feminine singular), which has the root $.m.n,
and the verb ?count,? whose root is m.n.i, preceded by a subordinating conjunction.
In the experiments we describe herein we ignore context completely, so our results
are handicapped by design. Adding contextual information renders the problem very
similar to that of word sense disambiguation (as different roots denote distinct senses),
and we opted to focus only on morphology here.
3. Data and Methodology
3.1 Machine-Learning Framework
In this work we apply several machine-learning techniques to the problem of root
identification. In all the experiments described in this article we use SNoW (Roth
1998; Carlson et al 1999) as the learning environment, with Winnow as the update
rule (using Perceptron yielded comparable results). SNoW is a multi-class classifier
that is specifically tailored for learning in domains in which the potential number of
information sources (features) taking part in decisions is very large. It is an on-line
linear classifier, as are most of the classifiers currently used in NLP, over a variable
number of expressive features. In addition to the ?standard? perceptron-like algorithms,
SNoW has a number of extensions such as regularization and good treatment of multi-
class classification. SNoW provides, in addition to classification, a reliable confidence in
the instance prediction which facilitates its use in an inference algorithm that combines
predictors to produce a coherent inference.
SNoW has already been used successfully as the learning vehicle in a large col-
lection of natural language related tasks, including part-of-speech tagging, shallow
parsing, information extraction tasks, and so forth, and compared favorably with other
classifiers (Roth 1998; Golding and Roth 1999; Banko and Brill 2001; Punyakanok and
432
Daya, Roth, and Wintner Identifying Semitic Roots
Roth 2001; Florian 2002; Punyakanok, Roth, and Yih 2005). The choice of SNoW as
the learning algorithm in this work is motivated by its good performance on other,
similar tasks and on the availability in SNoW of easy to tune state-of-the-art versions
of three linear algorithms (Perceptron, Winnow, and naive Bayes) in a single package.
As was shown by Roth (1998), Golding and Roth (1999), and in countless experimental
papers thereafter, most algorithms used today, from on-line variations of Winnow and
Perceptron to maximum entropy algorithms to SVMs, perform comparably if tuned
properly, and the eventual performance depends mostly on the selection of features.
3.2 Data and Evaluation
For training and testing, a Hebrew linguist manually tagged a corpus of 15,000 words
(from a set of newspaper articles). Of these, only 9,752 were annotated; the reason for
the gap is that some Hebrew words, mainly borrowed but also some frequent words
such as prepositions, are not formed by the root and pattern paradigm. Such words are
excluded from our experiments in this work; in an application, such words have to be
identified and handled separately. This can be rather easily done using simple heuristics
and a small list of frequent closed-class words, because words which do not conform
to the root and pattern paradigm are either (short, functional) closed-class words, or
loan words which tend to be longer and, in many cases, involve ?foreign? characters
(typically proper names). This problem is orthogonal to the problem of identifying the
root, and hence a pipeline approach is reasonable.
We further eliminated 168 roots with more than three consonants and were left
with 5,242 annotated word types, exhibiting 1,043 different roots. Table 1 shows the
distribution of word types according to root ambiguity.
Table 2 provides the distribution of the roots of the 5,242 word types in our corpus
according to root type, where Ri is the ith radical (note that some roots may belong to
more than one group).
Table 1
Root ambiguity in the corpus.
Number of roots 1 2 3 4
Number of word types 4,886 335 18 3
Table 2
Distribution of root paradigms.
Paradigm Number Percentage
R1 = i 414 7.90
R1 = w 28 0.53
R1 = n 419 7.99
R2 = i 297 5.66
R2 = w 517 9.86
R3 = h 18 0.19
R3 = i 677 12.92
R2 = R3 445 8.49
Regular 3,061 58.41
433
Computational Linguistics Volume 34, Number 3
As assurance for statistical reliability, in all the experiments discussed in the sequel
(unless otherwise mentioned) we performed 10-fold cross validation runs for every
classification task during evaluation. We also divided the annotated corpus into two
sets: a training set of 4,800 words and a test set of 442 words. Only the training set
was used for most experiments, and the results reported here refer to these data unless
stated otherwise. We used the training set to tune the parameter ? (see Section 5.4), and
once ? was set we report on results obtained by training on the training set and testing
on test data (Table 8).
A given example is a word type with all its (manually tagged) possible roots. In
the experiments we describe subsequently, our system produces one or more root
candidates for each example. For each example, we define tp as the number of correct
candidates produced by the system; fp as the number of candidates which are not cor-
rect roots; and fn as the number of roots the system did not produce. As usual, we define
recall as tp
tp+fp
, precision as tp
tp+fn
and F-score as 2?recall?precision
recall+precision
; we then (macro-) average
over all words to obtain the system?s overall F-score.
To estimate the difficulty of this task, we asked six human subjects to perform
it. Participants were asked to identify all the possible roots of all the words in a list
of 200 words (without context), randomly chosen from the training corpus. All par-
ticipants were computer science graduates, native Hebrew speakers with no linguistic
background. The average precision of humans on this task is 83.52%, and with recall at
80.27%, F-score is 81.86%. We conjecture that the main reasons for the low performance
of our subjects are the lack of context (people tend to pick the most prominent root and
ignore the less salient ones) and the ambiguity of some of the weak paradigms (Hebrew
speakers are unaware of the correct root in many of the weak paradigms, even when
only one exists).
3.3 Feature Design
All the experiments we describe in this work share the same features and differ only in
the target classifiers. One of the advantages of SNoW is that it makes use of variable-
sized feature vectors, represented as the list of the active (present) features in a given
instance, rather than the fixed-sized Boolean vectors. This facilitates the use of very long
(theoretically, unbounded) feature lists, which are typically very sparse. The features
that are used to characterize a word are both grammatical and statistical:
 Position of letters (e.g., the third letter of the word is b). We limit word
length to 20, as the longest string generated by a Hebrew morphological
generator (Yona and Wintner 2008) is 18. We thus obtain up to 440
features6 of this type (recall that the size of the alphabet is 22).
 Bigrams of letters, independently of their location (e.g., the substring gd
occurs in the word). This yields up to 484 features.
 Prefixes (e.g., the word is prefixed by k$h, ?when the?). We have 292
features of this type, corresponding to 17 prefixes and sequences thereof.
 Suffixes (e.g., the word ends with im, a plural suffix). There are 26 such
features.
6 Some of these features are never active and are thus never represented.
434
Daya, Roth, and Wintner Identifying Semitic Roots
The lists of suffixes and of prefix sequences were compiled from a morphological
grammar of Hebrew (Yona and Wintner 2008). In the general case, such features can be
elicited from (non-expert) native speakers or extracted from amorphologically analyzed
corpus, if one exists.
3.4 Linguistic Resources
One of our goals in this work is to demonstrate the contribution of limited linguistic
knowledge to a machine-learning approach to an NLP task. Specifically, we used the
following resources for Hebrew and Arabic:
 A list of roots (Section 2)
 Lists of common prefixes and suffixes (Section 3.3)
 Corpora annotated with roots (Section 3.2)
 Knowledge of word-formation processes, and in particular the behavior of
the weak roots in certain paradigms (see Section 5.4)
It is important to note that these resources do not constitute a method for identifying,
even approximately, the root of a given word. We are unaware of any set of rules which
attempts to address this task, or of the chances of solving this problem deterministically.
4. Naive Classification Methods
4.1 Direct Prediction
To establish a baseline, we first performed two experiments with simple, baseline clas-
sifiers. In the first of the two experiments, referred to as Experiment A, we trained a
classifier to learn roots as a single unit. The two obvious drawbacks of this approach
are the large set of targets and the sparseness of the training data. Of course, defining a
multi-class classification task with 2,152 targets, when only half of them are manifested
in the training corpus, does not leave much hope for ever learning to identify the
missing targets. There is no generalization when the whole root is predicted as a single
unit with a simple classifier.
In Experiment A, themacro-average precision of 10-fold cross validation runs of this
classification problem is 45.72%; recall is 44.37%, yielding an F-score of 45.03%. In order
to demonstrate the inadequacy of this method, we repeated the same experiment with
a different organization of the training data. We chose 30 roots and collected all their
occurrences in the corpus into a test file. We then trained the classifier on the remainder
of the corpus and tested on the test file. As expected, the accuracy was close to 0%.
4.2 Decoupling the Problem
In the second experiment, referred to as Experiment B, we separated the problem into
three different tasks. We trained three classifiers to learn each of the root consonants
in isolation and then combined the results in the straightforward way (a conjunction
of the decisions of the three classifiers). This is still a multi-class classification but the
435
Computational Linguistics Volume 34, Number 3
number of targets in every classification task is only 22 (the number of letters in the
Hebrew alphabet) and data sparseness is no longer a problem. Although each classifier
performs well in isolation, the clear limitation of this method is that it completely
ignores interdependencies between different targets: The decision on the first radical
is completely independent of the decision on the second and the third.
We observed a difference between recognizing the first and third radicals and
recognizing the second one, as can be seen in Table 3. These results correspond well
to our linguistic intuitions: The most difficult cases for humans are those in which
the second radical is w or i, and those where the second and the third consonants are
identical. Combining the three classifiers using logical conjunction yields an F-score of
52.84%. Repeating the same experiment, but testing only on unseen roots, yielded 18.1%
accuracy.
To demonstrate the difficulty of the problem, we conducted yet another experiment.
Here, we trained the system as described but we tested it on different wordswhose roots
were known to be in the training set. The results of Experiment A here were 46.35%,
whereas Experiment B was accurate in 57.66% of the cases. Evidently, even when testing
only on previously seen roots, both naive methods are unsuccessful.
5. Combining Interdependent Classifiers
5.1 Adding Linguistic Constraints
The experiments discussed previously are completely devoid of linguistic knowledge.
In particular, Experiment B inherently assumes that any sequence of three consonants
can be the root of a given word. This is obviously not the case: With few exceptions, all
radicals must be present in any inflected form. In fact, when roots and patterns combine,
the first radical can be deleted only when it is w, i, n, and in an exceptional case l ; the
second radical can only be deleted if it is w or i ; and the third, only if it is i. We therefore
trained the classifiers to consider as targets, during training and testing, only letters that
occurred in the observedword, plus w, i, n, and l (depending on the radical), rather than
any of the alphabet letters. The average number of targets is now 7.2 for the first radical,
5.7 for the second, and 5.2 for the third (compared to 22 each in the previous setup).
In this model, known as the sequential model (Even-Zohar and Roth 2001), SNoW?s
performance improved slightly, as can be seen in Table 4 (compare to Table 3). Com-
bining the results in the straight-forward way yields an F-score of 58.89%, a small
improvement over the 52.84% performance of the basic method. This new result should
be considered the baseline. In what follows we always employ the sequential model
for training and testing the classifiers, using the same constraints. However, we employ
more linguistic knowledge for a more sophisticated combination of the classifiers.
Table 3
Accuracy of identifying the correct radical.
R1 R2 R3 root
Precision 82.25 72.29 81.85 53.60
Recall 80.13 70.00 80.51 52.09
F-score 81.17 71.13 81.18 52.84
436
Daya, Roth, and Wintner Identifying Semitic Roots
Table 4
Accuracy of identifying the correct radical, sequential model.
R1 R2 R3 root
Precision 83.06 72.52 83.88 59.83
Recall 80.88 70.20 82.50 57.98
F-score 81.96 71.34 83.18 58.89
5.2 Sequential Combination
Evidently, simple combination of the results of the three classifiers leaves much room
for improvement. We therefore explore other ways for combining these results. We can
rely on the fact that SNoW provides insight into the decisions of the classifiers?it lists
not only the selected target, but rather all candidates, with an associated confidence
measure. Apparently, the correct radicals are chosen among SNoW?s top-n candidates
with high accuracy, as shown in Table 5. This observation calls for a different way
of combining the results of the classifiers which takes into account not only the first
candidate but also others, along with their confidence scores.
Given the sequential nature of the data and the fact that our classifier returns
a distribution over the possible outcomes for each radical, a natural approach is to
combine SNoW?s outcomes via a Markovian approach. Variations of this approach
are used in the context of several natural language problems, including part-of-speech
tagging (Schu?tze and Singer 1994), shallow parsing (Punyakanok and Roth 2001), and
named entity recognition (Tjong Kim Sang and De Meulder 2003).
However, perhaps not surprisingly given the difficulty of the problem, this model is
too simplistic. In fact, performance deteriorated to an F-score of 37.79%. We conjecture
that the static probabilities (the model) are too biased and cause the system to abandon
good choices obtained from SNoW in favor of worse candidates whose global behavior
is better. For example, the root q.r.n was correctly generated by SNoW as the best
candidate for the word mqrn, but because P(R3 = r | R2 = r), which is 0.066, is higher
than P(R3 = n | R2 = r), which is 0.025, the root q.r.r was produced instead.
Similar examples of interdependencies among radicals abound. In Hebrew and
Arabic, some letters cannot appear in a sequence, mostly due to phonetic restrictions.
For example, if the first radical is s, the second radical cannot be z, c, or e. Taking into
account the dependency between the root radicals is an interesting learning experiment
which may provide a better results. We therefore extend the naive HMM approach to
account for such dependencies, following the PMM model of Punyakanok and Roth
(2001).
Table 5
Recall of identifying the correct radical among top-n candidates
R1 R2 R3
top-1: 80.88 70.20 82.50
top-2: 92.98 86.99 93.85
top-5: 99.14 99.38 99.68
top-10: 99.69 99.90 99.70
437
Computational Linguistics Volume 34, Number 3
Consider a specific example of some word w. We already trained a classifier for
R1 (the first root radical), so we can look at the predictions of the R1 classifier for w.
Assume that this classifier predicts a1, a2, a3, . . . , ak with confidence scores c1, c2, . . . , ck
respectively (the maximum value of k is 22). For each value ai (1 ? i ? k) predicted by
the R1 classifier, we run the R2 classifier where the value of the feature for R1 is ai. That
is, we run the R2 classifier k times for each word w (k depends on w). Then, we check
which value of i (where i runs from 1 to k) gives the best sum of the two classifiers, both
the confidence measure of the R1 classifier on ai and the confidence measure of the R2
classifier. This gives a confidence ranking for R2. We perform the same evaluation for R3,
using the results of the R2 classifier as the value of the R3 added feature. We then select
the root which maximizes the confidence of R3; selecting the root which maximizes all
three classifiers yields similar results, as shown in Table 6.
As the results demonstrate, this is a promising approach which we believe can yield
even better results with more training data. However, as we show subsequently, adding
more linguistic knowledge can improve performance even further.
5.3 Learning Bigrams
In the first experiments of this research, we trained a classifier to learn roots as a single
unit. As already mentioned, the drawbacks of this approach are the large set of targets
and the sparseness of the training data. Then, we decoupled the problem into three
different classifiers to learn each of the root consonants in isolation and then combined
the results in various ways. Training the classifiers in the sequential model, considering
as targets only letters that occurred in the observed word, plus w, i, n, and l, reduced
the number of targets from 22 to approximately 7. This facilitates a different experiment
whereby bigrams of the root radicals, rather than each radical in isolation, are learned,
taking advantage of the reduced number of targets for each radical. On one hand, the
average number of targets is still relatively large (about 50), but on the other, we only
have to deal with a combination of two classifiers. In this method, each of the classifiers
should predict two letters at once. For example, we define one classifier to learn the
first and second radicals (R1R2), and a second classifier to learn the second and third
radicals (R2R3). Alternatively, the first and third radicals can be learned as a single unit
by a different classifier. In this case, we only need to combine this classifier with one of
the above mentioned classifiers to obtain the complete root.
It should be noted that the number of potential roots for a given word example
in combining three different classifiers (for each of the root radicals) is determined by
multiplying the number of targets of each of the classifiers. In this classification problem,
each classifier predicts two root radicals, meaning that the classifiers overlap in one
radical. This common radical should be identical in the combination (e.g., R1R2 and
Table 6
Results: Combining dependent classifiers.
Maximizing all radicals Maximizing R3 Baseline (Table 4)
Precision 76.99 76.87 59.83
Recall 84.78 84.78 57.98
F-score 80.70 80.63 58.89
438
Daya, Roth, and Wintner Identifying Semitic Roots
Table 7
Results: Combining classifiers of root radicals bigram.
R1R2&R2R3 R1R2&R1R3 R2R3&R1R3
Precision 82.11 79.71 79.11
Recall 85.28 86.40 86.64
F-score 83.67 82.92 82.71
R2R3 overlap in R2), and thus the number of potential roots is significantly reduced. The
results of these experiments are depicted in Table 7.
5.4 Combining Classifiers using Linguistic Knowledge
SNoW provides a ranking on all possible roots. We now describe the use of linguis-
tic constraints to re-rank this list. We implemented a function, dubbed the scoring
function, which uses knowledge pertaining to word-formation processes in Hebrew
in order to estimate the likelihood of a given candidate being the root of a given word.
The function practically classifies the candidate roots into one of three classes: good
candidates, which are likely to be the root of the word; bad candidates, which are highly
unlikely; and average cases.
It is important to note that the scoring function alone is not a function for extracting
roots from Hebrew words. First, it only scores a given root candidate against a given
word, rather than yield a root given a word. Although we could have used it exhaus-
tively on all possible roots in this case, in a general setting of a number of classifiers
the number of classes might be too high for this solution to be practical. Second, the
function only produces three different values; when given a number of candidate roots
it may return more than one root with the highest score. In the extreme case, when
called with all 223 potential roots, it returns on average more than 11 candidates which
score highest (and hence are ranked equally). The linguistic knowledge employed by the
system, although significant to the improved performance, is far from being sufficient
for devising a deterministic root extraction algorithm.
We now discuss the constraints employed by the scoring function in detail. In what
follows, a root r = r1r2r3 is said to be in Paradigm P1 if r1 ? {w, i,n}; in Paradigm P2 if
r2 ? {w, i}; in Paradigm P3 if r3 ? {h, i}; in Paradigm P4 if r2 = r3; and regular if none
of these holds. The constraints are deterministic, in the sense that they always hold in
the training data. They can be easily evaluated because determining the paradigm(s) a
given root belongs to is deterministic and efficient. In the examples, root consonants are
typeset in boldface.
1. If r is regular then r1, r2, r3 must occur in the word in this order.
Furthermore, either r1r2r3 are consecutive in the target word, or a single
letter intervenes between r1 and r2 or between r2 and r3 (or both). The
intervening letter between r1 and r2 can only be w, i, t (if r1 is $ or s),
d (if r1 is z), or v (if r1 is c). The intervening letter between r2 and r3
can only be w or i. For example, hgrywm hmsxri gdl bkt$yh ?xwzim;
hhstdrwt hzdrzh lhctlm wlhcvdq.
439
Computational Linguistics Volume 34, Number 3
2. If r is in Paradigm P1 and not in Paradigm P2, P3, or P4, then r2, r3 must
occur in the word in this order. Furthermore, either r2r3 are consecutive
in the target word, or a single letter intervenes between r2 and r3. The
intervening letter can only be w or i. Examples: l$bt (i.$.b); hwdiyh (i.d.y);
mpilim (n.p.l).
3. If r is in Paradigm P2 and not in Paradigm P1 or P3, then r1, r3 must
occur in the word in this order. Furthermore, either r1r3 are consecutive
in the target word, or a single letter intervenes between r1 and r3. The
intervening letter can only be w, i, t (if r1 is $ or s), d (if r1 is z), or v (if r1 is
c). Examples: hqmt (q.w.m) hmlwn hcviirh (c.i.r) kmbi?h (b.w.?) rwwxim.
4. If r is in Paradigm P3 and not in Paradigm P1 or P2, then r1, r2 must
occur in the word in this order. Furthermore, either r1r2 are consecutive
in the target word, or a single letter intervenes between r1 and r2. The
intervening letter can only be w, i, t (if r1 is $ or s), d (if r1 is z), or v (if r1
is c). Examples: tgliwt (g.l.i); hzdkw (z.k.i).
5. If r is in Paradigm P4 and not in Paradigm P1 or P2, then r1, r2 must
occur in the word in this order. Furthermore, either r1r2 are consecutive
in the target word, or a single letter intervenes between r1 and r2. The
intervening letter can only be w, i, t (if r1 is $ or s), d (if r1 is z), or v (if r1
is c). Examples: mgilh (g.l.l); hmginim (g.n.n).
6. r must occur in the pre-compiled list of roots.
The decision of the function is based on the observation that when a root is regular
it either occurs in a word consecutively or with a certain single letter between any two
of its radicals (constraint 1). The scoring function checks, given a root and a word,
whether this is the case. If the condition holds, the scoring function returns a high
value. The weak paradigm constraints (2?5) are assigned a middle score, because in
such paradigms we are limited to a partial check on the root as we only check for
the occurrence of two root radicals in the word. For roots that are in more than one
paradigm, the scoring function returns an average score as a default value. We also
make use in this function of our pre-compiled list of roots. A root candidate which does
not occur in the list (constraint 6) is assigned the low score.
The actual values that the function returns were chosen empirically by counting the
number of occurrences of each class in the training data. Thus, ?good? candidates make
up 74.26% of the data, hence the value the function returns for ?good? roots is set to
0.7426. Similarly, the middle value is set to 0.2416 and the low to 0.0155.
As an example, consider ki$lwn, whose only possible root is k.$.l. Here, the correct
candidate will be assigned the high score, because k.$.l is a regular root and its radicals
occur consecutively in the word with a single intervening letter i between k and $
(constraint 1). The candidate root $.l.i will be assigned a middle score, because this root
is in paradigm P3 and constraint 4 holds. The candidate root $.l.n will score low, as it
does not occur in the list of roots (constraint 6).
In addition to the scoring function we implemented a simple edit distance function
which returns, for a given root and a given word, the inverse of the edit distance
between the two. Edit distance (Levenshtein 1965) is the minimum number of character
insertions and deletions required to transform one string to another. For example, for
hipilw, the (correct) root n.p.l scores 1/5 whereas h.p.l scores 1/3. The inverse edit
440
Daya, Roth, and Wintner Identifying Semitic Roots
distance increases with the similarity between two strings; finer tuning of this similarity
measure is of course possible, but was not the focus of this work.
We then run SNoW on the test data and rank the results of the three classifiers glob-
ally, where the order is determined by the product of the three different classifiers. This
induces an order on roots, which are combinations of the decisions of three independent
classifiers. Each candidate root is assigned three scores: the product of the confidence
measures of the three classifiers; the result of the scoring function; and the inverse
edit distance between the candidate and the observed word. We rank the candidates
according to the product of the three scores (i.e., we give each score an equal weight in
the final ranking).
Recall that a given word formmay have several possible roots; our system therefore
has to determine how many roots to produce for each example. We observed that in the
?difficult? examples, the top ranking candidates are assigned close scores, whereas in
the easier cases, the top candidate is usually scored much higher than the next one.
We therefore decided to produce all those candidates whose scores are not much lower
than the score of the top ranking candidate. The drop in the score, ?, was determined
empirically on the training set and was set to ? = 0.4. With this value for ?, results for
the test data are presented in Table 8.
The results clearly demonstrate the added benefit of the linguistic knowledge.
Interestingly, even when testing the system on a set of roots which do not occur in the
training corpus, we obtain an F-score of 65.60%. This result demonstrates the robustness
of our method.
The additional linguistic knowledge is not merely eliminating illegitimate roots
from the ranking produced by SNoW. Using the linguistic constraints encoded in the
scoring function only to eliminate roots, while maintaining the ranking proposed by
SNoW, yields much lower accuracy. Specifically, when we use only the list of roots
as the single constraint when combining the three classifiers, thereby implementing
only a filter of infeasible results, we obtain a precision of 65.24%, recall of 73.87%,
and an F-score of 69.29%. Clearly, our linguistically motivated scoring does more than
elimination, and actually re-ranks the roots. We conclude that it is only the combination
of the classifiers with the linguistically motivated scoring function which boosts the
performance on this task.
5.5 Error Analysis
Looking at the questionnaires filled in by our subjects (Section 3.2), it is obvious that
humans have problems identifying the correct roots in two general cases: when the
root paradigm is weak (i.e., when the root is irregular) and when the word can be
read in more than one way and the subject chooses only one (presumably, the most
prominent one). Our system suffers from similar problems: First, its performance on
Table 8
Results: Using linguistic constraints for inference.
System Baseline
Precision 80.90 59.83
Recall 88.16 57.98
F-score 84.38 58.89
441
Computational Linguistics Volume 34, Number 3
the regular paradigms is far superior to its overall performance; second, it sometimes
cannot distinguish between several roots which are in principle possible, but only one
of which happens to be the correct one.
To demonstrate the first point, we evaluated the performance of the system on a
different organization of the data. We tested separately words whose roots are all reg-
ular, versus words all of whose roots are irregular. We also tested words which have at
least one regular root (this group is titled ?mixed? herein). As an additional experiment,
we extracted from the corpus a sample of 200 ?hard? words: these are surface forms in
which either one of the root characters is missing, or two root characters are transposed
due to metathesis. The results are presented in Table 9, and clearly demonstrate the
difficulty of the system on the weak paradigms, compared to almost 95% on the easier,
regular roots.
A more refined analysis reveals differences between the various weak paradigms.
Table 10 lists F-score for words whose roots are irregular, classified by paradigm. As
can be seen, the system has great difficulty in the cases of R2 = R3 and R3 = i. Refer to
Table 2 for the sizes of the different root classes.
Finally, we took a closer look at some of the errors, and in particular at cases where
the system produces several roots where fewer (usually only one) are correct. Such cases
include, for example, the word hkwtrt (?the title?), whose root is the regular k.t.r; but
the system produces, in addition, also w.t.r, mistaking the k to be a prefix. These are the
kinds of errors which are most difficult to fix.
However, in many cases the system?s errors are relatively easy to overcome. Con-
sider, for example, the word hmtndbim (?the volunteers?) whose root is the irregular
n.d.b. Our system produces as many as five possible roots for this word: n.d.b, i.t.d,
d.w.b, i.h.d, and i.d.d. Clearly some of these could be eliminated. For example, i.t.d
should not be produced, because if this were the root, nothing could explain the pres-
ence of the b in the word; i.h.d should be excluded because of the location of the h.
Similar phenomena abound in the errors the system makes; they indicate that a more
Table 9
Error analysis: Performance of the system on different cases.
Regular Irregular Mixed Hard
Number of words 2,598 2,019 2,781 200
Precision 92.79 60.02 92.54 47.87
Recall 96.92 73.45 94.28 55.11
F-score 94.81 66.06 93.40 51.23
Table 10
Error analysis: The weak paradigms.
Paradigm F-score
R1 = i 70.57
R1 = n 71.97
R2 = i/w 76.33
R3 = i 58.00
R2 = R3 47.42
442
Daya, Roth, and Wintner Identifying Semitic Roots
Table 11
Arabic root ambiguity in the corpus.
Number of roots 1 2 3 4 5 6
Number of words 28,741 2,258 664 277 48 3
careful design of the scoring function can yield still better results, and this is a direction
we intend to pursue in the future.
6. Extension to Arabic
Although Arabic and Hebrew have a very similar morphological system, being both
Semitic languages, the task of learning roots in Arabic is more difficult than in Hebrew,
for the following reasons.
 There are 28 letters in Arabic which are represented using
approximately 40 characters in the transliteration of Modern Standard
Arabic orthography of Buckwalter (2002). Thus, the learning problem is
more complicated due to the increased number of targets (potential root
radicals) as well as the number of characters available in a word.
 The number of roots in Arabic is significantly higher. We pre-compiled a
list of 3,822 trilateral roots from Buckwalter?s list of roots, 2,517 of which
occur in our corpus. According to our lists, Arabic has almost twice as
many roots as Hebrew.
 Not only is the number of roots high, the number of patterns in Arabic is
also much higher than in Hebrew.
 Whereas in Hebrew the only possible letters which can intervene between
root radicals in a word are i and w, in Arabic there are more possibilities.
The possible intervening letter sequences between r1 and r2 are y, w, A, t,
and wA, and between r2 and r3 y, w, A, and A}.7
We applied the same methods discussed previously to the problem of learning
(Modern Standard) Arabic roots. For training and testing, we produced a corpus
of 31,991 word types (we used the morphological analyzer of Buckwalter 2002 to ana-
lyze a corpus of 152,666 word tokens from which our annotated corpus was produced).
Table 11 shows the distribution of word types according to root ambiguity.
We then trained stand-alone classifiers to identify each radical of the root in iso-
lation, using features of the same categories as for Hebrew: location of letters, letter
bigrams (independently of their location), and prefixes and suffixes compiled manu-
ally from a morphological grammar (Buckwalter 2002). Despite the rather pessimistic
starting point, each classifier provides satisfying results, as shown in Table 12, probably
owing to the significantly larger training corpus. The first three columns present the
results of each of the three classifiers, and the fourth column is a straightforward
combination of the three classifiers.
7 ?}? is a character in Buckwalter?s transliteration.
443
Computational Linguistics Volume 34, Number 3
Table 12
Accuracy of identifying the correct radical in Arabic.
R1 R2 R3 root
Precision 86.02 70.71 82.95 54.08
Recall 89.84 80.29 88.99 68.10
F-score 87.89 75.20 85.86 60.29
We combined the classifiers using linguistic knowledge pertaining to word-
formation processes in Arabic, by implementing a function that approximates the like-
lihood of a given candidate to be the root of a given word. The function actually checks
the following cases:
 If a root candidate is indeed the root of a given word, then we expect
it to occur in the word consecutively or with one of {y, w, A, t, wA}
intervening between R1 and R2, or with one of { y, w, A, A} } between
R2 and R3 (or both).
 If a root candidate does not occur in our pre-compiled list of roots, it
cannot be a root of any word in the corpus.
We suppressed the constraints of weak paradigms in the Arabic experiments, be-
cause in such paradigms we are limited to a partial check on the root as we only check
for the occurrence of two root radicals instead of three in the word. This limitation seems
to be crucial in Arabic, considering the fact that the number of roots is much higher
and, in addition, there are more possible intervening letter sequences between the root
radicals. Consequently, more incorrect roots are wrongly extracted as correct ones. Of
course, this is an over-simplistic account of the linguistic facts, but it serves our purpose
of using very limited and very shallow linguistic constraints on the combination of
specialized ?expert? classifiers. Table 13 shows the final results.
The Arabic results are slightly worse than the Hebrew ones. One reason is that in
Hebrew the number of roots is smaller than in Arabic (2,152 vs. 3,822), which leaves
much room for wrong root selection. Another reason might be the fact that in Arabic
word formation is amore complicated process, for example by allowingmore characters
to occur in the word between the root letters as previously mentioned. This may have
caused the scoring function to wrongly tag some root candidates as possible roots.
7. Improving Local Classifiers by Applying Global Constraints
In Section 5 we presented several methods addressing the problem of learning roots. In
general, we trained stand-alone classifiers, each predicting a different root component,
Table 13
Results: Arabic root identification.
Precision 78.21
Recall 82.80
F-score 80.44
444
Daya, Roth, and Wintner Identifying Semitic Roots
in which the decision for the complete root depends on the outcomes of these different
but mutually dependent classifiers. The classifiers? outcomes need to respect some
constraints that arise from the dependency between the root radicals, requiring a level
of inference on top of the predictions, which is implemented by the scoring function
(Section 5.4).
In this section we show that applying global constraints, in the form of the scor-
ing function, not only improves global decisions but also significantly improves the
local classification task. Specifically, we show that the performance of identifying each
radical in isolation improves after the scoring function is applied. In this experiment
we trained each of the three radical classifiers as previously described, and then
applied inference to re-rank the results. The combined classifier now predicts the
complete root, and in particular, induces a new local classifier decision on each of
the radicals which, due to re-ranking, may differ from the original prediction of the
local classifiers.
Table 14 shows the results of each of the radical classifiers after inference with the
scoring function. There is a significant improvement in each of the three classifiers after
applying the global constraints (Table 14; cf. Table 4). The most remarkable improve-
ment is of the R2 classifier. The gap between R2 and other classifiers, as stand-alone
classifiers with no external knowledge, is 10?12%, due to linguistic reasons. Now, after
employing the global constraints, the gap is reduced to only 4%. In such scenarios,
global constraints can significantly aid local classification.
Because the most dominant constraint is the occurrence of the candidate root in
the pre-compiled list of roots, we examined the results of applying only this constraint
on each of the three classifiers, as a single global constraint. Although there is an
improvement in all classifiers, as shown in Table 15, applying this single constraint still
performs worse than applying all the constraints mentioned in Section 5.4. Again, we
conclude that re-ranking the candidates produced by the local classifiers is essential for
improving the accuracy, and filtering out infeasible results is not sufficient.
Finally, to further emphasize the contribution of global inference to local classifi-
cation, we repeated the same experiment, measuring accuracy of each of the radical
classifiers induced by the root identification system, for Arabic. The results are listed
Table 14
Accuracy of each classifier after applying global constraints.
R1 R2 R3
Precision 89.67 84.7 89.27
Recall 93.08 90.17 93.16
F-score 91.34 87.35 91.17
Table 15
Accuracy of each classifier applying the list of roots as a single constraint.
R1 R2 R3
Precision 86.33 78.58 83.63
Recall 88.59 83.75 88.82
F-score 87.45 81.08 86.15
445
Computational Linguistics Volume 34, Number 3
Table 16
Accuracy of each classifier after applying global constraints (Arabic).
R1 R2 R3
Precision 90.41 84.40 87.92
Recall 92.90 89.59 92.19
F-score 91.64 86.92 90.01
in Table 16, and show a significant improvement over the basic classifiers (compare to
Table 12).
8. Conclusions
We have shown that combining machine learning with limited linguistic knowledge
can produce state-of-the-art results on a difficult morphological task, the identification
of roots of Semitic words. Our best result, over 80% accuracy, was obtained using simple
classifiers for each of the root?s consonants, and then combining the outputs of the
classifiers using a linguistically motivated, yet extremely coarse and simplistic, scoring
function.
This work can be improved in a variety of ways. As is well known from other learn-
ing tasks, fine-tuning of the feature set can produce additional accuracy; we expect this
to be the case in this task, too. In particular, introducing features that capture contextual
information is likely to improve the results. Similarly, our scoring function is simplistic
andwe believe that it can be improved. The edit-distance function can be improved such
that the cost of replacing characters reflect phonological and orthographic constraints
(Kruskal 1999). Other, learning-based, re-ranking methods can also be used to improve
the results.
There are various other ways in which different inter-related classifiers can be
combined. Here we only used a simple multiplication of the three classifiers? confidence
measures, which is then combined with the linguistically motivated functions. We
intend to investigate more sophisticated methods for this combination.
Finally, we plan to extend these results to more complex cases of learning tasks with
a large number of targets, in particular such tasks in which the targets are structured.We
are currently working on morphological disambiguation in languages with non-trivial
morphology, which can be viewed as a part-of-speech tagging problem with a large
number of tags on which structure can be imposed using the various morphological
and morpho-syntactic features that morphological analyzers produce.
Acknowledgments
Previous versions of this work were
published as Daya, Roth, and Winter (2004,
2007). This work was supported by The
Caesarea Edmond Benjamin de Rothschild
Foundation Institute for Interdisciplinary
Applications of Computer Science at the
University of Haifa and the Israeli Ministry
of Science and Technology, under the
auspices of the Knowledge Center for
Processing Hebrew. Dan Roth is supported
by NSF grants CAREER IIS-9984168, ITR
IIS-0085836, and ITR-IIS 00-85980. We
are grateful to Ido Dagan, Alon Lavie,
and Idan Szpektor for useful comments.
We benefitted greatly from useful and
instructive comments by three reviewers.
References
Abu-Salem, Hani, Mahmoud Al-Omari,
and Martha W. Evens. 1999. Stemming
methodologies over individual query
words for an Arabic information retrieval
446
Daya, Roth, and Wintner Identifying Semitic Roots
system. Journal of the American Society for
Information Science, 50(6):524?529.
Al-Kharashi, Ibrahim A. and Martha W.
Evens. 1994. Comparing words, stems,
and roots as index terms in an Arabic
information retrieval system. Journal of the
American Society for Information Science,
45(8):548?560.
Banko, Michele and Eric Brill. 2001. Scaling
to very very large corpora for natural
language disambiguation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics, pages 26?33,
Morristown, NJ.
Beesley, Kenneth R. 1998a. Arabic
morphological analysis on the internet.
In Proceedings of the 6th International
Conference and Exhibition on Multi-lingual
Computing, Cambridge, UK.
Beesley, Kenneth R. 1998b. Arabic
morphology using only finite-state
operations. Proceedings of the Workshop
on Computational Approaches to Semitic
Languages, pages 50?57, Montreal, Quebec.
Buckwalter, Tim. 2002. Buckwalter Arabic
morphological analyzer. Linguistic Data
Consortium (LDC) catalog number
LDC2002L49 and ISBN 1-58563-257-0.
Carlson, Andrew J., Chad M. Cumby, Jeff L.
Rosen, and Dan Roth. 1999. The SNoW
learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer
Science Department.
Choueka, Yaacov. 1990. MLIM?A system for
full, exact, on-line grammatical analysis
of Modern Hebrew. In Proceedings of the
Annual Conference on Computers in
Education, page 63, Tel Aviv. [In Hebrew.]
Darwish, Kareem. 2002. Building a shallow
Arabic morphological analyzer in one
day. In Computational Approaches to
Semitic Languages, an ACL?02 Workshop,
pages 47?54, Philadelphia, PA.
Darwish, Kareem and Douglas W. Oard.
2002. Term selection for searching printed
Arabic. In SIGIR ?02: Proceedings of the 25th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, pages 261?268, New York, NY.
Daya, Ezra, Dan Roth, and Shuly Wintner.
2004. Learning Hebrew roots: Machine
learning with linguistic constraints. In
Proceedings of EMNLP?04, pages 357?364,
Barcelona, Spain.
Daya, Ezra, Dan Roth, and Shuly Wintner.
2007. Learning to identify Semitic roots.
In Abdelhadi Soudi, Guenter Neumann,
and Antal van den Bosch, editors, Arabic
Computational Morphology: Knowledge-based
and Empirical Methods, volume 38 of Text,
Speech and Language Technology. Springer,
New York, pages 143?158.
Even-Shoshan, Abraham. 1993. HaMillon
HaXadash (The New Dictionary). Kiryat
Sefer, Jerusalem. In Hebrew.
Even-Zohar, Yair and Dan Roth. 2001.
A sequential model for multi class
classification. In EMNLP-2001, the SIGDAT
Conference on Empirical Methods in Natural
Language Processing, pages 10?19,
Pittsburgh, PA.
Florian, Radu. 2002. Named entity
recognition as a house of cards: Classifier
stacking. In Proceedings of CoNLL-2002,
pages 175?178, Taiwan.
Golding, Andrew R. and Dan Roth.
1999. A Winnow based approach to
context-sensitive spelling correction.
Machine Learning, 34(1?3):107?130.
Habash, Nizar and Owen Rambow.
2005. Arabic tokenization, part-of-
speech tagging and morphological
disambiguation in one fell swoop.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL?05), pages 573?580,
Ann Arbor, MI.
Kruskal, Joseph. 1999. An overview of
sequence comparison. In David Sankoff
and Joseph Kruskal, editors, Time Warps,
String Edits and Macromolecules: The Theory
and Practice of Sequence Comparison. CSLI
Publications, Stanford, CA, pages 1?44.
Larkey, Leah S., Lisa Ballesteros, and
Margaret E. Connell. 2002. Improving
stemming for Arabic information retrieval:
Light stemming and co-occurrence
analysis. In SIGIR ?02: Proceedings of the
25th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 275?282,
New York, NY.
Levenshtein, Vladimir I. 1965. Binary codes
capable of correcting deletions, insertions
and reversals. Doklady Akademii Nauk
SSSR, 163(4):845?848.
McCarthy, John J. 1981. A prosodic theory of
nonconcatenative morphology. Linguistic
Inquiry, 12(3):373?418.
Ornan, Uzzi. 2003. The Final Word. University
of Haifa Press, Haifa, Israel. [In Hebrew.]
Owens, Jonathan. 1997. The Arabic
grammatical tradition. In Robert Hetzron,
editor, The Semitic Languages. Routledge,
London and New York, chapter 3,
pages 46?58.
Punyakanok, Vasin and Dan Roth. 2001. The
use of classifiers in sequential inference. In
447
Computational Linguistics Volume 34, Number 3
NIPS-13; The 2000 Conference on Advances in
Neural Information Processing Systems 13,
pages 995?1001, Denver, CO.
Punyakanok, Vasin, Dan Roth, and Wen-Tau
Yih. 2005. The necessity of syntactic
parsing for semantic role labeling. In
Proceedings of IJCAI 2005, pages 1117?1123,
Edinburgh.
Roth, Dan. 1998. Learning to resolve natural
language ambiguities: A unified approach.
In Proceedings of AAAI-98 and IAAI-98,
pages 806?813, Madison, WI.
Schu?tze, Hinrich and Yoram Singer. 1994.
Part-of-speech tagging using a variable
memory Markov model. In Proceedings
of the 32nd Annual Meeting of the
Association for Computational Linguistics,
pages 181?187, Las Cruses, NM.
Shimron, Joseph, editor. 2003. Language
Processing and Acquisition in Languages of
Semitic, Root-Based, Morphology. Number 28
in Language Acquisition and Language
Disorders. John Benjamins, Amsterdam.
Tjong Kim Sang, Erik F. and Fien
De Meulder. 2003. Introduction
to the CoNLL-2003 shared task:
Language-independent named entity
recognition. In Proceedings of CoNLL-2003,
pages 142?147, Edmonton, Canada.
Yona, Shlomo and Shuly Wintner. 2008. A
finite-state morphological grammar of
Hebrew. Natural Language Engineering,
14(2):173?190.
Zdaqa, Yizxaq. 1974. Luxot HaPoal [The
Verb Tables]. Kiryath Sepher, Jerusalem.
[In Hebrew.]
448
Robust Reading: Identification and Tracing of Ambiguous Names
Xin Li Paul Morie Dan Roth
Department of Computer Science
University of Illinois, Urbana, IL 61801
{xli1,morie,danr}@uiuc.edu
Abstract
A given entity, representing a person, a location
or an organization, may be mentioned in text
in multiple, ambiguous ways. Understanding
natural language requires identifying whether
different mentions of a name, within and across
documents, represent the same entity.
We develop an unsupervised learning approach
that is shown to resolve accurately the name
identification and tracing problem. At the heart
of our approach is a generative model of how
documents are generated and how names are
?sprinkled? into them. In its most general form,
our model assumes: (1) a joint distribution over
entities, (2) an ?author? model, that assumes
that at least one mention of an entity in a docu-
ment is easily identifiable, and then generates
other mentions via (3) an appearance model,
governing how mentions are transformed from
the ?representative? mention. We show how to
estimate the model and do inference with it and
how this resolves several aspects of the prob-
lem from the perspective of applications such
as questions answering.
1 Introduction
Reading and understanding text is a task that requires the
ability to disambiguate at several levels, abstracting away
details and using background knowledge in a variety of
ways. One of the difficulties that humans resolve instan-
taneously and unconsciously is that of reading names.
Most names of people, locations, organizations and oth-
ers, have multiple writings that are used freely within and
across documents.
The variability in writing a given concept, along with
the fact that different concepts may have very similar
writings, poses a significant challenge to progress in nat-
ural language processing. Consider, for example, an open
domain question answering system (Voorhees, 2002) that
attempts, given a question like: ?When was President
Kennedy born?? to search a large collection of articles in
order to pinpoint the concise answer: ?on May 29, 1917.?
The sentence, and even the document that contains the
answer, may not contain the name ?President Kennedy?;
it may refer to this entity as ?Kennedy?, ?JFK? or ?John
Fitzgerald Kennedy?. Other documents may state that
?John F. Kennedy, Jr. was born on November 25, 1960?,
but this fact refers to our target entity?s son. Other men-
tions, such as ?Senator Kennedy? or ?Mrs. Kennedy?
are even ?closer? to the writing of the target entity, but
clearly refer to different entities. Even the statement
?John Kennedy, born 5-29-1941? turns out to refer to a
different entity, as one can tell observing that the doc-
ument discusses Kennedy?s batting statistics. A similar
problem exists for other entity types, such as locations,
organizations etc. Ad hoc solutions to this problem, as
we show, fail to provide a reliable and accurate solution.
This paper presents the first attempt to apply a unified
approach to all major aspects of this problem, presented
here from the perspective of the question answering task:
(1) Entity Identity - do mentions A and B (typically,
occurring in different documents, or in a question and a
document, etc.) refer to the same entity? This problem
requires both identifying when different writings refer to
the same entity, and when similar or identical writings
refer to different entities. (2) Name Expansion - given a
writing of a name (say, in a question), find other likely
writings of the same name. (3) Prominence - given
question ?What is Bush?s foreign policy??, and given that
any large collection of documents may contain several
Bush?s, there is a need to identify the most prominent, or
relevant ?Bush?, perhaps taking into account also some
contextual information.
At the heart of our approach is a global probabilistic
view on how documents are generated and how names
(of different entity types) are ?sprinkled? into them. In
its most general form, our model assumes: (1) a joint dis-
tribution over entities, so that a document that mentions
?President Kennedy? is more likely to mention ?Oswald?
or ? White House? than ?Roger Clemens?; (2) an ?au-
thor? model, that makes sure that at least one mention
of a name in a document is easily identifiable, and then
generates other mentions via (3) an appearance model,
governing how mentions are transformed from the ?rep-
resentative? mention. Our goal is to learn the model from
a large corpus and use it to support robust reading - en-
abling ?on the fly? identification and tracing of entities.
This work presents the first study of our proposed
model and several relaxations of it. Given a collection of
documents we learn the models in an unsupervised way;
that is, the system is not told during training whether two
mentions represent the same entity. We only assume the
ability to recognize names, using a named entity recog-
nizer run as a preprocessor. We define several inferences
that correspond to the solutions we seek, and evaluate the
models by performing these inferences against a large
corpus we annotated. Our experimental results suggest
that the entity identity problem can be solved accurately,
giving accuracies (F1) close to 90%, depending on the
specific task, as opposed to 80% given by state of the art
ad-hoc approaches.
Previous work in the context of question answering
has not addressed this problem. Several works in NLP
and Databases, though, have addressed some aspects of
it. From the natural language perspective, there has
been a lot of work on the related problem of corefer-
ence resolution (Soon et al, 2001; Ng and Cardie, 2003;
Kehler, 2002) - which aims at linking occurrences of
noun phrases and pronouns within a document based on
their appearance and local context. (Charniak, 2001)
presents a solution to the problem of name structure
recognition by incorporating coreference information. In
the context of databases, several works have looked at the
problem of record linkage - recognizing duplicate records
in a database (Cohen and Richman, 2002; Hernandez and
Stolfo, 1995; Bilenko and Mooney, 2003). Specifically,
(Pasula et al, 2002) considers the problem of identity un-
certainty in the context of citation matching and suggests
a probabilistic model for that. Some of very few works
we are aware of that works directly with text data and
across documents, are (Bagga and Baldwin, 1998; Mann
and Yarowsky, 2003), which consider one aspect of the
problem ? that of distinguishing occurrences of identical
names in different documents, and only of people.
The rest of this paper is organized as follows: We for-
malize the ?robust reading? problem in Sec. 2. Sec. 3
describes a generative view of documents? creation and
three practical probabilistic models designed based on it,
and discusses inference in these models. Sec. 4 illustrates
how to learn these models in an unsupervised setting, and
Sec. 5 describes the experimental study. Sec. 6 concludes.
2 Robust Reading
We consider reading a collection of documents D =
{d1, d2, . . . , dm}, each of which may contain men-
tions (i.e. real occurrences) of |T | types of enti-
ties. In the current evaluation we consider T =
{Person, Location,Organization}.
An entity refers to the ?real? concept behind a mention
and can be viewed as a unique identifier to a real-world
object. Examples might be the person ?John F. Kennedy?
who became a president, ?White House? ? the residence
of the US presidents, etc. E denotes the collection of all
possible entities in the world and Ed = {edi }l
d
1 is the set
of entities mentioned in document d. M denotes the col-
lection of all possible mentions and Md = {mdi }n
d
1 is
the set of mentions in document d. Mdi (1 ? i ? ld) is
the set of mentions that refer to entity edi ? Ed. For en-
tity ?John F. Kennedy?, the corresponding set of mentions
in a document may contain ?Kennedy?, ?J. F. Kennedy?
and ?President Kennedy?. Among all mentions of an en-
tity edi in document d we distinguish the one occurring
first, rdi ? Mdi , as the representative of edi . In practice,
rdi is usually the longest mention of edi in the document
as well, and other mentions are variations of it. Repre-
sentatives are viewed as a typical representation of an
entity mentioned in a specific time and place. For ex-
ample, ?President J.F.Kennedy? and ?Congressman John
Kennedy? may be representatives of ?John F. Kennedy?
in different documents. R denotes the collection of all
possible representatives and Rd = {rdi }l
d
1 ? Md is the
set of representatives in document d. This way, each doc-
ument is represented as the collection of its entities, rep-
resentatives and mentions d = {Ed, Rd,Md}.
Elements in the name space W = E?R?M each have
an identifying writing (denoted as wrt(n) for n ? W )1
and an ordered list of attributes, A = {a1, . . . , ap},
which depends on the entity type. Attributes used in the
current evaluation include both internal attributes, such
as, for People, {title, firstname, middlename, lastname,
gender} as well as contextual attributes such as {time, lo-
cation, proper-names}. Proper-names refer to a list of
proper names that occur around the mention in the doc-
ument. All attributes are of string value and the values
could be missing or unknown2.
The fundamental problem we address in robust read-
ing is to decide what entities are mentioned in a given
document (given the observed set Md) and what the most
likely assignment of entity to each mention is.
3 A Model of Document Generation
We define a probability distribution over documents d =
{Ed, Rd,Md}, by describing how documents are being
generated. In its most general form the model has the
following three components:
(1) A joint probability distribution P (Ed) that governs
1The observed writing of a mention is its identifying writing.
For entities, it is a standard representation of them, i.e. the full
name of a person.
2Contextual attributes are not part of the current evaluation,
and will be evaluated in the next step of this work.
EEd
Rd
Md
e
eid
Mid
rid
d
John Fitzgerald Kennedy
John Fitzgerald Kennedy
President John F. Kennedy
{President Kennedy, Kennedy, JFK}
House of Representatives
House of Representatives
House of Representatives
{House of Representatives, The House}
Figure 1: Generating a document
how entities (of different types) are distributed into a doc-
ument and reflects their co-occurrence dependencies.
(2) The number of entities in a document, size(Ed),
and the number of mentions of each entity in Ed,
size(Mdi ), need to be decided. The current evaluation
makes the simplifying assumption that these numbers are
determined uniformly over a small plausible range.
(3) The appearance probability of a name generated
(transformed) from its representative is modelled as a
product distribution over relational transformations of at-
tribute values. This model captures the similarity be-
tween appearances of two names. In the current eval-
uation the same appearance model is used to calculate
both the probability P (r|e) that generates a representa-
tive r given an entity e and the probability P (m|r) that
generates a mention m given a representative r. Attribute
transformations are relational, in the sense that the dis-
tribution is over transformation types and independent of
the specific names.
Given these, a document d is assumed to be gener-
ated as follows (see Fig. 1): A set of size(Ed) entities
Ed ? E is selected to appear in a document d, accord-
ing to P (Ed). For each entity edi ? Ed, a representative
rdi ? R is chosen according to P (rdi |edi ), generating Rd.
Then mentions Mdi of an entity are generated from each
representative rdi ? Rd ? each mention mdj ? Mdi is
independently transformed from rdi according to the ap-
pearance probability P (mdj |rdi ). Assuming conditional
independency between Md and Ed given Rd, the proba-
bility distribution over documents is therefore
P (d) = P (Ed, Rd,Md) = P (Ed)P (Rd|Ed)P (Md|Rd),
and the probability of the document collection D is:
P (D) =
?
d?D
P (d).
Given a mention m in a document d (Md is the set of
observed mentions in d), the key inference problem is to
determine the most likely entity e?m that corresponds to
it. This is done by computing:
Ed = argmaxE??EP (Ed, Rd|Md, ?) (1)
= argmaxE??EP (Ed, Rd,Md|?), (2)
where ? is the learned model?s parameters. This gives the
assignment of the most likely entity e?m for m.
3.1 Relaxations of the Model
In order to simplify model estimation and to evaluate
some assumptions, several relaxations are made to form
three simpler probabilistic models.
Model I: (the simplest model) The key relaxation here
is in losing the notion of an ?author? ? rather than first
choosing a representative for each document, mentions
are generated independently and directly given an entity.
That is, an entity ei is selected from E according to the
prior probability P (ei); then its actual mention mi is se-
lected according to P (mi|ei). Also, an entity is selected
into a document independently of other entities. In this
way, the probability of the whole document set can be
computed simply as follows:
P (D) = P ({(ei,mi)}ni=1) =
n?
i=1
P (ei)P (mi|ei),
and the inference problem for the most likely entity given
m is:
e?m = argmaxe?EP (e|m, ?) = argmaxe?EP (e)P (m|e).
(3)
Model II: (more expressive) The major relaxation
made here is in assuming a simple model of choos-
ing entities to appear in documents. Thus, in order to
generate a document d, after we decide size(Ed) and
{size(Md1 , size(Md2 ), . . . } according to uniform distri-
butions, each entity edi is selected into d independently
of others according to P (edi ). Next, the representative rdi
for each entity edi is selected according to P (rdi |edi ) and
for each representative the actual mentions are selected
independently according to P (mdj |rdj ). Here, we have in-
dividual documents along with representatives, and the
distribution over documents is:
P (d) = P (Ed, Rd,Md) = P (Ed)P (Rd|Ed)P (Md|Rd)
?
|Ed|?
i=1
[P (edi )P (rdi |edi )]
?
(rdj ,m
d
j )
P (mdj |rdj )
after we ignore the size components (they do not influ-
ence inferences). The inference problem here is the same
as in Equ. (2).
Model III: This model performs the least relaxation.
After deciding size(Ed) according to a uniform distri-
bution, instead of assuming independency among enti-
ties which does not hold in reality (For example, ?Gore?
and ?George. W. Bush? occur together frequently, but
?Gore? and ?Steve. Bush? do not), we select entities us-
ing a graph based algorithm: entities in E are viewed
as nodes in a weighted directed graph with edges (i, j)
labelled P (ej |ei) representing the probability that entity
ej is chosen into a document that contains entity ei. We
distribute entities to Ed via a random walk on this graph
starting from ed1 with a prior probability P (edi ). Repre-
sentatives and mentions are generated in the same way
as in Model II. Therefore, a more general model for the
distribution over documents is:
P (d) ? P (ed1)P (rd1 |ed1)
|Ed|?
i=2
[P (edi |edi?1)P (rdi |edi )]?
?
(rdj ,m
d
j )
P (mdj |rdj ).
The inference problem is the same as in Equ. (2).
3.2 Inference Algorithms
The fundamental problem in robust reading can be solved
as inference with the models: given a mention m, seek the
most likely entity e ? E for m according to Equ. (3) for
Model I or Equ. (2) for Model II and III. Instead of all
entities in the real world, E can be viewed without loss
as the set of entities in a closed document collection that
we use to train the model parameters and it is known after
training. The inference algorithm for Model I (with time
complexity O(|E|)) is simple and direct: just compute
P (e,m) for each candidate entity e ? E and then choose
the one with the highest value. Due to exponential num-
ber of possible assignments of Ed, Rd to Md in Model
II and III, precise inference is infeasible and approximate
algorithms are therefore designed:
In Model II, we adopt a two-step algorithm: First, we
seek the representatives Rd for the mentions Md in docu-
ment d by sequentially clustering the mentions according
to the appearance model. The first mention in each group
is chosen as the representative. Specifically, when con-
sidering a mention m ? Md, P (m|r) is computed for
each representative r that have already been created and
a fixed threshold is then used to decide whether to create a
new group for m or to add it to one of the existing groups
with the largest P (m|r). In the second step, each rep-
resentative rdi ? Rd is assigned to its most likely entity
according to e? = argmaxe?EP (e) ?P (r|e). This algo-
rithm has a time complexity of O((|Md|+ |E|) ? |Md|).
Model III has a similar algorithm as Model II. The
only difference is that we need to consider the global
dependency between entities. Thus in the second step,
instead of seeking an entity e for each representative r
separately, we determine a set of entities Ed for Rd in
a Hidden Markov Model with entities in E as hidden
states and Rd as observations. The prior probabilities,
the transitive probabilities and the observation probabil-
ities are given by P (e), P (ej |ei) and P (r|e) respec-
tively. Here we seek the most likely sequence of enti-
ties given those representatives in their appearing order
using the Viterbi algorithm. The total time complexity is
e1= George Bush e2= George W. Bush e3= Steve Bush
m1,r1=President Bush
m2=Bush
m4,r2=Steve Bush
m5=Bushm3=J. Quayle
Entities E
d1 d2
Figure 2: An conceptual example. The arrows represent
the correct assignment of entities to mentions. r1, r2 are
representatives.
O(|Md|2 + |E|2 ? |Md|). The |E|2 component can be
simplified by filtering out unlikely entities for a represen-
tative according to their appearance similarity.
3.3 Discussion
Besides different assumptions, some fundamental differ-
ences exist in inference with the models as well. In Model
I, the entity of a mention is determined completely inde-
pendently of other mentions, while in Model II, it relies
on other mentions in the same document for clustering.
In Model III, it is not only related to other mentions but
to a global dependency over entities. The following con-
ceptual example illustrates those differences as in Fig. 2.
Example 3.1 Given E = {George Bush, George W. Bush,
Steve Bush}, documents d1, d2 and 5 mentions in them, and
suppose the prior probability of entity ?George W. Bush? is
higher than those of the other two entities, the entity assign-
ments to the five mentions in the models could be as follows:
For Model I, mentions(e1) = ?, mentions(e2) =
{m1,m2,m5} and mentions(e3) = {m4}. The result is
caused by the fact that a mention tends to be assigned to the
entity with higher prior probability when the appearance simi-
larity is not distinctive.
For Model II, mentions(e1) = ?, mentions(e2) =
{m1,m2} and mentions(e3) = {m4,m5}. Local depen-
dency (appearance similarity) between mentions inside each
document enforces the constraint that they should refer to the
same entity, like ?Steve Bush? and ?Bush? in d2.
For Model III, mentions(e1) = {m1,m2}, mentions(e2)
= ?, mentions(e3) = {m4,m5}. With the help of global
dependency between entities, for example, ?George Bush? and
?J. Quayle?, an entity can be distinguished from another one
with a similar writing.
3.4 Other Tasks
Other aspects of ?Robust Reading? can be solved based
on the above inference problem.
Entity Identity: Given two mentions m1 ? d1,m2 ? d2,
determine whether they correspond to the same entity by:
m1 ? m2 ?? argmaxe?EP (e,m1) = argmaxe?EP (e,m2)
for Model I and
m1 ? m2 ?? argmaxe?EP (Ed1 , Rd1 ,Md1) =
argmaxe?EP (Ed2 , Rd2 ,Md2).
for Model II and III.
Name Expansion: Given a mention mq in a query q,
decide whether mention m in the document collection D
is a ?legal? expansion of mq:
mq ? m ?? e?mq = argmaxe?EP (Eq, Rq,Mq)
& m ? mentions(e?).
Here it?s assumed that we already know the possible
mentions of e? after training the models with D.
Prominence: Given a name n ? W , the most promi-
nent entity for n is given by (P (e) is given by the prior
distribution PE and P (n|e) is given by the appearance
model.):
e? = argmaxe?EP (e)P (n|e).
4 Learning the Models
Confined by the labor of annotating data, we learn the
probabilistic models in an unsupervised way given a col-
lection of documents; that is, the system is not told dur-
ing training whether two mentions represent the same en-
tity. A greedy search algorithm modified after the stan-
dard EM algorithm (We call it Truncated EM algorithm)
is adopted here to avoid complex computation.
Given a set of documents D to be studied and the ob-
served mentions Md in each document, this algorithm
iteratively updates the model parameter ? (several under-
lying probabilistic distributions described before) and the
structure (that is, Ed and Rd) of each document d. Dif-
ferent from the standard EM algorithm, in the E-step, it
seeks the most likely Ed and Rd for each document rather
than the expected assignment.
4.1 Truncated EM Algorithm
The basic framework of the Truncated EM algorithm to
learn Model II and III is as follows:
1. In the initial (I-) step, an initial (Ed0 , Rd0) is assigned
to each document d by an initialization algorithm.
After this step, we can assume that the documents
are annotated with D0 = {(Ed0 , Rd0,Md)}.
2. In the M-step, we seek the model parameter ?t+1
that maximizes P (Dt|?). Given the ?labels? sup-
plied in the previous I- or E-step, this amounts to the
maximum likelihood estimation. (to be described in
Sec. 4.3).
3. In the E-step, we seek (Edt+1, Rdt+1) for each
document d that maximizes P (Dt+1|?t+1) where
Dt+1 = {(Edt+1, Rdt+1,Md)}. It?s the same infer-
ence problem as in Sec. 3.2.
4. Stopping Criterion: If no increase is achieved over
P (Dt|?t), the algorithm exits. Otherwise the algo-
rithm will iterate over the M-step and E-step.
The algorithm for Model I is similar to the above one,
but much simpler in the sense that it does not have the no-
tions of documents and representatives. So in the E-step
we only seek the most likely entity e for each mention
m ? D, and this simplifies the parameter estimation in
the M-step accordingly. It usually takes 3? 10 iterations
before the algorithms stop in our experiments.
4.2 Initialization
The purpose of the initial step is to acquire an initial guess
of document structures and the set of entities E in a closed
collection of documents D. The hope is to find all entities
without loss so duplicate entities are allowed. For all the
models, we use the same algorithm:
A local clustering is performed to group mentions in-
side each document: simple heuristics are applied to cal-
culating the similarity between mentions; and pairs of
mentions with similarity above a threshold are then clus-
tered together. The first mention in each group is chosen
as the representative (only in Model II and III) and an
entity having the same writing with the representative is
created for each cluster3. For all the models, the set of
entities created in different documents become the global
entity set E in the following M- and E-steps.
4.3 Estimating the Model Parameters
In the learning process, assuming documents have al-
ready been annotated D = {(e, r,m)}n1 from previous I-
or E-step, several underlying probability distributions of
the relaxed models are estimated by maximum likelihood
estimation in each M-step. The model parameters include
a set of prior probabilities for entities PE , a set of tran-
sitive probabilities for entity pairs PE|E (only in Model
III) and the appearance probabilities PW |W of each name
in the name space W being transformed from another.
? The prior distribution PE is modelled as a multi-
nomial distribution. Given a set of labelled entity-
mention pairs {(ei,mi)}n1 ,
P (e) = freq(e)n
where freq(e) denotes the number of pairs containing
entity e.
? Given all the entities appearing in D, the transitive
probability P (e|e) is estimated by
P (e2|e1) ? P (wrt(e2)|wrt(e1)) = doc
#(wrt(e2), wrt(e1))
doc#(wrt(e1)) .
Here, the conditional probability between two real-
world entities P (e2|e1) is backed off to the one be-
tween the identifying writings of the two entities
P (wrt(e2)|wrt(e1)) in the document set D to avoid
3Note that the performance of the initialization algorithm is
97.3% precision and 10.1% recall (measures are defined later.)
sparsity problem. doc#(w1, w2, ...) denotes the num-
ber of documents having the co-occurrence of writings
w1, w2, ....
? Appearance probability, the probability of one
name being transformed from another, denoted as
P (n2|n1) (n1, n2 ? W ), is modelled as a product
of the transformation probabilities over attribute val-
ues 4. The transformation probability for each attribute
is further modelled as a multi-nomial distribution over
a set of predetermined transformation types: TT =
{copy,missing, typical, non? typical}5.
Suppose n1 = (a1 = v1, a2 = v2, ..., ap = vp) and
n2 = (a1 = v?1, a2 = v?2, ..., ap = v?p) are two names be-
longing to the same entity type, the transformation prob-
abilities PM |R, PR|E and PM |E , are all modelled as a
product distribution (naive Bayes) over attributes:
P (n2|n1) = ?pk=1P (v?k|vk).
We manually collected typical and non-typical trans-
formations for attributes such as titles, first names,
last names, organizations and locations from multiple
sources such as U.S. government census and online dic-
tionaries. For other attributes like gender, only copy
transformation is allowed. The maximum likelihood es-
timation of the transformation probability P (t, k) (t ?
TT, ak ? A) from annotated representative-mention
pairs {(r,m)}n1 is:
P (t, k) = freq(r,m) : v
r
k ?t vmk
n (4)
vrk ?t vmk denotes the transformation from attribute
ak of r to that of m is of type t. Simple smoothing is
performed here for unseen transformations.
5 Experimental Study
Our experimental study focuses on (1) evaluating the
three models on identifying three entity types (Peo-
ple, Locations, Organization); (2) comparing our in-
duced similarity measure between names (the appearance
model) with other similarity measures; (3) evaluating the
contribution of the global nature of our model, and fi-
nally, (4) evaluating our models on name expansion and
prominence ranking.
5.1 Methodology
We randomly selected 300 documents from 1998-2000
New York Times articles in the TREC corpus (Voorhees,
4The appearance probability can be modelled differently by
using other string similarity between names. We will compare
the model described here with some other non-learning similar-
ity metrics later.
5copy denotes v?k is exactly the same as vk; missing denotes
?missing value? for v?k; typical denotes v?k is a typical variation
of vk, for example, ?Prof.? for ?Professor?, ?Andy? for ?An-
drew?; non-typical denotes a non-typical transformation.
2002). The documents were annotated by a named entity
tagger for People, Locations and Organizations. The an-
notation was then corrected and each name mention was
labelled with its corresponding entity by two annotators.
In total, about 8, 000 mentions of named entities which
correspond to about 2, 000 entities were labelled. The
training process gets to see only the 300 documents and
extracts attribute values for each mention. No supervision
is supplied. These records are used to learn the proba-
bilistic models.
In the 64 million possible mention pairs, most are triv-
ial non-matching one ? the appearances of the two men-
tions are very different. Therefore, direct evaluation over
all those pairs always get alost 100% accuracy in our
experiments. To avoid this, only the 130, 000 pairs of
matching mentions that correspond to the same entity are
used to evaluate the performance of the models. Since
the probabilistic models are learned in an unsupervised
setting, testing can be viewed simply as the evaluation of
the learned model, and is thus done on the same data. The
same setting was used for all models and all comparison
performed (see below).
To evaluate the performance, we pair two mentions
iff the learned model determined that they correspond
to the same entity. The list of predicted pairs is then
compared with the annotated pairs. We measure Preci-
sion (P ) ? Percentage of correctly predicted pairs, Recall
(R) ? Percentage of correct pairs that were predicted, and
F1 = 2PRP+R .
Comparisons: The appearance model induces a ?simi-
larity? measure between names, which is estimated dur-
ing the training process. In order to understand whether
the behavior of the generative model is dominated by
the quality of the induced pairwise similarity or by the
global aspects (for example, inference with the aid of
the document structure), we (1) replace this measure by
two other ?local? similarity measures, and (2) compare
three possible decision mechanisms ? pairwise classifica-
tion, straightforward clustering over local similarity, and
our global model. To obtain the similarity required by
pairwise classification and clustering, we use this for-
mula sima(n1, n2) = P (n1|n2) to convert the appear-
ance probability described in Sec. 4.3 to it.
The first similarity measure we use is a sim-
ple baseline approach: two names are similar iff
they have identical writings (that is, simb(n1, n2) =
1 if n1, n2 are identical or 0 otherwise). The second
one is a state-of-art similarity measure sims(n1, n2) ?
[0, 1] for entity names (SoftTFIDF with Jaro-Winkler dis-
tance and ? = 0.9); it was ranked the best measure in a
recent study (Cohen et al, 2003).
Pairwise classification is done by pairing two men-
tions iff the similarity between them is above a fixed
threshold. For Clustering, a graph-based clustering al-
All(P/L/O) Identity SoftTFIDF Appearance
Pairwise 70.7 (64.7/64.1/83.7) 82.1 (79.9/77.3/89.5) 81.5 (83.6/70.9/90.7)
Clustering 70.7 (64.7/64.1/83.7) 79.8 (70.6/76.7/91.0) 79.6 (70.9/76.1/91.0)
Model II 70.7 (64.7/64.1/83.7) 82.5 (79.8/77.4/90.2) 89.0 (92.7/81.9/92.9)
Table 1: Comparison of different decision levels and sim-
ilarity measures. Three similarity measures are evaluated
(rows) across three decision levels (columns). Performance is
evaluated by the F1 values over the whole test set. The first
number averages all entity types; numbers in parentheses repre-
sent People, Location and Organization respectively.
gorithm is used. Two nodes in the graph are connected
if the similarity between the corresponding mentions is
above a threshold. In evaluation, any two mentions be-
longing to the same connected component are paired the
same way as we did in Sec. 5.1 and all those pairs are then
compared with the annotated pairs to calculate Precision,
Recall and F1.
Finally, we evaluate the baseline and the SoftTFIDF
measure in the context of Model II, where the appear-
ance model is replaced. We found that the probabil-
ities directly converted from the SoftTFIDF similarity
behave badly so we adopt this formula P (n1|n2) =
e10?sims(n1,n2)?1
e10?1 instead to acquire P (n1|n2) needed by
Model II. Those probabilities are fixed as we estimate
other model parameters in training.
5.2 Results
The bottom line result is given in Tab. 1. All the similarity
measures are compared in the context of the three levels
of decisions ? local decision (pairwise), clustering and
our probabilistic model II. Only the best results in the
experiments, achieved by trying different thresholds in
pairwise classification and clustering, are shown.
The behavior across rows indicates that, locally, our
unsupervised learning based appearance model is about
the same as the state-of-the-art SoftTFIDF similarity. The
behavior across columns, though, shows the contribu-
tion of the global model, and that the local appearance
model behaves better with it than a fixed similarity mea-
sure does. A second observation is that the Location ap-
pearance model is not as good as the one for People and
Organization, probably due to the attribute transforma-
tion types chosen.
Tab. 2 presents a more detailed evaluation of the differ-
ent approaches on the entity identity task. All the three
probabilistic models outperform the discriminatory ap-
proaches in this experiment, an indication of the effec-
tiveness of the generative model.
We note that although Model III is more expressive
and reasonable than model II, it does not always perform
better. Indeed, the global dependency among entities in
Model III achieves two-folded outcomes: it achieves bet-
ter precision, but may degrade the recall. The following
example, taken from the corpus, illustrates the advantage
of this model.
Entity Type Mod InDoc InterDoc All
F1(%) F1(%) R(%) P(%) F1(%)
All Entities B 86.0 68.8 58.5 85.5 70.7
D 86.5 78.9 66.4 95.8 79.8
I 96.3 85.0 79.0 94.1 86.2
II 96.5 88.1 85.9 92.2 89.0
III 96.5 87.9 84.4 93.6 88.9
People B 82.4 59.0 48.5 86.3 64.7
D 82.4 67.1 54.5 91.5 70.6
I 96.2 84.8 80.6 94.8 87.4
II 96.4 91.7 94.0 91.5 92.7
III 96.4 88.9 89.8 91.3 90.5
Location B 88.8 63.0 54.8 75.0 64.1
D 91.4 76.0 61.3 95.9 76.7
I 92.9 78.9 70.9 89.1 79.5
II 93.8 81.4 76.2 88.1 81.9
III 93.8 82.8 76.0 91.2 83.3
Organization B 95.3 82.8 72.6 96.4 83.7
D 95.8 90.7 83.9 98.9 91.1
I 98.8 91.8 86.5 98.5 92.3
II 98.5 92.5 88.6 97.5 92.9
III 98.8 93.0 88.5 98.6 93.4
Table 2: Performance of different approaches over all test
examples. B, D, I, II and III denote the baseline model, the
SoftTFIDF similarity model with clustering, and the three prob-
abilistic models. We distinguish between pairs of mentions that
are inside the same document (InDoc, 15% of the pairs) or not
(InterDoc).
Example 5.1 ?Sherman Williams? is mentioned along with
the baseball team ?Dallas Cowboys? in 8 out of 300 documents,
while ?Jeff Williams? is mentioned along with ?LA Dodgers?
in two documents.
In all models but Model III, ?Jeff Williams? is judged to cor-
respond to the same entity as ?Sherman Williams? since their
appearances are similar and the prior probability of the latter is
higher than the former. Only Model III, due to the co-occurring
dependency between ?Jeff Williams? and ?Dodgers?, identi-
fies it as corresponding to an entity different from ?Sherman
Williams?.
While this shows that Model III achieves better preci-
sion, the recall may go down. The reason is that global
dependencies among entities enforces restrictions over
possible grouping of similar mentions; in addition, with
a limited document set, estimating this global depen-
dency is inaccurate, especially when the entities them-
selves need to be found when training the model.
Hard Cases: To analyze the experimental results further,
we evaluated separately two types of harder cases of the
entity identity task: (1) mentions with different writings
that refer to the same entity; and (2) mentions with sim-
ilar writings that refer to different entities. Model II and
III outperform other models in those two cases as well.
Tab. 3 presents F1 performance of different approaches
in the first case. The best F1 value is only 73.1%, indicat-
ing that appearance similarity and global dependency are
not sufficient to solve this problem when the writings are
very different. Tab. 4 shows the performance of differ-
ent approaches for disambiguating similar writings that
correspond to different entities.
Both these cases exhibit the difficulty of the problem,
and that our approach provides a significant improvement
over the state of the art similarity measure ? column D
vs. column II in Tab. 4. It also shows that it is necessary
to use contextual attributes of the names, which are not
yet included in this evaluation.
Model B D I II III
Peop 0 77.9 79.2 86.0 82.6
Loc 0 30.4 55.1 58.5 61.5
Org 0 77.7 69.5 71.7 71.2
All 0 63.3 68.4 73.1 72.5
Table 3: Identifying different writings of the same entity
(F1). We filter out identical writings and report only on cases
of different writings of the same entity. The test set contains
46, 376 matching pairs (but in different writings) in the whole
data set.
Model B D I II III
Peop 75.2 83.0 60.8 89.7 88.0
Loc 86.5 80.7 80.0 90.3 90.3
Org 80.0 89.4 71.0 93.1 92.6
All 78.7 78.9 68.1 90.7 89.7
Table 4: Identifying similar writings of different
entities(F1). The test set contains 39, 837 pairs of mentions
that associated with different entities in the 300 documents and
have at least one token in common.
5.3 Other Tasks
In the following experiments, we evaluate the genera-
tive model on other tasks related to robust reading. We
present results only for Model II, the best one in previous
experiments.
Name Expansion: Given a mention m in a query, we find
the most likely entity e ? E for m using the inference al-
gorithm as described in Sec. 3.2. All unique mentions of
the entity in the documents are output as the expansions
of m. The accuracy for a given mention is defined as the
percentage of correct expansions output by the system.
The average accuracy of name expansion of Model II is
shown in Tab. 5. Here is an example:
Query: Who is Gore ?
Expansions: Vice President Al Gore, Al Gore, Gore.
Prominence Ranking: We refer to Example 3.1 and use
it to exemplify quantitatively how our system supports
prominence ranking. Given a query name n, the ranking
of the entities with regard to the value of P (e) ? P (n|e)
(shown in brackets) by Model II is as follows.
Input: George Bush
1. George Bush (0.0448) 2. George W. Bush (0.0058)
Input: Bush
1. George W. Bush (0.0047) 2. George Bush (0.0015)
3. Steve Bush (0.0002)
6 Conclusion and Future Work
This paper presents an unsupervised learning approach to
several aspects of the ?robust reading? problem ? cross-
document identification and tracing of ambiguous names.
We developed a model that describes the natural gen-
eration process of a document and the process of how
Entity Type People Location Organization
Accuracy(%) 90.6 100 100
Table 5: Accuracy of name expansion. Accuracy is averaged
over 30 randomly chosen queries for each entity type.
names are ?sprinkled? into them, taking into account de-
pendencies between entities across types and an ?author?
model. Several relaxations of this model were developed
and studied experimentally, and compared with a state-
of-the-art discriminative model that does not take a global
view. The experiments exhibit encouraging results and
the advantages of our model.
This work is a preliminary exploration of the robust
reading problem. There are several critical issues that our
model can support, but were not included in this prelimi-
nary evaluation. Some of the issues that will be included
in future steps are: (1) integration with more contextual
information (like time and place) related to the target enti-
ties, both to support a better model and to allow temporal
tracing of entities; (2) studying an incremental approach
of training the model; that is, when a new document is
observed, coming, how to update existing model param-
eters ? (3) integration of this work with other aspects of
general coreference resolution (e.g., other terms like pro-
nouns that refer to an entity) and named entity recognition
(which we now take as given); and (4) scalability issues
in applying the system to large corpora.
Acknowledgments
This research is supported by NSF grants ITR-IIS-
0085836, ITR-IIS-0085980 and IIS-9984168 and an
ONR MURI Award.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-document
coreferencing using the vector space model. In ACL.
M. Bilenko and R. Mooney. 2003. Adaptive duplicate detection
using learnable string similarity measures. In KDD.
E. Charniak. 2001. Unsupervised learning of name structure
from coreference datal. In NAACL.
W. Cohen and J. Richman. 2002. Learning to match and clus-
ter large high-dimensional data sets for data integration. In
KDD.
W. Cohen, P. Ravikumar, and S. Fienberg. 2003. A comparison
of string metrics for name-matching tasks. In IIWeb Work-
shop 2003.
M. Hernandez and S. Stolfo. 1995. The merge/purge problem
for large databases. In SIGMOD.
A. Kehler. 2002. Coherence, Reference, and the Theory of
Grammar. CSLI Publications.
G. Mann and D. Yarowsky. 2003. Unsupervised personal name
disambiguation. In CoNLL.
V. Ng and C. Cardie. 2003. Improving machine learning ap-
proaches to coreference resolution. In ACL.
H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser.
2002. Identity uncertainty and citation matching. In NIPS.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. Computa-
tional Linguistics (Special Issue on Computational Anaphora
Resolution), 27:521?544.
E. Voorhees. 2002. Overview of the TREC-2002 question an-
swering track. In Proceedings of TREC, pages 115?123.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 82?88,
New York, June 2006. c?2006 Association for Computational Linguistics
Named Entity Transliteration and Discovery from Multilingual Comparable
Corpora
Alexandre Klementiev Dan Roth
Department of Computer Science
University of Illinois
Urbana, IL 61801
 
klementi,danr  @uiuc.edu
Abstract
Named Entity recognition (NER) is an im-
portant part of many natural language pro-
cessing tasks. Most current approaches
employ machine learning techniques and
require supervised data. However, many
languages lack such resources. This paper
presents an algorithm to automatically dis-
cover Named Entities (NEs) in a resource
free language, given a bilingual corpora
in which it is weakly temporally aligned
with a resource rich language. We ob-
serve that NEs have similar time distribu-
tions across such corpora, and that they
are often transliterated, and develop an al-
gorithm that exploits both iteratively. The
algorithm makes use of a new, frequency
based, metric for time distributions and a
resource free discriminative approach to
transliteration. We evaluate the algorithm
on an English-Russian corpus, and show
high level of NEs discovery in Russian.
1 Introduction
Named Entity recognition has been getting much
attention in NLP research in recent years, since it
is seen as a significant component of higher level
NLP tasks such as information distillation and ques-
tion answering, and an enabling technology for bet-
ter information access. Most successful approaches
to NER employ machine learning techniques, which
require supervised training data. However, for many
languages, these resources do not exist. Moreover,
it is often difficult to find experts in these languages
both for the expensive annotation effort and even for
language specific clues. On the other hand, compa-
rable multilingual data (such as multilingual news
streams) are increasingly available (see section 4).
In this work, we make two independent observa-
tions about Named Entities encountered in such cor-
pora, and use them to develop an algorithm that ex-
tracts pairs of NEs across languages. Specifically,
given a bilingual corpora that is weakly temporally
aligned, and a capability to annotate the text in one
of the languages with NEs, our algorithm identifies
the corresponding NEs in the second language text,
and annotates them with the appropriate type, as in
the source text.
The first observation is that NEs in one language
in such corpora tend to co-occur with their coun-
terparts in the other. E.g., Figure 1 shows a his-
togram of the number of occurrences of the word
Hussein and its Russian transliteration in our bilin-
gual news corpus spanning years 2001 through late
2005. One can see several common peaks in the two
histograms, largest one being around the time of the
beginning of the war in Iraq. The word Russia, on
the other hand, has a distinctly different temporal
signature. We can exploit such weak synchronicity
of NEs across languages as a way to associate them.
In order to score a pair of entities across languages,
we compute the similarity of their time distributions.
The second observation is that NEs are often
transliterated or have a common etymological origin
across languages, and thus are phonetically similar.
Figure 2 shows an example list of NEs and their pos-
82
 0
 5
 10
 15
 20
?hussein? (English)
 0
 5
 10
 15
 2
?hussein? (Russian)
 0
 5
 10
 15
 2
01/01/01 10/05/05
N
um
be
r o
f O
cc
ur
en
ce
s
Time
?russia? (English)
Figure 1: Temporal histograms for Hussein (top),
its Russian transliteration (middle), and of the word
Russia (bottom).
sible Russian transliterations.
Approaches that attempt to use these two charac-
teristics separately to identify NEs across languages
would have significant shortcomings. Translitera-
tion based approaches require a good model, typi-
cally handcrafted or trained on a clean set of translit-
eration pairs. On the other hand, time sequence sim-
ilarity based approaches would incorrectly match
words which happen to have similar time signatures
(e.g. Taliban and Afghanistan in recent news).
We introduce an algorithm we call co-ranking
which exploits these observations simultaneously to
match NEs on one side of the bilingual corpus to
their counterparts on the other. We use a Discrete
Fourier Transform (Arfken, 1985) based metric for
computing similarity of time distributions, and we
score NEs similarity with a linear transliteration
model. For a given NE in one language, the translit-
eration model chooses a top ranked list of candidates
in another language. Time sequence scoring is then
used to re-rank the candidates and choose the one
best temporally aligned with the NE. That is, we at-
tempt to choose a candidate which is both a good
transliteration (according to the current model) and
is well aligned with the NE. Finally, pairs of NEs
 	
  		 
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 299?307,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Unsupervised Constraint Driven Learning For Transliteration Discovery
Ming-Wei Chang Dan Goldwasser Dan Roth Yuancheng Tu
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang21,goldwas1,danr,ytu}@uiuc.edu
Abstract
This paper introduces a novel unsupervised
constraint-driven learning algorithm for iden-
tifying named-entity (NE) transliterations in
bilingual corpora. The proposed method does
not require any annotated data or aligned cor-
pora. Instead, it is bootstrapped using a simple
resource ? a romanization table. We show that
this resource, when used in conjunction with
constraints, can efficiently identify translitera-
tion pairs. We evaluate the proposed method
on transliterating English NEs to three differ-
ent languages - Chinese, Russian and Hebrew.
Our experiments show that constraint driven
learning can significantly outperform existing
unsupervised models and achieve competitive
results to existing supervised models.
1 Introduction
Named entity (NE) transliteration is the process of
transcribing a NE from a source language to some
target language while preserving its pronunciation in
the original language. Automatic NE transliteration
is an important component in many cross-language
applications, such as Cross-Lingual Information Re-
trieval (CLIR) and Machine Translation(MT) (Her-
mjakob et al, 2008; Klementiev and Roth, 2006a;
Meng et al, 2001; Knight and Graehl, 1998).
It might initially seem that transliteration is an
easy task, requiring only finding a phonetic mapping
between character sets. However simply matching
every source language character to its target lan-
guage counterpart is not likely to work well as in
practice this mapping depends on the context the
characters appear in and on transliteration conven-
tions which may change across domains. As a result,
current approaches employ machine learning meth-
ods which, given enough labeled training data learn
how to determine whether a pair of words consti-
tute a transliteration pair. These methods typically
require training data and language-specific expertise
which may not exist for many languages. In this pa-
per we try to overcome these difficulties and show
that when the problem is modeled correctly, a sim-
ple character level mapping is a sufficient resource.
In our experiments, English was used as the
source language, allowing us to use romanization ta-
bles, a resource commonly-available for many lan-
guages1. These tables contain an incomplete map-
ping between character sets, mapping every charac-
ter to its most common counterpart.
Our transliteration model takes a discriminative
approach. Given a word pair, the model determines
if one word is a transliteration of the other. The
features used by this model are character n-gram
matches across the two strings. For example, Fig-
ure 1 describes the decomposition of a word pair into
unigram features as a bipartite graph in which each
edge represents an active feature.
We enhance the initial model with constraints, by
framing the feature extraction process as a struc-
tured prediction problem - given a word pair, the set
of possible active features is defined as a set of latent
binary variables. The contextual dependency be-
1The romanization tables available at the Library of
Congress website (http://www.loc.gov/catdir/cpso/roman.html)
cover more than 150 languages written in various non-Roman
scripts
299
Figure 1: Top: The space of all possible features that can be
generated given the word pair. Bottom: A pruned features rep-
resentation generated by the inference process.
tween features is encoded as a set of constraints over
these variables. Features are extracted by finding
an assignment that maximizes the similarity score
between the two strings and conforms to the con-
straints. The model is bootstrapped using a roman-
ization table and uses a discriminatively self-trained
classifier as a way to improve over several training
iterations. Furthermore, when specific knowledge
about the source and target languages exists, it can
be directly injected into the model as constraints.
We tested our approach on three very differ-
ent languages - Russian, a Slavic language, He-
brew a Semitic language, and Chinese, a Sino-
Tibetan language. In all languages, using this sim-
ple resource in conjunction with constraints pro-
vided us with a robust transliteration system which
significantly outperforms existing unsupervised ap-
proaches and achieves comparable performance to
supervised methods.
The rest of the paper is organized as follows.
Sec. 2 briefly examines more related work. Sec. 3
explains our model and Sec. 4 provide a linguistic
intuition for it. Sec. 5 describes our experiments and
evaluates our results followed by sec. 6 which con-
cludes our paper.
2 Related Works
Transliteration methods typically fall into two cate-
gories: generative approaches (Li et al, 2004; Jung
et al, 2000; Knight and Graehl, 1998) that try to
produce the target transliteration given a source lan-
guage NE, and discriminative approaches (Gold-
wasser and Roth, 2008b; Bergsma and Kondrak,
2007; Sproat et al, 2006; Klementiev and Roth,
2006a), that try to identify the correct translitera-
tion for a word in the source language given several
candidates in the target language. Generative meth-
ods encounter the Out-Of-Vocabulary (OOV) prob-
lem and require substantial amounts of training data
and knowledge of the source and target languages.
Discriminative approaches, when used to for dis-
covering NE in a bilingual corpora avoid the OOV
problem by choosing the transliteration candidates
from the corpora. These methods typically make
very little assumptions about the source and target
languages and require considerably less data to con-
verge. Training the transliteration model is typi-
cally done under supervised settings (Bergsma and
Kondrak, 2007; Goldwasser and Roth, 2008b), or
weakly supervised settings with additional tempo-
ral information (Sproat et al, 2006; Klementiev and
Roth, 2006a). Our work differs from these works
in that it is completely unsupervised and makes no
assumptions about the training data.
Incorporating knowledge encoded as constraints
into learning problems has attracted a lot of atten-
tion in the NLP community recently. This has been
shown both in supervised settings (Roth and Yih,
2004; Riedel and Clarke, 2006) and unsupervised
settings (Haghighi and Klein, 2006; Chang et al,
2007) in which constraints are used to bootstrap the
model. (Chang et al, 2007) describes an unsuper-
vised training of a Constrained Conditional Model
(CCM), a general framework for combining statisti-
cal models with declarative constraints. We extend
this work to include constraints over possible assign-
ments to latent variables which, in turn, define the
underlying representation for the learning problem.
In the transliteration community there are sev-
eral works (Ristad and Yianilos, 1998; Bergsma and
Kondrak, 2007; Goldwasser and Roth, 2008b) that
show how the feature representation of a word pair
can be restricted to facilitate learning a string sim-
ilarity model. We follow the approach discussed
in (Goldwasser and Roth, 2008b), which considers
the feature representation as a structured prediction
problem and finds the set of optimal assignments (or
feature activations), under a set of legitimacy con-
straints. This approach stresses the importance of
interaction between learning and inference, as the
model iteratively uses inference to improve the sam-
ple representation for the learning problem and uses
the learned model to improve the accuracy of the in-
300
ference process. We adapt this approach to unsu-
pervised settings, where iterating over the data im-
proves the model in both of these dimensions.
3 Unsupervised Constraint Driven
Learning
In this section we present our Unsupervised Con-
straint Driven Learning (UCDL) model for discov-
ering transliteration pairs. Our task is in essence a
ranking task. Given a NE in the source language and
a list of candidate transliterations in the target lan-
guage, the model is supposed to rank the candidates
and output the one with the highest score. The model
is bootstrapped using two linguistic resources: a ro-
manization table and a set of general and linguistic
constraints. We use several iterations of self training
to learn the model. The details of the procedure are
explained in Algorithm 1.
In our model features are character pairs (cs, ct),
where cs ? Cs is a source word character and
ct ? Ct is a target word character. The feature
representation of a word pair vs, vt is denoted by
F (vs, vt). Each feature (cs, ct) is assigned a weight
W (cs, ct) ? R. In step 1 of the algorithm we initial-
ize the weights vector using the romanization table.
Given a pair (vs, vt), a feature extraction process
is used to determine the feature based representation
of the pair. Once features are extracted to represent
a pair, the sum of the weights of the extracted fea-
tures is the score assigned to the target translitera-
tion candidate. Unlike traditional feature extraction
approaches, our feature representation function does
not produce a fixed feature representation. In step
2.1, we formalize the feature extraction process as a
constrained optimization problem that captures the
interdependencies between the features used to rep-
resent the sample. That is, obtaining F (vs, vt) re-
quires solving an optimization problem. The techni-
cal details are described in Sec. 3.1. The constraints
we use are described in Sec. 3.2.
In step 2.2 the different candidates for every
source NE are ranked according to the similarity
score associated with their chosen representation.
This ranking is used to ?label? examples for a dis-
criminative learning process that learns increasingly
better weights, and thus improve the representation
of the pair: each source NE paired with its top
ranked transliteration is labeled as a positive exam-
ples (step 2.3) and the rest of the samples are consid-
ered as negative samples. In order to focus the learn-
ing process, we removed from the training set al
negative examples ruled-out by the constraints (step
2.4). As the learning process progresses, the initial
weights are replaced by weights which are discrimi-
natively learned (step 2.5). This process is repeated
several times until the model converges, and repeats
the same ranking over several iterations.
Input: Romanization table T : Cs ? Ct, Constraints
C, Source NEs: Vs, Target words: Vt
1. Initialize Model
Let W : Cs ? Ct ? R be a weight vector.
Initialize W using T by the following procedure
?(cs, ct), (cs, ct) ? T ? W(cs, ct) = 0,
?(cs, ct),?((cs, ct) ? T ) ?W(cs, ct) = ?1,
?cs,W(cs, ) = ?1, ?ct,W( , ct) = ?1.
2. Constraints driven unsupervised training
while not converged do
1. ?vs ? Vs, vt ? Vt, use C and W
to generate a representation F (vs, vt)
2. ?vs ? Vs, find the top ranking transliteration
pair (vs, v?t ) by solving
v?t = argmaxvt score(F (vs, vt)).
3. D = {(+, F (vs, v?t )) | ?vs ? Vs}.
4. ?vs ? Vs, vt ? Vt, if vt 6= v?t and
score(F (vs, vt)) 6= ??, then
D = D ? {(?, F (vs, vt))}.
5. W ? train(D)
end
Algorithm 1: UCDL Transliteration Framework.
In the rest of this section we explain this process
in detail. We define the feature extraction inference
process in Sec. 3.1, the constraints used in Sec. 3.2
and the inference algorithm in Sec. 3.3. The linguis-
tic intuition for our model is described in Sec. 4.
3.1 Finding Feature Representation as
Constrained Optimization
We use the formulation of Constrainted Conditional
Models (CCMs) (Roth and Yih, 2004; Roth and Yih,
2007; Chang et al, 2008). Previous work on CCM
models dependencies between different decisions in
structured prediction problems. Transliteration dis-
covery is a binary classification problem, however,
301
the underlying representation of each sample can be
modeled as a CCM, defined as a set of latent vari-
ables corresponding to the set of all possible features
for a given sample. The dependencies between the
features are captured using constraints.
Given a word pair, the set of all possible features
consists of all character mappings from the source
word to the target word. Since in many cases the
size of the words differ we augment each of the
words with a blank character (denoted as ? ?). We
model character omission by mapping the character
to the blank character. This process is formally de-
fined as an operator mapping a transliteration can-
didate pair to a set of binary variables, denoted as
All-Features (AF ).
AF = {(cs, ct)|cs ? vs ? { }, ct ? vt ? { }}
This representation is depicted at the top of Figure 1.
The initial sample representation (AF ) gener-
ates features by coupling substrings from the two
terms without considering the dependencies be-
tween the possible combinations. This representa-
tion is clearly noisy and in order to improve it we
select a subset F ? AF of the possible features.
The selection process is formulated as a linear op-
timization problem over binary variables encoding
feature activations in AF . Variables assigned 1 are
selected to be in F , and those assigned 0 are not.
The objective function maximized is a linear func-
tion over the variables in AF , each with its weight as
a coefficient, as in the left part of Equation 1 below.
We seek to maximize this linear sum subject to a set
of constraints. These represent the dependencies be-
tween selections and prior knowledge about possible
legitimate character mappings and correspond to the
right side of Equation 1. In our settings only hard
constraints are used and therefore the penalty (?) for
violating any of the constraints is set to ?. The spe-
cific constraints used are discussed in Sec. 3.2. The
score of the mapping F (vs, vt) can be written as fol-
lows:
1
|vt|
(W ? F (vs, vt)?
?
ci?C
?ci(F (vs, vt)) (1)
We normalize this score by dividing it by the size of
the target word, since the size of candidates varies,
normalization improved the ranking of candidates.
The result of the optimization process is a set F of
active features, defined in Equation 2. The result of
this process is described at the bottom of Figure 1.
F ?(vs, vt) = argmaxF?AF (vs,vt)score(F ). (2)
The ranking process done by our model can now be
naturally defined - given a source word vs, and a
set of candidates target words v0t , . . . , vnt , find the
candidate whose optimal representation maximizes
Equation 1. This process is defined in Equation 3.
v?t = argmaxvit
score(F (vs, vit)). (3)
3.2 Incorporating Mapping Constraints
We consider two types of constraints: language spe-
cific and general constraints that apply to all lan-
guages. Language specific constraints typically im-
pose a local restriction such as individually forcing
some of the possible character mapping decisions.
The linguistic intuition behind these constraints is
discussed in Section 4. General constraints encode
global restrictions, capturing the dependencies be-
tween different mapping decisions.
General constraints: To facilitate readability we
denote the feature mapping the i-th source word
character to the j-th target word character as a
Boolean variable aij that is 1 if that feature is active
and 0 otherwise.
? Coverage - Every character must be mapped
only to a single character or to the blank char-
acter. We can formulate this as: ?j aij = 1
and ?i aij = 1.
? No Crossing - Every character mapping, except
mapping to blank character, should preserve the
order of appearance in the source and target
words, or formally - ?i, j s.t. aij = 1 ? ?l <
i, ?k > j, alk = 0. Another constraint is ?i, j
s.t. aij = 1 ? ?l > i, ?k < j, alk = 0.
Language specific constraints
? Restricted Mapping: These constraints restrict
the possible local mappings between source
and target language characters. We maintain a
set of possible mappings {cs ? ?cs}, where
?cs ? Ct and {ct ? ?ct}, where ?ct ? Cs.
Any feature (cs, ct) such that cs /? ?ct or
ct /? ?cs is penalized in our model.
302
? Length restriction: An additional constraint
restricts the size difference between the two
words. We formulate this as follows: ?vs ?
Vs,?vt ? Vt, if ?|vt| > |vs| and ?|vs| > |vt|,
score(F (vs, vt)) = ??. Although ? can
take different values for different languages, we
simply set ? to 2 in this paper.
In addition to biasing the model to choose the
right candidate, the constraints also provide a com-
putational advantage: a given a word pair is elimi-
nated from consideration when the length restriction
is not satisfied or there is no way to satisfy the re-
stricted mapping constraints.
3.3 Inference
The optimization problem defined in Equation 2 is
an integer linear program (ILP). However, given
the structure of the problem it is possible to de-
velop an efficient dynamic programming algorithm
for it, based on the algorithm for finding the mini-
mal edit distance of two strings. The complexity of
finding the optimal set of features is only quadratic
in the size of the input pair, a clear improvement
over the ILP exponential time algorithm. The al-
gorithm minimizes the weighted edit distance be-
tween the strings, and produces a character align-
ment that satisfies the general constraints (Sec. 3.2).
Our modifications are only concerned with incorpo-
rating the language-specific constraints into the al-
gorithm. This can be done simply by assigning a
negative infinity score to any alignment decision not
satisfying these constraints.
4 Bootstrapping with Linguistic
Information
Our model is bootstrapped using two resources - a
romanization table and mapping constraints. Both
resources capture the same information - character
mapping between languages. The distinction be-
tween the two represents the difference in the con-
fidence we have in these resources - the romaniza-
tion table is a noisy mapping covering the character
set and is therefore better suited as a feature. Con-
straints, represented by pervasive, correct character
mapping, indicate the sound mapping tendency be-
tween source and target languages. For example,
certain n-gram phonemic mappings, such as r ? l
from English to Chinese, are language specific and
can be captured by language specific sound change
patterns.
Phonemes Constraints
Vowel i ? y; u ? w; a ? a
Nasal m ? m; m,n ? m
Approximant
r ? l; l, r ? l
l ? l; w ? h,w, f
h, o, u, v ? w; y ? y
Fricative v ? w, b, fs ? s, x, z; s, c ? s
Plosive
p ? b, p; p ? p
b ? b; t ? t
t, d ? d; q ? k
Table 1: All language specific constraints used in our English
to Chinese transliteration (see Sec. 3.2 for more details). Con-
straints in boldface apply to all positions, the rest apply only to
characters appearing in initial position.
These patterns have been used by other systems
as features or pseudofeatures (Yoon et al, 2007).
However, in our system these language specific rule-
of-thumbs are systematically used as constraints to
exclude impossible alignments and therefore gener-
ate better features for learning. We listed in Table 1
all 20 language specific constraints we used for Chi-
nese. There is a total of 24 constraints for Hebrew
and 17 for Russian.
The constraints in Table 1 indicate a systematic
sound mapping between English and Chinese un-
igram character mappings. Arranged by manners
of articulation each row of the table indicates the
sound change tendency among vowels, nasals, ap-
proximants (retroflex and glides), fricatives and plo-
sives. For example, voiceless plosive sounds such as
p, t in English, tend to map to both voiced (such as b,
d) and voiceless sounds in Chinese. However, if the
sound is voiceless in Chinese, its backtrack English
sound must be voiceless. This voice-voiceless sound
change tendency is captured by our constraints such
as p ? b, p and p ? p; t ? t.
5 Experiments and Analysis
In this section, we demonstrate the effectiveness
of constraint driven learning empirically. We start
by describing the datasets and experimental settings
and then proceed to describe the results. We eval-
uated our method on three very different target lan-
303
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  2  4  6  8  10  12  14  16  18  20
A
C
C
Number of Rounds
[KR 06] + temporal information[KR 06] All Cons. + unsupervsied learning
Figure 2: Comparison between our models and weakly su-
pervised learning methods (Klementiev and Roth, 2006b).
Note that one of the models proposed in (Klementiev and Roth,
2006b) takes advantage of the temporal information. Our best
model, the unsupervised learning with all constraints, outper-
forms both models in (Klementiev and Roth, 2006b), even
though we do not use any temporal information.
guages: Russian, Chinese, and Hebrew, and com-
pared our results to previously published results.
5.1 Experimental Settings
In our experiments the system is evaluated on its
ability to correctly identify the gold transliteration
for each source word. We evaluated the system?s
performance using two measures adopted in many
transliteration works. The first one is Mean Recip-
rocal Rank (MRR), used in (Tao et al, 2006; Sproat
et al, 2006), which is the average of the multiplica-
tive inverse of the rank of the correct answer. For-
mally, Let n be the number of source NEs. Let Gol-
dRank(i) be the rank the algorithm assigns to the
correct transliteration. Then, MRR is defined by:
MRR = 1n
n?
i=1
1
goldRank(i) .
Another measure is Accuracy (ACC) used in (Kle-
mentiev and Roth, 2006a; Goldwasser and Roth,
2008a), which is the percentage of the top rank can-
didates being the gold transliteration. In our im-
plementation we used the support vector machine
(SVM) learning algorithm with linear kernel as our
underlying learning algorithm (mentioned in part
2.5 of Algorithm 1) . We used the package LIB-
LINEAR (Hsieh et al, 2008) in our experiments.
Through all of our experiments, we used the 2-norm
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 0  2  4  6  8  10  12  14  16  18  20
M
R
R
Number of Rounds
[GR 08] 250 labeled ex. with cons[GR 08] 250 labeled ex. w/o consGeneral cons + unsupervised learningAll cons. + unsupervised learning
Figure 3: Comparison between our works and supervised
models in (Goldwasser and Roth, 2008b). We show the learn-
ing curves for Hebrew under two different settings: unsuper-
vised learning with general and all constraints. The results of
two supervised models (Goldwasser and Roth, 2008b) are also
included here. Note that our unsupervised model with all con-
straints is competitive to the supervised model with 250 labeled
examples. See the text for more comparisons and details.
hinge loss as our loss function and fixed the regular-
ization parameter C to be 0.5.
5.2 Datasets
We experimented using three different target lan-
guages Russian, Chinese, and Hebrew. We used En-
glish as the source language in all these experiments.
The Russian data set2, originally introduced in
(Klementiev and Roth, 2006b), is comprised of tem-
porally aligned news articles. The dataset contains
727 single word English NEs with a correspond-
ing set of 50,648 potential Russian candidate words
which include not only name entities, but also other
words appearing in the news articles.
The Chinese dataset is taken directly from an
English-Chinese transliteration dictionary, derived
from LDC Gigaword corpus3. The entire dictionary
consists of 74,396 pairs of English-Chinese NEs,
where Chinese NEs are written in Pinyin, a roman-
ized spelling system of Chinese. In (Tao et al, 2006)
a dataset which contains about 600 English NEs and
700 Chinese candidates is used. Since the dataset
is not publicly available, we created a dataset in a
similar way. We randomly selected approximately
600 NE pairs and then added about 100 candidates
which do not correspond to any of the English NE
2The corpus is available http://L2R.cs.uiuc.edu/?cogcomp.
3http://www.ldc.upenn.edu
304
Language UCDL Prev. works
Rus. (ACC) 73 63 (41) (KR?06)
Heb. (MRR) 0.899 0.894 (GR?08)
Table 2: Comparison to previously published results. UCDL
is our method, KR?06 is described in (Klementiev and Roth,
2006b) and GR?08 in (Goldwasser and Roth, 2008b). Note that
our results for Hebrew are comparable with a supervised sys-
tem.
previously selected.
The Hebrew dataset, originally introduced in
(Goldwasser and Roth, 2008a), consists of 300
English-Hebrew pairs extracted from Wikipedia.
5.3 Results
We begin by comparing our model to previously
published models tested over the same data, in two
different languages, Russian and Hebrew. For Rus-
sian, we compare to the model presented in (Kle-
mentiev and Roth, 2006b), a weakly supervised al-
gorithm that uses both phonetic information and
temporal information. The model is bootstrapped
using a set of 20 labeled examples. In their setting
the candidates are ranked by combining two scores,
one obtained using the transliteration model and a
second by comparing the relative occurrence fre-
quency of terms over time in both languages. Due
to computational tractability reasons we slightly
changed Algorithm 1 to use only a small subset of
the possible negative examples.
For Hebrew, we compare to the model presented
in (Goldwasser and Roth, 2008b), a supervised
model trained using 250 labeled examples. This
model uses a bigram model to represent the translit-
eration samples (i.e., features are generated by pair-
ing character unigrams and bigrams). The model
also uses constraints to restrict the feature extrac-
tion process, which are equivalent to the coverage
constraint we described in Sec. 3.2.
The results of these experiments are reported us-
ing the evaluation measures used in the original pa-
pers and are summarized in Table 2. The results
show a significant improvement over the Russian
data set and comparable performance to the super-
vised method used for Hebrew.
Figure 2 describes the learning curve of our
method over the Russian dataset. We compared our
algorithm to two models described in (Klementiev
and Roth, 2006b) - one uses only phonetic simi-
larity and the second also considers temporal co-
occurrence similarity when ranking the translitera-
tion candidates. Both models converge after 50 it-
erations. When comparing our model to their mod-
els, we found that even though our model ignores
the temporal information it achieves better results
and converges after fewer iterations. Their results
report a significant improvement when using tempo-
ral information - improving an ACC score of 41%
without temporal information to 63% when using
it. Since the temporal information is orthogonal to
the transliteration model, our model should similarly
benefit from incorporating the temporal information.
Figure 3 compares the learning curve of our
method to an existing supervised method over the
Hebrew data and shows we get comparable results.
Unfortunately, we could not find a published Chi-
nese dataset. However, our system achieved similar
results to other systems, over a different dataset with
similar number of training examples. For example,
(Sproat et al, 2006) presents a supervised system
that achieves a MRR score of 0.89, when evaluated
over a dataset consisting of 400 English NE and 627
Chinese words. Our results for a different dataset of
similar size are reported in Table 3.
5.4 Analysis
The resources used in our framework consist of
- a romanization table, general and language spe-
cific transliteration constraints. To reveal the impact
of each component we experimented with different
combination of the components, resulting in three
different testing configurations.
Romanization Table: We initialized the weight
vector using a romanization table and did not use any
constraints. To generate features we use a modified
version of our AF operator (see Sec. 3), which gen-
erates features by coupling characters in close posi-
tions in the source and target words. This configura-
tion is equivalent to the model used in (Klementiev
and Roth, 2006b).
+General Constraints: This configuration uses the
romanization table for initializing the weight vector
and general transliteration constraints (see Sec. 3.2)
for feature extraction.
+All Constraints: This configuration uses lan-
guage specific constraints in addition to the gen-
305
Settings Chinese Russian Hebrew
Romanization table 0.019 (0.5) 0.034 (1.0) 0.046 (1.7)
Romanization table +learning 0.020 (0.3) 0.048 (1.3) 0.028 (0.7)
+Gen Constraints 0.746 (67.1) 0.809 (74.3) 0.533 (45.0)
+Gen Constraints +learning 0.867 (82.2) 0.906 (86.7) 0.834 (76.0)
+All Constraints 0.801 (73.4) 0.849 (79.3) 0.743 (66.0)
+All Constraints +learning 0.889 (84.7) 0.931 (90.0) 0.899 (85.0)
Table 3: Results of an ablation study of unsupervised method for three target languages. Results for ACC are inside parentheses,
and for MRR outside. When the learning algorithm is used, the results after 20 rounds of constraint driven learning are reported.
Note that using linguistic constraints has a significant impact in the English-Hebrew experiments. Our results show that a small
amount of constraints can go a long way, and better constraints lead to better learning performance.
eral transliteration constraints to generate the feature
representation. (see Sec. 4).
+Learning: Indicates that after initializing the
weight vector, we update the weight using Algo-
rithm 1. In all of the experiments, we report the
results after 20 training iterations.
The results are summarized in Table 3. Due to the
size of the Russian dataset, we used a subset consist-
ing of 300 English NEs and their matching Russian
transliterations for the analysis presented here. Af-
ter observing the results, we discovered the follow-
ing regularities for all three languages. Using the
romanization table directly without constraints re-
sults in very poor performance, even after learning.
This can be used as an indication of the difficulty of
the transliteration problem and the difficulties ear-
lier works have had when using only romanization
tables, however, when used in conjunction with con-
straints results improve dramatically. For example,
in the English-Chinese data set, we improve MRR
from 0.02 to 0.746 and for the English-Russian data
set we improve 0.03 to 0.8. Interestingly, the results
for the English-Hebrew data set are lower than for
other languages - we achieve 0.53 MRR in this set-
ting. We attribute the difference to the quality of
the mapping in the romanization table for that lan-
guage. Indeed, the weights learned after 20 train-
ing iterations improve the results to 0.83. This im-
provement is consistent across all languages, after
learning we are able to achieve a MRR score of 0.87
for the English-Chinese data set and 0.91 for the
English-Russian data set. These results show that
romanization table contains enough information to
bootstrap the model when used in conjunction with
constraints. We are able to achieve results compa-
rable to supervised methods that use a similar set of
constraints and labeled examples.
Bootstrapping the weight vector using language
specific constraints can further improve the results.
They provide several advantages: a better starting
point, an improved learning rate and a better final
model. This is clear in all three languages, for exam-
ple results for the Russian and Chinese bootstrapped
models improve by 5%, and by over 20% for He-
brew. After training the difference is smaller- only
3% for the first two and 6% for Hebrew. Figure 3 de-
scribes the learning curve for models with and with-
out language specific constraints for the English-
Hebrew data set, it can be observed that using these
constraints the model converges faster and achieves
better results.
6 Conclusion
In this paper we develop a constraints driven ap-
proach to named entity transliteration. In doing it
we show that romanization tables are a very useful
resource for transliteration discovery if proper con-
straints are included. Our framework does not need
labeled data and does not assume that bilingual cor-
pus are temporally aligned. Even without using any
labeled data, our model is competitive to existing
supervised models and outperforms existing weakly
supervised models.
7 Acknowledgments
We wish to thank the reviewers for their insightful
comments. This work is partly supported by NSF
grant SoD-HCER-0613885 and DARPA funding un-
der the Bootstrap Learning Program.
306
References
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 656?663, Prague, Czech Republic,
June. Association for Computational Linguistics.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Proc.
of the Annual Meeting of the Association of Compu-
tational Linguistics (ACL), pages 280?287, Prague,
Czech Republic, Jun. Association for Computational
Linguistics.
M. Chang, L. Ratinov, N. Rizzolo, and D. Roth. 2008.
Learning and inference with constraints. In Proc.
of the National Conference on Artificial Intelligence
(AAAI), July.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), June.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In Proc. of the Conference
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 353?362, Oct.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proc. of the Annual Meet-
ing of the North American Association of Computa-
tional Linguistics (NAACL).
U. Hermjakob, K. Knight, and H. Daume? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 389?397, Columbus, Ohio, June.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In ICML
?08: Proceedings of the 25th international conference
on Machine learning, pages 408?415, New York, NY,
USA. ACM.
S. Jung, S. Hong, and E. Paek. 2000. An english to
korean transliteration model of extended markov win-
dow. In Proc. the International Conference on Com-
putational Linguistics (COLING), pages 383?389.
A. Klementiev and D. Roth. 2006a. Named entity
transliteration and discovery from multilingual com-
parable corpora. In Proc. of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL), pages 82?88, June.
A. Klementiev and D. Roth. 2006b. Weakly supervised
named entity transliteration and discovery from mul-
tilingual comparable corpora. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages USS,TL,ADAPT, July.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, pages 599?612.
H. Li, M. Zhang, and J. Su. 2004. A joint source-channel
model for machine transliteration. In Proc. of the An-
nual Meeting of the Association of Computational Lin-
guistics (ACL), pages 159?166, Barcelona, Spain, July.
H. Meng, W. Lo, B. Chen, and K. Tang. 2001.
Generating phonetic cognates to handle named en-
tities in english-chinese cross-langauge spoken doc-
ument retreival. In Proceedings of the Automatic
Speech Recognition and Understanding Workshop,
pages 389?397.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. of the Conference on Empirical Methods for
Natural Language Processing (EMNLP), pages 129?
137, Sydney, Australia.
E. S. Ristad and P. N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Recogni-
tion and Machine Intelligence, 20(5):522?532, May.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks.
pages 1?8. Association for Computational Linguistics.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
R. Sproat, T. Tao, and C. Zhai. 2006. Named entity
transliteration with comparable corpora. In Proc. of
the Annual Meeting of the Association of Computa-
tional Linguistics (ACL), pages 73?80, Sydney, Aus-
tralia, July.
T. Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai. 2006.
Unsupervised named entitly transliteration using tem-
poral and phonetic correlation. In Proc. of the Con-
ference on Empirical Methods for Natural Language
Processing (EMNLP), pages 250?257.
S. Yoon, K. Kim, and R. Sproat. 2007. Multilingual
transliteration using feature based phonetic method.
In Proc. of the Annual Meeting of the Association
of Computational Linguistics (ACL), pages 112?119,
Prague, Czech Republic, June.
307
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 817?824,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Weakly Supervised Named Entity Transliteration and Discovery from
Multilingual Comparable Corpora
Alexandre Klementiev Dan Roth
Dept. of Computer Science
University of Illinois
Urbana, IL 61801
 
klementi,danr  @uiuc.edu
Abstract
Named Entity recognition (NER) is an
important part of many natural language
processing tasks. Current approaches of-
ten employ machine learning techniques
and require supervised data. However,
many languages lack such resources. This
paper presents an (almost) unsupervised
learning algorithm for automatic discov-
ery of Named Entities (NEs) in a resource
free language, given a bilingual corpora in
which it is weakly temporally aligned with
a resource rich language. NEs have similar
time distributions across such corpora, and
often some of the tokens in a multi-word
NE are transliterated. We develop an algo-
rithm that exploits both observations itera-
tively. The algorithm makes use of a new,
frequency based, metric for time distribu-
tions and a resource free discriminative ap-
proach to transliteration. Seeded with a
small number of transliteration pairs, our
algorithm discovers multi-word NEs, and
takes advantage of a dictionary (if one ex-
ists) to account for translated or partially
translated NEs. We evaluate the algorithm
on an English-Russian corpus, and show
high level of NEs discovery in Russian.
1 Introduction
Named Entity recognition has been getting much
attention in NLP research in recent years, since it
is seen as significant component of higher level
NLP tasks such as information distillation and
question answering. Most successful approaches
to NER employ machine learning techniques,
which require supervised training data. However,
for many languages, these resources do not ex-
ist. Moreover, it is often difficult to find experts
in these languages both for the expensive anno-
tation effort and even for language specific clues.
On the other hand, comparable multilingual data
(such as multilingual news streams) are becoming
increasingly available (see section 4).
In this work, we make two independent obser-
vations about Named Entities encountered in such
corpora, and use them to develop an algorithm that
extracts pairs of NEs across languages. Specifi-
cally, given a bilingual corpora that is weakly tem-
porally aligned, and a capability to annotate the
text in one of the languages with NEs, our algo-
rithm identifies the corresponding NEs in the sec-
ond language text, and annotates them with the ap-
propriate type, as in the source text.
The first observation is that NEs in one language
in such corpora tend to co-occur with their coun-
terparts in the other. E.g., Figure 1 shows a his-
togram of the number of occurrences of the word
Hussein and its Russian transliteration in our bilin-
gual news corpus spanning years 2001 through
late 2005. One can see several common peaks
in the two histograms, largest one being around
the time of the beginning of the war in Iraq. The
word Russia, on the other hand, has a distinctly
different temporal signature. We can exploit such
weak synchronicity of NEs across languages to
associate them. In order to score a pair of enti-
ties across languages, we compute the similarity
of their time distributions.
The second observation is that NEs often con-
tain or are entirely made up of words that are pho-
netically transliterated or have a common etymo-
logical origin across languages (e.g. parliament in
English and 	
 , its Russian translation),
and thus are phonetically similar. Figure 2 shows
817
 0
 5
 10
 15
 20
?hussein? (English)
 0
 5
 10
 15
 2
?hussein? (Russian)
 0
 5
 10
 15
 2
01/01/01 10/05/05
N
um
be
r o
f O
cc
ur
en
ce
s
Time
?russia? (English)
Figure 1: Temporal histograms for Hussein (top),
its Russian transliteration (middle), and of the
word Russia (bottom).
an example list of NEs and their possible Russian
transliterations.
Approaches that attempt to use these two
characteristics separately to identify NEs across
languages would have significant shortcomings.
Transliteration based approaches require a good
model, typically handcrafted or trained on a clean
set of transliteration pairs. On the other hand, time
sequence similarity based approaches would in-
correctly match words which happen to have sim-
ilar time signatures (e.g., Taliban and Afghanistan
in recent news).
We introduce an algorithm we call co-ranking
which exploits these observations simultaneously
to match NEs on one side of the bilingual cor-
pus to their counterparts on the other. We use a
Discrete Fourier Transform (Arfken, 1985) based
metric for computing similarity of time distribu-
tions, and show that it has significant advantages
over other metrics traditionally used. We score
NEs similarity with a linear transliteration model.
We first train a transliteration model on single-
word NEs. During training, for a given NE in one
language, the current model chooses a list of top
ranked transliteration candidates in another lan-
guage. Time sequence scoring is then used to re-
rank the list and choose the candidate best tem-
porally aligned with the NE. Pairs of NEs and the
best candidates are then used to iteratively train the
 	
  		 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 65?72,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Pipeline Framework for Dependency Parsing
Ming-Wei Chang Quang Do Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{mchang21, quangdo2, danr}@uiuc.edu
Abstract
Pipeline computation, in which a task is
decomposed into several stages that are
solved sequentially, is a common compu-
tational strategy in natural language pro-
cessing. The key problem of this model
is that it results in error accumulation and
suffers from its inability to correct mis-
takes in previous stages. We develop
a framework for decisions made via in
pipeline models, which addresses these
difficulties, and presents and evaluates it
in the context of bottom up dependency
parsing for English. We show improve-
ments in the accuracy of the inferred trees
relative to existing models. Interestingly,
the proposed algorithm shines especially
when evaluated globally, at a sentence
level, where our results are significantly
better than those of existing approaches.
1 Introduction
A pipeline process over the decisions of learned
classifiers is a common computational strategy in
natural language processing. In this model a task
is decomposed into several stages that are solved
sequentially, where the computation in the ith
stage typically depends on the outcome of com-
putations done in previous stages. For example,
a semantic role labeling program (Punyakanok et
al., 2005) may start by using a part-of-speech tag-
ger, then apply a shallow parser to chunk the sen-
tence into phrases, identify predicates and argu-
ments and then classify them to types. In fact,
any left to right processing of an English sentence
may be viewed as a pipeline computation as it pro-
cesses a token and, potentially, makes use of this
result when processing the token to the right.
The pipeline model is a standard model of
computation in natural language processing for
good reasons. It is based on the assumption that
some decisions might be easier or more reliable
than others, and their outcomes, therefore, can be
counted on when making further decisions. Nev-
ertheless, it is clear that it results in error accu-
mulation and suffers from its inability to correct
mistakes in previous stages. Researchers have re-
cently started to address some of the disadvantages
of this model. E.g., (Roth and Yih, 2004) suggests
a model in which global constraints are taken into
account in a later stage to fix mistakes due to the
pipeline. (Punyakanok et al, 2005; Marciniak
and Strube, 2005) also address some aspects of
this problem. However, these solutions rely on the
fact that all decisions are made with respect to the
same input; specifically, all classifiers considered
use the same examples as their input. In addition,
the pipelines they study are shallow.
This paper develops a general framework for
decisions in pipeline models which addresses
these difficulties. Specifically, we are interested
in deep pipelines ? a large number of predictions
that are being chained.
A pipeline process is one in which decisions
made in the ith stage (1) depend on earlier deci-
sions and (2) feed on input that depends on earlier
decisions. The latter issue is especially important
at evaluation time since, at training time, a gold
standard data set might be used to avoid this issue.
We develop and study the framework in the con-
text of a bottom up approach to dependency pars-
ing. We suggest that two principles to guide the
pipeline algorithm development:
(i) Make local decisions as reliable as possible.
(ii) Reduce the number of decisions made.
Using these as guidelines we devise an algo-
65
rithm for dependency parsing, prove that it satis-
fies these principles, and show experimentally that
this improves the accuracy of the resulting tree.
Specifically, our approach is based on a shift-
reduced parsing as in (Yamada and Matsumoto,
2003). Our general framework provides insights
that allow us to improve their algorithm, and to
principally justify some of the algorithmic deci-
sions. Specifically, the first principle suggests to
improve the reliability of the local predictions,
which we do by improving the set of actions taken
by the parsing algorithm, and by using a look-
ahead search. The second principle is used to jus-
tify the control policy of the parsing algorithm ?
which edges to consider at any point of time. We
prove that our control policy is optimal in some
sense, and that the decisions we made, guided by
these, principles lead to a significant improvement
in the accuracy of the resulting parse tree.
1.1 Dependency Parsing and Pipeline Models
Dependency trees provide a syntactic reresenta-
tion that encodes functional relationships between
words; it is relatively independent of the grammar
theory and can be used to represent the structure
of sentences in different languages. Dependency
structures are more efficient to parse (Eisner,
1996) and are believed to be easier to learn, yet
they still capture much of the predicate-argument
information needed in applications (Haghighi et
al., 2005), which is one reason for the recent in-
terest in learning these structures (Eisner, 1996;
McDonald et al, 2005; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004).
Eisner?s work ? O(n3) parsing time generative
algorithm ? embarked the interest in this area.
His model, however, seems to be limited when
dealing with complex and long sentences. (Mc-
Donald et al, 2005) build on this work, and use
a global discriminative training approach to im-
prove the edges? scores, along with Eisner?s algo-
rithm, to yield the expected improvement. A dif-
ferent approach was studied by (Yamada and Mat-
sumoto, 2003), that develop a bottom-up approach
and learn the parsing decisions between consecu-
tive words in the sentence. Local actions are used
to generate a dependency tree using a shift-reduce
parsing approach (Aho et al, 1986). This is a
true pipeline approach, as was done in other suc-
cessful parsers, e.g. (Ratnaparkhi, 1997), in that
the classifiers are trained on individual decisions
rather than on the overall quality of the parser, and
chained to yield the global structure. Clearly, it
suffers from the limitations of pipeline process-
ing, such as accumulation of errors, but neverthe-
less, yields very competitive parsing results. A
somewhat similar approach was used in (Nivre and
Scholz, 2004) to develop a hybrid bottom-up/top-
down approach; there, the edges are also labeled
with semantic types, yielding lower accuracy than
the works mentioned above.
The overall goal of dependency parsing (DP)
learning is to infer a tree structure. A common
way to do that is to predict with respect to each
potential edge (i, j) in the tree, and then choose a
global structure that (1) is a tree and that (2) max-
imizes some score. In the context of DPs, this
?edge based factorization method? was proposed
by (Eisner, 1996). In other contexts, this is similar
to the approach of (Roth and Yih, 2004) in that
scoring each edge depends only on the raw data
observed and not on the classifications of other
edges, and that global considerations can be used
to overwrite the local (edge-based) decisions.
On the other hand, the key in a pipeline model
is that making a decision with respect to the edge
(i, j) may gain from taking into account deci-
sions already made with respect to neighboring
edges. However, given that these decisions are
noisy, there is a need to devise policies for reduc-
ing the number of predictions in order to make the
parser more robust. This is exemplified in (Ya-
mada and Matsumoto, 2003) ? a bottom-up ap-
proach, that is most related to the work presented
here. Their model is a ?traditional? pipeline model
? a classifier suggests a decision that, once taken,
determines the next action to be taken (as well as
the input the next action observes).
In the rest of this paper, we propose and jus-
tify a framework for improving pipeline process-
ing based on the principles mentioned above: (i)
make local decisions as reliably as possible, and
(ii) reduce the number of decisions made. We
use the proposed principles to examine the (Ya-
mada and Matsumoto, 2003) parsing algorithm
and show that this results in modifying some of
the decisions made there and, consequently, better
overall dependency trees.
2 Efficient Dependency Parsing
This section describes our DP algorithm and jus-
tifies its advantages as a pipeline model. We pro-
66
pose an improved pipeline framework based on the
mentioned principles.
For many languages such as English, Chinese
and Japanese (with a few exceptions), projective
dependency trees (that is, DPs without edge cross-
ings) are sufficient to analyze most sentences. Our
work is therefore concerned only with projective
trees, which we define below.
For words x, y in the sentence T we introduce
the following notations:
x ? y: x is the direct parent of y.
x ?? y: x is an ancestor of y;
x ? y: x ? y or y ? x.
x < y: x is to the left of y in T .
Definition 1 (Projective Language) (Nivre,
2003) ?a, b, c ? T, a ? b and a < c < b imply
that a ?? c or b ?? c.
2.1 A Pipeline DP Algorithm
Our parsing algorithm is a modified shift-reduce
parser that makes use of the actions described be-
low and applies them in a left to right manner
on consecutive pairs of words (a, b) (a < b) in
the sentence. This is a bottom-up approach that
uses machine learning algorithms to learn the pars-
ing decisions (actions) between consecutive words
in the sentences. The basic actions used in this
model, as in (Yamada and Matsumoto, 2003), are:
Shift: there is no relation between a and b, or
the action is deferred because the relationship be-
tween a and b cannot be determined at this point.
Right: b is the parent of a,
Left: a is the parent of b.
This is a true pipeline approach in that the clas-
sifiers are trained on individual decisions rather
than on the overall quality of the parsing, and
chained to yield the global structure. And, clearly,
decisions make with respect to a pair of words af-
fect what is considered next by the algorithm.
In order to complete the description of the algo-
rithm we need to describe which edge to consider
once an action is taken. We describe it via the no-
tion of the focus point: when the algorithm con-
siders the pair (a, b), a < b, we call the word a the
current focus point.
Next we describe several policies for determin-
ing the focus point of the algorithm following an
action. We note that, with a few exceptions, de-
termining the focus point does not affect the cor-
rectness of the algorithm. It is easy to show that
for (almost) any focus point chosen, if the correct
action is selected for the corresponding edge, the
algorithm will eventually yield the correct tree (but
may require multiple cycles through the sentence).
In practice, the actions selected are noisy, and a
wasteful focus point policy will result in a large
number of actions, and thus in error accumulation.
To minimize the number of actions taken, we want
to find a good focus point placement policy.
After S, the focus point always moves one word
to the right. After L or R there are there natural
placement policies to consider:
Start Over: Move focus to the first word in T .
Stay: Move focus to the next word to the right.
That is, for T = (a, b, c), and focus being a, an
L action will result is the focus being a, while R
action results in the focus being b.
Step Back: The focus moves to the previous word
(on the left). That is, for T = (a, b, c), and focus
being b, in both cases, a will be the focus point.
In practice, different placement policies have a
significant effect on the number of pairs consid-
ered by the algorithm and, therefore, on the fi-
nal accuracy1. The following analysis justifies the
Step Back policy. We claim that if Step Back
is used, the algorithm will not waste any action.
Thus, it achieves the goal of minimizing the num-
ber of actions in pipeline algorithms. Notice that
using this policy, when L is taken, the pair (a, b) is
reconsidered, but with new information, since now
it is known that c is the child of b. Although this
seems wasteful, we will show this is a necessary
movement to reduce the number of actions.
As mentioned above, each of these policies
yields the correct tree. Table 1 compares the three
policies in terms of the number of actions required
to build a tree.
Policy #Shift #Left #Right
Start over 156545 26351 27918
Stay 117819 26351 27918
Step back 43374 26351 27918
Table 1: The number of actions required to build
all the trees for the sentences in section 23 of Penn
Treebank (Marcus et al, 1993) as a function of
the focus point placement policy. The statistics are
taken with the correct (gold-standard) actions.
It is clear from Table 1 that the policies result
1Note that (Yamada and Matsumoto, 2003) mention that
they move the focus point back after R, but do not state what
they do after executing L actions, and why. (Yamada, 2006)
indicates that they also move focus point back after L.
67
Algorithm 2 Pseudo Code of the dependency
parsing algorithm. getFeatures extracts the fea-
tures describing the word pair currently consid-
ered; getAction determines the appropriate action
for the pair; assignParent assigns a parent for the
child word based on the action; and deleteWord
deletes the child word in T at the focus once the
action is taken.Let t represents for a word token
For sentence T = {t1, t2, . . . , tn}
focus= 1
while focus< |T | do
~v = getFeatures(tfocus, tfocus+1)
? = getAction(tfocus, tfocus+1, ~v)if ? = L or ? = R then
assignParent(tfocus, tfocus+1, ?)
deleteWord(T, focus, ?)
// performing Step Back here
focus = focus ? 1
else
focus = focus + 1
end if
end while
in very different number of actions and that Step
Back is the best choice. Note that, since the ac-
tions are the gold-standard actions, the policy af-
fects only the number of S actions used, and not
the L and R actions, which are a direct function
of the correct tree. The number of required ac-
tions in the testing stage shows the same trend and
the Step Back also gives the best dependency ac-
curacy. Algorithm 2 depicts the parsing algorithm.
2.2 Correctness and Pipeline Properties
We can prove two properties of our algorithm.
First we show that the algorithm builds the de-
pendency tree in only one pass over the sentence.
Then, we show that the algorithm does not waste
actions in the sense that it never considers a word
pair twice in the same situation. Consequently,
this shows that under the assumption of a perfect
action predictor, our algorithm makes the smallest
possible number of actions, among all algorithms
that build a tree sequentially in one pass.
Note that this may not be true if the action clas-
sifier is not perfect, and one can contrive examples
in which an algorithm that makes several passes on
a sentence can actually make fewer actions than a
single pass algorithm. In practice, however, as our
experimental data shows, this is unlikely.
Lemma 1 A dependency parsing algorithm that
uses the Step Back policy completes the tree when
it reaches the end of the sentence for the rst time.
In order to prove the algorithm we need the fol-
lowing definition. We call a pair of words (a, b) a
free pair if and only if there is a relation between
a and b and the algorithm can perform L or R ac-
tions on that pair when it is considered. Formally,
Definition 2 (free pair) A pair (a, b) considered
by the algorithm is a free pair, if it satises the
following conditions:
1. a ? b
2. a, b are consecutive in T (not necessary in
the original sentence).
3. No other word in T is the child of a or b. (a
and b are now part of a complete subtree.)
Proof. : It is easy to see that there is at least one
free pair in T , with |T | > 1. The reason is that
if no such pair exists, there must be three words
{a, b, c} s.t. a ? b, a < c < b and ?(a ? c ?
b ? c). However, this violates the properties of a
projective language.
Assume {a, b, d} are three consecutive words in
T . Now, we claim that when using Step Back, the
focus point is always to the left of all free pairs in
T . This is clearly true when the algorithm starts.
Assume that (a, b) is the first free pair in T and let
c be just to the left of a and b. Then, the algorithm
will not make a L or R action before the focus
point meets (a, b), and will make one of these ac-
tions then. It?s possible that (c, a ? b) becomes a
free pair after removing a or b in T so we need
to move the focus point back. However, we also
know that there is no free pair to the left of c.
Therefore, during the algorithm, the focus point
will always remain to the left of all free pairs. So,
when we reach the end of the sentence, every free
pair in the sentence has been taken care of, and the
sentence has been completely parsed. 2
Lemma 2 All actions made by a dependency
parsing algorithm that uses the Step Back policy
are necessary.
Proof. : We will show that a pair (a, b) will never
be considered again given the same situation, that
is, when there is no additional information about
relations a or b participate in. Note that if R or
68
L is taken, either a or b will become a child word
and be eliminate from further consideration by the
algorithm. Therefore, if the action taken on (a, b)
is R or L, it will never be considered again.
Assume that the action taken is S, and, w.l.o.g.
that this is the rightmost S action taken before a
non-S action happens. Note that it is possible that
there is a relation between a and b, but we can-
not perform R or L now. Therefore, we should
consider (a, b) again only if a child of a or b has
changed. When Step Back is used, we will con-
sider (a, b) again only if the next action is L. (If
next action is R, b will be eliminated.) This is true
because the focus point will move back after per-
forming L, which implies that b has a new child
so we are indeed in a new situation. Since, from
Lemma 1, the algorithm only requires one round.
we therefore consider (a, b) again only if the situ-
ation has changed. 2
2.3 Improving the Parsing Action Set
In order to improve the accuracy of the action pre-
dictors, we suggest a new (hierarchical) set of ac-
tions: Shift, Left, Right, WaitLeft, WaitRight. We
believe that predicting these is easier due to finer
granularity ? the S action is broken to sub-actions
in a natural way.
WaitLeft: a < b. a is the parent of b, but it?s
possible that b is a parent of other nodes. Action is
deferred. If we perform Left instead, the child of b
can not find its parents later.
WaitRight: a < b. b is the parent of a, but it?s
possible that a is a parent of other nodes. Similar
to WL, action is deferred.
Thus, we also change the algorithm to perform
S only if there is no relationship between a and b2.
The new set of actions is shown to better support
our parsing algorithm, when tested on different
placement policies. When WaitLeft or WaitRight
is performed, the focus will move to the next word.
It is very interesting to notice that WaitRight is
not needed in projective languages if Step Back
is used. This give us another strong reason to use
Step Back, since the classification becomes more
accurate ? a more natural class of actions, with a
smaller number of candidate actions.
Once the parsing algorithm, along with the fo-
cus point policy, is determined, we can train the
2Interestingly, (Yamada and Matsumoto, 2003) mention
the possibility of an additional single Wait action, but do not
add it to the model.
action classifiers. Given an annotated corpus, the
parsing algorithm is used to determine the action
taken for each consecutive pair; this is used to train
a classifier to predict one of the five actions. The
details of the classifier and the feature used are
given in Section 4.
When the learned model is evaluated on new
data, the sentence is processed left to right and the
parsing algorithm, along with the action classifier,
are used to produce the dependency tree. The eval-
uation process is somewhat more involved, since
the action classifier is not used as is, but rather via
a look ahead inference step described next.
3 A Pipeline Model with Look Ahead
The advantage of a pipeline model is that it can use
more information, based on the outcomes of previ-
ous predictions. As discussed earlier, this may re-
sult in accumulating error. The importance of hav-
ing a reliable action predictor in a pipeline model
motivates the following approach. We devise a
look ahead algorithm and use it as a look ahead
policy, when determining the predicted action.
This approach can be used in any pipeline
model but we illustrate it below in the context of
our dependency parser.
The following example illustrates a situation in
which an early mistake in predicting an action
causes a chain reaction and results in further mis-
takes. This stresses the importance of correct early
decisions, and motivates our look ahead policy.
Let (w, x, y, z) be a sentence of four words, and
assume that the correct dependency relations are
as shown in the top part of Figure 1. If the system
mistakenly predicts that x is a child of w before y
and z becomes x?s children, we can only consider
the relationship between w and y in the next stage.
Consequently, we will never find the correct parent
for y and z. The previous prediction error propa-
gates and impacts future predictions. On the other
hand, if the algorithm makes a correct prediction,
in the next stage, we do not need to consider w and
y. As shown, getting useful rather than misleading
information in a pipeline model, requires correct
early predictions. Therefore, it is necessary to uti-
lize some inference framework to that may help
resolving the error accumulation problem.
In order to improve the accuracy of the action
prediction, we might want to examine all possible
combinations of action sequences and choose the
one that maximizes some score. It is clearly in-
69
X YW Z
X
YW Z
Figure 1: Top figure: the correct dependency rela-
tions between w, x, y and z. Bottom figure: if the
algorithm mistakenly decides that x is a child of w
before deciding that y and z are x?s children, we
cannot find the correct parent for y and z.
tractable to find the global optimal prediction se-
quences in a pipeline model of the depth we con-
sider. Therefore, we use a look ahead strategy,
implemented via a local search framework, which
uses additional information but is still tractable.
The local search algorithm is presented in Algo-
rithm 3. The algorithm accepts three parameters,
model, depth and State. We assume a classifier
that can give a confidence in its prediction. This is
represented here by model.
As our learning algorithm we use a regularized
variation of the perceptron update rule, as incorpo-
rated in SNoW (Roth, 1998; Carlson et al, 1999),
a multi-class classifier that is tailored for large
scale learning tasks and has been used successfully
in a large number of NLP tasks (e.g., (Punyakanok
et al, 2005)). SNoW uses softmax over the raw
activation values as its confidence measure, which
can be shown to produce a reliable approximation
of the labels? conditional probabilities.
The parameter depth is to determine the depth
of the search procedure. State encodes the config-
uration of the environment (in the context of the
dependency parsing this includes the sentence, the
focus point and the current parent and children for
each word). Note that State changes when a pre-
diction is made and that the features extracted for
the action classifier also depend on State.
The search algorithm will perform a search of
length depth. Additive scoring is used to score
the sequence, and the first action in this sequence
is selected and performed. Then, the State is up-
dated, the new features for the action classifiers are
computed and search is called again.
One interesting property of this framework is
that it allows that use of future information in ad-
dition to past information. The pipeline model nat-
urally allows access to all the past information.
Algorithm 3 Pseudo code for the look ahead algo-
rithm. y represents a action sequence. The func-
tion search considers all possible action sequences
with |depth| actions and returns the sequence with
the highest score.
Algo predictAction(model, depth, State)
x = getNextFeature(State)
y = search(x, depth, model, State)
lab = y[1]
State = update(State, lab)
return lab
Algo search(x, depth, model, State)
maxScore = ??
F = {y | ?y? = depth}
for y in F do
s = 0, TmpState = State
for i = 1 . . . depth do
x = getNextFeature(TmpState)
s = s+ score(y[i], x, model)
TmpState = update(TmpState, y[i])
end for
if s > maxScore then
y? = y
maxScore = s
end if
end for
return y?
Since the algorithm uses a look ahead policy, it
also uses future predictions. The significance of
this becomes clear in Section 4.
There are several parameters, in addition to
depth that can be used to improve the efficiency of
the framework. For example, given that the action
predictor is a multi-class classifier, we do not need
to consider all future possibilities in order to de-
cide the current action. For example, in our exper-
iments, we only consider two actions with highest
score at each level (which was shown to produce
almost the same accuracy as considering all four
actions).
4 Experiments and Results
We use the standard corpus for this task, the Penn
Treebank (Marcus et al, 1993). The training set
consists of sections 02 to 21 and the testing set is
section 23. The POS tags for the evaluation data
sets were provided by the tagger of (Toutanova et
al., 2003) (which has an accuracy of 97.2% section
70
23 of the Penn Treebank).
4.1 Features for Action Classification
For each word pair (w1, w2) we use the words,their POS tags and also these features of the chil-
dren of w1 and w2. We also include the lexiconand POS tags of 2 words before w1 and 4 wordsafter w2 (as in (Yamada and Matsumoto, 2003)).The key additional feature we use, relative to (Ya-
mada and Matsumoto, 2003), is that we include
the previous predicted action as a feature. We
also add conjunctions of above features to ensure
expressiveness of the model. (Yamada and Mat-
sumoto, 2003) makes use of polynomial kernels
of degree 2 which is equivalent to using even more
conjunctive features. Overall, the average number
of active features in an example is about 50.
4.2 Evaluation
We use the same evaluation metrics as in (McDon-
ald et al, 2005). Dependency accuracy (DA) is the
proportion of non-root words that are assigned the
correct head. Complete accuracy (CA) indicates
the fraction of sentences that have a complete cor-
rect analysis. We also measure that root accuracy
(RA) and leaf accuracy (LA), as in (Yamada and
Matsumoto, 2003). When evaluating the result,
we exclude the punctuation marks, as done in (Mc-
Donald et al, 2005) and (Yamada and Matsumoto,
2003).
4.3 Results
We present the results of several of the experi-
ments that were intended to help us analyze and
understand several of the design decisions in our
pipeline algorithm.
To see the effect of the additional action, we
present in Table 2 a comparison between a system
that does not have the WaitLeft action (similar
to the (Yamada and Matsumoto, 2003) approach)
with one that does. In both cases, we do not use the
look ahead procedure. Note that, as stated above,
the action WaitRight is never needed for our pars-
ing algorithm. It is clear that adding WaitLeft in-
creases the accuracy significantly.
Table 3 investigates the effect of the look ahead,
and presents results with different depth param-
eters (depth= 1 means ?no search?), showing a
consistent trend of improvement.
Table 4 breaks down the results as a function
of the sentence length; it is especially noticeable
that the system also performs very well for long
method DA RA CA LA
w/o WaitLeft 90.27 90.73 39.28 93.87
w WaitLeft 90.53 90.76 39.74 93.94
Table 2: The significant of the action WaitLeft .
method DA RA CA LA
depth=1 90.53 90.76 39.74 93.94depth=2 90.67 91.51 40.23 93.96depth=3 90.69 92.05 40.52 93.94depth=4 90.79 92.26 40.68 93.95
Table 3: The effect of different depth settings.
sentences, another indication for its global perfor-
mance robustness.
Table 5 shows the results with three settings of
the POS tagger. The best result is, naturally, when
we use the gold standard also in testing. How-
ever, it is worthwhile noticing that it is better to
train with the same POS tagger available in test-
ing, even if its performance is somewhat lower.
Table 6 compares the performances of several
of the state of the art dependency parsing systems
with ours. When comparing with other depen-
dency parsing systems it is especially worth notic-
ing that our system gives significantly better accu-
racy on completely parsed sentences.
Interestingly, in the experiments, we allow the
parsing algorithm to run many rounds to parse a
sentece in the testing stage. However, we found
that over 99% sentences can be parsed in a single
round. This supports for our justification about the
correctness of our model.
5 Further Work and Conclusion
We have addressed the problem of using learned
classifiers in a pipeline fashion, where a task is de-
composed into several stages and stage classifiers
are used sequentially, where each stage may use
the outcome of previous stages as its input. This
is a common computational strategy in natural lan-
guage processing and is known to suffer from error
accumulation and an inability to correct mistakes
in previous stages.
Sent. Len. DA RA CA LA
<11 93.4 96.7 85.2 94.6
11-20 92.4 93.7 56.1 94.7
21-30 90.4 91.8 32.5 93.4
31-40 90.4 89.8 16.8 94.0
>40 89.7 87.9 8.7 93.3
Table 4: The effect of sentences length. The ex-
periment is done with depth = 4.
71
Train-Test DA RA CA LA
gold?pos 90.7 92.0 40.8 93.8
pos?pos 90.8 92.3 40.7 94.0
gold?gold 92.0 93.9 43.6 95.0
Table 5: Comparing different sources of POS tag-
ging in a pipeline model. We set depth= 4 in all
the experiments of this table.
System DA RA CA LA
Y&M03 90.3 91.6 38.4 93.5
N&S04 87.3 84.3 30.4 N/A
M&C&P05 90.9 94.2 37.5 N/ACurrent Work 90.8 92.3 40.7 94.0
Table 6: The comparison between the current
work with other dependency parsing systems.
We abstracted two natural principles, one which
calls for making the local classifiers used in the
computation more reliable and a second, which
suggests to devise the pipeline algorithm in such
a way that minimizes the number of decisions (ac-
tions) made.
We study this framework in the context of de-
signing a bottom up dependency parsing. Not only
we manage to use this framework to justify several
design decisions, but we also show experimentally
that following these results in improving the accu-
racy of the inferred trees relative to existing mod-
els. Interestingly, we can show that the trees pro-
duced by our algorithm are relatively good even
for long sentences, and that our algorithm is do-
ing especially well when evaluated globally, at a
sentence level, where our results are significantly
better than those of existing approaches ? perhaps
showing that the design goals were achieved.
Our future work includes trying to generalize
this work to non-projective dependency parsing,
as well as attempting to incorporate additional
sources of information (e.g., shallow parsing in-
formation) into the pipeline process.
6 Acknowledgements
We thank Ryan McDonald for providing the anno-
tated data set and to Vasin Punyakanok for useful
comments and suggestions.
This research is supported by the Advanced
Research and Development Activity (ARDA)?s
Advanced Question Answering for Intelligence
(AQUAINT) Program and a DOI grant under the
Reflex program.
References
A. V. Aho, R. Sethi, and J. D. Ullman. 1986. Compilers:
Principles, techniques, and tools. In Addison-Wesley Pub-lishing Company, Reading, MA.
A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer Science Depart-
ment, May.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. the Inter-national Conference on Computational Linguistics (COL-ING), pages 340?345, Copenhagen, August.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust textual
inference via graph matching. In Proceedings of HumanLanguage Technology Conference and Conference on Em-pirical Methods in Natural Language Processing, pages
387?394, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.
T. Marciniak and M. Strube. 2005. Beyond the pipeline: Dis-
crete optimization in NLP. In Proceedings of the NinthConference on Computational Natural Language Learn-ing (CoNLL-2005), pages 136?143, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330,
June.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc. ofthe Annual Meeting of the ACL, pages 91?98, Ann Arbor,
Michigan.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In COLING2004, pages 64?70.
Joakim Nivre. 2003. An efficient algorithm for projective
dependency parsing. In IWPT, Nancy, France.
V. Punyakanok, D. Roth, and W. Yih. 2005. The necessity
of syntactic parsing for semantic role labeling. In Proc.of the International Joint Conference on Artificial Intelli-gence (IJCAI), pages 1117?1123.
A. Ratnaparkhi. 1997. A linear observed time statistical
parser based on maximum entropy models. In EMNLP-97, The Second Conference on Empirical Methods in Nat-ural Language Processing, pages 1?10.
D. Roth and W. Yih. 2004. A linear programming for-
mulation for global inference in natural language tasks.
In Hwee Tou Ng and Ellen Riloff, editors, Proc. of theAnnual Conference on Computational Natural LanguageLearning (CoNLL), pages 1?8. Association for Computa-
tional Linguistics.
D. Roth. 1998. Learning to resolve natural language ambi-
guities: A unified approach. In Proc. National Conferenceon Artificial Intelligence, pages 806?813.
K. Toutanova, D. Klein, and C. Manning. ?2003?. Feature-
rich part-of-speech tagging with a cyclic dependency net-
work. In Proceedings of HLT-NAACL 03.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In IWPT2003.
H. Yamada. 2006. Private communication.
72
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 280?287,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Guiding Semi-Supervision with Constraint-Driven Learning
Ming-Wei Chang Lev Ratinov Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{mchang21, ratinov2, danr}@uiuc.edu
Abstract
Over the last few years, two of the main
research directions in machine learning of
natural language processing have been the
study of semi-supervised learning algo-
rithms as a way to train classifiers when the
labeled data is scarce, and the study of ways
to exploit knowledge and global information
in structured learning tasks. In this paper,
we suggest a method for incorporating do-
main knowledge in semi-supervised learn-
ing algorithms. Our novel framework unifies
and can exploit several kinds of task specic
constraints. The experimental results pre-
sented in the information extraction domain
demonstrate that applying constraints helps
the model to generate better feedback during
learning, and hence the framework allows
for high performance learning with signif-
icantly less training data than was possible
before on these tasks.
1 Introduction
Natural Language Processing (NLP) systems typi-
cally require large amounts of knowledge to achieve
good performance. Acquiring labeled data is a dif-
ficult and expensive task. Therefore, an increasing
attention has been recently given to semi-supervised
learning, where large amounts of unlabeled data are
used to improve the models learned from a small
training set (Collins and Singer, 1999; Thelen and
Riloff, 2002). The hope is that semi-supervised or
even unsupervised approaches, when given enough
knowledge about the structure of the problem, will
be competitive with the supervised models trained
on large training sets. However, in the general
case, semi-supervised approaches give mixed re-
sults, and sometimes even degrade the model per-
formance (Nigam et al, 2000). In many cases, im-
proving semi-supervised models was done by seed-
ing these models with domain information taken
from dictionaries or ontology (Cohen and Sarawagi,
2004; Collins and Singer, 1999; Haghighi and Klein,
2006; Thelen and Riloff, 2002). On the other hand,
in the supervised setting, it has been shown that
incorporating domain and problem specific struc-
tured information can result in substantial improve-
ments (Toutanova et al, 2005; Roth and Yih, 2005).
This paper proposes a novel constraints-based
learning protocol for guiding semi-supervised learn-
ing. We develop a formalism for constraints-based
learning that unifies several kinds of constraints:
unary, dictionary based and n-ary constraints, which
encode structural information and interdependencies
among possible labels. One advantage of our for-
malism is that it allows capturing different levels of
constraint violation. Our protocol can be used in
the presence of any learning model, including those
that acquire additional statistical constraints from
observed data while learning (see Section 5. In the
experimental part of this paper we use HMMs as the
underlying model, and exhibit significant reduction
in the number of training examples required in two
information extraction problems.
As is often the case in semi-supervised learning,
the algorithm can be viewed as a process that im-
proves the model by generating feedback through
280
labeling unlabeled examples. Our algorithm pushes
this intuition further, in that the use of constraints
allows us to better exploit domain information as a
way to label, along with the current learned model,
unlabeled examples. Given a small amount of la-
beled data and a large unlabeled pool, our frame-
work initializes the model with the labeled data and
then repeatedly:
(1) Uses constraints and the learned model to label
the instances in the pool.
(2) Updates the model by newly labeled data.
This way, we can generate better ?training? ex-
amples during the semi-supervised learning process.
The core of our approach, (1), is described in Sec-
tion 5. The task is described in Section 3 and the
Experimental study in Section 6. It is shown there
that the improvement on the training examples via
the constraints indeed boosts the learned model and
the proposed method significantly outperforms the
traditional semi-supervised framework.
2 Related Work
In the semi-supervised domain there are two main
approaches for injecting domain specific knowledge.
One is using the prior knowledge to accurately tailor
the generative model so that it captures the domain
structure. For example, (Grenager et al, 2005) pro-
poses Diagonal Transition Models for sequential la-
beling tasks where neighboring words tend to have
the same labels. This is done by constraining the
HMM transition matrix, which can be done also for
other models, such as CRF. However (Roth and Yih,
2005) showed that reasoning with more expressive,
non-sequential constraints can improve the perfor-
mance for the supervised protocol.
A second approach has been to use a small high-
accuracy set of labeled tokens as a way to seed and
bootstrap the semi-supervised learning. This was
used, for example, by (Thelen and Riloff, 2002;
Collins and Singer, 1999) in information extraction,
and by (Smith and Eisner, 2005) in POS tagging.
(Haghighi and Klein, 2006) extends the dictionary-
based approach to sequential labeling tasks by prop-
agating the information given in the seeds with con-
textual word similarity. This follows a conceptually
similar approach by (Cohen and Sarawagi, 2004)
that uses a large named-entity dictionary, where the
similarity between the candidate named-entity and
its matching prototype in the dictionary is encoded
as a feature in a supervised classifier.
In our framework, dictionary lookup approaches
are viewed as unary constraints on the output states.
We extend these kinds of constraints and allow for
more general, n-ary constraints.
In the supervised learning setting it has been es-
tablished that incorporating global information can
significantly improve performance on several NLP
tasks, including information extraction and semantic
role labeling. (Punyakanok et al, 2005; Toutanova
et al, 2005; Roth and Yih, 2005). Our formalism
is most related to this last work. But, we develop a
semi-supervised learning protocol based on this for-
malism. We also make use of soft constraints and,
furthermore, extend the notion of soft constraints to
account for multiple levels of constraints? violation.
Conceptually, although not technically, the most re-
lated work to ours is (Shen et al, 2005) that, in
a somewhat ad-hoc manner uses soft constraints to
guide an unsupervised model that was crafted for
mention tracking. To the best of our knowledge,
we are the first to suggest a general semi-supervised
protocol that is driven by soft constraints.
We propose learning with constraints - a frame-
work that combines the approaches described above
in a unified and intuitive way.
3 Tasks, Examples and Datasets
In Section 4 we will develop a general framework
for semi-supervised learning with constraints. How-
ever, it is useful to illustrate the ideas on concrete
problems. Therefore, in this section, we give a brief
introduction to the two domains on which we tested
our algorithms. We study two information extrac-
tion problems in each of which, given text, a set of
pre-defined fields is to be identified. Since the fields
are typically related and interdependent, these kinds
of applications provide a good test case for an ap-
proach like ours.1
The first task is to identify fields from citations
(McCallum et al, 2000) . The data originally in-
cluded 500 labeled references, and was later ex-
tended with 5,000 unannotated citations collected
from papers found on the Internet (Grenager et al,
2005). Given a citation, the task is to extract the
1The data for both problems is available at:
http://www.stanford.edu/ grenager/data/unsupie.tgz
281
(a) [ AUTHOR Lars Ole Andersen . ] [ TITLE Program analysis and specialization for the C programming language . ] [TECH-REPORT PhD thesis , ] [ INSTITUTION DIKU , University of Copenhagen , ] [ DATE May 1994 . ]
(b) [ AUTHOR Lars Ole Andersen . Program analysis and ] [TITLE specialization for the ] [EDITOR C ] [ BOOKTITLE
Programming language ] [ TECH-REPORT . PhD thesis , ] [ INSTITUTION DIKU , University of Copenhagen , May ] [ DATE
1994 . ]
Figure 1: Error analysis of a HMM model. The labels are annotated by underline and are to the right of
each open bracket. The correct assignment was shown in (a). While the predicted label assignment (b) is
generally coherent, some constraints are violated. Most obviously, punctuation marks are ignored as cues
for state transitions. The constraint ?Fields cannot end with stop words (such as ?the?)? may be also good.
fields that appear in the given reference. See Fig. 1.
There are 13 possible fields including author, title,
location, etc.
To gain an insight to how the constraints can guide
semi-supervised learning, assume that the sentence
shown in Figure 1 appears in the unlabeled data
pool. Part (a) of the figure shows the correct la-
beled assignment and part (b) shows the assignment
labeled by a HMM trained on 30 labels. However,
if we apply the constraint that state transition can
occur only on punctuation marks, the same HMM
model parameters will result in the correct labeling
(a). Therefore, by adding the improved labeled as-
signment we can generate better training samples
during semi-supervised learning. In fact, the punc-
tuation marks are only some of the constraints that
can be applied to this problem. The set of constraints
we used in our experiments appears in Table 1. Note
that some of the constraints are non-local and are
very intuitive for people, yet it is very difficult to
inject this knowledge into most models.
The second problem we consider is extracting
fields from advertisements (Grenager et al, 2005).
The dataset consists of 8,767 advertisements for
apartment rentals in the San Francisco Bay Area
downloaded in June 2004 from the Craigslist web-
site. In the dataset, only 302 entries have been la-
beled with 12 fields, including size, rent, neighbor-
hood, features, and so on. The data was prepro-
cessed using regular expressions for phone numbers,
email addresses and URLs. The list of the con-
straints for this domain is given in Table 1. We im-
plement some global constraints and include unary
constraints which were largely imported from the
list of seed words used in (Haghighi and Klein,
2006). We slightly modified the seedwords due to
difference in preprocessing.
4 Notation and Definitions
Consider a structured classification problem, where
given an input sequence x = (x1, . . . , xN ), the taskis to find the best assignment to the output variables
y = (y1, . . . , yM ). We denote X to be the space ofthe possible input sequences and Y to be the set of
possible output sequences.
We define a structured output classifier as a func-
tion h : X ? Y that uses a global scoring function
f : X ?Y ? R to assign scores to each possible in-
put/output pair. Given an input x, a desired function
f will assign the correct output y the highest score
among all the possible outputs. The global scoring
function is often decomposed as a weighted sum of
feature functions,
f(x, y) =
M
?
i=1
?ifi(x, y) = ? ? F (x, y).
This decomposition applies both to discriminative
linear models and to generative models such as
HMMs and CRFs, in which case the linear sum
corresponds to log likelihood assigned to the in-
put/output pair by the model (for details see (Roth,
1999) for the classification case and (Collins, 2002)
for the structured case). Even when not dictated by
the model, the feature functions fi(x, y) used arelocal to allow inference tractability. Local feature
function can capture some context for each input or
output variable, yet it is very limited to allow dy-
namic programming decoding during inference.
Now, consider a scenario where we have a set
of constraints C1, . . . , CK . We define a constraint
C : X ? Y ? {0, 1} as a function that indicates
whether the input/output sequence violates some de-
sired properties. When the constraints are hard, the
solution is given by
argmax
y?1C(x)
? ? F (x, y),
282
(a)-Citations
1) Each field must be a consecutive list of words, and can
appear at most once in a citation.
2) State transitions must occur on punctuation marks.
3) The citation can only start with author or editor.
4) The words pp., pages correspond to PAGE.
5) Four digits starting with 20xx and 19xx are DATE.
6) Quotations can appear only in titles.
7) The words note, submitted, appear are NOTE.
8) The words CA, Australia, NY are LOCATION.
9) The words tech, technical are TECH REPORT.
10) The words proc, journal, proceedings, ACM are JOUR-NAL or BOOKTITLE.
11) The words ed, editors correspond to EDITOR.
(b)-Advertisements
1) State transitions can occur only on punctuation marks or
the newline symbol.
2) Each field must be at least 3 words long.
3) The words laundry, kitchen, parking are FEATURES.
4) The words sq, ft, bdrm are SIZE.
5) The word $, *MONEY* are RENT.
6) The words close, near, shopping are NEIGHBORHOOD.
7) The words laundry kitchen, parking are FEATURES.
8) The (normalized) words phone, email are CONTACT.
9) The words immediately, begin, cheaper are AVAILABLE.
10) The words roommates, respectful, drama are ROOM-MATES.
11) The words smoking, dogs, cats are RESTRICTIONS.
12) The word http, image, link are PHOTOS.
13) The words address, carlmont, st, cross are ADDRESS.
14) The words utilities, pays, electricity are UTILITIES.
Table 1: The list of constraints for extracting fields
from citations and advertisements. Some constraints
(represented in the first block of each domain) are
global and are relatively difficult to inject into tradi-
tional models. While all the constraints hold for the
vast majority of the data, some of them are violated
by some correct labeled assignments.
where 1C(x) is a subset of Y for which all Ci as-sign the value 1 for the given (x, y).
When the constraints are soft, we want to in-
cur some penalty for their violation. Moreover, we
want to incorporate into our cost function a mea-
sure for the amount of violation incurred by vi-
olating the constraint. A generic way to capture
this intuition is to introduce a distance function
d(y, 1Ci(x)) between the space of outputs that re-spect the constraint,1Ci(x), and the given output se-quence y. One possible way to implement this dis-
tance function is as the minimal Hamming distance
to a sequence that respects the constraint Ci, that is:
d(y, 1Ci(x)) = min(y??1C(x)) H(y, y?). If the penaltyfor violating the soft constraint Ci is ?i, we write the
score function as:
argmax
y
? ? F (x, y) ?
K
?
i=1
?id(y, 1Ci(x)) (1)
We refer to d(y, 1C(x)) as the valuation of theconstraint C on (x, y). The intuition behind (1) is as
follows. Instead of merely maximizing the model?s
likelihood, we also want to bias the model using
some knowledge. The first term of (1) is used to
learn from data. The second term biases the mode
by using the knowledge encoded in the constraints.
Note that we do not normalize our objective function
to be a true probability distribution.
5 Learning and Inference with Constraints
In this section we present a new constraint-driven
learning algorithm (CODL) for using constraints to
guide semi-supervised learning. The task is to learn
the parameter vector ? by using the new objective
function (1). While our formulation allows us to
train also the coefficients of the constraints valua-
tion, ?i, we choose not to do it, since we view this asa way to bias (or enforce) the prior knowledge into
the learned model, rather than allowing the data to
brush it away. Our experiments demonstrate that the
proposed approach is robust to inaccurate approxi-
mation of the prior knowledge (assigning the same
penalty to all the ?i ).We note that in the presence of constraints, the
inference procedure (for finding the output y that
maximizes the cost function) is usually done with
search techniques (rather than Viterbi decoding,
see (Toutanova et al, 2005; Roth and Yih, 2005) for
a discussion), we chose beamsearch decoding.
The semi-supervised learning with constraints is
done with an EM-like procedure. We initialize the
model with traditional supervised learning (ignoring
the constraints) on a small labeled set. Given an un-
labeled set U , in the estimation step, the traditional
EM algorithm assigns a distribution over labeled as-
signmentsY of each x ? U , and in the maximization
step, the set of model parameters is learned from the
distributions assigned in the estimation step.
However, in the presence of constraints, assigning
the complete distributions in the estimation step is
infeasible since the constraints reshape the distribu-
tion in an arbitrary way. As in existing methods for
training a model by maximizing a linear cost func-
tion (maximize likelihood or discriminative maxi-
283
mization), the distribution over Y is represented as
the set of scores assigned to it; rather than consid-
ering the score assigned to all y?s, we truncate the
distribution to the top K assignments as returned
by the search. Given a set of K top assignments
y1, . . . , yK , we approximate the estimation step by
assigning uniform probability to the top K candi-
dates, and zero to the other output sequences. We
denote this algorithm top-K hard EM. In this pa-
per, we use beamsearch to generate K candidates
according to (1).
Our training algorithm is summarized in Figure 2.
Several things about the algorithm should be clari-
fied: the Top-K-Inference procedure in line 7, the
learning procedure in line 9, and the new parameter
estimation in line 9.
The Top-K-Inference is a procedure that returns
the K labeled assignments that maximize the new
objective function (1). In our case we used the top-
K elements in the beam, but this could be applied
to any other inference procedure. The fact that the
constraints are used in the inference procedure (in
particular, for generating new training examples) al-
lows us to use a learning algorithm that ignores the
constraints, which is a lot more efficient (although
algorithms that do take the constraints into account
can be used too). We used maximum likelihood es-
timation of ? but, in general, perceptron or quasi-
Newton can also be used.
It is known that traditional semi-supervised train-
ing can degrade the learned model?s performance.
(Nigam et al, 2000) has suggested to balance the
contribution of labeled and unlabeled data to the pa-
rameters. The intuition is that when iteratively esti-
mating the parameters with EM, we disallow the pa-
rameters to drift too far from the supervised model.
The parameter re-estimation in line 9, uses a similar
intuition, but instead of weighting data instances, we
introduced a smoothing parameter ? which controls
the convex combination of models induced by the la-
beled and the unlabeled data. Unlike the technique
mentioned above which focuses on naive Bayes, our
method allows us to weight linear models generated
by different learning algorithms.
Another way to look the algorithm is from the
self-training perspective (McClosky et al, 2006).
Similarly to self-training, we use the current model
to generate new training examples from the unla-
Input:
Cycles: learning cycles
Tr = {x, y}: labeled training set.
U : unlabeled dataset
F : set of feature functions.
{?i}: set of penalties.
{Ci}: set of constraints.
?: balancing parameter with the supervised model.
learn(Tr, F ): supervised learning algorithm
Top-K-Inference:
returns top-K labeled scored by the cost function (1)CODL:
1. Initialize ?0 = learn(Tr, F ).2. ? = ?0.3. For Cycles iterations do:
4. T = ?
5. For each x ? U
6. {(x, y1), . . . , (x, yK)} =
7. Top-K-Inference(x, ?, F, {Ci}, {?i})
8. T = T ? {(x, y1), . . . , (x, yK)}
9. ? = ??0 + (1 ? ?)learn(T, F )
Figure 2: COnstraint Driven Learning (CODL). In
Top-K-Inference, we use beamsearch to find the K-
best solution according to Eq. (1).
beled set. However, there are two important differ-
ences. One is that in self-training, once an unlabeled
sample was labeled, it is never labeled again. In
our case all the samples are relabeled in each iter-
ation. In self-training it is often the case that only
high-confidence samples are added to the labeled
data pool. While we include all the samples in the
training pool, we could also limit ourselves to the
high-confidence samples. The second difference is
that each unlabeled example generates K labeled in-
stances. The case of one iteration of top-1 hard EM
is equivalent to self training, where all the unlabeled
samples are added to the labeled pool.
There are several possible benefits to using K > 1
samples. (1) It effectively increases the training set
by a factor of K (albeit by somewhat noisy exam-
ples). In the structured scenario, each of the top-K
assignments is likely to have some good components
so generating top-K assignments helps leveraging
the noise. (2) Given an assignment that does not sat-
isfy some constraints, using top-K allows for mul-
tiple ways to correct it. For example, consider the
output 11101000 with the constraint that it should
belong to the language 1?0?. If the two top scoring
corrections are 11111000 and 11100000, consider-
ing only one of those can negatively bias the model.
284
6 Experiments and Results
In this section, we present empirical results of our
algorithms on two domains: citations and adver-
tisements. Both problems are modeled with a sim-
ple token-based HMM. We stress that token-based
HMM cannot represent many of our constraints. The
function d(y, 1C(x)) used is an approximation of aHamming distance function, discussed in Section 7.
For both domains, and all the experiments, ? was
set to 0.1. The constraints violation penalty ? is set
to ? log 10?4 and ? log 10?1 for citations and ad-
vertisements, resp.2 Note that all constraints share
the same penalty. The number of semi-supervised
training cycles (line 3 of Figure 2) was set to 5. The
constraints for the two domains are listed in Table 1.
We trained models on training sets of size vary-
ing from 5 to 300 for the citations and from 5 to
100 for the advertisements. Additionally, in all the
semi-supervised experiments, 1000 unlabeled exam-
ples are used. We report token-based3 accuracy on
100 held-out examples (which do not overlap neither
with the training nor with the unlabeled data). We
ran 5 experiments in each setting, randomly choos-
ing the training set. The results reported below are
the averages over these 5 runs.
To verify our claims we implemented several
baselines. The first baseline is the supervised learn-
ing protocol denoted by sup. The second baseline
was a traditional top-1 Hard EM also known as
truncated EM4 (denoted by H for Hard). In the third
baseline, denoted H&W, we balanced the weight
of the supervised and unsupervised models as de-
scribed in line 9 of Figure 2. We compare these base-
lines to our proposed protocol, H&W&C, where we
added the constraints to guide the H&W protocol.
We experimented with two flavors of the algorithm:
the top-1 and the top-K version. In the top-K ver-
sion, the algorithm uses K-best predictions (K=50)
for each instance in order to update the model as de-
scribed in Figure 2.
The experimental results for both domains are in
given Table 2. As hypothesized, hard EM sometimes
2The guiding intuition is that ?F (x, y) corresponds to a log-
likelihood of a HMM model and ? to a crude estimation of the
log probability that a constraint does not hold. ? was tuned on
a development set and kept fixed in all experiments.
3Each token (word or punctuation mark) is assigned a state.
4We also experimented with (soft) EM without constraints,
but the results were generally worse.
(a)- Citations
N Inf. sup. H H&W H&W&C H&W&C
(Top-1) (Top-K)
5 no I 55.1 60.9 63.6 70.6 71.0
I 66.6 69.0 72.5 76.0 77.8
10 no I 64.6 66.8 69.8 76.5 76.7
I 78.1 78.1 81.0 83.4 83.8
15 no I 68.7 70.6 73.7 78.6 79.4
I 81.3 81.9 84.1 85.5 86.2
20 no I 70.1 72.4 75.0 79.6 79.4
I 81.1 82.4 84.0 86.1 86.1
25 no I 72.7 73.2 77.0 81.6 82.0
I 84.3 84.2 86.2 87.4 87.6
300 no I 86.1 80.7 87.1 88.2 88.2
I 92.5 89.6 93.4 93.6 93.5
(b)-Advertisements
N Inf. sup. H H&W H&W&C H&W&C
(Top-1) (Top-K)
5 no I 55.2 61.8 60.5 66.0 66.0
I 59.4 65.2 63.6 69.3 69.6
10 no I 61.6 69.2 67.0 70.8 70.9
I 66.6 73.2 71.6 74.7 74.7
15 no I 66.3 71.7 70.1 73.0 73.0
I 70.4 75.6 74.5 76.6 76.9
20 no I 68.1 72.8 72.0 74.5 74.6
I 71.9 76.7 75.7 77.9 78.1
25 no I 70.0 73.8 73.0 74.9 74.8
I 73.7 77.7 76.6 78.4 78.5
100 no I 76.3 76.2 77.6 78.5 78.6
I 80.4 80.5 81.2 81.8 81.7
Table 2: Experimental results for extracting fields
from citations and advertisements. N is the number
of labeled samples. H is the traditional hard-EM and
H&W weighs labeled and unlabeled data as men-
tioned in Sec. 5. Our proposed model is H&W&C,
which uses constraints in the learning procedure. I
refers to using constraints during inference at eval-
uation time. Note that adding constraints improves
the accuracy during both learning and inference.
degrade the performance. Indeed, with 300 labeled
examples in the citations domain, the performance
decreases from 86.1 to 80.7. The usefulness of in-
jecting constraints in semi-supervised learning is ex-
hibited in the two right most columns: using con-
straints H&W&C improves the performance over
H&W quite significantly.
We carefully examined the contribution of us-
ing constraints to the learning stage and the testing
stage, and two separate results are presented: test-
ing with constraints (denoted I for inference) and
without constraints (no I). The I results are consis-
tently better. And, it is also clear from Table 2,
that using constraints in training always improves
285
the model and the amount of improvement depends
on the amount of labeled data.
Figure 3 compares two protocols on the adver-
tisements domain: H&W+I, where we first run the
H&W protocol and then apply the constraints dur-
ing testing stage, and H&W&C+I, which uses con-
straints to guide the model during learning and uses
it also in testing. Although injecting constraints in
the learning process helps, testing with constraints is
more important than using constraints during learn-
ing, especially when the labeled data size is large.
This confirms results reported for the supervised
learning case in (Punyakanok et al, 2005; Roth and
Yih, 2005). However, as shown, our proposed al-
gorithm H&W&C for training with constraints is
critical when the amount labeled data is small.
Figure 4 further strengthens this point. In the cita-
tions domain, H&W&C+I achieves with 20 labeled
samples similar performance to the supervised ver-
sion without constraints with 300 labeled samples.
(Grenager et al, 2005) and (Haghighi and Klein,
2006) also report results for semi-supervised learn-
ing for these domains. However, due to differ-
ent preprocessing, the comparison is not straight-
forward. For the citation domain, when 20 labeled
and 300 unlabeled samples are available, (Grenager
et al, 2005) observed an increase from 65.2% to
71.3%. Our improvement is from 70.1% to 79.4%.
For the advertisement domain, they observed no im-
provement, while our model improves from 68.1%
to 74.6% with 20 labeled samples. Moreover, we
successfully use out-of-domain data (web data) to
improve our model, while they report that this data
did not improve their unsupervised model.
(Haghighi and Klein, 2006) also worked on one of
our data sets. Their underlying model, Markov Ran-
dom Fields, allows more expressive features. Nev-
ertheless, when they use only unary constraints they
get 53.75%. When they use their final model, along
with a mechanism for extending the prototypes to
other tokens, they get results that are comparable to
our model with 10 labeled examples. Additionally,
in their framework, it is not clear how to use small
amounts of labeled data when available. Our model
outperforms theirs once we add 10 more examples.
 0.65
 0.7
 0.75
 0.8
 0.85
100252015105
H+N+I
H+N+C+I
Figure 3: Comparison between H&W+I and
H&W&C+I on the advertisements domain. When
there is a lot of labeled data, inference with con-
straints is more important than using constraints dur-
ing learning. However, it is important to train with
constraints when the amount of labeled data is small.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
100252015105
sup. (300)
H+N+C+I
Figure 4: With 20 labeled citations, our algorithm
performs competitively to the supervised version
trained on 300 samples.
7 Soft Constraints
This section discusses the importance of using soft
constraints rather than hard constraints, the choice
of Hamming distance for d(y, 1C(x)) and how weapproximate it. We use two constraints to illustrate
the ideas. (C1): ?state transitions can only occur onpunctuation marks or newlines?, and (C2): ?the field
TITLE must appear?.
First, we claim that defining d(y, 1C(x)) to bethe Hamming distance is superior to using a binary
value, d(y, 1C(x)) = 0 if y ? 1C(x) and 1 other-wise. Consider, for example, the constraint C1 inthe advertisements domain. While the vast majority
of the instances satisfy the constraint, some violate
it in more than one place. Therefore, once the binary
distance is set to 1, the algorithm looses the ability to
discriminate constraint violations in other locations
286
of the same instance. This may hurt the performance
in both the inference and the learning stage.
Computing the Hamming distance exactly can
be a computationally hard problem. Further-
more, it is unreasonable to implement the ex-
act computation for each constraint. Therefore,
we implemented a generic approximation for the
hamming distance assuming only that we are
given a boolean function ?C(yN ) that returnswhether labeling the token xN with state yN vio-lates constraint with respect to an already labeled
sequence (x1, . . . , xN?1, y1, . . . , yN?1). Then
d(y, 1C(x)) =
?N
i=1 ?C(yi). For example,consider the prefix x1, x2, x3, x4, which con-tains no punctuation or newlines and was labeled
AUTH, AUTH, DATE, DATE. This labeling
violates C1, the minimal hamming distance is 2, andour approximation gives 1, (since there is only one
transition that violates the constraint.)
For constraints which cannot be validated based
on prefix information, our approximation resorts to
binary violation count. For instance, the constraint
C2 cannot be implemented with prefix informationwhen the assignment is not complete. Otherwise, it
would mean that the field TITLE should appear as
early as possible in the assignment.
While (Roth and Yih, 2005) showed the signif-
icance of using hard constraints, our experiments
show that using soft constraints is a superior op-
tion. For example, in the advertisements domain,
C1 holds for the large majority of the gold-labeledinstances, but is sometimes violated. In supervised
training with 100 labeled examples on this domain,
sup gave 76.3% accuracy. When the constraint vio-
lation penalty ? was innity (equivalent to hard con-
straint), the accuracy improved to 78.7%, but when
the penalty was set to ?log(0.1), the accuracy of the
model jumped to 80.6%.
8 Conclusions and Future Work
We proposed to use constraints as a way to guide
semi-supervised learning. The framework devel-
oped is general both in terms of the representation
and expressiveness of the constraints, and in terms
of the underlying model being learned ? HMM in
the current implementation. Moreover, our frame-
work is a useful tool when the domain knowledge
cannot be expressed by the model.
The results show that constraints improve not
only the performance of the final inference stage but
also propagate useful information during the semi-
supervised learning process and that training with
the constraints is especially significant when the
number of labeled training data is small.
Acknowledgments: This work is supported by NSF SoD-
HCER-0613885 and by a grant from Boeing. Part of this work
was done while Dan Roth visited the Technion, Israel, sup-
ported by a Lady Davis Fellowship.
References
W. Cohen and S. Sarawagi. 2004. Exploiting dictionaries in
named entity extraction: Combining semi-markov extraction
processes and data integration methods. In Proc. of the ACMSIGKDD.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proc. of EMNLP.
M. Collins. 2002. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proc. of EMNLP.
T. Grenager, D. Klein, and C. Manning. 2005. Unsupervised
learning of field segmentation models for information extrac-
tion. In Proc. of the Annual Meeting of the ACL.
A. Haghighi and D. Klein. 2006. Prototype-driven learning for
sequence models. In Proc. of HTL-NAACL.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum
entropy markov models for information extraction and seg-
mentation. In Proc. of ICML.
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In Proceedings of HLT-NAACL.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text
classification from labeled and unlabeled documents using
EM. Machine Learning, 39(2/3):103?134.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005. Learn-
ing and inference over constrained output. In Proc. of IJCAI.
D. Roth and W. Yih. 2005. Integer linear programming infer-
ence for conditional random fields. In Proc. of ICML.
D. Roth. 1999. Learning in natural language. In Proc. of IJCAI,
pages 898?904.
W. Shen, X. Li, and A. Doan. 2005. Constraint-based entity
matching. In Proc. of AAAI).
N. Smith and J. Eisner. 2005. Contrastive estimation: Training
log-linear models on unlabeled data. In Proc. of the AnnualMeeting of the ACL.
M. Thelen and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern contexts.
In Proc. of EMNLP.
K. Toutanova, A. Haghighi, and C. D. Manning. 2005. Joint
learning improves semantic role labeling. In Proc. of theAnnual Meeting of the ACL.
287
Proceedings of ACL-08: HLT, pages 1030?1038,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extraction of Entailed Semantic Relations Through
Syntax-based Comma Resolution
Vivek Srikumar 1 Roi Reichart2 Mark Sammons1 Ari Rappoport2 Dan Roth1
1University of Illinois at Urbana-Champaign
{vsrikum2|mssammon|danr}@uiuc.edu
2Institute of Computer Science, Hebrew University of Jerusalem
{roiri|arir}@cs.huji.ac.il
Abstract
This paper studies textual inference by inves-
tigating comma structures, which are highly
frequent elements whose major role in the ex-
traction of semantic relations has not been
hitherto recognized. We introduce the prob-
lem of comma resolution, defined as under-
standing the role of commas and extracting the
relations they imply. We show the importance
of the problem using examples from Tex-
tual Entailment tasks, and present A Sentence
Transformation Rule Learner (ASTRL), a ma-
chine learning algorithm that uses a syntac-
tic analysis of the sentence to learn sentence
transformation rules that can then be used to
extract relations. We have manually annotated
a corpus identifying comma structures and re-
lations they entail and experimented with both
gold standard parses and parses created by a
leading statistical parser, obtaining F-scores of
80.2% and 70.4% respectively.
1 Introduction
Recognizing relations expressed in text sentences is
a major topic in NLP, fundamental in applications
such as Textual Entailment (or Inference), Question
Answering and Text Mining. In this paper we ad-
dress this issue from a novel perspective, that of un-
derstanding the role of the commas in a sentence,
which we argue is a key component in sentence
comprehension. Consider for example the following
three sentences:
1. Authorities have arrested John Smith, a retired
police officer.
2. Authorities have arrested John Smith, his friend
and his brother.
3. Authorities have arrested John Smith, a retired
police officer announced this morning.
Sentence (1) states that John Smith is a retired
police officer. The comma and surrounding sen-
tence structure represent the relation ?IsA?. In (2),
the comma and surrounding structure signifies a list,
so the sentence states that three people were ar-
rested: (i) John Smith, (ii) his friend, and (iii) his
brother. In (3), a retired police officer announced
that John Smith has been arrested. Here, the comma
and surrounding sentence structure indicate clause
boundaries.
In all three sentences, the comma and the sur-
rounding sentence structure signify relations essen-
tial to comprehending the meaning of the sentence,
in a way that is not easily captured using lexical-
or even shallow parse-level information. As a hu-
man reader, we understand them easily, but auto-
mated systems for Information Retrieval, Question
Answering, and Textual Entailment are likely to en-
counter problems when comparing structures like
these, which are lexically similar, but whose mean-
ings are so different.
In this paper we present an algorithm for comma
resolution, a task that we define to consist of (1) dis-
ambiguating comma type and (2) determining the
relations entailed from the sentence given the com-
mas? interpretation. Specifically, in (1) we assign
each comma to one of five possible types, and in
(2) we generate a set of natural language sentences
that express the relations, if any, signified by each
comma structure. The algorithm uses information
extracted from parse trees. This work, in addition to
having immediate significance for natural language
processing systems that use semantic content, has
potential applications in improving a range of auto-
1030
mated analysis by decomposing complex sentences
into a set of simpler sentences that capture the same
meaning. Although there are many other widely-
used structures that express relations in a similar
way, commas are one of the most commonly used
symbols1. By addressing comma resolution, we of-
fer a promising first step toward resolving relations
in sentences.
To evaluate the algorithm, we have developed an-
notation guidelines, and manually annotated sen-
tences from the WSJ PennTreebank corpus. We
present a range of experiments showing the good
performance of the system, using gold-standard and
parser-generated parse trees.
In Section 2 we motivate comma resolution
through Textual Entailment examples. Section 3 de-
scribes related work. Sections 4 and 5 present our
corpus annotation and learning algorithm. Results
are given in Section 6.
2 Motivating Comma Resolution Through
Textual Entailment
Comma resolution involves not only comma dis-
ambiguation but also inference of the arguments
(and argument boundaries) of the relationship repre-
sented by the comma structure, and the relationships
holding between these arguments and the sentence
as a whole. To our knowledge, this is the first pa-
per that deals with this problem, so in this section
we motivate it in depth by showing its importance
to the semantic inference task of Textual Entailment
(TE) (Dagan et al, 2006), which is increasingly rec-
ognized as a crucial direction for improving a range
of NLP tasks such as information extraction, ques-
tion answering and summarization.
TE is the task of deciding whether the meaning
of a text T (usually a short snippet) can be inferred
from the meaning of another text S. If this is the
case, we say that S entails T . For example2, we say
that sentence (1) entails sentence (2):
1. S: Parviz Davudi was representing Iran at a
meeting of the Shanghai Co-operation Orga-
nization (SCO), the fledgling association that
1For example, the WSJ corpus has 49K sentences, among
which 32K with one comma or more, 17K with two or more,
and 7K with three or more.
2The examples of this section are variations of pairs taken
from the Pascal RTE3 (Dagan et al, 2006) dataset.
binds two former Soviet republics of central
Asia, Russia and China to fight terrorism.
2. T: SCO is the fledgling association that binds
several countries.
To see that (1) entails (2), one must understand
that the first comma structure in sentence (1) is an
apposition structure, and does not indicate the begin-
ning of a list. The second comma marks a boundary
between entities in a list. To make the correct infer-
ence one must determine that the second comma is a
list separator, not an apposition marker. Misclassify-
ing the second comma in (1) as an apposition leads
to the conclusion that (1) entails (3):
3. T: Russia and China are two former Soviet re-
publics of central Asia .
Note that even to an educated native speaker of
English, sentence 1 may be initially confusing; dur-
ing the first reading, one might interpret the first
comma as indicating a list, and that ?the Shanghai
Co-operation Organization? and ?the fledgling asso-
ciation that binds...? are two separate entities that are
meeting, rather than two representations of the same
entity.
From these examples we draw the following con-
clusions: 1. Comma resolution is essential in com-
prehending natural language text. 2. Explicitly rep-
resenting relations derived from comma structures
can assist a wide range of NLP tasks; this can be
done by directly augmenting the lexical-level rep-
resentation, e.g., by bringing surface forms of two
text fragments with the same meaning closer to-
gether. 3. Comma structures might be highly am-
biguous, nested and overlapping, and consequently
their interpretation is a difficult task. The argument
boundaries of the corresponding extracted relations
are also not easy to detect.
The output of our system could be used to aug-
ment sentences with an explicit representation of en-
tailed relations that hold in them. In Textual Entail-
ment systems this can increase the likelihood of cor-
rect identification of entailed sentences, and in other
NLP systems it can help understanding the shallow
lexical/syntactic content of a sentence. A similar ap-
proach has been taken in (Bar-Haim et al, 2007; de
Salvo Braz et al, 2005), which augment the source
sentence with entailed relations.
1031
3 Related Work
Since we focus on extracting the relations repre-
sented by commas, there are two main strands of
research with similar goals: 1) systems that directly
analyze commas, whether labeling them with syn-
tactic information or correcting inappropriate use in
text; and 2) systems that extract relations from text,
typically by trying to identify paraphrases.
The significance of interpreting the role of com-
mas in sentences has already been identified by (van
Delden and Gomez, 2002; Bayraktar et al, 1998)
and others. A review of the first line of research is
given in (Say and Akman, 1997).
In (Bayraktar et al, 1998) the WSJ PennTreebank
corpus (Marcus et al, 1993) is analyzed and a very
detailed list of syntactic patterns that correspond to
different roles of commas is created. However, they
do not study the extraction of entailed relations as
a function of the comma?s interpretation. Further-
more, the syntactic patterns they identify are unlexi-
calized and would not support the level of semantic
relations that we show in this paper. Finally, theirs
is a manual process completely dependent on syn-
tactic patterns. While our comma resolution system
uses syntactic parse information as its main source
of features, the approach we have developed focuses
on the entailed relations, and does not limit imple-
mentations to using only syntactic information.
The most directly comparable prior work is that
of (van Delden and Gomez, 2002), who use fi-
nite state automata and a greedy algorithm to learn
comma syntactic roles. However, their approach dif-
fers from ours in a number of critical ways. First,
their comma annotation scheme does not identify
arguments of predicates, and therefore cannot be
used to extract complete relations. Second, for each
comma type they identify, a new Finite State Au-
tomaton must be hand-encoded; the learning com-
ponent of their work simply constrains which FSAs
that accept a given, comma containing, text span
may co-occur. Third, their corpus is preprocessed by
hand to identify specialized phrase types needed by
their FSAs; once our system has been trained, it can
be applied directly to raw text. Fourth, they exclude
from their analysis and evaluation any comma they
deem to have been incorrectly used in the source
text. We include all commas that are present in the
text in our annotation and evaluation.
There is a large body of NLP literature on punctu-
ation. Most of it, however, is concerned with aiding
syntactic analysis of sentences and with developing
comma checkers, much based on (Nunberg, 1990).
Pattern-based relation extraction methods (e.g.,
(Davidov and Rappoport, 2008; Davidov et al,
2007; Banko et al, 2007; Pasca et al, 2006; Sekine,
2006)) could in theory be used to extract relations
represented by commas. However, the types of
patterns used in web-scale lexical approaches cur-
rently constrain discovered patterns to relatively
short spans of text, so will most likely fail on
structures whose arguments cover large spans (for
example, appositional clauses containing relative
clauses). Relation extraction approaches such as
(Roth and Yih, 2004; Roth and Yih, 2007; Hirano
et al, 2007; Culotta and Sorenson, 2004; Zelenko et
al., 2003) focus on relations between Named Enti-
ties; such approaches miss the more general apposi-
tion and list relations we recognize in this work, as
the arguments in these relations are not confined to
Named Entities.
Paraphrase Acquisition work such as that by (Lin
and Pantel, 2001; Pantel and Pennacchiotti, 2006;
Szpektor et al, 2004) is not constrained to named
entities, and by using dependency trees, avoids the
locality problems of lexical methods. However,
these approaches have so far achieved limited accu-
racy, and are therefore hard to use to augment exist-
ing NLP systems.
4 Corpus Annotation
For our corpus, we selected 1,000 sentences con-
taining at least one comma from the Penn Treebank
(Marcus et al, 1993) WSJ section 00, and manu-
ally annotated them with comma information3. This
annotated corpus served as both training and test
datasets (using cross-validation).
By studying a number of sentences from WSJ (not
among the 1,000 selected), we identified four signif-
icant types of relations expressed through commas:
SUBSTITUTE, ATTRIBUTE, LOCATION, and LIST.
Each of these types can in principle be expressed us-
ing more than a single comma. We define the notion
3The guidelines and annotations are available at http://
L2R.cs.uiuc.edu/
?
cogcomp/data.php.
1032
of a comma structure as a set of one or more commas
that all relate to the same relation in the sentence.
SUBSTITUTE indicates an IS-A relation. An ex-
ample is ?John Smith, a Renaissance artist, was fa-
mous?. By removing the relation expressed by the
commas, we can derive three sentences: ?John Smith
is a Renaissance artist?, ?John Smith was famous?,
and ?a Renaissance artist was famous?. Note that in
theory, the third relation will not be valid: one exam-
ple is ?The brothers, all honest men, testified at the
trial?, which does not entail ?all honest men testified
at the trial?. However, we encountered no examples
of this kind in the corpus, and leave this refinement
to future work.
ATTRIBUTE indicates a relation where one argu-
ment describes an attribute of the other. For ex-
ample, from ?John, who loved chocolate, ate with
gusto?, we can derive ?John loved chocolate? and
?John ate with gusto?.
LOCATION indicates a LOCATED-IN relation. For
example, from ?Chicago, Illinois saw some heavy
snow today? we can derive ?Chicago is located in
Illinois? and ?Chicago saw some heavy snow today?.
LIST indicates that some predicate or property
is applied to multiple entities. In our annotation,
the list does not generate explicit relations; instead,
the boundaries of the units comprising the list are
marked so that they can be treated as a single unit,
and are considered to be related by the single rela-
tion ?GROUP?. For example, the derivation of ?John,
James and Kelly all left last week? is written as
?[John, James, and Kelly] [all left last week]?.
Any commas not fitting one of the descriptions
above are designated as OTHER. This does not in-
dicate that the comma signifies no relations, only
that it does not signify a relation of interest in this
work (future work will address relations currently
subsumed by this category). Analysis of 120 OTHER
commas show that approximately half signify clause
boundaries, which may occur when sentence con-
stituents are reordered for emphasis, but may also
encode implicit temporal, conditional, and other re-
lation types (for example, ?Opening the drawer, he
found the gun.?). The remainder comprises mainly
coordination structures (for example, ?Although he
won, he was sad?) and discourse markers indicating
inter-sentence relations (such as ?However, he soon
cheered up.?). While we plan to develop an anno-
Rel. Type Avg. Agreement # of Commas # of Rel.s
SUBSTITUTE 0.808 243 729
ATTRIBUTE 0.687 193 386
LOCATION 0.929 71 140
LIST 0.803 230 230
OTHER 0.949 909 0
Combined 0.869 1646 1485
Table 1: Average inter-annotator agreement for identify-
ing relations.
tation scheme for such relations, this is beyond the
scope of the present work.
Four annotators annotated the same 10% of the
WSJ sentences in order to evaluate inter-annotator
agreement. The remaining sentences were divided
among the four annotators. The resulting corpus was
checked by two judges and the annotation corrected
where appropriate; if the two judges disagreed, a
third judge was consulted and consensus reached.
Our annotators were asked to identify comma struc-
tures, and for each structure to write its relation type,
its arguments, and all possible simplified version(s)
of the original sentence in which the relation implied
by the comma has been removed. Arguments must
be contiguous units of the sentence and will be re-
ferred to as chunks hereafter. Agreement statistics
and the number of commas and relations of each
type are shown in Table 4. The Accuracy closely ap-
proximates Kappa score in this case, since the base-
line probability of chance agreement is close to zero.
5 A Sentence Tranformation Rule Learner
(ASTRL)
In this section, we describe a new machine learning
system that learns Sentence Transformation Rules
(STRs) for comma resolution. We first define the
hypothesis space (i.e., STRs) and two operations ?
substitution and introduction. We then define the
feature space, motivating the use of Syntactic Parse
annotation to learn STRs. Finally, we describe the
ASTRL algorithm.
5.1 Sentence Transformation Rules
A Sentence Transformation Rule (STR) takes a
parse tree as input and generates new sentences. We
formalize an STR as the pair l ? r, where l is a
tree fragment that can consist of non-terminals, POS
tags and lexical items. r is a set {ri}, each ele-
ment of which is a template that consists of the non-
1033
terminals of l and, possibly, some new tokens. This
template is used to generate a new sentence, called a
relation.
The process of applying an STR l ? r to a parse
tree T of a sentence s begins with finding a match for
l in T . A match is said to be found if l is a subtree
of T . If matched, the non-terminals of each ri are
instantiated with the terminals that they cover in T .
Instantiation is followed by generation of the output
relations in one of two ways: introduction or sub-
stitution, which is specified by the corresponding ri.
If an ri is marked as an introductory one, then the
relation is the terminal sequence obtained by replac-
ing the non-terminals in ri with their instantiations.
For substitution, firstly, the non-terminals of the ri
are replaced by their instantiations. The instantiated
ri replaces all the terminals in s that are covered by
the l-match. The notions of introduction and substi-
tution were motivated by ideas introduced in (Bar-
Haim et al, 2007).
Figure 1 shows an example of an STR and Figure
2 shows the application of this STR to a sentence. In
the first relation, NP1 and NP2 are instantiated with
the corresponding terminals in the parse tree. In the
second and third relations, the terminals of NP1 and
NP2 replace the terminals covered by NPp.
LHS: NPp
NP1 , NP2 ,
RHS:
1. NP1 be NP2 (introduction)
2. NP1 (substitution)
3. NP2 (substitution)
Figure 1: Example of a Sentence Transformation Rule. If
the LHS matches a part of a given parse tree, then the
RHS will generate three relations.
5.2 The Feature Space
In Section 2, we discussed the example where there
could be an ambiguity between a list and an apposi-
tion structure in the fragment two former Soviet re-
publics, Russia and China. In addition, simple sur-
face examination of the sentence could also identify
the noun phrases ?Shanghai Co-operation Organi-
zation (SCO)?, ?the fledgling association that binds
S
NPp
NP1
John Smith
, NP2
a renaissance
artist
,
V P
was
famous
RELATIONS:
1 [John Smith]/NP1 be [a renaissance artist]/NP2
2 [John Smith] /NP1 [was famous]
3 [a renaissance artist]/NP2 [was famous]
Figure 2: Example of application of the STR in Figure 1.
In the first relation, an introduction, we use the verb ?be?,
without dealing with its inflections. NP1 and NP2 are
both substitutions, each replacing NPp to generate the
last two relations.
two former Soviet Republics?, ?Russia? and ?China?
as the four members of a list. To resolve such ambi-
guities, we need a nested representation of the sen-
tence. This motivates the use of syntactic parse trees
as a logical choice of feature space. (Note, however,
that semantic and pragmatic ambiguities might still
remain.)
5.3 Algorithm Overview
In our corpus annotation, the relations and their ar-
gument boundaries (chunks) are explicitly marked.
For each training example, our learning algorithm
first finds the smallest valid STR ? the STR with the
smallest LHS in terms of depth. Then it refines the
LHS by specializing it using statistics taken from
the entire data set.
5.4 Generating the Smallest Valid STR
To transform an example into the smallest valid
STR, we utilize the augmented parse tree of the
sentence. For each chunk in the sentence, we find
the lowest node in the parse tree that covers the
chunk and does not cover other chunks (even par-
tially). It may, however, cover words that do not
belong to any chunk. We refer to such a node as
a chunk root. We then find the lowest node that cov-
ers all the chunk roots, referring to it as the pat-
tern root. The initial LHS consists of the sub-
tree of the parse tree rooted at the pattern root and
whose leaf nodes are all either chunk roots or nodes
that do not belong to any chunk. All the nodes are
labeled with the corresponding labels in the aug-
1034
mented parse tree. For example, if we consider the
parse tree and relations shown in Figure 2, then do-
ing the above procedure gives us the initial LHS
as S (NPp(NP1, NP2, ) V P ). The three relations
gives us the RHS with three elements ?NP1 be
NP2?, ?NP1 V P ? and ?NP1 V P ?, all three being
introduction.
This initial LHS need not be the smallest one that
explains the example. So, we proceed by finding the
lowest node in the initial LHS such that the sub-
tree of the LHS at that node can form a new STR
that covers the example using both introduction and
substitution. In our example, the initial LHS has a
subtree, NPp(NP1, NP2, ) that can cover all the re-
lations with the RHS consisting of ?NP1 be NP2?,
NP1 and NP2. The first RHS is an introduction,
while the second and the third are both substitutions.
Since no subtree of this LHS can generate all three
relations even with substitution, this is the required
STR. The final step ensures that we have the small-
est valid STR at this stage.
5.5 Statistical Refinement
The STR generated using the procedure outlined
above explains the relations generated by a single
example. In addition to covering the relations gen-
erated by the example, we wish to ensure that it does
not cover erroneous relations by matching any of the
other comma types in the annotated data.
Algorithm 1 ASTRL: A Sentence Transformation
Rule Learning.
1: for all t: Comma type do
2: Initialize STRList[t] = ?
3: p = Set of annotated examples of type t
4: n = Annotated examples of all other types
5: for all x ? p do
6: r = Smallest Valid STR that covers x
7: Get fringe of r.LHS using the parse tree
8: S = Score(r,p,n)
9: Sprev = ??
10: while S 6= Sprev do
11: if adding some fringe node to r.LHS causes a signifi-
cant change in score then
12: Set r = New rule that includes that fringe node
13: Sprev = S
14: S = Score(r,p,n)
15: Recompute new fringe nodes
16: end if
17: end while
18: Add r to STRList[t]
19: Remove all examples from p that are covered by r
20: end for
21: end for
For this purpose, we specialize the LHS so that it
covers as few examples from the other comma types
as possible, while covering as many examples from
the current comma type as possible. Given the most
general STR, we generate a set of additional, more
detailed, candidate rules. Each of these is obtained
from the original rule by adding a single node to
the tree pattern in the rule?s LHS, and updating the
rule?s RHS accordingly. We then score each of the
candidates (including the original rule). If there is
a clear winner, we continue with it using the same
procedure (i.e., specialize it). If there isn?t a clear
winner, we stop and use the current winner. After
finishing with a rule (line 18), we remove from the
set of positive examples of its comma type all exam-
ples that are covered by it (line 19).
To generate the additional candidate rules that we
add, we define the fringe of a rule as the siblings
and children of the nodes in its LHS in the original
parse tree. Each fringe node defines an additional
candidate rule, whose LHS is obtained by adding
the fringe node to the rule?s LHS tree. We refer to
the set of these candidate rules, plus the original one,
as the rule?s fringe rules. We define the score of an
STR as
Score(Rule,p,n) = Rp|p| ?
Rn
|n|
where p and n are the set of positive and negative
examples for this comma type, and Rp and Rn are
the number of positive and negative examples that
are covered by the STR. For each example, all exam-
ples annotated with the same comma type are pos-
itive while all examples of all other comma types
are negative. The score is used to select the win-
ner among the fringe rules. The complete algorithm
we have used is listed in Algorithm 1. For conve-
nience, the algorithm?s main loop is given in terms
of comma types, although this is not strictly nec-
essary. The stopping criterion in line 11 checks
whether any fringe rule has a significantly better
score than the rule it was derived from, and exits the
specialization loop if there is none.
Since we start with the smallest STR, we only
need to add nodes to it to refine it and never have
to delete any nodes from the tree. Also note that the
algorithm is essentially a greedy algorithm that per-
forms a single pass over the examples; other, more
1035
complex, search strategies could also be used.
6 Evaluation
6.1 Experimental Setup
To evaluate ASTRL, we used the WSJ derived cor-
pus. We experimented with three scenarios; in two
of them we trained using the gold standard trees
and then tested on gold standard parse trees (Gold-
Gold), and text annotated using a state-of-the-art sta-
tistical parser (Charniak and Johnson, 2005) (Gold-
Charniak), respectively. In the third, we trained and
tested on the Charniak Parser (Charniak-Charniak).
In gold standard parse trees the syntactic cate-
gories are annotated with functional tags. Since cur-
rent statistical parsers do not annotate sentences with
such tags, we augment the syntactic trees with the
output of a Named Entity tagger. For the Named
Entity information, we used a publicly available NE
Recognizer capable of recognizing a range of cat-
egories including Person, Location and Organiza-
tion. On the CoNLL-03 shared task, its f-score is
about 90%4. We evaluate our system from different
points of view, as described below. For all the eval-
uation methods, we performed five-fold cross vali-
dation and report the average precision, recall and
f-scores.
6.2 Relation Extraction Performance
Firstly, we present the evaluation of the performance
of ASTRL from the point of view of relation ex-
traction. After learning the STRs for the different
comma types using the gold standard parses, we
generated relations by applying the STRs on the test
set once. Table 2 shows the precision, recall and
f-score of the relations, without accounting for the
comma type of the STR that was used to generate
them. This metric, called the Relation metric in fur-
ther discussion, is the most relevant one from the
point of view of the TE task. Since a list does not
generate any relations in our annotation scheme, we
use the commas to identify the list elements. Treat-
ing each list in a sentence as a single relation, we
score the list with the fraction of its correctly identi-
fied elements.
In addition to the Gold-Gold and Gold-Charniak
4A web demo of the NER is at http://L2R.cs.uiuc.
edu/
?
cogcomp/demos.php.
settings described above, for this metric, we also
present the results of the Charniak-Charniak setting,
where both the train and test sets were annotated
with the output of the Charniak parser. The improve-
ment in recall in this setting over the Gold-Charniak
case indicates that the parser makes systematic er-
rors with respect to the phenomena considered.
Setting P R F
Gold-Gold 86.1 75.4 80.2
Gold-Charniak 77.3 60.1 68.1
Charniak-Charniak 77.2 64.8 70.4
Table 2: ASTRL performance (precision, recall and f-
score) for relation extraction. The comma types were
used only to learn the rules. During evaluation, only the
relations were scored.
6.3 Comma Resolution Performance
We present a detailed analysis of the performance of
the algorithm for comma resolution. Since this paper
is the first one that deals with the task, we could not
compare our results to previous work. Also, there
is no clear baseline to use. We tried a variant of
the most frequent baseline common in other disam-
biguation tasks, in which we labeled all commas as
OTHER (the most frequent type) except when there
are list indicators like and, or and but in adjacent
chunks (which are obtained using a shallow parser),
in which case the commas are labeled LIST. This
gives an average precision 0.85 and an average recall
of 0.36 for identifying the comma type. However,
this baseline does not help in identifying relations.
We use the following approach to evaluate the
comma type resolution and relation extraction per-
formance ? a relation extracted by the system is con-
sidered correct only if both the relation and the type
of the comma structure that generated it are correctly
identified. We call this metric the Relation-Type
metric. Another way of measuring the performance
of comma resolution is to measure the correctness of
the relations per comma type. In both cases, lists are
scored as in the Relation metric. The performance of
our system with respect to these two metrics are pre-
sented in Table 3. In this table, we also compare the
performance of the STRs learned by ASTRL with
the smallest valid STRs without further specializa-
tion (i.e., using just the procedure outlined in Sec-
tion 5.4).
1036
Type Gold-Gold Setting Gold-Charniak Setting
Relation-Type metric
Smallest Valid STRs ASTRL Smallest Valid STRs ASTRL
P R F P R F P R F P R F
Total 66.2 76.1 70.7 81.8 73.9 77.6 61.0 58.4 59.5 72.2 59.5 65.1
Relations Metric, Per Comma Type
ATTRIBUTE 40.4 68.2 50.4 70.6 59.4 64.1 35.5 39.7 36.2 56.6 37.7 44.9
SUBSTITUTE 80.0 84.3 81.9 87.9 84.8 86.1 75.8 72.9 74.3 78.0 76.1 76.9
LIST 70.9 58.1 63.5 76.2 57.8 65.5 58.7 53.4 55.6 65.2 53.3 58.5
LOCATION 93.8 86.4 89.1 93.8 86.4 89.1 70.3 37.2 47.2 70.3 37.2 47.2
Table 3: Performance of STRs learned by ASTRL and the smallest valid STRs in identifying comma types and
generating relations.
There is an important difference between the Re-
lation metric (Table 2) and the Relation-type met-
ric (top part of Table 3) that depends on the seman-
tic interpretation of the comma types. For example,
consider the sentence ?John Smith, 59, went home.?
If the system labels the commas in this as both AT-
TRIBUTE and SUBSTITUTE, then, both will gener-
ate the relation ?John Smith is 59.? According to
the Relation metric, there is no difference between
them. However, there is a semantic difference be-
tween the two sentences ? the ATTRIBUTE relation
says that being 59 is an attribute of John Smith while
the SUBSTITUTE relation says that John Smith is the
number 59. This difference is accounted for by the
Relation-Type metric.
From this standpoint, we can see that the special-
ization step performed in the full ASTRL algorithm
greatly helps in disambiguating between the AT-
TRIBUTE and SUBSTITUTE types and consequently,
the Relation-Type metric shows an error reduction
of 23.5% and 13.8% in the Gold-Gold and Gold-
Charniak settings respectively. In the Gold-Gold
scenario the performance of ASTRL is much better
than in the Gold-Charniak scenario. This reflects the
non-perfect performance of the parser in annotating
these sentences (parser F-score of 90%).
Another key evaluation question is the per-
formance of the method in identification of the
OTHER category. A comma is judged to be as
OTHER if no STR in the system applies to it.
The performance of ASTRL in this aspect is pre-
sented in Table 4. The categorization of this cate-
gory is important if we wish to further classify the
OTHER commas into finer categories.
Setting P R F
Gold-Gold 78.9 92.8 85.2
Gold-Charniak 72.5 92.2 81.2
Table 4: ASTRL performance (precision, recall and f-
score) for OTHER identification.
7 Conclusions
We defined the task of comma resolution, and devel-
oped a novel machine learning algorithm that learns
Sentence Transformation Rules to perform this task.
We experimented with both gold standard and parser
annotated sentences, and established a performance
level that seems good for a task of this complexity,
and which will provide a useful measure of future
systems developed for this task. When given au-
tomatically parsed sentences, performance degrades
but is still much higher than random, in both sce-
narios. We designed a comma annotation scheme,
where each comma unit is assigned one of four types
and an inference rule mapping the patterns of the
unit with the entailed relations. We created anno-
tated datasets which will be made available over the
web to facilitate further research.
Future work will investigate four main directions:
(i) studying the effects of inclusion of our approach
on the performance of Textual Entailment systems;
(ii) using features other than those derivable from
syntactic parse and named entity annotation of the
input sentence; (iii) recognizing a wider range of im-
plicit relations, represented by commas and in other
ways; (iv) adaptation to other domains.
Acknowledgement
The UIUC authors were supported by NSF grant
ITR IIS-0428472, DARPA funding under the Boot-
strap Learning Program and a grant from Boeing.
1037
References
M. Banko, M. Cafarella, M. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Proc. of IJCAI, pages 2670?2676.
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
2007. Semantic inference at the lexical-syntactic level.
In Proc. of AAAI, pages 871?876.
M. Bayraktar, B. Say, and V. Akman. 1998. An analysis
of english punctuation: The special case of comma.
International Journal of Corpus Linguistics, 3(1):33?
57.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
of the Annual Meeting of the ACL, pages 173?180.
A. Culotta and J. Sorenson. 2004. Dependency tree ker-
nels for relation extraction. In Proc. of the Annual
Meeting of the ACL, pages 423?429.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge., volume 3944. Springer-Verlag, Berlin.
D. Davidov and A. Rappoport. 2008. Unsupervised dis-
covery of generic relationships using pattern clusters
and its evaluation by automatically generated sat anal-
ogy questions. In Proc. of the Annual Meeting of the
ACL.
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by web mining. In Proc. of the Annual Meeting
of the ACL, pages 232?239.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and
M. Sammons. 2005. An inference model for seman-
tic entailment in natural language. In Proc. of AAAI,
pages 1678?1679.
T. Hirano, Y. Matsuo, and G. Kikui. 2007. Detecting
semantic relations between named entities in text using
contextual features. In Proc. of the Annual Meeting of
the ACL, pages 157?160.
D. Lin and P. Pantel. 2001. DIRT: discovery of inference
rules from text. In Proc. of ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining 2001,
pages 323?328.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
G. Nunberg. 1990. CSLI Lecture Notes 18: The Lin-
guistics of Punctuation. CSLI Publications, Stanford,
CA.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In Proc. of the Annual Meeting of the
ACL, pages 113?120.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Proc. of the Annual Meeting of
the ACL, pages 809?816.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 1?8. Association for
Computational Linguistics.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
B. Say and V. Akman. 1997. Current approaches to
punctuation in computational linguistics. Computers
and the Humanities, 30(6):457?469.
S. Sekine. 2006. On-demand information extraction. In
Proc. of the Annual Meeting of the ACL, pages 731?
738.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based of entailment relations. In Proc. of
EMNLP, pages 49?56.
S. van Delden and F. Gomez. 2002. Combining finite
state automata and a greedy learning algorithm to de-
termine the syntactic roles of commas. In Proc. of IC-
TAI, pages 293?300.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. Journal of Machine
Learning Research, 3:1083?1106.
1038
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 53?56,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Active Sample Selection for Named Entity Transliteration
Dan Goldwasser Dan Roth
Department of Computer Science
University of Illinois
Urbana, IL 61801
{goldwas1,danr}@uiuc.edu
Abstract
This paper introduces a new method for
identifying named-entity (NE) transliterations
within bilingual corpora. Current state-of-the-
art approaches usually require annotated data
and relevant linguistic knowledge which may
not be available for all languages. We show
how to effectively train an accurate transliter-
ation classifier using very little data, obtained
automatically. To perform this task, we intro-
duce a new active sampling paradigm for guid-
ing and adapting the sample selection process.
We also investigate how to improve the clas-
sifier by identifying repeated patterns in the
training data. We evaluated our approach us-
ing English, Russian and Hebrew corpora.
1 Introduction
This paper presents a new approach for constructing
a discriminative transliteration model.
Our approach is fully automated and requires little
knowledge of the source and target languages.
Named entity (NE) transliteration is the process of
transcribing a NE from a source language to a target
language based on phonetic similarity between the
entities. Figure 1 provides examples of NE translit-
erations in English Russian and Hebrew.
Identifying transliteration pairs is an important
component in many linguistic applications such as
machine translation and information retrieval, which
require identifying out-of-vocabulary words.
In our settings, we have access to source language
NE and the ability to label the data upon request.
We introduce a new active sampling paradigm that
Figure 1: NE in English, Russian and Hebrew.
aims to guide the learner toward informative sam-
ples, allowing learning from a small number of rep-
resentative examples. After the data is obtained it is
analyzed to identify repeating patterns which can be
used to focus the training process of the model.
Previous works usually take a generative approach,
(Knight and Graehl, 1997). Other approaches ex-
ploit similarities in aligned bilingual corpora; for ex-
ample, (Tao et al, 2006) combine two unsupervised
methods. (Klementiev and Roth, 2006) bootstrap
with a classifier used interchangeably with an un-
supervised temporal alignment method. Although
these approaches alleviate the problem of obtain-
ing annotated data, other resources are still required,
such as a large aligned bilingual corpus.
The idea of selectively sampling training samples
has been wildly discussed in machine learning the-
ory (Seung et al, 1992) and has been applied suc-
cessfully to several NLP applications (McCallum
and Nigam, 1998). Unlike other approaches,our ap-
proach is based on minimizing the distance between
the feature distribution of a comprehensive reference
set and the sampled set.
2 Training a Transliteration Model
Our framework works in several stages, as summa-
rized in Algorithm 1. First, a training set consisting
53
of NE transliteration pairs (ws, wt) is automatically
generated using an active sample selection scheme.
The sample selection process is guided by the Suf-
ficient Spanning Features criterion (SSF) introduced
in section 2.2, to identify informative samples in the
source language.An oracle capable of pairing a NE
in the source language with its counterpart in the tar-
get language is then used. Negative training samples
are generated by reshuffling the terms in these pairs.
Once the training data has been collected, the data
is analyzed to identify repeating patterns in the data
which are used to focus the training process by as-
signing weights to features corresponding to the ob-
served patterns. Finally, a linear model is trained us-
ing a variation of the averaged perceptron (Freund
and Schapire, 1998) algorithm. The remainder of
this section provides details about these stages; the
basic formulation of the transliteration model and
the feature extraction scheme is described in section
2.1, in section 2.2 the selective sampling process is
described and finally section 2.3 explains how learn-
ing is focused by using feature weights.
Input: Bilingual, comparable corpus (S , T ), set of
named entities NES from S, Reference
Corpus RS , Transliteration Oracle O,
Training Corpora D=DS ,DT
Output: Transliteration model M
Guiding the Sampling Process1
repeat2
select a set C ? NES randomly3
ws = argminw?Cdistance(R,DS ? {ws})4
D = D ? {Ws, O(Ws)}5
until distance(R,DS ? {Ws}) ? distance(R,DS) ;6
Determining Features Activation Strength7
Define W:f ? < s.t. foreach feature f ={fs, ft}8
W (f) = ](fs,ft)](fs) ?
](fs,ft)
](ft)9
Use D to train M;10
Algorithm 1: Constructing a transliteration
model.
2.1 Transliteration Model
Our transliteration model takes a discriminative ap-
proach; the classifier is presented with a word pair
(ws, wt) , where ws is a named entity and it is
asked to determine whether wt is a transliteration
Figure 2: Features extraction process
of the NE in the target language. We use a linear
classifier trained with a regularized perceptron up-
date rule (Grove and Roth, 2001) as implemented
in SNoW, (Roth, 1998). The classifier?s confi-
dence score is used for ranking of positively tagged
transliteration candidates. Our initial feature extrac-
tion scheme follows the one presented in (Klemen-
tiev and Roth, 2006), in which the feature space con-
sists of n-gram pairs from the two languages. Given
a sample, each word is decomposed into a set of sub-
strings of up to a given length (including the empty
string). Features are generated by pairing substrings
from the two sets whose relative positions in the
original words differ by one or less places; first each
word is decomposed into a set of substrings then
substrings from the two sets are coupled to complete
the pair representation. Figure 2 depicts this process.
2.2 Guiding the Sampling Process with SSF
The initial step in our framework is to generate a
training set of transliteration pairs; this is done by
pairing highly informative source language candi-
date NEs with target language counterparts. We de-
veloped a criterion for adding new samples, Suffi-
ciently Spanning Features (SSF), which quantifies
the sampled set ability to span the feature space.
This is done by evaluating the L-1 distance be-
tween the frequency distributions of source language
word fragments in the current sampled set and in
a comprehensive set of source language NEs, serv-
ing as reference. We argue that since the features
used for learning are n-gram features, once these
two distributions are close enough, our examples
space provides a good and concise characterization
of all named entities we will ever need to con-
sider. A special care should be given to choos-
ing an appropriate reference; as a general guide-
line the reference set should be representative of
the testing data. We collected a set R, consisting
54
of 50,000 NE by crawling through Wikipedia?s arti-
cles and using an English NER system available at
- http://L2R.cs.uiuc.edu/ cogcomp. The frequency
distribution was generated over all character level
bi-grams appearing in the text, as bi-grams best cor-
relate with the way features are extracted. Given a
reference text R, the n-grams distribution of R can be
defined as follows -DR(ngi) = ]ngi?
j ]ngj
,where ng
is an n-gram in R. Given a sample set S, we measure
the L1 distance between the distributions:
distance (R,S) =?ng?R | DR(ng)?DS(ng) | Sam-
ples decreasing the distance between the distribu-
tions were added to the training data. Given a set
C of candidates for annotation, a sample ws ? C
was added to the training set, if -
ws = argminw?Cdistance(R,DS ? {ws}).
A sample set is said to have SSF, if the distance re-
mains constant as more samples are added.
2.2.1 Transliteration Oracle Implementation
The transliteration oracle is essentially a mapping
between the named entities, i.e. given an NE in the
source language it provides the matching NE in the
target language. An automatic oracle was imple-
mented by crawling through Wikipedia topic aligned
document pairs. Given a pair of topic aligned doc-
uments in the two languages, the topic can be iden-
tified either by identifying the top ranking terms or
by simply identifying the title of the documents. By
choosing documents in Wikipedia?s biography cate-
gory we ensured that the topic of the documents is
person NE.
2.3 Training the transliteration model
The feature extraction scheme we use generates fea-
tures by coupling substrings from the two terms.
Ideally, given a positive sample, it is desirable that
paired substrings would encode phonetically simi-
lar or a distinctive context in which the two scripts
correlate. Given enough positive samples, such fea-
tures will appear with distinctive frequency. Tak-
ing this idea further, these features were recognized
by measuring the co-occurrence frequency of sub-
strings of up to two characters in both languages.
Each feature f=(fs, ft) composed of two substrings
taken from English and Hebrew words was associ-
ated with weight. W (f) = ](fs,ft)](fs) ?
](fs,ft)
](ft) where
Data Set Method Rus Heb
1 SSF 0.68 NA
1 KR?06 0.63 NA
2 SSF 0.71 0.52
Table 1: Results summary. The numbers are the pro-
portion of NE recognized in the target language. Lines 1
and 2 compare the results of SSF directed approach with
the baseline system on the first dataset. Line 3 summa-
rizes the results on the second dataset.
](fs, ft) is the number of occurrences of that feature
in the positive sample set, and ](fL) is the number of
occurrences of an individual substring, in any of the
features extracted from positive samples in the train-
ing set. The result of this process is a weight table,
in which, as we empirically tested, the highest rank-
ing weights were assigned to features that preserve
the phonetic correlation between the two languages.
To improve the classifier?s learning rate, the learn-
ing process is focused around these features. Given
a sample, the learner is presented with a real-valued
feature vector instead of a binary vector, in which
each value indicates both that the feature is active
and its activation strength - i.e. the weight assigned
to it.
3 Evaluation
We evaluated our approach in two settings; first, we
compared our system to a baseline system described
in (Klementiev and Roth, 2006). Given a bilingual
corpus with the English NE annotated, the system
had to discover the NE in target language text. We
used the English-Russian news corpus used in the
baseline system. NEs were grouped into equiva-
lence classes, each containing different variations of
the same NE. We randomly sampled 500 documents
from the corpus. Transliteration pairs were mapped
into 97 equivalence classes, identified by an expert.
In a second experiment, different learning parame-
ters such as selective sampling efficiency and feature
weights were checked. 300 English-Russian and
English-Hebrew NE pairs were used; negative sam-
ples were generated by coupling every English NE
with all other target language NEs. Table 1 presents
the key results of these experiments and compared
with the baseline system.
55
Extraction Number Recall Recall
method of Top one Top two
samples
Directed 200 0.68 0.74
Random 200 0.57 0.65
Random 400 0.63 0.71
Table 2: Comparison of correctly identified English-
Russian transliteration pairs in news corpus. The model
trained using selective sampling outperforms models
trained using random sampling, even when trained with
twice the data. The top one and top two results
columns describe the proportion of correctly identified
pairs ranked in the first and top two places, respectively.
3.1 Using SSF directed sampling
Table 2 describes the effect of directed sampling
in the English-Russian news corpora NE discovery
task. Results show that models trained using selec-
tive sampling can outperform models trained with
more than twice the amount of data.
3.2 Training using feature weights
Table 3 describes the effect training the model with
weights.The training set consisted of 150 samples
extracted using SSF directed sampling. Three varia-
tions were tested - training without feature weights,
using the feature weights as the initial network
weights without training and training with weights.
The results clearly show that using weights for train-
ing improve the classifier?s performance for both
Russian and Hebrew. It can also be observed that
in many cases the correct pair was ranked in any of
the top five places.
4 Conclusions and future work
In this paper we presented a new approach for con-
structing a transliteration model automatically and
efficiently by selectively extracting transliteration
samples covering relevant parts of the feature space
and focusing the learning process on these features.
We show that our approach can outperform sys-
tems requiring supervision, manual intervention and
a considerable amount of data. We propose a new
measure for selective sample selection which can be
used independently. We currently investigate apply-
ing it in other domains with potentially larger feature
Learning Russian Hebrew
Train- Feature Top Top Top Top
ing weights one five one five
+ + 0.71 0.89 0.52 0.88
- + 0.63 0.82 0.33 0.59
+ - 0.64 0.79 0.37 0.68
Table 3: The proportion of correctly identified transliter-
ation pairs with/out using weights and training. The top
one and top five results columns describe the proportion
of correctly identified pairs ranked in the first place and
in any of the top five places, respectively. The results
demonstrate that using feature weights improves perfor-
mance for both target languages.
space than used in this work. Another aspect inves-
tigated is using our selective sampling for adapting
the learning process for data originating from dif-
ferent sources; using the a reference set representa-
tive of the testing data, training samples, originating
from a different source , can be biased towards the
testing data.
5 Acknowledgments
Partly supported by NSF grant ITR IIS-0428472 and
DARPA funding under the Bootstrap Learning Pro-
gram.
References
Y. Freund and R. E. Schapire. 1998. Large margin clas-
sification using the perceptron algorithm. In COLT.
A. Grove and D. Roth. 2001. Linear concepts and hidden
variables. ML, 42.
A. Klementiev and D. Roth. 2006. Weakly supervised
named entity transliteration and discovery from multi-
lingual comparable corpora. In ACL.
K. Knight and J. Graehl. 1997. Machine transliteration.
In EACL.
D. K. McCallum and K. Nigam. 1998. Employing EM
in pool-based active learning for text classification. In
ICML.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In AAAI.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In COLT.
T. Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai. 2006.
Unsupervised named entity transliteration using tem-
poral and phonetic correlation. In EMNLP.
56
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 57?60,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Framework for Entailed Relation Recognition
Dan Roth Mark Sammons V.G.Vinod Vydiswaran
University of Illinois at Urbana-Champaign
{danr|mssammon|vgvinodv}@illinois.edu
Abstract
We define the problem of recognizing entailed re-
lations ? given an open set of relations, find all oc-
currences of the relations of interest in a given doc-
ument set ? and pose it as a challenge to scalable
information extraction and retrieval. Existing ap-
proaches to relation recognition do not address well
problems with an open set of relations and a need
for high recall: supervised methods are not eas-
ily scaled, while unsupervised and semi-supervised
methods address a limited aspect of the problem, as
they are restricted to frequent, explicit, highly lo-
calized patterns. We argue that textual entailment
(TE) is necessary to solve such problems, propose
a scalable TE architecture, and provide preliminary
results on an Entailed Relation Recognition task.
1 Introduction
In many information foraging tasks, there is a need
to find all text snippets relevant to a target concept.
Patent search services spend significant resources
looking for prior art relevant to a specified patent
claim. Before subpoenaed documents are used in
a court case or intelligence data is declassified, all
sensitive sections need to be redacted. While there
may be a specific domain for a given application,
the set of target concepts is broad and may change
over time. For these knowledge-intensive tasks,
we contend that feasible automated solutions re-
quire techniques which approximate an appropri-
ate level of natural language understanding.
Such problems can be formulated as a relation
recognition task, where the information need is ex-
pressed as tuples of arguments and relations. This
structure provides additional information which
can be exploited to precisely fulfill the informa-
tion need. Our work introduces the Entailed Rela-
tion Recognition paradigm, which leverages a tex-
tual entailment system to try to extract all relevant
passages for a given structured query without re-
quiring relation-specific training data. This con-
trasts with Open Information Extraction (Banko
and Etzioni, 2008) and On-Demand Information
Extraction (Sekine, 2006), which aim to extract
large databases of open-ended facts, and with su-
pervised relation extraction, which requires addi-
tional supervised data to learn new relations.
Specifically, the contributions of this paper are:
1. Introduction of the entailed relation recognition
framework; 2. Description of an architecture and a
system which uses structured queries and an exist-
ing entailment engine to perform relation extrac-
tion; 3. Empirical assessment of the system on a
corpus of entailed relations.
2 Entailed Relation Recognition (ERR)
In the task of Entailed Relation Recognition, a cor-
pus and an information need are specified. The
corpus comprises all text spans (e.g. paragraphs)
contained in a set of documents. The information
need is expressed as a set of tuples encoding rela-
tions and entities of interest, where entities can be
of arbitrary type. The objective is to retrieve all
relevant text spans that a human would recognize
as containing a relation of interest. For example:
Information Need: An organization acquires weapons.
Text 1: ...the recent theft of 500 assault rifles by FARC...
Text 2: ...the report on FARC activities made three main ob-
servations. First, their allies supplied them with the 3? mor-
tars used in recent operations. Second, ...
Text 3: Amnesty International objected to the use of artillery
to drive FARC militants from heavily populated areas.
An automated system should identify Texts 1 and
2 as containing the relation of interest, and Text 3
as irrelevant. The system must therefore detect
relation instances that cross sentence boundaries
(?them? maps to ?FARC?, Text 2), and that re-
quire inference (?theft? implies ?acquire?, Text 1).
It must also discern when sentence structure pre-
cludes a match (?Amnesty International... use...
artillery? does not imply ?Amnesty International
57
acquires artillery?, Text 3).
The problems posed by instances like Text 2
are beyond the scope of traditional unsuper-
vised and semi-supervised relation-extraction ap-
proaches such as those used by Open IE and On-
Demand IE, which are constrained by their de-
pendency on limited, sentence-level structure and
high-frequency, highly local patterns, in which
relations are explicitly expressed as verbs and
nouns. Supervised methods such as (Culotta and
Sorensen, 2004) and (Roth and Yih, 2004) pro-
vide only a partial solution, as there are many pos-
sible relations and entities of interest for a given
domain, and such approaches require new anno-
tated data each time a new relation or entity type is
needed. Information Retrieval approaches are op-
timized for document-level performance, and en-
hancements like pseudo-feedback (Rocchio, 1971)
are less applicable to the localized text spans
needed in the tasks of interest; as such, it is un-
likely that they will reliably retrieve all correct in-
stances, and not return superficially similar but in-
correct instances (such as Text 3) with high rank.
Attempts have been made to apply Textual En-
tailment in larger scale applications. For the task
of Question Answering, (Harabagiu and Hickl,
2006) applied a TE component to rerank candidate
answers returned by a retrieval step. However, QA
systems rely on redundancy in the same way Open
IE does: a large document set has so many in-
stances of a given relation that at least some will
be sufficiently explicit and simple that standard IR
approaches will retrieve them. A single correct in-
stance suffices to complete the QA task, but does
not meet the needs of the task outlined here.
Recognizing relation instances requiring infer-
ence steps, in the absence of labeled training data,
requires a level of text understanding. A suit-
able proxy for this would be a successful Textual
Entailment Recognition (TE) system. (Dagan et
al., 2006) define the task of Recognizing Textual
Entailment (RTE) as: ...a directional relation be-
tween two text fragments, termed T ? the entailing
text, and H ? the entailed text. T entails H if, typ-
ically, a human reading T would infer that H is
most likely true. For relation recognition, the rela-
tion triple (e.g. ?Organization acquires weapon?)
is the hypothesis, and a candidate text span that
might contain the relation is the text. The def-
inition of RTE clearly accommodates the range
of phenomena described for the examples above.
However, the more successful TE systems (e.g.
(Hickl and Bensley, 2007)) are typically resource
intensive, and cannot scale to large retrieval tasks
if a brute force approach is used.
We define the task of Entailed Relation Recog-
nition thus: Given a text collection D, and an in-
formation need specified in a set of [argument, re-
lation, argument] triples S: for each triple s ? S,
identify all texts d ? D such that d entails s.
The information need triples, or queries, encode
relations between arbitrary entities (specifically,
these are not constrained to be Named Entities).
This problem is distinct from recent work in
Textual Entailment as we constrain the structure
of the Hypothesis to be very simple, and we re-
quire that the task be of a significantly larger scale
than the RTE tasks to date (which are typically of
the order of 800 Text-Hypothesis pairs).
3 Scalable ERR Algorithm
Our scalable ERR approach, SERR, consists of
two stages: expanded lexical retrieval, and entail-
ment recognition. The SERR algorithm is pre-
sented in Fig. 1. The goal is to scale Textual
Entailment up to a task involving large corpora,
where hypotheses (queries) may be entailed by
multiple texts. The task is kept tractable by de-
composing TE capabilities into two steps.
The first step, Expanded Lexical Retrieval
(ELR), uses shallow semantic resources and simi-
larity measures, thereby incorporating some of the
semantic processing used in typical TE systems.
This is required to retrieve, with high recall, se-
mantically similar content that may not be lexi-
cally similar to query terms, to ensure return of
a set of texts that are highly likely to contain the
concept of interest.
The second step applies a textual entailment
system to this text set and the query in order to
label the texts as ?relevant? or ?irrelevant?, and re-
quires deeper semantic resources in order to dis-
cern texts containing the concept of interest from
those that do not. This step emphasizes higher pre-
cision, as it filters irrelevant texts.
3.1 Implementation of SERR
In the ELR stage, we use a structured query that
allows more precise search and differential query
expansion for each query element. Semantic units
in the texts (e.g. Named Entities, phrasal verbs)
are indexed separately from words; each index is
58
SERR Algorithm
SETUP:
Input: Text set D
Output: Indices {I} over D
for all texts d ? D
Annotate d with local semantic content
Build Search Indices {I} over D
APPLICATION:
Input: Information need S
EXPANDED LEXICAL RETRIEVAL (ELR)(s):
R? ?
Expand s with semantically similar words
Build search query q
s
from s
R? k top-ranked texts for q
s
using indices {I}
return R
SERR:
Answer set A? ?
for all queries s ? S
R? ELR(s)
Answer set A
s
? ?
for all results r ? R
Annotate s, r with NLP resources
if r entails s
A
s
? A
s
? r
A? A ? {A
s
}
return A
Figure 1. SERR algorithm
a hierarchical similarity structure based on a type-
specific metric (e.g. WordNet-based for phrasal
verbs). Query structure is also used to selectively
expand query terms using similarity measures re-
lated to types of semantic units, including distribu-
tional similarity (Lin and Pantel, 2001), and mea-
sures based on WordNet (Fellbaum, 1998).
We assess three different Textual Entailment
components: LexPlus, a lexical-level system
that achieves relatively good performance on the
RTE challenges, and two variants of Predicate-
based Textual Entailment, PTE-strict and PTE-
relaxed, which use a predicate-argument repre-
sentation. The former is constrained to select a
single predicate-argument structure from each re-
sult, which is compared to the query component-
by-component using similarity measures similar to
the LexPlus system. PTE-relaxed drops the single-
predicate constraint, and can be thought of as a
?bag-of-constituents? model. In both, features are
extracted based on the predicate-argument compo-
nents? match scores and their connecting structure,
and the rank assigned by ELR. These features are
used by a classifier that labels each result as ?rel-
evant? or ?irrelevant?. Training examples are se-
lected from the top 7 results returned by ELR for
queries corresponding to entailment pair hypothe-
ses from the RTE development corpora; test exam-
ples are similarly selected from results for queries
from the RTE test corpora (see section 3.2).
3.2 Entailed Relation Recognition Corpus
To assess performance on the ERR task, we de-
rive a corpus from the publicly available RTE
data. The corpus consists of a set S of informa-
tion needs in the form of [argument, relation, argu-
ment] triples, and a set D of text spans (short para-
graphs), half of which entail one or more s ? S
while the other half are unrelated to S. D com-
prises all 1, 950 Texts from the IE and IR sub-
tasks of the RTE Challenge 1?3 datasets. The
shorter hypotheses in these examples allow us to
automatically induce their structured query form
from their shallow semantic structure. S was au-
tomatically generated from the positive entailment
pairs in D, by annotating their hypotheses with a
publicly available SRL tagger (Punyakanok et al,
2008) and inferring the relation and two main ar-
guments to form the equivalent queries.
Since some Hypotheses and Texts appear mul-
tiple times in the RTE corpora, we automatically
extract mappings from positive Hypotheses to one
or more Texts by comparing hypotheses and texts
from different examples. This provides the label-
ing needed for evaluation. In the resulting corpus,
a wide range of relations are sparsely represented;
they exemplify many linguistic and semantic char-
acteristics required to infer the presence of non-
explicit relations.
4 Results and Discussion
Top # Basic ELR Rel.Impr. Err.Redu.
1 48.1% 55.2% +14.8% 13.7%
2 68.1% 72.8% +6.9% 14.7%
3 75.2% 78.5% +4.4% 17.7%
Table 1. Change in relevant results retrieved in top 3
positions for basic and expanded lexical retrieval
System Acc. Prec. Rec. F
1
Baseline 18.1 18.1 100.0 30.7
LexPlus 81.6 44.9 62.5 55.5
PTE-relax. 71.9 37.7 72.0 49.0
(0.1) (5.5) (6.2) (4.1)
PTE-strict 83.6 55.4 61.5 57.9
(1.3) (3.4) (7.9) (2.1)
Table 2. Comparison of performance of SERR with
different TE algorithms. Numbers in parentheses are
standard deviations.
Table 1 compares the results of SERR with and
59
# System RTE 1 RTE 2 RTE 3 Avg. Acc.
LexPlus 49.0 65.2 [3] 76.5 [2] 66.3
PTE-relaxed 54.5 (1.0) 68.7 (1.5) [3] 82.3 (2.0) [1] 71.2 (1.2)
PTE-strict 64.8 (2.3) [1] 71.2 (2.6) [3] 76.0 (3.2) [2] 71.8 (2.6)
Table 3. Performance (accuracy) of SERR system variants on RTE challenge
examples; numbers in parentheses are standard deviations, while numbers in
brackets indicate where systems would have ranked in the RTE evaluations.
Comparisons
Standard TE 3,802,500
SERR 13,650
Table 4. Entailment compar-
isons needed for standard TE
vs. SERR
without the ELR?s semantic enhancements. For
each rank k, the entries represent the proportion of
queries for which the correct answer was returned
in the top k positions. The semantic enhancements
improve the number of matched results at each of
the top 3 positions.
Table 2 compares variants of the SERR imple-
mentation. The baseline labels every result re-
turned by ELR as ?relevant?, giving high recall
but low precision. PTE-relaxed performs better
than baseline, but poorly compared to PTE-strict
and LexPlus. Our analysis shows that LexPlus
has a relatively high threshold, and correctly labels
as negative some examples mislabeled by PTE-
relaxed, which may match two of the three con-
stituents in a hypothesis and label that result as
positive. PTE-strict will correctly identify some
such examples as it will force some match edges to
be ignored, and will correctly identify some neg-
ative examples due to structural constraints even
when LexPlus finds matches for all query terms.
PTE-strict strikes the best balance between preci-
sion and recall on positive examples.
Table 3 shows the accuracy of SERR?s clas-
sification of the examples from each RTE chal-
lenge; results not returned in the top 7 ranks by
ELR are labeled ?irrelevant?. PTE-strict and PTE-
relaxed perform comparably overall, though PTE-
strict has more uniform results over the different
challenges. Both outperform the LexPlus system
overall, and perform well compared to the best re-
sults published for the RTE challenges.
The significant computational gain of SERR is
shown in Table 4, exhibiting the much greater
number of comparisons required by a brute force
TE approach compared to SERR: SERR performs
well compared to published results for RTE chal-
lenges 1-3, but makes only 0.36% of the TE com-
parisons needed by standard approaches on our
ERR task.
5 Conclusion
We have proposed an approach to solving the En-
tailed Relation Recognition task, based on Tex-
tual Entailment, and implemented a solution that
shows that a Textual Entailment Recognition sys-
tem can be scaled to a much larger IE problem
than that represented by the RTE challenges. Our
preliminary results demonstrate the utility of the
proposed architecture, which allows strong perfor-
mance in the RTE task and efficient application to
a large corpus (table 4).
Acknowledgments
We thank Quang Do, Yuancheng Tu, and Kevin
Small. This work is funded by a grant from Boeing
and by MIAS, a DHS-IDS Center for Multimodal
Information Access and Synthesis at UIUC.
References
[Banko and Etzioni2008] M. Banko and O. Etzioni. 2008.
The Tradeoffs Between Open and Traditional Relation Ex-
traction. In ACL-HLT, pages 28?36.
[Culotta and Sorensen2004] A. Culotta and J. Sorensen.
2004. Dependency Tree Kernels for Relation Extraction.
In ACL, pages 423?429.
[Dagan et al2006] I. Dagan, O. Glickman, and B. Magnini,
editors. 2006. The PASCAL Recognising Textual Entail-
ment Challenge., volume 3944. Springer-Verlag, Berlin.
[Fellbaum1998] C. Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
[Harabagiu and Hickl2006] S. Harabagiu and A. Hickl. 2006.
Methods for Using Textual Entailment in Open-Domain
Question Answering. In ACL, pages 905?912.
[Hickl and Bensley2007] A. Hickl and J. Bensley. 2007. A
Discourse Commitment-Based Framework for Recogniz-
ing Textual Entailment. In ACL, pages 171?176.
[Lin and Pantel2001] D. Lin and P. Pantel. 2001. Induction of
semantic classes from natural language text. In SIGKDD,
pages 317?322.
[Punyakanok et al2008] V. Punyakanok, D. Roth, and
W. Yih. 2008. The Importance of Syntactic Parsing and
Inference in Semantic Role Labeling. CL, 34(2).
[Rocchio1971] J. Rocchio, 1971. Relevance feedback in In-
formation Retrieval, pages 313?323. Prentice Hall.
[Roth and Yih2004] D. Roth and W. Yih. 2004. A linear pro-
gramming formulation for global inference in natural lan-
guage tasks. In CoNLL, pages 1?8.
[Sekine2006] S. Sekine. 2006. On-Demand Information Ex-
traction. In COLING/ACL, pages 731?738.
60
In: Proceedings of CoNLL-2000 and LLL-PO00, pages 1-6, Lisbon, Portugal, 2000. 
Learning in Natural Language: Theory and Algorithmic 
Approaches* 
Dan Roth 
Department  of Computer  Science 
University of Illinois at Urbana-Champaign 
1304 W Springfield Ave., Urbana, IL 61801 
danr@cs, uiuc. edu 
Abst rac t  
This article summarizes work on developing a
learning theory account for the major learning 
and statistics based approaches used in natural 
language processing. It shows that these ap- 
proaches can all be explained using a single dis- 
tribution free inductive principle related to the 
pac model of learning. Furthermore, they all 
make predictions using the same simple knowl- 
edge representation - a linear representation 
over a common feature space. This is signifi- 
cant both to explaining the generalization and 
robustness properties of these methods and to 
understanding how these methods might be ex- 
tended to learn from more structured, knowl- 
edge intensive xamples, as part of a learning 
centered approach to higher level natural lan- 
guage inferences. 
1 In t roduct ion  
Many important natural language inferences 
can be viewed as problems of resolving phonetic, 
syntactic, semantics or pragmatics ambiguities, 
based on properties of the surrounding context. 
It is generally accepted that a learning compo- 
nent must have a central role in resolving these 
context sensitive ambiguities, and a significant 
amount of work has been devoted in the last few 
years to developing learning methods for these 
tasks, with considerable success. Yet, our un- 
derstanding of when and why learning works in 
this domain and how it can be used to support 
increasingly higher level tasks is still lacking. 
This article summarizes work on developing a
learning theory account for the major learning 
approaches used in NL. 
While the major statistics based methods 
used in NLP are typically developed with a 
* This research is supported by NSF grants IIS-9801638, 
SBR-9873450 and IIS-9984168. 
Bayesian view in mind, the Bayesian principle 
cannot directly explain the success and robust- 
ness of these methods, since their probabilistic 
assumptions typically do not hold in the data. 
Instead, we provide this explanation using a sin- 
gle, distribution free inductive principle related 
to the pac model of learning. We describe the 
unified learning framework and show that, in 
addition to explaining the success and robust- 
ness of the statistics based methods, it also ap- 
plies to other machine learning methods, such 
as rule based and memory based methods. 
An important component of the view devel- 
oped is the observation that most methods use 
the same simple knowledge representation. This 
is a linear representation over a new feature 
space - a transformation f the original instance 
space to a higher dimensional nd more expres- 
sive space. Methods vary mostly algorithmicly, 
in ways they derive weights for features in this 
space. This is significant both to explaining 
the generalization properties of these methods 
and to developing an understanding forhow and 
when can these methods be extended to learn 
from more structured, knowledge intensive x- 
amples, perhaps hierarchically. These issues are 
briefly discussed and we emphasize the impor- 
tance of studying knowledge representation a d 
inference in developing a learning centered ap- 
proach to NL inferences. 
2 Learning Frameworks 
Generative probability models provide a princi- 
pled way to the study of statistical classification 
in complex domains uch as NL. It is common 
to assume a generative model for such data, es- 
timate its parameters from training data and 
then use Bayes rule to obtain a classifier for 
this model. In the context of NL most clas- 
sifters are derived from probabilistic language 
models which estimate the probability of a sen- 
tence 8 using Bayes rule, and then decompose 
this probability into a product of conditional 
probabilities according to the generative model. 
Pr(s) = Pr(wl, W2, . . .  Wn) ---- 
= H~=lPr(wilwl,...wi-1) = H~=lPr(wilhi) 
where hi is the relevant history when predicting 
wi, and s is any sequence of tokens, words, part- 
of-speech (pos) tags or other terms. 
This general scheme has been used to de- 
rive classifiers for a variety of natural lan- 
guage applications including speech applica- 
tions (Rab89), pos tagging (Kup92; Sch95), 
word-sense ambiguation (GCY93) and context- 
sensitive spelling correction (Go195). While the 
use of Bayes rule is harmless, most of the work 
in statistical language modeling and ambiguity 
resolution is devoted to estimating terms of the 
form Pr(wlh ). The generative models used to 
estimate these terms typically make Markov or 
other independence assumptions. It is evident 
from studying language data that these assump- 
tions are often patently false and that there 
are significant global dependencies both within 
and across sentences. For example, when using 
(Hidden) Markov Model (HMM) as a generative 
model for pos tagging, estimating the probabil- 
ity of a sequence of tags involves assuming that 
the pos tag ti of the word wi is independent of
other words in the sentence, given the preced- 
ing tag ti-1. It is not surprising therefore that 
this results in a poor estimate of the probabil- 
ity density function. However, classifiers built 
based on these false assumptions nevertheless 
seem to behave quite robustly in many cases. 
A different, distribution free inductive princi- 
ple that is related to the pac model of learning 
is the basis for the account developed here. 
In an instance of the agnostic variant of pac 
learning (Val84; Hau92; KSS94), a learner is 
given data elements (x, l) that are sampled ac- 
cording to some fixed but arbitrary distribu- 
tion D on X x {0, 1}. X is the instance space 
and I E {0, 1} is the label 1. D may simply re- 
flect the distribution of the data as it occurs 
"in nature" (including contradictions) without 
assuming that the labels are generated accord- 
ing to some "rule". Given a sample, the goal 
of the learning algorithm is to eventually out- 
put a hypothesis h from some hypothesis class 
7/ that closely approximates the data. The 
1The model can be extended todeal with any discrete 
or continuous range of the labels. 
true error of the hypothesis h is defined to 
be errorD(h) = Pr(x,O~D\[h(x) 7~ If, and the 
goal of the (agnostic) pac learner is to com- 
pute, for any distribution D, with high prob- 
ability (> 1 -5 ) ,  a hypothesis h E 7/ with 
true error no larger than ~ + inffhenerrorD(h). 
In practice, one cannot compute the true er- 
ror errorD(h). Instead, the input to the learn- 
ing algorithm is a sample S = {(x i,l)}i=li m of 
m labeled examples and the learner tries to 
find a hypothesis h with a small empirical er- 
ror errors(h) = I{x e Slh(x) ? l}l/ISl, and 
hopes that it behaves well on future examples. 
The hope that a classifier learned from a train- 
ing set will perform well on previously unseen 
examples is based on the basic inductive prin- 
ciple underlying learning theory (Val84; Vap95) 
which, stated informally, guarantees that if the 
training and the test data are sampled from the 
same distribution, good performance on large 
enough training sample guarantees good per- 
formance on the test data (i.e., good "true" er- 
ror). Moreover, the quality of the generalization 
is inversely proportional to the expressivity of 
the class 7-/. Equivalently, for a fixed sample 
size IsI, the quantified version of this princi- 
ple (e.g. (Hau92)) indicates how much can one 
count on a hypothesis elected according to its 
performance on S. Finally, notice the underly- 
ing assumption that the training and test data 
are sampled from the same distribution; this 
framework addresses this issue. (See (GR99).) 
In our discussion functions learned over the 
instance space X are not defined directly over 
the raw instances but rather over a transforma- 
tion of it to a feature space. A feature is an in- 
dicator function X : X ~ {0, 1} which defines a 
subset of the instance space - all those elements 
in X which are mapped to 1 by X- X denotes 
a class of such functions and can be viewed as 
a transformation f the instance space; each ex- 
ample (Xl, . . .  xn) E X is mapped to an example 
(Xi,...Xlxl) in the new space. We sometimes 
view a feature as an indicator function over the 
labeled instance space X x {0, 1} and say that 
X(x, l) = 1 for examples x E x (X)  with label l. 
3 Exp la in ing  Probabi l i s t ic  Methods  
Using the abovementioned inductive principle 
we describe a learning theory account hat ex- 
plains the success and robustness of statistics 
based classifiers (Rot99a). A variety of meth- 
ods used for learning in NL are shown to make 
their prediction using Linear Statistical Queries 
(LSQ) hypotheses. This is a family of linear 
predictors over a set of features which are di- 
rectly related to the independence assumptions 
of the probabilistic model assumed. The success 
of these classification methods is then shown to 
be due to the combination of two factors: 
? Low expressive power of the derived classifier. 
? Robustness properties hared by all linear sta- 
tistical queries hypotheses. 
Since the hypotheses are computed over a fea- 
ture space chosen so that they perform well on 
training data, learning theory implies that they 
perform well on previously unseen data, irre- 
spective of whether the underlying probabilistic 
assumptions hold. 
3.1 Robust  Learn ing  
This section defines a learning algorithm and 
a class of hypotheses with some generaliza- 
tion properties, that capture many probabilis- 
tic learning methods used in NLP. The learn- 
ing algorithm is a Statistical Queries(SQ) algo- 
r i thm (Kea93). An SQ algorithm can be viewed 
as a learning algorithm that interacts with its 
environment in a restricted way. Rather than 
viewing examples, the algorithm only requests 
the values of various statistics on the distribu- 
tion of the examples to construct its hypothesis. 
(E.g. "What is the probability that a randomly 
chosen example (x, l) has xi = 0 and l = 1"?) 
A statistical query has the form IX, l, 7-\], where 
X 6 X is a feature, l 6 {0, 1} is a further (op- 
tional) restriction imposed on the query and ~" 
is an error parameter. A call to the SQ oracle 
returns an estimate ~n of \[x,z,~\] 
P,\[xDj\] = PrD{ (X, i)lx(x) = 1 A i = l} 
which satisfies \]15x - Px\] < T. (We usually omit 
T and/or  l from this notation.) A statistical 
queries algorithm is a learning algorithm that 
constructs its hypothesis only using information 
received from an SQ oracle. An algorithm is 
said to use a query space X if it only makes 
queries of the form \[X, l, T\] where X 6 A'. An 
SQ algorithm is said to be a good learning al- 
gorithm if, with high probability, it outputs a 
hypothesis h with small error, using sample size 
that is polynomial in the relevant parameters. 
Given a query \[X, l, T\] the SQ oracle is sim- 
ulated by drawing a large sample S of labeled 
examples (x, l) according to D and evaluating 
Prs \ [x(x ,  l)\] = I{(x ,  l) : X(x, l) = l l} / IS l .  
Chernoff bounds guarantee that the nUmber of 
examples required to achieve tolerance T with 
probability at least 1 - 5 is polynomial in 1/T 
and log 1/5. (See (Zea93; Dec93; AD95)). 
Let X be a class of features and f : {0, 1} 
a function that depends only on the values 
~D for E X. Given x 6 X, a Linear Statis- \[x,~\] X 
tical Queries (LSQ) hypothesis predicts 
l argmaxte{o,1} ~xeX ^ D = f \ [x j \ ]  ({P\[x,z\] } ) "  X(x). 
Clearly, the LSQ is a linear discriminator over 
the feature space A', with coefficients f that 
are computed given (potentially all) the values 
^D P\[x,t\]" The definition generalizes naturally to 
non-binary classifiers; in this case, the discrim- 
inator between predicting l and other values is 
linear. A learning algorithm that outputs an 
LSQ hypothesis is called an LSQ algorithm. 
Example  3.1 The naive Bayes predictor 
(DH73) is derived using the assumption that 
given the label l E L the features' values are 
statistically independent. Consequently, the 
Bayes optimal prediction is given by: 
h(x) = argmaxteLH~n=l Pr(xill)Pr(1), 
where Pr(1) denotes the prior probability of l 
(the fraction of examples labeled l) and Pr(xill) 
are the conditional feature probabilities (the 
fraction of the examples labeled l in which the 
ith feature has value xi). Therefore, we get: 
Cla im:  The naive Bayes algorithm is an LSQ 
algorithm over a set ,.~ which consists of n + 1 
features: X0 --- 1, Xi -- xi for i = 1 , . . . ,n  
and where f\[1J\]O = log/5\[~z\],, and f\[x,J\]O = 
^D ^D logP\[x,,l\]/P\[1,l\], i = 1,... ,n. 
The observation that the LSQ hypothesis 
is linear over X' yields the first generalization 
property of LSQ. VC theory implies that the 
VC dimension of the class of LSQ hypothe- 
ses is bounded above by IXI. Moreover, if 
the LSQ hypothesis is sparse and does not 
make use of unobserved features in X (as in 
Ex. 3.1) it is bounded by the number of features 
used (Rot00). Together with the basic general- 
ization property described above this implies: 
Coro l la ry  3.1 For LSQ, the number of train- 
ing examples required in order to maintain a 
specific generalization performance guarantee 
scales linearly with the number o/features used. 
3 
The robustness property of LSQ can be cast 
for the case in which the hypothesis i learned 
using a training set sampled according to a dis- 
tribution D, but tested over a sample from D ~. 
It still performs well as long as the distributional 
distance d(D, D') is controlled (Rot99a; Rot00). 
Theorem 3.2 Let .A be an SQ(T, X) learning 
algorithm for a function class ~ over the distri- 
bution D and assume that d(D, D I) < V (for V 
inversely polynomial in T). Then .A is also an 
SQ(T, ,~') learning algorithm for ~ over D I. 
Finally, we mention that the robustness of the 
algorithm to different distributions depends on 
the sample size and the richness of the feature 
class 2? plays an important role here. Therefore, 
for a given size sample, the use of simpler fea- 
tures in the LSQ representation provides better 
robustness. This, in turn, can be traded off with 
the ability to express the learned function with 
an LSQ over a simpler set of features. 
3.2 Addi t ional  Examples  
In addition to the naive Bayes (NB) classifier 
described above several other widely used prob- 
abilistic classifiers can be cast as LSQ hypothe- 
ses. This property is maintained even if the NB 
predictor is generalized in several ways, by al- 
lowing hidden variables (GR00) or by assuming 
a more involved independence structure around 
the target variable. When the structure is mod- 
eled using a general Bayesian etwork (since we 
care only about predicting a value for a single 
variable having observed the others) the Bayes 
optimal predictor is an LSQ hypothesis over fea- 
tures that are polynomials X = IIxilxi2 ? .. xik of 
degree that depends on the number of neighbors 
of the target variable. A specific case of great 
interest o NLP is that of hidden Markov Mod- 
els. In this case there are two types of variables, 
state variables S and observed ones, O (Rab89). 
The task of predicting the value of a state vari- 
able given values of the others can be cast as an 
LSQ, where X C {S, O, 1} ? {S, O, 1}, a suitably 
defined set of singletons and pairs of observables 
and state variables (Rot99a). 
Finally, Maximum Entropy (ME) models 
(Jay82; Rat97) are also LSQ models. In this 
framework, constrains correspond to features; 
the distribution (and the induced classifier) are 
defined in terms of the expected value of the fea- 
tures over the training set. The induced clas- 
sifter is a linear classifier whose weights are de- 
rived from these expectations; the weights axe 
computed iteratively (DR72) since no closed 
form solution is known for the optimal values. 
4 Learning Linear Classifiers 
It was shown in (Rot98) that several other 
learning approaches widely used in NL work 
also make their predictions by utilizing a lin- 
ear representation. The SNoW learning archi- 
tecture (Rot98; CCRR99; MPRZ99) is explic- 
itly presented this way, but this holds also for 
methods that are presented in different ways, 
and some effort is required to cast them this 
way. These include Brill's transformation based 
method (Bri95) 2, decision lists (Yax94) and 
back-off estimation (Kat87; CB95). 
Moreover, even memory-based methods 
(ZD97; DBZ99) can be cast in a similar 
way (Rot99b). They can be reformulated as 
feature-based algorithms, with features of types 
that are commonly used by other features-based 
learning algorithms in NLP; the prediction com- 
puted by MBL can be computed by a linear 
function over this set of features. 
Some other methods that have been recently 
used in language related applications, such as 
Boosting (CS99) and support vector machines 
are also making use of the same representation. 
At a conceptual level all learning methods 
are therefore quite similar. They transform the 
original input (e.g., sentence, sentence+pos in-
formation) to a new, high dimensional, feature 
space, whose coordinates are typically small 
conjunctions (n-grams) over the original input. 
In this new space they search for a linear func- 
tion that best separates the training data, and 
rely on the inductive principle mentioned to 
yield good behavior on future data. Viewed this 
way, methods are easy to compare and analyze 
for their suitability to NL applications and fu- 
ture extensions, as we sketch below. 
The goal of blowing up the instance space to a 
high dimensional space is to increase the expres- 
sivity of the classifier so that a linear function 
could represent the target concepts. Within this 
space, probabilistic methods are the most lim- 
ited since they do not actually search in the 
eThis holds only in cases in which the TBL condi- 
tions do not depend on the labels, as in Context Sensi- 
tive Spelling (MB97) and Prepositional Phrase Attach- 
ment (BR94) and not in the general case. 
4 
space of linear functions. Given the feature 
space they directly compute the classifier. In 
general, even when a simple linear function gen- 
erates the training data, these methods are not 
guaranteed to be consistent with it (Rot99a). 
However, if the feature space is chosen so that 
they are, the robustness properties shown above 
become significant. Decision lists and MBL 
methods have advantages in their ability to rep- 
resent exceptions and small areas in the feature 
space. MBL, by using long and very specialized 
conjunctions (DBZ99) and decision lists, due to 
their functional form - a linear function with 
exponentially decreasing weights - at the cost 
of predicting with a single feature, rather than 
a combination (Go195). Learning methods that 
attempt to find the best linear function (relative 
to some loss function) are typically more flexi- 
ble. Of these, we highlight here the SNoW ar- 
chitecture, which has some specific advantages 
that favor NLP-like domains. 
SNoW determines the features' weights using 
an on-line algorithm that attempts to minimize 
the number of mistakes on the training data us- 
ing a multiplicative weight update rule (Lit88). 
The weight update rule is driven by the maxi- 
mum entropy principle (KW95). The main im- 
plication is that SNoW has significant advan- 
tages in sparse spaces, those in which a few of 
the features are actually relevant to the tar- 
get concept, as is typical in NLP. In domains 
with these characteristics, for a given number 
of training examples, SNoW generalizes better 
than additive update methods like perceptron 
and its close relative SVMs (Ros58; FS98) (and 
in general,it has better learning curves). 
Furthermore, although in SNoW the transfor- 
mation to a large dimensional space needs to be 
done explicitly (rather than via kernel functions 
as is possible in perceptron and SVMs) its use of 
variable size examples nevertheless gives it com- 
putational advantages, due to the sparse feature 
space in NLP applications. It is also significant 
for extensions to relational domain mentioned 
later. Finally, SNoW is a multi-class classifier. 
5 Future  Research  I ssues  
Research on learning in NLP needs to be inte- 
grated with work on knowledge representation 
and inference to enable studying higher level NL 
tasks. We mention two important directions the 
implications on the learning issues. 
The unified view presented reveals that all 
methods blow up the dimensionality of the orig- 
inal space in essentially the same way; they gen- 
erate conjunctive features over the linear struc- 
ture of the sentence (i.e., n-gram like features in 
the word and/or pos space). 
This does not seem to be expressive nough. 
Expressing complex concepts and relations 
necessary for higher level inferences will re- 
quire more involved intermediate representa- 
tions ("features") over the input; higher order 
structural and semantic properties, long term 
dependencies and relational predicates need to 
be represented. Learning will stay manageable 
if done in terms of these intermediate r presen- 
tations as done today, using functionally simple 
representations (perhaps cascaded). 
Inductive logic programming (MDR94; 
Coh95) is a natural paradigm for this. How- 
ever, computational limitations that include 
both learnability and subsumption render this 
approach inadequate for large scale knowledge 
intensive problems (KRV99; CR00). 
In (CR00) we suggest an approach that ad- 
dresses the generation of complex and relational 
intermediate r presentations and supports effi- 
cient learning on top of those. It allows the 
generation and use of structured examples which 
could encode relational information and long 
term functional dependencies. This is done us- 
ing a construct hat defines "types" of (poten- 
tially, relational) features the learning process 
might use. These represent infinitely many fea- 
tures, and are not generated explicitly; only 
those present in the data are generated, on the 
fly, as part of the learning process. Thus it 
yields hypotheses that are as expressive as re- 
lational earners in a scalable fashion. This ap- 
proach, however, makes some requirements on 
the learning process. Most importantly, the 
learning approach needs to be able to process 
variable size examples. And, it has to be feature 
efficient in that its complexity depends mostly 
on the number of relevant features. This seems 
to favor the SNoW approach over other algo- 
rithms that learn the same representation. 
Eventually, we would like to perform infer- 
ences that depend on the outcomes of several 
different classifiers; together these might need to 
coherently satisfy some constrains arising from 
the sequential nature of the data or task and do- 
main specific issues. There is a need to study, 
along with learning and knowledge representa- 
tion, inference methods that suit this frame- 
work (KR97). Work in this direction requires a 
consistent semantics of the learners (Val99) and 
will have implications on the knowledge repre- 
sentations and learning methods used. Prel im- 
inary work in (PRO0) suggests everal ways to 
formalize this problem and is evaluated in the 
context of identifying phrase structure. 
References  
J. A. Aslam and S. E. Decatur. Specification and simu- 
lation of statistical query algorithms for efficiency and 
noise tolerance. In COLT 1995, pages 437-446. 
E. Brill and P. Resnik. A rule-based approach to prepo- 
sitional phrase attachment disambiguation. In Proc. 
of COLING, 1994. 
E. Brill. Transformation-based error-driven learning 
and natural language processing: A case study in 
part of speech tagging. Computational Linguistics, 
21(4):543-565, 1995. 
M. Collins and J Brooks. Prepositional phrase attach- 
ment through a backed-off model. In WVLC 1995. 
A. Carleson, C. Cumby, J. Rosen, and D. Roth. 
The SNoW learning architecture. Technical Report 
UIUCDCS-R-99-2101, UIUC CS, May 1999. 
W. Cohen. PAC-learning recursive logic programs: Effi- 
cient algorithms. JAIR, 2:501-539, 1995. 
C. Cumby and D. Roth. Relational representations that 
facilitate learning. In KR 2000, pages 425-434. 
M. Collins and Y. Singer. Unsupervised models for name 
entity classification. In EMNLP- VLC'99. 
W. Daelemans, A. van den Bosch, and J. Zavrel. Forget- 
ting exceptions i harmful in language learning. Ma- 
chine Learning, 34(1-3):11-43, 1999. 
S. E. Decatur. Statistical queries and faulty PAC oracles. 
In COLT 1993, pages 262-268. 
R. O. Duda and P. E. Hart. Pattern Classification and 
Scene Analysis. Wiley, 1973. 
J. N. Darroch and D. Ratcliff. Generalized iterative 
scaling for log-linear models. Annals of Mathemati- 
cal Statistics, 43(5):1470-1480, 1972. 
Y. Freund and R. Schapire. Large margin classification 
using the Perceptron algorithm. In COLT 1998. 
W. Gale, K. Church, and D. Yarowsky. A method for 
disambiguating word senses in a large corpus. Com- 
puters and the Humanities, 26:415-439~ 1993. 
A. R. Golding. A Bayesian hybrid method for context- 
sensitive spelling correction. In Proceedings of the 3rd 
workshop on very large corpora, ACL-95, 1995. 
A. R. Golding and D. Roth. A Winnow based ap- 
proach to context-sensitive spelling correction. Ma- 
chine Learning, 34(1-3):107-130, 1999. 
A. Grove and D. Roth. Linear concepts and hidden vari- 
ables. Machine Learning, 2000. To Appear. 
D. Haussler. Decision theoretic generalizations of the 
PAC model for neural net and other learning appli- 
cations. Inform. Comput., 100(1):78-150, 1992. 
E. T. Jaynes. On the rationale of maximum-entropy 
methods. Proc. of the IEEE, 70(9):939-952, 1982. 
S. M. Katz. Estimation of probabilities from sparse data 
for the language model component of a speech recog- 
nizer. IEEE Transactions on Acoustics, speech, and 
Signal Processing, 35(3):400-401, 1987. 
M. Kearns. Efficient noise-tolerant learning from statis- 
tical queries. In COLT 1993, pages 392-401. 
R. Khardon and D. Roth. Learning to reason. Journal 
of the ACM, 44(5):697-725, Sept. 1997. 
R. Khardon, D. Roth, and L. G. Valiant. Relational 
learning for NLP using linear threshold elements. In 
IJCAI 1999, pages 911-917. 
M. J. Kearns, R. E. Schapire, and L. M. Sellie. To- 
ward efficient agnostic learning. Machine Learning, 
17(2/3):115-142, 1994. 
J. Kupiec. Robust part-of-speech tagging using a hid- 
den Markov model. Computer Speech and Language, 
6:225-242, 1992. 
J. Kivinen and M. K. Warmuth. Exponentiated gradi- 
ent versus gradient descent for linear predictors. In 
STOC, 1995. 
N. Littlestone. Learning quickly when irrelevant at- 
tributes abound: A new linear-threshold algorithm. 
Machine Learning, 2:285-318, 1988. 
L. Mangu and E. Brill. Automatic rule acquisition for 
spelling correction. In ICML 1997, pages 734-741. 
S. Muggleton and L. De Raedt. Inductive logic program- 
ming: Theory and methods. Journal of Logic Pro- 
gramming, 20:629-679, 1994. 
M. Munoz, V. Punyakanok, D. Roth, and D. Zimak. 
A learning approach to shallow parsing. In EMNLP- 
VLC'99, pages 168-178. 
V. Punyakanok and D. Roth. Inference with classifiers. 
Technical Report UIUCDCS-R-2000-2181, UIUC CS. 
L. R. Rabiner. A tutorial on hidden Markov models and 
selected applications in speech recognition. Proceed- 
ings of the IEEE, 77(2):257-285, 1989. 
A. Ratnaparkhi. A linear observed time statistical parser 
based on maximum entropy models. In EMNLP-97. 
F. Rosenblatt. The perceptron: A probabilistic model 
for information storage and organization i the brain. 
Psychological Review, 65:386-407, 1958. 
D. Roth. Learning to resolve natural anguage ambigui- 
ties: A unified approach. In AAAI'98, pages 806-813. 
D. Roth. Learning in natural anguage. In IJCAI 1999, 
pages 898-904. 
D. Roth. Memory based learning in NLP. Technical Re- 
port UIUCDCS-R-99-2125, UIUC CS, March 1999. 
D. Roth. Learning in natural anguage. Technical Re- 
port UIUCDCS-R-2000-2180, UIUC CS July 2000. 
H. Schfitze. Distributional part-of-speech tagging. In 
EACL 1995. 
L. G. Valiant. A theory of the learnable. Communica- 
tions of the ACM, 27(11):1134-1142, November 1984. 
L. G. Valiant. Robust logic. In STOC 1999. 
V. N. Vapnik. The Nature of Statistical Learning Theory. 
Springer-Verlag, New York, 1995. 
D. Yarowsky. Decision lists for lexical ambiguity resolu- 
tion: application to accent restoration i Spanish and 
French. In ACL 1994, pages 88-95. 
J. Zavrel and W. Daelemans. Memory-based learning: 
Using similarity for smoothing. In ACL, 1997. 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 107-110, Lisbon, Portugal, 2000. 
Shallow Parsing by Inferencing with Classifiers* 
Vas in  Punyakanok  and Dan Roth  
Depar tment  of Computer  Science 
University of Illinois at Urbana-Champaign  
Urbana,  IL 61801, USA 
{punyakan, danr}@cs.uiuc.edu 
Abst ract  
We study the problem of identifying phrase 
structure. We formalize it as the problem of 
combining the outcomes of several different clas- 
sifiers in a way that provides a coherent in- 
ference that satisfies some constraints, and de- 
velop two general approaches for it. The first 
is a Markovian approach that extends stan- 
dard HMMs to allow the use of a rich obser- 
vations structure and of general classifiers to 
model state-observation dependencies. The sec- 
ond is an extension of constraint satisfaction for- 
malisms. We also develop efficient algorithms 
under both models and study them experimen- 
tally in the context of shallow parsing. 1
1 Ident i fy ing  Phrase  St ructure  
The problem of identifying phrase structure can 
be formalized as follows. Given an input string 
O =< ol, 02, . . . ,  On >, a phrase is a substring 
of consecutive input symbols oi, oi+l,. . . ,oj .  
Some external mechanism is assumed to consis- 
tently (or stochastically) annotate substrings as 
phrases 2. Our goal is to come up with a mech- 
anism that, given an input string, identifies the 
phrases in this string, this is a fundamental task 
with applications in natural language (Church, 
1988; Ramshaw and Marcus, 1995; Mufioz et 
al., 1999; Cardie and Pierce, 1998). 
The identification mechanism works by using 
classifiers that process the input string and rec- 
ognize in the input string local signals which 
* This research is supported by NSF grants IIS-9801638, 
SBR-9873450 and IIS-9984168. 
1Full version is in (Punyakanok and Roth, 2000). 
2We assume here a single type of phrase, and thus 
each input symbol is either in a phrase or outside it. All 
the methods we discuss can be extended to deal with 
several kinds of phrases in a string, including different 
kinds of phrases and embedded phrases. 
are indicative to the existence of a phrase. Lo- 
cal signals can indicate that an input symbol o 
is inside or outside a phrase (IO modeling) or 
they can indicate that an input symbol o opens 
or closes a phrase (the OC modeling) or some 
combination of the two. In any case, the lo- 
cal signals can be combined to determine the 
phrases in the input string. This process, how- 
ever, needs to satisfy some constraints for the 
resulting set of phrases to be legitimate. Sev- 
eral types of constraints, such as length and or- 
der can be formalized and incorporated into the 
mechanisms studied here. For simplicity, we fo- 
cus only on the most basic and common con- 
straint - we assume that phrases do not overlap. 
The goal is thus two-fold: to learn classifiers 
that recognize the local signals and to combine 
these in a ways that respects the constraints. 
2 Markov  Mode l ing  
HMM is a probabilistic finite state automaton 
used to model the probabilistic generation of 
sequential processes. The model consists of 
a finite set S of states, a set (9 of observa- 
tions, an initial state distribution P1 (s), a state- 
transition distribution P(s\[s') for s, # E S and 
an observation distribution P(o\[s) for o E (9 
and s 6 S. 3 
In a supervised learning task, an observa- 
tion sequence O --< o l ,o2, . . .  On > is super- 
vised by a corresponding state sequence S =< 
sl, s2,. ? ? sn >. The supervision can also be sup- 
plied, as described in Sec. 1, using the local sig- 
nals. Constraints can be incorporated into the 
HMM by constraining the state transition prob- 
ability distribution P(s\]s'). For example, set 
P(sV)  = 0 for all s, s' such that the transition 
from s ~ to s is not allowed. 
aSee (Rabiner, 1989) for a comprehensive tutorial. 
107 
Combining HMM and classifiers (artificial 
neural networks) has been exploited in speech 
recognition (Morgan and Bourlard, 1995), how- 
ever, with some differences from this work. 
2.1 HMM wi th  Classif iers 
To recover the most likely state sequence in 
HMM, we wish to estimate all the required 
probability distributions. As in Sec. 1 we as- 
sume to have local signals that indicate the 
state. That is, we are given classifiers with 
states as their outcomes. Formally, we assume 
that Pt(slo ) is given where t is the time step in 
the sequence. In order to use this information 
in the HMM framework, we compute 
Pt(o\[s) = Pt(slo)Pt(o)/Pt(s). (1) 
instead of observing the conditional probability 
Pt (ols) directly from training data, we compute 
it from the classifiers' output. Pt(s) can be cal- 
culated by Pt(s) = Es'eS P(sls')Pt- l(s ')  where 
Pl(s) and P(sls' ) are the two required distri- 
bution for the HMM. For each t, we can treat 
Pt(ols ) in Eq. 1 as a constant r/t because our goal 
is only to find the most likely sequence of states 
for given observations which are the same for 
all compared sequences. Therefore, to compute 
the most likely sequence, standard ynamic pro- 
gramming (Viterbi) can still be applied. 
2.2 P ro jec t ion  based  Markov  Mode l  
In HMMs, observations are allowed to depend 
only on the current state and long term depen- 
dencies are not modeled. Equivalently, from the 
constraint point of view, the constraint struc- 
ture is restricted by having a stationary proba- 
bility distribution of a state given the previous 
one. We attempt o relax this by allowing the 
distribution of a state to depend, in addition 
to the previous state, on the observation. For- 
mally, we make the independence assumption: 
P(  s t lS t - l  , S t -  2 ,  . . . , s l  , o t ,  o t -1 ,  . . . , 01)  
= P(stlSt_l,Ot). (2) 
Thus, we can find the most likely state sequence 
S given O by maximizing 
n 
P(SIO) = II\[P(stls~,..., 8t-1, O)\]Pl(slIO) 
t=2 
n 
= H\[P(stlst_l,ot)\]Pl(sl lOl).  (3) 
t=2 
Hence, this model generalizes the standard 
HMM by combining the state-transition prob- 
ability and the observation probability into one 
function. The most likely state sequence can 
still be recovered using the dynamic program- 
ming algorithm over the Eq.3. 
In this model, the classifiers' decisions are in- 
corporated in the terms P(sls' ,o ) and Pl(slo ). 
In learning these classifiers we project P(sls ~, o) 
to many functions Ps' (slo) according to the pre- 
vious states s ~. A similar approach has been 
developed recently in the context of maximum 
entropy classifiers in (McCallum et al, 2000). 
3 Const ra in t  Sat i s fac t ion  w i th  
Class i f ie rs  
The approach is based on an extension of 
the Boolean constraint satisfaction formal- 
ism (Mackworth, 1992) to handle variables that 
are outcomes of classifiers. As before, we as- 
sume an observed string 0 =< ol,o2,. . .  On > 
and local classifiers that, w.l.o.g., take two dis- 
tinct values, one indicating the openning a 
phrase and a second indicating closing it (OC 
modeling). The classifiers provide their outputs 
in terms of the probability P(o) and P(c), given 
the observation. 
To formalize this, let E be the set of all possi- 
ble phrases. All the non-overlapping constraints 
can be encoded in: f --/ke~ overlaps ej (-~eiV-~ej). 
Each solution to this formulae corresponds to a 
legitimate set of phrases. 
Our problem, however, is not simply to find 
an assignment  : E -+ {0, 1} that satisfies f
but rather to optimize some criterion. Hence, 
we associate a cost function c : E ~ \[0,1\] 
with each variable, and then find a solution ~- 
of f of minimum cost, c(~-) = n Ei=l 
In phrase identification, the solution to the op- 
timization problem corresponds to a shortest 
path in a directed acyclic graph constructed 
on the observation symbols, with legitimate 
phrases (the variables in E) as its edges and 
their costs as the weights. Each path in this 
graph corresponds to a satisfying assignment 
and the shortest path is the optimal solution. 
A natural cost function is to use the classi- 
fiers probabilities P(o) and P(c) and define, for 
a phrase e = (o, c), c(e) = 1 - P(o)P(c) which 
means that the error in selecting e is the er- 
ror in selecting either o or c, and allowing those 
108 
to overlap 4. The constant in 1 - P(o)P(c) bi- 
ases the minimization to prefers selecting a few 
phrases, possibly no phrase, so instead we min- 
imize -P(o) P(c). 
4 Sha l low Pars ing  
The above mentioned approaches are evaluated 
on shallow parsing tasks, We use the OC mod- 
eling and learn two classifiers; one predicting 
whether there should be a open in location t 
or not, and the other whether there should a 
close in location t or not. For technical reasons 
it is easier to keep track of the constraints if 
the cases --1 o and --1 c are separated according to 
whether we are inside or outside a phrase. Con- 
sequently, each classifier may output three pos- 
sible outcomes O, nOi,  nOo (open, not open 
inside, not open outside) and C, nCi,  nCo,  
resp. The state-transition diagram in figure 1 
captures the order constraints. Our modeling of 
the problem is a modification of our earlier work 
on this topic that has been found to be quite 
successful compared to other learning methods 
attempted on this problem (Mufioz et al, 1999) 
and in particular, better than the IO modeling 
of the problem (Mufioz et al, 1999). 
Figure 1: State-transition diagramfor the 
phrase recognition problem. 
The classifier we use to learn the states as 
a function of the observations i SNoW (Roth, 
1998; Carleson et al, 1999), a multi-class clas- 
sifter that is specifically tailored for large scale 
learning tasks. The SNoW learning architec- 
ture learns a sparse network of linear functions, 
in which the targets (states, in this case) are 
represented as linear functions over a common 
feature space. Typically, SNoW is used as a 
classifier, and predicts using a winner-take-all 
4Another solution in which the classifiers' uggestions 
inside each phrase axe also accounted for is possible. 
mechanism over the activation value of the tax- 
get classes in this case. The activation value 
itself is computed using a sigmoid function over 
the linear sum. In this case, instead, we normal- 
ize the activation levels of all targets to sum to 1 
and output the outcomes for all targets (states). 
We verified experimentally on the training data 
that the output for each state is indeed a dis- 
tribution function and can be used in further 
processing as P(slo ) (details omitted). 
5 Experiments 
We experimented both with base noun phrases 
(NP) and subject-verb patterns (SV) and show 
results for two different representations of the 
observations (that is, different feature sets for 
the classifiers) - part of speech (POS) tags only 
and POS with additional exical information 
(words). The data sets used are the standard 
data sets for this problem (Ramshaw and Max- 
cus, 1995; Argamon et al, 1999; Mufioz et 
al., 1999; Tjong Kim Sang and Veenstra, 1999) 
taken from the Wall Street Journal corpus in 
the Penn Treebank (Marcus et al, 1993). 
For each model we study three different clas- 
sifiers. The simple classifier corresponds to the 
standard HMM in which P(ols ) is estimated i- 
rectly from the data. The NB (naive Bayes) and 
SNoW classifiers use the same feature set, con- 
junctions of size 3 of POS tags (+ words) in a 
window of size 6 around the target word. 
The first important observation is that the 
SV task is significantly more difficult than the 
NP task. This is consistent for all models and 
all features ets. When comparing between dif- 
ferent models and features ets, it is clear that 
the simple HMM formalism is not competitive 
with the other two models. What is interest- 
ing here is the very significant sensitivity to the 
wider notion of observations (features) used by 
the classifiers, despite the violation of the prob- 
abilistic assumptions. For the easier NP task, 
the HMM model is competitive with the oth- 
ers when the classifiers used are NB or SNOW. 
In particular, a significant improvement in both 
probabilistic methods is achieved when their in- 
put is given by SNOW. 
Our two main methods, PMM and CSCL, 
perform very well on predicting NP and SV 
phrases with CSCL at least as good as any other 
methods tried on these tasks. Both for NPs and 
109 
Table 1: Results (F~=l) of different methods 
and comparison to previous works on NP and 
SV recognition. Notice that, in case of simple, 
the data with lexical features are too sparse to 
directly estimate the observation probability so 
we leave these entries empty. 
Method POS POS 
Model\[ Classifier only +words 
SNoW 90.64 92.89 
HMM NB 90.50 92.26 
Simple 87.83 
SNoW 90.61 92.98 
NP PMM NB 90.22 91.98 
Simple 61.44 
SNoW 90.87 92.88 
CSCL NB 90.49 91.95 
Simple 54.42 
Ramshaw & Marcus 90.6 92.0 
Argamon et al 91.6 N/A 
Mufioz et al 90.6 92.8 
Tjong Kim Sang 
Veenstra N/A 92.37 
SNoW 64.15 77.54 
HMM NB 75.40 78.43 
Simple 64.85 
SNoW 74.98 86.07 
PMM NB 74.80 84.80 
Simple 40.18 
SNoW 85.36 90.09 
CSCL NB 80.63 88.28 
Simple 59.27 
Argamon et al 86.5 N/A 
i Mufioz et al 88.1 92.0 
SV 
SVs, CSCL performs better than the probabilis- 
tic method, more significantly on the harder, 
SV, task. We attr ibute it to CSCL's ability to 
cope better with the length of the phrase and 
the long term dependencies. 
Our methods compare favorably with others 
with the exception to SV in (Mufioz et al, 
1999). Their method is fundamentally simi- 
lar to our CSCL; however, they incorporated 
the features from open in the close classifier al- 
lowing to exploit the dependencies between two 
classifiers. We believe that this is the main fac- 
tor of the significant difference in performance. 
6 Conc lus ion  
We have addressed the problem of combining 
the outcomes of several different classifiers in a 
way that provides a coherent inference that sat- 
isfies some constraints, While the probabilistic 
approach extends tandard and commonly used 
techniques for sequential decisions, it seems 
that the constraint satisfaction formalisms can 
support complex constraints and dependencies 
more flexibly. Future work will concentrate on 
these formalisms. 
Re ferences  
S. Argamon, I. Dagan, and Y. Krymolowski. 1999. 
A memory-based approach to learning shallow 
natural anguage patterns. Journal of Experimen- 
tal and Theoretical Artificial Intelligence, 10:1-22. 
C. Cardie and D. Pierce. 1998. Error-driven prun- 
ing of treebanks grammars for base noun phrase 
identification. In Proc. of ACL-98, pages 218-224. 
A. Carleson, C. Cumby, J. Rosen, and D. Roth. 
1999. The SNoW learning architecture. Tech. Re- 
port UIUCDCS-R-99-2101, UIUC Computer Sci- 
ence Department, May. 
K. W. Church. 1988. A stochastic parts program 
and noun phrase parser for unrestricted text. In 
Proe. of A CL Conference on Applied Natural Lan- 
guage Processing. 
A. K. Mackworth. 1992. Constraint Satisfaction. In 
Stuart C. Shapiro, editor, Encyclopedia of Artifi- 
cial Intelligence, pages 285-293. Vol. 1, 2 nd ed. 
M. P. Marcus, B. Santorini, and M. Marcinkiewicz. 
1993. Building a large annotated corpus of En- 
glish: The Penn Treebank. Computational Lin- 
guistics, 19(2):313-330, June. 
A. McCallum, D. Freitag, and F. Pereira. 2000. 
Maximum entropy Markov models for information 
extraction and segmentation. In Proc. of ICML- 
2000. 
N. Morgan and H. Bourlard. 1995. Continuous 
speech recognition. IEEE Signal Processing Mag- 
azine, 12(3):25-42. 
M. Mufioz, V. Punyakanok, D. Roth, and D. Zimak. 
1999. A learning approach to shallow parsing. In 
Proc. of EMNLpo VLC'99. 
V. Punyakanok and D. Roth. 2000. Inference with 
classifiers. Tech. Report UIUCDCS-R-2000-2181, 
UIUC Computer Science Department, July. 
L. R. Rabiner. 1989. A tutorial on hidden Markov 
models and selected applications in speech recog- 
nition. Proc. of the IEEE, 77(2):257-285. 
L. A. Ramshaw and M. P. Marcus. 1995. Text 
chunking using transformation-based learning. In 
Proc. of WVLC'95. 
D. Roth. 1998. Learning to resolve natural lan- 
guage ambiguities: A unified approach. In Proc. 
of AAAI'98, pages 806-813. 
E. F. Tjong Kim Sang and J. Yeenstra. 1999. Rep- 
resenting text chunks. In Proc. of EA CL'99. 
110 
A Sequential Model for Multi-Class Classification  
Yair Even-Zohar Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign

evenzoha,danr  @uiuc.edu
Abstract
Many classification problems require decisions
among a large number of competing classes. These
tasks, however, are not handled well by general pur-
pose learning methods and are usually addressed in
an ad-hoc fashion. We suggest a general approach
? a sequential learning model that utilizes classi-
fiers to sequentially restrict the number of compet-
ing classes while maintaining, with high probability,
the presence of the true outcome in the candidates
set. Some theoretical and computational properties
of the model are discussed and we argue that these
are important in NLP-like domains. The advantages
of the model are illustrated in an experiment in part-
of-speech tagging.
1 Introduction
A large number of important natural language infer-
ences can be viewed as problems of resolving ambi-
guity, either semantic or syntactic, based on proper-
ties of the surrounding context. These, in turn, can
all be viewed as classification problems in which
the goal is to select a class label from among a
collection of candidates. Examples include part-of
speech tagging, word-sense disambiguation, accent
restoration, word choice selection in machine trans-
lation, context-sensitive spelling correction, word
selection in speech recognition and identifying dis-
course markers.
Machine learning methods have become the
most popular technique in a variety of classifi-
cation problems of these sort, and have shown
significant success. A partial list consists of
Bayesian classifiers (Gale et al, 1993), decision
lists (Yarowsky, 1994), Bayesian hybrids (Gold-
ing, 1995), HMMs (Charniak, 1993), inductive
logic methods (Zelle and Mooney, 1996), memory-

This research is supported by NSF grants IIS-9801638, IIS-
0085836 and SBR-987345.
based methods (Zavrel et al, 1997), linear classi-
fiers (Roth, 1998; Roth, 1999) and transformation-
based learning (Brill, 1995).
In many of these classification problems a signif-
icant source of difficulty is the fact that the number
of candidates is very large ? all words in words se-
lection problems, all possible tags in tagging prob-
lems etc. Since general purpose learning algorithms
do not handle these multi-class classification prob-
lems well (see below), most of the studies do not
address the whole problem; rather, a small set of
candidates (typically two) is first selected, and the
classifier is trained to choose among these. While
this approach is important in that it allows the re-
search community to develop better learning meth-
ods and evaluate them in a range of applications,
it is important to realize that an important stage is
missing. This could be significant when the clas-
sification methods are to be embedded as part of
a higher level NLP tasks such as machine transla-
tion or information extraction, where the small set
of candidates the classifier can handle may not be
fixed and could be hard to determine.
In this work we develop a general approach to
the study of multi-class classifiers. We suggest a se-
quential learning model that utilizes (almost) gen-
eral purpose classifiers to sequentially restrict the
number of competing classes while maintaining,
with high probability, the presence of the true out-
come in the candidate set.
In our paradigm the sought after classifier has to
choose a single class label (or a small set of la-
bels) from among a large set of labels. It works
by sequentially applying simpler classifiers, each
of which outputs a probability distribution over the
candidate labels. These distributions are multiplied
and thresholded, resulting in that each classifier in
the sequence needs to deal with a (significantly)
smaller number of the candidate labels than the pre-
vious classifier. The classifiers in the sequence are
selected to be simple in the sense that they typically
work only on part of the feature space where the de-
composition of feature space is done so as to achieve
statistical independence. Simple classifier are used
since they are more likely to be accurate; they are
chosen so that, with high probability (w.h.p.), they
have one sided error, and therefore the presence of
the true label in the candidate set is maintained. The
order of the sequence is determined so as to maxi-
mize the rate of decreasing the size of the candidate
labels set.
Beyond increased accuracy on multi-class classi-
fication problems , our scheme improves the com-
putation time of these problems several orders of
magnitude, relative to other standard schemes.
In this work we describe the approach, discuss
an experiment done in the context of part-of-speech
(pos) tagging, and provide some theoretical justifi-
cations to the approach. Sec. 2 provides some back-
ground on approaches to multi-class classification
in machine learning and in NLP. In Sec. 3 we de-
scribe the sequential model proposed here and in
Sec. 4 we describe an experiment the exhibits some
of its advantages. Some theoretical justifications are
outlined in Sec. 5.
2 Multi-Class Classification
Several works within the machine learning commu-
nity have attempted to develop general approaches
to multi-class classification. One of the most
promising approaches is that of error correcting out-
put codes (Dietterich and Bakiri, 1995); however,
this approach has not been able to handle well a
large number of classes (over 10 or 15, say) and its
use for most large scale NLP applications is there-
fore questionable. Statistician have studied several
schemes such as learning a single classifier for each
of the class labels (one vs. all) or learning a discrim-
inator for each pair of class labels, and discussed
their relative merits(Hastie and Tibshirani, 1998).
Although it has been argued that the latter should
provide better results than others, experimental re-
sults have been mixed (Allwein et al, 2000) and in
some cases, more involved schemes, e.g., learning a
classifier for each set of three class labels (and de-
ciding on the prediction in a tournament like fash-
ion) were shown to perform better (Teow and Loe,
2000). Moreover, none of these methods seem to be
computationally plausible for large scale problems,
since the number of classifiers one needs to train is,
at least, quadratic in the number of class labels.
Within NLP, several learning works have already
addressed the problem of multi-class classification.
In (Kudoh and Matsumoto, 2000) the methods of
?all pairs? was used to learn phrase annotations for
shallow parsing. More than  different classifiers
where used in this task, making it infeasible as a
general solution. All other cases we know of, have
taken into account some properties of the domain
and, in fact, several of the works can be viewed as
instantiations of the sequential model we formalize
here, albeit done in an ad-hoc fashion.
In speech recognition, a sequential model is used
to process speech signal. Abstracting away some
details, the first classifier used is a speech signal an-
alyzer; it assigns a positive probability only to some
of the words (using Levenshtein distance (Leven-
shtein, 1966) or somewhat more sophisticated tech-
niques (Levinson et al, 1990)). These words are
then assigned probabilities using a different contex-
tual classifier e.g., a language model, and then, (as
done in most current speech recognizers) an addi-
tional sentence level classifier uses the outcome of
the word classifiers in a word lattice to choose the
most likely sentence.
Several word prediction tasks make decisions in
a sequential way as well. In spell correction con-
fusion sets are created using a classifier that takes
as input the word transcription and outputs a posi-
tive probability for potential words. In conventional
spellers, the output of this classifier is then given
to the user who selects the intended word. In con-
text sensitive spelling correction (Golding and Roth,
1999; Mangu and Brill, 1997) an additional classi-
fier is then utilized to predict among words that are
supported by the first classifier, using contextual and
lexical information of the surrounding words. In all
studies done so far, however, the first classifier ? the
confusion sets ? were constructed manually by the
researchers.
Other word predictions tasks have also con-
structed manually the list of confusion sets (Lee
and Pereira, 1999; Dagan et al, 1999; Lee, 1999)
and justifications where given as to why this is a
reasonable way to construct it. (Even-Zohar and
Roth, 2000) present a similar task in which the con-
fusion sets generation was automated. Their study
also quantified experimentally the advantage in us-
ing early classifiers to restrict the size of the confu-
sion set.
Many other NLP tasks, such as pos tagging,
name entity recognition and shallow parsing require
multi-class classifiers. In several of these cases the
number of classes could be very large (e.g., pos tag-
ging in some languages, pos tagging when a finer
proper noun tag is used). The sequential model sug-
gested here is a natural solution.
3 The Sequential Model
We study the problem of learning a multi-class clas-
sifier, 
	   where   ,  
ffExploring Evidence for Shallow Parsing  
Xin Li Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
xli1@cs.uiuc.edu danr@cs.uiuc.edu
Abstract
Significant amount of work has been
devoted recently to develop learning
techniques that can be used to gener-
ate partial (shallow) analysis of natu-
ral language sentences rather than a full
parse. In this work we set out to evalu-
ate whether this direction is worthwhile
by comparing a learned shallow parser
to one of the best learned full parsers
on tasks both can perform ? identify-
ing phrases in sentences. We conclude
that directly learning to perform these
tasks as shallow parsers do is advanta-
geous over full parsers both in terms of
performance and robustness to new and
lower quality texts.
1 Introduction
Shallow parsing is studied as an alternative to
full-sentence parsing. Rather than producing a
complete analysis of sentences, the alternative
is to perform only partial analysis of the syn-
tactic structures in a text (Harris, 1957; Abney,
1991; Greffenstette, 1993). A lot of recent work
on shallow parsing has been influenced by Ab-
ney?s work (Abney, 1991), who has suggested to
?chunk? sentences to base level phrases. For ex-
ample, the sentence ?He reckons the current ac-
count deficit will narrow to only $ 1.8 billion in
September .? would be chunked as follows (Tjong
Kim Sang and Buchholz, 2000):
[NP He ] [VP reckons ] [NP the current
account deficit ] [VP will narrow ] [PP

This research is supported by NSF grants IIS-9801638,
ITR-IIS-0085836 and an ONR MURI Award.
to ] [NP only $ 1.8 billion ] [PP in ] [NP
September] .
While earlier work in this direction concen-
trated on manual construction of rules, most of
the recent work has been motivated by the obser-
vation that shallow syntactic information can be
extracted using local information ? by examin-
ing the pattern itself, its nearby context and the
local part-of-speech information. Thus, over the
past few years, along with advances in the use
of learning and statistical methods for acquisition
of full parsers (Collins, 1997; Charniak, 1997a;
Charniak, 1997b; Ratnaparkhi, 1997), significant
progress has been made on the use of statisti-
cal learning methods to recognize shallow pars-
ing patterns ? syntactic phrases or words that
participate in a syntactic relationship (Church,
1988; Ramshaw and Marcus, 1995; Argamon et
al., 1998; Cardie and Pierce, 1998; Munoz et al,
1999; Punyakanok and Roth, 2001; Buchholz et
al., 1999; Tjong Kim Sang and Buchholz, 2000).
Research on shallow parsing was inspired by
psycholinguistics arguments (Gee and Grosjean,
1983) that suggest that in many scenarios (e.g.,
conversational) full parsing is not a realistic strat-
egy for sentence processing and analysis, and was
further motivated by several arguments from a
natural language engineering viewpoint.
First, it has been noted that in many natural lan-
guage applications it is sufficient to use shallow
parsing information; information such as noun
phrases (NPs) and other syntactic sequences have
been found useful in many large-scale language
processing applications including information ex-
traction and text summarization (Grishman, 1995;
Appelt et al, 1993). Second, while training a full
parser requires a collection of fully parsed sen-
tences as training corpus, it is possible to train a
shallow parser incrementally. If all that is avail-
able is a collection of sentences annotated for
NPs, it can be used to produce this level of anal-
ysis. This can be augmented later if more infor-
mation is available. Finally, the hope behind this
research direction was that this incremental and
modular processing might result in more robust
parsing decisions, especially in cases of spoken
language or other cases in which the quality of the
natural language inputs is low ? sentences which
may have repeated words, missing words, or any
other lexical and syntactic mistakes.
Overall, the driving force behind the work on
learning shallow parsers was the desire to get bet-
ter performance and higher reliability. However,
since work in this direction has started, a sig-
nificant progress has also been made in the re-
search on statistical learning of full parsers, both
in terms of accuracy and processing time (Char-
niak, 1997b; Charniak, 1997a; Collins, 1997;
Ratnaparkhi, 1997).
This paper investigates the question of whether
work on shallow parsing is worthwhile. That
is, we attempt to evaluate quantitatively the intu-
itions described above ? that learning to perform
shallow parsing could be more accurate and more
robust than learning to generate full parses. We
do that by concentrating on the task of identify-
ing the phrase structure of sentences ? a byprod-
uct of full parsers that can also be produced by
shallow parsers. We investigate two instantiations
of this task, ?chucking? and identifying atomic
phrases. And, to study robustness, we run our
experiments both on standard Penn Treebank data
(part of which is used for training the parsers) and
on lower quality data ? the Switchboard data.
Our conclusions are quite clear. Indeed, shal-
low parsers that are specifically trained to per-
form the tasks of identifying the phrase structure
of a sentence are more accurate and more robust
than full parsers. We believe that this finding, not
only justifies work in this direction, but may even
suggest that it would be worthwhile to use this
methodology incrementally, to learn a more com-
plete parser, if needed.
2 Experimental Design
In order to run a fair comparison between full
parsers and shallow parsers ? which could pro-
duce quite different outputs ? we have chosen
the task of identifying the phrase structure of a
sentence. This structure can be easily extracted
from the outcome of a full parser and a shallow
parser can be trained specifically on this task.
There is no agreement on how to define phrases
in sentences. The definition could depend on
downstream applications and could range from
simple syntactic patterns to message units peo-
ple use in conversations. For the purpose of this
study, we chose to use two different definitions.
Both can be formally defined and they reflect dif-
ferent levels of shallow parsing patterns.
The first is the one used in the chunking com-
petition in CoNLL-2000 (Tjong Kim Sang and
Buchholz, 2000). In this case, a full parse tree
is represented in a flat form, producing a rep-
resentation as in the example above. The goal
in this case is therefore to accurately predict a
collection of  different types of phrases. The
chunk types are based on the syntactic category
part of the bracket label in the Treebank. Roughly,
a chunk contains everything to the left of and
including the syntactic head of the constituent
of the same name. The phrases are: adjective
phrase (ADJP), adverb phrase (ADVP), conjunc-
tion phrase (CONJP), interjection phrase (INTJ),
list marker (LST), noun phrase (NP), preposition
phrase (PP), particle (PRT), subordinated clause
(SBAR), unlike coordinated phrase (UCP), verb
phrase (VP). (See details in (Tjong Kim Sang and
Buchholz, 2000).)
The second definition used is that of atomic
phrases. An atomic phrase represents the most
basic phrase with no nested sub-phrases. For ex-
ample, in the parse tree,
( (S (NP (NP Pierre Vinken) , (ADJP
(NP 61 years) old) ,) (VP will (VP join
(NP the board) (PP as (NP a nonexecu-
tive director)) (NP Nov. 29))) .))
Pierre Vinken, 61 years, the board,
a nonexecutive director and Nov.
29 are atomic phrases while other higher-level
phrases are not. That is, an atomic phrase denotes
a tightly coupled message unit which is just
above the level of single words.
2.1 Parsers
We perform our comparison using two state-of-
the-art parsers. For the full parser, we use the
one developed by Michael Collins (Collins, 1996;
Collins, 1997) ? one of the most accurate full
parsers around. It represents a full parse tree as
a set of basic phrases and a set of dependency
relationships between them. Statistical learning
techniques are used to compute the probabilities
of these phrases and of candidate dependency re-
lations occurring in that sentence. After that, it
will choose the candidate parse tree with the high-
est probability as output. The experiments use
the version that was trained (by Collins) on sec-
tions 02-21 of the Penn Treebank. The reported
results for the full parse tree (on section 23) are
recall/precision of 88.1/87.5 (Collins, 1997).
The shallow parser used is the SNoW-based
CSCL parser (Punyakanok and Roth, 2001;
Munoz et al, 1999). SNoW (Carleson et al,
1999; Roth, 1998) is a multi-class classifier that
is specifically tailored for learning in domains
in which the potential number of information
sources (features) taking part in decisions is very
large, of which NLP is a principal example. It
works by learning a sparse network of linear func-
tions over a pre-defined or incrementally learned
feature space. Typically, SNoW is used as a
classifier, and predicts using a winner-take-all
mechanism over the activation value of the tar-
get classes. However, in addition to the predic-
tion, it provides a reliable confidence level in the
prediction, which enables its use in an inference
algorithm that combines predictors to produce a
coherent inference. Indeed, in CSCL (constraint
satisfaction with classifiers), SNoW is used to
learn several different classifiers ? each detects
the beginning or end of a phrase of some type
(noun phrase, verb phrase, etc.). The outcomes
of these classifiers are then combined in a way
that satisfies some constraints ? non-overlapping
constraints in this case ? using an efficient con-
straint satisfaction mechanism that makes use of
the confidence in the classifier?s outcomes.
Since earlier versions of the SNoW based
CSCL were used only to identify single
phrases (Punyakanok and Roth, 2001; Munoz
et al, 1999) and never to identify a collection
of several phrases at the same time, as we do
here, we also trained and tested it under the exact
conditions of CoNLL-2000 (Tjong Kim Sang and
Buchholz, 2000) to compare it to other shallow
parsers. Table 1 shows that it ranks among the
top shallow parsers evaluated there 1.
Table 1: Rankings of Shallow Parsers in
CoNLL-2000. See (Tjong Kim Sang and Buch-
holz, 2000) for details.
Parsers Precision( ) Recall(  )  (  )

KM00	 93.45 93.51 93.48

Hal00	 93.13 93.51 93.32

CSCL 	 * 93.41 92.64 93.02

TKS00 	 94.04 91.00 92.50

ZST00 	 91.99 92.25 92.12

Dej00	 91.87 91.31 92.09

Koe00	 92.08 91.86 91.97

Osb00	 91.65 92.23 91.94

VB00	 91.05 92.03 91.54

PMP00 	 90.63 89.65 90.14

Joh00	 86.24 88.25 87.23

VD00 	 88.82 82.91 85.76
Baseline 72.58 82.14 77.07
2.2 Data
Training was done on the Penn Treebank (Mar-
cus et al, 1993) Wall Street Journal data, sections
02-21. To train the CSCL shallow parser we had
first to convert the WSJ data to a flat format that
directly provides the phrase annotations. This is
done using the ?Chunklink? program provided for
CoNLL-2000 (Tjong Kim Sang and Buchholz,
2000).
Testing was done on two types of data. For
the first experiment, we used the WSJ section 00
(which contains about 45,000 tokens and 23,500
phrases). The goal here was simply to evaluate
the full parser and the shallow parser on text that
is similar to the one they were trained on.
1We note that some of the variations in the results are
due to variations in experimental methodology rather than
parser?s quality. For example, in [KM00], rather than learn-
ing a classifier for each of the 

 different phrases, a discrim-
inator is learned for each of the 


phrase pairs which, sta-
tistically, yields better results. [Hal00] also uses  different
parsers and reports the results of some voting mechanism on
top of these.
Our robustness test (section 3.2) makes use
of section 4 in the Switchboard (SWB) data
(which contains about 57,000 tokens and 17,000
phrases), taken from Treebank 3. The Switch-
board data contains conversation records tran-
scribed from phone calls. The goal here was two
fold. First, to evaluate the parsers on a data source
that is different from the training source. More
importantly, the goal was to evaluate the parsers
on low quality data and observe the absolute per-
formance as well as relative degradation in per-
formance.
The following sentence is a typical example of
the SWB data.
Huh/UH ,/, well/UH ,/, um/UH
,/, you/PRP know/VBP ,/, I/PRP
guess/VBP it/PRP ?s/BES pretty/RB
deep/JJ feelings/NNS ,/, uh/UH ,/,
I/PRP just/RB ,/, uh/UH ,/, went/VBD
back/RB and/CC rented/VBD ,/, uh/UH
,/, the/DT movie/NN ,/, what/WP is/VBZ
it/PRP ,/, GOOD/JJ MORNING/NN
VIET/NNP NAM/NNP ./.
The fact that it has some missing words, repeated
words and frequent interruptions makes it a suit-
able data to test robustness of parsers.
2.3 Representation
We had to do some work in order to unify the in-
put and output representations for both parsers.
Both parsers take sentences annotated with POS
tags as their input. We used the POS tags in the
WSJ and converted both the WSJ and the SWB
data into the parsers? slightly different input for-
mats. We also had to convert the outcomes of the
parsers in order to evaluate them in a fair way.
We choose CoNLL-2000?s chunking format as
our standard output format and converted Collins?
parser outcome into this format.
2.4 Performance Measure
The results are reported in terms of precision, re-
call, and  ffPhraseNet:
Towards Context Sensitive Lexical Semantics?
Xin Li?, Dan Roth?, Yuancheng Tu?
Dept. of Computer Science?
Dept. of Linguistics?
University of Illinois at Urbana-Champaign
{xli1,danr,ytu}@uiuc.edu
Abstract
This paper introduces PhraseNet, a context-
sensitive lexical semantic knowledge base sys-
tem. Based on the supposition that seman-
tic proximity is not simply a relation between
two words in isolation, but rather a relation
between them in their context, English nouns
and verbs, along with contexts they appear in,
are organized in PhraseNet into Consets; Con-
sets capture the underlying lexical concept, and
are connected with several semantic relations
that respect contextually sensitive lexical infor-
mation. PhraseNet makes use of WordNet as
an important knowledge source. It enhances
a WordNet synset with its contextual informa-
tion and refines its relational structure by main-
taining only those relations that respect con-
textual constraints. The contextual informa-
tion allows for supporting more functionali-
ties compared with those of WordNet. Nat-
ural language researchers as well as linguists
and language learners can gain from accessing
PhraseNet with a word token and its context, to
retrieve relevant semantic information.
We describe the design and construction of
PhraseNet and give preliminary experimental
evidence to its usefulness for NLP researches.
1 Introduction
Progress in natural language understanding research ne-
cessitates significant progress in lexical semantics and
the development of lexical semantics resources. In
a broad range of natural language applications, from
?Research supported by NSF grants IIS-99-84168,
ITR-IIS-00-85836 and an ONR MURI award.
Names of authors are listed alphabetically.
prepositional phrase attachment (Pantel and Lin, 2000;
Stetina and Nagao, 1997), co-reference resolution (Ng
and Cardie, 2002) to text summarization (Saggion and
Lapalme, 2002), semantic information is a necessary
component in the inference, by providing a level of ab-
straction that is necessary for robust decisions.
Inducing that the prepositional phrase in ?They ate
a cake with a fork? has the same grammatical
function as that in ?They ate a cake with a
spoon?, for example, depends on the knowledge that
?cutlery? and ?tableware? are the hypernyms of both
?fork? and ?spoon?. However, the noun ?fork? has five
senses listed in WordNet and each of them has several
different hypernyms. Choosing the correct one is a con-
text sensitive decision.
WordNet (Fellbaum, 1998), a manually constructed
lexical reference system provides a lexical database along
with semantic relations among the lexemes of English
and is widely used in NLP tasks today. However, Word-
Net is organized at the word level, and at this level, En-
glish suffers ambiguities. Stand-alone words may have
several meanings and take on relations (e.g., hypernyms,
hyponyms) that depend on their meanings. Consequently,
there are very few success stories of automatically us-
ing WordNet in natural language applications. In many
cases, reported (and unreported) problems are due to the
fact that WordNet enumerates all the senses of polyse-
mous words; attempts to use this resource automatically
often result in noisy and non-uniform information (Brill
and Resnik, 1994; Krymolowski and Roth, 1998).
PhraseNet is designed based on the assumption that,
by and large, semantic ambiguity in English disappears
when local context of words is taken into account. It
makes use of WordNet as an important knowledge source
and is generated automatically using WordNet and ma-
chine learning based processing of large English corpora.
It enhances a WordNet synset with its contextual informa-
tion and refines its relational structure, including relations
such as hypernym, hyponym, antonym and synonym, by
maintaining only those links that respect contextual con-
straints. However, PhraseNet is not just a functional ex-
tension of WordNet. It is an independent lexical semantic
system allied with proper user interfaces and access func-
tions that will allow researchers and practitioners to use
it in applications.
As stated before, PhraseNet, is built on the assumption
that linguistic context is an indispensable factor affecting
the perception of a semantic proximity between words.
In its current design, PhraseNet defines ?context? hierar-
chically with three abstraction levels: abstract syntactic
skeletons, such as
[(S)? (V )? (DO)? (IO)? (P )? (N)]
which stands for Subject, Verb, Direct Object, Indi-
rect Object, Preposition and Noun(Object) of the Prepo-
sition, respectively; syntactic skeletons whose compo-
nents are enhanced by semantic abstraction, such as
[Peop ? send ? Peop ? gift ? on ? Day] and fi-
nally concrete syntactic skeletons from real sentences as
[they ? send?mom? gift? on? Christmas].
Intuitively, while ?candle? and ?cigarette? would score
poorly on semantic similarity without any contextual in-
formation, their occurrence in sentences such as ?John
tried to light a candle/cigarette? may
highlight their connection with the process of burning.
PhraseNet captures such constraints from the contextual
structures extracted automatically from natural language
corpora and enumerates word lists with their hierarchical
contextual information. Several abstractions are made in
the process of extracting the context in order to prevent
superfluous information and support generalization.
The basic unit in PhraseNet is a conset, a word in its
context, together with all relations associated with it. In
the lexical database, consets are chained together via their
similar or hierarchical contexts. By listing every context
extracted from large corpora and all the generalized con-
texts based on those attested sentences, PhraseNet will
have much more consets than synsets in WordNet. How-
ever, the organization of PhraseNet respects the syntactic
structure together with the distinction of senses of each
word in its corresponding contexts.
For example, rather than linking all hypernyms of a
polysemous word to a single word token, PhraseNet con-
nects the hypernym of each sense to the target word in
every context that instantiates that sense. While in Word-
Net every word has an average of 5.4 hypernyms, in
PhraseNet, the average number of hypernyms of a word
in a conset is 1.51.
In addition to querying WordNet semantic relations
to disambiguate consets, PhraseNet alo maintains fre-
1The statistics is taken over 200, 000 words from a mixed
corpus of American English.
quency records of each word in its context to help dif-
ferentiate consets and makes use of defined similarity be-
tween contexts in this process 2.
Several access functions are built into PhraseNet that
allow retrieving information relevant to a word and its
context. When accessed with words and their contextual
information, the system tends to output more relevant se-
mantic information due to the constraint set by their syn-
tactic contexts.
While still in preliminary stages of development and
experimentation and with a lot of functionalities still
missing, we believe that PhraseNet is an important effort
towards building a contextually sensitive lexical semantic
resource, that will be of much value to NLP researchers
as well as linguists and language learners.
The rest of this paper is organized as follows. Sec. 2
presents the design principles of PhraseNet. Sec. 3 de-
scribes the construction of PhraseNet and the current
stage of the implementation. An application that pro-
vides a preliminary experimental evaluation is described
in Sec. 4. Sec. 5 discuses some related work on lexical se-
mantics resources and Sec. 6 discusses future directions
within PhraseNet.
2 The Design of PhraseNet
Context is one important notion in PhraseNet. While the
context may mean different things in natural language,
many previous work in statistically natural language pro-
cessing defined ?context? as an n-word window around
the target word (Gale et al, 1992; Brown et al, 1991;
Roth, 1998). In PhraseNet, ?context? has a more precise
definition that depends on the grammatical structure of a
sentence rather than simply counting surrounding words.
We define ?context? to be the syntactic structure of the
sentence in which the word of interest occurs. Specif-
ically, we define this notion at three abstraction levels.
The highest level is the abstract syntactic skeleton of the
sentence. That is, it is in the form of the different combi-
nations of six syntactic components. Some components
may be missing as long as the structure is from a legit-
imate English sentence. The most complete form of the
abstract syntactic skeleton is:
[(S)? (V )? (DO)? (IO)? (P )? (N)] (1)
which captures all of the six syntactic components such
as Subject, Verb, Direct Object, Indirect Object, Prepo-
sition and Noun(Object) of Preposition, respectively, in
the sentence. And all components are assumed to be
arranged to obey the word order in English. The low-
est level of contexts is the concrete instantiation of the
stated syntactic skeleton, such as [Mary(S)?give(V )?
John(DO) ? gift(IO) ? on(P ) ? birthday(N)] and
2See details in Sec. 3
[I(S)? eat(V )? bread(DO)? with(P )? hand(N)]
which are extracted directly from corpora with grammat-
ical lemmatization done during the process. Therefore,
all word tokens are in their lemma format. The middle
layer(s) consists of generalized formats of the syntactic
skeleton. For example, the first example given above can
be generalized as [Peop(S)?give(V )?Peop(DO)?
Possession(IO) ? on(P ) ?Day(N)] by replacing
some of its components with more abstract semantic con-
cepts.
PhraseNet organizes nouns and verbs into ?consets?
and a ?conset? is defined as a context with all its
corresponding pointers (edges) to other consets. The
context that forms a conset can be either directly ex-
tracted from the corpus, or at a certain level of ab-
straction. For example, both [Mary(S) ? eat(V ) ?
cake(DO) ? on(P ) ? birthday(N), {p1, p2, . . . , pn}]
and [Peop(S) ? eat(V ) ? Food(DO) ? on(P ) ?
Day(N), {p1, p2, . . . , pn}] are consets.
Two types of relational pointers are defined currently
in PhraseNet: Equal and Hyper. Both of these two re-
lations are based on the context of each conset. Equal
is defined among consets with same number of compo-
nents and same syntactic ordering, i.e, some contexts
under the same abstract syntactic structure (the highest
level of context as defined in this paper). It is defined
that the Equal relation exists among consets whose con-
texts are with same abstract syntactic skeleton, if there is
only one component at the same position that is differ-
ent. For example, [Mary(S)?give(V )?John(DO)?
gift(IO)?on(P )?birthday(N), {p1, p2, . . . , pn}] and
[Mary(S) ? send(V ) ? John(DO) ? gift(IO) ?
on(P ) ? birthday(N), {p1, p2, . . . , pk}] are equal be-
cause the syntactic skeleton each of them has is the
same, i.e., [(S) ? (V ) ? (DO) ? (IO) ? (P ) ? (N)]
and except one word in the verb position that is differ-
ent, i.e., ?give? and ?send?, all other five components
at the corresponding same position are the same. The
Equal relation is transitive only with regard to a spe-
cific component in the same position. For example,
to be transitive to the above two example consets, the
Equal conset should be also different from them only
by its verb. The Hyper relation is also defined for con-
sets with same abstract syntactic structure. For conset
A and conset B, if they have the same syntactic struc-
ture, and if there is at least one component of the con-
text in A that is the hypernym of the component in that
of B at the corresponding same position, and all other
components are the same respectively, A is the Hyper
conset of B. For example, both [Molly(S) ? hit(V ) ?
Body(DO), {p1, p2, . . . , pj}] and [Peop(S)?hit(V )?
Body(DO), {p1, p2, . . . , pn}] are Hyper consets of
[Molly(S)?hit(V )?nose(DO), {p1, p2, . . . , pk}]. The
intuition behind these two relations is that the Equal rela-
Figure 1: The basic organization of PhraseNet: The upward
arrow denotes the Hyper relation and the dotted two-way arrow
with a V above denotes the Equal relation that is transitive with
regard to the V component.
tion can cluster a list of words which occur in exactly the
same contextual structure and if the extreme case occurs,
namely when the same context in all these equal consets
with regard to a specific syntactic component groups vir-
tually any nouns or verbs, the Hyper relation can be used
here for further disambiguation.
To summarize, PhraseNet can be thought of as a graph
on consets. Each node is a context and edges between
nodes are relations defined by the context of each node.
They are either Equal or Hyper. Equal relation can be
derived by matching consets and it is easy to implement
while building the Hyper relation requires the assistance
of WordNet and the defined Equal relation. Semantic re-
lations among words can be generated using the two types
of defined edges. For example, it is likely that the target
words in all equal consets with transitivity have similar
meaning. If this is not true at the lowest lower of contexts,
it is more likely to be true at higher, i.e., more generalized
level. Figure 1 shows a simple example reflecting the pre-
liminary design of PhraseNet.
After we get the similar meaning lists based on their
contexts, we can build interaction from this word list to
WordNet and inherit other semantic relations from Word-
Net. However, each member of a word list can help to dis-
ambiguate other members in this list. Therefore, it is ex-
pected that with the pruning assisted by list members, i.e.,
the disambiguation by truncating semantic relations asso-
ciated with each synset in WordNet, the extract meaning
in the context together with all other semantic relations
such as hypernyms, holonyms, troponyms, antonyms can
be derived from WordNet.
In the next two sections we describe our current im-
plementation of these operations and preliminary experi-
ments we have done with them.
2.1 Accessing PhraseNet
Retrieval of information from PhraseNet is done via sev-
eral access functions that we describe below. PhraseNet
is designed to be accessed via multiple functions with
flexible input modes set by the user. These functions
may allow users to exploit several different functionali-
ties of PhraseNet, depending on their goal and amount of
resources they have.
An access function in PhraseNet has two components.
The first component is the input, which can vary from
a single word token to a word with its complete con-
text. The second component is the functionality, which
ranges over simple retrieval and several relational func-
tions, modelled after WordNet relations.
The most basic and simplest way to query PhraseNet
is with a single word. In this case, the system outputs all
contexts the word can occur in, and its related words in
each context.
PhraseNet can also be accessed with input that consists
of a single word token along with its context information.
Context information refers to any of the elements in the
syntactic skeleton defined in Eq. 1, namely, Subject(S),
Verb(V), Direct Object(DO), Indirect Object(IO), Prepo-
sition(P) and Noun(Object) of the Preposition(N). The
contextual roles S, V, DO, IO, P or N or any subset of
them, can be specified by the user or derived by an appli-
cation making use of a shallow or full parser. The more
information the user provides, the more specific the re-
trieved information is.
To ease the requirements from the user, say, in case
no information of this form is available to the user,
PhraseNet will, in the future, have functions that allow a
user to supply a word token and some context, where the
functionality of the word in the context is not specified.
See Sec. 6 for a discussion.
Function Name Input Variables Output
PN WL Word [, Context] Word List
PN RL Word [, Context] WordNet relations
PN SN Word [, Context] Sense
PN ST Context Sentence
Table 1: PhraseNet Access Functions: PhraseNet access
functions along with their input and output. [i] denotes optional
input. PN RL is a family of functions, modelled after WordNet
relations.
Table 1 lists the functionality of the access functions in
PhraseNet. If the user only input a word token without
any context, all those designed functions will return each
context the input word occurs together with the wordlist
in these contexts. Otherwise, the output is constrained by
the input context. The functions are described below:
PN WL takes the optional contextual skeleton and one
specified word in that context as inputs and returns
the corresponding wordlist occurring in that context
or a higher level of context. A parameter to this
function specifies if we want to get the complete
wordlist or those words in the list that satisfy a spe-
cific pruning criterion. (This is the function used in
the experiments in Sec. 4.)
PN RL is modelled after the WordNet access functions.
It will return all words in those contexts that are
linked in PhraseNet by their Equal or Hyper rela-
tion. Those words can help to access WordNet to
derive all lexical relations stored there.
PN SN is modelled after the semantic concordance
in (Landes et al, 1998). It takes a word token and
an optional context as input, and returns the sense
of the word in that context. Similarly to PN RL this
function is implemented by appealing to WordNet
senses and pruning the possible sense based on the
wordlist determined for the given context.
PN ST is not implemented at this point, but is designed
to output a sentence that has same structure as the
input context, but use different words. It is inspired
by the work on reformulation, e.g., (Barzilay and
McKeown, 2001).
We can envision many ways users of PhraseNet can
make use of the retrieved information. At this point in the
life of PhraseNet we focus mostly on using PhraseNet as
a way to acquire semantic features to aid learning based
natural language applications. This determines our prior-
ities in the implementation that we describe next.
3 Constructing PhraseNet
Constructing PhraseNet involves three main stages: (1)
extracting syntactic skeletons from corpora, (2) con-
structing the core element in PhraseNet: consets, and (3)
developing access functions.
The first stage makes use of fully parsed data. In
constructing the current version of PhraseNet we used
two corpora. The first, relatively small corpus of the
1.1 million-word Penn-State Treebank which consists
of American English news articles (WSJ), and is fully
parsed. The second corpus has about 5 million sentences
of the TREC-11 (Voorhees, 2002), also containing mostly
American English news articles (NYT, 1998) and parsed
with Dekang Lin?s minipar parser (Lin, 1998a).
In the near future we are planning to construct a much
larger version of PhraseNet, using Trec-10 and Trec-11
data sets, which cover about 8 GB of text. We believe that
the size is very important here, and will add significant
robustness to our results.
To reduce ostensibly different contexts, two important
abstractions take place at this stage. (1) Syntactic lemma-
tization to get the lemma for both nouns and verbs in
the context defined in Eq. 1. For data parsed via Lin?s
minipar, the lexeme of each word is already included
in the parser. (2) Sematic categorization to unify pro-
nouns, proper names of people, locations and organiza-
tion as well as numbers. This semantic abstraction cap-
tures the underlying semantic proximity by categorizing
multitudinous surface-form proper names into one repre-
senting symbol.
While the first abstraction is simple the second is not.
At this point we use an NE tagger we developed our-
selves based on the approach to phrase identification de-
veloped in (Punyakanok and Roth, 2001). Note that this
abstraction handles multiword phrases. While the accu-
racy of the NE tagger is around 90%, we have yet to ex-
periment with the implication of this additional noise on
PhraseNet.
At the end of this stage, each sentence in the original
corpora is transformed into a single context either at
the lowest level or a more generalized instantiation
(with name entity tagged). For example, ?For six
years, T. Marshall Hahn Jr. has made
corporate acquisitions in the George
Bush mode: kind and gentle.?, changes to:
[Peop?make? acquisition? in?mode].
The second stage of constructing PhraseNet concen-
trates on constructing the core element in PhraseNet:
consets.
To do that, for each context, we collect wordlists that
contain those words that we determine to be admissible in
the context(or contexts share the equal relation). The first
step in constructing the wordlists in PhraseNet is to fol-
low the most strict definition ? include those words that
actually occur in the same context in the corpus. This in-
volves all Equal consets with the transitive property to
a specific syntactic component. We then apply to the
wordlists three types of pruning operations that are based
on (1) frequency of word occurrences in identical or simi-
lar contexts; (2) categorization of words in wordlist based
on clustering all contexts they occur in, and (3) pruning
via the relational structure inherited from WordNet - we
prune from the wordlist outliers in terms of this relational
structure. Some of these operations are parameterized
and determining the optimal setting is an experimental
issue.
1. Every word in a conset wordlist has a frequency
record associated with it, which records the fre-
quency of the word in its exact context. We prune
words with a frequency below k (with the current
corpus we choose k = 3). A disadvantage of
this pruning method is that it might filter out some
appropriate words with a low frequency in reality.
For example, for the partial context [strategy ?
involve? ? ? ? ? ?], we have:
[strategy - involve - * - * - *, < DO : advertisement
4, abuse 1, campaign 2, compromise 1, everything 1,
fumigation 1, item 1, membership 1, option 3, stock-
option 1> ]
In this case,?strategy? is the subject and ?involve?
is the predicate and all words in the list serve as the
direct object. The number in the parentheses is the
frequency of the token. With k = 3 we actually get
as a wordlist only: < advertisment, option >.
2. There are several ways to prune wordlists based on
the different contexts words may occur in. This in-
volves a definition of similar contexts and threshold-
ing based on the number of such contexts a word oc-
curs in. At this point, we implement the construction
of PhraseNet using a clustering of contexts, as done
in (Pantel and Lin, 2002). An exhaustive PhraseNet
list is intersected with word lists generated based on
clustered contexts given by (Pantel and Lin, 2002).
3. We prune from the wordlist outliers in terms of the
relational structure inherited from WordNet. Cur-
rently, this is implemented only using the hypernym
relation. The hypernym shared by the highest num-
ber of words in the wordlist is kept in the database.
For example, by searching ?option? in WordNet, we
get its three senses. Then we collect the hypernyms
of ?option? from all the senses as follows:
05319492(a financial instrument whose value is
based on another security)
04869064(the cognitive process of reaching a deci-
sion)
00026065(something done)
We do this for every word in the original list and find
out the hypernym(s) shared by the highest number of
words in the original wordlist. The final pick in this
case is the synset 05319492 which is shared by both
?option? and ?stock option? as their hypernym.
The third stage is to develop the access functions. As
mentioned before, while we envision many ways users
of PhraseNet can use the retrieved information, at this
preliminary stage of PhraseNet we focus mostly on us-
ing PhraseNet as a way to supply abstract semantic fea-
tures that learning based natural language applications
can benefit from.
For this purpose, so far we have only used and evalu-
ated the function PN WL. PN WL takes as input as
specific word and (optionally) its context and returns a
lists of words which are semantically related to the target
word in the given context. For example,
PN WL ( V= protest, [peop - legislation - * - * - * ])=
[protest, resist, dissent, veto, blackball, negative, for-
bid, prohibit, interdict, proscribe, disallow ].
This function can be implemented via any of the three
pruning methods discussed earlier (see Sec. 4). This
wordlists that this function outputs, can be used to aug-
ment feature based representations for other, learning
based, NLP tasks. Other access functions of PhraseNet
can serve in other ways, e.g., expansions in information
retrieval, but we have not experimented with it yet.
With the experiments we are doing right now,
PhraseNet only takes inputs with the context information
in the format of Eq. 1. Semantic categorization and syn-
tactic lemmatization of the context is required in order to
get matched in the database. However, PhraseNet will,
in the future, have functions that allow a user to supply a
word token and more flexible contexts.
4 Evaluation and Application
In this section we provide a first evaluation of PhraseNet.
We do that in the context of a learning task.
Learning tasks in NLP are typically modelled as clas-
sification tasks, where one seeks a mapping g : X ?
c1, ..., ck, that maps an instance x ? X (e.g., a sentence)
to one of c1, ..., ck ? representing some properties of the
instance (e.g., a part-of-speech tag of a word in the con-
text of the sentence). Typically, the raw representation
? sentence or document ? are first mapped to some fea-
ture based representation, and then a learning algorithm
is applied to learn a mapping from this representation to
the desired property (Roth, 1998). It is clear that in most
cases representing the mapping g in terms of the raw rep-
resentation of the input instance ? words and their order
? is very complex. Functionally simple representations
of this mapping can only be formed if we augment the
information that is readily available in the input instance
with additional, more abstract information. For exam-
ple, it is common to augment sentence representations
with syntactic categories ? part-of-speech (POS), under
the assumption that the sought-after property, for which
we seek the classifier, depends on the syntactic role of a
word in the sentence rather than the specific word. Sim-
ilar logic can be applied to semantic categories. In many
cases, the property seems not to depend on the specific
word used in the sentence ? that could be replaced with-
out affecting this property ? but rather on its ?meaning?.
In this section we show the benefit of using PhraseNet
in doing that in the context of Question Classification.
Question classification (QC) is the task of determining
the semantic class of the answer of a given question.
For example, given the question: ?What Cuban
dictator did Fidel Castro force out
of power in 1958?? we would like to determine
that its answer should be a name of a person. Our
approach to QC follows that of (Li and Roth, 2002).
The question classifier used is a multi-class classifier
which can classify a question into one of 50 fine-grained
classes.
The baseline classifier makes use of syntactic features
like the standard POS information and information ex-
tracted by a shallow parser in addition to the words in
the sentence. The classifier is then augmented with stan-
dard WordNet or with PhraseNet information as follows.
In all cases, words in the sentence are augmented with
additional words that are supposed to be semantically re-
lated to them. The intuition, as described above, is that
this provides a level of abstract ? we could have poten-
tially seen an equivalent question, where other ?equiva-
lent? words occur.
For WordNet, for each word in a question, all its hyper-
nyms are added to its feature based representation (in ad-
dition to the syntactic features). For PhraseNet, for each
word in a question, all the words in the corresponding
conset wordlist are added (where the context is supplied
by the question).
Our experiments compare the three pruning operations
described above. Training is done on a data set of 21,500
questions. Performance is evaluated by the precision of
classifying 1,000 test questions, defined as follows:
Precison = # of correct predictions# of predictions (2)
Table 2 presents the classification precision before and
after incorporating WordNet and PhraseNet information
into the classifier. By augmenting the question classi-
fier with PhraseNet information, even in this preliminary
stage, the error rate of the classifier can be reduced by
12%, while an equivalent use of WordNet information re-
duces the error by only 5.7%.
Information Used Precision Err Reduction
Baseline 84.2% 0%
WordNet 85.1% 5.7%
PN: Freq. based Pruning 84.4% 1.3%
PN: Categ. based Pruning 85% 5.1%
PN: Relation based Pruning 86.1% 12%
Table 2: Question Classification with PhraseNet Informa-
tion Question classification precision and error rate reduction
compared with the baseline error rate(15.8%) by incorporat-
ing WordNet and PhraseNet(PN) information. ?Baseline? is
the classifier that uses only syntactic features. The classifier
is trained over 21,500 questions and tested over 1000 TREC 10
and 11 questions.
5 Related Work
In this section we point to some of the related work
on syntax, semantics interaction and lexical semantic re-
sources in computational linguistics and natural language
processing. Many current syntactic theories make the
common assumption that various aspects of syntactic al-
ternation are predicable via the meaning of the predi-
cate in the sentence (Fillmore, 1968; Jackendoff, 1990;
Levin, 1993). With the resurgence of lexical seman-
tics and corpus linguistics during the past two decades,
this so-called linking regularity triggers a broad interest
of using syntactic representations illustrated in corpora
to classify lexical meaning (Baker et al, 1998; Levin,
1993; Dorr and Jones, 1996; Lapata and Brew, 1999; Lin,
1998b; Pantel and Lin, 2002).
FrameNet (Baker et al, 1998) produces a seman-
tic dictionary that documents combinatorial properties
of English lexical items in semantic and syntactic terms
based on attestations in a very large corpus. In FrameNet,
a frame is an intuitive structure that formalizes the links
between semantics and syntax in the results of lexical
analysis. (Fillmore et al, 2001) However, instead of de-
rived via attested sentences from corpora automatically,
each conceptual frame together with all its frame ele-
ments has to be constructed via slow and labor-intensive
manual work. FrameNet is not constructed automatically
based on observed syntactic alternations. Though deep
semantic analysis is built for each frame, lack of auto-
matic derivation of the semantic roles from large corpora3
confines the usage of this network drastically.
Levin?s classes (Levin, 1993) of verbs are based on the
assumption that the semantics of a verb and its syntactic
behavior are predictably related. She defines 191 verb
classes by grouping 4183 verbs which pattern together
with respect to their diathesis alternations, namely alter-
nations in the expressions of arguments. In Levin?s clas-
sification, it is the syntactic skeletons (such as np-v-np-
pp)to classify verbs directly. Levin?s classification is val-
idated via experiments done by (Dorr and Jones, 1996)
and some counter-arguments are in (Baker and Ruppen-
hofer, 2002). Her work provides a a small knowledge
source that needs further expansion.
Lin?s work (Lin, 1998b; Pantel and Lin, 2002) makes
use of distributional syntactic contextual information to
define semantic proximity. Dekang Lin?s grouping of
similar words is a combination of the abstract syntactic
skeleton and concrete word tokens. Lin uses syntactic de-
pendencies such as ?Subj-people?, ?Modifier-red?, which
combine both abstract syntactic notations and their con-
crete word token representations. He applies this method
to classifying not only verbs, but also nouns and adjec-
tives. While no evaluation has ever been done to deter-
mine if concrete word tokens are necessary when the syn-
tactic phrase types are already presented, Lin?s work in-
directly shows that the concrete lexical representation is
effective.
WordNet (Fellbaum, 1998) by far is the most widely
used semantic database. However, this database does not
3The attempt to label these semantic roles automatically in
(Gildea and Jurafsky, 2002) assumes knowledge of the frame
and covers only 20% of them.
always work as successfully as researchers have expected
(Krymolowski and Roth, 1998; Montemagni and Pirelli,
1998). This seems to be due to lack of topical context
(Harabagiu et al, 1999; Agirre et al, 2001) as well as
local context (Fellbaum, 1998). By adding contextual in-
formation, many researchers, (e.g., (Green et al, 2001;
Lapata and Brew, 1999; Landes et al, 1998)), have al-
ready made some improvements over it.
The work on the importance of connecting syntax and
semantics in developing lexical semantic resources shows
the importance of contextual information as a step to-
wards deeper level of processing. With hierarchical sen-
tential local contexts embedded and used to categorize
word classes automatically, we believe that PhraseNet
provides the right direction for building useful lexical se-
mantic database.
6 Discussion and Further Work
We believe that progress in semantics and in develop-
ing lexical resources is a prerequisite to any signifi-
cant progress in natural language understanding. This
work makes a step in this direction by introducing a
context-sensitive lexical semantic knowledge base sys-
tem, PhraseNet. We have argued that while cur-
rent lexical resources like WordNet are invaluable, we
should move towards contextually sensitive resources.
PhraseNet is designed to fill this gap, and our preliminary
experiments with it are promising.
PhraseNet is an ongoing project and is still in its pre-
liminary stage. There are several key issues that we are
currently exploring. First, given that PhraseNet draws
part of it power from corpora, we are planning to en-
large the corpus used. We believe that the data size
is very important and will add significant robustness to
our current results. At the same time, since construct-
ing PhraseNet relies on machine learning techniques, we
need to study extensively the effect of tuning these on
the reliability of PhraseNet. Second, there are several
functionalities and access functions that we are planning
to augment PhraseNet with. Among those is the ability
of allowing a user to query PhraseNet even without ex-
plicitly specifying the role of words in the context. This
would reduce the requirement for users and applications
using PhraseNet. Finally, current PhraseNet has no lexi-
cal information about adjectives and adverbs, which may
contain important distributional information about their
modified nouns or verbs. We would like to take this in-
formation into consideration in the near future.
References
E. Agirre, O. Ansa, D. Martinez, and E. Hovy. 2001. Enriching
wordnet concepts with topic signatures.
C. Baker and J. Ruppenhofer. 2002. Framenet?s frames vs.
levin?s verb classes. In Proceedings of the 28th Annual Meet-
ing of the Berkeley Linguistics Society.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet project. In Christian Boitet and Pete Whitelock,
editors, Proceedings of the Thirty-Sixth Annual Meeting of
the Association for Computational Linguistics and Seven-
teenth International Conference on Computational Linguis-
tics, pages 86?90, San Francisco, California. Association for
Computational Linguistics, Morgan Kaufmann Publishers.
R. Barzilay and K. R. McKeown. 2001. Extracting paraphrases
from a parallel corpus. In Proceeding of the 10th Conference
of the European Chapter of ACL.
E. Brill and P. Resnik. 1994. A rule-based approach to prepo-
sitional phrase attachment disambiguation. In Proc. of COL-
ING.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mercer.
1991. Word sense disambiguation using statistical methods.
In Proceedings of ACL-1991.
B. Dorr and D. Jones. 1996. Role of word-sense disambigua-
tion in lexical acquisition.
C. Fellbaum. 1998. In C. Fellbaum, editor, WordNet: An Elec-
tronic Lexical Database. The MIT Press.
C. J. Fillmore, C. Wooters, and C. F. Baker. 2001. Building
a large lexical databank which provides deep semantics. In
Proceedings of the Pacific Asian Conference on Language,
Information and Computation, HongKong.
C. J. Fillmore. 1968. The case for case. In Bach and Harms,
editors, Universals in Linguistic Theory, pages 1?88. Holt,
Rinehart, and Winston, New York.
W. A. Gale, K. W. Church, and D. Jarowsky. 1992. A method
for disambiguation word senses in large corpora. Computers
and the Humanities, 26(5-6):415?439.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?288,
September.
R. Green, L. Pearl, B. J. Dorr, and P. Resnik. 2001. Lexical re-
source integration across the syntax-semantics interface. In
Proceedings of WordNet and Other Lexical Resources Work-
shop, NAACL, Pittsburg, June.
S. M. Harabagiu, G. A. Miller, and D. I. Moldovan. 1999.
Wordnet2 - a morphologically and semantically enhanced re-
sources. In Proceedings of ACL-SIGLEX99: Standardizing
Lexical Resources, pages 1?8, Maryland.
R. Jackendoff. 1990. Semantic Structures. MIT Press, Cam-
bridge, MA.
Y. Krymolowski and D. Roth. 1998. Incorporating knowledge
in natural language learning: A case study. In COLING-
ACL?98 workshop on the Usage of WordNet in Natural Lan-
guage Processing Systems.
S. Landes, C. Leacock, and R. I. Tengi. 1998. Building seman-
tic concordances. In C. Fellbaum, editor, WordNet: an Elec-
toronic Lexical Database, pages 199?216. The MIT Press.
M. Lapata and C. Brew. 1999. Using subcategorization to re-
solve verb class ambiguity. In Proceedings of EMNLP, pages
266?274.
B. Levin. 1993. English Verb Classes and Alternations:
A Preliminary Investigation. University of Chicago Press,
Chicago, IL.
X. Li and D. Roth. 2002. Learning question classifiers. In
Proceedings of COLING.
D. Lin. 1998a. Dependency-based evaluation of minipar. In
In Workshop on the Evaluation of Parsing Systems Granada
Spain.
D. Lin. 1998b. An information-theoretic definition of similar-
ity. In Proc. 15th International Conf. on Machine Learning,
pages 296?304. Morgan Kaufmann, San Francisco, CA.
S. Montemagni and V. Pirelli. 1998. Augmenting WordNet-
like lexical resources with distributional evidence. an
application-oriented perspective. In S. Harabagiu, editor,
Use of WordNet in Natural Language Processing Systems:
Proceedings of the Conference, pages 87?93. Association for
Computational Linguistics.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of 40th
Annual Meeting of the ACL, TaiPei.
P. Pantel and D. Lin. 2000. An unsupervised approach to
prepositional phrase attachment using contextually similar
words. In Proceedings of Association for Computational
Linguistics, Hongkong.
P. Pantel and D. Lin. 2002. Discovering word senses from text.
In The Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining.
V. Punyakanok and D. Roth. 2001. The use of classifiers in
sequential inference. In NIPS-13; The 2000 Conference on
Advances in Neural Information Processing Systems, pages
995?1001. MIT Press.
D. Roth. 1998. Learning to resolve natural language ambigu-
ities: A unified approach. In Proc. National Conference on
Artificial Intelligence, pages 806?813.
H. Saggion and G. Lapalme. 2002. Generating indicative-
informative summaries with sumum. Computational Lin-
guistics, 28(4):497?526.
J. Stetina and M. Nagao. 1997. Corpus based pp attachment
ambiguity rosolution with a semantic dictionary. In Proceed-
ings of the 5th Workshop on Very Large Corpora, Beijing and
Hongkong.
E. Voorhees. 2002. Overview of the TREC-2002 question an-
swering track. In The Eleventh TREC Conference, pages
115?123.
A Linear Programming Formulation for Global Inference in Natural
Language Tasks
Dan Roth Wen-tau Yih
Department of Computer Science
University of Illinois at Urbana-Champaign
{danr, yih}@uiuc.edu
Abstract
Given a collection of discrete random variables
representing outcomes of learned local predic-
tors in natural language, e.g., named entities
and relations, we seek an optimal global as-
signment to the variables in the presence of
general (non-sequential) constraints. Examples
of these constraints include the type of argu-
ments a relation can take, and the mutual activ-
ity of different relations, etc. We develop a lin-
ear programming formulation for this problem
and evaluate it in the context of simultaneously
learning named entities and relations. Our ap-
proach allows us to efficiently incorporate do-
main and task specific constraints at decision
time, resulting in significant improvements in
the accuracy and the ?human-like? quality of
the inferences.
1 Introduction
Natural language decisions often depend on the out-
comes of several different but mutually dependent predic-
tions. These predictions must respect some constraints
that could arise from the nature of the data or from do-
main or task specific conditions. For example, in part-of-
speech tagging, a sentence must have at least one verb,
and cannot have three consecutive verbs. These facts can
be used as constraints. In named entity recognition, ?no
entities can overlap? is a common constraint used in var-
ious works (Tjong Kim Sang and De Meulder, 2003).
Efficient solutions to problems of these sort have been
given when the constraints on the predictors are sequen-
tial (Dietterich, 2002). These solutions can be cate-
gorized into the following two frameworks. Learning
global models trains a probabilistic model under the con-
straints imposed by the domain. Examples include varia-
tions of HMMs, conditional models and sequential varia-
tions of Markov random fields (Lafferty et al, 2001). The
other framework, inference with classifiers (Roth, 2002),
views maintaining constraints and learning classifiers as
separate processes. Various local classifiers are trained
without the knowledge of constraints. The predictions
are taken as input on the inference procedure which then
finds the best global prediction. In addition to the concep-
tual simplicity of this approach, it also seems to perform
better experimentally (Tjong Kim Sang and De Meulder,
2003).
Typically, efficient inference procedures in both frame-
works rely on dynamic programming (e.g., Viterbi),
which works well in sequential data. However, in many
important problems, the structure is more general, result-
ing in computationally intractable inference. Problems of
these sorts have been studied in computer vision, where
inference is generally performed over low level measure-
ments rather than over higher level predictors (Levin et
al., 2002; Boykov et al, 2001).
This work develops a novel inference with classifiers
approach. Rather than being restricted on sequential data,
we study a fairly general setting. The problem is defined
in terms of a collection of discrete random variables rep-
resenting binary relations and their arguments; we seek
an optimal assignment to the variables in the presence of
the constraints on the binary relations between variables
and the relation types.
The key insight to this solution comes from re-
cent techniques developed for approximation algo-
rithms (Chekuri et al, 2001). Following this work, we
model inference as an optimization problem, and show
how to cast it as a linear program. Using existing numer-
ical packages, which are able to solve very large linear
programming problems in a very short time1, inference
can be done very quickly.
Our approach could be contrasted with other ap-
1For example, (CPLEX, 2003) is able to solve a linear pro-
gramming problem of 13 million variables within 5 minutes.
proaches to sequential inference or to general Markov
random field approaches (Lafferty et al, 2001; Taskar et
al., 2002). The key difference is that in these approaches,
the model is learned globally, under the constraints im-
posed by the domain. In our approach, predictors do not
need to be learned in the context of the decision tasks,
but rather can be learned in other contexts, or incorpo-
rated as background knowledge. This way, our approach
allows the incorporation of constraints into decisions in a
dynamic fashion and can therefore support task specific
inferences. The significance of this is clearly shown in
our experimental results.
We develop our models in the context of natural lan-
guage inferences and evaluate it here on the problem of
simultaneously recognizing named entities and relations
between them.
1.1 Entity and Relation Recognition
This is the problem of recognizing the kill (KFJ, Os-
wald) relation in the sentence ?J. V. Oswald was
murdered at JFK after his assassin,
R. U. KFJ...? This task requires making several
local decisions, such as identifying named entities in the
sentence, in order to support the relation identification.
For example, it may be useful to identify that Oswald
and KFJ are people, and JFK is a location. This, in turn,
may help to identify that the kill action is described in the
sentence. At the same time, the relation kill constrains its
arguments to be people (or at least, not to be locations)
and helps to enforce that Oswald and KFJ are likely to
be people, while JFK is not.
In our model, we first learn a collection of ?local? pre-
dictors, e.g., entity and relation identifiers. At decision
time, given a sentence, we produce a global decision that
optimizes over the suggestions of the classifiers that are
active in the sentence, known constraints among them
and, potentially, domain or tasks specific constraints rel-
evant to the current decision.
Although a brute-force algorithm may seem feasible
for short sentences, as the number of entity variable
grows, the computation becomes intractable very quickly.
Given n entities in a sentence, there are O(n2) possible
relations between them. Assume that each variable (en-
tity or relation) can take l labels (?none? is one of these
labels). Thus, there are ln2 possible assignments, which
is too large even for a small n.
When evaluated on simultaneous learning of named
entities and relations, our approach not only provides
a significant improvement in the predictors? accuracy;
more importantly, it provides coherent solutions. While
many statistical methods make ?stupid? mistakes (i.e.,
inconsistency among predictions), that no human ever
makes, as we show, our approach improves also the qual-
ity of the inference significantly.
The rest of the paper is organized as follows. Section 2
formally defines our problem and section 3 describes the
computational approach we propose. Experimental re-
sults are given in section 4, followed by some discussion
and conclusion in section 5.
2 The Relational Inference Problem
We consider the relational inference problem within the
reasoning with classifiers paradigm, and study a spe-
cific but fairly general instantiation of this problem, moti-
vated by the problem of recognizing named entities (e.g.,
persons, locations, organization names) and relations be-
tween them (e.g. work for, located in, live in). We con-
sider a set V which consists of two types of variables V =
E ? R. The first set of variables E = {E1, E2, ? ? ? , En}
ranges LE . The value (called ?label?) assigned to Ei ? E
is denoted fEi ? LE . The second set of variables
R = {Rij}{1?i,j?n;i6=j} is viewed as binary relations
over E . Specifically, for each pair of entities Ei and Ej ,
i 6= j, we use Rij and Rji to denote the (binary) relations
(Ei, Ej) and (Ej , Ei) respectively. The set of labels of
relations is LR and the label assigned to relation Rij ? R
is fRij ? LR.
Apparently, there exists some constraints on the labels
of corresponding relation and entity variables. For in-
stance, if the relation is live in, then the first entity should
be a person, and the second entity should be a location.
The correspondence between the relation and entity vari-
ables can be represented by a bipartite graph. Each rela-
tion variable Rij is connected to its first entity Ei , and
second entity Ej . We use N 1 and N 2 to denote the entity
variables of a relation Rij . Specifically, Ei = N 1(Rij)
and Ej = N 2(Rij).
In addition, we define a set of constraints on the out-
comes of the variables in V . C1 : LE ? LR ? {0, 1}
constraint values of the first argument of a relation. C2
is defined similarly and constrains the second argument
a relation can take. For example, (born in, person) is
in C1 but not in C2 because the first entity of relation
born in has to be a person and the second entity can only
be a location instead of a person. Note that while we
define the constraints here as Boolean, our formalisms
in fact allows for stochastic constraints. Also note that
we can define a large number of constraints, such as
CR : LR ? LR ? {0, 1} which constrain types of re-
lations, etc. In fact, as will be clear in Sec. 3 the language
for defining constraints is very rich ? linear (in)equalities
over V .
We exemplify the framework using the problem of si-
multaneous recognition of named entities and relations in
sentences. Briefly speaking, we assume a learning mech-
anism that can recognize entity phrases in sentences,
based on local contextual features. Similarly, we assume
a learning mechanism that can recognize the semantic re-
lation between two given phrases in a sentence.
We seek an inference algorithm that can produce a co-
herent labeling of entities and relations in a given sen-
tence. Furthermore, it follows, as best as possible the
recommendation of the entity and relation classifiers, but
also satisfies natural constraints that exist on whether spe-
cific entities can be the argument of specific relations,
whether two relations can occur together at the same
time, or any other information that might be available at
the inference time (e.g., suppose it is known that enti-
ties A and B represent the same location; one may like to
incorporate an additional constraint that prevents an in-
ference of the type: ?C lives in A; C does not live in B?).
We note that a large number of problems can be mod-
eled this way. Examples include problems such as chunk-
ing sentences (Punyakanok and Roth, 2001), coreference
resolution and sequencing problems in computational bi-
ology. In fact, each of the components of our problem
here, the separate task of recognizing named entities in
sentences and the task of recognizing semantic relations
between phrases, can be modeled this way. However,
our goal is specifically to consider interacting problems
at different levels, resulting in more complex constraints
among them, and exhibit the power of our method.
The most direct way to formalize our inference prob-
lem is via the formalism of Markov Random Field (MRF)
theory (Li, 2001). Rather than doing that, for compu-
tational reasons, we first use a fairly standard transfor-
mation of MRF to a discrete optimization problem (see
(Kleinberg and Tardos, 1999) for details). Specifically,
under weak assumptions we can view the inference prob-
lem as the following optimization problem, which aims
to minimize the objective function that is the sum of the
following two cost functions.
Assignment cost: the cost of deviating from the assign-
ment of the variables V given by the classifiers. The spe-
cific cost function we use is defined as follows: Let l be
the label assigned to variable u ? V . If the marginal prob-
ability estimation is p = P (fu = l), then the assignment
cost cu(l) is ? log p.
Constraint cost: the cost imposed by breaking con-
straints between neighboring nodes. The specific cost
function we use is defined as follows: Consider two en-
tity nodes Ei, Ej and its corresponding relation node Rij ;
that is, Ei = N 1(Rij) and Ej = N 2(Rij). The con-
straint cost indicates whether the labels are consistent
with the constraints. In particular, we use: d1(fEi , fRij )
is 0 if (fRij , fEi) ? C1; otherwise, d1(fEi , fRij ) is ? 2.
Similarly, we use d2 to force the consistency of the sec-
ond argument of a relation.
2In practice, we use a very large number (e.g., 915).
Since we are seeking the most probable global assign-
ment that satisfies the constraints, therefore, the overall
cost function we optimize, for a global labeling f of all
variables is:
C(f) =
?
u?V
cu(fu)
+
?
Rij?R
[
d1(fRij , fEi) + d2(fRij , fEj )
] (1)
3 A Computational Approach to
Relational Inference
Unfortunately, it is not hard to see that the combinatorial
problem (Eq. 1) is computationally intractable even when
placing assumptions on the cost function (Kleinberg and
Tardos, 1999). The computational approach we adopt is
to develop a linear programming (LP) formulation of the
problem, and then solve the corresponding integer lin-
ear programming (ILP) problem. Our LP formulation is
based on the method proposed by (Chekuri et al, 2001).
Since the objective function (Eq. 1) is not a linear func-
tion in terms of the labels, we introduce new binary vari-
ables to represent different possible assignments to each
original variable; we then represent the objective function
as a linear function of these binary variables.
Let x{u,i} be a {0, 1}-variable, defined to be 1 if and
only if variable u is labeled i, where u ? E , i ? LE or
u ? R, i ? LR. For example, x{E1,2} = 1 when the
label of entity E1 is 2; x{R23,3} = 0 when the label of re-
lation R23 is not 3. Let x{Rij ,r,Ei,e1} be a {0, 1}-variable
indicating whether relation Rij is assigned label r and
its first argument, Ei, is assigned label e1. For instance,
x{R12,1,E1,2} = 1 means the label of relation R12 is 1
and the label of its first argument, E1, is 2. Similarly,
x{Rij ,r,Ej ,e2} = 1 indicates that Rij is assigned label r
and its second argument, Ej , is assigned label e2. With
these definitions, the optimization problem can be repre-
sented as the following ILP problem (Figure 1).
Equations (2) and (3) require that each entity or rela-
tion variable can only be assigned one label. Equations
(4) and (5) assure that the assignment to each entity or
relation variable is consistent with the assignment to its
neighboring variables. (6), (7), and (8) are the integral
constraints on these binary variables.
There are several advantages of representing the prob-
lem in an LP formulation. First of all, linear (in)equalities
are fairly general and are able to represent many types
of constraints (e.g., the decision time constraint in the
experiment in Sec. 4). More importantly, an ILP prob-
lem at this scale can be solved very quickly using current
commercial LP/ILP packages, like (Xpress-MP, 2003) or
(CPLEX, 2003). We introduce the general strategies of
solving an ILP problem here.
min
?
E?E
?
e?LE
cE(e) ? x{E,e} +
?
R?R
?
r?LR
cR(r) ? x{R,r}
+
?
Ei,Ej?E
Ei 6=Ej
[
?
r?LR
?
e1?LE
d1(r, e1) ? x{Rij ,r,Ei,e1} +
?
r?LR
?
e2?LE
d2(r, e2) ? x{Rij ,r,Ej ,e2}
]
subject to:
?
e?LE
x{E,e} = 1 ?E ? E (2)
?
r?LR
x{R,r} = 1 ?R ? R (3)
x{E,e} =
?
r?LR
x{R,r,E,e} ?E ? E and ?R ? {R : E = N 1(R) or R : E = N 2(R)} (4)
x{R,r} =
?
e?LE
x{R,r,E,e} ?R ? R and ?E = N 1(R) or E = N 2(R) (5)
x{E,e} ? {0, 1} ?E ? E , e ? LE (6)
x{R,r} ? {0, 1} ?R ? R, r ? LR (7)
x{R,r,E,e} ? {0, 1} ?R ? R, r ? LR, E ? E , e ? LE (8)
Figure 1: Integer Linear Programming Formulation
3.1 Linear Programming Relaxation (LPR)
To solve an ILP problem, a natural idea is to relax the
integral constraints. That is, replacing (6), (7), and (8)
with:
x{E,e} ? 0 ?E ? E , e ? LE (9)
x{R,r} ? 0 ?R ? R, r ? LR (10)
x{R,r,E,e} ? 0 ?R ? R, r ? LR,
E ? E , e ? LE (11)
If LPR returns an integer solution, then it is also the
optimal solution to the ILP problem. If the solution is
non integer, then at least it gives a lower bound to the
value of the cost function, which can be used in modi-
fying the problem and getting closer to deriving an op-
timal integer solution. A direct way to handle the non
integer solution is called rounding, which finds an inte-
ger point that is close to the non integer solution. Un-
der some conditions of cost functions, which do not hold
here, a well designed rounding algorithm can be shown
that the rounded solution is a good approximation to the
optimal solution (Kleinberg and Tardos, 1999; Chekuri et
al., 2001). Nevertheless, in general, the outcomes of the
rounding procedure may not even be a legal solution to
the problem.
3.2 Branch & Bound and Cutting Plane
Branch and bound is the method that divides an ILP prob-
lem into several LP subproblems. It uses LPR as a sub-
routine to generate dual (upper and lower) bounds to re-
duce the search space, and finds the optimal solution as
well. When LPR finds a non integer solution, it splits the
problem on the non integer variable. For example, sup-
pose variable xi is fractional in an non integer solution to
the ILP problem min{cx : x ? S, x ? {0, 1}n}, where S
is the linear constraints. The ILP problem can be split into
two sub LPR problems, min{cx : x ? S?{xi = 0}} and
min{cx : x ? S?{xi = 1}}. Since any feasible solution
provides an upper bound and any LPR solution generates
a lower bound, the search tree can be effectively cut.
Another strategy of dealing with non integer points,
which is often combined with branch & bound, is called
cutting plane. When a non integer solution is given by
LPR, it adds a new linear constraint that makes the non in-
teger point infeasible, while still keeps the optimal integer
solution in the feasible region. As a result, the feasible
region is closer to the ideal polyhedron, which is the con-
vex hull of feasible integer solutions. The most famous
cutting plane algorithm is Gomory?s fractional cutting
plane method (Wolsey, 1998), which can be shown that
only finite number of additional constraints are needed.
Moreover, researchers develop different cutting plane al-
gorithms for different types of ILP problems. One exam-
ple is (Wang and Regan, 2000), which only focuses on
binary ILP problems.
Although in theory, a search based strategy may need
several steps to find the optimal solution, LPR always
generates integer solutions in our experiments. This phe-
nomenon may link to the theory of unimodularity.
3.3 Unimodularity
When the coefficient matrix of a given linear program
in its standard form is unimodular, it can be shown that
the optimal solution to the linear program is in fact inte-
gral (Schrijver, 1986). In other words, LPR is guaranteed
to produce an integer solution.
Definition 3.1 A matrix A of rank m is called unimodu-
lar if all the entries ofA are integers, and the determinant
of every square submatrix of A of order m is in 0,+1,-1.
Theorem 3.1 (Veinott & Dantzig) Let A be an (m,n)-
integral matrix with full row rank m. Then the polyhe-
dron {x|x ? 0;Ax = b} is integral for each integral
vector b, if and only if A is unimodular.
Theorem 3.1 indicates that if a linear programming
problem is in its standard form, then regardless of the
cost function and the integral vector b, the optimal so-
lution is an integer if and only if the coefficient matrix A
is unimodular.
Although the coefficient matrix in our problem is not
unimodular, LPR still produces integer solutions for all
the (thousands of cases) we have experimented with. This
may be due to the fact that the coefficient matrix shares
many properties of a unimodular matrix. As a result, most
of the vertices of the polyhedron are integer points. An-
other possible reason is that given the cost function we
have, the optimal solution is always integer. Because of
the availability of very efficient LP/ILP packages, we de-
fer the exploration of this direction for now.
4 Experiments
We describe below two experiments on the problem of
simultaneously recognizing entities and relations. In the
first, we view the task as a knowledge acquisition task
? we let the system read sentences and identify entities
and relations among them. Given that this is a difficult
task which may require quite often information beyond
the sentence, we consider also a ?forced decision? task,
in which we simulate a question answering situation ?
we ask the system, say, ?who killed whom? and evaluate
it on identifying correctly the relation and its arguments,
given that it is known that somewhere in this sentence
this relation is active. In addition, this evaluation exhibits
the ability of our approach to incorporate task specific
constraints at decision time.
Our experiments are based on the TREC data set
(which consists of articles from WSJ, AP, etc.) that we
annotated for named entities and relations. In order to
effectively observe the interaction between relations and
entities, we picked 1437 sentences that have at least one
active relation. Among those sentences, there are 5336
entities, and 19048 pairs of entities (binary relations). En-
tity labels include 1685 persons, 1968 locations, 978 or-
ganizations and 705 others. Relation labels include 406
located in, 394 work for, 451 orgBased in, 521 live in,
268 kill, and 17007 none. Note that most pairs of entities
have no active relations at all. Therefore, relation none
significantly outnumbers others. Examples of each rela-
tion label and the constraints between a relation variable
and its two entity arguments are shown as follows.
Relation Entity1 Entity2 Example
located in loc loc (New York, US)
work for per org (Bill Gates, Microsoft)
orgBased in org loc (HP, Palo Alto)
live in per loc (Bush, US)
kill per per (Oswald, JFK)
In order to focus on the evaluation of our inference
procedure, we assume the problem of segmentation (or
phrase detection) (Abney, 1991; Punyakanok and Roth,
2001) is solved, and the entity boundaries are given to us
as input; thus we only concentrate on their classifications.
We evaluate our LP based global inference procedure
against two simpler approaches and a third that is given
more information at learning time. Basic, only tests our
entity and relation classifiers, which are trained indepen-
dently using only local features. In particular, the relation
classifier does not know the labels of its entity arguments,
and the entity classifier does not know the labels of rela-
tions in the sentence either. Since basic classifiers are
used in all approaches, we describe how they are trained
here.
For the entity classifier, one set of features are ex-
tracted from words within a size 4 window around the
target phrase. They are: (1) words, part-of-speech tags,
and conjunctions of them; (2) bigrams and trigrams of
the mixture of words and tags. In addition, some other
features are extracted from the target phrase, including:
symbol explanation
icap the first character of a word is capitalized
acap all characters of a word are capitalized
incap some characters of a word are capitalized
suffix the suffix of a word is ?ing?, ?ment?, etc.
bigram bigram of words in the target phrase
len number of words in the target phrase
place3 the phrase is/has a known place?s name
prof3 the phrase is/has a professional title (e.g. Lt.)
name3 the phrase is/has a known person?s name
For the relation classifier, there are three sets of fea-
tures: (1) features similar to those used in the entity clas-
sification are extracted from the two argument entities of
3We collect names of famous places, people and popular ti-
tles from other data sources in advance.
Pattern Example
arg1 , arg2 San Jose, CA
arg1 , ? ? ? a ? ? ? arg2 prof John Smith, a Starbucks manager ? ? ?
in/at arg1 in/at/, arg2 Officials in Perugia in Umbria province said ? ? ?
arg2 prof arg1 CNN reporter David McKinley ? ? ?
arg1 ? ? ? native of ? ? ? arg2 Elizabeth Dole is a native of Salisbury, N.C.
arg1 ? ? ? based in/at arg2 Leslie Kota, a spokeswoman for K mart based in Troy, Mich. said ? ? ?
Table 1: Some patterns used in relation classification
the relation; (2) conjunctions of the features from the two
arguments; (3) some patterns extracted from the sentence
or between the two arguments. Some features in category
(3) are ?the number of words between arg1 and arg2 ?,
?whether arg1 and arg2 are the same word?, or ?arg1 is
the beginning of the sentence and has words that consist
of all capitalized characters?, where arg1 and arg2 rep-
resent the first and second argument entities respectively.
In addition, Table 1 presents some patterns we use.
The learning algorithm used is a variation of the Win-
now update rule incorporated in SNoW (Roth, 1998;
Roth and Yih, 2002), a multi-class classifier that is specif-
ically tailored for large scale learning tasks. SNoW learns
a sparse network of linear functions, in which the targets
(entity classes or relation classes, in this case) are repre-
sented as linear functions over a common feature space.
While SNoW can be used as a classifier and predicts us-
ing a winner-take-all mechanism over the activation value
of the target classes, we can also rely directly on the raw
activation value it outputs, which is the weighted linear
sum of the active features, to estimate the posteriors. It
can be verified that the resulting values are monotonic
with the confidence in the prediction, therefore provide a
good source of probability estimation. We use softmax
(Bishop, 1995) over the raw activation values as condi-
tional probabilities. Specifically, suppose the number of
classes is n, and the raw activation values of class i is
acti. The posterior estimation for class i is derived by the
following equation.
pi =
eacti
?
1?j?n eactj
Pipeline, mimics the typical strategy in solving com-
plex natural language problems ? separating a task into
several stages and solving them sequentially. For exam-
ple, a named entity recognizer may be trained using a dif-
ferent corpus in advance, and given to a relation classifier
as a tool to extract features. This approach first trains an
entity classifier as described in the basic approach, and
then uses the prediction of entities in addition to other
local features to learn the relation identifier. Note that
although the true labels of entities are known here when
training the relation identifier, this may not be the case
in general NLP problems. Since only the predicted en-
tity labels are available in testing, learning on the predic-
tions of the entity classifier presumably makes the rela-
tion classifier more tolerant to the mistakes of the entity
classifier. In fact, we also observe this phenomenon em-
pirically. When the relation classifier is trained using the
true entity labels, the performance is much worse than
using the predicted entity labels.
LP, is our global inference procedure. It takes as in-
put the constraints between a relation and its entity argu-
ments, and the output (the estimated probability distribu-
tion of labels) of the basic classifiers. Note that LP may
change the predictions for either entity labels or relation
labels, while pipeline fully trusts the labels of entity clas-
sifier, and only the relation predictions may be different
from the basic relation classifier. In other words, LP is
able to enhance the performance of entity classification,
which is impossible for pipeline.
The final approach, Omniscience, tests the conceptual
upper bound of this entity/relation classification problem.
It also trains the two classifiers separately as the basic
approach. However, it assumes that the entity classifier
knows the correct relation labels, and similarly the rela-
tion classifier knows the right entity labels as well. This
additional information is then used as features in training
and testing. Note that this assumption is totally unrealis-
tic. Nevertheless, it may give us a hint that how much a
global inference can achieve.
4.1 Results
Tables 2 & 3 show the performance of each approach in
F?=1 using 5-fold cross-validation. The results show that
LP performs consistently better than basic and pipeline,
both in entities and relations. Note that LP does not apply
learning at all, but still outperforms pipeline, which uses
entity predictions as new features in learning. The results
of the omniscient classifiers reveal that there is still room
for improvement. One option is to apply learning to tune
a better cost function in the LP approach.
One of the more significant results in our experiments,
we believe, is the improvement in the quality of the deci-
sions. As mentioned in Sec. 1, incorporating constraints
helps to avoid inconsistency in classification. It is in-
Approach person organization location
Rec. Prec. F1 Rec. Prec. F1 Rec. Prec. F1
Basic 89.4 89.2 89.3 86.9 91.4 89.1 68.2 90.9 77.9
Pipeline 89.4 89.2 89.3 86.9 91.4 89.1 68.2 90.9 77.9
LP 90.4 90.0 90.2 88.5 91.7 90.1 71.5 91.0 80.1
Omniscient 94.9 93.5 94.2 92.3 96.5 94.4 88.3 93.4 90.8
Table 2: Results of Entity Classification
Approach located in work for orgBased in
Rec. Prec. F1 Rec. Prec. F1 Rec. Prec. F1
Basic 54.7 43.0 48.2 42.1 51.6 46.4 36.1 84.9 50.6
Pipeline 51.2 51.6 51.4 41.4 55.6 47.5 36.9 76.6 49.9
LP 53.2 59.5 56.2 40.4 72.9 52.0 36.3 90.1 51.7
Omniscient 64.0 54.5 58.9 50.5 69.1 58.4 50.2 76.7 60.7
Approach live in kill
Rec. Prec. F1 Rec. Prec. F1
Basic 39.7 61.6 48.3 82.1 73.6 77.6
Pipeline 42.6 62.2 50.6 83.2 76.4 79.6
LP 41.5 68.1 51.6 81.3 82.2 81.7
Omniscient 57.0 60.7 58.8 82.1 74.6 78.2
Table 3: Results of Relation Classification
teresting to investigate how often such mistakes happen
without global inference, and see how effectively the
global inference enhances this.
For this purpose, we define the quality of the decision
as follows. For an active relation of which the label is
classified correctly, if both its argument entities are also
predicted correctly, we count it as a coherent prediction.
Quality is then the number of coherent predictions di-
vided by the sum of coherent and incoherent predictions.
Since the basic and pipeline approaches do not have a
global view of the labels of entities and relations, 5%
to 25% of the predictions are incoherent. Therefore, the
quality is not always good. On the other hand, our global
inference procedure, LP, takes the natural constraints into
account, so it never generates incoherent predictions. If
the relation classifier has the correct entity labels as fea-
tures, a good learner should learn the constraints as well.
As a result, the quality of omniscient is almost as good as
LP.
Another experiment we did is the forced decision test,
which boosts the F1 of ?kill? relation to 86.2%. Here
we consider only sentences in which the ?kill? relation
is active. We force the system to determine which of the
possible relations in a sentence (i.e., which pair of en-
tities) has this relation by adding a new linear equality.
This is a realistic situation (e.g., in the context of ques-
tion answering) in that it adds an external constraint, not
present at the time of learning the classifiers and it eval-
uates the ability of our inference algorithm to cope with
it. The results exhibit that our expectations are correct.
In fact, we believe that in natural situations the number
of constraints that can apply is even larger. Observing
the algorithm performs on other, specific, forced deci-
sion tasks verifies that LP is reliable in these situations.
As shown in the experiment, it even performs better than
omniscience, which is given more information at learning
time, but cannot adapt to the situation at decision time.
5 Discussion
We presented an linear programming based approach
for global inference where decisions depend on the out-
comes of several different but mutually dependent classi-
fiers. Even in the presence of a fairly general constraint
structure, deviating from the sequential nature typically
studied, this approach can find the optimal solution effi-
ciently.
Contrary to general search schemes (e.g., beam
search), which do not guarantee optimality, the linear pro-
gramming approach provides an efficient way to finding
the optimal solution. The key advantage of the linear
programming formulation is its generality and flexibility;
in particular, it supports the ability to incorporate classi-
fiers learned in other contexts, ?hints? supplied and de-
cision time constraints, and reason with all these for the
best global prediction. In sharp contrast with the typi-
cally used pipeline framework, our formulation does not
blindly trust the results of some classifiers, and therefore
is able to overcome mistakes made by classifiers with the
help of constraints.
Our experiments have demonstrated these advantages
by considering the interaction between entity and rela-
tion classifiers. In fact, more classifiers can be added and
used within the same framework. For example, if coref-
erence resolution is available, it is possible to incorporate
it in the form of constraints that force the labels of the co-
referred entities to be the same (but, of course, allowing
the global solution to reject the suggestion of these clas-
sifiers). Consequently, this may enhance the performance
of entity/relation recognition and, at the same time, cor-
rect possible coreference resolution errors. Another ex-
ample is to use chunking information for better relation
identification; suppose, for example, that we have avail-
able chunking information that identifies Subj+Verb and
Verb+Object phrases. Given a sentence that has the verb
?murder?, we may conclude that the subject and object of
this verb are in a ?kill? relation. Since the chunking in-
formation is used in the global inference procedure, this
information will contribute to enhancing its performance
and robustness, relying on having more constraints and
overcoming possible mistakes by some of the classifiers.
Moreover, in an interactive environment where a user can
supply new constraints (e.g., a question answering situa-
tion) this framework is able to make use of the new in-
formation and enhance the performance at decision time,
without retraining the classifiers.
As we show, our formulation supports not only im-
proved accuracy, but also improves the ?human-like?
quality of the decisions. We believe that it has the poten-
tial to be a powerful way for supporting natural language
inferences.
Acknowledgements This research has been supported
by NFS grants CAREER IIS-9984168, ITR IIS-0085836,
EIA-0224453, an ONR MURI Award, and an equipment
donation from AMD. We also thank the anonymous ref-
erees for their useful comments.
References
S. Abney. 1991. Parsing by chunks. In S. Abney
R. Berwick and C. Tenny, editors, Principle-based
parsing: Computation and Psycholinguistics, pages
257?278. Kluwer, Dordrecht.
C. Bishop, 1995. Neural Networks for Pattern Recogni-
tion, chapter 6.4: Modelling conditional distributions,
page 215. Oxford University Press.
Y. Boykov, O. Veksler, and R. Zabih. 2001. Fast ap-
proximate energy minimization via graph cuts. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 23(11):1222?1239, November.
C. Chekuri, S. Khanna, J. Naor, and L. Zosin. 2001. Ap-
proximation algorithms for the metric labeling prob-
lem via a new linear programming formulation. In
Symposium on Discrete Algorithms, pages 109?118.
CPLEX. 2003. ILOG, Inc. CPLEX.
http://www.ilog.com/products/cplex/.
T. Dietterich. 2002. Machine learning for sequential
data: A review. In Structural, Syntactic, and Statistical
Pattern Recognition, pages 15?30. Springer-Verlag.
J. Kleinberg and E. Tardos. 1999. Approximation algo-
rithms for classification problems with pairwise rela-
tionships: Metric labeling and markov random fields.
In IEEE Symposium on Foundations of Computer Sci-
ence, pages 14?23.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. 18th
International Conf. on Machine Learning, pages 282?
289. Morgan Kaufmann, San Francisco, CA.
A. Levin, A. Zomet, and Yair Weiss. 2002. Learning
to perceive transparency from the statistics of natu-
ral scenes. In NIPS-15; The 2002 Conference on Ad-
vances in Neural Information Processing Systems.
S. Li. 2001. Markov Random Field Modeling in Image
Analisys. Springer-Verlag.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS-13; The 2000 Confer-
ence on Advances in Neural Information Processing
Systems, pages 995?1001. MIT Press.
D. Roth and W. Yih. 2002. Probabilistic reasoning for
entity & relation recognition. In COLING 2002, The
19th International Conference on Computational Lin-
guistics, pages 835?841.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proc. of AAAI, pages
806?813.
D. Roth. 2002. Reasoning with classifiers. In Proc. of
the European Conference on Machine Learning, pages
506?510.
A. Schrijver. 1986. Theory of Linear and Integer Pro-
gramming. Wiley Interscience series in discrete math-
matics. John Wiley & Sons, December.
B. Taskar, A. Pieter, and D. Koller. 2002. Discrimina-
tive probabilistic models for relational data. In Proc. of
Uncertainty in Artificial Intelligence, pages 485?492.
E. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proc. of
CoNLL-2003, pages 142?147. Edmonton, Canada.
X. Wang and A. Regan. 2000. A cutting plane method
for integer programming problems with binary vari-
ables. Technical Report UCI-ITS-WP-00-12, Univer-
sity of California, Irvine.
L. Wolsey. 1998. Integer Programming. John Wiley &
Sons, Inc.
Xpress-MP. 2003. Dash Optimization. Xpress-MP.
http://www.dashoptimization.com/products.html.
Semantic Role Labeling Via Generalized Inference Over Classifiers
Vasin Punyakanok, Dan Roth, Wen-tau Yih, Dav Zimak Yuancheng Tu
Department of Computer Science Department of Linguistics
University of Illinois at Urbana-Champaign
{punyakan,danr,yih,davzimak,ytu}@uiuc.edu
Abstract
We present a system submitted to the CoNLL-
2004 shared task for semantic role labeling.
The system is composed of a set of classifiers
and an inference procedure used both to clean
the classification results and to ensure struc-
tural integrity of the final role labeling. Lin-
guistic information is used to generate features
during classification and constraints for the in-
ference process.
1 Introduction
Semantic role labeling is a complex task to discover pat-
terns within sentences corresponding to semantic mean-
ing. We believe it is hopeless to expect high levels of per-
formance from either purely manual classifiers or purely
learned classifiers. Rather, supplemental linguistic infor-
mation must be used to support and correct a learning
system. The system we present here is composed of two
phases.
First, a set of phrase candidates is produced using two
learned classifiers?one to discover beginning positions
and one to discover end positions for each argument type.
Hopefully, this phase discovers a small superset of all
phrases in the sentence (for each verb).
In the second phase, the final prediction is made. First,
candidate phrases from the first phase are re-scored using
a classifier designed to determine argument type, given
a candidate phrase. Because phrases are considered as a
whole, global properties of the candidates can be used to
discover how likely it is that a phrase is of a given ar-
gument type. However, the set of possible role-labelings
is restricted by structural and linguistic constraints. We
encode these constraints using linear functions and use
integer programming to ensure the final prediction is con-
sistent (see Section 4).
2 SNoW Learning Architecture
The learning algorithm used is a variation of the Winnow
update rule incorporated in SNoW (Roth, 1998; Roth and
Yih, 2002), a multi-class classifier that is specifically tai-
lored for large scale learning tasks. SNoW learns a sparse
network of linear functions, in which the targets (phrase
border predictions or argument type predictions, in this
case) are represented as linear functions over a common
feature space. It incorporates several improvements over
the basic Winnow update rule. In particular, a regular-
ization term is added, which has the affect of trying to
separate the data with a think separator (Grove and Roth,
2001; Hang et al, 2002). In the work presented here we
use this regularization with a fixed parameter.
Experimental evidence has shown that SNoW activa-
tions are monotonic with the confidence in the prediction
Therefore, it can provide a good source of probability es-
timation. We use softmax (Bishop, 1995) over the raw ac-
tivation values as conditional probabilities. Specifically,
suppose the number of classes is n, and the raw activa-
tion values of class i is acti. The posterior estimation for
class i is derived by the following equation.
score(i) = pi =
eacti
?
1?j?n eactj
3 First Phase: Find Argument Candidates
The first phase is to predict the phrases of a given sen-
tence that correspond to some argument (given the verb).
Unfortunately, it turns out that it is difficult to predict the
exact phrases accurately. Therefore, the goal of the first
phase is to output a superset of the correct phrases by fil-
tering out unlikely candidates.
Specifically, we learn two classifiers, one to detect
beginning phrase locations and a second to detect end
phrase locations. Each multi-class classifier makes pre-
dictions over forty-three classes ? thirty-two argument
types, ten continuous argument types, one class to detect
not begging and one class to detect not end. The follow-
ing features are used:
? Word feature includes the current word, two words
before and two words after.
? Part-of-speech tag (POS) feature includes the POS
tags of the current word, two words before and after.
? Chunk feature includes the BIO tags for chunks of
the current word, two words before and after.
? Predicate lemma & POS tag show the lemma form
and POS tag of the active predicate.
? Voice feature indicates the voice (active/passive) of
the current predicate. This is extracted with a simple
rule: a verb is identified as passive if it follows a to-
be verb in the same phrase chuck and its POS tag
is VBN(past participle) or it immediately follows a
noun phrase.
? Position feature describes if the current word is be-
fore of after the predicate.
? Chunk pattern feature encodes the sequence of
chunks from the current words to the predicate.
? Clause tag indicates the boundary of clauses.
? Clause path feature is a path formed from a semi-
parsed tree containing only clauses and chunks.
Each clause is named with the chunk immediately
preceding it. The clause path is the path from predi-
cate to target word in the semi-parsed tree.
? Clause position feature is the position of the tar-
get word relative to the predicate in the semi-parsed
tree containing only clauses. Specifically, there
are four configurations?target word and predicate
share same parent, parent of target word is ancestor
of predicate, parent of predicate is ancestor of target
word, or otherwise.
Because each phrase consists of a single beginning and
a single ending, these classifiers can be used to construct
a set of potential phrases (by combining each predicted
begin with each predicted end after it of the same type).
Although the outputs of this phase are potential ar-
gument candidates, along with their types, the second
phase re-scores the arguments using all possible types.
After eliminating the types from consideration, the first
phase achieves 98.96% and 88.65% recall (overall, with-
out verb) on the training and the development set, respec-
tively. Because these are the only candidates that are
passed to the second phase, 88.65% is an upper bound
of the recall for our overall system.
4 Second Phase: Phrase Classification
The second phase of our system assigns the final argu-
ment classes to (a subset) of the phrases supplied from the
first phase. This task is accomplished in two steps. First,
a multi-class classifier is used to supply confidence scores
corresponding to how likely individual phrases are to
have specific argument types. Then we look for the most
likely solution over the whole sentence, given the matrix
of confidences and linguistic information that serves as a
set of global constraints over the solution space.
Again, the SNoW learning architecture is used to train
a multi-class classifier to label each phrase to one of
the argument types, plus a special class ? no argument.
Training examples are created from the phrase candidates
supplied from the first phase using the following features:
? Predicate lemma & POS tag, voice, position,
clause Path, clause position, chunk pattern Same
features as the first phase.
? Word & POS tag from the phrase, including the
first/last word and tag, and the head word1.
? Named entity feature tells if the target phrase is,
embeds, overlaps, or is embedded in a named entity.
? Chunk features are the same as named entity (but
with chunks, e.g. noun phrases).
? Length of the target phrase, in the numbers of words
and chunks.
? Verb class feature is the class of the active predicate
described in the frame files.
? Phrase type uses simple heuristics to identify the
target phrase like VP, PP, or NP.
? Sub-categorization describes the phrase structure
around the predicate. We separate the clause where
the predicate is in into three part ? the predicate
chunk, segments before and after the predicate. The
sequence of the phrase types of these three segments
is our feature.
? Baseline follows the rule of identifying AM-NEG
and AM-MOD and uses them as features.
? Clause coverage describes how much of local
clause (from the predicate) is covered by the target
phrase.
? Chunk pattern length feature counts the number of
patterns in the phrase.
? Conjunctions join every pair of the above features
as new features.
? Boundary words & POS tags include one or two
words/tags before and after the target phrase.
1We use simple rules to first decide if a candidate phrase
type is VP, NP, or PP. The headword of an NP phrase is the
right-most noun. Similarly, the left-most verb/proposition of a
VP/PP phrase is extracted as the headword
? Bigrams are pairs of words/tags in the window from
two words before the target to the first word of the
target, and also from the last word to two words after
the phrase.
? Sparse colocation picks one word/tag from the two
words before the phrase, the first word/tag, the last
word/tag of the phrase, and one word/tag from the
two words after the phrase to join as features.
Alternately, we could have derived a scoring function
from the first phase confidences of the open and closed
predictors for each argument type. This method has
proved useful in the literature for shallow parsing (Pun-
yakanok and Roth, 2001). However, it is hoped that ad-
ditional global features of the phrase would be necessary
due to the variety and complexity of the argument types.
See Table 1 for a comparison.
Formally (but very briefly), the phrase classifier is at-
tempting to assign labels to a set of phrases, S1:M , in-
dexed from 1 to M . Each phrase Si can take any label
from a set of phrase labels, P , and the indexed set of
phrases can take a set of labels, s1:M ? PM . If we as-
sume that the classifier returns a score, score(Si = si),
corresponding to the likelihood of seeing label si for
phrase Si, then, given a sentence, the unaltered inference
task that is solved by our system maximizes the score of
the phrase, score(S1:M = s1:M ),
s?1:M = argmax
s1:M?PM
score(S1:M = s1:M )
= argmax
s1:M?PM
M
?
i=1
score(Si = si).
(1)
The second step for phrase identification is eliminating
labelings using global constraints derived from linguistic
information and structural considerations. Specifically,
we limit the solution space through the used of a filter
function, F , that eliminates many phrase labelings from
consideration. It is interesting to contrast this with previ-
ous work that filters individual phrases (see (Carreras and
Ma`rquez, 2003)). Here, we are concerned with global
constraints as well as constraints on the phrases. There-
fore, the final labeling becomes
s?1:M = argmax
s1:M?F(PM)
M
?
i=1
score(Si = si) (2)
The filter function used considers the following con-
straints:
1. Arguments cannot cover the predicate except those
that contain only the verb or the verb and the follow-
ing word.
2. Arguments cannot overlap with the clauses (they can
be embedded in one another).
3. If a predicate is outside a clause, its arguments can-
not be embedded in that clause.
4. No overlapping or embedding phrases.
5. No duplicate argument classes for A0-A5,V.
6. Exactly one V argument per sentence.
7. If there is C-V, then there has to be a V-A1-CV pat-
tern.
8. If there is a R-XXX argument, then there has to be a
XXX argument.
9. If there is a C-XXX argument, then there has to be
a XXX argument; in addition, the C-XXX argument
must occur after XXX.
10. Given the predicate, some argument classes are ille-
gal (e.g. predicate ?stalk? can take only A0 or A1).
Constraint 1 is valid because all the arguments of a pred-
icate must lie outside the predicate. The exception is for
the boundary of the predicate itself. Constraint 1 through
constraint 3 are actually constraints that can be evaluated
on a per-phrase basis and thus can be applied to the indi-
vidual phrases at any time. For efficiency sake, we elimi-
nate these even before the second phase scoring is begun.
Constraints 5, 8, and 9 are valid for only a subset of the
arguments.
These constraints are easy to transform into linear con-
straints (for example, for each class c, constraint 5 be-
comes
?M
i=1[Si = c] ? 1) 2. Then the optimum solution
of the cost function given in Equation 2 can be found by
integer linear programming3. A similar method was used
for entity/relation recognition (Roth and Yih, 2004).
Almost all previous work on shallow parsing and
phrase classification has used Constraint 4 to ensure that
there are no overlapping phrases. By considering addi-
tional constraints, we show improved performance (see
Table 1).
5 Results
In this section, we present results. For the second phase,
we evaluate the quality of the phrase predictor. The re-
sult first evaluates the phrase classifier, given the perfect
phrase locations without using inference (i.e. F(PM ) =
PM ). The second, adds inference to the phrase classifica-
tion over the perfect classifiers (see Table 2). We evaluate
the overall performance of our system (without assum-
ing perfect phrases) by training and evaluating the phrase
classifier on the output from the first phase (see Table 3).
Finally,since this is a tagging task, we compare this
system with the basic tagger that we have, the CLCL
2where [x] is 1 if x is true and 0 otherwise
3(Xpress-MP, 2003) was used in all experiments to solve in-
teger linear programming.
Precision Recall F1
1st Phase, non-Overlap 70.54% 61.50% 65.71
1st Phase, All Const. 70.97% 60.74% 65.46
2nd Phase, non-Overlap 69.69% 64.75% 67.13
2nd Phase, All Const. 71.96% 64.93% 68.26
Table 1: Summary of experiments on the development set.
The phrase scoring is choosen from either the first phase or the
second phase and each is evaluated by considering simply non-
overlapping constraints or the full set of linguistic constraints.
To make a fair comparison, parameters were set seperately to
optimize performance when using the first phase results. All
results are for overall performance.
Precision Recall F1
Without Inference 86.95% 87.24% 87.10
With Inference 88.03% 88.23% 88.13
Table 2: Results of second phase phrase prediction and in-
ference assuming perfect boundary detection in the first phase.
Inference improves performance by restricting label sequences
rather than restricting structural properties since the correct
boundaries are given. All results are for overall performance
on the development set.
shallow parser from (Punyakanok and Roth, 2001), which
is equivalent to using the scoring function from the first
phase with only the non-overlapping constraints. Table 1
shows how how additional constraints over the standard
non-overlapping constraints improve performance on the
development set4.
6 Conclusion
We show that linguistic information is useful for semantic
role labeling used both to derive features and to derive
hard constraints on the output. We show that it is possible
to use integer linear programming to perform inference
that incorporates a wide variety of hard constraints that
would be difficult to incorporate using existing methods.
In addition, we provide further evidence supporting the
use of scoring phrases over scoring phrase boundaries for
complex tasks.
Acknowledgments This research is supported by
NSF grants ITR-IIS-0085836, ITR-IIS-0085980 and IIS-
9984168, EIA-0224453 and an ONR MURI Award. We
also thank AMD for their equipment donation and Dash
Optimization for free academic use of their Xpress-MP
software.
References
C. Bishop, 1995. Neural Networks for Pattern Recognition,
chapter 6.4: Modelling conditional distributions, page 215.
Oxford University Press.
4The test set was not publicly available to evaluate these re-
sults.
Precision Recall F?=1
Overall 70.07% 63.07% 66.39
A0 81.13% 77.70% 79.38
A1 74.21% 63.02% 68.16
A2 54.16% 41.04% 46.69
A3 47.06% 26.67% 34.04
A4 71.43% 60.00% 65.22
A5 0.00% 0.00% 0.00
AM-ADV 39.36% 36.16% 37.69
AM-CAU 45.95% 34.69% 39.53
AM-DIR 42.50% 34.00% 37.78
AM-DIS 52.00% 67.14% 58.61
AM-EXT 46.67% 50.00% 48.28
AM-LOC 33.47% 34.65% 34.05
AM-MNR 45.19% 36.86% 40.60
AM-MOD 92.49% 94.96% 93.70
AM-NEG 85.92% 96.06% 90.71
AM-PNC 32.79% 23.53% 27.40
AM-PRD 0.00% 0.00% 0.00
AM-TMP 59.77% 56.89% 58.30
R-A0 81.33% 76.73% 78.96
R-A1 58.82% 57.14% 57.97
R-A2 100.00% 22.22% 36.36
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 54.55% 42.86% 48.00
V 98.37% 98.37% 98.37
Table 3: Results on the test set.
X. Carreras and L. Ma`rquez. 2003. Phrase recognition by filter-
ing and ranking with perceptrons. In Proceedings of RANLP-
2003.
A. Grove and D. Roth. 2001. Linear concepts and hidden vari-
ables. Machine Learning, 42(1/2):123?141.
T. Hang, F. Damerau, , and D. Johnson. 2002. Text chunking
based on a generalization of winnow. Journal of Machine
Learning Research, 2:615?637.
V. Punyakanok and D. Roth. 2001. The use of classifiers in
sequential inference. In NIPS-13; The 2000 Conference on
Advances in Neural Information Processing Systems, pages
995?1001. MIT Press.
D. Roth and W. Yih. 2002. Probabilistic reasoning for entity
& relation recognition. In COLING 2002, The 19th Interna-
tional Conference on Computational Linguistics, pages 835?
841.
D. Roth and W. Yih. 2004. A linear programming formulation
for global inference in natural language tasks. In Proc. of
CoNLL-2004.
D. Roth. 1998. Learning to resolve natural language ambigui-
ties: A unified approach. In Proc. of AAAI, pages 806?813.
Xpress-MP. 2003. Dash Optimization. Xpress-MP.
http://www.dashoptimization.com/products.html.
Learning Hebrew Roots: Machine Learning with Linguistic Constraints
Ezra Daya
Dept. of Computer Science
University of Haifa
31905 Haifa
Israel
edaya@cs.haifa.ac.il
Dan Roth
Dept. of Computer Science
University of Illinois
Urbana, IL 61801
USA
danr@cs.uiuc.edu
Shuly Wintner
Dept. of Computer Science
University of Haifa
31905 Haifa
Israel
shuly@cs.haifa.ac.il
Abstract
The morphology of Semitic languages is unique in
the sense that the major word-formation mechanism
is an inherently non-concatenative process of inter-
digitation, whereby two morphemes, a root and a
pattern, are interwoven. Identifying the root of a
given word in a Semitic language is an important
task, in some cases a crucial part of morphological
analysis. It is also a non-trivial task, which many
humans find challenging. We present a machine
learning approach to the problem of extracting roots
of Hebrew words. Given the large number of po-
tential roots (thousands), we address the problem as
one of combining several classifiers, each predict-
ing the value of one of the root?s consonants. We
show that when these predictors are combined by
enforcing some fairly simple linguistics constraints,
high accuracy, which compares favorably with hu-
man performance on this task, can be achieved.
1 Introduction
The standard account of word-formation processes
in Semitic languages describes words as combina-
tions of two morphemes: a root and a pattern.1 The
root consists of consonants only, by default three
(although longer roots are known), called radicals.
The pattern is a combination of vowels and, possi-
bly, consonants too, with ?slots? into which the root
consonants can be inserted. Words are created by
interdigitating roots into patterns: the first radical is
inserted into the first consonantal slot of the pattern,
the second radical fills the second slot and the third
fills the last slot. See Shimron (2003) for a survey.
Identifying the root of a given word is an im-
portant task. Although existing morphological an-
alyzers for Hebrew only provide a lexeme (which
is a combination of a root and a pattern), for other
Semitic languages, notably Arabic, the root is an
essential part of any morphological analysis sim-
1An additional morpheme, vocalization, is used to abstract
the pattern further; for the present purposes, this distinction is
irrelevant.
ply because traditional dictionaries are organized by
root, rather than by lexeme. Furthermore, roots are
known to carry some meaning, albeit vague. We be-
lieve that this information can be useful for compu-
tational applications and are currently experiment-
ing with the benefits of using root and pattern infor-
mation for automating the construction of a Word-
Net for Hebrew.
We present a machine learning approach, aug-
mented by limited linguistic knowledge, to the prob-
lem of identifying the roots of Hebrew words. To
the best of our knowledge, this is the first appli-
cation of machine learning to this problem. While
there exist programs which can extract the root of
words in Arabic (Beesley, 1998a; Beesley, 1998b)
and Hebrew (Choueka, 1990), they are all depen-
dent on labor intensive construction of large-scale
lexicons which are components of full-scale mor-
phological analyzers. Note that Tim Bockwalter?s
Arabic morphological analyzer2 only uses ?word
stems ? rather than root and pattern morphemes ? to
identify lexical items. (The information on root and
pattern morphemes could be added to each stem en-
try if this were desired.)? The challenge of our work
is to automate this process, avoiding the bottleneck
of having to laboriously list the root and pattern of
each lexeme in the language, and thereby gain in-
sights that can be used for more detailed morpho-
logical analysis of Semitic languages.
As we show in section 2, identifying roots is a
non-trivial problem even for humans, due to the
complex nature of Hebrew derivational and inflec-
tional morphology and the peculiarities of the He-
brew orthography. From a machine learning per-
spective, this is an interesting test case of interac-
tions among different yet interdependent classifiers.
After presenting the data in section 3, we discuss a
simple, baseline, learning approach (section 4) and
then propose two methods for combining the results
of interdependent classifiers (section 5), one which
is purely statistical and one which incorporates lin-
2http://www.qamus.org/morphology.htm
guistic constraints, demonstrating the improvement
of the hybrid approach. We conclude with sugges-
tions for future research.
2 Linguistic background
In this section we refer to Hebrew only, although
much of the description is valid for other Semitic
languages as well. As an example of root-and-
pattern morphology, consider the Hebrew roots
g.d.l, k.t.b and r.$.m and the patterns haCCaCa,
hitCaCCut and miCCaC, where the ?C?s indicate
the slots. When the roots combine with these pat-
terns the resulting lexemes are hagdala, hitgadlut,
migdal, haktaba, hitkatbut, miktab, har$ama, hi-
tra$mut, mir$am, respectively. After the root com-
bines with the pattern, some morpho-phonological
alternations take place, which may be non-trivial:
for example, the hitCaCCut pattern triggers assimi-
lation when the first consonant of the root is t or d :
thus, d.r.$+hitCaCCut yields hiddar$ut. The same
pattern triggers metathesis when the first radical is
s or $ : s.d.r+hitCaCCut yields histadrut rather than
the expected *hitsadrut. Semi-vowels such as w or
y in the root are frequently combined with the vow-
els of the pattern, so that q.w.m+haCCaCa yields
haqama, etc. Frequently, root consonants such as w
or y are altogether missing from the resulting form.
These matters are complicated further due to two
sources: first, the standard Hebrew orthography
leaves most of the vowels unspecified. It does not
explicate a and e vowels, does not distinguish be-
tween o and u vowels and leaves many of the i
vowels unspecified. Furthermore, the single letter
w is used both for the vowels o and u and for the
consonant v, whereas i is similarly used both for
the vowels i and for the consonant y. On top of
that, the script dictates that many particles, includ-
ing four of the most frequent prepositions, the def-
inite article, the coordinating conjunction and some
subordinating conjunctions all attach to the words
which immediately follow them. Thus, a form such
as mhgr can be read as a lexeme (?immigrant?), as
m-hgr ?from Hagar?or even as m-h-gr ?from the
foreigner?. Note that there is no deterministic way
to tell whether the first m of the form is part of the
pattern, the root or a prefixing particle (the preposi-
tion m ?from?).
The Hebrew script has 22 letters, all of which
can be considered consonants. The number of
tri-consonantal roots is thus theoretically bounded
by 223, although several phonological constraints
limit this number to a much smaller value. For
example, while roots whose second and third radi-
cals are identical abound in Semitic languages, roots
whose first and second radicals are identical are ex-
tremely rare (see McCarthy (1981) for a theoreti-
cal explanation). To estimate the number of roots
in Hebrew we compiled a list of roots from two
sources: a dictionary (Even-Shoshan, 1993) and the
verb paradigm tables of Zdaqa (1974). The union of
these yields a list of 2152 roots.3
While most Hebrew roots are regular, many be-
long to weak paradigms, which means that root con-
sonants undergo changes in some patterns. Exam-
ples include i or n as the first root consonant, w or
i as the second, i as the third and roots whose sec-
ond and third consonants are identical. For example,
consider the pattern hCCCh. Regular roots such as
p.s.q yield forms such as hpsqh. However, the irreg-
ular roots n.p.l, i.c.g, q.w.m and g.n.n in this pattern
yield the seemingly similar forms hplh, hcgh, hqmh
and hgnh, respectively. Note that in the first and sec-
ond examples, the first radical (n or i ) is missing, in
the third the second radical (w) is omitted and in
the last example one of the two identical radicals is
omitted. Consequently, a form such as hC1C2h can
have any of the roots n.C1.C2, C1.w.C2, C1.i.C2,
C1.C2.C2 and even, in some cases, i.C1.C2.
While the Hebrew script is highly ambiguous,
ambiguity is somewhat reduced for the task we con-
sider here, as many of the possible lexemes of a
given form share the same root. Still, in order to cor-
rectly identify the root of a given word, context must
be taken into consideration. For example, the form
$mnh has more than a dozen readings, including
the adjective ?fat? (feminine singular), which has
the root $.m.n, and the verb ?count?, whose root is
m.n.i, preceded by a subordinating conjunction. In
the experiments we describe below we ignore con-
text completely, so our results are handicapped by
design.
3 Data and methodology
We take a machine learning approach to the prob-
lem of determining the root of a given word. For
training and testing, a Hebrew linguist manually
tagged a corpus of 15,000 words (a set of newspa-
per articles). Of these, only 9752 were annotated;
the reason for the gap is that some Hebrew words,
mainly borrowed but also some frequent words such
as prepositions, do not have roots; we further elim-
inated 168 roots with more than three consonants
and were left with 5242 annotated word types, ex-
hibiting 1043 different roots. Table 1 shows the dis-
tribution of word types according to root ambiguity.
3Only tri-consonantal roots are counted. Ornan (2003) men-
tions 3407 roots, whereas the number of roots in Arabic is esti-
mated to be 10,000 (Darwish, 2002).
Number of roots 1 2 3 4
Number of words 4886 335 18 3
Table 1: Root ambiguity in the corpus
Table 2 provides the distribution of the roots of
the 5242 word types in our corpus according to root
type, where Ci is the i-th radical (note that some
roots may belong to more than one group).
Paradigm Number Percentage
C1 = i 414 7.90%
C1 = w 28 0.53%
C1 = n 419 7.99%
C2 = i 297 5.66%
C2 = w 517 9.86%
C3 = h 18 0.19%
C3 = i 677 12.92%
C2 = C3 445 8.49%
Regular 3061 58.41%
Table 2: Distribution of root paradigms
As assurance for statistical reliability, in all the
experiments discussed in the sequel (unless other-
wise mentioned) we performed 10-fold cross valida-
tion runs a for every classification task during evalu-
ation. We also divided the test corpus into two sets:
a development set of 4800 words and a held-out set
of 442 words. Only the development set was used
for parameter tuning. A given example is a word
type with all its (manually tagged) possible roots.
In the experiments we describe below, our system
produces one or more root candidates for each ex-
ample. For each example, we define tp as the num-
ber of candidates correctly produced by the system;
fp as the number of candidates which are not cor-
rect roots; and fn as the number of correct roots the
system did not produce. As usual, we define recall
as
tp
tp+fp and precision as
tp
tp+fn ; we then compute
f -measure for each example (with ? = 0.5) and
(macro-) average to obtain the system?s overall f -
measure.
To estimate the difficulty of this task, we asked
six human subjects to perform it. Subjects were
asked to identify all the possible roots of all the
words in a list of 200 words (without context), ran-
domly chosen from the test corpus. All subjects
were computer science graduates, native Hebrew
speakers with no linguistic background. The aver-
age precision of humans on this task is 83.52%, and
with recall at 80.27%, f -measure is 81.86%. Two
main reasons for the low performance of humans
are the lack of context and the ambiguity of some of
the weak paradigms.
4 A machine learning approach
To establish a baseline, we first performed two ex-
periments with simple, baseline classifiers. In all the
experiments described in this paper we use SNoW
(Roth, 1998) as the learning environment, with win-
now as the update rule (using perceptron yielded
comparable results). SNoW is a multi-class clas-
sifier that is specifically tailored for learning in do-
mains in which the potential number of information
sources (features) taking part in decisions is very
large, of which NLP is a principal example. It works
by learning a sparse network of linear functions
over a pre-defined or incrementally learned feature
space. SNoW has already been used successfully
as the learning vehicle in a large collection of nat-
ural language related tasks, including POS tagging,
shallow parsing, information extraction tasks, etc.,
and compared favorably with other classifiers (Roth,
1998; Punyakanok and Roth, 2001; Florian, 2002).
Typically, SNoW is used as a classifier, and predicts
using a winner-take-all mechanism over the activa-
tion values of the target classes. However, in addi-
tion to the prediction, it provides a reliable confi-
dence level in the prediction, which enables its use
in an inference algorithm that combines predictors
to produce a coherent inference.
4.1 Feature types
All the experiments we describe in this work share
the same features and differ only in the target clas-
sifiers. The features that are used to characterize a
word are both grammatical and statistical:
? Location of letters (e.g., the third letter of the
word is b ). We limit word length to 20, thus
obtaining 440 features of this type (recall the
the size of the alphabet is 22).
? Bigrams of letters, independently of their loca-
tion (e.g., the substring gd occurs in the word).
This yields 484 features.
? Prefixes (e.g., the word is prefixed by k$h
?when the?). We have 292 features of this type,
corresponding to 17 prefixes and sequences
thereof.
? Suffixes (e.g., the word ends with im, a plural
suffix). There are 26 such features.
4.2 Direct prediction
In the first of the two experiments, referred to as
Experiment A, we trained a classifier to learn roots
as a single unit. The two obvious drawbacks of
this approach are the large set of targets and the
sparseness of the training data. Of course, defin-
ing a multi-class classification task with 2152 tar-
gets, when only half of them are manifested in the
training corpus, does not leave much hope for ever
learning to identify the missing targets.
In Experiment A, the macro-average precision of
ten-fold cross validation runs of this classification
problem is 45.72%; recall is 44.37%, yielding an
f -score of 45.03%. In order to demonstrate the in-
adequacy of this method, we repeated the same ex-
periment with a different organization of the train-
ing data. We chose 30 roots and collected all their
occurrences in the corpus into a test file. We then
trained the classifier on the remainder of the corpus
and tested on the test file. As expected, the accuracy
was close to 0%,
4.3 Decoupling the problem
In the second experiment, referred to as Experi-
ment B, we separated the problem into three dif-
ferent tasks. We trained three classifiers to learn
each of the root consonants in isolation and then
combined the results in the straight-forward way
(a conjunction of the decisions of the three classi-
fiers). This is still a multi-class classification but
the number of targets in every classification task is
only 22 (the number of letters in the Hebrew al-
phabet) and data sparseness is no longer a problem.
As we show below, each classifier achieves much
better generalization, but the clear limitation of this
method is that it completely ignores interdependen-
cies between different targets: the decision on the
first radical is completely independent of the deci-
sion on the second and the third.
We observed a difference between recognizing
the first and third radicals and recognizing the sec-
ond one, as can be seen in table 3. These results cor-
respond well to our linguistic intuitions: the most
difficult cases for humans are those in which the
second radical is w or i, and those where the second
and the third consonants are identical. Combining
the three classifiers using logical conjunction yields
an f -measure of 52.84%. Here, repeating the same
experiment with the organization of the corpus such
that testing is done on unseen roots yielded 18.1%
accuracy.
To demonstrate the difficulty of the problem, we
conducted yet another experiment. Here, we trained
the system as above but we tested it on different
words whose roots were known to be in the training
set. The results of experiment A here were 46.35%,
whereas experiment B was accurate in 57.66% of
C1 C2 C3 root
Precision: 82.25 72.29 81.85 53.60
Recall: 80.13 70.00 80.51 52.09
f -measure: 81.17 71.13 81.18 52.84
Table 3: Accuracy of SNoW?s identifying the cor-
rect radical
the cases. Evidently, even when testing only on
previously seen roots, both na??ve methods are un-
successful (although method A here outperforms
method B).
5 Combining interdependent classifiers
Evidently, simple combination of the results of the
three classifiers leaves much room for improve-
ment. Therefore we explore other ways for com-
bining these results. We can rely on the fact that
SNoW provides insight into the decisions of the
classifiers ? it lists not only the selected target, but
rather all candidates, with an associated confidence
measure. Apparently, the correct radical is chosen
among SNoW?s top-n candidates with high accu-
racy, as the data in table 3 reveal.
This observation calls for a different way of com-
bining the results of the classifiers which takes into
account not only the first candidate but also others,
along with their confidence scores.
5.1 HMM combination
We considered several ways, e.g., via HMMs, of ap-
pealing to the sequential nature of the task (C1 fol-
lowed by C2, followed by C3). Not surprisingly, di-
rect applications of HMMs are too weak to provide
satisfactory results, as suggested by the following
discussion. The approach we eventually opted for
combines the predictive power of a classifier to es-
timate more accurate state probabilities.
Given the sequential nature of the data and the
fact that our classifier returns a distribution over
the possible outcomes for each radical, a natural
approach is to combine SNoW?s outcomes via a
Markovian approach. Variations of this approach
are used in the context of several NLP problems,
including POS tagging (Schu?tze and Singer, 1994),
shallow parsing (Punyakanok and Roth, 2001) and
named entity recognition (Tjong Kim Sang and
De Meulder, 2003).
Formally, we assume that the confidence supplied
by the classifier is the probability of a state (radical,
c) given the observation o (the word), P (c|o). This
information can be used in the HMM framework by
applying Bayes rule to compute
P (o|c) = P (c|o)P (o)P (c) ,
where P (o) and P (c) are the probabilities of ob-
serving o and being at c, respectively. That is,
instead of estimating the observation probability
P (o|c) directly from training data, we compute
it from the classifiers? output. Omitting details
(see Punyakanok and Roth (2001)), we can now
combine the predictions of the classifiers by finding
the most likely root for a given observation, as
r = argmaxP (c1c2c3|o, ?)
where ? is a Markov model that, in this case, can
be easily learned from the supervised data. Clearly,
given the short root and the relatively small number
of values of ci that are supported by the outcomes
of SNoW, there is no need to use dynamic program-
ming here and a direct computation is possible.
However, perhaps not surprisingly given the dif-
ficulty of the problem, this model turns out to be too
simplistic. In fact, performance deteriorated. We
conjecture that the static probabilities (the model)
are too biased and cause the system to abandon good
choices obtained from SNoW in favor of worse can-
didates whose global behavior is better.
For example, the root &.b.d was correctly gen-
erated by SNoW as the best candidate for the word
&obdim, but since P (C3 = b|C2 = b), which is
0.1, is higher than P (C3 = d|C2 = b), which is
0.04, the root &.b.b was produced instead. Note that
in the above example the root &.b.b cannot possibly
be the correct root of &obdim since no pattern in
Hebrew contains the letter d, which must therefore
be part of the root. It is this kind of observations that
motivate the addition of linguistic knowledge as a
vehicle for combining the results of the classifiers.
An alternative approach, which we intend to investi-
gate in the future, is the introduction of higher-level
classifiers which take into account interactions be-
tween the radicals (Punyakanok and Roth, 2001).
5.2 Adding linguistic constraints
The experiments discussed in section 4 are com-
pletely devoid of linguistic knowledge. In partic-
ular, experiment B inherently assumes that any se-
quence of three consonants can be the root of a
given word. This is obviously not the case: with
very few exceptions, all radicals must be present in
any inflected form (in fact, only w, i, n and in an ex-
ceptional case l can be deleted when roots combine
with patterns). We therefore trained the classifiers
to consider as targets only letters that occurred in
the observed word, plus w, i, n and l, rather than
any of the alphabet letters. The average number of
targets is now 7.2 for the first radical, 5.7 for the
second and 5.2 for the third (compared to 22 each in
the previous setup).
In this model, known as the sequential model
(Even-Zohar and Roth, 2001), SNoW?s perfor-
mance improved slightly, as can be seen in table 4
(compare to table 3). Combining the results in
the straight-forward way yields an f -measure of
58.89%, a small improvement over the 52.84% per-
formance of the basic method. This new result
should be considered baseline. In what follows we
always employ the sequential model for training and
testing the classifiers, using the same constraints.
However, we employ more linguistic knowledge for
a more sophisticated combination of the classifiers.
C1 C2 C3 root
Precision: 83.06 72.52 83.88 59.83
Recall: 80.88 70.20 82.50 57.98
f -measure: 81.96 71.34 83.18 58.89
Table 4: Accuracy of SNoW?s identifying the cor-
rect radical, sequential model
5.3 Combining classifiers using linguistic
knowledge
SNoW provides a ranking on all possible roots. We
now describe the use of linguistic constraints to re-
rank this list. We implemented a function which
uses knowledge pertaining to word-formation pro-
cesses in Hebrew in order to estimate the likeli-
hood of a given candidate being the root of a given
word. The function practically classifies the can-
didate roots into one of three classes: good candi-
dates, which are likely to be the root of the word;
bad candidates, which are highly unlikely; and av-
erage cases.
The decision of the function is based on the ob-
servation that when a root is regular it either occurs
in a word consecutively or with a single w or i be-
tween any two of its radicals. The scoring func-
tion checks, given a root and a word, whether this
is the case. Furthermore, the suffix of the word, af-
ter matching the root, must be a valid Hebrew suffix
(there is only a small number of such suffixes in He-
brew). If both conditions hold, the scoring function
returns a high value. Then, the function checks if
the root is an unlikely candidate for the given word.
For example, if the root is regular its consonants
must occur in the word in the same order they occur
in the root. If this is not the case, the function re-
turns a low value. We also make use in this function
of our pre-compiled list of roots. A root candidate
which does not occur in the list is assigned the low
score. In all other cases, a middle value is returned.
The actual values that the function returns were
chosen empirically by counting the number of oc-
currences of each class in the training data. For ex-
ample, ?good? candidates make up 74.26% of the
data, hence the value the function returns for ?good?
roots is set to 0.7426. Similarly, the middle value is
set to 0.2416 and the low ? to 0.0155.
As an example, consider hipltm, whose root is
n.p.l (note that the first n is missing in this form).
Here, the correct candidate will be assigned the mid-
dle score while p.l.t and l.t.m will score high.
In addition to the scoring function we imple-
mented a simple edit distance function which re-
turns, for a given root and a given word, the inverse
of the edit distance between the two. For exam-
ple, for hipltm, the (correct) root n.p.l scores 1/4
whereas p.l.t scores 1/3.
We then run SNoW on the test data and rank the
results of the three classifiers globally, where the
order is determined by the product of the three dif-
ferent classifiers. This induces an order on roots,
which are combinations of the decisions of three
independent classifiers. Each candidate root is as-
signed three scores: the product of the confidence
measures of the three classifiers; the result of the
scoring function; and the inverse edit distance be-
tween the candidate and the observed word. We
rank the candidates according to the product of
the three scores (i.e., we give each score an equal
weight in the final ranking).
In order to determine which of the candidates to
produce for each example, we experimented with
two methods. First, the system produced the top-i
candidates for a fixed value of i. The results on the
development set are given in table 5.
i = 1 2 3 4
Precision 82.02 46.17 32.81 25.19
Recall 79.10 87.83 92.93 94.91
f -measure 80.53 60.52 48.50 39.81
Table 5: Performance of the system when producing
top-i candidates.
Obviously, since most words have only one root,
precision drops dramatically when the system pro-
duces more than one candidate. This calls for a bet-
ter threshold, facilitating a non-fixed number of out-
puts for each example. We observed that in the ?dif-
ficult? examples, the top ranking candidates are as-
signed close scores, whereas in the easier cases, the
top candidate is usually scored much higher than the
next one. We therefore decided to produce all those
candidates whose scores are not much lower than
the score of the top ranking candidate. The drop
in the score, ?, was determined empirically on the
development set. The results are listed in table 6,
where ? varies from 0.1 to 1 (? is actually computed
on the log of the actual score, to avoid underflow).
These results show that choosing ? = 0.4 pro-
duces the highest f -measure. With this value for
?, results for the held-out data are presented in ta-
ble 7. The results clearly demonstrate the added
benefit of the linguistic knowledge. In fact, our re-
sults are slightly better than average human perfor-
mance, which we recall as well. Interestingly, even
when testing the system on a set of roots which do
not occur in the training corpus (see section 4), we
obtain an f -score of 65.60%. This result demon-
strates the robustness of our method.
Held-out data Humans
Precision: 80.90 83.52
Recall: 88.16 80.27
f -measure: 84.38 81.86
Table 7: Results: performance of the system on
held-out data.
It must be noted that the scoring function alone
is not a function for extracting roots from Hebrew
words. First, it only scores a given root candidate
against a given word, rather than yield a root given a
word. While we could have used it exhaustively on
all possible roots in this case, in a general setting of
a number of classifiers the number of classes might
be too high for this solution to be practical. Sec-
ond, the function only produces three different val-
ues; when given a number of candidate roots it may
return more than one root with the highest score. In
the extreme case, when called with all 223 potential
roots, it returns on the average more than 11 can-
didates which score highest (and hence are ranked
equally).
Similarly, the additional linguistic knowledge is
not merely eliminating illegitimate roots from the
ranking produced by SNoW. Using the linguistic
constraints encoded in the scoring function only
to eliminate roots, while maintaining the ranking
proposed by SNoW, yields much lower accuracy.
Clearly, our linguistically motivated scoring does
more than elimination, and actually re-ranks the
? = 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Precision 81.81 80.97 79.93 78.86 77.31 75.48 73.71 71.80 69.98 67.90
Recall 81.06 82.74 84.03 85.52 86.49 87.61 88.72 89.70 90.59 91.45
f -measure 81.43 81.85 81.93 82.06 81.64 81.10 80.52 79.76 78.96 77.93
Table 6: Performance of the system, producing candidates scoring no more than ? below the top score.
roots. It is only the combination of the classifiers
with the linguistically motivated scoring function
which boosts the performance on this task.
5.4 Error analysis
Looking at the questionnaires filled in by our sub-
jects (section 3), it is obvious that humans have
problems identifying the correct roots in two gen-
eral cases: when the root paradigm is weak (i.e.,
when the root is irregular) and when the word can be
read in more than way and the subject chooses only
one (presumably, the most prominent one). Our sys-
tem suffers from similar problems: first, its perfor-
mance on the regular paradigms is far superior to its
overall performance; second, it sometimes cannot
distinguish between several roots which are in prin-
ciple possible, but only one of which happens to be
the correct one.
To demonstrate the first point, we evaluated the
performance of the system on a different organiza-
tion of the data. We tested separately words whose
roots are all regular, vs. words all of whose roots are
irregular. We also tested words which have at least
one regular root (mixed). The results are presented
in table 8, and clearly demonstrate the difficulty of
the system on the weak paradigms, compared to al-
most 95% on the easier, regular roots.
Regular Irregular Mixed
Number of words 2598 2019 2781
Precision: 92.79 60.02 92.54
Recall: 96.92 73.45 94.28
f -measure: 94.81 66.06 93.40
Table 8: Error analysis: performance of the system
on different cases.
A more refined analysis reveals differences be-
tween the various weak paradigms. Table 9 lists f -
measure for words whose roots are irregular, classi-
fied by paradigm. As can be seen, the system has
great difficulty in the cases of C2 = C3 and C3 = i.
Finally, we took a closer look at some of the er-
rors, and in particular at cases where the system pro-
duces several roots where fewer (usually only one)
are correct. Such cases include, for example, the
Paradigm f -measure
C1 = i 70.57
C1 = n 71.97
C2 = i/w 76.33
C3 = i 58.00
C2 = C3 47.42
Table 9: Error analysis: the weak paradigms
word hkwtrt (?the title?), whose root is the regu-
lar k.t.r; but the system produces, in addition, also
w.t.r, mistaking the k to be a prefix. This is the kind
of errors which are most difficult to cope with.
However, in many cases the system?s errors are
relatively easy to overcome. Consider, for example,
the word hmtndbim (?the volunteers?) whose root is
the irregular n.d.b. Our system produces as many as
five possible roots for this word: n.d.b, i.t.d, d.w.b,
i.h.d, i.d.d. Clearly some of these could be elimi-
nated. For example, i.t.d should not be produced,
because if this were the root, nothing could explain
the presence of the b in the word; i.h.d should be
excluded because of the location of the h. Similar
phenomena abound in the errors the system makes;
they indicate that a more careful design of the scor-
ing function can yield still better results, and this is
the direction we intend to pursue in the future.
6 Conclusions
We have shown that combining machine learning
with limited linguistic knowledge can produce state-
of-the-art results on a difficult morphological task,
the identification of roots of Hebrew words. Our
best result, over 80% precision, was obtained using
simple classifiers for each of the root?s consonants,
and then combining the outputs of the classifiers us-
ing a linguistically motivated, yet extremely coarse
and simplistic, scoring function. This result is com-
parable to average human performance on this task.
This work can be improved in a variety of ways.
We intend to spend more effort on feature engineer-
ing. As is well-known from other learning tasks,
fine-tuning of the feature set can produce additional
accuracy; we expect this to be the case in this task,
too. In particular, introducing features that capture
contextual information is likely to improve the re-
sults. Similarly, our scoring function is simplistic
and we believe that it can be improved. We also in-
tend to improve the edit-distance function such that
the cost of replacing characters reflect phonological
and orthographic constraints (Kruskal, 1999).
In another track, there are various other ways in
which different inter-related classifiers can be com-
bined. Here we only used a simple multiplica-
tion of the three classifiers? confidence measures,
which is then combined with the linguistically mo-
tivated functions. We intend to investigate more so-
phisticated methods for this combination, including
higher-order machine learning techniques.
Finally, we plan to extend these results to more
complex cases of learning tasks with a large num-
ber of targets, in particular such tasks in which the
targets are structured. We are currently working on
similar experiments for Arabic root extraction. An-
other example is the case of morphological disam-
biguation in languages with non-trivial morphology,
which can be viewed as a POS tagging problem with
a large number of tags on which structure can be im-
posed using the various morphological and morpho-
syntactic features that morphological analyzers pro-
duce. We intend to investigate this problem for He-
brew in the future.
Acknowledgments
This work was supported by The Caesarea Edmond
Benjamin de Rothschild Foundation Institute for In-
terdisciplinary Applications of Computer Science.
Dan Roth is supported by NSF grants CAREER IIS-
9984168, ITR IIS-0085836, and ITR-IIS 00-85980.
We thank Meira Hess and Liron Ashkenazi for an-
notating the corpus and Alon Lavie and Ido Dagan
for useful comments.
References
Ken Beesley. 1998a. Arabic morphological analy-
sis on the internet. In Proceedings of the 6th In-
ternational Conference and Exhibition on Multi-
lingual Computing, Cambridge, April.
Kenneth R. Beesley. 1998b. Arabic morphology
using only finite-state operations. In Michael
Rosner, editor, Proceedings of the Workshop
on Computational Approaches to Semitic lan-
guages, pages 50?57, Montreal, Quebec, August.
COLING-ACL?98.
Yaacov Choueka. 1990. MLIM - a system for full,
exact, on-line grammatical analysis of Modern
Hebrew. In Yehuda Eizenberg, editor, Proceed-
ings of the Annual Conference on Computers in
Education, page 63, Tel Aviv, April. In Hebrew.
Kareem Darwish. 2002. Building a shallow Arabic
morphological analyzer in one day. In Mike Ros-
ner and Shuly Wintner, editors, Computational
Approaches to Semitic Languages, an ACL?02
Workshop, pages 47?54, Philadelphia, PA, July.
Abraham Even-Shoshan. 1993. HaMillon HaX-
adash (The New Dictionary). Kiryat Sefer,
Jerusalem. In Hebrew.
Y. Even-Zohar and Dan Roth. 2001. A sequential
model for multi class classification. In EMNLP-
2001, the SIGDAT Conference on Empirical
Methods in Natural Language Processing, pages
10?19.
Radu Florian. 2002. Named entity recognition as a
house of cards: Classifier stacking. In Proceed-
ings of CoNLL-2002, pages 175?178. Taiwan.
Joseph Kruskal. 1999. An overview of se-
quence comparison. In David Sankoff and Joseph
Kruskal, editors, Time Warps, String Edits and
Macromolecules: The Theory and Practice of Se-
quence Comparison, pages 1?44. CSLI Publica-
tions, Stanford, CA. Reprint, with a foreword by
John Nerbonne.
John J. McCarthy. 1981. A prosodic theory of non-
concatenative morphology. Linguistic Inquiry,
12(3):373?418.
Uzzi Ornan. 2003. The Final Word. University of
Haifa Press, Haifa, Israel. In Hebrew.
Vasin Punyakanok and Dan Roth. 2001. The use
of classifiers in sequential inference. In NIPS-
13; The 2000 Conference on Advances in Neural
Information Processing Systems 13, pages 995?
1001. MIT Press.
Dan Roth. 1998. Learning to resolve natural lan-
guage ambiguities: A unified approach. In Pro-
ceedings of AAAI-98 and IAAI-98, pages 806?
813, Madison, Wisconsin.
H. Schu?tze and Y. Singer. 1994. Part-of-speech tag-
ging using a variable memory markov model. In
Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics.
Joseph Shimron, editor. 2003. Language Process-
ing and Acquisition in Languages of Semitic,
Root-Based, Morphology. Number 28 in Lan-
guage Acquisition and Language Disorders. John
Benjamins.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recog-
nition. In Walter Daelemans and Miles Osborne,
editors, Proceedings of CoNLL-2003, pages 142?
147. Edmonton, Canada.
Yizxaq Zdaqa. 1974. Luxot HaPoal (The Verb Ta-
bles). Kiryath Sepher, Jerusalem. In Hebrew.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 64?71, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Discriminative Training of Clustering Functions:
Theory and Experiments with Entity Identification
Xin Li and Dan Roth
Department of Computer Science
University of Illinois, Urbana, IL 61801
(xli1,danr)@cs.uiuc.edu
Abstract
Clustering is an optimization procedure that
partitions a set of elements to optimize some
criteria, based on a fixed distance metric de-
fined between the elements. Clustering ap-
proaches have been widely applied in natural
language processing and it has been shown re-
peatedly that their success depends on defin-
ing a good distance metric, one that is appro-
priate for the task and the clustering algorithm
used. This paper develops a framework in
which clustering is viewed as a learning task,
and proposes a way to train a distance metric
that is appropriate for the chosen clustering al-
gorithm in the context of the given task. Ex-
periments in the context of the entity identifi-
cation problem exhibit significant performance
improvements over state-of-the-art clustering
approaches developed for this problem.
1 Introduction
Clustering approaches have been widely applied to nat-
ural language processing (NLP) problems. Typically,
natural language elements (words, phrases, sentences,
etc.) are partitioned into non-overlapping classes, based
on some distance (or similarity) metric defined between
them, in order to provide some level of syntactic or se-
mantic abstraction. A key example is that of class-based
language models (Brown et al, 1992; Dagan et al, 1999)
where clustering approaches are used in order to parti-
tion words, determined to be similar, into sets. This
enables estimating more robust statistics since these are
computed over collections of ?similar? words. A large
number of different metrics and algorithms have been ex-
perimented with these problems (Dagan et al, 1999; Lee,
1997; Weeds et al, 2004). Similarity between words was
also used as a metric in a distributional clustering algo-
rithm in (Pantel and Lin, 2002), and it shows that func-
tionally similar words can be grouped together and even
separated to smaller groups based on their senses. At a
higher level, (Mann and Yarowsky, 2003) disambiguated
personal names by clustering people?s home pages using
a TFIDF similarity, and several other researchers have ap-
plied clustering at the same level in the context of the
entity identification problem (Bilenko et al, 2003; Mc-
Callum and Wellner, 2003; Li et al, 2004). Similarly, ap-
proaches to coreference resolution (Cardie and Wagstaff,
1999) use clustering to identify groups of references to
the same entity.
Clustering is an optimization procedure that takes as
input (1) a collection of domain elements along with (2)
a distance metric between them and (3) an algorithm se-
lected to partition the data elements, with the goal of op-
timizing some form of clustering quality with respect to
the given distance metric. For example, the K-Means
clustering approach (Hartigan and Wong, 1979) seeks to
maximize a measure of tightness of the resulting clusters
based on the Euclidean distance. Clustering is typically
called an unsupervised method, since data elements are
used without labels during the clustering process and la-
bels are not used to provide feedback to the optimiza-
tion process. E.g., labels are not taken into account when
measuring the quality of the partition. However, in many
cases, supervision is used at the application level when
determining an appropriate distance metric (e.g., (Lee,
1997; Weeds et al, 2004; Bilenko et al, 2003) and more).
This scenario, however, has several setbacks. First, the
process of clustering, simply a function that partitions a
set of elements into different classes, involves no learn-
ing and thus lacks flexibility. Second, clustering quality is
typically defined with respect to a fixed distance metric,
without utilizing any direct supervision, so the practical
clustering outcome could be disparate from one?s inten-
tion. Third, when clustering with a given algorithm and
a fixed metric, one in fact makes some implicit assump-
tions on the data and the task (e.g., (Kamvar et al, 2002);
more on that below). For example, the optimal conditions
under which for K-means works are that the data is gen-
erated from a uniform mixture of Gaussian models; this
may not hold in reality.
This paper proposes a new clustering framework that
addresses all the problems discussed above. Specifically,
64
we define clustering as a learning task: in the training
stage, a partition function, parameterized by a distance
metric, is trained with respect to a specific clustering al-
gorithm, with supervision. Some of the distinct proper-
ties of this framework are that: (1) The training stage is
formalized as an optimization problem in which a parti-
tion function is learned in a way that minimizes a clus-
tering error. (2) The clustering error is well-defined and
driven by feedback from labeled data. (3) Training a
distance metric with respect to any given clustering al-
gorithm seeks to minimize the clustering error on train-
ing data that, under standard learning theory assumptions,
can be shown to imply small error also in the application
stage. (4) We develop a general learning algorithm that
can be used to learn an expressive distance metric over
the feature space (e.g., it can make use of kernels).
While our approach makes explicit use of labeled data,
we argue that, in fact, many clustering applications in nat-
ural language also exploit this information off-line, when
exploring which metrics are appropriate for the task. Our
framework makes better use of this resource by incorpo-
rating it directly into the metric training process; training
is driven by true clustering error, computed via the spe-
cific algorithm chosen to partition the data.
We study this new framework empirically on the en-
tity identification problem ? identifying whether differ-
ent mentions of real world entities, such as ?JFK? and
?John Kennedy?, within and across text documents, ac-
tually represent the same concept (McCallum and Well-
ner, 2003; Li et al, 2004). Our experimental results ex-
hibit a significant performance improvement over exist-
ing approaches (20% ? 30% F1 error reduction) on all
three types of entities we study, and indicate its promis-
ing prospective in other natural language tasks.
The rest of this paper discusses existing clustering ap-
proaches (Sec. 2) and then introduces our Supervised Dis-
criminative Clustering framework (SDC) (Sec. 3) and a
general learner for training in it (Sec. 4). Sec. 5 describes
the entity identification problem and Sec. 6 compares dif-
ferent clustering approaches on this task.
2 Clustering in Natural Language Tasks
Clustering is the task of partitioning a set of elements
S ? X into a disjoint decomposition 1 p(S) = {S1, S2,
? ? ? , SK} of S. We associate with it a partition function
p = pS : X ? C = {1, 2, . . .K} that maps each x ? S
to a class index pS(x) = k iff x ? Sk. The subscript S
in pS and pS(x) is omitted when clear from the context.
Notice that, unlike a classifier, the image x ? S under a
partition function depends on S.
In practice, a clustering algorithm A (e.g. K-Means),
and a distance metric d (e.g., Euclidean distance), are typ-
1Overlapping partitions will not be discussed here.
ically used to generate a function h to approximate the
true partition function p. Denote h(S) = Ad(S), the par-
tition of S by h. A distance (equivalently, a similarity)
function d that measures the proximity between two ele-
ments is a pairwise function X ? X ? R+, which can
be parameterized to represent a family of functions ?
metric properties are not discussed in this paper. For ex-
ample, given any two element x1 =< x(1)1 , ? ? ? , x(m)1 >
and x2 =< x(1)2 , ? ? ? , x(m)2 > in an m-dimensional space,
a linearly weighted Euclidean distance with parameters
? = {wl}m1 is defined as:
d?(x1, x2) ?
????
m?
l=1
wl ? |x(l)1 ? x(l)2 |2 (1)
When supervision (e.g. class index of elements) is un-
available, the quality of a partition function h operating
on S ? X , is measured with respect to the distance met-
ric defined over X . Suppose h partitions S into disjoint
sets h(S) = {S?k}K1 , one quality function used in the K-
Means algorithm is defined as:
qS(h) ?
K?
k=1
?
x?S?k
d(x, ??k)2, (2)
where ??k is the mean of elements in set S?k. However, this
measure can be computed irrespective of the algorithm.
2.1 What is a Good Metric?
A good metric is one in which close proximity correlates
well with the likelihood of being in the same class. When
applying clustering to some task, people typically decide
on the clustering quality measure qS(h) they want to op-
timize, and then chose a specific clustering algorithm A
and a distance metric d to generate a ?good? partition
function h. However, it is clear that without any super-
vision, the resulting function is not guaranteed to agree
with the target function p (or one?s original intention).
Given this realization, there has been some work on
selecting a good distance metric for a family of related
problems and on learning a metric for specific tasks. For
the former, the focus is on developing and selecting good
distance (similarity) metrics that reflect well pairwise
proximity between domain elements. The ?goodness?
of a metric is empirically measured when combined with
different clustering algorithms on different problems. For
example (Lee, 1997; Weeds et al, 2004) compare similar-
ity metrics such as the Cosine, Manhattan and Euclidean
distances, Kullback-Leibler divergence, Jensen-Shannon
divergence, and Jaccard?s Coefficient, that could be ap-
plied in general clustering tasks, on the task of measur-
ing distributional similarity. (Cohen et al, 2003) com-
pares a number of string and token-based similarity met-
rics on the task of matching entity names and found that,
65
overall, the best-performing method is a hybrid scheme
(SoftTFIDF) combining a TFIDF weighting scheme of
tokens with the Jaro-Winkler string-distance scheme that
is widely used for record linkage in databases.
d(x1,x2) = [(x1(1) -x2(1))2+(x1(2) -x2(2))2]1/2 d(x1,x2) = |(x1(1) +x2(1))-(x1(2) +x2(2))|
(a) Single-Linkage with Euclidean (b) K-Means with Euclidean (c) K-Means with a Linear Metric
Figure 1: Different combinations of clustering algorithms
with distance metrics. The 12 points, positioned in a two-
dimensional space < X(1), X(2) >, are clustered into two
groups containing solid and hollow points respectively.
Moreover, it is not clear whether there exists any
universal metric that is good for many different prob-
lems (or even different data sets for similar problems)
and is appropriate for any clustering algorithm. For the
word-based distributional similarity mentioned above,
this point was discussed in (Geffet and Dagan, 2004)
when it is shown that proximity metrics that are appro-
priate for class-based language models may not be ap-
propriate for other tasks. We illustrate this critical point in
Fig. 1. (a) and (b) show that even for the same data collec-
tion, different clustering algorithms with the same met-
ric could generate different outcomes. (b) and (c) show
that with the same clustering algorithm, different metrics
could also produce different outcomes. Therefore, a good
distance metric should be both domain-specific and asso-
ciated with a specific clustering algorithm.
2.2 Metric Learning via Pairwise Classification
Several works (Cohen et al, 2003; Cohen and Rich-
man, 2002; McCallum and Wellner, 2003; Li et al,
2004) have tried to remedy the aforementioned problems
by attempting to learn a distance function in a domain-
specific way via pairwise classification. In the training
stage, given a set of labeled element pairs, a function
f : X ? X ? {0, 1} is trained to classify any two el-
ements as to whether they belong to the same class (1)
or not (0), independently of other elements. The dis-
tance between the two elements is defined by converting
the prediction confidence of the pairwise classifier, and
clustering is then performed based on this distance func-
tion. Particularly, (Li et al, 2004) applied this approach
to measuring name similarity in the entity identification
problem, where a pairwise classifier (LMR) is trained us-
ing the SNoW learning architecture (Roth, 1998) based
on variations of Perceptron and Winnow, and using a col-
lection of relational features between a pair of names.
The distance between two names is defined as a softmax
over the classifier?s output. As expected, experimental
evidence (Cohen et al, 2003; Cohen and Richman, 2002;
Li et al, 2004) shows that domain-specific distance func-
tions improve over a fixed metric. This can be explained
by the flexibility provided by adapting the metric to the
domain as well as the contribution of supervision that
guides the adaptation of the metric.
A few works (Xing et al, 2002; Bar-Hillel et al, 2003;
Schultz and Joachims, 2004; Mochihashi et al, 2004)
outside the NLP domain have also pursued this general
direction, and some have tried to learn the metric with
limited amount of supervision, no supervision or by in-
corporating other information sources such as constraints
on the class memberships of the data elements. In most of
these cases, the algorithm practically used in clustering,
(e.g. K-Means), is not considered in the learning proce-
dure, or only implicitly exploited by optimizing the same
objective function. (Bach and Jordan, 2003; Bilenko et
al., 2004) indeed suggest to learn a metric directly in a
clustering task but the learning procedure is specific for
one clustering algorithm.
3 Supervised Discriminative Clustering
To solve the limitations of existing approaches, we de-
velop the Supervised Discriminative Clustering Frame-
work (SDC), that can train a distance function with re-
spect to any chosen clustering algorithm in the context of
a given task, guided by supervision.
A labeled data set S
A SupervisedLearner
Training Stage:Goal: h*=argmin errS(h,p)
A distancemetric d a clustering algorithm A+
A unlabeled data set S? A partition h(S?)
Application Stage: h(S? ) 
A partition function h(S) = Ad(S)
Figure 2: Supervised Discriminative Clustering
Fig. 2 presents this framework, in which a cluster-
ing task is explicitly split into training and application
stages, and the chosen clustering algorithm involves in
both stages. In the training stage, supervision is directly
integrated into measuring the clustering error errS(h, p)
of a partition function h by exploiting the feedback given
by the true partition p. The goal of training is to find a par-
tition function h? in a hypothesis space H that minimizes
the error. Consequently, given a new data set S? in the ap-
plication stage, under some standard learning theory as-
sumptions, the hope is that the learned partition function
66
can generalize well and achieve small error as well.
3.1 Supervised and Unsupervised Training
Let p be the target function over X , h be a function in the
hypothesis space H , and h(S) = {S?k}K1 . In principle,
given data set S ? X , if the true partition p(S) = {Sk}K1
of S is available, one can measure the deviation of h from
p over S, using an error function errS(h, p) ? R+. We
distinguish an error function from a quality function (as
in Equ. 2) as follows: an error function measures the dis-
agreement between clustering and the target partition (or
one?s intention) when supervision is given, while a qual-
ity is defined without any supervision.
For clustering, there is generally no direct way to com-
pare the true class index p(x) of each element with that
given by a hypothesis h(x), so an alternative is to mea-
sure the disagreement between p and h over pairs of el-
ements. Given a labeled data set S and p(S), one error
function, namely weighted clustering error, is defined as
a sum of the pairwise errors over any two elements in S,
weighted by the distance between them:
errS(h, p) ? 1|S|2
?
xi,xj?S
[d(xi, xj)?Aij+(D?d(xi, xj))?Bij ]
(3)
where D = maxxi,xj?S d(xi, xj) is the maximum dis-
tance between any two elements in S and I is an indica-
tor function. Aij ? I[(p(xi) = p(xj) & h(xi) 6= h(xj)]
and Bij ? I[(p(xi) 6= p(xj) & h(xi) = h(xj)] represent
two types of pairwise errors respectively.
Just like the quality defined in Equ. 2, this error is a
function of the metric d. Intuitively, the contribution of a
pair of elements that should belong to the same class but
are split by h, grows with their distance, and vice versa.
However, this measure is significantly different from the
quality, in that it does not just measure the tightness of the
partition given by h, but rather the difference between the
tightness of the partitions given by h and by p.
Given a set of observed data, the goal of training is to
learn a good partition function, parameterized by specific
clustering algorithms and distance functions. Depending
on whether training data is labeled or unlabeled, we can
further define supervised and unsupervised training.
Definition 3.1 Supervised Training: Given a labeled
data set S and p(S), a family of partition functions H ,
and the error function errS(h, p)(h ? H), the problem
is to find an optimal function h? s.t.
h? = argminh?H errS(h, p).
Definition 3.2 Unsupervised Training: Given an unla-
beled data set S (p(S) is unknown), a family of partition
functions H , and a quality function qS(h)(h ? H), the
problem is to find an optimal partition function h? s.t.
h? = argmaxh?H qS(h).
With this formalization, SDC along with supervised
training, can be distinguished clearly from (1) unsuper-
vised clustering approaches, (2) clustering over pairwise
classification; and (3) related works that exploit partial
supervision in metric learning as constraints.
3.2 Clustering via Metric Learning
By fixing the clustering algorithm in the training stage,
we can further define supervised metric learning, a spe-
cial case of supervised training.
Definition 3.3 Supervised Metric Learning: Given a la-
beled data set S and p(S), and a family of partition func-
tions H = {h} that are parameterized by a chosen clus-
tering algorithm A and a family of distance metrics d?
(? ? ?), the problem is to seek an optimal metric d??
with respect to A, s.t. for h(S) = A d? (S)
?? = argmin? errS(h, p). (4)
Learning the metric parameters ? requires parameteriz-
ing h as a function of ?, when the algorithm A is chosen
and fixed in h. In the later experiments of Sec. 5, we
try to learn weighted Manhattan distances for the single-
link algorithm and other algorithms, in the task of en-
tity identification. In this case, when pairwise features
are extracted for any elements x1, x2 ? X , (x1, x2) =<
?1, ?2, ? ? ? , ?m >, the linearly weighted Manhattan dis-
tance, parameterized by (? = {wl}m1 ) is defined as:
d(x1, x2) ?
m?
l=1
wl ? ?l(x1, x2) (5)
where wl is the weight over feature ?l(x1, x2). Since
measurement of the error is dependent on the metric,
as shown in Equ. 3, one needs to enforce some con-
straints on the parameters. One constraint is
?m
l=1 |wl| =
1, which prevents the error from being scale-dependent
(e.g., metrics giving smaller distance are always better).
4 A General Learner for SDC
In addition to the theoretical SDC framework, we also de-
velop a practical learning algorithm based on gradient de-
scent (in Fig. 3), that can train a distance function for any
chosen clustering algorithm (such as Single-Linkage and
K-Means), as in the setting of supervised metric learning.
The training procedure incorporates the clustering algo-
rithm (step 2.a) so that the metric is trained with respect
to the specific algorithm that will be applied in evalua-
tion. The convergence of this general training procedure
depends on the convexity of the error as a function of ?.
For example, since the error function we use is linear in ?,
the algorithm is guaranteed to converge to a global mini-
mum. In this case, for rate of convergence, one can appeal
to general results that typically imply, when there exists
a parameter vector with zero error, that convergence rate
67
depends on the ?separation? of the training data, which
roughly means the minimal error archived with this pa-
rameter vector. Results such as (Freund and Schapire,
1998) can be used to extend the rate of convergence re-
sult a bit beyond the separable case, when a small number
of the pairs are not separable.
Algorithm: SDC-Learner
Input: S and p(S): the labeled data set. A: the clustering
algorithm. errS(h, p): the clustering error function. ? > 0
: the learning rate. T (typically T is large) : the number of
iterations allowed.
Output: ?? : the parameters in the distance function d.
1. In the initial (I-) step, we randomly choose ?0 for d.
After this step we have the initial d0 and h0.
2. Then we iterate over t (t = 1, 2, ? ? ?),
(a) Partition S using ht?1(S) ? A dt?1(S);
(b) Compute errS(ht?1, p) and update ? using the
formula: ?t = ?t?1 ? ? ? ?errS(ht?1,p)??t?1 .
(c) Normalization: ?t = 1Z ? ?t, where Z = ||?t||.
3. Stopping Criterion: If t > T , the algorithm exits and
outputs the metric in the iteration with the least error.
Figure 3: A general training algorithm for SDC
For the weighted clustering error in Equ. 3, and linearly
weighted Manhattan distances as in Equ. 5, the update
rule in Step 2(b) becomes
wtl = wt?1l ? ? ? [?t?1l (p, S)? ?t?1l (h, S)]. (6)
where ?l(p, S) ? 1|S|2
?
xi,xj?S ?l(xi, xj) ? I[p(xi) =
p(xj)] and ?l(h, S) ? 1|S|2
?
xi,xj?S ?l(xi, xj) ?
I[h(xi) = h(xj)], and ? > 0 is the learning rate.
5 Entity Identification in Text
We conduct experimental study on the task of entity iden-
tification in text (Bilenko et al, 2003; McCallum and
Wellner, 2003; Li et al, 2004). A given entity ? rep-
resenting a person, a location or an organization ? may
be mentioned in text in multiple, ambiguous ways. Con-
sider, for example, an open domain question answering
system (Voorhees, 2002) that attempts, given a question
like: ?When was President Kennedy born?? to search a
large collection of articles in order to pinpoint the con-
cise answer: ?on May 29, 1917.? The sentence, and even
the document that contains the answer, may not contain
the name ?President Kennedy?; it may refer to this en-
tity as ?Kennedy?, ?JFK? or ?John Fitzgerald Kennedy?.
Other documents may state that ?John F. Kennedy, Jr.
was born on November 25, 1960?, but this fact refers to
our target entity?s son. Other mentions, such as ?Senator
Kennedy? or ?Mrs. Kennedy? are even ?closer? to the
writing of the target entity, but clearly refer to different
entities. Understanding natural language requires identi-
fying whether different mentions of a name, within and
across documents, represent the same entity.
We study this problem for three entity types ? People,
Location and Organization. Although deciding the coref-
erence of names within the same document might be rela-
tively easy, since within a single document identical men-
tions typically refer to the same entity, identifying coref-
erence across-document is much harder. With no stan-
dard corpora for studying the problem in a general setting
? both within and across documents, we created our own
corpus. This is done by collecting about 8, 600 names
from 300 randomly sampled 1998-2000 New York Times
articles in the TREC corpus (Voorhees, 2002). These
names are first annotated by a named entity tagger, then
manually verified and given as input to an entity identi-
fier.
Since the number of classes (entities) for names is very
large, standard multi-class classification is not feasible.
Instead, we compare SDC with several pairwise classifi-
cation and clustering approaches. Some of them (for ex-
ample, those based on SoftTFIDF similarity) do not make
use of any domain knowledge, while others do exploit su-
pervision, such as LMR and SDC. Other works (Bilenko
et al, 2003) also exploited supervision in this problem by
discriminative training of a pairwise classifier but were
shown to be inferior.
1. SoftTFIDF Classifier ? a pairwise classifier deciding
whether any two names refer to the same entity, imple-
mented by thresholding a state-of-art SoftTFIDF similar-
ity metric for string comparison (Cohen et al, 2003). Dif-
ferent thresholds have been experimented but only the best
results are reported.
2. LMR Classifier (P|W) ? a SNoW-based pairwise classi-
fier (Li et al, 2004) (described in Sec. 2.2) that learns a
linear function for each class over a collection of relational
features between two names: including string and token-
level features and structural features (listed in Table 1).
For pairwise classifiers like LMR and SoftTFIDF, predic-
tion is made over pairs of names so transitivity of predic-
tions is not guaranteed as in clustering.
3. Clustering over SoftTFIDF ? a clustering approach based
on the SoftTFIDF similarity metric.
4. Clustering over LMR (P|W) ? a clustering approach (Li et
al., 2004) by converting the LMR classifier into a similar-
ity metric (see Sec. 2.2).
5. SDC ? our new supervised clustering approach. The dis-
tance metric is represented as a linear function over a set
of pairwise features as defined in Equ. 5.
The above approaches (2), (4) and (5) learn a classifier
or a distance metric using the same feature set as in Ta-
ble 1. Different clustering algorithms 2, such as Single-
Linkage, Complete-Linkage, Graph clustering (George,
2The clustering package Cluster by Michael Eisen at Stan-
ford University is adopted for K-medoids and CLUTO by
(George, 2003) is used for other algorithms. Details of these
algorithms can be found there.
68
Honorific Equal active if both tokens are honorifics and identical.
Honorific Equivalence active if both tokens are honorifics, not identical, but equivalent.
Honorific Mismatch active for different honorifics.
Equality active if both tokens are identical.
Case-Insensitive Equal active if the tokens are case-insensitive equal.
Nickname active if tokens have a ?nickname? relation.
Prefix Equality active if the prefixes of both tokens are equal.
Substring active if one of the tokens is a substring of the other.
Abbreviation active if one of the tokens is an abbreviation of the other.
Prefix Edit Distance active if the prefixes of both tokens have an edit-distance of 1.
Edit Distance active if the tokens have an edit-distance of 1.
Initial active if one of the tokens is an initial of another.
Symbol Map active if one token is a symbolic representative of the other.
Structural recording the location of the tokens that generate other features in two names.
Table 1: Features employed by LMR and SDC.
2003) ? seeking a minimum cut of a nearest neighbor
graph, Repeated Bisections and K-medoids (Chu et al,
2001) (a variation of K-means) are experimented in (5).
The number of entities in a data set is always given.
6 Experimental Study
Our experimental study focuses on (1) evaluating the
supervised discriminative clustering approach on entity
identification; (2) comparing it with existing pairwise
classification and clustering approaches widely used in
similar tasks; and (3) further analyzing the characteris-
tics of this new framework.
We use the TREC corpus to evaluate different ap-
proaches in identifying three types of entities: People,
Locations and Organization. For each type, we generate
three pairs of training and test sets, each containing about
300 names. We note that the three entity types yield very
different data sets, exhibited by some statistical proper-
ties3. Results on each entity type will be averaged over
the three sets and ten runs of two-fold cross-validation for
each of them. For SDC, given a training set with anno-
tated name pairs, a distance function is first trained using
the algorithm in Fig. 3 (in 20 iterations) with respect to
a clustering algorithm and then be used to partition the
corresponding test set with the same algorithm.
For a comparative evaluation, the outcomes of each ap-
proach on a test set of names are converted to a classifi-
cation over all possible pairs of names (including non-
matching pairs). Only examples in the set Mp, those
that are predicated to belong to the same entity (posi-
tive predictions) are used in the evaluation, and are com-
pared with the set Ma of examples annotated as positive.
The performance of an approach is then evaluated by F1
value, defined as: F1 = 2|Mp
?
Ma|
|Mp|+|Ma| .
3The average SoftTFIDF similarity between names of the
same entity is 0.81, 0.89 and 0.95 for people, locations and or-
ganizations respectively.
6.1 Comparison of Different Approaches
Fig. 4 presents the performance of different approaches
(described in Sec. 5) on identifying the three entity types.
We experimented with different clustering algorithms but
only the results by Single-Linkage are reported for Clus-
ter over LMR (P|W) and SDC, since they are the best.
SDC works well for all three entity types in spite of
their different characteristics. The best F1 values of SDC
are 92.7%, 92.4% and 95.7% for people, locations and
organizations respectively, about 20% ? 30% error re-
duction compared with the best performance of the other
approaches. This is an indication that this new approach
which integrates metric learning and supervision in a uni-
fied framework, has significant advantages 4.
6.2 Further Analysis of SDC
In the next experiments, we will further analyze the char-
acteristics of SDC by evaluating it in different settings.
Different Training Sizes Fig. 5 reports the relationship
between the performance of SDC and different training
sizes. The learning curves for other learning-based ap-
proaches are also shown. We find that SDC exhibits good
learning ability with limited supervision. When training
examples are very limited, for example, only 10% of all
300 names, pairwise classifiers based on Perceptron and
Winnow exhibit advantages over SDC. However, when
supervision become reasonable (30%+ examples), SDC
starts to outperform all other approaches.
Different Clustering Algorithms Fig. 6 shows the
performance of applying different clustering algorithms
(see Sec. 5) in the SDC approach. Single-Linkage and
Complete-Linkage outperform all other algorithms. One
possible reason is that this task has a great number of
4We note that in this experiment, the relative comparison
between the pairwise classifiers and the clustering approaches
over them is not consistent for all entity types. This can be
partially explained by the theoretical analysis in (Li et al, 2004)
and the difference between entity types.
69
80
82
84
86
88
90
92
94
96
(a) People
F 1 (%
)
80
82
84
86
88
90
92
94
96
(b) Locations
F 1 (%
)
80
82
84
86
88
90
92
94
96
(c) Organizations
F 1 (%
)
SoftTFIDFLMR (P)LMR (W)Cluster over SoftTFIDFCluster over LMR (P)Cluster over LMR (W)SDC
Figure 4: Performance of different approaches. The results are reported for SDC with a learning rate ? = 100.0.
The Single-Linkage algorithm is applied whenever clustering is performed. Results are reported in F1 and averaged
over the three data sets for each entity type and 10 runs of two-fold cross-validation. Each training set typically
contains 300 annotated names.
10 20 30 40 50 60 70 80 90 10070
75
80
85
90
95
100
(a) Peopl
F 1 (%
)
LMR (P)LMR (W)Cluster over LMR (P)Cluster over LMR (W)SDC
10 20 30 40 50 60 70 80 90 10070
75
80
85
90
95
100
(b) Locations
F 1 (%
)
10 20 30 40 50 60 70 80 90 10070
75
80
85
90
95
100
(c) Organizations
F 1 (%
)
Figure 5: Performance for different training sizes. Five learning-based approaches are compared. Single-Linkage is
applied whenever clustering is performed. X-axis denotes different percentages of 300 names used in training. Results
are reported in F1 and averaged over the three data sets for each entity type.
People Locations Organizations40
50
60
70
80
90
Different Entity Types
F 1 (%)
GraphK?MedoidsRBComplete?LinkageSingle?Linkage
Figure 6: Different clustering algorithms. Five cluster-
ing algorithms are compared in SDC (? = 100.0). Re-
sults are averaged over the three data sets for each entity
type and 10 runs of two-fold cross-validations.
classes (100 ? 200 entities) for 300 names in each sin-
gle data set. The results indicate that the metric learn-
ing process relies on properties of the data set, as well as
the clustering algorithm. Even if a good distance metric
could be learned in SDC, choosing an appropriate algo-
rithm for the specific task is still important.
Different Learning Rates We also experimented with
different learning rates in the SDC approach as shown in
Fig. 7. It seems that SDC is not very sensitive to different
learning rates as long as it is in a reasonable range.
People Locations Organizations
86
88
90
92
94
96
Different Entity Types
F 1 (%)
?=1.0?=10.0?=100.0?=1000.0
Figure 7: Performance for different learning rates.
SDC with different learning rates (? = 1.0, 10.0, 100.0,
1000.0) compared in this setting. Single-Linkage cluster-
ing algorithm is applied.
6.3 Discussion
The reason that SDC can outperform existing clustering
approaches can be explained by the advantages of SDC ?
training the distance function with respect to the chosen
clustering algorithm, guided by supervision, but they do
not explain why it can also outperform the pairwise clas-
sifiers. One intuitive explanation is that supervision in the
entity identification task or similar tasks is typically given
on whether two names correspond to the same entity ?
entity-level annotation. Therefore it does not necessarily
mean whether they are similar in appearance. For exam-
70
ple, ?Brian? and ?Wilson? could both refer to a person
?Brian Wilson? in different contexts, and thus this name
pair is a positive example in training a pairwise classi-
fier. However, with features that only capture the appear-
ance similarity between names, such apparently different
names become training noise. This is what exactly hap-
pened when we train the LMR classifier with such name
pairs. SDC, however, can employ this entity-level anno-
tation and avoid the problem through transitivity in clus-
tering. In the above example, if there is ?Brian Wilson?
in the data set, then ?Brian? and ?Wilson? can be both
clustered into the same group with ?Brian Wilson?. Such
cases do not frequently occur for locations and organiza-
tion but still exist .
7 Conclusion
In this paper, we explicitly formalize clustering as a learn-
ing task, and propose a unified framework for training
a metric for any chosen clustering algorithm, guided by
domain-specific supervision. Our experiments exhibit the
advantage of this approach over existing approaches on
Entity Identification. Further research in this direction
will focus on (1) applying it to more NLP tasks, e.g.
coreference resolution; (2) analyzing the related theoret-
ical issues, e.g. the convergence of the algorithm; and
(3) comparing it experimentally with related approaches,
such as (Xing et al, 2002) and (McCallum and Wellner,
2003).
Acknowledgement This research is supported by
NSF grants IIS-9801638 and ITR IIS-0085836, an ONR
MURI Award and an equipment donation from AMD.
References
F. R. Bach and M. I. Jordan. 2003. Learning spectral clustering.
In NIPS-03.
A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. 2003.
Learning distance functions using equivalence relations. In
ICML-03, pages 11?18.
M. Bilenko, R. Mooney, W. Cohen, P. Ravikumar, and S. Fien-
berg. 2003. Adaptive name matching in information integra-
tion. IEEE Intelligent Systems, pages 16?23.
M Bilenko, S. Basu, and R. J. Mooney. 2004. Integrating con-
straints and metric learning in semi-supervised clustering. In
ICML-04, pages 81?88.
P. Brown, P. deSouza R. Mercer, V. Pietra, and J. Lai. 1992.
Class-based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
C. Cardie and K. Wagstaff. 1999. Noun phrase coreference as
clustering. In EMNLP-99, pages 82?89.
S. C. Chu, J. F. Roddick, and J. S. Pan. 2001. A comparative
study and extensions to k-medoids algorithms. In ICOTA-01.
W. Cohen and J. Richman. 2002. Learning to match and clus-
ter large high-dimensional data sets for data integration. In
KDD-02, pages 475?480.
W. Cohen, P. Ravikumar, and S. Fienberg. 2003. A comparison
of string metrics for name-matching tasks. In IIWeb Work-
shop 2003, pages 73?78.
I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-based mod-
els of word cooccurrence probabilities. Machine Learning,
34(1-3):43?69.
Y. Freund and R. Schapire. 1998. Large margin classification
using the Perceptron algorithm. In COLT-98.
M. Geffet and I. Dagan. 2004. Automatic feature vector quality
and distributional similarity. In COLING-04.
K. George. 2003. Cluto: A clustering toolkit. Technical report,
Dept of Computer Science, University of Minnesota.
J. Hartigan and M. Wong. 1979. A k-means clustering algo-
rithm. Applied Statistics, 28(1):100?108.
S. Kamvar, D. Klein, and C. Manning. 2002. Interpreting and
extending classical agglomerative clustering algorithms us-
ing a model-based approach. In ICML-02, pages 283?290.
L. Lee. 1997. Similarity-Based Approaches to Natural Lan-
guage Processing. Ph.D. thesis, Harvard University, Cam-
bridge, MA.
X. Li, P. Morie, and D. Roth. 2004. Identification and trac-
ing of ambiguous names: Discriminative and generative ap-
proaches. In AAAI-04, pages 419?424.
G. Mann and D. Yarowsky. 2003. Unsupervised personal name
disambiguation. In CoNLL-03, pages 33?40.
A. McCallum and B. Wellner. 2003. Toward conditional mod-
els of identity uncertainty with application to proper noun
coreference. In IJCAI Workshop on Information Integration
on the Web.
D. Mochihashi, G. Kikui, and K. Kita. 2004. Learning non-
structural distance metric by minimum cluster distortions. In
COLING-04.
P. Pantel and D. Lin. 2002. Discovering word senses from text.
In KDD-02, pages 613?619.
D. Roth. 1998. Learning to resolve natural language ambigui-
ties: A unified approach. In AAAI-98, pages 806?813.
M. Schultz and T. Joachims. 2004. Learning a distance metric
from relative comparisons. In NIPS-04.
E. Voorhees. 2002. Overview of the TREC-2002 question an-
swering track. In TREC-02, pages 115?123.
J. Weeds, D. Weir, and D. McCarthy. 2004. Characterising
measures of lexical distributional similarity. In COLING-04.
E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. 2002.
Distance metric learning, with application to clustering with
side-information. In NIPS-02.
71
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 181?184, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Generalized Inference with Multiple Semantic Role Labeling Systems
Peter Koomen Vasin Punyakanok Dan Roth Wen-tau Yih
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{pkoomen2,punyakan,danr,yih}@uiuc.edu
Abstract
We present an approach to semantic role
labeling (SRL) that takes the output of
multiple argument classifiers and com-
bines them into a coherent predicate-
argument output by solving an optimiza-
tion problem. The optimization stage,
which is solved via integer linear pro-
gramming, takes into account both the rec-
ommendation of the classifiers and a set
of problem specific constraints, and is thus
used both to clean the classification results
and to ensure structural integrity of the fi-
nal role labeling. We illustrate a signifi-
cant improvement in overall SRL perfor-
mance through this inference.
1 SRL System Architecture
Our SRL system consists of four stages: prun-
ing, argument identification, argument classifica-
tion, and inference. In particular, the goal of pruning
and argument identification is to identify argument
candidates for a given verb predicate. The system
only classifies the argument candidates into their
types during the argument classification stage. Lin-
guistic and structural constraints are incorporated
in the inference stage to resolve inconsistent global
predictions. The inference stage can take as its input
the output of the argument classification of a single
system or of multiple systems. We explain the infer-
ence for multiple systems in Sec. 2.
1.1 Pruning
Only the constituents in the parse tree are considered
as argument candidates. In addition, our system ex-
ploits the heuristic introduced by (Xue and Palmer,
2004) to filter out very unlikely constituents. The
heuristic is a recursive process starting from the verb
whose arguments are to be identified. It first returns
the siblings of the verb; then it moves to the parent of
the verb, and collects the siblings again. The process
goes on until it reaches the root. In addition, if a con-
stituent is a PP (propositional phrase), its children
are also collected. Candidates consisting of only a
single punctuation mark are not considered.
This heuristic works well with the correct parse
trees. However, one of the errors by automatic
parsers is due to incorrect PP attachment leading to
missing arguments. To attempt to fix this, we con-
sider as arguments the combination of any consec-
utive NP and PP, and the split of NP and PP inside
the NP that was chosen by the previous heuristics.
1.2 Argument Identification
The argument identification stage utilizes binary
classification to identify whether a candidate is an
argument or not. We train and apply the binary clas-
sifiers on the constituents supplied by the pruning
stage. Most of the features used in our system are
standard features, which include
? Predicate and POS tag of predicate indicate the lemma
of the predicate and its POS tag.
? Voice indicates tbe voice of the predicate.
? Phrase type of the constituent.
? Head word and POS tag of the head word include head
word and its POS tag of the constituent. We use rules
introduced by (Collins, 1999) to extract this feature.
? First and last words and POS tags of the constituent.
? Two POS tags before and after the constituent.
? Position feature describes if the constituent is before or
after the predicate relative to the position in the sentence.
181
? Path records the traversal path in the parse tree from the
predicate to the constituent.
? Subcategorization feature describes the phrase structure
around the predicate?s parent. It records the immediate
structure in the parse tree that expands to its parent.
? Verb class feature is the class of the active predicate de-
scribed in PropBank Frames.
? Lengths of the target constituent, in the numbers of words
and chunks separately.
? Chunk tells if the target argument is, embeds, overlaps,
or is embedded in a chunk with its type.
? Chunk pattern length feature counts the number of
chunks from the predicate to the argument.
? Clause relative position is the position of the target word
relative to the predicate in the pseudo-parse tree con-
structed only from clause constituent. There are four
configurations?target constituent and predicate share the
same parent, target constituent parent is an ancestor of
predicate, predicate parent is an ancestor of target word,
or otherwise.
? Clause coverage describes how much of the local clause
(from the predicate) is covered by the argument. It is
round to the multiples of 1/4.
1.3 Argument Classification
This stage assigns the final argument labels to the ar-
gument candidates supplied from the previous stage.
A multi-class classifier is trained to classify the
types of the arguments supplied by the argument
identification stage. To reduce the excessive candi-
dates mistakenly output by the previous stage, the
classifier can also classify the argument as NULL
(?not an argument?) to discard the argument.
The features used here are the same as those used
in the argument identification stage with the follow-
ing additional features.
? Syntactic frame describes the sequential pattern of the
noun phrases and the predicate in the sentence. This is
the feature introduced by (Xue and Palmer, 2004).
? Propositional phrase head is the head of the first phrase
after the preposition inside PP.
? NEG and MOD feature indicate if the argument is a
baseline for AM-NEG or AM-MOD. The rules of the
NEG and MOD features are used in a baseline SRL sys-
tem developed by Erik Tjong Kim Sang (Carreras and
Ma`rquez, 2004).
? NE indicates if the target argument is, embeds, overlaps,
or is embedded in a named-entity along with its type.
1.4 Inference
The purpose of this stage is to incorporate some
prior linguistic and structural knowledge, such as
?arguments do not overlap? or ?each verb takes at
most one argument of each type.? This knowledge is
used to resolve any inconsistencies of argument clas-
sification in order to generate final legitimate pre-
dictions. We use the inference process introduced
by (Punyakanok et al, 2004). The process is formu-
lated as an integer linear programming (ILP) prob-
lem that takes as inputs the confidences over each
type of the arguments supplied by the argument clas-
sifier. The output is the optimal solution that maxi-
mizes the linear sum of the confidence scores (e.g.,
the conditional probabilities estimated by the argu-
ment classifier), subject to the constraints that en-
code the domain knowledge.
Formally speaking, the argument classifier at-
tempts to assign labels to a set of arguments, S1:M ,
indexed from 1 to M . Each argument Si can take
any label from a set of argument labels, P , and the
indexed set of arguments can take a set of labels,
c1:M ? PM . If we assume that the argument classi-
fier returns an estimated conditional probability dis-
tribution, Prob(Si = ci), then, given a sentence, the
inference procedure seeks an global assignment that
maximizes the following objective function,
c?1:M = argmax
c1:M?PM
M
?
i=1
Prob(Si = ci),
subject to linguistic and structural constraints. In
other words, this objective function reflects the ex-
pected number of correct argument predictions, sub-
ject to the constraints. The constraints are encoded
as the followings.
? No overlapping or embedding arguments.
? No duplicate argument classes for A0-A5.
? Exactly one V argument per predicate considered.
? If there is C-V, then there has to be a V-A1-CV pattern.
? If there is an R-arg argument, then there has to be an arg
argument.
? If there is a C-arg argument, there must be an arg argu-
ment; moreover, the C-arg argument must occur after arg.
? Given the predicate, some argument types are illegal (e.g.
predicate ?stalk? can take only A0 or A1). The illegal
types may consist of A0-A5 and their corresponding C-
arg and R-arg arguments. For each predicate, we look
for the minimum value of i such that the class Ai is men-
tioned in its frame file as well as its maximum value j.
All argument types Ak such that k < i or k > j are
considered illegal.
182
2 Inference with Multiple SRL Systems
The inference process allows a natural way to com-
bine the outputs from multiple argument classi-
fiers. Specifically, given k argument classifiers
which perform classification on k argument sets,
{S1, . . . , Sk}. The inference process aims to opti-
mize the objective function:
c?1:N = argmax
c1:N?PN
N
?
i=1
Prob(Si = ci),
where S1:N = ?ki=1 Si, and
Prob(Si = ci) = 1k
k
?
j=1
Probj(Si = ci),
where Probj is the probability output by system j.
Note that all systems may not output with the
same set of argument candidates due to the pruning
and argument identification. For the systems that do
not output for any candidate, we assign the proba-
bility with a prior to this phantom candidate. In par-
ticular, the probability of the NULL class is set to be
0.6 based on empirical tests, and the probabilities of
the other classes are set proportionally to their oc-
currence frequencies in the training data.
For example, Figure 1 shows the two candidate
sets for a fragment of a sentence, ?..., traders say,
unable to cool the selling panic in both stocks and
futures.? In this example, system A has two argu-
ment candidates, a1 = ?traders? and a4 = ?the sell-
ing panic in both stocks and futures?; system B has
three argument candidates, b1 = ?traders?, b2 = ?the
selling panic?, and b3 = ?in both stocks and fu-
tures?. The phantom candidates are created for a2,
a3, and b4 of which probability is set to the prior.
Specifically for this implementation, we first train
two SRL systems that use Collins? parser and Char-
niak?s parser respectively. In fact, these two parsers
have noticeably different output. In evaluation, we
run the system that was trained with Charniak?s
parser 5 times with the top-5 parse trees output by
Charniak?s parser1. Together we have six different
outputs per predicate. Per each parse tree output, we
ran the first three stages, namely pruning, argument
1The top parse tree were from the official output by CoNLL.
The 2nd-5th parse trees were output by Charniak?s parser.
cool
1
b1
b4
a4
a2
2b 3b
a3
..., traders say, unable to the selling panic in both stocks and futures.
a
Figure 1: Two SRL systems? output (a1, a4, b1, b2,
and b3), and phantom candidates (a2, a3, and b4).
identification, and argument classification. Then a
joint inference stage is used to resolve the incon-
sistency of the output of argument classification in
these systems.
3 Learning and Evaluation
The learning algorithm used is a variation of the
Winnow update rule incorporated in SNoW (Roth,
1998; Roth and Yih, 2002), a multi-class classi-
fier that is tailored for large scale learning tasks.
SNoW learns a sparse network of linear functions,
in which the targets (argument border predictions
or argument type predictions, in this case) are rep-
resented as linear functions over a common feature
space. It improves the basic Winnow multiplicative
update rule with a regularization term, which has the
effect of trying to separate the data with a large mar-
gin separator (Grove and Roth, 2001; Hang et al,
2002) and voted (averaged) weight vector (Freund
and Schapire, 1999).
Softmax function (Bishop, 1995) is used to con-
vert raw activation to conditional probabilities. If
there are n classes and the raw activation of class i
is acti, the posterior estimation for class i is
Prob(i) = e
acti
?
1?j?n eactj
.
In summary, training used both full and partial
syntactic information as described in Section 1. In
training, SNoW?s default parameters were used with
the exception of the separator thickness 1.5, the use
of average weight vector, and 5 training cycles. The
parameters are optimized on the development set.
Training for each system took about 6 hours. The
evaluation on both test sets which included running
183
Precision Recall F?=1
Development 80.05% 74.83% 77.35
Test WSJ 82.28% 76.78% 79.44
Test Brown 73.38% 62.93% 67.75
Test WSJ+Brown 81.18% 74.92% 77.92
Test WSJ Precision Recall F?=1
Overall 82.28% 76.78% 79.44
A0 88.22% 87.88% 88.05
A1 82.25% 77.69% 79.91
A2 78.27% 60.36% 68.16
A3 82.73% 52.60% 64.31
A4 83.91% 71.57% 77.25
A5 0.00% 0.00% 0.00
AM-ADV 63.82% 56.13% 59.73
AM-CAU 64.15% 46.58% 53.97
AM-DIR 57.89% 38.82% 46.48
AM-DIS 75.44% 80.62% 77.95
AM-EXT 68.18% 46.88% 55.56
AM-LOC 66.67% 55.10% 60.33
AM-MNR 66.79% 53.20% 59.22
AM-MOD 96.11% 98.73% 97.40
AM-NEG 97.40% 97.83% 97.61
AM-PNC 60.00% 36.52% 45.41
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 78.16% 76.72% 77.44
R-A0 89.72% 85.71% 87.67
R-A1 70.00% 76.28% 73.01
R-A2 85.71% 37.50% 52.17
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 85.71% 57.14% 68.57
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 72.34% 65.38% 68.69
V 98.92% 97.10% 98.00
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
with all six different parse trees (assumed already
given) and the joint inference took about 4.5 hours.
Precision Recall F?=1
Charniak-1 75.40% 74.13% 74.76
Charniak-2 74.21% 73.06% 73.63
Charniak-3 73.52% 72.31% 72.91
Charniak-4 74.29% 72.92% 73.60
Charniak-5 72.57% 71.40% 71.98
Collins 73.89% 70.11% 71.95
Joint inference 80.05% 74.83% 77.35
Table 2: The results of individual systems and the
result with joint inference on the development set.
Overall results on the development and test sets
are shown in Table 1. Table 2 shows the results of
individual systems and the improvement gained by
the joint inference on the development set.
4 Conclusions
We present an implementation of SRL system which
composed of four stages?1) pruning, 2) argument
identification, 3) argument classification, and 4) in-
ference. The inference provides a natural way to
take the output of multiple argument classifiers and
combines them into a coherent predicate-argument
output. Significant improvement in overall SRL per-
formance through this inference is illustrated.
Acknowledgments
We are grateful to Dash Optimization for the free
academic use of Xpress-MP. This research is sup-
ported by ARDA?s AQUAINT Program, DOI?s Re-
flex program, and an ONR MURI Award.
References
C. Bishop, 1995. Neural Networks for Pattern Recognition,
chapter 6.4: Modelling conditional distributions, page 215.
Oxford University Press.
X. Carreras and L. Ma`rquez. 2004. Introduction to the conll-
2004 shared tasks: Semantic role labeling. In Proc. of
CoNLL-2004.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, Computer Science Depart-
ment, University of Pennsylvenia, Philadelphia.
Y. Freund and R. Schapire. 1999. Large margin classifica-
tion using the perceptron algorithm. Machine Learning,
37(3):277?296.
A. Grove and D. Roth. 2001. Linear concepts and hidden vari-
ables. Machine Learning, 42(1/2):123?141.
T. Hang, F. Damerau, and D. Johnson. 2002. Text chunking
based on a generalization of winnow. Journal of Machine
Learning Research, 2:615?637.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Seman-
tic role labeling via integer linear programming inference. In
Proc. of COLING-2004.
D. Roth and W. Yih. 2002. Probabilistic reasoning for entity &
relation recognition. In Proc. of COLING-2002, pages 835?
841.
D. Roth. 1998. Learning to resolve natural language ambigui-
ties: A unified approach. In Proc. of AAAI, pages 806?813.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proc. of the EMNLP-2004, pages 88?94,
Barcelona, Spain.
184
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 186?190, New York City, June 2006. c?2006 Association for Computational Linguistics
A Pipeline Model for Bottom-Up Dependency Parsing
Ming-Wei Chang Quang Do Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{mchang21, quangdo2, danr}@uiuc.edu
Abstract
We present a new machine learning frame-
work for multi-lingual dependency pars-
ing. The framework uses a linear, pipeline
based, bottom-up parsing algorithm, with
a look ahead local search that serves to
make the local predictions more robust.
As shown, the performance of the first
generation of this algorithm is promising.
1 System Description
1.1 Parsing as a Pipeline
Pipeline computation is a common computational
strategy in natural language processing, where a task
is decomposed into several stages that are solved se-
quentially. For example, a semantic role labeling
program may start by using a part-of-speech tagger,
than apply a shallow parser to chunk the sentence
into phrases, and continue by identifying predicates
and arguments and then classifying them.
(Yamada and Matsumoto, 2003) proposed a
bottom-up dependency parsing algorithm, where the
local actions, chosen from among Shift, Left, Right,
are used to generate a dependency tree using a
shift-reduce parsing approach. Moreover, they used
SVMs to learn the parsing decisions between pairs
of consecutive words in the sentences 1. This is
a true pipeline approach in that the classifiers are
trained on individual decisions rather than on the
overall quality of the parser, and chained to yield the
1A pair of words may become consecutive after the words
between them become the children of these two words
global structure. It suffers from the limitations of
pipeline processing, such as accumulation of errors,
but nevertheless, yields very competitive parsing re-
sults.
We devise two natural principles for enhancing
pipeline models. First, inference procedures should
be incorporated to make robust prediction for each
stage. Second, the number of predictions should
be minimized to prevent error accumulation. Ac-
cording to these two principles, we propose an im-
proved pipeline framework for multi-lingual depen-
dency parsing that aims at addressing the limitations
of the pipeline processing. Specifically, (1) we use
local search, a look ahead policy, to improve the ac-
curacy of the predicted actions, and (2) we argue that
the parsing algorithm we used minimizes the num-
ber of actions (Chang et al, 2006).
We use the set of actions: Shift, Left, Right, Wait-
Left, WaitRight for the parsing algorithm. The pure
Wait action was suggested in (Yamada and Mat-
sumoto, 2003). However, here we come up with
these five actions by separating actions Left into
(real) Left and WaitLeft, and Right into (real) Right
and WaitRight. Predicting these turns out to be eas-
ier due to finer granularity. We then use local search
over consecutive actions and better exploit the de-
pendencies among them.
The parsing algorithm is a modified shift-reduce
parser (Aho et al, 1986) that makes use of the ac-
tions described above and applies them in a left
to right manner on consecutive word pairs (a, b)
(a < b) in the word list T . T is initialized as the full
sentence. Latter, the actions will change the contents
of T . The actions are used as follows:
186
Shift: there is no relation between a and b.
Right: b is the parent of a,
Left: a is the parent of b
WaitLeft: a is the parent of b, but it?s possible that
b is a parent of other nodes. Action is deferred.
The actions control the procedure of building
trees. When Left or Right is performed, the algo-
rithm has found a parent and a child. Then, the func-
tion deleteWord will be called to eliminate the child
word, and the procedure will be repeated until the
tree is built. In projective languages, we discovered
that action WaitRight is not needed. Therefore, for
projective languages, we just need 4 actions.
In order to complete the description of the algo-
rithm we need to describe which pair of consecu-
tive words to consider once an action is taken. We
describe it via the notion of the focus point, which
represents the index of the current word in T . In
fact, determining the focus point does not affect the
correctness of the algorithm. It is easy to show that
any pair of consecutive words in the sentence can
be considered next. If the correct action is chosen
for the corresponding pair, this will eventually yield
the correct tree (but may necessitate multiple cycles
through the sentence).
In practice, however, the actions chosen will be
noisy, and a wasteful focus point policy will result
in a large number of actions, and thus in error accu-
mulation. To minimize the number of actions taken,
we want to find a good focus point placement policy.
There are many natural placement policies that we
can consider (Chang et al, 2006). In this paper, ac-
cording to the policy we used, after S and WL, the
focus point moves one word to the right. After L or
R, we adopt the policy Step Back: the focus moves
back one word to the left. Although the focus place-
ment policy here is similar to (Yamada and Mat-
sumoto, 2003), they did not explain why they made
this choice. In (Chang et al, 2006), we show that
the policy movement used here minimized the num-
ber of actions during the parsing procedure. We can
also show that the algorithm can parse a sentence
with projective relationships in only one round.
Once the parsing algorithm, along with the focus
point policy, is determined, we can train the action
classifiers. Given an annotated corpus, the parsing
algorithm is used to determine the action taken for
each consecutive pair; this is used to train a classifier
Algorithm 1 Pseudo Code of the dependency pars-
ing algorithm. getFeatures extracts the features
describing the currently considered pair of words;
getAction determines the appropriate action for the
pair; assignParent assigns the parent for the child
word based on the action; and deleteWord deletes the
word which become child once the action is taken.
Let t represents for a word and its part of speech
For sentence T = {t1, t2, . . . , tn}
focus= 1
while focus< |T | do
~v = getFeatures(tfocus, tfocus+1)
? = getAction(tfocus, tfocus+1, ~v)
if ? = L or ? = R then
assignParent(tfocus, tfocus+1, ?)
deleteWord(T, focus, ?)
// performing Step Back here
focus = focus ? 1
else
focus = focus + 1
end if
end while
to predict one of the four actions. The details of the
classifier and the features are given in Section 3.
When we apply the trained model on new data,
the sentence is processed from left to right to pro-
duce the predicted dependency tree. The evaluation
process is somewhat more involved, since the action
classifier is not used as it is, but rather via a local
search inference step. This is described in Section 2.
Algorithm 1 depicts the pseudo code of our parsing
algorithm.
Our algorithm is designed for projective lan-
guages. For non-projective relationships in some
languages, we convert them into near projective
ones. Then, we directly apply the algorithm on mod-
ified data in training stage. Because the sentences in
some language, such as Czech, etc. , may have multi
roots, in our experiment, we ran multiple rounds of
Algorithm 1 to build the tree.
1.2 Labeling the Type of Dependencies
In our work, labeling the type of dependencies is
a post-task after the phase of predicting the head
for the tokens in the sentences. This is a multi-
class classification task. The number of the de-
187
pendency types for each language can be found in
the organizer?s introduction paper of the shared task
of CoNLL-X. In the phase of learning dependency
types, the parent of the tokens, which was labeled
in the first phase, will be used as features. The pre-
dicted actions can help us to make accurate predic-
tions for dependency types.
1.3 Dealing with Crossing Edges
The algorithm described in previous section is pri-
marily designed for projective languages. To deal
with non-projective languages, we use a similar ap-
proach of (Nivre and Nilsson, 2005) to map non-
projective trees to projective trees. Any single
rooted projective dependency tree can be mapped
into a projective tree by the Lift operation. The
definition of Lift is as follows: Lift(wj ? wk) =
parent(wj) ? wk, where a ? b means that a is the
parent of b, and parent is a function which returns
the parent word of the given word. The procedure is
as follows. First, the mapping algorithm examines if
there is a crossing edge in the current tree. If there is
a crossing edge, it will perform Lift and replace the
edge until the tree becomes projective.
2 Local Search
The advantage of a pipeline model is that it can use
more information that is taken from the outcomes
of previous prediction. However, this may result in
accumulating error. Therefore, it is essential for our
algorithm to use a reliable action predictor. This mo-
tivates the following approach for making the local
prediction in a pipeline model more reliable. Infor-
mally, we devise a local search algorithm and use it
as a look ahead policy, when determining the pre-
dicted action.
In order to improve the accuracy, we might want
to examine all the combinations of actions proposed
and choose the one that maximizes the score. It is
clearly intractable to find the global optimal predic-
tion sequence in a pipeline model of the depth we
consider. The size of the possible action sequence
increases exponentially so that we can not examine
every possibility. Therefore, a local search frame-
work which uses additional information, however, is
suitable and tractable.
The local search algorithm is presented in Al-
Algorithm 2 Pseudo code for the local search al-
gorithm. In the algorithm, y represents the a action
sequence. The function search considers all possible
action sequences with |depth| actions and returns
the sequence with highest score.
Algo predictAction(model, depth, State)
x = getNextFeature(State)
y = search(x, depth, model, State)
lab = y[1]
State = update(State, lab)
return lab
Algo search(x, depth, model, State)
maxScore = ??
F = {y | ?y? = depth}
for y in F do
s = 0, TmpState = State
for i = 1 . . . depth do
x = getNextFeature(TmpState)
s = s + log(score(y[i], x))
TmpState = update(TmpState, y[i])
end for
if s > maxScore then
y? = y
maxScore = s
end if
end for
return y?
gorithm 2. The algorithm accepts two parameters,
model and depth. We assume a classifier that can
give a confidence in its prediction. This is repre-
sented here by model. depth is the parameter de-
termining the depth of the local search. State en-
codes the configuration of the environment (in the
context of the dependency parsing this includes the
sentence, the focus point and the current parent and
children for each node). Note that the features ex-
tracted for the action classifier depends on State, and
State changes by the update function when a predic-
tion is made. In this paper, the update function cares
about the child word elimination, relationship addi-
tion and focus point movement.
The search algorithm will perform a search of
length depth. Additive scoring is used to score the
sequence, and the first action in this sequence is per-
formed. Then, the State is updated, determining the
188
next features for the action classifiers and search is
called again.
One interesting property of this framework is that
we use future information in addition to past infor-
mation. The pipeline model naturally allows access
to all the past information. But, since our algorithm
uses the search as a look ahead policy, it can produce
more robust results.
3 Experiments and Results
In this work we used as our learning algorithm a
regularized variation of the perceptron update rule
as incorporated in SNoW (Roth, 1998; Carlson et
al., 1999), a multi-class classifier that is specifically
tailored for large scale learning tasks. SNoW uses
softmax over the raw activation values as its confi-
dence measure, which can be shown to be a reliable
approximation of the labels? probabilities. This is
used both for labeling the actions and types of de-
pendencies. There is no special language enhance-
ment required for each language. The resources pro-
vided for 12 languages are described in: (Hajic? et
al., 2004; Chen et al, 2003; Bo?hmova? et al, 2003;
Kromann, 2003; van der Beek et al, 2002; Brants
et al, 2002; Kawata and Bartels, 2000; Afonso et
al., 2002; Dz?eroski et al, 2006; Civit Torruella and
Mart?? Anton??n, 2002; Nilsson et al, 2005; Oflazer et
al., 2003; Atalay et al, 2003).
3.1 Experimental Setting
The feature set plays an important role in the qual-
ity of the classifier. Basically, we used the same
feature set for the action selection classifiers and
for the label classifiers. In our work, each exam-
ple has average fifty active features. For each word
pair (w1, w2), we used their LEMMA, the POSTAG
and also the POSTAG of the children of w1 and
w2. We also included the LEMMA and POSTAG
of surrounding words in a window of size (2, 4).
We considered 2 words before w1 and 4 words af-
ter w2 (we agree with the window size in (Yamada
and Matsumoto, 2003)). The major difference of
our feature set compared with the one in (Yamada
and Matsumoto, 2003) is that we included the pre-
vious predicted action. We also added some con-
junctions of the above features to ensure expressive-
ness of the model. (Yamada and Matsumoto, 2003)
made use of the polynomial kernel of degree 2 so
they in fact use more conjunctive features. Beside
these features, we incorporated the information of
FEATS for the languages when it is available. The
columns in the data files we used for our work are
the LEMMA, POSTAG, and the FEATS, which is
treated as atomic. Due to time limitation, we did not
apply the local search algorithm for the languages
having the FEATS features.
3.2 Results
Table 1 shows our results on Unlabeled Attachment
Scores (UAS), Labeled Attachment Scores (LAS),
and Label Accuracy score (LAC) for 12 languages.
Our results are compared with the average scores
(AV) and the standard deviations (SD), of all the sys-
tems participating in the shared task of CoNLL-X.
Our average UAS for 12 languages is 83.54%
with the standard deviation 6.01; and 76.80% with
the standard deviation 9.43 for average LAS.
4 Analysis and Discussion
We observed that our UAS for Arabic is generally
lower than for other languages. The reason for the
low accuracy of Arabic is that the sentence is very
long. In the training data for Arabic, there are 25%
sentences which have more than 50 words. Since
we use a pipeline model in our algorithm, it required
more predictions to complete a long sentence. More
predictions in pipeline models may result in more
mistakes. We think that this explains our relatively
low Arabic result. Moreover, in our current system,
we use the same window size (2,4) for feature ex-
traction in all languages. Changing the windows size
seems to be a reasonable step when the sentences are
longer.
For Czech, one reason for our relatively low result
is that we did not use the whole training corpus due
to time limitation 2 . Actually, in our experiment
on the development set, when we increase the size
of training data in the training phase we got signif-
icantly higher result than the system trained on the
smaller data. The other problem for Czech is that
Czech is one of the languages with many types of
part of speech and dependency types, and also the
2Training our system for most languages takes 30 minutes
or 1 hour for both phases of labeling HEAD and DEPREL. It
takes 6-7 hours for Czech with 50% training data.
189
Language UAS LAS LAC
Ours AV SD Ours AV SD Ours AV SD
Arabic 76.09 73.48 4.94 60.92 59.94 6.53 75.69 75.12 5.49
Chinese 89.60 84.85 5.99 85.05 78.32 8.82 87.28 81.66 7.92
Czech 81.78 77.01 6.70 72.88 67.17 8.93 80.42 76.59 7.69
Danish 86.85 84.52 8.97 80.60 78.31 11.34 86.51 84.50 4.35
Dutch 76.25 75.07 5.78 72.91 70.73 6.66 80.15 77.57 5.92
German 86.90 82.60 6.73 84.17 78.58 7.51 91.03 86.26 6.01
Japanese 90.77 89.05 5.20 89.07 85.86 7.09 92.18 89.90 5.36
Portuguese 88.60 86.46 4.17 83.99 80.63 5.83 88.84 85.35 5.45
Slovene 80.32 76.53 4.67 69.52 65.16 6.78 79.26 76.31 6.40
Spanish 83.09 77.76 7.81 79.72 73.52 8.41 89.26 85.71 4.56
Swedish 89.05 84.21 5.45 82.31 76.44 6.46 84.82 80.00 6.24
Turkish 73.15 69.35 5.51 60.51 55.95 7.71 73.75 69.59 7.94
Table 1: Our results are compared with the average scores. UAS=Unlabeled Attachment Score,
LAS=Labeled Attachment Score, LAC=Label Accuracy, AV=Average score, and SD=standard deviation.
length of the sentences in Czech is relatively long.
These facts make recognizing the HEAD and the
types of dependencies more difficult.
Another interesting aspect is that we have not
used the information about the syntactic and/or mor-
phological features (FEATS) properly. For the lan-
guages for which FEATS is available, we have a
larger gap, compared with the top system.
5 Further Work and Conclusion
In the shared task of CoNLL-X, we have shown that
our dependency parsing system can do well on mul-
tiple languages without requiring special knowledge
for each of the languages.
From a technical perspective, we have addressed
the problem of using learned classifiers in a pipeline
fashion, where a task is decomposed into several
stages and classifiers are used sequentially to solve
each stage. This is a common computational strat-
egy in natural language processing and is known to
suffer from error accumulation and an inability to
correct mistakes in previous stages. We abstracted
two natural principles, one which calls for making
the local classifiers used in the computation more
reliable and a second, which suggests to devise the
pipeline algorithm in such a way that it minimizes
the number of actions taken.
However, since we tried to build a single approach
for all languages, we have not fully utilized the capa-
bilities of our algorithms. In future work we will try
to specify both features and local search parameters
to the target language.
Acknowledgement This research is supported by
NSF ITR IIS-0428472, a DOI grant under the Reflex
program and ARDA?s Advanced Question Answer-
ing for Intelligence (AQUAINT) program.
References
A. V. Aho, R. Sethi, and J. D. Ullman. 1986. Compilers:
Principles, techniques, and tools. In Addison-Wesley
Publishing Company, Reading, MA.
A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer Science De-
partment, May.
M. Chang, Q. Do, and D. Roth. 2006. Local search
for bottom-up dependency parsing. Technical report,
UIUC Computer Science Department.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05).
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 806?813.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT2003.
190
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 107?112,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantic and Logical Inference Model for Textual Entailment
Dan Roth
University of Illinois
at Urbana-Champaign
Urbana, IL 61801
danr@cs.uiuc.edu
Mark Sammons
University of Illinois
at Urbana-Champaign
Urbana, IL 61801
mssammon@uiuc.edu
Abstract
We compare two approaches to the problem
of Textual Entailment: SLIM, a composi-
tional approach modeling the task based on
identifying relations in the entailment pair,
and BoLI, a lexical matching algorithm.
SLIM?s framework incorporates a range of
resources that solve local entailment prob-
lems. A search-based inference procedure
unifies these resources, permitting them to
interact flexibly. BoLI uses WordNet and
other lexical similarity resources to detect
correspondence between related words in
the Hypothesis and the Text. In this pa-
per we describe both systems in some detail
and evaluate their performance on the 3rd
PASCAL RTE Challenge. While the lex-
ical method outperforms the relation-based
approach, we argue that the relation-based
model offers better long-term prospects for
entailment recognition.
1 Introduction
We compare two Textual Entailment recognition
systems applied to the 3rd PASCAL RTE challenge.
Both systems model the entailment task in terms
of determining whether the Hypothesis can be ?ex-
plained? by the Text.
The first system, BoLI (Bag of Lexical Items)
uses WordNet and (optionally) other word similarity
resources to compare individual words in the Hy-
pothesis with the words in the Text.
The second system, the Semantic and Logical
Inference Model (SLIM) system, uses a relational
model, and follows the model-theory-based ap-
proach of (Braz et al, 2005).
SLIM uses a suite of resources to modify the orig-
inal entailment pair by augmenting or simplifying
either or both the Text and Hypothesis. Terms re-
lating to quantification, modality and negation are
detected and removed from the graphical represen-
tation of the entailment pair and resolved with an
entailment module that handles basic logic.
In this study we describe the BoLI and SLIM sys-
tems and evaluate their performance on the 3rd PAS-
CAL RTE Challenge corpora. We discuss some ex-
amples and possible improvements for each system.
2 System Description: Bag of Lexical
Items (BoLI)
The BoLI system compares each word in the text
with a word in the hypothesis. If a word is found in
the Text that entails a word in the Hypothesis, that
word is considered ?explained?. If the percentage of
the Hypothesis that can be explained is above a cer-
tain threshold, the Text is considered to entail the
Hypothesis. This threshold is determined using a
training set (in this case, the Development corpus),
by determining the percentage match for each entail-
ment pair and selecting the threshold that results in
the highest overall accuracy.
BoLI uses an extended set of stopwords includ-
ing auxiliary verbs, articles, exclamations, and dis-
course markers in order to improve the distinction
between Text and Hypothesis. Negation and modal-
ity are not explicitly handled.
The BoLI system can be changed by varying
the comparison resources it uses. The available
107
resources are: WordNet-derived (Fellbaum, 1998)
synonymy, meronymy, membership, and hyper-
nymy; a filtered version of Dekang Lin?s word sim-
ilarity list (Lin, 1998) (only the ten highest-scored
entries for each word); and a resource based on a
lexical comparison of WordNet glosses.
We tried three main versions; one that used the
four WordNet- derived resources (   ); a second
that adds to the first system the Dekang Lin resource
(   ); and a third that added to the second sys-
tem the Gloss resource (  	
 ). We ran them on
the Development corpus, and determined the thresh-
old that gave the highest overall score. We then
used the highest-scoring version and the correspond-
ing threshold to determine labels for the Test cor-
pus. The results and thresholds for each variation
are given in table 1.
3 System Description: Semantic and
Logical Inference Model (SLIM)
The SLIM system approaches the problem of entail-
ment via relations: the goal is to recognize the rela-
tions in the Text and Hypothesis, and use these to de-
termine whether the Text entails the Hypothesis. A
word in the Hypothesis is considered ?covered? by
a relation if it appears in that relation in some form
(either directly or via abstraction). For the Text to
entail the Hypothesis, sufficient relations in the Hy-
pothesis must be entailed by relations in the Text to
cover the underlying text.
The term ?Relation? is used here to describe a
predicate-argument structure where the predicate is
represented by a verb (which may be inferred from a
nominalized form), and the arguments by strings of
text from the original sentence. These constituents
may be (partially) abstracted by replacing tokens
in some constituent with attributes attached to that
or a related constituent (for example, modal terms
may be dropped and represented with an attribute
attached to the appropriate predicate).
Relations may take other relations as arguments.
Examples include ?before? and ?after? (when both
arguments are events) and complement structures.
3.1 Representation
The system compares the Text to the Hypothesis us-
ing a ?blackboard? representation of the two text
fragments (see figure 1). Different types of anno-
tation are specified on different layers, all of which
are ?visible? to the comparison algorithm. All lay-
ers map to the original representation of the text, and
each annotated constituent corresponds to some ini-
tial subset of this original representation. This al-
lows multiple representations of the same surface
form to be entertained.
Figure 1 shows some of the layers in this data
structure for a simple entailment pair: the origi-
nal text in the WORD layer; the relations induced
from this text in the PREDICATE layer; and for
the Text, a Coreference constituent aligned with the
word ?he? in the COREFERENCE layer. Note that
the argument labels for ?give? in the Text indicate
that ?he? is the theme/indirect object of the predi-
cate ?give?.
Figure 1: ?Blackboard? Representation of Entail-
ment Pairs in SLIM
The     President     was      happy     that     he     was     given    the    award     .  WORD
The     President      be       happy      that     he     was     given    the    award
ARG1 PRED ARG2
COREF The   President
PREDICATE
TEXT
HYPOTHESIS
WORD
PREDICATE
The     President     received    an     award     . 
The     President     receive     an     award
PRED ARG1ARG2
he          give            the     award
PRED ARG1ARG0
At compare-time, the coref constituent ?The Pres-
ident? will be considered as a substitute for ?he?
when comparing the relation in the Hypothesis with
the second relation in the Text. (The dashed lines
indicate that the coverage of the coreference con-
stituent is just that of the argument consisting of the
word ?he?.) The relation comparator has access to
a list of rules mapping between verbs and their ar-
gument types; this will allow it to recognize that the
relation ?give? can entail ?receive?, subject to the
constraint that the agent of ?give? must be the patient
of ?receive?, and vice versa. This, together with the
coreference constituent in the Text that aligns with
the argument ?he?, will allow the system to recog-
nize that the Text entails the Hypothesis.
108
3.2 Algorithm
The SLIM entailment system applies sequences of
transformations to the original entailment pair in or-
der to modify one or both members of the pair to
make it easier to determine whether the Text entails
the Hypothesis. The resources that make these trans-
formations are referred to here as ?operators?. Each
operator is required to use Purposeful Inference: be-
fore making a change to either entailment pair mem-
ber, they must take the other member into account.
For example, the conjunction expander will generate
only those expansions in a text fragment that match
structures in the paired text fragment more closely.
This constrains the number of transformations con-
sidered and can reduce the amount of noise intro-
duced by these operators.
Each such operator serves one of three purposes:
1. ANNOTATE. Make some implicit property of
the meaning of the sentence explicit.
2. SIMPLIFY/TRANSFORM. Remove or alter
some section of the Text in order to improve
annotation accuracy or make it more similar to
the Hypothesis.
3. COMPARE. Compare (some elements of) the
two members of the entailment pair and as-
sign a score that correlates to how successfully
(those elements of) the Hypothesis can be sub-
sumed by the Text.
The system?s operators are applied to an entail-
ment pair, potentially generating a number of new
versions of that entailment pair. They may then be
applied to these new versions. It is likely that only
a subset of the operators will fire. It is also possible
that multiple operators may affect overlapping sec-
tions of one or both members of the entailment pair,
and so the resulting perturbations of the original pair
may be sensitive to the order of application.
To explore these different subsets/orderings, the
system is implemented as a search process over the
different operators. The search terminates as soon
as a satisfactory entailment score is returned by the
comparison operator for a state reached by applying
transformation operators, or after some limit to the
depth of the search is reached. If entailment is de-
termined to hold, the set of operations that generated
the terminal state constitutes a proof of the solution.
3.2.1 Constraining the Search
To control the search to allow for the interdepen-
dence of certain operators, each operator may spec-
ify a set of pre- and post-conditions. Pre-conditions
specify which operators must have fired to provide
the necessary input for the current operator. Post-
conditions typically indicate whether or not it is de-
sirable to re-annotate the resulting entailment pair
(e.g. after an operation that appends a new relation
to an entailment pair member), or whether the Com-
parator should be called to check for entailment.
3.3 System Resources: Annotation
The SLIM system uses a number of standard an-
notation resources ? Part-of-Speech, Shallow- and
Full syntactic parsing, Named Entity tagging, and
Semantic Role Labelling ? but also has a number
of more specialized resources intended to recognize
implicit predicates from the surface representation
in the text, and append these relations to the original
text. These resources are listed below with a brief
description of each.
Apposition Detector. Uses full parse information
to detect appositive constructions, adding a relation
that makes the underlying meaning explicit. It uses
a set of rules specifying subtree structure and phrase
labels.
Complex Noun Phrase Relation Detector. An-
alyzes long noun phrases and annotates them with
their implicit relations. It applies a few general
rules expressed at the shallow parse and named en-
tity level.
Modality and Negation Annotator. Abstracts
modifiers of relations representing modality or nega-
tion into attributes attached to the relation.
Discourse Structure Annotator. Scans the rela-
tion structure (presently only at the sentence level)
to determine negation and modality of relations em-
bedded in factive and other constructions. It marks
the embedded relations accordingly, and where pos-
sible, discards the embedding relation.
Coreference Annotator. Uses Named Entity
information to map pronouns to possible replace-
ments.
Nominalization Rewriter. Detects certain com-
mon nominalized verb structures and makes the re-
lation explicit. The present version applies a small
set of very general rules instantiated with a list of
109
embedding verbs and a mapping from nominalized
to verbal forms.
3.4 System Resources:
Simplification/Transformation
The simplification resources all demonstrate pur-
poseful inference, as described in section 3.2.
Idiom Catcher. Identifies and replaces sequences
of words corresponding to a list of known idioms,
simplifying sentence structure. It can recognize a
range of surface representations for each idiom.
Phrasal Verb Replacer. Checks for phrasal verb
constructions, including those where the particle is
distant from the main verb, replacing them with sin-
gle verbs of equivalent meaning.
Conjunction Expander. Uses full parse informa-
tion to detect and rewrite conjunctive argument and
predicate structures by expanding them.
Multi-Word Expression Contractor. Scans both
members of the entailment pair for compound noun
phrases that can be replaced by just the head of the
phrase.
3.5 System Resources: Main Comparator
All comparator resources are combined in a single
operator for simplicity. This comparator uses the
blackboard architecture described in 3.1.
The main comparator compares each relation in
the Hypothesis to each relation in the Text, return-
ing ?True? if sufficient relations in the Hypothesis
are entailed by relations in the Text to cover the un-
derlying representation of the Hypothesis.
For a relation in the Text to entail a relation in the
Hypothesis, the Text predicate must entail the Hy-
pothesis predicate, and all arguments of the Hypoth-
esis relation must be entailed by arguments of the
Text relation. This entailment check also accounts
for attributes such as negation and modality.
As part of this process, a set of rules that map be-
tween predicate- argument structures (some hand-
written, most derived from VerbNet) are applied
on-the-fly to the pair of relations being compared.
These rules specify a mapping between predicates
and a set of constraints that apply to the mappings
between arguments of the predicates. For example,
the agent of the relation ?sell? should be the theme
of the relation ?buy?, and vice versa.
When comparing the arguments of predicates, the
system uses BoLI with the same configuration and
threshold that give the best performance on the de-
velopment set.
3.6 Comparison to Similar Approaches
Like (de Marneffe et al, 2005), SLIM?s represen-
tation abstracts away terms relating to negation,
modality and quantification. However, it uses them
as part of the comparison process, not as features
to be used in a classifier. In contrast to (Braz
et al, 2005), SLIM considers versions of the en-
tailment pair with and without simplifications of-
fered by preprocessing modules, rather than reason-
ing only about the simplified version; and rather
than formulating the subsumption (entailment) prob-
lem as a hierarchical linear program or classification
problem, SLIM defers local entailment decisions to
its modules and returns a positive label for a con-
stituent only if these resources return a positive la-
bel for all subconstituents. Finally, SLIM returns an
overall positive label if all words in the Hypothesis
can be ?explained? by relations detected in the Hy-
pothesis and matched in the Text, rather than requir-
ing all detected relations in the Text to be entailed
by relations in the Hypothesis.
4 Experimental Results
Table 3 presents the peformance of the BoLI and
SLIM systems on the 3rd PASCAL RTE Challenge.
The version of SLIM used for the Development cor-
pus was incomplete, as several modules (Multi-word
Expression, Conjunction, and Apposition) were still
being completed at that time. Table 1 indicates the
performance of   different versions of the BoLI sys-
tem on the Development corpus as described in sec-
tion 2.
To investigate the improvement of performance
for the SLIM system relative to the available re-
sources, we conducted a limited ablation study. Ta-
ble 2 shows the performance for   different ver-
sions of the SLIM system on 100 entailment pairs
each from the IE and QA subtasks of the Test cor-
pus. The ?full? (f) system includes all available re-
sources. The ?intermediate? (i) system excludes the
resources we consider most likely to introduce er-
rors, the Multiword Expression module and the most
general Nominalization rewrite rules in the Nom-
inalization Rewriter. The ?strict? (s) system also
omits the Apposition and Complex Noun Phrase
110
Table 1: Accuracy and corresponding threshold for
versions of BoLI on the Development corpus.
TASK Accuracy Threshold
 	
 0.675 0.667
 	

0.650 0.833
 	
 0.655 0.833
Table 2: Results for different versions of SLIM on
subsets of the Test and Develoment corpora.
System SLIM s SLIM i SLIM f
Dev IE - - 0.650
Dev QA - - 0.660
Test IE 0.480 0.480 0.470
Test QA 0.680 0.710 0.710
modules. To give a sense of how well the complete
SLIM system does on the Development corpus, the
results for the full SLIM system on equal-sized sub-
sets of the IE and QA subtasks of the Development
corpus are also shown.
5 Discussion
From Table 3, it is clear that BoLI outperforms
SLIM in every subtask.
The ablation study in Table 2 shows that adding
new resources to SLIM has mixed benefits; from
the samples we used for evaluation, the intermediate
system would be the best balance between module
coverage and module accuracy.
In the rest of this section, we analyze the re-
sults and each system?s behavior on several exam-
ples from the corpus.
5.1 BoLI
There is a significant drop in performance of the
BoLI from the Development corpus to the Test cor-
pus, indicating that the threshold somewhat overfit-
ted to the data used to train it. The performance drop
when adding the gloss and Dekang Lin word simi-
larity resources is not necessarily surprising, as these
resources are clearly noisy, and so may increase sim-
ilarity based on inappropriate word pairs.
In the following example, the word similarity is
high, but the structure of the two text fragments
gives the relevant words different overall meaning
(here, that one subset of the matched words does not
apply to the other):
id=26 Text: Born in Kingston-upon-Thames, Surrey, Brock-
well played his county cricket for the very strong Surrey side of
the last years of the 19th century.
Hypothesis: Brockwell was born in the last years of the 19th
century.
From this example it is clear that in addition to
the role of noise from these additional resources, the
structure of text plays a major role in meaning, and
this is exactly what BoLI cannot capture.
5.2 SLIM
The ablation study for the SLIM system shows a
trade-off between precision and recall for some re-
sources. In this instance, adding resources improves
performance significantly, but including noisy re-
sources also implies a ceiling on overall perfor-
mance will ultimately be reached.
The following example shows the potentially
noisy possessive rewrite operator permitting suc-
cessful entailment:
id=19 Text: During Reinsdorf?s 24 seasons as chairman of
the White Sox, the team has captured Americal League divi-
sion championships three times, including an AL Central title
in 2000.
Transformed Text: During Reinsdorf have 24 seasons as chair-
man of the White Sox ...
Hypothesis: Reinsdorf was chairman of the White Sox for 24
seasons.
There are a number of examples where relaxed
operators result in false positives, but where the neg-
ative label is debatable. In the next example, the ap-
position module adds a new relation and the Nomi-
nalization Rewriter detects the hypothesis using this
new relation:
id=102 Hypothesis: He was initially successful, negotiating
a 3/4 of 1 percent royalty on all cars sold by the Association of
Licensed Automobile Manufacturers, the ALAM.
Transformed Text: ... Association of Licensed Automobile
Manufacturers is the ALAM.
Hypothesis: The ALAM manufactured cars.
Finally, some modules did not fire as they should;
for example 15, the conjunction module did not ex-
pand the conjunction over predicates. For example
24, the nominalization rewriter did not detect ?plays
in the NHL? from ?is a NHL player?. In example 35,
the apposition module did not detect that ?Harriet
111
Table 3: Results for SLIM and BoLI on the Pascal Development and Test Corpora. Results marked with an
asterisk indicate not all system resources were available at the time the system was run.
Corpus Development Test
Subtask IE IR QA SUM OVERALL IE IR QA SUM OVERALL
BoLI 0.560 0.700 0.790 0.690 0.675 0.510 0.710 0.830 0.575 0.656
SLIM 0.580* 0.595* 0.650* 0.545* 0.593* 0.485 0.6150 0.715 0.575 0.5975
Lane, niece of President James? could be rewritten.
Of course, there are also many examples where
the SLIM system simply does not have appropriate
resources (e.g. numerical reasoning, coreference re-
quiring semantic categorization).
6 Conclusion
While BoLI outperforms SLIM on the PASCAL
RTE 3 task, there is no clear way to improve BoLI.
It is clear that for the PASCAL corpora, the distribu-
tions over word similarity between entailment pair
members in positive and negative examples are dif-
ferent, allowing this simple approach to perform rel-
atively well, but there is no guarantee that this is gen-
erally the case, and it is easy to create an adversar-
ial corpus on which BoLI performs very badly (e.g.,
exchanging arguments or predicates of different cre-
lations in the Text), no matter how good the word-
level entailment resources are. This approach also
offers no possibility of a meaningful explanation of
the entailment decision.
SLIM, on the other hand, by offering a framework
to which new resources can be added in a principled
way, can be extended to cover new entailment phe-
nomena in an incremental, local (i.e. compositional)
way. The results of the limited ablation study sup-
port this conclusion, though the poor performance
on the IE task indicates the problems with using
lower-precision, higher-recall resources.
Overall, we find the results for the SLIM system
very encouraging, as they support the underlying
concept of incremental improvement, and this offers
a clear path toward better performance.
6.1 Acknowledgements
We gratefully acknowledge the work on SLIM
modules by Ming-Wei Chang, Michael Connor,
Quang Do, Alex Klementiev, Lev Ratinov, and
Vivek Srikumar. This work was funded by
the Advanced Research and Development Activity
(ARDA)?s Advanced Question Answering for Intel-
ligence (AQUAINT) program, a grant from Boeing,
and a gift from Google.
References
Johan Bos and Katja Markert. 2005. When logical infer-
ence helps determining textual entailment (and when it
doesn?t). In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognizing Textual Entailment.
R. Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sam-
mons. 2005. Knowledge representation for seman-
tic entailment and question-answering. In IJCAI-05
Workshop on Knowledge and Reasoning for Question
Answering.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christopher
Manning. 2005. Learning to distinguish valid textual
entailments. In Proceedings of the Second PASCAL
Challenges Workshop on Recognizing Textual Entail-
ment.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Sophia Katrenko and Peter Adriaans. 2005. Using
maximal embedded syntactic subtrees for textual en-
tailment recognition. In Proceedings of the Second
PASCAL Challenges Workshop on Recognizing Tex-
tual Entailment.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proc. of the International Conference on
Machine Learning (ICML).
Marta Tatu, Brandon Iles, John Slavick, Adrian Novis-
chi, and Dan Moldovan. 2005. Cogex at the second
recognizing textual entailment challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognizing Textual Entailment.
112
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 66?74,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Interactive Feature Space Construction using Semantic Information
Dan Roth and Kevin Small
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{danr,ksmall}@illinois.edu
Abstract
Specifying an appropriate feature space is an
important aspect of achieving good perfor-
mance when designing systems based upon
learned classifiers. Effectively incorporat-
ing information regarding semantically related
words into the feature space is known to pro-
duce robust, accurate classifiers and is one ap-
parent motivation for efforts to automatically
generate such resources. However, naive in-
corporation of this semantic information may
result in poor performance due to increased
ambiguity. To overcome this limitation, we
introduce the interactive feature space con-
struction protocol, where the learner identi-
fies inadequate regions of the feature space
and in coordination with a domain expert adds
descriptiveness through existing semantic re-
sources. We demonstrate effectiveness on an
entity and relation extraction system includ-
ing both performance improvements and ro-
bustness to reductions in annotated data.
1 Introduction
An important natural language processing (NLP)
task is the design of learning systems which per-
form well over a wide range of domains with limited
training data. While the NLP community has a long
tradition of incorporating linguistic information into
statistical systems, machine learning approaches to
these problems often emphasize learning sophisti-
cated models over simple, mostly lexical, features.
This trend is not surprising as a primary motivation
for machine learning solutions is to reduce the man-
ual effort required to achieve state of the art perfor-
mance. However, one notable advantage of discrimi-
native classifiers is the capacity to encode arbitrarily
complex features, which partially accounts for their
popularity. While this flexibility is powerful, it often
overwhelms the system designer causing them to re-
sort to simple features. This work presents a method
to partially automate feature engineering through an
interactive learning protocol.
While it is widely accepted that classifier perfor-
mance is predicated on feature engineering, design-
ing good features requires significant effort. One un-
derutilized resource for descriptive features are ex-
isting semantically related word lists (SRWLs), gen-
erated both manually (Fellbaum, 1998) and automat-
ically (Pantel and Lin, 2002). Consider the follow-
ing named entity recognition (NER) example:
His father was rushed to [Westlake
Hospital]ORG, an arm of [Resurrection
Health Care]ORG, in west suburban
[Chicagoland]LOC.
For such tasks, it is helpful to know that west is
a member of the SRWL [Compass Direction] and
other such designations. If extracting features using
this information, we would require observing only
a subset of the SRWL in the data to learn the cor-
responding parameter. This statement suggests that
one method for learning robust classifiers is to in-
corporate semantic information through features ex-
tracted from the more descriptive representation:
His father was rushed to Westlake [Health
Care Institution], an [Subsidiary] of Resur-
rection Health Care, [Locative Preposition]
[Compass Direction] suburban Chicagoland.
66
Deriving discriminative features from this rep-
resentation often results in more informative fea-
tures and a correspondingly simpler classification
task. Although effective approaches along this vein
have been shown to induce more accurate classi-
fiers (Boggess et al, 1991; Miller et al, 2004; Li and
Roth, 2005), naive approaches may instead result in
higher sample complexity due to increased ambi-
guity introduced through these semantic resources.
Features based upon SRWLs must therefore balance
the tradeoff between descriptiveness and noise.
This paper introduces the interactive feature
space construction (IFSC) protocol, which facil-
itates coordination between a domain expert and
learning algorithm to interactively define the feature
space during training. This paper describes the par-
ticular instance of the IFSC protocol where seman-
tic information is introduced through abstraction of
lexical terms in the feature space with their SRWL
labels. Specifically, there are two notable contri-
butions of this work: (1) an interactive method for
the expert to directly encode semantic knowledge
into the feature space with minimal effort and (2) a
querying function which uses both the current state
of the learner and properties of the available SRWLs
to select informative instances for presentation to
the expert. We demonstrate the effectiveness of this
protocol on an entity and relation extraction task in
terms of performance and labeled data requirements.
2 Preliminaries
Following standard notation, let x ? X represent
members of an input domain and y ? Y represent
members of an output domain where a learning al-
gorithm uses a training sample S = {(xi, yi)}mi=1
to induce a prediction function h : X ? Y . We
are specifically interested in discriminative classi-
fiers which use a feature vector generating procedure
?(x) ? x, taking an input domain member x and
generating a feature vector x. We further assume the
output assignment of h is based upon a scoring func-
tion f : ?(X ) ? Y ? R such that the prediction is
stated as y? = h(x) = argmaxy??Y f(x, y?).
The feature vector generating procedure is com-
posed of a vector of feature generation functions
(FGFs), ?(x) = ??1(x),?2(x), . . . ,?n(x)?, where
each feature generation function, ?i(x) ? {0, 1},
takes the input x and returns the appropriate fea-
ture vector value. Consider the text ?in west sub-
urban Chicagoland? where we wish to predict the
entity classification for Chicagoland. In this case,
example active FGFs include ?text=Chicagoland,
?isCapitalized, and ?text(?2)=west while FGFs such
as ?text=and would remain inactive. Since we are
constructing sparse feature vectors, we use the infi-
nite attribute model (Blum, 1992).
Semantically related word list (SRWL) feature
abstraction begins with a set of variable sized
word lists {W} such that each member lexical
element (i.e. word, phrase) has at least one
sense that is semantically related to the concept
represented by W (e.g. Wcompass direction =
north, east, . . . , southwest). For the purpose of
feature extraction, whenever the sense of a lexical el-
ement associated with a particularW appears in the
corpus, it is replaced by the name of the correspond-
ing SRWL. This is equivalent to defining a FGF for
the specified W which is a disjunction of the func-
tionally related FGFs over the member lexical ele-
ments (e.g. ?text?Wcompass direction = ?text=north ??text=east ? . . . ? ?text=southwest).
3 Interactive Feature Space Construction
The machine learning community has become in-
creasingly interested in protocols which allow inter-
action with a domain expert during training, such as
the active learning protocol (Cohn et al, 1994). In
active learning, the learning algorithm reduces the
labeling effort by using a querying function to in-
crementally select unlabeled examples from a data
source for annotation during learning. By care-
fully selecting examples for annotation, active learn-
ing maximizes the quality of inductive information
while minimizing label acquisition cost.
While active learning has been shown to reduce
sample complexity, we contend that it significantly
underutilizes the domain expert ? particularly for
complex annotation tasks. More precisely, when a
domain expert receives an instance, world knowl-
edge is used to reason about the instance and sup-
ply an annotation. Once annotated and provided for
training, the learner must recover this world knowl-
edge and incorporate it into its model from a small
number of instances, exclusively through induction.
67
Learning algorithms generally assume that the
feature space and model are specified before learn-
ing begins and remain static throughout learning,
where training data is exclusively used for parameter
estimation. Conversely, the interactive feature space
construction (IFSC) protocol relaxes this static fea-
ture space assumption by using information about
the current state of the learner, properties of knowl-
edge resources (e.g. SRWLs, gazetteers, unlabeled
data, etc.), and access to the domain expert during
training to interactively improve the feature space.
Whereas active learning focuses on the labeling ef-
fort, IFSC reduces sample complexity and improves
performance by modifying the underlying represen-
tation to simplify the overall learning task.
The IFSC protocol for SRWL abstraction is pre-
sented in Algorithm 1. Given a labeled data set S,
an initial feature vector generating procedure ?0, a
querying function Q : S ? h ? Sselect, and an
existing set of semantically related word lists, {W}
(line 1), an initial hypothesis is learned (line 3). The
querying function scores the labeled examples and
selects an instance for interaction (line 6). The ex-
pert selects lexical elements from this instance for
which feature abstractions may be performed (line
8). If the expert doesn?t deem any elements vi-
able for interaction, the algorithm returns to line 5.
Once lexical elements are selected for interaction,
the SRWLW associated with each selected element
is retrieved (line 11) and refined by the expert (line
12). Using the validated SRWL definition W? , the
lexical FGFs are replaced with the SRWL FGF (line
14). This new feature vector generating procedure
?t+1 is used to train a new classifier (line 18) and
the algorithm is repeated until the annotator halts.
3.1 Method of Expert Interaction
The method of interaction for active learning is
very natural; data annotation is required regardless.
To increase the bandwidth between the expert and
learner, a more sophisticated interaction must be al-
lowed while ensuring that the expert task of remains
reasonable. We require the interaction be restricted
to mouse clicks. When using this protocol to in-
corporate semantic information, the primary tasks
of the expert are (1) selecting lexical elements for
SRWL feature abstraction and (2) validating mem-
bership of the SRWL for the specified application.
Algorithm 1 Interactive Feature Space Construction
1: Input: Labeled training data S, feature vector
generating procedure ?0, querying function Q,
set of known SRWLs {W}, domain expert A?
2: t? 0
3: ht ? A(?t,S); learn initial hypothesis
4: Sselected ? ?
5: while annotator is willing do
6: Sselect ? Q(S\Sselected, ht); Q proposes
(labeled) instance for interaction
7: Sselected ? Sselected ? Sselect; mark selected
examples to prevent reselection
8: Eselect ? A?(Sselect); the expert selects lex-
ical elements for semantic abstraction
9: ?t+1 ? ?t; initialize new FGF vector with
existing FGFs
10: for each  ? Eselect do
11: Retrieve word listW
12: W? ? A?(W); the expert refines the ex-
isting semantic classW for this task
13: for each ? ?  do
14: ?t+1 ? (?t+1\?) ? ?W? ; re-place features with SRWL features (e.g.
?text= ? ?text?W? )
15: end for
16: end for
17: t? t+ 1
18: ht ? A(?t,S); learn new hypothesis
19: end while
20: Output: Learned hypothesis hT , final feature
space ?T , refined semantic classes {W?}
3.1.1 Lexical Feature Selection (Line 8)
Once an instance is selected by the querying func-
tion (line 6), the the domain expert selects lexical el-
ements (i.e. words, phrases) believed appropriate for
SRWL feature abstraction. This step is summarized
by Figure 1 for the example introduced in Section 1.
For this NER example, features extracted include
the words and bigrams which form the named en-
tity and those within a surrounding two word win-
dow. All lexical elements which have membership
to at least one SRWL and are used for feature ex-
traction are marked with a box and may be selected
by the user for interaction. In this particular case,
the system has made a mistake in classification of
68
His father was rushed to [Westlake
Hospital ]ORG, an arm of [Resurrection
Health Care ]ORG, in west suburban
[Chicagoland]ORG.
Figure 1: Lexical Feature Selection ? All lexical ele-
ments with SRWL membership used to derive features
are boxed. Elements used for the incorrect prediction for
Chicagoland are double-boxed. The expert may select
any boxed element for SRWL validation.
Chicagoland and the lexical elements used to derive
features for this prediction are emphasized with a
double-box for expository purposes. The expert se-
lects lexical elements which they believe will result
in good feature abstractions; the querying function
must present examples believed to have high impact.
3.1.2 Word List Validation (Lines 11 &12)
Once the domain expert has selected a lexical el-
ement for SRWL feature abstraction, they are pre-
sented with the SRWL W to validate membership
for the target application as shown in Figure 2. In
this particular case, the expert has chosen to perform
two interactions, namely for the lexical elements
west and suburban. Once they have chosen which
words and phrases will be included in this particular
feature abstraction,W is updated and the associated
features are replaced with their SRWL counterpart.
For example, ?text=west, ?text=north, etc. would all
be replaced with ?text?WA1806 later in lines 13 & 14.
A1806: southeast, northeast, south
southeast, northeast, south, north, south-
west, west, east, northwest, inland, outside
A1558: suburban, nearby, downtown
suburban, nearby, downtown, urban,
metropolitan, neighboring, near, coastal
Figure 2: Word List Validation ? Completing two domain
expert interactions. Upon selecting either double-boxed
element in Figure 1, the expert validates the respective
SRWL for feature extraction.
Accurate sense disambiguation is helpful for ef-
fective SRWL feature abstraction to manage situa-
tions where lexical elements belong to multiple lists.
In this work, we first disambiguate by predicted part
of speech (POS) tags. In cases of multiple SRWL
senses for a POS, the given SRWLs (Pantel and Lin,
2002) rank list elements according their semantic
representativeness which we use to return the high-
est ranked sense for a particular lexical element.
Also, as SRWL resources emphasize recall over pre-
cision, we reduce expert effort by using the Google
n-gram counts (Brandts and Franz, 2006) to auto-
matically prune SRWLs.
3.2 Querying Function (Line 6)
A primary contribution of this work is designing an
appropriate querying function. In doing so, we look
to maximize the impact of interactions while min-
imizing the total number. Therefore, we look to
select instances for which (1) the current hypoth-
esis indicates the feature space is insufficient and
(2) the resulting SRWL feature abstraction will help
improve performance. To account for these two
somewhat orthogonal goals, we design two query-
ing functions and aggregate their results.
3.2.1 Hypothesis-Driven Querying
To find areas of the feature space which are be-
lieved to require more descriptiveness, we look to
emphasize those instances which will result in the
largest updates to the hypothesis. To accomplish
this, we adopt an idea from the active learning
community and score instances according to their
margin relative to the current learned hypothesis,
?(ft, xi, yi) (Tong and Koller, 2001). This results
in the hypothesis-driven querying function
Qmargin = argsort
i=1,...,m
?(ft, xi, yi)
where the argsort operator is used to sort the input
elements in ascending order (for multiple instance
selection). Unlike active learning, where selection
is from an unlabeled data source, the quantity of la-
beled data is fixed and labeled data is selected during
each round. Therefore, we use the true margin and
not the expected margin. This means that we will
first select instances which have large mistakes, fol-
lowed by those instances with small mistakes, and
finally instances that make correct predictions in the
order of their confidence.
69
3.2.2 SRWL-Driven Querying
An equally important goal of the querying func-
tion is to present examples which will result in
SRWL feature abstractions of broad usability. Intu-
itively, there are two criteria distinguishing desirable
SRWLs for this purpose. First of all, large lists are
desirable as there are many lists of cities, countries,
corporations, etc. which are extremely informative.
Secondly, preference should be given to lists where
the distribution of lexical elements within a particu-
lar word list,  ? W , is more uniform. For example,
consider W = {devour, feed on, eat, consume}.
While all of these terms belong to the same SRWL,
learning features based on eat is sufficient to cover
most examples. To derive a SRWL-driven querying
function based on these principles, we use the word
list entropy, H(W) = ???W p() log p(). The
querying score for a sentence is determined by its
highest entropy lexical element used for feature ex-
traction, resulting in the querying function
Qentropy = argsort
i=1,...,m
[
argmin
??xi
?H(W)
]
This querying function is supported by the under-
lying assumption of SRWL abstraction is that there
exists a true feature space ??(x) which is built upon
SRWLs and lexical elements but is being approxi-
mated by ?(x), which doesn?t use semantic infor-
mation. In this context, a lexical feature provides
one bit of information to the prediction function
while a SRWL feature provides information content
proportional to its SRWL entropy H(W).
To study one aspect of this phenomena empiri-
cally, we examine the rate at which words are first
encountered in our training corpus from Section 4,
as shown by Figure 3. The first observation is
the usefulness of SRWL feature abstraction in gen-
eral as we see that when including an entire SRWL
from (Pantel and Lin, 2002) whenever the first ele-
ment of the list is encountered, we cover the unigram
vocabulary much more rapidly. The second observa-
tion is that when sentences are presented in the or-
der of the average SRWL entropy of their words, this
coverage rate is further accelerated. Figure 3 helps
explain the recall focused aspect of SRWL abstrac-
tion while we rely on hypothesis-driven querying to
target interactions for the specific task at hand.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  200  400  600  800  1000
un
igr
am
 ty
pe
 co
ve
rag
e (
%)
sentences
SRWL - Entropy
SRWL - Sequence
Lexical - Sequence
Figure 3: The Impact of SRWL Abstraction and SRWL-
driven Querying ? The first occurrence of words occur at
a much lower rate than the first occurrence of words when
abstracted through SRWLs, particularly when sentences
are introduced as ranked by average SRWL entropy cal-
culated using (Brandts and Franz, 2006).
3.2.3 Aggregating Querying Functions
To combine these two measures, we use the Borda
count method of rank aggregation (Young, 1974) to
find a consensus between the two querying func-
tions without requiring calibration amongst the ac-
tual ranking scores. Defining the rank position of
an instance by r(x), the Borda count based querying
function is stated by
QBorda = argsort
i=1,...,m
[rmargin(xi) + rentropy(xi)]
QBorda selects instances which consider both wide
applicability through rentropy and which focus on
the specific task through rmargin.
4 Experimental Evaluation
To demonstrate the IFSC protocol on a practical ap-
plication, we examine a three-stage pipeline model
for entity and relation extraction, where the task is
decomposed into sequential stages of segmentation,
entity classification, and relation classification (Roth
and Small, 2008). Extending the standard classifi-
cation task, a pipeline model decomposes the over-
all classification into a sequence of D stages such
that each stage d = 1, . . . , D has access to the in-
put instance along with the classifications from all
previous stages, y?(d). Each stage of the pipeline
model uses a feature vector generating procedure
70
?(d)(x, y?(0), . . . , y?(d?1)) ? x(d) to learn a hypoth-
esis h(d). Once each stage of the pipelined classifier
is learned, predictions are made sequentially, where
y? = h(x) =
?
argmax
y??Y(d)
f (d)
(
x(d), y?
)?D
d=1
Each pipeline stage requires a classifier which
makes multiple interdependent predictions based on
input from multiple sentence elements x ? X1 ?
? ? ? ? Xnx using a structured output space, y(d) ?
Y(d)1 ? ? ? ? ? Y
(d)
ny . More specifically, segmenta-
tion makes a prediction for each sentence word over
Y ? {begin, inside, outside} and constraints are
enforced between predictions to ensure that an in-
side label can only follow a begin label. Entity clas-
sification begins with the results of the segmenta-
tion classifier and classifies each segment into Y ?
{person, location, organization}. Finally, rela-
tion classification labels each predicted entity pair
with Y ? {located in, work for, org based in,
live in, kill} ? {left, right}+ no relation.
The data used for empirical evaluation was taken
from (Roth and Yih, 2004) and consists of 1436 sen-
tences, which is split into a 1149 (80%) sentence
training set and a 287 (20%) sentence testing set
such that all have at least one active relation. SR-
WLs are provided by (Pantel and Lin, 2002) and
experiments were conducted using a custom graphi-
cal user interface (GUI) designed specifically for the
IFSC protocol. The learning algorithm used for each
stage of the classification task is a regularized vari-
ant of the structured Perceptron (Collins, 2002). Re-
sources used to perform experiments are available at
http://L2R.cs.uiuc.edu/?cogcomp/.
We extract features in a method similar to (Roth
and Small, 2008), except that we do not include
gazetteer features in ?(d)0 as we will include this
type of external information interactively. Secondly,
we use SRWL features as introduced. The segmen-
tation features include the word/SRWL itself along
with the word/SRWL of three words before and two
words after, bigrams of the word/SRWL surround-
ing the word, capitalization of the word, and capi-
talization of its neighbor on each side. Entity clas-
sification uses the segment size, the word/SRWL
members within the segment, and a window of two
word/SRWL elements on each side. Relation clas-
sification uses the same features as entity classifica-
tion along with the entity labels, the length of the
entities, and the number of tokens between them.
4.1 Interactive Querying Function
When using the interactive feature space construc-
tion protocol for this task, we require a querying
function which captures the hypothesis-driven as-
pect of instance selection. We observed that basing
Qmargin on the relation stage performs best, which
is not surprising given that this stage makes the most
mistakes, benefits the most from semantic informa-
tion, and also has many features which are similar to
features from previous stages. Therefore, we adapt
the querying function described by (Roth and Small,
2008) for the relation classification stage and define
our margin for the purposes of instance selection as
?relation = mini=1,...,ny
[
fy+(x, i)? fy?+(x, i)
]
where y? = argmaxy??Y\y fy?(x), the highest scor-
ing class which is not the true label, and Y+ =
Y\no relation.
4.2 Interactive Protocol on Entire Data Set
The first experiments we conduct uses all available
training data (i.e. |S| = 1149) to examine the im-
provement achieved with a fixed number of IFSC
interactions. A single interaction is defined by the
expert selecting a lexical element from a sentence
presented by the querying function and validating
the associated word list. Therefore, it is possible that
a single sentence may result in multiple interactions.
The results for this experimental setup are sum-
marized in Table 1. For each protocol configura-
tion, we report F1 measure for all three stages of
the pipeline. As our simplest baseline, we first train
using the default feature set without any semantic
features (Lexical Features). The second baseline
is to replace all instances of any lexical element
with its SRWL representation as provided by (Pan-
tel and Lin, 2002) (Semantic Features). The next
two baselines attempt to automatically increase pre-
cision by defining each semantic class using only the
top fraction of the elements in each SRWL (Pruned
Semantic (top {1/2,1/4})). This pruning procedure
often results in smaller SRWLs with a more precise
specification of the semantic concept.
71
Pruned Pruned 50 interactions
Lexical Semantic Semantic Semantic Interactive Interactive
Features Features (top 1/2) (top 1/4) (select only) (select & validate)
Segmentation 90.23 90.14 90.77 89.71 92.24 93.43
Entity Class. 82.17 83.28 83.93 83.04 85.81 88.76
Relation Class. 54.67 55.20 56.34 56.21 59.14 62.08
Table 1: Relative performance of the stated experiments conducted over the entire available dataset. The interactive
feature construction protocol outperforms all non-interactive baselines, particularly for later stages of the pipeline
while requiring only 50 interactions.
Finally, we consider the interactive feature space
construction protocol at two different stages. We
first consider the case where 50 interactions are per-
formed such that the algorithm assumes W? = W ,
that is, the expert selects features for abstraction,
but doesn?t perform validation (Interactive (select
only)). The second experiment performs the entire
protocol, including validation (Interactive (select &
validate)) for 50 interactions. On the relation ex-
traction task, we observe a 13.6% relative improve-
ment over the lexical model and a 10.2% relative im-
provement over the best SRWL baseline F1 score.
4.3 Examination of the Querying Function
As stated in section 3.2, an appropriate querying
function presents sentences which will result in the
expert selecting features from that example and for
which the resulting interactions will result in a large
performance increase. The former is difficult to
model, as it is dependent on properties of the sen-
tence (such as length), will differ from user to user,
and anecdotally is negligibly different for the three
querying functions for earlier interactions. How-
ever, we are able to measure the performance im-
provement of interactions associated with different
querying functions. For our second experiment, we
evaluate the relative performance of the three query-
ing functions defined after every ten interactions in
terms of the F1 measure for relation extraction. The
results of this experiment are shown in figure 4,
where we first see that the Qrandom generally leads
to the least useful interactions. Secondly, while
Qentropy performs well early, Qmargin works bet-
ter as more interactions are performed. Finally, we
also observe that QBorda exceeds the performance
envelope of the two constituent querying functions.
 0.54
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0.62
 0.63
 0  10  20  30  40  50
rel
ati
on
 ex
tra
cti
on
 (F
1)
interactions
QBordaQentropyQmarginQrandom
Figure 4: Relative performance of interactions generated
through the respective querying functions. We see that
Qentropy performs well for a small number of interac-
tions, Qmargin performs well as more interactions are
performed and QBorda outperforms both consistently.
4.4 Robustness to Reduced Annotation
The third set of experiments consider the relative
performance of the configurations from the first set
of experiments as the amount of available training
data is reduced. To study this scenario, we per-
form the same set of experiments with 50 interac-
tions while varying the size of the training set (e.g.
|S| = {250, 500, 600, 675, 750, 1000}), summariz-
ing the results in Figure 5. One observation is that
the interactive feature space construction protocol
outperforms all other configurations at all annota-
tion levels. A second important observation is made
when comparing these results to those presented in
(Roth and Small, 2008), where this data is labeled
using active learning. In (Roth and Small, 2008),
once 65% of the labeled data is observed, a perfor-
mance level is achieved comparable to training on
the entire labeled dataset. In this work, an interpo-
72
lation of the performance at 600 and 675 labeled in-
stances implies that we achieve a performance level
comparable to training on all of the data of the base-
line learner while about 55% of the labeled data is
observed at random. Furthermore, as more labeled
data is introduced, the performance continues to im-
prove with only 50 interactions. This supports the
hypothesis that a good representation is often more
important than additional training data, even when
the data is carefully selected.
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 300  400  500  600  700  800  900  1000
rel
ati
on
 ex
tra
cti
on
 (F
1)
labeled data
Interactive (select & verify) 
Pruned Semantic (top 1/2)
Semantic Features
Lexical Features
Baseline (Lexical Features)
Figure 5: Relative performance of several baseline al-
gorithm configurations and the interactive feature space
construction protocol with variable labeled dataset sizes.
The interactive protocol outperforms other baseline meth-
ods in all cases. Furthermore, the interactive protocol (In-
teractive) outperforms the baseline lexical system (Base-
line) trained on all 1149 sentences even when trained
with a significantly smaller subset of labeled data.
5 Related Work
There has been significant recent work on designing
learning algorithms which attempt to reduce annota-
tion requirements through a more sophisticated an-
notation method. These methods allow the annota-
tor to directly specify information about the feature
space in addition to providing labels, which is then
incorporated into the learning algorithm (Huang and
Mitchell, 2006; Raghavan and Allan, 2007; Zaidan
et al, 2007; Druck et al, 2008; Zaidan and Eisner,
2008). Additionally, there has been recent work us-
ing explanation-based learning techniques to encode
a more expressive feature space (Lim et al, 2007).
Amongst these works, the only interactive learning
protocol is (Raghavan and Allan, 2007) where in-
stances are presented to an expert and features are
labeled which are then emphasized by the learning
algorithm. Thus, in this case, although additional
information is provided the feature space itself re-
mains static. To the best of our knowledge, this is
the first work that interactively modifies the feature
space by abstracting the FGFs.
6 Conclusions and Future Work
This work introduces the interactive feature space
construction protocol, where the learning algorithm
selects examples for which the feature space is be-
lieved to be deficient and uses existing semantic
resources in coordination with a domain expert to
abstract lexical features with their SRWL names.
While the power of SRWL abstraction in terms of
sample complexity is evident, incorporating this in-
formation is fraught with pitfalls regarding the in-
troduction of additional ambiguity. This interactive
protocol finds examples for which the domain ex-
pert will recognize promising semantic abstractions
and for which those semantic abstraction will signif-
icantly improve the performance of the learner. We
demonstrate the effectiveness of this protocol on a
named entity and relation extraction system.
As a relatively new direction, there are many
possibilities for future work. The most immedi-
ate task is effectively quantifying interaction costs
with a user study, including the impact of includ-
ing users with varying levels of expertise. Recent
work on modeling the costs of the active learn-
ing protocol (Settles et al, 2009; Haertel et al,
2009) provides some insight on modeling costs as-
sociated with interactive learning protocols. A sec-
ond potentially interesting direction would be to
incorporate other semantic resources such as lexi-
cal patterns (Hearst, 1992) or Wikipedia-generated
gazetteers (Toral and Mun?oz, 2006).
Acknowledgments
The authors would like to thank Ming-Wei Chang,
Margaret Fleck, Julia Hockenmaier, Alex Klemen-
tiev, Ivan Titov, and the anonymous reviewers for
their valuable suggestions. This work is supported
by DARPA funding under the Bootstrap Learning
Program and by MIAS, a DHS-IDS Center for Mul-
timodal Information Access and Synthesis at UIUC.
73
References
Avrim Blum. 1992. Learning boolean functions in an
infinite attribute space. Machine Learning, 9(4):373?
386.
Lois Boggess, Rajeev Agarwal, and Ron Davis. 1991.
Disambiguation of prepositional phrases in automat-
ically labelled technical text. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 155?159.
Thorsten Brandts and Alex Franz. 2006. Web 1T 5-gram
Version 1.
David Cohn, Les Atlas, and Richard Ladner. 1994. Im-
proving generalization with active learning. Machine
Learning, 15(2):201?222.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of the Conference
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 1?8.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ization expectation criteria. In Proc. of International
Conference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 595?602.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Robbie Haertel, Kevin D. Seppi, Eric K. Ringger, and
James L. Carroll. 2009. Return on investment for
active learning. In NIPS Workshop on Cost Sensitive
Learning.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. the In-
ternational Conference on Computational Linguistics
(COLING), pages 539?545.
Yifen Huang and Tom M. Mitchell. 2006. Text clustering
with extended user feedback. In Proc. of International
Conference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 413?420.
Xin Li and Dan Roth. 2005. Learning question clas-
sifiers: The role of semantic information. Journal of
Natural Language Engineering, 11(4).
Siau Hong Lim, Li-Lun Wang, and Gerald DeJong. 2007.
Explanation-based feature construction. In Proc. of
the International Joint Conference on Artificial Intelli-
gence (IJCAI), pages 931?936.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proc. of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL), pages 337?342.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proc. of the International Con-
ference on Knowledge Discovery and Data Mining
(KDD), pages 613?619.
Hema Raghavan and James Allan. 2007. An interactive
algorithm for asking and incorporating feature feed-
back into support vector machines. In Proc. of Inter-
national Conference on Research and Development in
Information Retrieval (SIGIR), pages 79?86.
Dan Roth and Kevin Small. 2008. Active learning for
pipeline models. In Proceedings of the National Con-
ference on Artificial Intelligence (AAAI), pages 683?
688.
Dan Roth and Wen-Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proc. of the Annual Conference on
Computational Natural Language Learning (CoNLL),
pages 1?8.
Burr Settles, Mark Craven, and Lewis Friedland. 2009.
Active learning with real annotation costs. In NIPS
Workshop on Cost Sensitive Learning.
Simon Tong and Daphne Koller. 2001. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Research,
2:45?66.
Antonio Toral and Rafael Mun?oz. 2006. A proposal
to automatically build and maintain gazetteers using
wikipedia. In Proc. of the Annual Meeting of the
European Association of Computational Linguistics
(EACL), pages 56?61.
H. Peyton Young. 1974. An axiomatization of borda?s
rule. Journal of Economic Theory, 9(1):43?52.
Omar F. Zaidan and Jason Eisner. 2008. Modeling anno-
tators: A generative approach to learning from annota-
tor rationales. In Proc. of the Conference on Empirical
Methods for Natural Language Processing (EMNLP),
pages 31?40.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine
learning for text categorization. In Proc. of the Annual
Meeting of the North American Association of Compu-
tational Linguistics (NAACL), pages 260?267.
74
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 84?92,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Minimally Supervised Model of Early Language Acquisition
Michael Connor
Department of Computer Science
University of Illinois
connor2@uiuc.edu
Yael Gertner
Department of Psychology
University of Illinois
ygertner@cyrus.psych.uiuc.edu
Cynthia Fisher
Department of Psychology
University of Illinois
cfisher@cyrus.psych.uiuc.edu
Dan Roth
Department of Computer Science
University of Illinois
danr@uiuc.edu
Abstract
Theories of human language acquisition as-
sume that learning to understand sentences is
a partially-supervised task (at best). Instead
of using ?gold-standard? feedback, we train
a simplified ?Baby? Semantic Role Labeling
system by combining world knowledge and
simple grammatical constraints to form a po-
tentially noisy training signal. This combina-
tion of knowledge sources is vital for learn-
ing; a training signal derived from a single
component leads the learner astray. When this
largely unsupervised training approach is ap-
plied to a corpus of child directed speech, the
BabySRL learns shallow structural cues that
allow it to mimic striking behaviors found in
experiments with children and begin to cor-
rectly identify agents in a sentence.
1 Introduction
Sentence comprehension involves assigning seman-
tic roles to sentence constituents, determining who
does what to whom. How do young children be-
gin learning to interpret sentences? The structure-
mapping view of early verb and syntax acquisition
proposes that children treat the number of nouns
in the sentence as a cue to its semantic predicate-
argument structure (Fisher, 1996), and represent lan-
guage experience in an abstract format that promotes
generalization to new verbs (Gertner et al, 2006).
Theories of human language acquisition assume
that learning to understand sentences is naturally
a partially-supervised task: the fit of the learner?s
predicted meaning with the referential context and
background knowledge provides corrective feed-
back (e.g., Pinker (1989)). But this feedback must
be noisy; referential scenes provide ambiguous in-
formation about the semantic roles of sentence par-
ticipants. For example, the same participant could
be construed as an agent who ?fled? or as a patient
who is ?chased?.
In this paper, we address this problem by de-
signing a Semantic Role Labeling system (SRL),
equipped with shallow representations of sentence
structure motivated by the structure-mapping ac-
count, that learns with no gold-standard feedback at
all. Instead, the SRL provides its own internally-
generated feedback based on a combination of world
knowledge and linguistic constraints. As a sim-
ple stand-in for world knowledge, we assume that
the learner has animacy information for some set of
nouns, and uses this knowledge to determine their
likely roles. In terms of linguistic constraints, the
learner uses simple knowledge about the possible ar-
guments verbs can appear with.
This approach has two goals. The first is to in-
form theories of language learning by investigating
the utility of the proposed internally-generated feed-
back as one component of the human learner?s tools.
Second, from an NLP and Machine Learning per-
spective we propose to inject information into a su-
pervised learning algorithm through a channel other
than labeled training data. From both perspectives,
our key question is whether the algorithm can use
these internally labeled examples to extract general
patterns that can be applied to new cases.
By building a model that uses shallow representa-
tions of sentences and minimal feedback, but that
84
mimics features of language development in chil-
dren, we can explore the nature of initial representa-
tions of syntactic structure.
1.1 Background
The structure-mapping account of early verb and
syntax acquisition makes strong predictions. First,
it predicts early use of simple structural cues to sen-
tence interpretation. As soon as children can iden-
tify some nouns, they should assign different in-
terpretations to transitive and intransitive sentences,
simply by assuming that each noun in the sentence
bears a distinct semantic role. Similarly, language-
specific syntactic learning should transfer rapidly to
new verbs. Second, however, this account predicts
striking errors. In ?Fred and Ginger danced?, an
intransitive verb occurs with two nouns. If chil-
dren interpret any two-noun sentence as if it were
transitive, they should mistakenly interpret the order
of two nouns in such conjoined-subject intransitive
sentences as agent-patient. Experiments with young
children support these predictions. 21-month-olds
use the number of nouns to understand sentences
containing new verbs (Yuan et al, 2007), generalize
what they have learned about transitive word-order
to new verbs (Gertner et al, 2006), and make the
predicted error, treating intransitive sentences con-
taining two nouns as if they were transitive (Gert-
ner and Fisher, 2006). By 25 months, children have
learned enough about English syntax to interpret
conjoined-subject intransitives differently from tran-
sitives (Naigles, 1990).
Our previous computational experiments with a
system for automatic semantic role labeling (Con-
nor et al, 2008) suggest that it is possible to learn
to assign basic semantic roles based on the simple
representations proposed by the structure-mapping
view. The classifier?s features were limited to lexical
information (nouns and verbs only) and the number
and order of nouns in the sentence, and trained on a
sample of child-directed speech annotated in Prop-
Bank (Kingsbury and Palmer, 2002) style. Given
this training, our classifier learned to label the first
of two nouns as an agent and the second as a patient.
Even amid the variability of casual speech, simply
representing the target word as the first or the second
of two nouns significantly boosts SRL performance
(relative to a lexical baseline) on transitive sentences
containing novel verbs. This result depends on key
assumptions of the structure-mapping view, includ-
ing abstract representations of semantic roles, and
abstract but simple representations of sentence struc-
ture. Another approach was taken by (Alishahi and
Stevenson, 2007). Their model learned to assign se-
mantic roles without prior knowledge of abstract se-
mantic roles. Instead, it relied on built-in syntactic
knowledge and a rich hierarchical representation of
semantic knowledge to learn links between sentence
structure and meaning.
However, our previous experimental design has
a serious drawback that limits its relevance to the
study of how children learn their first language.
In training, our SRL received gold standard feed-
back consisting of correctly labeled sentences. Thus
when the SRL made a mistake in identifying the se-
mantic role of any noun in a sentence, it received
feedback about the ?true? semantic role of this noun.
As noted above, this is an unrealistic assumption for
the input to human learners.
Here we ask whether an SRL could learn to in-
terpret simple sentences even without gold-standard
feedback by relying on world knowledge to gen-
erate its own feedback. This internally-generated
feedback was based on the following assumptions.
First, nouns referring to animate entities are likely
to be agents, and nouns referring to inanimate en-
tities are not. Second, each predicate takes at most
one agent. Such role uniqueness constraints are typ-
ically included in linguistic discussions of thematic
roles (Bresnan, 1982; Carlson, 1998). The animacy
heuristic is not always correct, of course. For ex-
ample, in ?The door hit you?, an inanimate object
is the agent of action, and an animate being is the
patient. Nevertheless, it is useful for two reasons.
First, there is a strong cross-linguistic association
between agency and animacy (Aissen, 1999; Dowty,
1991). Second, from the first year of life, children
have strong expectations about the capacities of an-
imate and inanimate entities (Baillargeon et al, in
press). Given the universal tendency for speakers to
talk about animate action on less animate objects,
many sentences will present useful training data to
the SRL: In ordinary sentences such as ?You broke
it,? feedback generated based on animacy will re-
semble gold-standard feedback.
85
2 Learning Model
Our learning task is similar to the full SRL task (Car-
reras and Ma`rquez, 2004), except that we classify
the roles of individual words rather than full phrases.
A full automatic SRL system (e.g. (Punyakanok et
al., 2005a)) typically involves multiple stages to 1)
parse the input, 2) identify arguments, 3) classify
those arguments, and then 4) run inference to make
sure the final labeling for the full sentence does not
violate any linguistic constraints. Our simplified
BabySRL architecture essentially replaces the first
two steps with developmentally plausible heuris-
tics. Rather than identifying arguments via a learned
classifier with access to a full syntactic parse, the
BabySRL treats each noun in the sentence as a can-
didate argument and assigns a semantic role to it. A
simple heuristic collapsed compound or sequential
nouns to their final noun, an approximation of the
head noun of the noun phrase. For example, ?Mr.
Smith? was treated as the single noun ?Smith?. Other
complex noun phrases were not simplified in this
way. Thus, a phrase such as ?the toy on the floor?
would be treated as two separate nouns, ?toy? and
?floor?. This represents the assumption that young
children know ?Mr. Smith? is a single name, but
they do not know all the predicating terms that may
link multiple nouns into a single noun phrase. The
simplified learning task of the BabySRL implements
a key assumption of the structure-mapping account:
that at the start of multiword sentence comprehen-
sion children can tell which words in a sentence are
nouns (Waxman and Booth, 2001), and treat each
noun as a candidate argument.
We further simplify the SRL task such that clas-
sification is between two macro-roles: A0 (agent)
and A1 (non-agent; all non-A0 arguments). We did
so because we reason that this simplified feedback
scheme can be primarily informative for a first stage
of learning in which learners identify how their lan-
guage identifies agents vs. non-agents in sentences.
In addition, this level of role granularity is more con-
sistent across verbs (Palmer et al, 2005).
For argument classification we use a linear clas-
sifier trained with a regularized perceptron update
rule (Grove and Roth, 2001). This learning algo-
rithm provides a simple and general linear classifier
that works well in other language tasks, and allows
us to inspect the weights of key features to determine
their importance for classification.
For the final predictions, the classifier uses
predicate-level inference to ensure coherent argu-
ment assignments. In our task the only active con-
straints are that all nouns require a tag, and that they
have unique labels, which for this restricted case of
A0 vs. not A0 means there will be only one agent.
2.1 Training and Feedback
The key feature of our BabySRL lies in the way
feedback is provided. Ordinarily, during training,
SRL classifiers predict a semantic label for an argu-
ment and receive gold-standard feedback about its
correct semantic role. Such accurate feedback is not
available for the child learner. Children must rely on
their own error-prone interpretation of events to sup-
ply feedback. This internally-generated feedback
signal is presumably derived from multiple infor-
mation sources, including the plausibility of partic-
ular combinations of argument-roles given the cur-
rent situation (Chapman and Kohn, 1978). Here
we model this process by combining background
knowledge with linguistic constraints to generate
a training signal. The ?unsupervised? feedback is
based on: 1) nouns referring to animate entities are
assumed to be agents, while nouns referring to inan-
imate entities are non-agents and 2) each predicate
can have at most one agent.
This internally-generated feedback bears some
similarities to Inference Based Training (Pun-
yakanok et al, 2005b). In both cases the feedback to
local supervised classifiers depends on global con-
straints. With IBT, feedback for mistakes is only
considered after global inference, but for BabySRL
the global inference is applied to the feedback itself.
Figure 1 gives an overview of the training and test-
ing procedure, making clear the distinction between
training and testing inference.
The training data were samples of parental speech
to one child (?Sarah?; (Brown, 1973), available
via Childes (MacWhinney, 2000)). We trained
on parental utterances in samples 1 through 80,
recorded at child age 2;3-3;10 years. All verb-
containing utterances without symbols indicating
long pauses or unintelligible words were automat-
ically parsed with the Charniak parser (Charniak,
1997) and annotated using an existing SRL sys-
86
tem (Punyakanok et al, 2005a). In this initial
pass, sentences with parsing errors that misidenti-
fied argument boundaries were excluded. Role la-
bels were hand-corrected using the PropBank anno-
tation scheme. The child-directed speech training
set consists of about 8300 tagged arguments over
4700 sentences, of which a majority had a single
verb and two labeled nouns1. The annotator agree-
ment on this data set ranged between 95-97% at the
level of arguments. In the current paper these role-
tagged examples provide a comparison point for the
utility of animacy-based feedback during training.
Our BabySRL did not receive these hand-
corrected semantic roles during training. Instead,
for each training example it generated its own feed-
back based in part on an animacy table. To ob-
tain the animacy table we coded the 100 most fre-
quent nouns in our corpus (which constituted less
than 15% of the total number of nouns, but 65%
of noun occurrences). We considered 84 of these
nouns to be unambiguous in animacy: Personal pro-
nouns and nouns referring to people were coded as
animate (30). Nouns referring to objects, body parts,
locations, and times, were coded as inanimate (54).
The remaining 16 nouns were excluded because they
were ambiguous in animacy (e.g., dolls, actions).
We test 3 levels of feedback representing increas-
ing amounts of linguistic knowledge used to gener-
ate internal interpretations of the sentences. Using
the animacy table, Animacy feedback (Feedback 1)
was generated as follows: for each noun in training,
if it was coded as animate it was labeled A0, if it was
coded as inanimate it was labeled A1, otherwise no
feedback was given. Because of the frequency of an-
imate nouns this gives a skewed distribution of 4091
animate agents and 1337 inanimate non-agents.
(Feedback 2) builds on Feedback 1 by adding an-
other linguistic constraint: if a noun was not found
in the animacy-table and there is another noun in the
sentence that is labeled A0, then the unknown noun
is an A1. In the training set this adds non-agent
training examples, yielding 4091 A0 and 2627 A1
examples.
Feedback 1 and Feedback 2 allow two nouns in
a sentence to be labeled with A0. Feedback 3 pre-
1Corpus available at http://l2r.cs.uiuc.edu/
?
cogcomp
vents this; it implements a unique agent constraint
that incorporates bootstrapping to make an ?intelli-
gent guess? about which noun is the correct agent.
This decision is made based on the current predic-
tions of the classifier. Given a sentence with multi-
ple animate nouns, the classifier predicts a label for
each, and the one with the highest score for A0 is
declared the true agent and the rest are classified as
non-agent. Note that we cannot apply role unique-
ness to the A1 (not A0) role, given that this label en-
compasses multiple non-agent roles. This feedback
scheme, allowing at most one agent per sentence, re-
duces the number of A0 examples and increases the
number of A1 examples to 3019 A0 and 3699 A1.
2.2 Feature Sets
The basic feature we propose is the noun pattern fea-
ture (NPattern). We hypothesize that children use
the number and order of nouns to represent argument
structure. The NPattern feature indicates how many
nouns there are in the sentence and which noun the
target is. For example, in the two-noun sentence
?Did you see it??, ?you? has a feature active indicat-
ing that it is the first noun of two. Likewise, for ?it? a
feature is active indicating that it is the second of two
nouns. This feature is easy to compute once nouns
are identified, and does not require fine-grained part-
of-speech distinctions.
We compare the noun pattern feature to a baseline
lexical feature set (Words): the target noun and the
root form of the predicate. The NPattern feature set
includes lexical features as well as features indicat-
ing the number and order of the noun (first of two,
second of three, etc.). With gold-standard role feed-
back, (Connor et al, 2008) found that the NPattern
feature allowed the BabySRL to generalize to new
verbs: it increased the system?s tendency to predict
that the first of two nouns was A0 and the second of
two nouns A1 for verbs not seen in training.
To the extent that in child-directed speech the first
of two nouns tends to be an agent, and agents tend
to be animate, we anticipate that with the NPat-
tern feature the BabySRL will learn the same thing,
even when provided with internally-generated feed-
back based on animacy. In Connor et al (2008) we
showed that, because this NPattern feature set repre-
sents only the number and order of nouns, with this
feature set the BabySRL reproduced the errors chil-
87
Algorithm BABYSRL TRAINING
INPUT: Unlabeled Training Sentences
OUTPUT: Trained Argument Classifier
For each training sentence
Generate Internal Feedback: Find interpreted meaning
Feedback 1: Apply Animacy Heuristic
For each argument in the sentence (noun)
If noun is animate? mark as agent
If noun is inanimate? mark as non-agent
else leave unknown
end
Feedback 2: Known agent constraint
Beginning with Feedback 1
If an agent was found
Mark all unknown arguments as non-agent
Feedback 3: Unique agent constraint
Beginning with Feedback 2
If multiple agents found
Find argument with highest agent prediction
Leave this argument an agent, mark rest as non-agent
Train Supervised Classifier
Present each argument to classifier
Update if interpreted meaning does not match
classifier prediction
end
(a) Training
Algorithm BABYSRL TESTING
INPUT: Unlabeled Testing Sentences
OUTPUT: Role labels for each argument
For each test sentence
Predict roles for each argument
Test Inference:
Find assignment to whole sentence with highest sum of
predictions that doesn?t violate uniqueness constraint
end
(b) Testing
Figure 1: BabySRL training and testing procedures. In-
ternal feedback is generated using animacy plus optional
constraints. This feedback is fed to a supervised learning
algorithm to create an agent-identification classifier.
dren make as noted in the Introduction, mistakenly
assigning agent- and non-agent roles to the first and
second nouns in intransitive test sentences contain-
ing two nouns. In the present paper, the linguistic
constraints provide an additional cause for this er-
ror. In addition, as a first step in examining recov-
ery from the predicted error, Connor et al (2008)
added a verb position feature (VPosition) specifying
whether the target noun is before or after the verb.
Given these features, the BabySRL?s classification
of transitive and two-noun intransitive test sentences
diverged, because the gold-standard training sup-
ported the generalization that pre-verbal nouns tend
to be agents, and post-verbal nouns tend to be pa-
tients. In the present paper we include the VPosition
feature for comparison to Connor et al (2008).
2.3 Testing
To evaluate the BabySRL we tested it with both a
held-out sample of child-directed speech, and with
constructed sentences containing novel verbs, like
those used in the experiments with children de-
scribed above. These sentences provide a more
stringent test of generalization than the customary
test on a held-out section of the data. Although the
held-out section of data contains unseen sentences,
it may contain few unseen verbs. In a held out sec-
tion of our data, 650 out of 696 test examples contain
a verb that was encountered in training. Therefore,
the customary test cannot tell us whether the system
generalizes what it learned to novel verbs.
All constructed test sentences contained a novel
verb (?gorp?). We constructed two test sentence tem-
plates: ?A gorps B? and ?A and B gorp?, where A and
B were replaced with nouns that appeared more than
twice in training. For each test sentence template we
built a test set of 100 sentences by randomly sam-
pling nouns in two different ways described next.
Full distribution: The first nouns in the test sen-
tences (A) are chosen from the set of all first nouns
in our corpus, taking their frequency into account
when sampling. The second nouns in the sentences
(B) are chosen from the set of nouns appearing as
second nouns in the sentence of our corpus. This
way of sampling the nouns will maximize the SRL?s
test performance based on the baseline feature set
of lexical information alone (Words). This is so be-
cause in our data many sentences have an animate
first noun and an inanimate second noun. Based on
these words alone the SRL could learn to predict an
A0-A1 role sequence for our test sentences. Nev-
ertheless, we expect that when the BabySRL is also
given the NPattern feature it should be able to per-
form better than this high lexical baseline.
Two animate nouns: In these test sentences the
A and B nouns are chosen from our list of animate
nouns. We chose nouns from this list that were
fairly frequent (ranging from 8 to 240 uses in the
88
corpus), and that occurred roughly equally as the
first and second noun. This mimics the sentences
used in the experiments with children (e.g., ?The
girl is kradding the boy!?). The lexical baseline sys-
tem?s tendency to assign an A0-A1 sequence to these
nouns should be much lower for these test sentences.
We therefore expect the contribution of the NPattern
feature to be more apparent in these test sentences.
The test sentences with novel verbs ask whether
the classifier transfers its learning about argument
role assignment to unseen verbs. Does it assume
the first of two nouns in a simple transitive sentence
(?A gorps B?) is the agent (A0) and the second is
not an agent (A1)? In (Connor et al, 2008) we
showed that a system with the same feature and rep-
resentations also over-generalized this rule to two-
noun intransitives (?A and B gorp?), mimicking chil-
dren?s behavior. In the present paper this error is
over-determined, because the classifier learns only
an agent/non-agent contrast, and the linguistic con-
straints forbid duplicate agents in a sentence. How-
ever, for comparison to the earlier paper we test our
system on the ?A and B gorp? sentences as well.
3 Experimental Results
Our experiments use internally-generated feedback
to train simple, abstract structural features: the
NPattern features that proved useful with gold-
standard training in Connor et al (2008). Sec-
tion 3.1 tests the system on agent-identification in
held-out sentences from the corpus, and demon-
strates that the animacy-based feedback is useful,
yielding SRL performance comparable to that of a
system trained with 1000 sentences of gold-standard
feedback. Section 3.2 presents the critical novel-
verb test data, demonstrating that this system repli-
cates key findings of (Connor et al, 2008) with no
gold standard feedback. Using only noisy internally-
generated feedback, the BabySRL learned that the
first of two nouns is an agent, and generalized this
knowledge to sentences with novel verbs.
3.1 Comparing Self Generated Feedback with
Gold Standard Feedback
Table 1 reports for the varying feedback schemes,
the A0 F1 performance for a system with either lex-
ical baseline feature (Words) or structural features
Feedback Words +NPattern
1. Just Animacy 0.72 0.73
2. + non A0 Inference 0.74 0.75
3. + unique A0 bootstrap 0.70 0.74
10 Gold 0.43 0.47
100 Gold 0.61 0.65
1000 Gold 0.75 0.76
Table 1: Agent identification results (A0 F1) on held-
out sections of the Sarah Childes corpus. We compare
a classifier trained with various amounts of gold labeled
data (averaging over 10 different samples at each level
of data). For noun pattern features the internally gener-
ated bootstrap feedback provides comparable accuracy to
training with between 100-1000 fully labeled examples.
(+NPattern) when tested on a held-out section of
the Sarah Childes corpus section 84-90, recorded
at child ages 3;11-4;1 years. Agent identification
based on lexical features is quite accurate given an-
imacy feedback alone (Feedback 1). As expected,
because many agents are animate, the animacy tag-
ging heuristic itself is useful. As linguistic con-
straints are added via non-A0 inference (Feedback
2), performance increases for both the lexical base-
line and NPattern feature-set, because the system ex-
periences more non-A0 training examples.
When the unique A0 constraint is added (Feed-
back 3), the lexical baseline performance decreases,
because for the first time animate nouns are being
tagged as non-agents. With this feedback the NPat-
tern feature set yields a larger improvement over lex-
ical baseline, showing that it extracts more general
patterns. We discuss the source of these feedback
differences in the novel-verb test section below.
We compared the usefulness of the internally-
generated feedback to gold-standard feedback by
training a classifier equipped with the same features
on labeled sentences. We reduced the SRL labeling
for the training sentences to the binary agent/non-
agent set, and trained the classifier with 10, 100,
or 1000 labeled examples. Surprisingly, the simple
feedback derived from 84 nouns labeled with ani-
macy information yields performance equivalent to
between 100 and 1000 hand-labeled examples.
89
Full Distribution Nouns Animate Nouns
Feedback Words NPattern VPosition Words NPattern VPosition
?A gorps B?
1. Animacy 0.86 0.86 0.87 0.76 0.79 0.70
2. + non A0 Inference 0.87 0.92 0.90 0.63 0.86 0.85
3. + unique A0 bootstrap 0.87 0.95 0.89 0.63 0.82 0.66
?A and B gorp?
1. Animacy 0.86 0.86 0.84 0.76 0.79 0.68
2. + non A0 Inference 0.87 0.92 0.85 0.63 0.86 0.66
3. + unique A0 bootstrap 0.87 0.95 0.86 0.63 0.82 0.63
Table 2: Percentage of sentences interpreted as agent first (%A0-A1) by the BabySRL when trained on unlabeled data
with the 3 internally-generated feedback schemes described in the text. Two different two-noun sentence structures
were used (?A gorps B?, ?A and B gorp?), along with two different methods of sampling the nouns (Full Distribution,
Animate Nouns) to create test sets with 100 sentences each.
3.2 Comparing Structural Features with
Lexical Features
The previous section shows that the BabySRL
equipped with simple structural features can use
internally generated feedback to learn a simple
agent/non-agent classification, and apply it to un-
seen sentences. In this section we probe what the
SRL has learned by testing generalization to new
verbs in constructed sentences. Table 2 summarizes
these experiments. The results are broken down both
by what sentence structure is used in test (?A gorps
B?, ?A and B gorp?) and how the nouns ?A? and
?B? are sampled (Full Distribution, Animate Nouns).
The results are presented in terms of %A0A1: the
percentage of test sentences that are assigned an
Agent role for ?A? and a non-Agent role for ?B?.
For the transitive ?A gorps B? sentences, A0A1 is
the correct interpretation; A should be the agent. As
predicted, when A and B are sampled from the full
distribution of nouns, simply basing classification on
the Words feature-set aleady strongly predicts this
A0A1 ordering for the majority of cases. This is be-
cause the data (language in general, child directed
speech in particular here) are naturally distributed
such that particular nouns that refer to animates tend
to be agents, and tend to appear as first nouns, and
those that refer to inanimates tend to be non-agents
and second nouns. Thus, a learner representing sen-
tence information in terms of words only succeeds
with full-distribution ?A gorps B? test sentences even
with the simplest animacy feedback (Feedback 1);
the A and B nouns in these test sentences reproduce
the learned distribution. Also as predicted, given this
simple feedback, the additional higher-level features
(NPattern, VPosition) do not improve much upon
the lexical baseline. This is due to the strictly lexical
nature of the animacy feedback: each lexical item
(e.g., ?you? or ?it?) will always either be animate or
inanimate and therefore either A0 or A1. Therefore,
in this case lexical features are the best predictors.
Also as expected, higher-level features (NPat-
tern, and VPosition) improve performance with a
more sophisticated self-generated feedback scheme.
Adding inferred feedback to label unknown nouns
as A1 when the sentence contains a known animate
noun (Feedback 2) decreases the ratio of A0 to non-
A0 arguments. This feedback is less lexically deter-
mined: for nouns whose animacy is unknown, feed-
back will be provided only if there is another ani-
mate noun in the sentence. This leaves room for the
abstract structural features to play a role.
Next we test a form of the unique-A0 constraint.
In (Feedback 3), in addition to the non-A0 inference
added in (Feedback 2), the BabySRL intelligently
selects one noun as A0 in sentences with multiple
animate nouns. With this feedback we see a striking
increase in test performance based on the noun pat-
tern features over the lexical baseline. In principle,
this feedback mechanism might permit the classifier
to start to learn that animate nouns are not always
agents. Early in training, the noun pattern feature
learns that first nouns tend to be animate (and there-
fore interpreted as agents), and it feeds this informa-
90
tion back into subsequent training examples, gen-
erating new feedback that continues to interpret as
agents those animate nouns that appear first in sen-
tences containing two animates.
For the nouns sampled from the full distribution
we see that structural features improve over the lex-
ical baseline despite the high performance of the
lexical baseline. This finding tells us that simple
representations of sentence structure can be use-
ful in learning to interpret sentences even with no
gold-standard training. Provided only with sim-
ple internally-generated feedback based on animacy
knowledge and linguistic constraints, the BabySRL
learned that the first of two nouns tends to be an
agent, and the second of two does not.
The results for the ?A B gorp? test sentences
demonstrate an important way in which predictions
based on different simple structure representations
can diverge. As expected, the NPattern feature
makes the same overgeneralization error seen by
children and the system in (Connor et al, 2008).
However, when the VPosition feature is added, dif-
ferent results are obtained for the ?A gorp B? and
?A and B gorp? sentences. The SRL predicts fewer
A0A1 for ?A and B gorp? (it cannot predict the ex-
pected A0A0 because of the uniqueness constraint
used in test inference).
Next, we replicate our findings by performing the
same experiments with test sentences in which both
?A? and ?B? are animate. Because lexical features
alone cannot determine if ?A? or ?B? should be the
agent, it is a more sensitive test of generalization.
When we look at the lexical baseline for animate
sentences, the agent-first percentage is lower com-
pared to the full distribution results, because the
word features indicate nearly evenly that both nouns
should be agents, so the Words baseline model must
rely on small, chance differences in its experience
with particular words. This percentage is still well
above chance due to the method used to apply in-
ference during testing. Recall that the classifier uses
predicate-level inference at test to ensure that only
one argument is labeled A0. This inference is imple-
mented using a beam search that looks at arguments
in a fixed order and roles from A0 up. Thus in the
case of ties there is a preference for first seen solu-
tions, meaning A0A1 in this case. This bias has a
large effect on the SRL?s baseline performance with
the test sentences containing two animate nouns.
Despite this high baseline, however, because lexical
features alone cannot determine if ?A? or ?B? should
be the agent, we are able to see more clearly the im-
provement gained by including structural features.
Regardless of our testing scheme, we see that as
the feedback incorporates more information, both
added linguistic constraints and the SRL?s own prior
learning, the noun pattern structural feature is better
used to identify agents beyond the lexical baseline.
The largest improvement over this lexical baseline is
obtained by combining knowledge of animacy with
a single-agent constraint and bootstrapping predic-
tions based on prior learning.
4 Conclusion and Future Work
Conventional approaches to supervised learning re-
quire creating large amounts of hand-labeled data.
This is labor-intensive, and limits the relevance of
the work to the study of how children learn lan-
guages. Children do not receive perfect feedback
about sentence interpretation. Here we found that
our simple SRL classifier can, to a surprising de-
gree, attain performance comparable to training with
1000 sentences of labeled data. This suggests that
fully labeled training data can be supplemented by a
combination of simple world knowledge (animates
make good agents) and linguistic constraints (each
verb has only one agent). The combination of these
sources provides an informative training signal that
allows our BabySRL to learn a high-level seman-
tic task and generalize beyond the training data we
provided to it. The SRL learned, based on the dis-
tribution of animates in sentences of child-directed
speech, that the first of two nouns tends to be an
agent. It did so based on representations of sentence
structure as simple as the ordered set of nouns in
the sentence. This demonstrates that it is possible to
learn how to correctly assign semantic roles based
on these very simple cues. This together with exper-
imental work (e.g. (Fisher, 1996) suggests that such
representations might play a role in children?s early
sentence comprehension.
Acknowledgments
This research is supported by NSF grant BCS-
0620257 and NIH grant R01-HD054448.
91
References
J. Aissen. 1999. Markedness and subject choice in opti-
mality theory. Natural Language and Linguistic The-
ory, 17:673?711.
A. Alishahi and S. Stevenson. 2007. A computational
usage-based model for learning general properties of
semantic roles. In Proceedings of the 2nd European
Cognitive Science Conference.
R. Baillargeon, D. Wu, S. Yuan, J. Li, and Y. Luo.
(in press). Young infants expectations about self-
propelled objects. In B. Hood and L. Santos, editors,
The origins of object knowledge. Oxford University
Press, Oxford.
J. Bresnan. 1982. The mental representation of gram-
matical relations. MIT Press, Cambridge MA.
R. Brown. 1973. A First Language. Harvard University
Press, Cambridge, MA.
G. Carlson. 1998. Thematic roles and the individuation
of events. In S. D. Rothstein, editor, Events and Gram-
mar, pages 35?51. Kluwer, Dordrecht.
X. Carreras and L. Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared tasks: Semantic role labeling. In
Proceedings of CoNLL-2004, pages 89?97. Boston,
MA, USA.
R. S. Chapman and L. L. Kohn. 1978. Comprehension
strategies in two- and three-year-olds: Animate agents
or probable events? Journal of Speech and Hearing
Research, 21:746?761.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. National
Conference on Artificial Intelligence.
M. Connor, Y. Gertner, C. Fisher, and D. Roth. 2008.
Baby srl: Modeling early language acquisition. In
Proc. of the Annual Conference on Computational
Natural Language Learning (CoNLL), Aug.
D. Dowty. 1991. Thematic proto-roles and argument se-
lection. Language, 67:547?619.
C. Fisher. 1996. Structural limits on verb mapping:
The role of analogy in children?s interpretation of sen-
tences. Cognitive Psychology, 31:41?81.
Y. Gertner and C. Fisher. 2006. Predicted errors in early
verb learning. In 31st Annual Boston University Con-
ference on Language Development.
Y. Gertner, C. Fisher, and J. Eisengart. 2006. Learning
words and rules: Abstract knowledge of word order
in early sentence comprehension. Psychological Sci-
ence, 17:684?691.
A. Grove and D. Roth. 2001. Linear concepts and hidden
variables. Machine Learning, 42(1/2):123?141.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proceedings of LREC-2002, Spain.
B. MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Third Edition. Lawrence Elrbaum
Associates, Mahwah, NJ.
L. R. Naigles. 1990. Children use syntax to learn verb
meanings. Journal of Child Language, 17:357?374.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. In Computational Linguistics 31(1).
S. Pinker. 1989. Learnability and Cognition. Cam-
bridge: MIT Press.
V. Punyakanok, D. Roth, and W. Yih. 2005a. The neces-
sity of syntactic parsing for semantic role labeling. In
Proc. of the International Joint Conference on Artifi-
cial Intelligence (IJCAI), pages 1117?1123.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005b.
Learning and inference over constrained output. In
Proc. of the International Joint Conference on Artifi-
cial Intelligence (IJCAI), pages 1124?1129.
S. R. Waxman and A. Booth. 2001. Seeing pink ele-
phants: Fourteen-month-olds?s interpretations of novel
nouns and adjectives. Cognitive Psychology, 43:217?
242.
S. Yuan, C. Fisher, Y. Gertner, and J. Snedeker. 2007.
Participants are more than physical bodies: 21-month-
olds assign relational meaning to novel transitive
verbs. In Biennial Meeting of the Society for Research
in Child Development, Boston, MA.
92
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 147?155,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Design Challenges and Misconceptions in Named Entity Recognition? ? ?
Lev Ratinov Dan Roth
Computer Science Department
University of Illinois
Urbana, IL 61801 USA
{ratinov2,danr}@uiuc.edu
Abstract
We analyze some of the fundamental design
challenges and misconceptions that underlie
the development of an efficient and robust
NER system. In particular, we address issues
such as the representation of text chunks, the
inference approach needed to combine local
NER decisions, the sources of prior knowl-
edge and how to use them within an NER
system. In the process of comparing several
solutions to these challenges we reach some
surprising conclusions, as well as develop an
NER system that achieves 90.8 F1 score on
the CoNLL-2003 NER shared task, the best
reported result for this dataset.
1 Introduction
Natural Language Processing applications are char-
acterized by making complex interdependent deci-
sions that require large amounts of prior knowledge.
In this paper we investigate one such application?
Named Entity Recognition (NER). Figure 1 illus-
trates the necessity of using prior knowledge and
non-local decisions in NER. In the absence of mixed
case information it is difficult to understand that
? The system and the Webpages dataset are available at:
http://l2r.cs.uiuc.edu/?cogcomp/software.php
? This work was supported by NSF grant NSF SoD-HCER-
0613885, by MIAS, a DHS-IDS Center for Multimodal In-
formation Access and Synthesis at UIUC and by an NDIIPP
project from the National Library of Congress.
? We thank Nicholas Rizzolo for the baseline LBJ NER
system, Xavier Carreras for suggesting the word class models,
and multiple reviewers for insightful comments.
SOCCER - [PER BLINKER] BAN LIFTED .
[LOC LONDON] 1996-12-06 [MISC Dutch] forward
[PER Reggie Blinker] had his indefinite suspension
lifted by [ORG FIFA] on Friday and was set to make
his [ORG Sheffield Wednesday] comeback against
[ORG Liverpool] on Saturday . [PER Blinker] missed
his club?s last two games after [ORG FIFA] slapped a
worldwide ban on him for appearing to sign contracts for
both [ORG Wednesday] and [ORG Udinese] while he was
playing for [ORG Feyenoord].
Figure 1: Example illustrating challenges in NER.
?BLINKER? is a person. Likewise, it is not obvi-
ous that the last mention of ?Wednesday? is an orga-
nization (in fact, the first mention of ?Wednesday?
can also be understood as a ?comeback? which hap-
pens on Wednesday). An NER system could take ad-
vantage of the fact that ?blinker? is also mentioned
later in the text as the easily identifiable ?Reggie
Blinker?. It is also useful to know that Udinese
is a soccer club (an entry about this club appears
in Wikipedia), and the expression ?both Wednesday
and Udinese? implies that ?Wednesday? and ?Udi-
nese? should be assigned the same label.
The above discussion focuses on the need for ex-
ternal knowledge resources (for example, that Udi-
nese can be a soccer club) and the need for non-
local features to leverage the multiple occurrences
of named entities in the text. While these two needs
have motivated some of the research in NER in
the last decade, several other fundamental decisions
must be made. These include: what model to use for
147
sequential inference, how to represent text chunks
and what inference (decoding) algorithm to use.
Despite the recent progress in NER, the effort has
been dispersed in several directions and there are no
published attempts to compare or combine the re-
cent advances, leading to some design misconcep-
tions and less than optimal performance. In this
paper we analyze some of the fundamental design
challenges and misconceptions that underlie the de-
velopment of an efficient and robust NER system.
We find that BILOU representation of text chunks
significantly outperforms the widely adopted BIO.
Surprisingly, naive greedy inference performs com-
parably to beamsearch or Viterbi, while being con-
siderably more computationally efficient. We ana-
lyze several approaches for modeling non-local de-
pendencies proposed in the literature and find that
none of them clearly outperforms the others across
several datasets. However, as we show, these contri-
butions are, to a large extent, independent and, as we
show, the approaches can be used together to yield
better results. Our experiments corroborate recently
published results indicating that word class models
learned on unlabeled text can significantly improve
the performance of the system and can be an al-
ternative to the traditional semi-supervised learning
paradigm. Combining recent advances, we develop
a publicly available NER system that achieves 90.8
F1 score on the CoNLL-2003 NER shared task, the
best reported result for this dataset. Our system is ro-
bust ? it consistently outperforms all publicly avail-
able NER systems (e.g., the Stanford NER system)
on all three datasets.
2 Datasets and Evaluation Methodology
NER system should be robust across multiple do-
mains, as it is expected to be applied on a diverse set
of documents: historical texts, news articles, patent
applications, webpages etc. Therefore, we have con-
sidered three datasets: CoNLL03 shared task data,
MUC7 data and a set of Webpages we have anno-
tated manually. In the experiments throughout the
paper, we test the ability of the tagger to adapt to new
test domains. Throughout this work, we train on the
CoNLL03 data and test on the other datasets without
retraining. The differences in annotation schemes
across datasets created evaluation challenges. We
discuss the datasets and the evaluation methods be-
low.
The CoNLL03 shared task data is a subset of
Reuters 1996 news corpus annotated with 4 entity
types: PER,ORG, LOC, MISC. It is important to
notice that both the training and the development
datasets are news feeds from August 1996, while the
test set contains news feeds from December 1996.
The named entities mentioned in the test dataset are
considerably different from those that appear in the
training or the development set. As a result, the test
dataset is considerably harder than the development
set. Evaluation: Following the convention, we re-
port phrase-level F1 score.
The MUC7 dataset is a subset of the North
American News Text Corpora annotated with a wide
variety of entities including people, locations, or-
ganizations, temporal events, monetary units, and
so on. Since there was no direct mapping from
temporal events, monetary units, and other entities
from MUC7 and the MISC label in the CoNLL03
dataset, we measure performance only on PER,ORG
and LOC. Evaluation: There are several sources
of inconsistency in annotation between MUC7 and
CoNLL03. For example, since the MUC7 dataset
does not contain the MISC label, in the sentence
?balloon, called the Virgin Global Challenger? , the
expression Virgin Global Challenger should be la-
beled as MISC according to CoNLL03 guidelines.
However, the gold annotation in MUC7 is ?balloon,
called the [ORG Virgin] Global Challenger?. These
and other annotation inconsistencies have prompted
us to relax the requirements of finding the exact
phrase boundaries and measure performance using
token-level F1.
Webpages - we have assembled and manually an-
notated a collection of 20 webpages, including per-
sonal, academic and computer-science conference
homepages. The dataset contains 783 entities (96-
loc, 223-org, 276-per, 188-misc). Evaluation: The
named entities in the webpages were highly am-
biguous and very different from the named entities
seen in the training data. For example, the data in-
cluded sentences such as : ?Hear, O Israel, the Lord
our God, the Lord is one.? We could not agree on
whether ?O Israel? should be labeled as ORG, LOC,
or PER. Similarly, we could not agree on whether
?God? and ?Lord? is an ORG or PER. These issues
148
led us to report token-level entity-identification F1
score for this dataset. That is, if a named entity to-
ken was identified as such, we counted it as a correct
prediction ignoring the named entity type.
3 Design Challenges in NER
In this section we introduce the baseline NER sys-
tem, and raise the fundamental questions underlying
robust and efficient design. These questions define
the outline of this paper. NER is typically viewed
as a sequential prediction problem, the typical mod-
els include HMM (Rabiner, 1989), CRF (Lafferty
et al, 2001), and sequential application of Per-
ceptron or Winnow (Collins, 2002). That is, let
x = (x1, . . . , xN ) be an input sequence and y =
(y1, . . . , yN ) be the output sequence. The sequential
prediction problem is to estimate the probabilities
P (yi|xi?k . . . xi+l, yi?m . . . yi?1),
where k, l and m are small numbers to allow
tractable inference and avoid overfitting. This con-
ditional probability distribution is estimated in NER
using the following baseline set of features (Zhang
and Johnson, 2003): (1) previous two predictions
yi?1 and yi?2 (2) current word xi (3) xi word type
(all-capitalized, is-capitalized, all-digits, alphanu-
meric, etc.) (4) prefixes and suffixes of xi (5) tokens
in the window c = (xi?2, xi?1, xi, xi+1, xi+2) (6)
capitalization pattern in the window c (7) conjunc-
tion of c and yi?1.
Most NER systems use additional features, such
as POS tags, shallow parsing information and
gazetteers. We discuss additional features in the fol-
lowing sections. We note that we normalize dates
and numbers, that is 12/3/2008 becomes *Date*,
1980 becomes *DDDD* and 212-325-4751 becomes
*DDD*-*DDD*-*DDDD*. This allows a degree of ab-
straction to years, phone numbers, etc.
Our baseline NER system uses a regularized aver-
aged perceptron (Freund and Schapire, 1999). Sys-
tems based on perceptron have been shown to be
competitive in NER and text chunking (Kazama and
Torisawa, 2007b; Punyakanok and Roth, 2001; Car-
reras et al, 2003) We specify the model and the fea-
tures with the LBJ (Rizzolo and Roth, 2007) mod-
eling language. We now state the four fundamental
design decisions in NER system which define the
structure of this paper.
Algorithm Baseline system Final System
Greedy 83.29 90.57
Beam size=10 83.38 90.67
Beam size=100 83.38 90.67
Viterbi 83.71 N/A
Table 1: Phrase-level F1 performance of different inference
methods on CoNLL03 test data. Viterbi cannot be used in the
end system due to non-local features.
Key design decisions in an NER system.
1) How to represent text chunks in NER system?
2) What inference algorithm to use?
3) How to model non-local dependencies?
4) How to use external knowledge resources in NER?
4 Inference & Chunk Representation
In this section we compare the performance of sev-
eral inference (decoding) algorithms: greedy left-
to-right decoding, Viterbi and beamsearch. It may
appear that beamsearch or Viterbi will perform
much better than naive greedy left-to-right decoding,
which can be seen as beamsearch of size one. The
Viterbi algorithm has the limitation that it does not
allow incorporating some of the non-local features
which will be discussed later, therefore, we cannot
use it in our end system. However, it has the appeal-
ing quality of finding the most likely assignment to
a second-order model, and since the baseline fea-
tures only have second order dependencies, we have
tested it on the baseline configuration.
Table 1 compares between the greedy decoding,
beamsearch with varying beam size, and Viterbi,
both for the system with baseline features and for the
end system (to be presented later). Surprisingly, the
greedy policy performs well, this phenmenon was
also observed in the POS tagging task (Toutanova
et al, 2003; Roth and Zelenko, 1998). The impli-
cations are subtle. First, due to the second-order of
the model, the greedy decoding is over 100 times
faster than Viterbi. The reason is that with the
BILOU encoding of four NE types, each token can
take 21 states (O, B-PER, I-PER , U-PER, etc.). To
tag a token, the greedy policy requires 21 compar-
isons, while the Viterbi requires 213, and this analy-
sis carries over to the number of classifier invoca-
tions. Furthermore, both beamsearch and Viterbi
require transforming the predictions of the classi-
149
Rep. CoNLL03 MUC7
Scheme Test Dev Dev Test
BIO 89.15 93.61 86.76 85.15
BILOU 90.57 93.28 88.09 85.62
Table 2: End system performance with BILOU and BIO
schemes. BILOU outperforms the more widely used BIO.
fiers to probabilities as discussed in (Niculescu-
Mizil and Caruana, 2005), incurring additional time
overhead. Second, this result reinforces the intuition
that global inference over the second-order HMM
features does not capture the non-local properties
of the task. The reason is that the NEs tend to
be short chunks separated by multiple ?outside? to-
kens. This separation ?breaks? the Viterbi decision
process to independent maximization of assignment
over short chunks, where the greedy policy performs
well. On the other hand, dependencies between iso-
lated named entity chunks have longer-range depen-
dencies and are not captured by second-order tran-
sition features, therefore requiring separate mecha-
nisms, which we discuss in Section 5.
Another important question that has been stud-
ied extensively in the context of shallow parsing and
was somewhat overlooked in the NER literature is
the representation of text segments (Veenstra, 1999).
Related works include voting between several rep-
resentation schemes (Shen and Sarkar, 2005), lex-
icalizing the schemes (Molina and Pla, 2002) and
automatically searching for best encoding (Edward,
2007). However, we are not aware of similar work
in the NER settings. Due to space limitations, we do
not discuss all the representation schemes and com-
bining predictions by voting. We focus instead on
two most popular schemes? BIO and BILOU. The
BIO scheme suggests to learn classifiers that iden-
tify the Beginning, the Inside and the Outside of
the text segments. The BILOU scheme suggests
to learn classifiers that identify the Beginning, the
Inside and the Last tokens of multi-token chunks
as well as Unit-length chunks. The BILOU scheme
allows to learn a more expressive model with only
a small increase in the number of parameters to be
learned. Table 2 compares the end system?s perfor-
mance with BIO and BILOU. Examining the results,
we reach two conclusions: (1) choice of encod-
ing scheme has a big impact on the system perfor-
mance and (2) the less used BILOU formalism sig-
nificantly outperforms the widely adopted BIO tag-
ging scheme. We use the BILOU scheme throughout
the paper.
5 Non-Local Features
The key intuition behind non-local features in NER
has been that identical tokens should have identi-
cal label assignments. The sample text discussed
in the introduction shows one such example, where
all occurrences of ?blinker? are assigned the PER
label. However, in general, this is not always the
case; for example we might see in the same doc-
ument the word sequences ?Australia? and ?The
bank of Australia?. The first instance should be la-
beled as LOC, and the second as ORG. We consider
three approaches proposed in the literature in the fol-
lowing sections. Before continuing the discussion,
we note that we found that adjacent documents in
the CoNLL03 and the MUC7 datasets often discuss
the same entities. Therefore, we ignore document
boundaries and analyze global dependencies in 200
and 1000 token windows. These constants were se-
lected by hand after trying a small number of val-
ues. We believe that this approach will also make
our system more robust in cases when the document
boundaries are not given.
5.1 Context aggregation
(Chieu and Ng, 2003) used features that aggre-
gate, for each document, the context tokens appear
in. Sample features are: the longest capitilized se-
quence of words in the document which contains
the current token and the token appears before a
company marker such as ltd. elsewhere in text.
In this work, we call this type of features con-
text aggregation features. Manually designed con-
text aggregation features clearly have low coverage,
therefore we used the following approach. Recall
that for each token instance xi, we use as features
the tokens in the window of size two around it:
ci = (xi?2, xi?1, xi, xi+1, xi+2). When the same
token type t appears in several locations in the text,
say xi1 , xi2 , . . . , xiN , for each instance xij , in ad-
dition to the context features cij , we also aggregate
the context across all instances within 200 tokens:
C = ?j=Nj=1 cij .
150
CoNLL03 CoNLL03 MUC7 MUC7 Web
Component Test data Dev data Dev Test pages
1) Baseline 83.65 89.25 74.72 71.28 71.41
2) (1) + Context Aggregation 85.40 89.99 79.16 71.53 70.76
3) (1) + Extended Prediction History 85.57 90.97 78.56 74.27 72.19
4) (1)+ Two-stage Prediction Aggregation 85.01 89.97 75.48 72.16 72.72
5) All Non-local Features (1-4) 86.53 90.69 81.41 73.61 71.21
Table 3: The utility of non-local features. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and
Webpages. No single technique outperformed the rest on all domains. The combination of all techniques is the most robust.
5.2 Two-stage prediction aggregation
Context aggregation as done above can lead to ex-
cessive number of features. (Krishnan and Manning,
2006) used the intuition that some instances of a to-
ken appear in easily-identifiable contexts. Therefore
they apply a baseline NER system, and use the re-
sulting predictions as features in a second level of in-
ference. We call the technique two-stage prediction
aggregation. We implemented the token-majority
and the entity-majority features discussed in (Krish-
nan and Manning, 2006); however, instead of docu-
ment and corpus majority tags, we used relative fre-
quency of the tags in a 1000 token window.
5.3 Extended prediction history
Both context aggregation and two-stage prediction
aggregation treat all tokens in the text similarly.
However, we observed that the named entities in the
beginning of the documents tended to be more easily
identifiable and matched gazetteers more often. This
is due to the fact that when a named entity is intro-
duced for the first time in text, a canonical name is
used, while in the following discussion abbreviated
mentions, pronouns, and other references are used.
To break the symmetry, when using beamsearch or
greedy left-to-right decoding, we use the fact that
when we are making a prediction for token instance
xi, we have already made predictions y1, . . . , yi?1
for token instances x1, . . . , xi?1. When making the
prediction for token instance xi, we record the la-
bel assignment distribution for all token instances
for the same token type in the previous 1000 words.
That is, if the token instance is ?Australia?, and in
the previous 1000 tokens, the token type ?Australia?
was twice assigned the label L-ORG and three times
the label U-LOC, then the prediction history feature
will be: (L?ORG : 25 ;U ? LOC : 35).
5.4 Utility of non-local features
Table 3 summarizes the results. Surprisingly, no
single technique outperformed the others on all
datasets. The extended prediction history method
was the best on CoNLL03 data and MUC7 test set.
Context aggregation was the best method for MUC7
development set and two-stage prediction was the
best for Webpages. Non-local features proved less
effective for MUC7 test set and the Webpages. Since
the named entities in Webpages have less context,
this result is expected for the Webpages. However,
we are unsure why MUC7 test set benefits from non-
local features much less than MUC7 development
set. Our key conclusion is that no single approach
is better than the rest and that the approaches are
complimentary- their combination is the most stable
and best performing.
6 External Knowledge
As we have illustrated in the introduction, NER is
a knowledge-intensive task. In this section, we dis-
cuss two important knowledge resources? gazetteers
and unlabeled text.
6.1 Unlabeled Text
Recent successful semi-supervised systems (Ando
and Zhang, 2005; Suzuki and Isozaki, 2008) have
illustrated that unlabeled text can be used to im-
prove the performance of NER systems. In this
work, we analyze a simple technique of using word
clusters generated from unlabeled text, which has
been shown to improve performance of dependency
parsing (Koo et al, 2008), Chinese word segmen-
tation (Liang, 2005) and NER (Miller et al, 2004).
The technique is based on word class models, pio-
neered by (Brown et al, 1992), which hierarchically
151
CoNLL03 CoNLL03 MUC7 MUC7 Web
Component Test data Dev data Dev Test pages
1) Baseline 83.65 89.25 74.72 71.28 71.41
2) (1) + Gazetteer Match 87.22 91.61 85.83 80.43 74.46
3) (1) + Word Class Model 86.82 90.85 80.25 79.88 72.26
4) All External Knowledge 88.55 92.49 84.50 83.23 74.44
Table 4: Utility of external knowledge. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages.
clusters words, producing a binary tree as in Fig-
ure 2.
Figure 2: An extract from word cluster hierarchy.
The approach is related, but not identical, to dis-
tributional similarity (for details, see (Brown et al,
1992) and (Liang, 2005)). For example, since the
words Friday and Tuesday appear in similar con-
texts, the Brown algorithm will assign them to the
same cluster. Successful abstraction of both as a
day of the week, addresses the data sparsity prob-
lem common in NLP tasks. In this work, we use the
implementation and the clusters obtained in (Liang,
2005) from running the algorithm on the Reuters
1996 dataset, a superset of the CoNLL03 NER
dataset. Within the binary tree produced by the al-
gorithm, each word can be uniquely identified by
its path from the root, and this path can be com-
pactly represented with a bit string. Paths of dif-
ferent depths along the path from the root to the
word provide different levels of word abstraction.
For example, paths at depth 4 closely correspond
to POS tags. Since word class models use large
amounts of unlabeled data, they are essentially a
semi-supervised technique, which we use to consid-
erably improve the performance of our system.
In this work, we used path prefixes of length
4,6,10, and 20. When Brown clusters are used as
features in the following sections, it implies that all
features in the system which contain a word form
will be duplicated and a new set of features con-
taining the paths of varying length will be intro-
duced. For example, if the system contains the fea-
ture concatenation of the current token and the sys-
tem prediction on the previous word, four new fea-
tures will be introduced which are concatenations
of the previous prediction and the 4,6,10,20 length
path-representations of the current word.
6.2 Gazetteers
An important question at the inception of the NER
task was whether machine learning techniques are
necessary at all, and whether simple dictionary
lookup would be sufficient for good performance.
Indeed, the baseline for the CoNLL03 shared task
was essentially a dictionary lookup of the enti-
ties which appeared in the training data, and it
achieves 71.91 F1 score on the test set (Tjong and
De Meulder, 2003). It turns out that while prob-
lems of coverage and ambiguity prevent straightfor-
ward lookup, injection of gazetteer matches as fea-
tures in machine-learning based approaches is crit-
ical for good performance (Cohen, 2004; Kazama
and Torisawa, 2007a; Toral and Munoz, 2006; Flo-
rian et al, 2003). Given these findings, several ap-
proaches have been proposed to automatically ex-
tract comprehensive gazetteers from the web and
from large collections of unlabeled text (Etzioni
et al, 2005; Riloff and Jones, 1999) with lim-
ited impact on NER. Recently, (Toral and Munoz,
2006; Kazama and Torisawa, 2007a) have success-
fully constructed high quality and high coverage
gazetteers from Wikipedia.
In this work, we use a collection of 14 high-
precision, low-recall lists extracted from the web
that cover common names, countries, monetary
units, temporal expressions, etc. While these
gazetteers have excellent accuracy, they do not pro-
vide sufficient coverage. To further improve the
coverage, we have extracted 16 gazetteers from
Wikipedia, which collectively contain over 1.5M en-
tities. Overall, we have 30 gazetteers (available
for download with the system), and matches against
152
CoNLL03 CoNLL03 MUC7 MUC7 Web
Component Test data Dev data Dev Test pages
1) Baseline 83.65 89.25 74.72 71.28 71.41
2) (1) + External Knowledge 88.55 92.49 84.50 83.23 74.44
3) (1) + Non-local 86.53 90.69 81.41 73.61 71.21
4) All Features 90.57 93.50 89.19 86.15 74.53
5) All Features (train with dev) 90.80 N/A 89.19 86.15 74.33
Table 5: End system performance by component. Results confirm that NER is a knowledge-intensive task.
each one are weighted as a separate feature in the
system (this allows us to trust each gazetteer to a dif-
ferent degree). We also note that we have developed
a technique for injecting non-exact string matching
to gazetteers, which has marginally improved the
performance, but is not covered in the paper due to
space limitations. In the rest of this section, we dis-
cuss the construction of gazetteers from Wikipedia.
Wikipedia is an open, collaborative encyclopedia
with several attractive properties. (1) It is kept up-
dated manually by it collaborators, hence new enti-
ties are constantly added to it. (2) Wikipedia con-
tains redirection pages, mapping several variations
of spelling of the same name to one canonical en-
try. For example, Suker is redirected to an entry
about Davor S?uker, the Croatian footballer (3) The
entries in Wikipedia are manually tagged with cate-
gories. For example, the entry about the Microsoft
in Wikipedia has the following categories: Companies
listed on NASDAQ; Cloud computing vendors; etc.
Both (Toral and Munoz, 2006) and (Kazama and
Torisawa, 2007a) used the free-text description of
the Wikipedia entity to reason about the entity type.
We use a simpler method to extract high coverage
and high quality gazetteers from Wikipedia. By
inspection of the CoNLL03 shared task annotation
guidelines and of the training set, we manually ag-
gregated several categories into a higher-level con-
cept (not necessarily NER type). When a Wikipedia
entry was tagged by one of the categories in the ta-
ble, it was added to the corresponding gazetteer.
6.3 Utility of External Knowledge
Table 4 summarizes the results of the techniques
for injecting external knowledge. It is important
to note that, although the world class model was
learned on the superset of CoNLL03 data, and al-
though the Wikipedia gazetteers were constructed
Dataset Stanford-NER LBJ-NER
MUC7 Test 80.62 85.71
MUC7 Dev 84.67 87.99
Webpages 72.50 74.89
Reuters2003 test 87.04 90.74
Reuters2003 dev 92.36 93.94
Table 6: Comparison: token-based F1 score of LBJ-NER and
Stanford NER tagger across several domains
based on CoNLL03 annotation guidelines, these fea-
tures proved extremely good on all datasets. Word
class models discussed in Section 6.1 are computed
offline, are available online1, and provide an alter-
native to traditional semi-supervised learning. It is
important to note that the word class models and the
gazetteers and independednt and accumulative. Fur-
thermore, despite the number and the gigantic size
of the extracted gazetteers, the gazeteers alone are
not sufficient for adequate performance. When we
modified the CoNLL03 baseline to include gazetteer
matches, the performance went up from 71.91 to
82.3 on the CoNLL03 test set, below our baseline
system?s result of 83.65. When we have injected the
gazetteers into our system, the performance went up
to 87.22. Word class model and nonlocal features
further improve the performance to 90.57 (see Ta-
ble 5), by more than 3 F1 points.
7 Final System Performance Analysis
As a final experiment, we have trained our system
both on the training and on the development set,
which gave us our best F1 score of 90.8 on the
CoNLL03 data, yet it failed to improve the perfor-
mance on other datasets. Table 5 summarizes the
performance of the system.
Next, we have compared the performance of our
1http://people.csail.mit.edu/maestro/papers/bllip-clusters.gz
153
system to that of the Stanford NER tagger, across the
datasets discussed above. We have chosen to com-
pare against the Stanford tagger because to the best
of our knowledge, it is the best publicly available
system which is trained on the same data. We have
downloaded the Stanford NER tagger and used the
strongest provided model trained on the CoNLL03
data with distributional similarity features. The re-
sults we obtained on the CoNLL03 test set were
consistent with what was reported in (Finkel et al,
2005). Our goal was to compare the performance of
the taggers across several datasets. For the most re-
alistic comparison, we have presented each system
with a raw text, and relied on the system?s sentence
splitter and tokenizer. When evaluating the systems,
we matched against the gold tokenization ignoring
punctuation marks. Table 6 summarizes the results.
Note that due to differences in sentence splitting, to-
kenization and evaluation, these results are not iden-
tical to those reported in Table 5. Also note that in
this experiment we have used token-level accuracy
on the CoNLL dataset as well. Finally, to complete
the comparison to other systems, in Table 7 we sum-
marize the best results reported for the CoNLL03
dataset in literature.
8 Conclusions
We have presented a simple model for NER that
uses expressive features to achieve new state of the
art performance on the Named Entity recognition
task. We explored four fundamental design deci-
sions: text chunks representation, inference algo-
rithm, using non-local features and external knowl-
edge. We showed that BILOU encoding scheme sig-
nificantly outperforms BIO and that, surprisingly, a
conditional model that does not take into account in-
teractions at the output level performs comparably
to beamsearch or Viterbi, while being considerably
more efficient computationally. We analyzed sev-
eral approaches for modeling non-local dependen-
cies and found that none of them clearly outperforms
the others across several datasets. Our experiments
corroborate recently published results indicating that
word class models learned on unlabeled text can
be an alternative to the traditional semi-supervised
learning paradigm. NER proves to be a knowledge-
intensive task, and it was reassuring to observe that
System Resources Used F1
+ LBJ-NER Wikipedia, Nonlocal Fea-
tures, Word-class Model
90.80
- (Suzuki and
Isozaki, 2008)
Semi-supervised on 1G-
word unlabeled data
89.92
- (Ando and
Zhang, 2005)
Semi-supervised on 27M-
word unlabeled data
89.31
- (Kazama and
Torisawa, 2007a)
Wikipedia 88.02
- (Krishnan and
Manning, 2006)
Non-local Features 87.24
- (Kazama and
Torisawa, 2007b)
Non-local Features 87.17
+ (Finkel et al,
2005)
Non-local Features 86.86
Table 7: Results for CoNLL03 data reported in the literature.
publicly available systems marked by +.
knowledge-driven techniques adapt well across sev-
eral domains. We observed consistent performance
gains across several domains, most interestingly in
Webpages, where the named entities had less context
and were different in nature from the named entities
in the training set. Our system significantly outper-
forms the current state of the art and is available to
download under a research license.
Apendix? wikipedia gazetters & categories
1)People: people, births, deaths. Extracts 494,699 Wikipedia
titles and 382,336 redirect links. 2)Organizations: cooper-
atives, federations, teams, clubs, departments, organizations,
organisations, banks, legislatures, record labels, constructors,
manufacturers, ministries, ministers, military units, military
formations, universities, radio stations, newspapers, broad-
casters, political parties, television networks, companies, busi-
nesses, agencies. Extracts 124,403 titles and 130,588 redi-
rects. 3)Locations: airports, districts, regions, countries, ar-
eas, lakes, seas, oceans, towns, villages, parks, bays, bases,
cities, landmarks, rivers, valleys, deserts, locations, places,
neighborhoods. Extracts 211,872 titles and 194,049 redirects.
4)Named Objects: aircraft, spacecraft, tanks, rifles, weapons,
ships, firearms, automobiles, computers, boats. Extracts 28,739
titles and 31,389 redirects. 5)Art Work: novels, books, paint-
ings, operas, plays. Extracts 39,800 titles and 34037 redirects.
6)Films: films, telenovelas, shows, musicals. Extracts 50,454
titles and 49,252 redirects. 7)Songs: songs, singles, albums.
Extracts 109,645 titles and 67,473 redirects. 8)Events: playoffs,
championships, races, competitions, battles. Extracts 20,176 ti-
tles and 15,182 redirects.
154
References
R. K. Ando and T. Zhang. 2005. A high-performance
semi-supervised learning method for text chunking. In
ACL.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D.
Pietra, and J. C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
X. Carreras, L. Ma`rquez, and L. Padro?. 2003. Learn-
ing a perceptron-based named entity chunker via on-
line recognition feedback. In CoNLL.
H. Chieu and H. T. Ng. 2003. Named entity recognition
with a maximum entropy approach. In Proceedings of
CoNLL.
W. W. Cohen. 2004. Exploiting dictionaries in named
entity extraction: Combining semi-markov extraction
processes and data integration methods. In KDD.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In EMNLP.
L. Edward. 2007. Finding good sequential model struc-
tures using output transformations. In EMNLP).
O. Etzioni, M. J. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In ACL.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In CoNLL.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
J. Kazama and K. Torisawa. 2007a. Exploiting wikipedia
as external knowledge for named entity recognition. In
EMNLP.
J. Kazama and K. Torisawa. 2007b. A new perceptron al-
gorithm for sequence labeling with non-local features.
In EMNLP-CoNLL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
V. Krishnan and C. D. Manning. 2006. An effective two-
stage model for exploiting non-local dependencies in
named entity recognition. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML. Mor-
gan Kaufmann.
P. Liang. 2005. Semi-supervised learning for natural
language. Masters thesis, Massachusetts Institute of
Technology.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative training.
In HLT-NAACL.
A. Molina and F. Pla. 2002. Shallow parsing using spe-
cialized hmms. The Journal of Machine Learning Re-
search, 2:595?613.
A. Niculescu-Mizil and R. Caruana. 2005. Predicting
good probabilities with supervised learning. In ICML.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS.
L. R. Rabiner. 1989. A tutorial on hidden markov mod-
els and selected applications in speech recognition. In
IEEE.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In AAAI.
N. Rizzolo and D. Roth. 2007. Modeling discriminative
global inference. In ICSC.
D. Roth and D. Zelenko. 1998. Part of speech tagging us-
ing a network of linear separators. In COLING-ACL.
H. Shen and A. Sarkar. 2005. Voting between multiple
data representations for text chunking. Advances in
Artificial Intelligence, pages 389?400.
J. Suzuki and H. Isozaki. 2008. Semi-supervised sequen-
tial labeling and segmentation using giga-word scale
unlabeled data. In ACL.
E. Tjong, K. and F. De Meulder. 2003. Introduction
to the conll-2003 shared task: Language-independent
named entity recognition. In CoNLL.
A. Toral and R. Munoz. 2006. A proposal to automat-
ically build and maintain gazetteers for named entity
recognition by using wikipedia. In EACL.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In NAACL.
J. Veenstra. 1999. Representing text chunks. In EACL.
T. Zhang and D. Johnson. 2003. A robust risk mini-
mization based named entity recognition system. In
CoNLL.
155
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 152?160,
Beijing, August 2010
Exploiting Background Knowledge for Relation Extraction
Yee Seng Chan and Dan Roth
University of Illinois at Urbana-Champaign
{chanys,danr}@illinois.edu
Abstract
Relation extraction is the task of recog-
nizing semantic relations among entities.
Given a particular sentence supervised ap-
proaches to Relation Extraction employed
feature or kernel functions which usu-
ally have a single sentence in their scope.
The overall aim of this paper is to pro-
pose methods for using knowledge and re-
sources that are external to the target sen-
tence, as a way to improve relation ex-
traction. We demonstrate this by exploit-
ing background knowledge such as rela-
tionships among the target relations, as
well as by considering how target rela-
tions relate to some existing knowledge
resources. Our methods are general and
we suggest that some of them could be ap-
plied to other NLP tasks.
1 Introduction
Relation extraction (RE) is the task of detecting
and characterizing semantic relations expressed
between entities in text. For instance, given the
sentence ?Cone, a Kansas City native, was origi-
nally signed by the Royals and broke into the ma-
jors with the team.?, one of the relations we might
want to extract is the employment relation between
the pair of entity mentions ?Cone? and ?Royals?.
RE is important for many NLP applications such
as building an ontology of entities, biomedical in-
formation extraction, and question answering.
Prior work have employed diverse approaches
towards resolving the task. One approach is to
build supervised RE systems using sentences an-
notated with entity mentions and predefined target
relations. When given a new sentence, the RE sys-
tem has to detect and disambiguate the presence of
any predefined relations that might exist between
each of the mention pairs in the sentence. In build-
ing these systems, researchers used a wide variety
of features (Kambhatla, 2004; Zhou et al, 2005;
Jiang and Zhai, 2007). Some of the common fea-
tures used to analyze the target sentence include
the words appearing in the sentence, their part-of-
speech (POS) tags, the syntactic parse of the sen-
tence, and the dependency path between the pair
of mentions. In a related line of work, researchers
have also proposed various kernel functions based
on different structured representations (e.g. de-
pendency or syntactic tree parses) of the target
sentences (Bunescu and Mooney, 2005; Zhou et
al., 2007; Zelenko et al, 2003; Zhang et al,
2006). Additionally, researchers have tried to au-
tomatically extract examples for supervised learn-
ing from resources such as Wikipedia (Weld et al,
2008) and databases (Mintz et al, 2009), or at-
tempted open information extraction (IE) (Banko
et al, 2007) to extract all possible relations.
In this work, we focus on supervised RE. In
prior work, the feature and kernel functions em-
ployed are usually restricted to being defined on
the various representations (e.g. lexical or struc-
tural) of the target sentences. However, in recog-
nizing relations, humans are not thus constrained
and rely on an abundance of implicit world knowl-
edge or background information. What quantifies
as world or background knowledge is rarely ex-
plored in the RE literature and we do not attempt
to provide complete nor precise definitions in this
paper. However, we show that by considering the
relationship between our relations of interest, as
152
well as how they relate to some existing knowl-
edge resources, we improve the performance of
RE. Specifically, the contributions of this paper
are the following:
? When our relations of interest are clustered
or organized in a hierarchical ontology, we
show how to use this information to improve
performance. By defining appropriate con-
straints between the predictions of relations
at different levels of the hierarchy, we obtain
globally coherent predictions as well as im-
proved performance.
? Coreference is a generic relationship that
might exists among entity mentions and we
show how to exploit this information by as-
suming that co-referring mentions have no
other interesting relations. We capture this
intuition by using coreference information to
constraint the predictions of a RE system.
? When characterizing the relationship be-
tween a pair of mentions, one can use a
large encyclopedia such as Wikipedia to in-
fer more knowledge about the two mentions.
In this work, after probabilistically map-
ping mentions to their respective Wikipedia
pages, we check whether the mentions are
related. Another generic relationship that
might exists between a pair of mentions is
whether they have a parent-child relation and
we use this as additional information.
? The sparsity of features (especially lexical
features) is a common problem for super-
vised systems. In this work, we show that
one can make fruitful use of unlabeled data,
by using word clusters automatically gath-
ered from unlabeled texts as a way of gen-
eralizing the lexical features.
? We combine the various relational predic-
tions and background knowledge through a
global inference procedure, which we for-
malize via an Integer Linear Programming
(ILP) framework as a constraint optimization
problem (Roth and Yih, 2007). This allows
us to easily incorporate various constraints
that encode the background knowledge.
Roth and Yih (2004) develop a relation extrac-
tion approach that exploits constraints among en-
tity types and the relations allowed among them.
We extend this view significantly, within a simi-
lar computational framework, to exploit relations
among target relations, background information
and world knowledge, as a way to improve rela-
tion extraction and make globally coherent predic-
tions.
In the rest of this paper, we first describe the
features used in our basic RE system in Section 2.
We then describe how we make use of background
knowledge in Section 3. In Section 4, we show
our experimental results and perform analysis in
Section 5. In Section 6, we discuss related work,
before concluding in Section 7.
2 Relation Extraction System
In this section, we describe the features used in
our basic relation extraction (RE) system. Given
a pair of mentions m1 and m2 occurring within
the same sentence, the system predicts whether
any of the predefined relation holds between the
two mentions. Since relations are usually asym-
metric in nature, hence in all of our experi-
ments, unless otherwise stated, we distinguish be-
tween the argument ordering of the two mentions.
For instance, we consider m1:emp-org:m2 and
m2:emp-org:m1 to be distinct relation types.
Most of the features used in our system are
based on the work in (Zhou et al, 2005). In this
paper, we propose some new collocation features
inspired by word sense disambiguation (WSD).
We give an overview of the features in Table 1.
Due to space limitations, we only describe the col-
location features and refer the reader to (Zhou et
al., 2005) for the rest of the features.
2.1 Collocation Features
Following (Zhou et al, 2005), we use a single
word to represent the head word of a mention.
Since single words might be ambiguous or poly-
semous, we incorporate local collocation features
which were found to be very useful for WSD.
Given the head word hwm of a mention m, the
collocation feature Ci,j refers to the sequence of
tokens in the immediate context of hwm. The off-
sets i and j denote the position (relative to hwm)
153
Category Feature
Lexical hw of m1
hw of m2
hw of m1, m2
BOW in m1
BOW in m2
single word between m1, m2
BOW in between m1, m2
bigrams in between m1, m2
first word in between m1, m2
last word in between m1, m2
Collocations C?1,?1, C+1,+1
C?2,?1, C?1,+1, C+1,+2
Structural m1-in-m2
m2-in-m1
#mentions between m1, m2
any word between m1, m2
M-lvl M-lvl of m1, m2
and m1, m2 E-maintype
E-type m1, m2 E-subtype
m1, m2 M-lvl and E-maintype
m1, m2 M-lvl and E-subtype
m1, m2 E-subtype and m1-in-m2
m1, m2 E-subtype and m2-in-m1
Dependency path between m1, m2
bag-of dep labels between m1, m2
hw of m1 and dep-parent
hw of m2 and dep-parent
Table 1: Features in the basic RE system. The
abbreviations are as follows. hw: head word, M-
lvl: mention level, E-type: entity type, dep-parent:
the word?s parent in the dependency tree.
of the first and last token of the sequence respec-
tively. For instance, C?1,+1 denotes a sequence of
three tokens, consisting of the single token on the
immediate left of hwm, the token hwm itself, and
the single token on the immediate right of hwm.
For each mention, we extract 5 features: C?1,?1,
C+1,+1, C?2,?1, C?1,+1, and C+1,+2.
3 Using Background Knowledge
Now we describe how we inject additional knowl-
edge into our relation extraction system.
3.1 Hierarchy of Relations
When our relations of interest are arranged in a
hierarchical structure, one should leverage this in-
formation to learn more accurate relation predic-
tors. For instance, assume that our relations are
arranged in a two-level hierarchy and we learn
two classifiers, one for disambiguating between
the first level coarse-grained relations, and an-
other for disambiguating between the second level
fine-grained relations.
Since there are a lot more fine-grained relation
types than coarse-grained relation types, we pro-
pose using the coarse-grained predictions which
should intuitively be more reliable, to improve the
fine-grained predictions. We show how to achieve
this through defining appropriate constraints be-
tween the coarse-grained and fine-grained rela-
tions, which can be enforced through the Con-
strained Conditional Models framework (aka ILP)
(Roth and Yih, 2007; Chang et al, 2008). Due
to space limitations, we refer interested readers
to the papers for more information on the CCM
framework.
By doing this, not only are the predictions of
both classifiers coherent with each other (thus ob-
taining better predictions from both classifiers),
but more importantly, we are effectively using the
(more reliable) predictions of the coarse-grained
classifier to constrain the predictions of the fine-
grained classifier. To the best of our knowledge,
this approach for RE is novel.
In this paper, we work on the NIST Automatic
Content Extraction (ACE) 2004 corpus. ACE de-
fines several coarse-grained relations such as em-
ployment/membership, geo-political entity (GPE)
affiliation, etc. Each coarse-grained relation is
further refined into several fine-grained relations1
and each fine-grained relation has a unique par-
ent coarse-grained relation. For instance, the fine-
grained relations employed as ordinary staff, em-
ployed as an executive, etc. are children relations
of employment/membership.
Let mi and mj denote a pair of mentions i and
j drawn from a document containing N mentions.
Let Ri,j denote a relation between mi and mj , and
let R = {Ri,j}, where 1?i, j?N ; i 6=j denote the
set of relations in the document. Also, we denote
the set of predefined coarse-grained relation types
and fine-grained relation types as LRc and LRf
respectively. Since there could possibly be no re-
lation between a mention pair, we add the null la-
bel to LRc and LRf , allowing our classifiers to
predict null for Ri,j . Finally, for a fine-grained re-
lation type rf , let V(rf) denote its parent coarse-
grained relation type.
1With the exception of the Discourse coarse-grained re-
lation.
154
We learn two classifiers, one for disambiguat-
ing between the coarse-grained relations and one
for disambiguating between the fine-grained rela-
tions. Let ?c and ?f denote the feature weights
learned for predicting coarse-grained and fine-
grained relations respectively. Let pR(rc) =
logPc(rc|mi,mj ; ?c) be the log probability that
relation R is predicted to be of coarse-grained
relation type rc. Similarly, let pR(rf) =
logPf (rf |mi,mj ; ?f ) be the log probability that
relation R is predicted to be of fine-grained re-
lation type rf . Let x?R,rc? be a binary variable
which takes on the value of 1 if relation R is la-
beled with the coarse-grained label rc. Similarly,
let y?R,rf? be a binary variable which takes on the
value of 1 if relation R is labeled with the fine-
grained label rf . Our objective function is then:
max
?
R?R
?
rc?LRc
pR(rc) ? x?R,rc?
+
?
R?R
?
rf?LRf
pR(rf) ? y?R,rf? (1)
subject to the following constraints:
?
rc?LRc
x?R,rc? = 1 ?R ? R (2)
?
rf?LRf
y?R,rf? = 1 ?R ? R (3)
x?R,rc? ? {0, 1} ?R ? R, rc ? LRc (4)
y?R,rf? ? {0, 1} ?R ? R, rf ? LRf (5)
Equations (2) and (3) require that each relation
can only be assigned one coarse-grained label and
one fine-grained label. Equations (4) and (5) indi-
cate that x?R,rc? and y?R,rf? are binary variables.
Two more constraints follow:
x?R,rc? ?
?
{rf?LRf |V(rf)=rc}
y?R,rf?
?R ? R , rc ? LRc (6)
y?R,rf? ? x?R,V(rf)? ?R ? R, rf ? LRf (7)
The logical form of Equation (6) can be written
as: x?R,rc? ? y?R,rf1? ? y?R,rf2? . . . ? y?R,rfn?,
where rf1, rf2, . . . , rfn are (child) fine-grained
relations of the coarse-grained relation rc. This
states that if we assign rc to relation R, then we
must also assign to R a fine-grained relation rf
art: Ei ?{gpe, org, per},
Ej ?{fac, gpe, veh, wea}
emp-org: Ei ?{gpe, org, per},
Ej ?{gpe, org, per}
gpe-aff: Ei ?{gpe, org, per},
Ej ?{gpe, loc}
other-aff: Ei ?{gpe, org, per},
Ej ?{gpe, loc}
per-soc: Ei ?{per}, Ej ?{per}
Table 2: Entity type constraints.
which is a child of rc. The logical form of Equa-
tion (7) can be written as: y?R,rf? ? x?R,V(rf)?.
This captures the inverse relation and states that
if we assign rf to R, then we must also assign to
R the relation type V(rf), which is the parent of
rf . Together, Equations (6) and (7) constrain the
predictions of the coarse-grained and fine-grained
classifiers to be coherent with each other. Finally,
we note that one could automatically translate log-
ical constraints into linear inequalities (Chang et
al., 2008).
This method is general and is applicable to
other NLP tasks where a hierarchy exists, such
as WSD and question answering. For instance,
in WSD, one can predict coarse-grained and fine-
grained senses using suitably defined sense inven-
tories and then perform inference via ILP to obtain
coherent predictions.
3.2 Entity Type Constraints
Each mention in ACE-2004 is annotated with one
of seven coarse-grained entity types: person (per),
organization (org), location (loc), geo-political en-
tity (gpe), facility (fac), vehicle (veh), and weapon
(wea).
Roth and Yih (2007) had shown that entity type
information is useful for constraining the possible
labels that a relation R can assume. For instance,
both mentions involved in a personal/social re-
lation must be of entity type per. In this work,
we gather such information from the ACE-2004
documentation and inject it as constraints (on the
coarse-grained relations) into our system. Due
to space limitations, we do not state the con-
straint equations or objective function here, but
we list the entity type constraints we imposed for
each coarse-grained relation mi-R-mj in Table
155
22, where Ei (Ej) denotes the allowed set of en-
tity types for mention mi (mj). Applying the en-
tity type information improves the predictions of
the coarse-grained classifier and this in turn could
improve the predictions of the fine-grained classi-
fier.
3.3 Using Coreference Information
We can also utilize the coreference relations
among entity mentions. Assuming that we know
mentions mi and mj are coreferent with each
other, then there should be no relation between
them3. Let z?i,j? be a binary variable which takes
on the value of 1 if mentions mi and mj are coref-
erent, and 0 if they are not. When z?i,j?=1, we cap-
ture the above intuition with the following con-
straints:
z?i,j? ? x?Ri,j ,null? (8)
z?i,j? ? y?Ri,j ,null? (9)
which can be written in logical form as: z?i,j? ?
x?Ri,j ,null?, and z?i,j? ? y?Ri,j ,null?. We add the
following to our objective function in Equation
(1):
?
mi,mj?m2
co?i,j? ? z?i,j?+ c?o?i,j? ? (1? z?i,j?) (10)
where m is the set of mentions in a document,
co?i,j? and c?o?i,j? are the log probabilities of pre-
dicting that mi and mj are coreferent and not
coreferent respectively. In this work, we assume
we are given coreference information, which is
available from the ACE annotation.
3.4 Using Knowledge from Wikipedia
We propose two ways of using Wikipedia to
gather features for relation extraction. Wikipedia
is a huge online encyclopedia and mainly contains
articles describing entities or concepts.
The first intuition is that if we are able to cor-
rectly map a pair of mentions mi and mj to their
corresponding Wikipedia article (assuming they
2We do not impose entity type constraints on the coarse-
grained relations disc and phys.
3In this work, we assume that no relations are reflexive.
After the experiments in this paper are performed, we ver-
ified that in the ACE corpus we used, less than 1% of the
relations are reflexive.
are represented in Wikipedia), we could use the
content on their Wikipedia pages to check whether
they are related.
In this work, we use a Wiki system (Rati-
nov et al, 2010) which performs context-sensitive
mapping of mentions to Wikipedia pages. In
their work, the authors first identify phrases or
mentions that could be mapped. The correct
Wikipedia article for each mention is then prob-
abilistically predicted using a combination of fea-
tures based on Wikipedia hyperlink structure, se-
mantic coherence, etc. The authors? own evalua-
tion results indicate that the performance of their
system ranges from 70?80%. When given a pair
of mentions and the system returns the Wikipedia
page for either one of the mentions, we introduce
a feature:
w1(mi,mj) =
?
?
?
1, if Ami(mj)
or Amj (mi)
0, otherwise
where Ami(mj) returns true if the head extent
of mj is found (via simple string matching) in
the predicted Wikipedia article of mi. The in-
terpretation of Amj (mi) is similar. We introduce
a new feature into the RE system by combining
w1(mi,mj) with mi,mj E-maintype (defined as
in Table 1).
The second feature based on Wikipedia is as
follows. It will be useful to check whether there
is any parent-child relationship between two men-
tions. Intuitively, this will be useful for recogniz-
ing several relations such as physical part-whole
(e.g. a city is part of a state), subsidiary (a com-
pany is a child-company of another), citizenship
(a person is a citizen of a country), etc.
Given a pair of mentions mi and mj , we use a
Parent-Child system (Do and Roth, 2010) to pre-
dict whether they have a parent-child relation. To
achieve this, the system first gathers all Wikipedia
articles that are related to mi and mj . It then uses
the words in these pages and the category ontol-
ogy of Wikipedia to make its parent-child predic-
tions, while respecting certain defined constraints.
In this work, we use its prediction as follows:
w2(mi,mj) =
{
1, if parent-child(mi,mj)
0, otherwise
156
Figure 1: An example of Brown word cluster hi-
erarchy from (Koo et al, 2008).
where we combine w2(mi,mj) with mi,mj E-
maintype, introducing this as a new feature into
our RE system.
3.5 Using Word Clusters
An inherent problem faced by supervised systems
is that of data sparseness. To mitigate such is-
sues in the lexical features, we use word clusters
which are automatically generated from unlabeled
texts. In this work, we use the Brown clustering
algorithm (Brown et al, 1992), which has been
shown to improve performance in various NLP
applications such as dependency parsing (Koo et
al., 2008), named entity recognition (Ratinov and
Roth, 2009), and relation extraction (Boschee et
al., 2005). The algorithm performs a hierarchical
clustering of the words and represents them as a
binary tree.
Each word is uniquely identified by its path
from the root and every path is represented with
a bit string. Figure 1 shows an example clustering
where the maximum path length is 3. By using
path prefixes of different lengths, one can obtain
clusterings at different granularity. For instance,
using prefixes of length 2 will put apple and pear
into the same cluster, Apple and IBM into the same
cluster, etc. In our work, we use clusters gener-
ated from New York Times text and simply use a
path prefix of length 10. When Brown clusters are
used in our system, all lexical features consisting
of single words will be duplicated. For instance,
for the feature hw of m1, one new feature which is
the length-10 bit string path representing the orig-
inal lexical head word of m1, will be introduced
and presented to the classifier as a string feature.
4 Experiments
We used the ACE-2004 dataset (catalog
LDC2005T09 from the Linguistic Data Con-
sortium) to conduct our experiments. ACE-2004
defines 7 coarse-grained relations and 23 fine-
grained relations. In all of our experiments,
unless otherwise stated, we explicitly model the
argument order (of the mentions) when asked
to disambiguate the relation between a pair of
mentions. Hence, we built our coarse-grained
classifier with 15 relation labels to disambiguate
between (two for each coarse-grained relation
type and a null label when the two mentions are
not related). Likewise, our fine-grained classifier
has to disambiguate between 47 relation labels.
In the dataset, relations do not cross sentence
boundaries.
For our experiments, we trained regularized av-
eraged perceptrons (Freund and Schapire, 1999),
implemented within the Sparse Network of Win-
now framework (Carlson et al, 1999), one for pre-
dicting the coarse-grained relations and another
for predicting the fine-grained relations. Since the
dataset has no split of training, development, and
test sets, we followed prior work (Jiang and Zhai,
2007) and performed 5-fold cross validation to ob-
tain our performance results. For simplicity, we
used 5 rounds of training and a regularization pa-
rameter of 1.5 for the perceptrons in all our exper-
iments. Finally, we concentrate on the evaluation
of fine-grained relations.
4.1 Performance of the Basic RE system
As a gauge on the performance of our basic rela-
tion extraction system BasicRE using only the fea-
tures described in Section 2, we compare against
the state-of-the-art feature-based RE system of
Jiang and Zhai (2007). However, we note that in
that work, the authors performed their evaluation
using undirected coarse-grained relations. That is,
they do not distinguish on argument order of men-
tions and the classifier has to decide among 8 re-
lation labels (7 coarse-grained relation types and a
null label). Performing 5-fold cross validation on
the news wire (nwire) and broadcast news (bnews)
corpora in the ACE-2004 dataset, they reported a
F-measure of 71.5 using a maximum entropy clas-
sifier4. Evaluating BasicRE on the same setting,
4After they heuristically performed feature selection and
applied the heuristics giving the best evaluation performance,
they obtained a result of 72.9.
157
All nwire 10% of nwire
Features Rec% Pre% F1% Rec% Pre% F1%
BasicRE 49.9 51.0 50.5 33.2 29.0 31.0
+Hier +1.3 +1.3 +1.3 +1.1 +1.2 +1.1
+Hier+relEntC +1.5 +2.0 +1.8 +3.3 +3.5 +3.4
+Coref ? +1.4 +0.7 ?0.1 +1.0 +0.5
+Wiki +0.2 +1.9 +1.0 +1.5 +2.5 +2.0
+Cluster ?0.2 +3.2 +1.4 ?0.7 +3.9 +1.7
+ALL +1.5 +6.7 +3.9 +4.7 +10.2 +7.6
Table 3: BasicRE gives the performance of our basic RE system on predicting fine-grained relations,
obtained by performing 5-fold cross validation on only the news wire corpus of ACE-2004. Each sub-
sequent row +Hier, +Hier+relEntC, +Coref, +Wiki, and +Cluster gives the individual contribution
from using each knowledge. The bottom row +ALL gives the performance improvements from adding
+Hier+relEntC+Coref+Wiki+Cluster. ? indicates no change in score.
we obtained a competitive F-measure of 71.25.
4.2 Experimental Settings for Evaluating
Fine-grained Relations
Two of our knowledge sources, the Wiki system
described in Section 3.4 and the word clusters de-
scribed in Section 3.5, assume inputs of mixed-
cased text. We note that the bnews corpus of
ACE-2004 is entirely in lower-cased text. Hence,
we use only the nwire corpus for our experiments
here, from which we gathered 28,943 relation in-
stances and 2,226 of those have a valid (non-null)
relation6.
We also propose the following experimental
setting. First, since we made use of coreference
information, we made sure that while performing
our experiments, all instances from the same doc-
ument are either all used as training data or all
used as test data. Prior work in RE had not en-
sured this, but we argue that this provides a more
realistic setting. Our own experiments indicate
that this results in a 1-2% lower performance on
fine-grained relations.
Secondly, prior work calculate their perfor-
mance on relation extraction at the level of men-
tions. That is, each mention pair extracted is
scored individually. An issue with this way of
scoring on the ACE corpus is that ACE annota-
5Using 10 rounds of training and a regularization param-
eter of 2.5 improves the result to 72.2. In general, we found
that more rounds of training and a higher regularization value
benefits coarse-grained relation classification, but not fine-
grained relation classification.
6The number of relation instances in the nwire and bnews
corpora are about the same.
tors rarely duplicate a relation link for coreferent
mentions. For instance, assume that mentions mi,
mj , and mk exist in a given sentence, mentions
mi and mj are coreferent, and the annotator es-
tablishes a particular relation type r between mj
and mk. The annotator will not usually duplicate
the same relation r between mi and mk and thus
the label between these two mentions is then null.
We are not suggesting that this is an incorrect ap-
proach, but clearly there is an issue since an im-
portant goal of performing RE is to populate or
build an ontology of entities and establish the re-
lations existing among the entities. Thus, we eval-
uate our performance at the entity-level.7 That is,
given a pair of entities, we establish the set of re-
lation types existing between them, based on their
mention annotations. Then we calculate recall
and precision based on these established relations.
Of course, performing such an evaluation requires
knowledge about the coreference relations and in
this work, we assume we are given this informa-
tion.
4.3 Knowledge-Enriched System
Evaluating our system BasicRE (trained only on
the features described in Section 2) on the nwire
corpus, we obtained a F1 score of 50.5, as shown
in Table 3. Next, we exploited the relation hier-
archy as in Section 3.1 and obtained an improve-
ment of 1.3, as shown in the row +Hier. Next,
we added the entity type constraints of Section
7Our experiments indicate that performing the usual eval-
uation on mentions gives similar performance figures and the
trend in Table 3 stays the same.
158
3.2. Remember that these constraints are imposed
on the coarse-grained relations. Thus, they would
only affect the fine-grained relation predictions if
we also exploit the relation hierarchy. In the ta-
ble, we show that all the background knowledge
helped to improve performance, providing a to-
tal improvement of 3.9 to our basic RE system.
Though the focus of this work is on fine-grained
relations, our approach also improves the perfor-
mance of coarse-grained relation predictions. Ba-
sicRE obtains a F1 score of 65.3 on coarse-grained
relations and exploiting background knowledge
gives a total improvement of 2.9.
5 Analysis
We explore the situation where we have very little
training data. We assume during each cross val-
idation fold, we are given only 10% of the train-
ing data we originally had. Previously, when per-
forming 5-fold cross validation on 2,226 valid re-
lation instances, we had about 1780 as training
instances in each fold. Now, we assume we are
only given about 178 training instances in each
fold. Under this condition, BasicRE gives a F1
score of 31.0 on fine-grained relations. Adding all
the background knowledge gives an improvement
of 7.6 and this represents an error reduction of
39% when measured against the performance dif-
ference of 50.5 (31.0) when we have 1780 train-
ing instances vs. 178 training instances. On
the coarse-grained relations, BasicRE gives a F1
score of 51.1 and exploiting background knowl-
edge gives a total improvement of 5.0.
We also tabulated the list of fine-grained re-
lations that improved by more than 1 F1 score
when we incorporated +Wiki, on the experiment
using all of nwire data: phys:near (physically
near), other-aff:ideology (ideology affiliation),
art:user-or-owner (user or owner of artifact), per-
soc:business (business relationship), phys:part-
whole (physical part-whole), emp-org:subsidiary
(organization subsidiary), and gpe-aff:citizen-or-
resident (citizen or resident). Most of these intu-
itively seemed to be information one would find
being mentioned in an encyclopedia.
6 Related Work
Few prior work has explored using background
knowledge to improve relation extraction perfor-
mance. Zhou et al (2008) took advantage of
the hierarchical ontology of relations by propos-
ing methods customized for the perceptron learn-
ing algorithm and support vector machines. In
contrast, we propose a generic way of using the
relation hierarchy which at the same time, gives
globally coherent predictions and allows for easy
injection of knowledge as constraints. Recently,
Jiang (2009) proposed using features which are
common across all relations. Her method is com-
plementary to our approach, as she does not con-
sider information such as the relatedness between
different relations. On using semantic resources,
Zhou et al (2005) gathered two gazettes, one
containing country names and another containing
words indicating personal relationships. In relat-
ing the tasks of RE and coreference resolution, Ji
et al (2005) used the output of a RE system to
rescore coreference hypotheses. In our work, we
reverse the setting and explore using coreference
to improve RE.
7 Conclusion
In this paper, we proposed a broad range of meth-
ods to inject background knowledge into a rela-
tion extraction system. Some of these methods,
such as exploiting the relation hierarchy, are gen-
eral in nature and could be easily applied to other
NLP tasks. To combine the various relation pre-
dictions and knowledge, we perform global infer-
ence within an ILP framework. Besides allowing
for easy injection of knowledge as constraints, this
ensures globally coherent models and predictions.
Acknowledgements This research was partly
sponsored by Air Force Research Laboratory
(AFRL) under prime contract no. FA8750-09-
C-0181. We thank Ming-Wei Chang and James
Clarke for discussions on this research.
References
Banko, Michele, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
159
Open information extraction from the web. In Pro-
ceedings of IJCAI-07, pages 2670?2676.
Boschee, Elizabeth, Ralph Weischedel, and Alex Za-
manian. 2005. Automatic information extraction.
In Proceedings of the International Conference on
Intelligence Analysis.
Brown, Peter F., Vincent J. Della Pietra, Peter V. deS-
ouza, Jenifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of HLT/EMNLP-05, pages
724?731.
Carlson, Andrew J., Chad M. Cumby, Jeff L. Rosen,
and Dan Roth. 1999. The SNoW learning archi-
tecture. Technical Report UIUCDCS-R-99-2101,
UIUC Computer Science Department, May.
Chang, Ming-Wei, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In Proceedings of AAAI-08, pages 1513?
1518.
Do, Quang and Dan Roth. 2010. On-the-
fly constraint-based taxonomic relation identifica-
tion. Technical report, University of Illinois.
http://L2R.cs.uiuc.edu/?danr/Papers/DoRo10.pdf.
Freund, Yoav and Robert E. Schapire. 1999. Large
margin classification using the perceptron algo-
rithm. Machine Learning, 37(3):277?296.
Ji, Heng, David Westbrook, and Ralph Grishman.
2005. Using semantic relations to refine corefer-
ence decisions. In Proceedings of HLT/EMNLP-05,
pages 17?24.
Jiang, Jing and ChengXiang Zhai. 2007. A system-
atic exploration of the feature space for relation ex-
traction. In Proceedings of HLT-NAACL-07, pages
113?120.
Jiang, Jing. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceed-
ings of ACL-IJCNLP-09, pages 1012?1020.
Kambhatla, Nanda. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for information extraction. In Proceedings
of ACL-04, pages 178?181.
Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08:HLT, pages 595?603.
Mintz, Mike, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of
ACL-IJCNLP-09, pages 1003?1011.
Ratinov, Lev and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL-09, pages 147?155.
Ratinov, Lev, Doug Downey, and Dan
Roth. 2010. Wikification for informa-
tion retrieval. Technical report, Univer-
sity of Illinois. http://L2R.cs.uiuc.edu/?
danr/Papers/RatinovDoRo10.pdf.
Roth, Dan and Wen Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL-04, pages
1?8.
Roth, Dan and Wen Tau Yih. 2007. Global infer-
ence for entity and relation identification via a lin-
ear programming formulation. In Getoor, Lise and
Ben Taskar, editors, Introduction to Statistical Rela-
tional Learning. MIT Press.
Weld, Daniel S., Raphael Hoffman, and Fei Wu. 2008.
Using wikipedia to bootstrap open information ex-
traction. ACM SIGMOD Special Issue on Managing
Information Extraction, 37(4):62?68.
Zelenko, Dmitry, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
Zhang, Min, Jie Zhang, Jian Su, and GuoDong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of COLING-ACL-06, pages 825?
832.
Zhou, Guodong, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of ACL-05.
Zhou, GuoDong, Min Zhang, DongHong Ji, and
QiaoMing Zhu. 2007. Tree kernel-based re-
lation extraction with context-sensitive structured
parse tree information. In Proceedings of EMNLP-
CoNLL-07, pages 728?736.
Zhou, Guodong, Min Zhang, Dong-Hong Ji, and
Qiaoming Zhu. 2008. Hierarchical learning strat-
egy in semantic relation extraction. Information
Processing & Management, 44(3):1008?1021.
160
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 877?885,
Beijing, August 2010
Knowing What to Believe
(when you already know something)
Jeff Pasternack Dan Roth
University of Illinois, Urbana-Champaign
{jpaster2, danr}@uiuc.edu
Abstract
Although much work in NLP has focused
on simply determining what a document
means, we also must know whether or not
to believe it. Fact-finding algorithms at-
tempt to identify the ?truth? among com-
peting claims in a corpus, but fail to
take advantage of the user?s prior knowl-
edge and presume that truth itself is uni-
versal and objective rather than subjec-
tive. We introduce a framework for incor-
porating prior knowledge into any fact-
finding algorithm, expressing both gen-
eral ?common-sense? reasoning and spe-
cific facts already known to the user as
first-order logic and translating this into
a tractable linear program. As our results
show, this approach scales well to even
large problems, both reducing error and
allowing the system to determine truth re-
spective to the user rather than the major-
ity. Additionally, we introduce three new
fact-finding algorithms capable of outper-
forming existing fact-finders in many of
our experiments.
1 Introduction
Although establishing the trustworthiness of the
information presented to us has always been a
challenge, the advent of the Information Age and
the Internet has made it more critical. Blogs,
wikis, message boards and other collaborative
media have eliminated the high entry barrier?
and, with it, the enforced journalistic standards?of
older, established media such as newspapers and
television, and even these sometimes loosen their
fact-checking in the face of increased competitive
pressure. Consequently, we find that corpora de-
rived from these sources now offer far more nu-
merous views of far more questionable veracity.
If one author claims Mumbai is the largest city in
the world, and another claims it is Seoul, who do
we believe? One or both authors could be inten-
tionally lying, honestly mistaken or, alternatively,
of different viewpoints of what constitutes a ?city?
(the city proper? The metropolitan area?) Truth is
not objective: there may be many valid definitions
of ?city?, but we should believe the claim that ac-
cords with our user?s viewpoint. Note that the user
may be another computational system rather than
a human (e.g. building a knowledge base of city
sizes for question answering), and often neither
the user?s nor the information source?s perspective
will be explicit (e.g. an author will not fully elabo-
rate ?the largest city by metropolitan area bounded
by...?) but will instead be implied (e.g. a user?s
statement that ?I already know the population of
city A is X, city B is Y...? implies that his defini-
tion of a city accords with these figures).
The most basic approach is to take a vote: if
multiple claims are mutually exclusive of each
other, select the one asserted by the most sources.
In our experiments, sources will be the authors
of the document containing the claim, but other
sources could be publishers/websites (when no
authorship is given), an algorithm that outputs
claims, etc. Although sometimes competitive, we
found voting to be generally lackluster. A class of
algorithms called fact-finders are often a dramatic
improvement, but are incapable of taking advan-
tage of the user?s prior knowledge. Our framework
translates prior knowledge (expressed as first-
order logic) into a linear program that constrains
the claim beliefs produced by a fact-finder, en-
suring that our belief state is consistent with both
common sense (?cities usually grow?) and known
facts (?Los Angeles is more populous than Wi-
chita?). While in the past first-order logic has been
translated to NP-hard integer linear programs, we
use polynomial-time-solvable linear programs, al-
877
lowing us to readily scale to large problems with
extensive prior knowledge, as demonstrated by
our experiments.
We next discuss related work, followed by
a more in-depth description of the fact-finding
algorithms used in our experiments, includ-
ing three novel, high-performing algorithms:
Average?Log, Investment, and PooledInvestment.
We then present the framework?s mechanics and
the translation of first-order logic into a linear pro-
gram. Finally, we present our experimental setup
and results over three domains chosen to illustrate
different aspects of the framework, demonstrating
that both our new fact-finders and our framework
offer performance improvements over the current
state of the art.
2 Related Work
The broader field of trust can be split into three ar-
eas of interest1: theoretical, reputation-based, and
information-based.
2.1 Theoretical
Marsh (1994) observes that trust can be global
(e.g. eBay?s feedback scores), personal (each per-
son has their own trust values), or situational (per-
sonal and specific to a context). Fact-finding algo-
rithms are based on global trust, while our frame-
work establishes personal trust by exploiting the
user?s individual prior knowledge.
Probabilistic logics have been explored as an
alternate method of reasoning about trust. Man-
chala (1998) utilizes fuzzy logic (Novak et al,
1999), an extension of propositional logic permit-
ting [0,1] belief over propositions. Yu and Singh
(2003) employs Dempster-Shafer theory (Shafer,
1976), with belief triples (mass, belief, and plausi-
bility) over sets of possibilities to permit the mod-
eling of ignorance, while Josang et al (2006) uses
the related subjective logic (Josang, 1997). While
our belief in a claim is decidedly Bayesian (the
probability that the claim is true), ?unknowns?
(discussed later) allow us to reason about igno-
rance as subjective logic and Dempster-Shafer do,
but with less complexity.
1Following the division proposed by Artz and Gil (2007);
see also (Sabater and Sierra, 2005) for a survey from a dif-
ferent perspective.
2.2 Reputation-based
Reputation-based systems determine an entity?s
trust or standing among peers via transitive rec-
ommendations, as PageRank (Brin and Page,
1998) does among web pages, Advogato (Levien,
2008) does among people, and Eigentrust (Kam-
var et al, 2003) does among peers in a net-
work. Some, such as Hubs and Authorities (Klein-
berg, 1999), are readily adapted to fact-finding, as
demonstrated later.
2.3 Information-Based
Information-based approaches utilize content
(rather than peer recommendations) to compute
trust, and are often specialized for a particular do-
main. For example, (Zeng et al, 2006) and Wik-
itrust (Adler and de Alfaro, 2007) determine trust
in a wiki?s text passages from sequences of revi-
sions but lack the claim-level granularity and gen-
eral applicability of fact-finders.
Given a large set of sources making conflicting
claims, fact-finders determine ?the truth? by iter-
atively updating their parameters, calculating be-
lief in facts based on the trust in their sources, and
the trust in sources based on belief in their facts.
TruthFinder (Yin et al, 2008) is a straightforward
implementation of this idea. AccuVote (Dong et
al., 2009a; Dong et al, 2009b) improves on this
by using calculated source dependence (where
one source derives its information from another)
to give higher credibility to independent sources.
(Galland et al, 2010)?s 3-Estimates algorithm in-
corporates the estimated ?hardness? of a fact, such
that knowing the answer to an easy question earns
less trust than to a hard one. Except for AccuVote
(whose model of repeated source-to-source copy-
ing is inapplicable to our experimental domains)
we experimented over all of these algorithms.
3 Fact-Finding
We have a set of sources S each asserting a set of
claims Cs, with C = ?s?S Cs. Each claim c ? C
belongs to a mutual exclusion set Mc ? C, a set
of claims (including c) that are mutually exclusive
with one another; for example, ?John was born
in 1960? and ?John was born in 1965? are mutu-
ally exclusive because a person cannot be born in
more than one year. If c is not mutually exclusive
878
to any other claims, then Mc = {c}. Assuming
there exists exactly one true claim c in each mu-
tual exclusion set M , our goal is to predict c for
each M , with accuracy measured by the number
of successful predictions divided by the number
of mutual exclusion sets, ignoring trivially cor-
rect claims that are the sole members of their mu-
tual exclusion set. To this end, fact-finding algo-
rithms iterate to find the trustworthiness of each
source T i(s) at iteration i in terms of the belief
in its claims in the previous iteration Bi?1(Cs),
and belief in each claim Bi(c) in terms of T i(Sc),
where Sc = {s : s ? S, c ? Cs} is the set of
all sources asserting c. Note that ?trustworthiness?
and ?belief? as used within a fact-finding algo-
rithm typically do not have meaningful semantics
(i.e. they are not [0, 1] Bayesian probabilities). It-
eration continues until convergence or some pre-
defined stop criteria.
3.1 Priors
Except for 3-Estimates (where the priors are dic-
tated by the algorithm itself), every fact-finder
requires priors for B0(C). For each fact-finder
we chose from B0voted(c) = |Sc|/
?
d?Mc |Sd|,
B0uniform(c) = 1/|Mc|, and B0fixed(c) = 0.5.
3.2 Algorithms
3.2.1 Sums (Hubs and Authorities)
Hubs and Authorities (Kleinberg, 1999) gives
each page a hub score and an authority score,
where its hub score is the sum of the authority of
linked pages and its authority is the sum of the
hub scores of pages linking to it. This is adapted
to fact-finding by viewing sources as hubs (with
0 authority) and claims as authorities (with 0 hub
score):
T i(s) =
?
c?Cs
Bi?1(c) Bi(c) =
?
s?Sc
T i(s)
We normalize to prevent T i(s) and Bi(c) from
growing unbounded (dividing by maxs T i(s) and
maxc Bi(c), respectively), a technique also used
with the Investment and Average?Log algorithms
(discussed next); this avoids numerical overflow.
B0fixed priors are used.
3.2.2 Average?Log
Computing T (s) as an average of belief in
its claims overestimates the trustworthiness of
a source with relatively few claims; certainly a
source with 90% accuracy over a hundred ex-
amples is more trustworthy than a source with
90% accuracy over ten. However, summing the
belief in claims allows a source with 10% accu-
racy to obtain a high trustworthiness score by sim-
ply making many claims. Average?Log attempts
a compromise, while still using Sums? Bi update
rule and B0fixed priors.
T i(s) = log |Cs| ?
?
c?Cs Bi?1(c)
|Cs|
3.2.3 Investment
In the Investment algorithm, sources ?in-
vest? their trustworthiness uniformly among their
claims. The belief in each claim then grows ac-
cording to a non-linear function G, and a source?s
trustworthiness is calculated as the sum of the be-
liefs in their claims, weighted by the proportion
of trust previously contributed to each (relative to
the other investors). Since claims with higher-trust
sources get higher belief, these claims become rel-
atively more believed and their sources become
more trusted. We used G(x) = xg with g = 1.2 in
our experiments, together with B0voted priors.
T i(s) =
?
c?Cs
Bi?1(c) ? T
i?1(s)
|Cs| ?
?
r?Sc
T i?1(r)
|Cr|
Bi(c) = G
(?
s?Sc
T i(s)
|Cs|
)
3.2.4 PooledInvestment
Like Investment, sources uniformly invest their
trustworthiness in claims and obtain correspond-
ing returns, so T i(s) remains the same, but now
after the belief in the claims of mutual exclusion
set M have grown according to G, they are lin-
early scaled such that the total belief of the claims
in M remains the same as it was before apply-
ing G(x) = xg, with g = 1.4 and B0uniform
priors used in our experiments. Given H i(c) =?
s?Sc
T i(s)
|Cs| , we have:
Bi(c) = H i(c) ? G(H
i(c))?
d?Mc G(H i(d))
879
3.3 TruthFinder
TruthFinder (Yin et al, 2008) is pseudoprobabilis-
tic: the basic version of the algorithm below cal-
culates the ?probability? of a claim by assuming
that each source?s trustworthiness is the proba-
bility of it being correct and then averages claim
beliefs to obtain trustworthiness scores. We also
used the ?full?, more complex TruthFinder, omit-
ted here for brevity. B0uniform priors are used for
both.
T i(s) =
?
c?Cs Bi?1(c)
|Cs|
Bi(c) = 1?
?
s?Sc
(
1? T i(s)
)
3.3.1 3-Estimates
3-Estimates (Galland et al, 2010), also omit-
ted for brevity, differs from the other fact-finders
by adding a third set of parameters to capture the
?difficulty? of a claim, such that correctly assert-
ing a difficult claim confers more trustworthiness
than asserting an easy one; knowing the exact pop-
ulation of a city is harder than knowing the popu-
lation of Mars (presumably 0) and we should not
trust a source merely because they provide what is
already common knowledge.
4 The Framework
To apply prior knowledge to a fact-finding algo-
rithm, we translate the user?s prior knowledge into
a linear program. We then iterate the following un-
til convergence or other stopping criteria:
1. Compute T i(s) for all s ? S
2. Compute Bi(c) for all c ? C
3. ?Correct? beliefs Bi(C) with the LP
4.1 Propositional Linear Programming
To translate prior knowledge into a linear pro-
gram, we first propositionalize our first-order
formulae into propositional logic (Russell and
Norvig, 2003). For example, assume we know that
Tom is older than John and a person has exactly
one age (?x,yAge(Tom, x)?Age(John, y)?x >
y) ? (?x,y,zAge(x, y) ? y 6= z ? ?Age(x, z)),
and our system is considering the follow-
ing claims: Age(Tom, 30), Age(Tom, 40),
Age(John, 25), Age(John, 35). Our proposi-
tional clauses (after removing redundancies) are
then Age(Tom, 30) ? Age(John, 25) ?
(Age(Tom, 30) ? Age(Tom, 40)) ?
(Age(John, 25)?Age(John, 35)).
Each claim c will be represented by a propo-
sition, and ultimately a [0, 1] variable in the
linear program corresponding, informally, to
P (c).2 Propositionalized constraints have previ-
ously been used with integer linear programming
(ILP) using binary {0, 1} values corresponding
to {false, true}, to find an (exact) consistent
truth assignment minimizing some cost and solve
a global inference problem, e.g. (Roth and Yih,
2004; Roth and Yih, 2007). However, proposi-
tional linear programming has two significant ad-
vantages:
1. ILP is ?winner take all?, shifting all belief to
one claim in each mutual exclusion set (even
when other claims are nearly as plausible)
and finding the single most believable con-
sistent binary assignment; we instead wish to
find a distribution of belief over the claims
that is consistent with our prior knowledge
and as close as possible to the distribution
produced by the fact-finder.
2. Linear programs can be solved in polynomial
time (e.g. by interior point methods (Kar-
markar, 1984)), but ILP is NP-hard.
To create our constraints, we first convert our
propositional formula into conjunctive normal
form. Then, for each disjunctive clause consisting
of a set P of positive literals (claims) and a set
N of negations of literals, we add the constraint?
c?P cv +
?
c?N (1? cv) ? 1, where cv de-
notes the [0, 1] variable corresponding to each c.
The left-hand side is the union bound of at least
one of the claims being true (or false, in the case
of negated literals); if this bound is at least 1, the
constraint is satisfied. This optimism can dilute
the strength of our constraints by ignoring poten-
tial dependence among claims: x ? y, x ? y im-
plies y is true, but since we demand only yv ? xv
and xv + yv ? 1 we accept any yv ? 0.5 where
2This is a slight mischaracterization, since our linear con-
straints only approximate intersections and unions of events
(where each event is ?claim c is true?), and we will be satis-
fying them subject to a linear cost function.
880
yv ? xv ? 1 ? yv. However, when the claims
are mutually exclusive, the union bound is exact; a
common constraint is of the form q ? r1?r2?. . .,
where the r literals are mutually exclusive, which
translates exactly to r1v + r2v + . . . ? qv. Fi-
nally, observe that mutual exclusion amongst n
claims c1, c2, . . ., cn can be compactly written as
c1v + c2v + . . .+ cnv = 1.
4.2 The Cost Function
Having seen how first-order logic can be con-
verted to linear constraints, we now consider the
cost function, a distance between the new distri-
bution of belief satisfying our constraints and the
original distribution produced by the fact-finder.
First we determine the number of ?votes? re-
ceived by each claim c, computed as ?c =
?(B(c)), which should scale linearly with the cer-
tainty of the fact-finder?s belief in c. Recall that
the semantics of the belief score are particular
to the fact-finder, so different fact-finders require
different vote functions. TruthFinder has pseudo-
probabilistic [0,1] beliefs, so we use ?inv(x) =
min((1 ? x)-1,minv) with minv = 1010 limiting
the maximum number of votes possible; we as-
sume 1/0 = ?. ?inv intuitively scales with ?er-
ror?: a belief of 0.99 receives ten times the votes
of 0.9 and has a tenth the error (0.01 vs. 0.1).
For the remainder of the fact-finders whose beliefs
are already ?linear?, we use the identity function
?idn(x) = x.
The most obvious choice for the cost func-
tion might be to minimize ?frustrated votes?:?
c?C ?c(1 ? cv). Unfortunately, this results in
the linear solver generally assigning 1 to the vari-
able in each mutual exclusion set with the most
votes and 0 to all others (except when constraints
prevent this), shifting all belief to the highest-vote
claim and yielding poor performance. Instead, we
wish to satisfy the constraints while keeping each
cv close to ?c/?Mc , where ?Mc =
?
d?Mc ?d,and so shift belief among claims as little as possi-
ble. We use a weighted Manhattan distance called
VoteDistance, where the cost for increasing the
belief in a claim is proportional to the number of
votes against it, and the cost for decreasing belief
is proportional to the number of votes for it:
?
c?C
max
(
(?Mc ? ?c) ? (cv ? ?c/?Mc),
?c ? (?c/?Mc ? cv)
)
Thus, the belief distribution found by our LP
will be the one that satisfies the constraints while
simultaneously minimizing the number of votes
frustrated by the change from the original dis-
tribution. Note that for any linear expressions e
and f we can implement max(e, f) in the objec-
tive function by replacing it with a new [??,?]
helper variable x and adding the linear constraints
x ? e and x ? f .
4.3 From Values to Votes to Belief
Solving the LP gives us [0, 1] values for each vari-
able cv, but we need to calculate an updated belief
B(c). We propose two methods for this:
Vote Conservation: B(c) = ??1(cv ? ?Mc)
Vote Loss: B(c) = ??1(min(?c, cv ? ?Mc))
??1 is an inverse of the vote function:
??1idn(x) = x and ??1inv(x) = 1 ? (1 + y)?1. VoteConservation reallocates votes such that the total
number of votes in each mutual exclusion set, ?M ,
remains the same after the redistribution. How-
ever, if the constraints force c to lose votes, should
we believe the other claims in Mc more? Under
Vote Loss, a claim can only lose votes, ensuring
that if other claims in Mc become less believable,
c does not itself become more believable relative
to claims in other mutual exclusion sets. We found
Vote Loss just slightly better on average and used
it for all reported results.
4.4 ?Unknown? Augmentation
Augmenting our data with ?Unknown? claims en-
sures that every LP is feasible and can be used
to model our ignorance given a lack of suffi-
cient information or conflicting constraints. An
Unknown claim UM is added to every mutual ex-
clusion set M (but invisible to the fact-finder) and
represents our belief that none of the claims in
M are sufficiently supported. Now we can write
the mutual exclusion constraint for M as UM +?
c?M cv = 1. When propositionalizing FOL, if
a disjunctive clause contains a non-negated literal
for a claim c, then we add ?UMc to the clause.
881
For example, Age(John, 35) ? Age(Tom, 40)
becomes Age(John, 35) ? Age(Tom, 40) ?
Age(Tom,Unknown). The only exception is
when the clause contains claims from only one
mutual exclusion set (e.g. ?I know Sam is 50
or 60?), and so the LP can only be infeasible
if the user directly asserts a contradiction (e.g.
?Sam is 50 and Sam is 60?). The Unknown it-
self has a fixed number of votes that cannot be
lost; this effectively ?smooths? our belief in the
claims and imposes a floor for believability. If
Age(Kim, 30) has 5 votes, Age(Kim, 35) has
3 votes, and Age(Kim,Unknown) is fixed at 6
votes, we hold that Kim?s age is unknown due to
lack of evidence. The number of votes that should
be given to each Unknown for this purpose de-
pends, of course, on the particular fact-finder and
? function used; in our experiments, we are not
concerned with establishing ignorance and thus
assign 0 votes.
5 Experiments
Experiments were conducted over three domains
(city population, basic biographies, and Ameri-
can vs. British spelling) with four datasets, all
using the VoteDistance cost function and Vote
Loss vote redistribution. We fixed the number of
iterations of the framework (calculating T i(S),
Bi(S) and then solving the LP) at 20, which
was found sufficient for all fact-finders. To eval-
uate accuracy, after the final iteration we look
at each mutual exclusion set M and predict the
highest-belief claim c ? M (or, if uM had the
highest belief, the second-highest claim), break-
ing ties randomly, and check that it is the true
claim tM . We omit any M that does not contain
a true claim (all known claims are false) and any
M that is trivially correct (containing only one
claim other than uM ). All results are shown in
Table 1. Vote is the baseline, choosing either the
claim occurring in the most Wikipedia revisions
(in the Pop dataset) or claimed by the most sources
(for all other datasets). Sum is Sums (Hubs and
Authorities), 3Est is 3-Estimates, TFs is simpli-
fied TruthFinder, TFc is ?full? TruthFinder, A?L is
Average?Log, Inv1.2 is Investment with g = 1.2,
and Pool1.4 is PooledInvestment with g = 1.4.
5.1 IBT vs. L+I
We can enforce our prior knowledge against the
beliefs produced by the fact-finder in each itera-
tion, or we can apply these constraints just once,
after running the fact-finder for 20 iterations with-
out interference. By analogy to (Punyakanok et
al., 2005), we refer to these approaches as infer-
ence based training (IBT) and learning + inference
(L+I), respectively. Our results show that while
L+I does better when prior knowledge is not en-
tirely correct (e.g. ?Growth? in the city popula-
tion domain), generally performance is compara-
ble when the effect of the constraints is mild, but
IBT can outperform when prior knowledge is vital
(as in the spelling domain) by allowing the fact-
finder to learn from the provided corrections.
5.2 Wikipedia Infoboxes
To focus on the performance of the framework,
we (like previous fact-finding work) naively as-
sume that our data are accurately extracted, but we
also require large corpora. Wikipedia Infoboxes
(Wu and Weld, 2007) are a semi-structured source
covering many domains with readily available au-
thorship, and we produced our city population and
basic biographic datasets from the most recent
full-history dump of the English Wikipedia (taken
January 2008). However, attribution is difficult: if
an author edits the page but not the claim within
the infobox, is the author implicitly agreeing with
(and asserting) the claim? The best performance
was achieved by being strict for City Population
data, counting only the direct editing of a claim,
and lax for Biography data, counting any edit.
We hypothesize this is because editors may lack
specific knowledge about a city?s population (and
thus fail to correct an erroneous value) but incor-
rect birth or death dates are more noticeable.
5.3 Results
5.3.1 City Population
We collected infoboxes for settlements
(Geobox, Infobox Settlement, Infobox City, etc.)
to obtain 44,761 populations claims qualified
by year (e.g. pop(Denver, 598707, 2008)), with
4,107 authors total. We took as our ?truth?
U.S. census data, which gave us 308 non-
trivial true facts to test against. Our ?common
sense? knowledge is that population grows
882
Table 1: Experimental Results (? indicates no prior knowledge; all values are percent accuracy)
Some results are omitted here (see text). A?L, Inv1.2, Pool1.4 are our novel algorithms
Dataset Prior Knowledge Vote Sum 3Est TFs TFc A?L Inv1.2 Pool1.4
Pop ? 81.49 81.82 81.49 82.79 84.42 80.84 87.99 80.19
Pop GrowthIBT 82.79 79.87 77.92 82.79 86.36 80.52 85.39 79.87
Pop GrowthL+I 82.79 79.55 77.92 83.44 85.39 80.52 89.29 80.84
Pop Larger2500IBT 85.39 85.06 80.52 86.04 87.34 84.74 89.29 84.09
Pop Larger2500L+I 85.39 85.06 80.52 86.69 86.69 84.42 89.94 84.09
SynPop ? 73.45 87.76 84.87 56.12 87.07 90.23 89.41 90.00
SynPop Pop?8%IBT 88.31 95.46 92.16 96.42 95.46 96.15 95.46 96.42
SynPop Pop?8%L+I 88.31 94.77 92.43 82.39 95.32 95.59 96.29 96.01
Bio ? 89.80 89.53 89.80 73.04 90.09 89.24 88.34 90.01
Bio CSIBT 89.20 89.61 89.20 72.44 89.91 89.35 88.60 90.20
Bio CSL+I 89.20 89.61 89.20 57.10 90.09 89.35 88.49 90.24
Bio CS+DecadesIBT 90.58 90.88 90.58 80.30 91.25 90.91 90.02 91.32
Bio CS+DecadesL+I 90.58 90.91 90.58 69.27 90.95 90.91 90.09 91.17
Spell ? 13.54 9.37 11.96 41.93 7.93 10.23 9.36 9.65
Spell Words100IBT 13.69 9.02 12.72 44.28 8.05 9.98 11.11 8.86
Spell Words100L+I 13.69 8.86 12.08 46.54 8.05 9.98 9.34 7.89
Spell CS+Words100IBT 35.10 31.88 35.10 56.52 29.79 32.85 73.59 80.68
Spell CS+Words100L+I 35.10 31.72 34.62 55.39 22.06 32.21 30.92 29.95
over time (?Growth? in table 1); therefore,
?v,w,x,y,zpop(v, w, y) ? pop(v, x, z) ? y < z ?
x > w. Of course, this often does not hold
true: cities can shrink, but performance was
nevertheless superior to no prior knowledge
whatsoever. The L+I approach does appreciably
better because it avoids forcing these sometimes-
incorrect constraints onto the claim beliefs while
the fact-finder iterates (which would propagate
the resulting mistakes), instead applying them
only at the end where they can correct more errors
than they create. The sparsity of the data plays
a role?only a fraction of cities have population
claims for multiple years, and those that do are
typically larger cities where the correct claim is
asserted by an overwhelming majority, greatly
limiting the potential benefit of our Growth
constraints. We also considered prior knowledge
of the relative sizes of some cities, randomly
selecting 2500 pairs of them (a, b), where a
was more populous than b in year t, asserting
?x,ypop(a, x, t) ? pop(b, y, t) ? x > y. This
?Larger? prior knowledge proved more effective
than our oft-mistaken Growth constraint, with
modest improvement to the highest-performing
Investment fact-finder, and InvestmentL+I
reaches 90.91% with 10,000 such pairs.
5.3.2 Synthetic City Population
What if attribution were certain and the data
more dense? To this end we created a synthetic
dataset. We chose 100 random (real) cities and
created 100 authors whose individual accuracy
a was drawn uniformly from [0, 1]. Between 1
and 10 claims (also determined uniformly) were
made about each city in each year from 2000
to 2008 by randomly-selected authors. For each
city with true population p and year, four incor-
rect claims were created with populations selected
uniformly from [0.5p, 1.5p], each author claiming
p with probability a and otherwise asserting one
of the four incorrect claims. Our common-sense
knowledge was that population did not change
by more than 8% per year (also tried on the
Wikipedia dataset but with virtually no effect).
Like ?Growth?, ?Pop?8%? does not always hold,
but a change of more than 8% is much rarer than a
shrinking city. These constraints greatly improved
results, although we note this would diminish if
inaccurate claims had less variance around the
true population.
883
5.3.3 Basic Biographies
We scanned infoboxes to find 129,847 claimed
birth dates, 34,201 death dates, 10,418 parent-
child pairs, and 9,792 spouses. To get ?true? birth
and death dates, we extracted data from sev-
eral online repositories (after satisfying ourselves
that they were independent and not derived from
Wikipedia!), eliminating any date these sources
disagreed upon, and ultimately obtained a total
of 2,685 dates to test against. Our common sense
(?CS?) knowledge was: nobody dies before they
are born, people are infertile before the age of 7,
nobody lives past 125, all spouses have overlap-
ping lifetimes, no child is born more than a year
after a parent?s (father?s) death, nobody has more
than two parents, and nobody is born or dies after
2008 (the ?present day?, the year of the Wikipedia
dump). Applying this knowledge roughly halved
convergence times, but had little effect on the re-
sults due to data sparsity similar to that seen in
the population data?while we know many birth-
days and some death dates, relatively few biogra-
phies had parent-child and spouse claims. To this
we also added knowledge of the decade (but not
the exact date) in which 15,145 people were born
(?CS+Decades?). Although common sense alone
does not notably improve results, it does very well
in conjunction with specific knowledge.
5.3.4 American vs. British Spelling
Prior knowledge allows us to find a truth that
conforms with the user?s viewpoint, even if that
viewpoint differs from the norm. After obtaining
a list of words with spellings that differed be-
tween American and British English (e.g. ?color?
vs. ?colour?), we examined the British National
Corpus as well as Washington Post and Reuters
news articles, taking the source?s (the article au-
thor?s) use of a disputed word as a claim that
his spelling was correct. Our goal was to find the
?true? British spellings that conformed to a British
viewpoint, but American spellings predominate
by far. Consequently, without prior knowledge the
fact-finders do very poorly against our test set of
694 British words, predicting American spelling
instead in accordance with the great majority of
authors (note that accuracy from an American
perspective is 1??British? accuracy). Next we
assumed that the user already knew the correct
spelling of 100 random words (removing these
from the test set, of course), but with little ef-
fect. Finally, we added our common sense (?CS?)
knowledge: if a spelling a is correct and of length
? 4, then if a is a substring of b, a? b (e.g. colour
? colourful). Furthermore, while we do not know
a priori whether a spelling is American or British,
we do know if e and f are different spellings
of the same word, and, if two such spellings
have a chain of implication between them, we
can break all links in this chain (while some
American spellings will still be linked to British
spellings, this removes most such errors). Interest-
ingly, common sense alone actually hurts results
(e.g. PooledInvestment (IBT) gets 6.2%), as it es-
sentially makes the fact-finders more adept at find-
ing the predominant American spellings! How-
ever, when some correct spellings are known, re-
sults improve greatly and demonstrate IBT?s abil-
ity to spread strong prior knowledge, easily sur-
passing L+I. Results improve further with more
known spellings (PooledInvestment gets 84.86%
with CS+Words200IBT ).
6 Conclusion
We have introduced a new framework for in-
corporating prior knowledge into a fact-finding
system, along with several new high-performing
fact-finding algorithms (Investment, PooledIn-
vestment, and Average?Log). While the bene-
fits of prior knowledge were most dramatic in
the Spelling domain, we saw gains from both
?common sense? and specific knowledge in all
experiments?even the difficult Biography domain
saw faster convergence with common sense alone
and notably higher results when specific knowl-
edge was added. We find that while prior knowl-
edge is helpful in reducing error, when the user?s
viewpoint disagrees with the norm it becomes ab-
solutely essential and, formulated as a linear pro-
gram, it need not be the computational burden that
might otherwise be expected.
Acknowledgements
This research was partly sponsored by the Army Research
Laboratory (ARL) (accomplished under Cooperative Agree-
ment Number W911NF-09-2-0053). Any opinions, findings,
and conclusion or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect the
view of the ARL.
884
References
Adler, B T and L de Alfaro. 2007. A content-driven reputa-
tion system for the Wikipedia. WWW ?07, 7:261?270.
Artz, D and Y Gil. 2007. A survey of trust in computer
science and the Semantic Web. Web Semantics: Science,
Services and Agents on the World Wide Web, 5(2):58?71,
June.
Brin, S and L Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks and
ISDN Systems, 30(1-7):107?117.
Dong, X, L Berti-equille, and D Srivastava. 2009a. Integrat-
ing conflicting data: the role of source dependence. Tech-
nical report, AT&T Labs-Research, Florham Park, NJ.
Dong, X.L., L. Berti-Equille, and Divesh Srivastava. 2009b.
Truth discovery and copying detection in a dynamic
world. VLDB, 2(1):562?573.
Galland, Alban, Serge Abiteboul, A. Marian, and Pierre
Senellart. 2010. Corroborating information from dis-
agreeing views. In Proceedings of the third ACM interna-
tional conference on Web search and data mining, pages
131?140. ACM.
Josang, A., S. Marsh, and S. Pope. 2006. Exploring different
types of trust propagation. Lecture Notes in Computer
Science, 3986:179.
Josang, A. 1997. Artificial reasoning with subjective logic.
2nd Australian Workshop on Commonsense Reasoning.
Kamvar, S, M Schlosser, and H Garcia-molina. 2003. The
Eigentrust algorithm for reputation management in P2P
networks. WWW ?03.
Karmarkar, N. 1984. A new polynomial-time algorithm for
linear programming. Combinatorica, 4(4):373?395.
Kleinberg, J M. 1999. Authoritative sources in a hyperlinked
environment. Journal of the ACM, 46(5):604?632.
Levien, R. 2008. Attack-resistant trust metrics. Computing
with Social Trust, pages 121?132.
Manchala, D.W. 1998. Trust metrics, models and protocols
for electronic commerce transactions. Proceedings. 18th
International Conference on Distributed Computing Sys-
tems (Cat. No.98CB36183), pages 312?321.
Marsh, S. 1994. Formalising Trust as a Computational Con-
cept. PhD thesis, University of Stirling.
Novak, V, I Perfilieva, and J Mockof. 1999. Mathematical
principles of fuzzy logic. Kluwer Academic Publishers.
Punyakanok, V., D. Roth, W. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In Inter-
national Joint Conference on Artificial Intelligence, vol-
ume 19.
Roth, Dan and Wen-tau Yih. 2004. A linear programming
formulation for global inference in natural language tasks.
In Proc. of the Annual Conference on Computational Nat-
ural Language Learning (CoNLL), pages 1?8.
Roth, D and W Yih. 2007. Global Inference for Entity and
Relation Identification via a Linear Programming Formu-
lation. In Getoor, Lise and Ben Taskar, editors, Introduc-
tion to Statistical Relational Learning. MIT Press.
Russell, Stuart and Peter Norvig. 2003. Artificial Intelli-
gence: A Modern Approach. Prentice Hall, second edi-
tion.
Sabater, Jordi and Carles Sierra. 2005. Review on Compu-
tational Trust and Reputation Models. Artificial Intelli-
gence Review, 24(1):33?60, September.
Shafer, G. 1976. A mathematical theory of evidence. Prince-
ton University Press Princeton, NJ.
Wu, Fei and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. Proceedings of the sixteenth ACM
conference on Conference on information and knowledge
management - CIKM ?07, page 41.
Yin, Xiaoxin, Philip S. Yu, and Jiawei Han. 2008. Truth Dis-
covery with Multiple Conflicting Information Providers
on the Web. IEEE Transactions on Knowledge and Data
Engineering, 20(6):796?808.
Yu, Bin and Munindar P. Singh. 2003. Detecting deception
in reputation management. Proceedings of the second in-
ternational joint conference on Autonomous agents and
multiagent systems - AAMAS ?03, page 73.
Zeng, H, M Alhossaini, L Ding, R Fikes, and D L McGuin-
ness. 2006. Computing trust from revision history. Intl.
Conf. on Privacy, Security and Trust.
885
Coling 2010: Poster Volume, pages 1265?1273,
Beijing, August 2010
Citation Author Topic Model in Expert Search
Yuancheng Tu, Nikhil Johri, Dan Roth, Julia Hockenmaier
University of Illinois at Urbana-Champaign
{ytu,njohri2,danr,juliahmr}@illinois.edu
Abstract
This paper proposes a novel topic model,
Citation-Author-Topic (CAT) model that
addresses a semantic search task we define
as expert search ? given a research area as
a query, it returns names of experts in this
area. For example, Michael Collins would
be one of the top names retrieved given the
query Syntactic Parsing.
Our contribution in this paper is two-fold.
First, we model the cited author informa-
tion together with words and paper au-
thors. Such extra contextual information
directly models linkage among authors
and enhances the author-topic association,
thus produces more coherent author-topic
distribution. Second, we provide a prelim-
inary solution to the task of expert search
when the learning repository contains ex-
clusively research related documents au-
thored by the experts. When compared
with a previous proposed model (Johri
et al, 2010), the proposed model pro-
duces high quality author topic linkage
and achieves over 33% error reduction
evaluated by the standard MAP measure-
ment.
1 Introduction
This paper addresses the problem of searching for
people with similar interests and expertise, given
their field of expertise as the query. Many existing
people search engines need people?s names to do a
?keyword? style search, using a person?s name as
a query. However, in many situations, such infor-
mation is insufficient or impossible to know be-
forehand. Imagine a scenario where the statistics
department of a university invited a world-wide
known expert in Bayesian statistics and machine
learning to give a keynote speech; how can the
organizer notify all the people on campus who
are interested without spamming those who are
not? Our paper proposes a solution to the afore-
mentioned scenario by providing a search engine
which goes beyond ?keyword? search and can re-
trieve such information semantically. The orga-
nizer would only need to input the research do-
main of the keynote speaker, i.e. Bayesian statis-
tics, machine learning, and all professors and stu-
dents who are interested in this topic will be re-
trieved and an email agent will send out the infor-
mation automatically.
Specifically, we propose a Citation-Author-
Topic (CAT) model which extracts academic re-
search topics and discovers different research
communities by clustering experts with similar in-
terests and expertise. CAT assumes three steps of
a hierarchical generative process when producing
a document: first, an author is generated, then that
author generates topics which ultimately generate
the words and cited authors. This model links
authors to observed words and cited authors via
latent topics and captures the intuition that when
writing a paper, authors always first have topics
in their mind, based on which, they choose words
and cite related works.
Corpus linguists or forensic linguists usually
1265
identify authorship of disputed texts based on
stylistic features, such as vocabulary size, sen-
tence length, word usage that characterize a spe-
cific author and the general semantic content is
usually ignored (Diederich et al, 2003). On the
other hand, graph-based and network based mod-
els ignore the content information of documents
and only focus on network connectivity (Zhang
et al, 2007; Jurczyk and Agichtein, 2007). In
contrast, the model we propose in this paper fully
utilizes the content words of the documents and
combines them with the stylistic flavor contex-
tual information to link authors and documents to-
gether to not only identify the authorship, but also
to be used in many other applications such as pa-
per reviewer recommendation, research commu-
nity identification as well as academic social net-
work search.
The novelty of the work presented in this pa-
per lies in the proposal of jointly modeling the
cited author information and using a discrimi-
native multinomial distribution to model the co-
author information instead of an artificial uni-
form distribution. In addition, we apply and eval-
uate our model in a semantic search scenario.
While current search engines cannot support in-
teractive and exploratory search effectively, our
model supports search that can answer a range of
exploratory queries. This is done by semantically
linking the interests of authors to the topics of the
collection, and ultimately to the distribution of the
words in the documents.
In the rest of this paper, we first present some
related work on author topic modeling and expert
search in Sec. 2. Then our model is described in
Sec. 3. Sec. 4 introduces our expert search system
and Sec. 5 presents our experiments and the evalu-
ation. We conclude this paper in Sec. 6 with some
discussion and several further developments.
2 Related Work
Author topic modeling, originally proposed
in (Steyvers et al, 2004; Rosen-Zvi et al, 2004),
is an extension of Latent Dirichlet Allocation
(LDA) (Blei et al, 2003), a probabilistic genera-
tive model that can be used to estimate the proper-
ties of multinomial observations via unsupervised
learning. LDA represents each document as a
mixture of probabilistic topics and each topic as
a multinomial distribution over words. The Au-
thor topic model adds an author layer over LDA
and assumes that the topic proportion of a given
document is generated by the chosen author.
Author topic analysis has attracted much atten-
tion recently due to its broad applications in ma-
chine learning, text mining and information re-
trieval. For example, it has been used to pre-
dict authors for new documents (Steyvers et al,
2004), to recommend paper reviewers (Rosen-Zvi
et al, 2004), to model message data (Mccallum et
al., 2004), to conduct temporal author topic anal-
ysis (Mei and Zhai, 2006), to disambiguate proper
names (Song et al, 2007), to search academic so-
cial networks (Tang et al, 2008) and to generate
meeting status analyses for group decision mak-
ing (Broniatowski, 2009).
In addition, there are many related works on
expert search at the TREC enterprise track from
2005 to 2007, which focus on enterprise scale
search and discovering relationships between enti-
ties. In that setting, the task is to find the experts,
given a web domain, a list of candidate experts
and a set of topics 1. The task defined in our paper
is different in the sense that our topics are hid-
den and our document repositories are more ho-
mogeneous since our documents are all research
papers authored by the experts. Within this set-
ting, we can explore in depth the influence of the
hidden topics and contents to the ranking of our
experts. Similar to (Johri et al, 2010), in this pa-
per we apply CAT in a semantic retrieval scenario,
where searching people is associated with a set of
hidden semantically meaningful topics instead of
their personal names.
In recent literature, there are three main lines of
work that extend author topic analyses. One line
of work is to relax the model?s ?bag-of-words?
assumption by automatically discovering multi-
word phrases and adding them into the original
model (Johri et al, 2010). Similar work has also
been proposed for other topic models such as
Ngram topic models (Wallach, 2006; Wang and
McCallum, 2005; Wang et al, 2007; Griffiths et
al., 2007).
1http://trec.nist.gov/pubs.html
1266
Another line of work models authors informa-
tion as a general contextual information (Mei and
Zhai, 2006) or associates documents with network
structure analysis (Mei et al, 2008; Serdyukov et
al., 2008; Sun et al, 2009). This line of work
aims to propose a general framework to deal with
collections of texts with an associated networks
structure. However, it is based on a different topic
model than ours; for example, Mei?s works (Mei
and Zhai, 2006; Mei et al, 2008) extend proba-
bilistic latent semantic analysis (PLSA), and do
not have cited author information explicitly.
Our proposal follows the last line of work
which extends author topic modeling with spe-
cific contextual information and directly captures
the association between authors and topics to-
gether with this contextual information (Tang et
al., 2008; Mccallum et al, 2004). For exam-
ple, in (Tang et al, 2008), publication venue is
added as one extra piece of contextual informa-
tion and in (Mccallum et al, 2004), email recip-
ients, which are treated as extra contextual infor-
mation, are paired with email authors to model an
email message corpus. In our proposed method,
the extra contextual information consists of the
cited authors in each documents. Such contextual
information directly captures linkage among au-
thors and cited authors, enhances author-topic as-
sociations, and therefore produces more coherent
author-topic distributions.
3 The Citation-Author-Topic (CAT)
Model
CAT extends previously proposed author topic
models by explicitly modelling the cited author
information during the generative process. Com-
pared with these models (Rosen-Zvi et al, 2004;
Johri et al, 2010), whose plate notation is shown
in Fig. 1, CAT (shown in Fig. 2) adds cited au-
thor information and generates authors according
to the observed author distribution.
Four plates in Fig. 1 represent topic (T ), au-
thor (A), document (D) and words in each doc-
ument (Nd) respectively. CAT (Fig. 2) has one
more plate, cited-author topic plate, in which each
topic is represented as a multinomial distribution
over all cited authors (?c).
Within CAT, each author is associated with a
 
D
A
N d
Figure 1: Plate notation of the previously pro-
posed author topic models (Rosen-Zvi et al,
2004; Johri et al, 2010).

D
A
N d

Figure 2: Plate notation of our current model:
CAT generates words W and cited authors C in-
dependently given the topic.
multinomial distribution over all topics, ~?a, and
each topic is a multinomial distribution over all
words, ~?t, as well as a multinomial distribution
over all cited authors ~?c. Three symmetric Dirich-
let conjugate priors, ?, ? and ?, are defined for
each of these three multinomial distributions in
CAT as shown in Fig. 2.
The generative process of CAT is formally de-
fined in Algorithm 1. The model first samples
the word-topic, cited author-topic and the author-
topic distributions according to the three Dirich-
let hyperparameters. Then for each word in each
document, first the author k is drawn from the
observed multinomial distribution and that author
chooses the topic zi, based on which word wi and
cited author ci are generated independently.
CAT differs from previously proposed MAT
(Multiword-enhanced Author Topic) model (Johri
et al, 2010) in two aspects. First of all, CAT uses
1267
Algorithm 1: CAT: A, T ,D,N are four
plates as shown in Fig. 2. The generative pro-
cess of CAT modeling.
Data: A, T ,D,N
for each topic t ? T do
draw a distribution over words:
~?t ? DirN (?) ;
draw a distribution over cited authors:
~?c ? DirC(?) ;
for each author a ? A do
draw a distribution over topics:
~?a ? DirT (?) ;
for each document d ? D and k authors ? d
do
for each word w ? d do
choose an author
k ? Multinomial(Ad) ;
assign a topic i given the author:
zk,i|k ? Multinomial(?a) ;
draw a word from the chosen topic:
wd,k,i|zk,i ? Multinomial(?zk,i) ;
draw a cited author from the topic:
cd,k,i|zk,i ? Multinomial(?zk,i)
cited author information to enhance the model
and assumes independence between generating
the words and cited authors given the topic. Sec-
ondly, instead of an artificial uniform distribution
over all authors and co-authors, CAT uses the ob-
served discriminative multinomial distribution to
generate authors.
3.1 Parameter Estimation
CAT includes three sets of parameters. The T
topic distribution over words, ?t which is similar
to that in LDA. The author-topic distribution ?a as
well as the cited author-topic distribution ?c. Al-
though CAT is a relatively simple model, finding
its posterior distribution over these hidden vari-
ables is still intractable due to their high dimen-
sionality. Many efficient approximate inference
algorithms have been used to solve this problem
including Gibbs sampling (Griffiths and Steyvers,
2004; Steyvers and Griffiths, 2007; Griffiths et al,
2007) and mean-field variational methods (Blei et
al., 2003). Gibbs sampling is a special case of
Markov-Chain Monte Carlo (MCMC) sampling
and often yields relatively simple algorithms for
approximate inference in high dimensional mod-
els.
In our CAT modeling, we use a collapsed Gibbs
sampler for our parameter estimation. In this
Gibbs sampler, we integrated out the hidden vari-
ables ?, ? and ? using the Dirichlet delta func-
tion (Heinrich, 2009). The Dirichlet delta func-
tion with an M dimensional symmetric Dirichlet
prior ? is defined as:
?M (?) =
?
(
?M
)
? (M?)
Based on the independence assumptions de-
fined in Fig. 2, the joint distribution of topics,
words and cited authors given all hyperparame-
ters which originally represented by integrals can
be transformed into the delta function format and
formally derived in Equation 1.
P (~z, ~w,~c|?, ?, ?) (1)
= P (~z|?, ?, ?)P (~w,~c|~z, ?, ?, ?)
= P (~z)P (~w|~z)P (~c|~z)
=
A?
a=1
?(nA+?)
?(?)
T?
z=1
?(nZw+?)
?(?)
T?
z=1
?(nZc+?)
?(?)
The updating equation from which the Gibbs
sampler draws the hidden variable for the current
state j, i.e., the conditional probability of drawing
the kth author Kkj , the ith topic Zij , and the cth
cited author Ccj tuple, given all the hyperparame-
ters and all the observed documents and authors,
cited authors except the current assignment (the
exception is denoted by the symbol ??j), is de-
fined in Equation 2.
P (Zij ,Kkj , Ccj |Wwj ,??j, Ad, ?, ?, ?) (2)
? ?(nZ+?)?(nZ,?j+?)
?(nK+?)
?(nK,?j+?)
?(nC+?)
?(nC,?j+?)
= n
w
i,?j+?w
V
P
w=1
nwi,?j+V ?w
nik,?j+?i
T
P
i=1
nik,?j+T?i
nci,?j+?c
C
P
c=1
nci,?j+C?c
The parameter sets ? and ?, ? can be interpreted
as sufficient statistics on the state variables of
the Markov Chain due to the Dirichlet conjugate
priors we used for the multinomial distributions.
1268
These three sets of parameters are estimated based
on Equations 3 , 4 and 5 respectively, in which nwi
is defined as the number of times the word w is
generated by topic i; nik is defined as the number
of times that topic i is generated by author k and
nic is defined as the number of times that the cited
author c is generated by topic i. The vocabulary
size is V , the number of topics is T and the cited-
author size is C.
?w,i =
nwi + ?w
V?
w=1
nwi + V ?w
(3)
?k,i =
nik + ?i
T?
i=1
nik + T?i
(4)
?c,i =
nci + ?c
C?
c=1
nci + C?c
(5)
The Gibbs sampler used in our experiments is
adapted from the Matlab Topic Modeling Tool-
box 2.
4 Expert Search
In this section, we describe a preliminary re-
trieval system that supports expert search, which
is intended to identify groups of research experts
with similar research interests and expertise by in-
putting only general domain key words. For ex-
ample, we can retrieve Michael Collins via search
for natural language parsing.
Our setting is different from the standard TREC
expert search in that we do not have a pre-defined
list of experts and topics, and our documents are
all research papers authored by experts. Within
this setting, we do not need to identify the status of
our experts, i.e., a real expert or a communicator,
as in TREC expert search. All of our authors and
cited authors are experts and the task amounts to
ranking the experts according to different topics
given samples of their research papers.
The ranking function of this retrieval model is
derived through the CAT parameters. The search
2http://psiexp.ss.uci.edu/research/programs data/
aims to link research topics with authors to by-
pass the proper names of these authors. Our re-
trieval function ranks the joint probability of the
query words (W ) and the target author (a), i.e.,
P (W,a). This probability is marginalized over all
topics, and the probability that an author is cited
given the topic is used as an extra weight in our
ranking function. The intuition is that an author
who is cited frequently should be more prominent
and ranked higher. Formally, we define the rank-
ing function of our retrieval system in Equation 6.
ca denotes when the author is one of the cited au-
thors in our corpus. CAT assumes that words and
authors, and cited authors are conditionally inde-
pendent given the topic, i.e., wi ? a ? ca.
P (W,a) =
?
wi
?i
?
t
P (wi, a|t, ca)P (t, ca)
=
?
wi
?i
?
t
P (wi|t)P (a|t)P (ca|t)P (t)
(6)
W is the input query, which may contain one or
more words. If a multiword is detected within the
query, it is added into the query. The final score
is the sum of all words in this query weighted by
their inverse document frequency ?i.
In our experiments, we chose ten queries which
cover several popular research areas in computa-
tional linguistics and natural language processing
and run the retrieval system based on three mod-
els: the original author topic model (Rosen-Zvi
et al, 2004), the MAT model (Johri et al, 2010)
and the CAT model. In the original author topic
model, query words are treated token by token.
Both MAT and CAT expand the query terms with
multiwords if they are detected inside the original
query. For each query, top 10 authors are returned
from the system. We manually label the relevance
of these 10 authors based on the papers collected
in our corpus.
Two standard evaluation metrics are used to
measure the retrieving results. First we evaluate
the precision at a given cut-off rank, namely pre-
cision at rank k with k ranging from 1 to 10. We
then calculate the average precision (AP) for each
query and the mean average precision (MAP) for
1269
the queries. Unlike precision at k, MAP is sensi-
tive to the ranking and captures recall information
since it assumes the precision of the non-retrieved
documents to be zero. It is formally defined as
the average of precisions computed at the point of
each of the relevant documents in the ranked list
as shown in Equation 7.
AP =
?n
r=1(Precision(r)? rel(r))
| relevant documents | (7)
To evaluate the recall of our system, we col-
lected a pool of authors for six of our queries re-
turned from an academic search engine, Arnet-
Miner (Tang et al, 2008)3 as our reference author
pool and evaluate our recall based on the number
of authors we retrieved from that pool.
5 Experiments and Analysis
In this section, we describe the empirical evalua-
tion of our model qualitatively and quantitatively
by applying our model to the expert search we de-
fined in Sec. 4. We compare the retrieving results
with two other models: Multiword- enhanced Au-
thor Topic (MAT) model (Johri et al, 2010) and
the original author topic model (Rosen-Zvi et al,
2004).
5.1 Data set and Pre-processing
We crawled the ACL anthology website and col-
lected papers from ACL, EMNLP and CONLL
over a period of seven years. The ACL anthol-
ogy website explicitly lists each paper together
with its title and author information. Therefore,
the author information of each paper can be ob-
tained accurately without extracting it from the
original paper. However, many author names are
not represented consistently. For example, the
same author may have his/her middle name listed
in some papers, but not in others. We therefore
normalized all author names by eliminating mid-
dle names from all authors.
Cited authors of each paper are extracted from
the reference section and automatically identified
by a named entity recognizer tuned for citation ex-
traction (Ratinov and Roth, 2009). Similar to reg-
ular authors, all cited authors are also normalized
3http://www.arnetminer.org
Conf. Year Paper Author uni. Vocab.
ACL 03-09 1,326 2,084 34,012 205,260
EMNLP 93-09 912 1,453 40,785 219,496
CONLL 97-09 495 833 27,312 123,176
Total 93-09 2,733 2,911 62,958 366,565
Table 1: Statistics about our data set. Uni. denotes
unigram words and Vocab. denotes all unigrams
and multiword phrases discovered in the data set.
with their first name initial and their full last name.
We extracted about 20,000 cited authors from our
corpus. However, for the sake of efficiency, we
only keep those cited authors whose occurrence
frequency in our corpus is above a certain thresh-
old. We experimented with thresholds of 5, 10 and
20 and retained the total number of 2,996, 1,771
and 956 cited authors respectively.
We applied the same strategy to extract mul-
tiwords from our corpus and added them into
our vocabulary to implement the model described
in (Johri et al, 2010). Some basic statistics about
our data set are summarized in Table 1 4.
5.2 Qualitative Coherence Analysis
As shown by other previous works (Wallach,
2006; Griffiths et al, 2007; Johri et al, 2010),
our model also demonstrates that embedding mul-
tiword tokens into the model can achieve more co-
hesive and better interpretable topics. We list the
top 10 words from two topics of CAT and compare
them with those from the unigram model in Ta-
ble 2. Unigram topics contain more general words
which can occur in every topic and are usually less
discriminative among topics.
Our experiments also show that CAT achieves
better retrieval quality by modeling cited authors
jointly with authors and words. The rank of an
author is boosted if that author is cited more fre-
quently. We present in Table 3 the ranking of one
of our ten query terms to demostrate the high qual-
ity of our proposed model. When compared to the
model without cited author information, CAT not
only retrieves more comprehensive expert list, its
ranking is also more reasonable than the model
without cited author information.
Another observation in our experiments is that
4Download the data and the software package at:
http://L2R.cs.uiuc.edu/?cogcomp/software.php.
1270
Query term: parsing
Proposed CAT Model Model without cited authors
Rank Author Prob. Author Prob.
1 J. Nivre 0.125229 J. Nivre 0.033200
2 C. Manning 0.111252 R. Barzilay 0.023863
3 M. Johnson 0.101342 M. Johnson 0.023781
4 J. Eisner 0.063528 D. Klein 0.018937
5 M. Collins 0.047347 R. McDonald 0.017353
6 G. Satta 0.042081 L. Marquez 0.016003
7 R. McDonald 0.041372 A. Moschitti 0.015781
8 D. Klein 0.041149 N. Smith 0.014792
9 K. Toutanova 0.024946 C. Manning 0.014040
10 E. Charniak 0.020843 K. Sagae 0.013384
Table 3: Ranking for the query term: parsing. CAT achieves more comprehensive and reasonable rank
list than the model without cited author information.
CAT Uni. AT Model
TOPIC 49 Topic 27
pronoun resolution anaphor
antecedent antecedents
coreference resolution anaphoricity
network anphoric
resolution is
anaphor anaphora
pronouns soon
anaphor antecedent determination
semantic knowledge pronominal
proper names salience
TOPIC 14 Topic 95
translation quality hypernym
translation systems seeds
source sentence taxonomy
word alignments facts
paraphrases hyponym
decoder walk
parallel corpora hypernyms
translation system page
parallel corpus logs
translation models extractions
Table 2: CAT with embedded multiword com-
ponents achieves more interpretable topics com-
pared with the unigram Author Topic (AT) model.
some experts who published many papers, but on
heterogeneous topics, may not be ranked at the
very top by models without cited author infor-
mation. However, with cited author information,
those authors are ranked higher. Intuitively this
makes sense since many of these authors are also
the most cited ones.
5.3 Quantitative retrieval results
One annotator labeled the relevance of the re-
trieval results from our expert search system. The
annotator was also given all the paper titles of each
Precision@K
K CAT Model Model w/o Cited Authors
1 0.80 0.80
2 0.80 0.70
3 0.73 0.60
4 0.70 0.50
5 0.68 0.48
6 0.70 0.47
7 0.69 0.40
8 0.68 0.45
9 0.73 0.44
10 0.70 0.44
Table 4: Precision at K evaluation of our proposed
model and the model without cited author infor-
mation.
corresponding retrieved author to help make this
binary judgment. We experiment with ten queries
and retrieve the top ten authors for each query.
We first used the precision at k for evaluation.
We calculate the precision at k for both our pro-
posed CAT model and the MAT model, which
does not have the cited author information. The
results are listed in Table 4. It can be observed
that at every rank position, our CAT model works
better. In order to focus more on relevant retrieval
results, we also calculated the mean average pre-
cision (MAP) for both models. For the given ten
queries, the MAP score for the CAT model is 0.78,
while the MAT model without cited author infor-
mation has a MAP score of 0.67. The CAT model
with cited author information achieves about 33%
error reduction in this experiment.
1271
Query ID Query Term
1 parsing
2 machine translation
3 dependency parsing
4 transliteration
5 semantic role labeling
6 coreference resolution
7 language model
8 Unsupervised Learning
9 Supervised Learning
10 Hidden Markov Model
Table 5: Queries and their corresponding ids we
used in our experiments.
Recall for each query
Query ID CAT Model Model w/o Cite
1 0.53 0.20
2 0.13 0.20
3 0.27 0.13
4 0.13 0.2
5 0.27 0.20
6 0.13 0.26
Average 0.24 0.20
Table 6: Recall comparison between our proposed
model and the model without cited author infor-
mation.
Since we do not have a gold standard experts
pool for our queries, to evaluate recall, we col-
lected a pool of authors returned from an aca-
demic search engine, ArnetMiner (Tang et al,
2008) as our reference author pool and evaluated
our recall based on the number of authors we re-
trieved from that pool. Specifically, we get the
top 15 returned persons from that website for each
query and treat them as the whole set of relevant
experts for that query and our preliminary recall
results are shown in Table 6.
In most cases, the CAT recall is better than that
of the compared model, and the average recall is
better as well. All the queries we used in our ex-
periments are listed in Table 5. And the average
recall value is based on six of the queries which
have at least one overlap author with those in our
reference recall pool.
6 Conclusion and Further Development
This paper proposed a novel author topic model,
CAT, which extends the existing author topic
model with additional cited author information.
We applied it to the domain of expert retrieval
and demonstrated the effectiveness of our model
in improving coherence in topic clustering and au-
thor topic association. The proposed model also
provides an effective solution to the problem of
community mining as shown by the promising re-
trieval results derived in our expert search system.
One immediate improvement would result from
extending our corpus. For example, we can ap-
ply our model to the ACL ARC corpus (Bird et
al., 2008) to check the model?s robustness and en-
hance the ranking by learning from more data. We
can also apply our model to data sets with rich
linkage structure, such as the TREC benchmark
data set or ACL Anthology Network (Radev et al,
2009) and try to enhance our model with the ap-
propriate network analysis.
Acknowledgments
The authors would like to thank Lev Ratinov for
his help with the use of the NER package and the
three anonymous reviewers for their helpful com-
ments and suggestions. The research in this pa-
per was supported by the Multimodal Information
Access & Synthesis Center at UIUC, part of CCI-
CADA, a DHS Science and Technology Center of
Excellence.
References
Bird, S., R. Dale, B. Dorr, B. Gibson, M. Joseph,
M. Kan, D. Lee, B Powley, D. Radev, and Y. Tan.
2008. The acl anthology reference corpus: A refer-
ence dataset for bibliographic research in computa-
tional linguistics. In Proceedings of LREC?08.
Blei, D., A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
Broniatowski, D. 2009. Generating status hierar-
chies from meeting transcripts using the author-
topic model. In In Proceedings of the Workshop:
Applications for Topic Models: Text and Beyond.
Diederich, J., J. Kindermann, E. Leopold, and
G. Paass. 2003. Authorship attribution with support
vector machines. Applied Intelligence, 19:109?123.
1272
Griffiths, T. and M. Steyvers. 2004. Finding scientific
topic. In Proceedings of the National Academy of
Science.
Griffiths, T., M. Steyvers, and J. Tenenbaum. 2007.
Topics in semantic representation. Psychological
Review.
Heinrich, G. 2009. Parameter estimation for text anal-
ysis. Technical report, Fraunhofer IGD.
Johri, N., D. Roth, and Y. Tu. 2010. Experts? retrieval
with multiword-enhanced author topic model. In
Proceedings of NAACL-10 Semantic Search Work-
shop.
Jurczyk, P. and E. Agichtein. 2007. Discovering au-
thorities in question answer communities by using
link analysis. In Proceedings of CIKM?07.
Mccallum, A., A. Corrada-emmanuel, and X. Wang.
2004. The author-recipient-topic model for topic
and role discovery in social networks: Experiments
with enron and academic email. Technical report,
University of Massachusetts Amherst.
Mei, Q. and C. Zhai. 2006. A mixture model for con-
textual text mining. In Proceedings of KDD-2006,
pages 649?655.
Mei, Q., D. Cai, D. Zhang, and C. Zhai. 2008. Topic
modeling with network regularization. In Proceed-
ing of WWW-08:, pages 101?110.
Radev, D., M. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis
of the field of Computational Linguistics. Journal
of the American Society for Information Science and
Technology.
Ratinov, L. and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc. of the Annual Conference on Computational
Natural Language Learning (CoNLL).
Rosen-Zvi, M., T. Griffiths, M. Steyvers, and P. Smyth.
2004. the author-topic model for authors and docu-
ments. In Proceedings of UAI.
Serdyukov, P., H. Rode, and D. Hiemstra. 2008. Mod-
eling multi-step relevance propagation for expert
finding. In Proceedings of CIKM?08.
Song, Y., J. Huang, and I. Councill. 2007. Efficient
topic-based unsupervised name disambiguation. In
Proceedings of JCDL-2007, pages 342?351.
Steyvers, M. and T. Griffiths. 2007. Probabilistic topic
models. In Handbook of Latent Semantic Analysis.
Lawrence Erlbaum Associates.
Steyvers, M., P. Smyth, and T. Griffiths. 2004. Proba-
bilistic author-topic models for information discov-
ery. In Proceedings of KDD.
Sun, Y., J. Han, J. Gao, and Y. Yu. 2009. itopicmodel:
Information network-integrated topic modeling. In
Proceedings of ICDM-2009.
Tang, J., J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su.
2008. Arnetminer: Extraction and mining of aca-
demic social networks. In Proceedings of KDD-
2008, pages 990?998.
Wallach, H. 2006. Topic modeling; beyond bag of
words. In International Conference on Machine
Learning.
Wang, X. and A. McCallum. 2005. A note on topi-
cal n-grams. Technical report, University of Mas-
sachusetts.
Wang, X., A. McCallum, and X. Wei. 2007. Topical
n-grams: Phrase and topic discoery with an appli-
cation to information retrieval. In Proceedings of
ICDM.
Zhang, J., M. Ackerman, and L. Adamic. 2007. Ex-
pertise networks in online communities: Structure
and algorithms. In Proceedings of WWW 2007.
1273
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 767?777,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
The Necessity of Combining Adaptation Methods
Ming-Wei Chang, Michael Connor and Dan Roth
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang21,connor2,danr}@uiuc.edu
Abstract
Problems stemming from domain adaptation
continue to plague the statistical natural lan-
guage processing community. There has been
continuing work trying to find general purpose
algorithms to alleviate this problem. In this
paper we argue that existing general purpose
approaches usually only focus on one of two
issues related to the difficulties faced by adap-
tation: 1) difference in base feature statistics
or 2) task differences that can be detected with
labeled data.
We argue that it is necessary to combine these
two classes of adaptation algorithms, using
evidence collected through theoretical analy-
sis and simulated and real-world data exper-
iments. We find that the combined approach
often outperforms the individual adaptation
approaches. By combining simple approaches
from each class of adaptation algorithm, we
achieve state-of-the-art results for both Named
Entity Recognition adaptation task and the
Preposition Sense Disambiguation adaptation
task. Second, we also show that applying an
adaptation algorithm that finds shared repre-
sentation between domains often impacts the
choice in adaptation algorithm that makes use
of target labeled data.
1 Introduction
While recent advances in statistical modeling for
natural language processing are exciting, the prob-
lem of domain adaptation remains a big challenge.
It is widely known that a classifier trained on one do-
main (e.g. news domain) usually performs poorly on
a different domain (e.g. medical domain) (Jiang and
Zhai, 2007; Daume? III, 2007). The inability of cur-
rent statistical models to handle multiple domains is
one of the key obstacles hindering the progress of
NLP.
Several general purpose algorithms have been
proposed to address the domain adaptation prob-
lem: (Blitzer et al, 2006; Jiang and Zhai, 2007;
Daume? III, 2007; Finkel and Manning, 2009). It
is widely believed that the drop in performance of
statistical models on new domains is due to the
shift of the joint distribution of labels and examples,
P (Y,X), from domain to domain, where X repre-
sents the input space and Y represents the output
space. In general, we can separate existing adap-
tation algorithms into two categories:
Focuses on P (X) This type of adaptation algo-
rithm attempts to resolve the difference between the
feature space statistics of two domains. While many
different techniques have been proposed, the com-
mon goal of these algorithms is to find a better
shared representation that brings the source domain
and the target domain closer. Often these algorithms
do not use labeled examples in the target domain.
The works (Blitzer et al, 2006; Huang and Yates,
2009) all belong to this category.
Focuses on P (Y |X) These adaptation algorithms
assume that there exists a small amount of labeled
data for the target domain. Instead of training two
weight vectors independently (one for source and
the other for the target domain), these algorithms try
to relate the source and target weight vectors. This is
often achieved by using a special designed regular-
ization term. The works (Chelba and Acero, 2004;
Daume? III, 2007; Finkel and Manning, 2009) belong
to this category.
767
It is important to give the definition of an adapta-
tion framework. An adaptation framework is speci-
fied by the data/resources used and a specific learn-
ing algorithm. For example, a framework that used
only source labeled examples and one that used both
source and target labeled examples should be con-
sidered as two different frameworks, even though
they might use exactly the same training algorithm.
Note that the goal of a good adaptation framework is
to perform well on the target domain and quite often
we only need to change the data/resource used to in-
crease the performance without changing the train-
ing algorithm. We refer to frameworks that do not
use target labeled data and focus on P (X) as Unla-
beled Adaptation Frameworks and refer to frame-
works that use algorithms that focus on P (Y |X) as
Labeled Adaptation Frameworks.
The major difference between unlabeled adapta-
tion frameworks and labeled adaptation frameworks
is the use of target labeled examples. Unlabeled
adaptation frameworks do not use target labeled ex-
amples1, while the labeled adaptation frameworks
make use of target labeled examples. Under this
definition, we consider that a model trained on both
source and target labeled examples (later referred as
S+T) is a labeled adaptation framework.
It is important to combine the labeled and unla-
beled adaptation frameworks for two reasons:
? Mutual Benefit: We analyze these two types
of frameworks and find that they address dif-
ferent adaptation issues. Therefore, it is often
beneficial to apply them together.
? Complex Interaction: Another, probably
more important issue, is that these two types
of frameworks are not independent. Different
representations will impact howmuch a labeled
adaptation algorithm can transfer information
between domains. Therefore, in order to have a
clear picture of what is the best labeled adapta-
tion framework, it is necessary to analyze these
two domain adaptation frameworks together.
In this paper, we assume we have both a small
amount of target labeled data and a large amount
1Note that we still use labeled data from source domain in
an unlabeled adaptation framework.
of unlabeled data so that we can perform both unla-
beled and labeled adaptation. The goal of our paper
is to point out the necessity of applying these two
adaptation frameworks together. To the best of our
knowledge, this is the first paper that both theoreti-
cally and empirically analyzes the interdependence
between the impact of labeled and unlabeled adap-
tation frameworks.
The contribution of this paper is as follows:
? Propose a theoretical analysis of the ?Frustrat-
ingly Easy? (FE) framework (Daume? III, 2007)
(Section 3).
The theoretical analysis shows that for FE to be
effective the domains must already be ?close?.
At some threshold of ?closeness? it is better to
switch from FE to just pool all training together
as one domain.
? Demonstrate the complex interaction between
unlabeled and labeled approaches (Section 4)
We construct artificial experiments that demon-
strate how applying unlabeled adaptation may
impact the behavior of two labeled adaptation
approaches.
? Empirically analyze the interaction on real
datasets (Section 5).
We show that in general combining both ap-
proaches on the tasks of preposition sense
disambiguation and named entity recognition
works better than either individual method.
Our approach not only achieves state-of-the-
art results on these two tasks but it also re-
veals something surprising ? finding a bet-
ter shared representation often makes a sim-
ple source+target approach the best adaptation
framework in practice.
2 Two Adaptation Aspects: A Review
Why do we need two types of adaptation frame-
works? First, unlabeled adaptation frameworks are
necessary since many features only exist in one do-
main. Therefore, it is important to develop algo-
rithms that find features which work across domains.
On the other hand, labeled adaptation frameworks
768
are also required because we would like to take ad-
vantages of target labeled data. Even though differ-
ent domains may have different definitions for la-
bels (say in named entity recognition, specific defi-
nition of PER/LOC/ORG may change), labeled data
should still be useful. We summarize these distinc-
tions in Table 1.
While these two aspects of adaptation both saw
significant progress in the past few years, little anal-
ysis has been done on the interaction between these
two types of algorithms2.
In order to have a deep analysis, it is necessary to
choose specific adaptation algorithms for each as-
pect of adaptation framework. While we mainly
conduct analysis on the algorithms we picked, we
would like to point out that the necessity of com-
bining these two types of adaptation algorithms has
been largely ignored in the community.
As our example adaptation algorithms we se-
lected:
Labeled adaptation: FE framework One of the
most popular adaptation frameworks that requires
the use of labeled target data is the ?Frustrat-
ingly Easy? (FE) adaptation framework (Daume? III,
2007). However, why and when this framework
works remains unclear in the NLP community. The
FE framework can be viewed as an framework that
extends the feature space, and it requires source and
target labeled data to work. We denote n as the
total number of features3 and m is the number of
the ?domains?, where one of the domains is the tar-
get domain. The FE framework creates a global
weight vector in Rn(m+1), an extended space for all
domains. The representation x of the t-th domain
is mapped by ?t(x) ? Rn(m+1). In the extended
space, the first n features consist of the ?shared?
block, which is always active across all tasks. The
(t+1)-th block (the (nt+1)-th to the (nt+n)-th fea-
tures) is a ?specific? block, and is only active when
2Among the previously mentioned work, (Jiang and Zhai,
2007) is a special case given that it discusses both aspects of
adaptation algorithms. However, little analysis on the interac-
tion of the two aspects is discussed in that paper
3We assume that the number of features in each domain is
equal.
extracting examples from the task t. More formally,
?t(x) =
2
6
4 x|{z}
shared
(t?1) blocks
z }| {
0 . . .0 x
|{z}
specific
(m?t) blocks
z }| {
0 . . .0
3
7
5 . (1)
A single weight vector w? is obtained by training on
the modified labeled data {yti ,?t(x
t
i)}
m
t=1. Given
that this framework only extends the feature space,
in this paper, we also call it the feature extension
framework (still called FE). We will see in Section 3
that this framework is equivalent to applying a reg-
ularization trick that bridges the source and the tar-
get domains. As it will become clear in Section 3,
in fact, this framework is only effective when there
is target labeled data and hence belongs to labeled
adaptation frameworks.
Although FE framework is quite popular in the
community, there are other even simpler labeled
adaptation frameworks that allow the use of tar-
get labeled data. For example, one of the simplest
frameworks is the S+T framework, which simply
trains a single model on the pooled and unextended
source and target training data.
Unlabeled adaptation: Adding cluster-like fea-
tures Recall that unlabeled adaptation frameworks
find the features that ?work? across domain. In this
paper, we find such features in two steps. First,
we use word clusters generated from unlabeled text
and/or third party resources that spans domains.
Then, for every feature template that contains a
word, we append another feature template that uses
the word?s cluster instead of the word itself. This
technique is used in many recent works including
dependency parsing and NER (Koo et al, 2008;
Ratinov and Roth, 2009). Note that the unlabeled
text need not come from the source or target do-
main. In fact, in this paper, we use clusters gen-
erated with the Reuters 1996 dataset, a superset of
the CoNLL03 NER dataset (Koo et al, 2008; Liang,
2005). We adopt the Brown cluster algorithm to find
the word cluster (Brown et al, 1992; Liang, 2005).
We can use other resources to create clusters as well.
For example, in the NER domain, we also include
gazetteers4 as an unlabeled cluster resource, which
can bring the domains together quite effectively.
4Our gazetteers comes from (Ratinov and Roth, 2009).
769
Framework Labeled Data Unlabeled Data Common Approach
Unlabeled Adaptation
(Focus on P (X))
Source Encompasses Source and Target.
May use other third party resources
(dictionaries, gazetteers, etc.).
Generate features that span domains us-
ing unlabeled data and/or third party re-
sources.
Labeled Adaptation
(Focus on P (Y |X))
Source and Target None Train classifier(s) using both source and
target training data, relating the two.
Table 1: Comparison between two general adaptation frameworks discussed in this paper. Each framework is specified by its setting
(data required) and its learning algorithm. Multiple previous adaptation approaches fit in one of either framework.
While other more complex algorithms (Ando and
Zhang, 2005; Blitzer et al, 2006) for finding bet-
ter shared representation (without using labeled tar-
get data) have been proposed, we find that using
straightforward clustering features is quite effective
in general.
3 Analysis of the FE Framework
In this section, we propose a simple yet informative
analysis of the FE algorithm from the perspective of
multi-task learning. Note that we ignore the effect
of unlabeled adaptation in this section, and focus on
the analysis of the FE framework as a representative
labeled adaptation framework.
3.1 Mistake Bound Analysis
While (Daume? III, 2007) proposed this framework
for adaptation, a very similar idea had been proposed
in (Evgeniou and Pontil, 2004) as a novel regular-
ization term for multitask learning with support vec-
tor machines. Assume that w1,w2, . . . ,wm are the
weight vector for the first domain to the m-th do-
main, respectively. The baseline approach is to as-
sume that each weight vector is independent. As-
sume that we adopt a SVM-like optimization prob-
lem that consider all m tasks, the baseline approach
is equivalent to using the following regularization
term in the objective function:
?m
t=1 ?wt?
2.
In (Evgeniou and Pontil, 2004; Daume? III, 2007),
they assume that wt = u + vt, for t = 1, . . .m,
where vt is the specific weight vector for t-th do-
main and u is a shared weight vector across all do-
mains. The new regularization term then becomes
?u?2 +
m?
t=1
?vt?2. (2)
Note that these two regularization terms are differ-
ent, given that the new regularization term makes
w1,w2, . . . ,wm not independent anymore. It fol-
lows that
wTt x = (u+ vt)
Tx = w?T?t(x),
where
w?T =
[
uT vT1 . . . v
T
m
]
.
and ?w??2 equals to Eq. (2). Therefore, we can think
feature extension framework as a learning frame-
work that adopts Eq. (2) as its regularization term.
The FE framework was in fact originally designed
for the problem of multitask learning so in the fol-
lowing, we propose a simple mistake bound analysis
based on the multitask setting, where we calculate
the mistakes on all domains5. We focus on multi-
task setting for two reasons: 1) the analysis is very
easy and intuitive, and 2) in Section 4.1, we empiri-
cally confirm that the analysis holds for the adapta-
tion setting.
In the following, we assume that the training
algorithm used in the FE framework is the on-
line perceptron learning algorithm (Novikoff, 1963).
This allows us to analyze the mistake bound of the
FE framework with the perceptron algorithm. The
bound can give us an insight on when and why one
should adopt the FE framework. By using the stan-
dard mistake bound theorem (Novikoff, 1963), we
show:
Theorem 1. Let Dt be the labeled data of domain t.
Assume that there exist w1,w2, . . . ,wm such that
ywTt x ? ?,?(x, y) ? Dt,
and assume that max(x,y)?Dt ?x? ? R
2,?t =
1 . . .m. Then, the number of mistakes made with
online perceptron training (Novikoff, 1963) and the
5In the adaptation setting, one generally only cares about the
performance on the target domain.
770
FE framework is bounded by
2R2
?2
(
m?
t=1
?wt?2 ?
?
?m
t=1 wt?
2
m + 1
). (3)
Proof. Define w? as a vector in Rn(m+1). We claim
that there exists a set Sw? such that for all w? ? Sw?,
w?T?t(x) = wTt x for any domain t = 1 . . .m. Note
that?t(x) is defined in Eq. (1). We can construct Sw?
in the following way:
Sw? = {
[
s (w1 ? s) . . . (wm ? s)
]
| s ? Rn},
where s is an arbitrary vector with n elements.
In order to obtain the best possible bound, we
would like to find the most compressed weight vec-
tor in Sw?, w? = minw??Sw? ?w??
2.
The optimization problem has an analytical solu-
tion:
?w??2 =
m?
t=1
?wt?2 ? ?
m?
t=1
wt?2/(m + 1).
The proof is completed by the standard mis-
take bound theorem and the following fact:
maxx ??t(x)?2 = 2maxx ?x?2 ? 2R2.
3.2 Mistake Bound Comparison
In the following, we would like to explore under
what circumstances the FE framework can work bet-
ter than individual models and the S+T framework
using Theorem 1. The analysis is done based on the
assumption that all frameworks use the perceptron
algorithm.
Before showing the bound analysis, note that the
framework proposed by (Evgeniou and Pontil, 2004;
Finkel and Manning, 2009) is a generalization over
these three frameworks (FE, S+T, and the base-
line)6. However, our goal in this paper is different:
we try to provide a deep discussion onwhen and why
one should use a particular framework.
Here, we compare the mistake bounds of the fea-
ture sharing framework to that of the baseline ap-
proach, which learns each task independently7. In
6The framework proposed by (Evgeniou and Pontil, 2004;
Finkel and Manning, 2009) is a generalization of Eq. (1). It
allows the user to weight each block of features. If we put zero
weight on the shared block, it becomes the baseline approach.
On the other hand, if we put zero weight on all task-specific
blocks, the framework becomes the S+T approach.
7Note that mistake bound results can be generalized to gen-
eralization bound results. See (Zhang, 2002).
order to make the comparison easier, we make some
simplifying assumptions. First, we assume that the
problem contains only two tasks, 1 and 2. We also
assume that ?w1? = ?w2? = a. These assump-
tions greatly reduce the complexity of the analysis
and can give us greater insight into the comparisons.
Following the assumptions and Theorem 1, the
mistake bound for the FE frameworks is
4(2? cos(w1,w2))R2a2/(3?2) (4)
This line of analysis leads to interesting bound com-
parisons for two cases. In the first case, we assume
that task 1 and task 2 are essentially the same. In the
second, more common case, we assume that they are
different.
First, when we know a priori that task 1 and task
2 are essentially the same, we can combine the train-
ing data from the two tasks and train them as a sin-
gle task. Therefore, given that we do not need to
expand the feature space, the number of mistakes is
now bounded by R2a2/?2. Note that this bound is
in fact better than (4) with cos(w1,w2) = 1. There-
fore, if we know a priori that these two tasks are the
same, training a single model is better than using the
feature shared approach.
In practice, it is often the case that the two tasks
are not the same. In this case, the number of mis-
takes of an independent approach on both task 1 and
2 will be bounded by the summation of the mistake
bounds of task 1 and task 2. Therefore, using the
independent approach, the number of mistakes for
the perceptron algorithm on both tasks is bounded
by 2R2a2/?2. The following results can be obtained
by directly comparing the two bounds,
Corollary 1. Assume there exists w1 and w2 which
separate D1 and D2 respectively with functional
margin ?, and ?w1? = ?w2? = a. In this case:
(4) will be smaller than the bound of individual ap-
proach, 2R2a2/?2, if and only if cos(w1,w2) =
(wT1 w2)/(?w1??w2?) >
1
2 .
If we assume that there is no difference in
P (X) between domains and hence we can treat
cos(w1,w2) as the similarity between two tasks, the
above argument suggests:
? If the two tasks are very different, the baseline
approach (building two models) is better than
FE and S+T.
771
? If the tasks are similar enough, FE is better than
baseline and S+T.
? If the tasks are almost the same, S+T becomes
better than FE and baseline.
In Section 4.1, we will evaluate whether these claims
can be justified empirically.
4 Artificial Data Experiment Study
In this section we will present artificial experiments.
We have two primary goals: 1) verifying the analysis
proposed in Section 3, and 2) showing that the repre-
sentation shift will impact the behavior of the FE al-
gorithm. The second point will be verified again in
the real world experiments in Section 5.
Data Generation In the following artificial ex-
periments we experiment with domain adaptation
by generating training and test data for two tasks,
source and target, where we can control the differ-
ence between task definitions. The general proce-
dure can be divided into two steps: 1) generating
weight vectors z1 and z2 (for source and target re-
spectively), and 2) randomly generating labeled in-
stances for training and testing using z1 and z2.
The different experiments start with the same ba-
sic z1 and z2, but then may alter these weights to
introduce task dissimilarities or similarities. The ba-
sic z1 and z2 are both generated by a multivariate
Gaussian distribution with mean z and a diagonal
covariance matrix ?I:
z1 ? N (z, ?I), z2 ? N (z, ?I),
where N is the normal distribution and z is random
vector with zero mean. Note that z is only used to
generate z1 and z2. There is one parameter, ?, that
controls the variance of the Gaussian distribution.
Hence we use ? to roughly control the ?angle? of z1
and z2. When ? is close to zero, z1 and z2 will be
very similar. On the other hand, when ? is large, z1
and z2 can be very different. In these experiments,
we vary ? between 0.01 and 5 so that we are exper-
imenting only with tasks where the weight the task
difference is the ?angle? or cosine between z1 and
z2. Once we obtain the z1 and z2, we normalize
them to the unit length.
After selecting z1 and z2, we then generate la-
beled instances (x, y) for the source task in the fol-
lowing way. For each example x, we randomly gen-
erate n binary features, where each feature has 20%
chance to be active. We then label the example by
y = sign(zT1 x),
The data for the target task is generated similarly
with z2. In these experiments, we fix the number of
features n to be 500 and generate 100 source train-
ing examples and 40 target training examples, along
with 1000 target testing examples. This matches the
reasonable case in NLP where there are more fea-
tures than training examples and each feature vector
is sparse. In all of the experiments, we report the
averaged testing error rate on the target testing data.
4.1 Experiment 1, FE algorithm
Goal The goal here is to verify our theoretical
analysis in Section 3. Note that we do not introduce
representation shift in this experiment and assume
that both source and target domains use exactly the
same features.
Result Figure 1(a) shows the performance of the
three training algorithms as variance decreases and
thus cosine between weight vectors (or measure of
task similarity) goes to 1. Note that FE labeled adap-
tation framework beats TGT once the task cosine
passes approximately 0.6. Initially FE slightly out-
performs S+T until the tasks are close enough to-
gether that it is better to treat all the data as coming
from one task. Note that while the experiments are
based on the adaptation setting, the results match our
analysis based on the multitask setting in Section 3.
4.2 Experiment 2, Unseen Features
Goal So far we have not considered the difference
in P (X) between domains. In the previous exper-
iment, we used only cosine as our task similarity
measurement to decide what is the best framework.
However, task similarity should consider the differ-
ence in both P (X) and P (Y |X), and the cosine
measurement is not sufficient for this. Here we con-
struct a simple example to show that even a simple
representation shift can change the behavior of the
labeled adaptation framework. This case shows that
S+T can be better than FE even when the tasks are
not similar according to the cosine measurement.
772
 0.3
 0.32
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 0.46
 0  0.2  0.4  0.6  0.8  1
Ta
rg
et
 %
 E
rro
r
Cosine
Tgt
S+T
FE
(a) Basic Similarity
 0.32
 0.325
 0.33
 0.335
 0.34
 0.345
 0.35
 0.355
 0.36
 0  0.2  0.4  0.6  0.8  1
Ta
rg
et
 %
 E
rro
r
Original Cosine
Tgt
S+T
FE
(b) Shared Features
Figure 1: Artificial Experiment comparing labeled adaptation performance vs. cosine between base weight vectors that defines
two tasks, before and after cross-domain shared features are added. Figure (a) shows results from experiment 1. For FE adaptation
algorithm to work the tasks need to be close (cosine > 0.6), and if the tasks are close enough (cosine ? 1, dividing line) then
it is better to just pool source and target training data together (the S+T algorithm). Figure (b) shows results for experiment 3
when shared features are added to the base weight vectors as used in experiment 1. Here the cosine similarity measure is between
the base task weight vectors before the shared features have been added. Both labeled adaptation algorithms effectively use the
shared features to improve over just training on target. With shared features added the dividing line where S+T improves over
FE decreases so even for tasks that are initially further apart, once clusters are added the S+T algorithm does better than FE. Each
point represents the average of 2000 training runs with random initial z1 and z2 generating data.
Result The second experiment deals with the case
where features may appear in only one domain but
should be treated like known features in the other
domain. An example of this are out of vocabulary
words that may not exist in a small target train-
ing task, but have synonyms in the source train-
ing data. In this case if we had features grouping
words (say by word meanings) then we would re-
cover this cross-domain information. In this experi-
ment we want to explore which adaptation algorithm
performs best before these features are applied.
To simulate this case we start with similar weight
vectors z1 and z2 (sampled with variance = 0.00001,
cos(z1,z2) ? 1), but then shift some set of dimen-
sions so that they represent features that appear only
in one domain.
z1 = (a1,b1) ? z?1 = (0,b1,a1)
z2 = (a2,b2) ? z?2 = (a2,b2,0)
By changing the ratio of the size of the dissimilar
subset a to the similar subset b we can make the
two weight vectors z?1 and z
?
2 more or less similar.
Using these two new weight vectors we can proceed
as above, generating training and testing data.
Figure 2 shows the performance of the three algo-
 0.315
 0.32
 0.325
 0.33
 0.335
 0.34
 0.345
 0.35
 0.355
 0  0.2  0.4  0.6  0.8  1
Ta
rg
et
 %
 E
rro
r
Cosine
Tgt
S+T
FE
Figure 2: Artificial Experiment where unknown features are
included in source or target domains, but not the other. The
simple S+T adaptation framework is best able to exploit the
set of shared features so performs best over the whole space of
similarity in this setting.
rithms on this data as the number of unrelated fea-
tures are decreased. Over the entire range the com-
bined algorithm S+T does better since it more ef-
ficiently exploits the shared similar b subset of the
feature space. When the FE algorithm tries to cre-
ate the shared features, it considers both the similar
subset b and dissimilar subset a. However, since
a should not be shared, FE algorithm becomes less
773
effective than the S+T algorithm. See the bound
comparison in Section 3.2 for more intuitions. With
this experiment we have demonstrated that there is
a need to consider label and unlabeled adaptation
frameworks together.
4.3 Experiment 3, Shared Features
Goal A good unlabeled adaptation framework
should try to find features that ?work? across do-
mains. However, it is not clear how these newly
added features will impact the behavior of the la-
beled adaptation frameworks. In this experiment, we
show that the new shared features will bring the do-
mains together, and hence make S+T a very strong
adaptation framework.
Result For the third experiment we start with the
same setup as in the first experiment, but then aug-
ment the initial weight vector with additional shared
weights. These shared weights correspond to the in-
troduction of features that appear in both domains
and have the same meaning relative to the tasks, the
ideal result of unlabeled adaptation methods.
To generate this case we again start with z1 and
z2 of varying similarity as in section 4.1, then gen-
erate a random weight vector for shared features and
append this to both weight vectors.
zs ? N (0, I), z??1 = (z1, ?zs), z
??
2 = (z2, ?zs),
where ? is used to put increased importance on the
shared weight vectors by increasing the total weight
of that section relative to the base z1 and z2 subsets.
In our experiments we use 100 shared features to the
500 base features and set ? to 2.
Figure 1(b) shows the performance of the labeled
adaptation algorithms once shared features had been
added. Here the x-axis is the cosine between the
original task weight vectors, demonstrating how the
shared features improve performance on potentially
dissimilar tasks. Whereas in the first experiment
FE does not improve over just training on target data
until the cosine is greater than 0.6, once shared fea-
tures have been added then both FE and S+T use
these features to learn with originally dissimilar
tasks. Furthermore the shared features tend to push
the tasks ?closer? so that S+T improves over FE ear-
lier. Comparing to Figure 1(a), there are regions
where before shared features are added it is better
to use FE, and after shared features are added it is
better to use S+T. This shows that labeled adapta-
tion and unlabeled are not independent. Therefore,
it is important to combine these two aspects to see
the real contribution of each adaptation framework.
In these three artificial experiments we have
demonstrated cases where both FE or S+T are
the best algorithm before and after representation
changes like those created with unlabeled adaptation
are imposed. This fact points to the perhaps obvi-
ous conclusion that there is not a single best adapta-
tion algorithm, and the determination of specific best
practices depends on task similarity (in both P (X)
and P (Y |X)), especially after being brought closer
together with other adaptation approaches. If there
is one common trend it is that often once two tasks
have been brought close together using a shared rep-
resentation, then the tasks are now close enough
such that the simple S+T algorithm does well.
5 Real World Experiments
In Section 4, we have shown through artificial data
experiments that labeled and unlabeled adaptation
algorithms are not independent. In this section, we
focus on experiments with real datasets.
For the labeled adaptation algorithms, we have the
following options:
? TGT: Only uses target labeled training dataset.
? FE: Uses both labeled datasets.
? FE+: Uses both labeled datasets. A modifica-
tion of the FE algorithm, equivalent to multi-
plying the ?shared? part of the FE feature vec-
tor (Eq. (1)) by 10 (Finkel andManning, 2009).
? S+T: Uses both source and target labeled
datasets to train a single model with all labeled
data directly.
Throughout all of our experiments, we use SVMs
trained with a modified java implementation8 of
LIBLINEAR as our underlying learning classi-
fier (Hsieh et al, 2008). For the tasks that require
structures, we model each individual decision using
8Our code is modified from the version available on http:
//www.bwaldvogel.de/liblinear-java/
774
Algorithm TGT FE FE+ S+T
SRC labeled data? no yes
Target labeled data Token F1
(a) MUC7 Dev 58.6 70.5 74.3 73.1
(a) + cluster 77.5 82.5 83.3 83.3
(b) MUC7 Train 73.0 78.2 80.1 78.7
(b) + cluster 85.4 86.4 86.2 86.5
Table 2: NER Experiments. We bold face the best accuracy
in a row and underline the runner up. Both unlabeled adapta-
tion algorithms (adding cluster features) and labeled adaptation
algorithm (using source labeled data) help the performance sig-
nificantly. Moreover, adding cluster-like features also changes
the behavior of the labeled adaptation algorithms. Note that
after adding cluster features, S+T becomes quite competitive
with (or slightly better than) the FE+ approach. The size of
MUC7 develop set is roughly 20% of the size of the MUC7
training set.
a local SVM classifier then make our prediction us-
ing a greedy approach from left to right. While we
could use a more complex model such as Condi-
tional Random Field (Lafferty et al, 2001), as we
will see later, our simple model generates state-of-
the-art results for many tasks. Regarding parameter
selection, we selected the SVM regularization pa-
rameter for the baseline model (TGT) and then fix it
for all algorithms9.
Named Entity Recognition Our first task is
Named Entity Recognition (NER). The source do-
main is from the CoNLL03 shared task (Tjong
Kim Sang and De Meulder, 2003) and the target do-
main is from the MUC7 dataset. The goal of this
adaptation system is to maximize the performance
on the test data of MUC7 dataset with CoNLL train-
ing data and (some) MUC7 labeled data. As an unla-
beled adaptation method to address feature sparsity,
we add cluster-like features based on the gazetteers
and word clustering resources used in (Ratinov and
Roth, 2009) to bridge the source and target domain.
We experiment with both MUC development and
training set as our target labeled sets.
The experimental results are in Table 2. First, no-
tice that addressing the feature sparsity issue helps
the performance significantly. Adding cluster-like
9We use L2-hinge loss for all of the experiments, with
C = 2?4 for NER experiments and C = 2?5 for the PSD
experiments.
features improves the Token-F1 by around 10%. On
the other hand, adding target labeled data also helps
the results significantly. Moreover, using both tar-
get labeled data and cluster-like shared representa-
tion are mutually beneficial in all cases.
Importantly, adding cluster-like features changes
the behavior of the labeled adaptation algorithms.
When the cluster-like features are not added, the
FE+ algorithm is in general the best labeled adap-
tation framework. This result agrees with the re-
sults showed in (Finkel and Manning, 2009), where
the authors show that FE+ is the best labeled adap-
tation framework in their settings. However, after
adding the cluster-like features, the simple S+T ap-
proach becomes very competitive to both FE and
FE+. This matches our analysis in Section 4: re-
solving features sparsity will change the behavior of
labeled adaptation frameworks.
We compare the simple S+T algorithm with
cluster-like features to other published results on
adapting from CoNLL dataset to MUC7 dataset in
table 3. Past works on this setting often only fo-
cus on one class of adaption approach. For example,
(Ratinov and Roth, 2009) only use the cluster-like
features to address the feature sparsity problem, and
(Finkel and Manning, 2009) only use target labeled
data without using gazetteers and word-cluster in-
formation. Notice that because of combining two
classes of adaption algorithms, our approach is sig-
nificantly better than these two systems10.
Preposition Sense Disambiguation We also test
the combination of unlabeled and labeled adaption
on the task of Preposition Sense Disambiguation.
Here the data contains multiple prepositions where
each preposition has many different senses. The
goal is to predict the right sense for a given prepo-
sition in the testing data. The source domain is the
SemEval 2007 preposition WSD Task and the target
domain is from the dataset annotated in (Dahlmeier
et al, 2009). Our feature design mainly comes
from (Tratz and Hovy, 2009) (who do not evalu-
ate their system on our target data). As our un-
10The work (Ratinov and Roth, 2009) also combines their
system with several document-level features. While it is possi-
ble to add these features in our system, we do not include any
global features for the sake of simplicity. Note that our sys-
tem is competitive to (Ratinov and Roth, 2009) even though our
system does not use global features.
775
Systems Cluster? TGT? P.F1 T.F1
Our NER y y 84.1 86.5
FM09 n y 79.98 N/A
RR09 y n N/A 83.2
RR09 + global y n N/A 86.2
Table 3: Comparisons between different NER systems. P.F1
and T.F1 represent the phrase-level and token-level F1 score,
respectively. We use ?Cluster?? to indicate if cluster features
are used and use ?TGT?? to indicate if target labeled data is
used. Previous systems often only use one class of adaptation
algorithms. Using both adaptation aspects makes our system
perform significantly better than FM09 and RR09.
Algorithm TGT FE FE+ S+T
SRC labeled data? no yes
Target labeled data Accuracy
10% Tgt 43.8 48.2 51.3 49.7
10% Tgt + Cluster 44.9 50.5 51.8 52.0
100% Tgt 59.5 60.5 60.3 61.2
100% Tgt + Cluster 61.3 62.0 61.2 62.1
Table 4: Preposition Sense Disambiguation. We mark the best
accuracy in a row using the bold font and underline the runner
up. Note that both adding cluster features and adding source la-
beled data help the performance significantly. Moreover, adding
clusters also changes the behavior of the labeled adaptation al-
gorithms.
labeled adaptation approach we augment all word
based features with cluster information from sepa-
rately generated hierarchical Brown clusters (Brown
et al, 1992).
The experimental results are in Table 4. Note that
we see phenomena similar to what happened in the
NER experiments. First, both labeled and unlabeled
adaptation improves the system. When only 10% of
the target labeled data is used, the inclusion of the
source labeled data helps significantly. When there
is more labeled data, labeled and unlabeled adaption
have similar impact. Again, using unlabeled adap-
tion changes the behavior of the labeled adaption al-
gorithms.
In Table 5, we compare our system to (Dahlmeier
et al, 2009), who do not use the SemEval data but
jointly train their preposition sense disambiguation
system with a semantic role labeling system. With
both labeled and unlabeled adaption, our system is
significantly better.
Systems ACC
Our PSD (S+T and cluster) 62.1
DNS09 56.5
DNS09 + SRL 58.8
Table 5: Comparison between different PSD systems. Note
that after adding cluster features and source labeled data with
S+T approach, our system outperforms the state-of-the-art sys-
tem proposed in (Dahlmeier et al, 2009), even though they
jointly learn a PSD and SRL system together.
6 Conclusion
In this paper, we point out the necessities of com-
bining labeled and unlabeled adaptation algorithms.
We analyzed the FE algorithm both theoretically
and empirically, demonstrating that it requires both
a minimal amount of task similarity to work, and
past a certain level of similarity other, simpler ap-
proaches are better. More importantly, through arti-
ficial data experiments we found that applying unla-
beled adaptation algorithms may change the behav-
ior of labeled adaptation algorithms as representa-
tions change, and hence affect the choice of labeled
adaptation algorithm. Experiments with real-world
datasets confirmed that combinations of both adap-
tation methods provide the best results, often allow-
ing the use of simple labeled adaptation approaches.
In the future, we hope to develop a joint algorithm
which addresses both labeled and unlabeled adapta-
tion at the same time.
Acknowledgment We thank Vivek Srikumar for provid-
ing the baseline implementation of preposition sense disam-
biguation. We also thank anonymous reviewers for their use-
ful comments. University of Illinois gratefully acknowledges
the support of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL, or the US
government.
References
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res.
776
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-Based n-gram Models
of Natural Language. Computational Linguistics.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Dekang Lin and Dekai Wu, editors, EMNLP.
D. Dahlmeier, H. T. Ng, and T. Schultz. 2009. Joint
learning of preposition senses and semantic roles of
prepositional phrases. In EMNLP.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
T. Evgeniou and M. Pontil. 2004. Regularized multi?
task learning. In KDD.
J. R. Finkel and C. D. Manning. 2009. Hierarchical
bayesian domain adaptation. In NAACL.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear svm. In ICML.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling. In ACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
A. Novikoff. 1963. On convergence proofs for percep-
trons. In Proceeding of the Symposium on the Mathe-
matical Theory of Automata.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003.
S. Tratz and D. Hovy. 2009. Disambiguation of prepo-
sition sense using linguistically motivated features. In
NAACL.
Tong Zhang. 2002. Covering number bounds of certain
regularized linear function classes. J. Mach. Learn.
Res.
777
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961?970,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Generating Confusion Sets for Context-Sensitive Error Correction
Alla Rozovskaya and Dan Roth
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,danr}@illinois.edu
Abstract
In this paper, we consider the problem of gen-
erating candidate corrections for the task of
correcting errors in text. We focus on the
task of correcting errors in preposition usage
made by non-native English speakers, using
discriminative classifiers. The standard ap-
proach to the problem assumes that the set of
candidate corrections for a preposition con-
sists of all preposition choices participating
in the task. We determine likely preposition
confusions using an annotated corpus of non-
native text and use this knowledge to produce
smaller sets of candidates.
We propose several methods of restricting
candidate sets. These methods exclude candi-
date prepositions that are not observed as valid
corrections in the annotated corpus and take
into account the likelihood of each preposi-
tion confusion in the non-native text. We find
that restricting candidates to those that are ob-
served in the non-native data improves both
the precision and the recall compared to the
approach that views all prepositions as pos-
sible candidates. Furthermore, the approach
that takes into account the likelihood of each
preposition confusion is shown to be the most
effective.
1 Introduction
We address the problem of generating candidate cor-
rections for the task of correcting context-dependent
mistakes in text, mistakes that involve confusing
valid words in a language. A well-studied instance
of this problem ? context-sensitive spelling errors ?
has received a lot of attention in natural language
research (Golding and Roth, 1999; Carlson et al,
2001; Carlson and Fette, 2007; Banko and Brill,
2001). The context-sensitive spelling correction task
addresses the problem of correcting spelling mis-
takes that result in legitimate words, such as confus-
ing their and there or your and you?re. In this task, a
candidate set or a confusion set is defined that spec-
ifies a list of confusable words, e.g., {their, there}
or {cite, site, sight}. Each occurrence of a confus-
able word in text is represented as a vector of fea-
tures derived from a small context window around
the target. A classifier is trained on text assumed
to be error-free, replacing each target word occur-
rence (e.g. their) with a confusion set consisting of
{their, there}, thus generating both positive and neg-
ative examples, respectively, from the same context.
Given a text to correct, for each word in text that be-
longs to the confusion set the classifier predicts the
most likely candidate in the confusion set.
More recently, work in error correction has taken
an interesting turn and focused on correcting errors
made by English as a Second Language (ESL) learn-
ers, with a special interest given to errors in article
and preposition usage. These mistakes are some of
the most common mistakes for non-native English
speakers of all proficiency levels (Dalgish, 1985;
Bitchener et al, 2005; Leacock et al, 2010). Ap-
proaches to correcting these mistakes have adopted
the methods of the context-sensitive spelling cor-
rection task. A system is usually trained on well-
formed native English text (Izumi et al, 2003; Eeg-
Olofsson and Knuttson, 2003; Han et al, 2006; Fe-
lice and Pulman, 2008; Gamon et al, 2008; Tetreault
961
and Chodorow, 2008; Elghaari et al, 2010; Tetreault
et al, 2010), but several works incorporate into
training error-tagged data (Gamon, 2010; Han et
al., 2010) or error statistics (Rozovskaya and Roth,
2010b). The classifier is then applied to non-native
text to predict the correct article/preposition in con-
text. The possible candidate selections include the
set of all articles or all prepositions.
While in the article correction task the candidate
set is small (a, the, no article), systems for correct-
ing preposition errors, even when they consider the
most common prepositions, may include between 9
to 34 preposition classes. For each preposition in
the non-native text, every other candidate in the con-
fusion set is viewed as a potential correction. This
approach, however, does not take into account that
writers do not make mistakes randomly: Not all can-
didates are equally likely given the preposition cho-
sen by the author and errors may depend on the first
language (L1) of the writer. In this paper, we de-
fine L1-dependent candidate sets for the preposition
correction task (Section 4.1). L1-dependent can-
didate sets reflect preposition confusions observed
with the speakers of the first language L1. We pro-
pose methods of enforcing L1-dependent candidate
sets in training and testing.
We consider mistakes involving the top ten En-
glish prepositions. As our baseline system, we train
a multi-class classifier in one-vs-all approach, which
is a standard approach to multi-class classification.
In this approach, a separate binary classifier for each
preposition pi, 1 ? i ? 10, is trained, s.t. all pi
examples are positive examples for the classifier and
all other nine classes act as negative examples. Thus,
for each preposition pi in non-native text there are
ten1 possible prepositions that the classifier can pro-
pose as corrections for pi.
We contrast this baseline method to two methods
that enforce L1-dependent candidate sets in train-
ing. First, we train a separate classifier for each
preposition pi on the prepositions that belong to L1-
dependent candidate set of pi. In this setting, the
negative examples for pi are those that belong to L1-
dependent candidate set of pi.
The second method of enforcing L1-dependent
1This includes the preposition pi itself. If proposed by the
classifier, it would not be flagged as an error.
candidate sets in training is to train on native data
with artificial preposition errors in the spirit of Ro-
zovskaya and Roth (2010b), where the errors mimic
the error rates and error patterns of the non-native
text. This method requires more knowledge, since
it uses a distribution of errors from an error-tagged
corpus.
We also propose a method of enforcing L1-
dependent candidate sets in testing, through the use
of a confidence threshold. We consider two ways of
applying a threshold: (1) the standard way, when a
correction is proposed only if the classifier?s con-
fidence is sufficiently high and (2) L1-dependent
threshold, when a correction is proposed only if it
belongs to L1-dependent candidate set.
We show that the methods of restricting candidate
sets to L1-dependent confusions improve the prepo-
sition correction system. We demonstrate that re-
stricting candidate sets to those prepositions that are
confusable in the data by L1 writers is beneficial,
when compared to a system that assumes an unre-
stricted candidate set by considering as valid correc-
tions all prepositions participating in the task. Fur-
thermore, we find that the most effective method is
the one that uses knowledge about the likelihoods of
preposition confusions in the non-native text intro-
duced through artificial errors in training.
The rest of the paper is organized as follows.
First, we describe related work on error correction.
Section 3 presents the ESL data and statistics on
preposition errors. Section 4 describes the meth-
ods of restricting candidate sets in training and test-
ing. Section 5 describes the experimental setup. We
present and discuss the results in Section 6. The key
findings are summarized in Table 5 and Fig. 1 in
Section 6. We conclude with a brief discussion of
directions for future work.
2 Related Work
Work in text correction has focused primarily on
correcting context-sensitive spelling errors (Golding
and Roth, 1999; Banko and Brill, 2001; Carlson et
al., 2001; Carlson and Fette, 2007) and mistakes
made by ESL learners, especially errors in article
and preposition usage.
Roth (1998) takes a unified approach to resolving
semantic and syntactic ambiguities in natural lan-
962
guage by treating several related problems, includ-
ing word sense disambiguation, word selection, and
context-sensitive spelling correction as instances of
the disambiguation task. Given a candidate set or a
confusion set of confusable words, the task is to se-
lect the most likely candidate in context. Examples
of confusion sets are {sight, site, cite} for context-
sensitive spelling correction, {among, between} for
word selection, or a set of prepositions for the prepo-
sition correction problem.
Each occurrence of a candidate word in text is
represented as a vector of features. A classifier is
trained on a large corpus of error-free text. Given
text to correct, for each word in text that belongs to
the confusion set the classifier is used to predict the
most likely candidate in the confusion set given the
word?s context.
In the same spirit, models for correcting ESL er-
rors are generally trained on well-formed native text.
Han et al (2006) train a maximum entropy model to
correct article mistakes. Chodorow et. al (2007),
Tetreault and Chodorow (2008), and De Felice and
Pulman (2008) train a maximum entropy model and
De Felice and Pulman (2007) train a voted percep-
tron algorithm to correct preposition errors. Gamon
et al (2008) train a decision tree model and a lan-
guage model to correct errors in article and preposi-
tion usage. Bergsma et al (2009) propose a Na??ve
Bayes algorithm with web-scale N-grams as fea-
tures, for preposition selection and context-sensitive
spelling correction.
The set of valid candidate corrections for a target
word includes all words in the confusion set. For the
preposition correction task, the entire set of prepo-
sitions considered for the task is viewed as the set
of possible corrections for each preposition in non-
native text. Given a preposition with its surround-
ing context, the model selects the most likely prepo-
sition from the set of all candidates, where the set
of candidates consists of nine (Felice and Pulman,
2008), 12 (Gamon, 2010), or 34 (Tetreault et al,
2010; Tetreault and Chodorow, 2008) prepositions.
2.1 Using Error-tagged Data in Training
Several recent works explore ways of using anno-
tated non-native text when training error correction
models.
One way to incorporate knowledge about which
confusions are likely with ESL learners into the error
correction system is to train a model on error-tagged
data. Preposition confusions observed in the non-
native text can then be included in training, by us-
ing the preposition chosen by the author (the source
preposition) as a feature. This is not possible with a
system trained on native data, because each source
preposition is always the correct preposition.
Han et al (2010) train a model on partially anno-
tated Korean learner data. The error-tagged model
trained on one million prepositions obtains a slightly
higher recall and a significant improvement in preci-
sion (from 0.484 to 0.817) over a model fives times
larger trained on well-formed text.
Gamon (2010) proposes a hybrid system for
preposition and article correction, by incorporating
the scores of a language model and class probabil-
ities of a maximum entropy model, both trained on
native data, into a meta-classifier that is trained on
a smaller amount of annotated ESL data. The meta-
classifier outperforms by a large margin both of the
native models, but it requires large amounts of ex-
pensive annotated data, especially in order to correct
preposition errors, where the problem complexity is
much larger.
Rozovskaya and Roth (2010b) show that by intro-
ducing into native training data artificial article er-
rors it is possible to improve the performance of the
article correction system, when compared to a clas-
sifier trained on native data. In contrast to Gamon
(2010) and Han et al (2010) that use annotated data
for training, the system is trained on native data, but
the native data are transformed to be more like L1
data through artificial article errors that mimic the
error rates and error patterns of non-native writers.
This method is cheaper, since obtaining error statis-
tics requires much less annotated data than training.
Moreover, the training data size is not restricted by
the amount of the error-tagged data available. Fi-
nally, the source article of the writer can be used in
training as a feature, in the exact same way as with
the models trained on error-tagged data, providing
knowledge about which confusions are likely. Un-
like article errors, preposition errors lend themselves
very well to a study of confusion sets because the set
of prepositions participating in the task is a lot big-
ger than the set of article choices.
963
3 ESL Data
3.1 Preposition Errors in Learner Data
Preposition errors are one of the most common mis-
takes that non-native speakers make. In the Cam-
bridge Learner Corpus2 (CLC), which contains data
by learners of different first language backgrounds
and different proficiency levels, preposition errors
account for about 13.5% of all errors and occur on
average in 10% of all sentences (Leacock et al,
2010). Similar error rates have been reported for
other annotated ESL corpora, e.g. (Izumi et al,
2003; Rozovskaya and Roth, 2010a; Tetreault et al,
2010). Learning correct preposition usage in En-
glish is challenging for learners of all first language
backgrounds (Dalgish, 1985; Bitchener et al, 2005;
Gamon, 2010; Leacock et al, 2010).
3.2 The Annotated Corpus
We use data from an annotated corpus of essays
written by ESL students. The essays were fully cor-
rected and error-tagged by native English speakers.
For each preposition used incorrectly by the author,
the annotator also indicated the correct preposition
choice. Rozovskaya and Roth (2010a) provide a de-
tailed description of the annotation of the data.
The annotated data include sentences by speakers
of five first language backgrounds: Chinese, Czech,
Italian, Russian, and Spanish. The Czech, Italian,
Russian and Spanish data come from the Interna-
tional Corpus of Learner English (ICLE, (Granger
et al, 2002)), which is a collection of essays writ-
ten by advanced learners of English. The Chinese
data is a part of the Chinese Learners of English cor-
pus (CLEC, (Gui and Yang, 2003)) that contains es-
says by students of all levels of proficiency. Table 1
shows preposition statistics based on the annotated
data.
The combined data include 4185 prepositions,
8.4% of which were judged to be incorrect by the
annotators. Table 1 demonstrates that the error rates
in the Chinese speaker data, for which different pro-
ficiency levels are available, are 2 or 3 times higher
than the error rates in other language groups. The
data for other languages come from very advanced
learners and, while there are also proficiency differ-
2http://www.cambridge.org/elt
Source Total Incorrect Error
language preps. preps. rate
Chinese 953 144 15.1%
Czech 627 28 4.5%
Italian 687 43 6.3%
Russian 1210 85 7.0%
Spanish 708 52 7.3%
All 4185 352 8.4%
Table 1: Statistics on prepositions in the ESL data.
Column Incorrect denotes the number of prepositions
judged to be incorrect by the native annotators. Column
Error rate denotes the proportion of prepositions used in-
correctly.
ences among advanced speakers, their error rates are
much lower.
We would also like to point out that we take as
the baseline3 for the task the accuracy of the non-
native data, or the proportion of prepositions used
correctly. Using the error rate numbers shown in
Table 1, the baseline for Chinese speakers is thus
84.9%, and for all the data combined it is 91.6%.
3.3 Preposition Errors and L1
We focus on preposition confusion errors, mistakes
that involve an incorrectly selected preposition4. We
consider ten most frequent prepositions in English:
on, from, for, of, about, to, at, in, with, and by5.
We mentioned in Section 2 that not all preposition
confusions are equally likely to occur and preposi-
tion errors may depend on the first language of the
writer. Han et al (2010) show that preposition er-
rors in the annotated corpus by Korean learners are
not evenly distributed, some confusions occurring
more often than others. We also observe that con-
fusion frequencies differ by L1. This is consistent
with other studies, which show that learners? errors
are influenced by their first language (Lee and Sen-
eff, 2008; Leacock et al, 2010).
3It is argued in Rozovskaya and Roth (2010b) that the most
frequent class baselines are not relevant for error correction
tasks. Instead, the error rate in the data need to be considered,
when determining the baseline.
4We do not address errors of missing or extraneous preposi-
tions.
5It is common to restrict the systems that detect errors in
preposition usage to the top prepositions. In the CLC corpus,
the usage of the ten most frequent prepositions accounts for
82% of all preposition errors (Leacock et al, 2010).
964
4 Methods of Improving Candidate Sets
In this section, we describe methods of restricting
candidate sets according to the first language of the
writer. For the preposition correction task, the stan-
dard approach considers all prepositions participat-
ing in the task as valid corrections for every prepo-
sition in the non-native data.
In Section 3.3, we pointed out that (1) not all
preposition confusions are equally likely to occur
and (2) preposition errors may depend on the first
language of the writer. The methods of restricting
confusion sets proposed in this work use knowledge
about which prepositions are confusable based on
the data by speakers of language L1.
We refer to the preposition originally chosen by
the author in the non-native text as the source prepo-
sition, and label denotes the correct preposition
choice, as chosen by the annotator. Consider, for ex-
ample, the following sentences from the annotated
corpus.
1. We ate by*/with our hands .
2. To tell the truth , time spent in jail often changes prisoners to*/for
the worse.
3. And the problem that immediately appeared was that men were
unable to cope with the new woman image .
In example 1, the annotator replaced by with with;
by is the source preposition and with is the label. In
example 2, to is the source and for is the label. In
example 3, the preposition with is judged as correct.
Thus, with is both the source and the label.
4.1 L1-Dependent Confusion Sets
Let source preposition pi denote a preposition that
appears in the data by speakers of L1. Let Conf-
Set denote the set of all prepositions that the sys-
tem can propose as a correction for source preposi-
tion pi. We define two types of confusion sets Con-
fSet. An unrestricted confusion set AllConfSet in-
cludes all ten prepositions. L1-dependent confusion
set L1ConfSet(pi) is defined as follows:
Definition L1ConfSet(pi) = {pj |? a sentence in
which an L1 writer replaced preposition pj with pi }
For example, in the Spanish speaker data, from
is used incorrectly in place of of and for. Then for
Spanish speakers, L1ConfSet(from)={from, of, for}.
Source L1ConfSet(pi)
prep. pi
on {on, about, of, to, at, in, with, by}
by {with, by, in}
from {of, from, for}
Table 2: L1-dependent confusion sets for three preposi-
tions based on data by Chinese speakers.
Table 2 shows for Chinese speakers three preposi-
tions and their L1-dependent confusion sets.
We now describe methods of enforcing L1-
dependent confusion sets in training and testing.
4.2 Enforcing L1-dependent Confusion Sets in
Training
We propose two methods of enforcing L1-dependent
confusion sets in training. They are contrasted to
the typical method of training a multi-class 10-way
classifier, where each class corresponds to one of the
ten participating prepositions.
First, we describe the typical training setting.
NegAll Training proceeds in a standard way of
training a multi-class classifier (one-vs-all ap-
proach) on all ten prepositions using well-
formed native English data. For each prepo-
sition pi, pi examples are positive and the other
nine prepositions are negative examples.
We now describe two methods of enforcing L1-
dependent confusion sets in training.
NegL1 This method explores the difference be-
tween training with nine types as negative ex-
amples and (fewer than nine) L1-dependent
negative examples.
For every preposition pi, we train a classifier
using only examples that are in L1ConfSet(pi).
In contrast to NegAll, for each source prepo-
sition, the negative examples are not all other
nine types, but only those that belong in
L1ConfSet(pi). For each language L1, we train
ten classifiers, one for each source preposition.
For source preposition pi in test, we consult
the classifier for pi. In this model, the con-
fusion set for source pi is restricted through
training, since for source pi, the possible can-
didate replacements are only those that the
classifier sees in training, and they are all in
L1ConfSet(pi).
965
Training Negative examples
data NegAll NegL1
Clean NegAll-Clean NegL1-Clean
ErrorL1 NegAll-ErrorL1 -
Table 3: Training conditions that result in unrestricted
(All) and L1-dependent training paradigms.
ErrorL1 This method restricts the candidate set to
L1ConfSet(pi) by generating artificial preposi-
tion errors in the spirit of Rozovskaya and Roth
(2010b). The training data are thus no longer
well-formed or clean, but augmented with L1
error statistics. Specifically, each preposition
pi in training is replaced with a different prepo-
sition pj with probability probConf, s.t.
probConf = prob(pi|pj) (1)
Suppose 10% of all source prepositions to in
the Russian speaker data correspond to label
for. Then for is replaced with to with proba-
bility 0.1.
The classifier uses in training the source prepo-
sition as a feature, which cannot be done when
training on well-formed text, as discussed in
Section 2.1. By providing the source prepo-
sition as a feature, we enforce L1-dependent
confusion sets in training, because the system
learns which candidate corrections occur with
source preposition pi. An important distinction
of this approach is that it does not simply pro-
vide L1-dependent confusion sets in training:
Because errors are generated using L1 writers?
error statistics, the likelihood of each candidate
correction is also provided. This approach is
also more knowledge-intensive, as it requires
annotated data to obtain error statistics.
It should be noted that this method is orthogo-
nal to the NegAll and NegL1 methods of train-
ing described above and can be used in con-
junction with each of them, only that it trans-
forms the training data to account in a more
natural way for ESL writing.
We combine the proposed methods NegAll,
NegL1 with the Clean or ErrorL1 methods and cre-
ate three training approaches shown in Table 3.
4.3 Restricting Confusion Sets in Testing
To reduce the number of false alarms, correction
systems generally use a threshold on the confidence
of the classifier, following (Carlson et al, 2001), and
propose a correction only when the confidence of the
classifier is above the threshold. We show in Section
5 that the system trained on data with artificial er-
rors performs competitively even without a thresh-
old. The other systems use a threshold. We consider
two ways of applying a threshold6:
1. ThreshAll A correction for source preposition
pi is proposed only when the confidence of
the classifier exceeds the threshold. For each
preposition in the non-native data, this method
considers all candidates as valid corrections.
2. ThreshL1Conf A correction for source prepo-
sition pi is proposed only when the confi-
dence of the classifier exceeds the empirically
found threshold and the preposition proposed
as a correction for pi is in the confusion set
L1ConfSet(pi).
5 Experimental Setup
In this section, we describe experiments with L1-
dependent confusion sets. Combining the three
training conditions shown in Table 3 with the two
ways of thresholding described in Section 4.3, we
build four systems7:
1. NegAll-Clean-ThreshAll This system assumes
both in training and in testing stages that all
preposition confusions are possible. The sys-
tem is trained as a multi-class 10-way classifier,
where for each preposition pi, all other nine
prepositions are negative examples. In testing,
when applying the threshold, all prepositions
are considered as valid corrections.
2. NegAll-Clean-ThreshL1 This system is
trained exactly as NegAll-Clean-ThreshAll
but in testing only corrections that belong
6Thresholds are found empirically: We divide the evaluation
data into three equal parts and to each part apply the threshold,
which is optimized on the other two parts of the data.
7In testing, it is not possible to consider a confusion set
larger than the one used in training. Therefore, ThreshAll is
only possible with NegAll training condition.
966
to L1ConfSet(pi) are considered as valid
corrections for pi.
3. NegL1-Clean-ThreshL1 For each preposition
pi, a separate classifier is trained on the prepo-
sitions that are in L1ConfSet(pi), where pi ex-
amples are positive and a set of (fewer than
nine) pi-dependent prepositions are negative.
Only corrections that belong to L1ConfSet(pi)
are considered as valid corrections for pi.8 Ten
pi-dependent classifiers for each L1 are trained.
4. NegAll-ErrorL1-NoThresh A system is trained
as a multi-class 10-way classifier with artifi-
cial preposition errors that mimic the errors
rates and confusion patterns of the non-native
text. For each L1, an L1-dependent system is
trained. This system does not use a threshold.
We discuss this in more detail below.
The system NegAll-Clean-ThreshAll is our base-
line system. It assumes both in training and in test-
ing that all preposition confusions are possible.
All of the systems are trained on the same set of
word and part-of-speech features using the same set
of training examples. Features are extracted from a
window of eight words around the preposition and
include words, part-of-speech tags and conjunctions
of words and tags of lengths two, three, and four.
Training data are extracted from English Wikipedia
and the New York Times section of the Gigaword
corpus (Linguistic Data Consortium, 2003).
In each training paradigm, we follow a discrimi-
native approach, using an online learning paradigm
and making use of the Averaged Perceptron Algo-
rithm (Freund and Schapire, 1999) ? we use the
regularized version in Learning Based Java9 (LBJ,
(Rizzolo and Roth, 2007)). While classical Per-
ceptron comes with generalization bound related to
the margin of the data, Averaged Perceptron also
comes with a PAC-like generalization bound (Fre-
und and Schapire, 1999). This linear learning al-
gorithm is known, both theoretically and experi-
mentally, to be among the best linear learning ap-
proaches and is competitive with SVM and Logistic
8ThreshAll is not possible with this training option, as the
system never proposes a correction that is not in L1ConfSet(pi).
9LBJ code is available at http://cogcomp.cs.
illinois.edu/page/software
Regression, while being more efficient in training.
It also has been shown to produce state-of-the-art
results on many natural language applications (Pun-
yakanok et al, 2008).
6 Results and Discussion
Table 4 shows performance of the four systems
by the source language. For each source lan-
guage, the methods that restrict candidate sets in
training or testing outperform the baseline system
NegAll-Clean-ThreshAll that does not restrict can-
didate sets. The NegAll-ErrorL1-NoThresh system
performs better than the other three systems for all
languages, except for Italian. In fact, for the Czech
speaker data, all systems other than NegAll-ErrorL1-
NoThresh, have a precision and a recall of 0, since
no errors are detected10.
Source System Acc. P R
lang.
CH
NegAll-Clean-ThreshAll 84.78 47.58 11.46
NegAll-Clean-ThreshL1 84.84 48.05 15.28
NegL1-Clean-ThreshL1 84.94 50.87 11.46
NegAll-ErrorL1-NoThresh 86.36 55.27 27.43
Baseline 84.89
CZ
NegAll-Clean-ThreshAll 94.74 0.00 0.00
NegAll-Clean-ThreshL1 94.98 0.00 0.00
NegL1-Clean-ThreshL1 94.66 0.00 0.00
NegAll-ErrorL1-NoThresh 95.85 75.00 10.71
Baseline 95.53
IT
NegAll-Clean-ThreshAll 93.23 26.14 8.14
NegAll-Clean-ThreshL1 94.03 51.59 18.60
NegL1-Clean-ThreshL1 93.16 35.00 16.28
NegAll-ErrorL1-NoThresh 93.60 44.95 10.47
Baseline 93.74
RU
NegAll-Clean-ThreshAll 92.73 31.11 3.53
NegAll-Clean-ThreshL1 93.02 48.81 8.24
NegL1-Clean-ThreshL1 92.44 34.42 8.82
NegAll-ErrorL1-NoThresh 93.14 52.38 12.94
Baseline 92.98
SP
NegAll-Clean-ThreshAll 91.95 26.14 5.77
NegAll-Clean-ThreshL1 92.02 28.64 5.77
NegL1-Clean-ThreshL1 92.44 40.00 7.69
NegAll-ErrorL1-NoThresh 93.71 77.50 19.23
Baseline 92.66
Table 4: Performance results for the 4 systems. All sys-
tems, except for NegAll-ErrorL1-NoThresh, use a thresh-
old, which is optimized for accuracy on the development
set. Baseline denotes the percentage of prepositions used
correctly in the data. The baseline allows us to evaluate
the systems with respect to accuracy, the percentage of
prepositions, on which the prediction of the system is the
same as the label. Averaged results over 2 runs.
10The Czech data set is the smallest and contains a total of
627 prepositions and only 28 errors.
967
The NegAll-ErrorL1-NoThresh system does not
use a threshold. However, as shown in Fig. 1, it
is possible to increase the precision of the NegAll-
ErrorL1-NoThresh system by applying a threshold,
at the expense of a lower recall.
While the ordering of the systems with respect to
quality is not consistent from Table 4, due to modest
test data sizes, Table 5 and Fig. 1 show results for the
models on all data combined and thus give a better
idea of how the systems compare against each other.
Table 5 shows performance results for all
data combined. Both NegAll-Clean-ThreshL1 and
NegL1-Clean-ThreshL1 achieve a better precision
and recall over the system with an unrestricted can-
didate set NegAll-Clean-ThreshAll. Recall that both
of the systems restrict candidate sets, the former at
testing stage, the latter by training a separate clas-
sifier for each source preposition. NegAll-Clean-
ThreshL1 performs slightly better than NegL1-
Clean-ThreshL1. We hypothesize that the NegAll-
Clean-ThreshAll performance may be affected be-
cause the classifiers for different source preposi-
tions contain different number of classes, depend-
ing on the size of L1ConfSet confusion sets, which
makes it more difficult to find a unified thresh-
old. The best performing system overall is NegAll-
ErrorL1-NoThresh. While NegAll-Clean-ThreshL1
and NegL1-Clean-ThreshL1 restrict candidate sets,
NegAll-ErrorL1-NoThresh also provides informa-
tion about the likelihood of each confusion, which
benefits the classifier. The differences between
NegAll-ErrorL1-ThreshL1 and each of the other
three systems are statistically significant11 (McNe-
mar?s test, p < 0.01). The table also demon-
strates that the results on the correction task may
vary widely. For example, the recall varies by lan-
guage between 10.47% and 27.43% for the NegAll-
ErrorL1-NoThresh system. The highest recall num-
bers are obtained for Chinese speakers. These
speakers also have the highest error rate, as we noted
in Section 3.
11Tests of statistical significance compare the combined re-
sults from all language groups for each model. For example, to
compare the model NegAll-Clean-ThreshAll to NegAll-ErrorL1-
NoThresh, we combine the results from the five language-
specific models NegAll-ErrorL1-NoThresh and compare them
to the results on the combined data from the five language
groups achieved by the model NegAll-Clean-ThreshAll.
System Acc. P R
NegAll-Clean-ThreshAll 90.90 31.11 7.95
NegAll-Clean-ThreshL1 91.11 37.82 12.78
NegL1-Clean-ThreshL1 90.97 34.34 9.66
NegAll-ErrorL1-NoThresh 92.23 58.47 19.60
Table 5: Comparison of the performance of the 4 sys-
tems on all data combined. All systems, except for
NegAll-ErrorL1-NoThresh, use a threshold, which is op-
timized for accuracy on the development set. The dif-
ferences between NegAll-ErrorL1-ThreshL1 and each of
the other three systems are statistically significant (Mc-
Nemar?s test, p < 0.01).
Finally, Fig. 1 shows precision/recall curves for
the systems12. The curves are obtained by varying
a decision threshold for each system. Before we ex-
amine the differences between the models, it should
be noted that in error correction tasks precision is
favored over recall due to the low level of error.
0
20
40
60
80
100
0 10 20 30 40 50 60
P
R
NegAll-Clean-ThreshAll
NegAll-Clean-ThreshL1
NegAll-ErrorL1-ThreshL1
?
?
????????????????????
?
Figure 1: Precision and recall (%) for three mod-
els: NegAll-Clean-ThreshAll, NegAll-Clean-ThreshL1,
and NegAll-ErrorL1-ThreshL1.
The curves demonstrate that NegAll-Clean-
ThreshL1 and NegAll-ErrorL1-ThreshL1 are supe-
rior to the baseline system NegAll-Clean-ThreshAll:
on the same recall points, the precision for both
systems is consistently better than for the base-
12NegL1-Clean-ThreshL1 is not shown, since it is similar in
its behavior to NegAll-Clean-ThreshL1.
968
line model13. Moreover, while restricting candi-
date sets improves the results, providing informa-
tion to the classifier about the likelihoods of differ-
ent confusions is more helpful, which is reflected
in the precision differences between NegAll-Clean-
ThreshL1 and NegAll-ErrorL1-ThreshL1. In fact,
NegAll-ErrorL1-ThreshL1 achieves a higher preci-
sion compared to the other systems, even when no
threshold is used (Tables 4 and 5). This is because,
unlike the other models, this system does not tend to
propose too many false alarms.
6.1 Comparison to Other Systems
It is difficult to compare performance to other sys-
tems, since training and evaluation are not per-
formed on the same data, and results may vary
widely depending on the first language and profi-
ciency level of the writer. However, in Table 6 we
list several systems and their performance on the
task. Tetreault et al (2010) train on native data and
obtain a precision of 48.6% and a recall of 22.5%
with top 34 prepositions on essays from the Test
of English as a Foreign Language exams. Han et
al. (2010) obtain a precision of 81.7% and a recall
of 13.2% using a model trained on partially error-
tagged data by Korean speakers on top ten preposi-
tions. A model trained on 2 million examples from
clean text achieved on the same data set a precision
of 46.3% and a recall of 11.6%.
Gamon (2010) shows precision/recall curves on
the combined task of detecting missing, extrane-
ous and confused prepositions. For recall points
10% and 20%, precisions of 55% and 40%, respec-
tively, are obtained. For our data, a recall of 10%
corresponds to a precision of 46% for the worst-
performing model and 78% for the best-performing
model. For 20% recall, we obtain a precision of
33% for the worst-performing model and 58% for
the best-performing model. We would like to em-
phasize that these comparisons should be interpreted
with caution.
13While significance tests did not show differences between
NegAll-Clean-ThreshAll and NegAll-Clean-ThreshL1, perhaps
due to a modest test set size, the curves demonstrate that the lat-
ter system indeed provides a stable advantage over the baseline
unrestricted approach.
7 Conclusion and Future Work
In this paper, we proposed methods for improving
candidate sets for the task of detecting and correct-
ing errors in text. To correct errors in preposition
usage made by non-native speakers of English, we
proposed L1-dependent confusion sets that deter-
mine valid candidate corrections using knowledge
about preposition confusions observed in the non-
native text. We found that restricting candidates to
System Training Data P R
Tetreault et al, 2010 native; 34 preps. 48.6 22.5
Han et al, 2010 partially error-tagged; 81.7 13.2
10 preps.
Han et al, 2010 native; 10 preps. 46.3 11.6
Gamon, 2010 native; 12 preps.+ 33.0 10.0
extraneous+missing
Gamon, 2010 native+error-tagged; 55.0 10.0
12 preps.+
extraneous+missing
NegAll-Clean-ThreshAll native; 10 preps. 46.0 10.0
NegAll-ErrorL1-ThreshL1 native with 78.0 10.0
L1 error statistics;
10 preps.
Table 6: Comparison to other systems. Please note
that a direct comparison is not possible, since the systems
are trained and evaluated on different data sets. Gamon
(2010) also considers missing and extraneous preposition
errors.
those that are observed in the non-native data im-
proves both the precision and the recall compared to
a classifier that considers as possible candidates the
set of all prepositions. Furthermore, the approach
that takes into account the likelihood of each prepo-
sition confusion is shown to be the most effective.
The methods proposed in this paper make use of
select characteristics that the error-tagged data can
provide. We would also like to compare the pro-
posed methods to the quality of a model trained on
error-tagged data. Improving the system is also in
our future work, but orthogonal to the current con-
tribution.
Acknowledgments
We thank Nick Rizzolo for helpful discussions on
LBJ. We also thank Peter Chew and the anonymous
reviewers for their insightful comments. This re-
search is partly supported by a grant from the U.S.
Department of Education.
969
References
M. Banko and E. Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Pro-
ceedings of 39th Annual Meeting of the Association for
Computational Linguistics, pages 26?33, Toulouse,
France, July.
J. Bitchener, S. Young, and D. Cameron. 2005. The ef-
fect of different types of corrective feedback on ESL
student writing. Journal of Second Language Writing.
A. Carlson and I. Fette. 2007. Memory-based context-
sensitive spelling correction at web scale. In Proceed-
ings of the IEEE International Conference on Machine
Learning and Applications (ICMLA).
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling
up context sensitive text correction. In Proceedings of
the National Conference on Innovative Applications of
Artificial Intelligence (IAAI), pages 45?50.
M. Chodorow, J. Tetreault, and N.-R. Han. 2007. Detec-
tion of grammatical errors involving prepositions. In
Proceedings of the Fourth ACL-SIGSEM Workshop on
Prepositions, pages 25?30, Prague, Czech Republic,
June. Association for Computational Linguistics.
G. Dalgish. 1985. Computer-assisted ESL research.
CALICO Journal, 2(2).
J. Eeg-Olofsson and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. Nodalida.
A. Elghaari, D. Meurers, and H. Wunsch. 2010. Ex-
ploring the data-driven prediction of prepositions in
english. In Proceedings of COLING 2010, Beijing,
China.
R. De Felice and S. Pulman. 2007. Automatically ac-
quiring models of preposition use. In Proceedings of
the Fourth ACL-SIGSEM Workshop on Prepositions,
pages 45?50, Prague, Czech Republic, June.
R. De Felice and S. Pulman. 2008. A classifier-based ap-
proach to preposition and determiner error correction
in L2 English. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 169?176, Manchester, UK, August.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings of
IJCNLP.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?171,
Los Angeles, California, June.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
S. Granger, E. Dagneaux, and F. Meunier. 2002. Inter-
national Corpus of Learner English. Presses universi-
taires de Louvain.
S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu
Yuliaohu. (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe. (In Chinese).
N. Han, M. Chodorow, and C. Leacock. 2006. Detecting
errors in English article usage by non-native speakers.
Journal of Natural Language Engineering, 12(2):115?
129.
N. Han, J. Tetreault, S. Lee, and J. Ha. 2010. Us-
ing an error-annotated learner corpus to develop and
ESL/EFL error correction system. In LREC, Malta,
May.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the Japanese
learners? English spoken data. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
145?148, Sapporo, Japan, July.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Morgan and Claypool Publishers.
J. Lee and S. Seneff. 2008. An analysis of grammatical
errors in non-native speech in English. In Proceedings
of the 2008 Spoken Language Technology Workshop.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
N. Rizzolo and D. Roth. 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Inter-
national Conference on Semantic Computing (ICSC),
pages 597?604, Irvine, California, September. IEEE.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 806?813.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
NAACL Workshop on Innovative Use of NLP for Build-
ing Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Training paradigms
for correcting errors in grammar and usage. In Pro-
ceedings of the NAACL-HLT.
J. Tetreault and M. Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
865?872, Manchester, UK, August.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In ACL.
970
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1099?1109,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Constraints based Taxonomic Relation Classification
Quang Xuan Do Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{quangdo2,danr}@illinois.edu
Abstract
Determining whether two terms in text have
an ancestor relation (e.g. Toyota and car) or
a sibling relation (e.g. Toyota and Honda) is
an essential component of textual inference in
NLP applications such as Question Answer-
ing, Summarization, and Recognizing Textual
Entailment. Significant work has been done
on developing stationary knowledge sources
that could potentially support these tasks, but
these resources often suffer from low cover-
age, noise, and are inflexible when needed to
support terms that are not identical to those
placed in them, making their use as general
purpose background knowledge resources dif-
ficult. In this paper, rather than building a sta-
tionary hierarchical structure of terms and re-
lations, we describe a system that, given two
terms, determines the taxonomic relation be-
tween them using a machine learning-based
approach that makes use of existing resources.
Moreover, we develop a global constraint opti-
mization inference process and use it to lever-
age an existing knowledge base also to enforce
relational constraints among terms and thus
improve the classifier predictions. Our exper-
imental evaluation shows that our approach
significantly outperforms other systems built
upon existing well-known knowledge sources.
1 Introduction
Taxonomic relations that are read off of structured
ontological knowledge bases have been shown to
play important roles in many computational linguis-
tics tasks, such as document clustering (Hotho et
al., 2003), navigating text databases (Chakrabarti et
al., 1997), Question Answering (QA) (Saxena et al,
2007) and summarization (Vikas et al, 2008). It
is clear that the recognition of taxonomic relation
between terms in sentences is essential to support
textual inference tasks such as Recognizing Textual
Entailment (RTE) (Dagan et al, 2006). For exam-
ple, it may be important to know that a blue Toy-
ota is neither a red Toyota nor a blue Honda, but
that all are cars, and even Japanese cars. Work in
Textual Entailment has argued quite convincingly
(MacCartney and Manning, 2008; MacCartney and
Manning, 2009) that many such textual inferences
are largely compositional and depend on the ability
to recognize some basic taxonomic relations such
as the ancestor or sibling relations between terms.
To date, these taxonomic relations can be read off
manually generated ontologies such as Wordnet that
explicitly represent these, and there has also been
some work trying to extend the manually built re-
sources using automatic acquisition methods result-
ing in structured knowledge bases such as the Ex-
tended WordNet (Snow et al, 2006) and the YAGO
ontology (Suchanek et al, 2007).
However, identifying when these relations hold
using fixed stationary hierarchical structures may
be impaired by noise in the resource and by uncer-
tainty in mapping targeted terms to concepts in the
structures. In addition, for knowledge sources de-
rived using bootstrapping algorithms and distribu-
tional semantic models such as (Pantel and Pen-
nacchiotti, 2006; Kozareva et al, 2008; Baroni and
Lenci, 2010), there is typically a trade-off between
precision and recall, resulting either in a relatively
accurate resource with low coverage or a noisy re-
1099
source with broader coverage. In the current work,
we take a different approach, identifying directly
whether a pair of terms hold a taxonomic relation.
Fixed resources, as we observe, are inflexible
when dealing with targeted terms not being cov-
ered. This often happens when targeted terms have
the same meaning, but different surface forms, than
the terms used in the resources (e.g. Toyota Camry
and Camry). We argue that it is essential to have a
classifier that, given two terms, can build a semantic
representation of the terms and determines the tax-
onomic relations between them. This classifier will
make use of existing knowledge bases in multiple
ways, but will provide significantly larger coverage
and more precise results. We make use of a dynamic
resource such as Wikipedia to guarantee increased
coverage without changing our model and also per-
form normalization-to-Wikipedia to find appropri-
ate Wikipedia replacements for outside-Wikipedia
terms. Moreover, stationary resources are usually
brittle because of the way most of them are built:
using local relational patterns (e.g. (Hearst, 1992;
Snow et al, 2005)). Infrequent terms are less likely
to be covered, and some relations may not be sup-
ported well by these methods because their cor-
responding terms rarely appear in close proximity
(e.g., an Israeli tennis player Dudi Sela and Roger
Federrer). Our approach uses search techniques to
gather relevant Wikipedia pages of input terms and
performs a learning-based classification w.r.t. to the
features extracted from these pages as a way to get
around this brittleness.
Motivated by the needs of NLP applications such
as RTE, QA, Summarization, and the composition-
ality argument alluded to above, we focus on identi-
fying two fundamental types of taxonomic relations
- ancestor and sibling. An ancestor relation and its
directionality can help us infer that a statement with
respect to the child (e.g. cannabis) holds for an
ancestor (e.g. drugs) as in the following example,
taken from a textual entailment challenge dataset:
T: Nigeria?s NDLEA has seized 80 metric
tonnes of cannabis in one of its largest ever
hauls, officials say.
H: Nigeria seizes 80 tonnes of drugs.
Similarly, it is important to know of a sibling re-
lation to infer that a statement about Taiwan may
(without additional information) contradict a simi-
lar statement with respect to Japan since these are
different countries, as in the following:
T: A strong earthquake struck off the southern
tip of Taiwan at 12:26 UTC, triggering a warn-
ing from Japan?s Meteorological Agency that
a 3.3 foot tsunami could be heading towards
Basco, in the Philippines.
H: An earthquake strikes Japan.
Several recent TE studies (Abad et al, 2010; Sam-
mons et al, 2010) suggest to isolate TE phenomena,
such as recognizing taxonomic relations, and study
them separately; they discuss some of characteristics
of phenomena such as contradiction from a similar
perspective to ours, but do not provide a solution.
In this paper, we present TAxonomic RElation
Classifier (TAREC), a system that classifies taxo-
nomic relations between a given pair of terms us-
ing a machine learning based classifier. An inte-
gral part of TAREC is also our inference model that
makes use of relational constraints to enforce co-
herency among several related predictions. TAREC
does not aim at building or extracting a hierarchi-
cal structure of concepts and relations, but rather to
directly recognize taxonomic relations given a pair
of terms. Target terms are represented using vector
of features that are extracted from retrieved corre-
sponding Wikipedia pages. In addition, we make
use of existing stationary ontologies to find related
terms to the target terms, and classify those too. This
allows us to make use of a constraint-based infer-
ence model (following (Roth and Yih, 2004; Roth
and Yih, 2007) that enforces coherency of decisions
across related pairs (e.g., if x is-a y and y is-a z, it
cannot be that x is a sibling of z).
In the rest of the paper, after discussing re-
lated work in Section 2, we present an overview of
TAREC in Section 3. The learning component and
the inference model of TAREC are described in Sec-
tions 4 and 5. We experimentally evaluate TAREC
in Section 6 and conclude our paper in Section 7.
2 Related Work
There are several works that aim at building tax-
onomies and ontologies which organize concepts
and their taxonomic relations into hierarchical struc-
tures. (Snow et al, 2005; Snow et al, 2006) con-
1100
structed classifiers to identify hypernym relation-
ship between terms from dependency trees of large
corpora. Terms with recognized hypernym rela-
tion are extracted and incorporated into a man-made
lexical database, WordNet (Fellbaum, 1998), re-
sulting in the extended WordNet, which has been
augmented with over 400, 000 synsets. (Ponzetto
and Strube, 2007) and (Suchanek et al, 2007) both
mined Wikipedia to construct hierarchical structures
of concepts and relations. While the former ex-
ploited Wikipedia category system as a conceptual
network and extracted a taxonomy consisting of sub-
sumption relations, the latter presented the YAGO
ontology, which was automatically constructed by
mining and combining Wikipedia and WordNet. A
natural way to use these hierarchical structures to
support taxonomic relation classification is to map
targeted terms onto the hierarchies and check if
they subsume each other or share a common sub-
sumer. However, this approach is limited because
constructed hierarchies may suffer from noise and
require exact mapping (Section 6). TAREC over-
comes these limitations by searching and selecting
the top relevant articles in Wikipedia for each input
term; taxonomic relations are then recognized based
on the features extracted from these articles.
On the other hand, information extraction boot-
strapping algorithms, such as (Pantel and Pennac-
chiotti, 2006; Kozareva et al, 2008), automatically
harvest related terms on large corpora by starting
with a few seeds of pre-specified relations (e.g. is-
a, part-of). Bootstrapping algorithms rely on some
scoring function to assess the quality of terms and
additional patterns extracted during bootstrapping it-
erations. Similarly, but with a different focus, Open
IE, (Banko and Etzioni, 2008; Davidov and Rap-
poport, 2008), deals with a large number of relations
which are not pre-specified. Either way, the out-
put of these algorithms is usually limited to a small
number of high-quality terms while sacrificing cov-
erage (or vice versa). Moreover, an Open IE sys-
tem cannot control the extracted relations and this is
essential when identifying taxonomic relations. Re-
cently, (Baroni and Lenci, 2010) described a gen-
eral framework of distributional semantic models
that extracts significant contexts of given terms from
large corpora. Consequently, a term can be repre-
sented by a vector of contexts in which it frequently
appears. Any vector space model could then use the
terms? vectors to cluster terms into categories. Sib-
ling terms (e.g. Honda, Toyota), therefore, have very
high chance to be clustered together. Nevertheless,
this approach cannot recognize ancestor relations.
In this paper, we compare TAREC with this frame-
work only on recognizing sibling vs. no relation, in
a strict experimental setting which pre-specifies the
categories to which the terms belong.
3 An Overview of the TAREC Algorithm
3.1 Preliminaries
In the TAREC algorithm, a term refers to any men-
tion in text, such as mountain, George W. Bush, bat-
tle of Normandy. TAREC does not aim at extracting
terms and building a stationary hierarchical structure
of terms, but rather recognize the taxonomic relation
between any two given terms. TAREC focuses on
classifying two fundamental types of taxonomic re-
lations: ancestor and sibling. Determining whether
two terms hold a taxonomic relation depends on a
pragmatic decision of how far one wants to climb up
a taxonomy to find a common subsumer. For exam-
ple, George W. Bush is a child of Presidents of the
United States as well as people, even more, that term
could also be considered as a child of mammals or
organisms w.r.t. the Wikipedia category system; in
that sense, George W. Bush may be considered as a
sibling of oak because they have organisms as a least
common subsumer. TAREC makes use of a hierar-
chical structure as background knowledge and con-
siders two terms to hold a taxonomic relation only
if the relation can be recognized from information
acquired by climbing up at most K levels from the
representation of the target terms in the structure. It
is also possible that the sibling relation can be rec-
ognized by clustering terms together by using vector
space models. If so, two terms are siblings if they
belong to the same cluster.
To cast the problem of identifying taxonomic rela-
tions between two terms x and y in a machine learn-
ing perspective, we model it as a multi-class classi-
fication problem. Table 1 defines four relations with
some examples in our experiment data sets.
This paper focuses on studying a fundamental
problem of recognizing taxonomic relations (given
well-segmented terms) and leaves the orthogonal is-
1101
Examples
Relation Meaning Term x Term y
x? y x is an ancestor actor Mel Gibson
of y food rice
x? y x is a child Makalu mountain
of y Monopoly game
x? y x and y are Paris London
siblings copper oxygen
x= y x and y have Roja C++
no relation egg Vega
Table 1: Taxonomic relations and some examples in our
data sets.
sues of how to take contexts into account and how it
should be used in applications to a future work.
3.2 The Overview of TAREC
Assume that we already have a learned local clas-
sifier that can classify taxonomic relations between
any two terms. Given two terms, TAREC uses
Wikipedia and the local classifier in an inference
model to make a final prediction on the taxonomic
relation between these two. To motivate the need for
an inference model, beyond the local classifier itself,
we observe that the presence of other terms in addi-
tion to the two input terms, can provide some natural
constraints on the possible taxonomic relations and
thus can be used to make the final prediction (which
we also refer as global prediction) more coherent. In
practice, we first train a local classifier (Section 4),
then incorporate it into an inference model (Section
5) to classify taxonomic relations between terms.
The TAREC algorithm consists of three steps and
is summarized in Figure 1 and explained below.
1. Normalizing input terms to Wikipedia: Al-
though most commonly used terms have corre-
sponding Wikipedia articles, there are still a lot of
terms with no corresponding Wikipedia articles. For
a non-Wikipedia term, we make an attempt to find
a replacement by using Web search. We wish to
find a replacement such that the taxonomic relation
is unchanged. For example, for input pair (Lojze
Kovac?ic?, Rudi S?eligo), there is no English Wikipedia
page for Lojze Kovac?ic?, but if we can find Marjan
Roz?anc and use it as a replacement of Lojze Kovac?ic?
(two terms are siblings and refer to two writers), we
can continue classifying the taxonomic relation of
the pair (Marjan Roz?anc, Rudi S?eligo). This part
of the algorithm was motivated by (Sarmento et al,
TAxonomic RElation Classifier (TAREC)
INPUT: A pair of terms (x, y)
A learned local classifierR (Sec. 4)
WikipediaW
OUTPUT: Taxonomic relation r? between x and y
1. (x, y)? NormalizeToWikipedia(x, y,W)
2. Z ? GetAddionalTerms(x, y) (Sec. 5.2)
3. r? = ClassifyAndInference(x, y,Z,R,W) (Sec. 5.1)
RETURN: r?;
Figure 1: The TAREC algorithm.
2007). We first make a query with the two input
terms (e.g. ?Lojze Kovac?ic?? AND ?Rudi S?eligo?)
to search for list-structure snippets in Web docu-
ments1 such as ?... ?delimiter? ca ?delimiter? cb
?delimiter? cc ?delimiter? ...? (the two input terms
should be among ca, cb, cc, ...). The delimiter could
be commas, periods, or asterisks2. For snippets that
contain the patterns of interest, we extract ca, cb, cc
etc. as replacement candidates. To reduce noise,
we empirically constrain the list to contain at least
4 terms that are no longer than 20 characters each.
The candidates are ranked based on their occurrence
frequency. The top candidate with Wikipedia pages
is used as a replacement.
2. Getting additional terms (Section 5.2): TAREC
leverage an existing knowledge base to extract addi-
tional terms related to the input terms, to be used in
the inference model in step 3.
3. Making global prediction with relational con-
straints (Section 5.1): TAREC performs several lo-
cal predictions using the local classifier R (Section
4) on the two input terms and these terms with the
additional ones. The global prediction is then in-
ferred by enforcing relational constraints among the
terms? relations.
4 Learning Taxonomic Relations
The local classifier of TAREC is trained on the
pairs of terms with correct taxonomic relation labels
(some examples are showed in Table 1). The trained
classifier when applied on a new input pair of terms
will return a real valued number which can be inter-
preted as the probability of the predicted label. In
this section, we describe the learning features used
1We use http://developer.yahoo.com/search/web/
2Periods and asterisks capture enumerations.
1102
Title/Term Text Categories
President of
the United
States
The President of the United States is the head of state and head of government of the United States and is the
highest political official in the United States by influence and recognition. The President leads the executive
branch of the federal government and is one of only two elected members of the executive branch...
Presidents of the United States, Presidency of
the United States
George W.
Bush
George Walker Bush; born July 6, 1946) served as the 43rd President of the United States from 2001 to 2009.
He was the 46th Governor of Texas from 1995 to 2000 before being sworn in as President on January 20, 2001...
Children of Presidents of the United States, Gov-
ernors of Texas, Presidents of the United States,
Texas Republicans...
Gerald Ford Gerald Rudolff Ford (born Leslie Lynch King, Jr.) (July 14, 1913 December 26, 2006) was the 38th President
of the United States, serving from 1974 to 1977, and the 40th Vice President of the United States serving from
1973 to 1974.
Presidents of the United States, Vice Presidents
of the United States, Republican Party (United
States) presidential nominees...
Table 2: Examples of texts and categories of Wikipedia articles.
by our local taxonomic relation classifier.
Given two input terms, we first build a semantic
representation for each term by using a local search
engine3 to retrieve a list of top articles in Wikipedia
that are relevant to the term. To do this, we use the
following procedure: (1) Using both terms to make a
query (e.g. ?George W. Bush? AND ?Bill Clinton?)
to search in Wikipedia ; (2) Extracting important
keywords in the titles and categories of the retrieved
articles using TF-IDF (e.g. president, politician); (3)
Combining each input term with the extracted key-
words (e.g. ?George W. Bush? AND ?president?
AND ?politician?) to create a final query used to
search for the term?s relevant articles in Wikipedia.
This is motivated by the assumption that the real
world applications calling TAREC typically does so
with two terms that are related in some sense, so our
procedure is designed to exploit that. For example,
it?s more likely that term Ford in the pair (George
W. Bush, Ford) refers to the former president of the
United States, Gerald Ford, than the founder of Ford
Motor Company, Henry Ford.
Once we have a semantic representation of each
term, in the form of the extracted articles, we extract
from it features that we use as the representation of
the two input terms in our learning algorithm. It is
worth noting that a Wikipedia page usually consists
of a title (i.e. the term), a body text, and a list of
categories to which the page belongs. Table 2 shows
some Wikipedia articles. From now on, we use the
titles of x, the texts of x, and the categories of x to
refer to the titles, texts, and categories of the asso-
ciated articles in the representation of x. Below are
the learning features extracted for input pair (x,y).
Bags-of-words Similarity: We use cosine simi-
larity metric to measure the degree of similarity be-
tween bags of words. We define four bags-of-words
features as the degree of similarity between the texts
3E.g. http://lucene.apache.org/
Degree of similarity
texts(x) vs. categories(y)
categories(x) vs. texts(y)
texts(x) vs. texts(y)
categories(x) vs. categories(y)
Table 3: Bag-of-word features of the pair of terms (x,y);
texts(.) and categories(.) are two functions that extract
associated texts and categories from the semantic repre-
sentation of x and y.
and categories associated with two input terms x and
y in Table 3. To collect categories of a term, we take
the categories of its associated articles and go up K
levels in the Wikipedia category system. In our ex-
periments, we use abstracts of Wikipedia articles in-
stead of whole texts.
Association Information: This features repre-
sents a measure of association between the terms
by considering their information overlap. We cap-
ture this feature by the pointwise mutual informa-
tion (pmi) which quantifies the discrepancy between
the probability of two terms appearing together ver-
sus the probability of each term appearing indepen-
dently4. The pmi of two terms x and y is estimated
as follows:
pmi(x, y) = log
p(x, y)
p(x)p(y)
= log
Nf(x, y)
f(x)f(y)
,
where N is the total number of Wikipedia articles,
and f(.) is the function which counts the number of
appearances of its argument.
Overlap Ratios: The overlap ratio features cap-
ture the fact that the titles of a term usually overlap
with the categories of its descendants. We measure
this overlap as the ratio of the number of common
phrases used in the titles of one term and the cate-
gories of the other term. In our context, a phrase is
4pmi is different than mutual information. The former ap-
plies to specific outcomes, while the latter is to measure the
mutual dependence of two random variables.
1103
considered to be a common phrase if it appears in the
titles of one term and the categories of the other term
and it is also of the following types: (1) the whole
string of a category, or (2) the head in the root form
of a category, or (3) the post-modifier of a category.
We use the Noun Group Parser from (Suchanek et
al., 2007) to extract the head and post-modifier from
a category. For example, one of the categories of an
article about Chicago is Cities in Illinois. This cate-
gory can be parsed into a head in its root form City,
and a post-modifier Illinois. Given term pair (City,
Chicago), we observe that City matches the head of
the category Cities in Illinois of term Chicago. This
is a strong indication that Chicago is a child of City.
We also use a feature that captures the overlap
ratio of common phrases between the categories of
two input terms. For this feature, we do not use the
post-modifier of the categories. We use Jaccard sim-
ilarity coefficient to measure these overlaps ratios.
5 Inference with Relational Constraints
Once we have a local multi-class classifier that maps
a given pair of terms to one of the four possible rela-
tions, we use a constraint-based optimization algo-
rithm to improve this prediction. The key insight
behind the way we model the inference model is
that if we consider more than two terms, there are
logical constraints that restrict the possible relations
among them. For instance, George W. Bush can-
not be an ancestor or sibling of president if we are
confident that president is an ancestor of Bill Clin-
ton, and Bill Clinton is a sibling of George W. Bush.
We call the combination of terms and their relations
a term network. Figure 2 shows some n-term net-
works consisting of two input terms (x, y), and ad-
ditional terms z, w, v.
The aforementioned observations show that if we
can obtain additional terms that are related to the
two target terms, we can enforce such coherency
relational constraints and make a global prediction
that would improve the prediction of the taxonomic
relation between the two given terms. Our infer-
ence model follows constraint-based formulations
that were introduced in the NLP community and
were shown to be very effective in exploiting declar-
ative background knowledge (Roth and Yih, 2004;
Denis and Baldridge, 2007; Punyakanok et al, 2008;
Chang et al, 2008).
George W.
Bush
President
Bill Clinton
x
y
z
Red Green
Blue
x
y
z
(a) (b)
Honda Toyota
car
manufacturer
x
y
z
w
BMW
Celcius meter
temperature
x
y
z
w
length
(d)
(c)
v
physical
quantities
Figure 2: Examples of n-term networks with two input
term x and y. (a) and (c) show valid combinations of
edges, whereas (b) and (d) are two relational constraints.
For simplicity, we do not draw no relation edges in (d).
5.1 Enforcing Coherency through Inference
Let x, y be two input terms, and Z =
{z1, z2, ..., zm} be a set of additional terms. For a
subset Z ? Z , we construct a set of term networks
whose nodes are x, y and all elements in Z, and the
edge, e, between every two nodes is one of four tax-
onomic relations whose weight, w(e), is given by
a local classifier (Section 4). If l = |Z|, there are
n = 2 + l nodes in each network, and 4[
1
2n(n?1)]
term networks can be constructed. In our experi-
ments we only use 3-term networks (i.e. l = 1).
For example, for the input pair (red, green) and
Z = {blue, yellow}, we can construct 64 networks
for the triple ?red, green, Z = {blue}? and 64 net-
works for ?red, green, Z = {yellow}? by trying all
possible relations between the terms.
A relational constraint is defined as a term net-
work consisting of only its ?illegitimate? edge set-
tings, those that belongs to a pre-defined list of in-
valid edge combinations. For example, Figure 2b
shows an invalid network where red is a sibling of
both green and blue, and green is an ancestor of blue.
In Figure 2d, Celcius and meter cannot be siblings
because they are children of two sibling terms tem-
perature and length. The relational constraints used
in our experiments are manually constructed.
Let C be a list of relational constraints. Equation
(1) defines the network scoring function, which is a
linear combination of the edge weights, w(e), and
the penalties, ?k, of term networks matching con-
straint Ck ? C.
score(t) =
?
e?t
w(e)?
|C|?
k=1
?kdCk(t) (1)
function dCk(t) indicates if t matches Ck. In our
work, we use relational constraints as hard con-
1104
YAGO Query Patterns
INPUT: term ?x?
OUTPUT: lists of ancestors, siblings, and children of ?x?
Pattern 1 Pattern 2 Pattern 3
?x? MEANS ?A ?x? MEANS ?A ?x? MEANS ?D
?A SUBCLASSOF ?B ?A TYPE ?B ?E TYPE ?D
?C SUBCLASSOF ?B ?C TYPE ?B
RETURN: ?B, ?C, ?E as
lists of ancestors, siblings, and children, respectively.
Figure 3: Our YAGO query patterns used to obtain related
terms for ?x?.
straints and set their penalty ?k to ?. For a set of
term networks formed by ?x, y, Z? and all possible
relations between the terms, we select the best net-
work, t? = argmaxtscore(t).
After picking the best term network t? for every
Z ? Z , we make the final decision on the taxonomic
relation between x and y. Let r denote the relation
between x and y in a particular t? (e.g. r = x? y.)
The set of all t? is divided into 4 groups with respect
to r (e.g. a group of all t? having r = x ? y, a
group of all t? having r = x ? y.) We denote a
group with term networks holding r as the relation
between x and y by Tr. To choose the best taxo-
nomic relation, r?, of x and y, we solve the objective
function defined in Equation 2.
r? = argmaxr
1
|Tr|
?
t??Tr
?t?score(t
?) (2)
where ?t is the weight of term network t, defined
as the occurrence probability of t (regarding only its
edges? setting) in the training data, which is aug-
mented with additional terms. Equation (2) finds the
best taxonomic relation of two input terms by com-
puting the average score of every group of the best
term networks representing a particular relation of
two input terms.
5.2 Extracting Related Terms
In the inference model, we need to obtain other
terms that are related to the two input terms. Here-
after, we refer to additional terms as related terms.
The related term space is a space of direct ancestors,
siblings and children in a particular resource.
We propose an approach that uses the YAGO on-
tology (Suchanek et al, 2007) to provide related
terms. It is worth noting that YAGO is chosen over
the Wikipedia category system used in our work be-
cause YAGO is a clean ontology built by carefully
combining Wikipedia and WordNet.5
In YAGO model, all objects (e.g. cities, people,
etc.) are represented as entities. To map our input
terms to entities in YAGO, we use the MEANS re-
lation defined in the YAGO ontology. Furthermore,
similar entities are grouped into classes. This allows
us to obtain direct ancestors of an entity by using
the TYPE relation which gives the entity?s classes.
Furthermore, we can get ancestors of a class with
the SUBCLASSOF relation6. By using three relations
MEANS, TYPE and SUBCLASSOF in YAGO model,
we can obtain Proposals for direct ancestors, sib-
lings, and children, if any, for any input term. We
then evaluate our classifier on all pairs, and run the
inference to improve the prediction using the co-
herency constraints. Figure 3 presents three patterns
that we used to query related terms from YAGO.
6 Experimental Study
In this section, we evaluate TAREC against several
systems built upon existing well-known knowledge
sources. The resources are either hierarchical struc-
tures or extracted by using distributional semantic
models. We also perform several experimental anal-
yses to understand TAREC?s behavior in details.
6.1 Comparison to Hierarchical Structures
We create and use two main data sets in our ex-
periments. Dataset-I is generated from 40 seman-
tic classes of about 11,000 instances. The orig-
inal semantic classes and instances were manu-
ally constructed with a limited amount of manual
post-filtering and were used to evaluate informa-
tion extraction tasks in (Pas?ca, 2007; Pas?ca and
Van Durme, 2008) (we refer to this original data as
OrgData-I). This dataset contains both terms with
Wikipedia pages (e.g. George W. Bush) and non-
Wikipedia terms (e.g. hindu mysticism). Pairs of
terms are generated by randomly pairing seman-
tic class names and instances. We generate dis-
joint training and test sets of 8,000 and 12,000 pairs
of terms, respectively. We call the test set of this
5However, YAGO by itself is weaker than our approach in
identifying taxonomic relations (see Section 6.)
6These relations are defined in the YAGO ontology.
1105
dataset Test-I. Dataset-II is generated from 44 se-
mantic classes of more than 10,000 instances used
in (Vyas and Pantel, 2009)7. The original semantic
classes and instances were extracted from Wikipedia
lists. This data, therefore, only contains terms with
corresponding Wikipedia pages. We also generate
disjoint training and test sets of 8,000 and 12,000
pairs of terms, respectively, and call the test set of
this dataset Test-II.8
Several semantic class names in the original data
are written in short forms (e.g. chemicalelem,
proglanguage). We expand these names to some
meaningful names which are used by all systems in
our experiments. For example, terroristgroup is ex-
panded to terrorist group, terrorism. Table 1 shows
some pairs of terms which are generated. Four types
of taxonomic relations are covered with balanced
numbers of examples in all data sets. To evaluate our
systems, we use a snapshot of Wikipedia from July,
2008. After cleaning and removing articles without
categories (except redirect pages), 5,503,763 articles
remain. We index these articles using Lucene9. As
a learning algorithm, we use a regularized averaged
Perceptron (Freund and Schapire, 1999).
We compare TAREC with three systems that we
built using recently developed large-scale hierarchi-
cal structures. Strube07 is built on the latest ver-
sion of a taxonomy, TStrube, which was derived from
Wikipedia (Ponzetto and Strube, 2007). It is worth
noting that the structure of TStrube is similar to the
page structure of Wikipedia. For a fair comparison,
we first generate a semantic representation for each
input term by following the same procedure used in
TAREC described in Section 4. The titles and cat-
egories of the articles in the representation of each
input term are then extracted. Only titles and their
corresponding categories that are in TStrube are con-
sidered. A term is an ancestor of the other if at
least one of its titles is in the categories of the other
term. If two terms share a common category, they
are considered siblings; and no relation, otherwise.
The ancestor relation is checked first, then sibling,
and finally no relation. Snow06 uses the extended
7There were 50 semantic classes in the original dataset. We
grouped some semantically similar classes for the purpose of
classifying taxonomic relations.
8Published at http://cogcomp.cs.illinois.edu/page/software
9http://lucene.apache.org, version 2.3.2
Test-I Test-II
Strube07 24.32 25.63
Snow06 41.97 36.26
Yago07 65.93 70.63
TAREC (local) 81.89 84.7
TAREC 85.34 86.98
Table 4: Evaluating and comparing performances, in ac-
curacy, of the systems on Test-I and Test-II. TAREC (lo-
cal) uses only our local classifier to identify taxonomic re-
lations by choosing the relation with highest confidence.
WordNet (Snow et al, 2006). Words in the extended
WordNet can be common nouns or proper nouns.
Given two input terms, we first map them onto the
hierarchical structure of the extended WordNet by
exact string matching. A term is an ancestor of the
other if it can be found as an hypernym after going
up K levels in the hierarchy from the other term. If
two terms share a common subsumer within some
levels, then they are considered as siblings. Oth-
erwise, there is no relation between the two input
terms. Similar to Strube07, we first check ancestor,
then sibling, and finally no relation. Yago07 uses
the YAGO ontology (Suchanek et al, 2007) as its
main source of background knowledge. Because the
YAGO ontology is a combination of Wikipedia and
WordNet, this system is expected to perform well at
recognizing taxonomic relations. To access a term?s
ancestors and siblings, we use patterns 1 and 2 in
Figure 3 to map a term to the ontology and move up
on the ontology. The relation identification process
is then similar to those of Snow06 and Strube07. If
an input term is not recognized by these systems,
they return no relation.
Our overall algorithm, TAREC, is described in
Figure 1. We manually construct a pre-defined list
of 35 relational constraints to use in the inference
model. We also evaluate our local classifier (Section
4), which is referred as TAREC (local). To make
classification decision with TAREC (local), for a
pair of terms, we choose the predicted relation with
highest confidence returned by the classifier.
In all systems compared, we vary the value ofK10
from 1 to 4. The best result of each system is re-
ported. Table 4 shows the comparison of all sys-
tems evaluated on both Test-I and Test-II. Our sys-
tems, as shown, significantly outperform the other
10See Section 3.1 for the meaning of K.
1106
systems. In Table 4, the improvement of TAREC
over TAREC (local) on Test-I shows the contribu-
tion of both the normalization procedure (that is, go-
ing outside Wikipedia terms) and the global infer-
ence model to the classification decisions, whereas
the improvement on Test-II shows only the contribu-
tion of the inference model, because Test-II contains
only terms with corresponding Wikipedia articles.
Observing the results we see that our algorithms
is doing significantly better that fixed taxonomies
based algorithms. This is true both for TAREC (lo-
cal) and for TAREC. We believe that our machine
learning based classifier is very flexible in extract-
ing features of the two input terms and thus in pre-
dicting their taxonomic Relation. On the other hand,
other system rely heavily on string matching tech-
niques to map input terms to their respective ontolo-
gies, and these are very inflexible and brittle. This
clearly shows one limitation of using existing struc-
tured resources to classify taxonomic relations.
We do not use special tactics to handle polyse-
mous terms. However, our procedure of building se-
mantic representations for input terms described in
Section 4 ties the senses of the two input terms and
thus, implicitly, may get some sense information.
We do not use this procedure in Snow06 because
WordNet and Wikipedia are two different knowl-
edge bases. We also do not use this procedure in
Yago07 because in YAGO, a term is mapped onto the
ontology by using the MEANS operator (in Pattern 1,
Figure 3). This cannot follow our procedure.
6.2 Comparison to Harvested Knowledge
As we discussed in Section 2, the output of
bootstrapping-based algorithms is usually limited to
a small number of high-quality terms while sacri-
ficing coverage (or vice versa). For example, the
full Espresso algorithm in (Pantel and Pennacchiotti,
2006) extracted 69,156 instances of is-a relation
with 36.2% precision. Similarly, (Kozareva et al,
2008) evaluated only a small number (a few hun-
dreds) of harvested instances. Recently, (Baroni
and Lenci, 2010) proposed a general framework to
extract properties of input terms. Their TypeDM
model harvested 5,000 significant properties for
each term out of 20,410 noun terms. For exam-
ple, the properties of marine include ?own, bomb?,
?use, gun?. Using vector space models we could
measure the similarity between terms using their
property vectors. However, since the information
available in TypeDM does not support predicting the
ancestor relation between terms, we only evaluate
TypeDM in classifying sibling vs. no relation. We
do this by giving a list of semantic classes using the
following procedure: (1) For each semantic class,
use some seeds to compute a centroid vector from
the seeds? vectors in TypeDM, (2) each term in an
input pair is classified into its best semantic class
based on the cosine similarity between its vector and
the centroid vector of the category, (3) two terms are
siblings if they are classified into the same category;
and have no relation, otherwise. Out of 20,410 noun
terms in TypeDM, there are only 345 terms overlap-
ping with the instances in OrgData-I and belonging
to 10 significant semantic classes. For each seman-
tic class, we randomly pick 5 instances as its seeds to
make a centroid vector. The rest of the overlapping
instances are randomly paired to make a dataset of
4,000 pairs of terms balanced in the number of sib-
ling and no relation pairs. On this dataset, TypeDM
achieves the accuracy of 79.75%. TAREC (local),
with the local classifier trained on the training set
(with 4 relation classes) of Dataset-I, gives 78.35%
of accuracy. The full TAREC system with relational
constraints achieves 82.65%. We also re-train and
evaluate the local classifier of TAREC on the same
training set but without ancestor relation pairs. This
local classifier has an accuracy of 81.08%.
These results show that although the full TAREC
system gives better performance, TypeDM is very
competitive in recognizing sibling vs. no relation.
However, TypeDM can only work in a limited set-
ting where semantic classes are given in advance,
which is not practical in real-world applications; and
of course, TypeDM does not help to recognize an-
cestor relations between two terms.
6.3 Experimental Analysis
In this section, we discuss some experimental anal-
yses to better understand our systems.
Precision and Recall: We want to study TAREC
on individual taxonomic relations using Precision
and Recall. Table 5 shows that TAREC performs
very well on ancestor relation. Sibling and no rela-
tion are the most difficult relations to classify. In
the same experimental setting on Test-I, Yago07
1107
TAREC
Test-I Test-II
Prec Rec Prec Rec
x? y 95.82 88.01 96.46 88.48
x? y 94.61 89.29 96.15 88.86
x? y 79.23 84.01 83.15 81.87
x= y 73.94 79.9 75.54 88.27
Average 85.9 85.3 87.83 86.87
Table 5: Performance of TAREC on individual taxo-
nomic relation.
Wiki WordNet non-Wiki
Strube07 24.59 24.13 21.18
Snow06 41.23 46.91 34.46
Yago07 69.95 70.42 34.26
TAREC (local) 89.37 89.72 31.22
TAREC 91.03 91.2 45.21
Table 6: Performance of the systems on special data sets,
in accuracy. On the non-Wikipedia test set, TAREC (lo-
cal) simply returns sibling relation.
achieves 79.34% and 66.03% of average Precision
and Recall, respectively. These numbers on Test-II
are 81.33% and 70.44%.
Special Data Sets: We evaluate all systems that
use hierarchical structures as background knowl-
edge on three special data sets derived from Test-I.
From 12,000 pairs in Test-I, we created a test set,
Wiki, consisting of 10, 456 pairs with all terms in
Wikipedia. We use the rest of 1, 544 pairs with at
least one non-Wikipedia term to build a non-Wiki
test set. The third dataset, WordNet, contains 8, 625
pairs with all terms in WordNet and Wikipedia. Ta-
ble 6 shows the performance of the systems on these
data sets. Unsurprisingly, Yago07 gets better results
on Wiki than on Test-I. Snow06, as expected, gives
better performance on the WordNet test set. TAREC
still significantly outperforms these systems. The
improvement of TAREC over TAREC (local) on the
Wiki and WordNet test sets shows the contribution
of the inference model, whereas the improvement on
the non-Wikipedia test set shows the contribution of
normalizing input terms to Wikipedia.
Contribution of Related Terms in Inference:
We evaluate TAREC when the inference procedure
is fed by related terms that are generated using a
?gold standard? source instead of YAGO. To do this,
we use the original data which was used to generate
Test-I. For each term in the examples of Test-I, we
get its ancestors, siblings, and children, if any, from
K=1 K=2 K=3 K=4
TAREC 82.93 85.34 85.23 83.95
TAREC (Gold Infer.) 83.46 86.18 85.9 84.93
Table 7: Evaluating TAREC with different sources pro-
viding related terms to do inference.
the original data and use them as related terms in the
inference model. This system is referred as TAREC
(Gold Infer.). Table 7 shows the results of the two
systems on different K as the number of levels to
go up on the Wikipedia category system. We see
that TAREC gets better results when doing inference
with better related terms. In this experiment, the two
systems use the same number of related terms.
7 Conclusions
We studied an important component of many com-
putational linguistics tasks: given two target terms,
determine that taxonomic relation between them.
We have argued that static structured knowledge
bases cannot support this task well enough, and pro-
vided empirical support for this claim. We have de-
veloped TAREC, a novel algorithm that leverages in-
formation from existing knowledge sources and uses
machine learning and a constraint-based inference
model to mitigate the noise and the level of uncer-
tainty inherent in these resources. Our evaluations
show that TAREC significantly outperforms other
systems built upon existing well-known knowledge
sources. Our approach generalizes and handles non-
Wikipedia term well across semantic classes. Our
future work will include an evaluation of TAREC in
the context of textual inference applications.
Acknowledgments
The authors thank Mark Sammons, Vivek Srikumar, James
Clarke and the anonymous reviewers for their insightful com-
ments and suggestions. University of Illinois at Urbana-
Champaign gratefully acknowledges the support of Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract No. FA8750-09-C-0181. The first author also
thanks the Vietnam Education Foundation (VEF) for its spon-
sorship. Any opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the view of the VEF, DARPA, AFRL,
or the US government.
1108
References
A. Abad, L. Bentivogli, I. Dagan, D. Giampiccolo,
S. Mirkin, E. Pianta, and A. Stern. 2010. A resource
for investigating the impact of anaphora and corefer-
ence on inference. In LREC.
M. Banko and O. Etzioni. 2008. The tradeoffs between
open and traditional relation extraction. In ACL-HLT.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36.
S. Chakrabarti, B. Dom, R. Agrawal, and P. Raghavan.
1997. Using taxonomy, discriminants, and signatures
for navigating in text databases. In VLDB.
M. Chang, L. Ratinov, and D. Roth. 2008. Constraints as
prior knowledge. In ICML Workshop on Prior Knowl-
edge for Text and Language Processing.
D. Davidov and A. Rappoport. 2008. Unsupervised dis-
covery of generic relationships using pattern clusters
and its evaluation by automatically generated sat anal-
ogy questions. In ACL.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In NAACL.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning.
M. A. Hearst. 1992. Acquisition of hyponyms from large
text corpora. In COLING.
A. Hotho, S. Staab, and G. Stumme. 2003. Ontologies
improve text document clustering. In ICDM.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pattern
linkage graphs. In ACL-HLT.
B. MacCartney and C. D. Manning. 2008. Modeling se-
mantic containment and exclusion in natural language
inference. In COLING.
B. MacCartney and C. D. Manning. 2009. An extended
model of natural logic. In IWCS-8.
M. Pas?ca and B. Van Durme. 2008. Weakly-supervised
acquisition of open-domain classes and class attributes
from web documents and query logs. In ACL-HLT.
M. Pas?ca. 2007. Organizing and searching the world
wide web of facts step two: Harnessing the wisdom
of the crowds. In WWW.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In ACL, pages 113?120.
S. P. Ponzetto and M. Strube. 2007. Deriving a large
scale taxonomy from wikipedia. AAAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
CoNLL.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
M. Sammons, V.G. Vydiswaran, and D. Roth. 2010. Ask
not what textual entailment can do for you... In ACL.
L. Sarmento, V. Jijkuon, M. de Rijke, and E. Oliveira.
2007. ?more like these?: growing entity classes from
seeds. In CIKM.
A. K. Saxena, G. V. Sambhu, S. Kaushik, and L. V. Sub-
ramaniam. 2007. Iitd-ibmirl system for question an-
swering using pattern matching, semantic type and se-
mantic category recognition. In TREC.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning
syntactic patterns for automatic hypernym discovery.
In NIPS.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
ACL.
F. M. Suchanek, G. Kasneci, and G. Weikum. 2007.
Yago: A Core of Semantic Knowledge. In WWW.
O. Vikas, A. K. Meshram, G. Meena, and A. Gupta.
2008. Multiple document summarization using princi-
pal component analysis incorporating semantic vector
space model. In Computational Linguistics and Chi-
nese Language Processing.
V. Vyas and P. Pantel. 2009. Semi-automatic entity set
refinement. In NAACL-HLT.
D. Yarowsky. 1995. Unsupervised woed sense disam-
biguation rivaling supervied methods. In Proceedings
of ACL-95.
1109
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 129?139,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Joint Model for Extended Semantic Role Labeling
Vivek Srikumar and Dan Roth
University of Illinois, Urbana-Champaign
Urbana, IL 61801
{vsrikum2, danr}@illinois.edu
Abstract
This paper presents a model that extends se-
mantic role labeling. Existing approaches in-
dependently analyze relations expressed by
verb predicates or those expressed as nominal-
izations. However, sentences express relations
via other linguistic phenomena as well. Fur-
thermore, these phenomena interact with each
other, thus restricting the structures they artic-
ulate. In this paper, we use this intuition to
define a joint inference model that captures
the inter-dependencies between verb seman-
tic role labeling and relations expressed us-
ing prepositions. The scarcity of jointly la-
beled data presents a crucial technical chal-
lenge for learning a joint model. The key
strength of our model is that we use existing
structure predictors as black boxes. By en-
forcing consistency constraints between their
predictions, we show improvements in the per-
formance of both tasks without retraining the
individual models.
1 Introduction
The identification of semantic relations between
sentence constituents has been an important task in
NLP research. It finds applications in various natural
language understanding tasks that require complex
inference going beyond the surface representation.
In the literature, semantic role extraction has been
studied mostly in the context of verb predicates, us-
ing the Propbank annotation of Palmer et al (2005),
and also for nominal predicates, using the Nombank
corpus of Meyers et al (2004).
However, sentences express semantic relations
through other linguistic phenomena. For example,
consider the following sentence:
(1) The field goal by Brien changed the game in the
fourth quarter.
Verb centered semantic role labeling would identify
the arguments of the predicate change as (a) The
field goal by Brien (A0, the causer of the change),
(b) the game (A1, the thing changing), and (c) in
the fourth quarter (temporal modifier). However,
this does not tell us that the scorer of the field goal
was Brien, which is expressed by the preposition by.
Also, note that the in indicates a temporal relation,
which overlaps with the verb?s analysis.
In this paper, we propose an extension of the stan-
dard semantic role labeling task to include relations
expressed by lexical items other than verbs and nom-
inalizations. Further, we argue that there are interac-
tions between the different phenomena which sug-
gest that there is a benefit in studying them together.
However, one key challenge is that large jointly la-
beled corpora do not exist. This motivates the need
for novel learning and inference schemes that ad-
dress the data problem and can still benefit from the
interactions among the phenomena.
This paper has two main contributions.
1. From the machine learning standpoint, we pro-
pose a joint inference scheme to combine exist-
ing structure predictors for multiple linguistic
phenomena. We do so using hard constraints
that involve only the labels of the phenomena.
The strength of our model is that it is easily
129
extensible, since adding new phenomena does
not require fully retraining the joint model from
scratch. Furthermore, our approach minimizes
the need for extensive jointly labeled corpora
and, instead, uses existing predictors as black
boxes.
2. From an NLP perspective, we motivate the ex-
tension of semantic role labeling beyond verbs
and nominalizations. We instantiate our joint
model for the case of extracting preposition and
verb relations together. Our model uses exist-
ing systems that identify verb semantic roles
and preposition object roles and jointly pre-
dicts the output of the two systems in the pres-
ence of linguistic constraints that enforce co-
herence between the predictions. We show that
using constraints to combine models improves
the performance on both tasks. Furthermore,
since the constraints depend only on the labels
of the two tasks and not on any specific dataset,
our experiments also demonstrate that enforc-
ing them allows for better domain adaptation.
The rest of the paper is organized as follows: We
motivate the need for extending semantic role label-
ing and the necessity for joint inference in Section 2.
In Section 3, we describe the component verb SRL
and preposition role systems. The global model is
defined in Section 4. Section 5 provides details on
the coherence constraints we use and demonstrates
the effectiveness of the joint model through experi-
ments. Section 6 discusses our approach in compar-
ison to existing work and Section 7 provides con-
cluding remarks.
2 Problem Definition and Motivation
Semantic Role Labeling has been extensively stud-
ied in the context of verbs and nominalizations.
While this analysis is crucial to understanding a
sentence, it is clear that in many natural language
sentences, information is conveyed via other lexi-
cal items. Consider, for example, the following sen-
tences:
(2) Einstein?s theory of relativity changed physics.
(3) The plays of Shakespeare are widely read.
(4) The bus, which was heading for Nairobi
in Kenya, crashed in the Kabale district of
Uganda.
The examples contain information that cannot be
captured by analyzing the verbs and the nominaliza-
tions. In sentence (2), the possessive form tells us
that the theory of relativity was discovered by Ein-
stein. Furthermore, the theory is on the subject of
relativity. The usage of the preposition of is dif-
ferent in sentence (3), where it indicates a creator-
creation relationship. In the last sentence, the same
preposition tells us that the Kabale district is located
in Uganda. Prepositions, compound nouns, posses-
sives, adjectival forms and punctuation marks of-
ten express relations, the identification of which is
crucial for text understanding tasks like recognizing
textual entailment, paraphrasing and question an-
swering.
The relations expressed by different linguistic
phenomena often overlap. For example, consider the
following sentence:
(5) Construction of the library began in 1968.
The relation expressed by the nominalization con-
struction recognizes the library as the argument of
the predicate construct. However, the same analy-
sis can also be obtained by identifying the sense of
the preposition of, which tells us that the subject of
the preposition is a nominalization of the underlying
verb. A similar redundancy can be observed with
analyses of the verb began and the preposition in.
The above example motivates the following key in-
tuition: The correct interpretation of a sentence is
the one that gives a consistent analysis across all
the linguistic phenomena expressed in it.
An inference mechanism that simultaneously pre-
dicts the structure for different phenomena should
account for consistency between the phenomena. A
model designed to address this has the following
desiderata:
1. It should account for the dependencies between
phenomena.
2. It should be extensible to allow easy addition of
new linguistic phenomena.
130
3. It should be able to leverage existing state-of-
the-art models with minimal use of jointly la-
beled data, which is expensive to obtain.
Systems that are trained on each task indepen-
dently do not account for the interplay between
them. One approach for tackling this is to define
pipelines, where the predictions for one of the tasks
acts as the input for another. However, a pipeline
does not capture the two-way dependency between
the tasks. Training a fully joint model from scratch
is also unrealistic because it requires text that is an-
notated with all the tasks, thus making joint train-
ing implausible from a learning theoretic perspective
(See Punyakanok et al (2005) for a discussion about
the learning theoretic requirements of joint training.)
3 Tasks and Individual Systems
Before defining our proposed model that captures
the requirements listed in the previous section, we
introduce the tasks we consider and their indepen-
dently trained systems that we improve using the
joint system. Though the model proposed here is
general and can be extended to several linguistic
phenomena, in this paper, we focus on relations ex-
pressed by verbs and prepositions. This section de-
scribes the tasks, the data sets we used for our exper-
iments and the current state-of-the-art systems for
these tasks.
We use the following sentence as our running ex-
ample to illustrate the phenomena: The company
calculated the price trends on the major stock mar-
kets on Monday.
3.1 Preposition Relations
Prepositions indicate a relation between the attach-
ment point of the preposition and its object. As we
have seen, the same preposition can indicate dif-
ferent types of relations. In the literature, the pol-
ysemy of prepositions is addressed by The Prepo-
sition Project1 of Litkowski and Hargraves (2005),
which is a large lexical resource for English that la-
bels prepositions with their sense. This sense inven-
tory formed the basis of the SemEval-2007 task of
preposition word sense disambiguation of Litkowski
and Hargraves (2007). In our example, the first on
1http://www.clres.com/prepositions.html
would be labeled with the sense 8(3) which identifies
the object of the preposition as the topic, while the
second instance would be labeled as 17(8), which
indicates that argument is the day of the occurrence.
The preposition sense inventory, while useful to
identify the fine grained distinctions between prepo-
sition usage, defines a unique sense label for each
preposition by indexing the definitions of the prepo-
sitions in the Oxford Dictionary of English. For ex-
ample, in the phrase at noon, the at would be labeled
with the sense 2(2), while the preposition in I will
see you in an hour will be labeled 4(3). Note that
both these (and also the second on in our running ex-
ample) indicate a temporal relation, but are assigned
different labels based on the preposition. To counter
this problem we collapsed preposition senses that
are semantically similar to define a new label space,
which we refer to as Preposition Roles.
We retrained classifiers for preposition sense for
the new label space. Before describing the prepo-
sition role dataset, we briefly describe the datasets
and the features for the sense problem. The best
performing system at the SemEval-2007 shared task
of preposition sense disambiguation (Ye and Bald-
win (2007)) achieves a mean precision of 69.3% for
predicting the fine grained senses. Tratz and Hovy
(2009) and Hovy et al (2010) attained significant
improvements in performance using features derived
from the preposition?s neighbors in the parse tree.
We extended the feature set defined in the former
for our independent system. Table 1 summarizes the
rules for identifying the syntactically related words
for each preposition. We used dependencies from
the easy-first dependency parser of Goldberg and El-
hadad (2010).
For each word extracted from these rules, the fea-
tures include the word itself, its lemma, the POS
tag, synonyms and hypernyms of the first WordNet
sense and an indicator for capitalization. These fea-
tures improved the accuracy of sense identification
to 75.1% on the SemEval test set. In addition, we
also added the following new features for each word:
1. Indicators for gerunds and nominalizations of
verbs.
2. The named entity tag (Person, Location or Or-
ganization) associated with a word, if any. We
131
Id. Feature
1. Head noun/verb that dominates the
preposition along with its modifiers
2. Head noun/verb that is dominated by
the preposition along with its modifiers
3. Subject, negator and object(s) of the
immediately dominating verb
4. Heads of sibling prepositions
5. Words withing a window of 5 centered
at the preposition
Table 1: Features for preposition relation from Tratz and
Hovy (2009). These rules were used to identify syntacti-
cally related words for each preposition.
used the state-of-the-art named entity tagger of
Ratinov and Roth (2009) to label the text.
3. Gazetteer features, which are active if a word is
a part of a phrase that belongs to a gazetteer list.
We used the gazetteer lists which were used
by the NER system. We also used the CBC
word clusters of Pantel and Lin (2002) as ad-
ditional gazetteers and Brown cluster features
as used by Ratinov and Roth (2009) and Koo et
al. (2008).
Dahlmeier et al (2009) annotated senses for the
prepositions at, for, in, of, on, to and with in the sec-
tions 2-4 and 23 of the Wall Street Journal portion of
the Penn Treebank2. We trained sense classifiers on
both datasets using the Averaged Perceptron algo-
rithm with the one-vs-all scheme using the Learning
Based Java framework of Rizzolo and Roth (2010)3.
Table 2 reports the performance of our sense disam-
biguation systems for the Treebank prepositions.
As mentioned earlier, we collapsed the sense la-
bels onto the newly defined preposition role labels.
Table 3 shows this label set alng with frequencies
of the labels in the Treebank dataset. According to
this labeling scheme, the first on in our running ex-
ample will be labeled TOPIC and the second one will
2This dataset does not annotate all prepositions and re-
stricts itself mainly to prepositions that start a Propbank ar-
gument. The data is available at http://nlp.comp.nus.
edu.sg/corpora
3Learning Based Java can be downloaded from http://
cogcomp.cs.illinois.edu.
Test set
Train Treebank Sec. 23 SemEval
Penn Treebank 61.41 38.22
SemEval 47.00 78.25
Table 2: Preposition sense performance. This table re-
ports accuracy of sense prediction on the prepositions that
have been annotated for the Penn Treebank dataset.
Role Train Test
ACTIVITY 57 23
ATTRIBUTE 119 51
BENEFICIARY 78 17
CAUSE 255 116
CONCOMITANT 156 74
ENDCONDITION 88 66
EXPERIENCER 88 42
INSTRUMENT 37 19
LOCATION 1141 414
MEDIUMOFCOMMUNICATION 39 30
NUMERIC/LEVEL 301 174
OBJECTOFVERB 365 112
OTHER 65 49
PARTWHOLE 485 133
PARTICIPANT/ACCOMPANIER 122 58
PHYSICALSUPPORT 32 18
POSSESSOR 195 56
PROFESSIONALASPECT 24 10
RECIPIENT 150 70
SPECIES 240 58
TEMPORAL 582 270
TOPIC 148 54
Table 3: Preposition role data statistics for the Penn Tree-
bank preposition dataset.
be labeled TEMPORAL4. We re-trained the sense
disambiguation system to predict preposition roles.
When trained on the Treebank data, our system at-
tains an accuracy of 67.82% on Section 23 of the
Treebank. We use this system as our independent
baseline for preposition role identification.
3.2 Verb SRL
The goal of verb Semantic Role Labeling (SRL)
is to identify the predicate-argument structure de-
fined by verbs in sentences. The CoNLL Shared
Tasks of 2004 and 2005 (See Carreras and Ma`rquez
4The mapping from the preposition senses to the roles de-
fines a new dataset and is available for download at http:
//cogcomp.cs.illinois.edu/.
132
(2004), Carreras and Ma`rquez (2005)) studied the
identification of the predicate-argument structure of
verbs using the PropBank corpus of Palmer et al
(2005). Punyakanok et al (2008) and Toutanova et
al. (2008) used global inference to ensure that the
predictions across all arguments of the same predi-
cate are coherent. We re-implemented the system of
Punyakanok et al (2008), which we briefly describe
here, to serve as our baseline verb semantic role la-
beler 5. We refer the reader to the original paper for
further details.
The verb SRL system of Punyakanok et al (2008)
consists of four stages ? candidate generation, argu-
ment identification, argument classification and in-
ference. The candidate generation stage involves us-
ing the heuristic of Xue and Palmer (2004) to gener-
ate an over-complete set of argument candidates for
each predicate. The identification stage uses a clas-
sifier to prune the candidates. In the argument clas-
sification step, the candidates that remain after the
identification step are assigned scores for the SRL
arguments using a multiclass classifier. One of the
labels of the classifier is ?, which indicates that the
candidate is, in fact, not an argument. The inference
step produces a combined prediction for all argu-
ment candidates of a verb proposition by enforcing
global constraints.
The inference enforces the following structural
and linguistic constraints: (1) Each candidate can
have at most one label. (2) No duplicate core argu-
ments. (3) No overlapping or embedding arguments.
(4) Given the predicate, some argument classes are
illegal. (5) If a candidate is labeled as an R-arg,
then there should be one labeled as arg. (6) If a
candidate is labeled as a C-arg, there should be one
labeled arg that occurs before the C-arg.
Instead of using the identifier to filter candidates
for the classifier, in our SRL system, we added
the identifier to the global inference and enforced
consistency constraints between the identifier and
the argument classifier predictions ? the identifier
should predict that a candidate is an argument if,
and only if, the argument classifier does not predict
the label ?. This change is in keeping with the idea
of using joint inference to combine independently
5The verb SRL system be downloaded from http://
cogcomp.cs.illinois.edu/page/software
learned systems, in this case, the argument identifier
and the role classifier. Furthermore, we do not need
to explicitly tune the identifier for high recall.
We phrase the inference task as an integer lin-
ear program (ILP) following the approach devel-
oped in Roth and Yih (2004). Integer linear pro-
grams were used by Roth and Yih (2005) to add gen-
eral constraints for inference with conditional ran-
dom fields. ILPs have since been used successfully
in many NLP applications involving complex struc-
tures ? Punyakanok et al (2008) for semantic role
labeling, Riedel and Clarke (2006) and Martins et al
(2009) for dependency parsing and several others6.
Let vCi,a be the Boolean indicator variable that de-
notes that the ith argument candidate for a predicate
is assigned a label a and let ?Ci,a represent the score
assigned by the argument classifier for this decision.
Similarly, let vIi denote the identifier decision for the
ith argument candidate of the predicate and ?Ii de-
note its identifier score. Then, the objective of infer-
ence is to maximize the total score of the assignment
max
vC ,vI
?
i,a
?Ci,avCi,a +
?
i
?Ii vIi (1)
Here, vC and vI denote all the argument classifier
and identifier variables respectively. This maximiza-
tion is subject to the constraints described above,
which can be transformed to linear (in)equalities.
We denote these constraints as CSRL. In addition
to CSRL which were defined by Punyakanok et al
(2008), we also have the constraints linking the pre-
dictions of the identifier and classifier:
vCv,i,? + vIv,i = 1; ?v, i. (2)
Inference in our baseline SRL system is, thus, the
maximization of the objective defined in (1) sub-
ject to constraints CSRL, the identifier-classifier con-
straints defined in (2) and the restriction of the vari-
ables to take values in {0, 1}.
To train the classifiers, we used parse trees from
the Charniak and Johnson (2005) parser with the
6The primary advantage of using ILP for inference is that
this representation enables us to add arbitrary coherence con-
straints between the phenomena. If the underlying optimization
problem itself is tractable, then so is the corresponding integer
program. However, other approaches to solve the constrained
maximization problem can also be used for inference.
133
same feature representation as in the original sys-
tem. We trained the classifiers on the standard
Propbank training set using the one-vs-all extension
of the average Perceptron algorithm. As with the
preposition roles, we implemented our system using
Learning Based Java of Rizzolo and Roth (2010).
We normalized all classifier scores using the soft-
max function. Compared to the 76.29% F1 score
reported by Punyakanok et al (2008) using single
parse tree predictions from the parser, our system
obtained 76.22% F1 score on section 23 of the Penn
Treebank.
4 A Joint Model for Verbs and
Prepositions
We now introduce our model that captures the needs
identified in Section 2. The approach we develop
in this paper follows the one proposed by Roth and
Yih (2004) of training individual models and com-
bining them at inference time. Our joint model
is a Constrained Conditional Model (See Chang et
al. (2011)), which allows us to build upon existing
learned models using declarative constraints.
We represent our component inference problems
as integer linear program instances. As we saw in
Section 3.2, the inference for SRL is instantiated as
an ILP problem. The problem of predicting prepo-
sition roles can be easily transformed into an ILP
instance. Let vRp,r denote the decision variable that
encodes the prediction that the preposition p is as-
signed a role r and let ?Rp,r denote its score. Let
vR denote all the role variables for a sentence. Then
role prediction is equivalent to the following maxi-
mization problem:
max
vR
?
p,r
?Rp,r ? vRp,r (3)
subj. to ?
r
vRp,r = 1, ?p (4)
vRp,r ? {0, 1}, ?p, r. (5)
In general, let p denote a linguistic structure pre-
diction task of interest and let P denote all such
tasks. Let Zp denote the set of labels that the parts
of the structure associated with phenomenon p can
take. For example, for the SRL argument classifica-
tion component, the parts of the structure are all the
candidates that need to be labeled for a given sen-
tence and the set Zp is the set of all argument labels.
For each phenomenon p ? P , we use vp to denote
its set of inference variables for a given sentence.
Each inference variable vpZ,y ? vp corresponds to
the prediction that the part y has the label Z in the
final structure. Each variable is associated with a
score ?pZ,y that is obtained from a learned score pre-
dictor. Let Cp denote the structural constraints that
are ?local? to the phenomenon. Thus, for verb SRL,
these would be the constraints defined in the previ-
ous section, and for preposition role, the only local
constraint would be the constraint (4) defined above.
The independent inference problem for the phe-
nomenon p is the following integer program:
max
vp
?
Z?Zp
?
vp
vpZ,y ??
p
Z,y, (6)
subj. to Cp(vp), (7)
vpZ,y ? {0, 1}, ?v
p
Z,y. (8)
As a technical point, this defines one inference
problem per sentence, rather than per predicate
as in the verb SRL system of Punyakanok et al
(2008). This simple extension enabled Surdeanu et
al. (2007) to study the impact of incorporating cross-
predicate constraints for verb SRL. In this work, this
extension allows us to incorporate cross-phenomena
inference.
4.1 Joint inference
We consider the problem of jointly predicting sev-
eral phenomena incorporating linguistic knowledge
that enforce consistency between the output labels.
Suppose p1 and p2 are two phenomena. If zp11 is a la-
bel associated with the former and zp21 , zp22 , ? ? ? are
labels associated with the latter, we consider con-
straints of the form
zp11 ? zp21 ? zp22 ? ? ? ? ? zp2n (9)
We expand this language of constraints by allowing
the specification of pre-conditions for a constraint to
apply. This allows us to enforce constraints of the
form ?If an argument that starts with the preposi-
tion ?at? is labeled AM-TMP, then the preposition
can be labeled either NUMERIC/LEVEL or TEMPO-
RAL.? This constraint is universally quantified for
134
all arguments that satisfy the precondition of start-
ing with the preposition at.
Given a first-order constraint in this form and an
input sentence, suppose the inference variable vp11 is
a grounding of zp11 and vp21 , vp22 , ? ? ? are groundings
of the right hand labels such that the preconditions
are satisfied, then the constraint can be phrased as
the following linear inequality.
?vp11 +
?
i
vp2i ? 0
In the context of the preposition role and verb
SRL, we consider constraints between labels for a
preposition and SRL argument candidates that begin
with that preposition. This restriction forms the pre-
condition for all the joint constraints considered in
this paper. Since the joint constraints involve only
the labels, they can be derived either manually from
the definition of the tasks or using statistical rela-
tion learning techniques. In addition to mining con-
straints of the form (9), we also use manually spec-
ified joint constraints. The constraints used in our
experiments are described further in Section 5.
In general, let J denote a set of pairwise joint
constraints. The joint inference problem can be
phrased as that of maximizing the score of the as-
signment subject to the structural constraints of each
phenomenon (Cp) and the joint linguistic constraints
(J). However, since, the individual tasks were not
trained on the same datasets, the scoring functions
need not be in the same numeric scale. In our model,
each labelZ for a phenomenon p is associated with a
scoring function ?pZ,y for a part y. To scale the scor-
ing functions, we associate each label with a param-
eter ?pZ . This gives us the following integer linear
program for joint inference:
max
v
?
p?P
?
Z?Zp
?pZ
(
?
yp
vpZ,y ??
p
Z,y
)
, (10)
subj. to Cp(vp), ?p ? P (11)
J(v), (12)
vpZ,y ? {0, 1}, ?v
p
Z,y. (13)
Here, v is the vector of inference variables which
is obtained by stacking all the inference variables of
each phenomena.
For our experiments, we use a cutting plane solver
to solve the integer linear program as in Riedel
(2009). This allows us to solve the inference prob-
lem without explicitly having to instantiate all the
joint constraints.
4.2 Learning to rescale the individual systems
Given the individual models and the constraints, we
only need to learn the scaling parameters ?pZ . Note
that the number of scaling parameters is the total
number of labels. When we jointly predict verb SRL
and preposition role, we have 22 preposition roles
(from table 3), one SRL identifier label and 54 SRL
argument classifier labels. Thus we learn only 77
parameters for our joint model. This means that we
only need a very small dataset that is jointly anno-
tated with all the phenomena.
We use the Structure Perceptron of Collins (2002)
to learn the scaling weights. Note that for learning
the scaling weights, we need each label to be associ-
ated with a real-valued feature. Given an assignment
of the inference variables v, the value of the feature
corresponding to the label Z of task p is given by the
sum of scores of all parts in the structure for p that
have been assigned this label, i.e. ?
yp
vpZ,y??
p
Z,y. This
feature is computed for the gold and the predicted
structures and is used for updating the weights.
5 Experiments
In this section, we describe our experimental setup
and evaluate the performance of our approach. The
research question addressed by the experiments is
the following: Given independently trained systems
for verb SRL and preposition roles, can their per-
formance be improved using joint inference between
the two tasks? To address this, we report the results
of the following two experiments:
1. First, we compare the joint system against the
baseline systems and with pipelines in both di-
rections. In this setting, both base systems are
trained on the Penn Treebank data.
2. Second, we show that using joint inference can
provide strong a performance gain even when
the underlying systems are trained on different
domains.
In all experiments, we report the F1 measure for
the verb SRL performance using the CoNLL 2005
135
evaluation metric and the accuracy for the preposi-
tion role labeling task.
5.1 Data and Constraints
For both the verb SRL and preposition roles, we
used the first 500 sentences of section 2 of the Penn
Treebank corpus to train our scaling parameters. For
the first set of experiments, we trained our underly-
ing systems on the rest of the available Penn Tree-
bank training data for each task. For the adaptation
experiment, we train the role classifier on the Se-
mEval data (restricted to the same Treebank prepo-
sitions). In both cases, we report performance on
section 23 of the Treebank.
We mined consistency constraints from the sec-
tions 2, 3 and 4 of the Treebank data. As mentioned
in Section 4.1, we considered joint constraints re-
lating preposition roles to verb argument candidates
that start with the preposition. We identified the fol-
lowing types of constraints: (1) For each preposi-
tion, the set of invalid verb arguments and prepo-
sition roles. (2) For each preposition role, the set
of allowed verb argument labels if the role occurred
more than ten times in the data, and (3) For each
verb argument, the set of allowed preposition roles,
similarly with a support of ten. Note that, while the
constraints were obtained from jointly labeled data,
the constraints could be written down because they
encode linguistic intuition about the labels.
The following is a constraint extracted from the
data, which applies to the preposition with:
srlarg(A2) ? prep-role(ATTRIBUTE)
? prep-role(CAUSE)
? prep-role(INSTRUMENT)
? prep-role(OBJECTOFVERB)
? prep-role(PARTWHOLE)
? prep-role(PARTICIPANT/ACCOMPAINER)
? prep-role(PROFESSIONALASPECT).
This constraint says that if any candidate that starts
with with is labeled as an A2, then the preposition
can be labeled only with one of the roles on the right
hand side.
Some of the mined constraints have negated vari-
ables to enforce that a role or an argument label
should not be allowed. These can be similarly con-
verted to linear inequalities. See Rizzolo and Roth
(2010) for a further discussion about converting log-
ical expressions into linear constraints.
In addition to these constraints that were mined
from data, we also enforce the following hand-
written constraints: (1) If the role of a verb at-
tached preposition is labeled TEMPORAL, then there
should be a verb predicate for which this preposi-
tional phrase is labeled AM-TMP. (2) For verb at-
tached prepositions, if the preposition is labeled with
one of ACTIVITY, ENDCONDITION, INSTRUMENT
or PROFESSIONALASPECT, there should be at least
one predicate for which the corresponding preposi-
tional phrase is not labeled ?.
The conversion of the first constraint to a linear
inequality is similar to the earlier cases. For each
of the roles in the second constraint, let r denote a
role variable that assigns the label to some prepo-
sition. Suppose there are n SRL candidates across
all verb predicates begin with that preposition, and
let s1, s2, ? ? ? , sn denote the SRL variables that as-
sign these candidates to the label ?. Then the second
constraint corresponds to the following inequality:
r +
n?
i=1
si ? n
5.2 Results of joint learning
First, we compare our approach to the performance
of the baseline independent systems and to pipelines
in both directions in Table 4. For one pipeline, we
added the prediction of the baseline preposition role
system as an additional feature to both the identifier
and the argument classifier for argument candidates
that start with a preposition. Similarly, for the sec-
ond pipeline, we added the SRL predictions as fea-
tures for prepositions that were the first word of an
SRL argument. In all cases, we performed five-fold
cross validation to train the classifiers.
The results show that both pipelines improve per-
formance. This justifies the need for a joint sys-
tem because the pipeline can improve only one of
the tasks. The last line of the table shows that the
joint inference system improves upon both the base-
lines. We achieve this improvement without retrain-
ing the underlying models, as done in the case of the
pipelines.
On analyzing the output of the systems, we found
that the SRL precision improved by 2.75% but the
136
Setting SRL Preposition Role
(F1) (Accuracy)
Baseline SRL 76.22 ?
Baseline Prep. ? 67.82
Prep. ? SRL 76.84 ?
SRL? Prep. ? 68.55
Joint inference 77.07 68.39
Table 4: Performance of the joint system, compared to
the individual systems and the pipelines. All performance
measures are reported on Section 23 of the Penn Tree-
bank. The verb SRL systems were trained on sections
2-21, while the preposition role classifiers were trained
on sections 2-4. For the joint inference system, the scal-
ing parameters were trained on the first 500 sentences of
section 2, which were held out. All the improvements in
this table are statistically significant at the 0.05 level.
recall decreased by 0.98%, contributing to the over-
all F1 improvement. The decrease in recall is due to
the joint hard constraints that prohibit certain assign-
ments to the variables which would have otherwise
been possible. Note that, for a given sentence, even
if the joint constraints affect only a few argument
candidates directly, they can alter the labels of the
other candidates via the ?local? SRL constraints.
Consider the following example of the system
output which highlights the effect of the constraints.
(6) Weatherford said market conditions led to the
cancellation of the planned exchange.
The independent preposition role system incor-
rectly identifies the to as a LOCATION. The semantic
role labeling component identifies the phrase to the
cancellation of the planned exchange as the A2 of
the verb led. One of the constraints mined from the
data prohibits the label LOCATION for the preposi-
tion to if the argument it starts is labeled A2. This
forces the system to change the preposition label
to the correct one, namely ENDCONDITION. Both
the independent and the joint systems also label the
preposition of as OBJECTOFVERB, which indicates
that the phrase the planned exchange is the object of
the deverbal noun cancellation.
5.3 Effect of constraints on adaptation
Our second experiment compares the performance
of the preposition role classifier that has been trained
on the SemEval dataset with and without joint con-
straints. Note that Table 2 in Section 3, shows
the drop in performance when applying the prepo-
sition sense classifier. We see that the SemEval-
trained preposition role classifier (baseline in the ta-
ble) achieves an accuracy of 53.29% when tested on
the Treebank dataset. Using this classifier jointly
with the verb SRL classifier via joint constraints gets
an improvement of almost 3 percent in accuracy.
Setting Preposition Role
(Accuracy)
Baseline 53.29
Joint inference 56.22
Table 5: Performance of the SemEval-trained preposition
role classifier, when tested on the Treebank dataset with
and without joint inference with the verb SRL system.
The improvement, in this case is statistically significant
at the 0.01 level using the sign test.
The primary reason for this improvement, even
without re-training the classifier, is that the con-
straints are defined using only the labels of the sys-
tems. This avoids the standard adaptation problems
of differing vocabularies and unseen features.
6 Discussion and Related work
Roth and Yih (2004) formulated the problem of ex-
tracting entities and relations as an integer linear
program, allowing them to use global structural con-
straints at inference time even though the component
classifiers were trained independently. In this pa-
per, we use this idea to combine classifiers that were
trained for two different tasks on different datasets
using constraints to encode linguistic knowledge.
In the recent years, we have seen several joint
models that combine two or more NLP tasks . An-
drew et al (2004) studied verb subcategorization
and sense disambiguation of verbs by treating it as
a problem of learning with partially labeled struc-
tures and proposed to use EM to train the joint
model. Finkel and Manning (2009) modeled the task
of named entity recognition together with parsing.
Meza-Ruiz and Riedel (2009) modeled verb SRL,
predicate identification and predicate sense recogni-
tion jointly using Markov Logic. Henderson et al
(2008) was designed for jointly learning to predict
syntactic and semantic dependencies. Dahlmeier et
137
al. (2009) addressed the problem of jointly learning
verb SRL and preposition sense using the Penn Tree-
bank annotation that was introduced in that work.
The key difference between these and the model
presented in this paper lies in the simplicity of our
model and its easy extensibility because it leverages
existing trained systems. Moreover, our model has
the advantage that the complexity of the joint param-
eters is small, hence does not require a large jointly
labeled dataset to train the scaling parameters.
Our approach is conceptually similar to that of
Rush et al (2010), which combined separately
trained models by enforcing agreement using global
inference and solving its linear programming relax-
ation. They applied this idea to jointly predict de-
pendency and phrase structure parse trees and on the
task of predicting full parses together with part-of-
speech tags. The main difference in our approach is
that we treat the scaling problem as a separate learn-
ing problem in itself and train a joint model specifi-
cally for re-scaling the output of the trained systems.
The SRL combination system of Surdeanu et al
(2007) studied the combination of three different
SRL systems using constraints and also by training
secondary scoring functions over the individual sys-
tems. Their approach is similar to the one presented
in this paper in that, unlike standard reranking, as
in Collins (2000), we entertain all possible solutions
during inference, while reranking approaches train
a discriminative scorer for the top-K solutions of
an underlying system. Unlike the SRL combination
system, however, our approach spans multiple phe-
nomena. Moreover, in contrast to their re-scoring
approaches, we do not define joint features drawn
from the predictions of the underlying components
to define our global model.
We consider the tasks verb SRL and preposition
roles and combine their predictions to provide a
richer semantic annotation of text. This approach
can be easily extended to include systems that pre-
dict structures for other linguistic phenomena be-
cause we do not retrain the underlying systems. The
semantic relations can be enriched by incorporating
more linguistic phenomena such as nominal SRL,
defined by the Nombank annotation scheme of Mey-
ers et al (2004), the preposition function analysis
of O?Hara and Wiebe (2009) and noun compound
analysis as defined by Girju (2007) and Girju et al
(2009) and others. This presents an exciting direc-
tion for future work.
7 Conclusion
This paper presents a strategy for extending seman-
tic role labeling without the need for extensive re-
training or data annotation. While standard seman-
tic role labeling focuses on verb and nominal re-
lations, sentences can express relations using other
lexical items also. Moreover, the different relations
interact with each other and constrain the possible
structures that they can take. We use this intuition
to define a joint model for inference. We instanti-
ate our model using verb semantic role labeling and
preposition role labeling and show that, using lin-
guistic constraints between the tasks and minimal
joint learning, we can improve the performance of
both tasks. The main advantage of our approach
is that we can use existing trained models without
re-training them, thus making it easy to extend this
work to include other linguistic phenomena.
Acknowledgments
The authors thank the members of the Cognitive
Computation Group at the University of Illinois for
insightful discussions and the anonymous reviewers
for valuable feedback.
This research is supported by the Defense Ad-
vanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-
09-C-0181. Any opinions, ndings, and conclusion
or recommendations expressed in this material are
those of the authors and do not necessarily reect the
view of the DARPA, AFRL, or the US government.
References
G. Andrew, T. Grenager, and C. D. Manning. 2004.
Verb sense and subcategorization: Using joint infer-
ence to improve performance on complementary tasks.
In Proceedings of EMNLP.
X. Carreras and L. Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared tasks: Semantic role labeling. In
Proceedings of CoNLL-2004.
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL-2005.
138
M. Chang, L. Ratinov, and D. Roth. 2011. Structured
learning with constrained conditional models. Ma-
chine Learning (To appear).
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
ACL.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In ICML.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In EMNLP.
D. Dahlmeier, H. T. Ng, and T. Schultz. 2009. Joint
learning of preposition senses and semantic roles of
prepositional phrases. In EMNLP.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2009. Classification of semantic
relations between nominals. Language Resources and
Evaluation.
R. Girju. 2007. Improving the interpretation of noun
phrases with cross-linguistic information. In ACL.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In NAACL.
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008. A
latent variable model of synchronous parsing for syn-
tactic and semantic dependencies. In CoNLL.
D. Hovy, S. Tratz, and E. Hovy. 2010. What?s in a prepo-
sition? dimensions of sense disambiguation for an in-
teresting word class. In Coling 2010: Posters.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
K. Litkowski and O. Hargraves. 2005. The preposition
project. In Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of Preposi-
tions and their Use in Computational Linguistics For-
malisms and Applications.
K. Litkowski and O. Hargraves. 2007. Semeval-2007
task 06: Word-sense disambiguation of prepositions.
In SemEval-2007: 4th International Workshop on Se-
mantic Evaluations.
A. Martins, N. A. Smith, and E. Xing. 2009. Concise
integer linear programming formulations for depen-
dency parsing. In ACL.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: An interim report. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation.
I Meza-Ruiz and S. Riedel. 2009. Jointly identifying
predicates, arguments and senses using markov logic.
In NAACL.
T. O?Hara and J. Wiebe. 2009. Exploiting semantic role
resources for preposition disambiguation. Computa-
tional Linguistics, 35(2), June.
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In The Eighth ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In IJ-
CAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
L. Ratinov and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP.
S. Riedel. 2009. Cutting plane map inference for markov
logic. In SRL 2009.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In Language Re-
sources and Evaluation.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
CoNLL.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In ICML.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP. Association for Computational Linguistics.
M. Surdeanu, L. Ma`rquez, X. Carreras, and P. R. Comas.
2007. Combination strategies for semantic role label-
ing. J. Artif. Int. Res., 29:105?151, June.
K. Toutanova, A. Haghighi, and C. D. Manning. 2008. A
global joint model for semantic role labeling. Compu-
tational Linguistics, 34(2).
S. Tratz and D. Hovy. 2009. Disambiguation of prepo-
sition sense using linguistically motivated features.
In NAACL: Student Research Workshop and Doctoral
Consortium.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In EMNLP.
P. Ye and T. Baldwin. 2007. MELB-YB: Preposition
Sense Disambiguation Using Rich Semantic Features.
In SemEval-2007.
139
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 294?303,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Minimally Supervised Event Causality Identification
Quang Xuan Do Yee Seng Chan Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{quangdo2,chanys,danr}@illinois.edu
Abstract
This paper develops a minimally supervised
approach, based on focused distributional sim-
ilarity methods and discourse connectives,
for identifying of causality relations between
events in context. While it has been shown
that distributional similarity can help identify-
ing causality, we observe that discourse con-
nectives and the particular discourse relation
they evoke in context provide additional in-
formation towards determining causality be-
tween events. We show that combining dis-
course relation predictions and distributional
similarity methods in a global inference pro-
cedure provides additional improvements to-
wards determining event causality.
1 Introduction
An important part of text understanding arises from
understanding the semantics of events described in
the narrative, such as identifying the events that are
mentioned and how they are related semantically.
For instance, when given a sentence ?The police
arrested him because he killed someone.?, humans
understand that there are two events, triggered by
the words ?arrested? and ?killed?, and that there is
a causality relationship between these two events.
Besides being an important component of discourse
understanding, automatically identifying causal re-
lations between events is important for various nat-
ural language processing (NLP) applications such
as question answering, etc. In this work, we auto-
matically detect and extract causal relations between
events in text.
Despite its importance, prior work on event
causality extraction in context in the NLP litera-
ture is relatively sparse. In (Girju, 2003), the au-
thor used noun-verb-noun lexico-syntactic patterns
to learn that ?mosquitoes cause malaria?, where the
cause and effect mentions are nominals and not nec-
essarily event evoking words. In (Sun et al, 2007),
the authors focused on detecting causality between
search query pairs in temporal query logs. (Beamer
and Girju, 2009) tried to detect causal relations be-
tween verbs in a corpus of screen plays, but limited
themselves to consecutive, or adjacent verb pairs.
In (Riaz and Girju, 2010), the authors first cluster
sentences into topic-specific scenarios, and then fo-
cus on building a dataset of causal text spans, where
each span is headed by a verb. Thus, their focus was
not on identifying causal relations between events in
a given text document.
In this paper, given a text document, we first iden-
tify events and their associated arguments. We then
identify causality or relatedness relations between
event pairs. To do this, we develop a minimally su-
pervised approach using focused distributional sim-
ilarity methods, such as co-occurrence counts of
events collected automatically from an unannotated
corpus, to measure and predict existence of causal-
ity relations between event pairs. Then, we build on
the observation that discourse connectives and the
particular discourse relation they evoke in context
provide additional information towards determining
causality between events. For instance, in the ex-
ample sentence provided at the beginning of this
section, the words ?arrested? and ?killed? probably
have a relatively high apriori likelihood of being ca-
294
sually related. However, knowing that the connec-
tive ?because? evokes a contingency discourse re-
lation between the text spans ?The police arrested
him? and ?he killed someone? provides further ev-
idence towards predicting causality. The contribu-
tions of this paper are summarized below:
? Our focus is on identifying causality between
event pairs in context. Since events are of-
ten triggered by either verbs (e.g. ?attack?) or
nouns (e.g. ?explosion?), we allow for detec-
tion of causality between verb-verb, verb-noun,
and noun-noun triggered event pairs. To the
best of our knowledge, this formulation of the
task is novel.
? We developed a minimally supervised ap-
proach for the task using focused distributional
similarity methods that are automatically col-
lected from an unannotated corpus. We show
that our approach achieves better performance
than two approaches: one based on a frequently
used metric that measures association, and an-
other based on the effect-control-dependency
(ECD) metric described in a prior work (Riaz
and Girju, 2010).
? We leverage on the interactions between event
causality prediction and discourse relations
prediction. We combine these knowledge
sources through a global inference procedure,
which we formalize via an Integer Linear Pro-
gramming (ILP) framework as a constraint op-
timization problem (Roth and Yih, 2004). This
allows us to easily define appropriate con-
straints to ensure that the causality and dis-
course predictions are coherent with each other,
thereby improving the performance of causality
identification.
2 Event Causality
In this work, we define an event as an action or oc-
currence that happens with associated participants
or arguments. Formally, we define an event e
as: p(a1, a2, . . . , an), where the predicate p is the
word that triggers the presence of e in text, and
a1, a2, . . . , an are the arguments associated with
e. Examples of predicates could be verbs such as
?attacked?, ?employs?, nouns such as ?explosion?,
?protest?, etc., and examples of the arguments of
?attacked? could be its subject and object nouns.
To measure the causality association between a
pair of events ei and ej (in general, ei and ej
could be extracted from the same or different doc-
uments), we should use information gathered about
their predicates and arguments. A simple approach
would be to directly calculate the pointwise mu-
tual information (PMI)1 between pi(ai1, ai2, . . . , ain)
and pj(aj1, aj2, . . . , ajm). However, this leads to very
sparse counts as the predicate pi with its list of ar-
guments ai1, . . . , ain would rarely co-occur (within
some reasonable context distance) with predicate pj
and its entire list of arguments aj1, . . . , ajm. Hence,
in this work, we measure causality association us-
ing three separate components and focused distribu-
tional similarity methods collected about event pairs
as described in the rest of this section.
2.1 Cause-Effect Association
We measure the causality or cause-effect association
(CEA) between two events ei and ej using the fol-
lowing equation:
CEA(ei, ej) =
spp(ei, ej) + spa(ei, ej) + saa(ei, ej) (1)
where spp measures the association between event
predicates, spa measures the association between the
predicate of an event and the arguments of the other
event, and saa measures the association between
event arguments. In our work, we regard each event
e as being triggered and rooted at a predicate p.
2.1.1 Predicate-Predicate Association
We define spp as follows:
spp(ei, ej) = PMI(pi, pj)?max(ui, uj)
?IDF (pi, pj)?Dist(pi, pj) (2)
which takes into account the PMI between pred-
icates pi and pj of events ei and ej respectively,
as well as various other pieces of information. In
Suppes? Probabilistic theory of Casuality (Suppes,
1970), he highlighted that event e is a possible cause
of event e?, if e? happens more frequently with e than
1PMI is frequently used to measure association between
variables.
295
by itself, i.e. P (e?|e) > P (e?). This can be easily
rewritten as P (e,e?)P (e)P (e?) > 1, similar to the definitionof PMI:
PMI(e, e?) = log P (e, e
?)
P (e)P (e?)
which is only positive when P (e,e?)P (e)P (e?) > 1.
Next, we build on the intuition that event predi-
cates appearing in a large number of documents are
probably not important or discriminative. Thus, we
penalize these predicates when calculating spp by
adopting the inverse document frequency (idf):
IDF (pi, pj) = idf(pi)? idf(pj)? idf(pi, pj),
where idf(p) = log D1+N , D is the total number ofdocuments in the collection and N is the number of
documents that p occurs in.
We also award event pairs that are closer together,
while penalizing event pairs that are further apart in
texts, by incorporating the distance measure of Lea-
cock and Chodorow (1998), which was originally
used to measure similarity between concepts:
Dist(pi, pj) = ?log |sent(p
i)? sent(pj)|+ 1
2? ws ,
where sent(p) gives the sentence number (index) in
which p occurs and ws indicates the window-size
(of sentences) used. If pi and pj are drawn from the
same sentence, the numerator of the above fraction
will return 1. In our work, we set ws to 3 and thus,
if pi occurs in sentence k, the furthest sentence that
pj will be drawn from, is sentence k + 2.
The final component of Equation 2, max(ui, uj),
takes into account whether predicates (events) pi and
pj appear most frequently with each other. ui and uj
are defined as follows:
ui = P (p
i, pj)
maxk[P (pi, pk)]? P (pi, pj) + 
uj = P (p
i, pj)
maxk[P (pk, pj)]? P (pi, pj) +  ,
where we set  = 0.01 to avoid zeros in the denom-
inators. ui will be maximized if there is no other
predicate pk having a higher co-occurrence proba-
bility with pi, i.e. pk = pj . uj is treated similarly.
2.1.2 Predicate-Argument and
Argument-Argument Association
We define spa as follows:
spa(ei, ej) =
1
|Aej |
?
a?Aej
PMI(pi, a)
+ 1|Aei |
?
a?Aei
PMI(pj , a), (3)
where Aei and Aej are the sets of arguments of ei
and ej respectively.
Finally, we define saa as follows:
saa(ei, ej) =
1
|Aei ||Aej |
?
a?Aei
?
a??Aej
PMI(a, a?) (4)
Together, spa and saa provide additional contexts
and robustness (in addition to spp) for measuring the
cause-effect association between events ei and ej .
Our formulation of CEA is inspired by the ECD
metric defined in (Riaz and Girju, 2010):
ECD(a, b) = max(v, w)??log dis(a, b)2?maxDistance , (5)
where
v = P (a, b)P (b)? P (a, b) +  ?
P (a, b)
maxt[P (a, bt)]? P (a, b) + 
w= P (a, b)P (a)? P (a, b) +  ?
P (a, b)
maxt[P (at, b)]? P (a, b) +  ,
where ECD(a,b) measures the causality between two
events a and b (headed by verbs), and the sec-
ond component in the ECD equation is similar to
Dist(pi, pj). In our experiments, we will evaluate
the performance of ECD against our proposed ap-
proach.
So far, our definitions in this section are generic
and allow for any list of event argument types. In
this work, we focus on two argument types: agent
(subject) and patient (object), which are typical core
arguments of any event. We describe how we extract
event predicates and their associated arguments in
the section below.
3 Verbal and Nominal Predicates
We consider that events are not only triggered by
verbs but also by nouns. For a verb (verbal predi-
cate), we extract its subject and object from its as-
sociated dependency parse. On the other hand, since
296
events are also frequently triggered by nominal pred-
icates, it is important to identify an appropriate list
of event triggering nouns. In our work, we gathered
such a list using the following approach:
? We first gather a list of deverbal nouns from the
set of most frequently occurring (in the Giga-
word corpus) 3,000 verbal predicate types. For
each verb type v, we go through all its Word-
Net2 senses and gather all its derivationally re-
lated nouns Nv 3.
? From Nv, we heuristically remove nouns that
are less than three characters in length. We also
remove nouns whose first three characters are
different from the first three characters of v. For
each of the remaining nouns in Nv, we mea-
sured its Levenstein (edit) distance from v and
keep the noun(s) with the minimum distance.
When multiple nouns have the same minimum
distance from v, we keep all of them.
? To further prune the list of nouns, we next re-
moved all nouns ending in ?er?, ?or?, or ?ee?,
as these nouns typically refer to a person, e.g.
?writer?, ?doctor?, ?employee?. We also re-
move nouns that are not hyponyms (children)
of the first WordNet sense of the noun ?event?4.
? Since we are concerned with nouns denoting
events, FrameNet (Ruppenhofer et al, 2010)
(FN) is a good resource for mining such nouns.
FN consists of frames denoting situations and
events. As part of the FN resource, each FN
frame consists of a list of lexical units (mainly
verbs and nouns) representing the semantics of
the frame. Various frame-to-frame relations are
also defined (in particular the inheritance re-
lation). Hence, we gathered all the children
frames of the FN frame ?Event?. From these
children frames, we then gathered all their noun
lexical units (words) and add them to our list of
2http://wordnet.princeton.edu/
3The WordNet resource provides derivational information
on words that are in different syntactic (i.e. part-of-speech) cat-
egories, but having the same root (lemma) form and that are
semantically related.
4The first WordNet sense of the noun ?event? has the mean-
ing: ?something that happens at a given place and time?
nouns. Finally, we also add a few nouns denot-
ing natural disaster from Wikipedia5.
Using the above approach, we gathered a list of
about 2,000 noun types. This current approach is
heuristics based which we intend to improve in the
future, and any such improvements should subse-
quently improve the performance of our causality
identification approach.
Event triggering deverbal nouns could have as-
sociated arguments (for instance, acting as subject,
object of the deverbal noun). To extract these ar-
guments, we followed the approach of (Gurevich
et al, 2008). Briefly, the approach uses linguistic
patterns to extract subjects and objects for deverbal
nouns, using information from dependency parses.
For more details, we refer the reader to (Gurevich et
al., 2008).
4 Discourse and Causality
Discourse connectives are important for relating dif-
ferent text spans, helping us to understand a piece of
text in relation to its context:
[The police arrested him] because [he killed someone].
In the example sentence above, the discourse con-
nective (?because?) and the discourse relation it
evokes (in this case, the Cause relation) allows read-
ers to relate its two associated text spans, ?The po-
lice arrested him? and ?he killed someone?. Also,
notice that the verbs ?arrested? and ?killed?, which
cross the two text spans, are causally related. To
aid in extracting causal relations, we leverage on the
identification of discourse relations to provide addi-
tional contextual information.
To identify discourse relations, we use the Penn
Discourse Treebank (PDTB) (Prasad et al, 2007),
which contains annotations of discourse relations
in context. The annotations are done over the
Wall Street Journal corpus and the PDTB adopts a
predicate-argument view of discourse relations. A
discourse connective (e.g. because) takes two text
spans as its arguments. In the rest of this section,
we briefly describe the discourse relations in PDTB
and highlight how we might leverage them to aid in
determining event causality.
5http://en.wikipedia.org/wiki/Natural disaster
297
Coarse-grained relations Fine-grained relations
Comparison Concession, Contrast, Pragmatic-concession, Pragmatic-contrast
Contingency Cause, Condition, Pragmatic-cause, Pragmatic-condition
Expansion Alternative, Conjunction, Exception, Instantiation, List, Restatement
Temporal Asynchronous, Synchronous
Table 1: Coarse-grained and fine-grained discourse relations.
4.1 Discourse Relations
PDTB contains annotations for four coarse-grained
discourse relation types, as shown in the left column
of Table 1. Each of these are further refined into
several fine-grained discourse relations, as shown in
the right column of the table.6 Next, we briefly de-
scribe these relations, highlighting those that could
potentially help to determine event causality.
Comparison A Comparison discourse relation
between two text spans highlights prominent differ-
ences between the situations described in the text
spans. An example sentence is:
Contrast: [According to the survey, x% of Chinese Inter-
net users prefer Google] whereas [y% prefer Baidu].
According to the PDTB annotation manual
(Prasad et al, 2007), the truth of both spans is in-
dependent of the established discourse relation. This
means that the text spans are not causally related and
thus, the existence of a Comparison relation should
imply that there is no causality relation across the
two text spans.
Contingency A Contingency relation between
two text spans indicates that the situation described
in one text span causally influences the situation in
the other. An example sentence is:
Cause: [The first priority is search and rescue] because
[many people are trapped under the rubble].
Existence of a Contingency relation potentially
implies that there exists at least one causal event
pair crossing the two text spans. The PDTB an-
notation manual states that while the Cause and
Condition discourse relations indicate casual influ-
ence in their text spans, there is no causal in-
fluence in the text spans of the Pragmatic-cause
and Pragmatic-condition relations. For instance,
Pragmatic-condition indicates that one span pro-
6PDTB further refines these fine-grained relations into a fi-
nal third level of relations, but we do not use them in this work.
vides the context in which the description of the sit-
uation in the other span is relevant; for example:
Pragmatic-condition: If [you are thirsty], [there?s beer in
the fridge].
Hence, there is a need to also identify fine-grained
discourse relations.
Expansion Connectives evoking Expansion dis-
course relations expand the discourse, such as by
providing additional information, illustrating alter-
native situations, etc. An example sentence is:
Conjunction: [Over the past decade, x women were
killed] and [y went missing].
Most of the Expansion fine-grained relations (ex-
cept for Conjunction, which could connect arbitrary
pieces of text spans) should not contain causality re-
lations across its text spans.
Temporal These indicate that the situations de-
scribed in the text spans are related temporally. An
example sentence is:
Synchrony: [He was sitting at his home] when [the whole
world started to shake].
Temporal precedence of the (cause) event over the
(effect) event is a necessary, but not sufficient req-
uisite for causality. Hence by itself, Temporal re-
lations are probably not discriminative enough for
determining event causality.
4.2 Discourse Relation Extraction System
Our work follows the approach and features de-
scribed in the state-of-the-art Ruby-based discourse
system of (Lin et al, 2010), to build an in-
house Java-based discourse relation extraction sys-
tem. Our system identifies explicit connectives in
text, predict their discourse relations, as well as their
associated text spans. Similar to (Lin et al, 2010),
we achieved a competitive performance of slightly
over 80% F1-score in identifying fine-grained rela-
tions for explicit connectives. Our system is devel-
oped using the Learning Based Java modeling lan-
298
guage (LBJ) (Rizzolo and Roth, 2010) and will be
made available soon. Due to space constraints, we
refer interested readers to (Lin et al, 2010) for de-
tails on the features, etc.
In the example sentences given thus far in this sec-
tion, all the connectives were explicit, as they appear
in the texts. PDTB also provides annotations for im-
plicit connectives, which we do not use in this work.
Identifying implicit connectives is a harder task and
incorporating these is a possible future work.
5 Joint Inference for Causality Extraction
To exploit the interactions between event pair
causality extraction and discourse relation identifi-
cation, we define appropriate constraints between
them, which can be enforced through the Con-
strained Conditional Models framework (aka ILP for
NLP) (Roth and Yih, 2007; Chang et al, 2008). In
doing this, the predictions of CEA (Section 2.1) and
the discourse system are forced to cohere with each
other. More importantly, this should improve the
performance of using only CEA to extract causal
event pairs. To the best of our knowledge, this ap-
proach for causality extraction is novel.
5.1 CEA & Discourse: Implementation Details
Let E denote the set of event mentions in a docu-
ment. Let EP = {(ei, ej) ? E ? E | ei ? E , ej ?
E , i < j, |sent(ei) ? sent(ej)| ? 2} denote the
set of event mention pairs in the document, where
sent(e) gives the sentence number in which event e
occurs. Note that in this work, we only extract event
pairs that are at most two sentences apart. Next, we
define LER = {?causal?, ?? causal?} to be the set of
event relation labels that an event pair ep ? EP can
be associated with.
Note that the CEA metric as defined in Section 2.1
simply gives a score without it being bounded to be
between 0 and 1.0. However, to use the CEA score
as part of the inference process, we require that it be
bounded and thus can be used as a binary prediction,
that is, predicting an event pair as causal or ?causal.
To enable this, we use a few development documents
to automatically find a threshold CEA score that sep-
arates scores indicating causal vs ?causal. Based
on this threshold, the original CEA scores are then
rescaled to fall within 0 to 1.0. More details on this
are in Section 6.2.
Let C denote the set of connective mentions in a
document. We slightly modify our discourse sys-
tem as follows. We define LDR to be the set of
discourse relations. We initially add all the fine-
grained discourse relations listed in Table 1 to LDR.
In the PDTB corpus, some connective examples are
labeled with just a coarse-grained relation, with-
out further specifying a fine-grained relation. To
accommodate these examples, we add the coarse-
grained relations Comparison, Expansion, and Tem-
poral to LDR. We omit the coarse-grained Con-
tingency relation from LDR, as we want to sepa-
rate Cause and Condition from Pragmatic-cause and
Pragmatic-condition. This discards very few exam-
ples as only a very small number of connective ex-
amples are simply labeled with a Contingency label
without further specifying a fine-grained label. We
then retrained our discourse system to predict labels
in LDR.
5.2 Constraints
We now describe the constraints used to support
joint inference, based on the predictions of the CEA
metric and the discourse classifier. Let sc(dr) be
the probability that connective c is predicated to be
of discourse relation dr, based on the output of our
discourse classifier. Let sep(er) be the CEA pre-
diction score (rescaled to range in [0,1]) that event
pair ep takes on the causal or ?causal label er. Let
x?c,dr? be a binary indicator variable which takes on
the value 1 iff c is labeled with the discourse relation
dr. Similarly, let y?ep,er? be a binary variable which
takes on the value 1 iff ep is labeled as er. We then
define our objective function as follows:
max
[
|LDR|
?
c?C
?
dr?LDR
sc(dr) ? x?c,dr?
+|LER|
?
ep?EP
?
er?LER
sep(er) ? y?ep,er?
]
(6)
subject to the following constraints:
?
dr?LDR
x?c,dr? = 1 ?c ? C (7)
?
er?LER
y?ep,er? = 1 ?ep ? EP (8)
x?c,dr? ? {0, 1} ?c ? C, dr ? LDR (9)
y?ep,er? ? {0, 1} ?ep ? EP, er ? LER(10)
299
Equation (7) requires that each connective c can
only be assigned one discourse relation. Equation
(8) requires that each event pair ep can only be
causal or ?causal. Equations (9) and (10) indicate
that x?c,dr? and y?ep,er? are binary variables.
To capture the relationship between event pair
causality and discourse relations, we use the follow-
ing constraints:
x?c,?Cause?? ?
?
ep?EPc
y?ep,?causal?? (11)
x?c,?Condition?? ?
?
ep?EPc
y?ep,?causal??, (12)
where both equations are defined ?c ? C. EPc is
defined to be the set of event pairs that cross the two
text spans associated with c. For instance, if the first
text span of c contains two event mentions ei, ej ,
and there is one event mention ek in the second text
span of c, then EPc = {(ei, ek), (ej , ek)}. Finally,
the logical form of Equation (11) can be written as:
x?c,?Cause?? ? y?epi,?causal?? ? . . . ? y?epj ,?causal??,
where epi, . . . , epj are elements in EPc. This states
that if we assign the Cause discourse label to c,
then at least one of epi, . . . , epj must be assigned as
causal. The interpretation of Equation (12) is simi-
lar.
We use two more constraints to capture the inter-
actions between event causality and discourse rela-
tions. First, we defined Cep as the set of connectives
c enclosing each event of ep in each of its text spans,
i.e.: one of the text spans of c contain one of the
event in ep, while the other text span of c contain the
other event in ep. Next, based on the discourse rela-
tions in Section 4.1, we propose that when an event
pair ep is judged to be causal, then the connective
c that encloses it should be evoking one of the dis-
course relations in LDRa = {?Cause?, ?Condition?,
?Temporal?, ?Asynchronous?, ?Synchrony?, ?Con-
junction?}. We capture this using the following con-
straint:
y?ep,?causal?? ?
?
dra?LDRa
x?c,dra? ?c ? Cep (13)
The logical form of Equation (13) can be written as:
y?ep,?causal?? ? x?c,?Cause?? ? x?c,?Condition?? . . . ?
x?c,?Conjunction??. This states that if we assign ep as
causal, then we must assign to c one of the labels in
LDRa .
Finally, we propose that for any connectives evok-
ing discourse relations LDRb = {?Comparison?,
?Concession?, ?Contrast?, ?Pragmatic-concession?,
?Pragmatic-contrast?, ?Expansion?, ?Alternative?,
?Exception?, ?Instantiation?, ?List?, ?Restate-
ment?}, any event pair(s) that it encloses should be
?causal. We capture this using the following con-
straint:
x?c,drb? ? y?ep,??causal??
? drb ? LDRb , ep ? EPc, (14)
where the logical form of Equation (14) can be writ-
ten as: x?c,drb? ? y?ep,??causal??.
6 Experiments
6.1 Experimental Settings
To collect the distributional statistics for measuring
CEA as defined in Equation (1), we applied part-
of-speech tagging, lemmatization, and dependency
parsing (Marneffe et al, 2006) on about 760K docu-
ments in the English Gigaword corpus (LDC catalog
number LDC2003T05).
We are not aware of any benchmark corpus for
evaluating event causality extraction in contexts.
Hence, we created an evaluation corpus using the
following process: Using news articles collected
from CNN7 during the first three months of 2010, we
randomly selected 20 articles (documents) as evalu-
ation data, and 5 documents as development data.
Two annotators annotated the documents for
causal event pairs, using two simple notions for
causality: the Cause event should temporally pre-
cede the Effect event, and the Effect event occurs be-
cause the Cause event occurs. However, sometimes
it is debatable whether two events are involved in a
causal relation, or whether they are simply involved
in an uninteresting temporal relation. Hence, we al-
lowed annotations of C to indicate causality, and R
to indicate relatedness (for situations when the exis-
tence of causality is debatable). The annotators will
simply identify and annotate the C or R relations be-
tween predicates of event pairs. Event arguments are
not explicitly annotated, although the annotators are
free to look at the entire document text while mak-
ing their annotation decisions. Finally, they are free
7http://www.cnn.com
300
System Rec% Pre% F1%
PMIpp 26.6 20.8 23.3
ECDpp &PMIpa,aa 40.9 23.5 29.9
CEA 62.2 28.0 38.6
CEA+Discourse 65.1 30.7 41.7
Table 2: Performance of baseline systems and our ap-
proaches on extracting Causal event relations.
System Rec% Pre% F1%
PMIpp 27.8 24.9 26.2
ECDpp &PMIpa,aa 42.4 28.5 34.1
CEA 63.1 33.7 43.9
CEA+Discourse 65.3 36.5 46.9
Table 3: Performance of the systems on extracting Causal
and Related event relations.
to annotate relations between predicates that have
any number of sentences in between and are not re-
stricted to a fixed sentence window-size.
After adjudication, we obtained a total of 492
C+R relation annotations, and 414C relation anno-
tations on the evaluation documents. On the devel-
opment documents, we obtained 92 C+R and 71 C
relation annotations. The annotators overlapped on
10 evaluation documents. On these documents, the
first (second) annotator annotated 215 (199) C + R
relations, agreeing on 166 of these relations. To-
gether, they annotated 248 distinct relations. Us-
ing this number, their agreement ratio would be 0.67
(166/248). The corresponding agreement ratio for
C relations is 0.58. These numbers highlight that
causality identification is a difficult task, as there
could be as many as N2 event pairs in a document
(N is the number of events in the document). We
plan to make this annotated dataset available soon.8
6.2 Evaluation
As mentioned in Section 5.1, to enable translat-
ing (the unbounded) CEA scores into binary causal,
?causal predictions, we need to rescale or calibrate
these scores to range in [0,1]. To do this, we first
rank all the CEA scores of all event pairs in the de-
velopment documents. Most of these event pairs will
be ?causal. Based on the relation annotations in
these development documents, we scanned through
8http://cogcomp.cs.illinois.edu/page/publication view/663
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 5  10  15  20  25  30  35  40
Pr
ec
is
io
n(
%
)
K (number of causality predictions)
Precision(%) on top K event causality predictions
CEA
ECDpp & PMIpa,aa
PMIpp
Figure 1: Precision of the top K causality C predictions.
this ranked list of scores to locate the CEA score
t that gives the highest F1-score (on the develop-
ment documents) when used as a threshold between
causal vs ?causal decisions. We then ranked all
the CEA scores of all event pairs gathered from the
760K Gigaword documents, discretized all scores
higher than t into B bins, and all scores lower than
t into B bins. Together, these 2B bins represent the
range [0,1]. We used B = 500. Thus, consecu-
tive bins represent a difference of 0.001 in calibrated
scores.
To measure the causality between a pair of
events ei and ej , a simple baseline is to calculate
PMI(pi, pj). Using a similar thresholding and cali-
bration process to translate PMI(pi, pj) scores into
binary causality decisions, we obtained a F1 score of
23.1 when measured over the causality C relations,
as shown in the row PMIpp of Table 2.
As mentioned in Section 2.1.2, Riaz and Girju
(2010) proposed the ECD metric to measure
causality between two events. Thus, as a point of
comparison, we replaced spp of Equation (1) with
ECD(a, b) of Equation (5), substituting a = pi and
b = pj . After thresholding and calibrating the scores
of this approach, we obtained a F1-score of 29.7, as
shown in the row ECDpp&PMIpa,aa of Table 2.
Next, we evaluated our proposed CEA approach
and obtained a F1-score of 38.6, as shown in the row
CEA of Table 2. Thus, our proposed approach ob-
tained significantly better performance than the PMI
baseline and the ECD approach. Next, we per-
formed joint inference with the discourse relation
predictions as described in Section 5 and obtained
301
an improved F1-score of 41.7. We note that we ob-
tained improvements in both recall and precision.
This means that with the aid of discourse relations,
we are able to recover more causal relations, as well
as reduce false-positive predictions.
Constraint Equations (11) and (12) help to re-
cover causal relations. For improvements in pre-
cision, as stated in the last paragraph of Section
5.2, identifying other discourse relations such as
?Comparison?, ?Contrast?, etc., provides counter-
evidence to causality. Together with constraint
Equation (14), this helps to eliminate false-positive
event pairs as classified by CEA and contributes
towards CEA+Discourse having a higher precision
than CEA.
The corresponding results for extracting both
causality and relatedness C + R relations are given
in Table 3. For these experiments, the aim was for a
more relaxed evaluation and we simply collapsed C
and R into a single label.
Finally, we also measured the precision of the
top K causality C predictions, showing the preci-
sion trends in Figure 1. As shown, CEA in general
achieves higher precision when compared toPMIpp
and ECDpp&PMIpa,aa. The trends for C+R pre-
dictions are similar.
Thus far, we had included both verbal and nom-
inal predicates in our evaluation. When we repeat
the experiments for ECDpp&PMIpa,aa and CEA
on just verbal predicates, we obtained the respective
F1-scores of 31.8 and 38.3 on causality relations.
The corresponding F1-scores for casuality and relat-
edness relations are 35.7 and 43.3. These absolute
F1-scores are similar to those in Tables 2 and 3, dif-
fering by 1-2%.
7 Analysis
We randomly selected 50 false-positive predictions
and 50 false-negative causality relations to analyze
the mistakes made by CEA.
Among the false-positives (precision errors), the
most frequent error type (56% of the errors) is that
CEA simply assigns a high score to event pairs that
are not causal; more knowledge sources are required
to support better predictions in these cases. The next
largest group of error (22%) involves events contain-
ing pronouns (e.g. ?he?, ?it?) as arguments. Ap-
plying coreference to replace these pronouns with
their canonical entity strings or labeling them with
semantic class information might be useful.
Among the false-negatives (recall errors), 23%
of the errors are due to CEA simply assigning a
low score to causal event pairs and more contex-
tual knowledge seems necessary for better predic-
tions. 19% of the recall errors arises from causal
event pairs involving nominal predicates that are not
in our list of event evoking noun types (described in
Section 3). A related 17% of recall errors involves
nominal predicates without any argument. For these,
less information is available for CEA to make pre-
dictions. The remaining group (15% of errors) in-
volves events containing pronouns as arguments.
8 Related Work
Although prior work in event causality extraction
in context is relatively sparse, there are many prior
works concerning other semantic aspects of event
extraction. Ji and Grishman (2008) extracts event
mentions (belonging to a predefined list of target
event types) and their associated arguments. In other
prior work (Chen et al, 2009; Bejan and Harabagiu,
2010), the authors focused on identifying another
type of event pair semantic relation: event corefer-
ence. Chambers and Jurafsky (2008; 2009) chain
events sharing a common (protagonist) participant.
They defined events as verbs and given an existing
chain of events, they predict the next likely event in-
volving the protagonist. This is different from our
task of detecting causality between arbitrary event
pairs that might or might not share common argu-
ments. Also, we defined events more broadly, as
those that are triggered by either verbs or nouns. Fi-
nally, although our proposed CEA metric has resem-
blance the ECD metric in (Riaz and Girju, 2010), our
task is different from theirs and our work differs in
many aspects. They focused on building a dataset of
causal text spans, whereas we focused on identifying
causal relations between events in a given text doc-
ument. They considered text spans headed by verbs
while we considered events triggered by both verbs
and nouns. Moreover, we combined event causality
prediction and discourse relation prediction through
a global inference procedure to further improve the
performance of event causality prediction.
302
9 Conclusion
In this paper, using general tools such as the depen-
dency and discourse parsers which are not trained
specifically towards our target task, and a minimal
set of development documents for threshold tuning,
we developed a minimally supervised approach to
identify causality relations between events in con-
text. We also showed how to incorporate discourse
relation predictions to aid event causality predictions
through a global inference procedure. There are sev-
eral interesting directions for future work, including
the incorporation of other knowledge sources such
as coreference and semantic class predictions, which
were shown to be potentially important in our er-
ror analysis. We could also use discourse relations
to aid in extracting other semantic relations between
events.
Acknowledgments
The authors thank the anonymous reviewers for their
insightful comments and suggestions. University of
Illinois at Urbana-Champaign gratefully acknowl-
edges the support of Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract No. FA8750-09-C-0181. The first
author thanks the Vietnam Education Foundation
(VEF) for its sponsorship. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the view of the VEF, DARPA, AFRL,
or the US government.
References
Brandon Beamer and Roxana Girju. 2009. Using a bi-
gram event model to predict causal potential. In CI-
CLING.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Un-
supervised event coreference resolution with rich lin-
guistic features. In ACL.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In ACL-HLT.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In ACL.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In AAAI.
Zheng Chen, Heng Ji, and Robert Haralick. 2009. A
pairwise event coreference model, feature impact and
evaluation for event coreference resolution. In RANLP
workshop on Events in Emerging Text Types.
Roxana Girju. 2003. Automatic detection of causal re-
lations for question answering. In ACL workshop on
Multilingual Summarization and Question Answering.
Olga Gurevich, Richard Crouch, Tracy Holloway King,
and Valeria de Paiva. 2008. Deverbal nouns in knowl-
edge representation. Journal of Logic and Computa-
tion, 18, June.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through unsupervised cross-document infer-
ence. In ACL.
Claudia Leacock and Martin Chodorow, 1998. Combin-
ing Local Context and WordNet Similarity for Word
Sense Identification. MIT Press.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A pdtb-styled end-to-end discourse parser. Tech-
nical report. http://www.comp.nus.edu.sg/ linzi-
hen/publications/tech2010.pdf.
Marie-catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh,
Alan Lee, Aravind Joshi, Livio Robaldo, and
Bonnie Webber. 2007. The penn discourse tree-
bank 2.0 annotation manual. Technical report.
http://www.seas.upenn.edu/ pdtb/PDTBAPI/pdtb-
annotation-manual.pdf.
Mehwish Riaz and Roxana Girju. 2010. Another look at
causality: Discovering scenario-specific contingency
relationships with no supervision. In ICSC.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In LREC.
Dan Roth and Wen Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In CoNLL.
Dan Roth and Wen Tau Yih. 2007. Global inference for
entity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Scheffczyk.
2010. FrameNet II: Extended Theory and Practice.
http://framenet.icsi.berkeley.edu.
Yizhou Sun, Ning Liu, Kunqing Xie, Shuicheng Yan,
Benyu Zhang, and Zheng Chen. 2007. Causal rela-
tion of queries from temporal logs. In WWW.
Patrick Suppes. 1970. A Probabilistic Theory of Causal-
ity. Amsterdam: North-Holland Publishing Company.
303
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 677?687, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Inference for Event Timeline Construction
Quang Xuan Do Wei Lu Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{quangdo2,luwei,danr}@illinois.edu
Abstract
This paper addresses the task of construct-
ing a timeline of events mentioned in a given
text. To accomplish that, we present a novel
representation of the temporal structure of a
news article based on time intervals. We then
present an algorithmic approach that jointly
optimizes the temporal structure by coupling
local classifiers that predict associations and
temporal relations between pairs of tempo-
ral entities with global constraints. Moreover,
we present ways to leverage knowledge pro-
vided by event coreference to further improve
the system performance. Overall, our experi-
ments show that the joint inference model sig-
nificantly outperformed the local classifiers by
9.2% of relative improvement in F1. The ex-
periments also suggest that good event coref-
erence could make remarkable contribution to
a robust event timeline construction system.
1 Introduction
Inferring temporal relations amongst a collection of
events in a text is a significant step towards vari-
ous important tasks such as automatic information
extraction and document comprehension. Over the
past few years, with the development of the Time-
Bank corpus (Pustejovsky et al2003) , there have
been several works on building automatic systems
for such a task (Mani et al2006; Chambers and
Jurafsky, 2008; Yoshikawa et al2009; Denis and
Muller, 2011).
Most previous works devoted much efforts to the
task of identifying relative temporal relations (such
as before, or overlap) amongst events (Chambers
?
??
|
?
t1
|
?
t2
|
?
t3
?
|
t4
|
?
+?
|
Time
??
I1
I2
I3
e1
e2
e4
e3/e5
e7 ? e6
Figure 1: A graphical illustration of our timeline representation.
The e?s, t?s and I?s are events, time points and time intervals,
respectively.
and Jurafsky, 2008; Denis and Muller, 2011), with-
out addressing the task of identifying correct asso-
ciations between events and their absolute time of
occurrence. Even if this issue is addressed, certain
restrictions are often imposed for efficiency reasons
(Yoshikawa et al2009; Verhagen et al2010). In
practice, however, being able to automatically infer
the correct time of occurrence associated with each
event is crucial. Such information not only leads to
better text comprehension, but also enables fusion
of event structures extracted from multiple articles
or domains.
In this work, we are specifically interested in map-
ping events into an universal timeline representa-
tion. Besides inferring the relative temporal rela-
tions amongst the events, we would also like to au-
tomatically infer a specific absolute time of occur-
rence for each event mentioned in the text. Unlike
previous work, we associate each event with a spe-
cific absolute time interval inferred from the text. An
example timeline representation is illustrated in Fig.
677
1. Further details of our timeline representation are
given in Sec. 2.3.
We perform global inference by combining a col-
lection of local pairwise classifiers through the use
of an Integer Linear Programming (ILP) formula-
tion that promotes global coherence among local de-
cisions. The formulation allows our model to pre-
dict both event-event relations and event-time inter-
val associations simultaneously. We show that, with
the use of time intervals instead of time points, our
approach leads to a more concise ILP formulation
with reduced number of variables and constraints.
Moreover, we observed that event coreference can
reveal important information for such a task. We
propose that different event mentions that refer to
the same event can be grouped together before clas-
sification and performing global inference. This can
reduce the amount of efforts in both classification
and inference stages and can potentially eliminate
mistakes that would be made otherwise without such
coreference information. To the best of our knowl-
edge, our proposal of leveraging event coreference
to support event timeline construction is novel.
Our experiments on a collection of annotated
news articles from the standard ACE dataset demon-
strate that our approach produces robust timelines of
events. We show that our algorithmic approach is
able to combine various local evidences to produce
a global coherent temporal structure, with improved
overall performance. Furthermore, the experiments
show that the overall performance can be further im-
proved by exploiting knowledge from event corefer-
ence.
2 Background
We focus on the task of mapping event mentions in
a news article to a timeline. We first briefly describe
and define several basic concepts.
2.1 Events
Following the annotation guidelines of the ACE
project, we define an event as an action or occur-
rence that happens with associated participants or
arguments. We also distinguish between events and
event mentions, where a unique event can be core-
ferred to by a set of explicit event mentions in an
article. Formally, an event Ei is co-referred to by
a set of event mentions (ei1, e
i
2, . . . , e
i
k). Each event
mention e can be written as p(a1, a2, . . . , al), where
the predicate p is the word that triggers the presence
of e in text, and a1, a2, . . . al are the arguments asso-
ciated with e. In this work we focus on four tempo-
ral relations between two event mentions including
before, after, overlap and no relation.
2.2 Time Intervals
Similar to Denis and Muller (2011), we define time
intervals as pairs of time endpoints. Each time in-
terval I is denoted by [t?, t+], where t? and t+ are
two time endpoints representing the lower and upper
bound of the interval I , respectively, with t? ? t+.
The general form of a time endpoint is written as
?YYYY-MM-DD hh:mm:ss?. An endpoint can be un-
defined, in which case it is set to an infinity value:
??, or +?. There are two types of time intervals:
Explicit intervals are time intervals that can be
extracted directly from a given text. For example,
consider the following snippet of an article in our
data set: The litigation covers buyers in auctions
outside the United States between January 1, 1993
and February 7, 2000. In this example, we can ex-
tract and normalize two time intervals which are ex-
plicitly written, including January 1, 1993? [1993-
01-01 00:00:00, 1993-01-01 23:59:59] and Febru-
ary 7, 2000 ? [2000-02-07 00:00:00, 2000-02-07
23:59:59]. Moreover, an explicit interval can also
be formed by one or more separate explicit temporal
expressions. In the example above, the connective
term between relates the two expressions to form a
single time interval: between January 1, 1993 and
February 7, 2000 ? [1993-01-01 00:00:00, 2000-
02-07 23:59:59]. To extract explicit time intervals
from text, we use the time interval extractor de-
scribed in Zhao et al2012).
Implicit intervals are time intervals that are not
explicitly mentioned in the text. We observed that
there are events that cannot be assigned to any pre-
cise time interval but are roughly known to occur
in the past or in the future relative to the Doc-
ument Creation Time (DCT) of the article. We
introduce two implicit time intervals to represent
the past and the future events as (??, t?DCT ] and
[t+DCT ,+?), respectively. In addition, we also al-
low an event mention to be assigned into the entire
timeline, which is denoted by (??,+?) if we can-
678
not identify its time of occurrence. We also consider
DCT as an implicit interval.
We say that the time interval Ii precedes the time
interval Ij on a timeline if and only if t
+
i ? t
?
j ,
which also implies that Ii succeeds Ij if and only if
t?i ? t
+
j . The two intervals overlap, otherwise.
2.3 Timeline
We define a timeline as a partially ordered set of time
intervals. Fig. 1 gives a graphical illustration of an
example timeline, where events are annotated and
associated with time intervals. Relations amongst
events can be properly reflected in the timeline rep-
resentation. For example, in the figure, the events e1
and e2 are both associated with the interval I1. The
relation between them is no relation, since it is un-
clear which occurs first. On the other hand, e5 and
e3 both happen in the interval I2 but they form an
overlap relation. The events e6 and e7 occur within
the same interval I3, but e7 precedes (i.e. before) e6
on the timeline. The event e4 is associated with the
interval (??,+?), indicating there is no knowl-
edge about its time of occurrence.
We believe that such a timeline representation
for temporally ordering events has several advan-
tages over the temporal graph representations used
in previous works (Chambers and Jurafsky, 2008;
Yoshikawa et al2009; Denis and Muller, 2011).
Unlike previous works, in our model the events are
partially ordered in a single timeline, where each
event is associated with a precise time interval. This
improves human interpretability of the temporal re-
lations amongst events and time. This property of
our timeline representation, thus, facilitates merg-
ing multiple timelines induced from different arti-
cles. Furthermore, as we will show later, the use
of time intervals within the timeline representation
simplifies the global inference formulation and thus
the inference process.
3 A Joint Timeline Model
Our task is to induce a globally coherent timeline
for a given article. We thus adopt a global infer-
ence model for performing the task. The model
consists of two components: (1) two local pairwise
classifiers, one between event mentions and time in-
tervals (the E?T classifier) and one between event
mentions themselves (the E?E classifier), and (2)
a joint inference module that enforces global co-
herency constraints on the final outputs of the two
local classifiers. Fig. 2 shows a simplified temporal
structure of event mentions and time intervals of an
article in our model.
Our E?T classifier is different from previous
work (Chambers and Jurafsky, 2008; Yoshikawa et
al., 2009; Denis and Muller, 2011), where such clas-
sifiers were trained to identify temporal relations be-
tween event mentions and a temporal expression. In
our work, in order to construct absolute timeline of
event mentions, temporal expressions are captured
and normalized as absolute time intervals. The E?T
classifiers are then used to assign event mentions to
their contextually corresponding time intervals.
We also lifted several restrictions imposed in pre-
vious work (Bethard et al2007; Yoshikawa et al
2009; Verhagen et al2010). Specifically, we do
not require that event mentions and time expressions
have to appear in the same sentence, and we do not
require two event mentions have to appear very close
to each other (e.g., main event mentions in adjacent
sentences) in order to be considered as candidate
pairs for classification. Instead, we performed clas-
sifications over all pairs of event mentions and time
intervals as well as over all pairs of event mentions.
We show through experiments that lifting these re-
strictions is indeed important (see Sec. 5).
Another important improvement over previous
work is our global inference model We would like
to highlight that our work is also distinct from most
previous works in the global inference component.
Specifically, our global inference model jointly op-
timizes the E-E relations amongst event mentions
and their associations, E-T, with temporal informa-
tion (intervals in our case). Previous work (Cham-
bers and Jurafsky, 2008; Denis and Muller, 2011),
on the other hand, assumed that the E-T information
is given and only tried to improve E-E.
3.1 The Pairwise Classifiers
We first describe our local classifiers that associate
event mention with time interval and classify tempo-
ral relations between event mentions, respectively.
CE?T : is the E?T classifier that associates an
event mention with a time interval. Given an event
mention and a time interval, the classifier predicts
679
e1
e
2
e
3
e
4
e
n-1
e
n
e
5
? ? ?
I
1
I
2
I
3
I
m? ? ?
Figure 2: A simplified temporal structure of an article. There
are m time intervals I1 ? ? ? Im and n event mentions e1 ? ? ? en.
A solid edge indicates an association between an interval and
an event mention, whereas a dash edge illustrates a temporal
relation between two event mentions.
whether the former associates with the latter.
CE?T (ei, Ij)? {0, 1},
?i, j, 1 ? i ? n, 1 ? j ? m, (1)
where n and m are the number of event mentions
and time intervals in an article, respectively.
CE?E : is the E?E classifier that identifies
the temporal relation between two event mentions.
Given a pair of event mentions, the classifier predicts
one of the four temporal relations between them:
b?efore, a?fter, o?verlap and n?o relation. Specifically:
CE?E(ei, ej)? {b?, a?, o?, n?},
?i, j, 1 ? i, j ? n, i 6= j, (2)
For training of the classifiers, we define a set of
features following some previous work (Bethard et
al., 2007; Chambers and Jurafsky, 2008; Yoshikawa
et al2009), together with some additional features
that we believe to be helpful for the interval-based
representation. We describe the base features below
and use ? and ? to denote the features used for CE?T
and CE?E , respectively. We use the term temporal
entity (or entity, for short) to refer to either an event
mention or a time interval.
Lexical Features: A set of lexical features related
to the temporal entities: (i)?? the word, lemma and
part-of-speech of the input event mentions and the
context surrounding them, where the context is de-
fined as a window of 2 words before and after the
mention; (ii)? the modal verbs to the left and to the
right of the event mention; (iii)? the temporal con-
nectives between the event mentions1.
1We define a list of temporal connectives including before,
after, since, when, meanwhile, lately, etc.
Syntactic Features: (i)?? which entity appears
first in the text; (ii)?? whether the two entities appear
in the same sentence; (iii)?? the quantized number of
sentences between the two entities2; (iv)?? whether
the input event mentions are covered by preposi-
tional phrases and what are the heads of the phrases;
(v)?? if the entities are in the same sentence, what is
their least common constituent on the syntactic parse
tree; (vi)? whether there is any other temporal entity
that is closer to one of the two entities.
Semantic Features?: A set of semantic features,
mostly related to the input event mentions: (i)
whether the input event mentions have a common
synonym from their synsets in WordNet (Fellbaum,
1998); (ii) whether the input event mentions have a
common derivational form derived from WordNet.
Linguistic Features??: The tense and the aspect
of the input event mentions. We use an in-house
rule-based recognizer to extract these features.
Time Interval Features?: A set of features re-
lated to the input time interval: (i) whether the
interval is implicit; (ii) if it is implicit, identify
its interval type: ?dct? = [t?DCT , t
+
DCT ], ?past? =
(??, t?DCT ], ?feature? = [t
+
DCT ,+?), and ?en-
tire? = (??,+?); (iii) the interval is before, after
or overlapping with the DCT.
We note that unlike many previous work (Mani et
al., 2006; Chambers and Jurafsky, 2008; Denis and
Muller, 2011), our classifiers do not use any gold
annotations of event attributes (event class, tense, as-
pect, modal and polarity) provided in the TimeBank
corpus as features.
In our work, we use a regularized averaged Per-
ceptron (Freund and Schapire, 1999) as our classifi-
cation algorithm3. We used the one-vs.-all scheme
to transform a set of binary classifiers into a multi-
class classifier (for CE?E). The raw prediction
scores were converted into probability distribution
using the Softmax function (Bishop 1996). If there
are n classes and the raw score of class i is acti, the
posterior estimation for class i is:
P? (i) =
eacti
?
1?j?n e
actj
2We quantize the number of sentences between two entities
to 0, 1, 2, less than 5 and greater than or equal to 5
3Other algorithm (e.g. SVM) gave comparable or worse re-
sults, so we only show the results from Averaged Perceptron.
680
3.2 Joint Inference for Event Timeline
To exploit the interaction among the temporal enti-
ties in an article, we optimize the predicted tempo-
ral structure, formed by predictions from CE?T and
CE?E , w.r.t. a set of global constraints that enforce
coherency on the final structure. We perform exact
inference using Integer Linear Programming (ILP)
as in (Roth and Yih, 2007; Clarke and Lapata, 2008).
We use the Gurobi Optimizer4 as a solver.
Let I = {I1, I2, . . . , Im} denote the set of time
intervals extracted from an article, and let E =
{e1, e2, . . . , en} denote all event mentions in the
same article. Let EI = {(ei, Ij) ? E ? I|ei ?
E , Ij ? I} denote the set of all pairs of event
mentions and time intervals. We also denote the
set of event mention pairs by EE = {(ei, ej) ?
E ? E|ei ? E , ej ? E , i 6= j}. The prediction prob-
ability of an association of a pair eI ? EI, given
by classifier CE?T , is denoted by p?eI,1?
5. Now, let
R = {b?, a?, o?, n?} be the set of temporal relations be-
tween two event mentions. The prediction proba-
bility of an event mention pair ee ? EE that takes
temporal relation r, given by CE?E , is denoted by
p?ee,r?. Furthermore, we define x?eI,1? to be a binary
indicator variable that takes on the value 1 iff an as-
sociation is predicted between e and I . Similarly,
we define a binary indicator variable y?ee,r? of a pair
of event mentions ee that takes on the value 1 iff ee
is predicted to hold the relation r.
The objective function is then defined as a linear
combination of the prediction probabilities from the
two local classifiers as follows:
arg max
x,y
[
?
?
eI?EI
p?eI,1? ? x?eI,1?
+ (1? ?)
?
ee?EE
?
r?R
p?ee,r? ? y?ee,r?
]
(3)
subject to the following constraints:
x?eI,1? ? {0, 1}, ?eI ? EI (4)
y?ee,r? ? {0, 1}, ?ee ? EE , r ? R (5)
?
r?R
y?ee,r? = 1, ?ee ? EE (6)
4http://gurobi.com/
5This value is complementary to the non-association proba-
bility, denoted by p?eI,0? = 1? p?eI,1?
We use the single parameter ? to balance the over-
all contribution of two components E-T and E-E.
? is determined through cross validation tuning on
a development set. We use (4) and (5) to make sure
x?eI,1? and y?ee,r? are binary values. The equality
constraint (6) ensures that exactly one particular re-
lation can be assigned to each event mention pair.
In addition, we also require that each event is as-
sociated with only one time interval. These con-
straints are encoded as follows:
?
I?I
x?eI,1? = 1, ?e ? E (7)
Our model also enforces reflexivity and transitiv-
ity constraints on the relations among event men-
tions as follows:
y?eiej ,r? ? y?ejei,r?? = 0,
?eiej = (ei, ej) ? EE , i 6= j (8)
y?eiej ,r1? + y?ejek,r2? ? y?eiek,r3? ? 1,
?eiej , ejek, eiek ? EE , i 6= j 6= k (9)
The equality constraints in (8) encode reflexive
property of event-event relations, where the rela-
tion r? denotes the inversion of the relation r. The
set of possible (r, r?) pairs is defined as follows:
{
(b?, a?), (a?, b?), (o?, o?), (n?, n?)
}
. Following the work
of (Bramsen et al2006; Chambers and Jurafsky,
2008), we encode transitive closure of relations be-
tween event mentions with inequality constraints in
(9), which states that if the pair (ei, ej) has a certain
relation r1, and the pair (ej , ek) has the relation r2,
then the relation r3 must be satisfied between ei and
ek. Examples of such triple (r1, r2, r3) include (b?, b?,
b?) and (a?, a?, a?).
Finally, to capture the interactions between our
local pairwise classifiers we add the following con-
straints:
x?eiIk,1? + x?ejIl,1? ? y?eiej ,b?? ? 1,
?eiIk, ejIl ? EI, ?eiej ? EE ,
Ik precedes Il, i 6= j, k 6= l (10)
Intuitively, the inequality constraints in (10) spec-
ify that a temporal relation between two event men-
tions can be inferred from their respective associated
681
time intervals. Specifically, if two event mentions ei
and ej are associated with two time intervals Ik and
Il respectively, and Ik precedes Il in the timeline,
then ei must happen before ej .
It is important to note that our interval-based for-
mulation is more concise in terms of the number of
variables and constraints needed in the ILP relative
to time expression-based (or timepoint-based) for-
mulations used in previous work (Chambers and Ju-
rafsky, 2008). Specifically, in such timepoint-based
formulations, the relation between each event men-
tion and each time expression needs to be inferred,
resulting in |E||T ||RT | variables, where |E|, |T |,
and |RT | are the numbers of event mentions, time
points, and temporal relations respectively. In con-
trast, only |E||I| variables are required in our for-
mulation, where |I| is the number of intervals (since
we extract intervals explicitly, |I| is roughly equal
to |T |). Furthermore, performing inference with the
timepoint-based formulation would require |E||T |
equality constraints to enforce that each event men-
tion can take only one relation inRT for a particular
time point, whereas our interval-based model only
requires |E| constraints, since each event is strictly
associated with one interval (see Eqn. (7)). We jus-
tify the benefits of our formulation later in Sec. 5.4.
4 Incorporating Knowledge from Event
Coreference
One of the key contributions of our work is using
event coreference information to enhance the time-
line construction performance. This is motivated by
the following two principles:
(P1) All mentions of a unique event are associ-
ated with the same time interval, and overlap with
each other.
(P2) All mentions of an event have the same tem-
poral relation with all mentions of another event.
The example below, extracted from an article pub-
lished on 03/11/2003 in the Automatic Content Ex-
traction (ACE), 2005, corpus6 serves to illustrate the
significance of event coreference to our task.
6http://www.itl.nist.gov/iad/mig/tests/ace/2005/
The world?s most powerful fine art auction houses,
Sotheby?s and Christie?s, have agreed to [e11 =
pay] 40 million dollars to settle an international
price-fixing scam, Sotheby?s said. The [e12 = pay-
ment], if approved by the courts, would settle a
slew of [e21 = suits] by clients over auctions held
between 1993 and 2000 outside the US. ... Sotheby?s
and Christie?s will each [e13 = pay] 20 million dol-
lars,? said Sotheby?s, which operates in 34 countries.
In this example, there are 4 event mentions, whose
trigger words are highlighted in bold face. The un-
derlined text gives an explicit time interval: I1 =
[1993-01-01 00:00:00, 2000-12-31 23:59:59] (we
ignore 2 other intervals given by 1993 and 2000
to simplify the illustration). Now if we consider
the event mention e12, it actually belongs to the im-
plicit future interval I2 = [2003-03-11 23:59:59,
+?). Nevertheless, there is a reasonable chance
that CE?T associates it with I1, given that they both
appear in the same sentence, and there is no di-
rect evident feature indicating the event will actu-
ally happen in the future. In such a situation, using
a local classifier to identify the correct temporal as-
sociation could be challenging.
Fortunately, precise knowledge from event coref-
erence may help alleviate such a problem. The
knowledge reveals that the 4 event mentions can be
grouped into 2 distinct events: E1 = {e11, e
1
2, e
1
3},
E2 = {e21}. If CE?T can make a strong prediction
in associating the event mention e11 (or e
1
3) to I2, in-
stead of I1, the system will have a high chance to
re-assign e12 to I2 based on principle (P1). Similarly,
if CE?E is effective in figuring out that some men-
tion of event E1 occurs after some mention of E2,
then all the mentions of E1 would be predicted to
occur after all mentions in E2 according to (P2).
To incorporate knowledge from event coreference
into our classifiers and the joint inference model, we
use the following procedure: (1) performing classi-
fication with CE?T and CE?E on the data, (2) using
the knowledge from event coreference to overwrite
the prediction probabilities obtained by the two lo-
cal classifiers in step (1), and (3) applying the joint
inference model on the new prediction probabilities
obtained from (2). We note that if we stop at step (2),
we get the outputs of the local classifiers enhanced
by event coreference knowledge.
To overwrite the classification probabilities using
682
event coreference knowledge, we propose two ap-
proaches as follows:
MaxScore: We define the probability between
any mention e ? Ei and an interval I as follows:
p?eI,1? = max
e??Ei
P? (e?, I) (11)
where P? (e?, I) is the classifier (CE?T ) probability
for associating event mention e? to the time interval.
On the other hand, the probabilities for associat-
ing the set of temporal relations, R, to each pair of
mentions in Ei?Ej , is given by the following pair:
(ei, ej)? = arg max
(ei? ,ej? )?Ei?Ej ,r?R
P?
(
(ei
?
, ej
?
), r)
)
p?ee,r? = P?
(
(ei, ej)?, r
)
,?r ? R (12)
In other words, over all possible event mention
pairs and relations, we first pick the pair who glob-
ally obtains the highest probability for some rela-
tion. Next, we simply take the probability distri-
bution of that event mention pair as the distribution
over the relations, for the event pair.
SumScore: The probability between any mention
e ? Ei and an interval I is obtained by:
p?eI,1? =
1
|Ei|
?
e??Ei
P? (e?, I) (13)
To obtain the probability distribution over the set
of temporal relations,R, for any pair of mentions in
Ei ? Ej , we used the following procedure:
r? = arg max
r?R
?
ei?Ei
?
ej?Ej
P?
(
(ei, ej), r
)
(ei, ej)? = arg max
(ei? ,ej? )?Ei?Ej
P?
(
(ei
?
, ej
?
), r?
)
p?ee,r? = P?
(
(ei, ej)?, r
)
,?r ? R (14)
In other words, given two groups of event men-
tions, we first compute the total score of each rela-
tion, and select the relation which has the highest
score. Next from the list of pairs of event mentions
from the two groups, we select the pair which has the
relation r* with highest score compared to all other
pairs. The probability distribution of this pair will
be used as the probability distribution of all event
mention pairs between the two events.
In both approaches, we assign the overlap rela-
tions to all pairs of event mentions in the same event
with probability 1.0.
5 Experimental Study
We first describe the experimental data and then
present and discuss the experimental results.
5.1 Data and Setup
Most previous works in temporal reasoning used
the TimeBank corpus as a benchmark. The cor-
pus contains a fairly diverse collection of anno-
tated event mentions, without any specific focus on
certain event types. According to the annotation
guideline of the corpus, most of verbs, nominal-
izations, adjectives, predicative clauses and preposi-
tional phrases can be tagged as events. However, in
practice, when performing temporal reasoning about
events in a given text, one is typically interested in
significant and typed events, such as Killing, Leg-
islation, Election. Furthermore, event mentions in
TimeBank are annotated with neither event argu-
ments nor event coreference information.
We noticed that the ACE 2005 corpus contains the
annotation that we are interested in. The corpus con-
sists of articles annotated with event mentions (with
event triggers and arguments) and event coreference
information. To create an experimental data set for
our work, we selected from the corpus 20 newswire
articles published in March 2003. To extract time
intervals from the articles, we used the time inter-
val extractor described in (Zhao et al2012) with
minimal post-processing. Implicit intervals are also
added according to Sec. 2.2. We then hired an anno-
tator with expertise in the field to annotate the data
with the following information: (i) event mention
and time interval association, and (ii) the temporal
relations between event mentions, including {b?, a?,
o?}. The annotator was not required to annotate all
pairs of event mentions, but as many as possible.
Next, we saturated the relations based on the ini-
tial annotations as follows: (i) event mentions that
had not been associated with any time intervals were
assigned to the entire timeline interval (??,+?),
and (ii) added inferred temporal relations between
event mentions with reflectivity and transitivity. Ta-
ble 1 shows the data statistics before and after sat-
uration. There are totally 8312 event pairs from 20
documents, including no relation pairs. We note that
in a separate experiment, we still evaluated CE?E
on the TimeBank corpus and got better performance
683
Data #Intervals #E-mentions #E-T #E-E
Initial 232 324 305 376
Saturated 232 324 324 5940
Table 1: The statistics of our experimental data set.
than a corresponding classifier in an existing work
(see Sec. 5.4).
We conducted all experiments with 5-fold cross
validation at the instance level on our data set after
saturation. The global inference model was applied
on a whole document. The results of the systems are
reported in averaged precision, recall and F1 score
on the association performance, for CE?T , and the
temporal relations (we excluded the n? relation, for
CE?E). We also measured the overall performance
of the systems by computing the average of the per-
formance of the classifiers.
5.2 A Baseline
We developed a baseline system that works as fol-
lows. It associates an event mention with the closest
time interval found in the same sentence. If such
an interval is not found, the baseline associates the
mention with the closest time interval to the left.
If the interval is again not found, the mention will
be associated with the DCT interval. The baseline
is based on the intuition of natural reading order:
events that are mentioned earlier are likely to pre-
cede those mentioned later. For the temporal rela-
tion between a pair of event mentions, the baseline
treats the event mention that appears earlier in the
text as temporally happening before the other men-
tion. The baseline performance is shown in the first
group of results in Table 2.
5.3 Our Systems
For our systems, we first evaluated the performance
of our local pairwise classifiers and the global in-
ference model. The second group of results in Ta-
ble 2 shows the systems? performance. Overall,
the results show that our global inference model
relatively outperformed the baseline and the local
classifiers by 57.8% and 9.2% in F1, respectively.
We perform a bootstrap resampling significance test
(Koehn, 2004) on the output predictions of the lo-
cal classifiers with and without the inference model.
The test shows that the overall improvement with
the inference model is statistically significant (p <
0.01). This indicates the effectiveness of our joint
inference model with global coherence constraints.
Next, we integrated event coreference knowledge
into our systems (as described in Sec. 4) and eval-
uated their performance. Our experiments showed
that the SumScore approach works better for CE?T ,
while MaxScore is more suitable for CE?E . Our ob-
servations showed that event mentions of an event
may appear in close proximity with multiple time
intervals in the text, making CE?T produce high
prediction scores for many event mention-interval
pairs. This, consequently, confuses MaxScore on
the best association of the event and the time inter-
vals, whereas SumScore overcomes the problem by
averaging out the association scores. On the other
hand, CE?E gets more benefit from MaxScore be-
causeCE?E works better on pairs of event mentions
that appear closely in the text, which activate more
valuable learning features. We will report the results
using the best approach of each classifier.
To evaluate our systems with event coreference
knowledge, we first experimented our systems with
gold event coreference as given by the ACE 2005
corpus. Table 2 shows the contribution of event
coreference to our systems in the third group of the
results. The results show that injecting knowledge
from event coreference remarkably improved both
the local classifiers and the joint inference model.
Overall, the system that combined event corefer-
ence and the global inference model achieved the
best performance, which significantly overtook all
other compared systems. Specifically, it outper-
formed the baseline system, the local classifiers, and
the joint inference model without event coreference
with 80%, 25%, and 14% of relative improvement in
F1, respectively. It also consistently outperformed
the local classifiers enhanced with event corefer-
ence. We note that the precision and recall of CE?T
in the joint inference model are the same because
the inference model enforced each event mention to
be associated with exactly one time interval. This
is also true for the systems integrated with event
coreference because our integration approaches as-
sign only one time interval to an event mention.
We next move to experimenting with automati-
cally learned event coreference systems. In this ex-
684
Model
CE?T CE?E Overall
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
1 Baseline 33.29 33.29 33.29 20.86 32.81 25.03 27.06 33.05 29.16
2
No Event Coref.
Local classifiers 62.70 34.50 43.29 40.46 42.42 40.96 51.58 38.46 42.13
Global inference 47.88 47.88 47.88 41.42 48.04 44.14 44.65 47.96 46.01
3
With Gold Event Coref.
Local classifiers 50.88 50.88 50.88 43.86 52.65 47.46 47.37 51.77 49.17
Global inference 50.88 50.88 50.88 48.04 62.45 54.05 49.46 56.67 52.47
4
With Learned Event Coref.
Local classifiers 46.37 46.37 46.37 40.83 45.28 42.60 43.60 45.83 44.49
Global inference 46.37 46.37 46.37 42.09 52.50 46.47 44.23 49.44 46.42
Table 2: Performance under various evaluation settings. All figures are averaged scores from 5-fold cross-validation experiments.
periment, we re-trained the event coreference sys-
tem described in Chen et al2009) on all arti-
cles in the ACE 2005 corpus, excluding the 20 ar-
ticles used in our data set. The performance of these
systems are shown in the fourth group of the re-
sults in Table 2. The results show that by using a
learned event coreference system, we achieved the
same improvement trends as with gold event coref-
erence. However, we did not obtain significant im-
provement when comparing with global inference
without event coreference information. This result
shows that the performance of an event coreference
system can have a significant impact on the over-
all performance. While this suggests that a better
event coreference system could potentially help the
task more, it also opens the question whether event
coreference can be benefited from our local classi-
fiers through the use of a joint inference framework.
We would like to leave this for future investigations.
5.4 Previous Work-Related Experiments
We also performed experiments using the same set-
ting as in (Yoshikawa et al2009), which followed
the guidelines of the TempEval challenges (Verha-
gen et al2007; Verhagen et al2010), on our sat-
urated data. Several assumptions were made to sim-
plify the task. For example, only main events in
adjacent sentences are considered when identifying
event-event relations. See (Yoshikawa et al2009)
for more details. We performed 5-fold cross valida-
tion without event coreference. Overall, the system
achieved 29.99 F1 for the local classifiers and 34.69
when the global inference is used. These results are
better than the baseline but underperform our full
models where those simplification assumptions are
not imposed, as shown in Table 2, indicating the im-
portance of relaxing their assumptions in practice.
We also evaluated our CE?E on the TimeBank
corpus. We followed the settings of Chambers and
Jurafsky (2008) to extract all event mention pairs
that were annotated with before (or ibefore, ?imme-
diately before?) and after (or iafter) relations in 183
news articles in the corpus. We trained and evalu-
ated ourCE?E on these examples with the same fea-
ture set that we evaluated in our experiments above,
with gold tense and aspect features but without event
type. Following their work, we performed 10-fold
cross validation. Our classifier achieved a micro-
averaged accuracy of 73.45%, whereas Chambers
and Jurafsky (2008) reported 66.8%. We next in-
jected the knowledge of an event coreference sys-
tem trained on the ACE2005 corpus into our CE?E ,
and obtained a micro-averaged accuracy of 73.39%.
It was not surprising that event coreference did not
help in this dataset because: (i) different domains
? the event coreference was trained on ACE 05 but
applied on TimeBank, and (ii) different annotation
guidelines on events in ACE 2005 and TimeBank.
Finally, we conducted an experiment that justi-
fies the advantages of our interval-based inference
model over a time point-based inference. To do this,
we first converted our data in Table 1 from inter-
vals to time points and infer the temporal relations
between the annotated event mentions and the time
points: before, after, overlap, and unknown. We
modified the first component in the objective func-
tion in (3) to accommodate these temporal relations.
We also made several changes to the constraints,
including removing those in (7) since they are no
longer required, and adding constraints that ensure
685
the relation between a time point and an event men-
tion takes exactly one value. Proper changes were
also made to other constraints in (10) to reflect the
fact that time points are considered rather than inter-
vals. We observed that experiment with such a for-
mulation was unable to finish within 5 hours (we ter-
minated the ILP inference after waiting for 5 hours),
whereas our interval-based model finished the ex-
periment with an average of 21 seconds per article.
6 Related Work
Research in temporal reasoning recently received
much attention. Allen (1983) introduced an interval
based temporal logic which has been used widely
in the field. Recent efforts in building an annotated
temporal corpus (Pustejovsky et al2003) has pop-
ularized the use of machine learning techniques for
the task (Mani et al2006; Bethard et al2007).
This corpus was later used (with simplifications) in
two TempEval challenges (Verhagen et al2007;
Verhagen et al2010). In these challenges, several
temporal-related tasks were defined including the
tasks of identifying the temporal relation between an
event mention and a temporal expression in the same
sentence, and recognizing temporal relations of pairs
of event mentions in adjacent sentences. However,
with several restrictions imposed to these tasks, the
developed systems were not practical.
Recently, there has been much work attempting
to leverage Allen?s interval algebra of temporal re-
lations to enforce global constraints on local pre-
dictions. The work of Tatu and Srikanth (2008)
used global relational constraints to not only expand
the training data but also identifies temporal incon-
sistencies to improve local classifiers. They used
greedy search to select the most appropriate config-
uration of temporal relations among events and tem-
poral expressions. For exact inferences, Bramsen et
al. (2006), Chambers and Jurafsky (2008), Denis
and Muller (2011), and Talukdar et al2012) for-
mulated the temporal reasoning problem in an ILP.
However, the inference models in their work were
not a joint model involving multiple local classifiers
but only one local classifier was involved in their ob-
jective functions.
The work of Yoshikawa et al2009) did formu-
late a joint inference model with Markov Logic Net-
work (MLN). They, however, used the same setting
as the TempEval challenges, thus only pairs of tem-
poral entities in the same or adjacent sentences are
considered. Our work, on the other hand, focuses on
constructing an event timeline with time intervals,
taking multiple local pairwise predictions into a joint
inference model and removing the restrictions on the
positions of the temporal entities. Furthermore, we
propose for the first time to use event coreference
and evaluate the importance of its role in the task of
event timeline construction.
7 Conclusions and Future Work
We proposed an interval-based representation of the
timeline of event mentions in an article. Our rep-
resentation allowed us to formalize the joint infer-
ence model that can be solved efficiently, compared
to a time point-based inference model, thus open-
ing up the possibility of building more practical
event temporal inference systems. Our inference
model achieved significant improvement over the lo-
cal classifiers. We also showed that event coref-
erence can naturally support timeline construction,
and good event coreference led to significant im-
provement in the system performance. Specifically,
when such gold event coreference knowledge was
injected into the model, a significant improvement
in the overall performance could be obtained. While
our experiments suggest that the temporal classi-
fiers can potentially help enhance the performance
of event coreference, in future work we would like
to investigate into coupling event coreference with
other components in a global inference framework.
Acknowledgments
The authors gratefully acknowledge the support
of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
No. FA8750-09-C-0181, and the Army Research
Laboratory (ARL) under agreement W911NF-09-2-
0053. The first author also thanks the Vietnam Ed-
ucation Foundation (VEF) for its sponsorship. Any
opinions, findings, and conclusion or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
VEF, DARPA, AFRL, ARL, or the US government.
686
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM.
Steven Bethard, James H. Martin, and Sara Klingenstein.
2007. Timelines from text: Identification of syntactic
temporal relations. In ICSC.
P. Bramsen, P. Deshpande, Y. K. Lee, and R. Barzilay.
2006. Inducing temporal graphs. In EMNLP.
N. Chambers and D. Jurafsky. 2008. Jointly combin-
ing implicit constraints improves temporal ordering.
In EMNLP.
Zheng Chen, Heng Ji, and Robert Haralick. 2009. A
pairwise event coreference model, feature impact and
evaluation for event coreference resolution. In Work-
shop on Events in Emerging Text Types.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research.
Pascal Denis and Philippe Muller. 2011. Predicting
globally-coherent temporal structures from texts via
endpoint inference and graph decomposition. In IJ-
CAI.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In ACL.
J. Pustejovsky, P. Hanks, R. Sauri, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The TIMEBANK corpus. In
Corpus Linguistics.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Introduction to Statistical Relational
Learning.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.
2012. Coupled temporal scoping of relational facts. In
WSDM.
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In COLING.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In SemEval-2007.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In SemEval-2010.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identify-
ing temporal relations with markov logic. In ACL-
IJCNLP.
Ran Zhao, Quang Do, and Dan Roth. 2012. A robust
shallow temporal reasoning system. In NAACL-HLT
Demo.
687
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1114?1124, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
On Amortizing Inference Cost for Structured Prediction
Vivek Srikumar? and Gourab Kundu? and Dan Roth
University of Illinois, Urbana-Champaign
Urbana, IL. 61801
{vsrikum2, kundu2, danr}@illinois.edu
Abstract
This paper deals with the problem of predict-
ing structures in the context of NLP. Typically,
in structured prediction, an inference proce-
dure is applied to each example independently
of the others. In this paper, we seek to op-
timize the time complexity of inference over
entire datasets, rather than individual exam-
ples. By considering the general inference
representation provided by integer linear pro-
grams, we propose three exact inference the-
orems which allow us to re-use earlier solu-
tions for certain instances, thereby completely
avoiding possibly expensive calls to the infer-
ence procedure. We also identify several ap-
proximation schemes which can provide fur-
ther speedup. We instantiate these ideas to the
structured prediction task of semantic role la-
beling and show that we can achieve a speedup
of over 2.5 using our approach while retain-
ing the guarantees of exactness and a further
speedup of over 3 using approximations that
do not degrade performance.
1 Introduction
Typically, in structured prediction applications, ev-
ery example is treated independently and an infer-
ence algorithm is applied to each one of them. For
example, consider a dependency parser that uses the
maximum spanning tree algorithm (McDonald et al
2005) or its integer linear program variants (Riedel
and Clarke, 2006; Martins et al 2009) to make pre-
dictions. Given a trained model, the parser addresses
* These authors contributed equally to this work.
each sentence separately and runs the inference al-
gorithm to predict the parse tree. Thus, the time
complexity of inference over the test set is linear in
the size of the corpus.
In this paper, we ask the following question: For
a given task, since the inference procedure predicts
structures from the same family of structures (depen-
dency trees, semantic role structures, etc.), can the
fact that we are running inference for a large num-
ber of examples help us improve the time complexity
of inference? In the dependency parsing example,
this question translates to asking whether, having
parsed many sentences, we can decrease the parsing
time for the next sentence.
Since any combinatorial optimization problem
can be phrased as an integer linear program (ILP),
we frame inference problems as ILPs for the purpose
of analysis. By analyzing the objective functions
of integer linear programs, we identify conditions
when two ILPs have the same solution. This allows
us to reuse solutions of previously solved problems
and theoretically guarantee the optimality of the so-
lution. Furthermore, in some cases, even when the
conditions are not satisfied, we can reuse previous
solutions with high probability of being correct.
Given the extensive use of integer linear programs
for structured prediction in Natural Language Pro-
cessing over the last few years, these ideas can be ap-
plied broadly to NLP problems. We instantiate our
improved inference approaches in the structured pre-
diction task of semantic role labeling, where we use
an existing implementation and a previous trained
model that is based on the approach of (Punyakanok
et al 2008). We merely modify the inference pro-
1114
cess to show that we can realize the theoretical gains
by making fewer calls to the underlying ILP solver.
Algorithm Speedup
Theorem 1 2.44
Theorem 2 2.18
Theorem 3 2.50
Table 1: The speedup for semantic role labeling cor-
responding to the three theorems described in this
paper. These theorems guarantee the optimality of
the solution, thus ensuring that the speedup is not
accompanied by any loss in performance.
Table 1 presents a preview of our results, which
are discussed in Section 4. All three approaches in
this table improve running time, while guaranteeing
optimum solutions. Allowing small violations to the
conditions of the theorems provide an even higher
improvement in speedup (over 3), without loss of
performance.
The primary contributions of this paper are:
1. We pose the problem of optimizing inference
costs over entire datasets rather than individ-
ual examples. Our approach is agnostic to the
underlying models and allows us to use pre-
trained scoring functions.
2. We identify equivalence classes of ILP prob-
lems and use this notion to prove exact con-
ditions under which no inference is required.
These conditions lead to algorithms that can
speed up inference problem without losing the
exactness guarantees. We also use these con-
ditions to develop approximate inference algo-
rithms that can provide a further speedup.
3. We apply our approach to the structured pre-
diction task of semantic role labeling. By not
having to perform inference on some of the in-
stances, those that are equivalent to previously
seen instances, we show significant speed up in
terms of the number of times inference needs to
be performed. These gains are also realized in
terms of wall-clock times.
The rest of this paper is organized as follows: In
section 2, we formulate the problem of amortized
inference and provide motivation for why amortized
gains can be possible. This leads to the theoretical
discussion in section 3, where we present the meta-
algorithm for amortized inference along with sev-
eral exact and approximate inference schemes. We
instantiate these schemes for the task of semantic
role labeling (Section 4). Section 5 discusses related
work and future research directions.
2 Motivation
Many NLP tasks can be phrased as structured pre-
diction problems, where the goal is to jointly assign
values to many inference variables while account-
ing for possible dependencies among them. This de-
cision task is a combinatorial optimization problem
and can be solved using a dynamic programming ap-
proach if the structure permits. In general, the infer-
ence problem can be formulated and solved as inte-
ger linear programs (ILPs).
Following (Roth and Yih, 2004) Integer linear
programs have been used broadly in NLP. For exam-
ple, (Riedel and Clarke, 2006) and (Martins et al
2009) addressed the problem of dependency pars-
ing and (Punyakanok et al 2005; Punyakanok et
al., 2008) dealt with semantic role labeling with this
technique.
In this section, we will use the ILP formulation
of dependency parsing to introduce notation. The
standard approach to framing dependency parsing as
an integer linear program was introduced by (Riedel
and Clarke, 2006), who converted the MST parser
of (McDonald et al 2005) to use ILP for inference.
The key idea is to build a complete graph consist-
ing of tokens of the sentence where each edge is
weighted by a learned scoring function. The goal
of inference is to select the maximum spanning tree
of this weighted graph.
2.1 Problem Formulation
In this work, we consider the general inference prob-
lem of solving a 0-1 integer linear program. To per-
form inference, we assume that we have a model that
assigns scores to the ILP decision variables. Thus,
our work is applicable not only in cases where in-
ference is done after a separate learning phase, as in
(Roth and Yih, 2004; Clarke and Lapata, 2006; Roth
and Yih, 2007) and others, but also when inference
is done during the training phase, for algorithms like
1115
the structured perceptron of (Collins, 2002), struc-
tured SVM (Tsochantaridis et al 2005) or the con-
straints driven learning approach of (Chang et al
2007).
Since structured prediction assigns values to a
collection of inter-related binary decisions, we de-
note the ith binary decision by yi ? {0, 1} and the
entire structure as y, the vector composed of all the
binary decisions. In our running example, each edge
in the weighted graph generates a single decision
variable (for unlabeled dependency parsing). For
each yi, let ci ? < denote the weight associated with
it. We denote the entire collection of weights by the
vector c, forming the objective for this ILP.
Not all assignments to these variables are valid.
Without loss of generality, these constraints can be
expressed using linear inequalities over the infer-
ence variables, which we write as MTy ? b for
a real valued matrix M and a vector b. In depen-
dency parsing, for example, these constraints ensure
that the final output is a spanning tree.
Now, the overall goal of inference is to find the
highest scoring structure. Thus, we can frame infer-
ence as an optimization problem p with n inference
variables as follows:
arg max
y?{0,1}n
cTy (1)
subject to MTy ? b. (2)
For brevity, we denote the space of feasible solutions
that satisfy the constraints for the ILP problem p as
Kp = {y ? {0, 1}n|MTy ? b}. Thus, the goal of
inference is to find
arg max
y?Kp
cTy.
We refer to Kp as the feasible set for the inference
problem p and yp as its solution.
In the worst case, integer linear programs are
known to be NP-hard. Hence, solving large prob-
lems, (that is, problems with a large number of con-
straints and/or variables) can be infeasible.
For structured prediction problems seen in NLP,
we typically solve many instances of inference prob-
lems. In this paper, we investigate whether an infer-
ence algorithm can use previous predictions to speed
up inference time, thus giving us an amortized gain
in inference time over the lifetime of the program.
We refer to inference algorithms that have this capa-
bility as amortized inference algorithms.
In our running example, each sentence corre-
sponds to a separate ILP. Over the lifetime of the
dependency parser, we create one inference instance
(that is, one ILP) per sentence and solve it. An amor-
tized inference algorithm becomes faster at parsing
as it parses more and more sentences.
2.2 Why can inference costs be amortized over
datasets?
In the rest of this section, we will argue that the time
cost of inference can be amortized because of the
nature of inference in NLP tasks. Our argument is
based on two observations, which are summarized in
Figure (1): (1) Though the space of possible struc-
tures may be large, only a very small fraction of
these occur. (2) The distribution of observed struc-
tures is heavily skewed towards a small number of
them.
x?s p?s y?s
ILP
formulation
Inference
Examples
ILPs
Solutions
Figure 1: For a structured prediction task, the infer-
ence problem p for an example x needs to be for-
mulated before solving it to get the structure y. In
structured prediction problems seen in NLP, while
an exponential number of structures is possible for a
given instance, in practice, only a small fraction of
these ever occur. This figure illustrates the empirical
observation that there are fewer inference problems
p?s than the number of examples and the number of
observed structures y?s is even lesser.
As an illustration, consider the problem of part-
of-speech tagging. With the standard Penn Treebank
tag set, each token can be assigned one of 45 labels.
Thus, for a sentence of size n, we could have 45n
structures out of which the inference process needs
to choose one. However, a majority of these struc-
tures never occur. For example, we cannot have a
1116
 0
 100000
 200000
 300000
 400000
 500000
 0  10  20  30  40  50Number of tokens
Part-of-speech statistics, using tagged Gigaword text
Number of examples of size Number of unique POS tag sequences
(a) Part-of-speech tagging
 0
 100000
 200000
 300000
 400000
 500000
 0  10  20  30  40  50Size of sentence
Unlabeled dependency parsing statistics, using tagged Gigaword text
Number of examples of sizeNumber of unique dependency trees
(b) Unlabeled dependency parsing
 0
 20000
 40000
 60000
 80000
 100000
 120000
 140000
 160000
 0  1  2  3  4  5  6  7  8Size of the input (number of argument candidates)
SRL statistics , using tagged Gigaword text
Number of examples of sizeNumber of unique SRL structures
(c) Semantic role labeling
Figure 2: Number of inference instances for different input sizes (red solid lines) and the number of unique
structures for each size (blue dotted lines). The x-axis indicates the size of the input (number of tokens
for part of speech and dependency, and number of argument candidates for SRL.) Note that the number of
instances is not the number of unique examples of a given length, but the number of times an inference
procedure is called for an input of a given size.
 0
 2
 4
 6
 8
 10
 12
 0  5000  10000  15000  20000Solution Id
Log frequency of solutions for sentences with 5 tokens
(a) Sentence length = 5
-1
 0
 1
 2
 3
 4
 5
 6
 7
 8
 0  50000  100000  150000  200000  250000Solution Id
Log frequency of solutions for sentences with 10 tokens
(b) Sentence length = 10
-1
 0
 1
 2
 3
 4
 5
 6
 7
 8
 0  50000  100000  150000  200000  250000  300000  350000Solution Id
Log frequency of solutions for sentences with 15 tokens
(c) Sentence length = 15
Figure 3: These plots show the log-frequencies of occurrences of part-of-speech sequences for sentences
with five, ten and fifteen tokens. The x-axes list different unique part-of-speech tag sequences for the entire
sentence. These plots show that for sentences of a given length, most structures (solutions) that are possible
never occur, or occur very infrequently; only a few of the possible structures (solutions) actually occur
frequently.
sentence where all the tokens are determiners.
Furthermore, many sentences of the same size
share the same part-of-speech tag sequence. To
quantify the redundancy of structures, we part-of-
speech tagged the English Gigaword corpus (Graff
and Cieri, 2003). Figure (2a) shows the number
of sentences in the corpus for different sentence
lengths. In addition, it also shows the number of
unique part-of-speech tag sequences (over the en-
tire sentence) for each size. We see that the number
of structures is much fewer than the number of in-
stances for any sentence size. Note that 45n quickly
outgrows the number of sentences as n increases.
The figures (2b) and (2c) show similar statistics for
unlabeled dependency parsing and semantic role la-
beling. In the former case, the size of the instance is
the number of tokens in a sentence, while in the lat-
ter, the size is the number of argument candidates
that need to be labeled for a given predicate. In
both cases, we see that the number of empirically
observed structures is far fewer than the number of
instances to be labeled.
Thus, for any given input size, the number of in-
stances of that size (over the lifetime of the program)
far exceeds the number of observed structures for
that size. Moreover, the number of observed struc-
tures is significantly smaller than the number of the-
oretically possible structures. Thus, we have a small
number of structures that form optimum structures
for many inference instances of the same size.
Our second observation deals with the distribu-
tion of structures for a given input size. Figure (3)
1117
shows the log frequencies of part-of-speech tagging
sequences for sentences of lengths five, ten and fif-
teen. In all cases, we see that a few structures are
most frequent. We observed similar distributions of
structures for all input sizes for dependency parsing
and semantic role labeling as well.
Since the number of structures for a given exam-
ple size is small, many examples x?s, and hence
many inference problems p?s, are associated with
the same structure y. These observations suggest the
possibility of getting an amortized gain in inference
time by characterizing the set of inference problems
that produce the same structure. Then, for a new in-
ference problem, if we can identify that it belongs to
a known set, that is, will yield a solution that we have
already seen, we do not have to run inference at all.
The second observation also suggests that this char-
acterization of sets of problems that have the same
solution can be done in a data-driven way because
characterizing a small number of structures can give
us high coverage.
3 Amortizing inference costs
In this section, we present different schemes for
amortized inference leading up to an inference meta-
algorithm. The meta-algorithm is both agnostic to
the underlying inference algorithm that is used by
the problem and maintains the exactness properties
of the underlying inference scheme. That is, if we
have an exact/approximate inference algorithm with
a certain guarantees, the meta-algorithm will have
the same guarantees, but with a speedup.
3.1 Notation
For an integer linear program p with np variables,
we denote its objective coefficients by cp and its fea-
sible set byKp. We denote its solution as as yp. We
represent vectors by boldfaced symbols and their ith
component using subscripts.
We consider many instantiations of the inference
problem and use superscripts to denote each indi-
vidual instance. Thus, we have a large collection of
inference instances P = {p1,p2, ? ? ? } along with
their respective solutions {y1p,y
2
p, ? ? ? }.
Definition 1 (Equivalence classes of ILPs). Two in-
teger linear programs are said to be in the same
equivalence class if they have the same number of
inference variables and the same feasible set.
We square brackets to denote equivalence classes.
If [P ] is an equivalence class of ILPs, we use the
notation K[P ] to denote its feasible set and n[P ] to
denote the number of variables. Also, for a program
p, we use the notation p ? [P ] to indicate that it
belongs to the equivalence class [P ].
3.2 Exact theorems
Our goal is to characterize the set of objective func-
tions which will have the same solution for a given
equivalence class of problems.
Suppose we have solved an ILP p to get a solution
yp. For every inference variable that is active in the
solution (i.e., whose value is 1), increasing the corre-
sponding objective value will not change the optimal
assignment to the variables. Similarly, for all other
variables (whose value in the solution is 0), decreas-
ing the objective value will not change the optimal
solution. This intuition gives us our first theorem for
checking whether two ILPs have the same solution
by looking at the difference between their objective
coefficients.
Theorem 1. Let p denote an inference problem
posed as an integer linear program belonging to an
equivalence class [P ]. Let q ? [P ] be another infer-
ence instance in the same equivalence class. Define
?c = cq ? cp to be the difference of the objective
coefficients of the ILPs. Then, yp is the solution of
the problem q if for each i ? {1, ? ? ? , np}, we have
(2yp,i ? 1)?ci ? 0 (3)
The condition in the theorem, that is, inequal-
ity (3), requires that the objective coefficients corre-
sponding to values yp,i that are set to 1 in p increase,
and those that correspond to values of yp,i set to 0,
decrease. Under these conditions, if yp is the max-
imizer of the original objective, then it maximizes
the new objective too.
Theorem 1 identifies perturbations of an ILP?s ob-
jective coefficients that will not change the optimal
assignment. Next, we will characterize the sets of
objective values that will have the same solution us-
ing a criterion that is independent of the actual so-
lution. Suppose we have two ILPs p and q in an
equivalence class [P ] whose objective values are cp
and cq respectively. Suppose y? is the solution to
1118
both these programs. That is, for every y ? K[P ],
we have cTpy ? c
T
py
? and cTqy ? c
T
qy
?. Multiply-
ing these inequalities by any two positive real num-
bers x1 and x2 and adding them shows us that y?
is also the solution for the ILP in [P ] which has the
objective coefficients x1cp + x2cq. Extending this
to an arbitrary number of inference problems gives
us our next theorem.
Theorem 2. Let P denote a collection
{p1,p2, ? ? ? ,pm} of m inference problems in
the same equivalence class [P ] and suppose that
all the problems have the same solution, yp. Let
q ? [P ] be a new inference program whose optimal
solution is y. Then y = yp if there is some x ? <m
such that x ? 0 and
cq =
?
j
xjcjp. (4)
From the geometric perspective, the pre-condition
of this theorem implies that if the new coefficients
lie in the cone formed by the coefficients of the pro-
grams that have the same solution, then the new pro-
gram shares the solution.
Theorems 1 and 2 suggest two different ap-
proaches for identifying whether a new ILP can
use the solution of previously solved inference in-
stances. These theorems can be combined to get a
single criterion that uses the objective coefficients of
previously solved inference problems and their com-
mon solution to determine whether a new inference
problem will have the same solution. Given a collec-
tion of solved ILPs that have the same solution, from
theorem 2, we know that an ILP with the objective
coefficients c =
?
j xjc
j
p will share the solution.
Considering an ILP whose objective vector is c and
applying theorem 1 to it gives us the next theorem.
Theorem 3. Let P denote a collection
{p1,p2, ? ? ? ,pm} of m inference problems
belonging to the same equivalence class [P ].
Furthermore, suppose all the programs have the
same solution yp. Let q ? [P ] be a new inference
program in the equivalence class. For any x ? <m,
define ?c(x) = cq ?
?
j xjc
j
p. The assignment
yp is the optimal solution of the problem q if there
is some x ? <m such that x ? 0 and for each
i ? {1, np}, we have
(2yp,i ? 1)?ci ? 0 (5)
Theorem Condition
Theorem 1 ?i ? {1, ? ? ? , np},
(2yp,i ? 1)?ci ? 0; ?i.
Theorem 2 ? x ? <m, such that
x ? 0 and cq =
?
j xjc
j
p
Theorem 3 ? x ? <m, such that
x ? 0 and (2yp,i ? 1)?ci ? 0; ?i.
Table 2: Conditions for checking whether yp is the
solution for an inference problem q ? [P ] according
to theorems 1, 2 and 3. Please refer to the statements
of the theorems for details about the notation.
3.3 Implementation
Theorems 1, 2 and 3 each specify a condition that
checks whether a pre-existing solution is the opti-
mal assignment for a new inference problem. These
conditions are summarized in Table 2. In all cases,
if the condition matches, the theorems guarantee that
the two solutions will be the same. That is, applying
the theorems will not change the performance of the
underlying inference procedure. Only the number of
inference calls will be decreased.
In our implementation of the conditions, we used
a database1 to cache ILPs and implemented the
retrieval of equivalence classes and solutions as
queries to the database. To implement theorem 1,
we iterate over all ILPs in the equivalence class and
check if the condition is satisfied for one of them.
The conditions of theorems 2 and 3 check whether a
collection of linear (in)equalities has a feasible solu-
tion using a linear program solver.
We optimize the wall-clock time of theorems 2
and 3 by making two observations. First, we do not
need to solve linear programs for all possible ob-
served structures. Given an objective vector, we only
need consider the highest scoring structures within
an equivalence class. (All other structures cannot
be the solution to the ILP.) Second, since theorem
2 checks whether an ILP lies within a cone, we can
optimize the cache for theorem 2 by only storing the
ILPs that form on the boundary of the cone. A sim-
ilar optimization can be performed for theorem 3 as
well. Our implementation uses the following weaker
version of this optimization: while caching ILPs, we
1We used the H2 database engine, which can be downloaded
from http://h2database.com, for all caching.
1119
do not add an instance to the cache if it already satis-
fies the theorem. This optimization reduces the size
of the linear programs used to check feasibility.
3.4 Approximation schemes
So far, in the above three theorems, we retain the
guarantees (in terms of exactness and performance)
of the underlying inference procedure. Now, we will
look at schemes for approximate inference. Unlike
the three theorems listed above, with the following
amortized inference schemes, we are not guaranteed
an optimal solution.
3.4.1 Most frequent solution
The first scheme for approximation uses the ob-
servation that the most frequent solution occurs an
overwhelmingly large number of times, compared to
the others. (See the discussion in section 2.2 and fig-
ures 3a, 3b and 3c for part-of-speech tagging.) Un-
der this approximation scheme, given an ILP prob-
lem, we simply pick the most frequent solution for
that equivalence class as the solution, provided this
solution has been seen a sufficient number of times.
If the support available in the cache is insufficient,
we call the underlying inference procedure.
3.4.2 Top-K approximation
The previous scheme for approximate amortized
inference is agnostic to the objective coefficients of
integer linear program to be solved and uses only
its equivalence class to find a candidate structure.
The top-K approach extends this by scoring the K
most frequent solutions using the objective coeffi-
cients and selecting the highest scoring one as the
solution to the ILP problem. As with the previous
scheme, we only consider solutions that have suffi-
cient support.
3.4.3 Approximations to theorems 1 and 3
The next approximate inference schemes relaxes
the conditions in theorems 1 and 3 by allowing the
inequalities to be violated by . That is, the inequal-
ity (3) from Theorem 1 now becomes
(2yp,i ? 1)?ci +  ? 0. (6)
The inequality (5) from Theorem 3 is similarly re-
laxed as follows:
(2yp,i ? 1)?ci +  ? 0 (7)
3.5 Amortized inference algorithm
Each exact and approximate inference approach de-
scribed above specifies a condition to check whether
an inference procedure should be called for a
new problem. This gives us the following meta-
algorithm for amortized inference, parameterized by
the actual scheme used: If the given input instance p
satisfies the condition specified by the scheme, then
use the cached solution. Otherwise, call the infer-
ence procedure and cache the solution for future use.
4 Experiments
In this section, we apply the theory from Section 3 to
the structure prediction problem of semantic role la-
beling. Since the inference schemes presented above
are independent of the learning aspects, we use an
off-the-shelf implementation and merely modify the
inference as discussed in Section 3.5.
The goal of the experiments is to show that us-
ing an amortized inference algorithm, we can make
fewer calls to the underlying inference procedure.
For the exact inference algorithms, doing so will not
change the performance as compared to the under-
lying system. For the approximations, we can make
a trade-off between the inference time and perfor-
mance.
4.1 Experimental setup
Our goal is to simulate a long-running NLP process
that can use a cache of already solved problems to
improve inference time. Given a new input problem,
our theorems require us to find all elements in the
equivalence class of that problem along with their
solutions. Intuitively, we expect a higher probability
of finding members of an arbitrary equivalence class
if the size of the cache is large. Hence, we processed
sentences from the Gigaword corpus and cached the
inference problems for our task.
The wall-clock time is strongly dependent on such
specific implementation of the components, which
are independent of the main contributions of this
work. Also, in most interesting applications, the
computation time for each step will be typically
dominated by the number of inference steps, espe-
cially with efficient implementations of caching and
retrieval. Hence, the number of calls to the underly-
ing procedure is the appropriate complexity param-
1120
eter. Let NBase be the number of times we would
need to call the underlying inference procedure had
we not used an amortized algorithm. (This is the
same as the number of inference problems.) Let NA
be the number of times the underlying inference pro-
cedure is actually called using an amortized algo-
rithm A. We define the speedup of A as
Speedup(A) =
NBase
NA
. (8)
We also report the clock speedup of our implemen-
tation for all algorithms, which is the ratio of the
wall-clock time taken by the baseline algorithm to
that of the amortized algorithm. For measuring time,
we only measure the time for inference as the other
aspects (feature extraction, scoring, etc.) are not
changed.
4.2 Semantic Role Labeling
The goal of Semantic Role Labeling (SRL) (Palmer
et al 2010) is to identify and assign semantic roles
to arguments of verb predicates in a sentence. For
example, consider the the sentence John gave the
ball to Mary. The verb give takes three arguments,
John, the ball and to Mary, which are labeled A0,
A1 and A2 respectively.
We used the system of (Punyakanok et al 2008)
as our base SRL system. It consists of two classi-
fiers trained on the Propbank corpus. The first one,
called the argument identifier, filters argument can-
didates which are generated using a syntactic parse-
based heuristic. The second model scores each can-
didate that has not been filtered for all possible argu-
ment labels. The scores for all candidates of a pred-
icate are combined via inference. As in the system
of (Punyakanok et al 2008), the softmax function
is applied to the raw classifier scores to ensure that
they are in the same numeric range.
Inference mandates that certain structural and
linguistic constraints hold over the full predicate-
argument structure for a verb. (Punyakanok et al
2008) modeled inference via an integer linear pro-
gram instance, where each assignment of labels
to candidates corresponds to one decision variable.
Given a set of argument candidates, the feasible set
of decisions is dependent of the number of argument
candidates and the verb predicate. Thus, in terms
of the notation used in this paper, the equivalence
classes are defined by the pair (predicate, number of
argument candidates).
We ran the semantic role labeler on 225,000 verb
predicates from the Gigaword corpus and cached
the equivalence classes, objective coefficients and
solutions generated by the SRL system. We re-
port speedup for the various amortized inference
schemes on the standard Penn Treebank test set. On
this data, the unaltered baseline system, processes
5127 integer linear programs and achieves an F1 of
75.85%.
Table 3 shows the speedup and performance for
the various inference schemes. The most frequent
and top-K systems are both naive solutions that take
advantage of the cache of stored problems. In spite
of their simplicity, they attain F1 scores of 62%
and 70.06% because few structures occur most fre-
quently, as described in section 2.2. We see that all
the exact theorems attain a speedup higher than two
without losing performance. (The variation in F1 be-
tween them is because of the existence of different
equivalent solutions in terms of the objective value.)
This shows us that we can achieve an amortized gain
in inference. Note that a speedup of 2.5 indicates
that the solver is called only for 40% of the exam-
ples. The approximate versions of theorems 1 and 3
(with  = 0.3 in both cases, which was not tuned)
attain an even higher gain in speedup over the base-
line than the base versions of the theorems. Interest-
ingly, the SRL performance in both cases does not
decline much even though the conditions of the the-
orems may be violated.
5 Related work and Future directions
In recent years, we have seen several approaches to
speeding up inference using ideas like using the cut-
ting plane approach (Riedel, 2009), dual decompo-
sition and Lagrangian relaxation (Rush et al 2010;
Chang and Collins, 2011). The key difference be-
tween these and the work in this paper is that all
these approaches solve one instance at a time. Since
we can use any inference procedure as a underlying
system, the speedup reported in this paper is appli-
cable to all these algorithms.
Decomposed amortized inference In this paper,
we have taken advantage of redundancy of struc-
tures that can lead to the re-use of solutions. In the
1121
Type Algorithm # instances # solver Speedup Clock F1
calls speedup
Exact Baseline 5127 5217 1.0 1.0 75.85
Exact Theorem 1 5127 2134 2.44 1.54 75.90
Exact Theorem 2 5127 2390 2.18 1.14 75.79
Exact Theorem 3 5127 2089 2.50 1.36 75.77
Approx. Most frequent (Support = 50) 5127 2812 1.86 1.57 62.00
Approx. Top-10 solutions (Support = 50) 5127 2812 1.86 1.58 70.06
Approx. Theorem 1 (approx,  = 0.3) 5127 1634 3.19 1.81 75.76
Approx. Theorem 3 (approx,  = 0.3) 5127 1607 3.25 1.50 75.46
Table 3: Speedup and performance for various inference methods for the task of Semantic Role Labeling.
All the exact inference algorithms get a speedup higher than two. The speedup of the approximate version
of the theorems is even higher without loss of performance. The clock speedup is defined as the ratio of the
inference times of the baseline and the given algorithm. All numbers are averaged over ten trials.
part of speech example, we showed redundancy of
structures at the sentence level (Figure 2a). How-
ever, for part-of-speech tagging, the decisions are
rarely, if at all, dependent on a very large context.
One direction of future work is to take advantage of
the fact that the inference problem can be split into
smaller sub-problems. To support this hypothesis,
we counted the number of occurrences of ngrams
of tokens (including overlapping and repeated men-
tions) for n <= 10 and compared this to the number
of unique part-of-speech ngrams of this length. Fig-
ure 4 shows these two counts. Following the argu-
ment in Section 2.2, this promises a large amortized
gain in inference time. We believe that such decom-
position can also be applied to other, more complex
structured prediction tasks.
The value of approximate inference From the
experiments, we see that the first two approximate
inference schemes (most frequent solution and the
top-K scheme) can speed up inference with the
only computational cost being the check for pre-
conditions of the exact theorems. Effectively, these
algorithms have parameters (i.e., the support param-
eter) that allow us to choose between the inference
time and performance. Figure 5 shows the perfor-
mance of the most frequent and top-K baselines for
different values of the support parameter, which in-
dicates how often a structure must occur for it to be
considered. We see that for lower values of support,
we can get a very high speedup but pay with poorer
performance.
 0
 1e+06
 2e+06
 3e+06
 4e+06
 5e+06
 6e+06
 7e+06
 0  2  4  6  8  10
Number of tokens
Part-of-speech ngram statistics, using tagged Gigaword text
Number of instances for size Number of unique structures for given length
Figure 4: The red line shows the number of ngrams
of tokens (including overlapping and repeated oc-
currences) in the Gigaword corpus and the blue line
shows the number of unique POS tag sequences.
However, the prediction of the approximate al-
gorithms can be used to warm-start any solver that
can accept an external initialization. Warm-starting
a solver can give a way to get the exact solution and
yet take advantage of the frequency of structures that
have been observed.
Lifted inference The idea of amortizing inference
time over the dataset is conceptually related to the
idea of lifted inference (de Salvo Braz et al 2005).
We abstract many instances into equivalence classes
and deal with the inference problem with respect to
the equivalence classes in the same way as done in
lifted inference algorithms.
1122
 1
 1.5
 2
 2.5
 3
 3.5
 0  200  400  600  800  1000 50
 55
 60
 65
 70
 75
 80
Support
Performance of the most frequent and top-K schemes for different values of support
SpeedupPerformance of most frequent solution (F1)Performance of top-K solution (F1)
Figure 5: Most frequent solutions and top-K:
Speedup and SRL performance (F1) for different
values of the support parameter, using the most-
frequent solutions (dashed blue line) and the top-
K scheme (thick gray line). Support indicates how
many times a structure should be seen for it to be
considered. Note that the speedup values for both
schemes are identical (red line).
6 Conclusion
In this paper, we addressed structured prediction in
the context of NLP and proposed an approach to im-
prove inference costs over an entire dataset, rather
than individual instances. By treating inference
problems as instances of integer linear programs, we
proposed three exact theorems which identify exam-
ples for which the inference procedure need not be
called at all and previous solutions can be re-used
with the guarantee of optimality. In addition, we
also proposed several approximate algorithms. We
applied our algorithms, which are agnostic to the
actual tasks, to the problem semantic role labeling,
showing significant decrease in the number of infer-
ence calls without any loss in performance. While
the approach suggested in this paper is evaluated in
semantic role labeling, it is generally applicable to
any NLP task that deals with structured prediction.
Acknowledgements
The authors wish to thank Sariel Har-Peled and the members
of the Cognitive Computation Group at the University of Illi-
nois for insightful discussions and the anonymous reviewers for
their valuable feedback. This research is sponsored by the Army
Research Laboratory (ARL) under agreement W911NF-09-2-
0053. The authors also gratefully acknowledge the support
of the Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research Labo-
ratory (AFRL) prime contract no. FA8750-09-C-0181. This
work is also supported by the Intelligence Advanced Research
Projects Activity (IARPA) Foresight and Understanding from
Scientific Exposition (FUSE) Program via Department of In-
terior National Business Center contract number D11PC2015.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not
necessarily reflect the view of ARL, DARPA, AFRL, IARPA,
or the US government.
References
Y-W. Chang and M. Collins. 2011. Exact decoding
of phrase-based translation models through lagrangian
relaxation. EMNLP.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In ACL.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In EMNLP.
R. de Salvo Braz, E. Amir, and D. Roth. 2005. Lifted
first-order probabilistic inference. In IJCAI.
D Graff and C. Cieri. 2003. English gigaword.
A. Martins, N. A. Smith, and E. Xing. 2009. Concise
integer linear programming formulations for depen-
dency parsing. In ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In EMNLP, pages 523?530, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
M. Palmer, D. Gildea, and N. Xue. 2010. Semantic Role
Labeling, volume 3. Morgan & Claypool Publishers.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling. In
IJCAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP.
S. Riedel. 2009. Cutting Plane MAP Inference for
Markov Logic. Machine Learning.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, CoNLL.
1123
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research.
1124
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1234?1244, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning-based Multi-Sieve Co-reference Resolution with Knowledge?
Lev Ratinov
Google Inc.?
ratinov@google.com
Dan Roth
University of Illinois at Urbana-Champaign
danr@illinois.edu
Abstract
We explore the interplay of knowledge and
structure in co-reference resolution. To inject
knowledge, we use a state-of-the-art system
which cross-links (or ?grounds?) expressions
in free text to Wikipedia. We explore ways
of using the resulting grounding to boost the
performance of a state-of-the-art co-reference
resolution system. To maximize the utility of
the injected knowledge, we deploy a learning-
based multi-sieve approach and develop novel
entity-based features. Our end system outper-
forms the state-of-the-art baseline by 2 B3 F1
points on non-transcript portion of the ACE
2004 dataset.
1 Introduction
Co-reference resolution is the task of grouping men-
tions to entities. For example, consider the text snip-
pet in Fig. 11. The correct output groups the men-
tions {m1,m2,m5} to one entity while leaving m3
?We thank Nicholas Rizzolo and Kai Wei Chang for their
invaluable help with modifying the baseline co-reference sys-
tem. We thank the anonymous EMNLP reviewers for con-
structive comments. This research was supported by the Army
Research Laboratory (ARL) under agreement W911NF-09-2-
0053 and by the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, conclusions or recommendations are
those of the authors and do not necessarily reflect the view of
the ARL, DARPA, AFRL, or the US government.
? The majority of this work was done while the first author
was at the University of Illinois.
1Throughout this paper, curly brackets {} denote the extent
and square brackets [] denote the head.
?After the {[vessel]}m1 suffered a catastrophic torpedo
detonation, {[Kursk]}m2 sank in the waters of {[Barents
Sea]}m3 with all hands lost. Though rescue attempts were
offered by a nearby {Norwegian [ship]}m4 , Russia declined
initial rescue offers, and all 118 sailors and officers aboard
{[Kursk]}m5 perished.?
Figure 1: Example illustrating the challenges in co-reference
resolution.
and m4 as singletons. Resolving co-reference is fun-
damental for understanding natural language. For
example in Fig. 1, to infer that Kusrk has suffered
a torpedo detonation, we have to understand that
{[vessel]}m1 refers to {[Kursk]}m2.
This inference is typically trivial for humans, but
proves extremely challenging for state-of-the-art co-
reference resolution systems. We believe that it is
world knowledge that gives people the ability to un-
derstand text with such ease. A human reader can in-
fer that since Kursk sank, it must be a vessel and ves-
sels which suffer catastrophic torpedo detonations
can sink. Moreover, some readers might just know
that Kursk is a Russian submarine named after the
city of Kursk, where the largest tank battle in his-
tory took place in 1943. In this work we are using
Wikipedia as a source of encyclopedic knowledge.
The key contributions of this work are:
(1) Using Wikipedia to assign a set of knowledge
attributes to mentions in a context-sensitive way.
For example, for the text in Fig. 1, our system as-
signs to the mention ?Kursk? the nationalities: Rus-
sian, Soviet and the attributes ship, incident, subma-
rine, shipwreck (as opposed to city or battle). We
are using a publicly available system for context-
1234
sensitive disambiguation to Wikipedia. We then
extract attributes from the cross-linked Wikipedia
pages (described in Sec. 3.1), assign these attributes
to the document mentions (Sec. 3.2) and develop
knowledge-rich compatibility metric between men-
tions (Sec. 3.3)2.
(2) Integrating the strength of rule-based systems
such as (Haghighi and Klein, 2009; Raghunathan et
al., 2010) into a machine learning framework. We
are using a multi-sieve approach (Raghunathan et
al., 2010), which splits pairwise ?co-reference? vs.
?non-coreference? decisions to different types and
attempts to make the easy decisions first (Goldberg
and Elhadad, 2010). Our multi-sieve approach is
different from (Raghunathan et al 2010) in sev-
eral respects: (a) our sieves are machine-learning
classifiers, (b) the same pair of mentions can fall
into multiple sieves, (c) later sieves can override
the decisions made by earlier sieves, allowing to re-
cover from errors as additional evidence becomes
available. In our running example, the decision
of whether {[vessel]}m1 refers to {[Kursk]}m2 is
made before the decision of whether {[vessel]}m1
refers to {Norwegian [ship]}m4 since decisions in
the same sentence are believed to be easier than
cross-sentence ones. We describe our learning-
based multi-sieve approach in Sec. 4.
(3) A novel approach for entity-based features. As
sieves of classifiers are applied, our system attempts
to model entities and share the attributes between the
mentions belonging to the same entity. Once the de-
cision that {[vessel]}m1 and {[Kursk]}m2 co-refer is
made, we want the two mentions to share the Rus-
sian nationality. This allows us to avoid erroneously
linking {[vessel]}m1 to {Norwegian [ship]}m4 de-
spite vessel and ship being synonyms in Word-
Net. However, in this work we allow the sieves to
make conflicting decisions on the same pair of men-
tions. Hence, obtaining entities and their attributes
by straightforward transitive closure of co-reference
predictions is impossible. We describe our approach
for leveraging possibly contradicting predictions in
Sec. 5.
(4) By adding word-knowledge features and us-
2The extracted attributes and the related re-
sources are available for public download at
http://cogcomp.cs.illinois.edu/Data/
Ace2004CorefWikiAttributes.zip
Input: document d; mentions M = {m1, . . . ,mN}
1) For each mi ? M , assign it a Wikipedia page pi in a
context-sensitive way (pi may be null).
- If pi 6= null: extract knowledge attributes from pi and
assign to m.
- Else extract knowledge attributes directly from m via
noun-phrase parsing techniques (Vadas and Curran, 2008).
3) Let Q = {(mi,mj)}i6=j , be the queue of mention
pairs approximately sorted by ?easy-first? (Goldberg and
Elhadad, 2010).
4) Let G be a partial clustering graph.
5) While Q is not empty
- Extract a pair p = (mi,mj) from Q.
- Using the knowledge attributes of mi and mj as well as
the structure of G, classify whether p is co-referent.
- Update G with the classification decision.
6) Construct an end clustering from G.
Figure 2: High-level system architecture.
ing learning-based multi-sieve approach, we im-
prove the performance of the state-of-the-art system
of (Bengtson and Roth, 2008) by 3 MUC, 2 B3 and
2 CEAF F1 points on the non-transcript portion of
the ACE 2004 dataset. We report our experimen-
tal results in Sec. 6 and conclude with discussion in
Sec. 7.
We conclude the introduction by giving a high-
level overview of our system in Fig. 2.
2 Baseline System
In this work, we are using the state-of-the-art sys-
tem of (Bengtson and Roth, 2008), which relies
on a pairwise scoring function pc to assign an or-
dered pair of mentions a probability that they are
coreferential. It uses a rich set of features includ-
ing: string edit distance, gender match, whether the
mentions appear in the same sentence, whether the
heads are synonyms in WordNet etc. The function
pc is modeled using regularized averaged percep-
tron for a tuned number of training rounds, learn-
ing rate and margin. For the end system, we keep
these parameters intact, our only modifications will
be adding knowledge-rich features and adding inter-
mediate classification sieves to the training and the
inference, which we will discuss in the following
sections.
At inference time, given a document d and a
pairwise co-reference scoring function pc, (Bengt-
son and Roth, 2008) generate a graph Gd accord-
1235
ing to the Best-Link decision model (Ng and Cardie,
2002) as follows. For each mention m in docu-
ment d, let Bm be the set of mentions appearing
before m in d. Let a be the highest scoring an-
tecedent: a = argmaxb?Bm(pc(b,m)). We will add
the edge (a,m) to Gd if pc(a,m) predicts the pair to
be co-referent with a confidence exceeding a chosen
threshold, then we take the transitive closure3.
The properties of the Best-Link inference are il-
lustrated in Fig. 3. At this stage, we ask the reader
to ignore the knowledge attributes at the bottom of
the figure. Let us assume that the pairwise classi-
fier labeled the mentions (m2,m5) co-referent be-
cause they have identical surface form; mentions
(m1,m4) are co-referred because the heads are syn-
onyms in WordNet. Let us assume that since m1
and m2 appear in the same sentence, the pairwise
classifier managed to leverage the dependency parse
tree to correctly co-ref the pair (m1,m2). The tran-
sitive closure would correctly link (m1,m5) despite
the incorrect prediction of the pairwise classifier on
(m1,m5), and would incorrectly link m4 with all
other mentions because of the incorrect pairwise
prediction on (m1,m4) and despite the correct pre-
dictions on (m2,m4) and (m4,m5).
Figure 3: A sample output of a pairwise co-reference classifier.
The full edges represent a co-ref prediction and the empty edges
represent a non-coref prediction. A set of knowledge attributes
for selected mentions is shown as well.
3 Wikipedia as Knowledge
In this section we describe our methodology for us-
ing Wikipedia as a knowledge resource. In Sec. 3.1
we cover the process of knowledge extraction from
3We use Platt Scaling while (Bengtson and Roth, 2008) used
the raw output value of pc.
Wikipedia pages. We describe how to inject this
knowledge into mentions in Sec. 3.2. The bottom
part of Fig. 3 illustrates the knowledge attributes our
system injects to two sample mentions at this stage.
Finally, in Sec. 3.3 we describe a compatibility met-
ric our system learns over the injected knowledge.
3.1 Wikipedia Knowledge Attributes
Our goal in this section is to extract from Wikipedia
pages a compact and highly-accurate set of knowl-
edge attributes, which nevertheless possesses dis-
criminative power for co-reference4. We concentrate
on three types of knowledge attributes: fine-grained
semantic categories, gender information and nation-
ality where applicable.
Each Wikipedia page is assigned a set of cat-
egories. There are over 100K categories in
Wikipedia, many are extremely fine-grained and
contain very few pages. The value of the Wikipedia
category structure for knowledge acquisition has
long been noticed in several influential works, such
as (Suchanek et al 2007; Nastase and Strube, 2008)
to name a few. However, while the recall of the
above resources is excellent, we found their preci-
sion insufficient for our purposes. We have imple-
mented a simple high-precision low-recall heuris-
tic for extracting the head words of Wikipedia cat-
egories as follows.
We noticed that Wikipedia categories have a sim-
ple structure of either <noun-phrase> or <noun-
phrase><relation-token><noun-phrase>, where
in the second case the category information is al-
ways on the left. Therefore, we first remove the
text succeeding a set of carefully chosen relation to-
kens5. With this heuristic ?Recipients of the Gold
Medal of the Royal Astronomical Society? becomes
just ?Recipients?; ?Populated places in Africa? be-
comes ?places?; however ?Institute for Advanced
Study faculty? becomes ?Institute? (rather than
?faculty?). At the second step, we apply the Illi-
nois POS tagger and keep only the tokens labeled as
NNS. This step allows us to exclude singular nouns
incorrectly identified as heads, such as ?Institute?
above. To further reduce the noise in the category
4We justify the reasons for our choice of high-precision low-
recall knowledge extraction in Sec. 3.2.
5The selected set was: {of, in, with, from, ?,?, at, who,
which, for, and, by}
1236
extraction, we also remove all rare category tokens
which appeared in less than 100 titles ending up with
2088 fine-grained entity types. We manually map
popular fine-grained categories to coarser-grained
ones, more consistent with ACE entity typing. A
sample of the mapping is shown in the table below:
Fine-grained Coarse-grained
departments, organizations, banks, . . . ORG
venues, trails, areas, buildings, . . . LOC
countries, towns, villages, . . . GPE
churches, highways, schools, . . . FACILITY
Manual inspection of the extracted category key-
words has led us to believe that this heuristic
achieves a higher precision at a considerable loss
of recall when compared to the more sophisticated
approach of (Nastase and Strube, 2008), which
correctly identifies ?faculty? as the head of ?Insti-
tute for Advanced Study faculty?, but incorrectly
identifies ?statistical organizations? as the head of
?Presidents of statistical organizations? in about
half the titles containing the category6.
We assign gender to the titles using the follow-
ing simple heuristic. The first paragraph of each
Wikipedia article provides a very brief summary
of the entity in focus. If the first paragraph of a
Wikipedia page contains the pronoun ?she?, but not
?he?, the article is considered to be about a female
(and vice-versa). However, when the page is as-
signed a non-person-related fine-grained NE type
(e.g. school) and at the same time is not assigned
a person-related fine-grained NE type (e.g. novel-
ist), we mark the page as inanimate regardless of the
presence of he/she pronouns. The nationality is as-
signed by matching the tokens in the original (un-
processed) categories of the Wikipedia page to a list
of countries. We assign nationality not only to the
Wikipedia titles, but also to single tokens. For each
token, we track the list of titles it appears in, and if
the union of the nationalities assigned to the titles it
appears in is less than 7, we mark the token compat-
ible with these nationalities. This allows us to iden-
tify Ivan Lebedev as Russian and Ronen Ben-Zohar
as Israeli, even though Wikipedia may not contain
pages for these specific people.
6 (Nastase and Strube, 2008) analyze a set of categories S
assigned to Wikipedia page p jointly, hence the same category
expression can be interpreted differently, depending on S.
3.2 Injecting Knowledge Attributes
Once we have extracted the knowledge attributes of
Wikipedia pages, we need to inject them into the
mentions. (Rahman and Ng, 2011) used YAGO for
similar purposes, but noticed that knowledge injec-
tion is often noisy. Therefore they used YAGO only
for mention pairs where one mention was an NE
of type PER/LOC/ORG and the other was a com-
mon noun. This implies that all MISC NEs were
discarded, and all NE-NE pairs were discarded as
well. We also note that (Rahman and Ng, 2011)
reports low utility of FrameNet-based features. In
fact, when incrementally added to other features in
cluster-ranking model the FrameNet-based features
sometimes led to performance drops. This observa-
tion has motivated our choice of high-precision low-
recall heuristic in Sec. 3.1 and will motivate us to
add features conservatively when building attribute
compatibility metric in Sec. 3.3.
Additionally, while (Rahman and Ng, 2011) uses
the union of all possible meanings a mention may
have in Wikipedia, we deploy GLOW (Ratinov et
al., 2011)7, a context-sensitive system for disam-
biguation to Wikipedia. Using context-sensitive dis-
ambiguation to Wikipedia as well as high-precision
set of knowledge attributes allows us to inject the
knowledge to more mention pairs when compared
to (Rahman and Ng, 2011). Our exact heuristic for
injecting knowledge attributes to mentions is as fol-
lows:
Named Entities with Wikipedia Disambiguation
If the mention head is an NE matched to a Wikipedia
page p by GLOW, we import all the knowledge at-
tributes from p. GLOW allows us to map ?Ephraim
Sneh? to http://en.wikipedia.org/wiki/Efraim Sneh
and to assign it the Israeli nationality, male gender,
and the fine-grained entity types: {member, politi-
cian, person, minister, alumnus, physician, gen-
eral}.
Head and Extent Keywords
If the mention head is not mapped to Wikipedia by
GLOW and the head contains keywords which ap-
pear in the list of 2088 fine-grained entity types,
then the rightmost such keyword is added to the list
of mention knowledge attributes. If the head does
7Available at: http://cogcomp.cs.illinois.
edu/page/software_view/Wikifier
1237
not contain any entity-type keywords but the extent
does, we add the rightmost such keyword of the ex-
tent. In both cases, we apply the heuristic of re-
moving clauses starting with a select set of punctua-
tions, prepositions and pronouns, annotating what is
left with POS tagger and restricting to noun tokens
only8. This allows us to inject knowledge to men-
tions unmapped to Wikipedia, such as: ?{current
Cycle World publisher [Larry Little]}?, which is as-
signed the attribute publisher but not world or cy-
cle. Likewise, ?{[Joseph Conrad Parkhurst], who
founded the motorcycle magazine Cycle World in
1962 }?, is not assigned the attribute magazine,
since the text following ?who? is discarded.
3.3 Learning Attributes Compatibility
In the previous section we have assigned knowledge
attributes to the mentions. Some of this information,
such as gender and coarse-grained entity types are
also modeled in the baseline system of (Bengtson
and Roth, 2008). Our goal is to build a compatibility
metric on top of this redundant, yet often inconsis-
tent information.
The majority of the features we are using are
straightforward, such as: (1) whether the two men-
tions mapped to the same Wikipedia page, (2)
gender agreement (both Wikipedia and dictionary-
based), (3) nationality agreement (here we measure
only whether the sets intersect, since mentions can
have multiple nationalities in the real world), (4)
coarse-grained entity type match, etc.
The only non-trivial feature is measuring com-
patibility between sets of fine-grained entity types,
which we describe below. Let us assume that men-
tion m1 was assigned the set of fine-grained entity
types S1 and the mention m2 was assigned the set
of fine-grained entity types S2. We record whether
S1 and S2 share elements. If they do, than, in addi-
tion to the Boolean feature, the list of the shared el-
ements also appears as a list of discrete features. We
do the same for the most similar and most dissimilar
elements of S1 and S2 (along with their discretized
similarity score) according to a WordNet-based sim-
ilarity metric of (Do et al 2009). The reason for ex-
plicitly listing the shared, the most similar and dis-
8This heuristic is similar to the one we used for extracting
Wikipedia category headwords and seems to be a reasonable
baseline for parsing noun structures (Vadas and Curran, 2008).
similar elements is that the WordNet similarity does
not always correspond to co-reference compatibil-
ity. For example, the pair (company, rival) has a
low similarity score according to WordNet, but char-
acterizes co-referent mentions. On the other hand,
the pair (city, region) has a high WordNet simi-
larity score, but characterizes non-coreferent men-
tions. We want to allow our system to ?memorize?
the discrepancy between the WordNet similarity and
co-reference compatibility of specific pairs.
We also note that we generate a set of selected
conjunctive features, most notably of fine-grained
categories with NER predictions. The reason is
that the pair of mentions ?(Microsoft, Google)? are
not co-referent despite the fact that they both have
the company attribute. On the other hand ?(Mi-
crosoft, Redmond-based company)? is a co-referent
pair. To capture this difference, we generate the
feature ORG-ORG&&share attribute for the first
pair, and ORG-O&&share attribute for the second
pair9. These features are also used in conjunction
with string edit distance. Therefore, if our system
sees two named entities which share the same fine-
grained type but have a large string edit distance, it
will label the pair as non-coref.
4 Learning-based Multi-Sieve Aproach
State-of-the-art machine-learning co-ref systems,
e.g. (Bengtson and Roth, 2008; Rahman and Ng,
2011) train a single model for predicting co-
reference of all mention pairs. However, rule-based
systems, e.g. (Haghighi and Klein, 2009; Raghu-
nathan et al 2010) characterize mention pairs by
discourse structure and linguistic properties and ap-
ply rules in a prescribed order (high-precision rules
first). Somewhat surprisingly, such hybrid approach
of applying rules on top of structures produced by
statistical tools (such as dependency parse trees) per-
forms better than pure machine-learning approach10.
In this work, we attempt to integrate the strength
of linguistically motivated rule-based systems with
the robustness of a machine learning approach. We
started with a hypothesis that different types of men-
9The head of ?Redmond-based company? is ?company?,
which is not a named entity, and is marked O.
10(Raghunathan et al 2010) recorded the best result on
CoNLL 2011 shared task.
1238
tion pairs may require a different co-ref model. For
example, consider the text below:
Queen Rania of Jordan , Egypt?s [Suzanne Mubarak]m1 and
others were using their charisma and influence to campaign
for equality of the sexes. [Mubarak]m2 , wife of Egyptian
President [Hosni Mubarak]m3 , and one of the conference
organizers, said they must find ways to . . .
There is a subtle difference between mention pairs
(m1,m2) and (m2,m3). One of the differences is
purely structural. The first pair appears in different
sentences, while the second pair ? in the same sen-
tence. It turns out that string edit distance feature be-
tween two named entities has different ?semantics?
depending on whether the two mentions appear in
the same sentence. The reason is that to avoid redun-
dancy, humans refer to the same entity differently
within the sentence, preferring titles, nicknames and
pronouns. Therefore, when a similar-looking named
entities appear in the same sentence, they are ac-
tually likely to refer to different entities. On the
other hand, in the sentence ?Reggie Jackson, nick-
named Mr. October . . . ? we have to rely heavily on
sentence structure rather than string edit distance to
make the correct co-ref prediction.
Trained on Sieve-specific
Sieve All Data Training
AllSentencePairs 61.37 67.46
ClosestNonProDiffSent 60.71 63.33
NonProSameSentence 62.97 63.80
NerMentionsDiffSent 86.44 87.12
SameSentenceOneNer 64.10 68.88
Adjacent 71.00 78.80
SameSenBothNer 75.30 73.75
Nested 76.11 79.00
Table 1: F1 performance on co-referent mention pairs by sieve
type when trained with all data versus sieve-specific data only.
Our second intuition is that ?easy-first? inference
is necessary to effectively leverage knowledge. For
example, in Fig. 3, our goal is to link vessel to
Kursk and assign it the Russian/Soviet nationality
prior to applying the pairwise co-reference classi-
fier on (vessel, Norwegian ship). Therefore, our
goal is to apply the pairwise classifier on pairs in
prescribed order and to propagate the knowledge
across mentions. The ordering should be such that
(a) maximum amount of information is injected at
early stages (b) the precision at the early stages is as
high as possible (Raghunathan et al 2010). Hence,
we divide the mention pairs as follows:
Nested: are pairs such as ?{{[city]m1} of [Jerusalem]m2}?
where the extent of one of the mentions contains the extent of
the other. For some mentions, the extent is the entire clause, so
we also added a requirement that mention heads are at most 7
tokens apart. Intuitively, it is the easiest case of co-reference.
There are 5,804 training samples and 992 testing samples, out
of which 208 are co-referent.
SameSenBothNer: are pairs of named entities which appear
in the same sentence. We already saw an example for this
case involving [Mubarak]m2 and [Hosni Mubarak]m3. There
are 13,041 training samples and 1,746 testing samples, out of
which 86 are co-referent.
Adjacent: are pairs of mentions which appear closest to each
other on the dependency tree. We note that most of the nested
pairs are also adjacent. There are training 5,872 samples and
895 testing samples, out of which 219 are co-referent.
SameSentenceOneNer: are pairs which appear in the same
sentence and exactly one of the mentions is a named entity, and
the other is not a pronoun. Typical pairs are ?Israel-country?,
as opposed to ?Bill Clinton - reporter?. This type of pairs is
fairly difficult, but our hope is to use encyclopedic knowledge
to boost the performance. There are 15,715 training samples
and 2,635 testing samples, out of which 207 are co-referent.
NerMentionsDiffSent: are pairs of mentions in different sen-
tences, both of which are named entities. There are 189,807
training samples and 24,342 testing samples, out of which 1,628
are co-referent.
NonProSameSentence: are pairs in the same sentence, where
both mentions are non-pronouns. This sieve includes all the
pairs in the SameSentenceOneNer sieve. Typical pairs are
?city-capital? and ?reporter-celebrity?. There are 33,895
training samples and 5,393 testing samples, out of which 336
are co-referent..
ClosestNonProDiffSent: are pairs of mentions in different sen-
tences with no other mentions between the two. 3,707 train-
ing samples and 488 testing samples, out of which 38 are co-
referent.
AllSentencePairs: All mention pairs within same sentence.
There are 49,953 training samples and 7,809 testing samples,
out of which 846 are co-referent.
TopSieve: The set of mention pairs classified by the baseline
system. 525,398 training samples and 85,358 testing samples,
out of which 1,387 are co-referent.
In Tab. 1 we compare the performance at each
sieve in two scenarios11. First, we train with the en-
tire 525,398 training samples, and then we train on
11The data is described in Sec. 6.1.
1239
whatever training data is available for the specific
sieve12. We were surprised to see that the F1 on the
nested mentions, when trained on the 5,804 sieve-
specific samples improves to 79.00 versus 76.11
when trained on the 525,398 top sieve samples.
There are several things to note when interpreting
the results in Tab 1. First, the sheer ratio of positive
to negative samples fluctuates drastically. For exam-
ple, 208 out of the 992 testing samples at the nested
sieve are positive, while there are only 86 positive
samples out of 1,746 testing samples in the Same-
SenBothNer sieve. It seems unreasonable to use the
same model for inference at both sieves. Second, the
data for intermediate sieves is not always a subset of
the top sieve. The reason is that top sieve extracts
a positive instance only for the closest co-referent
mentions, while sieves such as AllSentencePairs ex-
tract samples for all co-referent pairs which appear
in the same sentence. Third, while our division to
sieves may resemble witchcraft, it is motivated by
the intuition that mentions appearing close to one
another are easier instances of co-ref as well as lin-
guistic insights of (Raghunathan et al 2010).
5 Entity-Based Features
In this section we describe our approach for build-
ing entity-based features. Let {C1, C2, . . . CN} be
the set of sieve-specific classifiers. In our case, C1 is
the nested mention pairs classifier, C2 is the Same-
SenBothNer classifier, and C9 is the top sieve clas-
sifier. We design entity-based features so that the
subsequent sieves ?see? the decisions of the previ-
ous sieves and use entity-based features based on the
intermediate clustering. However, unlike (Raghu-
nathan et al 2010), we allow the subsequent sieves
to change the decisions made by the lower sieves
(since additional information becomes available).
5.1 Intermediate Clustering Features (IC)
Let Ri(m) be the set of all mentions which, when
paired with the mention m, form valid sample pairs
for sieve i. E.g. in our running example of Fig. 1,
12We report pairwise performance on mention pairs because
it is the more natural metric for the intermediate sieves. We
report only performance on co-referent pairs, because for many
sieves, such as the top sieve, 99% of the mention pairs are non-
coreferent, hence the baseline of labeling all samples as non-
coreferent would result in 99% accuracy. We are interested in a
more challenging baseline, the co-referent pairs.
R2([Kursk]m2) = {[Barents Sea]m3}, since both
m1 and m2 are NEs and appear in the same sen-
tence. Let R+i (m) be the set of all mentions which
were labeled as co-referent to the mention m by the
classifier Ci (including m, which is co-referent to
itself). We define R?i (m) similarly. We denote the
union of mentions co-refed to m during inference
up to sieve i as E+i (m) = ?
i?1
j=1R
+
j (m). Similarly,
E?i (m) = ?
i?1
j=1R
?
j (m). Using these definitions
we can introduce entity-based prediction features
which allow subsequent sieves to use information
aggregated from previous sieves:
ICRi (mj ,mk) =
?
?
?
?1 mj ? R?i?1(mk)
+1 mj ? R+i?1(mk)
0 Otherwise
ICEi (mj ,mk) =
?
?
?
?1 mj ? E?i?1(mk)
+1 mj ? E+i?1(mk)
0 Otherwise
ICRi stores the pairwise prediction history, thus
when classifying a pair (mj ,mk) at sieve i, a
classifier can see the predictions of all the previous
sieves applicable on that pair. ICEi stores the
transitive closures of the sieve-specific predictions.
We note that both ICRi and IC
E
i can have the values
+1 and -1 active at the same time if intermediate
sieve classifiers generated conflicting predictions.
However, a classifier at sieve i will use as features
both ICR1 ,. . . IC
R
i?1 and IC
E
1 ,. . . IC
E
i?1, thus it
will know the lowest sieve at which the conflicting
evidence occurs. The classifier at sieve i also
uses set identity, set containment, set overlap and
other set comparison features between E+/?i?1 (mj)
and E+/?i?1 (mk). We check whether the sets have
symmetric difference, whether the size of the
intersection between the two sets is at least half
the size of the smallest set etc. We also generate
subtypes of set comparison features when restricting
the elements to NE-mentions and non-pronominal
mentions (e.g ?what percent of named entities do
the sets have in common??).
5.2 Surface Form Compatibility (SFC)
The intermediate clustering features do not allow us
to generalize predictions from pairs of mentions to
pairs of surface strings. For example, if we have
three mentions: {[vessel]m1 , [Kursk]m2 , [Kursk]m5},
then the prediction on the pair (m1,m2) will not be
1240
(B)aseline (B)+Knowledge (B)+Predictions (B)+Knowledge+Predictions
TopSieve 66.58 69.08 68.77 70.43
AllSentencePairs 67.46 71.79 69.59 73.50
ClosestNonProDiffSent 63.33 65.62 65.57 70.76
NonProSameSentence 63.80 69.62 67.03 71.11
NerMentionsDiffSent 87.12 88.23 88.68 89.07
SameSentenceOneNer 68.88 70.58 67.89 73.17
Adjacent 78.80 81.32 80.00 81.79
SameSenBothNer 73.75 80.50 77.21 80.98
Nested 79.00 83.59 80.65 83.37
Table 2: Utility of knowledge and prediction features (F1 on co-referent mention pairs) by inference sieves. Both knowledge and
entity-based features significantly and independently improve the performance for all sieves. The goal of entity-based features is
to propagate knowledge effectively, thus it is encouraging that the combination of entity-based and knowledge features performed
significantly better than any of the approaches individually at the top sieve.
used for the prediction on the pair (m1,m5), even
though in both pairs we are asking whether Kursk
can be referred to as vessel. The surface form com-
patibility features mirror the intermediate clustering
features, but relax mention IDs and replace them
by surface forms. Similarly to intermediate cluster-
ing features, both +1 and -1 values can be active at
the same time. We also generate subtypes of set-
comparison features for NE-mentions and optionally
stemmed non-pronominal mentions. For example,
in a text discussing President Clinton and President
Putin, some instances of the surface from president
will refer to Putin but not Clinton and vice-versa.
Therefore, both for (Putin, president) and for (Clin-
ton, president), the surface from compatibility will
be +1 and -1 simultaneously. This indicates to the
system that Putin can be referred to as president, but
president can refer to other entities in the document
as well.
6 Experiments and Results
6.1 Data
We use the official ACE 2004 English training
data (NIST, 2004). We started with the data split
used in (Culotta et al 2007), which used 336 doc-
uments for training and 107 documents for testing.
We note that ACE data contains both newswire text
and transcripts. In this work, we are using NLP tools
such as POS tagger, named entity recognizer, shal-
low parser, and a disambiguation to Wikipedia sys-
tem to inject expressive features into a co-reference
system.
Unfortunately, current state-of-the-art NLP tools
do not work well on transcribed text. Therefore, we
discard all the transcripts. Our criteria was simple.
The ACE annotators have marked the named enti-
ties both in newswire and in the transcripts. We kept
only those documents which contained named en-
tities (according to manual ACE annotation) and at
least 1/3 of the named entities started with a capital
letter. After this pre-processing step, we were left
with 275 out of the original 336 training documents,
and 42 out of the 107 testing documents.
For the experiments throughout this paper, fol-
lowing Culotta et al(Culotta et al 2007) and much
other work, to make experiments more compara-
ble across systems, we assume that perfect mention
boundaries and mention type labels are given. How-
ever, we do not use the gold named entity types such
as person/location/facility etc. available in the data.
In all experiments we automatically split words and
sentences, and annotate the text with part-of-speech
tags, named entities and cross-link concepts from
the text to Wikipedia using publicly available tools.
6.2 Ablation Study
In Tab. 2 we report the pairwise F1 scores on co-
referent mention pairs broken down by sieve and
using different components. This allows us to see,
for example, that adding only the knowledge at-
tributes improved the performance at NonProSame-
Sentence sieve from 63.80 to 69.62. We have or-
dered the sieves according to our initial intuition of
?easy first?. We were surprised to see that co-ref res-
olution for named entities in the same sentence was
harder than cross-sentence (73.75 vs. 87.12 base-
1241
 75
 76
 77
 78
 79
 80
 81
 82
0.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0
F1
 - M
UC
Confidence threshold for a positive prediction
Baseline
Knowledge&Predictions
KnowledgeOnly
PredictionsOnly
 81.5
 82
 82.5
 83
 83.5
 84
 84.5
 85
 85.5
0.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0
F1
 - B
3
Confidence threshold for a positive prediction
Baseline
Knowledge&Predictions
KnowledgeOnly
PredictionsOnly
 73
 74
 75
 76
 77
 78
 79
 80
0.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0
F1
 - C
EA
F
Confidence threshold for a positive prediction
Baseline
Knowledge&Predictions
KnowledgeOnly
PredictionsOnly
Figure 4: End performance for various systems.
line F1). We were also surprised to see that resolv-
ing all mention pairs within sentence when includ-
ing pronouns was easier than resolving pairs where
both mentions were non-pronouns (67.46 vs. 63.80
baseline F1).
We note that conceptually, the nested
(B)+Predictions sieve should be identical to
the baseline. However, in practice, the surface
form compatibility (SFC) features are generated
for the nested sieve as well. Given two mentions
m1 and m2, the SFC features capture how many
surface forms E+(m1) and E+(m2) share. At the
nested sieve, E+(m) and R+(m) are just m, which
is identical to string comparison features already
existing in the baseline system. While the SFC
features do not add new information, they influence
the weight the features get (essentially leading to
a different regularization), which in turn leads to
slightly different results.
6.3 End system performance
Recall that the Best-Link algorithm applies transi-
tive closure on the graph generated by thresholding
the pairwise co-reference scoring function pc. The
lower the threshold on the positive prediction, the
lower is the precision and the higher is the recall. In
Fig. 4 we compare the end clustering quality across
a variety of thresholds and for various system fla-
vors using three metrics: MUC (Vilain et al 1995),
B3 (Bagga and Baldwin, 1998) and CEAF (Luo,
2005)13. The purpose of this comparison is to see
the impact of the knowledge and the prediction fea-
tures on the final output and to see whether the per-
formance gains are due to (mis-)tuning of one of
the systems or are they consistent across a variety
of thresholds.
The end performance of the baseline system
on our training/testing split peaks at around 78.39
MUC, 83.03 B3 and 77.52 CEAF, which is higher
(e.g. 3 B3 F1 points) than the originally reported
result on the entire dataset (which includes the tran-
scripts). This is expected, since well-formed text is
easier to process than transcripts. We note that our
baseline is a state-of-the art system which recorded
the highest B3 and BLANC scores at CoNLL 2011
shared task and took the third place overall. Fig. 4
shows a minimum improvement of 3 MUC, 2 B3
and 1.25 CEAF F1 points across all thresholds when
comparing the baseline to our end system. Surpris-
ingly, the knowledge features outperformed predic-
tion features on pairwise, MUC and B3 metrics, but
not on the CEAF metric. This shows that pairwise
performance is not always indicative of cluster-level
performance for all metrics.
7 Conclusions and Related Work
To illustrate the strengths of our approach, let us
consider the following text:
Another terminal was made available in {[Jiangxi]m1}, an
{inland [province]m2}. . . . The previous situation whereby
large amount of goods for {Jiangxi [province]m3} had to
be re-shipped through Guangzhou and Shanghai will be
changed completely.
The baseline system assigns each mention to a
separate cluster. The pairs (m1,m2) and (m1,m3)
13In the interest of space, we refer the reader to the literature
for details about the different metrics.
1242
are misclassified because the baseline classifier does
not know that Jiangxi is a province and the preposi-
tion an before m2 is interpreted to mean it is a pre-
viously unmentioned entity. The pair (m2,m3) is
misclassified because identical heads have different
modifiers, as in (big province, small province). Our
end system first co-refs (m1,m2) at the AllSameSen-
tence sieve due to the knowledge features, and then
co-refs (m1,m3) at the top sieve due to surface form
compatibility features indicating that province was
observed to refer to Jiangxi in the document. The
transitivity of Best-Link takes care of (m2,m3).
However, our approach has multiple limitations.
Entity-based features currently do not propagate
knowledge attributes directly, but through aggregat-
ing pairwise predictions at knowledge-infused inter-
mediate sieves. We rely on gold mention bound-
aries and exhaustive gold co-reference annotation.
This prevented us from applying our approach to
the Ontonotes dataset where singleton clusters and
co-referent nested mentions are removed. There-
fore the gold annotation for training several sieves
of our scheme is missing (e.g. nested mentions).
Another limitation is our somewhat preliminary di-
vision to sieves. (Vilalta and Rish, 2003) have ex-
perimented with approaches for automatic decom-
position of data to subclasses and learning multiple
models to improve data separability. We hope that
similar approach would be useful for co-reference
resolution. Ideally, we want to make ?simple de-
cisions? first, similarly to what was done in (Gold-
berg and Elhadad, 2010) for dependency parsing,
and model clustering as a structured problem, sim-
ilarly to (Joachims et al 2009; Wick et al 2011).
However, our experience with multi-sieve approach
with classifiers suggests that a single model would
not perform well for both lower sieves with little
entity-based information and higher sieves with a lot
of entity-based features. Addressing the aforemen-
tioned challenges is a subject for future work.
There has been an increasing interest in
knowledge-rich co-reference resolution (Ponzetto
and Strube, 2006; Haghighi and Klein, 2010; Rah-
man and Ng, 2011). We use Wikipedia differently
from (Ponzetto and Strube, 2006) who focus on
using WikiRelate, a Wikipedia-based relatedness
metric (Strube and Ponzetto, 2006). (Rahman and
Ng, 2011) used the union of all possible inter-
pretations a mention may have in YAGO, which
means that Michael Jordan could be co-refed both
to a scientist and basketball player in the same
document. Additionally, (Rahman and Ng, 2011)
use exact word matching, relying on YAGO?s ability
to extract a comprehensive set of facts offline14. We
are the first to use context-sensitive disambiguation
to Wikipedia, which received a lot of attention re-
cently (Bunescu and Pasca, 2006; Cucerzan, 2007;
Mihalcea and Csomai, 2007; Milne and Witten,
2008; Ratinov et al 2011). We extract context-
sensitive, high-precision knowledge attributes from
Wikipedia pages and apply (among other features)
WordNet similarity metric on pairs of knowledge
attributes to determine attribute compatibility.
We have integrated the strengths of rule-based
systems such as (Haghighi and Klein, 2009; Raghu-
nathan et al 2010) into a multi-sieve machine learn-
ing framework. We show that training sieve-specific
models significantly increases the performance on
most intermediate sievesieves.
We develop a novel approach for entity-based in-
ference. Unlike (Rahman and Ng, 2011) who con-
struct entities left-to-right, and similarly to (Raghu-
nathan et al 2010) we resolve easy instances of co-
ref to reduce error propagation in entity-based fea-
tures. Unlike (Raghunathan et al 2010), we al-
low later stages of inference to change the decisions
made at lower stages as additional entity-based evi-
dence becomes available.
By adding word-knowledge features and refin-
ing the inference, we improve the performance of a
state-of-the-art system of (Bengtson and Roth, 2008)
by 3 MUC, 2 B3 and 2 CEAF F1 points on the non-
transcript portion of the ACE 2004 dataset.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In MUC7.
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
14YAGO uses WordNet to expand its set of facts. For ex-
ample, if Martha Stewart is assigned the meaning personality
from category head words analysis, YAGO adds the meaning
celebrity because personality is a direct hyponym of celebrity in
WordNet. However, this is done offline in a context-insensitive
way, which is inherently limited.
1243
R. C. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
EACL.
S. Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In EMNLP-
CoNLL.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In HLT/NAACL, pages 81?88.
Q. Do, D. Roth, M. Sammons, Y. Tu, and V. Vydiswaran.
2009. Robust, light-weight approaches to compute
lexical similarity. Technical report, University of Illi-
nois at Urbana-Champaign.
A. Fader, S. Soderland, and O. Etzioni. 2009. Scaling
wikipedia-based named entity disambiguation to arbi-
trary web text. In WikiAI (IJCAI workshop).
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In NAACL.
A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In EMNLP.
A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In HLT-ACL. As-
sociation for Computational Linguistics.
T. Joachims, T. Hofmann, Y. Yue, and C. Yu. 2009.
Predicting structured objects with support vector ma-
chines. Communications of the ACM, Research High-
light, 52(11):97?104, November.
X. Luo. 2005. On coreference resolution performance
metrics. In HLT.
R. Mihalcea and A. Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In CIKM.
D. Milne and I. H. Witten. 2008. Learning to link with
wikipedia. In CIKM.
V. Nastase and M. Strube. 2008. Decoding wikipedia
categories for knowledge acquisition. In AAAI.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In ACL.
NIST. 2004. The ace evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
S. P. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, wordnet and wikipedia for coreference
resolution. In HLT-ACL.
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning. 2010.
A multi-pass sieve for coreference resolution. In
EMNLP.
A. Rahman and V. Ng. 2011. Coreference resolution
with world knowledge. In HLT-ACL.
L. Ratinov, D. Downey, M. Anderson, and D. Roth.
2011. Local and global algorithms for disambiguation
to wikipedia. In ACL.
M. Strube and S. P. Ponzetto. 2006. WikiRelate! Com-
puting Semantic Relatedness Using Wikipedia. In
Proceedings of the Twenty-First National Conference
on Artificial Intelligence, July.
F. M. Suchanek, G. Kasneci, and G. Weikum. 2007.
Yago: A core of semantic knowledge. In WWW.
D. Vadas and J. R. Curran. 2008. Parsing noun phrase
structure with CCG. In ACL.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In MUC6, pages 45?52.
R. Vilalta and I. Rish. 2003. A decomposition of classes
via clustering to explain and improve naive bayes. In
ECML.
M. Wick, K. Rohanimanesh, K. Bellare, A. Culotta, and
A. McCallum. 2011. Samplerank: Training factor
graphs with atomic gradients. In ICML.
1244
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1511?1521, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Discriminative Model for Query Spelling Correction with Latent
Structural SVM
Huizhong Duan, Yanen Li, ChengXiang Zhai and Dan Roth
University of Illinois at Urbana-Champaign
201 N Goodwin Ave
Urbana, IL 61801
{duan9, yanenli2, czhai, danr}@illinois.edu
Abstract
Discriminative training in query spelling cor-
rection is difficult due to the complex inter-
nal structures of the data. Recent work on
query spelling correction suggests a two stage
approach a noisy channel model that is used
to retrieve a number of candidate corrections,
followed by discriminatively trained ranker
applied to these candidates. The ranker, how-
ever, suffers from the fact the low recall of the
first, suboptimal, search stage.
This paper proposes to directly optimize the
search stage with a discriminative model
based on latent structural SVM. In this model,
we treat query spelling correction as a multi-
class classification problem with structured in-
put and output. The latent structural informa-
tion is used to model the alignment of words
in the spelling correction process. Experiment
results show that as a standalone speller, our
model outperforms all the baseline systems. It
also attains a higher recall compared with the
noisy channel model, and can therefore serve
as a better filtering stage when combined with
a ranker.
1 Introduction
Query spelling correction has become a crucial com-
ponent in modern information systems. Particularly,
search engine users rely heavily on the query cor-
rection mechanism to formulate effective queries.
Given a user query q, which is potentially mis-
spelled, the goal of query spelling correction is to
find a correction of the query c that could lead to a
better search experience. A typical query spelling
correction system employs a noisy channel model
(Kernighan et al 1990). The model assumes that
the correct query c is formed in the user?s mind be-
fore entering the noisy channels, e.g., typing, and
get misspelled. Formally, the model maximizes the
posterior probability p(c|q):
c? = arg max
c
p(c|q). (1)
Applying Bayes rule, the formulation can be
rewritten as:
c? = arg max
c
p(q|c)p(c)
= arg max
c
[log p(q|c) + log p(c)].
(2)
The model uses two probabilities. The prior prob-
ability p(c) represents how likely it is that c is the
original correct query in the user?s mind. The prob-
ability is usually modeled by a language model es-
timated from a sizable corpus. The transformation
probability p(q|c) measures how likely it is that q is
the output given that c has been formed by the user.
This probability can be either heuristic-based (edit
distance) or learned from samples of well aligned
corrections. One problem with the noisy channel
model is that there is no weighting for the two kinds
of probabilities, and since they are estimated from
different sources, there are usually issues regarding
their scale and comparability, resulting in subopti-
mal performance (Gao et al 2010). Another limita-
tion of this generative model is that it is not able to
take advantage of additional useful features.
1511
A discriminative model may solve these problems
by adding the flexibility of using features and apply-
ing weights. But training such a model is not easy.
The difficulty is that the output space of query cor-
rection is enormous, as the candidate corrections for
each a query term could be the entire vocabulary.
This is even worse when word boundary errors (i.e.
merging and splitting of words) exist. The problem
is intractable with standard discriminative models as
we cannot enumerate every candidate correction.
To solve the problem, (Gao et al 2010) proposed
a two stage approach. In this approach, a ranker is
trained to score each candidate correction of a query.
When a query is issued, the system first uses the
noisy channel model with a standard search algo-
rithm to find the 20 best candidates. Then the ranker
is used to re-rank these candidates and find the best
correction for the query. This ranker based system
has one critical limitation, though. Since the ranking
stage is decoupled from the search, it relies on the
outsourced search algorithm to find the candidates.
Because query spelling correction is an online oper-
ation, only a small number of candidates can enter
the ranker due to efficiency concerns, thus limiting
the ability of the ranker to the ceiling of recall set by
the suboptimal search phase.
The research question we address here is whether
we can directly optimize the search phase of query
spelling correction using a discriminative model
without loss of efficiency. More specifically, we
want 1) a learning process that is aware of the
search phase and interacts with its result; 2) an ef-
ficient search algorithm that is able to incorporate
the learned model and guide the search to the target
spelling correction.
In this paper, we propose a new discriminative
model for query correction that maintains the ad-
vantage of a discriminative model in accommodat-
ing flexible combination of features and naturally in-
corporates an efficient search algorithm in learning
and inference. Similarly to (Chang et al 2010) we
collapse a two stage process into a single discrim-
inatively trained process, by considering the output
of the first stage as an intermediate latent represen-
tation for the joint learning process. Specifically, we
make use of the latent structural SVM (LS-SVM)
(Yu and Joachims, 2009) formulation. We formu-
late the problem query spelling correction as a multi-
class classification problem on structured inputs and
outputs. The advantage of the structural SVM model
is that it allows task specific, customizable solutions
for the inference problem. This allows us to adapt
the model to make it work directly with the search
algorithm we use for finding the best correction of
the query. To account for word boundary errors, we
model the word alignment between the query and
the correction as a latent structural variable. The
LS-SVM model allows us to jointly search over the
output space and the latent structure space.
As the inference algorithm in the proposed dis-
criminative model we use an algorithm that resem-
bles a traditional noisy channel model. To adapt
the LS-SVM model to enable the efficient search of
query spelling correction, we study how features can
be designed. We analyze the properties of features
that can be used in the search algorithm and propose
a criteria for selecting and designing new features.
We demonstrate the use of the criteria by design-
ing separate features for different types of spelling
errors (e.g. splitting, merging). With the proposed
discriminative model, we can directly optimize the
search phase of query spelling correction without
loss of efficiency. Our model can be used not only as
a standalone speller with high accuracy, but also as
a high recall candidate generation stage for a ranker
based system.
Experiments verify the effectiveness of the dis-
criminative model, as the accuracy of correction can
be improved significantly over baseline systems in-
cluding an award winning query spelling system.
Even though the optimization is primarily based on
the top correction, the weights trained by LS-SVM
can be used to search for more candidate corrections.
The improvement in recall at different levels over the
noisy channel model demonstrates that our model is
superior even when used in the two-stage approach..
2 Related Work
Spelling correction has a long history (Levenshtein,
1966). Traditional techniques were on small scale
and depended on having a small trusted lexicons
(Kukich, 1992). Later, statistical generative mod-
els were shown to be effective in spelling correc-
tion, where a source language model and an er-
ror model were identified as two major components
1512
(Brill and Moore, 2000). Note that we are not deal-
ing here with the standard models in context sen-
sitive spelling (Golding and Roth, 1999) where the
set of candidate correction is a known ?confusion
set?. Query spelling correction, a special form of
the problem, has received much attention in recent
years. Compared with traditional spelling correc-
tion task, query spelling deals with more complex
types of misspellings and a much larger scale of lan-
guage. Research in this direction includes utiliz-
ing large web corpora and query log (Chen et al
2007; Cucerzan and Brill, 2004; Ahmad and Kon-
drak, 2005), employing large-scale n-gram models,
training phrase-based error model from clickthrough
data (Sun et al 2010) and developing additional fea-
tures (Gao et al 2010).
Query alteration/refinement is a very relevant
topic to query spelling correction. The goal of
query alteration/refinement is to modify the inef-
fective query so that it could . Researches on this
track include query expansion (Xu and Croft, 1996;
Qiu and Frei, 1993; Mitra et al 1998), query con-
traction(Kumaran and Allan, 2008; Bendersky and
Croft, 2008; Kumaran and Carvalho, 2009) and
other types of query reformulations for bridging the
vocabulary gap (Wang and Zhai, 2008). (Guo et al
2008) proposed a unified model to perform a broad
set of query refinements including correction, seg-
mentation and stemming. However, it has very lim-
ited ability in query correction. In this paper, we
study the discriminative training of query spelling
correction, which is potentially beneficial to many
existing studies.
Noisy channel model (or source channel model)
has been widely used in NLP. Many approaches have
been proposed to perform discriminative training of
the model (McCallum et al 2000; Lafferty, 2001).
However, these approaches mostly deal with a rela-
tively small search space where the number of can-
didates at each step is limited (e.g. POS tagging). A
typically used search algorithm is dynamic program-
ming. In spelling correction, however, the search
space is much bigger and the existing approaches
featuring dynamic programming are difficult to be
applied.
Structural learning and latent structural learning
has been studied a lot in NLP in recent years(Chang
et al 2010; Dyer et al 2011), and has been
shown to be useful in a range of NLP applications
from Textual Entailment, Paraphrasing and Translit-
eration (Chang et al 2010) to sentiment analysis
(Yessenalina et al 2010).
Work has also been done on integrating discrimi-
native learning in search. Freitag and Khadivi used a
perceptron algorithm to train for sequence alignment
problem. A beam search algorithm was utilized in
the search (Freitag and Khadivi, 2007). Daume et
al. proposed the Searn framework for search based
structural prediction (Daume et al 2009). Our
model differs from the Searn framework in that it
learns to make global decisions rather than accumu-
lating local decisions. The global decision was made
possible by an efficient search algorithm.
Query spelling correction also shares many sim-
ilarities with statistical machine translation (SMT).
Sun et al(2010) has formulated the problem within
an SMT framework. However, SMT usually in-
volves more complex alignments, while in query
spelling correction search is the more challenging
part. Our main contribution in this paper is a novel
unified way to directly optimize the search phase of
query spelling correction with the use of LS-SVM.
3 Discriminative Model for Query Spelling
Correction Based on LS-SVM
In this section, we first present the discriminative
formulation of the problem of query spelling correc-
tion. Then we introduce in detail the model we use
for solving the problem.
3.1 The Discriminative Form of Query Spelling
Correction
In query spelling correction, given a user entered
query q, which is potentially misspelled, the goal is
to find a correction c, such that it could be a more
effective query which improves the quality of search
results. A general discriminative formulation of the
problem is of the following form:
f(q) = arg max
c?V?
[w ??(q, c)], (3)
where ?(q, c) is a vector of features and w is the
model parameter. This discriminative formulation is
more general compared to the noisy channel model.
It has the flexibility of using features and applying
1513
weights. The noisy channel model is a special case
of the discriminative form where only two features,
the source probability and the transformation proba-
bility, are used and uniform weightings are applied.
However, this problem formulation does not give us
much insight on how to proceed to design the model.
Especially, it is unclear how ?(q, c) can be com-
puted.
To enhance the formulation, we explore the fact
that spelling correction follows a word-by-word pro-
cedure. Let us first consider a scenario where word
boundary errors does not exist. In this scenario,
each query term matches and only matches to a sin-
gle term in the correction. Formally, let us denote
q = q1, ..., qn and c = c1, ..., cm as structured ob-
jects from the space of V?, where V is our vocabu-
lary of words and V? is all possible phrases formed
by words in V . Both q and c have an intrinsic se-
quential structure. When no word boundary error
exists, |c| = |q| holds for any candidate correction
c. qi and ci establish a one-to-one mapping. In this
case, we have a more detailed discriminative form:
f(q) = arg max
c?V|q|
[w ? (?0 +
|q|?
i=1
?1(qi, ci))], (4)
where ?0 is a vector of normalizing factors,
?1(qi, ci) is the decomposed computation of ?(q, c)
for each query term qi and ci, for i = 1 to |q|.
Equation 4 is a clearer formulation. The major
challenge of solving this discriminative problem is
the complexity. Theoretically, each term has |V|
candidates and it is impossible to enumerate over
all possible combinations. To make it even worse,
merging and splitting errors are quite common in
misspelling. As a result, the assumption of one-to-
one mapping does not hold in practice.
To account for these word boundary errors and
enhance the discriminative formulation, we intro-
duce a latent variable a to model the unobserved
structural information. More specifically, a =
a1, a2, ...a|a| is the alignment between q and c. Each
alignment node at is a represented by a quadruple
(qstart, qend, cstart, cend). Figure 1 shows a com-
mon merge error and its best alignment. The phrase
?credit card?, in this case, is incorrectly merged into
one word ?creditcard? by the user. Figure 2 shows
Figure 1: Example of Merge Error and Alignment
Figure 2: Example of Split Error and Alignment
the best alignment for a common split error, where
the word ?gamespot? is incorrectly split into a two
word phrase ?game spot?.
Taking into consideration the latent variable, we
arrive at our final discriminative form of query
spelling correction:
f(q) = arg max(c,a)?Vn?A[w ??(q, c, a)]
= arg max(c,a)?V??A[w ? (?0
+
?|a|
t=0 ?1(qat , cat , at))],
(5)
The challenges of successfully applying a dis-
criminative model to this problem formulation are
1) how can we design a learning algorithm to learn
the model parameter w to directly optimize the max-
imization problem; 2) how can we solve the maxi-
mization efficiently without having to enumerate all
candidates; 3) how can we design features to guar-
antee the correctness of the search algorithm. In the
following subsections we introduce our solutions to
the three challenges in detail.
3.2 Latent Structural SVM
We employ the latent structural SVM (LS-SVM)
model for learning the discriminative model of query
spelling correction. LS-SVM is a large margin
method that deals with structured prediction prob-
lems with latent structural information (Yu and
Joachims, 2009). LS-SVM has the merit of allowing
1514
task specific, customizable solutions for the infer-
ence problem. This makes it easy to adapt to learn-
ing the model parameters for different problems.
The following is a brief introduction of LS-SVM
that largely mirrors the work by (Yu and Joachims,
2009).
Without loss of generality, let us aim at learning
a prediction function f : X ? Y that maps input
x ? X to an output y ? Y with latent structural
information h ? H. The decision function is of the
following form:
f(x) = arg max
(y,h)?Y?H
[w ??(x, y, h)], (6)
where ?(x, y, h) is the set of feature functions de-
fined jointly over the input x, the output y and the
latent variable h. w is the parameter of the model.
Given a set of training examples that consist of input
and output pairs {(x1, y1), ...(xn, yn)} ? (X ?Y)n,
the LS-SVM method solves the following optimiza-
tion problem:
minw
1
2
?w?2
+C
n?
i=1
max
(y?,h?)?Y?H
[w ??(xi, y?, h?) + ?(yi, y?)]
?C
n?
i=1
max
h?H
[w ??(xi, yi, h)],
(7)
where ?(yi, y?) is the loss function for the ith ex-
ample. The details of the derivation is omitted in
this paper. Readers who are interested can read more
from (Yu and Joachims, 2009).
There are two maximization problems that are es-
sential in Equation 7. The first one is the loss aug-
mented decision function:
max
(y?,h?)?Y?H
[w ??(xi, y?, h?) + ?(yi, y?)], (8)
and the second is the inference of latent variable
given the label of the training data:
max
h?H
[w ??(xi, yi, h)]. (9)
The Latent Structural SVM framework does not
specify how the maximization problems in Equation
8 and Equation 9 are solved, as well as the infer-
ence problem in 6. These maximization problems
are task dependent. Being able to efficiently solve
them is the key to successfully applying the Latent
Structural SVM method. We will show in detail how
we solve these maximization problems to make LS-
SVM work for query spelling correction in the fol-
lowing subsection.
For training the LS-SVM model, a Concave-
Convex Procedure (CCCP) was proposed to solve
this optimization problem (Yu and Joachims, 2009).
The method resembles the Expect-Maximization
(EM) training method as it updates the model by it-
eratively recomputing the latent variable. However,
rather than performing ?sum-product? training as in
EM where a distribution over the hidden variable is
maintained, the CCCP method used for LS-SVM is
more similar to the ?max-product? paradigm where
we ?guess? the best hidden variable in each iteration,
except here we ?guess? by minimizing a regularized
loss function instead of maximizing the likelihood.
3.3 Solving the Inference Problems
The essential inference problem is to find the correc-
tion that maximizes the scoring function according
to the model (i.e., the decision function in Equation
6). For this purpose we design a best first search al-
gorithm similar to the standard search algorithm in
the noisy channel model. The essence of the search
algorithm is to bound the score of each candidate
so that we could evaluate the most promising candi-
dates first. The algorithm is given in Algorithm 1.
Essentially, the algorithm maintains a priority
queue of all search paths. Each time the best path is
de-queued, it is expanded with up to m ? 1 words
in q by searching over a vocabulary trie of up to
m-gram. Each path is represented as a quadruple
(pos, str, sc, a), representing the current term posi-
tion in query, the string of the path, the path?s score
and the alignment so far. The priority queue is sorted
according to the score of each path in descending or-
der. The GetSuggestions() function retrieves the
top n similar words to the given word with a vocab-
ulary trie according to an error model.
Splitting errors are dealt with in Algorithm 1 by
?looking forward? m words in the query when gen-
erating candidate words. Merging errors are ac-
counted for by including up to m-gram in the vocab-
1515
ulary trie. It is worth mentioning that performance
of Algorithm 1 could be further improved by com-
puting heuristic scores for each path.
Algorithm 1: Best First Search Algorithm
Input: Vocabulary Trie V , query q, output size k,
max order m, candidate pool size n
Output: List l of top k corrections for q
1 Initialize List l;
2 Initialize PriorityQueue pq;
3 Enqueue to pq a start path with position set to 0,
string set to empty string, score set to w ??0, and
path alignment set to empty set;
4 while pq is not Empty do
5 Path pi ? pq.Dequeue();
6 if pi.pos < q.terms.length then
7 for i? 0 tom do
8 ph? q.terms[pi.pos+ 1...pi.pos+ i];
9 sug ? GetSuggestions(ph, V, n);
10 foreach s in sug do
11 pos? ? pi.pos+ i;
12 str? ? concat(pi.str, s.str);
13 a? ? pi.a ? s.a;
14 sc? ? pi.sc+w ??1(qs.a, cs.a, s.a);
15 Enqueue pq with the new path
(pos?, str?, sc?, a?);
16 else
17 Add suggestion string pi.str to l;
18 if l.Count > k then return l;
19 return l;
As Algorithm 1 originates from the noisy channel
model, the two known features that work with the
algorithm are log p(c) and log p(q|c) from the noisy
channel model. However, it is unknown whether
other features can work with the search algorithm
and how we can develop new features to ensure it.
After analyzing the properties of the features and the
search algorithm, we find that a feature ? has to sat-
isfy the following monotonicity constraint in order
to be used in Algorithm 1.
Monotonicity Property. Given query q, for
any alignment At = At?1 ? {at} at time t,
?(qAt , cAt , At) ? ?(qAt?1 , cAt?1 , At?1), where
qAt is the concatenation of qa0 to qat and cAt is the
concatenation of ca0 to cat .
That is, the value of the feature (which is com-
puted in an accumulative manner) cannot increase
as the candidate is extended with a new term at
any search step. This ensures that the score of the
best candidate at any search step is guaranteed to be
higher than the score of any future candidates. It
also implies ?t(qat , cat , at) ? 0 for any t ? T . The
monotonicity feature ensures the correctness of Al-
gorithm 1. We show how we design features with
the guidance of the monotonicity constraint in Sec-
tion 4.
The solution to to the loss augmented inference
depends on the loss function we use. In spelling cor-
rection, usually only one correction is valid for an
input query. Therefore, we apply the 0-1 loss to our
model:
?(c, c?) =
{
0 c = c?
1 c 6= c?
(10)
Given this loss function, the loss augmented infer-
ence problem can be solved easily with an algorithm
similar to Algorithm 1. This is done by initializing
the loss to be 1 at the beginning of each search path.
During the search procedure, we check if the loss
decreases to 0 given the correction string so far. If
this is the case, we decreases the score by 1 and add
the path back to the priority queue. More advanced
functions may also be used (Dreyer et al 2006),
which may lead to better training performance. We
plan to further study different loss functions in our
future work.
The inference of the latent alignment variable can
be solved with dynamic programming, as the num-
ber of possible alignments is limited given the query
and the correction.
4 Features
In the following discussions, we will describe how
the features in our discriminative model are devel-
oped under the guidance of the monotonicity con-
straint.
4.1 Source Probability and Transformation
Probability
We know from empirical experience that the source
probability and the transformation probability are
the two most important features in query spelling
correction. We include them in our model in a nor-
malized form. Taking the source probability for ex-
ample, we define the following feature:
1516
?(q, c, a) = ?+
?|a|
1 log p(c)
?
= 1 +
?|a|
1
log p(c)
? ,
(11)
where ? is a normalizing factor computed as:
? = ?|q| log pmin, (12)
where pmin is the smallest probability we use in
practice.
The formula fits the general form we define in 5
in that ?0 = 1 and ?1(qat , cat , at) =
log p(c)
? for any
t = 1 to |a|.
Similarly, we have the follow feature for the trans-
formation probability:
??(q, c, a) = ?+
?|a|
1 log p(q|c)
?
= 1 +
?|a|
1
log p(q|c)
? .
(13)
We use the web Microsoft n-gram model1 to com-
pute source model p(c). We train the unigram trans-
formation model for the transformation probability
p(q|c) according to (Duan and Hsu, 2011).
In generative models, we treat transformation
probabilities from merging and splitting errors in the
same way as single word errors. In our discrimi-
native model we can assign separate weight to the
transformation probabilities resulted from different
types of errors. This allows fine tuning of the query
spelling correction system, making it more adaptive
to environments where the ratio of different types of
errors may vary. Moreover, the model also allows
us to include language models trained over different
resources, such as query log, title of webpages or
anchor texts.
4.2 Local Heuristic Features
Despite the goal of query spelling correction is to
deal with misspellings, in real world most queries
are correctly spelled. A good query spelling correc-
tion system shall prevent as much as possible from
misjudging an correctly spelled query as misspelled.
With this idea in mind, we invent some heuristic
functions to avoid misjudging.
1http://research.microsoft.com/en-
us/collaboration/focus/cs/web-ngram.aspx
Local Heuristic 1. When a query term is matched
against trustable vocabulary, it increases the chance
that the term is already in its correct form. For ex-
ample, we extract a reliable vocabulary from the title
field of Wikipedia2. We therefore design the follow-
ing feature:
?(q, c, a) = 1 +
|a|?
t=1
?1(qat , cat , at), (14)
where ?1(qat , cat , at) is defined as:
?1(qat , cat , at) =
?
?
?
0 qat /? W
0 qat ? W, qat = ct
? 1|q| qat ? W, qat 6= cat
(15)
where W is the vocabulary of Wikipedia titles.
Since |q| > |a| always holds, the feature is normal-
ized between 0 and 1.
Local Heuristic 2. Another heuristic is that
words with numbers in it, despite usually not in-
cluded in any vocabulary, should be treated care-
fully as they tend to be correct words. Such words
could be a model, a serial number or a special en-
tity name. Since the number keys on keyboard are
away from the letter keys, they are more likely to be
intentionally typed in if found in user queries. Simi-
lar to Heuristic 1, we design the following feature to
capture this heuristic:
??(q, c, a) = 1 +
|a|?
t=1
??1(qat , cat , at), (16)
where ??1(qat , cat , at) is defined as:
??1(qat , cat , aat) =
?
?
?
0 [0...9] /? qat
0 [0...9] ? qat , qat = cat
? 1|q| [0...9] ? qat , qat 6= cat
(17)
4.3 Global Heuristic Features
Some global heuristics are also important in query
spelling correction. For instance, the total number
2http://www.wikipedia.org
1517
of words being corrected in the query may be an
indicator of whether the system has leaned towards
overcorrecting. To account for this global heuristic,
we design the following feature:
?(q, c, a) =
{
1 wc(q, c, a) < wcmax
0 otherwise
(18)
where wc(q, c, a) is the number of word changes
at step t, wcmax is the maximum number of word
changes we allow in our system (in a soft way). Sim-
ilarly, other thresholded features can be designed
such as the number of total edit operations. The use
of global features is similar to the use of loss func-
tion in the search algorithm.
5 Experiments
In order to test the effectiveness and efficiency of our
proposed discriminative training method, in this sec-
tion we conduct extensive experiments on two web
query spelling datasets. Below we first present the
dataset and evaluation metrics, followed by the ex-
periment results on query spelling correction.
5.1 Dataset Preparation
The experiments are conducted on two query
spelling correction datasets. One is the TREC
dataset based on the publicly available TREC
queries (2008 Million Query Track). This dataset
contains 5892 queries and the corresponding correc-
tions annotated by the MSR Speller Challenge 3 or-
ganizers. There could be more than one plausible
corrections for a query. In this dataset only 5.3% of
queries are judged as misspelled.
We have also annotated another dataset that con-
tains 4926 MSN queries, where for each query there
is at most one correction. Three experts are involved
in the annotation process. For each query, we con-
sult the speller from two major search engines (i.e.
Google and Bing). If they agree on the returned
results (including the case if the query is just un-
changed), we take it as the corrected form of the in-
put query. If the results are not the same from the
two, as least one human expert will manually anno-
tate the most likely corrected form of the query. Fi-
nally, about 13% of queries are judged as misspelled
3http://web-ngram.research.microsoft.com/spellerchallenge/
in this dataset, which is close to the error rate of real
web queries. We?ve made this dataset publicly avail-
able to all researchers4.
Both the two datasets are split randomly into two
equal subsets for training and testing.
5.2 Evaluation Metrics
We evaluate our system based on the evaluation met-
rics proposed in Microsoft Speller Challenge, in-
cluding expected precision, expected recall and ex-
pected F1 measure.
Let q be a user query and C(q) = (c1, c2, , ck)
be the set of system output with posterior probabil-
ities P (ci|q). Let S(q) denote the set of plausible
spelling variations annotated by the human experts
for q. Expected Precision is computed as:
Precision =
1
|Q|
?
q?Q
?
c?C(q)
Ip(c, q)P (c|q), (19)
where Ip(c, q) = 1 if c ? S(q), and 0 otherwise.
And expected recall is defined as:
Recall =
1
|Q|
?
q?Q
?
a?S(q)
Ir(C(q), a)/|S(q)|, (20)
where Ir(C(q), a) = 1 if a ? C(q) for a ? S(q),
and 0 otherwise. We use R@N to denote recall for
systems limited to output top N corrections.
Expected F1 measure can be computed as:
F1 =
2 ? precision ? recall
precision+ recall
(21)
5.3 Experiment Results
Table 1 compares the performance of our LS-SVM
based model with two strong baseline systems. The
first baseline system is an Echo system which sim-
ply echos the input. The echo system is usually con-
sidered as a strong baseline in query spelling cor-
rection as the majority of the queries are correctly
spelled queries. The second baseline Lueck-2011
we use is a award winning speller system5 (Luec,
2011), which was ranked at the first place in Mi-
crosoft Spelling Challenge 2011.
4http://times.cs.uiuc.edu/duan9/msn speller.tar.gz
5http://www.phraselink.com
1518
Table 1: LSSVM vs Baselines Serving as Standalone Speller
All Queries Misspelled Queries
Dataset Method Precision R@10 F1 Precision R@10 F1
Echo 0.949 0.876 0.911 0 0 0
TREC Lueck-2011 0.963 0.932 0.947 0.391 0.479 0.430
LS-SVM 0.955 0.944 0.949 0.331 0.678? 0.445?
Echo 0.869 0.869 0.869 0 0 0
MSN Lueck-2011 0.896 0.921 0.908 0.334 0.397 0.363
LS-SVM 0.903 0.953 0.928 0.353? 0.662? 0.461?
We show performances for the entire query sets
as well as the query sets consisting only the mis-
spelled queries. As we can see, our system out-
performs both baseline systems on almost all met-
rics, except the precision of Lueck-2011 is better
than ours on TREC dataset. We perform statistical
test and measures where our system shows statisti-
cal significant improvement over both baseline sys-
tems are noted by ?. It is theoretically impossible
to achieve statistical significance in the entire query
set as majority queries have almost identical perfor-
mance in different systems due to the large amount
of correct queries. But our method shows signifi-
cant improvement in the dealing with the misspelled
queries. This experiment verified the effectiveness
of our proposed discriminative model. As a stan-
dalone speller, our system achieves very high accu-
racy.
Despite we are primarily focused on optimizing
the top correction in our discriminative model, we
can also use the trained system to output multiple
candidate corrections. Table 2 compare our system
with the noisy channel model (N-C) in terms of re-
call at different levels of cutoff. For all levels, we see
that our system achieves higher recall than the noisy
channel model. This indicates that when used to-
gether with a secondary ranker, our system serves as
a better filtering method than the unoptimized noisy
channel model. Since the ranker makes use of arbi-
trary features, it has the potential of further improv-
ing the accuracy of query spelling correction. We
plan to further explore this idea as a future work.
In Table 3 we study the effect of treating the trans-
formation probability of merging and splitting er-
rors as separate features and including the local and
global heuristic features (rich features). We see that
Table 2: LS-SVM vs Noisy Channel Model Serving as
Filtering Method
Dataset Method R@5 R@10 R@20
TREC N-C 0.896 0.899 0.901
LS-SVM 0.923 0.944 0.955
MSN N-C 0.870 0.873 0.876
LS-SVM 0.950 0.953 0.960
the precision of query spelling correction can bene-
fits from the use of rich features. However, it does
not result in much improvement in recall. This is
reasonable as the additional features are primarily
designed to improve the accuracy of the top correc-
tion generated by the system. In doing so, it actu-
ally regularizes the ability of the system in retrieving
diversified results. For instance, the global heuris-
tic feature on the number of word change tries to
prevent the system from returning candidates hav-
ing more than a certain number of changed words.
For the TREC collection where more than one cor-
rections can be labeled for a query, this phenomena
is aggravated.
Table 3: LSSVM w/ and w/o Rich Features
Dataset Method Precision R@10 F1
TREC w/o 0.942 0.946 0.944
w/ 0.955 0.944 0.949
MSN w/o 0.898 0.952 0.924
w/ 0.903 0.953 0.928
6 Conclusions
In this paper, we present a novel discriminative
model for query spelling correction. The paper made
the following contributions:
1519
First, to the best of our knowledge, this is a novel
exploration of directly optimizing the search phase
in query spelling correction with a discriminative
model. By modeling word alignment as the latent
structural information, our formulation also deals
with word boundary errors. We propose to use LS-
SVM for learning the discriminative model which
naturally incorporates search in the learning process.
Second, we develop an efficient search algorithm
that solves the inference problems in the LS-SVM
based model. We analyze the criteria for selecting
and designing features to ensure the correctness and
efficiency of the search algorithm. Third, we explore
effective features to improve the accuracy of the
model. Finally, experiments are conducted to verify
the effectiveness of the proposed model. It is shown
that as a standalone speller our system achieves high
accuracy. When used in a two stage approach, it at-
tains higher recall than the noisy channel model and
can thus serve as a superior method for candidate
generation. We also verify that through the use of
rich features, we can further improve the accuracy
of our query spelling correction system.
7 Acknowledgments
This paper is based upon work supported in part by
MIAS, the Multimodal Information Access and Syn-
thesis center at UIUC, part of CCICADA, a DHS
Center of Excellence, and by the National Science
Foundation under grant CNS-1027965, and by a Mi-
crosoft grant.
References
F. Ahmad and G. Kondrak. 2005. Learning a spelling
error model from search query logs. In HLT/EMNLP.
The Association for Computational Linguistics.
M. Bendersky and W. B. Croft. 2008. Discovering key
concepts in verbose queries. In Proceedings of the 31st
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?08. ACM, New York, NY, USA, 491-498.
E. Brill and R. Moore. 2000. An improved error model
for noisy channel spelling correction. In Proceed-
ings of the 38th Annual Meeting of the Association for
Computational Linguistics, Hong Kong.
M. Chang, D. Goldwasser, D. Roth and V. Srikumar.
2010. Discriminative Learning over Constrained La-
tent Representations. In Proceedings of NAACL.
Q. Chen, M. Li, and M. Zhou. 2007. Improving
query spelling correction using web search results. In
EMNLP-CoNLL, pages 181?189.
S. Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of web users. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
H. Daume, J. Langford and D. Marcu. 2009. Search-
based Structured Prediction. Machine Learning Jour-
nal (MLJ).
M. Dreyer, D. Smith and N. Smith. 2006. Vine parsing
and minimum risk reranking for speed and precision.
In Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning. 201-205.
H. Duan and B.-J. P. Hsu. 2011. Online spelling correc-
tion for query completion. In Proceedings of the 20th
international conference on World wide web, WWW
?11, pages 117?126, New York, NY, USA.
C. Dyer, J. H. Clark, A. Lavie, and N. A. Smith. 2011.
Unsupervised Word Alignment with Arbitrary Fea-
tures. In Proceedings of ACL.
D. Freitag, S. Khadivi. 2007. A Sequence Alignment
Model Based on the Averaged Perceptron. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning. 238-247.
J. Gao, X. Li, D. Micol, C. Quirk, and X. Sun. 2010.
A large scale ranker-based system for search query
spelling correction. In COLING, pages 358?366.
A. R. Golding and D. Roth 1999. A Winnow based ap-
proach to Context-Sensitive Spelling Correction. In
Machine Learning, vol 34, pages 107?130.
J. Guo, G. Xu, H. Li, and X. Cheng. 2008. A unified and
discriminative model for query refinement. In Pro-
ceedings of the 31st annual international ACM SIGIR,
SIGIR ?08, pages 379?386, New York, NY, USA.
C. John Yu and T. Joachims. 2009. Learning structural
SVMs with latent variables. In Proceedings of the 26th
Annual International Conference on Machine Learn-
ing (ICML ?09). ACM, New York, NY, USA, 1169-
1176.
M. D. Kernighan , K. W. Church , W. A. Gale. 1990. A
spelling correction program based on a noisy channel
model. In Proceedings of the 13th conference on Com-
putational linguistics. 205-210. August 20-25, 1990,
Helsinki, Finland.
K. Kukich. 1992. Techniques for automatically correct-
ing words in text. ACM computing surveys, 24(4).
G. Kumaran and J. Allan. 2008. Effective and efficient
user interaction for long queries. In Proceedings of
the 31st annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?08. ACM, New York, NY, USA.
1520
G. Kumaran and V. R. Carvalho. 2009. Reducing long
queries using query quality predictors. In Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?09. ACM, New York, NY, USA, 564-571.
J. Lafferty. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning (ICML ?01). 282?289.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. In Soviet
Physics Doklady, 10(8), 707-710.
G. Luec. 2011. A data-driven approach for correcting
search quaries. In Spelling Alteration for Web Search
Workshop.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum Entropy Markov Models for Information Extrac-
tion and Segmentation. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML ?00). 591-598.
M. Mitra, A. Singhal, and C. Buckley. 1998. Improving
automatic query expansion. In Proceedings of the 21st
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?98.
Y. Qiu and H. Frei. 1993. Concept based query expan-
sion. In Proceedings of the 16th annual international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ?93. ACM, New York,
NY, USA, 160-169.
X. Sun, J. Gao, D. Micol, and C. Quirk. 2010. Learning
phrase-based spelling error models from clickthrough
data. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, ACL
?10, pages 266?274, Stroudsburg, PA, USA.
X. Wang, C. Zhai. 2008. Mining Term Association Pat-
terns from Search Logs for Effective Query Reformu-
lation. In Proceedings of the 17th ACM International
Conference on Information and Knowledge Manage-
ment 2008, CIKM?08. 479-488.
J. Xu and W. B. Croft. 1996. Query expansion using
local and global document analysis. In Proceedings of
the 19th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?96. ACM, New York, NY.
A. Yessenalina, Y. Yue, C. Cardie. 2010. Multi-
level Structured Models for Document-level Sentiment
Classification. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP ?10). 10461056.
1521
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 601?612,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Constrained Latent Variable Model for Coreference Resolution
Kai-Wei Chang Rajhans Samdani Dan Roth
University of Illinois at Urbana-Champaign
{kchang10|rsamdan2|danr}@illinois.edu
Abstract
Coreference resolution is a well known clus-
tering task in Natural Language Processing. In
this paper, we describe the Latent Left Linking
model (L3M), a novel, principled, and linguis-
tically motivated latent structured prediction
approach to coreference resolution. We show
that L3M admits efficient inference and can be
augmented with knowledge-based constraints;
we also present a fast stochastic gradient based
learning. Experiments on ACE and Ontonotes
data show that L3M and its constrained ver-
sion, CL3M, are more accurate than several
state-of-the-art approaches as well as some
structured prediction models proposed in the
literature.
1 Introduction
Coreference resolution is a challenging task, that in-
volves identification and clustering of noun phrases
mentions that refer to the same real-world entity.
Most machine learning approaches to coreference
resolution learn a scoring function to estimate the
compatibility between two mentions or two sets of
previously clustered mentions. Then, a decoding al-
gorithm is designed to aggregate these scores and
find an optimal clustering assignment.
The most popular of these frameworks is the pair-
wise mention model (Soon et al, 2001; Ng and
Cardie, 2002; Bengtson and Roth, 2008), which
learns a compatibility score of mention-pairs and
uses these pairwise scores to obtain a global cluster-
ing. Recently, efforts have been made (Haghighi and
Klein, 2010; Rahman and Ng, 2011b; Rahman and
Ng, 2011c) to consider models that capture higher
order interactions, in particular, between mentions
and previously identified entities (that is, between
mentions and clusters). While such models are po-
tentially more expressive, they are largely based on
heuristics to achieve computational tractability.
This paper focuses on a novel and principled ma-
chine learning framework that pushes the state-of-
the-art while operating at a mention-pair granularity.
We present two models ? the Latent Left-Linking
Model (L3M), and a version of that is augmented
with domain knowledge-based constraints, the Con-
strained Latent Left-Linking Model (CL3M). L3M
admits efficient inference, linking each mention to a
previously occurring mention to its left, much like
the existing best-left-link inference models (Ng and
Cardie, 2002; Bengtson and Roth, 2008). How-
ever, unlike previous best-link techniques, learning
in our case is performed jointly with decoding ? we
present a novel latent structural SVM approach, op-
timized using a fast stochastic gradient-based tech-
nique. Furthermore, we present a probabilistic gen-
eralization of L3M that is more expressive in that
it is capable of considering mention-entity interac-
tions using scores at the mention-pair granularity.
We augment this model with a temperature-like pa-
rameter (Samdani et al, 2012) to provide additional
flexibility.
CL3M augments L3M with knowledge-based
constraints following (Roth and Yih, 2004; Denis
and Baldridge, 2007). This capability is very de-
sirable as shown by the success of the rule-based de-
terministic approach of Raghunathan et al (2010)
in the CoNLL shared task 2011 (Pradhan et al,
2011). In L3M, domain-specific constraints are in-
corporated into learning and inference in a straight-
forward way. CL3M scores a mention?s contribution
to its cluster by combining the corresponding score
601
of the underlying L3M model with that from a set of
constraints.
Most importantly, in our experiments on bench-
mark coreference datasets, we show that CL3M,
with just five constraints, compares favorably with
other, more complicated, state-of-the-art algorithms
on a variety of evaluation metrics. Over-
all, the main contribution of this paper is a
principled machine learning model operating at
mention-pair granularity, using easy to implement
constraint-augmented inference and learning, that
yields competitive results on coreference resolution
on Ontonotes-5.0 (Pradhan et al, 2012) and ACE
2004 (NIST, 2004).
2 Related Work
The idea of Latent Left-linking Model (L3M) is in-
spired by a popular inference approach to corefer-
ence which we call the Best-Left-Link approach (Ng
and Cardie, 2002; Bengtson and Roth, 2008). In the
best-left-link strategy, each mention i is connected
to the best antecedent mention j with j < i (i.e. a
mention occurring to the left of i, assuming a left-
to-right reading order), thereby creating a left-link.
The ?best? antecedent mention is the one with the
highest pairwise score, wij ; furthermore, if wij is
below some threshold, say 0, then i is not connected
to any antecedent mention. The final clustering is
a transitive closure of these ?best? links. The intu-
ition behind best-left-link strategy is based on how
humans read and decipher coreference links ? they
mostly rely on information to the left of the men-
tion when deciding whether to add it to a previously
constructed cluster or not. This strategy has been
successful and commonly used in coreference res-
olution (Ng and Cardie, 2002; Bengtson and Roth,
2008; Stoyanov et al, 2009). However, most works
have developed ad-hoc approaches to implement this
idea. For instance, Bengtson and Roth (2008) train
a model w on binary training data generated by tak-
ing for each mention, the closest antecedent corefer-
ent mention as a positive example, and all the other
mentions as negative examples. Similar approaches
to training and, additionally, decoupling the training
stage from the clustering stage were used by other
systems. In this paper, we formalize the learning
problem of the best-left-link model as a structured
prediction problem and analyze our system with de-
tailed experiments. Furthermore, we generalize this
approach by considering multiple pairwise left-links
instead of just the best link, efficiently capturing the
notion of a mention-to-cluster link.
Many techniques in the coreference literature
break away from the mention pair-based, best-left-
link paradigm. Denis and Baldridge (2008) and Ng
(2005) learn a local ranker to rank the mention
pairs based on their compatibility. While these ap-
proaches achieve decent empirical performance, it
is unclear why these are the right ways to train the
model. Some techniques consider a more expres-
sive model by using features defined over mention-
cluster or cluster-cluster (Rahman and Ng, 2011c;
Stoyanov and Eisner, 2012; Haghighi and Klein,
2010). For these models, the inference and learn-
ing algorithms are usually complicated. Very re-
cently, Durrett et al (2013) propose a probabilis-
tic model which enforces structural agreement con-
straints between specified properties of mention
cluster when using a mention-pair model. This ap-
proach is very related to the probabilistic extension
of our method as both models attempt to leverage
entity-level information from mention-pair features.
However, our approach is simpler because it directly
considers the probabilities of multiple links. Fur-
thermore, while their model performs only slightly
better than the Stanford rule-based system (Lee et
al., 2011), we significantly outperform this system.
Most importantly, our model obtains state-of-the-art
performance on OntoNotes-5.0 while still operating
at the mention-pair granularity. We believe that this
is due to our novel and principled structured predic-
tion framework which results in accurate (and effi-
cient) training.
Several structured prediction techniques have
been applied to coreference resolution in the ma-
chine learning literature. For example, McCallum
and Wellner (2003) and Finley and Joachims (2005)
model coreference as a correlational clustering prob-
lem (Bansal et al, 2002) on a complete graph over
the mentions with edge weights given by the pair-
wise classifier. However, correlational clustering is
known to be NP Hard (Bansal et al, 2002); nonethe-
less, an ILP solver or an approximate inference algo-
rithm can be used to solve this problem. Another ap-
proach proposed by Yu and Joachims (2009) formu-
602
lates coreference with latent spanning trees. How-
ever, their approach has no directionality between
mentions, whereas our latent structure captures the
natural left-to-right ordering of mentions. In our
experiments (Sec. 5), we show that our technique
vastly outperforms both the spanning tree and the
correlational clustering techniques. We also com-
pare with (Fernandes et al, 2012) and the pub-
licly available Stanford coreference system (Raghu-
nathan et al, 2010; Lee et al, 2011), a state-of-the-
art rule-based system.
Finally, some research (Ratinov and Roth, 2012;
Bansal and Klein, 2012; Rahman and Ng, 2011a)
has tried to integrate world knowledge from web-
based statistics or knowledge bases into a corefer-
ence system. World knowledge is potentially use-
ful for resolving coreference and can be injected
into our system in a straightforward way via the
constraints framework. We will show an example
of incorporating our system with name-entity and
WordNet-based similarity metric (Q. Do, 2009) in
Sec. 5. Including massive amount of information
from knowledge resources is not the focus of this
paper and may distort the comparison with other
relevant models but our results indicate that this is
doable in our model, and may provide significant
improvements.
3 Latent Left-Linking Model with
Constraints
In this section, we describe our Constrained Latent
Left-Linking Model (CL3M). CL3M is inspired by
a few ideas from the literature: (a) the popular Best-
Left-Link inference approach to coreference (Ng and
Cardie, 2002; Bengtson and Roth, 2008), and (b) the
injection of domain knowledge-based constraints for
structured prediction (Roth and Yih, 2004; Clarke
and Lapata, 2006; Chang et al, 2012b; Ganchev et
al., 2010; Koo et al, 2010; Pascal and Baldridge,
2009).
We first introduce the notion of a pairwise
mention-scorer, then introduce our Left-Linking
Model (L3M), and finally describe how to inject con-
straints into our model.
Let d be a document with md mentions. Mentions
are denoted solely using their indices, ranging from
1 to md. A coreference clustering C for document
d is a collection of disjoint sets partitioning the set
{1, . . . , md}. We represent C as a binary function
with C(i, j) = 1 if mentions i and j are coreferent,
otherwise C(i, j) = 0. Let s(C;w, d) be the score
of a given clustering C for a given document and a
given pairwise weight vector w. Then, during infer-
ence, a clustering C is predicted by maximizing the
scoring function s(C;w, d), over all valid (i.e. sat-
isfying symmetry and transitivity) clustering binary
functions C : {1, . . . , md}?{1, . . . , md} ? {0, 1}.
3.1 Mention Pair Scorer
We model the task of coreference resolution using a
pairwise scorer which indicates the compatibility of
a pair of mentions. The inference routine then pre-
dicts the final clustering ? a structured prediction
problem ? using these pairwise scores.
Specifically, for any two mentions i and j (w.l.o.g.
j < i), we produce a pairwise compatibility score
wji using extracted features ?(j, i) as
wji = w ? ?(j, i) , (1)
where w is a weight parameter that is learned.
3.2 Latent Left-Linking Model
Our inference algorithm is inspired by the best-left-
link approach. In particular, the score s(C; d,w) is
defined so that each mention links to the antecedent
mention (to its left) with the highest score (as long
as the score is above some threshold, say, 0). Specif-
ically:
s(C; d,w) =
md
?
i=1
max
0?j<i,C(i,j)=1
w ? ?(j, i) . (2)
In order to simplify the notation, we introduce a
dummy mention with index 0, which is to the left
(i.e. appears before) of all other mentions and has
w0i = 0 for all actual mentions i > 0. For a given
clustering C, if a mention i is not co-clustered with
any previous actual mention j, 0 < j < i, then we
assume that i links to 0 and C(i, 0) = 1. In other
words, C(i, 0) = 1 iff i is the first actual item of a
cluster in C. However, such an item i is not consid-
ered to be co-clustered with 0 and for any valid clus-
tering, item 0 is always in a singleton dummy clus-
ter, which is eventually discarded. The important
property of the score s is that it is exactly maximized
603
by the best-left-link inference, as it maximizes indi-
vidual left link scores and the creation of one left-
link does not affect the creation of other left-links.
3.3 Learning
We use a max-margin approach to learn w. We are
given a training set D of documents where for each
document d ? D, Cd refers to the annotated ground
truth clustering. Then we learn w by minimizing
L(w) =
?
2
?w?2 + 1|D|
?
d?D
1
md
(
max
C
(
s(C; d,w)
+ ?(C, Cd)
)
? s(Cd; d,w)
)
,
where ?(C, Cd) is a loss function used in corefer-
ence. In order to achieve tractable loss-augmented
minimization ? something not possible with stan-
dard loss functions used in coreference (e.g.
B3 (Bagga and Baldwin, 1998)) ? we use a de-
composable loss function that just counts the num-
ber of mention pairs on which C and Cd disagree:
?(C, Cd) =
?md
i,j=0,j<i IC(i,j)=Cd(i,j), where I is
a binary indicator function. This loss function
is equivalent to the numerator of the Rand index
loss (Rand, 1971). With this form of loss function
and using the scoring function in Eq. (2), we can
write L(w) as
?
2
?w?2 + 1|D|
?
d?D
1
md
md
?
i=1
(
max
0?j<i
(
w ? ?(j, i)
+ ?(Cd, i, j)
)
? max
0?j<i,C(i,j)=1
(w ? ?(j, i))
)
,
(3)
where ?(Cd, i, j) = 1 ? Cd(i, j) is the loss-based
margin that is 1 if i and j are not coreferent in Cd,
and is 0 otherwise. In the above objective function,
the left-links remain latent while we get to observe
the clustering. This objective function is related to
latent structural SVMs (Yu and Joachims, 2009).
However Yu and Joachims (2009) use a spanning
tree based latent structure which does not have the
left-to-right directionality we exploit. We can mini-
mize the above function using Concave Convex Pro-
cedure (Yuille and Rangarajan, 2003), which is guar-
anteed to reach the local minima. However, such a
procedure is costly as it requires doing inference on
all the documents to compute a single gradient up-
date. Consequently, we choose a faster stochastic
sub-gradient descent (SGD) approach. Since L(w)
in Eq. (3) decomposes not only over training doc-
uments, but also over individual mentions in each
document, we can perform SGD on a per-mention
basis. The stochastic sub-gradient w.r.t. mention i
in document d is given by
?L(w)id ? ?(j?, i) ? ?(j??, i) + ?w, where (4)
j? = arg max
0?j<i
(w ? ?(j, i) + 1 ? Cd(i, j))
j?? = arg max
0?j<i,C(i,j)=1
w ? ?(j, i)
While SGD has no theoretically convergence guar-
antee, it works excellently in our experiments.
Specifically, we observe that SGD achieves similar
training performance to CCCP with a speed-up of
around 10,000.
3.4 Incorporating Constraints
Next, we show how to incorporate domain
knowledge-based constraints into L3M and gener-
alize it to CL3M. In CL3M, we obtain a cluster-
ing by maximizing a constraint-augmented scoring
function f given by
s(C; d,w) +
nc
?
p=1
?p?p(d, C),
where the second term on the R.H.S. is the
score contributed by domain specific constraints
?1, . . . , ?nc with their respective scores ?1, . . . , ?nc .
In particular, ?p(d, C) measures the extent to which
a given clustering C satisfies the pth constraint. Note
that this framework is general and can be applied to
inject mention-to-cluster or cluster-to-cluster level
constraints too. However, for simplicity, we con-
sider here only constraints between mention pairs.
This allows us derive fast greedy algorithm to solve
the inference problem. The details of our constraints
are presented in Sec. 5.
All of our constraints can be categorized into two
groups: ?must-link? and ?cannot-link?.?Must-link?
constraints encourage a pair of mentions to connect,
while ?cannot-link? constraints discourage mention
pairs from being linked. Consequently, the coeffi-
cients ?p associated with ?must-link? constraints are
positive while ?p for ?cannot-link? constraints are
negative. In the following, we briefly discuss how to
604
solve the inference problem with these two types of
constraints.
We slightly abuse notations and use ?p(j, i) to in-
dicate the pth constraint on a pair of mentions (i, j).
?p(j, i) is a binary function that is 1 iff two mentions
i and j satisfy the conditions specified in constraint
p. Chang et al (2011) shows that best-left-link in-
ference can be formulated as an ILP problem. When
we add constraints, the ILP becomes:
arg max
B,C?{0,1}
?
i,j:j<i
wjiBji +
?
i,j
?p?p(j, i)Cij
s.t Ckj ? Cij + Cki ? 1, ?i, j, k,
?i?1
j=0
Bji = 1, ?i
Bji < Cji, Cji = Cji,?i, j,
(5)
where Cij ? C(i, j) is a binary variable indicating
whether i and j are in the same cluster or not and
Bji is an auxiliary variable indicating the best-left-
link for mention i. The first set of inequality con-
straints in (5) enforces the transitive closure of the
clustering. The constraints Bji < Cji,?i, j enforce
the consistency between these two sets of variables.
One can use an off-the-shelf solver to solve Eq.
(5). However, when the absolute values of the con-
straint scores (|?p|) are high (the hard constraint
case), then the following greedy algorithm approxi-
mately solves the inference efficiently. We scan the
document from left-to-right (or in any other arbitrary
order). When processing mention i, we find
j? = arg max
j<i
wji +
?
k:C?(k,j)=1
?
p
?p?p(k, i),
(6)
where C? is the current clustering obtained from the
previous inference steps. Then, we add a link be-
tween mention i and j?. The rest of the infer-
ence process is the same as in the original best-left-
link inference. Specifically, this inference procedure
combines the classifier score for mention pair i, j,
with the constraints score of all mentions currently
co-clustered with j. We discuss this further in Sec-
tion 5.
4 Probabilistic Latent Left-Linking Model
In this section, we extend and generalize our left-
linking model approach to a probabilistic model,
Probabilistic Latent Left-Linking Model (PL3M),
that allows us to naturally consider mention-to-
entity (or mention-to-cluster) links. While in L3M,
we assumed that each mention links determinis-
tically to the max-scoring mention on its left, in
PL3M, we assume that mention i links to mention
j, j ? i, with probability given by
Pr[j ? i; d,w] = e
1
? (w??(i,j))
Zi(w, ?)
. (7)
Here Zi(w, ?) =
?
0?k<i e
1
? (w??(i,k)) is a normal-
izing constant and ? ? (0, 1] is a constant tem-
perature parameter that is tuned on a development
set (Samdani et al, 2012). We assume that the event
that mention i links to a mention j is independent of
the event that mention i? links to j? for i 6= i?.
Inference with PL3M: Given the probability of a
link as in Eq. (7), the probability that mention i joins
an existing cluster c, Pr[c ? i; d,w], is simply the
sum of the probabilities of i linking to the mentions
inside c:
Pr[c ? i; d,w] =
?
j?c,0?j<i
Pr[j ? i; d,w]
=
?
j?c,0?j<i
e
1
? (w??(i,j))
Zi(d,w, ?)
. (8)
Based on Eq. (8) and making use of the indepen-
dence assumption of left-links, we follow a simple
greedy clustering (or inference) algorithm: sequen-
tially add each mention i to a previously formed
cluster c?, where c? = arg maxc Pr[c ? i; d,w].
If the arg max cluster is the singleton cluster with
the dummy mention 0 (i.e. the score of all other
clusters is below the threshold of 0), then i starts a
new cluster and is not included in the dummy clus-
ter. Note that we link a mention to a cluster tak-
ing into account all the mentions inside that cluster,
mimicking the notion of a mention-to-cluster link.
This provides more expressiveness than the Best-
Left-Link inference, where a mention connects to
a cluster solely based on a single pairwise link to
some antecedent mention (the best-link mention) in
that cluster.
The case of ? = 0: As ? approaches zero, it is
easy to show that the probability P [j ? i; d, w]
605
in Eq. (7) approaches a Kronecker delta function
that puts probability 1 on the max-scoring mention
j = arg max0?k<i w??(i, j) (assuming no ties), and
0 everywhere else (Pletscher et al, 2010; Samdani et
al., 2012). Consequently, as ? ? 0, Pr[c ? i; d,w]
in Eq. 8 approaches a Kronecker delta function cen-
tered on the cluster containing the max-scoring men-
tion, thus reducing to the best-link case of L3M.
Thus, PL3M, when tuning the value of ?, is a strictly
more general model than L3M.
Learning with PL3M We use a likelihood-based
approach to learning with PL3M, and first compute
the probability Pr[C; d,w] of generating a cluster-
ing C, given w. We then learn w by minimizing
the regularized negative log-likelihood of the data,
augmenting the partition function with a loss-based
margin (Gimpel and Smith, 2010). We omit the de-
tails of likelihood computation due to lack of space.
With PL3M, we again follow a stochastic gradi-
ent descent technique instead of CCCP for the same
reasons mentioned in Sec. 3.3. The stochastic gra-
dient (subgradient when ? = 0) w.r.t. mention i in
document d is given by
?LL(w)id ?
?
0?j<i
pj?(i, j) ?
?
0?j<i
p?j?(i, j) + ?w,
where pj and p?j , j = 0, . . . , i ? 1, are non-negative
weights that sum to one and are given by
pj =
e
1
? (w??(i,j)+?(Cd,i,j))
?
0?k<i e
1
? (w??(i,k)+?(Cd,i,k))
and
p?j =
Cd(i, j)Zi(d,w, ?)
Zi(Cd; d,w, ?)
Pr[j ? i; d,w] .
Interestingly, the above update rule generalizes the
one for L3M, as we are incorporating a weighted
sum of all previous mentions in the update rule.
With ? ? 0, the SGD in Eq. (4) converges to the
SGD update in L3M (Eq. (4)). Finally, in the pres-
ence of constraints, we can fold them inside the pair-
wise link probabilities as in Eq. (6).
5 Experiments and Results
In this section, we present our experiments on the
two commonly used benchmarks for coreference
? Ontonotes-5.0 (Pradhan et al, 2012) and ACE
2004 (NIST, 2004). Table 1 exhibits our bottom line
results: CL3M achieves the best result reported on
Ontonotes-5.0 development set and essentially ties
with (Fernandes et al, 2012) on the test set. As
shown in Table 3, CL3M is also the best algorithm
on ACE and when evaluated on the gold mentions
of Ontonotes. We show that CL3M performs partic-
ularly well on clusters containing named entity men-
tions, which are more important for many informa-
tion extraction applications. In the rest of this sec-
tion, after describing our experimental setting, we
provide careful analysis of our algorithms and com-
pare them to competitive coreference approaches in
the literature.
5.1 Experimental Setup
Datasets: ACE 2004 contains 443 documents ?
we used a standard split of these documents into
268 training, 68 development, and 106 testing doc-
uments used by Culotta et al (2007) and Bengt-
son and Roth (2008). OntoNotes-5.0 dataset, re-
leased for the CoNLL 2012 Shared Task (Pradhan et
al., 2012), is by far the largest annotated corpus on
coreference. It contains 3,145 annotated documents
drawn from a wide variety of sources ? newswire,
bible, broadcast transcripts, magazine articles, and
web blogs. We report results on both development
set and test set. To test on the development set, we
further split the training data into training and devel-
opment sets.
Classifier details: For each of the pairwise ap-
proaches, we assume the pairwise score is given by
w??(?, ?)+t where ? are the features, w is the weight
vector learned by the approach, and t is a threshold
which we set to 0 during learning (as in Eq. (1)), but
use a tuned value (tuned on a development set) dur-
ing testing. For learning with L3M, we do stochastic
gradient descent with 5 passes over the data. Empir-
ically, we observe that this is enough to generate a
stable model. For PL3M (Sec. 4), we tune the value
of ? using the development set picking the best ?
from {0.0, 0.2, . . . , 1.0}. Recall that when ? = 0,
PL3M is the same as L3M. We refer to L3M and
PL3M with incorporating constraints during infer-
ence as CL3M and CPL3M (Sec. 3.4), respectively.
Metrics: We compare the systems using three
popular metrics for coreference ? MUC (Vilain et
al., 1995), BCUB (Bagga and Baldwin, 1998), and
606
Entity-based CEAF (CEAFe) (Luo, 2005). Follow-
ing, the CoNLL shared tasks (Pradhan et al, 2012),
we use the average F1 scores of these three metrics
as the main metric of comparison.
Features: We build our system on the publicly
available Illinois-Coref system1 primarily because it
contains a rich set of features presented in Bengtson
and Roth (2008) and Chang et al (2012a) (the latter
adds features for pronominal anaphora resolution).
We also compare with the Best-Left-Link approach
described by Bengtson and Roth (2008).
Constraints: We consider the following con-
straints in CL3M and CPL3M.
? SameSpan: two mentions must be linked to
each other if they share the same surface text
span and the number of words in the text span
is larger than a threshold (set as 5 in our imple-
mentation).
? SameDetNom: two mentions must be linked
to each other if both mentions start with a de-
terminer and the [0,1] wordnet-based similarity
score between the mention head words is above
a threshold (set to 0.8).
? SameProperName: two mentions must be
linked if they are both proper names and the
similarity score measured by a named entity-
based similarity metric, Illinois NESim2, are
higher than a threshold (set to 0.8). For a per-
son entity we add additional rules to extract the
first name, last name and professional title as
properties.
? ModifierMismatch: the constraint prevents two
mentions to be linked if the head modifiers
conflict. For example, the constraint prevents
?northern Taiwan? from linking to ?southern
Taiwan?. We gather a list of mutual exclusive
modifiers from the training data.
? PropertyMismatch: the constraint prevents two
mentions to be linked if their properties con-
flict. For example, it prevents male pronouns
to link to female pronouns and ?Mr. Clinton?
to link to ?Mrs. Clinton? by checking the gen-
der property. The properties we consider are
gender, number, professional title and the na-
1The system is available at http://cogcomp.cs.
illinois.edu/page/software_view/Coref/
2http://cogcomp.cs.illinois.edu/page/
software_view/NESim
MUC BCUB CEAFe AVG
Dev Set
Stanford 64.30 70.46 46.35 60.37
(Chang et al, 2012a) 65.75 70.25 45.30 60.43
(Martschat et al, 2012) 66.76 71.91 47.52 62.06
(Bjo?rkelund and Farkas, ) 67.12 71.18 46.84 61.71
(Chen and Ng, 2012) 66.4 71.8 48.8 62.3
(Fernandes et al, 2012) 69.46 71.93 48.66 63.35
L3M 67.88 71.88 47.16 62.30
CL3M 69.20 72.89 48.67 63.59
Test Set
Stanford 63.83 68.52 45.36 59.23
(Chang et al, 2012a) 66.38 69.34 44.81 60.18
(Martschat et al, 2012) 66.97 70.36 46.60 61.31
(Bjo?rkelund and Farkas, ) 67.58 70.26 45.87 61.24
(Chen and Ng, 2012) 63.7 69.0 46.4 59.7
(Fernandes et al, 2012) 70.51 71.24 48.37 63.37
L3M 68.31 70.81 46.73 61.95
CL3M 69.64 71.93 48.32 63.30
Table 1: Performance on OntoNotes-5.0 with predicted
mentions. We report the F1 scores (%) on various coref-
erence metrics (MUC, BCUB, CEAF). The column AVG
shows the average scores of the three. We observe that
PL3M and CPL3M (see Sec. 4) yields the same perfor-
mance as L3M and CL3M, respectively as the tuned ? for
all the datasets turned out to be 0.
tionality.
While the ?must-link? constraints described in the
paper can be treated as features, due to their high
precision, treating them as hard constraints (set ? to
a high value) is a safe and direct way to inject hu-
man knowledge into the learning model. Moreover,
our framework allows a constraint to use informa-
tion from previous decisions (such as ?cannot-link?
constraints). Treating such constraints as features
will complicate the learning model.
5.2 Performance of the End-to-End System
We compare our system with the top systems re-
ported in the CoNLL shared task 2012 as well as
with the Stanford?s publicly released rule-based sys-
tem (Lee et al, 2013; Lee et al, 2011), which won
the CoNLL 2011 Shared Task (Pradhan et al, 2011).
Note that all the systems use the same annotations
(e.g., gender prediction, part-of-speech tags, name
entity tags) provided by the shared task organizers.
607
However, each system implements its own mention
detector and pipelines the identified mentions into
the coreference clustering component. Moreover,
different systems use a different set of features. In
order to partially control for errors on mention de-
tection and better evaluate the clustering component
in our coreference system, we will also present re-
sults on correct (gold) mentions in the next section.
Table 1 shows the end-to-end results. On the
development set, only the best performing system
of Fernandes et al (2012) is better than L3M, but this
difference disappears when we use our system with
constraints, CL3M. Although our system is much
simple, it achieves the best B3 score on the test set
and is competitive with the best system participated
in the CoNLL shared task 2012.
Performance on named entities: The corefer-
ence annotation in Ontonotes 5.0 includes various
types of mentions. However, not all mention types
are equally interesting. In particular, clusters which
contain at least one proper name or a named entity
mention are more important for information extrac-
tion tasks like Wikification (Mihalcea and Csomai,
2007; Ratinov et al, 2011), cross-document coref-
erence resolution (Bagga and Baldwin, 1998), and
entity linking and knowledge based population (Ji
and Grishman, 2011).
Inspired by this, we compare our system to the
best systems in the CoNLL shared task of 2011
(Stanford (Lee et al, 2011)) and 2012 (Fernan-
des (Fernandes et al, 2012)) on the following spe-
cific tasks on Ontonotes-5.0.
? ENT-C: Evaluate the system on clusters that
contain at least one proper name mention. We
generate the gold annotation and system out-
puts by using the gold and predicted name en-
tity tag annotations provided by the CoNLL
shard task 2012. That is, if a cluster does not
include any name entity mention, then it will
be removed from the final clustering.
? PER-C: As in the construction of ENT-C, but
here we only consider clusters which contain at
least one ?Person (PER)? entity.
? ORG-C: As in the construction of Entity-C, but
here we only consider clusters which contain at
least one ?Organization (ORG)? entity.
Typically, the clusters that get ignored in the above
definitions contain only first and second person
Task Stanford Fernandes L3M CL3M
ENT-C 44.06 47.05 46.63 48.02
PER-C 34.04 36.43 37.01 37.57
ORG-C 25.02 26.23 26.22 27.01
Table 2: Performance on named entities for OntoNotes-
5.0 data. We compare our system to Fernandes (Fernan-
des et al, 2012) and Stanford (Lee et al, 2013) systems.
pronouns (which often happens in transcribed dis-
course.) Also note that all the systems are trained
with the same name entity tags, provided by the
shared task organizers, and we use the same name
entity tags to construct the specific clustering. Also,
in order to further ensure fairness, we do not tune
our system to favor the evaluation of these specific
types of clusters. We chose to do so because we only
have access to the system output of Fernandes et al
(2012).
Table 2 shows the results. The performance of
all systems degrades when considering only clusters
that contain name entities, indicating that ENT-C is
actually a harder task than the original coreference
resolution problem. In particular, resolving ORG
coreferent clusters is hard, because names of organi-
zations are sometimes confused with person names,
and they can be referred to using a range of pronouns
(including ?we? and ?it?). Overall, CL3M outper-
forms all the competing systems on the clusters that
contain at least one specific type of entity by a mar-
gin larger than that for the overall coreference.
5.3 Analysis on Gold Mentions
To better understand the contribution of our joint
learning and clustering model, we present experi-
ments assuming that gold mentions are given. The
definitions of gold mentions in ACE and Ontonotes
are different because Ontonotes-5.0 excludes single-
ton clusters in the annotation. In addition, Ontonotes
includes longer mentions; for example, it includes
NP and appositives in the same mention. We com-
pare with the publicly available Stanford (Lee et al,
2011) and IllinoisCoref (Chang et al, 2012a) sys-
tems; the system of Fernandes et al (2012) is not
publicly available. In addition, we also compare
with the following two structured prediction base-
lines that use the same set of features as L3M and
PL3M.
608
MUC BCUB CEAFe AVG
ACE 2004 Gold Ment.
All-Link-Red. 77.45 81.10 77.57 78.71
Spanning 73.31 79.25 74.66 75.74
IllinoisCoref 76.02 81.04 77.6 78.22
Stanford 75.04 80.45 76.75 77.41
(Stoyanov and Eisner, 2012) 80.1 81.8 - -
L3M 77.57 81.77 78.15 79.16
PL3M 78.18 82.09 79.21 79.83
CL3M 78.17 81.64 78.45 79.42
CPL3M 78.29 82.20 79.26 79.91
Ontonotes 5.0 Gold Ment.
All-Link-Red. 83.72 75.59 64.00 74.44
Spanning 83.64 74.83 61.07 73.18
IllinoisCoref 80.84 74.29 65.96 73.70
Stanford 82.26 76.82 61.69 73.59
L3M 83.44 78.12 64.56 75.37
PL3M 83.97 78.25 65.69 75.97
CL3M 84.10 78.30 68.74 77.05
CPL3M 84.80 78.74 68.75 77.43
Table 3: Performance on ACE 2004 and OntoNotes-5.0.
All-Link-Red. is based on correlational clustering; Span-
ning is based on latent spanning forest based clustering
(see Sec. 2). Our proposed approach is L3M (Sec. 3) and
PL3M (sec. 4). CL3M and CPL3M are the version with
incorporating constraints.
1. All-Link-Red: a reduced and faster alterna-
tive to the correlational clustering based ap-
proach (Finley and Joachims, 2005). We im-
plemented this algorithm as an ILP and droped
one of the three transitivity constraints for each
triplet of mention variables. Following Pascal
and Baldridge (2009) and Chang et al (2011)
we observe that this slightly improves the ac-
curacy over a pure correlation clustering ap-
proach, in addition to speeding up inference.
2. Spanning: the latent spanning forest based ap-
proach presented by Yu and Joachims (2009).
We use the publicly available implementation
provided by the authors3 for the ACE data;
since their CCCP implementation is slow, we
implemented our own stochastic gradient de-
scent version to scale it to the much larger
Ontonotes data.
3Available at http://www.cs.cornell.edu/ cnyu/latentssvm/
Table 3 lists the results. Although L3M is simple
and use only the features defined on pairwise men-
tions, it compares favorably with all recently pub-
lished results. Moreover, the probabilistic general-
ization of L3M, PL3M, achieves even better perfor-
mance. For example, L3M with ? = 0.2 improves
L3M with ? = 0 by 0.7 points in ACE 2004. In par-
ticular, This shows that considering more than a one
left-links is helpful. This is in contrast with the pre-
dicted mentions where ? = 0 performed best. We
suspect that this is because noisy mentions can hurt
the performance of PL3M that takes into account
not just the best scoring links, but also weaker links
which are likely to be less reliable (more false pos-
itives). Also, as opposed to what is reported by Yu
and Joachims (2009), the correlation clustering ap-
proach performs better than the spanning forest ap-
proach. We think that this is because we compare
the systems on different metrics than they did and
also because we use exact ILP inference for corre-
lational clustering whereas Yu and Joachims (2009)
used approximate greedy inference.
Both L3M and PL3M can be benefit from using
constraints. However, The constraints improve only
marginally on the ACE 2004 data because ACE uses
shorter phrases as mentions. Consequently, con-
straints designed for leveraging information from
long mention spans are less effective. Overall, the
experiments show that L3M and PL3M perform well
on modeling coreference clustering.
5.4 Ablation Study of Constrains
Finally, we study the value of individual constraints
by adding one constraint at a time to the corefer-
ence system starting with the simple L3M model.
The system with all the constraints added is the
CL3M model introduced in Table 1. We then re-
move individual constraints from CL3M to assess
its contribution. Table 4 shows the results on the
Ontonotes dataset with predicted mentions. Overall,
it is shown that each one of the constraints has a con-
tribution, and that using all the constraints improves
the performance of the system by 1.29% in the AVG
F1 score. In particular, most of this improvement
(1.19%) is due to the must-link constraints (the first
four constraints in the table). The must-link con-
straints are more useful for L3M as L3M achieves
higher precision than recall (e.g., the precision and
609
MUC BCUB CEAFe AVG
L3M 67.88 71.88 47.16 62.30
+SameSpan 68.27 72.27 47.73 62.75
+SameDetNom 68.79 72.57 48.30 63.22
+SameProperName 69.11 72.81 48.56 63.49
+ModifierMismatch 69.11 72.81 48.58 63.50
+PropertyMismatch 69.20 72.89 48.67 63.59(i.e. CL3M)
-SameSpan 68.91 72.66 48.36 63.31
-SameDetNom 68.62 72.51 48.06 63.06
-SameProperName 68.97 72.69 48.50 63.39
-ModifierMismatch 69.12 72.80 48.63 63.52
-PropertyMismatch 69.11 72.81 48.58 63.50
Table 4: Ablation study on constraints. We first show
cumulative performance on OntoNotes-5.0 data with pre-
dicted mentions as constraints are added one at a time into
the coreference system. Then we demonstrate the value
of individual constraints by leaving out one constraint at
each time.
recall of L3M are 78.38% and 67.96%, respectively
in B3). As a result, the must-link constraints, which
aim at improving the recall, do better when optimiz-
ing F1.
6 Conclusions
We presented a principled yet simple framework for
coreference resolution. Furthermore, we showed
that our model can be augmented in a straightfor-
ward way with knowledge based constraints, to im-
prove performance. We also presented a probabilis-
tic generalization of this model that can take into
account entity-mention links by considering mul-
tiple possible coreference links. We proposed a
fast stochastic gradient-based learning technique for
our model. Our model, while operating at men-
tion pair granularity, obtains state-of-the-art results
on OntoNotes-5.0, and performs especially well on
mention clusters containing named entities. We pro-
vided a detailed analysis of our experimental results.
Acknowledgments Supported by the Intelligence Advanced
Research Projects Activity (IARPA) via Department of Interior National
Business Center contract number D11PC20155. The U.S. Government
is authorized to reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright annotation thereon. Disclaimer:
The views and conclusions contained herein are those of the authors and
should not be interpreted as necessarily representing the official policies
or endorsements, either expressed or implied, of IARPA, DoI/NBC, or
the U.S. Government.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In In The First International Con-
ference on Language Resources and Evaluation Work-
shop on Linguistics Coreference.
M. Bansal and D. Klein. 2012. Coreference semantics
from web features. In Proceedings of ACL, Jeju Island,
South Korea, July.
N. Bansal, A. Blum, and S. Chawla. 2002. Correlation
clustering. In Proceedings of the 43rd Symposium on
Foundations of Computer Science.
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP, 10.
A. Bjo?rkelund and R. Farkas.
K.-W. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference protocols
for coreference resolution. In CoNLL Shared Task.
K.-W. Chang, R. Samdani, A. Rozovskaya, M. Sammons,
and D. Roth. 2012a. Illinois-coref: The UI system
in the CoNLL-2012 Shared Task. In CoNLL Shared
Task.
M. Chang, L. Ratinov, and D. Roth. 2012b. Structured
learning with constrained conditional models. Ma-
chine Learning, 88(3):399?431, 6.
C. Chen and V. Ng. 2012. Combining the best of two
worlds: A hybrid approach to multilingual corefer-
ence resolution. In Joint Conference on EMNLP and
CoNLL - Shared Task.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 144?151, Sydney, Australia, July. ACL.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In HLT/NAACL.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).
P. Denis and J. Baldridge. 2008. Specialized models and
ranking for coreference resolution. In EMNLP, pages
660?669.
G. Durrett, D. Hall, and D. Klein. 2013. Decentral-
ized entity-level modeling for coreference resolution.
In Proceedings of ACL, August.
610
E. R. Fernandes, C. N. dos Santos, and R. L. Milidiu?.
2012. Latent structure perceptron with feature induc-
tion for unrestricted coreference resolution. In Joint
Conference on EMNLP and CoNLL - Shared Task.
T. Finley and T. Joachims. 2005. Supervised cluster-
ing with support vector machines. In Proceedings
of the International Conference on Machine Learning
(ICML).
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
CRFs: Training log-linear models with cost functions.
In NAACL.
A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In NAACL.
H. Ji and R. Grishman. 2011. Knowledge base popula-
tion: successful approaches and challenges. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies - Volume 1.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2011. Stanford?s multi-
pass sieve coreference resolution system at the conll-
2011 shared task. In Proceedings of the CoNLL-2011
Shared Task.
H. Lee, A. Chang, Y. Peirsman, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2013. Deterministic coref-
erence resolution based on entity-centric, precision-
ranked rules. Computational Linguistics, 39(4).
X. Luo. 2005. On coreference resolution performance
metrics. In EMNLP.
S. Martschat, J. Cai, S. Broscheit, ?E. Mu?jdricza-Maydt,
and M. Strube. 2012. A multigraph model for corefer-
ence resolution. In Joint Conference on EMNLP and
CoNLL - Shared Task, July.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).
R. Mihalcea and A. Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In CIKM.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In ACL.
Vincent Ng. 2005. Supervised ranking for pronoun res-
olution: Some recent improvements. In AAAI, pages
1081?1086.
NIST. 2004. The ACE evaluation plan.
D. Pascal and J. Baldridge. 2009. Global joint models for
coreference resolution and named entity classification.
In Procesamiento del Lenguaje Natural.
P. Pletscher, C. S. Ong, and J. M. Buhmann. 2010. En-
tropy and margin maximization for structured output
learning. In ECML PKDD.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. Conll-2011 shared
task: Modeling unrestricted coreference in ontonotes.
In CoNLL.
S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and
Y. Zhang. 2012. CoNLL-2012 shared task: Modeling
multilingual unrestricted coreference in OntoNotes. In
CoNLL 2012.
M. Sammons Y. Tu V. Vydiswaran Q. Do, D. Roth. 2009.
Robust, light-weight approaches to compute lexical
similarity. Technical report.
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning. 2010.
A multi-pass sieve for coreference resolution. In
EMNLP.
A. Rahman and V. Ng. 2011a. Coreference resolution
with world knowledge. In ACL, pages 814?824.
A. Rahman and V. Ng. 2011b. Ensemble-based corefer-
ence resolution. In IJCAI.
A. Rahman and V. Ng. 2011c. Narrowing the modeling
gap: a cluster-ranking approach to coreference resolu-
tion. JAIR.
W.M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statis-
tical Association, 66(336):846?850.
L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In
EMNLP.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Dan Roth and Wen Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL-04, pages 1?8.
R. Samdani, M. Chang, and D. Roth. 2012. Unified ex-
pectation maximization. In NAACL.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Comput. Linguist.
V. Stoyanov and J. Eisner. 2012. Easy-first coreference
resolution. In COLING, pages 2519?2534.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009.
Conundrums in noun phrase coreference resolution:
making sense of the state-of-the-art. In ACL.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
611
scoring scheme. In Proceedings of the 6th conference
on Message understanding.
C. Yu and T. Joachims. 2009. Learning structural svms
with latent variables. In Proceedings of the Interna-
tional Conference on Machine Learning (ICML).
A. L. Yuille and A. Rangarajan. 2003. The concave-
convex procedure. Neural Computation, 15(4).
612
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 791?802,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Learning and Inference for Grammatical Error Correction
Alla Rozovskaya and Dan Roth
Cognitive Computation Group
University of Illinois at Urbana-Champaign
201 N. Goodwin Avenue
Urbana, IL 61801
{rozovska,danr}@illinois.edu
Abstract
State-of-the-art systems for grammatical er-
ror correction are based on a collection of
independently-trained models for specific er-
rors. Such models ignore linguistic interac-
tions at the sentence level and thus do poorly
on mistakes that involve grammatical depen-
dencies among several words. In this paper,
we identify linguistic structures with interact-
ing grammatical properties and propose to ad-
dress such dependencies via joint inference
and joint learning.
We show that it is possible to identify interac-
tions well enough to facilitate a joint approach
and, consequently, that joint methods correct
incoherent predictions that independently-
trained classifiers tend to produce. Further-
more, because the joint learning model con-
siders interacting phenomena during training,
it is able to identify mistakes that require mak-
ing multiple changes simultaneously and that
standard approaches miss. Overall, our model
significantly outperforms the Illinois system
that placed first in the CoNLL-2013 shared
task on grammatical error correction.
1 Introduction
There has recently been a lot of work addressing er-
rors made by English as a Second Language (ESL)
learners. In the past two years, three competitions
devoted to grammatical error correction for non-
native writers took place: HOO-2011 (Dale and Kil-
garriff, 2011), HOO-2012 (Dale et al, 2012), and
the CoNLL-2013 shared task (Ng et al, 2013).
Nowadays *phone/phones *has/have many
functionalities, *included/including *?/a
camera and *?/a Wi-Fi receiver.
Figure 1: Examples of representative ESL errors.
Most of the work in the area of ESL error cor-
rection has addressed the task by building statistical
models that specialize in correcting a specific type
of a mistake. Figure 1 illustrates several types of
errors common among non-native speakers of En-
glish: article, subject-verb agreement, noun num-
ber, and verb form. A significant proportion of re-
search has focused on correcting mistakes in article
and preposition usage (Izumi et al, 2003; Han et
al., 2006; Felice and Pulman, 2008; Gamon et al,
2008; Tetreault and Chodorow, 2008; Gamon, 2010;
Rozovskaya and Roth, 2010b). Several studies also
consider verb-related and noun-related errors (Lee
and Seneff, 2008; Gamon et al, 2008; Dahlmeier
and Ng, 2012). The predictions made by individual
models are then applied independently (Rozovskaya
et al, 2011) or pipelined (Dahlmeier and Ng, 2012).
The standard approach of training individual clas-
sifiers considers each word independently and thus
assumes that there are no interactions between er-
rors and between grammatical phenomena. But an
ESL writer may make multiple mistakes in a single
sentence and these result in misleading local cues
given to individual classifiers. In the example shown
in Figure 1, the agreement error on the verb ?have?
interacts with the noun number error: a correction
system that takes into account the context may in-
fer, because of the word ?phone?, that the verb num-
ber is correct. For this reason, a system that consid-
791
ers noun and agreement errors separately will fail to
identify and correct the interacting errors shown in
Fig. 1. Furthermore, it may also produce inconsis-
tent predictions.
Even though it is quite clear that grammatical er-
rors interact, for various conceptual and technical
reasons, this issue has not been addressed in a sig-
nificant way in the literature. We believe that the
reasons for that are three-fold: (1) Data: until very
recently we did not have data that jointly annotates
sufficiently many errors of interacting phenomena
(see Sec. 2). (2) Conceptual: Correcting errors in
interacting linguistic phenomena requires that one
identifies those phenomena and, more importantly,
can recognize reliably the interacting components
(e.g., given a verb, identify the subject to enable en-
forcing agreement). The perception has been that
this cannot be done reliably (Sec. 4). (3) Technical:
The NLP community has started to better understand
joint learning and inference and apply it to various
phenomena (Roth and Yih, 2004; Punyakanok et al,
2008; Martins et al, 2011; Clarke and Lapata, 2007;
Sutton and McCallum, 2007) (Sec. 5).
In this paper we present, for the first time, a suc-
cessful approach to jointly resolving grammatical er-
rors. Specifically:
? We identify two pairs of interacting phenomena,
subject-verb and article-NPhead agreements; we
show how to reliably identify these pairs in noisy
ESL data, thereby facilitating the joint correction of
these phenomena.
?We propose two joint approaches: (1) a joint infer-
ence approach implemented on top of individually
learned models using an integer linear programming
formulation (ILP, (Roth and Yih, 2004)), and (2) a
model that jointly learns each pair of these phenom-
ena. We show that each of these methods has its ad-
vantages, and that both solve the two challenges out-
lined above: the joint models exclude inconsistent
predictions that violate linguistic constraints. The
joint learning model exhibits superior performance,
as it is also able to overcome the problem of the
noisy context encountered by the individual mod-
els and to identify errors in contexts, where multiple
changes need to be applied at the same time.
We show that our joint models produce state-of-
the-art performance and, in particular, significantly
outperform the University of Illinois system that
placed first in the CoNLL-2013 shared task, increas-
ing the F1 score by 2 and 4 points in different evalu-
ation settings.
2 Task Description and Motivation
To illustrate the utility of jointly addressing interact-
ing grammatical phenomena, we consider the cor-
pus of the CoNLL-2013 shared task on grammatical
error correction (Ng et al, 2013), which we found
to be particularly well-suited for addressing interac-
tions between grammatical phenomena. The task fo-
cuses on the following five common mistakes made
by ESL writers: article, preposition, noun number,
subject-verb agreement, and verb form, and we ad-
dress two interactions: article-NPhead and subject-
verb.
The training data for the task is from the NUCLE
corpus (Dahlmeier et al, 2013), an error-tagged col-
lection of essays written by non-native learners of
English. The test data is an additional set of essays
by learners from the same linguistic background.
The training and the test data contain 1.2M and 29K
words, respectively. Although the corpus contains
errors of other types, the task focuses on five types
of errors. Table 1 shows the number of mistakes1 of
each type and the error rates, i.e. the percentage of
erroneous words by error type.
Error Number of errors and error rate
Training Test
Article 6658 (2.4%) 690 (10.0%)
Prep. 2404 (2.0%) 311 (10.7%)
Noun 3779 (1.6%) 396 (6.0%)
Verb Agr. 1527(2.0%) 124 (5.2%)
Verb Form 1453 (0.8%) 122 (2.5%)
Table 1: Number of annotated errors in the CoNLL-
2013 shared task. Percentage denotes the error rates, i.e.
the number of erroneous instances with respect to the to-
tal number of relevant instances in the data. For example,
10.7% of prepositions in the test data are used incorrectly.
The numbers in the revised data set are slightly higher.
We note that the CoNLL-2013 data set is the first
annotated collection that makes a study like ours
feasible. The presence of a common test set that
1System performance in the shared task is evaluated on data
with and without additional revisions added based on the input
from participants. The number of mistakes in the revised test
data is slightly higher.
792
contains a good number of interacting errors ? ar-
ticle, noun, and verb agreement mistakes ? makes
the data set well-suited for studying which approach
works best for addressing interacting phenomena.
The HOO-2011 shared task collection (Dale and
Kilgarriff, 2011) contains a very small number of
noun and agreement errors (41 and 11 in test, re-
spectively), while the HOO-2012 competition (Dale
et al, 2012) only addresses article and preposition
mistakes. Indeed, in parallel to the work presented
here, Wu and Ng (2013) attempted the ILP-based
approach of Roth and Yih (2004) in this domain.
They were not able to show any improvement, for
two reasons. First, the HOO-2011 data set which
they used does not contain a good number of errors
in interacting structures. Second, and most impor-
tantly, they applied constraints in an indiscriminate
manner. In contrast, we show how to identify the
interacting structures? components in a reliable way,
and this plays a key role in the joint modeling im-
provements.
Lack of data hindered other earlier efforts for
error correction beyond individual language phe-
nomena. Brockett et al (2006) applied machine-
translation techniques to correct noun number errors
on mass nouns and article usage but their application
was restricted to a small set of constructions. Park
and Levy (2011) proposed a language-modeling ap-
proach to whole sentence error correction but their
model is not competitive with individually trained
models. Finally, Dahlmeier and Ng (2012) proposed
a decoder model, focusing on four types of errors
in the data set of the HOO-2011 competition (Dale
and Kilgarriff, 2011). The decoder optimized the se-
quence in which individual classifiers were to be ap-
plied to the sentence. However, because the decoder
still corrected mistakes in a pipeline fashion, one at
a time, it is unlikely that it could deal with cases that
require simultaneous changes.
3 The University of Illinois System
Below, we briefly describe the University of Illinois
system (henceforth Illinois; in the overview paper of
the shared task the system is referred to as UI) that
achieved the best result in the CoNLL-2013 shared
task and which we use as our baseline model. For
a complete description, we refer the reader to Ro-
zovskaya et al (2013).
The Illinois system implements five machine-
learning independently-trained classifiers that fol-
low the popular approach to ESL error correction
borrowed from the context-sensitive spelling correc-
tion task (Golding and Roth, 1999; Carlson et al,
2001). A confusion set is defined that specifies a
list of confusable words. Each occurrence of a con-
fusable word in text is represented as a vector of
features derived from a context window around the
target. The problem is cast as a multi-class classi-
fication task and a classifier is trained on native or
learner data. At prediction time, the model selects
the most likely candidate from the confusion set.
The confusion set for prepositions includes the
top 12 most frequent English prepositions. The arti-
cle confusion set is as follows: {a,the,?}2. The con-
fusion sets for noun, agreement, and form modules
depend on the target word and include its morpho-
logical variants (Table 2).
?Hence, the environmental *factor/factors also
*contributes/contribute to various difficulties,
*included/including problems in nuclear technol-
ogy.?
Error type Confusion set
Noun {factor, factors}
Verb Agr. {contribute, contributes}
Verb Form {included, including, includes, include}
Table 2: Confusion sets for noun number, agreement,
and form classifiers.
The article classifier is a discriminative model
that draws on the state-of-the-art approach described
in Rozovskaya et al (2012). The model makes use
of the Averaged Perceptron algorithm (Freund and
Schapire, 1996) and is trained on the training data of
the shared task with rich features.
The other models are trained on native English
data, the Google Web 1T 5-gram corpus (henceforth,
Google, (Brants and Franz, 2006)) with the Na??ve
Bayes (NB) algorithm. All models use word n-gram
features derived from the 4-word window around the
target word. In the preposition model, priors for
preposition preferences are learned from the shared
task training data (Rozovskaya and Roth, 2011).
2? denotes noun-phrase-initial contexts where an article is
likely to have been omitted. The variants ?a? and ?an? are con-
flated and are restored later.
793
Example Predictions made by the Illinois system
?They believe that such situation must be avoided.? such situation? such a situations
?Nevertheless , electric cars is still regarded as a great trial innovation.? cars is? car are
?Every students have appointments with the head of the department.? No change
Table 3: Examples of predictions of the Illinois system that combines independently-trained models.
The words that are selected as input to classifiers
are called candidates. Article and preposition can-
didates are identified with a closed list of words;
noun-phrase-initial contexts for the article classifier
are determined using a shallow parser3 (Punyakanok
and Roth, 2001). Candidates for the noun, agree-
ment, and form classifiers are identified with a part-
of-speech tagger4, e.g. noun candidates are words
that are tagged as NN or NNS. Table 4 shows the
total number of candidates for each classifier.
Classifier
Art. P N Agr. F
Train 254K 103K 240K 75K 175K
Test 6K 2.5K 2.6K 2.4K 4.8K
Table 4: Number of candidate words by classifier type
in training and test data.
4 Interacting Mistakes
The approach of addressing each type of mistake in-
dividually is problematic when multiple phenomena
interact. Consider the examples in Table 3 and the
predictions made by the Illinois system. In the first
and second sentences, there are two possible ways
to correct the structures ?such situation? and ?cars
is?. In the former, either the article or the noun num-
ber should be changed; in the latter, either the noun
number or the verb agreement marker5. In these ex-
amples, each of the independently-trained classifiers
identifies the problem because each system makes a
decision using the second error as part of its contex-
tual cues, and thus the individual systems produce
inconsistent predictions.
3http://cogcomp.cs.illinois.edu/page/
software view/Chunker
4http://cogcomp.cs.illinois.edu/page/
software view/POS
5Both of these solutions will result in grammatical output
and the specific choice between the two depends on the wider
essay context.
The second type of interaction concerns cases that
require correcting more than one word at a time:
the last example in Table 3 requires making changes
both to the verb and the subject. Since each of the in-
dependent classifiers (for nouns and for verb agree-
ment) takes into account the other word as part of
its features, they both infer that the verb number is
correct and that the grammatical subject ?student?
should be plural.
We refer to the words whose grammatical prop-
erties interact as structures. The independently-
trained classifiers tend to fail to provide valid cor-
rections in contexts where it is important to consider
both words of the structure.
4.1 Structures for Joint Modeling
We address two linguistic structures that are relevant
for the grammatical phenomena considered: article-
NPhead and subject-verb. In the article-NPhead
structures, the interaction is between the head of
the noun phrase (NP) and the article that refers to
the NP (first example in Table 3). In particular,
the model should take into account that the article
?a? cannot be combined with a noun in plural form.
For subject-verb agreement, the subject and the verb
should agree in number.
We now need to identify all pairs of candidates
that form the relevant structures. Article-NPhead
structures are pairs of words, such that the first word
is a candidate of type article, while the second word
is a noun candidate. Given an article candidate, the
head of its NP is determined using the POS infor-
mation (this information is obtained from the article
feature vector because the NP head is a feature used
by the article system)6. Subject-verb structures are
pairs of noun-agreement candidates. Given a verb,
its subject is identified with a dependency parser
(Marneffe et al, 2006).
To evaluate the accuracy of subject and NP head
6Some heads are not identified or belong to a different part
of speech.
794
predictions, a random sample of 500 structures of
each type from the training data was examined by
a human annotator with formal training in Linguis-
tics. The human annotations were then compared
against the automatic predictions. The results of
the evaluation for subject-verb and article-NPhead
structures are shown in Tables 5 and 6, respectively.
Although the overall accuracy is above 90% for both
structures, the accuracy varies by the distance be-
tween the structure components and drops signifi-
cantly as the distance increases. For article-NPhead
structures, distance indicates the position of the NP
head with respect to the article, e.g. distance of 1
means that the head immediately follows the arti-
cle. For subject-verb structures, distance is shown
with respect to the verb: a distance of -1 means that
the subject immediately precedes the verb. Although
in most cases the subject is located to the left of
the verb, in some constructions, such as existential
clauses and inversions, it occurs after the verb.
Based on the accuracy results for identifying the
structure components, we select those structures
where the components are reliably identified. For
article-NPhead, valid structures are those where the
distance is at most three words. For subject-verb, we
consider as valid those structures where the identi-
fied subject is located within two words to the left or
three words to the right of the verb.
The valid structures are selected as input to the
joint model (Sec. 5). The joint learning model con-
siders only those valid structures whose components
are adjacent. In adjacent structures the NP head im-
mediately follows the article, and the verb immedi-
ately follows the subject. Joint inference is not re-
stricted to adjacent structures.
The last column of Table 5 shows that valid
subject-verb structures account for 67.5% of all
verbs whose subjects are common nouns (51.7% are
cases where the words are adjacent). Verbs whose
subjects are common nouns account for 57.8% of all
verbs that have subjects (verbs with different types
of subjects, most of which are personal pronouns,
are not considered here, since these subjects are not
part of the noun classifier).
Valid article-NPhead structures account for
98.0% of all articles whose NP heads are common
nouns (47.5% of those are adjacent structures), as
shown in the last column of Table 6. 71.0% of arti-
cles in the training data belong to an NP whose head
is a common noun; NPs whose heads belong to dif-
ferent parts of speech are not considered.
Note also that because a noun may belong both to
an article-NPhead and a subject-verb structure, the
structures contain an overlap.
Distance Accuracy % of all subj. Cumul.
predictions
-1 97.6% 51.7% 51.7%
1,2,3 100.0% 8.9% 60.6%
-2 88.2% 6.9% 67.5%
Other 80.8% 32.5% 100.0%
Table 5: Accuracy of subject identification on a random
sample of subject-verb structures from the training data.
The overall accuracy is 91.52%. For each distance, the follow-
ing are shown: accuracy based on comparison with human eval-
uation; the percentage of all predictions that have this distance;
the cumulative percentage.
Distance Accuracy % of all head Cumul.
predictions
1 94.8% 47.5% 47.5%
2 94.4% 44.0% 91.5%
3 92.3% 6.5% 98.0%
Other 89.1% 2.0% 100%
Table 6: Accuracy of NP head identification on a random
sample of article-NPhead structures from training data. The
overall accuracy is 94.45%. For each distance, the following are
shown: accuracy based on comparison with human evaluation;
the percentage of all predictions that have this distance; the cu-
mulative percentage.
5 The Joint Model
In this section, we present the joint inference and
the joint learning approaches. In the joint inference
approach, we use the independently-learned models
from the Illinois system, and the interacting target
words identified earlier are considered only at infer-
ence stage. In the joint learning method, we jointly
learn a model for the interacting phenomena.
The label space in the joint models corresponds
to sequences of labels from the confusion sets of
the individual classifiers: {a ? sing, a ? pl, the ?
sing, the ? pl,? ? sing,? ? pl} and {sing ?
sing, sing?pl, pl?sing, pl?pl} for article-NPhead
and subject-verb structures, respectively7. Invalid
7?sing? and ?pl? refer to the grammatical number of noun
795
structures, such as pl-sing are excluded via hard con-
straints (when we run joint inference) or via implicit
soft constraints (when we use joint learning).
5.1 Joint Inference
In the individual model approach, decisions are
made for each word independently, ignoring the in-
teractions among linguistic phenomena. The pur-
pose of joint inference is to include linguistic (i.e.
structural) knowledge, such as ?plural nouns do not
take an indefinite article?, and ?agreement consis-
tency between the verb and the subject that controls
it?. This knowledge should be useful for resolving
inconsistencies produced by individual classifiers.
The inference approach we develop in this paper
follows the one proposed by Roth and Yih (2004)
of training individual models and combining them
at decision time via joint inference. The advantage
of this method is that it allows us to build upon
any existing independently-learned models that pro-
vide a distribution over their outcome, and produce
a coherent global output that respects our declarative
constraints. We formulate our component inference
problems as integer linear program (ILP) instances
as in Roth and Yih (2004).
The inference takes as input the individual clas-
sifiers? confidence scores for each prediction, along
with a list of constraints. The output is the optimal
solution that maximizes the linear sum of the confi-
dence scores, subject to the constraints that encode
the interactions. The joint model thus selects a hy-
pothesis that both obtains the best score according
to the individual models and satisfies the constraints
that reflect the interactions among the grammatical
phenomena at the level of linguistic structures, as
defined in Sec. 4.
Inference The joint inference is enforced at the
level of structures, and each structure corresponds
to one ILP instance. All structures consist of two or
three words: when an article-NPhead structure and
a subject-verb structure include the same noun, the
structure input to the ILP consists of an article-noun-
and verb agreement candidates. The candidates themselves are
the surface forms of specific words that realize these grammat-
ical properties. Note that a subject in subject-verb structures is
always third person, since all subjects in subject-verb structures
are common nouns; other subjects, including pronouns, are ex-
cluded. Thus the agreement distinction is singular vs. plural.
verb triple. We formulate the inference problem as
follows: Given a structure s that consists of n words,
let wi correspond to the ith word in the structure. Let
h denote a hypothesis from the hypothesis space H
for s, and score(wi, h, li) denote the score assigned
by the appropriate error-specific model to wi under
h for label l from the confusion set of word wi. We
denote by ew,l the Boolean variable that indicates
whether the prediction on word w is assigned the
value l (ew,l = 1) or not (ew,l = 0).
We assume that each independent classifier re-
turns a score that corresponds to the likelihood of
word wi under h being labeled li. The softmax func-
tion (Bishop, 1995) is used to convert raw activation
scores to conditional probabilities for the discrimi-
native article model. The NB scores are also normal-
ized and correspond to probabilities. Then the infer-
ence task is solved by maximizing the overall score
of a candidate assignment of labels l to words w (this
set of feasible assignments is denoted H here) sub-
ject to the constraints C for the structure s:
h? = argmax
h?H
score(h) =
= argmax
h?H
n?
i=1
score(wi, h, l
i)ewi,li
subject to C(s)
Constraints In the {0, 1} linear programming for-
mulation described above, we can encode linguis-
tic constraints that reflect the interactions among the
linguistic phenomena. The inference enforces the
following structural and linguistic constraints:
1. The indefinite article ?a? cannot refer to an NP headed by
a plural noun.
2. Subject and verb must agree in number.
In addition, we encode ?legitimacy? constraints, that
make sure that each w is assigned a single label. All
constraints are encoded as hard constraints.
5.2 Joint Learning
We now describe how we learn the subject-verb and
article-NPhead structures jointly. The joint model is
implemented as a NB classifier and is trained in the
same way as the independent models on the Google
corpus with word n-gram features. Unlike the inde-
pendent models, where the target corresponds to one
796
System Adjacent structures All distances
F1 (Orig.) F1 (Revised) F1 (Orig.) F1 (Revised)
Illinois 31.20 42.14 31.20 42.14
Na??veVerb 31.19 42.20 31.13 42.16
Na??veNoun 31.03 41.87 30.91 41.70
This paper joint systems Joint Inference (adjacent) Joint Inference (all distances)
F1 (Orig.) F1 (Revised) F1 (Orig.) F1 (Revised)
Subject-verb 31.90 42.94 31.97 42.86
Article-NPhead 31.63 42.48 31.79 42.59
Subject-verb + article-NPhead 32.35 43.16 32.51 43.19
Table 7: Joint Inference Results. All results are on the CoNLL-2013 test data using the original and revised gold annotations.
Adjacent denotes a setting, where the joint inference is applied to structures with consecutive components (article-NPhead or
subject-verb). All distances denotes a setting, where the constraints are applied to all valid structures, as described in Sec. 4.1.
Illinois denotes the result obtained by the top CoNLL-2013 shared task system. In all cases, the candidates that are not part of the
structures are handled by the respective components of the Illinois system. Na??veVerb and Na??veNoun denote heuristics, where a
verb or subject are changed to ensure agreement. All improvements over the Illinois system are statistically significant (McNemar?s
test, p < 0.01).
word, here the target corresponds to two words that
are part of the structure and the label space of the
model is modified accordingly. Since we use fea-
tures that can be computed from the small windows
in the Google corpus, the joint learning model han-
dles only adjacent structures (Sec. 4.1). Because the
target consists of two words and the Google corpus
contains counts for n-grams of length at most five,
the features are collected in the three word window
around the target.8
Unlike with the joint inference, here we do not
explicitly encode linguistic constraints. One reason
for this is that the NP head and subject predictions
are not 100% accurate, so input structures will have
noise. However, the joint model learns these con-
straints through the evidence seen in training.
6 Experiments
In this section, we describe our experimental setup
and evaluate the performance of the joint approach.
In the joint approach, the joint components pre-
sented in Sec. 5 handle the interacting structures de-
scribed in Sec. 4. The individual classifiers of the
Illinois system make predictions for the remaining
words. The research question addressed by the ex-
periments is the following: Given independently-
trained systems for different types of errors, can we
improve the performance by considering the phe-
8Also note that when the article is ?, the surface form of
the structure corresponds to the NP head alone; this does not
present a problem because in the NB model the context counts
are normalized with the prior counts.
nomena that interact jointly? To address this, we
report the results in the following settings:
1. Joint Inference: we compare the Illinois sys-
tem that is a collection of individually-trained mod-
els that are applied independently with a model
that uses joint inference encoded as declarative con-
straints in the ILP formulation and show that using
joint inference results in a strong performance gain.
2. Joint Learning: we compare the Illinois system
with a model that incorporates jointly-trained com-
ponents for the two linguistic structures that we de-
scribed in Sec. 4. We show that joint training pro-
duces an even stronger gain in performance com-
pared to the Illinois model.
2. Joint Learning and Inference: we apply joint in-
ference to the output of the joint learning system to
account for dependencies not covered by the joint
learning model.
We report F1 performance scored using the offi-
cial scorer from the shared task (Dahlmeier and Ng,
2012). The task reports two types of evaluation: on
the original gold data and on gold data with addi-
tional corrections. We refer to the results as Origi-
nal and Revised.
6.1 Joint Inference Results
Table 7 shows the results of applying joint infer-
ence to the Illinois system. Both the article-NPhead
and the subject-verb constraints improve the perfor-
mance. The results for the joint inference are shown
in two settings, adjacent and all structures, so that
later we can compare joint inference with the joint
learning model that handles only adjacent structures.
797
Illinois system Illinois-NBArticle
F1 (Orig.) F1 (Revised) F1 (Orig.) F1 (Revised)
Illinois 31.20 42.14 31.71 41.38
This paper joint systems Joint Learning (adjacent) Joint Learning (adjacent)
F1 (Orig.) F1 (Revised) F1 (Orig.) F1 (Revised)
Subject-verb 32.64* 43.37* 33.09* 42.78*
Article-NPhead 33.89* 42.57* 33.16* 41.51
Subject-verb + article-NPhead 35.12* 43.73* 34.41* 42.76*
Table 8: Joint Learning Results. All results are on the CoNLL-2013 test data using the original and revised gold annotations.
Illinois-NBArticle denotes the Illinois system, where the discriminative article model is replaced with a NB classifier. Adjacent
denotes a setting, where the structure components are consecutive (article-NPhead or subject-verb), as described in Sec. 4.1.
Illinois denotes the result obtained by the top CoNLL-2013 shared task system. In all cases, the candidates that are not part of the
structures are handled by the respective components of the Illinois system. Statistically significant improvements (McNemar?s test,
p < 0.01) over the Illinois system are marked with an asterisk (*).
It is also interesting to note that the key improvement
comes from considering structures whose compo-
nents are adjacent. This is not surprising given that
the accuracy for subject and NP head identification
drops as the distance increases.
For subject-verb constraints, we also implement
a na??ve approach that looks for contradictions and
changes either the verb or the subject if they do not
satisfy the number agreement. These two heuris-
tics are denoted as Na??veVerb and Na??veNoun. The
heuristics differ from the joint inference in that they
enforce agreement by always changing either the
noun (Na??veNoun) or the verb (Na??veVerb), while
the joint inference does this using the scores pro-
duced by the independent models. In other words,
the key is the objective function, while the compo-
nents of the objective function are the same in the
heuristics and the joint inference. The results in Ta-
ble 7 show that simply enforcing agreement does not
work well and that the ILP formulation is indeed ef-
fective and improves over the independently-trained
models in all cases.
Recall that valid structures include only those
whose components can be identified in a reliable
way (Sec. 4.1). To evaluate the impact of that filter-
ing, we perform two experiments with subject-verb
structures (long-distance dependencies are more
common in those constructions than in the article-
NPhead structures): first, we apply joint inference
to all subject-verb structures. We obtain F1 scores of
31.61 and 42.28, on original and revised gold data,
respectively, which is significantly worse than the
results on subject-verb structures in Table 7 (31.97
and 42.86, respectively) and only slightly better than
the baseline performance of the Illinois system. Fur-
thermore, when we apply joint inference to those
structures which were excluded by filtering in Sec.
4.1, we find that the performance degrades com-
pared to the Illinois system (30.85 and 41.58). These
results demonstrate that the joint inference improve-
ments are due to structures whose components can
be identified with high accuracy and that it is essen-
tial to identify these structures; bad structures, on the
other hand, hurt performance.
6.2 Joint Learning Results
Now we show experimental results of the joint learn-
ing (Table 8). Note that the joint learning component
considers only those structures where the words are
adjacent. Because the Illinois system presented in
Sec. 3 makes use of a discriminative article model,
while the joint model uses NB, we also show results,
where the article model is replaced by a NB classi-
fier trained on the Google corpus. In all cases, joint
learning demonstrates a strong performance gain.
6.3 Joint Learning and Inference Results
Finally, we apply joint inference to the output of the
joint learning system in Sec. 6.2. Table 9 shows
the results of the Illinois model, the model that ap-
plies joint inference and joint learning separately,
and both. Even though the joint learning performs
better than the joint inference, the joint learning
covers only adjacent structures. Furthermore, joint
learning does not address overlapping structures of
triples that consist of article, subject, and verb (6%
of all structures). Joint inference allows us to ensure
consistent predictions in cases not addressed by the
798
Example Illinois system JL and JI
?Moreover, the increased technologies help people to overcome
different natural disasters.
No change technology helps
?At that time,... there are surveillances in everyone?s heart and
criminals are more difficult to hide.?
there are* surveillance* there is surveillance
?In such situation, individuals will lose their basic privacy.? such a* situations* such a situation
?In supermarket monitor is needed because we have to track
thieves.?
No change monitors are
Table 10: Examples of mistakes that are corrected by the joint model but not by the Illinois model. Illinois denotes the result
obtained by the top CoNLL-2013 shared task system from the University of Illinois. JL and JI stand for joint learning and joint
inference, respectively. Inconsistent predictions are starred.
F1 (Orig.) F1 (Revised)
Illinois 31.20 42.14
Joint Inference 32.51 43.19
Joint Learning 35.12 43.73
Joint Learn. + Inf. 35.21 43.74
Table 9: Joint Learning and Inference. All results are on the
CoNLL-2013 test data using the original and revised gold anno-
tations. Results of the joint models that include the joint infer-
ence component are shown for structures of all distances. Illi-
nois denotes the result obtained by the top CoNLL-2013 shared
task system. All joint systems demonstrate a statistically sig-
nificant improvement over the Illinois system; joint learning
improvements are also statistically significant compared to the
joint inference results (McNemar?s test, p < 0.01).
joint learning model. Indeed, we can get a small im-
provement by adding joint inference on top of the
joint learning on original annotations. Since the re-
vised corrections are based on the participants? input
and are most likely biased towards system predic-
tions for corrections missed by the original annota-
tors (Ng et al, 2013), it is more difficult to show
improvement on revised data.
7 Discussion and Error Analysis
In the previous section, we evaluated the proposed
joint inference and joint learning models that han-
dle interacting grammatical phenomena. We showed
that the joint models produce significant improve-
ments over the highest-scoring CoNLL-2013 shared
task system that consists of independently-trained
classifiers: the joint approaches increase the F1
score by 4 F1 points on the original gold data and
almost 2 points on the revised data (Table 9).
These results are interesting from the point of
view of developing a practical error correction sys-
tem. However, recall that the errors in the interact-
ing structures are only a subset of mistakes in the
CoNLL-2013 data set but the evaluation in Sec. 6 is
performed with respect to all of these errors. From
a scientific point of view, it is interesting to evalu-
ate the impact of the joint models more precisely by
considering the improvements on the relevant struc-
tures only. Table 11 shows how much the joint learn-
ing approach improves on the subset of relevant mis-
takes.
Structure
Performance (F1)
Illinois Joint Learning
Subject-verb 39.64 52.25
Article-NPhead 30.65 35.90
Table 11: Evaluation of the joint learning performance on
the subset of the data containing interacting errors. All re-
sults are on the CoNLL-2013 test data using the original anno-
tations. Illinois denotes the result obtained by the top CoNLL-
2013 shared task system. All improvements are statistically sig-
nificant over the Illinois system (McNemar?s test, p < 0.01).
Error Analysis To better understand where the joint
models have an advantage over the independently-
trained classifiers, we analyze the output produced
by each of the approaches. In Table 10 we show
examples of mistakes that the model that uses joint
learning and inference is able to identify correctly,
along with the original predictions made by the Illi-
nois system.
Joint Inference vs. Joint Learning We wish
to stress that the joint approaches do not simply
perform better but also make coherent decisions
by disallowing illegitimate outputs. The joint in-
ference approach does this by enforcing linguis-
tic constraints on the output. The joint learning
model, while not explicitly encoding these con-
straints, learns them from the distribution of the
training data.
799
Joint inference is a less expensive model, since it
uses the scores produced by the individual classifiers
and thus does not require additional training. Joint
learning, on the other hand, is superior to joint infer-
ence, since it is better at modeling interactions where
multiple errors occur simultaneously ? it eliminates
the noisy context present when learning the inde-
pendent classifiers. Consider the first example from
Table 10, where both the noun and the agreement
classifiers receive noisy input: the verb ?help? and
the noun ?technologies? act as part of input features
for the noun and agreement classifiers, respectively.
The noisy features prevent both modules from iden-
tifying the two errors.
Finally, an important distinction of the joint learn-
ing method is that it considers all possible output se-
quences in training, and thus it is able to better iden-
tify errors that require multiple changes, such as the
last example in Table 10, where the Illinois system
proposes no changes.
7.1 Error Correction: Challenges
We finalize our discussion with a few comments on
the challenges of the error correction task.
Task Difficulty As shown in Table 1 in Sec. 2, only
a small percentage of words have mistakes, while
over 90% (about 98% in training) are used correctly.
The low error rates are the key reason the error cor-
rection task is so difficult: it is quite challenging for
a system to improve over a writer that already per-
forms at the level of over 90%. Indeed, very few
NLP tasks already have systems that perform at that
level, even when the data is not as noisy as the ESL
data.
Evaluation Metrics In the CoNLL-2013 competi-
tion, as well as the competitions alluded to earlier,
systems were compared on F1 performance, and,
consequently, this is the metric we optimize in this
paper. Practical error correction systems, however,
should be tuned to minimize recall to guarantee that
the overall quality of the text does not go down. In-
deed, the error sparsity makes it very challenging to
identify mistakes accurately, and no system in the
shared task achieves a precision over 50%. How-
ever, once the precision drops below 50%, the sys-
tem introduces more mistakes than it identifies.
Clearly, optimizing the F1 measure does not en-
sure that the quality of the text improves as a re-
sult of running the system. Thus, it can be argued
that the F1 measure is not the right measure for er-
ror correction. A different evaluation metric based
on the accuracy of the data before and after running
the system was proposed in Rozovskaya and Roth
(2010c). When optimizing for this metric, the noun
module, for instance, at recall point 20%, achieves
a precision of 63.93%. This translates into accuracy
of 94.46%, while the baseline on noun errors in the
test data (i.e. the accuracy of the data before running
the system) is 94.0% (Table 1). This means that the
system improves the quality of the data.
Annotation Lastly, we believe that it is important
to provide alternative corrections, as the agreement
on what constitutes a mistake even among native
English speakers can be quite low (Madnani et al,
2011).
8 Conclusion
This work presented the first successful study that
jointly corrects grammatical mistakes. We ad-
dressed two pairs of interacting phenomena and
showed that it is possible to reliably identify their
components, thereby facilitating the joint approach.
We described two joint methods: a joint in-
ference approach implemented via ILP and a
joint learning model. The joint inference en-
forces constraints using the scores produced by the
independently-trained models. The joint learning
model learns the interacting phenomena as struc-
tures. The joint methods produce a significant im-
provement over a state-of-the-art system that com-
bines independently-trained models and, impor-
tantly, produce linguistically legitimate output.
Acknowledgments
The authors thank Peter Chew, Jennifer Cole, Mark Sam-
mons, and the anonymous reviewers for their helpful
feedback. The authors thank Josh Gioja for the code
that performs phonetic disambiguation of the indefinite
article. This material is based on research sponsored
by DARPA under agreement number FA8750-13-2-0008.
The U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwithstand-
ing any copyright notation thereon. The views and con-
clusions contained herein are those of the authors and
should not be interpreted as necessarily representing the
official policies or endorsements, either expressed or im-
plied, of DARPA or the U.S. Government.
800
References
C. Bishop. 1995. Neural Networks for Pattern Recogni-
tion, chapter 6.4: Modelling conditional distributions.
Oxford University Press.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1.
Linguistic Data Consortium, Philadelphia, PA.
C. Brockett, D. B. William, and M. Gamon. 2006.
Correcting ESL errors using phrasal SMT techniques.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 249?256, Sydney, Australia, July. Association
for Computational Linguistics.
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In IAAI.
J. Clarke and M. Lapata. 2007. Modelling compression
with discourse constraints. In Proceedings of the 2007
Joint Conference of EMNLP-CoNLL.
D. Dahlmeier and H.T Ng. 2012. A beam-search de-
coder for grammatical error correction. In EMNLP-
CoNLL, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL HLT
2013 Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, Atlanta, Georgia,
June. Association for Computational Linguistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own: The
HOO 2011 pilot shared task. In Proceedings of the
13th European Workshop on Natural Language Gen-
eration.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A re-
port on the preposition and determiner error correction
shared task. In Proc. of the NAACL HLT 2012 Seventh
Workshop on Innovative Use of NLP for Building Edu-
cational Applications, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
R. De Felice and S. Pulman. 2008. A classifier-based ap-
proach to preposition and determiner error correction
in L2 English. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 169?176, Manchester, UK, August.
Y. Freund and R. E. Schapire. 1996. Experiments with
a new boosting algorithm. In Proc. 13th International
Conference on Machine Learning.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings of
IJCNLP.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?171,
Los Angeles, California, June.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning.
N. Han, M. Chodorow, and C. Leacock. 2006. Detecting
errors in English article usage by non-native speakers.
Journal of Natural Language Engineering, 12(2):115?
129.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the Japanese
learners? English spoken data. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
145?148, Sapporo, Japan, July.
J. Lee and S. Seneff. 2008. Correcting misuse of verb
forms. In ACL, pages 174?182, Columbus, Ohio,
June. Association for Computational Linguistics.
N. Madnani, M. Chodorow, J. Tetreault, and A. Ro-
zovskaya. 2011. They can help: Using crowdsourcing
to improve the evaluation of grammatical error detec-
tion systems. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 508?513, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
M. Marneffe, B. MacCartney, and Ch. Manning. 2006.
Generating typed dependency parses from phrase
structure parses. In LREC.
A. Martins, Noah N. Smith, M. Figueiredo, and P. Aguiar.
2011. Dual decomposition with many overlapping
components. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, pages 238?249, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task on
grammatical error correction. In Proc. of the Sev-
enteenth Conference on Computational Natural Lan-
guage Learning. Association for Computational Lin-
guistics.
A. Park and R. Levy. 2011. Automated whole sentence
grammar correction using a noisy channel model. In
ACL, Portland, Oregon, USA, June. Association for
Computational Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, CoNLL.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
801
NAACL Workshop on Innovative Use of NLP for Build-
ing Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Generating con-
fusion sets for context-sensitive error correction. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
A. Rozovskaya and D. Roth. 2010c. Training paradigms
for correcting errors in grammar and usage. In
NAACL.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks. In
ACL.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task.
A. Rozovskaya, M. Sammons, and D. Roth. 2012. The
UI system in the HOO 2012 shared task on error cor-
rection.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system in
the CoNLL-2013 shared task. In CoNLL Shared Task.
C. Sutton and A. McCallum. 2007. Piecewise pseudo-
likelihood for efficient training of conditional random
fields. In Zoubin Ghahramani, editor, ICML.
J. Tetreault and M. Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
865?872, Manchester, UK, August.
Y. Wu and H.T. Ng. 2013. Grammatical error correction
using integer linear programming. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
1456?1465, Sofia, Bulgaria, August. Association for
Computational Linguistics.
802
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1787?1796,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Relational Inference for Wikification
Xiao Cheng Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{cheng88,danr}@illinois.edu
Abstract
Wikification, commonly referred to as Disam-
biguation to Wikipedia (D2W), is the task of
identifying concepts and entities in text and
disambiguating them into the most specific
corresponding Wikipedia pages. Previous ap-
proaches to D2W focused on the use of lo-
cal and global statistics over the given text,
Wikipedia articles and its link structures, to
evaluate context compatibility among a list of
probable candidates. However, these meth-
ods fail (often, embarrassingly), when some
level of text understanding is needed to sup-
port Wikification. In this paper we introduce
a novel approach to Wikification by incorpo-
rating, along with statistical methods, richer
relational analysis of the text. We provide an
extensible, efficient and modular Integer Lin-
ear Programming (ILP) formulation of Wik-
ification that incorporates the entity-relation
inference problem, and show that the ability
to identify relations in text helps both candi-
date generation and ranking Wikipedia titles
considerably. Our results show significant im-
provements in both Wikification and the TAC
Entity Linking task.
1 Introduction
Wikification (D2W), the task of identifying concepts
and entities in text and disambiguating them into
their corresponding Wikipedia page, is an important
step toward supporting deeper textual understand-
ing, by augmenting the ability to ground text in ex-
isting knowledge and facilitating knowledge expan-
sion.
D2W has been studied extensively recently
(Cucerzan, 2007; Mihalcea and Csomai, 2007;
Milne and Witten, 2008; Ferragina and Scaiella,
2010; Ratinov et al, 2011) and has already found
broad applications in NLP, Information Extraction,
and Knowledge Acquisition from text, from coref-
erence resolution (Ratinov and Roth, 2012) to entity
linking and knowledge population (Ellis et al, 2011;
Ji et al, 2010; Cucerzan, 2011).
Given a document D containing a set of concept
and entity mentionsM ( referred to later as surface),
the goal of Wikification is to find the most accurate
mapping from mentions to Wikipedia titles T ; this
mapping needs to take into account our understand-
ing of the text as well as background knowledge that
is often needed to determine the most appropriate ti-
tle. We also allow a special NIL title that captures
all mentions that are outside Wikipedia.
Earlier approaches treated this task as a word-
sense disambiguation (WSD) problem, which was
later enhanced with a certain level of global rea-
soning, but essentially all approaches focused on
generic statistical features in order to achieve robust
disambiguation. It was shown that by disambiguat-
ing to the most likely title for every surface, in-
dependently maximizing the conditional probability
Pr(title|surface), we already achieve a very com-
petitive baseline on several Wikification datasets
(Ratinov et al, 2011). This strong statistical baseline
makes use of the relatively comprehensive coverage
of the existing Wikipedia links from surface strings
to Wikipedia titles. Although more involved statis-
tical features are required in order to make substan-
tial improvements, global features such as context
TF-IDF, better string similarity, etc., statistics-based
Wikification systems give a fairly coherent set of
disambiguation when sufficient context is available.
Consider the following example: Earth?s biosphere
1787
then significantly altered the atmospheric and other ba-
sic physical conditions, which enabled the proliferation
of organisms. The atmosphere is composed of 78.09%
nitrogen, 20.95% oxygen, 0.93% argon, 0.039% carbon
dioxide, and small amounts of...
The baseline system we adopted (Ratinov et al,
2011), one of the best Wikification systems, al-
ready disambiguates atmosphere correctly to the ti-
tle Earth?s atmosphere instead of the more general
title Atmosphere, making use of the concept Earth in
its local context to resolve the mention to the more
specific title that better coheres with the topic. How-
ever, consider the following example:
Ex. 1 ?As Mubarak, the wife of deposed Egyptian
President Hosni Mubarak got older, her influence...?
The bold faced name should be mapped to Suzanne
Mubarak, but all existing Wikification systems map
both names in this sentence to the dominant page
(the most linked page) of Hosni Mubarak, failing to
understand the relation between them, which should
prevent them from being mapped to the same page.
A certain level of text understanding is required even
to be able to generate a good list of title candidates.
For example, in:
Ex. 2 ?...ousted long time Yugoslav President Slo-
bodan Milos?evic? in October. Mr. Milos?evic??s So-
cialist Party...?
the bold-faced concept should be mapped to the
page of the Socialist Party of Serbia, which is far
down the list of titles that could be related to ?So-
cialist Party?; making this title a likely candidate
requires understanding the possessive relation with
Milos?evic? and then making the knowledge-informed
decision that he is more related to Socialist Party of
Serbia than any other possible titles. Finally, in
Ex. 3 ?James Senn, director of Robinson College?s
Center for Global Business Leadership at Georgia
State University...?
we must link Robinson College to J. Mack Robin-
son College of Business which is located at Geor-
gia State University instead of Robinson College,
Cambridge, which is the only probable title linked
by the surface Robinson College in the version of
the Wikipedia dump we used.
These examples further illustrate that, along with
understanding the relation expressed in the text, we
need to access background knowledge sources and
to deal with variability in surface representation
across the text, Wikipedia, and knowledge, in order
to reliably address the Wikification problem.
In this paper we focus on understanding those nat-
ural language constructs that will allow eliminat-
ing these ?obvious? (to a human reader) mistakes
from Wikification. In particular, we focus on resolv-
ing coreference and a collection of local syntactico-
semantic relations (Chan and Roth, 2011); better un-
derstanding the relational structure of the text allows
us to generate title candidates more accurately given
the text, rank these candidates better and determine
when a mention in text has no corresponding title
in Wikipedia and should be mapped to NIL, a key
problem in Wikification. Moreover, it allows us to
access external knowledge based resources more ef-
fectively in order to support these decisions.
We incorporate the outcome of our relational
analysis, along with the associated features extracted
from external sources and the ?standard? wikifica-
tion statistical features, into an ILP-based inference
framework that globally determines the best assign-
ment of mentions to titles in a given document. We
show that by leveraging a better understanding of
the textual relations, we can substantially improve
the Wikification performance. Our system signifi-
cantly outperforms all the top Wikification systems
on the widely adopted standard datasets and shows
state-of-the-art results when evaluated (without be-
ing trained directly) on the TAC 2011 Entity Linking
task.
2 The Wikification Approach
A general Wikification decision consists of three
computational components: (1) generating a ranked
list of title candidates for each mention, (2) rank-
ing candidates globally, and (3) dealing with NIL
mentions. For (1), the ?standard? way of using
Pr(title|surface) is often not sufficient; consider
the case where the mention is the single word ?Presi-
dent?; disambiguating such mentions depends heav-
ily on the context, i.e. to determine the relevant
country or organization. However, it is intractable to
search the entire surface-to-title space, and using an
arbitrary top-K list will inevitably leave out a large
number of potential solutions. For (2), even though
1788
the anchor texts cover many possible ways of para-
phrasing the Wikipedia article titles and thus using
the top Pr(title|surface) is proven to be a fairly
strong baseline, it is never comprehensive. There is
a need to disambiguate titles that were never linked
by any anchor text, and to disambiguate mentions
that have never been observed as the linked text. For
(3) the Wikifier needs to determine when a mention
corresponds to no title, and map it to a NIL entity.
Simply training a classifier using coherency features
or topical models turns out to be insufficient, since it
has a predetermined granularity at which it can dis-
tinguish entities.
Next we provide a high-level description (Alg. 1)
of our approach to improve Wikification by leverag-
ing textual relations in these three stages.
Algorithm 1 Relational Inference for Wikification
Note: ? : M ? T is the sought after mapping from
all mentions in the document to all candidate titles
in Wikipedia.
Require: Document D, Knowledge Base K con-
sisting of relation triples ? = (ta, p, tb), where
p is the relation predicate.
1: Generate initial mentions M = {mi} from D.
2: Generate candidates ti = {tki } for mention mi
and initialize candidate priors Pr(tki |mi) with
existing Wikification system, for all mi ?M .
3: Instantiate non-coreference relational con-
straints and add relational candidates.
4: Instantiate coreference relational constraints
and add relational candidates.
5: Construct an ILP objective function and solve
for the arg max? Pr(?).
6: return ?.
Most of our discussion addresses the relational
analysis and its impact on stage (2) and (3) above.
We will only briefly discuss improvements to the
standard candidate generation stage in Sec. 4.4
3 Problem Formulation
We now describe how we formulate our global deci-
sion problem as an Integer Linear Program (ILP).
We use two types of boolean variables: eki is
used to denote whether we disambiguate mi to tki
(?(mi) = tki ) or not. r
(k,l)
ij is used to denote if
titles tki and t
l
j are chosen simultaneously, that is,
r(k,l)ij = e
k
i ? e
l
j .
Our models determine two types of score for
the boolean variables above: ski = Pr(e
k
i ) =
Pr(?(mi) = tki ), represents the initial score for the
kth candidate title being chosen for mentionmi. For
a pair of titles (tki , t
l
j), we denote the confidence of
finding a relation between them by w(k,l)ij . Its value
depends on the textual relation type and on how co-
herent it is with our existing knowledge.
Our goal is to find the best assignment to vari-
ables eki , such that it satisfies some legitimacy (hard)
constraints and the soft constraints dictated by the
relational constraints (via scores w(k,l)ij ). To accom-
plish that we define our objective function as a Con-
strained Conditional Model (CCM) (Roth and Yih,
2004; Chang et al, 2012) that is used to reward or
penalize a pair of candidates tki , t
l
j by w
(k,l)
ij when
they are chosen in the same document. Specifically,
we choose the assignment ?D that optimizes:
?D = arg max
?
?
i
?
k
ski e
k
i +
?
i,j
?
k,l
w(k,l)ij r
(k,l)
ij
s.t. r(k,l)ij ? {0, 1} Integral constraints
eki ? {0, 1} Integral constraints
?i
?
k e
k
i = 1 Unique solution
2r(k,l)ij ? e
k
i + e
l
j Relation definition
Note that as in most NLP problems, the prob-
lem is very sparse, resulting in a tractable ILP
that is solved quickly by off-the-shelf ILP packages
(Gurobi Optimization, 2013). In our case the key
reason for the sparseness is that w(k,l)ij = 0 for most
pairs considered, which does not require explicit in-
stantiation of r(k,l)ij .
4 Relational Analysis
The key challenge in incorporating relational anal-
ysis into the Wikification decision is to systemati-
cally construct the relational constraints (the solid
edges between candidates in Figure 1) and incorpo-
rate them into our inference framework. Two main
components are needed: first, we need to extract
high precision textual relations from the text; then,
1789
Slobodan Milo?evi?
...
...
Savo Milo?evi?
Slobodan Milo?evi? Socialist Party (France)
Socialist Party
...
Socialist Party of Serbia
Yugoslavia President
...
President of the Federal Republic of Yugoslavia
...ousted long time [ Yugoslav President ] [Slobodan Milo?evi?] in October. Mr. [Milo?evi?]'s [Socialist Party] ...
CoreferenceApposition
search in lexical/relational space
search in lexical and P(title|surface) space
Possessive
m
1
m
2
m
3
m
4
t
1
t
2
...
founder_ofholds_office
=
Figure 1: Textual relation inference framework: The goal is to maximize the objective function assigning mentions
to titles while enforcing coherency with relations extracted from both text and an external knowledge base. Here,
searching the external KB reveals that Slobodan Milos?evic? is the founder of the Socialist Party of Serbia, which can be
referred to by the surface Socialist Party; we therefore reward the output containing this pair of candidates. The same
idea applies for the relation ?Slobodan Milos?evic? holds office as President of the Federal Republic of Yugoslavia? as
well as to the coreference relation between two mentions of Slobodan Milos?evic?.
we need to assign weights to these semantic rela-
tions. We determine the weights by combining type
and confidence of the relation extracted from text
with the confidence in relations retrieved from an ex-
ternal Knowledge Base (KB) by using the mention
pairs as a query. It is noteworthy that although con-
text window based coherency objective functions
capture many proximity relations, using these unfil-
tered relations as constraints in our experiments in-
troduced excessive amount of false-positives for the
intrinsically sparse textual relations and resulted in
severe performance hit.
In Sec. 4.1 we describe how we extract relations
from text; our goal is to reliably identify arguments
that we hypothesize to be in a relation; we show
that this is essential both to our candidate genera-
tion, our ranking and the mapping to NIL. Sec. 4.2
describes how we use an external KB to verify that
these arguments are indeed in a relation. Finally,
Sec. 4.3 shows how we generate scores for the men-
tions and relations, as coefficients in the objective
function of Sec. 3. The process is illustrated in Fig-
ure 1. Overall, our approach is an ambiguity-aware
approach that identifies, filters and scores the rele-
vant relations; this is essential due to the ambiguity,
variability and noise inherent in directly matching
surface forms to titles.
4.1 Relation Extraction
Even though relation extraction is an open prob-
lem, analysis on the ACE2004 Relation Detection
and Characterization (RDC) dataset shows that ap-
proximately 80% of the relations are expressed
through syntactico-semantic structures (Chan and
Roth, 2011) that are easy to extract with high pre-
cision. Unlike the general ACE RDC task, we can
restrict relation arguments to be named entities and
thus leverage the large number of known relations in
existing databases (e.g. Wikipedia infoboxes). We
also consider conference relations that potentially
aid mapping different mentions to the same title.
4.1.1 Syntactico-semantic Relations
We introduce our approach using the following
example. Consider a news article discussing Israeli
politics while briefly mentioning:
Ex. 4 An official at the [Iranian]1 [Ministry of
Defense]2 told Tehran Radio that...
A purely statistical approach would very likely map
the entity [Ministry of Defense]2 to Ministry of De-
fense (Israel) instead of Ministry of Defense and
Armed Forces Logistics (Iran) because the context is
more coherent with concepts related to Israel rather
than to Iran. Nevertheless, the pre-modifier relation
between [Iranian]1 and [Ministry of Defense]2 de-
mands the answer to be tightly related to Iran. Even
though human readers may not know the correct ti-
tle needed here, understanding the pre-modifier re-
lation allows them to easily filter through a list of
candidates and enforce constraints that are derived
jointly from the relation expressed in the text and
their background knowledge.
In our attempt to mimic this general approach, we
employ several high precision classifiers to resolve
1790
a range of local relations that are used to retrieve
relevant background knowledge, and consequently
integrated into our inference framework. Our in-
put for relation extraction is any segment matched
by the regular expression to be mentioned in sec-
tion 4.4 in the candidate generation stage; we ana-
lyze its constituents by decomposing it into the two
largest sub-entities that have (in Wikipedia) corre-
sponding candidates. In the above example, Ira-
nian Ministry of Defense would be decomposed into
Iranian and Ministry of Defense and our relation
extraction process hypothesizes a relation between
these arguments.
Note that we do not use any full parsing since it
does not address our needs directly nor does it scale
well with the typical amount of data used in Wikifi-
cation.
4.1.2 Coreference Relations
In addition to syntactico-semantic relations, we
could also encounter other textual relations. The fol-
lowing example illustrates the importance of under-
standing co-reference relations in Wikification:
Ex. 5 [Al Goldman]1, chief market strategist at
A.G. Edwards, said ... [Goldman]2 told us that...
There is no Wikipedia entry (or redirection) that
matches the name Al Goldman. Clearly [Goldman]2
refers to the same person and should be mapped to
the same entity (or to NIL) rather than popular en-
tities frequently referred to as Goldman, coherent
with context or not, such as Goldman Sachs. To ac-
complish that, we cluster named entities that share
tokens or are acronyms of each other when there
is no ambiguity (e.g. no other longer named en-
tity mentions containing Goldman in the document)
and use a voting algorithm (Algorithm 2) to generate
candidates locally from within the clusters. We also
experimented with using full-fledged coference sys-
tems, but found it to be time consuming while pro-
viding no significant end-to-end performance differ-
ence.
4.1.3 Coreferent Nominal Mentions
Document level coreference also provides impor-
tant relations between named entities and nominal
mentions. Extracting these relations proved to be
very useful for classifying NIL entities, as unfamil-
iar concepts tend to be introduced with these suc-
cinct appositional nominal mentions. These descrip-
tions provide a clean ?definition? of the entity, al-
lowing us to abstract the inference to a limited ?noun
phrase entailment problem?. That is, it allows us to
determine whether the target mention corresponds to
a candidate title. Consider, for example, wikifying
Dorothy Byrne in: Dorothy Byrne, a state coordi-
nator for the Florida Green Party, . . .
Identifying the apposition relation allows us to de-
termine that this Dorothy Byrne is not the baseline
Wikipedia title. We use the TF-IDF cosine similar-
ity between the nominal description and the lexical
context (Ratinov et al, 2011) of the candidate page,
head word attributes and entity relation (i.e. between
Dorothy Byrne and Florida Green Party) to deter-
mine whether any candidates of Dorothy Byrne can
entail the nominal mention.
4.2 Relational Queries
Statistics based candidate generation algorithms al-
ways generate the same list of candidates given the
same surface string; even though this approach has
a competitive coverage rate, it will not work well
in some ?obvious? (to human) cases; for example,
it offers very little information on highly ambigu-
ous surface strings such as ?President? for which it
is even intractable to rank all the candidates. Top-
K lists which were used in previous literature suf-
fer from the same problem. Instead, we make use
of relational queries to generate a more likely set of
candidates.
Once mention pairs are generated from text us-
ing the syntactico-semantic structures and corefer-
ence, we use these to query our KB of relational
triples. We first indexed all Wikipedia links and DB-
pedia relations as unordered triples ? = (ti, p, tj),
where the arguments ti, tj are tokenized, stemmed
and lowercased for best recall. p is either a relation
predicate from the DBpedia ontology or the predi-
cate LINK indicating a hyperlink relation. Since
our baseline system has approximately 80% accu-
racy at this stage, it is reasonable to assume that at
least one of the argument mentions is correctly dis-
ambiguated. Therefore we prune the search space
by making only two queries for each mention pair
(mi,mj): q0 = (t?i ,mj) and q1 = (mi, t
?
j ) where
t?i , t
?
j are the strings representing the top titles cho-
sen by the current model for mentions mi,mj re-
1791
spectively.
We also aggressively prune the search results in
a way similar to the process in Sec. 4.4, only keep-
ing the arguments that are known to be possible or
very likely candidates of the mention, based on the
ambiguity that exists in the query result.
4.3 Relation Scoring
For the final assignment made using our objective
function (Sec. 3) we need to normalize and rescale
the output of individual components of our system as
they come from different scoring functions. We con-
sider adding new title candidates from two sources,
through the coreference module and through the
combined DBpedia and Wikipedia inter-page link
structures. Next we describe how to compute and
combine these scores.
4.3.1 Scoring Knowledge Base Relations
Our model uses both explicit relations p 6=
LINK from DBpedia and Wikipedia hyperlinks
p = LINK (implicit relation). We want to favor
relations with explicit predicate, each weighted as ?
implicit relation (we use ? = 5 in our experiments,
noting the results are insensitive to slight changes of
this parameter).
For each query, we denote the score returned by
our KB search engine1 given query q and triple ?
as Sim?,q. The relational weight w
k,l
i,j between two
candidates (see Sec. 3) is determined as:
wk,li,j =
1
Z
?
?
??Sim?,q
where the sum is over the top 20 KB triples, ?? is
the relation type scaling constant (? or 1), and Z is
a normalization factor that normalizes all wk,li,j to the
range [0, 1].
Note that we do not check the type of the relation
against the textual relation. The key reason is that
explicit relations are not as robust, especially con-
sidering that we restrict one of the arguments in the
relation and constraining the other argument?s lexi-
cal form. Moreover, we back off to restricting the re-
lations to be between known candidates when mul-
tiple lexically matched arguments are retrieved with
high ambiguity. Additionally, most of our relations
1http://lucene.apache.org/
Algorithm 2 Coreferent Candidates Voting
Require: Coreference cluster C
1: Vote collector vt denotes the score for a candi-
date t, which by default is 0.
2: ti = {t1i . . . t
n
i } is the set of candidates of men-
tion mi.
3: li is the token count of mi
4: for all mi ? C, li ? 2 do
5: for all tki ? ti do
6: vtki = vtki + s
k
i
7: end for
8: end for
9: Let AllSingle denote whether ?i, li = 1
10: for all mi ? C where li = 1 do
11: for all tki ? ti do
12: if AllSingle or vtki > 0 then
13: vtki = vtki + s
k
i
14: end if
15: end for
16: end for
17: return v
do not have explicit predicates in the text anyhow,
and extracting a type would add noise to our deci-
sion.
4.3.2 Scoring Coreference Relations
For coreference relations, we simply use hard
constraints by assigning candidates in the same
coreference cluster a high relational weight, which
is a cheap approximation to penalizing the output
where the coreferent mentions disambiguate to dif-
ferent titles. In practice, using a weight of 10 is suf-
ficient. Another important issue here is that the cor-
rect coreferent candidate might not exist in the can-
didate list of the shorter mentions in the cluster. For
example, if a mention has the surface Richard, the
number of potential candidates is so large that any
top K list of titles will not be informative. We there-
fore ignore candidates generated from short surface
strings and give it the same candidate list as the head
mentions in its cluster. Figure 2 shows the voting al-
gorithm we use to elect the potential candidates for
the cluster.
The reason for separating the votes of longer and
shorter mentions is that shorter mentions are inher-
ently more ambiguous. Once a coreferent relation
1792
is determined, longer mentions in the cluster should
dictate what this cluster should collectively refer to.
4.4 Candidate Generation
Beyond the algorithmic improvements, the mention
and candidate generation stage is aided by a few
systematic preprocessing improvement briefly de-
scribed below.
4.4.1 Mention Segmentation
Since named entities may sometimes overlap with
each other, we use regular expressions to match
longer surface forms that are often incorrectly seg-
mented or ignored by NER 2 due to different an-
notation standards. For example, this will capture:
Prime Minister of the United Kingdom. The regu-
lar expression pattern we used for Step 1 in Algo-
rithm 1 simply adds mentions formed by any two
consecutive capitalized word chunks connected by
up to 2 punctuation marks, prepositions, and the to-
kens ?the?, ??s? & ?and?. These segments are also
used as arguments for relation extraction.
4.4.2 Lexical Search
We link certain mentions directly to their exact
matching titles in Step 3 when there is very low am-
biguity. Specifically, when no title is known for a
mention that is relatively long and fuzzily matches
the lexically retrieved title, we perform this aggres-
sive linking. The lexical similarity metrics are com-
puted using the publicly available NESim 3 package
(Do et al, 2009) with a threshold tuned on a subset
of Wikipedia redirects, and by insisting that ORG
type entities must have the same head word as the
candidate titles. We only accept the link if there ex-
ists exactly one title in the lexical searching result
after pruning.
5 Experiments and Evaluation
This section describes our experimental evaluation.
We compare our system against the top D2W sys-
tems and perform several experiments to analyze
and better understand the power of our approach.
We based our work on the GLOW system from
2We used the IllinoisNER package http://cogcomp.
cs.illinois.edu/page/software_view/4
3http://cogcomp.cs.illinois.edu/page/
software_view/22
(Ratinov et al, 2011) to initialize the candidates and
corresponding priors ski in our objective function.
Both the baseline system and our new system are
publicly available 4.
5.1 Comparison with other Wikification
systems
We first evaluate on the same 4 datasets5 used in
(Ratinov et al, 2011). The AQUAINT dataset, orig-
inally introduced in (Milne and Witten, 2008), re-
sembles the Wikipedia annotation structure in that
only the first mention of a title is linked, and is
thus less sensitive to coreference capabilities. The
MSNBC dataset is from (Cucerzan, 2007) and in-
cludes many mentions that do not easily map to
Wikipedia titles due to rare surface or other idiosyn-
cratic lexicalization (Cucerzan, 2007; Ratinov et al,
2011). Both of these datasets came from the news
domain and do not contain any annotated NIL enti-
ties. The ACE and Wikipedia datasets are both taken
from (Ratinov et al, 2011) where ACE is a subset
of ACE2004 Coreference documents annotated by
Amazon Mechanical Turkers in a similar standard as
in AQUAINT but with NIL entities. The Wikipedia
dataset is a sample of Wikipedia pages with its orig-
inal hyperlink annotation.
The evaluation methodology Bag of Titles (BOT)
F1 was used in both (Milne and Witten, 2008; Rati-
nov et al, 2011). For each document, the gold bag
of titles is evaluated against our bag of system out-
put titles requiring exact segmentation match.
Dataset
System ACE MSNBC AQUAINT Wiki
M&W 72.76 68.49 83.61 80.32
R&R 77.25 74.88 83.94 90.54
RI 85.30 81.20 88.88 93.09
Table 1: Performance on Wikification datasets, BOT F1
Performance. Our system, Relational Inference (RI) ex-
hibits significant improvements over M&W (Milne and
Witten, 2008) and R&R (Ratinov et al, 2011).
4http://cogcomp.cs.illinois.edu/page/
download_view/Wikifier
5http://cogcomp.cs.illinois.edu/page/
resource_view/4
1793
5.2 Ablation study
We incrementally add various components to the
system and study their impact on the end perfor-
mance. Due to the changes in Wikipedia since the
datasets were generated, some of the pages no longer
exist; in order to minimize the interference caused
by these inconsistencies to an accurate evaluation
of various componenents, we consider all non-NIL
gold annotations that do not exist in the current
Wikipedia index as NIL entities. Additionally in the
MSNBC dataset, 127 out of 756 surface forms are
known to be non-recallable. This explains the per-
formance difference between the final rows in Tab.
1 and 2.
Dataset
Components ACE MSNBC AQUAINT Wiki
Baseline 80.68 83.00 83.93 91.93
+Lexical Match 83.47 84.13 88.88 93.41
+Coreference 83.40 87.88 88.88 93.09
RI 85.83 88.16 88.88 93.09
Table 2: Ablation study on Wikification datasets, BOT F1
Performance
The Baseline refers to the best performing configu-
ration that was used in (Ratinov et al, 2011) except
for using the current Wikipedia redirects. The Lexi-
cal Match refers to the applying solely the method-
ology introduced in Sec. 4.4. The Coreference per-
formance includes all the inference performed with-
out the KB triples, while the Relational Inference
(RI) line represents all aspects of the proposed re-
lational inference. It is clear that different datasets
show somewhat different characteristics and conse-
quently different gains from the various aspects of
our approach but that, overall, all aspects contribute
to improved performance.
5.3 TAC Entity Linking 2011
Next we evaluate our approach on the TAC English
Entity Linking Task, which provides standardized
evaluation metrics, allowing us to compare to a large
number of other systems. We did not evaluate on the
2012 English Entity Linking due to the significant
amount of ambiguous NIL entities included (Ellis et
al., 2011) in the queries and the need to cluster them,
which our D2W task definition does not address in
depth. We compare our system with the Top 3 TAC
2011 systems (LCC, MS-MLI and NUSchime) as
well as our baseline system GLOW that participated
in TAC 2011 English Entity Linking (Ratinov and
Roth, 2011) in table 3. The evaluation metric is the
official modified B3 and Micro-Average explained
in (Ji et al, 2011).
Given the TAC Knowledge Base (TKB), which is
a subset of the 2009 Wikipedia Dump, the TAC En-
tity Linking objective is to answer a named entity
query string with either a TKB entry ID or a NIL
entity ID, where the NIL entity IDs should be clus-
tered across documents.
It is important to note that we did not retrain our
system on the TAC data as the top three systems did,
even though the objective function is slightly differ-
ent. Instead, we ran our system on the TAC doc-
uments directly without any query expansion. For
the final output of each query, we simply use the
most confident candidate among all matched men-
tions. Due to the clustering requirement, we also
trivially cluster NIL entities that either are mapped
to the same out-of-KB Wikipedia URL or have the
same surface form.
Performance
System MA B3 P B3 R B3 F1
LCC 86.1 84.4 84.7 84.6
MS-MLI 86.8 84.8 83.4 84.1
RI 86.1 82.9 84.5 83.7
NUSchime 86.3 81.5 84.9 83.1
RI-0 81.4 78.6 79.1 78.8
Cogcomp 78.7 75.7 76.5 76.1
Table 3: TAC2011 Entity Linking performance. MA is
Micro-Average. LLC (Monahan et al, 2011) is the best
performing system in terms of B3 F1 while MS-MLI
(Cucerzan, 2011) is the best in terms of Micro-Average.
Cogcomp (Ratinov and Roth, 2011) is the GLOW based
system that participated in TAC 2011.RI is the complete
relational inference system described in this paper; as de-
scribed in the text, RI was not trained on the TAC data,
unlike the other top systems.
We performed two runs on the TAC2011 data to
study the effects of relational inference. The first
run, RI-0, uses the current Wikipedia index and
1794
0.65
0.7
0.75
0.8
0.85
0.9
LCCMS_
MLI RI
NUS
chim
e
CUN
Y_UI
UC_S
RI
COG
COM
P
CMC
RC
Stan
ford_
UBC
CUN
Y_BL
END
ER
HLTC
OE
THU
NLP HIT
DMIR
_INE
SIDMSR
A
WBS
G 
 
Micro?averaged Accuracy
B?cubed+ F?Measure
B?cubed+ Precision
B?cubed+ Recall
Figure 2: The RI compared with the other top 14
TAC2011 English Entity Linking systems ranked by
modified B3 F1 measure. Original figure from (Ji et al,
2011).
redirects for lexical matching without any inference,
which scored 2.7% higher than the original GLOW
system (Cogcomp). We can regard this performance
as the new baseline that benefited from the fuzzy
lexical matching capabilities that we have added, as
well as the broader set of surface forms and redirects
from the current Wikipedia dump. In the second run,
RI, the complete relational inference described in
this paper, scored 4.9% higher than the new base-
line and sits on par with the top tier systems despite
not being trained on the given data. The LCC sys-
tem used sophisticated clustering algorithms trained
on the TAC development set (Monahan et al, 2011).
The second-ranked MS-MLI system relied on topic
modeling, external web search engine logs as well as
training on the development data (Cucerzan, 2011).
This shows the robustness of our methods as well
as the general importance of understanding textual
relations in the task of Entity Linking and Wikifica-
tion.
6 Related Work and Discussion
Earlier works on Wikification formulated the task
as a WSD problem (Bunescu and Pasca, 2006; Mi-
halcea and Csomai, 2007) and focused primarily on
training a model using local context. Later, various
global statistical approaches were proposed to em-
phasize different coherence measures between the ti-
tles of the disambiguated mentions in the same doc-
ument (Cucerzan, 2007; Milne and Witten, 2008;
Ratinov et al, 2011). Built on top of the statisti-
cal models, our work focuses on leveraging deeper
understanding of the text to more effectively and ac-
curately utilize existing knowledge.
We have demonstrated that, by incorporating tex-
tual relations and semantic knowledge as linguistic
constraints in an inference framework, it is possible
to significantly improve Wikification performance.
In particular, we have shown that our system is ca-
pable of making ?intelligent? inferences that makes
use of basic text understanding and has the ability to
reason with it and verify it against relevant informa-
tion sources. This allows our Relational Inference
approach to resolve a variety of difficult examples
illustrated in the Introduction.
Our system features high modularity since the re-
lations are considered only at inference time; con-
sequently, we can use any underlying Wikification
system as long as it outputs a distribution of title
candidates for each mention.
One possibility for future work is to supply this
framework with a richer set of relations from the
text, such as verbal relations. It will also be inter-
esting to incorporate high-level typed relations and
relax the relation arguments to be general concepts
rather than only named entities.
Acknowledgments
We sincerely thank the three anonymous review-
ers for their suggestions on the paper. This ma-
terial is based on research sponsored by DARPA
under agreement number FA8750-13-2-0008, and
partly supported by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department
of Interior National Business Center contract num-
ber D11PC20155, by the Army Research Labora-
tory (ARL) under agreement W911NF-09-2-0053,
and by the Multimodal Information Access & Syn-
thesis Center at UIUC, part of CCICADA, a DHS
Science and Technology Center of Excellence. The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. Dis-
claimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
1795
policies or endorsements, either expressed or im-
plied, of DARPA, IARPA, DoI/NBC, ARL, or the
U.S. Government.
References
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of the European Chapter of the ACL (EACL).
Y. Chan and D. Roth. 2011. Exploiting syntactico-
semantic structures for relation extraction. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL), Portland, Oregon.
M. Chang, L. Ratinov, and D. Roth. 2012. Structured
learning with constrained conditional models. Ma-
chine Learning, 88(3):399?431, 6.
Silviu Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings of
the 2007 Joint Conference of EMNLP-CoNLL, pages
708?716.
Silviu Cucerzan. 2011. Tac entity linking by perform-
ing full-document entity extraction and disambigua-
tion. In Proceedings of the Text Analysis Conference.
Q. Do, D. Roth, M. Sammons, Y. Tu, and V. Vydiswaran.
2009. Robust, light-weight approaches to compute
lexical similarity. Technical report, Computer Science
Department, University of Illinois.
Joe Ellis, Xuansong Li, Kira Griffitt, Stephanie M
Strassel, and Jonathan Wright. 2011. Linguistic re-
sources for 2012 knowledge base population evalua-
tions.
Paolo Ferragina and Ugo Scaiella. 2010. Tagme: on-the-
fly annotation of short text fragments (by wikipedia
entities). In Proceedings of the 19th ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ?10, pages 1625?1628, New York,
NY, USA. ACM.
Inc. Gurobi Optimization. 2013. Gurobi optimizer refer-
ence manual.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the tac 2010
knowledge base population track. In Third Text Anal-
ysis Conference (TAC 2010).
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac 2011 knowledge base population
track. In Fourth Text Analysis Conference (TAC 2011).
R. Mihalcea and A. Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In Proceedings
of ACM Conference on Information and Knowledge
Management (CIKM), pages 233?242.
D. Milne and I. H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of ACM Conference
on Information and Knowledge Management (CIKM),
pages 509?518.
Sean Monahan, John Lehmann, Timothy Nyberg, Jesse
Plymale, and Arnold Jung. 2011. Cross-lingual cross-
document coreference with entity linking. In Proceed-
ings of the Text Analysis Conference.
L. Ratinov and D. Roth. 2011. Glow tac-kbp 2011 entity
linking system. In TAC. Text Analysis Conference, 11.
L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In
EMNLP.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In ACL.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, Proceedings
of the Annual Conference on Computational Natural
Language Learning (CoNLL), pages 1?8. Association
for Computational Linguistics.
1796
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1808?1814,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Using Soft Constraints in Joint Inference for
Clinical Concept Recognition
Prateek Jindal and Dan Roth
Department of Computer Science, UIUC
201 N. Goodwin Ave, Urbana, IL 61801, USA
{jindal2, danr}@illinois.edu
Abstract
This paper introduces IQPs (Integer Quadratic
Programs) as a way to model joint inference
for the task of concept recognition in clinical
domain. IQPs make it possible to easily in-
corporate soft constraints in the optimization
framework and still support exact global infer-
ence. We show that soft constraints give statis-
tically significant performance improvements
when compared to hard constraints.
1 Introduction
In this paper, we study the problem of concept
recognition in the clinical domain. State-of-the-art
approaches (de Bruijn et al, 2011; Patrick et al,
2011; Torii et al, 2011; Minard et al, 2011; Jiang
et al, 2011; Xu et al, 2012; Roberts and Harabagiu,
2011; Jindal and Roth, 2013) for concept recogni-
tion in clinical domain (Uzuner et al, 2011) use
sequence-prediction models like CRF (Lafferty et
al., 2001), MEMM (McCallum et al, 2000) etc.
These approaches are limited by the fact that they
can model only local dependencies (most often,
first-order models like linear chain CRFs are used
to allow tractable inference).
Clinical narratives, unlike newswire data, provide
a domain with significant knowledge that can be ex-
ploited systematically to improve the accuracy of
the prediction task. Knowledge in this domain can
be thought of as belonging to two categories: (1)
Background Knowledge captured in medical ontolo-
gies like UMLS (Url1, 2013), MeSH and SNOMED
CT and (2) Discourse Knowledge driven by the
fact that the narratives adhere to a specific writing
style. While the former can be used by generating
more expressive knowledge-rich features, the lat-
ter is more interesting from our current perspective,
since it provides global constraints on what output
structures are likely and what are not. We exploit
this structural knowledge in our global inference for-
mulation.
Integer Linear Programming (ILP) based ap-
proaches have been used for global inference in
many works (Roth and Yih, 2004; Punyakanok et
al., 2004; Punyakanok et al, 2008; Marciniak and
Strube, 2005; Bramsen et al, 2006; Barzilay and
Lapata, 2006; Riedel and Clarke, 2006; Clarke and
Lapata, 2007; Clarke and Lapata, 2008; Denis et al,
2007; Chang et al, 2011). However, in most of these
works, researchers have focussed only on hard con-
straints while formulating the inference problem.
Formulating all the constraints as hard constraints
is not always desirable because the constraints are
not perfect in many cases. In this paper, we pro-
pose Integer Quadratic Programs (IQPs) as a way
of formulating the inference problem. IQPs is a
richer family of models than ILPs and it enables
us to easily incorporate soft constraints into the in-
ference procedure. Our experimental results show
that soft constraints indeed give much better perfor-
mance than hard constraints.
2 Identifying Medical Concepts
Task Description Our input consists of clinical re-
ports in free-text (unstructured) format. The task is:
(1) to identify the boundaries of medical concepts
and (2) to assign types to such concepts. Each con-
cept can have 3 possible types: (1) Test, (2) Treat-
ment, and (3) Problem. We would refer to these
three types by TEST, TRE and PROB in the follow-
ing discussion.
Our Approach In the first step, we identify the
concept boundaries using a CRF (with BIO encod-
1808
[Chest x-ray] gave positive evidence for [atelectasis] and [sarcoidosis].Test Problem Problem
(a) Example 1
No [hemoptysis], [hematemesis], [urgency], [abdominal pain], [black or tarry stools], [dysuria].Problem ProblemProblem ProblemProblemProblem
(b) Example 2
Figure 1: This figure motivates the global inference procedure we used. For discussion, please refer to ?2.
ing). Features used by the CRF include the con-
stituents given by MetaMap (Aronson and Lang,
2010; Url2, 2013), shallow parse constituents, sur-
face form and part-of-speech (Url3, 2013) of words
in a window of size 3. We also use conjunctions of
the features.
After finding concept boundaries, we determine
the probability distribution for each concept over 4
possible types (TEST, TRE, PROB or NULL). These
probability distributions are found using a multi-
class SVM classifier (Chang and Lin, 2011). Fea-
tures used for training this classifier include con-
cept tokens, full text of concept, bi-grams, head-
word, suffixes of headword, capitalization pattern,
shallow parse constituent, Metamap type of concept,
MetaMap type of headword, occurrence of concept
in MeSH (Url4, 2013) and SNOMED CT (Url5,
2013), MeSH and SNOMED CT descriptors.
Inference Procedure: The final assignment of
types to concepts is determined by an inference pro-
cedure. The basic principle behind our inference
procedure is: ?Types of concepts which appear close
to one another are often closely related. For some
concepts, type can be determined with more confi-
dence. And relations between concepts? types guide
the inference procedure to determine the types of
other concepts.? We will now explain it in more de-
tail with the help of examples. Figure 1 shows two
sentences in which the concepts are shown in brack-
ets and correct (gold) types of concepts are shown
above them.
First, consider first and second concepts in Fig-
ure 1a. These concepts follow the pattern: [Con-
cept1] gave positive evidence for [Concept2]. In
clinical narratives, such a pattern strongly suggests
that Concept1 is of type TEST and Concept2 is of
type PROB. Table 1 shows additional such patterns.
Next, consider different concepts in Figure 1b. All
Pattern
1 using [TRE] for [PROB]
2 [TEST] showed [PROB]
3 Patient presents with [PROB] status post
[TRE]
4 use [TRE] to correct [PROB]
5 [TEST] to rule out [PROB]
6 Unfortunately, [TRE] has caused [PROB]
Table 1: Some patterns that were used in constraints.
these concepts are separated by commas and hence,
form a list. It is highly likely that such concepts
should have the same type.
3 Modeling Global Inference
Inference is done at the level of sentences. Sup-
pose there are m concepts in a sentence. Each of
the m concepts has to be assigned one of the follow-
ing types: TEST, TRE, PROB or NULL. To represent
this as an inference problem, we define the indicator
variables xi,j where i takes values from 1 to m (cor-
responding to concepts) and j takes values from 1 to
4 (corresponding to 4 possible types). pi,j refers to
the probability that the ith concept has type j.
We can now write the following optimization
problem to find the optimal concept types:
max
x
m?
i=1
4?
j=1
xi,j ? pi,j (1)
subject to
4?
j=1
xi,j = 1 ?i (2)
xi,j ? {0, 1} ?i, j (3)
The objective function in Equation (1) expresses
the fact that we want to maximize the expected num-
ber of correct predictions in each sentence. Equa-
tion (2) enforces the constraint that each concept has
1809
a unique type. We would refer to these as Type-1
constraints.
3.1 Constraints Used
In this subsection, we will describe two addi-
tional types of constraints (Type-2 and Type-3)
that were added to the optimization procedure de-
scribed above. Whereas Type-1 constraints de-
scribed above were formulated as hard constraints,
Type-2 and Type-3 constraints are formulated as
soft constraints.
3.1.1 Type-2 Constraints
Certain constructs like comma, conjunction, etc.
suggest that the 2 concepts appearing in them should
have the same type. Figure 1b shows an example of
such a constraint. Suppose that there are n2 such
constraints. Also, assume that the lth constraint says
that the concepts Rl and Sl should have the same
type. To model this, we define a variable wl as fol-
lows:
wl =
4?
m=1
(xRl,m ? xSl,m)
2 (4)
Now, if the concepts Rl and Sl have the same
type, then wl would be equal to 0; otherwise, wl
would be equal to 2. So, the lth constraint can be
enforced by subtracting (?2 ?
wl
2 ) from the objective
function given by Equation (1). Thus, a penalty of
?2 would be enforced iff this constraint is violated.
3.1.2 Type-3 Constraints
Some short patterns suggest possible types for the
concepts which appear in them. Each such pattern,
thus, enforces a constraint on the types of corre-
sponding concepts. Figure 1a shows an example
of such a constraint. Suppose that there are n3
such constraints. Also, assume that the kth con-
straint says that the concept A1,k should have the
type B1,k and that the concept A2,k should have
the type B2,k. Equivalently, the kth constraint can
be written as follows in boolean algebra notation:
(xA1,k,B1,k = 1)?(xA2,k,B2,k = 1). For the k
th con-
straint, we introduce one more variable zk ? {0, 1}
which satisfies the following condition:
zk = 1? xA1,k,B1,k ? xA2,k,B2,k (5)
Using boolean algebra, it is easy to show that
Equation (5) can be reduced to a set of linear in-
equalities. Thus, we can incorporate the kth con-
max
x
m?
i=1
4?
j=1
xi,j ? pi,j ?
n3?
k=1
?3(1? zk)
?
n2?
l=1
(
?2 ?
?4
m=1(xRl,m ? xSl,m)
2
2
) (6)
subject to
4?
j=1
xi,j = 1 ?i (7)
xi,j ? {0, 1} ?i, j (8)
zk = 1? xA1,k,B1,k ? xA2,k,B2,k?k ? {1...n3} (9)
Figure 2: Final Optimization Problem (an IQP)
straint in the optimization problem by adding to it
the constraint given by Equation (5) and by subtract-
ing (?3(1 ? zk)) from the objective function given
by Equation (1). Thus, a penalty of ?3 is imposed iff
kth constraint is not satisfied (zk = 0).
3.2 Final Optimization Problem - An IQP
After incorporating all the constraints mentioned
above, the final optimization problem (an IQP) is
shown in Figure 2. We used Gurobi toolkit (Url6,
2013) to solve such IQPs. In our case, it solves
76 IQPs per second on a quad-core server with In-
tel Xeon X5650 @ 2.67 GHz processors and 50 GB
RAM.
4 Experiments and Results
4.1 Datasets and Evaluation Metrics
For our experiments, we used the datasets pro-
vided by i2b2/VA team as part of 2010 i2b2/VA
shared task (Uzuner et al, 2011). The datasets
used for this shared task contained de-identied clin-
ical reports from three medical institutions: Part-
ners Healthcare (PH), Beth-Israel Deaconess Med-
ical Center (BIDMC) and the University of Pitts-
burgh Medical Center (UPMC). UPMC data was di-
vided into 2 sections, namely discharge (UPMCD)
and progress notes (UPMCP). A total of 349 train-
ing reports and 477 test reports were made available
to the participants. However, data which came from
UPMC (more than 50% data) was not made avail-
able for public use. As a result, we had only 170
clinical reports for training and 256 clinical reports
for testing. Table 3 shows the number of clinical re-
ports made available by different institutions. The
1810
B BK BC BKC
P R F1 P R F1 P R F1 P R F1
TEST 92.4 79.4 85.4 91.9 80.2 85.7 92.7 79.6 85.7 92.1 80.4 85.8
TRE 92.1 73.6 81.8 92.0 79.5 85.3 92.3 76.8 83.8 92.0 80.2 85.7
PROB 83.6 83.6 83.6 88.9 83.7 86.3 85.9 83.8 84.8 89.6 83.9 86.7
OVERALL 88.4 79.4 83.6 90.7 81.4 85.8 89.6 80.5 84.8 91.0 81.7 86.1
Table 2: Our final system, BKC, consistently performed the best among all 4 systems (B, BK, BC and BKC).
PH BIDMC UPMCD UPMCP
Train 97 73 98 81
Test 133 123 102 119
Table 3: Dataset Characteristics
strikethrough text in this table indicates that the data
was not made available for public use and hence, we
couldn?t use it. We used about 20% of the training
data as a development set. For evaluation, we report
precision, recall and F1 scores.
4.2 Results
In this section, we would refer to following 4
systems: (1) Baseline (B), (2) Baseline + Knowl-
edge (BK), (3) Baseline + Constraints (BC) and
(4) Baseline + Knowledge + Constraints (BKC).
Please note that the difference between B and
BK is that B does not use the features derived
from domain-specific knowledge sources (namely
MetaMap, UMLS, MeSH and SNOMED CT) for
training the classifiers. Both B and BK do not use
the inference procedure. BKC uses all the features
and also the inference procedure. In addition to
these 4 systems, we would refer to another system,
namely, BKC-HARD. This is similar to BKC sys-
tem. However, it sets ?2 = ?3 = 1 which effectively
turns Type-2 and Type-3 constraints into hard con-
straints by imposing very high penalty.
4.2.1 Importance of Soft Constraints
Figures 3a and 3b show the effect of varying the
penalties (?2 and ?3) for Type-2 and Type-3 con-
straints respectively. These figures show the F1-
score of BKC on the development set. Penalty of
0 means that the constraint is not active. As we in-
crease the penalty, the constraint becomes stronger.
As the penalty becomes 1, the constraint becomes
hard in the sense that final assignments must respect
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 179.7
79.879.9
8080.1
80.280.3
80.480.5
80.680.7
Tuning Penalty Parameter for Type?2 Constraints
Penalty Parameter for Type?2 Constraints ( ?2)
F1?sco
re
(a) Type-2 Constraints
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 180.3
80.480.5
80.680.7
80.880.9
8181.1
81.281.3
81.481.5
Tuning Penalty Parameter for Type?3 Constraints
Penalty Parameter for Type?3 Constraints ( ?3)
F1?sco
re
(b) Type-3 Constraints
Figure 3: These figures show the result of tuning the
penalty parameters (?2 and ?3) for soft constraints.
BKC-HARD BKC
TEST 84.7 85.8
TRE 84.7 85.7
PROB 85.6 86.7
OVERALL 85.1 86.1
Table 4: Soft constraints (BKC) consistently perform
much better than hard constraints (BKC-HARD).
the constraint. We observe from Figures 3a and 3b
that for Type-2 and Type-3 constraints, global max-
ima is attained at ?2 = 0.6 and ?3 = 0.3 respec-
tively.
Hard vs Soft Constraints Table 4 compares the
performance of BKC-HARD with that of BKC.
First 3 rows in this table show the performance of
both systems for the individual categories (TEST,
TRE and PROB). The fourth row shows the overall
score of both systems. BKC outperformed BKC-
HARD on all the categories by statistically signifi-
cant differences at p = 0.05 according to Bootstrap
Resampling Test (Koehn, 2004). For the OVERALL
category, BKC improved over BKC-HARD by 1.0
F1 points.
1811
40 50 60 70 80 90 100 110 120 13080
81
82
83
84
85
86
Training Data Size (# clinical reports)
F1?s
core
Effect of Training Data Size on Performance
 
 
BKCBK
Figure 4: This figure shows the effect of training data
size on performance of concept recognition.
4.2.2 Comparing with state-of-the-art baseline
In the 2010 i2b2/VA shared task, majority of
top systems were CRF-based models, motivating
the use of CRF as our baseline. Table 2 com-
pares the performance of 4 systems: B, BK, BC
and BKC. As pointed out before, our BK system
uses CRF for boundary detection, employs all the
knowledge-based features and is very similar to the
top-performing systems in i2b2 challenge. We see
from Table 2 that BKC consistently performed the
best for individual as well as overall categories1.
This result is statistically significant at p = 0.05
according to Bootstrap Resampling Test (Koehn,
2004). It should also be noted that BC performed
significantly better than B for all the categories.
Thus, the constraints are helpful even in the ab-
sence of knowledge-based features. Since we report
results on publicly available datasets, future works
would be able to compare their results with ours.
4.2.3 Effect of training data size
In Figure 4, we report the overall F1-score on a
part of the development set as we vary the size of the
training data from 40 documents to 130 documents.
We notice that the performance increases steadily as
more and more training data is provided. This sug-
gests that if we could train on full training data as
was made available in the challenge, the final scores
could be much higher. We also notice from the fig-
ure that BKC consistently outperforms the state-of-
the-art BK system as we vary the size of the training
data, indicating the robustness of the joint inference
procedure.
1Please note that the results reported in Table 2 can not be
directly compared with those reported in the challenge because
we only had a fraction of the original training and testing data.
5 Discussion and Related Work
In this paper, we chose to train a rather simple se-
quential model (using CRF), and focused on incor-
porating global constraints only at inference time2.
While it is possible to jointly train the model with
the global constraints (as illustrated by Chang et al
(2007), Mann and McCallum (2007), Mann and Mc-
Callum (2008), Ganchev et al (2010) etc.), this pro-
cess will be a lot less efficient, and prior work (Roth
and Yih, 2005) has shown that it may not be benefi-
cial.
Roth and Yih (2004, 2007) suggested the use of
integer programs to model joint inference in a fully
supervised setting. Our paper follows their concep-
tual approach. However, they used only hard con-
straints in their inference formulation. Chang et
al. (2012) extended the ILP formulation and used
soft constraints within the Constrained Conditional
Model formulation (Chang, 2011). However, their
implementation performed only approximate infer-
ence. In this paper, we extended the integer lin-
ear programming to a quadratic formulation, argu-
ing that it simplifies the modeling step3, and showed
that it is possible to do exact inference efficiently.
Conclusion
This paper presented a global inference strategy
(using IQP) for concept recognition which allows
us to model structural knowledge of the clinical do-
main as soft constraints in the optimization frame-
work. Our results showed that soft constraints are
more effective than hard constraints.
Acknowledgments
This research was supported by Grant HHS
90TR0003/01 and by IARPA FUSE program via
DoI/NBC contract #D11PC2015. Its contents are
solely the responsibility of the authors and do not
necessarily represent the official views, either ex-
pressed or implied, of the HHS, IARPA, DoI/NBC
or the US government. The US Government is
authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copy-
right annotation thereon.
2In another experiment, we replaced the CRF with an
MEMM. Surprisingly, MEMM performed as well as CRF.
3It should be noted that it is possible to reduce IQPs to ILPs
using variable substitution. However, the resulting ILPs can be
exponentially larger than original IQPs.
1812
References
A.R. Aronson and F.M. Lang. 2010. An overview of
metamap: historical perspective and recent advances.
Journal of the American Medical Informatics Associa-
tion, 17(3):229.
R. Barzilay and M. Lapata. 2006. Aggregation via set
partitioning for natural language generation. In Pro-
ceedings of the main conference on Human Language
Technology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
pages 359?366. Association for Computational Lin-
guistics.
P. Bramsen, P. Deshpande, Y.K. Lee, and R. Barzilay.
2006. Inducing temporal graphs. In Proceedings of
the 2006 Conference on Empirical Methods in Natural
Language Processing, pages 189?198. Association for
Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Associ-
ation for Computational Linguistics, pages 280?287,
Prague, Czech Republic, 6. Association for Computa-
tional Linguistics.
K.-W. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference proto-
cols for coreference resolution. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 40?44, Portland,
Oregon, USA. Association for Computational Linguis-
tics.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2012.
Structured learning with constrained conditional mod-
els. Machine learning, pages 1?33.
M. Chang. 2011. Structured Prediction with Indirect
Supervision. Ph.D. thesis, University of Illinois at
Urbana-Champaign.
James Clarke and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1?11.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research,
31(1):399?429.
B. de Bruijn, C. Cherry, S. Kiritchenko, J. Martin, and
X. Zhu. 2011. Machine-learned solutions for three
stages of clinical information extraction: the state of
the art at i2b2 2010. Journal of the American Medical
Informatics Association, 18(5):557?562.
P. Denis, J. Baldridge, et al 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 236?243.
Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. The Journal of Machine
Learning Research, 11:2001?2049.
M. Jiang, Y. Chen, M. Liu, S.T. Rosenbloom, S. Mani,
J.C. Denny, and H. Xu. 2011. A study of machine-
learning-based approaches to extract clinical entities
and their assertions from discharge summaries. J Am
Med Info Assoc, 18(5):601?606.
P. Jindal and D. Roth. 2013. End-to-end coreference res-
olution for clinical narratives. In Proceedings of In-
ternational Joint Conference on Artificial Intelligence
(IJCAI), pages 2106?2112, 8.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of Empirical
Methods in Natural Language Processing, volume 4,
pages 388?395.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Gideon S Mann and Andrew McCallum. 2007. Sim-
ple, robust, scalable semi-supervised learning via ex-
pectation regularization. In Proceedings of the 24th
international conference on Machine learning, pages
593?600. ACM.
Gideon Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learning
of conditional random fields. In Proceedings of Asso-
ciation for Computational Linguistics, pages 870?878.
T. Marciniak and M. Strube. 2005. Beyond the
pipeline: Discrete optimization in nlp. In Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning, pages 136?143. Association
for Computational Linguistics.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy markov models for information extrac-
tion and segmentation. In Proceedings of the Seven-
teenth International Conference on Machine Learning,
pages 591?598.
A.L. Minard, A.L. Ligozat, A.B. Abacha, D. Bernhard,
B. Cartoni, L. Dele?ger, B. Grau, S. Rosset, P. Zweigen-
baum, and C. Grouin. 2011. Hybrid methods for
1813
improving information access in clinical documents:
Concept, assertion, and relation identification. J Am
Med Info Assoc, 18(5):588?593.
J.D. Patrick, D.H.M. Nguyen, Y. Wang, and M. Li.
2011. A knowledge discovery and reuse pipeline
for information extraction in clinical notes. Jour-
nal of the American Medical Informatics Association,
18(5):574?579.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear programming
inference. In Proceedings of the 20th international
conference on Computational Linguistics, page 1346.
Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 129?
137. Association for Computational Linguistics.
K. Roberts and S.M. Harabagiu. 2011. A flexible frame-
work for deriving assertions from electronic medical
records. Journal of the American Medical Informatics
Association, 18(5):568?573.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of conference on Computational Natural
Language Learning (CoNLL), pages 1?8. Association
for Computational Linguistics.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In Proceed-
ings of International Conference on Machine Learning
(ICML), pages 737?744.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. Introduction to Statistical Rela-
tional Learning, pages 553?580.
M. Torii, K. Wagholikar, and H. Liu. 2011. Us-
ing machine learning for concept extraction on clin-
ical documents from multiple data sources. Jour-
nal of the American Medical Informatics Association,
18(5):580?587.
Url1. 2013. Umls: Unified medical language
system (http://www.nlm.nih.gov/research/umls/) (ac-
cessed july 1, 2013).
Url2. 2013. Metamap (http://metamap.nlm.nih.gov/)
(accessed july 1, 2013).
Url3. 2013. Illinois part-of-speech tagger
(http://cogcomp.cs.illinois.edu/page/software view/
pos) (accessed july 1, 2013).
Url4. 2013. Mesh: Medical subject headings
(http://www.nlm.nih.gov/mesh/meshhome.html) (ac-
cessed july 1, 2013).
Url5. 2013. Snomed ct: Snomed clinical terms
(http://www.ihtsdo.org/snomed-ct/) (accessed july 1,
2013).
Url6. 2013. Gurobi optimization toolkit
(http://www.gurobi.com/) (accessed july 1, 2013).
O. Uzuner, B.R. South, S. Shen, and S.L. DuVall. 2011.
2010 i2b2/va challenge on concepts, assertions, and
relations in clinical text. Journal of American Medical
Informatics Association.
Y. Xu, K. Hong, J. Tsujii, I. Eric, and C. Chang. 2012.
Feature engineering combined with machine learning
and rule-based methods for structured information ex-
traction from narrative clinical discharge summaries.
Journal of the American Medical Informatics Associa-
tion, 19(5):824?832.
1814
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 358?367,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Correcting Grammatical Verb Errors
Alla Rozovskaya
Columbia University
New York, NY 10115
ar3366@columbia.edu
Dan Roth
University of Illinois
Urbana, IL 61801
danr@illinois.edu
Vivek Srikumar
Stanford University
Stanford, CA 94305
svivek@cs.stanford.edu
Abstract
Verb errors are some of the most com-
mon mistakes made by non-native writers
of English but some of the least studied.
The reason is that dealing with verb er-
rors requires a new paradigm; essentially
all research done on correcting grammat-
ical errors assumes a closed set of trig-
gers ? e.g., correcting the use of prepo-
sitions or articles ? but identifying mis-
takes in verbs necessitates identifying po-
tentially ambiguous triggers first, and then
determining the type of mistake made and
correcting it. Moreover, once the verb is
identified, modeling verb errors is chal-
lenging because verbs fulfill many gram-
matical functions, resulting in a variety of
mistakes. Consequently, the little earlier
work done on verb errors assumed that the
error type is known in advance.
We propose a linguistically-motivated ap-
proach to verb error correction that makes
use of the notion of verb finiteness to iden-
tify triggers and types of mistakes, before
using a statistical machine learning ap-
proach to correct these mistakes. We show
that the linguistically-informed model sig-
nificantly improves the accuracy of the
verb correction approach.
1 Introduction
We address the problem of correcting grammati-
cal verb mistakes made by English as a Second
Language (ESL) learners. Recent work in ESL er-
ror correction has focused on errors in article and
preposition usage (Han et al., 2006; Felice and
Pulman, 2008; Gamon et al., 2008; Tetreault et
al., 2010; Gamon, 2010; Rozovskaya and Roth,
2010b; Dahlmeier and Ng, 2011).
While verb errors occur as often as article and
preposition mistakes, with a few exceptions (Lee
and Seneff, 2008; Gamon et al., 2009; Tajiri et al.,
2012), there has been little work on verbs. There
are two reasons for why it is difficult to deal with
verb mistakes. First, in contrast to articles and
prepositions, verbs are more difficult to identify
in text, as they can often be confused with other
parts of speech, and processing tools are known to
make more errors on noisy ESL data (Nagata et al.,
2011). Second, verbs are more complex linguisti-
cally: they fulfill several grammatical functions,
and these different roles imply different types of
errors.
These difficulties have led all previous work
on verb mistakes to assume prior knowledge of
the mistake type; however, identifying the specific
category of a verb error is nontrivial, since the sur-
face form of the verb may be ambiguous, espe-
cially when that verb is used incorrectly. Consider
the following examples of verb mistakes:
1. ?We discusses*/discuss this every time.?
2. ?I will be lucky if I {will find}*/find something that
fits.?
3. ?They wanted to visit many places without
spend*/spending a lot of money.?
4. ?They arrived early to organized*/organize every-
thing?.
These examples illustrate three grammatical
verb properties: Agreement, Tense, and non-finite
Form choice that encompass the most common
grammatical verb problems for ESL learners. The
first two examples show mistakes on verbs that
function as main verbs in a clause: sentence (1)
shows an example of subject-verb Agreement er-
ror; (2) is an example of a Tense mistake where
the ambiguity is between {will find} (Future tense)
358
and find (Present tense). Examples (3) and (4) dis-
play Form mistakes: confusing the infinitive and
gerund forms in (3) and including an inflection on
an infinitive verb in (4).
This paper addresses the specific challenges of
verb error correction that have not been addressed
previously ? identifying candidates for mistakes
and determining which class of errors is present,
before proceeding to correct the error. The ex-
perimental results show that our linguistically-
motivated approach benefits verb error correction.
In particular, in order to determine the error type,
we build on the notion of verb finiteness to distin-
guish between finite and non-finite verbs (Quirk et
al., 1985), that correspond to Agreement and Tense
mistakes (examples (1) and (2) above) and Form
mistakes (examples (3) and (4) above), respec-
tively (see Sec. 3). The approach presented in this
work was evaluated empirically and competitively
in the context of the CoNLL shared task on error
correction (Ng et al., 2013) where it was imple-
mented as part of the highest-scoring University
of Illinois system (Rozovskaya et al., 2013) and
demonstrated superior performance on the verb er-
ror correction sub-task.
This paper makes the following contributions:
?We present a holistic, linguistically-motivated
framework for correcting grammatical verb mis-
takes; our approach ?starts from scratch? with-
out any knowledge of which mistakes should be
corrected or of the mistake type; in doing that
we show that the specific challenges of verb error
correction are better addressed by first identifying
the finiteness of the verb in the error identification
stage.
? Within the proposed model, we describe and
evaluate several methods of selecting verb candi-
dates, an algorithm for determining the verb type,
and a type-driven verb error correction system.
?We annotate a subset of the FCE data set with
gold verb candidates and gold verb type.
1
2 Related Work
Earlier work in ESL error correction follows the
methodology of the context-sensitive spelling cor-
rection task (Golding and Roth, 1996; Golding
and Roth, 1999; Banko and Brill, 2001; Carlson
et al., 2001; Carlson and Fette, 2007). Most of
the effort in ESL error correction so far has been
1
The annotation is available at http://cogcomp.cs.illinois.
edu/page/publication view/743
on article and preposition usage errors, as these
are some of the most common mistakes among
non-native English speakers (Dalgish, 1985; Lea-
cock et al., 2010). These phenomena are generally
modeled as multiclass classification problems: a
single classifier is trained for a given error type
where the set of classes includes all articles or the
top n most frequent English prepositions (Izumi
et al., 2003; Han et al., 2006; Felice and Pul-
man, 2008; Gamon et al., 2008; Tetreault et al.,
2010; Rozovskaya and Roth, 2010b; Rozovskaya
and Roth, 2011; Dahlmeier and Ng, 2011).
Mistakes on verbs have attracted significantly
less attention in the error correction literature.
Moreover, the little earlier work done on verb er-
rors only considered subsets of these errors and
assumed the error sub-type is known in advance.
Gamon et al. (2009) mentioned a model for learn-
ing gerund/infinitive confusions and auxiliary verb
presence/choice. Lee and Seneff (2008) proposed
an approach based on pattern matching on trees
combined with word n-gram counts for correcting
agreement misuse and some types of verb form
errors. However, they excluded tense mistakes,
which is the most common error category for ESL
learners (40% of all verb errors, Sec. 3). Tajiri
et al. (2012) considered only tense mistakes. In
the above studies, it was assumed that the type of
mistake that needs to be corrected is known, and
irrelevant verb errors were excluded (e.g., Tajiri
et al. (2012) addressed only tense mistakes and
excluded from the evaluation other kinds of verb
errors). In other words, it was assumed that part
of the task was solved. But, unlike in article and
preposition error correction where the type of mis-
take is known based on the surface form of the
word, in verb error correction, it is not obvious.
The key distinction of our work is that we pro-
pose a holistic approach that starts from ?scratch?
and, given an instance, first detects a mistake and
identifies its type, and then proceeds to correct
it. We also evaluate several methods for select-
ing verb candidates and show the significance of
this step for improving verb error correction per-
formance, while earlier studies do not discuss this
aspect of the problem. In the CoNLL shared task
(Ng et al., 2013) that included verb errors in agree-
ment and form, the participating teams did not pro-
vide details on how specific challenges were han-
dled, but the University of Illinois system obtained
the highest score on the verb sub-task, even though
359
Tag Error type Rel. freq. (%)
TV Tense 40.0
FV Form 22.3
AGV Verb-subject agreement 11.5
MV Missing verb 11.7
UV Unneccesary verb 7.3
IV Inflection 5.4
DV Derivation 1.8
Total 6640
Table 1: Grammatical verb errors in FCE.
all teams used similar resources (Ng et al., 2013).
3 Verb Errors in ESL Writing
Verb-related errors are very prominent among
non-native English speakers: grammatical mis-
use of verbs constitutes one of the most com-
mon errors in several learner corpora, including
those previously used (Izumi et al., 2003; Lee
and Seneff, 2008) and the one employed in this
work. We study verb errors using the FCE cor-
pus (Yannakoudakis et al., 2011). The corpus
possesses several desirable characteristics: it is
large (500,000 words), has been annotated by na-
tive English speakers, and contains data by learn-
ers of multiple first-language backgrounds. The
FCE corpus contains 5056 determiner errors, 5347
preposition errors, and 6640 grammatical verb
mistakes (Table 1).
3.1 Verb Finiteness
There are many grammatical categories for which
English verbs can be marked. The linguistic no-
tion of verb finiteness or verb type (Radford, 1988;
Quirk et al., 1985) distinguishes between verbs
that function on their own in a clause as main verbs
(finite) and those that do not (non-finite). Gram-
matical properties associated with each group are
mutually exclusive: tense and agreement markers,
for example, do not apply to non-finite verbs; non-
finite verbs are not marked for many grammatical
functions but may appear in several forms.
The most common verb problems for ESL
learners ? Tense, Agreement, non-finite Form ?
involve verbs both in finite and non-finite roles.
Table 2 illustrates contexts that license finite and
non-finite verbs.
Our intuition is that, because properties associ-
ated with each verb type are mutually exclusive,
verb finiteness should benefit verb error correc-
tion models: an observed verb error may be due
to several grammatical phenomena, and knowing
which phenomena are active depends on the func-
tion of the verb in the current context. Note that
Agreement, Tense, and Form errors account for
Category Agreement Kappa Random
Correct verbs 0.97 0.95 0.51
Erroneous verbs 0.88 0.81 0.41
Table 3: Inter-annotator agreement based on 250 verb
errors and 250 correct verbs, randomly selected.
about 74% of all grammatical verb errors in Ta-
ble 1 but the finiteness distinction applies to all
English verbs ? every verb is either finite or non-
finite in a specific syntactic context ? and is also
relevant for the remaining mistakes not addressed
here.
2
4 Annotation for Verb Finiteness
In order to evaluate the quality of the algorithm
for verb finiteness and of the candidate selection
methods, we annotated all verbs ? correct and er-
roneous ? in a random set of 124 documents from
our corpus with the information about verb finite-
ness. We refer to these 124 documents as gold sub-
set. We also annotated erroneous verbs in the re-
maining 1120 documents of the corpus. The anno-
tation was performed by two students with back-
ground in Linguistics. The inter-annotator agree-
ment is shown in Table 3 and is high.
Annotating Verb Errors For each verb error that
was tagged as Tense (TV), Agreement (AGV), and
Form (FV), the annotators marked verb finiteness.
Additionally, the annotators also specified the type
of error (Tense, Agreement, or Form) (Table 4),
since the FCE tags do not always correspond to
the three error types we study here. For exam-
ple, the FV tag may mark errors on finite verbs.
Overall, about 7% of verb errors have to do with
phenomena different from the three verb proper-
ties considered in this work and thus are excluded
from the present study.
Annotating Correct Verbs Correct verbs were
identified in text using an automated proce-
dure that relies on part-of-speech information
(Sec. 5.1). Valid candidates were specified for
verb finiteness. The candidates that were iden-
tified incorrectly due to mistakes by the part-of-
speech tagger were marked as invalid.
5 The Computational Model
The verb error correction problem is formulated
as a classification task in the spirit of the learn-
2
For instance, the missing verb errors (MV, 11.7%) re-
quire an additional step to identify contexts for missing verbs,
and then appropriate verb properties need to be determined
based on verb finiteness.
360
Verb type Example Verb properties
Agreement Tense Form
Finite
?He discussed this with me last week? - Past Simple -
?He discusses this with me every week.? 3rd person,Sing. Present Simple -
Non-finite
?He left without discussing it with me.? - - Gerund
?They let him discuss this with me.? - - Infinitive
?To discuss this now would be ill-advised.? - - to-Infinitive
Table 2: Contexts that license finite and non-finite verbs and the corresponding active properties.
Error on Verb Type Subcategory Example
Finite (67.7%)
Agreement (20%) ?We discusses*/discuss this every time.?
Tense (80%) ?If you buy something, you {would be}*/{will be} happy.?
Non-finite (25.3%)
?If one is famous he has to accept the disadvantages of be*/being famous.? ?I am very
glad {for receiving}*/{to receive} it.?
?They arrived early to organized*/organize everything.?
Other errors (7.0%)
Passive/Active(42.3%) ?Our end-of-conference party {is included}*/includes dinner and dancing.?
Compound (40.7%) ?You ask me for some informations*/information- here they*/it are*/is.?
Other (16.8%) ?Nobody {has to be}*/{should be} late.?
Table 4: Verb error classification based on 4864 mistakes marked as TV, AGV, and FV errors in the FCE corpus.
ing paradigm commonly used for correcting other
ESL errors (Sec. 2), with the exception that the
verb model includes additional components. All
of the components are listed below:
1. Candidate selection (5.1)
2. Verb finiteness prediction (5.2)
3. Feature generation (5.3)
4. Error identification (5.4)
5. Error correction (5.5)
After verb candidates are selected, verb finite-
ness is determined and features are generated for
each candidate. The finiteness prediction is used
in the error identification component. Given the
output of the error identification stage, the corre-
sponding classifiers for each error type are invoked
to propose an appropriate correction.
We split the corpus documents into two equal
parts ? training and test. We chose a train-test split
and not cross-validation, since the FCE data set is
quite large to allow for such a split. The training
data is also used to develop the components for
candidate selection and verb finiteness prediction.
5.1 Candidate Selection
This stage selects the set of verb instances that
are presented as input to the classifier. A verb in-
stance refers to the verb, including its auxiliaries
or the infinitive marker (e.g. ?found?, ?will find?,
?to find?). Candidate selection is a crucial step for
models that correct mistakes on open-class words
because those errors that are missed at this stage
have no chance of being detected. We implement
four candidate selection methods. Method (1) ex-
tracts all verbs heading a verb phrase, as identi-
fied by a shallow parser (Punyakanok and Roth,
2001).
3
Method (2) also includes words tagged
with one of the verb tags: {VB, VBN, VBG,
VBD, VBP, VBZ} predicted by the POS tagger.
4
However, relying on the POS information is not
good enough, since the POS tagger performance
on ESL data is known to be suboptimal (Nagata et
al., 2011). For example, verbs lacking agreement
markers are likely to be mistagged as nouns (Lee
and Seneff, 2008). Methods (3) and (4) address
the problem of pre-processing errors. Method (3)
adds words that are on the list of valid English
verb lemmas; the lemma list is constructed us-
ing a POS-tagged version of the NYT section of
the Gigaword corpus and contains about 2,600 of
frequently-occurring words tagged as VB; for ex-
ample, (3) will add shop but not shopping, but (4)
will add both.
For methods (3) and (4), we developed verb-
Morph,
5
a tool that performs morphological anal-
ysis on verbs and is used to lemmatize verbs and
to generate morphological variants. The module
makes uses of (1) the verb lemma list and (2) a list
of irregular English verbs.
The quality of the candidate selection methods
is evaluated in Table 5 on the gold subset by com-
puting the recall, i.e. the percentage of erroneous
verbs that have been selected as candidates. Meth-
ods that address pre-processing mistakes are able
to recover more erroneous verb candidates in text.
It is also interesting to note that across all methods,
the highest recall is obtained for tense errors. This
suggests that the POS tagger is more prone to fail-
3
http://cogcomp.cs.illinois.edu/demo/shallowparse
4
http://cogcomp.cs.illinois.edu/page/software view/POS
5
The tool and more detail about it can be found at
http://cogcomp.cs.illinois.edu/page/publication view/743
361
Method Recall Recall by error group (%)
(%) Agr. Tense Form
(1) All verb phrases 83.00 86.62 93.55 59.08
(2) + tokens tagged as verbs 91.96 90.30 94.33 87.79
(3) + tokens that are valid
verb lemmas
95.50 95.99 96.46 93.23
(4) + tokens with inflections
that are valid verb lemmas
96.09 96.32 96.62 94.84
Table 5: Candidate selection methods performance.
ure due to errors in agreement and form. The eval-
uation in Table 5 uses recall, as the goal is to assess
the ability of the methods to select erroneous verbs
as candidates. In Sec. 6.1, the contribution of each
method to error identification is evaluated.
5.2 Predicting Verb Finiteness
Predicting verb finiteness is not trivial, as almost
all English verbs can occur in both finite and non-
finite form and the surface forms of a verb in finite
and non-finite form may be the same (see Table 2).
While we cannot learn verb type automatically
due to lack of annotation, we show, however, that,
for the majority of verbs, finiteness can be reliably
predicted using linguistic knowledge. We imple-
ment a decision-list classifier that makes use of
linguistically-motivated rules (Table 6). The algo-
rithm covers about 92% of all verb candidates, ab-
staining on the remaining highly-ambiguous 8%.
The evaluation of the method on the gold sub-
set (last column in Table 6) shows that despite its
simplicity, this method is highly effective: 98% on
correct verbs and over 89% on errors.
5.3 Features
The baseline features are word n-grams in the 4-
word window around the verb instance. Addi-
tional features are intended to characterize a given
error type and are selected based on previous stud-
ies: for Agreement and Form errors, we use a
parser (Klein and Manning, 2003) and define fea-
tures that reflect dependency relations between the
verb and its neighbors. We denote these features
by syntax. Syntactic knowledge via tree patterns
has been shown useful for Agreement mistakes
(Lee and Seneff, 2008). Features for Tense in-
clude temporal adverbs in the sentence and tenses
of other verbs in the sentence and are similar to
the features used in other verb classification tasks
(Reichart and Rappoport, 2010; Lee, 2011; Tajiri
et al., 2012). The features are shown in Table 7.
5.4 Error Identification
The goal of this stage is to identify errors and to
predict their type. We define a linear model where,
given a verb, a weight vector w assigns a score
to each label in the label space {Correct, Form,
Agreement, Tense}. The prediction of the classi-
fier is the label with the highest score.
The baseline error identification model, called
combined, is agnostic to the type of the verb. In
the combined model, for each verb v and label l,
we generate a feature vector, ?(v, l) and the best
label is predicted as
argmax
l
w
T
?(v, l).
The combined model makes use of all the fea-
tures we have defined earlier for each verb.
The type-based model uses the verb finiteness
prediction made by the verb finiteness classifier.
A soft way to use the finiteness prediction is to
add the predicted finiteness value as a feature. The
other ? hard-decision approach ? is to use only
a subset of the features depending on the pre-
dicted finiteness: Agreement and Tense for the fi-
nite verbs, and Form features for non-finite. The
hard-decision type-driven approach defines a fea-
ture vector for a verb based on its type. Thus,
given the verb v and its type t, we define fea-
tures ?(v, t, l) for each label l. Thus, the label is
predicted as
argmax
l
w
T
?(v, t, l).
5.5 Error Correction
The correction module consists of three compo-
nents, one for each type of mistake. Given the
output of the error identification model, the ap-
propriate correction component is run for each in-
stance predicted to be a mistake.
6
The verb finite-
ness prediction is used to select finite instances for
training the Agreement and Tense components and
non-finite ? for the Form component. The label
space for Tense specifies tense and aspect prop-
erties of the English verbs (see Tajiri et al., 2012
for more detail), the Agreement component spec-
ifies the person and number properties, while the
Form component includes the commonly confus-
able non-finite English forms (see Table 2). These
components are trained as multiclass classifiers.
6
We assume that each verb contains at most one mistake.
Less than 1% of all erroneous verbs have more than one error
present.
362
A verb is Non-Finite if any of the following hold: A verb is Finite if any of the following hold Accuracy on
Correct Erroneous
verbs verbs
(1) All verbs identified by shallow parser
98.01 89.4
(1) [numTokens = 2] ? [firstToken = to] (2) can; could
(2) firstToken = be (3) [numTokens = 1] ? [pos ? {V BD, V BP, V BZ}]
(3) [numTokens = 1] ? [pos = V BG] (4) [numTokens = 2] ? [firstToken! = to]
(5) numTokens > 2
Table 6: Algorithm for determining verb type. numTokens denotes the number of tokens in the verb instance, e.g., for the
verb instance ?to go?, numTokens = 2. Verbs not covered by the rules, e.g. those that are not tagged with a verb-related POS
in methods (3) and (4), are not assigned any verb type. The last column shows algorithm accuracy on the gold subset separately
for correct and incorrect verbs.
Agreement Description
(1) subjHead, subjPOS The surface form and the POS tag of the subject head
(2) subjDet {those,this,..} Determiner of the subject phrase
(3) subjDistance Distance between the verb and the subject head
(4) subjNumber {Sing, Pl} Sing ? singular pronouns and nouns; Pl ? plural pronouns and nouns
(5) subjPerson {3rdSing, Not3rdSing, 1stSing} 3rdSing ? she,he,it,singular nouns; Not3rdSing ? we,you,they, plural nouns; 1stSing ? ?I?
(6) conjunctions (1)&(3);(4)&(5)
Tense Description
(1) verb phrase (VP) verb lemma, negation, surface forms and POS tags of all words in the verb phrase
(2) verbs in sentence(4 features) tenses and lemmas of the finite verbs preceding and following the verb instance
(3) time adverbs (2 features) temporal adverb before and after the verb instance
(4) bag-of-words (BOW) (8 features) Includes the following words in the sentence: {if, when, since, then, wish, hope, when, since,
after}
Form Description
(1) closest word surface form, lemma, POS tag, and distance of the closest open-class word to the left of the
verb
(2) governor surface form, POS tag and dependency type of the target
(3) preposition if the verb is preceded by a preposition: preposition itself and the surface form, POS tag and
dependency of the governor of the preposition
(4) pos and lemma POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word
ngrams
Table 7: Features used, grouped by error type.
6 Experiments
The main goal of this work is to propose a uni-
fied framework for correcting verb mistakes and
to address the specific challenges of the problem.
We thus do not focus on features or on the spe-
cific learning algorithm. Our experimental study
addresses the following research questions:
I. Linguistic questions: (i) candidate selection
methods; (ii) verb finiteness contribution to
error identification
II. Computational Framework: error identifi-
cation vs. correction
III. Gold annotation: (i) using gold candidates
and verb type vs. automatic; (ii) performance
comparison by error type
Learning Framework There is a lot of under-
standing for which algorithmic methods work
best for ESL correction tasks, how they compare
among themselves, and how they compare to n-
gram based methods. Specifically, despite their in-
tuitive appeal, language models were shown to not
work well on these tasks, while the discriminative
learning framework has been shown to be superior
to other approaches and thus is commonly used
for error correction tasks (see Sec. 2). Since we
do not address the algorithmic aspect of the prob-
lem, we refer the reader to Rozovskaya and Roth
(2011) for a discussion of these issues. We train
all our models with the SVM learning algorithm
implemented in JLIS (Chang et al., 2010).
Evaluation We report both Precision/Recall
curves and AAUC (as a summary). Error cor-
rection is generally evaluated using F1 (Dale et
al., 2012); Precision and Recall (Gamon, 2010;
Tajiri et al., 2012); or Average Area Under Curve
(AAUC) (Rozovskaya and Roth, 2011). For a dis-
cussion on these metrics with respect to error cor-
rection tasks, we refer the reader to Rozovskaya
(2013). AAUC (Hanley and McNeil, 1983)) is a
measure commonly used to generate a summary
statistic, computed as an average precision value
over a range of recall points. In this paper, AAUC
is computed over the first 15 recall points:
AAUC =
1
15
?
15
?
i=1
Precision(i).
6.1 Linguistic Questions
Candidate Selection Methods The contribution
of the candidate selection component with respect
to error identification is evaluated in Table 8, us-
ing the methods presented in Sec. 5.1. Overall,
363
Recall of candidate AAUC
selection method (%) Combined Type-based
(1) (83.00) 73.38 79.49
(2) (91.96) 80.36 86.48
(3) (95.50) 81.39 87.05
(4) (96.09) 81.27 86.81
Table 8: Impact of candidate selection methods on error
identification performance. The first column shows the per-
centage of erroneous verbs selected by each method. Type-
based models are discussed in Sec. 6.1.
Correct verbs Erroneous verbs Error rate
Training 41721 1981 4.75%
Test 41836 2014 4.81%
Table 9: Training and test data statistics. Candidates are
selected using method (3).
better performance is achieved by methods with
higher recall, with the exception of method (4); its
performance on error identification is behind that
of method (3), perhaps due to the amount of noise
that is also added. While the difference is small,
method (3) is also simpler than method (4). We
thus use method (3) in the rest of the paper. Table
9 shows the number of verb instances in training
and test selected with this method.
Verb Finiteness Sec. 5.4 presented two ways of
adding verb finiteness: (1) adding the predicted
verb type as a feature and (2) selecting only the
relevant features depending on the finiteness of the
verb. Table 10 shows the results of using verb type
in the error identification stage. While the first
approach does not provide improvement over the
combined model, the second method is very ef-
fective. We conjecture that because verb type pre-
diction is quite accurate, the second, hard-decision
approach is preferred, as it provides knowledge in
a direct way. Henceforth, we will use the second
method in the type-based model.
Fig. 1 compares the performance of the com-
bined and the hard-decision type-based models
shown in Table 10. Precision/Recall curves are
generated by varying the threshold on the confi-
dence of the classifier. This graph reveals the be-
havior of the systems at multiple recall points: we
observe that at every recall point the type-based
classifier has higher precision.
So far, the models used all features defined in
Sec. 5.3. Table 11 reveals that the type-driven
Model AAUC
Combined 81.39
Type-based I (soft) 81.11
Type-based II (hard) 87.05
Table 10: Verb finiteness contribution to error identifi-
cation.
 
70
 
75
 
80
 
85
 
90
 
95  0
 
2
 
4
 
6
 
8
 
10
 
12
 
14
PRECISION
RECAL
L
Comb
ined
Type-
based
Figure 1: Verb finiteness contribution to error identifi-
cation: key result. AAUC shown in Table 10. The combined
model uses no verb type information. In the hard-decision
type-based model, each verb uses the features according to
its finiteness. The differences are statistically significant (Mc-
Nemar?s test, p < 0.0001).
Feature set AAUC
Combined Type-based
Baseline 46.62 49.72
All?Syntax 79.47 84.88
Full feature set 81.39 87.05
Table 11: Verb finiteness contribution to error identifi-
cation for different features.
approach is superior to the combined approach
across different feature sets, and the performance
gap increases with more sophisticated feature sets,
which is to be expected, since more complex fea-
tures are tailored toward relevant verb errors. Fur-
thermore, adding features specific to each error
type significantly improves the performance over
the word n-gram features. The rest of the experi-
ments use all features (denoted Full feature set).
6.2 Identification vs. Correction
After running the error identification component,
we apply the appropriate correction models to
those instances identified as errors. The results
for identification and correction are shown in Ta-
ble 12. The correction models are also finiteness-
aware models trained on the relevant verb in-
stances (finite or non-finite), as predicted by the
verb finiteness classifier.
We evaluate the correction components by fix-
ing a recall point in the error identification stage.
7
We observe the relatively low recall obtained by
the models. Error correction models tend to have
low recall (see, for example, the recent shared
tasks on ESL error correction (Dale and Kilgar-
riff, 2011; Dale et al., 2012; Ng et al., 2013)). The
key reason for the low recall is the error sparsity:
over 95% of verbs are correct, as shown in Table 9.
7
We can increase recall using a different threshold but
higher precision is preferred in error correction tasks.
364
Error type Correction Identification
P R F1 P R F1
Agreement 90.62 9.70 17.52 90.62 9.70 17.52
Tense 60.51 7.47 13.31 86.62 10.70 19.05
Form 81.82 16.34 27.24 83.47 16.67 27.79
Total 71.94 10.24 17.94 85.81 12.22 21.20
Table 12: Performance of the complete model after the
correction stage. The results on Agreement mistakes are the
same, since Agreement errors are always binary decisions,
unlike Tense and Form mistakes.
The only way to improve over this 95% baseline is
by forcing the system to have very good precision
(at the expense of recall). The performance shown
in Table 12 corresponds to an accuracy of 95.60%
in identification (error reduction of 8.7%) and
95.40% in correction (error reduction of 4.5%)
over the baseline of 95.19%.
6.3 Analysis on Gold Data
To further study the impact of each step of the sys-
tem, we analyze our model on the gold subset of
the data. The gold subset contains two additional
pieces of information not available for the rest of
the corpus: gold verb candidates and gold verb
finiteness (Sec. 4). The set contains 7784 gold
verbs, including 464 errors. Experiments are run
in 10-fold cross-validation where on each run 90%
of the documents are used for training and the re-
maining 10% are used for evaluation. The gold
annotation can be used instead of automatic pre-
dictions in two system components: (1) candidate
selection and (2) verb finiteness.
Table 13 shows the performance on error identi-
fication when gold vs. automatic settings are used.
As expected, using the gold verb type is more ef-
fective than using the automatic one, both with au-
tomatic and gold candidates. The same is true for
candidate selection. For instance, the combined
model improves by 14 AAUC points (from 55.90
to 69.86) with gold candidates. These results indi-
cate that candidate selection is an important com-
ponent of the verb error correction system.
Note that compared to the performance on the
entire data set (Table 10), the performance of the
models shown here that use automatic components
is lower, since the training size is smaller. On the
other hand, because of the smaller training size,
the gain due to the type-based approach is larger
on the gold subset (19 vs. 6 AAUC points).
Finally, in Table 14, we evaluate the contribu-
tion of verb finiteness to error identification by er-
ror type. While performance varies by error, it is
clear that all errors benefit from verb typing.
Candidate selection Verb type prediction AAUC
Automatic
None 55.90
Automatic 74.72
Gold 89.45
Gold
None 69.86
Automatic 90.89
Gold 96.42
Table 13: Gold subset: error identification with gold vs.
automatic candidates and finiteness information. Value
None for verb type prediction denotes the combined model.
Error type AAUC
Combined Type-based Type-based
Automatic Gold
Agreement 86.80 88.43 89.21
Tense 18.07 25.62 26.87
Form 97.08 98.23 98.36
Table 14: Gold subset: gold vs. automatic finiteness con-
tribution to error identification by error type.
7 Conclusion
Verb errors are commonly made by ESL writers
but difficult to address due to to their diversity
and the fact that identifying verbs in (noisy) text
may itself be difficult. We develop a linguistically-
inspired approach that first identifies verb candi-
dates in noisy learner text and then makes use
of verb finiteness to identify errors and character-
ize the type of mistake. This is important, since
most errors made by non-native speakers cannot
be identified by considering only closed classes
(e.g., prepositions and articles). Our model inte-
grates a statistical machine learning approach with
a rule-based system that encodes linguistic knowl-
edge to yield the first general correction approach
to verb errors (that is, one that does not assume
prior knowledge of which mistake was made).
This work thus provides a first step in consider-
ing more general algorithmic paradigms for cor-
recting grammatical errors and paves the way for
developing models to address other ?open-class?
mistakes.
Acknowledgments
The authors thank Graeme Hirst, Julia Hockenmaier, Mark
Sammons, and the anonymous reviewers for their helpful
feedback. This work was done while the first and the third
authors were at the University of Illinois. This material is
based on research sponsored by DARPA under agreement
number FA8750-13-2-0008 and by the Army Research Lab-
oratory (ARL) under agreement W911NF-09-2-0053. Any
opinions, findings, conclusions or recommendations are those
of the authors and do not necessarily reflect the view of the
agencies.
365
References
M. Banko and E. Brill. 2001. Scaling to very very
large corpora for natural language disambiguation.
In Proceedings of 39th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 26?33,
Toulouse, France, July.
A. Carlson and I. Fette. 2007. Memory-based context-
sensitive spelling correction at web scale. In Pro-
ceedings of the IEEE International Conference on
Machine Learning and Applications (ICMLA).
A. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In Proceedings of
the National Conference on Innovative Applications
of Artificial Intelligence (IAAI), pages 45?50.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010. Structured output learning with indirect su-
pervision. In Proc. of the International Conference
on Machine Learning (ICML).
D. Dahlmeier and H. T. Ng. 2011. Grammatical er-
ror correction with alternating structure optimiza-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 915?923, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A
report on the preposition and determiner error cor-
rection shared task. In Proc. of the NAACL HLT
2012 Seventh Workshop on Innovative Use of NLP
for Building Educational Applications, Montreal,
Canada, June. Association for Computational Lin-
guistics.
G. Dalgish. 1985. Computer-assisted ESL research.
CALICO Journal, 2(2).
R. De Felice and S. Pulman. 2008. A classifier-based
approach to preposition and determiner error correc-
tion in L2 English. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 169?176, Manchester, UK,
August.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings
of IJCNLP.
M. Gamon, C. Leacock, C. Brockett, W. B. Dolan,
J. Gao, D. Belenko, and A. Klementiev. 2009. Us-
ing statistical techniques and web search to correct
ESL errors. CALICO Journal, Special Issue on Au-
tomatic Analysis of Learner Language, 26(3):491?
511.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?
171, Los Angeles, California, June.
A. R. Golding and D. Roth. 1996. Applying Winnow
to context-sensitive spelling correction. In Proc. of
the International Conference on Machine Learning
(ICML), pages 182?190.
A. R. Golding and D. Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
N. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Journal of Natural Language Engineer-
ing, 12(2):115?129.
J. Hanley and B. McNeil. 1983. A method of com-
paring the areas under receiver operating character-
istic curves derived from the same cases. Radiology,
148(3):839?843.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and
H. Isahara. 2003. Automatic error detection in
the Japanese learners? English spoken data. In The
Companion Volume to the Proceedings of 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 145?148, Sapporo, Japan, July.
T.-H. Kao, Y.-W. Chang, H. w. Chiu, T-.H. Yen, J. Bois-
son, J. c. Wu, and J.S. Chang. 2013. Conll-2013
shared task: Grammatical error correction nthu sys-
tem description. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 20?25, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Advances in Neural Information Pro-
cessing Systems 15 NIPS, pages 3?10. MIT Press.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan and Claypool Publish-
ers.
J. Lee and S. Seneff. 2008. Correcting misuse of verb
forms. In ACL, pages 174?182, Columbus, Ohio,
June. Association for Computational Linguistics.
J. Lee. 2011. Verb tense generation. Social and Be-
havioral Sciences, 27:122?130.
R. Nagata, E. Whittaker, and V. Sheinman. 2011. Cre-
ating a manually error-tagged and shallow-parsed
learner corpus. In ACL, pages 1210?1219, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The CoNLL-2013 shared task
on grammatical error correction. In Proc. of the
Seventeenth Conference on Computational Natural
366
Language Learning. Association for Computational
Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 995?1001. MIT Press.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman, New York.
A. Radford. 1988. Transformational Grammar. Cam-
bridge University Press.
R. Reichart and A. Rappoport. 2010. Tense sense
disambiguation: A new syntactic polysemy task.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
325?334, Cambridge, MA, October. Association for
Computational Linguistics.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of
the NAACL Workshop on Innovative Use of NLP for
Building Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Training
paradigms for correcting errors in grammar and us-
age. In Proceedings of the NAACL-HLT.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL, Portland, Oregon, 6. Association for Com-
putational Linguistics.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system
in the CoNLL-2013 shared task. In CoNLL Shared
Task.
A. Rozovskaya. 2013. Automated Methods for Text
Correction. Ph.D. thesis.
T. Tajiri, M. Komachi, and Y. Matsumoto. 2012. Tense
and aspect error correction for esl learners using
global context. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 198?202,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Us-
ing parse features for preposition selection and error
detection. In ACL.
H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011.
A new dataset and method for automatically grading
esol texts. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 180?189, Portland, Oregon, USA, June.
Association for Computational Linguistics.
367
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 154?162,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Training Paradigms for Correcting Errors in Grammar and Usage
Alla Rozovskaya and Dan Roth
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,danr}@illinois.edu
Abstract
This paper proposes a novel approach to the
problem of training classifiers to detect and
correct grammar and usage errors in text by
selectively introducing mistakes into the train-
ing data. When training a classifier, we would
like the distribution of examples seen in train-
ing to be as similar as possible to the one seen
in testing. In error correction problems, such
as correcting mistakes made by second lan-
guage learners, a system is generally trained
on correct data, since annotating data for train-
ing is expensive. Error generation methods
avoid expensive data annotation and create
training data that resemble non-native data
with errors.
We apply error generation methods and train
classifiers for detecting and correcting arti-
cle errors in essays written by non-native En-
glish speakers; we show that training on data
that contain errors produces higher accuracy
when compared to a system that is trained on
clean native data. We propose several train-
ing paradigms with error generation and show
that each such paradigm is superior to training
a classifier on native data. We also show that
the most successful error generation methods
are those that use knowledge about the arti-
cle distribution and error patterns observed in
non-native text.
1 Introduction
This paper considers the problem of training clas-
sifiers to detect and correct errors in grammar and
word usage in text. Both native and non-native
speakers make a variety of errors that are not always
easy to detect. Consider, for example, the problem
of context-sensitive spelling correction (e.g., (Gold-
ing and Roth, 1996; Golding and Roth, 1999; Carl-
son et al, 2001)). Unlike spelling errors that result in
non-words and are easy to detect, context-sensitive
spelling correction task involves correcting spelling
errors that result in legitimate words, such as confus-
ing peace and piece or your and you?re. The typical
training paradigm for these context-sensitive ambi-
guities is to use text assumed to be error free, replac-
ing each target word occurrence (e.g. peace) with a
confusion set consisting of, say {peace, piece}, thus
generating both positive and negative examples, re-
spectively, from the same context.
This paper proposes a novel error generation ap-
proach to the problem of training classifiers for the
purpose of detecting and correcting grammar and
usage errors in text. Unlike previous work (e.g.,
(Sjo?bergh and Knutsson, 2005; Brockett et al, 2006;
Foster and Andersen, 2009)), we selectively intro-
duce mistakes in an appropriate proportion. In par-
ticular, to create training data that closely resemble
text with naturally occurring errors, we use error fre-
quency information and error distribution statistics
obtained from corrected non-native text. We apply
the method to the problem of detecting and correct-
ing article mistakes made by learners of English as
a Second Language (ESL).
The problem of correcting article errors is gener-
ally viewed as that of article selection, cast as a clas-
sification problem and is trained as described above:
a machine learning algorithm is used to train a clas-
sifier on native English data, where the possible se-
lections are used to generate positive and negative
154
examples (e.g., (Izumi et al, 2003; Han et al, 2006;
De Felice and Pulman, 2008; Gamon et al, 2008)).
The classifier is then applied to non-native text to
predict the correct article in context. But the article
correction problem differs from the problem of ar-
ticle selection in that we know the original (source)
article that the writer used. When proposing a cor-
rection, we would like to use information about the
original article. One reason for this is that about 90%
of articles are used correctly by ESL learners; this is
higher than the performance of state-of-the-art clas-
sifiers for article selection. Consequently, not us-
ing the writer?s article, when making a prediction,
may result in making more mistakes than there are
in the data. Another reason is that statistics on ar-
ticle errors (e.g., (Han et al, 2006; Lee and Sen-
eff, 2008)) and in the annotation performed for the
present study reveal that non-native English speak-
ers make article mistakes in a consistent manner.
The system can consider the article used by the
writer at evaluation time, by proposing a correction
only when the confidence of the classifier is high
enough, but the article cannot be used in training
if the classifier is trained on clean native data that
do not have errors. Learning Theory says that the
distribution of examples seen in testing should be
as similar as possible to the one seen in training, so
one would like to train on errors similar to those ob-
served in testing. Ideally, we would like to train us-
ing corrected non-native text. In that case, the orig-
inal article of the writer can be used as a feature for
the classifier and the correct article, as judged by
a native English speaker, will be viewed as the la-
bel. However, obtaining annotated data for training
is expensive and, since the native training data do
not contain errors, we cannot use the writer?s article
as a feature for the classifier.
This paper compares the traditional training
paradigm that uses native data to training paradigms
that use data with artificial mistakes. We propose
several methods of generating mistakes in native
training data and demonstrate that they outperform
the traditional training paradigm. We also show that
the most successful error generation methods use
knowledge about the article distribution and error
patterns observed in the ESL data.
The rest of the paper is organized as follows.
First, we discuss the baseline on the error correc-
tion task and show why the baselines used in selec-
tion tasks are not relevant for the error correction
task. Next, we describe prior work in error genera-
tion and show the key difference of our approach.
Section 4 presents the ESL data that we use and
statistics on article errors. Section 5 describes train-
ing paradigms that employ error generation. In Sec-
tions 6 and 7 we present the results and discuss the
results. The key findings are summarized in Table 7
in Section 6. We conclude with a brief discussion of
directions for future work.
2 Measuring Success in Error Correction
Tasks
The distinction between the selection and the error
correction tasks alluded to earlier is important not
only for training but also in determining an appro-
priate evaluation method.
The standard baseline used in selection tasks is
the relative frequency of the most common class.
For example, in word sense disambiguation, the
baseline is the most frequent sense. In the task
of article selection, the standard baseline used is
to predict the article that occurs most frequently in
the data (usually, it is the zero article, whose fre-
quency is 60-70%). In this context, the performance
of a state-of-the-art classifier (Knight and Chander,
1994; Minnen et al, 2000; Turner and Charniak,
2007; Gamon et al, 2008) whose accuracy is 85-
87% is a significant improvement over the base-
line. The majority has been used as the baseline also
in the context-sensitive spelling task (e.g., (Golding
and Roth, 1999)).
However, in article correction, spelling correc-
tion, and other text correction applications the split
of the classes is not an appropriate baseline since the
majority of the words in the confusion set are used
correctly in the text. Han et al (2006) report an av-
erage error rate of 13% on article data from TOEFL
essays, which gives a baseline of 87%, versus the
baseline of 60-70% used in the article selection task.
Statistics on article mistakes in our data suggest a
baseline of about 90%, depending on the source lan-
guage of the writer. So the real baseline on the task
is ?do nothing?. Therefore, to determine the base-
line for a correction task, one needs to consider the
error rate in the data.
155
Using the definitions of precision and recall and
the ?real? baseline, we can also relate the resulting
accuracy of the classifier to the precision and recall
on an error correction task as follows: Let P and R
denote the precision and recall, respectively, of the
system on an error correction task, and Base denote
the error rate in the data. Then the task baseline (i.e.,
accuracy of the data before running the system) is:
Baseline = 1?Base
It can be shown that the error rate after running the
classifier is:
Error =
Base ? (P + R? 2RP )
P
It follows that the accuracy of the system on the task
is 1? Error.
For example, we can obtain a rough estimate on
the accuracy of the system in Han et al (2006), us-
ing precision and recall numbers by error type. Ex-
cluding the error type of category other, we can esti-
mate that Base = 0.1, so the baseline is 0.9, average
precision and recall are 0.85 and 0.25, respectively,
and the resulting overall accuracy of the system is
92.2%.
3 Related Work
3.1 Generating Errors in Text
In text correction, adding mistakes in training has
been explored before. Although the general ap-
proach has been to produce errors similar to those
observed in the data to be corrected, mistakes were
added in an ad-hoc way, without respecting the er-
ror frequencies and error patterns observed in non-
native text. Izumi et al (2003) train a maxi-
mum entropy model on error-tagged data from the
Japanese Learners of English corpus (JLE, (Izumi et
al., 2004)) to detect 8 error types in the same cor-
pus. They show improvement when the training set
is enhanced with sentences from the same corpus
to which artificial article mistakes have been added.
Though it is not clear in what proportion mistakes
were added, it is also possible that the improvement
was due to a larger training set. Foster and Ander-
sen (2009) attempt to replicate naturally occurring
learner mistakes in the Cambridge Learner Corpus
(CLC)1, but show a drop in accuracy when the orig-
inal error-tagged data in training are replaced with
corrected CLC sentences containing artificial errors.
Brockett et al (2006) generate mass noun er-
rors in native English data using relevant exam-
ples found in the Chinese Learners English Cor-
pus (CLEC, (Gui and Yang, 2003)). Training data
consist of an equal number of correct and incor-
rect sentences. Sjo?bergh and Knutsson (2005) in-
troduce split compound and agreement errors into
native Swedish text: agreement errors are added in
every sentence and for compound errors, the train-
ing set consists of an equal number of negative and
positive examples. Their method gives higher recall
at the expense of lower precision compared to rule-
based grammar checkers.
To sum up, although the idea of using data with ar-
tificial mistakes is not new, the advantage of training
on such data has not been investigated. Moreover,
training on error-tagged data is currently unrealistic
in the majority of error correction scenarios, which
suggests that using text with artificial mistakes is the
only alternative to using clean data. However, it has
not been shown whether training on data with artifi-
cial errors is beneficial when compared to utilizing
clean data. More importantly, error statistics have
not been considered for error correction tasks. Lee
and Seneff (2008) examine statistics on article and
preposition mistakes in the JLE corpus. While they
do not suggest a specific approach, they hypothesize
that it might be helpful to incorporate this knowl-
edge into a correction system that targets these two
language phenomena.
3.2 Approaches to Detecting Article Mistakes
Automated methods for detecting article mistakes
generally use a machine learning algorithm. Ga-
mon et al (2008) use a decision tree model and a
5-gram language model trained on the English Giga-
word corpus (LDC2005T12) to correct errors in En-
glish article and preposition usage. Han et al (2006)
and De Felice and Pulman (2008) train a maximum
entropy classifier. Yi et al (2008) propose a web
count-based system to correct determiner errors. In
the above approaches, the classifiers are trained on
native data. Therefore the classifiers cannot use the
1http://www.cambridge.org/elt
156
original article that the writer used as a feature. Han
et al (2006) use the source article at evaluation time
and propose a correction only when the score of the
classifier is high enough, but the source article is not
used in training.
4 Article Errors in ESL Data
Article errors are one of the most common mistakes
that non-native speakers make, especially those
whose native language does not have an article sys-
tem. For example, Han et al (2006) report that in
the annotated TOEFL data by Russian, Chinese, and
Japanese speakers 13% of all noun phrases have an
incorrect article. It is interesting to note that article
errors are present even with very advanced speakers.
While the TOEFL data include essays by students of
different proficiency levels, we use data from very
advanced learners and find that error rates on articles
are similar to those reported by Han et al (2006).
We use data from speakers of three first language
backgrounds: Chinese, Czech, and Russian. None
of these languages has an article system. The Czech
and the Russian data come from the ICLE corpus
(Granger et al, 2002), which is a collection of es-
says written by advanced learners of English. The
Chinese data is a part of the CLEC corpus that con-
tains essays by students of all levels of proficiency.
4.1 Data Annotation
A portion of data for each source language was cor-
rected and error-tagged by native speakers. The an-
notation was performed at the sentence level: a sen-
tence was presented to the annotator in the context
of the entire essay. Essay context can become nec-
essary, when an article is acceptable in the context
of a sentence, but is incorrect in the context of the
essay. Our goal was to correct all article errors, in-
cluding those that, while acceptable in the context of
the sentence, were not correct in the context of the
essay. The annotators were also encouraged to pro-
pose more than one correction, as long as all of their
suggestions were consistent with the essay context.
The annotators were asked to correct all mistakes
in the sentence. The annotation schema included
the following error types: mistakes in article and
preposition usage, errors in noun number, spelling,
verb form, and word form2. All other corrections
were marked as word replacement, word deletion,
and word insertion. For details about annotation and
data selection, please refer to the companion paper
(Rozovskaya and Roth, 2010).
4.2 Statistics on Article Errors
Traditionally, three article classes are distinguished:
the, a(an)3 and None (no article). The training and
the test data are thus composed of two types of
events:
1. All articles in the data
2. Spaces in front of a noun phrase if that noun
phrase does not start with an article. To identify
the beginning of a noun phrase, we ran a part-
of-speech tagger and a phrase chunker4 and ex-
cluded all noun phrases not headed5 by a per-
sonal or demonstrative pronoun.
Table 1 shows the size of the test data by source
language, proportion of errors and distribution of ar-
ticle classes before and after annotation and com-
pares these distributions to the distribution of articles
in English Wikipedia. The distribution before anno-
tation shows statistics on article usage by the writers
and the distribution after annotation shows statistics
after the corrections made by the annotators were
applied. As the table shows, the distribution of arti-
cles is quite different for native data (Wikipedia) and
non-native text. In particular, non-native data have a
lower proportion of the.
The annotation statistics also reveal that learn-
ers do not confuse articles randomly. From Table
2, which shows the distribution of article errors by
type, we observe that the majority of mistakes are
omissions and extraneous articles. Table 3 shows
statistics on corrections by source and label, where
source refers to the article used by the writer, and
label refers to the article chosen by the annotator.
Each entry in the table indicates Prob(source =
2Our classification, was inspired by the classification pre-
sented in Tetreault and Chodorow (2008)
3Henceforth, we will use a to refer to both a and an
4The tagger and the chunker are available at http://
L2R.cs.uiuc.edu/?cogcomp/software.php
5We assume that the last word of the noun phrase is its head.
157
Source Number of Proportion of Errors Article Classes
language test examples errors total distribution a the None
Chinese 1713 9.2% 158
Before annotation 8.5 28.2 63.3
After annotation 9.9 24.9 65.2
Czech 1061 9.6% 102
Before annotation 9.1 22.9 68.0
After annotation 9.9 22.3 67.8
Russian 2146 10.4% 224
Before annotation 10.5 21.7 67.9
After annotation 12.5 20.1 67.4
English Wikipedia 9.6 29.1 61.4
Table 1: Statistics on articles in the annotated data before and after annotation.
Source Proportion of Errors total Errors by Type
language errors in the data Extraneous Missing a Missing the Confusion
Chinese 9.2% 158 57.0% 13.3% 22.8% 7.0%
Czech 9.6% 102 45.1% 14.7% 33.3% 6.9%
Russian 10.4% 224 41.5% 20.1% 25.5% 12.3%
Table 2: Distribution of article errors in the annotated data by error type. Extraneous refers to using a or the where
None (no article) is correct. Confusion is using a instead of the or vice versa.
Label Source Source
language a the None
a
Chinese 81.7% 5.9% 12.4%
Czech 81.0% 4.8% 14.3%
Russian 75.3% 7.9% 16.9%
the
Chinese 0.2% 91.3% 8.5%
Czech 0.9% 84.7% 14.4%
Russian 1.9% 84.9% 13.2%
None
Chinese 0.6% 7.4%% 92.0%
Czech 1.3% 5.2% 93.6%
Russian 1.0% 5.4%% 93.6%
Table 3: Statistics on article corrections by the original
article (source) and the annotator?s choice (label). Each
entry in the table indicates Prob(source = s|label = l)
for each article pair.
s|label = l) for each article pair. We can also ob-
serve specific error patterns. For example, the is
more likely than a to be used superfluously.
5 Introducing Article Errors into Training
Data
This section describes experiments with error gener-
ation methods. We conduct four sets of experiments.
Each set differs in how article errors are generated in
the training data. We now give a description of error
generation paradigms in each experimental set.
5.1 Methods of error generation
We refer to the article that the writer used in the ESL
data as source, and label refers to the article that
the annotator chose. Similarly, when we introduce
errors into the training data, we refer to the original
article as label and to the replacement as source.
This is because the original article is the correct
article choice, and the replacement that the classifier
will see as a feature can be an error. We call this
feature source feature. In other words, both for
training (native data) and test (ESL data), source
denotes the form that the classifier sees as a feature
(which could be an error) and label denotes the
correct article. Below we describe how errors are
generated in each set of experiments.
Method 1: General With probability x each ar-
ticle in the training data is replaced with
a different article uniformly at random, and
with probability (1 ? x) it remains un-
changed. We build six classifiers, where x
? {5%, 10%, 12%, 14%, 16%, 18%}. We call
this method general since it uses no informa-
tion about article distribution in the ESL data.
Method 2: ArticleDistrBeforeAnnot We use the
distribution of articles in the ESL data before
the annotation to change the distribution of ar-
ticles in the training. Specifically, we change
the articles so that their distribution approxi-
mates the distribution of articles in the ESL
data. For example, the relative frequency of
the in English Wikipedia data is 29.1%, while
in the writing by Czech speakers it is 22.3%.
It should be noted that this method changes
the distribution only of source articles, but the
158
distribution of labels is not affected. An ad-
ditional constraint that we impose is the mini-
mum error rate r for each article class, so that
Prob(s|l) ? r ?l ? labels. In this fashion, for
each source language we train four classifiers,
where we use article distribution from Chinese,
Czech, and Russian, and where we set the min-
imum error rate r to be ? {2%, 3%, 4%, 5%}.
Method 3: ArticleDistrAfterAnnot This method
is similar to the one above but we use the dis-
tribution of articles in the ESL data after the
corrections have been made by the annotators.
Method 4: ErrorDistr This method uses informa-
tion about error patterns in the annotated ESL
data. For example, in the Czech annotated sub-
corpus, label the corresponds to source the in
85% of the cases and corresponds to source
None in 14% of the cases. In other words, in
14% of the cases where the article the should
have been used, the writer used no article at all.
Thus, with probability 14% we change the in
the training data to None.
6 Experimental Results
In this section, we compare the quality of the sys-
tem trained on clean native English data to the qual-
ity of the systems trained on data with errors. The
errors were introduced into the training data using
error generation methods presented in Section 5.
In each training paradigm, we follow a discrimi-
native approach, using an online learning paradigm
and making use of the Averaged Perceptron Al-
gorithm (Freund and Schapire, 1999) implemented
within the Sparse Network of Winnow framework
(Carlson et al, 1999) ? we use the regularized
version in Learning Based Java6 (LBJ, (Rizzolo
and Roth, 2007)). While classical Perceptron
comes with generalization bound related to the mar-
gin of the data, Averaged Perceptron also comes
with a PAC-like generalization bound (Freund and
Schapire, 1999). This linear learning algorithm is
known, both theoretically and experimentally, to
be among the best linear learning approaches and
is competitive with SVM and Logistic Regression,
6LBJ code is available at http://L2R.cs.uiuc.edu/
?cogcomp/asoftware.php?skey=LBJ
while being more efficient in training. It also has
been shown to produce state-of-the-art results on
many natural language applications (Punyakanok et
al., 2008).
Since the methods of error generation described in
Section 5 rely on the distribution of articles and ar-
ticle mistakes and these statistics are specific to the
first language of the writer, we conduct evaluation
separately for each source language. Thus, for each
language group, we train five system types: one sys-
tem is trained on clean English data without errors
(the same classifier for the three language groups)
and four systems are trained on data with errors,
where errors are produced using the four methods
described in Section 5. Training data are extracted
from English Wikipedia.
All of the five systems employ the same set of fea-
tures based on three tokens to the right and to the left
of the target article. For each context word, we use
its relative position, its part-of-speech tag and the
word token itself. We also use the head of the noun
phrase and the conjunctions of the pairs and triples
of the six tokens and their part-of-speech tags7. In
addition to these features, the classifiers trained on
data with errors also use the source article as a fea-
ture. The classifier that is trained on clean English
data cannot use the source feature, since in training
the source always corresponds to the label. By con-
trast, when the training data contain mistakes, the
source is not always the same as the label, the situa-
tion that we also have with the test (ESL) data.
We refer to the classifier trained on clean data
as TrainClean. We refer to the classifiers trained
on data with mistakes as TWE (TrainWithErrors).
There are four types of TWE systems for each lan-
guage group, one for each of the methods of error
generation described in Section 5. All results are the
averaged results of training on three random sam-
ples from Wikipedia with two million training ex-
amples on each round. All five classifiers are trained
on exactly the same set of Wikipedia examples, ex-
cept that we add article mistakes to the data used
by the TWE systems. The TrainClean system
achieves an accuracy of 87.10% on data from En-
glish Wikipedia. This performance is state-of-the-
7Details about the features are given in the paper?s web page,
accessible from http://L2R.cs.uiuc.edu/?cogcomp/
159
art compared to other systems reported in the lit-
erature (Knight and Chander, 1994; Minnen et al,
2000; Turner and Charniak, 2007; Han et al, 2006;
De Felice and Pulman, 2008). The best results
of 92.15% are reported by De Felice and Pulman
(2008). But their system uses sophisticated syntac-
tic features and they observe that the parser does not
perform well on non-native data.
As mentioned in Section 4, the annotation of the
ESL data consisted of correcting all errors in the sen-
tence. We exclude from evaluation examples that
have spelling errors in the 3-word window around
the target article and errors on words that immedi-
ately precede or immediately follow the article, as
such examples would obscure the evaluation of the
training paradigms.
Tables 4, 5 and 6 show performance by language
group. The tables show the accuracy and the er-
ror reduction on the test set. The results of systems
TWE (methods 2 and 3) that use the distribution of
articles before and after annotation are merged and
appear as ArtDistr in the tables, since, as shown
in Table 1, these distributions are very similar and
thus produce similar results. Each table compares
the performance of the TrainClean system to the
performance of the four systems trained on data with
errors.
For all language groups, all classifiers of type
TWE outperform the TrainClean system. The
reduction in error rate is consistent when the TWE
classifiers are compared to the TrainClean system.
Table 7 shows results for all three languages, com-
paring for each language group the TrainClean
classifier to the best performing system of type
TWE.
Training Errors in Accuracy Error
paradigm training reduction
TrainClean 0.0% 91.85% -2.26%
TWE(General) 10.0% 92.57% 6.78%
TWE(ArtDistr) 13.2% 92.67% 8.33%
TWE(ErrorDistr) 9.2% 92.31% 3.51%
Baseline 92.03%
Table 4: Chinese speakers: Performance of the
TrainClean system (without errors in training) and of
the best classifiers of type TWE. Rows 2-4 show the
performance of the systems trained with error generation
methods described in 5. Error reduction denotes the per-
centage reduction in the number of errors when compared
to the number of errors in the ESL data.
Training Errors in Accuracy Error
paradigm training reduction
TrainClean 0.0% 91.82% 10.31%
TWE(General) 18.0% 92.22% 14.69%
TWE(ArtDistr) 21.6% 92.00% 12.28%
TWE(ErrorDistr) 10.2% 92.15% 13.93%
Baseline 90.88%
Table 5: Czech speakers: Performance of the
TrainClean system (without errors in training) and of
the best classifiers of type TWE. Rows 2-4 show the
performance of the systems trained with error generation
methods described in 5. Error reduction denotes the per-
centage reduction in the number of errors when compared
to the number of errors in the ESL data.
Training Errors in Accuracy Error
paradigm training reduction
TrainClean 0.0% 90.62% 5.92%
TWE(General) 14.0% 91.25% 12.24%
TWE(ArtDistr) 18.8% 91.52% 14.94%
TWE(ErrorDistr) 10.7% 91.63% 16.05%
Baseline 90.03%
Table 6: Russian speakers: Performance of the
TrainClean system (without errors in training) and of
the best classifiers of type TWE. Rows 2-4 show the
performance of the systems trained with error generation
methods described in 5. Error reduction denotes the per-
centage reduction in the number of errors when compared
to the number of errors in the ESL data.
7 Discussion
As shown in Section 6, training a classifier on
data that contain errors produces better results when
compared to the TrainClean classifier trained on
clean native data. The key results for all language
groups are summarized in Table 7. It should be
noted that the TrainClean system also makes use
of the article chosen by the author through a confi-
dence threshold8; it prefers to keep the article chosen
by the user. The difference is that the TrainClean
system does not consider the author?s article in train-
ing. The results of training with error generation
are better, which shows that training on automati-
cally corrupted data indeed helps. While the per-
formance is different by language group, there is an
observable reduction in error rate for each language
group when TWE systems are used compared to
TrainClean approach. The reduction in error rate
8The decision threshold is found empirically on a subset of
the ESL data set aside for development.
160
achieved by the best performing TWE system when
compared to the error rate of the TrainClean sys-
tem is 10.06% for Chinese, 4.89% for Czech and
10.77% for Russian, as shown in Table 7. We also
note that the best performing TWE systems for Chi-
nese and Russian speakers are those that rely on the
distribution of articles (Chinese) and the distribution
of errors (Russian), but for Czech it is the General
TWE system that performs the best, maybe because
we had less data for Czech speakers, so their statis-
tics are less reliable.
There are several additional observations to be
made. First, training paradigms that use error gen-
eration methods work better than the training ap-
proach of using clean data. Every system of type
TWE outperforms the TrainClean system, as ev-
idenced by Tables 4, 5, and 6. Second, the propor-
tion of errors in the training data should be similar
to the error rate in the test data. The proportion of
errors in training is shown in Tables 4, 5 and 6 in col-
umn 2. Furthermore, TWE systems ArtDistr and
ErrorDistr that use specific knowledge about arti-
cle and error distributions, respectively, work better
for Russian and Chinese groups than the General
method that adds errors to the data uniformly at ran-
dom. Since ArtDistr and ErrorDistr depend on
the statistics of learner mistakes, the success of the
systems that use these methods for error generation
depends on the accuracy of these statistics, and we
only have between 100 and 250 errors for each lan-
guage group. It would be interesting to see whether
better results can be achieved with these methods if
more annotated data are available. Finally, for the
same reason, there is no significant difference in the
performance of methods ArtDistrBeforeAnnot
and ArtDistrAfterAnnot: With small sizes of an-
notated data there is no difference in article distribu-
tions before and after annotation.
8 Conclusion and Future Work
We have shown that error correction training
paradigms that introduce artificial errors are supe-
rior to training classifiers on clean data. We pro-
posed several methods of error generation that ac-
count for error frequency information and error dis-
tribution statistics from non-native text and demon-
strated that the methods that work best are those that
Source Accuracy Error
language Train TWE reduction
Clean
Chinese 91.85% 92.67% 10.06%
Czech 91.82% 92.22% 4.89%
Russian 90.62% 91.63% 10.77%
Table 7: Improvement due to training with errors. For
each source language, the last column of the table shows
the reduction in error rate achieved by the best perform-
ing TWE system when compared to the error rate of the
TrainClean system. The error rate for each system is
computed by subtracting the accuracy achieved by the
system, as shown in columns 2 and 3.
result in a training corpus that statistically resembles
the non-native text. Adding information about arti-
cle distribution in non-native data and statistics on
specific error types is even more helpful.
We have also argued that the baselines used ear-
lier in the relevant literature ? all based on the major-
ity of the most commonly used class ? suit selection
tasks, but are inappropriate for error correction. In-
stead, the error rate in the data should be taken into
account when determining the baseline.
The focus of the present study was on training
paradigms. While it is quite possible that the article
correction system presented here can be improved
? we would like to explore improving the system
by using a more sophisticated feature set ? we be-
lieve that the performance gap due to the error driven
training paradigms shown here will remain. The rea-
son is that even with better features, some of the fea-
tures that hold in the native data will not be active in
in the ESL writing.
Finally, while this study focused on the problem
of correcting article mistakes, we plan to apply the
proposed training paradigms to similar text correc-
tion problems.
Acknowledgments
We thank Nick Rizzolo for helpful discussions on
LBJ. We also thank Peter Chew and the anonymous
reviewers for their insightful comments. This re-
search is partly supported by a grant from the U.S.
Department of Education.
161
References
C. Brockett, W. B. Dolan, and M. Gamon. 2006. Cor-
recting ESL errors using phrasal SMT techniques. In
Proceedings of the 21st COLING and the 44th ACL,
Sydney.
A. Carlson, C. Cumby, J. Rosen, and D. Roth. The
SNoW learning architecture. Technical report.
A. J. Carlson and J. Rosen and D. Roth. 2001. Scaling
Up Context Sensitive Text Correction. IAAI, 45?50.
R. De Felice and S. Pulman. 2008. A Classifier-Based
Approach to Preposition and Determiner Error Correc-
tion in L2 English. In Proceedings of COLING-08.
J. Foster and ?. Andersen. 2009. GenERRate: Gener-
ating Errors for Use in Grammatical Error Detection.
In Proceedings of the NAACL Workshop on Innovative
Use of NLP for Building Educational Applications.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277-296.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W.
Dolan, D. Belenko and L. Vanderwende. 2008. Using
Contextual Speller Techniques and Language Model-
ing for ESL Error Correction. Proceedings of IJCNLP.
A. R. Golding and D. Roth. 1996. Applying Winnow
to Context-Sensitive Spelling Correction. ICML, 182?
190.
A. R. Golding and D. Roth. 1999. A Winnow based ap-
proach to Context-Sensitive Spelling Correction. Ma-
chine Learning, 34(1-3):107?130.
S. Granger, E. Dagneaux and F. Meunier 2002. Interna-
tional Corpus of Learner English.
S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu
Yuliaohu. (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe. (In Chinese).
N. Han, M. Chodorow and C. Leacock. 2006. De-
tecting Errors in English Article Usage by Non-native
Speakers. Journal of Natural Language Engineering,
12(2):115?129.
E. Izumi, K. Uchimoto, T. Saiga and H. Isahara. 2003.
Automatic Error Detection in the Japanese Leaners
English Spoken Data. ACL.
E. Izumi, K. Uchimoto and H. Isahara. 2004. The
NICT JLE Corpus: Exploiting the Language Learner?s
Speech Database for Research and Education. Inter-
national Journal of the Computer, the Internet and
Management, 12(2):119?125.
K. Knight and I. Chander. 1994. Automatic Postediting
of Documents. In Proceedings of the American Asso-
ciation of Artificial Intelligence, pp 779?784.
J. Lee and S. Seneff. 2008. An analysis of grammatical
errors in non-native speech in English. In Proceedings
of the 2008 Spoken Language Technology Workshop,
Goa.
G. Minnen, F. Bond and A. Copestake 2000. Memory-
Based Learning for Article Generation. In Proceed-
ings of the Fourth Conference on Computational Nat-
ural Language Learning and of the Second Learning
Language in Logic Workshop, pp 43?48.
V. Punyakanok, D. Roth, and W. Yih. The importance of
syntactic parsing and inference in semantic role label-
ing. Computational Linguistics, 34(2).
N. Rizzolo and D. Roth 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Interna-
tional Conference on Semantic Computing (ICSC), pp
597?604.
A. Rozovskaya and D. Roth 2010. Annotating ESL Er-
rors: Challenges and Rewards. In Proceedings of the
NAACL Workshop on Innovative Use of NLP for Build-
ing Educational Applications.
J. Sjo?bergh and O. Knutsson. 2005. Faking errors to
avoid making errors. In Proceedings of RANLP 2005,
Borovets.
J. Tetreault and M. Chodorow. 2008. Native Judgments
of Non-Native Usage: Experiments in Preposition Er-
ror Detection. COLING Workshop on Human Judg-
ments in Computational Linguistics, Manchester, UK.
J. Turner and E. Charniak. 2007. Language Modeling
for Determiner Selection. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Companion Volume, Short Papers, pp 177?180.
J. Wagner, J. Foster, and J. van Genabith. 2009. Judg-
ing grammaticality: Experiments in sentence classifi-
cation. CALICO Journal. Special Issue on the 2008
Automatic Analysis of Learner Language CALICO
Workshop.
Y. Xing, J. Gao, and W. Dolan. 2009. A web-based En-
glish proofing system for ESL users. In Proceedings
of IJCNLP.
162
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429?437,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Discriminative Learning over Constrained Latent Representations
Ming-Wei Chang and Dan Goldwasser and Dan Roth and Vivek Srikumar
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang,goldwas1,danr,vsrikum2}@uiuc.edu
Abstract
This paper proposes a general learning frame-
work for a class of problems that require learn-
ing over latent intermediate representations.
Many natural language processing (NLP) de-
cision problems are defined over an expressive
intermediate representation that is not explicit
in the input, leaving the algorithm with both
the task of recovering a good intermediate rep-
resentation and learning to classify correctly.
Most current systems separate the learning
problem into two stages by solving the first
step of recovering the intermediate representa-
tion heuristically and using it to learn the final
classifier. This paper develops a novel joint
learning algorithm for both tasks, that uses the
final prediction to guide the selection of the
best intermediate representation. We evalu-
ate our algorithm on three different NLP tasks
? transliteration, paraphrase identification and
textual entailment ? and show that our joint
method significantly improves performance.
1 Introduction
Many NLP tasks can be phrased as decision prob-
lems over complex linguistic structures. Successful
learning depends on correctly encoding these (of-
ten latent) structures as features for the learning sys-
tem. Tasks such as transliteration discovery (Kle-
mentiev and Roth, 2008), recognizing textual en-
tailment (RTE) (Dagan et al, 2006) and paraphrase
identification (Dolan et al, 2004) are a few proto-
typical examples. However, the input to such prob-
lems does not specify the latent structures and the
problem is defined in terms of surface forms only.
Most current solutions transform the raw input into
a meaningful intermediate representation1, and then
encode its structural properties as features for the
learning algorithm.
Consider the RTE task of identifying whether the
meaning of a short text snippet (called the hypoth-
esis) can be inferred from that of another snippet
(called the text). A common solution (MacCartney
et al, 2008; Roth et al, 2009) is to begin by defining
an alignment over the corresponding entities, pred-
icates and their arguments as an intermediate rep-
resentation. A classifier is then trained using fea-
tures extracted from the intermediate representation.
The idea of using a intermediate representation also
occurs frequently in other NLP tasks (Bergsma and
Kondrak, 2007; Qiu et al, 2006).
While the importance of finding a good inter-
mediate representation is clear, emphasis is typi-
cally placed on the later stage of extracting features
over this intermediate representation, thus separat-
ing learning into two stages ? specifying the la-
tent representation, and then extracting features for
learning. The latent representation is obtained by an
inference process using predefined models or well-
designed heuristics. While these approaches often
perform well, they ignore a useful resource when
generating the latent structure ? the labeled data for
the final learning task. As we will show in this pa-
per, this results in degraded performance for the ac-
tual classification task at hand. Several works have
considered this issue (McCallum et al, 2005; Gold-
wasser and Roth, 2008b; Chang et al, 2009; Das
and Smith, 2009); however, they provide solutions
1In this paper, the phrases ?intermediate representation? and
?latent representation? are used interchangeably.
429
that do not easily generalize to new tasks.
In this paper, we propose a unified solution to the
problem of learning to make the classification deci-
sion jointly with determining the intermediate rep-
resentation. Our Learning Constrained Latent Rep-
resentations (LCLR) framework is guided by the in-
tuition that there is no intrinsically good intermedi-
ate representation, but rather that a representation is
good only to the extent to which it improves perfor-
mance on the final classification task. In the rest of
this section we discuss the properties of our frame-
work and highlight its contributions.
Learning over Latent Representations This pa-
per formulates the problem of learning over latent
representations and presents a novel and general so-
lution applicable to a wide range of NLP applica-
tions. We analyze the properties of our learning
solution, thus allowing new research to take advan-
tage of a well understood learning and optimization
framework rather than an ad-hoc solution. We show
the generality of our framework by successfully ap-
plying it to three domains: transliteration, RTE and
paraphrase identification.
Joint Learning Algorithm In contrast to most
existing approaches that employ domain specific
heuristics to construct intermediate representations
to learn the final classifier, our algorithm learns to
construct the optimal intermediate representation to
support the learning problem. Learning to represent
is a difficult structured learning problem however,
unlike other works that use labeled data at the in-
termediate level, our algorithm only uses the binary
supervision supplied for the final learning problem.
Flexible Inference Successful learning depends
on constraining the intermediate representation with
task-specific knowledge. Our framework uses the
declarative Integer Linear Programming (ILP) infer-
ence formulation, which makes it easy to define the
intermediate representation and to inject knowledge
in the form of constraints. While ILP has been ap-
plied to structured output learning, to the best of our
knowledge, this is the first work that makes use of
ILP in formalizing the general problem of learning
intermediate representations.
2 Preliminaries
We introduce notation using the Paraphrase Iden-
tification task as a running example. This is the bi-
nary classification task of identifying whether one
sentence is a paraphrase of another. A paraphrase
pair from the MSR Paraphrase corpus (Quirk et al,
2004) is shown in Figure 1. In order to identify
that the sentences paraphrase each other , we need
to align constituents of these sentences. One possi-
ble alignment is shown in the figure, in which the
dotted edges correspond to the aligned constituents.
An alignment can be specified using binary variables
corresponding to every edge between constituents,
indicating whether the edge is included in the align-
ment. Different activations of these variables induce
the space of intermediate representations.
The notification was first reported Friday by MSNBC.
MSNBC.com first reported the CIA request on Friday.
Figure 1: The dotted lines represent a possible intermediate
representation for the paraphrase identification task. Since dif-
ferent representation choices will impact the binary identifica-
tion decision directly, our approach chooses the representation
that facilitates the binary learning task.
To formalize this setting, let x denote the input
to a decision function, which maps x to {?1, 1}.
We consider problems where this decision depends
on an intermediate representation (for example, the
collection of all dotted edges in Figure 1), which can
be represented by a binary vector h.
In the literature, a common approach is to sepa-
rate the problem into two stages. First, a genera-
tion stage predicts h for each x using a pre-defined
model or a heuristic. This is followed by a learn-
ing stage, in which the classifier is trained using h.
In our example, if the generation stage predicts the
alignment shown, then the learning stage would use
the features computed based on the alignments. For-
mally, the two-stage approach uses a pre-defined in-
ference procedure that finds an intermediate repre-
sentation h?. Using features ?(x,h?) and a learned
weight vector ?, the example is classified as positive
if ?T?(x,h?) ? 0.
However, in the two stage approach, the latent
representation, which is provided to the learning al-
gorithm, is determined before learning starts, and
without any feedback from the final task. It is dic-
tated by the intuition of the developer. This approach
makes two implicit assumptions: first, it assumes
430
the existence of a ?correct? latent representation and,
second, that the model or heuristic used to generate
it is the correct one for the learning problem at hand.
3 Joint Learning with an Intermediate
Representation
In contrast to two-stage approaches, we use the
annotated data for the final classification task to
learn a suitable intermediate representation which,
in turn, helps the final classification.
Choosing a good representation is an optimization
problem that selects which of the elements (features)
of the representation best contribute to success-
ful classification given some legitimacy constraints;
therefore, we (1) set up the optimization framework
that finds legitimate representations (Section 3.1),
and (2) learn an objective function for this optimiza-
tion problem, such that it makes the best final deci-
sion (Section 3.2.)
3.1 Inference
Our goal is to correctly predict the final label
rather than matching a ?gold? intermediate repre-
sentation. In our framework, attempting to learn the
final decision drives both the selection of the inter-
mediate representation and the final predictions.
For each x, let ?(x) be the set of all substructures
of all possible intermediate representations. In Fig-
ure 1, this could be the set of all alignment edges
connecting the constituents of the sentences. Given
a vocabulary of such structures of sizeN , we denote
intermediate representations by h ? {0, 1}N , which
?select? the components of the vocabulary that con-
stitute the intermediate representation. We define
?s(x) to be a feature vector over the substructure
s, which is used to describe the characteristics of s,
and define a weight vector u over these features.
Let C denote the set of feasible intermediate repre-
sentations h, specified by means of linear constraints
over h. While ?(x) might be large, the set of those
elements in h that are active can be constrained by
controlling C. After we have learned a weight vec-
tor u that scores intermediate representations for the
final classification task, we define our decision func-
tion as
fu(x) = max
h?C
uT
?
s??(x)
hs?s(x), (1)
and classify the input as positive if fu(x) ? 0.
In Eq. (1), uT?s(x) is the score associated with
the substructure s, and fu(x) is the score for the en-
tire intermediate representation. Therefore, our de-
cision function fu(x) ? 0 makes use of the interme-
diate representation and its score to classify the in-
put. An input is labeled as positive if its underlying
intermediate structure allows it to cross the decision
threshold. The intermediate representation is cho-
sen to maximize the overall score of the input. This
design is especially beneficial for many phenomena
in NLP, where only positive examples have a mean-
ingful underlying structure. In our paraphrase iden-
tification example, good alignments generally exist
only for positive examples.
One unique feature of our framework is that we
treat Eq. (1) as an Integer Linear Programming
(ILP) instance. A concrete instantiation of this set-
ting to the paraphrase identification problem, along
with the actual ILP formulation is shown in Section
4.
3.2 Learning
We now present an algorithm that learns the
weight vector u. For a loss function ` : R ? R,
the goal of learning is to solve the following opti-
mization problem:
min
u
?
2
?u?2 +
?
i
` (?yifu(xi)) (2)
Here, ? is the regularization parameter. Substituting
Eq. (1) into Eq. (2), we get
min
u
?
2
?u?2+
?
i
`
?
??yi max
h?C
uT
?
s??(x)
hs?s(xi)
?
? (3)
Note that there is a maximization term inside the
global minimization problem, making Eq. (3) a non-
convex problem. The minimization drives u towards
smaller empirical loss while the maximization uses
u to find the best representation for each example.
The algorithm for Learning over Constrained La-
tent Representations (LCLR) is listed in Algorithm
1. In each iteration, first, we find the best feature
representations for all positive examples (lines 3-5).
This step can be solved with an off-the-shelf ILP
solver. Having fixed the representations for the pos-
itive examples, we update the u by solving Eq. (4)
at line 6 in the algorithm. It is important to observe
431
Algorithm 1 LCLR :The algorithm that optimizes (3)
1: initialize: u? u0
2: repeat
3: for all positive examples (xi, yi = 1) do
4: Find h?i ? arg maxh?C
?
s
hsuT?s(xi)
5: end for
6: Update u by solving
min
u
?
2
?u?2 +
?
i:yi=1
`(?uT
?
s
h?i,s?s(xi))
+
?
i:yi=?1
`(max
h?C
uT
?
s
hs?s(xi)) (4)
7: until convergence
8: return u
that for positive examples in Eq. (4), we use the in-
termediate representations h? from line 4.
Algorithm 1 satisfies the following property:
Theorem 1 If the loss function ` is a non-
decreasing function, then the objective function
value of Eq. (3) is guaranteed to decrease in every
iteration of Algorithm 1. Moreover, if the loss func-
tion is also convex, then Eq. (4) in Algorithm 1 is
convex.
Due to the space limitation, we omit the proof.
Theoretically, we can use any loss function that
satisfies the conditions of the theorem. In the exper-
iments in this paper, we use the squared-hinge loss
function: `(?yfu(x)) = max(0, 1? yfu(x))2.
Recall that Eq. (4) is not the traditional SVM or
logistic regression formulation. This is because in-
side the inner loop, the best representation for each
negative example must be found. Therefore, we
need to perform inference for every negative exam-
ple when updating the weight vector solution. In-
stead of solving a difficult non-convex optimization
problem (Eq. (3)), LCLR iteratively solves a series
of easier problems (Eq. (4)). This is especially true
for our loss function because Eq. (4) is convex and
can be solved efficiently.
We use a cutting plane algorithm to solve Eq. (4).
A similar idea has been proposed in (Joachims et al,
2009). The algorithm for solving Eq. (4) is presented
as Algorithm 2. This algorithm uses a ?cache? Hj
to store all intermediate representations for negative
examples that have been seen in previous iterations
Algorithm 2 Cutting plane algorithm to optimize Eq. (4)
1: for each negative example xj , Hj ? ?
2: repeat
3: for each negative example xj do
4: Find h?j ? arg maxh?C
?
s hsu
T?s(xj)
5: Hj ? Hj ? {h?j}
6: end for
7: Solve
min
u
?
2
?u?2 +
?
i:yi=1
`(?uT
?
s
h?i,s?s(xi))
+
?
i:yi=?1
`(max
h?Hj
uT
?
s
hs?s(xi)) (5)
8: until no new element is added to any Hj
9: return u
(lines 3-6) 2. The difference between Eq. (5) in line
7 of Algorithm 2 and Eq. (4) is that in Eq. (5), we do
not search over the entire space of intermediate rep-
resentations. The search space for the minimization
problem Eq. (5) is restricted to the cache Hj . There-
fore, instead of solving the minimization problem
Eq. (4), we can now solve several simpler problems
shown in Eq. (5). The algorithm is guaranteed to
stop (line 8) because the space of intermediate rep-
resentations is finite. Furthermore, in practice, the
algorithm needs to consider only a small subset of
?hard? examples before it converges.
Inspired by (Hsieh et al, 2008), we apply an effi-
cient coordinate descent algorithm for the dual for-
mulation of (5) which is guaranteed to find its global
minimum. Due to space considerations, we do not
present the derivation of dual formulation and the
details of the optimization algorithm.
4 Encoding with ILP: A Paraphrase
Identification Example
In this section, we define the latent representation
for the paraphrase identification task. Unlike the ear-
lier example, where we considered the alignment of
lexical items, we describe a more complex interme-
diate representation by aligning graphs created using
semantic resources.
An input example is represented as two acyclic
2In our implementation, we keep a global cache Hj for each
negative example xj . Therefore, in Algorithm 2, we start with
a non-empty cache improving the speed significantly.
432
graphs, G1 and G2, corresponding to the first
and second input sentences. Each vertex in the
graph contains word information (lemma and part-
of-speech) and the edges denote dependency rela-
tions, generated by the Stanford parser (Klein and
Manning, 2003). The intermediate representation
for this task can now be defined as an alignment be-
tween the graphs, which captures lexical and syntac-
tic correlations between the sentences.
We use V (G) and E(G) to denote the set of ver-
tices and edges in G respectively, and define four
hidden variable types to encode vertex and edge
mappings between G1 and G2.
? The word-mapping variables, denoted by
hv1,v2 , define possible pairings of vertices,
where v1 ? V (G1) and v2 ? V (G2).
? The edge-mapping variables, denoted by
he1,e2 , define possible pairings of the graphs
edges, where e1 ? E(G1) and e2 ? E(G2).
? The word-deletion variables hv1,? (or h?,v2) al-
low for vertices v1 ? V (G1) (or v2 ? V (G2))
to be deleted. This accounts for omission of
words (like function words).
? The edge-deletion variables, he1,? (or h?,e2) al-
low for deletion of edges from G1 (or G2).
Our inference problem is to find the optimal set of
hidden variable activations, restricted according to
the following set of linear constraints
? Each vertex inG1 (orG2) can either be mapped
to a single vertex in G2 (or G1) or marked as
deleted. In terms of the word-mapping and
word-deletion variables, we have
?v1 ? V (G1);hv1,? +
?
v2?V (G2)
hv1,v2 = 1 (6)
?v2 ? V (G2);h?,v2 +
?
v1?V (G1)
hv1,v2 = 1 (7)
? Each edge in G1 (or G2) can either be mapped
to a single edge in G2 (or G1) or marked as
deleted. In terms of the edge-mapping and
edge-deletion variables, we have
?e1 ? E(G1);he1,? +
?
e2?E(G2)
he1,e2 = 1 (8)
?e2 ? E(G2);h?,e2 +
?
e1?E(G1)
he1,e2 = 1 (9)
? The edge mappings can be active if, and only
if, the corresponding node mappings are ac-
tive. Suppose e1 = (v1, v?1) ? E(G1) and
e2 = (v2, v?2) ? E(G2), where v1, v
?
1 ? V (G1)
and v2, v?2 ? V (G2). Then, we have
hv1,v2 + hv?1,v?2 ? he1,e2 ? 1 (10)
hv1,v2 ? he1,e2 ;hv?1,v?2 ? he1,e2 (11)
These constraints define the feasible set for the in-
ference problem specified in Equation (1). This in-
ference problem can be formulated as an ILP prob-
lem with the objective function from Equation (1):
max
h
?
s
hsuT?s(x)
subject to (6)-(11); ?s;hs ? {0, 1} (12)
This example demonstrates the use of integer linear
programming to define intermediate representations
incorporating domain intuition.
5 Experiments
We applied our framework to three different NLP
tasks: transliteration discovery (Klementiev and
Roth, 2008), RTE (Dagan et al, 2006), and para-
phrase identification (Dolan et al, 2004).
Our experiments are designed to answer the fol-
lowing research question: ?Given a binary classifi-
cation problem defined over latent representations,
will the joint LCLR algorithm perform better than a
two-stage approach?? To ensure a fair comparison,
both systems use the same feature functions and def-
inition of intermediate representation. We use the
same ILP formulation in both configurations, with a
single exception ? the objective function parameters:
the two stage approach uses a task-specific heuristic,
while LCLR learns it iteratively.
The ILP formulation results in very strong two
stage systems. For example, in the paraphrase iden-
tification task, even our two stage system is the cur-
rent state-of-the-art performance. In these settings,
the improvement obtained by our joint approach is
non-trivial and can be clearly attributed to the su-
periority of the joint learning algorithm. Interest-
ingly, we find that our more general approach is bet-
ter than specially designed joint approaches (Gold-
wasser and Roth, 2008b; Das and Smith, 2009).
Since the objective function (3) of the joint ap-
proach is not convex, a good initialization is re-
quired. We use the weight vector learned by the two
433
stage approach as the starting point for the joint ap-
proach. The algorithm terminates when the relative
improvement of the objective is smaller than 10?5.
5.1 Transliteration Discovery
Transliteration discovery is the problem of iden-
tifying if a word pair, possibly written using two
different character sets, refers to the same underly-
ing entity. The intermediate representation consists
of all possible character mappings between the two
character sets. Identifying this mapping is not easy,
as most writing systems do not perfectly align pho-
netically and orthographically; rather, this mapping
can be context-dependent and ambiguous.
For an input pair of words (w1, w2), the interme-
diate structure h is a mapping between their charac-
ters, with the latent variable hij indicating if the ith
character in w1 is aligned to the jth character in w2.
The feature vector associated with the variable hij
contains unigram character mapping, bigram char-
acter mapping (by considering surrounding charac-
ters). We adopt the one-to-one mapping and non-
crossing constraint used in (Chang et al, 2009).
We evaluated our system using the English-
Hebrew corpus (Goldwasser and Roth, 2008a),
which consists of 250 positive transliteration pairs
for training, and 300 pairs for testing. As negative
examples for training, we sample 10% from random
pairings of words from the positive data. We report
two evaluation measurements ? (1) the Mean Recip-
rocal Rank (MRR), which is the average of the mul-
tiplicative inverse of the rank of the correct answer,
and (2) the accuracy (Acc), which is the percentage
of the top rank candidates being correct.
We initialized the two stage inference process as
detailed in (Chang et al, 2009) using a Romaniza-
tion table to assign uniform weights to prominent
character mappings. This initialization procedure
resembles the approach used in (Bergsma and Kon-
drak, 2007). An alignment is first built by solving
the constrained optimization problem. Then, a sup-
port vector machine with squared-hinge loss func-
tion is used to train a classifier using features ex-
tracted from the alignment. We refer to this two
stage approach as Alignment+Learning.
The results summarized in Table 1 show the sig-
nificant improvement obtained by the joint approach
(95.4% MRR) compared to the two stage approach
Transliteration System Acc MRR
(Goldwasser and Roth,
2008b)
N/A 89.4
Alignment + Learning 80.0 85.7
LCLR 92.3 95.4
Table 1: Experimental results for transliteration. We compare
a two-stage system: ?Alignment+Learning? with LCLR, our
joint algorithm. Both ?Alignment+Learning? and LCLR use
the same features and the same intermediate representation def-
inition.
(85.7%). Moreover, LCLR outperforms the joint
system introduced in (Goldwasser and Roth, 2008b).
5.2 Textual Entailment
Recognizing Textual Entailment (RTE) is an im-
portant textual inference task of predicting if a given
text snippet, entails the meaning of another (the hy-
pothesis). In many current RTE systems, the entail-
ment decision depends on successfully aligning the
constituents of the text and hypothesis, accounting
for the internal linguistic structure of the input.
The raw input ? the text and hypothesis ? are
represented as directed acyclic graphs, where ver-
tices correspond to words. Directed edges link verbs
to the head words of semantic role labeling argu-
ments produced by (Punyakanok et al, 2008). All
other words are connected by dependency edges.
The intermediate representation is an alignment be-
tween the nodes and edges of the graphs. We used
three hidden variable types from Section 4 ? word-
mapping, word-deletion and edge-mapping, along
with the associated constraints as defined earlier.
Since the text is typically much longer than the hy-
pothesis, we create word-deletion latent variables
(and features) only for the hypothesis.
The second column of Table 2 lists the resources
used to generate features corresponding to each hid-
den variable type. For word-mapping variables, the
features include a WordNet based metric (WNSim),
indicators for the POS tags and negation identifiers.
We used the state-of-the-art coreference resolution
system of (Bengtson and Roth, 2008) to identify the
canonical entities for pronouns and extract features
accordingly. For word deletion, we use only the POS
tags of the corresponding tokens (generated by the
LBJ POS tagger3) to generate features. For edge
3
http://L2R.cs.uiuc.edu/?cogcomp/software.php
434
Hidden RTE Paraphrase
Variable features features
word-mapping WordNet, POS,
Coref, Neg
WordNet, POS,
NE, ED
word-deletion POS POS, NE
edge-mapping NODE-INFO NODE-INFO,
DEP
edge-deletion N/A DEP
Table 2: Summary of latent variables and feature resources for
the entailment and paraphrase identification tasks. See Section
4 for an explanation of the hidden variable types. The linguistic
resources used to generate features are abbreviated as follows ?
POS: Part of speech, Coref: Canonical coreferent entities; NE:
Named Entity, ED: Edit distance, Neg: Negation markers, DEP:
Dependency labels, NODE-INFO: corresponding node align-
ment resources, N/A: Hidden variable not used.
Entailment System Acc
Median of TAC 2009 systems 61.5
Alignment + Learning 65.0
LCLR 66.8
Table 3: Experimental results for recognizing textual entail-
ment. The first row is the median of best performing systems of
all teams that participated in the RTE5 challenge (Bentivogli et
al., 2009). Alignment + Learning is our two-stage system im-
plementation, and LCLR is our joint learning algorithm. Details
about these systems are provided in the text.
mapping variables, we include the features of the
corresponding word mapping variables, scaled by
the word similarity of the words forming the edge.
We evaluated our system using the RTE-5
data (Bentivogli et al, 2009), consisting of 600 sen-
tence pairs for training and testing respectively, in
which positive and negative examples are equally
distributed. In these experiments the joint LCLR al-
gorithm converged after 5 iterations.
For the two stage system, we used WN-
Sim to score alignments during inference. The
word-based scores influence the edge variables
via the constraints. This two-stage system (the
Alignment+Learning system) is significantly better
than the median performance of the RTE-5 submis-
sions. Using LCLR further improves the result by al-
most 2%, a substantial improvement in this domain.
5.3 Paraphrase Identification
Our final task is Paraphrase Identification, dis-
cussed in detail at Section 4. We use all the four
hidden variable types described in that section. The
features used are similar to those described earlier
Paraphrase System Acc
Experiments using (Dolan et al, 2004)
(Qiu et al, 2006) 72.00
(Das and Smith, 2009) 73.86
(Wan et al, 2006) 75.60
Alignment + Learning 76.23
LCLR 76.41
Experiments using Extended data set
Alignment + Learning 72.00
LCLR 72.75
Table 4: Experimental Result For Paraphrasing Identification.
Our joint LCLR approach achieves the best results compared
to several previously published systems, and our own two stage
system implementation (Alignment + Learning). We evaluated
the systems performance across two datasets: (Dolan et al,
2004) dataset and the Extended dataset, see the text for details.
Note that LCLR outperforms (Das and Smith, 2009), which is a
specifically designed joint approach for this task.
for the RTE system and are summarized in Table 2.
We used the MSR paraphrase dataset of (Dolan
et al, 2004) for empirical evaluation. Additionally,
we generated a second corpus (called the Extended
dataset) by sampling 500 sentence pairs from the
MSR dataset for training and using the entire test
collection of the original dataset. In the Extended
dataset, for every sentence pair, we extended the
longer sentence by concatenating it with itself. This
results in a more difficult inference problem because
it allows more mappings between words. Note that
the performance on the original dataset sets the ceil-
ing on the second one.
The results are summarized in Table 4. The first
part of the table compares the LCLR system with
a two stage system (Alignment + Learning) and
three published results that use the MSR dataset.
(We only list single systems in the table4) Inter-
estingly, although still outperformed by our joint
LCLR algorithm, the two stage system is able per-
form significantly better than existing systems for
that dataset (Qiu et al, 2006; Das and Smith, 2009;
Wan et al, 2006). We attribute this improvement,
consistent across both the ILP based systems, to the
intermediate representation we defined.
We hypothesize that the similarity in performance
between the joint LCLR algorithm and the two stage
4Previous work (Das and Smith, 2009) has shown that com-
bining the results of several systems improves performance.
435
(Alignment + Learning) systems is due to the limited
intermediate representation space for input pairs in
this dataset. We evaluated these systems on the more
difficult Extended dataset. Results indeed show that
the margin between the two systems increases as the
inference problem becomes harder.
6 Related Work
Recent NLP research has largely focused on two-
stage approaches. Examples include RTE (Zanzotto
and Moschitti, 2006; MacCartney et al, 2008; Roth
et al, 2009); string matching (Bergsma and Kon-
drak, 2007); transliteration (Klementiev and Roth,
2008); and paraphrase identification (Qiu et al,
2006; Wan et al, 2006).
(MacCartney et al, 2008) considered construct-
ing a latent representation to be an independent task
and used manually labeled alignment data (Brockett,
2007) to tune the inference procedure parameters.
While this method identifies alignments well, it does
not improve entailment decisions. This strengthens
our intuition that the latent representation should be
guided by the final task.
There are several exceptions to the two-stage ap-
proach in the NLP community (Haghighi et al,
2005; McCallum et al, 2005; Goldwasser and Roth,
2008b; Das and Smith, 2009); however, the interme-
diate representation and the inference for construct-
ing it are closely coupled with the application task.
In contrast, LCLR provides a general formulation
that allows the use of expressive constraints, mak-
ing it applicable to many NLP tasks.
Unlike other latent variable SVM frameworks
(Felzenszwalb et al, 2009; Yu and Joachims, 2009)
which often use task-specific inference procedure,
LCLR utilizes the declarative inference framework
that allows using constraints over intermediate rep-
resentation and provides a general platform for a
wide range of NLP tasks.
The optimization procedure in this work and
(Felzenszwalb et al, 2009) are quite different.
We use the coordinate descent and cutting-plane
methods ensuring we have fewer parameters and
the inference procedure can be easily parallelized.
Our procedure also allows different loss functions.
(Cherry and Quirk, 2008) adopts the Latent SVM al-
gorithm to define a language model. Unfortunately,
their implementation is not guaranteed to converge.
In CRF-like models with latent variables (McCal-
lum et al, 2005), the decision function marginal-
izes over the all hidden states when presented with
an input example. Unfortunately, the computational
cost of applying their framework is prohibitive with
constrained latent representations. In contrast, our
framework requires only the best hidden representa-
tion instead of marginalizing over all possible repre-
sentations, thus reducing the computational effort.
7 Conclusion
We consider the problem of learning over an inter-
mediate representation. We assume the existence of
a latent structure in the input, relevant to the learn-
ing problem, but not accessible to the learning algo-
rithm. Many NLP tasks fall into these settings and
each can consider a different hidden input structure.
We propose a unifying thread for the different prob-
lems and present a novel framework for Learning
over Constrained Latent Representations (LCLR).
Our framework can be applied to many different la-
tent representations such as parse trees, orthographic
mapping and tree alignments. Our approach con-
trasts with existing work in which learning is done
over a fixed representation, as we advocate jointly
learning it with the final task.
We successfully apply the proposed framework to
three learning tasks ? Transliteration, Textual En-
tailment and Paraphrase Identification. Our joint
LCLR algorithm achieves superior performance in
all three tasks. We attribute the performance im-
provement to our novel training algorithm and flex-
ible inference procedure, allowing us to encode do-
main knowledge. This presents an interesting line of
future work in which more linguistic intuitions can
be encoded into the learning problem. For these rea-
sons, we believe that our framework provides an im-
portant step forward in understanding the problem
of learning over hidden structured inputs.
Acknowledgment We thank James Clarke and Mark Sam-
mons for their insightful comments. This research was partly sponsored
by the Army Research Laboratory (ARL) (accomplished under Cooper-
ative Agreement Number W911NF-09-2-0053) and by Air Force Re-
search Laboratory (AFRL) under prime contract no. FA8750-09-C-
0181. Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not necessarily
reflect the view of the ARL or of AFRL.
436
References
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and
B. Magnini. 2009. The fifth PASCAL recognizing
textual entailment challenge. In Proc. of TAC Work-
shop.
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In ACL.
C. Brockett. 2007. Aligning the RTE 2006 corpus.
In Technical Report MSR-TR-2007-77, Microsoft Re-
search.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for transliter-
ation discovery. In NAACL.
C. Cherry and C. Quirk. 2008. Discriminative, syntactic
language modeling through latent svms. In Proc. of
the Eighth Conference of AMTA.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
ACL.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In COLING.
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2009. Object detection with discrimina-
tively trained part based models. IEEE Transactions
on Pattern Analysis and Machine Intelligence.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In ACL. Short
Paper.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In EMNLP.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust
textual inference via graph matching. In HLT-EMNLP.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear svm. In ICML.
T. Joachims, T. Finley, and Chun-Nam Yu. 2009.
Cutting-plane training of structural svms. Machine
Learning.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
NIPS.
A. Klementiev and D. Roth. 2008. Named entity translit-
eration and discovery in multilingual corpora. In
Cyril Goutte, Nicola Cancedda, Marc Dymetman, and
George Foster, editors, Learning Machine Translation.
B. MacCartney, M. Galley, and C. D. Manning. 2008.
A phrase-based alignment model for natural language
inference. In EMNLP.
A. McCallum, K. Bellare, and F. Pereira. 2005. A condi-
tional random field for discriminatively-trained finite-
state string edit distance. In UAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2006. Paraphrase
recognition via dissimilarity significance classifica-
tion. In EMNLP.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation. In
EMNLP.
D. Roth, M. Sammons, and V.G. Vydiswaran. 2009. A
framework for entailed relation recognition. In ACL.
S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using
dependency-based features to take the p?ara-farceo?ut
of paraphrase. In Proc. of the Australasian Language
Technology Workshop (ALTW).
C. Yu and T. Joachims. 2009. Learning structural svms
with latent variables. In ICML.
F. M. Zanzotto and A. Moschitti. 2006. Automatic learn-
ing of textual entailments with cross-pair similarities.
In ACL.
437
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 9?14,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Integer Linear Programming in NLP - Constrained Conditional Models
Ming-Wei Chang, Nicholas Rizzolo, Dan Roth
University of Illinois at Urbana-Champaign
 
Making decisions in natural language processing problems often involves assigning 
values to sets of interdependent variables where the expressive dependency structure 
can influence, or even dictate, what assignments are possible. Structured learning 
problems such as semantic role labeling provide one such example, but the setting is 
broader and includes a range of problems such as name entity and relation recognition 
and co-reference resolution. The setting is also appropriate for cases that may require a 
solution to make use of multiple (possible pre-designed or pre-learned components) as 
in summarization, textual entailment and question answering. In all these cases, it is 
natural to formulate the decision problem as a constrained optimization problem, with an 
objective function that is composed of learned models, subject to domain or problem 
specific constraints. 
Constrained Conditional Models (aka Integer Linear Programming formulation of NLP 
problems) is a learning and inference framework that augments the learning of 
conditional (probabilistic or discriminative) models with declarative constraints (written, 
for example, using a first-order representation) as a way to support decisions in an 
expressive output space while maintaining modularity and tractability of training and 
inference. In most applications of this framework in NLP, following [Roth & Yih, 
CoNLL?04], Integer Linear Programming (ILP) was used as the inference framework, 
although other algorithms can be used for that purpose.  
This framework, with and without Integer Linear Programming as its inference engine, 
has recently attracted much attention within the NLP community, with multiple papers in 
all the recent major conferences, and a related workshop in NAACL?09. Formulating 
problems as constrained optimization problems over the output of learned models has 
several advantages. It allows one to focus on the modeling of problems by providing the 
opportunity to incorporate problem specific global constraints using a first order 
language ? thus frees the developer from (much of the) low level feature engineering ? 
and it guarantees exact inference. It provides also the freedom of decoupling the stage 
of model generation (learning) from that of the constrained inference stage, often 
resulting in simplifying the learning stage as well as the engineering problem of building 
an NLP system, while improving the quality of the solutions.
These advantages and the availability of off-the-shelf solvers have led to a large variety 
of natural language processing tasks being formulated within framework, including 
semantic role labeling, syntactic parsing, coreference resolution, summarization, 
transliteration and joint information extraction. 
The goal of this tutorial is to introduce the framework of Constrained Conditional Models 
(CCMs) to the broader ACL community, motivate it as a generic framework for learning 
and inference in global NLP decision problems, present some of the key theoretical and 
9
practical issues involved in using CCMs and survey some of the existing applications of 
it as a way to promote further development of the framework and additional 
applications. The tutorial will thus be useful for many of the senior and junior 
researchers that have interest in global decision problems in NLP, providing a concise 
overview of recent perspectives and research results. 
Tutorial Outline 
After shortly motivating and introducing the general framework, the main part of the 
tutorial is a methodological presentation of some of the key computational issues 
studied within CCMs that we will present by looking at case studies published in the 
NLP literature. In the last part of the tutorial, we will discuss engineering issues that 
arise in using CCMs and present some tool that facilitate developing CCM models.  
1. Motivation and Task Definition [30 min] 
We will motivate the framework of Constrained Conditional Models and exemplify it 
using the example of Semantic Role Labeling.  
2. Examples of Existing Applications [30 min] 
We will present in details several applications that made use of CCMs ? including 
coreference resolution, sentence compression and information extraction and use these 
to explain several of the key advantages the framework offers. We will discuss in this 
context several ways in which constraints can be introduced to an application.
3. Training Paradigms [30 min] 
The objective function used by CCMs can be decomposed and learned in several ways, 
ranging from a complete joint training of the model along with the constraints to a 
complete decoupling between the learning and the inference stage. We will present the 
advantages and disadvantages offered by different training paradigms and provide 
theoretical and experimental understanding. In this part we will also discuss comparison 
to other approaches studied in the literature. 
4. Inference methods and Constraints [30 min] 
We will present and discuss several possibilities for modeling inference in CCMs, from 
Integer Linear Programming to search techniques. We will also discuss the use of hard 
constraints and soft constraints and present ways for modeling constraints.  
5. Introducing background knowledge via CCMs [30 min]  
We will look at ways in which Constrained Conditional Models (CCMs)can be used to 
augment probabilistic models with declarative constraints in order to support decisions 
10
in an expressive output space, and how declarative constraints can be used to aid 
supervised and semi-supervised training. 
 
6. Developing CCMs Applications [30 min] 
We present a modeling language that facilitates developing applications within the CCM 
framework and present some ?templates? for possible applications.  
Tutorial Instructors  
Ming-Wei Chang
Computer Science Department, University of Illinois at Urbana-Champaign, IL, 61801
Email: mchang21@uiuc.edu
Ming-Wei Chang is a Phd candidate in University of Illinois at Urbana-Champaign.
He has done work on Machine Learning in Natural Language Processing and 
Information Extraction and has published a number of papers in several international 
conferences including "Learning and Inference with Constraints" (AAAI?08), "Guiding 
Semi-Supervision with Constraint-Driven Learning" (ACL?07) and ?Unsupervised 
Constraint Driven Learning For Transliteration Discovery. (NAACL?09). He co-presented 
a tutorial on CCMs in EACL?09.  
 
Nicholas Rizzolo
Computer Science Department, University of Illinois at Urbana-Champaign, IL, 61801
Email: ratinov2@uiuc.edu
Nicholas Rizollo is a Phd candidate in University of Illinois at Urbana-Champaign.
He has done work on Machine Learning in Natural Language Processing and is the 
principal developer of Learning Based Java (LBJ) a modeling language for Constrained 
Conditional Models. He has published a number of papers on these topics, including 
"Learning and Inference with Constraints" (AAAI?08) and  ?Modeling Discriminative 
Global Inference? (ICSC?07) 
Dan Roth
Computer Science Department, University of Illinois at Urbana-Champaign, IL, 61801
Phone: +(217) 244-7068; Email: danr@cs.uiuc.edu
11
Dan Roth is a Professor in the Department of Computer Science at the University of 
Illinois at Urbana-Champaign and the Beckman Institute of Advanced Science and 
Technology (UIUC) and a Willett Faculty Scholar of the College of Engineering. He has 
published broadly in machine learning, natural language processing, knowledge 
representation and reasoning and received several best paper and research awards. He 
has developed several machine learning based natural language processing systems 
including an award winning semantic parser, and has presented invited talks in several 
international conferences, and several tutorials on machine learning for NLP. Dan Roth 
has written the first paper on Constrained Conditional Models along with his student 
Scott Yih, presented in CoNLL?04, and since then has worked on learning and inference 
issue within this framework as well as on applying it for several NLP problems, including 
Semantic Role Labeling, Information Extraction and Transliteration. He has presented 
several invited talks that have addresses aspect of this model.   
 
Bibliography        
 
    Dan Roth and Wen-tau Yih. A Linear Programming Formulation for Global Inference 
in Natural Language Tasks. In Proceedings of the Eighth Conference on Computational 
Natural Language Learning (CoNLL-2004), pages 1-8, 2004. 
    Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak. Semantic Role Labeling 
Via Integer Linear Programming Inference. In Proceedings of the International 
Conference on Computational Linguistics (COLING-2004), pages 1346-1352, 2004.)  
    Tomacz Marciniak and Michael Strube. Beyond the Pipeline: Discrete Optimization in 
NLP. In Proceedings of the Ninth Conference on Computational Natural Language 
Learning (CoNLL-2005), pages 136-145, 2005.  
      Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, and Wen-Lian Hsu. Exploiting Full 
Parsing Information to Label Semantic Roles Using an Ensemble of ME and SVM via 
Integer Linear Programming. In Proceedings of the Ninth Conference on Computational 
Natural Language Learning: Shared Task (CoNLL-2005) Shared Task, pages 233-236, 
2005. 
      Vasin Punyakanok, Dan Roth and Wen-tau Yih. The Necessity of Syntactic Parsing 
for Semantic Role Labeling. In Proceedings of the International Joint Conference on 
Artificial Intelligence (IJCAI-2005), pages 1117-1123, 2005. 
      Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zimak. Learning and Inference 
over Constrained Output. In Proceedings of the International Joint Conference on 
Artificial Intelligence (IJCAI-2005), pages 1124-1129, 2005. 
12
      Dan Roth and Wen-tau Yih. Integer Linear Programming Inference for Conditional 
Random Fields. In Proceedings of the International Conference on Machine Learning 
(ICML-2005), pages 737-744, 2005. 
      Regina Barzilay and Mirella Lapata. Aggregation via Set Partitioning for Natural 
Language Generation. In Proceedings of the Human Language Technology Conference 
of the North American Chapter of the Association of Computational Linguistics (HLT-
NAACL-2006), pages 359-366, 2006. 
    James Clarke and Mirella Lapata. Constraint-Based Sentence Compression: An 
Integer Programming Approach. In Proceedings of the COLING/ACL 2006 Main 
Conference Poster Sessions (ACL-2006), pages 144-151, 2006. 
    Sebastian Riedel and James Clarke. Incremental Integer Linear Programming for 
Non-projective Dependency Parsing. In Proceedings of the 2006 Conference on 
Empirical Methods in Natural Language Processing (EMNLP-2006), pages 129-137, 
2006. 
    Philip Bramsen, Pawan Deshpande, Yoong Keok Lee, and Regina Barzilay. Inducing 
Temporal Graphs. In Proceedings of the 2006 Conference on Empirical Methods in 
Natural Language Processing (EMNLP-2006), 189-198, 2006. 
    Yejin Choi, Eric Breck, and Claire Cardie. Joint Extraction of Entities and Relations for 
Opinion Recognition. In Proceedings of the 2006 Conference on Empirical Methods in 
Natural Language Processing (EMNLP-2006), 431-439, 2006. 
      Manfred Klenner. Grammatical Role Labeling with Integer Linear Programming. In 
Proceedings of the 11th Conference of the European Chapter of the Association for 
Computational Linguistics, Conference Companion (EACL-2006), pages 187-190, 2006. 
    Pascal Denis and Jason Baldridge. Joint Determination of Anaphoricity and 
Coreference Resolution using Integer Programming. In Proceedings of the Annual 
Meeting of the North American Chapter of the Association for Computational Linguistics 
- Human Language Technology Conference (NAACL-HLT-2007), pages 236-243, 2007. 
    James Clarke and Mirella Lapata. Modelling Compression with Discourse 
Constraints. In Proceedings of the Conference on Empirical Methods in Natural 
Language Processing and on Computational Natural Language Learning (EMNLP-
CoNLL-2007), pages 1-11, 2007. 
    Manfred Klenner. Enforcing Consistency on Coreference Sets. In Recent Advances in 
Natural Language Processing (RANLP), pages 323-328, 2007 
     
    Dan Roth and Wen-tau Yih. Global Inference for Entity and Relation Identification via 
a Linear Programming Formulation. Introduction to Statistical Relational Learning, 2007.  
     
13
    K. Ganchev, Jo?o Gra?a and B. Taskar. Expectation Maximization and Posterior 
Constraints, Neural Information Processing Systems Conference (NIPS), Vancouver, 
BC, December 2007. 
    James Clarke and Mirella Lapata. Global Inference for Sentence Compression: An 
Integer Linear Programming Approach. Journal of Artificial Intelligence Research (JAIR), 
31, pages 399-429, 2008. 
    Vasin Punyakanok, Dan Roth and Wen-tau Yih. The Importance of Syntactic Parsing 
and Inference in Semantic Role Labeling. Computational Linguistics 34(2), pages 
257-287, 2008. 
    Jenny Rose Finkel and Christopher D. Manning. Enforcing Transitivity in Coreference 
Resolution. In Proceedings of the Annual Meeting of the Association for Computational 
Linguistics - Human Language Technology Conference, Short Papers (ACL-HLT-2008), 
pages 45-48, 2008.  
    K. Ganchev, Jo?o Gra?a and B. Taskar. Better Alignments = Better Translations?, 
Association for Computational Linguistics (ACL), Columbus, Ohio, June 2008.  
    Hal Daum?. Cross-Task Knowledge-Constrained Self Training In Proceedings of the 
2008 Conference on Empirical Methods in Natural Language Processing 
(EMNLP-2008). 
     
    Dan Goldwasser and Dan Roth. Transliteration as Constrained Optimization. In 
Proceedings of the 2008 Conference on Empirical Methods in Natural Language 
Processing (EMNLP-2008), pages 353-362, 2008. 
     
     
14
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 688?698,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unified Expectation Maximization
Rajhans Samdani
University of Illinois
rsamdan2@illinois.edu
Ming-Wei Chang
Microsoft Research
minchang@microsoft.com
Dan Roth
University of Illinois
danr@illinois.edu
Abstract
We present a general framework containing a
graded spectrum of Expectation Maximization
(EM) algorithms called Unified Expectation
Maximization (UEM.) UEM is parameterized
by a single parameter and covers existing al-
gorithms like standard EM and hard EM, con-
strained versions of EM such as Constraint-
Driven Learning (Chang et al, 2007) and Pos-
terior Regularization (Ganchev et al, 2010),
along with a range of new EM algorithms.
For the constrained inference step in UEM we
present an efficient dual projected gradient as-
cent algorithm which generalizes several dual
decomposition and Lagrange relaxation algo-
rithms popularized recently in the NLP litera-
ture (Ganchev et al, 2008; Koo et al, 2010;
Rush and Collins, 2011). UEM is as efficient
and easy to implement as standard EM. Fur-
thermore, experiments on POS tagging, infor-
mation extraction, and word-alignment show
that often the best performing algorithm in the
UEM family is a new algorithm that wasn?t
available earlier, exhibiting the benefits of the
UEM framework.
1 Introduction
Expectation Maximization (EM) (Dempster et al,
1977) is inarguably the most widely used algo-
rithm for unsupervised and semi-supervised learn-
ing. Many successful applications of unsupervised
and semi-supervised learning in NLP use EM in-
cluding text classification (McCallum et al, 1998;
Nigam et al, 2000), machine translation (Brown et
al., 1993), and parsing (Klein and Manning, 2004).
Recently, EM algorithms which incorporate con-
straints on structured output spaces have been pro-
posed (Chang et al, 2007; Ganchev et al, 2010).
Several variations of EM (e.g. hard EM) exist in
the literature and choosing a suitable variation is of-
ten very task-specific. Some works have shown that
for certain tasks, hard EM is more suitable than reg-
ular EM (Spitkovsky et al, 2010). The same issue
continues in the presence of constraints where Poste-
rior Regularization (PR) (Ganchev et al, 2010) cor-
responds to EM while Constraint-Driven Learning
(CoDL)1 (Chang et al, 2007) corresponds to hard
EM. The problem of choosing between EM and hard
EM (or between PR and CoDL) remains elusive,
along with the possibility of simple and better alter-
natives, to practitioners. Unfortunately, little study
has been done to understand the relationships be-
tween these variations in the NLP community.
In this paper, we approach various EM-based
techniques from a novel perspective. We believe that
?EM or Hard-EM?? and ?PR or CoDL?? are not the
right questions to ask. Instead, we present a unified
framework for EM, Unified EM (UEM), that covers
many EM variations including the constrained cases
along with a continuum of new ones. UEM allows us
to compare and investigate the properties of EM in a
systematic way and helps find better alternatives.
The contributions of this paper are as follows:
1. We propose a general framework called Uni-
fied Expectation Maximization (UEM) that
presents a continuous spectrum of EM algo-
rithms parameterized by a simple temperature-
like tuning parameter. The framework covers
both constrained and unconstrained EM algo-
rithms. UEM thus connects EM, hard EM, PR,
and CoDL so that the relation between differ-
ent algorithms can be better understood. It also
enables us to find new EM algorithms.
2. To solve UEM (with constraints), we propose
1To be more precise, (Chang et al, 2007) mentioned using
hard constraints as well as soft constraints in EM. In this paper,
we refer to CoDL only as the EM framework with hard con-
straints.
688
a dual projected subgradient ascent algorithm
that generalizes several dual decomposition
and Lagrange relaxation algorithms (Bertsekas,
1999) introduced recently in NLP (Ganchev et
al., 2008; Rush and Collins, 2011).
3. We provide a way to implement a family of
EM algorithms and choose the appropriate one,
given the data and problem setting, rather than
a single EM variation. We conduct experi-
ments on unsupervised POS tagging, unsuper-
vised word-alignment, and semi-supervised in-
formation extraction and show that choosing
the right UEM variation outperforms existing
EM algorithms by a significant margin.
2 Preliminaries
Let x denote an input or observed features and h be
a discrete output variable to be predicted from a fi-
nite set of possible outputs H(x). Let P?(x,h) be
a probability distribution over (x,h) parameterized
by ?. Let P?(h|x) refer to the conditional probabil-
ity of h given x. For instance, in part-of-speech tag-
ging, x is a sentence, h the corresponding POS tags,
and ? could be an HMM model; in word-alignment,
x can be an English-French sentence pair, h the
word alignment between the sentences, and ? the
probabilistic alignment model. Let ?(h = h?) be
the Kronecker-Delta distribution centered at h?, i.e.,
it puts a probability of 1 at h? and 0 elsewhere.
In the rest of this section, we review EM and
constraints-based learning with EM.
2.1 EM Algorithm
To obtain the parameter ? in an unsupervised way,
one maximizes log-likelihood of the observed data:
L(?) = logP?(x) = log
?
h?H(x)
P?(x,h) . (1)
EM (Dempster et al, 1977) is the most common
technique for learning ?, which maximizes a tight
lower bound onL(?). While there are a few different
styles of expressing EM, following the style of (Neal
and Hinton, 1998), we define
F (?, q) = L(?)?KL(q, P?(h|x)), (2)
where q is a posterior distribution over H(x) and
KL(p1, p2) is the KL divergence between two dis-
tributions p1 and p2. Given this formulation, EM can
be shown to maximize F via block coordinate ascent
alternating over q (E-step) and ? (M-step) (Neal and
Hinton, 1998). In particular, the E-step for EM can
be written as
q = arg min
q??Q
KL(q?, P?(h|x)) , (3)
where Q is the space of all distributions. While EM
produces a distribution in the E-step, hard EM is
thought of as producing a single output given by
h? = arg max
h?H(x)
P?(h|x) . (4)
However, one can also think of hard EM as pro-
ducing a distribution given by q = ?(h = h?). In
this paper, we pursue this distributional view of both
EM and hard EM and show its benefits.
EM for Discriminative Models EM-like algo-
rithms can also be used in discriminative set-
tings (Bellare et al, 2009; Ganchev et al, 2010)
specifically for semi-supervised learning (SSL.)
Given some labeled and unlabeled data, such algo-
rithms maximize a modified F (?, q) function:
F (?, q) = Lc(?)? c1???
2 ? c2KL(q, P?(h|x)) , (5)
where, q, as before, is a probability distribution over
H(x), Lc(?) is the conditional log-likelihood of the
labels given the features for the labeled data, and c1
and c2 are constants specified by the user; the KL
divergence is measured only over the unlabeled data.
The EM algorithm in this case has the same E-step
as unsupervised EM, but the M-step is different. The
M-step is similar to supervised learning as it finds ?
by maximizing a regularized conditional likelihood
of the data w.r.t. the labels ? true labels are used for
labeled data and ?soft? pseudo labels based on q are
used for unlabeled data.
2.2 Constraints in EM
It has become a common practice in the NLP com-
munity to use constraints on output variables to
guide inference. Few of many examples include
type constraints between relations and entities (Roth
and Yih, 2004), sentential and modifier constraints
during sentence compression (Clarke and Lapata,
2006), and agreement constraints between word-
alignment directions (Ganchev et al, 2008) or var-
ious parsing models (Koo et al, 2010). In the con-
689
text of EM, constraints can be imposed on the pos-
terior probabilities, q, to guide the learning proce-
dure (Chang et al, 2007; Ganchev et al, 2010).
In this paper, we focus on linear constraints over
h (potentially non-linear over x.) This is a very gen-
eral formulation as it is known that all Boolean con-
straints can be transformed into sets of linear con-
straints over binary variables (Roth and Yih, 2007).
Assume that we have m linear constraints on out-
puts where the kth constraint can be written as
uk
Th ? bk .
Defining a matrix U as UT =
[
u1T . . . umT
]
and a vector b as bT = [b1, . . . , bm], we write down
the set of all feasible2 structures as
{h | h ? H(x),Uh ? b} .
Constraint-Driven Learning (CoDL) (Chang et
al., 2007) augments the E-step of hard EM (4) by
imposing these constraints on the outputs.
Constraints on structures can be relaxed to expec-
tation constraints by requiring the distribution q to
satisfy them only in expectation. Define expecta-
tion w.r.t. a distribution q over H(x) as Eq[Uh] =?
h?H(x) q(h)Uh. In the expectation constraints
setting, q is required to satisfy:
Eq[Uh] ? b .
The space of distributions Q can be modified as:
Q = {q | q(h) ? 0, Eq[Uh] ? b,
?
h?H(x)
q(h) = 1}.
Augmenting these constraints into the E-step of
EM (3), gives the Posterior Regularization (PR)
framework (Ganchev et al, 2010). In this paper, we
adopt the expectation constraint setting. Later, we
show that UEM naturally includes and generalizes
both PR and CoDL.
3 Unified Expectation Maximization
We now present the Unified Expectation Maximiza-
tion (UEM) framework which captures a continuum
of (constrained and unconstrained) EM algorithms
2Note that this set is a finite set of discrete variables not to
be confused with a polytope. Polytopes are also specified as
{z|Az ? d} but are over real variables whereas h is discrete.
Algorithm 1 The UEM algorithm for both the genera-
tive (G) and discriminative (D) cases.
Initialize ?0
for t = 0, . . . , T do
UEM E-step:
qt+1 ? arg minq?QKL(q, P?t(h|x); ?)
UEM M-step:
G: ?t+1 = arg max? Eqt+1 [logP?(x,h)]
D: ?t+1 = arg max? Eqt+1 [logP?(h|x)]? c1???
2
end for
including EM and hard EM by modulating the en-
tropy of the posterior. A key observation underlying
the development of UEM is that hard EM (or CoDL)
finds a distribution with zero entropy while EM (or
PR) finds a distribution with the same entropy as P?
(or close to it). Specifically, we modify the objective
of the E-step of EM (3) as
q = arg min
q??Q
KL(q?, P?(h|x); ?) , (6)
where KL(q, p; ?) is a modified KL divergence:
KL(q, p; ?) =
?
h?H(x)
?q(h) log q(h)?q(h) log p(h). (7)
In other words, UEM projects P?(h|x) on the
space of feasible distributions Q w.r.t. a metric3
KL(?, ?; ?) to obtain the posterior q. By simply vary-
ing ?, UEM changes the metric of projection and ob-
tains different variations of EM including EM (PR,
in the presence of constraints) and hard EM (CoDL.)
The M-step for UEM is exactly the same as EM (or
discriminative EM.)
The UEM Algorithm: Alg. 1 shows the UEM al-
gorithm for both the generative (G) and the discrimi-
native (D) case. We refer to the UEM algorithm with
parameter ? as UEM? .
3.1 Relationship between UEM and Other EM
Algorithms
The relation between unconstrained versions of EM
has been mentioned before (Ueda and Nakano,
1998; Smith and Eisner, 2004). We show that the
relationship takes novel aspects in the presence of
constraints. In order to better understand different
UEM variations, we write the UEM E-step (6) ex-
plicitly as an optimization problem:
3The term ?metric? is used very loosely. KL(?, ?; ?) does
not satisfy the mathematical properties of a metric.
690
Framework ? = ?? ? = 0 ? ? (0, 1) ? = 1 ? =?? 1
Constrained Hard EM Hard EM (NEW) UEM? Standard EM Deterministic
Annealing EM
Unconstrained CoDL (Chang et
al., 2007)
(NEW) EM
with Lin. Prog.
(NEW) constrained
UEM?
PR (Ganchev et al,
2010)
Table 1: Summary of different UEM algorithms. The entries marked with ?(NEW)? have not been proposed before.
Eq. (8) is the objective function for all the EM frameworks listed in this table. Note that, in the absence of constraints,
? ? (??, 0] corresponds to hard EM (Sec. 3.1.1.) Please see Sec. 3.1 for a detailed explanation.
min
q
?
h?H(x)
?q(h) log q(h)? q(h) logP?(h|x)(8)
s.t. Eq[Uh] ? b,
q(h) ? 0,?h ? H(x),
?
h?H(x) q(h) = 1 .
We discuss below, both the constrained and the
unconstrained cases. Tab. 1 summarizes different
EM algorithms in the UEM family.
3.1.1 UEM Without Constraints
The E-step in this case, computes a q obeying
only the simplex constraints:
?
h?H(x) q(h) = 1.
For ? = 1, UEM minimizes KL(q, P?(h|x); 1)
which is the same as minimizing KL(q, P?(h|x))
as in the standard EM (3). For ? = 0, UEM
is solving arg minq?Q
?
h?H(x)?q(h) logP?(h|x)
which is a linear programming (LP) problem. Due to
the unimodularity of the simplex constraints (Schri-
jver, 1986), this LP outputs an integral q =
?
(
h = arg maxh?H(x) P?(h|x)
)
which is the same
as hard EM (4). It has already been noted in the liter-
ature (Kearns et al, 1997; Smith and Eisner, 2004;
Hofmann, 2001) that this formulation (correspond-
ing to our ? = 0) is the same as hard EM. In fact,
for ? ? 0, UEM stays the same as hard EM be-
cause of negative penalty on the entropy. The range
? ? (0, 1) has not been discussed in the literature,
to the best of our knowledge. In Sec. 5, we show
the impact of using UEM?for ? ? {0, 1}. Lastly,
the range of ? from? to 1 has been used in deter-
ministic annealing for EM (Rose, 1998; Ueda and
Nakano, 1998; Hofmann, 2001). However, the focus
of deterministic annealing is solely to solve the stan-
dard EM while avoiding local maxima problems.
3.1.2 UEM With Constraints
UEM and Posterior Regularization (? = 1) For
? = 1, UEM solves arg minq?QKL (q, P?(h|x))
which is the same as Posterior Regulariza-
tion (Ganchev et al, 2010).
UEM and CoDL (? = ??) When ? ? ??
then due to an infinite penalty on the entropy of the
posterior, the entropy must become zero. Thus, now
the E-step, as expressed by Eq. (8), can be written as
q = ?(h = h?) where h? is obtained as
arg max
h?H(x)
logP?(h|x) (9)
s.t. Uh ? b ,
which is the same as CoDL. This combinatorial
maximization can be solved using the Viterbi algo-
rithm in some cases or, in general, using Integer Lin-
ear Programming (ILP.)
3.2 UEM with ? ? [0, 1]
Tab. 1 lists different EM variations and their associ-
ated values ?. This paper focuses on values of ? be-
tween 0 and 1 for the following reasons. First, the E-
step (8) is non-convex for ? < 0 and hence compu-
tationally expensive; e.g., hard EM (i.e. ? = ??)
requires ILP inference. For ? ? 0, (8) is a convex
optimization problem which can be solved exactly
and efficiently. Second, for ? = 0, the E-step solves
max
q
?
h?H(x) q(h) logP?(h|x) (10)
s.t. Eq[Uh] ? b,
q(h) ? 0, ?h ? H(x),
?
h?H(x) q(h) = 1 ,
which is an LP-relaxation of hard EM (Eq. (4)
and (9)). LP relaxations often provide a decent
proxy to ILP (Roth and Yih, 2004; Martins et al,
2009). Third, ? ? [0, 1] covers standard EM/PR.
3.2.1 Discussion: Role of ?
The modified KL divergence can be related to
standard KL divergence as KL(q, P?(h|x); ?) =
691
KL(q, P?(y|x)) + (1? ?)H(q) ? UEM (6) mini-
mizes the former during the E-step, while Standard
EM (3) minimizes the latter. The additional term
(1 ? ?)H(q) is essentially an entropic prior on the
posterior distribution q which can be used to regu-
larize the entropy as desired.
For ? < 1, the regularization term penalizes the
entropy of the posterior thus reducing the probability
mass on the tail of the distribution. This is signifi-
cant, for instance, in unsupervised structured predic-
tion where the tail can carry a substantial amount of
probability mass as the output space is massive. This
notion aligns with the observation of (Spitkovsky
et al, 2010) who criticize EM for frittering away
too much probability mass on unimportant outputs
while showing that hard EM does much better in
PCFG parsing. In particular, they empirically show
that when initialized with a ?good? set of parame-
ters obtained by supervised learning, EM drifts away
(thus losing accuracy) much farther than hard-EM.
4 Solving Constrained E-step with
Lagrangian Dual
In this section, we discuss how to solve the E-
step (8) for UEM. It is a non-convex problem for
? < 0; however, for ? = ?? (CoDL) one can use
ILP solvers. We focus here on solving the E-step for
? ? 0 for which it is a convex optimization problem,
and use a Lagrange relaxation algorithm (Bertsekas,
1999). Our contributions are two fold:
? We describe an algorithm for UEM with con-
straints that is as easy to implement as PR or
CoDL. Existing code for constrained EM (PR
or CoDL) can be easily extended to run UEM.
? We solve the E-step (8) using a Lagrangian
dual-based algorithm which performs projected
subgradient-ascent on dual variables. Our al-
gorithm covers Lagrange relaxation and dual
decomposition techniques (Bertsekas, 1999)
which were recently popularized in NLP (Rush
and Collins, 2011; Rush et al, 2010; Koo et al,
2010). Not only do we extend the algorithmic
framework to a continuum of algorithms, we
also allow, unlike the aforementioned works,
general inequality constraints over the output
variables. Furthermore, we establish new and
interesting connections between existing con-
strained inference techniques.
4.1 Projected Subgradient Ascent with
Lagrangian Dual
We provide below a high-level view of our algo-
rithm, omitting the technical derivations due to lack
of space. To solve the E-step (8), we introduce dual
variables ? ? one for each expectation constraint in
Q. The subgradient O? of the dual of Eq. (8) w.r.t.
? is given by
O? ? Eq[Uh]? b . (11)
For ? > 0, the primal variable q can be written in
terms of ? as
q(h) ? P?t(h|x)
1
? e?
?TUh
? . (12)
For ? = 0, the q above is not well defined and so
we take the limit ? ? 0 in (12) and since lp norm
approaches the max-norm as p??, this yields
q(h) = ?(h = arg max
h??H(x)
P?(h
?|x)e??
TUh?). (13)
We combine both the ideas by setting q(h) =
G(h, P?t(?|x), ?TU, ?) where
G(h, P,v, ?) =
?
?
??
?
??
P (h)
1
? e
? vh?
?
h? P (h
?)
1
? e
? vh
?
?
? > 0 ,
?(h= argmax
h??H(x)
P (h?)e?vh
?
) ? = 0 .
(14)
Alg. 2 shows the overall optimization scheme.
The dual variables for inequality constraints are re-
stricted to be positive and hence after a gradient up-
date, negative dual variables are projected to 0.
Note that for ? = 0, our algorithm is a Lagrange
relaxation algorithm for approximately solving the
E-step for CoDL (which uses exact arg max infer-
ence). Lagrange relaxation has been recently shown
to provide exact and optimal results in a large num-
ber of cases (Rush and Collins, 2011). This shows
that our range of algorithms is very broad ? it in-
cludes PR and a good approximation to CoDL.
Overall, the required optimization (8) can be
solved efficiently if the expected value computation
in the dual gradient (Eq. (11)) w.r.t. the posterior q
in the primal (Eq (14)) can be performed efficiently.
In cases where we can enumerate the possible out-
puts h efficiently, e.g. multi-class classification, we
692
Algorithm 2 Solving E-step of UEM? for ? ? 0.
1: Initialize and normalize q; initialize ? = 0.
2: for t = 0, . . . , R or until convergence do
3: ?? max (?+ ?t (Eq[Uh]? b) , 0)
4: q(h) = G(h, P?t(?|x), ?TU, ?)
5: end for
can compute the posterior probability q explicitly
using the dual variables. In cases where the out-
put space is structured and exponential in size, e.g.
word alignment, we can optimize (8) efficiently if
the constraints and the model P?(h|x) decompose
in the same way. To elucidate, we give a more con-
crete example in the next section.
4.2 Projected Subgradient based Dual
Decomposition Algorithm
Solving the inference (8) using Lagrangian dual can
often help us decompose the problem into compo-
nents and handle complex constraints in the dual
space as we show in this section. Suppose our
task is to predict two output variables h1 and h2
coupled via linear constraints. Specifically, they
obey Ueh1 = Ueh2 (agreement constraints) and
Uih1 ? Uih2 (inequality constraints)4 for given
matrices Ue and Ui. Let their respective probabilis-
tic models be P 1?1 and P
2
?2 . The E-step (8) can be
written as
arg min
q1,q2
A(q1, q2; ?) (15)
s.t. Eq1 [Ueh
1] = Eq2 [Ueh
2]
Eq1 [Uih
1] ? Eq2 [Uih
2] ,
where A(q1, q2; ?) = KL(q1(h1), P 1?1(h
1|x); ?) +
KL(q2(h2), P 2?2(h
2|x); ?).
The application of Alg. 2 results in a dual decom-
position scheme which is described in Alg. 3.
Note that in the absence of inequality constraints
and for ? = 0, our algorithm reduces to a simpler
dual decomposition algorithm with agreement con-
straints described in (Rush et al, 2010; Koo et al,
2010). For ? = 1 with agreement constraints, our
algorithm specializes to an earlier proposed tech-
nique by (Ganchev et al, 2008). Thus our algo-
rithm puts these dual decomposition techniques with
4The analysis remains the same for a more general formu-
lation with a constant offset vector on the R.H.S. and different
matrices for h1 and h2.
Algorithm 3 Projected Subgradient-based Lagrange
Relaxation Algorithm that optimizes Eq. (15)
1: Input: Two distributions P 1?1 and P
2
?2 .
2: Output: Output distributions q1 and q2 in (15)
3: Define ?T =
[
?e
T ?i
T
]
and UT =
[
Ue
T Ui
T
]
4: ?? 0
5: for t = 0, . . . , R or until convergence do
6: q1(h1)? G(h1, P 1?1(?|x), ?
TU, ?)
7: q2(h2)? G(h2, P 2?2(?|x),??
TU, ?)
8: ?e ? ?e + ?t(?Eq1 [Ueh
1] + Eq2 [Ueh
2])
9: ?i ? ?i + ?t(?Eq1 [Uih
1] + Eq2 [Uih
2])
10: ?i ? max(?i, 0) {Projection step}
11: end for
12: return (q1, q2)
agreement constraints on the same spectrum. More-
over, dual-decomposition is just a special case of
Lagrangian dual-based techniques. Hence Alg. 2
is more broadly applicable (see Sec. 5). Lines 6-9
show that the required computation is decomposed
over each sub-component.
Thus if computing the posterior and expected val-
ues of linear functions over each subcomponent is
easy, then the algorithm works efficiently. Con-
sider the case when constraints decompose linearly
over h and each component is modeled as an HMM
with ?S as the initial state distribution, ?E as em-
mision probabilities, and ?T as transition probabil-
ities. An instance of this is word alignment over
language pair (S, T ) modeled using an HMM aug-
mented with agreement constraints which constrain
alignment probabilities in one direction (P?1 : from
S to T ) to agree with the alignment probabilities in
the other direction (P?2 : from T to S.) The agree-
ment constraints are linear over the alignments, h.
Now, the HMM probability is given by
P?(h|x) = ?S(h0)
?
i ?E(xi|hi)?T (hi+1|hi)
where vi denotes the ith component of a vector v.
For ? > 0, the resulting q (14) can be expressed
using a vector ? =+/-?TU (see lines 6-7) as
q(h) ?
(
?S(h0)
?
i
?E(xi|hi)?T (hi+1|hi)
) 1
?
e
?
i ?ihi
?
?
?
i
?S(h0)
1
?
(
?E(xi|hi)e?ihi
) 1
? ?T (hi+1|hi)
1
? .
The dual variables-based term can be folded into
the emission probabilities, ?E . Now, the resulting q
can be expressed as an HMM by raising ?S , ?E , and
693
?T to the power 1/? and normalizing. For ? = 0, q
can be computed as the most probable output. The
required computations in lines 6-9 can be performed
using the forward-backward algorithm or the Viterbi
algorithm. Note that we can efficiently compute ev-
ery step because the linear constraints decompose
nicely along the probability model.
5 Experiments
Our experiments are designed to explore tuning ?
in the UEM framework as a way to obtain gains
over EM and hard EM in the constrained and uncon-
strained cases. We conduct experiments on POS-
tagging, word-alignment, and information extrac-
tion; we inject constraints in the latter two. In all the
cases we use our unified inference step to implement
general UEM and the special cases of existing EM
algorithms. Since both of our constrained problems
involve large scale constrained inference during the
E-step, we use UEM0 (with a Lagrange relaxation
based E-step) as a proxy for ILP-based CoDL .
As we vary ? over [0, 1], we circumvent much of
the debate over EM vs hard EM (Spitkovsky et al,
2010) by exploring the space of EM algorithms in a
?continuous? way. Furthermore, we also study the
relation between quality of model initialization and
the value of ? in the case of POS tagging. This is
inspired by a general ?research wisdom? that hard
EM is a better choice than EM with a good initial-
ization point whereas the opposite is true with an
?uninformed? initialization.
Unsupervised POS Tagging We conduct exper-
iments on unsupervised POS learning experiment
with the tagging dictionary assumption. We use a
standard subset of Penn Treebank containing 24,115
tokens (Ravi and Knight, 2009) with the tagging dic-
tionary derived from the entire Penn Treebank. We
run UEM with a first order (bigram) HMM model5.
We consider initialization points of varying quality
and observe the performance for ? ? [0, 1].
Different initialization points are constructed as
follows. The ?posterior uniform? initialization is
created by spreading the probability uniformly over
all possible tags for each token. Our EM model on
5(Ravi and Knight, 2009) showed that a first order HMM
model performs much better than a second order HMM model
on unsupervised POS tagging
-0.15-0.1-0.05 0 0.05 1
.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Relative performance to EM (Gamma=1)
Gamm
a
unifor
m pos
terior 
initiali
zer
5 labe
led ex
ample
s initia
lizer
10 lab
eled e
xampl
es init
ializer
20 lab
eled e
xampl
es init
ializer
40 lab
eled e
xampl
es init
ializer
80 lab
eled e
xampl
es init
ializer
Figure 1: POS Experiments showing the relation between
initial model parameters and ?. We report the relative per-
formance compared to EM (see Eq. (16)). The posterior
uniform initialization does not use any labeled examples.
As the no. of labeled examples used to create the initial
HMM model increases, the quality of the initial model
improves. The results show that the value of the best ? is
sensitive to the initialization point and EM (? = 1) and
hard EM (? = 0) are often not the best choice.
this dataset obtains 84.9% accuracy on all tokens
and 72.3% accuracy on ambiguous tokens, which
is competitive with results reported in (Ravi and
Knight, 2009). To construct better initialization
points, we train a supervised HMM tagger on hold-
out labeled data. The quality of the initialization
points is varied by varying the size of the labeled
data over {5, 10, 20, 40, 80}. Those initialization
points are then fed into different UEM algorithms.
Results For a particular ?, we report the perfor-
mance of UEM? w.r.t. EM (? = 1.0) as given by
rel(?) =
Acc(UEM?)?Acc(UEM?=1.0)
Acc(UEM?=1.0)
(16)
where Acc represents the accuracy as evaluated on
the ambiguous words of the given data. Note that
rel(?) ? 0, implies performance better or worse
than EM. The results are summarized in Figure 1.
Note that when we use the ?posterior uniform?
initialization, EM wins by a significant margin. Sur-
prisingly, with the initialization point constructed
with merely 5 or 10 examples, EM is not the best
algorithm anymore. The best result for most cases is
obtained at ? somewhere between 0 (hard EM) and 1
(EM). Furthermore, the results not only indicate that
a measure of ?hardness? of EM i.e. the best value
694
of ?, is closely related to the quality of the ini-
tialization point but also elicit a more fine-grained
relationship between initialization and UEM.
This experiment agrees with (Merialdo, 1994),
which shows that EM performs poorly in the semi-
supervised setting. In (Spitkovsky et al, 2010), the
authors show that hard EM (Viterbi EM) works bet-
ter than standard EM. We extend these results by
showing that this issue can be overcome with the
UEM framework by picking appropriate ? based on
the amount of available labeled data.
Semi-Supervised Entity-Relation Extraction
We conduct semi-supervised learning (SSL) ex-
periments on entity and relation type prediction
assuming that we are given mention boundaries.
We borrow the data and the setting from (Roth and
Yih, 2004). The dataset has 1437 sentences; four
entity types: PER, ORG, LOC, OTHERS and;
five relation types LIVE IN, KILL, ORG BASED IN,
WORKS FOR, LOCATED IN. We consider relations
between all within-sentence pairs of entities. We
add a relation type NONE indicating no relation
exists between a given pair of entities.
We train two log linear models for entity type and
relation type prediction, respectively via discrimina-
tive UEM. We work in a discriminative setting in
order to use several informative features which we
borrow from (Roth and Small, 2009). Using these
features, we obtain 56% average F1 for relations and
88% average F1 for entities in a fully supervised set-
ting with an 80-20 split which is competitive with
the reported results on this data (Roth and Yih, 2004;
Roth and Small, 2009). For our SSL experiments,
we use 20% of data for testing, a small amount, ?%,
as labeled training data (we vary ?), and the remain-
ing as unlabeled training data. We initialize with a
classifier trained on the given labeled data.
We use the following constraints on the posterior.
1) Type constraints: For two entities e1 and e2, the
relation type ?(e1, e2) between them dictates a par-
ticular entity type (or in general, a set of entity types)
for both e1 and e2. These type constraints can be
expressed as simple logical rules which can be con-
verted into linear constraints. E.g. if the pair (e1, e2)
has relation type LOCATED IN then e2 must have en-
tity type LOC. This yields a logical rule which is
converted into a linear constraint as
0.3	 ?0.32	 ?
0.34	 ?0.36	 ?
0.38	 ?0.4	 ?
0.42	 ?0.44	 ?
0.46	 ?0.48	 ?
5	 ? 10	 ? 20	 ?
Avg.	 ?F1
	 ?for	 ?rel
a?ons	 ?
%	 ?of	 ?labeled	 ?data	 ?
Sup.	 ?Bas.	 ? PR	 ?
CoDL	 ? UEM	 ?
Figure 2: Average F1 for relation prediction for varying
sizes of labeled data comparing the supervised baseline,
PR, CoDL, and UEM. UEM is statistically significantly
better than supervised baseline and PR in all the cases.
(?(e1, e2) == LOCATED IN) ? (e2 == LOC)
? q (LOCATED IN; e1, e2) ? q (LOC; e2) .
Refer to (Roth and Yih, 2004) for more statistics on
this data and a list of all the type constraints used.
2) Expected count constraints: Since most entity
pairs are not covered by the given relation types, the
presence of a large number of NONE relations can
overwhelm SSL. To guide learning in the right direc-
tion, we use corpus-wide expected count constraints
for each non-NONE relation type. These constraints
are very similar to the label regularization technique
mentioned in (Mann and McCallum, 2010). Let Dr
be the set of entity pairs as candidate relations in the
entire corpus. For each non-NONE relation type ?,
we impose the constraints
L? ?
?
(e1,e2)?Dr
q(?; e1, e2) ? U? ,
where L? and U? are lower and upper bound on the
expected number of ? relations in the entire corpus.
Assuming that the labeled and the unlabeled data are
drawn from the same distribution, we obtain these
bounds using the fractional counts of ? over the la-
beled data and then perturbing it by +/- 20%.
Results We use Alg. 2 for solving the constrained
E-step. We report results averaged over 10 random
splits of the data and measure statistical significance
using paired t-test with p = 0.05. The results for
relation prediction are shown in Fig. 2. For each
trial, we split the labeled data into half to tune the
value of ?. For ? = 5%, 10%, and 20%, the average
695
value of gamma is 0.52, 0.6, and 0.57, respectively;
the median values are 0.5, 0.6, and 0.5, respectively.
For relation extraction, UEM is always statistically
significantly better than the baseline and PR. The
difference between UEM and CoDL is small which
is not very surprising because hard EM approaches
like CoDL are known to work very well for discrim-
inative SSL. We omit the graph for entity predic-
tion because EM-based approaches do not outper-
form the supervised baseline there. However, no-
tably, for entities, for ? = 10%, UEM outperforms
CoDL and PR and for 20%, the supervised baseline
outperforms PR statistically significantly.
Word Alignment Statistical word alignment is a
well known structured output application of unsu-
pervised learning and is a key step towards ma-
chine translation from a source language S to a tar-
get language T . We experiment with two language-
pairs: English-French and English-Spanish. We
use Hansards corpus for French-English trans-
lation (Och and Ney, 2000) and Europarl cor-
pus (Koehn, 2002) for Spanish-English translation
with EPPS (Lambert et al, 2005) annotation.
We use an HMM-based model for word-
alignment (Vogel et al, 1996) and add agreement
constraints (Liang et al, 2008; Ganchev et al, 2008)
to constrain alignment probabilities in one direction
(P?1 : from S to T ) to agree with the alignment prob-
abilities in the other direction (P?2 : from T to S.)
We use a small development set of size 50 to tune
the model. Note that the amount of labeled data we
use is much smaller than the supervised approaches
reported in (Taskar et al, 2005; Moore et al, 2006)
and unsupervised approaches mentioned in (Liang et
al., 2008; Ganchev et al, 2008) and hence our results
are not directly comparable. For the E-step, we use
Alg. 3 with R=5 and pick ? from {0.0, 0.1, . . . , 1.0},
tuning it over the development set.
During testing, instead of running HMM mod-
els for each direction separately, we obtain posterior
probabilities by performing agreement constraints-
based inference as in Alg. 3. This results in a
posterior probability distribution over all possible
alignments. To obtain final alignments, follow-
ing (Ganchev et al, 2008) we use minimum Bayes
risk decoding: we align all word pairs with poste-
rior marginal alignment probability above a certain
Size EM PR CoDL UEM EM PR CoDL UEM
En-Fr Fr-En
10k 23.54 10.63 14.76 9.10 19.63 10.71 14.68 9.21
50k 18.02 8.30 10.08 7.34 16.17 8.40 10.09 7.40
100k 16.31 8.16 9.17 7.05 15.03 8.09 8.93 6.87
En-Es Es-En
10k 33.92 22.24 28.19 20.80 31.94 22.00 28.13 20.83
50k 25.31 19.84 22.99 18.93 24.46 20.08 23.01 18.95
100k 24.48 19.49 21.62 18.75 23.78 19.70 21.60 18.64
Table 2: AER (Alignment Error Rate) comparisons
for French-English (above) and Spanish-English (below)
alignment for various data sizes. For French-English set-
ting, tuned ? for all data-sizes is either 0.5 or 0.6. For
Spanish-English, tuned ? for all data-sizes is 0.7.
threshold, tuned over the development set.
Results We compare UEM with EM, PR, and
CoDL on the basis of Alignment Error Rate (AER)
for different sizes of unlabeled data (See Tab. 2.)
See (Och and Ney, 2003) for the definition of AER.
UEM consistently outperforms EM, PR, and CoDL
with a wide margin.
6 Conclusion
We proposed a continuum of EM algorithms
parameterized by a single parameter. Our frame-
work naturally incorporates constraints on output
variables and generalizes existing constrained and
unconstrained EM algorithms like standard and
hard EM, PR, and CoDL. We provided an efficient
Lagrange relaxation algorithm for inference with
constraints in the E-step and empirically showed
how important it is to choose the right EM version.
Our technique is amenable to be combined with
many existing variations of EM (Berg-Kirkpatrick
et al, 2010). We leave this as future work.
Acknowledgments: We thank Joa?o Grac?a for provid-
ing the code and data for alignment with agreement. This
research is sponsored by the Army Research Laboratory
(ARL) under agreement W911NF-09-2-0053, Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-018, and an
ONR Award on Guiding Learning and Decision Making
in the Presence of Multiple Forms of Information. Any
opinions, findings, conclusions or recommendations are
those of the authors and do not necessarily reflect the
views of the funding agencies.
696
References
K. Bellare, G. Druck, and A. McCallum. 2009. Alter-
nating projections for learning with expectation con-
straints. In UAI.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In ACL, HLT ?10.
D. P. Bertsekas. 1999. Nonlinear Programming. Athena
Scientific, 2nd edition.
P. Brown, S. D. Pietra, V. D. Pietra, and R. Mercer. 1993.
The mathematics of statistical machine translation: pa-
rameter estimation. Computational Linguistics.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In ACL.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society.
K. Ganchev, J. Graca, and B. Taskar. 2008. Better align-
ments = better translations. In ACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
T. Hofmann. 2001. Unsupervised learning by probabilis-
tic latent semantic analysis. MlJ.
M. Kearns, Y. Mansour, and A. Y. Ng. 1997. An
information-theoretic analysis of hard and soft assign-
ment methods for clustering. In ICML.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: models of dependency and
constituency. In ACL.
P. Koehn. 2002. Europarl: A multilingual corpus for
evaluation of machine translation.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.
P. Lambert, A. De Gispert, R. Banchs, and J. Marino.
2005. Guidelines for word alignment evaluation and
manual alignment. Language Resources and Evalua-
tion.
P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-
based learning. In NIPS.
G. S. Mann and A. McCallum. 2010. Generalized
expectation criteria for semi-supervised learning with
weakly labeled data. JMLR, 11.
A. Martins, N. A. Smith, and E. Xing. 2009. Concise
integer linear programming formulations for depen-
dency parsing. In ACL.
A. K. McCallum, R. Rosenfeld, T. M. Mitchell, and A. Y.
Ng. 1998. Improving text classification by shrinkage
in a hierarchy of classes. In ICML.
B. Merialdo. 1994. Tagging text with a probabilistic
model. Computational Linguistics.
R. C. Moore, W. Yih, and A. Bode. 2006. Improved
discriminative bilingual word alignment. In ACL.
R. M. Neal and G. E. Hinton. 1998. A new view of
the EM algorithm that justifies incremental, sparse and
other variants. In M. I. Jordan, editor, Learning in
Graphical Models.
K. Nigam, A. K. Mccallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using EM. Machine Learning.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. CL, 29.
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. ACL, 1(August).
K. Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression, and related op-
timization problems. In IEEE, pages 2210?2239.
D. Roth and K. Small. 2009. Interactive feature space
construction using semantic information. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
H. T. Ng and E. Riloff, editors, CoNLL.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In L. Getoor and B. Taskar, editors, In-
troduction to Statistical Relational Learning.
A. M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through lagrangian relax-
ation. In ACL.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
A. Schrijver. 1986. Theory of linear and integer pro-
gramming. John Wiley & Sons, Inc.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In ACL.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
HLT-EMNLP.
N. Ueda and R. Nakano. 1998. Deterministic annealing
em algorithm. Neural Network.
697
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-based
word alignment in statistical translation. In COLING.
698
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 29?32,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
A Robust Shallow Temporal Reasoning System
Ran Zhao Quang Xuan Do Dan Roth
Computer Science Department
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{ranzhao1,quangdo2,danr}@illinois.edu
Abstract
This paper presents a demonstration of a tem-
poral reasoning system that addresses three
fundamental tasks related to temporal expres-
sions in text: extraction, normalization to time
intervals and comparison. Our system makes
use of an existing state-of-the-art temporal ex-
traction system, on top of which we add sev-
eral important novel contributions. In addi-
tion, we demonstrate that our system can per-
form temporal reasoning by comparing nor-
malized temporal expressions with respect
to several temporal relations. Experimental
study shows that the system achieves excellent
performance on all the tasks we address.
1 Introduction
Performing temporal reasoning with respect to tem-
poral expressions is important in many NLP tasks
such as text summarization, information extraction,
discourse understanding and information retrieval.
Recently, the Knowledge Base Population track (Ji
et al, 2011) introduced the temporal slot filling task
that requires identifying and extracting temporal in-
formation for a limited set of binary relations such as
(person, employee of), (person, spouse). In the work
of (Wang et al, 2010), the authors presented the
Timely Yago ontology, which extracted and incorpo-
rated temporal information as part of the description
of the events and relations in the ontology. Temporal
reasoning is also essential in supporting the emerg-
ing temporal information retrieval research direction
(Alonso et al, 2011).
In this paper, we present a system that addresses
three fundamental tasks in temporal reasoning:
? Extraction: Capturing the extent of time expres-
sions in a given text. This task is based on task A in
the TempEval-2 challenge (Verhagen et al, 2010).
Consider the following sentence:
Seventy-five million copies of the rifle have been
built since it entered production in February 1947.
In this sentence, February 1947 is a basic temporal
expression that should be extracted by the extraction
module. More importantly, we further extend the
task to support also the extraction of complex tem-
poral expressions that are not addressed by existing
systems. In the example above, it is important to rec-
ognize and capture the phrase since it entered pro-
duction in February 1947 as another temporal ex-
pression that expresses the time period of the manu-
facturing event (triggered by built.) For the best of
our knowledge, this extension is novel.
? Normalization: Normalizing temporal expres-
sions, which are extracted by the extraction module,
to a canonical form. Our system normalizes tem-
poral expressions (including complex ones) to time
intervals of the form [start point, end point]. The
endpoints follow a standard date and time format:
YYYY-MM-DD hh:mm:ss. Our system accounts for
an input reference date when performing the normal-
ization. For example, given March 20th, 1947 as a
reference date, our system normalizes the temporal
expressions extracted in the example above as fol-
lows: [1947-02-01 00:00:00, 1947-02-28 23:59:59]
and [1947-02-01 00:00:00, 1947-03-20 23:59:59],
respectively.
? Comparison: Comparing two time intervals
(i.e. normalized temporal expressions). This mod-
ule identifies the temporal relation that holds be-
29
tween intervals, including the before, before-and-
overlap, containing, equal, inside , after and after-
and-overlap relations. For example, when compar-
ing the two normalized time intervals above, we get
the following result: [1947-02-01 00:00:00, 1947-
02-28 23:59:59] is inside [1947-02-01 00:00:00,
1947-03-20 23:59:59].
There has been much work addressing the prob-
lems of temporal expression extraction and normal-
ization, i.e. the systems developed in TempEval-2
challenge (Verhagen et al, 2010). However, our sys-
tem is different from them in several aspects. First,
we extend the extraction task to capture complex
temporal expressions. Second, our system normal-
izes temporal expressions (including complex ones)
to time intervals instead of time points. Finally, our
system performs temporal comparison of time inter-
vals with respect to multiple relations. We believe
that with the rapid progress in NLP and IR, more
tasks will require temporal information and reason-
ing, and a system that addresses these three funda-
mental tasks well will be able to support and facili-
tate temporal reasoning systems efficiently.
2 The System
2.1 Temporal Expression Extraction
We built the temporal expression extraction module
on top of the Heideltime system (Stro?tgen and Gertz,
2010) to take advantage of a state-of-the-art tempo-
ral extraction system in capturing basic expressions.
We use the Illinois POS tagger1 (Roth and Zelenko,
1998) to provide part-of-speech tags for the input
text before passing it to HeidelTime. Below is an
example of the HeidelTime output of the example in
the previous section:
Seventy-five million copies of the rifle have been
built since it entered production in <TIMEX3
tid=?t2? type=?DATE? value=?1947-02?>February
1947</TIMEX3>
In this example, HeidelTime captures a basic tem-
poral expression: February 1947. However, Heidel-
Time cannot capture the complex temporal expres-
sion since it entered production in February 1947,
which expresses a period of time from February
1947 until the document creation time. This is ac-
tually the time period of the manufacturing event
1http://cogcomp.cs.illinois.edu/page/software view/POS
NP
PP
VP
  SBAR
Seventy-five million copies of the rifle have been built   since it entered production in Feburary 1947
VP
NP
S
Figure 1: The SBAR constituent in the parse tree de-
termines an extended temporal expression given that in
February 1947 is already captured by HeidelTime.
(triggered by built). To capture complex phrases, we
make use of a syntactic parse tree2 as illustrated in
Figure 1. A complex temporal expression is recog-
nized if it satisfies the following conditions:
? It is covered by a PP or SBAR constituent
in the parse tree.
? The constituent starts with a temporal con-
nective. In this work, we focus on an impor-
tant subset of temporal connectives, consist-
ing of since, between, from, before and after.
? It contains at least one basic temporal ex-
pression extracted by HeidelTime.
In addition, our extraction module also handles
holidays in several countries. For example, in
the sentence ?The gas price increased rapidly after
Christmas.?, we are able to extract two temporal ex-
pressions Christmas and after Christmas, which re-
fer to different time intervals.
2.2 Normalization to Time Intervals
Our system normalizes a temporal expression to a
time interval of the form [start point, end point],
where start point? end point. Each time endpoint of
an interval follows a standard date and time format:
YYYY-MM-DD hh:mm:ss. It is worth noting that this
format augments the date format in TimeML, used
by HeidelTime and other existing systems. Our date
and time format of each time endpoint refer to an
absolute time point on a universal timeline, making
our time intervals absolute as well. Furthermore, we
take advantage of the predicted temporal value of
each temporal expression from the HeidelTime out-
put. For instance, in the HeidelTime output example
above, we extract 1947-02 as the normalized date
of February 1947 and then convert it to the inter-
val [1947-02-01 00:00:00, 1947-02-28 23:59:59]. If
HeidelTime cannot identify an exact date, month or
year, we then resort to our own temporal normalizer,
2We use nlparser (Charniak and Johnson, 2005)
30
which consists of a set of conversion rules, regard-
ing to the document creation time of the input text.
An interval endpoint can get infinity value if its tem-
poral boundary cannot be specified.
2.3 Comparison
To compare two time intervals (i.e. normalized
temporal expressions), we define six temporal rela-
tions: before, before-and-overlap, contains, equals,
inside, after and after-and-overlap. The temporal
relation between two normalized intervals is deter-
mined by a set of comparison rules that take the four
interval endpoints into consideration. For example,
A = [sA, eA] contains B = [sB, eB] if and only if
(sA < sB)? (eA > eB), where s and e are intervals
start and end points, respectively.
3 Experimental Study
In this section, we present an evaluation of our ex-
tended temporal extractor, the normalizer and the
comparator. We do not evaluate the HeidelTime
temporal extractor again because its performance
was reported in the TempEval-2 challenge (Verha-
gen et al, 2010), where it achieved 0.86 F1 score on
the TimeBank data sets (Pustejovsky et al, 2003).
3.1 Data Preparation
We focus on scaling up temporal systems to deal
with complex expressions. Therefore, we prepared
an evaluation data set that consists of a list of sen-
tences containing at least one of the five temporal
connectives since, betwen, from, before and after.
To do this, we extract all sentences that satisfy the
condition from 183 articles in the TimeBank 1.2
corpus3. This results in a total of 486 sentences.
Each sentence in the data set comes with the doc-
ument creation time (DCT) of its corresponding ar-
ticle. The second and the third columns of Table
1 summarize the number of sentences and appear-
ances of each temporal connective.
We use this data set to evaluate the extended tem-
poral extractor, the normalizer and also the com-
parator of our system. We note that although this
data set is driven by our focused temporal connec-
tives, it does not lose the generality of evaluating
3http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?
catalogId=LDC2006T08
Connective # sent. # appear. Prec Rec F1
since 31 31 1.0 1.0 1.0
between 32 33 1.0 1.0 1.0
from 340 366 0.8 1.0 0.89
before 33 33 0.8 1.0 0.89
after 78 81 0.72 1.0 0.84
Avg. 0.86 1.0 0.92
Table 1: The performance of our extended temporal ex-
tractor on complex expressions which contain at least one
of the connectives shown in the first column. These ex-
pressions cannot be identified by existing systems.
Module Correct Incorrect Acc
Normalizer 191 16 0.92
Comparator 191 0 1.0
Table 2: The performance of the normalization and com-
parison modules. We only compare the 191 correctly
identified time intervals with their corresponding docu-
ment creation time.
the normalization and comparison modules because
the sentences in this data set alo contain many ba-
sic temporal expressions. Moreover, there are many
cases where the connectives in our data are not actu-
ally temporal connectives. Our system is supposed
to not capture them as temporal expressions. This is
also reflected in the experimental results.
3.2 Experimental Results
We report the performance of our extended tem-
poral extraction module using precision, recall and
F1 score as shown in the last three columns of Ta-
ble 1. We evaluate the normalization module on
the correctly extracted temporal expressions, includ-
ing basic expressions captured by HeidelTime and
the extended expressions identified by our extrac-
tor. A normalization is correct if and only if both
time interval endpoints are correctly identified. We
study the comparison module by evaluating it on
the comparisons of the correctly normalized expres-
sions against the corresponding DCT of the sen-
tences from which they are extracted. Because the
normalization and comparison outputs are judged as
correct or incorrect, we report the performance of
these modules in accuracy (Acc) as shown in Ta-
ble 2. Overall, the experimental study shows that
all modules in our system are robust and achieve ex-
cellent performance.
31
Figure 2: A screenshot of the input panel.
Figure 3: A screenshot of the output panel.
4 The Demonstration
4.1 Visualization
We have implemented our system in a web-based
demo4. Figure 2 shows a screenshot of the input
panel of the system. The input panel includes a main
text box that allows users to input the text, and some
other input fields that allow users to customize the
system?s outputs. Among the fields, the reference
date serves as the document creation time (DCT) of
the input text. All temporal expressions captured
from the text will be normalized based on the ref-
erence date and compared also to the reference date
as illustrated in Figure 3.
4.2 Script Outline
First, we will give an overview of existing temporal
reasoning systems. Then we will introduce the novel
contributions of our system. After that, we will go
over our web-based demonstration, including (i) the
input panel: reference date and the text to be ana-
lyzed, and (ii) the output panel: the extracted basic
and extended temporal expressions, the normalized
intervals, and the comparison results.
5 Conclusions
In this demonstration paper, we introduced a tempo-
ral reasoning system that addresses three fundamen-
tal problems related to temporal expressions in text,
4http://cogcomp.cs.illinois.edu/page/demo view/TempSys
including extraction, normalization and comparison.
Our system consists of a temporal expression ex-
tractor capable of dealing with complex temporal
phrases, a time interval normalizer and a time inter-
val comparator. The experimental study shows that
our system achieves a high level of performance,
which will allow it to support other systems that re-
quire complicated temporal reasoning.
Acknowledgement
This research is supported by the Army Research Laboratory
(ARL) under agreement W911NF-09-2-0053 and the Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. The second author also
thanks the Vietnam Education Foundation (VEF) for its spon-
sorship. Any opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the view of the VEF, ARL, DARPA,
AFRL, or the US government.
References
Omar Alonso, Jannik Stro?tgen, Ricardo Baeza-Yates, and
Michael Gertz. 2011. Temporal information retrieval:
Challenges and opportunities. In TWAW.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac2011 knowledge base population
track. In TAC.
James Pustejovsky, Jose Castano, Robert Ingria, Roser
Sauri, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. Timeml: Robust specification of event
and temporal expressions in text. In IWCS-5.
D. Roth and D. Zelenko. 1998. Part of speech tagging us-
ing a network of linear separators. In COLING-ACL,
The 17th International Conference on Computational
Linguistics.
Jannik Stro?tgen and Michael Gertz. 2010. Heideltime:
High quality rule-based extraction and normalization
of temporal expressions. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation.
Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2010. Timely yago: harvesting,
querying, and visualizing temporal knowledge from
wikipedia. In EDBT.
32
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 989?998,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Starting From Scratch in Semantic Role Labeling
Michael Connor
University of Illinois
connor2@uiuc.edu
Yael Gertner
University of Illinois
ygertner@cyrus.psych.uiuc.edu
Cynthia Fisher
University of Illinois
cfisher@cyrus.psych.uiuc.edu
Dan Roth
University of Illinois
danr@illinois.edu
Abstract
A fundamental step in sentence compre-
hension involves assigning semantic roles
to sentence constituents. To accomplish
this, the listener must parse the sentence,
find constituents that are candidate argu-
ments, and assign semantic roles to those
constituents. Each step depends on prior
lexical and syntactic knowledge. Where
do children learning their first languages
begin in solving this problem? In this pa-
per we focus on the parsing and argument-
identification steps that precede Seman-
tic Role Labeling (SRL) training. We
combine a simplified SRL with an un-
supervised HMM part of speech tagger,
and experiment with psycholinguistically-
motivated ways to label clusters resulting
from the HMM so that they can be used
to parse input for the SRL system. The
results show that proposed shallow rep-
resentations of sentence structure are ro-
bust to reductions in parsing accuracy, and
that the contribution of alternative repre-
sentations of sentence structure to suc-
cessful semantic role labeling varies with
the integrity of the parsing and argument-
identification stages.
1 Introduction
In this paper we present experiments with an au-
tomatic system for semantic role labeling (SRL)
that is designed to model aspects of human lan-
guage acquisition. This simplified SRL system is
inspired by the syntactic bootstrapping theory, and
by an account of syntactic bootstrapping known
as ?structure-mapping? (Fisher, 1996; Gillette et
al., 1999; Lidz et al, 2003). Syntactic bootstrap-
ping theory proposes that young children use their
very partial knowledge of syntax to guide sen-
tence comprehension. The structure-mapping ac-
count makes three key assumptions: First, sen-
tence comprehension is grounded by the acquisi-
tion of an initial set of concrete nouns. Nouns are
arguably less dependent on prior linguistic knowl-
edge for their acquisition than are verbs; thus chil-
dren are assumed to be able to identify the refer-
ents of some nouns via cross-situational observa-
tion (Gillette et al, 1999). Second, these nouns,
once identified, yield a skeletal sentence structure.
Children treat each noun as a candidate argument,
and thus interpret the number of nouns in the sen-
tence as a cue to its semantic predicate-argument
structure (Fisher, 1996). Third, children represent
sentences in an abstract format that permits gener-
alization to new verbs (Gertner et al, 2006).
The structure-mapping account of early syn-
tactic bootstrapping makes strong predictions, in-
cluding predictions of tell-tale errors. In the sen-
tence ?Ellen and John laughed?, an intransitive
verb appears with two nouns. If young chil-
dren rely on representations of sentences as sim-
ple as an ordered set of nouns, then they should
have trouble distinguishing such sentences from
transitive sentences. Experimental evidence sug-
gests that they do: 21-month-olds mistakenly in-
terpreted word order in sentences such as ?The girl
and the boy kradded? as conveying agent-patient
roles (Gertner and Fisher, 2006).
Previous computational experiments with a
system for automatic semantic role labeling
(BabySRL: (Connor et al, 2008)) showed that
it is possible to learn to assign basic semantic
roles based on the shallow sentence representa-
tions proposed by the structure-mapping view.
Furthermore, these simple structural features were
robust to drastic reductions in the integrity of
the semantic-role feedback (Connor et al, 2009).
These experiments showed that representations of
sentence structure as simple as ?first of two nouns?
are useful, but the experiments relied on perfect
989
knowledge of arguments and predicates as a start
to classification.
Perfect built-in parsing finesses two problems
facing the human learner. The first problem in-
volves classifying words by part-of-speech. Pro-
posed solutions to this problem in the NLP and
human language acquisition literatures focus on
distributional learning as a key data source (e.g.,
(Mintz, 2003; Johnson, 2007)). Importantly,
infants are good at learning distributional pat-
terns (Gomez and Gerken, 1999; Saffran et al,
1996). Here we use a fairly standard Hidden
Markov Model (HMM) to generate clusters of
words that occur in similar distributional contexts
in a corpus of input sentences.
The second problem facing the learner is
more contentious: Having identified clusters of
distributionally-similar words, how do children
figure out what role these clusters of words should
play in a sentence interpretation system? Some
clusters contain nouns, which are candidate ar-
guments; others contain verbs, which take argu-
ments. How is the child to know which are which?
In order to use the output of the HMM tagger to
process sentences for input to an SRL model, we
must find a way to automatically label the clusters.
Our strategies for automatic argument and pred-
icate identification, spelled out below, reflect core
claims of the structure-mapping theory: (1) The
meanings of some concrete nouns can be learned
without prior linguistic knowledge; these concrete
nouns are assumed based on their meanings to be
possible arguments; (2) verbs are identified, not
primarily by learning their meanings via observa-
tion, but rather by learning about their syntactic
argument-taking behavior in sentences.
By using the HMM part-of-speech tagger in this
way, we can ask how the simple structural fea-
tures that we propose children start with stand up
to reductions in parsing accuracy. In doing so, we
move to a parser derived from a particular theoret-
ical account of how the human learner might clas-
sify words, and link them into a system for sen-
tence comprehension.
2 Model
We model language learning as a Semantic Role
Labeling (SRL) task (Carreras and Ma`rquez,
2004). This allows us to ask whether a learner,
equipped with particular theoretically-motivated
representations of the input, can learn to under-
stand sentences at the level of who did what to
whom. The architecture of our system is similar
to a previous approach to modeling early language
acquisition (Connor et al, 2009), which is itself
based on the standard architecture of a full SRL
system (e.g. (Punyakanok et al, 2008)).
This basic approach follows a multi-stage
pipeline, with each stage feeding in to the next.
The stages are: (1) Parsing the sentence, (2) Iden-
tifying potential predicates and arguments based
on the parse, (3) Classifying role labels for each
potential argument relative to a predicate, (4) Ap-
plying constraints to find the best labeling of ar-
guments for a sentence. In this work we attempt
to limit the knowledge available at each stage to
the automatic output of the previous stage, con-
strained by knowledge that we argue is available
to children in the early stages of language learn-
ing.
In the parsing stage we use an unsupervised
parser based on Hidden Markov Models (HMM),
modeling a simple ?predict the next word? parser.
Next the argument identification stage identifies
HMM states that correspond to possible argu-
ments and predicates. The candidate arguments
and predicates identified in each input sentence are
passed to an SRL classifier that uses simple ab-
stract features based on the number and order of
arguments to learn to assign semantic roles.
As input to our learner we use samples of
natural child directed speech (CDS) from the
CHILDES corpora (MacWhinney, 2000). During
initial unsupervised parsing we experiment with
incorporating knowledge through a combination
of statistical priors favoring a skewed distribution
of words into classes, and an initial hard cluster-
ing of the vocabulary into function and content
words. The argument identifier uses a small set
of frequent nouns to seed argument states, relying
on the assumptions that some concrete nouns can
be learned as a prerequisite to sentence interpreta-
tion, and are interpreted as candidate arguments.
The SRL classifier starts with noisy largely un-
supervised argument identification, and receives
feedback based on annotation in the PropBank
style; in training, each word identified as an argu-
ment receives the true role label of the phrase that
word is part of. This represents the assumption
that learning to interpret sentences is naturally su-
pervised by the fit of the learner?s predicted mean-
ing with the referential context. The provision
990
of perfect ?gold-standard? feedback over-estimates
the real child?s access to this supervision, but al-
lows us to investigate the consequences of noisy
argument identification for SRL performance. We
show that even with imperfect parsing, a learner
can identify useful abstract patterns for sentence
interpretation. Our ultimate goal is to ?close the
loop? of this system, by using learning in the SRL
system to improve the initial unsupervised parse
and argument identification.
The training data were samples of parental
speech to three children (Adam, Eve, and
Sarah; (Brown, 1973)), available via CHILDES.
The SRL training corpus consists of parental utter-
ances in samples Adam 01-20 (child age 2;3 - 3;1),
Eve 01-18 (1;6 - 2;2), and Sarah 01-83 (2;3 - 3;11).
All verb-containing utterances without symbols
indicating disfluencies were automatically parsed
with the Charniak parser (Charniak, 1997), anno-
tated using an existing SRL system (Punyakanok
et al, 2008) and then errors were hand-corrected.
The final annotated sample contains about 16,730
propositions, with 32,205 arguments.
3 Unsupervised Parsing
As a first step of processing, we feed the learner
large amounts of unlabeled text and expect it to
learn some structure over this data that will facil-
itate future processing. The source of this text
is child directed speech collected from various
projects in the CHILDES repository1. We re-
moved sentences with fewer than three words or
markers of disfluency. In the end we used 160
thousand sentences from this set, totaling over 1
million tokens and 10 thousand unique words.
The goal of the parsing stage is to give the
learner a representation permitting it to generalize
over word forms. The exact parse we are after is
a distributional and context-sensitive clustering of
words based on sequential processing. We chose
an HMM based parser for this since, in essence
the HMM yields an unsupervised POS classifier,
but without names for states. An HMM trained
with expectation maximization (EM) is analogous
to a simple process of predicting the next word in a
stream and correcting connections accordingly for
each sentence.
1We used parts of the Bloom (Bloom, 1970; Bloom,
1973), Brent (Brent and Siskind, 2001), Brown (Brown,
1973), Clark (Clark, 1978), Cornell, MacWhin-
ney (MacWhinney, 2000), Post (Demetras et al, 1986)
and Providence (Demuth et al, 2006) collections.
With HMM we can also easily incorporate ad-
ditional knowledge during parameter estimation.
The first (and simplest) parser we used was an
HMM trained using EM with 80 hidden states.
The number of hidden states was made relatively
large to increase the likelihood of clusters corre-
sponding to a single part of speech, while preserv-
ing some degree of generalization.
Johnson (2007) observed that EM tends to cre-
ate word clusters of uniform size, which does
not reflect the way words cluster into parts of
speech in natural languages. The addition of pri-
ors biasing the system toward a skewed alloca-
tion of words to classes can help. The second
parser was an 80 state HMM trained with Varia-
tional Bayes EM (VB) incorporating Dirichlet pri-
ors (Beal, 2003).2
In the third and fourth parsers we experi-
ment with enriching the HMM POS-tagger with
other psycholinguistically plausible knowledge.
Words of different grammatical categories dif-
fer in their phonological as well as in their dis-
tributional properties (e.g., (Kelly, 1992; Mon-
aghan et al, 2005; Shi et al, 1998)); combining
phonological and distributional information im-
proves the clustering of words into grammatical
categories. The phonological difference between
content and function words is particularly strik-
ing (Shi et al, 1998). Even newborns can cate-
gorically distinguish content and function words,
based on the phonological difference between the
two classes (Shi et al, 1999). Human learners may
treat content and function words as distinct classes
from the start.
To implement this division into function and
content words3, we start with a list of function
word POS tags4 and then find words that appear
predominantly with these POS tags, using tagged
WSJ data (Marcus et al, 1993). We allocated a
fixed number of states for these function words,
and left the rest of the states for the rest of the
words. This amounts to initializing the emission
matrix for the HMM with a block structure; words
from one class cannot be emitted by states al-
located to the other class. This trick has been
used before in speech recognition work (Rabiner,
2We tuned the prior using the same set of 8 value pairs
suggested by Gao and Johnson (2008), using a held out set of
POS-tagged CDS to evaluate final performance.
3We also include a small third class for punctuation,
which is discarded.
4TO,IN,EX,POS,WDT,PDT,WRB,MD,CC,DT,RP,UH
991
1989), and requires far fewer resources than the
full tagging dictionary that is often used to intel-
ligently initialize an unsupervised POS classifier
(e.g. (Brill, 1997; Toutanova and Johnson, 2007;
Ravi and Knight, 2009)).
Because the function and content word preclus-
tering preceded parameter estimation, it can be
combined with either EM or VB learning. Al-
though this initial split forces sparsity on the emis-
sion matrix and allows more uniform sized clus-
ters, Dirichlet priors may still help, if word clus-
ters within the function or content word subsets
vary in size and frequency. The third parser was
an 80 state HMM trained with EM estimation,
with 30 states pre-allocated to function words;
the fourth parser was the same except that it was
trained with VB EM.
3.1 Parser Evaluation
 3.2
 3.4
 3.6
 3.8
 4
 4.2
 4.4
 4.6
 4.8
 5
 5.2
 100  1000  10000  100000  1e+06
Va
ria
tio
n 
of
 In
fo
rm
at
io
n
Training Sentences
EM
VB
EM+Funct
VB+Funct
Figure 1: Unsupervised Part of Speech results, match-
ing states to gold POS labels. All systems use 80 states, and
comparison is to gold labeled CDS text, which makes up a
subset of the HMM training data. Variation of Information is
an information-theoretic measure summing mutual informa-
tion between tags and states, proposed by (Meila?, 2002), and
first used for Unsupervised Part of Speech in (Goldwater and
Griffiths, 2007). Smaller numbers are better, indicating less
information lost in moving from the HMM states to the gold
POS tags. Note that incorporating function word precluster-
ing allows both EM and VB algorithms to achieve the same
performance with an order of magnitude fewer sentences.
We first evaluate these parsers (the first stage
of our SRL system) on unsupervised POS tag-
ging. Figure 1 shows the performance of the four
systems using Variation of Information to mea-
sure match between gold states and unsupervised
parsers as we vary the amount of text they receive.
Each point on the graph represents the average re-
sult over 10 runs of the HMM with different sam-
ples of the unlabeled CDS. Another common mea-
sure for unsupervised POS (when there are more
states than tags) is a many to one greedy mapping
of states to tags. It is known that EM gives a better
many to one score than VB trained HMM (John-
son, 2007), and likewise we see that here: with
all data EM gives 0.75 matching, VB gives 0.74,
while both EM+Funct and VB+Funct reach 0.80.
Adding the function/content word split to the
HMM structure improves both EM and VB esti-
mation in terms of both tag matching accuracy and
information. However, these measures look at the
parser only in isolation. What is more important to
us is how useful the provided word clusters are for
future semantic processing. In the next sections
we use the outputs of our four parsers to identify
arguments and predicates.
4 Argument Identification
The unsupervised parser provides a state label for
each word in each sentence; the goal of the ar-
gument identification stage is to use these states
to label words as potential arguments, predicates
or neither. As described in the introduction, core
premises of the structure-mapping account offer
routes whereby we could label some HMM states
as argument or predicate states.
The structure-mapping account holds that sen-
tence comprehension is grounded in the learning
of an initial set of nouns. Children are assumed
to identify the referents of some concrete nouns
via cross-situational learning (Gillette et al, 1999;
Smith and Yu, 2008). Children then assume, by
virtue of the meanings of these nouns, that they are
candidate arguments. This is a simple form of se-
mantic bootstrapping, requiring the use of built-in
links between semantics and syntax to identify the
grammatical type of known words (Pinker, 1984).
We use a small set of known nouns to transform
unlabeled word clusters into candidate arguments
for the SRL: HMM states that are dominated by
known names for animate or inanimate objects are
assumed to be argument states.
Given text parsed by the HMM parser and a
list of known nouns, the argument identifier pro-
ceeds in multiple steps as illustrated in figure 2.
The first stage identifies as argument states those
states that appear at least half the time in the train-
ing data with known nouns. This use of a seed
list and distributional clustering is similar to Proto-
type Driven Learning (Haghighi and Klein, 2006),
except we are only providing information on one
specific class.
992
Algorithm ARGUMENT STATE IDENTIFICATION
INPUT: Parsed Text T = list of (word, state) pairs
Set of concrete nouns N
OUTPUT: Set of argument states A
Argument count likelihood ArgLike(s, c)
Identify Argument States
Let freq(s) = |{(?, s) ? T}|
Let freqN (s) = |{(w, s) ? T |w ? N}|
For each s:
If freqN (s) ? freq(s)/2
Add s to A
Collect Per Sentence Argument Count statistics
For each Sentence S ? T :
Let Arg(S) = |{(w, s) ? S|s ? A}|
For (w, s) ? S s.t. s /? A
Increment ArgCount(s, Arg(S))
For each s /? A, and argument count c:
ArgLike(s, c) = ArgCount(s, c)/freq(s)
(a) Argument Identification
Algorithm PREDICATE STATE IDENTIFICATION
INPUT: Parsed Sentence S = list of (word, state) pairs
Set of argument states A
Sentence Argument Count ArgLike(s, c)
OUTPUT: Most likely predicate (v, sv)
Find Number of arguments in sentence
Let Arg(S) = |{(w, s) ? S|s ? A}|
Find Non-argument state in sentence most likely
to appear with this number of arguments
(v, sv) = argmax(w,s)?SArgLike(s, Arg(S))
(b) Predicate Identification
Figure 2: Argument identification algorithm. This is a two
stage process: argument state identification based on statis-
tics collected over entire text and per sentence predicate iden-
tification.
As a list of known nouns we collected all those
nouns that appear three times or more in the child
directed speech training data and judged to be ei-
ther animate or inanimate nouns. The full set of
365 nouns covers over 93% of noun occurences
in our data. In upcoming sections we experiment
with varying the number of seed nouns used from
this set, selecting the most frequent set of nouns.
Reflecting the spoken nature of the child directed
speech, the most frequent nouns are pronouns,
but beyond the top 10 we see nouns naming peo-
ple (?daddy?, ?ursula?) and object nouns (?chair?,
?lunch?).
What about verbs? A typical SRL model iden-
tifies candidate arguments and tries to assign roles
to them relative to each verb in the sentence. In
principle one might suppose that children learn
the meanings of verbs via cross-situational ob-
servation just as they learn the meanings of con-
crete nouns. But identifying the meanings of
verbs is much more troublesome. Verbs? mean-
ings are abstract, therefore harder to identify based
on scene information alone (Gillette et al, 1999).
As a result, early vocabularies are dominated by
nouns (Gentner, 2006). On the structure-mapping
account, learners identify verbs, and begin to de-
termine their meanings, based on sentence struc-
ture cues. Verbs take noun arguments; thus, learn-
ers could learn which words are verbs by detect-
ing each verb?s syntactic argument-taking behav-
ior. Experimental evidence provides some support
for this procedure: 2-year-olds keep track of the
syntactic structures in which a new verb appears,
even without a concurrent scene that provides cues
to the verb?s semantic content (Yuan and Fisher,
2009).
We implement this behavior by identifying as
predicate states the HMM states that appear com-
monly with a particular number of previously
identified arguments. First, we collect statistics
over the entire HMM training corpus regarding
how many arguments are identified per sentence,
and which states that are not identified as argu-
ment states appear with each number of argu-
ments. Next, for each parsed sentence that serves
as SRL input, the algorithm chooses as the most
likely predicate the word whose state is most likely
to appear with the number of arguments found in
the current input sentence. Note that this algo-
rithm assumes exactly one predicate per sentence.
Implicitly, the argument count likelihood divides
predicate states up into transitive and intransitive
predicates based on appearances in the simple sen-
tences of CDS.
4.1 Argument Identification Evaluation
Figure 3 shows argument and predicate identifi-
cation accuracy for each of the four parsers when
provided with different numbers of known nouns.
The known word list is very skewed with its most
frequent members dominating the total noun oc-
currences in the data. The ten most frequent
words5 account for 60% of the total noun occur-
rences. We achieve the different occurrence cov-
erage numbers of figure 3 by using the most fre-
quent N words from the list that give the specific
coverage6. Pronouns refer to people or objects,
but are abstract in that they can refer to any person
or object. The inclusion of pronouns in our list of
5you, it, I, what, he, me, ya, she, we, her
6N of 5, 10, 30, 83, 227 cover 50%, 60%, 70%, 80%,
90% of all noun occurrences
993
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.45  0.5  0.55  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95
F1
%Noun Occurences Covered
EM
VB
EM+Funct
VB+Funct
Figure 3: Effect of number of concrete nouns for seeding
argument identification with various unsupervised parsers.
Argument identification accuracy is computed against true ar-
gument boundaries from hand labeled data. The upper set of
results show primary argument (A0-4) identification F1, and
bottom lines show predicate identification F1.
known nouns represents the assumption that tod-
dlers have already identified pronouns as referen-
tial terms. Even 19-month-olds assign appropri-
ately different interpretations to novel verbs pre-
sented in simple transitive versus intransitive sen-
tences with pronoun arguments (?He?s kradding
him!? vs. ?He?s kradding!?; (Yuan et al, 2007)).
In ongoing work we experiment with other meth-
ods of identifying seed nouns.
Two groups of curves appear in figure 3: the
upper group shows the primary argument iden-
tification accuracy and the bottom group shows
the predicate identification accuracy. We evaluate
compared to gold tagged data with true argument
and predicate boundaries. The primary argument
(A0-4) identification accuracy is the F1 value, with
precision calculated as the proportion of identified
arguments that appear as part of a true argument,
and recall as the proportion of true arguments that
have some state identified as an argument. F1 is
calculated similarly for predicate identification, as
one state per sentence is identified as the predicate.
As shown in figure 3, argument identification F1
is higher than predicate identification (which is to
be expected, given that predicate identification de-
pends on accurate arguments), and as we add more
seed nouns the argument identification improves.
Surprisingly, despite the clear differences in un-
supervised POS performance seen in figure 1, the
different parsers do not yield very different argu-
ment and predicate identification. As we will see
in the next section, however, when the arguments
identified in this step are used to train SRL clas-
sifier, distinctions between parsers reappear, sug-
gesting that argument identification F1 masks sys-
tematic patterns in the errors.
5 Testing SRL Performance
Finally, we used the results of the previous pars-
ing and argument-identification stages in training
a simplified SRL classifier (BabySRL), equipped
with sets of features derived from the structure-
mapping account. For argument classification we
used a linear classifier trained with a regularized
perceptron update rule (Grove and Roth, 2001).
In the results reported below the BabySRL did
not use sentence-level inference for the final clas-
sification, every identified argument is classified
independently; thus multiple nouns can have the
same role. In what follows, we compare the per-
formance of the BabySRL across the four parsers.
We evaluated SRL performance by testing the
BabySRL with constructed sentences like those
used for the experiments with children described
in the Introduction. All test sentences contained a
novel verb, to test the model?s ability to general-
ize.
We examine the performance of four versions
of the BabySRL, varying in the features used to
represent sentences. All four versions include
lexical features consisting of the target argument
and predicate (as identified in the previous steps).
The baseline model has only these lexical features
(Lexical). Following Connor et al (2008; 2009),
the key feature type we propose is noun pattern
features (NounPat). Noun pattern features indi-
cate how many nouns there are in the sentence and
which noun the target is. For example, in ?You
dropped it!?, ?you? has a feature active indicating
that it is the first of two nouns, while ?it? has a fea-
ture active indicating that it is the second of two
nouns. We compared the behavior of noun pat-
tern features to another simple representation of
word order, position relative to the verb (VerbPos).
In the same example sentence, ?you? has a feature
active indicating that it is pre-verbal; for ?it? a fea-
ture is active indicating that it is post-verbal. A
fourth version of the BabySRL (Combined) used
both NounPat and VerbPos features.
We structured our tests of the BabySRL to test
the predictions of the structure-mapping account.
(1) NounPat features will improve the SRL?s abil-
ity to interpret simple transitive test sentences
containing two nouns and a novel verb, relative
994
to a lexical baseline. Like 21-month-old chil-
dren (Gertner et al, 2006), the SRL should inter-
pret the first noun as an agent and the second as
a patient. (2) Because NounPat features represent
word order solely in terms of a sequence of nouns,
an SRL equipped with these features will make the
errors predicted by the structure-mapping account
and documented in children (Gertner and Fisher,
2006). (3) NounPat features permit the SRL to
assign different roles to the subjects of transitive
and intransitive sentences that differ in their num-
ber of nouns. This effect follows from the nature
of the NounPat features: These features partition
the training data based on the number of nouns,
and therefore learn separately the likely roles of
the ?1st of 1 noun? and the ?1st of 2 nouns?.
These patterns contrast with the behavior of the
VerbPos features: When the BabySRL was trained
with perfect parsing, VerbPos promoted agent-
patient interpretations of transitive test sentences,
and did so even more successfully than Noun-
Pat features did, reflecting the usefulness of po-
sition relative to the verb in understanding English
sentences. In addition, VerbPos features elimi-
nated the errors with two-noun intransitive sen-
tences. Given test sentences such as ?You and
Mommy krad?, VerbPos features represented both
nouns as pre-verbal, and therefore identified both
as likely agents. However, VerbPos features did
not help the SRL assign different roles to the
subjects of simple transitive and intransitive sen-
tences: ?Mommy? in ?Mommy krads you? and
?Mommy krads? are both represented simply as
pre-verbal.
To test the system?s predictions on transitive and
intransitive two noun sentences, we constructed
two test sentence templates: ?A krads B? and ?A
and B krad?, where A and B were replaced with
familiar animate nouns. The animate nouns were
selected from all three children?s data in the train-
ing set and paired together in the templates such
that all pairs are represented.
Figure 4 shows SRL performance on test sen-
tences containing a novel verb and two animate
nouns. Each plot shows the proportion of test sen-
tences that were assigned an agent-patient (A0-
A1) role sequence; this sequence is correct for
transitive sentences but is an error for two-noun
intransitive sentences. Each group of bars shows
the performance of the BabySRL trained using one
of the four parsers, equipped with each of our four
feature sets. The top and bottom panels in Figure 4
differ in the number of nouns provided to seed the
argument identification stage. The top row shows
performance with 10 seed nouns (the 10 most fre-
quent nouns, mostly animate pronouns), and the
bottom row shows performance with 365 concrete
(animate or inanimate) nouns treated as known.
Relative to the lexical baseline, NounPat features
fared well: they promoted the assignment of A0-
A1 interpretations to transitive sentences, across
all parser versions and both sets of known nouns.
Both VB estimation and the content-function word
split increased the ability of NounPat features to
learn that the first of two nouns was an agent, and
the second a patient. The NounPat features also
promote the predicted error with two-noun intran-
sitive sentences (Figures 4(b), 4(d)). Despite the
relatively low accuracy of predicate identification
noted in section 4.1, the VerbPos features did suc-
ceed in promoting an A0A1 interpretation for tran-
sitive sentences containing novel verbs relative to
the lexical baseline. In every case the performance
of the Combined model that includes both Noun-
Pat and VerbPos features exceeds the performance
of either NounPat or VerbPos alone, suggesting
both contribute to correct predictions for transitive
sentences. However, the performance of VerbPos
features did not improve with parsing accuracy as
did the performance of the NounPat features. Most
strikingly, the VerbPos features did not eliminate
the predicted error with two-noun intransitive sen-
tences, as shown in panels 4(b) and 4(d). The
Combined model predicted an A0A1 sequence for
these sentences, showing no reduction in this error
due to the participation of VerbPos features.
Table 1 shows SRL performance on the same
transitive test sentences (?A krads B?), compared
to simple one-noun intransitive sentences (?A
krads?). To permit a direct comparison, the table
reports the proportion of transitive test sentences
for which the first noun was assigned an agent
(A0) interpretation, and the proportion of intran-
sitive test sentences with the agent (A0) role as-
signed to the single noun in the sentence. Here we
report only the results from the best-performing
parser (trained with VB EM, and content/function
word pre-clustering), compared to the same clas-
sifiers trained with gold standard argument iden-
tification. When trained on arguments identified
via the unsupervised POS tagger, noun pattern
features promoted agent interpretations of tran-
995
Two Noun Transitive, % Agent First One Noun Intransitive, % Agent Prediction
Lexical NounPat VerbPos Combine Lexical NounPat VerbPos Combine
VB+Funct 10 seed 0.48 0.61 0.55 0.71 0.48 0.57 0.56 0.59
VB+Funct 365 seed 0.22 0.64 0.41 0.74 0.23 0.33 0.43 0.41
Gold Arguments 0.16 0.41 0.69 0.77 0.17 0.18 0.70 0.58
Table 1: SRL result comparison when trained with best unsupervised argument identifier versus trained with gold arguments.
Comparison is between agent first prediction of two noun transitive sentences vs. one noun intransitive sentences. The unsu-
pervised arguments lead the classifier to rely more on noun pattern features; when the true arguments and predicate are known
the verb position feature leads the classifier to strongly indicate agent first in both settings.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
EM VB EM+Funct VB+Funct Gold
%
A0
A1
Lexical
NounPat
VerbPos
Combine
(a) Two Noun Transitive Sentence, 10 seed nouns
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
EM VB EM+Funct VB+Funct Gold
%
A0
A1
Lexical
NounPat
VerbPos
Combine
(b) Two Noun Intransitive Sentence, 10 seed nouns
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
EM VB EM+Funct VB+Funct Gold
%
A0
A1
Lexical
NounPat
VerbPos
Combine
(c) Two Noun Transitive Sentence, 365 seed nouns
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
EM VB EM+Funct VB+Funct Gold
%
A0
A1
Lexical
NounPat
VerbPos
Combine
(d) Two Noun Intransitive Sentence, 365 seed nouns
Figure 4: SRL classification performance on transitive and intransitive test sentences containing two nouns and a novel
verb. Performance with gold-standard argument identification is included for comparison. Across parses, noun pattern features
promote agent-patient (A0A1) interpretations of both transitive (?You krad Mommy?) and two-noun intransitive sentences
(?You and Mommy krad?); the latter is an error found in young children. Unsupervised parsing is less accurate in identifying
the verb, so verb position features fail to eliminate errors with two-noun intransitive sentences.
sitive subjects, but not for intransitive subjects.
This differentiation between transitive and intran-
sitive sentences was clearer when more known
nouns were provided. Verb position features, in
contrast, promote agent interpretations of subjects
weakly with unsupervised argument identification,
but equally for transitive and intransitive.
Noun pattern features were robust to increases
in parsing noise. The behavior of verb position
features suggests that variations in the identifiabil-
ity of different parts of speech can affect the use-
fulness of alternative representations of sentence
structure. Representations that reflect the posi-
tion of the verb may be powerful guides for un-
derstanding simple English sentences, but repre-
sentations reflecting only the number and order of
nouns can dominate early in acquisition, depend-
ing on the integrity of parsing decisions.
6 Conclusion and Future Work
The key innovation in the present work is the
combination of unsupervised part-of-speech tag-
ging and argument identification to permit learn-
ing in a simplified SRL system. Children do not
996
have the luxury of treating part-of-speech tagging
and semantic role labeling as separable tasks. In-
stead, they must learn to understand sentences
starting from scratch, learning the meanings of
some words, and using those words and their pat-
terns of arrangement into sentences to bootstrap
their way into more mature knowledge.
We have created a first step toward modeling
this incremental process. We combined unsuper-
vised parsing with minimal supervision to begin to
identify arguments and predicates. An SRL clas-
sifier used simple representations built from these
identified arguments to extract useful abstract pat-
terns for classifying semantic roles. Our results
suggest that multiple simple representations of
sentence structure could co-exist in the child?s sys-
tem for sentence comprehension; representations
that will ultimately turn out to be powerful guides
to role identification may be less powerful early in
acquisition because of the noise introduced by the
unsupervised parsing.
The next step is to ?close the loop?, using higher
level semantic feedback to improve the earlier ar-
gument identification and parsing stages. Per-
haps with the help of semantic feedback the sys-
tem can automatically improve predicate identifi-
cation, which in turn allows it to correct the ob-
served intransitive sentence error. This approach
will move us closer to the goal of using initial sim-
ple structural patterns and natural observation of
the world (semantic feedback) to bootstrap more
and more sophisticated representations of linguis-
tic structure.
Acknowledgments
This research is supported by NSF grant BCS-
0620257 and NIH grant R01-HD054448.
References
M.J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience Unit, University Col-
lege London.
L. Bloom. 1970. Language development: Form and
function in emerging grammars. MIT Press, Cam-
bridge, MA.
L. Bloom. 1973. One word at a time: The use of
single-word utterances before syntax. Mouton, The
Hague.
M.R. Brent and J.M. Siskind. 2001. The role of expo-
sure to isolated words in early vocabulary develop-
ment. Cognition, 81:31?44.
E. Brill. 1997. Unsupervised learning of disambigua-
tion rules for part of speech tagging. In Natural
Language Processing Using Very Large Corpora.
Kluwer Academic Press.
R. Brown. 1973. A First Language. Harvard Univer-
sity Press, Cambridge, MA.
X. Carreras and L. Ma`rquez. 2004. Introduction to
the CoNLL-2004 shared tasks: Semantic role label-
ing. In Proceedings of CoNLL-2004, pages 89?97.
Boston, MA, USA.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. National
Conference on Artificial Intelligence.
E.V. Clark. 1978. Awwareness of language: Some ev-
idence from what children say and do. In R. J. A.
Sinclair and W. Levelt, editors, The child?s concep-
tion of language. Springer Verlag, Berlin.
M. Connor, Y. Gertner, C. Fisher, and D. Roth. 2008.
Baby srl: Modeling early language acquisition. In
Proc. of the Annual Conference on Computational
Natural Language Learning (CoNLL), pages xx?yy,
Aug.
M. Connor, Y. Gertner, C. Fisher, and D. Roth.
2009. Minimally supervised model of early lan-
guage acquisition. In Proc. of the Annual Confer-
ence on Computational Natural Language Learning
(CoNLL), Jun.
M. Demetras, K. Post, and C. Snow. 1986. Feedback
to first-language learners. Journal of Child Lan-
guage, 13:275?292.
K. Demuth, J. Culbertson, and J. Alter. 2006. Word-
minimality, epenthesis, and coda licensing in the ac-
quisition of english. Language & Speech, 49:137?
174.
C. Fisher. 1996. Structural limits on verb mapping:
The role of analogy in children?s interpretation of
sentences. Cognitive Psychology, 31:41?81.
Jianfeng Gao and Mark Johnson. 2008. A compar-
ison of bayesian estimators for unsupervised hid-
den markov model pos taggers. In Proceedings of
EMNLP-2008, pages 344?352.
D. Gentner. 2006. Why verbs are hard to learn. In
K. Hirsh-Pasek and R. Golinkoff, editors, Action
meets word: How children learn verbs, pages 544?
564. Oxford University Press.
Y. Gertner and C. Fisher. 2006. Predicted errors in
early verb learning. In 31st Annual Boston Univer-
sity Conference on Language Development.
997
Y. Gertner, C. Fisher, and J. Eisengart. 2006. Learning
words and rules: Abstract knowledge of word or-
der in early sentence comprehension. Psychological
Science, 17:684?691.
J. Gillette, H. Gleitman, L. R. Gleitman, and A. Led-
erer. 1999. Human simulations of vocabulary learn-
ing. Cognition, 73:135?176.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of 45th Annual Meeting of
the Association of Computational Linguists, pages
744?751.
R. Gomez and L. Gerken. 1999. Artificial grammar
learning by 1-year-olds leads to specific and abstract
knowledge. Cognition, 70:109?135.
A. Haghighi and D. Klein. 2006. Prototype-drive
learning for sequence models. In Proceedings of
NAACL-2006, pages 320?327.
Mark Johnson. 2007. Why doesnt em find good hmm
pos-taggers? In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 296?305.
M.H. Kelly. 1992. Using sound to solve syntac-
tic problems: The role of phonology in grammat-
ical category assignments. Psychological Review,
99:349?364.
J. Lidz, H. Gleitman, and L. R. Gleitman. 2003. Un-
derstanding how input matters: verb learning and the
footprint of universal grammar. Cognition, 87:151?
178.
B. MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Third Edition. Lawrence Elr-
baum Associates, Mahwah, NJ.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330, June.
Marina Meila?. 2002. Comparing clusterings. Techni-
cal Report 418, University of Washington Statistics
Department.
T. Mintz. 2003. Frequent frames as a cue for grammat-
ical categories in child directed speech. Cognition,
90:91?117.
P. Monaghan, N. Chater, and M.H. Christiansen. 2005.
The differential role of phonological and distribu-
tional cues in grammatical categorisation. Cogni-
tion, 96:143?182.
S. Pinker. 1984. Language learnability and language
development. Harvard University Press, Cambridge,
MA.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2).
L. R. Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?285.
Sujith Ravi and Kevin Knight. 2009. Minimized
models for unsupervised part-of-speech tagging. In
Proceedings of the Joint Conferenceof the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP).
J.R. Saffran, R.N. Aslin, and E.L. Newport. 1996. Sta-
tistical learning by 8-month-old infants. Science,
274:1926?1928.
Rushen Shi, James L. Morgan, and Paul Allopenna.
1998. Phonological and acoustic bases for ear-
liest grammatical category assignment: a cross-
linguistic perspective. Journal of Child Language,
25(01):169?201.
Rushen Shi, Janet F. Werker, and James L. Morgan.
1999. Newborn infants? sensitivity to perceptual
cues to lexical and grammatical words. Cognition,
72(2):B11 ? B21.
L.B. Smith and C. Yu. 2008. Infants rapidly learn
word-referent mappings via cross-situational statis-
tics. Cognition, 106:1558?1568.
Kiristina Toutanova and Mark Johnson. 2007. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In Proceedings of NIPS.
S. Yuan and C. Fisher. 2009. ?really? she blicked
the baby??: Two-year-olds learn combinatorial facts
about verbs by listening. Psychological Science,
20:619?626.
S. Yuan, C. Fisher, Y. Gertner, and J. Snedeker. 2007.
Participants are more than physical bodies: 21-
month-olds assign relational meaning to novel tran-
sitive verbs. In Biennial Meeting of the Society for
Research in Child Development, Boston, MA.
998
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1199?1208,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
?Ask not what Textual Entailment can do for You...?
Mark Sammons V.G.Vinod Vydiswaran Dan Roth
University of Illinois at Urbana-Champaign
{mssammon|vgvinodv|danr}@illinois.edu
Abstract
We challenge the NLP community to par-
ticipate in a large-scale, distributed effort
to design and build resources for devel-
oping and evaluating solutions to new and
existing NLP tasks in the context of Rec-
ognizing Textual Entailment. We argue
that the single global label with which
RTE examples are annotated is insufficient
to effectively evaluate RTE system perfor-
mance; to promote research on smaller, re-
lated NLP tasks, we believe more detailed
annotation and evaluation are needed, and
that this effort will benefit not just RTE
researchers, but the NLP community as
a whole. We use insights from success-
ful RTE systems to propose a model for
identifying and annotating textual infer-
ence phenomena in textual entailment ex-
amples, and we present the results of a pi-
lot annotation study that show this model
is feasible and the results immediately use-
ful.
1 Introduction
Much of the work in the field of Natural Lan-
guage Processing is founded on an assumption
of semantic compositionality: that there are iden-
tifiable, separable components of an unspecified
inference process that will develop as research
in NLP progresses. Tasks such as Named En-
tity and coreference resolution, syntactic and shal-
low semantic parsing, and information and rela-
tion extraction have been identified as worthwhile
tasks and pursued by numerous researchers. While
many have (nearly) immediate application to real
world tasks like search, many are also motivated
by their potential contribution to more ambitious
Natural Language tasks. It is clear that the compo-
nents/tasks identified so far do not suffice in them-
selves to solve tasks requiring more complex rea-
soning and synthesis of information; many other
tasks must be solved to achieve human-like perfor-
mance on tasks such as Question Answering. But
there is no clear process for identifying potential
tasks (other than consensus by a sufficient num-
ber of researchers), nor for quantifying their po-
tential contribution to existing NLP tasks, let alne
to Natural Language Understanding.
Recent ?grand challenges? such as Learning by
Reading, Learning To Read, andMachine Reading
are prompting more careful thought about the way
these tasks relate, and what tasks must be solved
in order to understand text sufficiently well to re-
liably reason with it. This is an appropriate time
to consider a systematic process for identifying
semantic analysis tasks relevant to natural lan-
guage understanding, and for assessing their
potential impact on NLU system performance.
Research on Recognizing Textual Entailment
(RTE), largely motivated by a ?grand challenge?
now in its sixth year, has already begun to address
some of the problems identified above. Tech-
niques developed for RTE have now been suc-
cessfully applied in the domains of Question An-
swering (Harabagiu and Hickl, 2006) and Ma-
chine Translation (Pado et al, 2009), (Mirkin
et al, 2009). The RTE challenge examples are
drawn from multiple domains, providing a rel-
atively task-neutral setting in which to evaluate
contributions of different component solutions,
and RTE researchers have already made incremen-
tal progress by identifying sub-problems of entail-
ment, and developing ad-hoc solutions for them.
In this paper we challenge the NLP community
to contribute to a joint, long-term effort to iden-
tify, formalize, and solve textual inference prob-
lems motivated by the Recognizing Textual Entail-
ment setting, in the following ways:
(a) Making the Recognizing Textual Entailment
setting a central component of evaluation for
1199
relevant NLP tasks such as NER, Coreference,
parsing, data acquisition and application, and oth-
ers. While many ?component? tasks are consid-
ered (almost) solved in terms of expected improve-
ments in performance on task-specific corpora, it
is not clear that this translates to strong perfor-
mance in the RTE domain, due either to prob-
lems arising from unrelated, unsolved entailment
phenomena that co-occur in the same examples,
or to domain change effects. The RTE task of-
fers an application-driven setting for evaluating a
broad range of NLP solutions, and will reinforce
good practices by NLP researchers. The RTE
task has been designed specifically to exercise tex-
tual inference capabilities, in a format that would
make RTE systems potentially useful components
in other ?deep? NLP tasks such as Question An-
swering and Machine Translation. 1
(b) Identifying relevant linguistic phenomena,
interactions between phenomena, and their
likely impact on RTE/textual inference. Deter-
mining the correct label for a single textual en-
tailment example requires human analysts to make
many smaller, localized decisions which may de-
pend on each other. A broad, carefully conducted
effort to identify and annotate such local phenom-
ena in RTE corpora would allow their distributions
in RTE examples to be quantified, and allow eval-
uation of NLP solutions in the context of RTE. It
would also allow assessment of the potential im-
pact of a solution to a specific sub-problem on the
RTE task, and of interactions between phenomena.
Such phenomena will almost certainly correspond
to elements of linguistic theory; but this approach
brings a data-driven approach to focus attention on
those phenomena that are well-represented in the
RTE corpora, and which can be identified with suf-
ficiently close agreement.
(c) Developing resources and approaches that
allow more detailed assessment of RTE sys-
tems. At present, it is hard to know what spe-
cific capabilities different RTE systems have, and
hence, which aspects of successful systems are
worth emulating or reusing. An evaluation frame-
work that could offer insights into the kinds of
sub-problems a given system can reliably solve
would make it easier to identify significant ad-
vances, and thereby promote more rapid advances
1The Parser Training and Evaluation using Textual En-
tailment track of SemEval 2 takes this idea one step further,
by evaluating performance of an isolated NLP task using the
RTE methodology.
through reuse of successful solutions and focus on
unresolved problems.
In this paper we demonstrate that Textual En-
tailment systems are already ?interesting?, in that
they have made significant progress beyond a
?smart? lexical baseline that is surprisingly hard
to beat (section 2). We argue that Textual Entail-
ment, as an application that clearly requires so-
phisticated textual inference to perform well, re-
quires the solution of a range of sub-problems,
some familiar and some not yet known. We there-
fore propose RTE as a promising and worthwhile
task for large-scale community involvement, as it
motivates the study of many other NLP problems
in the context of general textual inference.
We outline the limitations of the present model
of evaluation of RTE performance, and identify
kinds of evaluation that would promote under-
standing of the way individual components can
impact Textual Entailment system performance,
and allow better objective evaluation of RTE sys-
tem behavior without imposing additional burdens
on RTE participants. We use this to motivate a
large-scale annotation effort to provide data with
the mark-up sufficient to support these goals.
To stimulate discussion of suitable annotation
and evaluation models, we propose a candidate
model, and provide results from a pilot annota-
tion effort (section 3). This pilot study establishes
the feasibility of an inference-motivated annota-
tion effort, and its results offer a quantitative in-
sight into the difficulty of the TE task, and the dis-
tribution of a number of entailment-relevant lin-
guistic phenomena over a representative sample
from the NIST TAC RTE 5 challenge corpus. We
argue that such an evaluation and annotation ef-
fort can identify relevant subproblems whose so-
lution will benefit not only Textual Entailment but
a range of other long-standing NLP tasks, and can
stimulate development of new ones. We also show
how this data can be used to investigate the behav-
ior of some of the highest-scoring RTE systems
from the most recent challenge (section 4).
2 NLP Insights from Textual Entailment
The task of Recognizing Textual Entailment
(RTE), as formulated by (Dagan et al, 2006), re-
quires automated systems to identify when a hu-
man reader would judge that given one span of text
(the Text) and some unspecified (but restricted)
world knowledge, a second span of text (the Hy-
1200
Text: The purchase of LexCorp by BMI for $2Bn
prompted widespread sell-offs by traders as they
sought to minimize exposure.
Hyp 1: BMI acquired another company.
Hyp 2: BMI bought LexCorp for $3.4Bn.
Figure 1: Some representative RTE examples.
pothesis) is true. The task was extended in (Gi-
ampiccolo et al, 2007) to include the additional
requirement that systems identify when the Hy-
pothesis contradicts the Text. In the example
shown in figure 1, this means recognizing that the
Text entails Hypothesis 1, while Hypothesis 2 con-
tradicts the Text. This operational definition of
Textual Entailment avoids commitment to any spe-
cific knowledge representation, inference method,
or learning approach, thus encouraging applica-
tion of a wide range of techniques to the problem.
2.1 An Illustrative Example
The simple RTE examples in figure 1 (most RTE
examples have much longer Texts) illustrate some
typical inference capabilities demonstrated by hu-
man readers in determining whether one span of
text contains the meaning of another.
To recognize that Hypothesis 1 is entailed by the
text, a human reader must recognize that ?another
company? in the Hypothesis can match ?Lex-
Corp?. She must also identify the nominalized
relation ?purchase?, and determine that ?A pur-
chased by B? implies ?B acquires A?.
To recognize that Hypothesis 2 contradicts the
Text, similar steps are required, together with the
inference that because the stated purchase price is
different in the Text and Hypothesis, but with high
probability refers to the same transaction, Hypoth-
esis 2 contradicts the Text.
It could be argued that this particular example
might be resolved by simple lexical matching; but
it should be evident that the Text can be made
lexically very dissimilar to Hypothesis 1 while
maintaining the Entailment relation, and that con-
versely, the lexical overlap between the Text and
Hypothesis 2 can be made very high, while main-
taining the Contradiction relation. This intuition
is borne out by the results of the RTE challenges,
which show that lexical similarity-based systems
are outperformed by systems that use other, more
structured analysis, as shown in the next section.
Rank System id Accuracy
1 I 0.735
2 E 0.685
3 H 0.670
4 J 0.667
5 G 0.662
6 B 0.638
7 D 0.633
8 F 0.632
9 A 0.615
9 C 0.615
9 K 0.615
- Lex 0.612
Table 1: Top performing systems in the RTE 5 2-
way task.
Lex E G H I J
Lex 1.000 0.667 0.693 0.678 0.660 0.778
(184,183) (157,132) (168,122) (152,136) (165,137) (165,135)
E 1.000 0.667 0.675 0.673 0.702
(224,187) (192,112) (178,131) (201,127) (186,131)
G 1.000 0.688 0.713 0.745
(247,150) (186,120) (218,115) (198,125)
H 1.000 0.705 0.707
(219,183) (194,139) (178,136)
I 1.000 0.705
(260,181) (198,135)
J 1.000
(224,178)
Table 2: In each cell, top row shows observed
agreement and bottom row shows the number of
correct (positive, negative) examples on which the
pair of systems agree.
2.2 The State of the Art in RTE 5
The outputs for all systems that participated in the
RTE 5 challenge were made available to partici-
pants. We compared these to each other and to
a smart lexical baseline (Do et al, 2010) (lexical
match augmented with a WordNet similarity mea-
sure, stemming, and a large set of low-semantic-
content stopwords) to assess the diversity of the
approaches of different research groups. To get
the fullest range of participants, we used results
from the two-way RTE task. We have anonymized
the system names.
Table 1 shows that many participating systems
significantly outperform our smart lexical base-
line. Table 2 reports the observed agreement be-
tween systems and the lexical baseline in terms of
the percentage of examples on which a pair of sys-
tems gave the same label. The agreement between
most systems and the baseline is about 67%, which
suggests that systems are not simply augmented
versions of the lexical baseline, and are also dis-
tinct from each other in their behaviors.2
Common characteristics of RTE systems re-
2Note that the expected agreement between two random
RTE decision-makers is 0.5, so the agreement scores accord-
ing to Cohen?s Kappa measure (Cohen, 1960) are between
0.3 and 0.4.
1201
ported by their designers were the use of struc-
tured representations of shallow semantic content
(such as augmented dependency parse trees and
semantic role labels); the application of NLP re-
sources such as Named Entity recognizers, syn-
tactic and dependency parsers, and coreference
resolvers; and the use of special-purpose ad-hoc
modules designed to address specific entailment
phenomena the researchers had identified, such as
the need for numeric reasoning. However, it is
not possible to objectively assess the role these ca-
pabilities play in each system?s performance from
the system outputs alone.
2.3 The Need for Detailed Evaluation
An ablation study that formed part of the of-
ficial RTE 5 evaluation attempted to evaluate
the contribution of publicly available knowledge
resources such as WordNet (Fellbaum, 1998),
VerbOcean (Chklovski and Pantel, 2004), and
DIRT (Lin and Pantel, 2001) used by many of
the systems. The observed contribution was in
most cases limited or non-existent. It is premature,
however, to conclude that these resources have lit-
tle potential impact on RTE system performance:
most RTE researchers agree that the real contribu-
tion of individual resources is difficult to assess.
As the example in figure 1 illustrates, most RTE
examples require a number of phenomena to be
correctly resolved in order to reliably determine
the correct label (the Interaction problem); a per-
fect coreference resolver might as a result yield lit-
tle improvement on the standard RTE evaluation,
even though coreference resolution is clearly re-
quired by human readers in a significant percent-
age of RTE examples.
Various efforts have been made by individ-
ual research teams to address specific capabili-
ties that are intuitively required for good RTE
performance, such as (de Marneffe et al, 2008),
and the formal treatment of entailment phenomena
in (MacCartney and Manning, 2009) depends on
and formalizes a divide-and-conquer approach to
entailment resolution. But the phenomena-specific
capabilities described in these approaches are far
from complete, and many are not yet invented. To
devote real effort to identify and develop such ca-
pabilities, researchers must be confident that the
resources (and the will!) exist to create and eval-
uate their solutions, and that the resource can be
shown to be relevant to a sufficiently large subset
of the NLP community. While there is widespread
belief that there are many relevant entailment phe-
nomena, though each individually may be rele-
vant to relatively few RTE examples (the Sparse-
ness problem), we know of no systematic analysis
to determine what those phenomena are, and how
sparsely represented they are in existing RTE data.
If it were even known what phenomena were
relevant to specific entailment examples, it might
be possible to more accurately distinguish system
capabilities, and promote adoption of successful
solutions to sub-problems. An annotation-side
solution also maintains the desirable agnosticism
of the RTE problem formulation, by not imposing
the requirement on system developers of generat-
ing an explanation for each answer. Of course, if
examples were also annotated with explanations
in a consistent format, this could form the basis of
a new evaluation of the kind essayed in the pilot
study in (Giampiccolo et al, 2007).
3 Annotation Proposal and Pilot Study
As part of our challenge to the NLP commu-
nity, we propose a distributed OntoNotes-style ap-
proach (Hovy et al, 2006) to this annotation ef-
fort: distributed, because it should be undertaken
by a diverse range of researchers with interests
in different semantic phenomena; and similar to
the OntoNotes annotation effort because it should
not presuppose a fixed, closed ontology of entail-
ment phenomena, but rather, iteratively hypoth-
esize and refine such an ontology using inter-
annotator agreement as a guiding principle. Such
an effort would require a steady output of RTE ex-
amples to form the underpinning of these annota-
tions; and in order to get sufficient data to repre-
sent less common, but nonetheless important, phe-
nomena, a large body of data is ultimately needed.
A research team interested in annotating a new
phenomenon should use examples drawn from the
common corpus. Aside from any task-specific
gold standard annotation they add to the entail-
ment pairs, they should augment existing explana-
tions by indicating in which examples their phe-
nomenon occurs, and at which point in the exist-
ing explanation for each example. In fact, this
latter effort ? identifying phenomena relevant to
textual inference, marking relevant RTE examples,
and generating explanations ? itself enables other
researchers to select from known problems, assess
their likely impact, and automatically generate rel-
1202
evant corpora.
To assess the feasibility of annotating RTE-
oriented local entailment phenomena, we devel-
oped an inference model that could be followed by
annotators, and conducted a pilot annotation study.
We based our initial effort on observations about
RTE data we made while participating in RTE
challenges, together with intuitive conceptions of
the kinds of knowledge that might be available in
semi-structured or structured form. In this sec-
tion, we present our annotation inference model,
and the results of our pilot annotation effort.
3.1 Inference Process
To identify and annotate RTE sub-phenomena in
RTE examples, we need a defensible model for the
entailment process that will lead to consistent an-
notation by different researchers, and to an exten-
sible framework that can accommodate new phe-
nomena as they are identified.
We modeled the entailment process as one of
manipulating the text and hypothesis to be as sim-
ilar as possible, by first identifying parts of the
text that matched parts of the hypothesis, and then
identifying connecting structure. Our inherent as-
sumption was that the meanings of the Text and
Hypothesis could be represented as sets of n-ary
relations, where relations could be connected to
other relations (i.e., could take other relations as
arguments). As we followed this procedure for a
given example, we marked which entailment phe-
nomena were required for the inference. We illus-
trate the process using the example in figure 1.
First, we would identify the arguments ?BMI?
and ?another company? in the Hypothesis as
matching ?BMI? and ?LexCorp? respectively, re-
quiring 1) Parent-Sibling to recognize that ?Lex-
Corp? can match ?company?. We would tag the
example as requiring 2) Nominalization Resolu-
tion to make ?purchase? the active relation and
3) Passivization to move ?BMI? to the subject po-
sition. We would then tag it with 4) Simple Verb
Rule to map ?A purchase B? to ?A acquire B?.
These operations make the relevant portion of the
Text identical to the Hypothesis, so we are done.
For the same Text, but with Hypothesis 2 (a neg-
ative example), we follow the same steps 1-3. We
would then use 4) Lexical Relation to map ?pur-
chase? to ?buy?. We would then observe that the
only possible match for the hypothesis argument
?for $3.4Bn? is the text argument ?for $2Bn?. We
would label this as a 5) Numerical Quantity Mis-
match and 6) Excluding Argument (it can?t be the
case that in the same transaction, the same com-
pany was sold for two different prices).
Note that neither explanation mentions
the anaphora resolution connecting ?they? to
?traders?, because it is not strictly required to
determine the entailment label.
As our example illustrates, this process makes
sense for both positive and negative examples. It
also reflects common approaches in RTE systems,
many of which have explicit alignment compo-
nents that map parts of the Hypothesis to parts of
the Text prior to a final decision stage.
3.2 Annotation Labels
We sought to identify roles for background knowl-
edge in terms of domains and general inference
steps, and the types of linguistic phenomena that
are involved in representing the same information
in different ways, or in detecting key differences
in two similar spans of text that indicate a differ-
ence in meaning. We annotated examples with do-
mains (such as ?Work?) for two reasons: to estab-
lish whether some phenomena are correlated with
particular domains; and to identify domains that
are sufficiently well-represented that a knowledge
engineering study might be possible.
While we did not generate an explicit repre-
sentation of our entailment process, i.e. explana-
tions, we tracked which phenomena were strictly
required for inference. The annotated corpora and
simple CGI scripts for annotation are available at
http://cogcomp.cs.illinois.edu/Data/ACL2010 RTE.php.
The phenomena that we considered during an-
notation are presented in Tables 3, 4, 5, and 6. We
tried to define each phenomenon so that it would
apply to both positive and negative examples, but
ran into a problem: often, negative examples can
be identified principally by structural differences:
the components of the Hypothesis all match com-
ponents in the Text, but they are not connected
by the appropriate structure in the Text. In the
case of contradictions, it is often the case that a
key relation in the Hypothesis must be matched to
an incompatible relation in the Text. We selected
names for these structural behaviors, and tagged
them when we observed them, but the counterpart
for positive examples must always hold: it must
necessarily be the case that the structure in the
Text linking the arguments that match those in the
1203
Hypothesis must be comparable to the Hypothesis
structure. We therefore did not tag this for positive
examples.
We selected a subset of 210 examples from the
NIST TAC RTE 5 (Bentivogli et al, 2009) Test
set drawn equally from the three sub-tasks (IE, IR
and QA). Each example was tagged by both an-
notators. Two passes were made over the data: the
first covered 50 examples from each RTE sub-task,
while the second covered an additional 20 exam-
ples from each sub-task. Between the two passes,
concepts the annotators identified as difficult to
annotate were discussed and more carefully spec-
ified, and several new concepts were introduced
based on annotator observations.
Tables 3, 4, 5, and 6 present information
about the distribution of the phenomena we
tagged, and the inter-annotator agreement (Co-
hen?s Kappa (Cohen, 1960)) for each. ?Occur-
rence? lists the average percentage of examples la-
beled with a phenomenon by the two annotators.
Domain Occurrence Agreement
work 16.90% 0.918
name 12.38% 0.833
die kill injure 12.14% 0.979
group 9.52% 0.794
be in 8.57% 0.888
kinship 7.14% 1.000
create 6.19% 1.000
cause 6.19% 0.854
come from 5.48% 0.879
win compete 3.10% 0.813
Others 29.52% 0.864
Table 3: Occurrence statistics for domains in the
annotated data.
Phenomenon Occurrence Agreement
Named Entity 91.67% 0.856
locative 17.62% 0.623
Numerical Quantity 14.05% 0.905
temporal 5.48% 0.960
nominalization 4.05% 0.245
implicit relation 1.90% 0.651
Table 4: Occurrence statistics for hypothesis struc-
ture features.
From the tables it is apparent that good perfor-
mance on a range of phenomena in our inference
model are likely to have a significant effect on
RTE results, with coreference being deemed es-
sential to the inference process for 35% of exam-
ples, and a number of other phenomena are suffi-
ciently well represented to merit near-future atten-
tion (assuming that RTE systems do not already
handle these phenomena, a question we address in
section 4). It is also clear from the predominance
of Simple Rewrite Rule instances, together with
Phenomenon Occurrence Agreement
coreference 35.00% 0.698
simple rewrite rule 32.62% 0.580
lexical relation 25.00% 0.738
implicit relation 23.33% 0.633
factoid 15.00% 0.412
parent-sibling 11.67% 0.500
genetive relation 9.29% 0.608
nominalization 8.33% 0.514
event chain 6.67% 0.589
coerced relation 6.43% 0.540
passive-active 5.24% 0.583
numeric reasoning 4.05% 0.847
spatial reasoning 3.57% 0.720
Table 5: Occurrence statistics for entailment phe-
nomena and knowledge resources
Phenomenon Occurrence Agreement
missing argument 16.19% 0.763
missing relation 14.76% 0.708
excluding argument 10.48% 0.952
Named Entity mismatch 9.29% 0.921
excluding relation 5.00% 0.870
disconnected relation 4.52% 0.580
missing modifier 3.81% 0.465
disconnected argument 3.33% 0.764
Numeric Quant. mismatch 3.33% 0.882
Table 6: Occurrences of negative-only phenomena
the frequency of most of the domains we selected,
that knowledge engineering efforts also have a key
role in improving RTE performance.
3.3 Discussion
Perhaps surprisingly, given the difficulty of the
task, inter-annotator agreement was consistently
good to excellent (above 0.6 and 0.8, respec-
tively), with few exceptions, indicating that for
most targeted phenomena, the concepts were well-
specified. The results confirmed our initial intu-
ition about some phenomena: for example, that
coreference resolution is central to RTE, and that
detecting the connecting structure is crucial in dis-
cerning negative from positive examples. We also
found strong evidence that the difference between
contradiction and unknown entailment examples
is often due to the behavior of certain relations that
either preclude certain other relations holding be-
tween the same arguments (for example, winning
a contest vs. losing a contest), or which can only
hold for a single referent in one argument position
(for example, ?work? relations such as job title are
typically constrained so that a single person holds
one position).
We found that for some examples, there was
more than one way to infer the hypothesis from the
text. Typically, for positive examples this involved
overlap between phenomena; for example, Coref-
erence might be expected to resolve implicit rela-
1204
tions induced from appositive structures. In such
cases we annotated every way we could find.
In future efforts, annotators should record the
entailment steps they used to reach their decision.
This will make disagreement resolution simpler,
and could also form a possible basis for generating
gold standard explanations. At a minimum, each
inference step must identify the spans of the Text
and Hypothesis that are involved and the name of
the entailment phenomenon represented; in addi-
tion, a partial order over steps must be specified
when one inference step requires that another has
been completed.
Future annotation efforts should also add a
category ?Other?, to indicate for each example
whether the annotator considers the listed entail-
ment phenomena sufficient to identify the label. It
might also be useful to assess the difficulty of each
example based on the time required by the anno-
tator to determine an explanation, for comparison
with RTE system errors.
These, together with specifications that mini-
mize the likely disagreements between different
groups of annotators, are processes that must be
refined as part of the broad community effort we
seek to stimulate.
4 Pilot RTE System Analysis
In this section, we sketch out ways in which
the proposed analysis can be applied to learn
something about RTE system behavior, even
when those systems do not provide anything
beyond the output label. We present the analysis
in terms of sample questions we hope to answer
with such an analysis.
1. If a system needs to improve its performance,
which features should it concentrate on? To an-
swer this question, we looked at the top-5 systems
and tried to find which phenomena are active in
the mistakes they make.
(a) Most systems seem to fail on examples that
need numeric reasoning to get the entailment de-
cision right. For example, system H got all 10 ex-
amples with numeric reasoning wrong.
(b) All top-5 systems make consistent errors in
cases where identifying a mismatch in named en-
tities (NE) or numerical quantities (NQ) is impor-
tant to make the right decision. System G got 69%
of cases with NE/NQ mismatches wrong.
(c) Most systems make errors in examples that
have a disconnected or exclusion component (ar-
gument/relation). System J got 81% of cases with
a disconnected component wrong.
(d) Some phenomena are handled well by certain
systems, but not by others. For example, failing
to recognize a parent-sibling relation between
entities/concepts seems to be one of the top-5
phenomena active in systems E and H. System
H also fails to correctly label over 53% of the
examples having kinship relation.
2. Which phenomena have strong correlations
to the entailment labels among hard examples?
We called an example hard if at least 4 of the top 5
systems got the example wrong. In our annotation
dataset, there were 41 hard examples. Some of
the phenomena that strongly correlate with the
TE labels on hard examples are: deeper lexical
relation between words (? = 0.542), and need
for external knowledge (? = 0.345). Further, we
find that the top-5 systems tend to make mistakes
in cases where the lexical approach also makes
mistakes (? = 0.355).
3. What more can be said about individual
systems? In order to better understand the system
behavior, we wanted to check if we could predict
the system behavior based on the phenomena
we identified as important in the examples.
We learned SVM classifiers over the identified
phenomena and the lexical similarity score to
predict both the labels and errors systems make
for each of the top-5 systems. We could predict all
10 system behaviors with over 70% accuracy, and
could predict labels and mistakes made by two of
the top-5 systems with over 77% accuracy. This
indicates that although the identified phenomena
are indicative of the system performance, it is
probably too simplistic to assume that system
behavior can be easily reproduced solely as a
disjunction of phenomena present in the examples.
4. Does identifying the phenomena correctly
help learn a better TE system? We tried to
learn an entailment classifier over the phenomenon
identified and the top 5 system outputs. The results
are summarized in Table 7. All reported num-
bers are 20-fold cross-validation accuracy from
an SVM classifier learned over the features men-
tioned. The results show that correctly identify-
ing the named-entity and numeric quantity mis-
1205
No. Feature description No. of Accuracy over which features
feats phenomena pheno. + sys. labels
(0) Only system labels 5 ? 0.714
(1) Domain and hypothesis features (Tables 3, 4) 16 0.510 0.705
(2) (1) + NE + NQ 18 0.619 0.762
(3) (1) + Knowledge resources (subset of Table 5) 22 0.662 0.762
(4) (3) + NE + NQ 24 0.738 0.805
(5) (1) + Entailment and Knowledge resources (Table 5) 29 0.748 0.791
(6) (5) + negative-only phenomena (Table 6) 38 0.971 0.943
Table 7: Accuracy in predicting the label based on the phenomena and top-5 system labels.
matches improves the overall accuracy signifi-
cantly. If we further recognize the need for knowl-
edge resources correctly, we can correctly explain
the label for 80% of the examples. Adding the
entailment and negation features helps us explain
the label for 97% of the examples in the annotated
corpus.
It must be clarified that the results do not show
the textual entailment problem itself is solved with
97% accuracy. However, we believe that if a
system could recognize key negation phenomena
such as Named Entity mismatch, presence of Ex-
cluding arguments, etc. correctly and consistently,
it could model them as a Contradiction features
in the final inference process to significantly im-
prove its overall accuracy. Similarly, identifying
and resolving the key entailment phenomena in
the examples, would boost the inference process
in positive examples. However, significant effort
is still required to obtain near-accurate knowledge
and linguistic resources.
5 Discussion
NLP researchers in the broader community contin-
ually seek new problems to solve, and pose more
ambitious tasks to develop NLP and NLU capabil-
ities, yet recognize that even solutions to problems
which are considered ?solved? may not perform as
well on domains different from the resources used
to train and develop them. Solutions to such NLP
tasks could benefit from evaluation and further de-
velopment on corpora drawn from a range of do-
mains, like those used in RTE evaluations.
It is also worthwhile to consider each task as
part of a larger inference process, and therefore
motivated not just by performance statistics on
special-purpose corpora, but as part of an inter-
connected web of resources; and the task of Rec-
ognizing Textual Entailment has been designed to
exercise a wide range of linguistic and reasoning
capabilities.
The entailment setting introduces a potentially
broader context to resource development and as-
sessment, as the hypothesis and text provide con-
text for each other in a way different than local
context from, say, the same paragraph in a docu-
ment: in RTE?s positive examples, the Hypothe-
sis either restates some part of the Text, or makes
statements inferable from the statements in the
Text. This is not generally true of neighboring sen-
tences in a document. This distinction opens the
door to ?purposeful?, or goal-directed, inference
in a way that may not be relevant to a task studied
in isolation.
The RTE community seems mainly convinced
that incremental advances in local entailment phe-
nomena (including application of world knowl-
edge) are needed to make significant progress.
They need ways to identify sub-problems of tex-
tual inference, and to evaluate those solutions both
in isolation and in the context of RTE. RTE system
developers are likely to reward well-engineered
solutions by adopting them and citing their au-
thors, because such solutions are easier to incor-
porate into RTE systems. They are also more
likely to adopt solutions with established perfor-
mance levels. These characteristics promote pub-
lication of software developed to solve NLP tasks,
attention to its usability, and publication of mate-
rials supporting reproduction of results presented
in technical papers.
For these reasons, we assert that RTE is a nat-
ural motivator of new NLP tasks, as researchers
look for components capable of improving perfor-
mance; and that RTE is a natural setting for evalu-
ating solutions to a broad range of NLP problems,
though not in its present formulation: we must
solve the problem of credit assignment, to recog-
nize component contributions. We have therefore
proposed a suitable annotation effort, to provide
the resources necessary for more detailed evalua-
tion of RTE systems.
We have presented a linguistically-motivated
1206
analysis of entailment data based on a step-wise
procedure to resolve entailment decisions, in-
tended to allow independent annotators to reach
consistent decisions, and conducted a pilot anno-
tation effort to assess the feasibility of such a task.
We do not claim that our set of domains or phe-
nomena are complete: for example, our illustra-
tive example could be tagged with a domain Merg-
ers and Acquisitions, and a different team of re-
searchers might consider Nominalization Resolu-
tion to be a subset of Simple Verb Rules. This kind
of disagreement in coverage is inevitable, but we
believe that in many cases it suffices to introduce
a new domain or phenomenon, and indicate its re-
lation (if any) to existing domains or phenomena.
In the case of introducing a non-overlapping cate-
gory, no additional information is needed. In other
cases, the annotators can simply indicate the phe-
nomena being merged or split (or even replaced).
This information will allow other researchers to
integrate different annotation sources and main-
tain a consistent set of annotations.
6 Conclusions
In this paper, we have presented a case for a broad,
long-term effort by the NLP community to coordi-
nate annotation efforts around RTE corpora, and to
evaluate solutions to NLP tasks relating to textual
inference in the context of RTE. We have iden-
tified limitations in the existing RTE evaluation
scheme, proposed a more detailed evaluation to
address these limitations, and sketched a process
for generating this annotation. We have proposed
an initial annotation scheme to prompt discussion,
and through a pilot study, demonstrated that such
annotation is both feasible and useful.
We ask that researchers not only contribute
task specific annotation to the general pool, and
indicate how their task relates to those already
added to the annotated RTE corpora, but also in-
vest the additional effort required to augment the
cross-domain annotation: marking the examples
in which their phenomenon occurs, and augment-
ing the annotator-generated explanations with the
relevant inference steps.
These efforts will allow a more meaningful
evaluation of RTE systems, and of the compo-
nent NLP technologies they depend on. We see
the potential for great synergy between different
NLP subfields, and believe that all parties stand to
gain from this collaborative effort. We therefore
respectfully suggest that you ?ask not what RTE
can do for you, but what you can do for RTE...?
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments and suggestions. This research was
partly sponsored by Air Force Research Labora-
tory (AFRL) under prime contract no. FA8750-
09-C-0181, by a grant from Boeing and by MIAS,
the Multimodal Information Access and Synthesis
center at UIUC, part of CCICADA, a DHS Center
of Excellence. Any opinions, findings, and con-
clusion or recommendations expressed in this ma-
terial are those of the author(s) and do not neces-
sarily reflect the view of the sponsors.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The
fifth pascal recognizing textual entailment chal-
lenge. In Notebook papers and Results, Text Analy-
sis Conference (TAC), pages 14?24.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP-04), pages 33?40.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge., volume 3944. Springer-Verlag, Berlin.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL-08: HLT, pages
1039?1047, Columbus, Ohio, June. Association for
Computational Linguistics.
Quang Do, Dan Roth, Mark Sammons, Yuancheng
Tu, and V.G.Vinod Vydiswaran. 2010. Robust,
Light-weight Approaches to compute Lexi-
cal Similarity. Computer Science Research
and Technical Reports, University of Illinois.
http://L2R.cs.uiuc.edu/?danr/Papers/DRSTV10.pdf.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recognizing
textual entailment challenge. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 1?9, Prague, June. Association
for Computational Linguistics.
1207
Sanda Harabagiu and Andrew Hickl. 2006. Meth-
ods for Using Textual Entailment in Open-Domain
Question Answering. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 905?912, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of HLT/NAACL,
New York.
D. Lin and P. Pantel. 2001. DIRT: discovery of in-
ference rules from text. In Proc. of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing 2001, pages 323?328.
Bill MacCartney and Christopher D. Manning. 2009.
An extended model of natural logic. In The Eighth
International Conference on Computational Seman-
tics (IWCS-8), Tilburg, Netherlands.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translat-
ing unknown terms. In ACL/AFNLP, pages 791?
799, Suntec, Singapore, August. Association for
Computational Linguistics.
Sebastian Pado, Michel Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Robust machine
translation evaluation with entailment features. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 297?305, Suntec, Singapore,
August. Association for Computational Linguistics.
1208
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 551?560,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Exploiting Syntactico-Semantic Structures for Relation Extraction
Yee Seng Chan and Dan Roth
University of Illinois at Urbana-Champaign
{chanys,danr}@illinois.edu
Abstract
In this paper, we observe that there exists a
second dimension to the relation extraction
(RE) problem that is orthogonal to the relation
type dimension. We show that most of these
second dimensional structures are relatively
constrained and not difficult to identify. We
propose a novel algorithmic approach to RE
that starts by first identifying these structures
and then, within these, identifying the seman-
tic type of the relation. In the real RE problem
where relation arguments need to be identi-
fied, exploiting these structures also allows re-
ducing pipelined propagated errors. We show
that this RE framework provides significant
improvement in RE performance.
1 Introduction
Relation extraction (RE) has been defined as the task
of identifying a given set of semantic binary rela-
tions in text. For instance, given the span of text
?. . . the Seattle zoo . . . ?, one would like to extract the
relation that ?the Seattle zoo? is located-at ?Seattle?.
RE has been frequently studied over the last few
years as a supervised learning task, learning from
spans of text that are annotated with a set of seman-
tic relations of interest. However, most approaches
to RE have assumed that the relations? arguments
are given as input (Chan and Roth, 2010; Jiang and
Zhai, 2007; Jiang, 2009; Zhou et al, 2005), and
therefore offer only a partial solution to the problem.
Conceptually, this is a rather simple approach as
all spans of texts are treated uniformly and are be-
ing mapped to one of several relation types of in-
terest. However, these approaches to RE require a
large amount of manually annotated training data to
achieve good performance, making it difficult to ex-
pand the set of target relations. Moreover, as we
show, these approaches become brittle when the re-
lations? arguments are not given but rather need to
be identified in the data too.
In this paper we build on the observation that there
exists a second dimension to the relation extraction
problem that is orthogonal to the relation type di-
mension: all relation types are expressed in one of
several constrained syntactico-semantic structures.
As we show, identifying where the text span is on the
syntactico-semantic structure dimension first, can be
leveraged in the RE process to yield improved per-
formance. Moreover, working in the second dimen-
sion provides robustness to the real RE problem, that
of identifying arguments along with the relations be-
tween them.
For example, in ?the Seattle zoo?, the entity men-
tion ?Seattle? modifies the noun ?zoo?. Thus, the
two mentions ?Seattle? and ?the Seattle zoo?, are
involved in what we later call a premodifier rela-
tion, one of several syntactico-semantic structures
we identify in Section 3.
We highlight that all relation types can be ex-
pressed in one of several syntactico-semantic struc-
tures ? Premodifiers, Possessive, Preposition, For-
mulaic and Verbal. As it turns out, most of these
structures are relatively constrained and are not dif-
ficult to identify. This suggests a novel algorith-
mic approach to RE that starts by first identifying
these structures and then, within these, identifying
the semantic type of the relation. Not only does this
approach provide significantly improved RE perfor-
551
mance, it carries with it two additional advantages.
First, leveraging the syntactico-semantic struc-
ture is especially beneficial in the presence of small
amounts of data. Second, and more important, is the
fact that exploiting the syntactico-semantic dimen-
sion provides several new options for dealing with
the full RE problem ? incorporating the argument
identification into the problem. We explore one of
these possibilities, making use of the constrained
structures as a way to aid in the identification of the
relations? arguments. We show that this already pro-
vides significant gain, and discuss other possibilities
that can be explored. The contributions of this paper
are summarized below:
? We highlight that all relation types are ex-
pressed as one of several syntactico-semantic
structures and show that most of these are rela-
tively constrained and not difficult to identify.
Consequently, working first in this structural
dimension can be leveraged in the RE process
to improve performance.
? We show that when one does not have a large
number of training examples, exploiting the
syntactico-semantic structures is crucial for RE
performance.
? We show how to leverage these constrained
structures to improve RE when the relations?
arguments are not given. The constrained struc-
tures allow us to jointly entertain argument can-
didates and relations built with them as argu-
ments. Specifically, we show that considering
argument candidates which otherwise would
have been discarded (provided they exist in
syntactico-semantic structures), we reduce er-
ror propagation along a standard pipeline RE
architecture, and that this joint inference pro-
cess leads to improved RE performance.
In the next section, we describe our relation ex-
traction framework that leverages the syntactico-
semantic structures. We then present these struc-
tures in Section 3. We describe our mention entity
typing system in Section 4 and features for the RE
system in Section 5. We present our RE experiments
in Section 6 and perform analysis in Section 7, be-
fore concluding in Section 8.
S = {premodifier, possessive, preposition, formulaic}
gold mentions in training data Mtrain
Dg = {(mi,mj) ?Mtrain ?Mtrain |
mi in same sentence as mj ? i 6= j ? i < j}
REbase = RE classifier trained on Dg
Ds = ?
for each (mi,mj) ? Dg
do
p = structure inference on (mi,mj) using patterns
if p ? S ? (mi,mj) was annotated with a S structure
Ds = Ds ? (mi,mj)
done
REs = RE classifier trained on Ds
Output: REbase and REs
Figure 1: Training a regular baseline RE classi-
fier REbase and a RE classifier leveraging syntactico-
semantic structures REs.
2 Relation Extraction Framework
In Figure 1, we show the algorithm for training
a typical baseline RE classifier (REbase), and for
training a RE classifier that leverages the syntactico-
semantic structures (REs).
During evaluation and when the gold mentions are
already annotated, we apply REs as follows. When
given a test example mention pair (xi,xj), we per-
form structure inference on it using the patterns de-
scribed in Section 3. If (xi,xj) is identified as hav-
ing any of the four syntactico-semantic structures S,
apply REs to predict the relation label, else apply
REbase.
Next, we show in Figure 2 our joint inference al-
gorithmic framework that leverages the syntactico-
semantic structures for RE, when mentions need to
be predicted. Since the structures are fairly con-
strained, we can use them to consider mention can-
didates that are originally predicted as non men-
tions. As shown in Figure 2, we conservatively in-
clude such mentions when forming mention pairs,
provided their null labels are predicted with a low
probability t1.
1In this work, we arbitrary set t=0.2. After the experiments,
and in our own analysis, we observe that t=0.25 achieves better
performance. Besides using the probability of the 1-best predic-
tion, one could also for instance, use the probability difference
between the first and second best predictions. However, select-
ing an optimal t value is not the main focus of this work.
552
S = {premodifier, possessive, preposition, formulaic}
candidate mentions Mcand
Let Lm = argmax
y
PMET (y|m, ?),m ?Mcand
selected mentions Msel = {m ?Mcand |
Lm 6= null ? PMET (null|m, ?) ? t}
QhasNull = {(mi,mj) ?Msel ?Msel |
mi in same sentence as mj ? i 6= j ? i < j ?
(Lmi 6= null ? Lmj 6= null)}
Let pool of relation predictions R = ?
for each (mi,mj) ? QhasNull
do
p = structure inference on (mi,mj) using patterns
if p ? S
r = relation prediction for (mi,mj) using REs
R = R? r
else if Lmi 6= null ? Lmj 6= null
r = relation prediction for (mi,mj) using REbase
R = R? r
done
Output: R
Figure 2: RE using predicted mentions and patterns. Ab-
breviations: Lm: predicted entity label for mention m us-
ing the mention entity typing (MET) classifier described
in Section 4; PMET : prediction probability according to
the MET classifier; t: used for thresholding.
There is a large body of work in using patterns
to extract relations (Fundel et al, 2007; Greenwood
and Stevenson, 2006; Zhu et al, 2009). However,
these works operate along the first dimension, that
of using patterns to mine for relation type examples.
In contrast, in our RE framework, we apply patterns
to identify the syntactico-semantic structure dimen-
sion first, and leverage this in the RE process. In
(Roth and Yih, 2007), the authors used entity types
to constrain the (first dimensional) relation types al-
lowed among them. In our work, although a few of
our patterns involve semantic type comparison, most
of the patterns are syntactic in nature.
In this work, we performed RE evaluation on the
NIST Automatic Content Extraction (ACE) corpus.
Most prior RE evaluation on ACE data assumed that
mentions are already pre-annotated and given as in-
put (Chan and Roth, 2010; Jiang and Zhai, 2007;
Zhou et al, 2005). An exception is the work of
(Kambhatla, 2004), where the author evaluated on
the ACE-2003 corpus. In that work, the author did
not address the pipelined errors propagated from the
mention identification process.
3 Syntactico-Semantic Structures
In this paper, we performed RE on the ACE-2004
corpus. In ACE-2004 when the annotators tagged a
pair of mentions with a relation, they also specified
the type of syntactico-semantic structure2. ACE-
2004 identified five types of structures: premodi-
fier, possessive, preposition, formulaic, and verbal.
We are unaware of any previous computational ap-
proaches that recognize these structures automati-
cally in text, as we do, and use it in the context of
RE (or any other problem). In (Qian et al, 2008), the
authors reported the recall scores of their RE system
on the various syntactico-semantic structures. But
they do not attempt to recognize nor leverage these
structures.
In this work, we focus on detecting the first four
structures. These four structures cover 80% of the
mention pairs having valid semantic relations (we
give the detailed breakdown in Section 7) and we
show that they are relatively easy to identify using
simple rules or patterns. In this section, we indicate
mentions using square bracket pairs, and use mi and
mj to represent a mention pair. We now describe the
four structures.
Premodifier relations specify the proper adjective
or proper noun premodifier and the following noun
it modifies, e.g.: [the [Seattle] zoo]
Possessive indicates that the first mention is in a
possessive case, e.g.: [[California] ?s Governor]
Preposition indicates that the two mentions are
semantically related via the existence of a preposi-
tion, e.g.: [officials] in [California]
Formulaic The ACE04 annotation guideline3 in-
dicates the annotation of several formulaic relations,
including for example address: [Medford] , [Mas-
sachusetts]
2ACE-2004 termed it as lexical condition. We use the term
syntactico-semantic structure in this paper as the mention pair
exists in specific syntactic structures, and we use rules or pat-
terns that are syntactically and semantically motivated to detect
these structures.
3http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV4-3-
2.PDF
553
Structure type Pattern
Premodifier Basic pattern: [u* [v+] w+] , where u, v, w represent words
Each w is a noun or adjective
If u* is not empty, then u*: JJ+ ? JJ ?and? JJ? ? CD JJ* ? RB DT JJ? ? RB CD JJ ?
DT (RB|JJ|VBG|VBD|VBN|CD)?
Let w1 = first word in w+. w1 6= ??s? and POS tag of w1 6= POS
Let vl = last word in v+. POS tag of vl 6= PRP$ nor WP$
Possessive Basic pattern: [u? [v+] w+] , where u, v, w represent words
Let w1 = first word in w+. If w1 = ??s? ? POS tag of w1 = POS, accept mention pair
Let vl = last word in v+. If POS tag of vl = PRP$ or WP$, accept mention pair
Preposition Basic pattern: [mi] v* [mj], where v represent words
and number of prepositions in the text span v* between them = 0, 1, or 2
If satisfy pattern: IN [mi][mj], accept mention pair
If satisfy pattern: [mi] (IN|TO) [mj], accept mention pair
If all labels in Ld start with ?prep?, accept mention pair
Formulaic If satisfy pattern: [mi] / [mj] ? Ec(mi) = PER ? Ec(mj) = ORG, accept mention pair
If satisfy pattern: [mi][mj]
If Ec(mi) = PER ? Ec(mj) = ORG ? GPE, accept mention pair
Table 1: Rules and patterns for the four syntactico-semantic structures. Regular expression notations: ?*? matches
the preceding element zero or more times; ?+? matches the preceding element one or more times; ??? indicates that
the preceding element is optional; ?|? indicates or. Abbreviations: Ec(m): coarse-grained entity type of mention m;
Ld: labels in dependency path between the headword of two mentions. We use square brackets ?[? and ?]? to denote
mention boundaries. The ?/? in the Formulaic row denotes the occurrence of a lexical ?/? in text.
In this rest of this section, we present the
rules/patterns for detecting the above four
syntactico-semantic structure, giving an overview
of them in Table 1. We plan to release all of the
rules/patterns along with associated code4. Notice
that the patterns are intuitive and mostly syntactic in
nature.
3.1 Premodifier Structures
? We require that one of the mentions completely
include the other mention. Thus, the basic pat-
tern is [u* [v+] w+].
? If u* is not empty, we require that it satisfies
any of the following POS tag sequences: JJ+ ?
JJ and JJ? ? CD JJ*, etc. These are (optional)
POS tag sequences that normally start a valid
noun phrase.
? We use two patterns to differentiate between
premodifier relations and possessive relations,
by checking for the existence of POS tags
PRP$, WP$, POS, and the word ??s?.
4http://cogcomp.cs.illinois.edu/page/publications
3.2 Possessive Structures
? The basic pattern for possessive is similar to
that for premodifier: [u? [v+] w+]
? If the word immediately following v+ is ??s? or
its POS tag is ?POS?, we accept the mention
pair. If the POS tag of the last word in v+ is ei-
ther PRP$ or WP$, we accept the mention pair.
3.3 Preposition Structures
? We first require the two mentions to be non-
overlapping, and check for the existence of
patterns such as ?IN [mi] [mj]? and ?[mi]
(IN|TO) [mj]?.
? If the only dependency labels in the depen-
dency path between the head words of mi and
mj are ?prep? (prepositional modifier), accept
the mention pair.
3.4 Formulaic Structures
? The ACE-2004 annotator guidelines specify
that several relations such as reporter signing
off, addresses, etc. are often specified in stan-
dard structures. We check for the existence of
patterns such as ?[mi] / [mj]?, ?[mi] [mj]?,
554
Category Feature
For every POS of wk and offset from lw
word wk wk and offset from lw
in POS of wk, wk, and offset from lw
mention mi POS of wk, offset from lw, and lw
Bc(wk) and offset from lw
POS of wk, Bc(wk), and offset from lw
POS of wk, offset from lw, and Bc(lw)
Contextual C?1,?1 of mi
C+1,+1 of mi
P?1,?1 of mi
P+1,+1 of mi
NE tags tag of NE, if lw of NE coincides
with lw of mi in the sentence
Syntactic parse-label of parse tree constituent
parse that exactly covers mi
parse-labels of parse tree constituents
covering mi
Table 2: Features used in our mention entity typing
(MET) system. The abbreviations are as follows. lw:
last word in the mention; Bc(w): the brown cluster bit
string representing w; NE: named entity
and whether they satisfy certain semantic entity
type constraints.
4 Mention Extraction System
As part of our experiments, we perform RE using
predicted mentions. We first describe the features
(an overview is given in Table 2) and then describe
how we extract candidate mentions from sentences
during evaluation.
4.1 Mention Extraction Features
Features for every word in the mention For ev-
ery word wk in a mention mi, we extract seven fea-
tures. These are a combination of wk itself, its POS
tag, and its integer offset from the last word (lw) in
the mention. For instance, given the mention ?the
operation room?, the offsets for the three words in
the mention are -2, -1, and 0 respectively. These
features are meant to capture the word and POS tag
sequences in mentions.
We also use word clusters which are automat-
ically generated from unlabeled texts, using the
Brown clustering (Bc) algorithm of (Brown et al,
1992). This algorithm outputs a binary tree where
words are leaves in the tree. Each word (leaf) in the
tree can be represented by its unique path from the
Category Feature
POS POS of single word between m1, m2
hw of mi, mj and P?1,?1 of mi, mj
hw of mi, mj and P?1,?1 of mi, mj
hw of mi, mj and P+1,+1 of mi, mj
hw of mi, mj and P?2,?1 of mi, mj
hw of mi, mj and P?1,+1 of mi, mj
hw of mi, mj and P+1,+2 of mi, mj
Base chunk any base phrase chunk between mi, mj
Table 3: Additional RE features.
root and this path can be represented as a simple bit
string. As part of our features, we use the cluster bit
string representation of wk and lw.
Contextual We extract the word C?1,?1 immedi-
ately before mi, the word C+1,+1 immediately after
mi, and their associated POS tags P .
NE tags We automatically annotate the sentences
with named entity (NE) tags using the named en-
tity tagger of (Ratinov and Roth, 2009). This tagger
annotates proper nouns with the tags PER (person),
ORG (organization), LOC (location), or MISC (mis-
cellaneous). If the lw of mi coincides (actual token
offset) with the lw of any NE annotated by the NE
tagger, we extract the NE tag as a feature.
Syntactic parse We parse the sentences using the
syntactic parser of (Klein and Manning, 2003). We
extract the label of the parse tree constituent (if it ex-
ists) that exactly covers the mention, and also labels
of all constituents that covers the mention.
4.2 Extracting Candidate Mentions
From a sentence, we gather the following as candi-
date mentions: all nouns and possessive pronouns,
all named entities annotated by the the NE tagger
(Ratinov and Roth, 2009), all base noun phrase (NP)
chunks, all chunks satisfying the pattern: NP (PP
NP)+, all NP constituents in the syntactic parse tree,
and from each of these constituents, all substrings
consisting of two or more words, provided the sub-
strings do not start nor end on punctuation marks.
These mention candidates are then fed to our men-
tion entity typing (MET) classifier for type predic-
tion (more details in Section 6.3).
555
5 Relation Extraction System
We build a supervised RE system using sentences
annotated with entity mentions and predefined target
relations. During evaluation, when given a pair of
mentions mi, mj , the system predicts whether any
of the predefined target relation holds between the
mention pair.
Most of our features are based on the work of
(Zhou et al, 2005; Chan and Roth, 2010). Due to
space limitations, we refer the reader to our prior
work (Chan and Roth, 2010) for the lexical, struc-
tural, mention-level, entity type, and dependency
features. Here, we only describe the features that
were not used in that work.
As part of our RE system, we need to extract the
head word (hw) of a mention (m), which we heuris-
tically determine as follows: if m contains a prepo-
sition and a noun preceding the preposition, we use
the noun as the hw. If there is no preposition in m,
we use the last noun in m as the hw.
POS features If there is a single word between the
two mentions, we extract its POS tag. Given the hw
of m, Pi,j refers to the sequence of POS tags in the
immediate context of hw (we exclude the POS tag
of hw). The offsets i and j denote the position (rela-
tive to hw) of the first and last POS tag respectively.
For instance, P?2,?1 denotes the sequence of two
POS tags on the immediate left of hw, and P?1,+1
denotes the POS tag on the immediate left of hw and
the POS tag on the immediate right of hw.
Base phrase chunk We add a boolean feature to
detect whether there is any base phrase chunk in the
text span between the two mentions.
6 Experiments
We use the ACE-2004 dataset (catalog
LDC2005T09 from the Linguistic Data Con-
sortium) to conduct our experiments. Following
prior work, we use the news wire (nwire) and
broadcast news (bnews) corpora of ACE-2004 for
our experiments, which consists of 345 documents.
To build our RE system, we use the LIBLINEAR
(Fan et al, 2008) package, with its default settings
of L2-loss SVM (dual) as the solver, and we use an
epsilon of 0.1. To ensure that this baseline RE sys-
tem based on the features in Section 5 is competi-
tive, we compare against the state-of-the-art feature-
based RE systems of (Jiang and Zhai, 2007) and
(Chan and Roth, 2010). In these works, the au-
thors reported performance on undirected coarse-
grained RE. Performing 5-fold cross validation on
the nwire and bnews corpora, (Jiang and Zhai, 2007)
and (Chan and Roth, 2010) reported F-measures of
71.5 and 71.2, respectively. Using the same evalua-
tion setting, our baseline RE system achieves a com-
petitive 71.4 F-measure.
We build three RE classifiers: binary, coarse, fine.
Lumping all the predefined target relations into a
single label, we build a binary classifier to predict
whether any of the predefined relations exists be-
tween a given mention pair.
In this work, we model the argument order of the
mentions when performing RE, since relations are
usually asymmetric in nature. For instance, we con-
sider mi:EMP-ORG:mj and mj :EMP-ORG:mi to
be distinct relation types. In our experiments, we ex-
tracted a total of 55,520 examples or mention pairs.
Out of these, 4,011 are positive relation examples
annotated with 6 coarse-grained relation types and
22 fine-grained relation types5.
We build a coarse-grained classifier to disam-
biguate between 13 relation labels (two asymmetric
labels for each of the 6 coarse-grained relation types
and a null label). We similarly build a fine-grained
classifier to disambiguate between 45 relation labels.
6.1 Evaluation Method
For our experiments, we adopt the experimental set-
ting in our prior work (Chan and Roth, 2010) of en-
suring that all examples from a single document are
either all used for training, or all used for evaluation.
In that work, we also highlight that ACE anno-
tators rarely duplicate a relation link for coreferent
mentions. For instance, assume mentions mi, mj ,
and mk are in the same sentence, mentions mi and
mj are coreferent, and the annotators tag the men-
tion pair mj , mk with a particular relation r. The
annotators will rarely duplicate the same (implicit)
5We omit a single relation: Discourse (DISC). The ACE-
2004 annotation guidelines states that the DISC relation is es-
tablished only for the purposes of the discourse and does not
reference an official entity relevant to world knowledge. In this
work, we focus on semantically meaningful relations. Further-
more, the DISC relation is dropped in ACE-2005.
556
10 documents 5% of data 80% of data
RE model Rec% Pre% F1% Rec% Pre% F1% Rec% Pre% F1%
Binary 58.0 80.3 67.4 64.4 80.6 71.6 73.2 84.0 78.2
Binary+Patterns 73.1 78.5 75.7 (+8.3) 75.3 80.6 77.9 80.1 84.2 82.1
Coarse 33.5 62.5 43.6 42.4 66.2 51.7 62.1 75.5 68.1
Coarse+Patterns 44.2 59.6 50.8 (+7.2) 51.2 64.2 56.9 68.0 75.4 71.5
Fine 18.1 47.0 26.1 26.3 51.6 34.9 51.6 68.4 58.8
Fine+Patterns 24.8 43.5 31.6 (+5.5) 32.2 48.9 38.9 56.4 67.5 61.5
Table 4: Micro-averaged (across the 5 folds) RE results using gold mentions.
10 documents 5% of data 80% of data
RE model Rec% Pre% F1% Rec% Pre% F1% Rec% Pre% F1%
Binary 32.2 46.6 38.1 35.5 48.9 41.1 40.1 52.7 45.5
Binary+Patterns 46.7 45.9 46.3 (+8.2) 47.6 47.8 47.2 50.2 50.4 50.3
Coarse 18.6 41.1 25.6 22.4 40.9 28.9 32.3 47.5 38.5
Coarse+Patterns 26.8 34.7 30.2 (+4.6) 30.3 37.0 33.3 38.9 42.9 40.8
Fine 10.7 32.2 16.1 14.6 33.4 20.3 26.9 44.3 33.5
Fine+Patterns 15.7 26.3 19.7 (+3.6) 19.4 29.2 23.3 31.7 38.3 34.7
Table 5: Micro-averaged (across the 5 folds) RE results using predicted mentions.
relation r between mi and mk, thus leaving the gold
relation label as null. Whether this is correct or not is
debatable. However, to avoid being penalized when
our RE system actually correctly predicts the label
of an implicit relation, we take the following ap-
proach.
During evaluation, if our system correctly pre-
dicts an implicit label, we simply switch its predic-
tion to the null label. Since the RE recall scores
only take into account non-null relation labels, this
scoring method does not change the recall, but could
marginally increase the precision scores by decreas-
ing the count of RE predictions. In our experi-
ments, we observe that both the usual and our scor-
ing method give very similar RE results and the ex-
perimental trends remain the same. Of course, us-
ing this scoring method requires coreference infor-
mation, which is available in the ACE data.
6.2 RE Evaluation Using Gold Mentions
To perform our experiments, we split the 345 docu-
ments into 5 equal sets. In each of the 5 folds, 4 sets
(276 documents) are reserved for drawing training
examples, while the remaining set (69 documents)
is used as evaluation data. In the experiments de-
scribed in this section, we use the gold mentions
available in the data.
When one only has a small amount of train-
ing data, it is crucial to take advantage of external
knowledge such as the syntactico-semantic struc-
tures. To simulate this setting, in each fold, we ran-
domly selected 10 documents from the fold?s avail-
able training documents (about 3% of the total 345
documents) as training data. We built one binary,
one coarse-grained, and one fine-grained classifier
for each fold.
In Section 2, we described how we trained a base-
line RE classifier (REbase) and a RE classifier using
the syntactico-semantic patterns (REs).
We first apply REbase on each test example men-
tion pair (mi,mj) to obtain the RE baseline results,
showing these in Table 4 under the column ?10 doc-
uments?, and in the rows ?Binary?, ?Coarse?, and
?Fine?. We then applied REs on the test exam-
ples as described in Section 2, showing the results
in the rows ?Binary+Patterns?, ?Coarse+Patterns?,
and ?Fine+Patterns?. The results show that by us-
ing syntactico-semantic structures, we obtain signif-
icant F-measure improvements of 8.3, 7.2, and 5.5
for binary, coarse-grained, and fine-grained relation
predictions respectively.
6.3 RE Evaluation Using Predicted Mentions
Next, we perform our experiments using predicted
mentions. ACE-2004 defines 7 coarse-grained entity
types, each of which are then refined into 43 fine-
557
 0
 1
 2
 3
 4
 5
 6
 7
 8
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80
RE
 F
1(%
) Im
pro
ve
me
nt
Proportion (%) of data used for training
Improvement in (gold mentions) RE by using patterns
Binary+Pattern
Coarse+Pattern
Fine+Pattern
Figure 3: Improvement in (gold mention) RE.
grained entity types. Using the ACE data annotated
with mentions and predefined entity types, we build
a fine-grained mention entity typing (MET) clas-
sifier to disambiguate between 44 labels (43 fine-
grained and a null label to indicate not a mention).
To obtain the coarse-grained entity type predictions
from the classifier, we simply check which coarse-
grained type the fine-grained prediction belongs to.
We use the LIBLINEAR package with the same set-
tings as earlier specified for the RE system. In each
fold, we build a MET classifier using all the (276)
training documents in that fold.
We apply REbase on all mention pairs (mi,mj)
where both mi and mj have non null entity type pre-
dictions. We show these baseline results in the Rows
?Binary?, ?Coarse?, and ?Fine? of Table 5.
In Section 2, we described our algorithmic ap-
proach (Figure 2) that takes advantage of the struc-
tures with predicted mentions. We show the results
of this approach in the Rows ?Binary+Patterns?,
?Coarse+Patterns?, and ?Fine+Patterns? of Table
5. The results show that by leveraging syntactico-
semantic structures, we obtain significant F-measure
improvements of 8.2, 4.6, and 3.6 for binary, coarse-
grained, and fine-grained relation predictions re-
spectively.
7 Analysis
We first show statistics regarding the syntactico-
semantic structures. In Section 3, we mentioned
that ACE-2004 identified five types of structures:
premodifier, possessive, preposition, formulaic, and
 0
 1
 2
 3
 4
 5
 6
 7
 8
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80
RE
 F
1(%
) Im
pro
ve
me
nt
Proportion (%) of data used for training
Improvement in (predicted mentions) RE by using patterns
Binary+Pattern
Coarse+Pattern
Fine+Pattern
Figure 4: Improvement in (predicted mention) RE.
Pattern type Rec% Pre%
PreMod 86.8 79.7
Poss 94.3 88.3
Prep 94.6 20.0
Formula 85.5 62.2
Table 6: Recall and precision of the patterns.
verbal. On the 4,011 examples that we experimented
on, premodifiers are the most frequent, account-
ing for 30.5% of the examples (or about 1,224 ex-
amples). The occurrence distributions of the other
structures are 18.9% (possessive), 23.9% (preposi-
tion), 7.2% (formulaic), and 19.5% (verbal). Hence,
the four syntactico-semantic structures that we fo-
cused on in this paper account for a large majority
(80%) of the relations.
In Section 6, we note that out of 55,520 men-
tion pairs, only 4,011 exhibit valid relations. Thus,
the proportion of positive relation examples is very
sparse at 7.2%. If we can effectively identify and
discard most of the negative relation examples, it
should improve RE performance, including yielding
training data with a more balanced label distribution.
We now analyze the utility of the patterns. As
shown in Table 6, the patterns are effective in infer-
ring the structure of mention pairs. For instance, ap-
plying the premodifier patterns on the 55,520 men-
tion pairs, we correctly identified 86.8% of the 1,224
premodifier occurrences as premodifiers, while in-
curring a false-positive rate of only about 20%6. We
6Random selection will give a precision of about 2.2%
(1,224 out of 55,520) and thus a false-positive rate of 97.8%
558
note that preposition structures are relatively harder
to identify. Some of the reasons are due to possi-
bly multiple prepositions in between a mention pair,
preposition sense ambiguity, pp-attachment ambigu-
ity, etc. However, in general, we observe that infer-
ring the structures allows us to discard a large por-
tion of the mention pairs which have no valid re-
lation between them. The intuition behind this is
the following: if we infer that there is a syntactico-
semantic structure between a mention pair, then it
is likely that the mention pair exhibits a valid rela-
tion. Conversely, if there is a valid relation between
a mention pair, then it is likely that there exists a
syntactico-semantic structure between the mentions.
Next, we repeat the experiments in Section 6.2
and Section 6.3, while gradually increasing the
amount of training data used for training the RE
classifiers. The detailed results of using 5% and 80%
of all available data are shown in Table 4 and Table
5. Note that these settings are with respect to all 345
documents and thus the 80% setting represents us-
ing all 276 training documents in each fold. We plot
the intermediate results in Figure 3 and Figure 4. We
note that leveraging the structures provides improve-
ments on all experimental settings. Also, intuitively,
the binary predictions benefit the most from lever-
aging the structures. How to further exploit this is a
possible future work.
8 Conclusion
In this paper, we propose a novel algorithmic ap-
proach to RE by exploiting syntactico-semantic
structures. We show that this approach provides
several advantages and improves RE performance.
There are several interesting directions for future
work. There are probably many near misses when
we apply our structure patterns on predicted men-
tions. For instance, for both premodifier and posses-
sive structures, we require that one mention com-
pletely includes the other. Relaxing this might
potentially recover additional valid mention pairs
and improve performance. We could also try to
learn classifiers to automatically identify and disam-
biguate between the different syntactico-semantic
structures. It will also be interesting to feedback the
predictions of the structure patterns to the mention
entity typing classifier and possibly retrain to obtain
a better classifier.
Acknowledgements This research is supported by
the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
We thank Ming-Wei Chang and Quang Do for
building the mention extraction system.
References
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Yee Seng Chan and Dan Roth. 2010. Exploiting back-
ground knowledge for relation extraction. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING), pages 152?160.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer. 2007.
Relex ? Relation extraction using dependency parse
trees. Bioinformatics, 23(3):365?371.
Mark A. Greenwood and Mark Stevenson. 2006. Im-
proving semi-supervised acquisition of relation extrac-
tion patterns. In Proceedings of the COLING-ACL
Workshop on Information Extraction Beyond The Doc-
ument, pages 29?35.
Jing Jiang and ChengXiang Zhai. 2007. A systematic
exploration of the feature space for relation extraction.
In Proceedings of Human Language Technologies -
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL), pages 113?120.
Jing Jiang. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics and International Joint Confer-
ence on Natural Language Processing (ACL-IJCNLP),
pages 1012?1020.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for information extraction. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 178?181.
559
Dan Klein and Christoper D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In The Conference on Advances in Neural
Information Processing Systems (NIPS), pages 3?10.
Longhua Qian, Guodong Zhou, Qiaomin Zhu, and Peide
Qian. 2008. Relation extraction using convolution
tree kernel expanded with entity features. In Pacific
Asia Conference on Language, Information and Com-
putation, pages 415?421.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Annual Conference on Computational
Natural Language Learning (CoNLL), pages 147?155.
Dan Roth and Wen Tau Yih. 2007. Global inference for
entity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac-
tion. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
427?434.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-
Rong Wen. 2009. Statsnowball: a statistical approach
to extracting entity relationships. In The International
World Wide Web Conference, pages 101?110.
560
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 924?933,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Algorithm Selection and Model Adaptation for ESL Correction Tasks
Alla Rozovskaya and Dan Roth
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,danr}@illinois.edu
Abstract
We consider the problem of correcting errors
made by English as a Second Language (ESL)
writers and address two issues that are essen-
tial to making progress in ESL error correction
- algorithm selection and model adaptation to
the first language of the ESL learner.
A variety of learning algorithms have been
applied to correct ESL mistakes, but often
comparisons were made between incompara-
ble data sets. We conduct an extensive, fair
comparison of four popular learning methods
for the task, reversing conclusions from ear-
lier evaluations. Our results hold for different
training sets, genres, and feature sets.
A second key issue in ESL error correction
is the adaptation of a model to the first lan-
guage of the writer. Errors made by non-native
speakers exhibit certain regularities and, as we
show, models perform much better when they
use knowledge about error patterns of the non-
native writers. We propose a novel way to
adapt a learned algorithm to the first language
of the writer that is both cheaper to imple-
ment and performs better than other adapta-
tion methods.
1 Introduction
There has been a lot of recent work on correct-
ing writing mistakes made by English as a Second
Language (ESL) learners (Izumi et al, 2003; Eeg-
Olofsson and Knuttson, 2003; Han et al, 2006; Fe-
lice and Pulman, 2008; Gamon et al, 2008; Tetreault
and Chodorow, 2008; Elghaari et al, 2010; Tetreault
et al, 2010; Gamon, 2010; Rozovskaya and Roth,
2010c). Most of this work has focused on correcting
mistakes in article and preposition usage, which are
some of the most common error types among non-
native writers of English (Dalgish, 1985; Bitchener
et al, 2005; Leacock et al, 2010). Examples below
illustrate some of these errors:
1. ?They listen to None*/the lecture carefully.?
2. ?He is an engineer with a passion to*/for what he
does.?
In (1) the definite article is incorrectly omitted. In
(2), the writer uses an incorrect preposition.
Approaches to correcting preposition and article
mistakes have adopted the methods of the context-
sensitive spelling correction task, which addresses
the problem of correcting spelling mistakes that re-
sult in legitimate words, such as confusing their
and there (Carlson et al, 2001; Golding and Roth,
1999). A candidate set or a confusion set is defined
that specifies a list of confusable words, e.g., {their,
there}. Each occurrence of a confusable word in text
is represented as a vector of features derived from a
context window around the target, e.g., words and
part-of-speech tags. A classifier is trained on text
assumed to be error-free. At decision time, for each
word in text, e.g. there, the classifier predicts the
most likely candidate from the corresponding con-
fusion set {their, there}.
Models for correcting article and preposition er-
rors are similarly trained on error-free native English
text, where the confusion set includes all articles
or prepositions (Izumi et al, 2003; Eeg-Olofsson
and Knuttson, 2003; Han et al, 2006; Felice and
Pulman, 2008; Gamon et al, 2008; Tetreault and
Chodorow, 2008; Tetreault et al, 2010).
924
Although the choice of a particular learning al-
gorithm differs, with the exception of decision trees
(Gamon et al, 2008), all algorithms used are lin-
ear learning algorithms, some discriminative (Han
et al, 2006; Felice and Pulman, 2008; Tetreault
and Chodorow, 2008; Rozovskaya and Roth, 2010c;
Rozovskaya and Roth, 2010b), some probabilistic
(Gamon et al, 2008; Gamon, 2010), or ?counting?
(Bergsma et al, 2009; Elghaari et al, 2010).
While model comparison has not been the goal
of the earlier studies, it is quite common to com-
pare systems, even when they are trained on dif-
ferent data sets and use different features. Further-
more, since there is no shared ESL data set, sys-
tems are also evaluated on data from different ESL
sources or even on native data. Several conclusions
have been made when comparing systems devel-
oped for ESL correction tasks. A language model
was found to outperform a maximum entropy classi-
fier (Gamon, 2010). However, the language model
was trained on the Gigaword corpus, 17 ? 109 words
(Linguistic Data Consortium, 2003), a corpus sev-
eral orders of magnitude larger than the corpus used
to train the classifier. Similarly, web-based models
built on Google Web1T 5-gram Corpus (Bergsma et
al., 2009) achieve better results when compared to a
maximum entropy model that uses a corpus 10, 000
times smaller (Chodorow et al, 2007)1.
In this work, we compare four popular learning
methods applied to the problem of correcting prepo-
sition and article errors and evaluate on a common
ESL data set. We compare two probabilistic ap-
proaches ? Na??ve Bayes and language modeling; a
discriminative algorithm Averaged Perceptron; and a
count-based method SumLM (Bergsma et al, 2009),
which, as we show, is very similar to Na??ve Bayes,
but with a different free coefficient. We train our
models on data from several sources, varying train-
ing sizes and feature sets, and show that there are
significant differences in the performance of these
algorithms. Contrary to previous results (Bergsma et
al., 2009; Gamon, 2010), we find that when trained
on the same data with the same features, Averaged
Perceptron achieves the best performance, followed
by Na??ve Bayes, then the language model, and fi-
nally the count-based approach. Our results hold for
1These two models also use different features.
training sets of different sizes, genres, and feature
sets. We also explain the performance differences
from the perspective of each algorithm.
The second important question that we address is
that of adapting the decision to the source language
of the writer. Errors made by non-native speakers
exhibit certain regularities. Adapting a model so
that it takes into consideration the specific error pat-
terns of the non-native writers was shown to be ex-
tremely helpful in the context of discriminative clas-
sifiers (Rozovskaya and Roth, 2010c; Rozovskaya
and Roth, 2010b). However, this method requires
generating new training data and training a separate
classifier for each source language. Our key contri-
bution here is a novel, simple, and elegant adaptation
method within the framework of the Na??ve Bayes
algorithm, which yields even greater performance
gains. Specifically, we show how the error patterns
of the non-native writers can be viewed as a different
distribution on candidate priors in the confusion set.
Following this observation, we train Na??ve Bayes in
a traditional way, regardless of the source language
of the writer, and then, only at decision time, change
the prior probabilities of the model from the ones
observed in the native training data to the ones corre-
sponding to error patterns in the non-native writer?s
source language (Section 4). A related idea has been
applied in Word Sense Disambiguation to adjust the
model priors to a new domain with different sense
distributions (Chan and Ng, 2005).
The paper has two main contributions. First, we
conduct a fair comparison of four learning algo-
rithms and show that the discriminative approach
Averaged Perceptron is the best performing model
(Sec. 3). Our results do not support earlier conclu-
sions with respect to the performance of count-based
models (Bergsma et al, 2009) and language mod-
els (Gamon, 2010). In fact, we show that SumLM
is comparable to Averaged Perceptron trained with
a 10 times smaller corpus, and language model is
comparable to Averaged Perceptron trained with a 2
times smaller corpus.
The second, and most significant, of our contribu-
tions is a novel way to adapt a model to the source
language of the writer, without re-training the model
(Sec. 4). As we show, adapting to the source lan-
guage of the writer provides significant performance
improvement, and our new method also performs
925
better than previous, more complicated methods.
Section 2 presents the theoretical component of
the linear learning framework. In Section 3, we
describe the experiments, which compare the four
learning models. Section 4 presents the key result of
this work, a novel method of adapting the model to
the source language of the learner.
2 The Models
The standard approach to preposition correction
is to cast the problem as a multi-class classifica-
tion task and train a classifier on features defined
on the surrounding context2. The model selects
the most likely candidate from the confusion set,
where the set of candidates includes the top n most
frequent English prepositions. Our confusion set
includes the top ten prepositions3: ConfSet =
{on, from, for, of, about, to, at, in, with, by}. We
use p to refer to a candidate preposition from
ConfSet.
Let preposition context denote the preposition and
the window around it. For instance, ?a passion to
what he? is a context for window size 2. We use
three feature sets, varying window size from 2 to 4
words on each side (see Table 1). All feature sets
consist of word n-grams of various lengths span-
ning p and all the features are of the form s?kps+m,
where s?k and s+m denote k words before and m
words after p; we show two 3-gram features for il-
lustration:
1. a passion p
2. passion p what
We implement four linear learning models: the
discriminative method Averaged Perceptron (AP);
two probabilistic methods ? a language model (LM)
and Na??ve Bayes (NB); and a ?counting? method
SumLM (Bergsma et al, 2009).
Each model produces a score for a candidate in
the confusion set. Since all of the models are lin-
ear, the hypotheses generated by the algorithms dif-
fer only in the weights they assign to the features
2We also report one experiment on the article correction
task. We take the preposition correction task as an example;
the article case is treated in the same way.
3This set of prepositions is also considered in other works,
e.g. (Rozovskaya and Roth, 2010b). The usage of the ten most
frequent prepositions accounts for 82% of all preposition errors
(Leacock et al, 2010).
Feature Preposition context N-gram
set lengths
Win2 a passion [to] what he 2,3,4
Win3 with a passion [to] what he does 2,3,4
Win4 engineer with a passion [to] what he does . 2,3,4,5
Table 1: Description of the three feature sets used in
the experiments. All feature sets consist of word n-grams
of various lengths spanning the preposition and vary by
n-gram length and window size.
Method Free Coefficient Feature weights
AP bias parameter mistake-driven
LM ? ? prior(p)
?
vl?vr
?vr ? log(P (u|vr))
NB log(prior(p)) log(P (f |p))
SumLM |F (S, p)| ? log(C(p)) log(P (f |p))
Table 2: Summary of the learning methods. C(p) de-
notes the number of times preposition p occurred in train-
ing. ? is a smoothing parameter, u is the rightmost word
in f , vl ? vr denotes all concatenations of substrings vl
and vr of feature f without u.
(Roth, 1998; Roth, 1999). Thus a score computed
by each of the models for a preposition p in the con-
text S can be expressed as follows:
g(S, p) = C(p) +
?
f?F (S,p)
wa(f), (1)
where F (S, p) is the set of features active in con-
text S relative to preposition p, wa(f) is the weight
algorithm a assigns to feature f ? F , and C(p) is
a free coefficient. Predictions are made using the
winner-take-all approach: argmaxpg(S, p). The al-
gorithms make use of the same feature set F and
differ only by how the weights wa(f) and C(p) are
computed. Below we explain how the weights are
determined in each method. Table 2 summarizes the
four approaches.
2.1 Averaged Perceptron
Discriminative classifiers represent the most com-
mon learning paradigm in error correction. AP (Fre-
und and Schapire, 1999) is a discriminative mistake-
driven online learning algorithm. It maintains a vec-
tor of feature weights w and processes one training
example at a time, updating w if the current weight
assignment makes a mistake on the training exam-
ple. In the case of AP, the C(p) coefficient refers to
the bias parameter (see Table 2).
926
We use the regularized version of AP in Learn-
ing Based Java4 (LBJ, (Rizzolo and Roth, 2007)).
While classical Perceptron comes with a generaliza-
tion bound related to the margin of the data, Aver-
aged Perceptron also comes with a PAC-like gener-
alization bound (Freund and Schapire, 1999). This
linear learning algorithm is known, both theoreti-
cally and experimentally, to be among the best linear
learning approaches and is competitive with SVM
and Logistic Regression, while being more efficient
in training. It also has been shown to produce state-
of-the-art results on many natural language applica-
tions (Punyakanok et al, 2008).
2.2 Language Modeling
Given a feature f = s?kps+m, let u denote the
rightmost word in f and vl ? vr denote all concate-
nations of substrings vl and vr of feature f without
u. The language model computes several probabil-
ities of the form P (u|vr). If f =?with a passion
p what?, then u =?what?, and vr ? {?with a pas-
sion p?, ?a passion p?, ?passion p?, ?p? }. In prac-
tice, these probabilities are smoothed and replaced
with their corresponding log values, and the total
weight contribution of f to the scoring function of
p is
?
vl?vr
?vr ? log(P (u|vr)). In addition, this
scoring function has a coefficient that only depends
on p: C(p) = ? ? prior(p) (see Table 2). The prior
probability of a candidate p is:
prior(p) =
C(p)
?
q?ConfSetC(q)
, (2)
where C(p) and C(q) denote the number of
times preposition p and q, respectively, occurred in
the training data. We implement a count-based
LM with Jelinek-Mercer linear interpolation as a
smoothing method5 (Chen and Goodman, 1996),
where each n-gram length, from 1 to n, is associated
with an interpolation smoothing weight ?. Weights
are optimized on a held-out set of ESL sentences.
Win2 and Win3 features correspond to 4-gram
LMs and Win4 to 5-gram LMs. Language models
are trained with SRILM (Stolcke, 2002).
4LBJ can be downloaded from http://cogcomp.cs.
illinois.edu.
5Unlike other LM methods, this approach allows us to train
LMs on very large data sets. Although we found that backoff
LMs may perform slightly better, they still maintain the same
hierarchy in the order of algorithm performance.
2.3 Na??ve Bayes
NB is another linear model, which is often hard to
beat using more sophisticated approaches. NB ar-
chitecture is also particularly well-suited for adapt-
ing the model to the first language of the writer (Sec-
tion 4). Weights in NB are determined, similarly to
LM, by the feature counts and the prior probability
of each candidate p (Eq. (2)). For each candidate
p, NB computes the joint probability of p and the
feature space F , assuming that the features are con-
ditionally independent given p:
g(S, p) = log{prior(p) ?
?
f?F (S,p)
P (f |p)}
= log(prior(p)) +
+
?
f?F (S,p)
log(P (f |p)) (3)
NB weights and its free coefficient are also summa-
rized in Table 2.
2.4 SumLM
For candidate p, SumLM (Bergsma et al, 2009)6
produces a score by summing over the logs of all
feature counts:
g(S, p) =
?
f?F (S,p)
log(C(f))
=
?
f?F (S,p)
log(P (f |p)C(p))
= |F (S, p)|C(p) +
?
f?F (S,p)
log(P (f |p))
where C(f) denotes the number of times n-gram
feature f was observed with p in training. It should
be clear from equation 3 that SumLM is very similar
to NB, with a different free coefficient (Table 2).
3 Comparison of Algorithms
3.1 Evaluation Data
We evaluate the models using a corpus of ESL es-
says, annotated7 by native English speakers (Ro-
zovskaya and Roth, 2010a). For each preposition
6SumLM is one of several related methods proposed in this
work; its accuracy on the preposition selection task on native
English data nearly matches the best model, SuperLM (73.7%
vs. 75.4%), while being much simpler to implement.
7The annotation of the ESL corpus can be downloaded from
http://cogcomp.cs.illinois.edu.
927
Source Prepositions Articles
language Total Incorrect Total Incorrect
Chinese 953 144 1864 150
Czech 627 28 575 55
Italian 687 43 - -
Russian 1210 85 2292 213
Spanish 708 52 - -
All 4185 352 4731 418
Table 3: Statistics on prepositions and articles in the
ESL data. Column Incorrect denotes the number of
cases judged to be incorrect by the annotator.
(article) used incorrectly, the annotator indicated the
correct choice. The data include sentences by speak-
ers of five first languages. Table 3 shows statistics by
the source language of the writer.
3.2 Training Corpora
We use two training corpora. The first corpus,
WikiNYT, is a selection of texts from English
Wikipedia and the New York Times section of the
Gigaword corpus and contains 107 preposition con-
texts. We build models of 3 sizes8: 106, 5 ? 106, and
107.
To experiment with larger data sets, we use the
Google Web1T 5-gram Corpus, which is a collec-
tion of n-gram counts of length one to five over a
corpus of 1012 words. The corpus contains 2.6 ?1010
prepositions. We refer to this corpus as GoogleWeb.
We stress that GoogleWeb does not contain com-
plete sentences, but only n-gram counts. Thus, we
cannot generate training data for AP for feature sets
Win3 and Win4: Since the algorithm does not as-
sume feature independence, we need to have 7 and
9-word sequences, respectively, with a preposition
in the middle (as shown in Table 1) and their corpus
frequencies. The other three models can be eval-
uated with the n-gram counts available. For exam-
ple, we compute NB scores by obtaining the count
of each feature independently, e.g. the count for left
context 5-gram ?engineer with a passion p? and right
context 5-gram ?p what he does .?, due to the con-
ditional independence assumption that NB makes.
On GoogleWeb, we train NB, SumLM, and LM with
three feature sets: Win2, Win3, and Win4.
From GoogleWeb, we also generate a smaller
training set of size 108: We use 5-grams with
a preposition in the middle and generate a new
8Training size refers to the number of preposition contexts.
count, proportional to the size of the smaller cor-
pus9. For instance, a preposition 5-gram with a
count of 2600 in GoogleWeb, will have a count of
10 in GoogleWeb-108.
3.3 Results
Our key results of the fair comparison of the four
algorithms are shown in Fig. 1 and summarized in
Table 4. The table shows that AP trained on 5 ? 106
preposition contexts performs as well as NB trained
on 107 (i.e., with twice as much data; the perfor-
mance of LM trained on 107 contexts is better than
that of AP trained with 10 times less data (106), but
not as good as that of AP trained with half as much
data (5?106); AP outperforms SumLM, when the lat-
ter uses 10 times more data. Fig. 1 demonstrates the
performance results reported in Table 4; it shows the
behavior of different systems with respect to preci-
sion and recall on the error correction task. We gen-
erate the curves by varying the decision threshold on
the confidence of the classifier (Carlson et al, 2001)
and propose a correction only when the confidence
of the classifier is above the threshold. A higher pre-
cision and a lower recall are obtained when the de-
cision threshold is high, and vice versa.
Key results
AP > NB > LM > SumLM
AP ? 2 ?NB
5 ?AP > 10 ? LM > AP
AP > 10 ? SumLM
Table 4: Key results on the comparison of algorithms.
2 ?NB refers to NB trained with twice as much data as
AP ; 10 ? LM refers to LM trained with 10 times more
data asAP ; 10?SumLM refers to SumLM trained with
10 times more data as AP . These results are also shown
in Fig. 1.
We now show a fair comparison of the four algo-
rithms for different window sizes, training data and
training sizes. Figure 2 compares the models trained
on WikiNY T -107 corpus for Win4. AP is the su-
perior model, followed by NB, then LM, and finally
SumLM.
Results for other training sizes and feature10 set
9Scaling down GoogleWeb introduces some bias but we be-
lieve that it should not have an effect on our experiments.
10We have also experimented with additional POS-based fea-
tures that are commonly used in these tasks and observed simi-
lar behavior.
928
 
0
 
10
 
20
 
30
 
40
 
50
 
60  0
 
10
 
20
 
30
 
40
 
50
 
60
PRECISION
RECA
LL
SumL
M-10
7
LM-1
07
NB-1
07
AP-1
06
AP-5
*106 AP-1
07
Figure 1: Algorithm comparison across different
training sizes. (WikiNYT, Win3). AP (106 preposition
contexts) performs as well as SumLM with 10 times more
data, and LM requires at least twice as much data to
achieve the performance of AP.
configurations show similar behavior and are re-
ported in Table 5, which provides model compari-
son in terms of Average Area Under Curve (AAUC,
(Hanley and McNeil, 1983)). AAUC is a measure
commonly used to generate a summary statistic and
is computed here as an average precision value over
12 recall points (from 5 to 60):
AAUC =
1
12
?
12?
i=1
Precision(i ? 5)
The Table also shows results on the article correc-
tion task11.
Training data Feature Performance (AAUC)
set AP NB LM SumLM
WikiNY T -5 ? 106 Win3 26 22 20 13
WikiNY T -107 Win4 33 28 24 16
GoogleWeb-108 Win2 30 29 28 15
GoogleWeb Win4 - 44 41 32
Article
WikiNY T -5 ? 106 Win3 40 39 - 30
Table 5: Performance Comparison of the four algo-
rithms for different training data, training sizes, and win-
dow sizes. Each row shows results for training data of the
same size. The last row shows performance on the article
correction task. All other results are for prepositions.
11We do not evaluate the LM approach on the article correc-
tion task, since with LM it is difficult to handle missing article
errors, one of the most common error types for articles, but the
expectation is that it will behave as it does for prepositions.
 
0
 
10
 
20
 
30
 
40
 
50
 
60  0
 
10
 
20
 
30
 
40
 
50
 
60
PRECISION
RECA
LL
SumL
M LM NB AP
Figure 2: Model Comparison for training data of the
same size: Performance of models for feature set Win4
trained on WikiNY T -107.
3.3.1 Effects of Window Size
We found that expanding window size from 2 to 3
is helpful for all of the models, but expanding win-
dow to 4 is only helpful for the models trained on
GoogleWeb (Table 6). Compared to Win3, Win4 has
five additional 5-gram features. We look at the pro-
portion of features in the ESL data that occurred in
two corpora: WikiNY T -107 and GoogleWeb (Ta-
ble 7). We observe that only 4% of test 5-grams oc-
cur inWikiNY T -107. This number goes up 7 times
to 28% for GoogleWeb, which explains why increas-
ing the window size is helpful for this model. By
comparison, a set of native English sentences (dif-
ferent from the training data) has 50% more 4-grams
and about 3 times more 5-grams, because ESL sen-
tences often contain expressions not common for na-
tive speakers.
Training data Performance (AAUC)
Win2 Win3 Win4
GoogleWeb 35 39 44
Table 6: Effect of Window Size in terms ofAAUC. Per-
formance improves, as the window increases.
4 Adapting to Writer?s Source Language
In this section, we discuss adapting error correction
systems to the first language of the writer. Non-
native speakers make mistakes in a systematic man-
ner, and errors often depend on the first language of
the writer (Lee and Seneff, 2008; Rozovskaya and
929
Test Train N-gram length
2 3 4 5
ESL WikiNY T -107 98% 66% 22% 4%
Native WikiNY T -107 98% 67% 32% 13%
ESL GoogleWeb 99% 92% 64% 28%
Native-B09 GoogleWeb - 99% 93% 70%
Table 7: Feature coverage for ESL and native data.
Percentage of test n-gram features that occurred in train-
ing. Native refers to data from Wikipedia and NYT. B09
refers to statistics from Bergsma et al (2009).
Roth, 2010a). For instance, a Chinese learner of
English might say ?congratulations to this achieve-
ment? instead of ?congratulations on this achieve-
ment?, while a Russian speaker might say ?congrat-
ulations with this achievement?.
A system performs much better when it makes use
of knowledge about typical errors. When trained
on annotated ESL data instead of native data, sys-
tems improve both precision and recall (Han et al,
2010; Gamon, 2010). Annotated data include both
the writer?s preposition and the intended (correct)
one, and thus the knowledge about typical errors is
made available to the system.
Another way to adapt a model to the first language
is to generate in native training data artificial errors
mimicking the typical errors of the non-native writ-
ers (Rozovskaya and Roth, 2010c; Rozovskaya and
Roth, 2010b). Henceforth, we refer to this method,
proposed within the discriminative framework AP,
as AP-adapted. To determine typical mistakes, error
statistics are collected on a small set of annotated
ESL sentences. However, for the model to use these
language-specific error statistics, a separate classi-
fier for each source language needs to be trained.
We propose a novel adaptation method, which
shows performance improvement over AP-adapted.
Moreover, this method is much simpler to imple-
ment, since there is no need to train per source lan-
guage; only one classifier is trained. The method
relies on the observation that error regularities can
be viewed as a distribution on priors over the cor-
rection candidates. Given a preposition s in text, the
prior for candidate p is the probability that p is the
correct preposition for s. If a model is trained on na-
tive data without adaptation to the source language,
candidate priors correspond to the relative frequen-
cies of the candidates in the native training data.
More importantly, these priors remain the same re-
gardless of the source language of the writer or of
the preposition used in text. From the model?s per-
spective, it means that a correction candidate, for
example to, is equally likely given that the author?s
preposition is for or from, which is clearly incorrect
and disagrees with the notion that errors are regular
and language-dependent.
We use the annotated ESL data and define
adapted candidate priors that are dependent on the
author?s preposition and the author?s source lan-
guage. Let s be a preposition appearing in text by
a writer of source language L1, and p a correction
candidate. Then the adapted prior of p given s is:
prior(p, s, L1) =
CL1(s, p)
CL1(s)
,
where CL1(s) denotes the number of times s ap-
peared in the ESL data by L1 writers, and CL1(s, p)
denotes the number of times p was the correct prepo-
sition when s was used by an L1 writer.
Table 8 shows adapted candidate priors for two
author?s choices ? when an ESL writer used on and
at ? based on the data from Chinese learners. One
key distinction of the adapted priors is the high prob-
ability assigned to the author?s preposition: the new
prior for on given that it is also the preposition found
in text is 0.70, vs. the 0.07 prior based on the native
data. The adapted prior of preposition p, when p is
used, is always high, because the majority of prepo-
sitions are used correctly. Higher probabilities are
also assigned to those candidates that are most often
observed as corrections for the author?s preposition.
For example, the adapted prior for at when the writer
chose on is 0.10, since on is frequently incorrectly
chosen instead of at.
To determine a mechanism to inject the adapted
priors into a model, we note that while all of our
models use priors in some way, NB architecture di-
rectly specifies the prior probability as one of its pa-
rameters (Sec. 2.3). We thus train NB in a traditional
way, on native data, and then replace the prior com-
ponent in Eq. (3) with the adapted prior, language
and preposition dependent, to get the score for p of
the NB-adapted model:
g(S, p) = log{prior(p, s, L1) ?
?
f?F (S,p)
P (f |p)}
930
Candidate Global Adapted prior
prior author?s prior author?s prior
choice choice
of 0.25 on 0.03 at 0.02
to 0.22 on 0.06 at 0.00
in 0.15 on 0.04 at 0.16
for 0.10 on 0.00 at 0.03
on 0.07 on 0.70 at 0.09
by 0.06 on 0.00 at 0.02
with 0.06 on 0.04 at 0.00
at 0.04 on 0.10 at 0.75
from 0.04 on 0.00 at 0.02
about 0.01 on 0.03 at 0.00
Table 8: Examples of adapted candidate priors for
two author?s choices ? on and at ? based on the er-
rors made by Chinese learners. Global prior denotes
the probability of the candidate in the standard model
and is based on the relative frequency of the candidate
in native training data. Adapted priors are dependent on
the author?s preposition and the author?s first language.
Adapted priors for the author?s choice are very high.
Other candidates are given higher priors if they often ap-
pear as corrections for the author?s choice.
We stress that in the new method there is no need
to train per source language, as with previous adap-
tion methods. Only one model is trained, and only
at decision time, we change the prior probabilities of
the model. Also, while we need a lot of data to train
the model, only one parameter depends on annotated
data. Therefore, with rather small amounts of data, it
is possible to get reasonably good estimates of these
prior parameters.
In the experiments below, we compare four mod-
els: AP, NB AP-adapted and NB-adapted. AP-
adapted is the adaptation through artificial errors
and NB-adapted is the method proposed here. Both
of the adapted models use the same error statistics in
k-fold cross-validation (CV): We randomly partition
the ESL data into k parts, with each part tested on
the model that uses error statistics estimated on the
remaining k ? 1 parts. We also remove all prepo-
sition errors that occurred only once (23% of all er-
rors) to allow for a better evaluation of the adapted
models. Although we observe similar behavior on
all the data, the models especially benefit from the
adapted priors when a particular error occurred more
than once. Since the majority of errors are not due
to chance, we focus on those errors that the writers
will make repeatedly.
Fig. 3 shows the four models trained on
WikiNY T -107. First, we note that the adapted
 
0
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80  0
 
10
 
20
 
30
 
40
 
50
PRECISION
RECA
LL
NB-a
dapte
d
AP-a
dapte
d AP NB
Figure 3: Adapting to Writer?s Source Language. NB-
adapted is the method proposed here. AP-adapted and
NB-adapted results are obtained using 2-fold CV, with
50% of the ESL data used for estimating the new priors.
All models are trained on WikiNY T -107.
models outperform their non-adapted counterparts
with respect to precision. Second, for the recall
points less than 20%, the adapted models obtain very
similar precision values. This is interesting, espe-
cially because NB does not perform as well as AP, as
we also showed in Sec. 3.3. Thus, NB-adapted not
only improves over NB, but its gap compared to the
latter is much wider than the gap between the AP-
based systems. Finally, an important performance
distinction between the two adapted models is the
loss in recall exhibited by AP-adapted ? its curve is
shorter because AP-adapted is very conservative and
does not propose many corrections. In contrast, NB-
adapted succeeds in improving its precision over NB
with almost no recall loss.
To evaluate the effect of the size of the data used
to estimate the new priors, we compare the perfor-
mance of NB-adapted models in three settings: 2-
fold CV, 10-fold CV, and Leave-One-Out (Figure 4).
In 2-fold CV, priors are estimated on 50% of the ESL
data, in 10-fold on 90%, and in Leave-One-Out on
all data but the testing example. Figure 4 shows the
averaged results over 5 runs of CV for each setting.
The model converges very quickly: there is almost
no difference between 10-fold CV and Leave-One-
Out, which suggests that we can get a good estimate
of the priors using just a little annotated data.
Table 9 compares NB and NB-adapted for two
corpora: WikiNY T -107 and GoogleWeb. Since
931
 
0
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80
 
90  0
 
10
 
20
 
30
 
40
 
50
 
60
PRECISION
RECA
LL
NB-a
dapte
d-Lea
veOn
eOut
NB-a
dapte
d-10-
fold
NB-a
dapte
d-2-fo
ld NB
Figure 4: How much data are needed to estimate
adapted priors. Comparison of NB-adapted models
trained on GoogleWeb that use different amounts of data
to estimate the new priors. In 2-fold CV, priors are es-
timated on 50% of the data; in 10-fold on 90% of the
data; in Leave-One-Out, the new priors are based on all
the data but the testing example.
GoogleWeb is several orders of magnitude larger,
the adapted model behaves better for this corpus.
So far, we have discussed performance in terms
of precision and recall, but we can also discuss it
in terms of accuracy, to see how well the algorithm
is performing compared to the baseline on the task.
Following Rozovskaya and Roth (2010c), we con-
sider as the baseline the accuracy of the ESL data
before applying the model12, or the percentage of
prepositions used correctly in the test data. From
Table 3, the baseline is 93.44%13. Compared to
this high baseline, NB trained on WikiNY T -107
achieves an accuracy of 93.54, and NB-adapted
achieves an accuracy of 93.9314.
Training data Algorithms
NB NB-adapted
WikiNY T -107 29 53
GoogleWeb 38 62
Table 9: Adapting to writer?s source language. Re-
sults are reported in terms of AAUC. NB-adapted is the
model with adapted priors. Results for NB-adapted are
based on 10-fold CV.
12Note that this baseline is different from the majority base-
line used in the preposition selection task, since here we have
the author?s preposition in text.
13This is the baseline after removing the singleton errors.
14We select the best accuracy among different values that can
be achieved by varying the decision threshold.
5 Conclusion
We have addressed two important issues in ESL
error correction, which are essential to making
progress in this task. First, we presented an exten-
sive, fair comparison of four popular linear learning
models for the task and demonstrated that there are
significant performance differences between the ap-
proaches. Since all of the algorithms presented here
are linear, the only difference is in how they learn
the weights. Our experiments demonstrated that the
discriminative approach (AP) is able to generalize
better than any of the other models. These results
correct earlier conclusions, made with incompara-
ble data sets. The model comparison was performed
using two popular tasks ? correcting errors in article
and preposition usage ? and we expect that our re-
sults will generalize to other ESL correction tasks.
The second, and most important, contribution of
the paper is a novel method that allows one to
adapt the learned model to the source language of
the writer. We showed that error patterns can be
viewed as a distribution on priors over the correc-
tion candidates and proposed a method of injecting
the adapted priors into the learned model. In ad-
dition to performing much better than the previous
approaches, this method is also very cheap to im-
plement, since it does not require training a separate
model for each source language, but adapts the sys-
tem to the writer?s language at decision time.
Acknowledgments
The authors thank Nick Rizzolo for many helpful
discussions. The authors also thank Josh Gioja, Nick
Rizzolo, Mark Sammons, Joel Tetreault, Yuancheng
Tu, and the anonymous reviewers for their insight-
ful comments. This research is partly supported by
a grant from the U.S. Department of Education.
References
S. Bergsma, D. Lin, and R. Goebel. 2009. Web-scale
n-gram models for lexical disambiguation. In 21st In-
ternational Joint Conference on Artificial Intelligence,
pages 1507?1512.
J. Bitchener, S. Young, and D. Cameron. 2005. The ef-
fect of different types of corrective feedback on ESL
student writing. Journal of Second Language Writing.
A. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In Proceedings of the
932
National Conference on Innovative Applications of Ar-
tificial Intelligence (IAAI), pages 45?50.
Y. S. Chan and H. T. Ng. 2005. Word sense disambigua-
tion with distribution estimation. In Proceedings of
IJCAI 2005.
S. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Pro-
ceedings of ACL 1996.
M. Chodorow, J. Tetreault, and N.-R. Han. 2007. Detec-
tion of grammatical errors involving prepositions. In
Proceedings of the Fourth ACL-SIGSEM Workshop on
Prepositions, pages 25?30, Prague, Czech Republic,
June. Association for Computational Linguistics.
G. Dalgish. 1985. Computer-assisted ESL research.
CALICO Journal, 2(2).
J. Eeg-Olofsson and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. Nodalida.
A. Elghaari, D. Meurers, and H. Wunsch. 2010. Ex-
ploring the data-driven prediction of prepositions in
english. In Proceedings of COLING 2010, Beijing,
China.
R. De Felice and S. Pulman. 2008. A classifier-based ap-
proach to preposition and determiner error correction
in L2 English. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 169?176, Manchester, UK, August.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings of
IJCNLP.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?171,
Los Angeles, California, June.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
N. Han, M. Chodorow, and C. Leacock. 2006. Detecting
errors in English article usage by non-native speakers.
Journal of Natural Language Engineering, 12(2):115?
129.
N. Han, J. Tetreault, S. Lee, and J. Ha. 2010. Us-
ing an error-annotated learner corpus to develop and
ESL/EFL error correction system. In LREC, Malta,
May.
J. Hanley and B. McNeil. 1983. A method of comparing
the areas under receiver operating characteristic curves
derived from the same cases. Radiology, 148(3):839?
843.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the Japanese
learners? English spoken data. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
145?148, Sapporo, Japan, July.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Morgan and Claypool Publishers.
J. Lee and S. Seneff. 2008. An analysis of grammatical
errors in non-native speech in English. In Proceedings
of the 2008 Spoken Language Technology Workshop.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
N. Rizzolo and D. Roth. 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Inter-
national Conference on Semantic Computing (ICSC),
pages 597?604, Irvine, California, September. IEEE.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 806?813.
D. Roth. 1999. Learning in natural language. In Proc. of
the International Joint Conference on Artificial Intelli-
gence (IJCAI), pages 898?904.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
NAACL Workshop on Innovative Use of NLP for Build-
ing Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Generating con-
fusion sets for context-sensitive error correction. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
A. Rozovskaya and D. Roth. 2010c. Training paradigms
for correcting errors in grammar and usage. In Pro-
ceedings of the NAACL-HLT.
A. Stolcke. 2002. Srilm-an extensible language mod-
eling toolkit. In Proceedings International Confer-
ence on Spoken Language Processing, pages 257?286,
November.
J. Tetreault and M. Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
865?872, Manchester, UK, August.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In ACL.
933
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1375?1384,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Local and Global Algorithms for Disambiguation to Wikipedia
Lev Ratinov 1 Dan Roth1 Doug Downey2 Mike Anderson3
1University of Illinois at Urbana-Champaign
{ratinov2|danr}@uiuc.edu
2Northwestern University
ddowney@eecs.northwestern.edu
3Rexonomy
mrander@gmail.com
Abstract
Disambiguating concepts and entities in a con-
text sensitive way is a fundamental problem
in natural language processing. The compre-
hensiveness of Wikipedia has made the on-
line encyclopedia an increasingly popular tar-
get for disambiguation. Disambiguation to
Wikipedia is similar to a traditional Word
Sense Disambiguation task, but distinct in that
the Wikipedia link structure provides addi-
tional information about which disambigua-
tions are compatible. In this work we analyze
approaches that utilize this information to ar-
rive at coherent sets of disambiguations for a
given document (which we call ?global? ap-
proaches), and compare them to more tradi-
tional (local) approaches. We show that previ-
ous approaches for global disambiguation can
be improved, but even then the local disam-
biguation provides a baseline which is very
hard to beat.
1 Introduction
Wikification is the task of identifying and link-
ing expressions in text to their referent Wikipedia
pages. Recently, Wikification has been shown to
form a valuable component for numerous natural
language processing tasks including text classifica-
tion (Gabrilovich and Markovitch, 2007b; Chang et
al., 2008), measuring semantic similarity between
texts (Gabrilovich and Markovitch, 2007a), cross-
document co-reference resolution (Finin et al, 2009;
Mayfield et al, 2009), and other tasks (Kulkarni et
al., 2009).
Previous studies on Wikification differ with re-
spect to the corpora they address and the subset
of expressions they attempt to link. For exam-
ple, some studies focus on linking only named en-
tities, whereas others attempt to link all ?interest-
ing? expressions, mimicking the link structure found
in Wikipedia. Regardless, all Wikification systems
are faced with a key Disambiguation to Wikipedia
(D2W) task. In the D2W task, we?re given a text
along with explicitly identified substrings (called
mentions) to disambiguate, and the goal is to out-
put the corresponding Wikipedia page, if any, for
each mention. For example, given the input sen-
tence ?I am visiting friends in <Chicago>,? we
output http://en.wikipedia.org/wiki/Chicago ? the
Wikipedia page for the city of Chicago, Illinois, and
not (for example) the page for the 2002 film of the
same name.
Local D2W approaches disambiguate each men-
tion in a document separately, utilizing clues such
as the textual similarity between the document and
each candidate disambiguation?s Wikipedia page.
Recent work on D2W has tended to focus on more
sophisticated global approaches to the problem, in
which all mentions in a document are disambiguated
simultaneously to arrive at a coherent set of dis-
ambiguations (Cucerzan, 2007; Milne and Witten,
2008b; Han and Zhao, 2009). For example, if a
mention of ?Michael Jordan? refers to the computer
scientist rather than the basketball player, then we
would expect a mention of ?Monte Carlo? in the
same document to refer to the statistical technique
rather than the location. Global approaches utilize
the Wikipedia link graph to estimate coherence.
1375
m1 = Taiwan m2 = China m3 = Jiangsu Province..............
t1 = Taiwan t5 =People's Republic of China t7 = Jiangsu
..............
Document text  with mentions
t2 = Chinese Taipei t3 =Republic of China t4 = China t6 = History of China
?(m1, t1)
?(m1, t2)
?(m1, t3)
?(t1, t7) ?(t3, t7) ?(t5, t7)
Figure 1: Sample Disambiguation to Wikipedia problem with three mentions. The mention ?Jiangsu? is unambiguous.
The correct mapping from mentions to titles is marked by heavy edges
In this paper, we analyze global and local ap-
proaches to the D2W task. Our contributions are
as follows: (1) We present a formulation of the
D2W task as an optimization problem with local and
global variants, and identify the strengths and the
weaknesses of each, (2) Using this formulation, we
present a new global D2W system, called GLOW. In
experiments on existing and novel D2W data sets,1
GLOW is shown to outperform the previous state-
of-the-art system of (Milne and Witten, 2008b), (3)
We present an error analysis and identify the key re-
maining challenge: determining when mentions re-
fer to concepts not captured in Wikipedia.
2 Problem Definition and Approach
We formalize our Disambiguation to Wikipedia
(D2W) task as follows. We are given a document
d with a set of mentions M = {m1, . . . ,mN},
and our goal is to produce a mapping from the set
of mentions to the set of Wikipedia titles W =
{t1, . . . , t|W |}. Often, mentions correspond to a
concept without a Wikipedia page; we treat this case
by adding a special null title to the set W .
The D2W task can be visualized as finding a
many-to-one matching on a bipartite graph, with
mentions forming one partition and Wikipedia ti-
tles the other (see Figure 1). We denote the output
matching as an N -tuple ? = (t1, . . . , tN ) where ti
is the output disambiguation for mention mi.
1The data sets are available for download at
http://cogcomp.cs.illinois.edu/Data
2.1 Local and Global Disambiguation
A local D2W approach disambiguates each men-
tion mi separately. Specifically, let ?(mi, tj) be a
score function reflecting the likelihood that the can-
didate title tj ?W is the correct disambiguation for
mi ? M . A local approach solves the following
optimization problem:
??local = argmax?
N
?
i=1
?(mi, ti) (1)
Local D2W approaches, exemplified by (Bunescu
and Pasca, 2006) and (Mihalcea and Csomai, 2007),
utilize ? functions that assign higher scores to titles
with content similar to that of the input document.
We expect, all else being equal, that the correct
disambiguations will form a ?coherent? set of re-
lated concepts. Global approaches define a coher-
ence function ?, and attempt to solve the following
disambiguation problem:
?? = argmax
?
[
N
?
i=1
?(mi, ti) + ?(?)] (2)
The global optimization problem in Eq. 2 is NP-
hard, and approximations are required (Cucerzan,
2007). The common approach is to utilize the
Wikipedia link graph to obtain an estimate pairwise
relatedness between titles ?(ti, tj) and to efficiently
generate a disambiguation context ??, a rough ap-
proximation to the optimal ??. We then solve the
easier problem:
?? ? argmax
?
N
?
i=1
[?(mi, ti) +
?
tj???
?(ti, tj)] (3)
1376
Eq. 3 can be solved by finding each ti and then map-
ping mi independently as in a local approach, but
still enforces some degree of coherence among the
disambiguations.
3 Related Work
Wikipedia was first explored as an information
source for named entity disambiguation and in-
formation retrieval by Bunescu and Pasca (2006).
There, disambiguation is performed using an SVM
kernel that compares the lexical context around the
ambiguous named entity to the content of the can-
didate disambiguation?s Wikipedia page. However,
since each ambiguous mention required a separate
SVM model, the experiment was on a very limited
scale. Mihalcea and Csomai applied Word Sense
Disambiguation methods to the Disambiguation to
Wikipedia task (2007). They experimented with
two methods: (a) the lexical overlap between the
Wikipedia page of the candidate disambiguations
and the context of the ambiguous mention, and (b)
training a Naive Bayes classiffier for each ambigu-
ous mention, using the hyperlink information found
in Wikipedia as ground truth. Both (Bunescu and
Pasca, 2006) and (Mihalcea and Csomai, 2007) fall
into the local framework.
Subsequent work on Wikification has stressed that
assigned disambiguations for the same document
should be related, introducing the global approach
(Cucerzan, 2007; Milne and Witten, 2008b; Han and
Zhao, 2009; Ferragina and Scaiella, 2010). The two
critical components of a global approach are the se-
mantic relatedness function ? between two titles,
and the disambiguation context ??. In (Milne and
Witten, 2008b), the semantic context is defined to
be a set of ?unambiguous surface forms? in the text,
and the title relatedness ? is computed as Normal-
ized Google Distance (NGD) (Cilibrasi and Vitanyi,
2007).2 On the other hand, in (Cucerzan, 2007) the
disambiguation context is taken to be all plausible
disambiguations of the named entities in the text,
and title relatedness is based on the overlap in cat-
egories and incoming links. Both approaches have
limitations. The first approach relies on the pres-
2(Milne and Witten, 2008b) also weight each mention in ??
by its estimated disambiguation utility, which can be modeled
by augmenting ? on per-problem basis.
ence of unambiguous mentions in the input docu-
ment, and the second approach inevitably adds ir-
relevant titles to the disambiguation context. As we
demonstrate in our experiments, by utilizing a more
accurate disambiguation context, GLOW is able to
achieve better performance.
4 System Architecture
In this section, we present our global D2W system,
which solves the optimization problem in Eq. 3. We
refer to the system as GLOW, for Global Wikifica-
tion. We use GLOW as a test bed for evaluating local
and global approaches for D2W. GLOW combines
a powerful local model ? with an novel method
for choosing an accurate disambiguation context ??,
which as we show in our experiments allows it to
outperform the previous state of the art.
We represent the functions ? and ? as weighted
sums of features. Specifically, we set:
?(m, t) =
?
i
wi?i(m, t) (4)
where each feature ?i(m, t) captures some aspect
of the relatedness between the mention m and the
Wikipedia title t. Feature functions ?i(t, t?) are de-
fined analogously. We detail the specific feature
functions utilized in GLOW in following sections.
The coefficients wi are learned using a Support Vec-
tor Machine over bootstrapped training data from
Wikipedia, as described in Section 4.5.
At a high level, the GLOW system optimizes the
objective function in Eq. 3 in a two-stage process.
We first execute a ranker to obtain the best non-null
disambiguation for each mention in the document,
and then execute a linker that decides whether the
mention should be linked to Wikipedia, or whether
instead switching the top-ranked disambiguation to
null improves the objective function. As our exper-
iments illustrate, the linking task is the more chal-
lenging of the two by a significant margin.
Figure 2 provides detailed pseudocode for GLOW.
Given a document d and a set of mentions M , we
start by augmenting the set of mentions with all
phrases in the document that could be linked to
Wikipedia, but were not included in M . Introducing
these additional mentions provides context that may
be informative for the global coherence computation
(it has no effect on local approaches). In the second
1377
Algorithm: Disambiguate to Wikipedia
Input: document d, Mentions M = {m1, . . . ,mN}
Output: a disambiguation ? = (t1, . . . , tN ).
1) Let M ? = M? { Other potential mentions in d}
2) For each mention m?i ? M ?, construct a set of disam-
biguation candidates Ti = {ti1, . . . , tiki}, t
i
j 6= null
3) Ranker: Find a solution ? = (t?1, . . . , t?|M?|), where
t?i ? Ti is the best non-null disambiguation of m?i.
4) Linker: For each m?i, map t?i to null in ? iff doing so
improves the objective function
5) Return ? entries for the original mentions M .
Figure 2: High-level pseudocode for GLOW.
step, we construct for each mention mi a limited set
of candidate Wikipedia titles Ti thatmi may refer to.
Considering only a small subset of Wikipedia titles
as potential disambiguations is crucial for tractabil-
ity (we detail which titles are selected below). In the
third step, the ranker outputs the most appropriate
non-null disambiguation ti for each mention mi.
In the final step, the linker decides whether the
top-ranked disambiguation is correct. The disam-
biguation (mi, ti) may be incorrect for several rea-
sons: (1) mention mi does not have a corresponding
Wikipedia page, (2) mi does have a corresponding
Wikipedia page, but it was not included in Ti, or
(3) the ranker erroneously chose an incorrect disam-
biguation over the correct one.
In the below sections, we describe each step of the
GLOW algorithm, and the local and global features
utilized, in detail. Because we desire a system that
can process documents at scale, each step requires
trade-offs between accuracy and efficiency.
4.1 Disambiguation Candidates Generation
The first step in GLOW is to extract all mentions that
can refer to Wikipedia titles, and to construct a set
of disambiguation candidates for each mention. Fol-
lowing previous work, we use Wikipedia hyperlinks
to perform these steps. GLOW utilizes an anchor-
title index, computed by crawling Wikipedia, that
maps each distinct hyperlink anchor text to its tar-
get Wikipedia titles. For example, the anchor text
?Chicago? is used in Wikipedia to refer both to the
city in Illinois and to the movie. Anchor texts in the
index that appear in document d are used to supple-
ment the mention setM in Step 1 of the GLOW algo-
rithm in Figure 2. Because checking all substrings
Baseline Feature: P (t|m), P (t)
Local Features: ?i(t,m)
cosine-sim(Text(t),Text(m)) : Naive/Reweighted
cosine-sim(Text(t),Context(m)): Naive/Reweighted
cosine-sim(Context(t),Text(m)): Naive/Reweighted
cosine-sim(Context(t),Context(m)): Naive/Reweighted
Global Features: ?i(ti, tj)
I[ti?tj ]?PMI(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?NGD(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?PMI(OutLinks(ti),OutLinks(tj)) : avg/max
I[ti?tj ]?NGD(OutLinks(ti),OutLinks(tj)) : avg/max
I[ti?tj ] : avg/max
I[ti?tj ]?PMI(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?NGD(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?PMI(OutLinks(ti),OutLinks(tj)) : avg/max
I[ti?tj ]?NGD(OutLinks(ti),OutLinks(tj)) : avg/max
Table 1: Ranker features. I[ti?tj ] is an indicator variable
which is 1 iff ti links to tj or vise-versa. I[ti?tj ] is 1 iff
the titles point to each other.
in the input text against the index is computation-
ally inefficient, we instead prune the search space
by applying a publicly available shallow parser and
named entity recognition system.3 We consider only
the expressions marked as named entities by the
NER tagger, the noun-phrase chunks extracted by
the shallow parser, and all sub-expressions of up to
5 tokens of the noun-phrase chunks.
To retrieve the disambiguation candidates Ti for
a given mention mi in Step 2 of the algorithm, we
query the anchor-title index. Ti is taken to be the
set of titles most frequently linked to with anchor
text mi in Wikipedia. For computational efficiency,
we utilize only the top 20 most frequent target pages
for the anchor text; the accuracy impact of this opti-
mization is analyzed in Section 6.
From the anchor-title index, we compute two lo-
cal features ?i(m, t). The first, P (t|m), is the frac-
tion of times the title t is the target page for an an-
chor text m. This single feature is a very reliable
indicator of the correct disambiguation (Fader et al,
2009), and we use it as a baseline in our experiments.
The second, P (t), gives the fraction of all Wikipedia
articles that link to t.
4.2 Local Features ?
In addition to the two baseline features mentioned in
the previous section, we compute a set of text-based
3Available at http://cogcomp.cs.illinois.edu/page/software.
1378
local features ?(t,m). These features capture the in-
tuition that a given Wikipedia title t is more likely to
be referred to by mention m appearing in document
d if the Wikipedia page for t has high textual simi-
larity to d, or if the context surrounding hyperlinks
to t are similar to m?s context in d.
For each Wikipedia title t, we construct a top-
200 token TF-IDF summary of the Wikipedia page
t, which we denote as Text(t) and a top-200 to-
ken TF-IDF summary of the context within which
t was hyperlinked to in Wikipedia, which we denote
as Context(t). We keep the IDF vector for all to-
kens in Wikipedia, and given an input mention m in
a document d, we extract the TF-IDF representation
of d, which we denote Text(d), and a TF-IDF rep-
resentation of a 100-token window around m, which
we denote Context(m). This allows us to define
four local features described in Table 1.
We additionally compute weighted versions of
the features described above. Error analysis has
shown that in many cases the summaries of the dif-
ferent disambiguation candidates for the same sur-
face form s were very similar. For example, con-
sider the disambiguation candidates of ?China? and
their TF-IDF summaries in Figure 1. The major-
ity of the terms selected in all summaries refer to
the general issues related to China, such as ?legal-
ism, reform, military, control, etc.?, while a minority
of the terms actually allow disambiguation between
the candidates. The problem stems from the fact
that the TF-IDF summaries are constructed against
the entire Wikipedia, and not against the confusion
set of disambiguation candidates of m. Therefore,
we re-weigh the TF-IDF vectors using the TF-IDF
scheme on the disambiguation candidates as a ad-
hoc document collection, similarly to an approach
in (Joachims, 1997) for classifying documents. In
our scenario, the TF of the a token is the original
TF-IDF summary score (a real number), and the IDF
term is the sum of all the TF-IDF scores for the to-
ken within the set of disambiguation candidates for
m. This adds 4 more ?reweighted local? features in
Table 1.
4.3 Global Features ?
Global approaches require a disambiguation context
?? and a relatedness measure ? in Eq. 3. In this sec-
tion, we describe our method for generating a dis-
ambiguation context, and the set of global features
?i(t, t?) forming our relatedness measure.
In previous work, Cucerzan defined the disam-
biguation context as the union of disambiguation
candidates for all the named entity mentions in the
input document (2007). The disadvantage of this ap-
proach is that irrelevant titles are inevitably added to
the disambiguation context, creating noise. Milne
and Witten, on the other hand, use a set of un-
ambiguous mentions (2008b). This approach uti-
lizes only a fraction of the available mentions for
context, and relies on the presence of unambigu-
ous mentions with high disambiguation utility. In
GLOW, we utilize a simple and efficient alternative
approach: we first train a local disambiguation sys-
tem, and then use the predictions of that system as
the disambiguation context. The advantage of this
approach is that unlike (Milne and Witten, 2008b)
we use all the available mentions in the document,
and unlike (Cucerzan, 2007) we reduce the amount
of irrelevant titles in the disambiguation context by
taking only the top-ranked disambiguation per men-
tion.
Our global features are refinements of previously
proposed semantic relatedness measures between
Wikipedia titles. We are aware of two previous
methods for estimating the relatedness between two
Wikipedia concepts: (Strube and Ponzetto, 2006),
which uses category overlap, and (Milne and Wit-
ten, 2008a), which uses the incoming link structure.
Previous work experimented with two relatedness
measures: NGD, and Specificity-weighted Cosine
Similarity. Consistent with previous work, we found
NGD to be the better-performing of the two. Thus
we use only NGD along with a well-known Pon-
twise Mutual Information (PMI) relatedness mea-
sure. Given a Wikipedia title collection W , titles
t1 and t2 with a set of incoming links L1, and L2
respectively, PMI and NGD are defined as follows:
NGD(L1, L2) = Log(Max(|L1|, |L2|))? Log(|L1 ? L2|)Log(|W |)? Log(Min(|L1|, |L2|))
PMI(L1, L2) = |L1 ? L2|/|W ||L1|/|W ||L2|/|W |
The NGD and the PMI measures can also be com-
puted over the set of outgoing links, and we include
these as features as well. We also included a fea-
ture indicating whether the articles each link to one
1379
another. Lastly, rather than taking the sum of the re-
latedness scores as suggested by Eq. 3, we use two
features: the average and the maximum relatedness
to ??. We expect the average to be informative for
many documents. The intuition for also including
the maximum relatedness is that for longer docu-
ments that may cover many different subtopics, the
maximum may be more informative than the aver-
age.
We have experimented with other semantic fea-
tures, such as category overlap or cosine similar-
ity between the TF-IDF summaries of the titles, but
these did not improve performance in our experi-
ments. The complete set of global features used in
GLOW is given in Table 1.
4.4 Linker Features
Given the mention m and the top-ranked disam-
biguation t, the linker attempts to decide whether t is
indeed the correct disambiguation of m. The linker
includes the same features as the ranker, plus addi-
tional features we expect to be particularly relevant
to the task. We include the confidence of the ranker
in t with respect to second-best disambiguation t?,
intended to estimate whether the ranker may have
made a mistake. We also include several properties
of the mention m: the entropy of the distribution
P (t|m), the percent of Wikipedia titles in which m
appears hyperlinked versus the percent of times m
appears as plain text, whether m was detected by
NER as a named entity, and a Good-Turing estimate
of how likely m is to be out-of-Wikipedia concept
based on the counts in P (t|m).
4.5 Linker and Ranker Training
We train the coefficients for the ranker features us-
ing a linear Ranking Support Vector Machine, using
training data gathered from Wikipedia. Wikipedia
links are considered gold-standard links for the
training process. The methods for compiling the
Wikipedia training corpus are given in Section 5.
We train the linker as a separate linear Support
Vector Machine. Training data for the linker is ob-
tained by applying the ranker on the training set. The
mentions for which the top-ranked disambiguation
did not match the gold disambiguation are treated
as negative examples, while the mentions the ranker
got correct serve as positive examples.
Mentions/Distinct titles
data set Gold Identified Solvable
ACE 257/255 213/212 185/184
MSNBC 747/372 530/287 470/273
AQUAINT 727/727 601/601 588/588
Wikipedia 928/813 855/751 843/742
Table 2: Number of mentions and corresponding dis-
tinct titles by data set. Listed are (number of men-
tions)/(number of distinct titles) for each data set, for each
of three mention types. Gold mentions include all dis-
ambiguated mentions in the data set. Identified mentions
are gold mentions whose correct disambiguations exist in
GLOW?s author-title index. Solvable mentions are identi-
fied mentions whose correct disambiguations are among
the candidates selected by GLOW (see Table 3).
5 Data sets and Evaluation Methodology
We evaluate GLOW on four data sets, of which
two are from previous work. The first data set,
from (Milne and Witten, 2008b), is a subset of the
AQUAINT corpus of newswire text that is annotated
to mimic the hyperlink structure in Wikipedia. That
is, only the first mentions of ?important? titles were
hyperlinked. Titles deemed uninteresting and re-
dundant mentions of the same title are not linked.
The second data set, from (Cucerzan, 2007), is taken
from MSNBC news and focuses on disambiguating
named entities after running NER and co-reference
resolution systems on newsire text. In this case,
all mentions of all the detected named entities are
linked.
We also constructed two additional data sets. The
first is a subset of the ACE co-reference data set,
which has the advantage that mentions and their
types are given, and the co-reference is resolved. We
asked annotators on Amazon?s Mechanical Turk to
link the first nominal mention of each co-reference
chain to Wikipedia, if possible. Finding the accu-
racy of a majority vote of these annotations to be
approximately 85%, we manually corrected the an-
notations to obtain ground truth for our experiments.
The second data set we constructed, Wiki, is a sam-
ple of paragraphs from Wikipedia pages. Mentions
in this data set correspond to existing hyperlinks in
the Wikipedia text. Because Wikipedia editors ex-
plicitly link mentions to Wikipedia pages, their an-
chor text tends to match the title of the linked-to-
page?as a result, in the overwhelming majority of
1380
cases, the disambiguation decision is as trivial as
string matching. In an attempt to generate more
challenging data, we extracted 10,000 random para-
graphs for which choosing the top disambiguation
according to P (t|m) results in at least a 10% ranker
error rate. 40 paragraphs of this data was utilized for
testing, while the remainder was used for training.
The data sets are summarized in Table 2. The ta-
ble shows the number of annotated mentions which
were hyperlinked to non-null Wikipedia pages, and
the number of titles in the documents (without
counting repetitions). For example, the AQUAINT
data set contains 727 mentions,4 all of which refer
to distinct titles. The MSNBC data set contains 747
mentions mapped to non-null Wikipedia pages, but
some mentions within the same document refer to
the same titles. There are 372 titles in the data set,
when multiple instances of the same title within one
document are not counted.
To isolate the performance of the individual com-
ponents of GLOW, we use multiple distinct metrics
for evaluation. Ranker accuracy, which measures
the performance of the ranker alone, is computed
only over those mentions with a non-null gold dis-
ambiguation that appears in the candidate set. It is
equal to the fraction of these mentions for which the
ranker returns the correct disambiguation. Thus, a
perfect ranker should achieve a ranker accuracy of
1.0, irrespective of limitations of the candidate gen-
erator. Linker accuracy is defined as the fraction of
all mentions for which the linker outputs the correct
disambiguation (note that, when the title produced
by the ranker is incorrect, this penalizes linker accu-
racy). Lastly, we evaluate our whole system against
other baselines using a previously-employed ?bag of
titles? (BOT) evaluation (Milne and Witten, 2008b).
In BOT, we compare the set of titles output for a doc-
ument with the gold set of titles for that document
(ignoring duplicates), and utilize standard precision,
recall, and F1 measures.
In BOT, the set of titles is collected from the men-
tions hyperlinked in the gold annotation. That is,
if the gold annotation is { (China, People?s Repub-
lic of China), (Taiwan, Taiwan), (Jiangsu, Jiangsu)}
4The data set contains votes on how important the mentions
are. We believe that the results in (Milne and Witten, 2008b)
were reported on mentions which the majority of annotators
considered important. In contrast, we used all the mentions.
Generated data sets
Candidates k ACE MSNBC AQUAINT Wiki
1 81.69 72.26 91.01 84.79
3 85.44 86.22 96.83 94.73
5 86.38 87.35 97.17 96.37
20 86.85 88.67 97.83 98.59
Table 3: Percent of ?solvable? mentions as a function
of the number of generated disambiguation candidates.
Listed is the fraction of identified mentions m whose
target disambiguation t is among the top k candidates
ranked in descending order of P (t|m).
and the predicted anotation is: { (China, People?s
Republic of China), (China, History of China), (Tai-
wan, null), (Jiangsu, Jiangsu), (republic, Govern-
ment)} , then the BOT for the gold annotation is:
{People?s Republic of China, Taiwan, Jiangsu} , and
the BOT for the predicted annotation is: {People?s
Republic of China, History of China, Jiangsu} . The
title Government is not included in the BOT for pre-
dicted annotation, because its associate mention re-
public did not appear as a mention in the gold anno-
tation. Both the precision and the recall of the above
prediction is 0.66. We note that in the BOT evalua-
tion, following (Milne and Witten, 2008b) we con-
sider all the titles within a document, even if some
the titles were due to mentions we failed to identify.5
6 Experiments and Results
In this section, we evaluate and analyze GLOW?s
performance on the D2W task. We begin by eval-
uating the mention detection component (Step 1 of
the algorithm). The second column of Table 2 shows
how many of the ?non-null? mentions and corre-
sponding titles we could successfully identify (e.g.
out of 747 mentions in the MSNBC data set, only
530 appeared in our anchor-title index). Missing en-
tities were primarily due to especially rare surface
forms, or sometimes due to idiosyncratic capitaliza-
tion in the corpus. Improving the number of iden-
tified mentions substantially is non-trivial; (Zhou et
al., 2010) managed to successfully identify only 59
more entities than we do in the MSNBC data set, us-
ing a much more powerful detection method based
on search engine query logs.
We generate disambiguation candidates for a
5We evaluate the mention identification stage in Section 6.
1381
Data sets
Features ACE MSNBC AQUAINT Wiki
P (t|m) 94.05 81.91 93.19 85.88
P (t|m)+Local
Naive 95.67 84.04 94.38 92.76
Reweighted 96.21 85.10 95.57 93.59
All above 95.67 84.68 95.40 93.59
P (t|m)+Global
NER 96.21 84.04 94.04 89.56
Unambiguous 94.59 84.46 95.40 89.67
Predictions 96.75 88.51 95.91 89.79
P (t|m)+Local+Global
All features 97.83 87.02 94.38 94.18
Table 4: Ranker Accuracy. Bold values indicate the
best performance in each feature group. The global ap-
proaches marginally outperform the local approaches on
ranker accuracy , while combing the approaches leads to
further marginal performance improvement.
mention m using an anchor-title index, choosing
the 20 titles with maximal P (t|m). Table 3 eval-
uates the accuracy of this generation policy. We
report the percent of mentions for which the cor-
rect disambiguation is generated in the top k can-
didates (called ?solvable? mentions). We see that
the baseline prediction of choosing the disambigua-
tion t which maximizes P (t|m) is very strong (80%
of the correct mentions have maximal P (t|m) in all
data sets except MSNBC). The fraction of solvable
mentions increases until about five candidates per
mention are generated, after which the increase is
rather slow. Thus, we believe choosing a limit of 20
candidates per mention offers an attractive trade-off
of accuracy and efficiency. The last column of Ta-
ble 2 reports the number of solvable mentions and
the corresponding number of titles with a cutoff of
20 disambiguation candidates, which we use in our
experiments.
Next, we evaluate the accuracy of the ranker. Ta-
ble 4 compares the ranker performance with base-
line, local and global features. The reweighted lo-
cal features outperform the unweighted (?Naive?)
version, and the global approach outperforms the
local approach on all data sets except Wikipedia.
As the table shows, our approach of defining the
disambiguation context to be the predicted dis-
ambiguations of a simpler local model (?Predic-
tions?) performs better than using NER entities as
in (Cucerzan, 2007), or only the unambiguous enti-
Data set Local Global Local+Global
ACE 80.1 ? 82.8 80.6 ? 80.6 81.5 ? 85.1
MSNBC 74.9 ? 76.0 77.9 ? 77.9 76.5 ? 76.9
AQUAINT 93.5 ? 91.5 93.8 ? 92.1 92.3 ? 91.3
Wiki 92.2 ? 92.0 88.5 ? 87.2 92.8 ? 92.6
Table 5: Linker performance. The notation X ? Y
means that when linking all mentions, the linking accu-
racy is X , while when applying the trained linker, the
performance is Y . The local approaches are better suited
for linking than the global approaches. The linking accu-
racy is very sensitive to domain changes.
System ACE MSNBC AQUAINT Wiki
Baseline: P (t|m) 69.52 72.83 82.67 81.77
GLOW Local 75.60 74.39 84.52 90.20
GLOW Global 74.73 74.58 84.37 86.62
GLOW 77.25 74.88 83.94 90.54
M&W 72.76 68.49 83.61 80.32
Table 6: End systems performance - BOT F1. The per-
formance of the full system (GLOW) is similar to that of
the local version. GLOW outperforms (Milne and Witten,
2008b) on all data sets.
ties as in (Milne and Witten, 2008b).6 Combining
the local and the global approaches typically results
in minor improvements.
While the global approaches are most effective for
ranking, the linking problem has different charac-
teristics as shown in Table 5. We can see that the
global features are not helpful in general for predict-
ing whether the top-ranked disambiguation is indeed
the correct one.
Further, although the trained linker improves ac-
curacy in some cases, the gains are marginal?and
the linker decreases performance on some data sets.
One explanation for the decrease is that the linker
is trained on Wikipedia, but is being tested on non-
Wikipedia text which has different characteristics.
However, in separate experiments we found that
training a linker on out-of-Wikipedia text only in-
creased test set performance by approximately 3
percentage points. Clearly, while ranking accuracy
is high overall, different strategies are needed to
achieve consistently high linking performance.
A few examples from the ACE data set help il-
6In NER we used only the top prediction, because using all
candidates as in (Cucerzan, 2007) proved prohibitively ineffi-
cient.
1382
lustrate the tradeoffs between local and global fea-
tures in GLOW. The global system mistakenly links
?<Dorothy Byrne>, a state coordinator for the
Florida Green Party, said . . . ? to the British jour-
nalist, because the journalist sense has high coher-
ence with other mentions in the newswire text. How-
ever, the local approach correctly maps the men-
tion to null because of a lack of local contextual
clues. On the other hand, in the sentence ?In-
stead of Los Angeles International, for example,
consider flying into <Burbank> or John Wayne Air-
port in Orange County, Calif.?, the local ranker
links the mention Burbank to Burbank, California,
while the global system correctly maps the entity to
Bob Hope Airport, because the three airports men-
tioned in the sentence are highly related to one an-
other.
Lastly, in Table 6 we compare the end system
BOT F1 performance. The local approach proves
a very competitive baseline which is hard to beat.
Combining the global and the local approach leads
to marginal improvements. The full GLOW sys-
tem outperforms the existing state-of-the-art system
from (Milne and Witten, 2008b), denoted as M&W,
on all data sets. We also compared our system with
the recent TAGME Wikification system (Ferragina
and Scaiella, 2010). However, TAGME is designed
for a different setting than ours: extremely short
texts, like Twitter posts. The TAGME RESTful API
was unable to process some of our documents at
once. We attempted to input test documents one sen-
tence at a time, disambiguating each sentence inde-
pendently, which resulted in poor performance (0.07
points in F1 lower than the P (t|m) baseline). This
happened mainly because the same mentions were
linked to different titles in different sentences, lead-
ing to low precision.
An important question is why M&W underper-
forms the baseline on the MSNBC and Wikipedia
data sets. In an error analysis, M&W performed
poorly on the MSNBC data not due to poor disam-
biguations, but instead because the data set contains
only named entities, which were often delimited in-
correctly by M&W. Wikipedia was challenging for
a different reason: M&W performs less well on the
short (one paragraph) texts in that set, because they
contain relatively few of the unambiguous entities
the system relies on for disambiguation.
7 Conclusions
We have formalized the Disambiguation to
Wikipedia (D2W) task as an optimization problem
with local and global variants, and analyzed the
strengths and weaknesses of each. Our experiments
revealed that previous approaches for global disam-
biguation can be improved, but even then the local
disambiguation provides a baseline which is very
hard to beat.
As our error analysis illustrates, the primary re-
maining challenge is determining when a mention
does not have a corresponding Wikipedia page.
Wikipedia?s hyperlinks offer a wealth of disam-
biguated mentions that can be leveraged to train
a D2W system. However, when compared with
mentions from general text, Wikipedia mentions
are disproportionately likely to have corresponding
Wikipedia pages. Our initial experiments suggest
that accounting for this bias requires more than sim-
ply training a D2W system on a moderate num-
ber of examples from non-Wikipedia text. Apply-
ing distinct semi-supervised and active learning ap-
proaches to the task is a primary area of future work.
Acknowledgments
This research supported by the Army Research
Laboratory (ARL) under agreement W911NF-09-
2-0053 and by the Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. The third
author was supported by a Microsoft New Faculty
Fellowship. Any opinions, findings, conclusions or
recommendations are those of the authors and do not
necessarily reflect the view of the ARL, DARPA,
AFRL, or the US government.
References
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), Trento, Italy, pages 9?16, April.
Ming-Wei Chang, Lev Ratinov, Dan Roth, and Vivek
Srikumar. 2008. Importance of semantic represen-
tation: dataless classification. In Proceedings of the
1383
23rd national conference on Artificial intelligence -
Volume 2, pages 830?835. AAAI Press.
Rudi L. Cilibrasi and Paul M. B. Vitanyi. 2007. The
google similarity distance. IEEE Trans. on Knowl. and
Data Eng., 19(3):370?383.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708?716, Prague, Czech Republic, June. Association
for Computational Linguistics.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling wikipedia-based named entity disam-
biguation to arbitrary web text. In Proceedings of
the WikiAI 09 - IJCAI Workshop: User Contributed
Knowledge and Artificial Intelligence: An Evolving
Synergy, Pasadena, CA, USA, July.
Paolo Ferragina and Ugo Scaiella. 2010. Tagme: on-the-
fly annotation of short text fragments (by wikipedia
entities). In Jimmy Huang, Nick Koudas, Gareth J. F.
Jones, Xindong Wu, Kevyn Collins-Thompson, and
Aijun An, editors, Proceedings of the 19th ACM con-
ference on Information and knowledge management,
pages 1625?1628. ACM.
Tim Finin, Zareen Syed, James Mayfield, Paul Mc-
Namee, and Christine Piatko. 2009. Using Wikitol-
ogy for Cross-Document Entity Coreference Resolu-
tion. In Proceedings of the AAAI Spring Symposium
on Learning by Reading and Learning to Read. AAAI
Press, March.
Evgeniy Gabrilovich and Shaul Markovitch. 2007a.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, pages 1606?1611, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Evgeniy Gabrilovich and Shaul Markovitch. 2007b.
Harnessing the expertise of 70,000 human editors:
Knowledge-based feature generation for text catego-
rization. J. Mach. Learn. Res., 8:2297?2345, Decem-
ber.
Xianpei Han and Jun Zhao. 2009. Named entity dis-
ambiguation by leveraging wikipedia semantic knowl-
edge. In Proceeding of the 18th ACM conference on
Information and knowledge management, CIKM ?09,
pages 215?224, New York, NY, USA. ACM.
Thorsten Joachims. 1997. A probabilistic analysis of
the rocchio algorithm with tfidf for text categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, ICML ?97, pages
143?151, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation
of wikipedia entities in web text. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?09,
pages 457?466, New York, NY, USA. ACM.
James Mayfield, David Alexander, Bonnie Dorr, Jason
Eisner, Tamer Elsayed, Tim Finin, Clay Fink, Mar-
jorie Freedman, Nikesh Garera, James Mayfield, Paul
McNamee, Saif Mohammad, Douglas Oard, Chris-
tine Piatko, Asad Sayeed, Zareen Syed, and Ralph
Weischede. 2009. Cross-Document Coreference Res-
olution: A Key Technology for Learning by Reading.
In Proceedings of the AAAI 2009 Spring Symposium
on Learning by Reading and Learning to Read. AAAI
Press, March.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
CIKM ?07, pages 233?242, New York, NY, USA.
ACM.
David Milne and Ian H. Witten. 2008a. An effec-
tive, low-cost measure of semantic relatedness ob-
tained from wikipedia links. In In the Wikipedia and
AI Workshop of AAAI.
David Milne and Ian H. Witten. 2008b. Learning to link
with wikipedia. In Proceedings of the 17th ACM con-
ference on Information and knowledge management,
CIKM ?08, pages 509?518, New York, NY, USA.
ACM.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In proceedings of the 21st national confer-
ence on Artificial intelligence - Volume 2, pages 1419?
1424. AAAI Press.
Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Flavian
Vasile, and Scott Gaffney. 2010. Resolving surface
forms to wikipedia topics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling 2010), pages 1335?1343, Beijing, China,
August. Coling 2010 Organizing Committee.
1384
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1486?1495,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Confidence Driven Unsupervised Semantic Parsing
Dan Goldwasser ? Roi Reichart ? James Clarke ? Dan Roth ?
?Department of Computer Science, University of Illinois at Urbana-Champaign
{goldwas1,clarkeje,danr}@illinois.edu
?Computer Science and Artificial Intelligence Laboratory, MIT
roiri@csail.mit.edu
Abstract
Current approaches for semantic parsing take
a supervised approach requiring a consider-
able amount of training data which is expen-
sive and difficult to obtain. This supervision
bottleneck is one of the major difficulties in
scaling up semantic parsing.
We argue that a semantic parser can be trained
effectively without annotated data, and in-
troduce an unsupervised learning algorithm.
The algorithm takes a self training approach
driven by confidence estimation. Evaluated
over Geoquery, a standard dataset for this
task, our system achieved 66% accuracy, com-
pared to 80% of its fully supervised counter-
part, demonstrating the promise of unsuper-
vised approaches for this task.
1 Introduction
Semantic parsing, the ability to transform Natural
Language (NL) input into a formal Meaning Repre-
sentation (MR), is one of the longest standing goals
of natural language processing. The importance of
the problem stems from both theoretical and practi-
cal reasons, as the ability to convert NL into a formal
MR has countless applications.
The term semantic parsing has been used ambigu-
ously to refer to several semantic tasks (e.g., se-
mantic role labeling). We follow the most common
definition of this task: finding a mapping between
NL input and its interpretation expressed in a well-
defined formal MR language. Unlike shallow se-
mantic analysis tasks, the output of a semantic parser
is complete and unambiguous to the extent it can be
understood or even executed by a computer system.
Current approaches for this task take a data driven
approach (Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007), in which the learning algorithm is
given a set of NL sentences as input and their cor-
responding MR, and learns a statistical semantic
parser ? a set of parameterized rules mapping lex-
ical items and syntactic patterns to their MR. Given
a sentence, these rules are applied recursively to de-
rive the most probable interpretation.
Since semantic interpretation is limited to the syn-
tactic patterns observed in the training data, in or-
der to work well these approaches require consider-
able amounts of annotated data. Unfortunately an-
notating sentences with their MR is a time consum-
ing task which requires specialized domain knowl-
edge and therefore minimizing the supervision ef-
fort is one of the key challenges in scaling semantic
parsers.
In this work we present the first unsupervised
approach for this task. Our model compensates
for the lack of training data by employing a self
training protocol based on identifying high confi-
dence self labeled examples and using them to re-
train the model. We base our approach on a sim-
ple observation: semantic parsing is a difficult struc-
tured prediction task, which requires learning a com-
plex model, however identifying good predictions
can be done with a far simpler model capturing re-
peating patterns in the predicted data. We present
several simple, yet highly effective confidence mea-
sures capturing such patterns, and show how to use
them to train a semantic parser without manually an-
notated sentences.
Our basic premise, that predictions with high con-
fidence score are of high quality, is further used to
improve the performance of the unsupervised train-
1486
ing procedure. Our learning algorithm takes an EM-
like iterative approach, in which the predictions of
the previous stage are used to bias the model. While
this basic scheme was successfully applied to many
unsupervised tasks, it is known to converge to a
sub optimal point. We show that by using confi-
dence estimation as a proxy for the model?s pre-
diction quality, the learning algorithm can identify
a better model compared to the default convergence
criterion.
We evaluate our learning approach and model
on the well studied Geoquery domain (Zelle and
Mooney, 1996; Tang and Mooney, 2001), consist-
ing of natural language questions and their prolog
interpretations used to query a database consisting
of U.S. geographical information. Our experimental
results show that using our approach we are able to
train a good semantic parser without annotated data,
and that using a confidence score to identify good
models results in a significant performance improve-
ment.
2 Semantic Parsing
We formulate semantic parsing as a structured pre-
diction problem, mapping a NL input sentence (de-
noted x), to its highest ranking MR (denoted z). In
order to correctly parametrize and weight the pos-
sible outputs, the decision relies on an intermediate
representation: an alignment between textual frag-
ments and their meaning representation (denoted y).
Fig. 1 describes a concrete example of this termi-
nology. In our experiments the input sentences x
are natural language queries about U.S. geography
taken from the Geoquery dataset. The meaning rep-
resentation z is a formal language database query,
this output representation language is described in
Sec. 2.1.
The prediction function, mapping a sentence to its
corresponding MR, is formalized as follows:
z? = Fw(x) = arg max
y?Y,z?Z
wT?(x,y, z) (1)
Where ? is a feature function defined over an input
sentence x, alignment y and output z. The weight
vector w contains the model?s parameters, whose
values are determined by the learning process.
We refer to the arg max above as the inference
problem. Given an input sentence, solving this in-
How many states does the Colorado river run through? 
count( state( traverse( river( const(colorado))))
x 
z 
y 
Figure 1: Example of an input sentence (x), meaning rep-
resentation (z) and the alignment between the two (y) for
the Geoquery domain
ference problem based on ? and w is what com-
promises our semantic parser. In practice the pars-
ing decision is decomposed into smaller decisions
(Sec. 2.2). Sec. 4 provides more details about the
feature representation and inference procedure used.
Current approaches obtain w using annotated
data, typically consisting of (x, z) pairs. In Sec. 3 we
describe our unsupervised learning procedure, that is
how to obtain w without annotated data.
2.1 Target Meaning Representation
The output of the semantic parser is a logical for-
mula, grounding the semantics of the input sen-
tence in the domain language (i.e., the Geoquery
domain). We use a subset of first order logic con-
sisting of typed constants (corresponding to specific
states, etc.) and functions, which capture relations
between domains entities and properties of entities
(e.g., population : E ? N ). The seman-
tics of the input sentence is constructed via func-
tional composition, done by the substitution oper-
ator. For example, given the function next to(x)
and the expression const(texas), substitution
replaces the occurrence of the free variable x
with the expression, resulting in a new formula:
next to(const(texas)). For further details
we refer the reader to (Zelle and Mooney, 1996).
2.2 Semantic Parsing Decisions
The inference problem described in Eq. 1 selects the
top ranking output formula. In practice this decision
is decomposed into smaller decisions, capturing lo-
cal mapping of input tokens to logical fragments and
their composition into larger fragments. These deci-
sions are further decomposed into a feature repre-
sentation, described in Sec. 4.
The first type of decisions are encoded directly by
the alignment (y) between the input tokens and their
corresponding predicates. We refer to these as first
1487
order decisions. The pairs connected by the align-
ment (y) in Fig. 1 are examples of such decisions.
The final output structure z is constructed by
composing individual predicates into a complete
formula. For example, consider the formula pre-
sented in Fig. 1: river( const(colorado))
is a composition of two predicates river and
const(colorado). We refer to the composition
of two predicates, associated with their respective
input tokens, as second order decisions.
In order to formulate these decisions, we intro-
duce the following notation. c is a constituent in the
input sentence x and D is the set of all function and
constant symbols in the domain. The alignment y is
a set of mappings between constituents and symbols
in the domain y = {(c, s)} where s ? D.
We denote by si the i-th output predicate compo-
sition in z, by si?1(si) the composition of the (i?1)-
th predicate on the i-th predicate and by y(si) the in-
put word corresponding to that predicate according
to the alignment y.
3 Unsupervised Semantic Parsing
Our learning framework takes a self training ap-
proach in which the learner is iteratively trained over
its own predictions. Successful application of this
approach depends heavily on two important factors
- how to select high quality examples to train the
model on, and how to define the learning objective
so that learning can halt once a good model is found.
Both of these questions are trivially answered
when working in a supervised setting: by using the
labeled data for training the model, and defining the
learning objective with respect to the annotated data
(for example, loss-minimization in the supervised
version of our system).
In this work we suggest to address both of the
above concerns by approximating the quality of
the model?s predictions using a confidence measure
computed over the statistics of the self generated
predictions. Output structures which fall close to the
center of mass of these statistics will receive a high
confidence score.
The first issue is addressed by using examples as-
signed a high confidence score to train the model,
acting as labeled examples.
We also note that since the confidence score pro-
vides a good indication for the model?s prediction
performance, it can be used to approximate the over-
all model performance, by observing the model?s to-
tal confidence score over all its predictions. This
allows us to set a performance driven goal for our
learning process - return the model maximizing the
confidence score over all predictions. We describe
the details of integrating the confidence score into
the learning framework in Sec. 3.1.
Although using the model?s prediction score (i.e.,
wT?(x,y, z)) as an indication of correctness is a
natural choice, we argue and show empirically, that
unsupervised learning driven by confidence estima-
tion results in a better performing model. This
empirical behavior also has theoretical justification:
training the model using examples selected accord-
ing to the model?s parameters (i.e., the top rank-
ing structures) may not generalize much further be-
yond the existing model, as the training examples
will simply reinforce the existing model. The statis-
tics used for confidence estimation are different than
those used by the model to create the output struc-
tures, and can therefore capture additional informa-
tion unobserved by the prediction model. This as-
sumption is based on the well established idea of
multi-view learning, applied successfully to many
NL applications (Blum and Mitchell, 1998; Collins
and Singer, 1999). According to this idea if two
models use different views of the data, each of them
can enhance the learning process of the other.
The success of our learning procedure hinges
on finding good confidence measures, whose confi-
dence prediction correlates well with the true quality
of the prediction. The ability of unsupervised confi-
dence estimation to provide high quality confidence
predictions can be explained by the observation that
prominent prediction patterns are more likely to be
correct. If a non-random model produces a predic-
tion pattern multiple times it is likely to be an in-
dication of an underlying phenomenon in the data,
and therefore more likely to be correct. Our specific
choice of confidence measures is guided by the intu-
ition that unlike structure prediction (i.e., solving the
inference problem) which requires taking statistics
over complex and intricate patterns, identifying high
quality predictions can be done using much simpler
patterns that are significantly easier to capture.
In the reminder of this section we describe our
1488
Algorithm 1 Unsupervised Confidence driven
Learning
Input: Sentences {xl}Nl=1,
initial weight vector w
1: define Confidence : X ? Y ? Z ? R,
i = 0, Si = ?
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = arg maxy,zw
T?(xl,y, z)
5: Si = Si ? {xl, y?, z?}
6: end for
7: Confidence = compute confidence statistics
8: Sconfi = select from Si using Confidence
9: wi ? Learn(?iS
conf
i )
10: i = i+ 1
11: until Sconfi has no new unique examples
12: best = arg maxi(
?
s?Si
Confidence(s))/|S|
13: return wbest
learning approach. We begin by introducing the
overall learning framework (Sec. 3.1), we then ex-
plain the rational behind confidence estimation over
self-generated data and introduce the confidence
measures used in our experiments (Sec. 3.2). We
conclude with a description of the specific learning
algorithms used for updating the model (Sec. 3.3).
3.1 Unsupervised Confidence-Driven Learning
Our learning framework works in an EM-like
manner, iterating between two stages: making pre-
dictions based on its current set of parameters and
then retraining the model using a subset of the pre-
dictions, assigned high confidence. The learning
process ?discovers? new high confidence training
examples to add to its training set over multiple it-
erations, and converges when the model no longer
adds new training examples.
While this is a natural convergence criterion, it
provides no performance guarantees, and in practice
it is very likely that the quality of the model (i.e., its
performance) fluctuates during the learning process.
We follow the observation that confidence estima-
tion can be used to approximate the performance of
the entire model and return the model with the high-
est overall prediction confidence.
We describe this algorithmic framework in detail
in Alg. 1. Our algorithm takes as input a set of
natural language sentences and a set of parameters
used for making the initial predictions1. The algo-
rithm then iterates between the two stages - predict-
ing the output structure for each sentence (line 4),
and updating the set of parameters (line 9). The
specific learning algorithms used are discussed in
Sec. 3.3. The training examples required for learn-
ing are obtained by selecting high confidence exam-
ples - the algorithm first takes statistics over the cur-
rent predicted set of output structures (line 7), and
then based on these statistics computes a confidence
score for each structure, selecting the top ranked
ones as positive training examples, and if needed,
the bottom ones as negative examples (line 8). The
set of top confidence examples (for either correct or
incorrect prediction), at iteration i of the algorithm,
is denoted Sconfi . The exact nature of the confidence
computation is discussed in Sec. 3.2.
The algorithm iterates between these two stages,
at each iteration it adds more self-annotated exam-
ples to its training set, learning therefore converges
when no new examples are added (line 11). The al-
gorithm keeps track of the models it trained at each
stage throughout this process, and returns the one
with the highest averaged overall confidence score
(lines 12-13). At each stage, the overall confidence
score is computed by averaging over all the confi-
dence scores of the predictions made at that stage.
3.2 Unsupervised Confidence Estimation
Confidence estimation is calculated over a batch of
input (x) - output (z) pairs. Each pair decomposes
into smaller first order and second order decisions
(defined Sec. 2.2). Confidence estimation is done by
computing the statistics of these decisions, over the
entire set of predicted structures. In the rest of this
section we introduce the confidence measures used
by our system.
Translation Model The first approach essentially
constructs a simplified translation model, capturing
word-to-predicate mapping patterns. This can be
considered as an abstraction of the prediction model:
we collapse the intricate feature representation into
1Since we commit to the max-score output prediction, rather
than summing over all possibilities, we require a reasonable ini-
tialization point. We initialized the weight vector using simple,
straight-forward heuristics described in Sec. 5.
1489
high level decisions and take statistics over these de-
cisions. Since it takes statistics over considerably
less variables than the actual prediction model, we
expect this model to make reliable confidence pre-
dictions. We consider two variations of this ap-
proach, the first constructs a unigram model over the
first order decisions and the second a bigram model
over the second order decisions. Formally, given a
set of predicted structures we define the following
confidence scores:
Unigram Score:
p(z|x) =
|z|?
i=1
p(si|y(si))
Bigram Score:
p(z|x) =
|z|?
i=1
p(si?1(si)|y(si?1), y(si))
Structural Proportion Unlike the first approach
which decomposes the predicted structure into in-
dividual decisions, this approach approximates the
model?s performance by observing global properties
of the structure. We take statistics over the propor-
tion between the number of predicates in z and the
number of words in x.
Given a set of structure predictions S, we com-
pute this proportion for each structure (denoted as
Prop(x, z)) and calculate the average proportion
over the entire set (denoted as AvProp(S)). The
confidence score assigned to a given structure (x,y)
is simply the difference between its proportion and
the averaged proportion, or formally
PropScore(S, (x, z)) = AvProp(S)?Prop(x, z)
This measure captures the global complexity of the
predicted structure and penalizes structures which
are too complex (high negative values) or too sim-
plistic (high positive values).
Combined The two approaches defined above
capture different views of the data, a natural question
is then - can these two measures be combined to pro-
vide a more powerful estimation? We suggest a third
approach which combines the first two approaches.
It first uses the score produced by the latter approach
to filter out unlikely candidates, and then ranks the
remaining ones with the former approach and selects
those with the highest rank.
3.3 Learning Algorithms
Given a set of self generated structures, the param-
eter vector can be updated (line 9 in Alg. 1). We
consider two learning algorithm for this purpose.
The first is a binary learning algorithm, which
considers learning as a classification problem, that
is finding a set of weights w that can best sepa-
rate correct from incorrect structures. The algo-
rithm decomposes each predicted formula and its
corresponding input sentence into a feature vector
?(x,y, z) normalized by the size of the input sen-
tence |x|, and assigns a binary label to this vector2.
The learning process is defined over both positive
and negative training examples. To accommodate
that we modify line 8 in Alg. 1, and use the con-
fidence score to select the top ranking examples as
positive examples, and the bottom ranking examples
as negative examples. We use a linear kernel SVM
with squared-hinge loss as the underlying learning
algorithm.
The second is a structured learning algorithm
which considers learning as a ranking problem, i.e.,
finding a set of weights w such that the ?gold struc-
ture? will be ranked on top, preferably by a large
margin to allow generalization.The structured learn-
ing algorithm can directly use the top ranking pre-
dictions of the model (line 8 in Alg. 1) as training
data. In this case the underlying algorithm is a struc-
tural SVM with squared-hinge loss, using hamming
distance as the distance function. We use the cutting-
plane method to efficiently optimize the learning
process? objective function.
4 Model
Semantic parsing as formulated in Eq. 1 is an in-
ference procedure selecting the top ranked output
logical formula. We follow the inference approach
in (Roth and Yih, 2007; Clarke et al, 2010) and
formalize this process as an Integer Linear Program
(ILP). Due to space consideration we provide a brief
description, and refer the reader to that paper for
more details.
2Without normalization longer sentences would have more
influence on binary learning problem. Normalization is there-
fore required to ensure that each sentence contributes equally to
the binary learning problem regardless of its length.
1490
4.1 Inference
The inference decision (Eq. 1) is decomposed into
smaller decisions, capturing mapping of input to-
kens to logical fragments (first order) and their com-
position into larger fragments (second order). We
encode a first-order decision as ?cs, a binary vari-
able indicating that constituent c is aligned with the
logical symbol s. A second-order decision ?cs,dt, is
encoded as a binary variable indicating that the sym-
bol t (associated with constituent d) is an argument
of a function s (associated with constituent c). We
frame the inference problem over these decisions:
Fw(x) = arg max
?,?
?
c?x
?
s?D
?cs ?wT?1(x, c, s)
+
?
c,d?x
?
s,t?D
?cs,dt ?w
T?2(x, c, s, d, t) (2)
We restrict the possible assignments to the deci-
sion variables, forcing the resulting output formula
to be syntactically legal, for example by restricting
active ?-variables to be type consistent, and force
the resulting functional composition to be acyclic.
We take advantage of the flexible ILP framework,
and encode these restrictions as global constraints
over Eq. 2. We refer the reader to (Clarke et al,
2010) for a full description of the constraints used.
4.2 Features
The inference problem defined in Eq. (2) uses two
feature functions: ?1 and ?2.
First-order decision features ?1 Determining if
a logical symbol is aligned with a specific con-
stituent depends mostly on lexical information.
Following previous work (e.g., (Zettlemoyer and
Collins, 2005)) we create a small lexicon, mapping
logical symbols to surface forms.3 Existing ap-
proaches rely on annotated data to extend the lexi-
con. Instead we rely on external knowledge (Miller
et al, 1990) and add features which measure the lex-
ical similarity between a constituent and a logical
symbol?s surface forms (as defined by the lexicon).
3The lexicon contains on average 1.42 words per function
and 1.07 words per constant.
Model Description
INITIAL MODEL Manually set weights (Sec. 5.1)
PRED. SCORE normalized prediction (Sec. 5.1)
ALL EXAMPLES All top structures (Sec. 5.1)
UNIGRAM Unigram score (Sec. 3.2)
BIGRAM Bigram score (Sec. 3.2)
PROPORTION Words-predicate prop (Sec. 3.2)
COMBINED Combined estimators (Sec. 3.2)
RESPONSE BASED Supervised (binary) (Sec. 5.1)
SUPERVISED Fully Supervised (Sec. 5.1)
Table 1: Compared systems and naming conventions.
Second-order decision features ?2 Second order
decisions rely on syntactic information. We use
the dependency tree of the input sentence. Given
a second-order decision ?cs,dt, the dependency fea-
ture takes the normalized distance between the head
words in the constituents c and d. In addition, a set
of features indicate which logical symbols are usu-
ally composed together, without considering their
alignment to the text.
5 Experiments
In this section we describe our experimental evalua-
tion. We compare several confidence measures and
analyze their properties. Tab. 1 defines the naming
conventions used throughout this section to refer to
the different models we evaluated. We begin by de-
scribing our experimental setup and then proceed to
describe the experiments and their results. For the
sake of clarity we focus on the best performing mod-
els (COMBINED using BIGRAM and PROPORTION)
first and discuss other models later in the section.
5.1 Experimental Settings
In all our experiments we used the Geoquery
dataset (Zelle and Mooney, 1996), consisting of U.S.
geography NL questions and their corresponding
Prolog logical MR. We used the data split described
in (Clarke et al, 2010), consisting of 250 queries for
evaluation purposes. We compared our system to
several supervised models, which were trained us-
ing a disjoint set of queries. Our learning system
had access only to the NL questions, and the log-
ical forms were only used to evaluate the system?s
performance. We report the proportion of correct
structures (accuracy). Note that this evaluation cor-
1491
responds to the 0/1 loss over the predicted structures.
Initialization Our learning framework requires an
initial weight vector as input. We use a straight for-
ward heuristic and provide uniform positive weights
to three features. This approach is similar in spirit
to previous works (Clarke et al, 2010; Zettlemoyer
and Collins, 2007). We refer to this system as INI-
TIAL MODEL throughout this section.
Competing Systems We compared our system to
several other systems:
(1) PRED. SCORE: An unsupervised frame-
work using the model?s internal prediction score
(wT?(x,y, z)) for confidence estimation.
(2) ALL EXAMPLES: Treating all predicted struc-
tures as correct, i.e., at each iteration the model is
trained over all the predictions it made. The re-
ported score was obtained by selecting the model at
the training iteration with the highest overall confi-
dence score (see line 12 in Alg. 1).
(3) RESPONSE BASED: A natural upper bound to
our framework is the approach used in (Clarke et al,
2010). While our approach is based on assessing
the correctness os the model?s predictions according
to unsupervised confidence estimation, their frame-
work is provided with external supervision for these
decisions, indicating if the predicted structures are
correct.
(4) SUPERVISED: A fully supervised framework
trained over 250 (x, z) pairs using structured SVM.
5.2 Results
Our experiments aim to clarify three key points:
(1) Can a semantic parser indeed be trained with-
out any form of external supervision? this is our
key question, as this is the first attempt to approach
this task with an unsupervised learning protocol.4 In
order to answer it, we report the overall performance
of our system in Tab. 2.
The manually constructed model INITIALMODEL
achieves a performance of 0.22. We can expect
learning to improve on this baseline. We com-
pare three self-trained systems, ALL EXAMPLES,
PREDICTIONSCORE and COMBINED, which differ
4While unsupervised learning for various semantic tasks has
been widely discussed, this is the first attempt to tackle this task.
We refer the reader to Sec. 6 for further discussion of this point.
in their sample selection strategy, but all use con-
fidence estimation for selecting the final seman-
tic parsing model. The ALL EXAMPLES approach
achieves an accuracy score of 0.656. PREDICTION-
SCORE only achieves a performance of 0.164 us-
ing the binary learning algorithm and 0.348 us-
ing the structured learning algorithm. Finally, our
confidence-driven technique COMBINED achieved a
score of 0.536 for the binary case and 0.664 for the
structured case, the best performing models in both
cases. As expected, the supervised systems RE-
SPONSE BASED and SUPERVISED achieve the best
performance.
These results show that training the model with
training examples selected carefully will improve
learning - as the best performance is achieved with
perfect knowledge of the predictions correctness
(RESPONSE BASED). Interestingly the difference
between the structured version of our system and
that of RESPONSE BASED is only 0.07, suggesting
that we can recover the binary feedback signal with
high precision. The low performance of the PRE-
DICTIONSCORE model is also not surprising, and it
demonstrates one of the key principles in confidence
estimation - the score should be comparable across
predictions done over different inputs, and not the
same input, as done in PREDICTIONSCORE model.
(2) How does confidence driven sample selection
contribute to the learning process? Comparing
the systems driven by confidence sample-selection
to the ALL EXAMPLES approach uncovers an inter-
esting tradeoff between training with more (noisy)
data and selectively training the system with higher
quality examples. We argue that carefully select-
ing high quality training examples will result in bet-
ter performance. The empirical results indeed sup-
port our argument, as the best performing model
(RESPONSE BASED) is achieved by sample selec-
tion with perfect knowledge of prediction correct-
ness. The confidence-based sample selection system
(COMBINED) is the best performing system out of
all the self-trained systems. Nonetheless, the ALL
EXAMPLES strategy performs well when compared
to COMBINED, justifying a closer look at that aspect
of our system.
We argue that different confidence measures cap-
ture different properties of the data, and hypothe-
1492
size that combining their scores will improve the re-
sulting model. In Tab. 3 we compare the results of
the COMBINED measure to the results of its individ-
ual components - PROPORTION and BIGRAM. We
compare these results both when using the binary
and structured learning algorithms. Results show
that using the COMBINED measure leads to an im-
proved performance, better than any of the individ-
ual measures, suggesting that it can effectively ex-
ploit the properties of each confidence measure. Fur-
thermore, COMBINED is the only sample selection
strategy that outperforms ALL EXAMPLES.
(3) Can confidence measures serve as a good
proxy for the model?s performance? In the unsu-
pervised settings we study the learning process may
not converge to an optimal model. We argue that
by selecting the model that maximizes the averaged
confidence score, a better model can be found. We
validate this claim empirically in Tab. 4. We com-
pare the performance of the model selected using
the confidence score to the performance of the fi-
nal model considered by the learning algorithm (see
Sec. 3.1 for details). We also compare it to the best
model achieved in any of the learning iterations.
Since these experiments required running the
learning algorithm many times, we focused on the
binary learning algorithm as it converges consider-
ably faster. In order to focus the evaluation on the
effects of learning, we ignore the initial model gen-
erated manually (INITIAL MODEL) in these exper-
iments. In order to compare models performance
across the different iterations fairly, a uniform scale,
such as UNIGRAM and BIGRAM, is required. In the
case of the COMBINED measure we used the BI-
GRAM measure for performance estimation, since it
is one of its underlying components. In the PRED.
SCORE and PROPORTION models we used both their
confidence prediction, and the simple UNIGRAM
confidence score to evaluate model performance (the
latter appear in parentheses in Tab. 4).
Results show that the over overall confidence
score serves as a reliable proxy for the model perfor-
mance - using UNIGRAM and BIGRAM the frame-
work can select the best performing model, far better
than the performance of the default model to which
the system converged.
Algorithm Supervision Acc.
INITIAL MODEL ? 0.222
SELF-TRAIN: (Structured)
PRED. SCORE ? 0.348
ALL EXAMPLES ? 0.656
COMBINED ? 0.664
SELF-TRAIN: (Binary)
PRED. SCORE ? 0.164
COMBINED ? 0.536
RESPONSE BASED
BINARY 250 (binary) 0.692
STRUCTURED 250 (binary) 0.732
SUPERVISED
STRUCTURED 250 (struct.) 0.804
Table 2: Comparing our Self-trained systems with
Response-based and supervised models. Results show
that our COMBINED approach outperforms all other un-
supervised models.
Algorithm Accuracy
SELF-TRAIN: (Structured)
PROPORTION 0.6
BIGRAM 0.644
COMBINED 0.664
SELF-TRAIN: (Binary)
BIGRAM 0.532
PROPORTION 0.504
COMBINED 0.536
Table 3: Comparing COMBINED to its components BI-
GRAM and PROPORTION. COMBINED results in a better
score than any of its components, suggesting that it can
exploit the properties of each measure effectively.
Algorithm Best Conf. estim. Default
PRED. SCORE 0.164 0.128 (0.164) 0.134
UNIGRAM 0.52 0.52 0.4
BIGRAM 0.532 0.532 0.472
PROPORTION 0.504 0.27 (0.504) 0.44
COMBINED 0.536 0.536 0.328
Table 4: Using confidence to approximate model perfor-
mance. We compare the best result obtained in any of the
learning algorithm iterations (Best), the result obtained
by approximating the best result using the averaged pre-
diction confidence (Conf. estim.) and the result of us-
ing the default convergence criterion (Default). Results
in parentheses are the result of using the UNIGRAM con-
fidence to approximate the model?s performance.
1493
6 Related Work
Semantic parsing has attracted considerable interest
in recent years. Current approaches employ various
machine learning techniques for this task, such as In-
ductive Logic Programming in earlier systems (Zelle
and Mooney, 1996; Tang and Mooney, 2000) and
statistical learning methods in modern ones (Ge and
Mooney, 2005; Nguyen et al, 2006; Wong and
Mooney, 2006; Kate and Mooney, 2006; Zettle-
moyer and Collins, 2005; Zettlemoyer and Collins,
2007; Zettlemoyer and Collins, 2009).
The difficulty of providing the required supervi-
sion motivated learning approaches using weaker
forms of supervision. (Chen and Mooney, 2008;
Liang et al, 2009; Branavan et al, 2009; Titov and
Kozhevnikov, 2010) ground NL in an external world
state directly referenced by the text. The NL input in
our setting is not restricted to such grounded settings
and therefore we cannot exploit this form of supervi-
sion. Recent work (Clarke et al, 2010; Liang et al,
2011) suggest using response-based learning proto-
cols, which alleviate some of the supervision effort.
This work takes an additional step in this direction
and suggest an unsupervised protocol.
Other approaches to unsupervised semantic anal-
ysis (Poon and Domingos, 2009; Titov and Kle-
mentiev, 2011) take a different approach to seman-
tic representation, by clustering semantically equiv-
alent dependency tree fragments, and identifying
their predicate-argument structure. While these ap-
proaches have been applied successfully to semantic
tasks such as question answering, they do not ground
the input in a well defined output language, an essen-
tial component in our task.
Our unsupervised approach follows a self training
protocol (Yarowsky, 1995; McClosky et al, 2006;
Reichart and Rappoport, 2007b) enhanced with con-
straints restricting the output space (Chang et al,
2007; Chang et al, 2009). A Self training proto-
col uses its own predictions for training. We esti-
mate the quality of the predictions and use only high
confidence examples for training. This selection cri-
terion provides an additional view, different than the
one used by the prediction model. Multi-view learn-
ing is a well established idea, implemented in meth-
ods such as co-training (Blum and Mitchell, 1998).
Quality assessment of a learned model output was
explored by many previous works (see (Caruana and
Niculescu-Mizil, 2006) for a survey), and applied
to several NL processing tasks such as syntactic
parsing (Reichart and Rappoport, 2007a; Yates et
al., 2006), machine translation (Ueffing and Ney,
2007), speech (Koo et al, 2001), relation extrac-
tion (Rosenfeld and Feldman, 2007), IE (Culotta and
McCallum, 2004), QA (Chu-Carroll et al, 2003)
and dialog systems (Lin and Weng, 2008).
In addition to sample selection we use confidence
estimation as a way to approximate the overall qual-
ity of the model and use it for model selection. This
use of confidence estimation was explored in (Re-
ichart et al, 2010), to select between models trained
with different random starting points. In this work
we integrate this estimation deeper into the learning
process, thus allowing our training procedure to re-
turn the best performing model.
7 Conclusions
We introduced an unsupervised learning algorithm
for semantic parsing, the first for this task to the best
of our knowledge. To compensate for the lack of
training data we use a self-training protocol, driven
by unsupervised confidence estimation. We demon-
strate empirically that our approach results in a high
preforming semantic parser and show that confi-
dence estimation plays a vital role in this success,
both by identifying good training examples as well
as identifying good over all performance, used to
improve the final model selection.
In future work we hope to further improve un-
supervised semantic parsing performance. Particu-
larly, we intend to explore new approaches for confi-
dence estimation and their usage in the unsupervised
and semi-supervised versions of the task.
Acknowledgments We thank the anonymous re-
viewers for their helpful feedback. This material
is based upon work supported by DARPA under
the Bootstrap Learning Program and Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the view
of the DARPA, AFRL, or the US government.
1494
References
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT.
S.R.K. Branavan, H. Chen, L. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In ACL.
R. Caruana and A. Niculescu-Mizil. 2006. An empiri-
cal comparison of supervised l earning algorithms. In
ICML.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Proc.
of the Annual Meeting of the ACL.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for transliter-
ation discovery. In NAACL.
D. Chen and R. Mooney. 2008. Learning to sportscast: a
test of grounded language acquisition. In ICML.
J. Chu-Carroll, J. Prager K. Czuba, and A. Ittycheriah.
2003. In question answering, two heads are better than
on. In HLT-NAACL.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In CoNLL, 7.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In EMNLP?VLC.
A. Culotta and A. McCallum. 2004. Confidence estima-
tion for information extraction. In HLT-NAACL.
R. Ge and R. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In CoNLL.
R. Kate and R. Mooney. 2006. Using string-kernels for
learning semantic parsers. In ACL.
Y. Koo, C. Lee, and B. Juang. 2001. Speech recogni-
tion and utterance verification based on a generalized
confidence score. IEEE Transactions on Speech and
Audio Processing, 9(8):821?832.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
ACL.
P. Liang, M.I. Jordan, and D. Klein. 2011. Deep compo-
sitional semantics from shallow supervision. In ACL.
F. Lin and F. Weng. 2008. Computing confidence scores
for all sub parse trees. In ACL.
D. McClosky, E. Charniak, and Mark Johnson. 2006.
Effective self-training for parsing. In HLT-NAACL.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K.J.
Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography.
L. Nguyen, A. Shimazu, and X. Phan. 2006. Seman-
tic parsing with structured svm ensemble classification
models. In ACL.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In EMNLP.
R. Reichart and A. Rappoport. 2007a. An ensemble
method for selection of high quality parses. In ACL.
R. Reichart and A. Rappoport. 2007b. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In ACL.
R. Reichart, R. Fattal, and A. Rappoport. 2010. Im-
proved unsupervised pos induction using intrinsic
clustering quality and a zipfian constraint. In CoNLL.
B. Rosenfeld and R. Feldman. 2007. Using corpus statis-
tics on entities to improve semi?supervised relation
extraction from the web. In ACL.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
L. Tang and R. Mooney. 2000. Automated construction
of database interfaces: integrating statistical and rela-
tional learning for semantic parsing. In EMNLP.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In ECML.
I. Titov and A. Klementiev. 2011. A bayesian model for
unsupervised semantic parsing. In ACL.
I. Titov and M. Kozhevnikov. 2010. Bootstrapping
semantic analyzers from non-contradictory texts. In
ACL.
N. Ueffing and H. Ney. 2007. Word-level confidence es-
timation for machine translation. Computational Lin-
guistics, 33(1):9?40.
Y.W. Wong and R. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In
NAACL.
Y.W. Wong and R. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In ACL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised method. In ACL.
A. Yates, S. Schoenmackers, and O. Etzioni. 2006. De-
tecting parser errors using web-based semantic filters.
In EMNLP.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
AAAI.
L. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI.
L. Zettlemoyer and M. Collins. 2007. Online learning of
relaxed CCG grammars for parsing to logical form. In
CoNLL.
L. Zettlemoyer and M. Collins. 2009. Learning context-
dependent mappings from sentences to logical form.
In ACL.
1495
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 835?844,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Automatic Event Extraction with Structured Preference Modeling
Wei Lu and Dan Roth
University of Illinois at Urbana-Champaign
{luwei,danr}@illinois.edu
Abstract
This paper presents a novel sequence label-
ing model based on the latent-variable semi-
Markov conditional random fields for jointly
extracting argument roles of events from texts.
The model takes in coarse mention and type
information and predicts argument roles for a
given event template.
This paper addresses the event extraction
problem in a primarily unsupervised setting,
where no labeled training instances are avail-
able. Our key contribution is a novel learning
framework called structured preference mod-
eling (PM), that allows arbitrary preference
to be assigned to certain structures during the
learning procedure. We establish and discuss
connections between this framework and other
existing works. We show empirically that the
structured preferences are crucial to the suc-
cess of our task. Our model, trained with-
out annotated data and with a small number
of structured preferences, yields performance
competitive to some baseline supervised ap-
proaches.
1 Introduction
Automatic template-filling-based event extraction is
an important and challenging task. Consider the fol-
lowing text span that describes an ?Attack? event:
. . . North Korea?s military may have fired a laser
at a U.S. helicopter in March, a U.S. official
said Tuesday, as the communist state ditched its
last legal obligation to keep itself free of nuclear
weapons . . .
A partial event template for the ?Attack? event is
shown on the left of Figure 1. Each row shows an
argument for the event, together with a set of its ac-
ceptable mention types, where the type specifies a
high-level semantic class a mention belongs to.
The task is to automatically fill the template en-
tries with texts extracted from the text span above.
The correct filling of the template for this particular
example is shown on the right of Figure 1.
Performing such a task without any knowledge
about the semantics of the texts is hard. One typi-
cal assumption is that certain coarse mention-level
information, such as mention boundaries and their
semantic class (a.k.a. types), are available. E.g.:
. . . [North Korea?s military]ORG may have fired
[a laser]WEA at [a U.S. helicopter]VEH in
[March]TME, a U.S. official said Tuesday, as the
communist state ditched its last legal obligation
to keep itself free of nuclear weapons . . .
Such mention type information as shown on the
left of Figure 1 can be obtained from various sources
such as dictionaries, gazetteers, rule-based systems
(Stro?tgen and Gertz, 2010), statistically trained clas-
sifiers (Ratinov and Roth, 2009), or some web re-
sources such as Wikipedia (Ratinov et al, 2011).
However, in practice, outputs from existing men-
tion identification and typing systems can be far
from ideal. Instead of obtaining the above ideal an-
notation, one might observe the following noisy and
ambiguous annotation for the given event span:
. . . [[North Korea?s]GPE|LOC military]ORG may have
fired a laser at [a [U.S.]GPE|LOC helicopter]VEH
in [March]TME, [a [U.S.]GPE|LOC official]PER said
[Tuesday]TME, as [the communist state]ORG|FAC|LOC
ditched its last legal obligation to keep [itself ]ORG
free of [nuclear weapons]WEA . . .
Our task is to design a model to effectively select
mentions in an event span and assign them with cor-
responding argument information, given such coarse
835
Argument Possible Types Extracted Text
ATTACKER GPE, ORG, PER N. Korea?s military
INSTRUMENT VEH, WEA a laser
PLACE FAC, GPE, LOC -
TARGET
FAC, GPE, LOC
a U.S. helicopter
ORG, PER, VEH
TIME-WITHIN TME March
Figure 1: The partial event template for the Attack event (left),
and the correct event template annotation for the example event
span given in Sec 1 (right). We primarily follow the ACE stan-
dard in defining arguments and types.
and often noisy mention type annotations.
This work addresses this problem by making the
following contributions:
? Naturally, we are interested in identifying the
active mentions (the mentions that serve as ar-
guments) and their correct boundaries from the
data. This motivates us to build a novel latent-
variable semi-Markov conditional random fields
model (Sarawagi and Cohen, 2004) for such an
event extraction task. The learned model takes
in coarse information as produced by existing
mention identification and typing modules, and
jointly outputs selected mentions and their cor-
responding argument roles.
? We address the problem in a more realistic sce-
nario where annotated training instances are not
available. We propose a novel general learning
framework called structured preference model-
ing (or preference modeling, PM), which en-
compasses both the fully supervised and the
latent-variable conditional models as special
cases. The framework allows arbitrary declar-
ative structured preference knowledge to be in-
troduced to guide the learning procedure in a pri-
marily unsupervised setting.
We present our semi-Markov model and discuss
our preference modeling framework in Section 2 and
3 respectively. We then discuss the model?s relation
with existing constraint-driven learning frameworks
in Section 4. Finally, we demonstrate through ex-
periments that structured preference information is
crucial to model and present empirical results on a
standard dataset in Section 5.
2 The Model
It is not hard to observe from the example presented
in the previous section that dependencies between
A1
T1
C1
B2
C2
A3
T3
C3
B4
C4
. . .
. . .
. . .
An
Tn
Cn
Figure 2: A simplified graphical illustration for the semi-
Markov CRF, under a specific segmentation S ? C1C2 . . . Cn.
In a supervised setting, only correct arguments are observed but
their associated correct mention types are hidden (shaded).
arguments can be important and need to be properly
modeled. This motivates us to build a joint model
for extracting the event structures from the text.
We show a simplified graphical representation of
our model in Figure 2. In the graph, C1, C2 . . . Cn
refer to a particular segmentation of the event
span, where C1, C3 . . . correspond to mentions
(e.g., ?North Korea?s military?, ?a laser?) and C2,
C4 . . . correspond to in-between mention word se-
quences (we call them gaps) (e.g., ?may have
fired?). The symbols T1, T3 . . . refer to mention
types (e.g., GPE, ORG). The symbols A1, A3 . . . re-
fer to event arguments that carry specific roles (e.g.,
ATTACKER). We also introduce symbols B2, B4 . . .
to refer to inter-argument gaps. The event span is
split into segments, where each segment is either
linked to a mention type (Ti; these segments can
be referred to as ?argument segments?), or directly
linked to an inter-argument gap (Bj ; they can be
referred to as ?gap segments?). The two types of
segments appear in the sequence in a strictly alter-
nate manner, where the gaps can be of length zero.
In the figure, for example, the segments C1 and C3
are identified as two argument segments (which are
mentions of types T1 and T3 respectively) and are
mapped to two ?nodes?, and the segment C2 is iden-
tified as a gap segment that connects the two argu-
ments A1 and A3. Note that no overlapping argu-
ments are allowed in this model 1.
We use s to denote an event span and t to denote
a specific realization (filling) of the event template.
Templates consist of a set of arguments. Denote by h
a particular mention boundary and type assignment
for an event span, which gives us a specific segmen-
tation of the given span. Following the conditional
1Extending the model to support certain argument overlap-
ping is possible ? we leave it for future work.
836
random fields model (Lafferty et al, 2001), we pa-
rameterize the conditional probability of the (t, h)
pair given an event span s as follows:
P?(t, h|s) =
ef(s,h,t)??
?
t,h e
f(s,h,t)??
(1)
where f gives the feature functions defined on the
tuple (s, h, t), and ? defines the parameter vector.
Our objective function is the logarithm of the joint
conditional probability of observing the template re-
alization for the observed event span s:
L(?) =
?
i
logP?(ti|si)
=
?
i
log
?
h e
f(si,h,ti)??
?
t,h e
f(si,h,t)??
(2)
This function is not convex due to the summation
over the hidden variable h. To optimize it, we take
its partial derivative with respect to ?j :
?L(?)
??j
=
?
i
Ep?(h|si,ti)[fj(si, h, ti)]
?
?
i
Ep?(t,h|si)[fj(si, h, t)] (3)
which requires computation of expectations terms
under two different distributions. Such statistics
can be collected efficiently with a forward-backward
style algorithm in polynomial time (Okanohara et
al., 2006). We will discuss the time complexity for
our case in the next section.
Given its partial derivatives in Equation 3, one
could optimize the objective function of Equation 2
with stochastic gradient ascent (LeCun et al, 1998)
or L-BFGS (Liu and Nocedal, 1989). We choose to
use L-BFGS for all our experiments in this paper.
Inference involves computing the most probable
template realization t for a given event span:
arg max
t
P?(t|s) = arg max
t
?
h
P?(t, h|s) (4)
where the possible hidden assignments h need to be
marginalized out. In this task, a particular realiza-
tion t already uniquely defines a particular segmen-
tation (mention boundaries) of the event span, thus
the h only contributes type information to t. As we
will discuss in Section 2.3, only a collection of local
features are defined. Thus, a Viterbi-style dynamic
programming algorithm is used to efficiently com-
pute the desired solution.
2.1 Possible Segmentations
According to Equation 3, summing over all possi-
ble h is required. Since one primary assumption is
that we have access to the output of existing mention
identification and typing systems, the set of all possi-
ble mentions defines a lattice representation contain-
ing the set of all possible segmentations that com-
ply with such mention-level information. Assuming
there are A possible arguments for the event and K
annotated mentions, the complexity of the forward-
backward style algorithm is in O(A3K2) under the
?second-order? setting that we will discuss in Sec-
tion 2.2. Typically, K is smaller than the number of
words in the span, and the factor A3 can be regarded
as a constant. Thus, the algorithm is very efficient.
As we have mentioned earlier, such coarse infor-
mation, as produced by existing resources, could be
highly ambiguous and noisy. Also, the output men-
tions can highly overlap with each other. For exam-
ple, the phrase ?North Korea? as in ?North Korea?s
military? can be assigned both type GPE and LOC,
while ?North Korea?s military? can be assigned the
type ORG. Our model will need to disambiguate the
mention boundaries as well as their types.
2.2 The Gap Segments
We believe the gap segments2 are important to
model since they can potentially capture depen-
dencies between two or more adjacent arguments.
For example, the word sequence ?may have fired?
clearly indicates an Attacker-Instrument relation be-
tween the two mentions ?North Korea?s military?
and ?a laser?. Since we are only interested in
modeling dependencies between adjacent argument
segments, we assign hard labels to each gap seg-
ment based on its contextual argument informa-
tion. Specifically, the label of each gap segment
is uniquely determined by its surrounding argu-
ment segments with a list representation. For ex-
ample, in a ?first-order? setting, the gap segment
that appears between its previous argument seg-
ment ?ATTACKER? and its next argument segment
?INSTRUMENT? is annotated as the list consisting
of two elements: [ATTACKER, INSTRUMENT]. To
capture longer-range dependencies, in this work we
use a ?second-order? setting (as shown in Figure 2),
2The length of a gap segment is arbitrary (including zero),
unlike the seminal semi-Markov CRF model of Sarawagi and
Cohen (2004).
837
which means each gap segment is annotated with a
list that consists of its previous two argument seg-
ments as well as its subsequent one.
2.3 Features
Feature functions are factorized as products of two
indicator functions: one defined on the input se-
quence (input features) and the other on the output
labels (output features). In other words, we could
re-write fj(s, h, t) as f ink (s)? f
out
l (h, t).
For gap segments, we consider the following in-
put feature templates:
N-GRAM: Indicator function for n-gram appeared
in the segment (n = 1, 2)
ANCHOR: Indicator function for its relative position
to the event anchor words (to the left, to
the right, overlaps, contains)
and the following output feature templates:
1STORDER: Indicator function for the combination of
its immediate left argument and its imme-
diate right argument.
2NDORDER: Indicator function for the combination of
its immediate two left arguments and its
immediate right argument.
For argument segments, we also define the same
input feature templates as above, with the following
additional ones to capture contextual information:
CWORDS: Indicator function for the previous and
next k (= 1, 2, 3) words.
CPOS: Indicator function for the previous and
next k (= 1, 2, 3) words? POS tags.
and we define the following output feature template:
ARGTYPE: Indicator function for the combination of
the argument and its associated type.
Although the semi-Markov CRF model gives us
the flexibility in introducing features that can not be
exploited in a standard CRF, such as entity name
similarity scores and distance measures, in prac-
tice we found the above simple and general features
work well. This way, the unnormalized score as-
signed to each structure is essentially a linear sum
of the feature weights, each corresponding to an in-
dicator function.
3 Learning without Annotated Data
The supervised model presented in the previous sec-
tion requires substantial human efforts to annotate
the training instances. Human annotations can be
very expensive and sometimes impractical. Even if
annotators are available, getting annotators to agree
with each other is often a difficult task in itself.
Worse still, annotations often can not be reused: ex-
perimenting on a different domain or dataset typi-
cally require annotating new training instances for
that particular domain or dataset.
We investigate inexpensive methods to alleviate
this issue in this section. We introduce a novel gen-
eral learning framework called structured preference
modeling, which allows arbitrary prior knowledge
about structures to be introduced to the learning pro-
cess in a declarative manner.
3.1 Structured Preference Modeling
Denote by X? and Y? the entire input and output
space, respectively. For a particular input x ? X?,
the set x ? Y? gives us all possible structures that
contain x. However, structures are not equally good.
Some structures are generally regarded as better
structures while some are worse.
Let?s asume there is a function ? :
{
x ? Y? ?
[0, 1]
}
that measures the quality of the structures.
This function returns the quality of a certain struc-
ture (x, y), where the value 1 indicates a perfect
structure, and 0 an impossible structure.
Under such an assumption, it is easy to observe
that for a good structure (x, y), we have p?(x, y)?
?(x, y) = p?(x, y), while for a bad structure (x, y),
we have p?(x, y)? ?(x, y) = 0.
This motivates us to optimize the following objec-
tive function:
Lu(?) =
?
i
log
?
y p?(xi, y)? ?(xi, y)
?
y p?(xi, y)
(5)
Intuitively, optimizing such an objective function
is equivalent to pushing the probability mass from
bad structures to good structures corresponding to
the same input.
When the preference function ? is defined as the
indicator function for the correct structure (xi, yi),
the numerator terms of the above formula are simply
of the forms p?(xi, yi), and the model corresponds
to the fully supervised CRF model.
The model also contains the latent-variable CRF
as a special case. In a latent-variable CRF, we have
input-output pairs (xi, yi), but the underlying spe-
cific structure h that contains both xi and yi is hid-
den. The objective function is:
?
i
log
?
h p?(xi, h, yi)?
h,y? p?(xi, h, y
?)
(6)
838
where p?(xi, h, yi) = 0 unless h contains (xi, yi).
We define the following two functions:
q?(xi, h) =
?
y?
p?(xi, h, y
?) (7)
?(xi, h) =
{
1 h contains (xi, yi)
0 otherwise
(8)
Note that this definition of ? models instance-
specific preferences since it relies on yi, which can
be thought of as certain external prior knowledge re-
lated to xi. It is easy to verify that p?(xi, h, yi) =
q?(xi, h)??(xi, h), with q? remains a distribution.
Thus, we could re-write the objective function as:
?
i=1
log
?
h q?(xi, h)? ?(xi, h)?
h q?(xi, h)
(9)
This shows that the latent-variable CRF is a spe-
cial case of our objective function, with the above-
defined ? function. Thus, this new objective func-
tion of Equation 5 is a generalization of both the su-
pervised CRF and the latent-variable CRF.
The preference function ? serves as a source from
which certain prior knowledge about the structure
can be injected into our model in a principled way.
Note that the function is defined at the complete
structure level. This allows us to incorporate both
local and arbitrary global structured information into
the preference function.
Under the log-linear parameterization, we have:
L?(?) =
?
i
log
?
y e
f(xi,y)?? ? ?(xi, y)
?
y e
f(xi,y)??
(10)
This is again a non-convex optimization problem
in general, and to solve it we take its partial deriva-
tive with respect to ?k:
?L?(?)
??k
=
?
i
Ep?(y|xi;?)[fk(xi, y)]
?
?
i
Ep?(y|xi)[fk(xi, y)] (11)
p?(y|xi;?) ? e
f(xi,y)?? ? ?(xi, y)
p?(y|xi) ? e
f(xi,y)??
3.2 Approximate Learning
Computation of the denominator terms of Equation
10 (and the second term of Equation 11) can be done
efficiently and exactly with dynamic programming.
Our main concern is the computation of its numera-
tor terms (and the first term of Equation 11).
The preference function ? is defined at the com-
plete structure level. Unless the function is defined
in specific forms that allow tractable dynamic pro-
gramming (in the supervised case, which gives a
unique term, or in the hidden variable case, which
can define a packed representations of derivations),
the efficient dynamic programming algorithm used
by CRF is no longer generally applicable for arbi-
trary ?. In general, we resort to approximations.
In this work, we exploit a specific form of the
preference function ?. We assume that there exists
a projection from another decomposable function to
?. Specifically, we assume a collection of auxiliary
functions, each of the form ?p : (x, y) ? R, that
scores a property p of the complete structure (x, y).
Each such function measures certain aspect of the
quality of the structure. These functions assign pos-
itive scores to good structural properties and nega-
tive scores to bad ones. We then define ?(x, y) = 1
for all structures that appear at the top-n positions
as ranked by
?
p ?p(x, y) for all possible y?s, and
?(x, y) = 0 otherwise. We show some actual ?p
functions used for a particular event in Section 5.
At each iteration of the training process, to gen-
erate such a n-best list, we first use our model to
produce top n ? b candidate outputs as scored by
the current model parameters, and extract the top n
outputs as scored by
?
p ?p(x, y). In practice we set
n = 10 and b = 1000.
3.3 Event Extraction
Now we can obtain the objective function for our
event extraction task. We replace x by s and y by
(h, t) in Equation 10. This gives us the following
function:
Lu(?) =
?
i
log
?
t,h e
f(si,h,t)?? ? ?(si, h, t)
?
t,h e
f(si,h,t)??
(12)
The partial derivatives are as follows:
?Lu(?)
??k
=
?
i
Ep?(t,h|si;?)[fk(si, h, t)]
?
?
i
Ep?(t,h|si)[fk(si, h, t)] (13)
p?(t, h|si;?) ? e
f(si,h,t)?? ? ?(si, h, t)
p?(t, h|si) ? e
f(si,h,t)??
839
Recall that s is an event span, t is a specfic re-
alization of the event template, and h is the hidden
mention information for the event span.
4 Discussion: Preferences v.s. Constraints
Note that the objective function in Equation 5, if
written in the additive form, leads to a cost func-
tion reminiscent of the one used in constraint-driven
learning algorithm (CoDL) (Chang et al, 2007) (and
similarly, posterior regularization (Ganchev et al,
2010), which we will discuss later at Section 6).
Specifically, in CoDL, the following cost function
is involved in its EM-like inference procedure:
arg max
y
? ? f(x, y)? ?
?
c
d(y,Yc) (14)
where Yc defines the set of y?s that all satisfy a cer-
tain constraint c, and d defines a distance function
from y to that set. The parameter ? controls the de-
gree of the penalty when constraints are violated.
There are some important distinctions between
structured preference modeling (PM) and CoDL.
CoDL primarily concerns constraints, which pe-
nalizes bad structures without explicitly rewarding
good ones. On the other hand, PM concerns prefer-
ences, which can explicitly reward good structures.
Constraints are typically useful when one works
on structured prediction problems for data with cer-
tain (often rigid) regularities, such as citations, ad-
vertisements, or POS tagging for complete sen-
tences. In such tasks, desired structures typically
present certain canonical forms. This allows declar-
ative constraints to be specified as either local struc-
ture prototypes (e.g., in citation extraction, the word
pp. always corresponds to the PAGES field, while
proceedings is always associated with BOOKTITLE
or JOURNAL), or as certain global regulations about
complete structures (e.g., at least one word should
be tagged as verb when performing a sentence-level
POS tagging).
Unfortunately, imposing such (hard or soft) con-
straints for certain tasks such as ours, where the data
tends to be of arbitrary forms without many rigid
regularities, can be difficult and often inappropri-
ate. For example, there is no guarantee that a cer-
tain argument will always be present in the event
span, nor should a particular mention, if appeared,
always be selected and assigned to a specific argu-
ment. For example, in the example event span given
in Section 1, both ?March? and ?Tuesday? are valid
candidate mentions for the TIME-WITHIN argument
given their annotated type TME. One important clue
is that March appears after the word in and is lo-
cated nearer to other mentions that can be poten-
tially useful arguments. However, encoding such
information as a general constraint can be inappro-
priate, as potentially better structures can be found
if one considers other alternatives. On the other
hand, if we believe the structural pattern ?at TAR-
GET in TIME-WITHIN? is in general considered a
better sub-structure than ?said TIME-WITHIN? for
the ?Attack? event, we may want to assign structured
preference to a complete structure that contains the
former, unless there exist other structured evidence
showing the latter turns out to be better.
In this work, our preference function is related
to another function that can be decomposed into a
collection of property functions ?p. Each of them
scores a certain aspect of the complete structure.
This formulation gives us a complete flexibility to
assign arbitrary structured preferences, where posi-
tive scores can be assigned to good properties, and
negative scores to bad ones. Thus, in this way, the
quality of a complete structure is jointly measured
with multiple different property functions.
To summarize, preferences are an effective way to
?define? the event structure to the learner, which is
essential in an unsupervised setting, which may not
be easy to do with other forms of constraints. Prefer-
ences are naturally decomposable, which allows us
to extend their impact without significantly effecting
the complexity of inference.
5 Experiments
In this section, we present our experimental results
on the standard ACE053 dataset (newswire portion).
We choose to perform our evaluations on 4 events
(namely, ?Attack?, ?Meet?, ?Die? and ?Transport?),
which are the only events in this dataset that have
more than 50 instances. For each event, we ran-
domly split the instances into two portions, where
70% are used for learning, and the remaining 30%
for evaluation. We list the corpus statistics in Table
2.
To present general results while making minimal
assumptions, our primary event extraction results
3http://www.itl.nist.gov/iad/mig/tests/ace/2005/doc/
840
Event
Without Annotated Training Data With Annotated Training Data
Random Unsup Rule PM MaxEnt-b MaxEnt-t MaxEnt-p semi-CRF
Attack 20.47 30.12 39.25 42.02 54.03 58.82 65.18 63.11
Meet 35.48 26.09 44.07 63.55 65.42 70.48 75.47 76.64
Die 30.03 13.04 40.58 55.38 51.61 59.65 63.18 67.65
Transport 20.40 6.11 44.34 57.29 53.76 57.63 61.02 64.19
Table 1: Performance for different events under different experimental settings, with gold mention boundaries and types. We report
F1-measure percentages.
Event #A
Learning Set Evaluation Set
#P
#I #M #I #M
Attack 8 188 300/509 78 121/228 7
Meet 7 57 134/244 24 52/98 7
Die 9 41 89/174 19 33/61 6
Transport 13 85 243/426 38 104/159 6
Table 2: Corpus statistics (#A: number of possible arguments
for the event; #I: number of instances; #M: number of ac-
tive/total mentions; #P: number of preference patterns used
for performing our structured preference modeling.)
are independent of mention identification and typing
modules, which are based on the gold mention in-
formation as given by the dataset. Additionally, we
present results obtained by exploiting our in-house
automatic mention identification and typing mod-
ule, which is a hybrid system that combines statis-
tical and rule-based approaches. The module?s sta-
tistical component is trained on the ACE04 dataset
(newswire portion) and overall it achieves a micro-
averaged F1-measure of 71.25% at our dataset.
5.1 With Annotated Training Data
With hand-annotated training data, we are able to
train our model in a fully supervised manner. The
right part of Table 1 shows the performance for
the fully supervised models. For comparison, we
present results from several alternative approaches
based a collection of locally trained maximum en-
tropy (MaxEnt) classifiers. In these approaches, we
treat each argument of the template as one possi-
ble output class, plus a special ?NONE? class for
not selecting it as an argument. We train and apply
the classifiers on argument segments (i.e., mentions)
only. All the models are trained with the same fea-
ture set used in the semi-CRF model.
In the simplest baseline approach MaxEnt-b, type
information for each mention is simply treated as
one special feature. In the approach MaxEnt-t, we
instead use the type information to constrain the
classifier?s predictions based on the acceptable types
associated with each argument. This approach gives
better performance than that of MaxEnt-b. This in-
dicates that such locally trained classifiers are not
robust enough to disambiguate arguments that take
different types. As such, type information serving as
additional constraints at the end does help.
To assess the importance of structured preference,
we also perform experiments where structured pref-
erence information is incorporated at the inference
time of the MaxEnt classifiers. Specifically, for each
event, we first generate n-best lists for output struc-
tures. Next, we re-rank this list based on scores
from our structured preference functions (we used
the same preferences as to be discussed in the next
section). The results for these approaches are given
in the column of MaxEnt-p of Table 1. This simple
approach gives us significant improvements, clos-
ing the gap between locally trained classifiers and
the joint model (in one case the former even out-
performs the latter). Note that no structured pref-
erence information is used when training and eval-
uating our semi-CRF model. This set of results is
not surprising. In fact, similar observations are also
reported in previous works when comparing joint
model against local models with constraints incor-
porated (Roth and Yih, 2005). This clearly indicates
that structured preference information is crucial to
model.
5.2 Without Annotated Training Data
Now we turn to experiments for the more realistic
scenario where human annotations are not available.
We first build our simplest baseline by randomly
assigning arguments to each mention with mention
type information serving as constraints. Averaged
results over 1000 runs are reported in the first col-
umn of Table 1.
Since our model formulation leaves us with com-
plete freedom in designing the preference function,
841
Type Preference pattern (p)
General
{at|in|on} followed by PLACE
{during|at|in|on} followed by TIME-WITHIN
Die
AGENT (immediately) followed by {killed}
{killed} (immediately) followed by VICTIM
VICTIM (immediately) followed by {be killed}
AGENT followed by {killed} (immediately) followed by VICTIM
Transport
X immediately followed by {,|and} immediately followed by X, where X ? {ORIGIN|DESTINATION}
{from|leave} (immediately) followed by ORIGIN
{at|in|to|into} immediately followed by DESTINATION
PERSON followed by {to|visit|arrived}
Figure 3: The complete list of preference patterns used for the ?Die? and ?Transport? event. We simply set ?p = 1.0 for all p?s. In
other words, when a structure contains a pattern, its score is incremented by 1.0. We use {} to refer to a set of possible words or
arguments. For example, {from|leave} means a word which is either from or leave. The symbol () denotes optional. For example,
?{killed} (immediately) followed by VICTIM? is equivalent to the following two preferences: ?{killed} immediately followed by
VICTIM?, and ?{killed} followed by VICTIM?.
one could design arbitrarily good, domain-specific
or even instance-specific preferences. However, to
demonstrate its general effectiveness, in this work
we only choose a minimal amount of general prefer-
ence patterns for evaluations.
We make our preference patterns as general as
possible. As shown in the last column (#P) of Table
2, we use only 7 preference patterns each for the ?At-
tack? and ?Meet? events, and 6 patterns each for the
other two events. In Figure 3, we show the complete
list of the 6 preference patterns for the ?Die? and
?Transport? event used for our experiments. Out of
those 6 patterns, 2 are more general patterns shared
across different events, and 4 are event-specific. In
contrast, for example, for the ?Die? event, the super-
vised approach requires human to select from 174
candidate mentions and annotate 89 of them.
Despite its simplicity, it works very well in prac-
tice. Results are given in the column of ?PM? of
Table 1. It generally gives competitive performance
as compared to the supervised MaxEnt baselines.
On the other hand, a completely unsupervised ap-
proach where structured preferences are not speci-
fied, performs substantially worse. To run such com-
pletely unsupervised models, we essentially follow
the same training procedure as that of the prefer-
ence modeling, except that structured preference in-
formation is not in place when generating the n-best
list. In the absence of proper guidances, such a pro-
cedure can easily converge to bad local minima. The
results are reported in the ?Unsup? column of Ta-
ble 1. In practice, we found that very often, such
a model would prefer short structures where many
mentions are not selected as desired. As a result, the
unsupervised model without preference information
can even perform worse than the random baseline 4.
Finally, we also compare against an approach that
regards the preferences as rules. All such rules are
associated with a same weight and are used to jointly
score each structure. We then output the structure
that is assigned the highest total weight. Such an ap-
proach performs worse than our approach with pref-
erence modeling. The results are presented in the
column of ?Rule? of Table 1. This indicates that
our model is able to learn to generalize with features
through the guidance of our informative preferences.
However, we also note that the performance of pref-
erence modeling depends on the actual quality and
amount of preferences used for learning. In the ex-
treme case, where only few preferences are used, the
performance of preference modeling will be close to
that of the unsupervised approach, while the rule-
based approach will yield performance close to that
of the random baseline.
The results with automatically predicted mention
boundaries and types are given in Table 3. Simi-
lar observations can be made when comparing the
performance of preference modeling with other ap-
proaches. This set of results further confirms the ef-
fectiveness of our approach using preference model-
ing for the event extraction task.
6 Related Work
Structured prediction with limited supervision is a
popular topic in natural language processing.
4For each event, we only performed 1 run with all the initial
feature weights set to zeros.
842
Event Random Unsup PM semi-CRF
Attack 14.26 26.19 32.89 46.92
Meet 26.65 14.08 45.28 58.18
Die 19.17 9.09 44.44 48.57
Transport 15.78 10.14 49.73 52.34
Table 3: Event extraction performance with automatic mention
identifier and typer. We report F1 percentage scores for pref-
erence modeling (PM) as well as two baseline approaches. We
also report performance of the supervised approach trained with
the semi-CRF model for comparison.
Prototype driven learning (Haghighi and Klein,
2006) tackled the sequence labeling problem in a
primarily unsupervised setting. In their work, a
Markov random fields model was used, where some
local constraints are specified via their prototype list.
Constraint-driven learning (CoDL) (Chang et al,
2007) and posterior regularization (PR) (Ganchev et
al., 2010) are both primarily semi-supervised mod-
els. They define a constrained EM framework that
regularizes posterior distribution at the E-step of
each EM iteration, by pushing posterior distributions
towards a constrained posterior set. We have already
discussed CoDL in Section 4 and gave a comparison
to our model. Unlike CoDL, in the PR framework
constraints are relaxed to expectation constraints, in
order to allow tractable dynamic programming. See
also Samdani et al (2012) for more discussions.
Contrastive estimation (CE) (Smith and Eisner,
2005a) is another log-linear framework for primar-
ily unsupervised structured prediction. Their objec-
tive function is related to the pseudolikelihood es-
timator proposed by Besag (1975). One challenge
is that it requires one to design a priori an effective
neighborhood (which also needs to be designed in
certain forms to allow efficient computation of the
normalization terms) in order to obtain optimal per-
formance. The model has been shown to work in un-
supervised tasks such as POS induction (Smith and
Eisner, 2005a), grammar induction (Smith and Eis-
ner, 2005b), and morphological segmentation (Poon
et al, 2009), where good neighborhoods can be
identified. However, it is less intuitive what consti-
tutes a good neighborhood in this task.
The neighborhood assumption of CE is relaxed
in another latent structure approach (Chang et al,
2010a; Chang et al, 2010b) that focuses on semi-
supervised learning with indirect supervisions, in-
spired by the CoDL model described above.
The locally normalized logistic regression (Berg-
Kirkpatrick et al, 2010) is another recently proposed
framework for unsupervised structured prediction.
Their model can be regarded as a generative model
whose component multinomial is replaced with a
miniature logistic regression where a rich set of local
features can be incorporated. Empirically the model
is effective in various unsupervised structured pre-
diction tasks, and outperforms the globally normal-
ized model. Although modeling the semi-Markov
properties of our segments (especially the gap seg-
ments) in our task is potentially challenging, we plan
to investigate in the future the feasibility for our task
with such a framework.
7 Conclusions
In this paper, we present a novel model based on
the semi-Markov conditional random fields for the
challenging event extraction task. The model takes
in coarse mention boundary and type information
and predicts complete structures indicating the cor-
responding argument role for each mention.
To learn the model in an unsupervised manner,
we further develop a novel learning approach called
structured preference modeling that allows struc-
tured knowledge to be incorporated effectively in a
declarative manner.
Empirically, we show that knowledge about struc-
tured preference is crucial to model and the prefer-
ence modeling is an effective way to guide learn-
ing in this setting. Trained in a primarily unsuper-
vised manner, our model incorporating structured
preference information exhibits performance that is
competitive to that of some supervised baseline ap-
proaches. Our event extraction system and code will
be available for download from our group web page.
Acknowledgments
We would like to thank Yee Seng Chan, Mark Sam-
mons, and Quang Xuan Do for their help with the
mention identification and typing system used in
this paper. We gratefully acknowledge the sup-
port of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the view of DARPA,
AFRL, or the US government.
843
References
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In Proc. of HLT-NAACL?10, pages 582?590.
J. Besag. 1975. Statistical analysis of non-lattice data.
The Statistician, pages 179?195.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Proc.
of ACL?07, pages 280?287.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010a. Discriminative learning over constrained latent
representations. In Proc. of NAACL?10, 6.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010b. Structured output learning with indirect super-
vision. In Proc. ICML?10.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. The Journal of Machine Learning
Research (JMLR), 11:2001?2049.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proc. of HLT-NAACL?06,
pages 320?327.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc. of
ICML?01, pages 282?289.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recogni-
tion. Proc. of the IEEE, pages 2278?2324.
D.C. Liu and J. Nocedal. 1989. On the limited memory
bfgs method for large scale optimization. Mathemati-
cal programming, 45(1):503?528.
D. Okanohara, Y. Miyao, Y. Tsuruoka, and J. Tsujii.
2006. Improving the scalability of semi-markov con-
ditional random fields for named entity recognition. In
Proc. of ACL?06, pages 465?472.
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsu-
pervised morphological segmentation with log-linear
models. In Proc. of HLT-NAACL?09, pages 209?217.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of CoNLL?09, pages 147?155.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proc. of ACL-HLT?11, pages 1375?
1384.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In Proc. of
ICML?05, pages 736?743.
R. Samdani, M. Chang, and D. Roth. 2012. Unified ex-
pectation maximization. In Proc. NAACL?12.
S. Sarawagi and W.W. Cohen. 2004. Semi-markov
conditional random fields for information extraction.
NIPS?04, pages 1185?1192.
N.A. Smith and J. Eisner. 2005a. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL?05, pages 354?362.
N.A. Smith and J. Eisner. 2005b. Guiding unsupervised
grammar induction using contrastive estimation. In
Proc. of IJCAI Workshop on Grammatical Inference
Applications, pages 73?82.
J. Stro?tgen and M. Gertz. 2010. Heideltime: High qual-
ity rule-based extraction and normalization of tempo-
ral expressions. In Proc. of SemEval?10, pages 321?
324.
844
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 905?913,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Margin-based Decomposed Amortized Inference
Gourab Kundu? and Vivek Srikumar? and Dan Roth
University of Illinois, Urbana-Champaign
Urbana, IL. 61801
{kundu2, vsrikum2, danr}@illinois.edu
Abstract
Given that structured output prediction is
typically performed over entire datasets,
one natural question is whether it is pos-
sible to re-use computation from earlier
inference instances to speed up inference
for future instances. Amortized inference
has been proposed as a way to accomplish
this. In this paper, first, we introduce a new
amortized inference algorithm called the
Margin-based Amortized Inference, which
uses the notion of structured margin to
identify inference problems for which pre-
vious solutions are provably optimal. Sec-
ond, we introduce decomposed amortized
inference, which is designed to address
very large inference problems, where ear-
lier amortization methods become less ef-
fective. This approach works by decom-
posing the output structure and applying
amortization piece-wise, thus increasing
the chance that we can re-use previous so-
lutions for parts of the output structure.
These parts are then combined to a global
coherent solution using Lagrangian relax-
ation. In our experiments, using the NLP
tasks of semantic role labeling and entity-
relation extraction, we demonstrate that
with the margin-based algorithm, we need
to call the inference engine only for a third
of the test examples. Further, we show that
the decomposed variant of margin-based
amortized inference achieves a greater re-
duction in the number of inference calls.
1 Introduction
A wide variety of NLP problems can be natu-
rally cast as structured prediction problems. For
* These authors contributed equally to this work.
some structures like sequences or parse trees, spe-
cialized and tractable dynamic programming algo-
rithms have proven to be very effective. However,
as the structures under consideration become in-
creasingly complex, the computational problem of
predicting structures can become very expensive,
and in the worst case, intractable.
In this paper, we focus on an inference tech-
nique called amortized inference (Srikumar et al,
2012), where previous solutions to inference prob-
lems are used to speed up new instances. The
main observation that leads to amortized inference
is that, very often, for different examples of the
same size, the structures that maximize the score
are identical. If we can efficiently identify that two
inference problems have the same solution, then
we can re-use previously computed structures for
newer examples, thus giving us a speedup.
This paper has two contributions. First, we de-
scribe a novel algorithm for amortized inference
called margin-based amortization. This algorithm
is on an examination of the structured margin of
a prediction. For a new inference problem, if this
margin is larger than the sum of the decrease in the
score of the previous prediction and any increase
in the score of the second best one, then the previ-
ous solution will be the highest scoring one for the
new problem. We formalize this intuition to derive
an algorithm that finds provably optimal solutions
and show that this approach is a generalization of
previously identified schemes (based on Theorem
1 of (Srikumar et al, 2012)).
Second, we argue that the idea of amortization
is best exploited at the level of parts of the struc-
tures rather than the entire structure because we
expect a much higher redundancy in the parts.
We introduce the notion of decomposed amor-
tized inference, whereby we can attain a significant
improvement in speedup by considering repeated
sub-structures across the dataset and applying any
amortized inference algorithm for the parts.
905
We evaluate the two schemes and their combi-
nation on two NLP tasks where the output is en-
coded as a structure: PropBank semantic role la-
beling (Punyakanok et al, 2008) and the problem
of recognizing entities and relations in text (Roth
and Yih, 2007; Kate and Mooney, 2010). In these
problems, the inference problem has been framed
as an integer linear program (ILP). We compare
our methods with previous amortized inference
methods and show that margin-based amortization
combined with decomposition significantly out-
performs existing methods.
2 Problem Definition and Notation
Structured output prediction encompasses a wide
variety of NLP problems like part-of-speech tag-
ging, parsing and machine translation. The lan-
guage of 0-1 integer linear programs (ILP) pro-
vides a convenient analytical tool for representing
structured prediction problems. The general set-
ting consists of binary inference variables each of
which is associated with a score. The goal of in-
ference is to find the highest scoring global assign-
ment of the variables from a feasible set of assign-
ments, which is defined by linear inequalities.
While efficient inference algorithms exist for
special families of structures (like linear chains
and trees), in the general case, inference can be
computationally intractable. One approach to deal
with the computational complexity of inference
is to use an off-the-shelf ILP solver for solv-
ing the inference problem. This approach has
seen increasing use in the NLP community over
the last several years (for example, (Roth and
Yih, 2004; Clarke and Lapata, 2006; Riedel and
Clarke, 2006) and many others). Other approaches
for solving inference include the use of cutting
plane inference (Riedel, 2009), dual decomposi-
tion (Koo et al, 2010; Rush et al, 2010) and
the related method of Lagrangian relaxation (Rush
and Collins, 2011; Chang and Collins, 2011).
(Srikumar et al, 2012) introduced the notion of
an amortized inference algorithm, defined as an
inference algorithm that can use previous predic-
tions to speed up inference time, thereby giving an
amortized gain in inference time over the lifetime
of the program.
The motivation for amortized inference comes
from the observation that though the number of
possible structures could be large, in practice, only
a small number of these are ever seen in real
data. Furthermore, among the observed structures,
a small subset typically occurs much more fre-
quently than the others. Figure 1 illustrates this
observation in the context of part-of-speech tag-
ging. If we can efficiently characterize and iden-
tify inference instances that have the same solu-
tion, we can take advantage of previously per-
formed computation without paying the high com-
putational cost of inference.
Figure 1: Comparison of number of instances and the num-
ber of unique observed part-of-speech structures in the Gi-
gaword corpus. Note that the number of observed structures
(blue solid line) is much lower than the number of sentences
(red dotted line) for all sentence lengths, with the difference
being very pronounced for shorter sentences. Embedded in
the graph are three histograms that show the distribution of
observed structures for sentences of length 15, 20 and 30. In
all cases, we see that a small number of tag sequences are
much more frequent than the others.
We denote inference problems by the bold-
faced letters p and q. For a problem p, the goal
of inference is to jointly assign values to the parts
of the structure, which are represented by a col-
lection of inference variables y ? {0, 1}n. For all
vectors, subscripts represent their ith component.
Each yi is associated with a real valued cp,i ? <
which is the score for the variable yi being as-
signed the value 1. We denote the vector com-
prising of all the cp,i as cp. The search space
for assignments is restricted via constraints, which
can be written as a collection of linear inequalities,
MTy ? b. For a problem p, we denote this fea-
sible set of structures by Kp.
The inference problem is that of finding the fea-
sible assignment to the structure which maximizes
the dot product cTy. Thus, the prediction problem
can be written as
arg max
y?Kp
cTy. (1)
906
We denote the solution of this maximization prob-
lem as yp.
Let the set P = {p1,p2, ? ? ? } denote previously
solved inference problems, along with their re-
spective solutions {y1p,y2p, ? ? ? }. An equivalence
class of integer linear programs, denoted by [P ],
consists of ILPs which have the same number of
inference variables and the same feasible set. Let
K[P ] denote the feasible set of an equivalence class
[P ]. For a program p, the notation p ? [P ] indi-
cates that it belongs to the equivalence class [P ].
(Srikumar et al, 2012) introduced a set of amor-
tized inference schemes, each of which provides a
condition for a new ILP to have the same solu-
tion as a previously seen problem. We will briefly
review one exact inference scheme introduced in
that work. Suppose q belongs to the same equiv-
alence class of ILPs as p. Then the solution to q
will be the same as that of p if the following con-
dition holds for all inference variables:
(2yp,i ? 1)(cq,i ? cp,i) ? 0. (2)
This condition, referred to as Theorem 1 in that
work, is the baseline for our experiments.
In general, for any amortization scheme
A, we can define two primitive operators
TESTCONDITIONA and SOLUTIONA. Given
a collection of previously solved prob-
lems P and a new inference problem q,
TESTCONDITIONA(P,q) checks if the solu-
tion of the new problem is the same as that
of some previously solved one and if so,
SOLUTIONA(P,q) returns the solution.
3 Margin-based Amortization
In this section, we will introduce a new method
for amortizing inference costs over time. The key
observation that leads to this theorem stems from
the structured margin ? for an inference problem
p ? [P ], which is defined as follows:
? = min
y?K[P ],y 6=yp
cTp(yp ? y). (3)
That is, for all feasible y, we have cTpyp ? cTpy+
?. The margin ? is the upper limit on the change in
objective that is allowed for the constraint setK[P ]
for which the solution will not change.
For a new inference problem q ? [P ], we define
? as the maximum change in objective value that
can be effected by an assignment that is not the
A B = yp
cp
cq?
?
decrease in
value of yp
inc
rea
sin
go
bje
cti
ve
cpTyp
Two assignments
Figure 2: An illustration of the margin-based amortization
scheme showing the very simple case with only two compet-
ing assignments A and B. Suppose B is the solution yp for
the inference problem p with coefficients cp, denoted by the
red hyperplane, and A is the second-best assignment. For a
new coefficient vector cq, if the margin ? is greater than the
sum of the decrease in the objective value of yp and the max-
imum increase in the objective of another solution (?), then
the solution to the new inference problem will still be yp. The
margin-based amortization theorem captures this intuition.
solution. That is,
? = max
y?K[P ],y 6=yp
(cq ? cp)T y (4)
Before stating the theorem, we will provide an in-
tuitive explanation for it. Moving from cp to cq,
consider the sum of the decrease in the value of
the objective for the solution yp and ?, the maxi-
mum change in objective value for an assignment
that is not the solution. If this sum is less than the
margin ?, then no other solution will have an ob-
jective value higher than yp. Figure 2 illustrates
this using a simple example where there are only
two competing solutions.
This intuition is captured by our main theorem
which provides a condition for problems p and q
to have the same solution yp.
Theorem 1 (Margin-based Amortization). Let p
denote an inference problem posed as an inte-
ger linear program belonging to an equivalence
class [P ] with optimal solution yp. Let p have
a structured margin ?, i.e., for any y, we have
cTpyp ? cTpy + ?. Let q ? [P ] be another infer-
ence instance in the same equivalence class and
let ? be defined as in Equation 4. Then, yp is the
solution of the problem q if the following holds:
?(cq ? cp)Typ + ? ? ? (5)
907
Proof. For some feasible y, we have
cTqyp ? cTqy ? cTqyp ? cTpy ??
? cTqyp ? cTpyp + ? ??
? 0
The first inequality comes from the definition of ?
in (4) and the second one follows from the defini-
tion of ?. The condition of the theorem in (5) gives
us the final step. For any feasible y, the objective
score assigned to yp is greater than the score as-
signed to y according to problem q. That is, yp is
the solution to the new problem.
The margin-based amortization theorem pro-
vides a general, new amortized inference algo-
rithm. Given a new inference problem, we check
whether the inequality (5) holds for any previously
seen problems in the same equivalence class. If so,
we return the cached solution. If no such problem
exists, then we make a call to an ILP solver.
Even though the theorem provides a condition
for two integer linear programs to have the same
solution, checking the validity of the condition re-
quires the computation of ?, which in itself is an-
other integer linear program. To get around this,
we observe that if any constraints in Equation 4
are relaxed, the value of the resulting maximum
can only increase. Even with the increased ?, if
the condition of the theorem holds, then the rest
of the proof follows and hence the new problem
will have the same solution. In other words, we
can solve relaxed, tractable variants of the maxi-
mization in Equation 4 and still retain the guaran-
tees provided by the theorem. The tradeoff is that,
by doing so, the condition of the theorem will ap-
ply to fewer examples than theoretically possible.
In our experiments, we will define the relaxation
for each problem individually and even with the
relaxations, the inference algorithm based on the
margin-based amortization theorem outperforms
all previous amortized inference algorithms.
The condition in inequality (5) is, in fact, a strict
generalization of the condition for Theorem 1 in
(Srikumar et al, 2012), stated in (2). If the latter
condition holds, then we can show that ? ? 0 and
(cq ? cp)Typ ? 0. Since ? is, by definition, non-
negative, the margin-based condition is satisfied.
4 Decomposed Amortized Inference
One limitation in previously considered ap-
proaches for amortized inference stems from the
expectation that the same full assignment maxi-
mizes the objective score for different inference
problems, or equivalently, that the entire structure
is repeated multiple times. Even with this assump-
tion, we observe a speedup in prediction.
However, intuitively, even if entire structures
are not repeated, we expect parts of the assign-
ment to be the same across different instances. In
this section, we address the following question:
Can we take advantage of the redundancy in com-
ponents of structures to extend amortization tech-
niques to cases where the full structured output is
not repeated? By doing so, we can store partial
computation for future inference problems.
For example, consider the task of part of speech
tagging. While the likelihood of two long sen-
tences having the same part of speech tag sequence
is not high, it is much more likely that shorter sec-
tions of the sentences will share the same tag se-
quence. We see from Figure 1 that the number of
possible structures for shorter sentences is much
smaller than the number of sentences. This im-
plies that many shorter sentences share the same
structure, thus improving the performance of an
amortized inference scheme for such inputs. The
goal of decomposed amortized inference is to ex-
tend this improvement to larger problems by in-
creasing the size of equivalence classes.
To decompose an inference problem, we use the
approach of Lagrangian Relaxation (Lemare?chal,
2001) that has been used successfully for various
NLP tasks (Chang and Collins, 2011; Rush and
Collins, 2011). We will briefly review the under-
lying idea1. The goal is to solve an integer linear
program q, which is defined as
q : max
MTy?b
cTqy
We partition the constraints into two sets, say C1
denoting M1Ty ? b1 and C2, denoting con-
straints M2Ty ? b2. The assumption is that in
the absence the constraints C2, the inference prob-
lem becomes computationally easier to solve. In
other words, we can assume the existence of a sub-
routine that can efficiently compute the solution of
the relaxed problem q?:
q? : max
M1Ty?b1
cTqy
1For simplicity, we only write inequality constraints in
the paper. However, all the results here are easily extensible
to equality constraints by removing the non-negativity con-
straints from the corresponding dual variables.
908
We define Lagrange multipliers ? ? 0, with one
?i for each constraint in C2. For problem q, we
can define the Lagrangian as
L(y,?) = cTqy ? ?T
(
M2Ty ? b2
)
Here, the domain of y is specified by the constraint
set C1. The dual objective is
L(?) = max
M1Ty?b1
cTqy ? ?T
(
M2Ty ? b2
)
= max
M1Ty?b1
(
cq ? ?TM2
)T y + ?Tb2.
Note that the maximization in the definition of the
dual objective has the same functional form as q?
and any approach to solve q? can be used here to
find the dual objective L(?). The dual of the prob-
lem q, given by min??0 L(?), can be solved us-
ing subgradient descent over the dual variables.
Relaxing the constraints C2 to define the prob-
lem q? has several effects. First, it can make the re-
sulting inference problem q? easier to solve. More
importantly, removing constraints can also lead to
the merging of multiple equivalence classes, lead-
ing to fewer, more populous equivalence classes.
Finally, removing constraints can decompose the
inference problem q? into smaller independent
sub-problems {q1,q2, ? ? ? } such that no constraint
that is inC1 has active variables from two different
sets in the partition.
For the sub-problem qi comprising of variables
yi, let the corresponding objective coefficients be
cqi and the corresponding sub-matrix of M2 be
Mi2. Now, we can define the dual-augmented sub-
problem as
max
Mi1
Ty?bi1
(
cqi ? ?TMi2
)T
yi (6)
Solving all such sub-problems will give us a com-
plete assignment for all the output variables.
We can now define the decomposed amortized
inference algorithm (Algorithm 1) that performs
sub-gradient descent over the dual variables. The
input to the algorithm is a collection of previ-
ously solved problems with their solutions, a new
inference problem q and an amortized inference
scheme A (such as the margin-based amortization
scheme). In addition, for the task at hand, we first
need to identify the set of constraints C2 that can
be introduced via the Lagrangian.
First, we check if the solution can be obtained
without decomposition (lines 1?2). Otherwise,
Algorithm 1 Decomposed Amortized Inference
Input: A collection of previously solved infer-
ence problems P , a new problem q, an amor-
tized inference algorithm A.
Output: The solution to problem q
1: if TESTCONDITION(A, q, P ) then
2: return SOLUTION(A, q, P )
3: else
4: Initialize ?i ? 0 for each constraint in C2.
5: for t = 1 ? ? ?T do
6: Partition the problem q into sub-
problems q1,q2, ? ? ? such that no con-
straint in C1 has active variables from
two partitions.
7: for partition qi do
8: yi ? Solve the maximization prob-
lem for qi (Eq. 6) using the amortized
scheme A.
9: end for
10: Let y? [y1;y2; ? ? ? ]
11: if M2y ? b2 and (b2 ?M2y)i?i = 0
then
12: return y
13: else
14: ??
[
?? ?t
(
b2 ?M2Ty
)]
+
15: end if
16: end for
17: return solution of q using a standard infer-
ence algorithm
18: end if
we initialize the dual variables ? and try to ob-
tain the solution iteratively. At the tth itera-
tion, we partition the problem q into sub-problems
{q1,q2, ? ? ? } as described earlier (line 6). Each
partition defines a smaller inference problem with
its own objective coefficients and constraints. We
can apply the amortization scheme A to each sub-
problem to obtain a complete solution for the re-
laxed problem (lines 7?10). If this solution satis-
fies the constraints C2 and complementary slack-
ness conditions, then the solution is provably the
maximum of the problem q. Otherwise, we take a
subgradient step to update the value of ? using a
step-size ?t, subject to the constraint that all dual
variables must be non-negative (line 14). If we do
not converge to a solution in T iterations, we call
the underlying solver on the full problem.
In line 8 of the algorithm, we make multiple
calls to the underlying amortized inference pro-
cedure to solve each sub-problem. If the sub-
909
problem cannot be solved using the procedure,
then we can either solve the sub-problem using a
different approach (effectively giving us the stan-
dard Lagrangian relaxation algorithm for infer-
ence), or we can treat the full instance as a cache
miss and make a call to an ILP solver. In our ex-
periments, we choose the latter strategy.
5 Experiments and Results
Our experiments show two results: 1. The margin-
based scheme outperforms the amortized infer-
ence approaches from (Srikumar et al, 2012).
2. Decomposed amortized inference gives further
gains in terms of re-using previous solutions.
5.1 Tasks
We report the performance of inference on two
NLP tasks: semantic role labeling and the task of
extracting entities and relations from text. In both
cases, we used an existing formulation for struc-
tured inference and only modified the inference
calls. We will briefly describe the problems and
the implementation and point the reader to the lit-
erature for further details.
Semantic Role Labeling (SRL) Our first task is
that of identifying arguments of verbs in a sen-
tence and annotating them with semantic roles
(Gildea and Jurafsky, 2002; Palmer et al, 2010)
. For example, in the sentence Mrs. Haag plays
Eltiani., the verb plays takes two arguments: Mrs.
Haag, the actor, labeled as A0 and Eltiani, the
role, labeled as A1. It has been shown in prior
work (Punyakanok et al, 2008; Toutanova et al,
2008) that making a globally coherent prediction
boosts performance of SRL.
In this work, we used the SRL system of (Pun-
yakanok et al, 2008), where one inference prob-
lem is generated for each verb and each infer-
ence variables encodes the decision that a given
constituent in the sentence takes a specific role.
The scores for the inference variables are obtained
from a classifier trained on the PropBank cor-
pus. Constraints encode structural and linguistic
knowledge about the problem. For details about
the formulations of the inference problem, please
see (Punyakanok et al, 2008).
Recall from Section 3 that we need to define a
relaxed version of the inference problem to effi-
ciently compute ? for the margin-based approach.
For a problem instance with coefficients cq and
cached coefficients cp, we take the sum of the
highest n values of cq ? cp as our ?, where n is
the number of argument candidates to be labeled.
To identify constraints that can be relaxed for
the decomposed algorithm, we observe that most
constraints are not predicate specific and apply for
all predicates. The only constraint that is predi-
cate specific requires that each predicate can only
accept roles from a list of roles that is defined for
that predicate. By relaxing this constraint in the
decomposed algorithm, we effectively merge all
the equivalence classes for all predicates with a
specific number of argument candidates.
Entity-Relation extraction Our second task is
that of identifying the types of entities in a sen-
tence and the relations among them, which has
been studied by (Roth and Yih, 2007; Kate and
Mooney, 2010) and others. For the sentence
Oswald killed Kennedy, the words Oswald and
Kennedy will be labeled by the type PERSON, and
the KILL relation exists between them.
We followed the experimental setup as de-
scribed in (Roth and Yih, 2007). We defined one
inference problem for each sentence. For every
entity (which is identified by a constituent in the
sentence), an inference variable is introduced for
each entity type. For each pair of constituents, an
inference variable is introduced for each relation
type. Clearly, the assignment of types to entities
and relations are not independent. For example, an
entity of type ORGANIZATION cannot participate
in a relation of type BORN-IN because this rela-
tion label can connect entities of type PERSON and
LOCATION only. Incorporating these natural con-
straints during inference were shown to improve
performance significantly in (Roth and Yih, 2007).
We trained independent classifiers for entities and
relations and framed the inference problem as in
(Roth and Yih, 2007). For further details, we refer
the reader to that paper.
To compute the value of ? for the margin-based
algorithm, for a new instance with coefficients cq
and cached coefficients cp, we define ? to be the
sum of all non-negative values of cq ? cp.
For the decomposed inference algorithm, if the
number of entities is less than 5, no decomposi-
tion is performed. Otherwise, the entities are par-
titioned into two sets: set A includes the first four
entities and set B includes the rest of the entities.
We relaxed the relation constraints that go across
these two sets of entities to obtain two independent
inference problems.
910
5.2 Experimental Setup
We follow the experimental setup of (Srikumar et
al., 2012) and simulate a long-running NLP pro-
cess by caching problems and solutions from the
Gigaword corpus. We used a database engine to
cache ILP and their solutions along with identi-
fiers for the equivalence class and the value of ?.
For the margin-based algorithm and the Theo-
rem 1 from (Srikumar et al, 2012), for a new in-
ference problem p ? [P ], we retrieve all infer-
ence problems from the database that belong to
the same equivalence class [P ] as the test prob-
lem p and find the cached assignment y that has
the highest score according to the coefficients of
p. We only consider cached ILPs whose solution
is y for checking the conditions of the theorem.
This optimization ensures that we only process a
small number of cached coefficient vectors.
In a second efficiency optimization, we pruned
the database to remove redundant inference prob-
lems. A problem is redundant if solution to that
problem can be inferred from the other problems
stored in the database that have the same solution
and belong to the same equivalence class. How-
ever, this pruning can be computationally expen-
sive if the number of problems with the same so-
lution and the same equivalence class is very large.
In that case, we first sampled a 5000 problems ran-
domly and selected the non-redundant problems
from this set to keep in the database.
5.3 Results
We compare our approach to a state-of-the-art ILP
solver2 and also to Theorem 1 from (Srikumar
et al, 2012). We choose this baseline because
it is shown to give the highest improvement in
wall-clock time and also in terms of the num-
ber of cache hits. However, we note that the re-
sults presented in our work outperform all the pre-
vious amortization algorithms, including the ap-
proximate inference methods.
We report two performance metrics ? the per-
centage decrease in the number of ILP calls, and
the percentage decrease in the wall-clock infer-
ence time. These are comparable to the speedup
and clock speedup defined in (Srikumar et al,
2012). For measuring time, since other aspects
of prediction (like feature extraction) are the same
across all settings, we only measure the time taken
for inference and ignore other aspects. For both
2We used the Gurobi optimizer for our experiments.
tasks, we report the runtime performance on sec-
tion 23 of the Penn Treebank. Note that our amor-
tization schemes guarantee optimal solution. Con-
sequently, using amortization, task accuracy re-
mains the same as using the original solver.
Table 1 shows the percentage reduction in the
number of calls to the ILP solver. Note that for
both the SRL and entity-relation problems, the
margin-based approach, even without using de-
composition (the columns labeled Original), out-
performs the previous work. Applying the de-
composed inference algorithm improves both the
baseline and the margin-based approach. Overall,
however, the fewest number of calls to the solver is
made when combining the decomposed inference
algorithm with the margin-based scheme. For the
semantic role labeling task, we need to call the
solver only for one in six examples while for the
entity-relations task, only one in four examples re-
quire a solver call.
Table 2 shows the corresponding reduction in
the wall-clock time for the various settings. We
see that once again, the margin based approach
outperforms the baseline. While the decomposed
inference algorithm improves running time for
SRL, it leads to a slight increase for the entity-
relation problem. Since this increase occurs in
spite of a reduction in the number of solver calls,
we believe that this aspect can be further improved
with an efficient implementation of the decom-
posed inference algorithm.
6 Discussion
Lagrangian Relaxation in the literature In the
literature, in applications of the Lagrangian relax-
ation technique (such as (Rush and Collins, 2011;
Chang and Collins, 2011; Reichart and Barzilay,
2012) and others), the relaxed problems are solved
using specialized algorithms. However, in both the
relaxations considered in this paper, even the re-
laxed problems cannot be solved without an ILP
solver, and yet we can see improvements from de-
composition in Table 1.
To study the impact of amortization on running
time, we modified our decomposition based infer-
ence algorithm to solve each sub-problem using
the ILP solver instead of amortization. In these ex-
periments, we ran Lagrangian relaxation for until
convergence or at most T iterations. After T itera-
tions, we call the ILP solver and solve the original
problem. We set T to 100 in one set of exper-
911
% ILP Solver calls required
Method Semantic Role Labeling Entity-Relation Extraction
Original + Decomp. Original + Decomp.
ILP Solver 100 ? 100 ?
(Srikumar et al, 2012) 41 24.4 59.5 57.0
Margin-based 32.7 16.6 28.2 25.4
Table 1: Reduction in number of inference calls
% time required compared to ILP Solver
Method Semantic Role Labeling Entity-Relation Extraction
Original + Decomp. Original + Decomp.
ILP Solver 100 ? 100 ?
(Srikumar et al, 2012) 54.8 40.0 81 86
Margin-based 45.9 38.1 58.1 61.3
Table 2: Reduction in inference time
iments (call it Lag1) and T to 1 (call it Lag2).
In SRL, compared to solving the original problem
with ILP Solver, both Lag1 and Lag2 are roughly
2 times slower. For entity relation task, compared
to ILP Solver, Lag1 is 186 times slower and Lag2
is 1.91 times slower. Since we used the same im-
plementation of the decomposition in all experi-
ments, this shows that the decomposed inference
algorithm crucially benefits from the underlying
amortization scheme.
Decomposed amortized inference The decom-
posed amortized inference algorithm helps im-
prove amortized inference in two ways. First,
since the number of structures is a function of its
size, considering smaller sub-structures will allow
us to cache inference problems that cover a larger
subset of the space of possible sub-structures. We
observed this effect in the problem of extracting
entities and relations in text. Second, removing a
constraint need not always partition the structure
into a set of smaller structures. Instead, by re-
moving the constraint, examples that might have
otherwise been in different equivalence classes be-
come part of a combined, larger equivalence class.
Increasing the size of the equivalence classes in-
creases the probability of a cache-hit. In our ex-
periments, we observed this effect in the SRL task.
7 Conclusion
Amortized inference takes advantage of the reg-
ularities in structured output to re-use previous
computation and improve running time over the
lifetime of a structured output predictor. In this pa-
per, we have described two approaches for amor-
tizing inference costs over datasets. The first,
called the margin-based amortized inference, is a
new, provably exact inference algorithm that uses
the notion of a structured margin to identify previ-
ously solved problems whose solutions can be re-
used. The second, called decomposed amortized
inference, is a meta-algorithm over any amortized
inference that takes advantage of previously com-
puted sub-structures to provide further reductions
in the number of inference calls. We show via ex-
periments that these methods individually give a
reduction in the number of calls made to an infer-
ence engine for semantic role labeling and entity-
relation extraction. Furthermore, these approaches
complement each other and, together give an addi-
tional significant improvement.
Acknowledgments
The authors thank the members of the Cognitive Computa-
tion Group at the University of Illinois for insightful discus-
sions and the anonymous reviewers for valuable feedback.
This research is sponsored by the Army Research Laboratory
(ARL) under agreement W911NF-09-2-0053. The authors
also gratefully acknowledge the support of the Defense Ad-
vanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. This material also
is based on research sponsored by DARPA under agreement
number FA8750-13-2-0008. This work has also been sup-
ported by the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Department of Interior National Business
Center contract number D11PC20155. The U.S. Govern-
ment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright an-
notation thereon. Any opinions, findings, and conclusions
or recommendations expressed in this material are those of
the author(s) and do not necessarily reflect the view of ARL,
DARPA, AFRL, IARPA, DoI/NBC or the US government.
912
References
Y-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through Lagrangian
relaxation. EMNLP.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics.
R. Kate and R. Mooney. 2010. Joint entity and relation
extraction using card-pyramid parsing. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 203?212. Asso-
ciation for Computational Linguistics.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and
D. Sontag. 2010. Dual decomposition for parsing
with non-projective head automata. In EMNLP.
C. Lemare?chal. 2001. Lagrangian Relaxation. In
Computational Combinatorial Optimization, pages
112?156.
M. Palmer, D. Gildea, and N. Xue. 2010. Semantic
Role Labeling, volume 3. Morgan & Claypool Pub-
lishers.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics.
R. Reichart and R. Barzilay. 2012. Multi event extrac-
tion guided by global constraints. In NAACL, pages
70?79.
S. Riedel and J. Clarke. 2006. Incremental integer
linear programming for non-projective dependency
parsing. In EMNLP.
S. Riedel. 2009. Cutting plane MAP inference for
Markov logic. Machine Learning.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Hwee Tou Ng and Ellen Riloff, editors,
CoNLL.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
A.M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through Lagrangian re-
laxation. In ACL, pages 72?82, Portland, Oregon,
USA, June.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
V. Srikumar, G. Kundu, and D. Roth. 2012. On amor-
tizing inference cost for structured prediction. In
EMNLP.
K. Toutanova, A. Haghighi, and C. D. Manning. 2008.
A global joint model for semantic role labeling.
Computational Linguistics, 34:161?191.
913
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 462?466,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Leveraging Domain-Independent Information in Semantic Parsing
Dan Goldwasser
University of Maryland
College Park, MD 20740
goldwas1@umiacs.umd.edu
Dan Roth
University of Illinois
Urbana, IL 61801
danr@illinois.edu
Abstract
Semantic parsing is a domain-dependent
process by nature, as its output is defined
over a set of domain symbols. Motivated
by the observation that interpretation can
be decomposed into domain-dependent
and independent components, we suggest
a novel interpretation model, which aug-
ments a domain dependent model with ab-
stract information that can be shared by
multiple domains. Our experiments show
that this type of information is useful and
can reduce the annotation effort signifi-
cantly when moving between domains.
1 Introduction
Natural Language (NL) understanding can be intu-
itively understood as a general capacity, mapping
words to entities and their relationships. However,
current work on automated NL understanding
(typically referenced as semantic parsing (Zettle-
moyer and Collins, 2005; Wong and Mooney,
2007; Chen and Mooney, 2008; Kwiatkowski et
al., 2010; Bo?rschinger et al, 2011)) is restricted
to a given output domain1 (or task) consisting of a
closed set of meaning representation symbols, de-
scribing domains such as robotic soccer, database
queries and flight ordering systems.
In this work, we take a first step towards con-
structing a semantic interpreter that can leverage
information from multiple tasks. This is not a
straight forward objective ? the domain specific
nature of semantic interpretation, as described in
the current literature, does not allow for an easy
move between domains. For example, a sys-
tem trained for the task of understanding database
queries will not be of any use when it will be given
a sentence describing robotic soccer instructions.
In order to understand this difficulty, a closer
look at semantic parsing is required. Given a sen-
tence, the interpretation process breaks it into a
1The term domain is overloaded in NLP; in this work we
use it to refer to the set of output symbols.
set of interdependent decisions, which rely on an
underlying representation mapping words to sym-
bols and syntactic patterns into compositional de-
cisions. This representation takes into account do-
main specific information (e.g., a lexicon mapping
phrases to a domain predicate) and is therefore of
little use when moving to a different domain.
In this work, we attempt to develop a domain in-
dependent approach to semantic parsing. We do it
by developing a layer of representation that is ap-
plicable to multiple domains. Specifically, we add
an intermediate layer capturing shallow semantic
relations between the input sentence constituents.
Unlike semantic parsing which maps the input to
a closed set of symbols, this layer can be used to
identify general predicate-argument structures in
the input sentence.The following example demon-
strates the key idea behind our representation ?
two sentences from two different domains have a
similar intermediate structure.
Example 1. Domains with similar intermediate structures
? The [Pink goalie]ARG [kicks]PRED to [Pink11]ARG
pass(pink1, pink11)
? [She]ARG [walks]PRED to the [kitchen]ARG
go(sister, kitchen)
In this case, the constituents of the first
sentence (from the Robocup domain (Chen
and Mooney, 2008)), are assigned domain-
independent predicate-argument labels (e.g., the
word corresponding to a logical function is identi-
fied as a PRED). Note that it does not use any do-
main specific information, for example, the PRED
label assigned to the word ?kicks? indicates that
this word is the predicate of the sentence, not a
specific domain predicate (e.g., pass(?)). The in-
termediate layer can be reused across domains.
The logical output associated with the second sen-
tence is taken from a different domain, using a dif-
ferent set of output symbols, however it shares the
same predicate-argument structure.
Despite the idealized example, in practice,
462
leveraging this information is challenging, as the
logical structure is assumed to only weakly corre-
spond to the domain-independent structure, a cor-
respondence which may change in different do-
mains. The mismatch between the domain in-
dependent (linguistic) structure and logical struc-
tures typically stems from technical considera-
tions, as the domain logical language is designed
according to an application-specific logic and not
according to linguistic considerations. This situa-
tion is depicted in the following example, in which
one of the domain-independent labels is omitted.
? The [Pink goalie]ARG [kicks]PRED the [ball]ARG to [Pink11]ARG
pass(pink1, pink11)
In order to overcome this difficulty, we suggest
a flexible model that is able to leverage the super-
vision provided in one domain to learn an abstract
intermediate layer, and show empirically that it
learns a robust model, improving results signifi-
cantly in a second domain.
2 Semantic Interpretation Model
Our model consists of both domain-dependent
(mapping between text and a closed set of sym-
bols) and domain independent (abstract predicate-
argument structures) information. We formulate
the joint interpretation process as a structured pre-
diction problem, mapping a NL input sentence (x),
to its highest ranking interpretation and abstract
structure (y). The decision is quantified using a
linear objective, which uses a vector w, mapping
features to weights and a feature function ? which
maps the output decision to a feature vector. The
output interpretation y is described using a sub-
set of first order logic, consisting of typed con-
stants (e.g., robotic soccer player), functions cap-
turing relations between entities, and their prop-
erties (e.g., pass(x, y), where pass is a function
symbol and x, y are typed arguments). We use
data taken from two grounded domains, describing
robotic soccer events and household situations.
We begin by formulating the domain-specific
process. We follow (Goldwasser et al, 2011;
Clarke et al, 2010) and formalize semantic infer-
ence as an Integer Linear Program (ILP). Due to
space consideration, we provide a brief descrip-
tion (see (Clarke et al, 2010) for more details).
We then proceed to augment this model with
domain-independent information, and connect the
two models by constraining the ILP model.
2.1 Domain-Dependent Model
Interpretation is composed of several decisions,
capturing mapping of input tokens to logical frag-
ments (first order) and their composition into
larger fragments (second). We encode a first-order
decision as ?cs, a binary variable indicating that
constituent c is aligned with the logical symbol s.
A second-order decision ?cs,dt, is encoded as a bi-
nary variable indicating that the symbol t (associ-
ated with constituent d) is an argument of a func-
tion s (associated with constituent c). The overall
inference problem (Eq. 1) is as follows:
Fw(x) = arg max?,?
?
c?x
?
s?D ?cs ?wT?1(x, c, s)
+
?
c,d?x
?
s,t?D ?cs,dt ?wT?2(x, c, s, d, t) (1)
We restrict the possible assignments to the deci-
sion variables, forcing the resulting output formula
to be syntactically legal, for example by restrict-
ing active ?-variables to be type consistent, and
forcing the resulting functional composition to be
acyclic and fully connected (we refer the reader to
(Clarke et al, 2010) for more details). We take ad-
vantage of the flexible ILP framework and encode
these restrictions as global constraints.
Features We use two types of feature, first-order
?1 and second-order ?2. ?1 depends on lexical
information: each mapping of a lexical item c to a
domain symbol s generates a feature. In addition
each combination of a lexical item c and an sym-
bol type generates a feature.
?2 captures a pair of symbols and their alignment
to lexical items. Given a second-order decision
?cs,dt, a feature is generated considering the nor-
malized distance between the head words in the
constituents c and d. Another feature is gener-
ated for every composition of symbols (ignoring
the alignment to the text).
2.2 Domain-Independent Information
We enhance the decision process with informa-
tion that abstracts over the attributes of specific
domains by adding an intermediate layer consist-
ing of the predicate-argument structure of the sen-
tence. Consider the mappings described in Exam-
ple 1. Instead of relying on the mapping between
Pink goalie and pink1, this model tries to iden-
tify an ARG using different means. For example, the
fact that it is preceded by a determiner, or capital-
ized provide useful cues. We do not assume any
language specific knowledge and use features that
help capture these cues.
463
This information is used to assist the overall
learning process. We assume that these labels cor-
respond to a binding to some logical symbol, and
encode it as a constraint forcing the relations be-
tween the two models. Moreover, since learning
this layer is a by-product of the learning process
(as it does not use any labeled data) forcing the
connection between the decisions is the mecha-
nism that drives learning this model.
Our domain-independent layer bears some
similarity to other semantic tasks, most no-
tably Semantic-Role Labeling (SRL) introduced
in (Gildea and Jurafsky, 2002), in which identi-
fying the predicate-argument structure is consid-
ered a preprocessing step, prior to assigning ar-
gument labels. Unlike SRL, which aims to iden-
tify linguistic structures alone, in our framework
these structures capture both natural-language and
domain-language considerations.
Domain-Independent Decision Variables We
add two new types of decisions abstracting over
the domain-specific decisions. We encode the new
decisions as ?c and ?cd. The first (?) captures local
information helping to determine if a given con-
stituent c is likely to have a label (i.e., ?Pc for pred-
icate or ?Ac for argument). The second (?) consid-
ers higher level structures, quantifying decisions
over both the labels of the constituents c,d as a
predicate-argument pair. Note, a given word c can
be labeled as PRED or ARG if ?c and ?cd are active.
Model?s Features We use the following fea-
tures: (1) Local Decisions ?3(?(c)) use a feature
indicating if c is capitalized, a set of features cap-
turing the context of c (window of size 2), such
as determiner and quantifier occurrences. Finally
we use a set of features capturing the suffix letters
of c, these features are useful in identifying verb
patterns. Features indicate if c is mapped to an ARG
or PRED. (2) Global Decision ?4(?(c, d)): a feature
indicating the relative location of c compared to d
in the input sentence. Additional features indicate
properties of the relative location, such as if the
word appears initially or finally in the sentence.
Combined Model In order to consider both
types of information we augment our decision
model with the new variables, resulting in the fol-
lowing objective function (Eq. 2).
Fw(x) = arg max?,?
?
c?x
?
s?D ?cs?w1T?1(x, c, s)+
?
c,d?x
?
s,t?D
?
i,j ?csi,dtj ? w2T?2(x, c, si, d, tj) +?
c?x ?c ?w3T?3(x, c)+
?
c,d?x ?cd ?w4T?4(x, c, d) (2)
For notational convenience we decompose the
weight vector w into four parts, w1,w2 for fea-
tures of (first, second) order domain-dependent de-
cisions, and similarly for the independent ones.
In addition, we also add new constraints tying
these new variables to semantic interpretation :
?c ? x (?c ? ?c,s1 ? ?c,s2 ? ... ? ?c,sn)
?c ? x, ?d ? x (?c,d ? ?c,s1,dt1??c,s2,dt1?...??c,sn,dtn)
(where n is the length of x).
2.3 Learning the Combined Model
The supervision to the learning process is given
via data consisting of pairs of sentences and (do-
main specific) semantic interpretation. Given that
we have introduced additional variables that cap-
ture the more abstract predicate-argument struc-
ture of the text, we need to induce these as la-
tent variables. Our decision model maps an input
sentence x, into a logical output y and predicate-
argument structure h. We are only supplied with
training data pertaining to the input (x) and out-
put (y). We use a variant of the latent structure
perceptron to learn in these settings2.
3 Experimental Settings
Situated Language This dataset, introduced in
(Bordes et al, 2010), describes situations in a sim-
ulated world. The dataset consists of triplets of the
form - (x,u, y), where x is a NL sentence describ-
ing a situation (e.g., ?He goes to the kitchen?), u
is a world state consisting of grounded relations
(e.g., loc(John, Kitchen)) description, and y is
a logical interpretation corresponding to x.
The original dataset was used for concept tag-
ging, which does not include a compositional as-
pect. We automatically generated the full logical
structure by mapping the constants to function ar-
guments. We generated additional function sym-
bols of the same relation, but of different arity
when needed 3. Our new dataset consists of 25 re-
lation symbols (originally 15). In our experiments
we used a set of 5000 of the training triplets.
Robocup The Robocup dataset, originally in-
troduced in (Chen and Mooney, 2008), describes
robotic soccer events. The dataset was collected
for the purpose of constructing semantic parsers
from ambiguous supervision and consists of both
?noisy? and gold labeled data. The noisy dataset
2Details omitted, see (Chang et al, 2010) for more details.
3For example, a unary relation symbol for ?He plays?,
and a binary for ?He plays with a ball?.
464
System Training Procedure
DOM-INIT w1: Noisy probabilistic model, described below.
PRED-ARGS Onlyw3,w4 Trained over the Situ. dataset.
COMBINEDRL w1,w2,w3,w4:learned from Robocup gold
COMBINEDRI+S w3,w4: learned from the Situ. dataset,
w1 uses the DOM-INIT Robocup model.
COMBINEDRL+S w3,w4: Initially learned over the Situ. dataset,
updated jointly with w1,w2 over Robocup gold
Table 1: Evaluated System descriptions.
was constructed by temporally aligning a stream
of soccer events occurring during a robotic soc-
cer match with human commentary describing the
game. This dataset consists of pairs (x, {y0, yk}),
x is a sentence and {y0, yk} is a set of events (log-
ical formulas). One of these events is assumed to
correspond to the comment, however this is not
guaranteed. The gold labeled labeled data con-
sists of pairs (x, y). The data was collected from
four Robocup games. In our experiments we fol-
low other works and use 4-fold cross validation,
training over 3 games and testing over the remain-
ing game. We evaluate the Accuracy of the parser
over the test game data.4 Due to space consider-
ations, we refer the reader to (Chen and Mooney,
2008) for further details about this dataset.
Semantic Interpretation Tasks We consider
two of the tasks described in (Chen and Mooney,
2008) (1) Semantic Parsing requires generating
the correct logical form given an input sentence.
(2) Matching, given a NL sentence and a set of
several possible interpretation candidates, the sys-
tem is required to identify the correct one. In all
systems, the source for domain-independent infor-
mation is the Situated domain, and the results are
evaluated over the Robocup domain.
Experimental Systems We tested several vari-
ations, all solving Eq. 2, however different re-
sources were used to obtain Eq. 2 parameters (see
sec. 2.2). Tab. 1 describes the different varia-
tions. We used the noisy Robocup dataset to ini-
tialize DOM-INIT, a noisy probabilistic model, con-
structed by taking statistics over the noisy robocup
data and computing p(y|x). Given the training set
{(x, {y1, .., yk})}, every word in x is aligned to
every symbol in every y that is aligned with it. The
probability of a matching (x, y)is computed as the
product: ?ni=1 p(yi|xi), where n is the number
of symbols appearing in y, and xi, yi is the word
4In our model accuracy is equivalent to F-measure.
System Matching Parsing
PRED-ARGS 0.692 ?
DOM-INIT 0.823 0.357
COMBINEDRI+S 0.905 0.627
(BO?RSCHINGER ET AL., 2011) ? 0.86
(KIM AND MOONEY, 2010) 0.885 0.742
Table 2: Results for the matching and parsing tasks. Our
system performs well on the matching task without any do-
main information. Results for both parsing and matching
tasks show that using domain-independent information im-
proves results dramatically.
level matching to a logical symbol. Note that this
model uses lexical information only.
4 Knowledge Transfer Experiments
We begin by studying the role of domain-
independent information when very little domain
information is available. Domain-independent in-
formation is learned from the situated domain
and domain-specific information (Robocup) avail-
able is the simple probabilistic model (DOM-INIT).
This model can be considered as a noisy proba-
bilistic lexicon, without any domain-specific com-
positional information, which is only available
through domain-independent information.
The results, summarized in Table 2, show that
in both tasks domain-independent information is
extremely useful and can make up for missing do-
main information. Most notably, performance for
the matching task using only domain independent
information (PRED-ARGS) was surprisingly good,
with an accuracy of 0.69. Adding domain-specific
lexical information (COMBINEDRI+S) pushes this
result to over 0.9, currently the highest for this task
? achieved without domain specific learning.
The second set of experiments study whether
using domain independent information, when rel-
evant (gold) domain-specific training data is avail-
able, improves learning. In this scenario, the
domain-independent model is updated according
to training data available for the Robocup domain.
We compare two system over varying amounts
of training data (25, 50, 200 training samples
and the full set of 3 Robocup games), one boot-
strapped using the Situ. domain (COMBINEDRL+S)
and one relying on the Robocup training data
alone (COMBINEDRL). The results, summarized in
table 3, consistently show that transferring domain
independent information is helpful, and helps push
the learned models beyond the supervision offered
by the relevant domain training data. Our final
system, trained over the entire dataset achieves a
465
System # training Parsing
COMBINEDRL+S (COMBINEDRL) 25 0.16 (0.03)
COMBINEDRL+S (COMBINEDRL) 50 0.323 (0.16)
COMBINEDRL+S (COMBINEDRL) 200 0.385 (0.36)
COMBINEDRL+S (COMBINEDRL) full game 0.86 (0.79)
(CHEN ET AL., 2010) full game 0.81
Table 3: Evaluating our model in a learning settings. The
domain-independent information is used to bootstrap learn-
ing from the Robocup domain. Results show that this infor-
mation improves performance significantly, especially when
little data is available
score of 0.86, significantly outperforming (Chen
et al, 2010), a competing supervised model. It
achieves similar results to (Bo?rschinger et al,
2011), the current state-of-the-art for the pars-
ing task over this dataset. The system used in
(Bo?rschinger et al, 2011) learns from ambigu-
ous training data and achieves this score by using
global information. We hypothesize that it can be
used by our model and leave it for future work.
5 Conclusions
In this paper, we took a first step towards a new
kind of generalization in semantic parsing: con-
structing a model that is able to generalize to a
new domain defined over a different set of sym-
bols. Our approach adds an additional hidden
layer to the semantic interpretation process, cap-
turing shallow but domain-independent semantic
information, which can be shared by different do-
mains. Our experiments consistently show that
domain-independent knowledge can be transferred
between domains. We describe two settings; in
the first, where only noisy lexical-level domain-
specific information is available, we observe that
the model learned in the other domain can be used
to make up for the missing compositional infor-
mation. For example, in the matching task, even
when no domain information is available, iden-
tifying the abstract predicate argument structure
provides sufficient discriminatory power to iden-
tify the correct event in over 69% of the times.
In the second setting domain-specific examples
are available. The learning process can still utilize
the transferred knowledge, as it provides scaffold-
ing for the latent learning process, resulting in a
significant improvement in performance.
6 Acknowledgement
The authors would like to thank Julia Hockenmaier, Gerald
DeJong, Raymond Mooney and the anonymous reviewers for
their efforts and insightful comments.
Most of this work was done while the first author was
at the University of Illinois. The authors gratefully ac-
knowledge the support of the Defense Advanced Research
Projects Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. In addition, this material is based
on research sponsored by DARPA under agreement number
FA8750-13-2-0008. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon. The views
and conclusions contained herein are those of the authors and
should not be interpreted as necessarily representing the offi-
cial policies or endorsements, either expressed or implied, of
DARPA,AFRL, or the U.S. Government.
References
A. Bordes, N. Usunier, R. Collobert, and J. Weston.
2010. Towards understanding situated natural lan-
guage. In AISTATS.
B. Bo?rschinger, B. K. Jones, and M. Johnson. 2011.
Reducing grounded learning tasks to grammatical
inference. In EMNLP.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained la-
tent representations. In NAACL.
D. Chen and R. Mooney. 2008. Learning to sportscast:
a test of grounded language acquisition. In ICML.
D. L. Chen, J. Kim, and R. J. Mooney. 2010. Training
a multilingual sportscaster: Using perceptual con-
text to learn language. Journal of Artificial Intelli-
gence Research, 37:397?435.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s
response. In CoNLL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics.
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic
parsing. In ACL.
J. Kim and R. J. Mooney. 2010. Generative alignment
and semantic parsing for learning from ambiguous
supervision. In COLING.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, , and
M. Steedman. 2010. Inducing probabilistic ccg
grammars from logical form with higher-order uni-
fication. In EMNLP.
Y.W. Wong and R. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In ACL.
L. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In UAI.
466
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, page 7,
Baltimore, Maryland, USA, 22 June 2014. c?2014 Association for Computational Linguistics
Wikification and Beyond:  
The Challenges of Entity and Concept Grounding 
Dan Roth Heng Ji 
University of Illinois at Urbana-Champaign Rensselaer Polytechnic Institute 
danr@illinois.edu jih@rpi.edu 
  
Ming-Wei Chang Taylor Cassidy 
Microsoft Research Army Research Lab & IBM Research 
minchang@microsoft.com taylor.cassidy.ctr@mail.mil 
 
 
 
  
1 Introduction 
Contextual disambiguation and grounding of 
concepts and entities in natural language are es-
sential to progress in many natural language un-
derstanding tasks and fundamental to many ap-
plications. Wikification aims at automatically 
identifying concept mentions in text and linking 
them to referents in a knowledge base (KB) (e.g., 
Wikipedia). Consider the sentence, "The Times 
report on Blumenthal (D) has the potential to 
fundamentally reshape the contest in the Nutmeg 
State.". A Wikifier should identify the key enti-
ties and concepts and map them to an encyclope-
dic resource (e.g., ?D? refers to Democratic Par-
ty, and ?the Nutmeg State? refers to Connecticut.  
   Wikification benefits end-users and Natural 
Language Processing (NLP) systems. Readers 
can better comprehend Wikified documents as 
information about related topics is readily acces-
sible. For systems, a Wikified document eluci-
dates concepts and entities by grounding them in 
an encyclopedic resource or an ontology. Wikifi-
cation output has improved NLP down-stream 
tasks, including coreference resolution, user in-
terest discovery , recommendation and search. 
  This task has received increased attention in 
recent years from the NLP and Data Mining 
communities, partly fostered by the U.S. NIST 
Text Analysis Conference Knowledge Base Pop-
ulation (KBP) track, and several versions of it 
has been studied. These include Wikifying all 
concept mentions in a single text document; 
Wikifying a cluster of co-referential named enti-
ty mentions that appear across documents (Entity 
Linking), and Wikifying a whole document to a 
single concept. Other works relate this task to 
coreference resolution within and across docu-
ments and in the context of multiple text genres. 
2 Content Overview 
This tutorial will motivate Wikification as a 
broad paradigm for cross-source linking for 
knowledge enrichment. We will discuss multiple 
dimensions of the task definition, present the 
building blocks of a state-of-the-art Wikifier, 
share key lessons learned from analysis of re-
sults, and discuss recently proposed ideas for 
advancing work in this area in response to key 
challenges. We will touch on new research areas 
including interactive Wikification, social media, 
and censorship. The tutorial will be useful for all 
those with interests in cross-source information 
extraction and linking, knowledge acquisition, 
and the use of acquired knowledge in NLP. We 
will provide a concise roadmap of recent per-
spectives and results, and point to some of our 
available Wikification resources.  
3 Outline 
? Introduction and Motivation 
? Methodological presentation of a skeletal Wik-
ification system 
o Mention and candidate identification 
o Knowledge representation  
o Local and global context analysis 
o Role of Machine Learning 
? Obstacles & Advanced Methods 
o Joint modeling 
o Collective inference 
o Scarcity of supervision signals 
o Diverse text genres and social media 
? Remaining Challenges and Future Work 
o Rich semantic knowledge acquisition 
o Cross-lingual Wikification 
References 
http://nlp.cs.rpi.edu/kbp/2014/elreading.html 7
Transactions of the Association for Computational Linguistics, 1 (2013) 231?242. Action Editor: Noah Smith.
Submitted 11/2012; Revised 2/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Modeling Semantic Relations Expressed by Prepositions
Vivek Srikumar and Dan Roth
University of Illinois, Urbana-Champaign
Urbana, IL. 61801.
{vsrikum2, danr}@illinois.edu
Abstract
This paper introduces the problem of predict-
ing semantic relations expressed by preposi-
tions and develops statistical learning models
for predicting the relations, their arguments
and the semantic types of the arguments. We
define an inventory of 32 relations, build-
ing on the word sense disambiguation task
for prepositions and collapsing related senses
across prepositions. Given a preposition in
a sentence, our computational task to jointly
model the preposition relation and its argu-
ments along with their semantic types, as a
way to support the relation prediction. The an-
notated data, however, only provides labels for
the relation label, and not the arguments and
types. We address this by presenting two mod-
els for preposition relation labeling. Our gen-
eralization of latent structure SVM gives close
to 90% accuracy on relation labeling. Further,
by jointly predicting the relation, arguments,
and their types along with preposition sense,
we show that we can not only improve the re-
lation accuracy, but also significantly improve
sense prediction accuracy.
1 Introduction
This paper addresses the problem of predicting se-
mantic relations conveyed by prepositions in text.
Prepositions express many semantic relations be-
tween their governor and object. Predicting these
can help advancing text understanding tasks like
question answering and textual entailment. Consider
the sentence:
(1) The book of Prof. Alexander on primary school
methods is a valuable teaching resource.
Here, the preposition on indicates that the book
and primary school methods are connected by the
relation Topic and of indicates the Creator-
Creation relation between Prof. Alexander and
the book. Predicting these relations can help answer
questions about the subject of the book and also rec-
ognize the entailment of sentences like Prof. Alexan-
der has written about primary school methods.
Being highly polysemous, the same preposition
can indicate different kinds of relations, depending
on its governor and object. Furthermore, several
prepositions can indicate the same semantic relation.
For example, consider the sentence:
(2) Poor care led to her death from pneumonia.
The preposition from in this sentence expresses the
relation Cause(death, pneumonia). In a differ-
ent context, it can denote other relations, as in the
phrases copied from the film (Source) and recog-
nized from the start (Temporal). On the other
hand, the relation Cause can be expressed by sev-
eral prepositions; for example, the following phrases
express a Cause relation: died of pneumonia and
tired after the surgery.
We characterize semantic relations expressed by
transitive prepositions and develop accurate models
for predicting the relations, identifying their argu-
ments and recognizing the semantic types of the ar-
guments. Building on the word sense disambigua-
tion task for prepositions, we collapse semantically
related senses across prepositions to derive our re-
lation inventory. These relations act as predicates
in a predicate-argument representation, where the
arguments are the governor and the object of the
231
preposition. While ascertaining the arguments is a
largely syntactic decision, we point out that syn-
tactic parsers do not always make this prediction
correctly. However, as illustrated in the examples
above, identifying the relation depends on the gov-
ernor and object of the preposition.
Given a sentence and a preposition, our goal is
to model the predicate (i.e. the preposition rela-
tion) and its arguments (i.e. the governor and ob-
ject). Very often, the relation label is not influenced
by the surface form of the arguments but rather by
their semantic types. In sentence (2) above, we
want the predicate to be Cause when the object of
the preposition is any illness. We thus suggest to
model the argument types along with the preposi-
tion relations and arguments, using different notions
of types. These three related aspects of the rela-
tion prediction task are further explained in Section
3 leading up to the problem definition.
Though we wish to predict relations, arguments
and types, there is no corpus which annotates all
three. The SemEval 2007 shared task of word sense
disambiguation for prepositions provides sense an-
notations for prepositions. We use this data to gen-
erate training and test corpora for the relation la-
bels. In Section 4, we present two models for the
prepositional relation identification problem. The
first model considers all possible argument candi-
dates from various sources along with all argument
types to predict the preposition relation label. The
second model treats the arguments and types as la-
tent variables during learning using a generalization
of the latent structural SVM of (Yu and Joachims,
2009). We show in Section 5 that this model not
only predicts the arguments and types, but also im-
proves relation prediction performance.
The primary contributions of this paper are:
1. We introduce a new inventory of preposition
relations that covers the 34 prepositions that
formed the basis of the SemEval 2007 task of
preposition sense disambiguation.
2. We model preposition relations, arguments and
their types jointly and propose a learning algo-
rithm that learns to predict all three using train-
ing data that annotates only relation labels.
3. We show that jointly predicting relations with
word sense not only improves the relation pre-
dictor, but also gives a significant improvement
in sense prediction.
2 Prepositions & Predicate-Argument
Semantics
Semantic role labeling (cf. (Gildea and Jurafsky,
2002; Palmer et al, 2010; Punyakanok et al, 2008)
and others) is the task of converting text into a
predicate-argument representation. Given a trigger
word or phrase in a sentence, this task solves two
related prediction problems: (a) identifying the rela-
tion label, and (b) identifying and labeling the argu-
ments of the relation.
This problem has been studied in the con-
text of verb and nominal triggers using the Prop-
Bank (Palmer et al, 2005) and NomBank (Meyers
et al, 2004) annotations over the Penn Treebank,
and also using the FrameNet lexicon (Fillmore et
al., 2003), which allows arbitrary words to trigger
semantic frames.
This paper focuses on semantic relations ex-
pressed by transitive prepositions1. We can define
the two prediction tasks for prepositions as follows:
identifying the relation label for a preposition, and
predicting the arguments of the relation. Preposi-
tions can mark arguments (both core and adjunct)
for verbal and nominal predicates. In addition, they
can also trigger relations that are not part of other
predicates. For example, in sentence (3) below, the
prepositional phrase starting with to is an argument
of the verb visit, but the in triggers an independent
relation indicating the location of the aquarium.
(3) The children enjoyed the visit to the aquarium
in Coney Island.
FrameNet covers some prepositional relations, but
allows only temporal, locative and directional senses
of prepositions to evoke frames, accounting for only
3% of the targets in the SemEval 2007 shared task
of FrameNet parsing. In fact, the state-of-the-art
FrameNet parser of (Das et al, 2010) does not con-
sider any frame inducing prepositions.
(Baldwin et al, 2009) highlights the importance
of studying prepositions for a complete linguistic
1By transitive prepositions we refer to the standard usage of
prepositions that take an object. In particular, we do not con-
sider prepositional particles in our analysis.
232
analysis of sentences and surveys work in the NLP
literature that addresses the syntax and semantics
of prepositions. One line of work (Ye and Bald-
win, 2006) addressed the problem of preposition
semantic role labeling by considering prepositional
phrases that act as arguments of verbs according
to the PropBank annotation. They built a system
that predicts the labels of these prepositional phrases
alone. However, by definition, this covered only
verb-attached prepositions. (Zapirain et al, 2012)
studied the impact of automatically learned selec-
tional preferences for predicting arguments of verbs
and showed that modeling prepositional phrases sep-
arately improves the performance of argument pre-
diction.
Preposition semantics has also been studied
via the Preposition Project (Litkowski and Har-
graves, 2005) and the related SemEval 2007 shared
task of word sense disambiguation of prepositions
(Litkowski and Hargraves, 2007). The Preposi-
tion Project identifies preposition senses based on
their definitions in the Oxford Dictionary of English.
There are 332 different labels to be predicted with a
wide variance in the number of senses per preposi-
tion ranging from 2 (during and as) to 25 (on). For
example, according to the preposition sense inven-
tory, the preposition from in sentence (2) above will
be labeled with the sense from:12(9) to indicate a
cause. (Dahlmeier et al, 2009) added sense anno-
tation to seven prepositions in four sections of the
Penn Treebank with the goal of studying their inter-
action with verb arguments.
Using the SemEval data, (Tratz and Hovy, 2009)
and (Hovy et al, 2010) showed that the arguments
offer an important cue to identify the sense of the
preposition and (Tratz, 2011) showed further im-
provements by refining the sense inventory. How-
ever, though these works used a dependency parser
to identify arguments, in order to overcome parsing
errors, they augment the parser?s predictions using
part-of-speech based heuristics.
We argue that, while disambiguating the sense
of a preposition does indeed reveal nuances of its
meaning, it leads to a proliferation of labels to be
predicted. Most importantly, sense labels do not
transfer to other prepositions that express the same
meaning. For example, both finish lunch before
noon and finish lunch by noon express a Temporal
relation. According to the Preposition Project, the
sense label for the first preposition is before:1(1),
and that for the second is by:17(4). This both de-
feats the purpose of identifying the relations to aid
natural language understanding and makes the pre-
diction task harder than it should be: using the stan-
dard word sense classification approach, we need to
train a separate classifier for each word because the
labels are defined per-preposition. In other words,
we cannot share features across the different prepo-
sitions. This motivates the need to combine such
senses of prepositions into the same class label.
In this direction, (O?Hara and Wiebe, 2009) de-
scribes an inventory of preposition relations ob-
tained using Penn Treebank function tags and frame
elements from FrameNet. (Srikumar and Roth,
2011) merged preposition senses of seven preposi-
tions into relation labels. (Litkowski, 2012) also
suggests collapsing the definitions of prepositions
into a smaller set of semantic classes. To aid bet-
ter generalization and to reduce the label complex-
ity, we follow this line of work to define a set of rela-
tion labels which abstract word senses across prepo-
sitions2.
3 Preposition-triggered Relations
This section describes the inventory of preposition
relations introduced in this paper, and then identifies
the components of the preposition relation extraction
problem.
3.1 Preposition Relation Inventory
We build our relation inventory using the sense an-
notation in the Preposition Project, focusing on the
34 prepositions3 annotated for the SemEval-2007
shared task of preposition sense disambiguation.
As discussed in Section 2, we construct the in-
ventory of preposition relations by collapsing se-
mantically related preposition senses across differ-
2Since the preposition sense data is annotated over
FrameNet sentences, sense annotation can be used to extend
FrameNet (Litkowski, 2012). We believe that the abstract la-
bels proposed in this paper can further help in this effort.
3We consider the following prepositions: about, above,
across, after, against, along, among, around, as, at, before, be-
hind, beneath, beside, between, by, down, during, for, from, in,
inside, into, like, of, off, on, onto, over, round, through, to, to-
wards, and with. This does not include multi-word prepositions
such as because of and due to.
233
ent prepositions. For each sense that is defined,
the Preposition Project also specifies related prepo-
sitions. These definitions and related prepositions
provide a starting point to identify senses that can
be merged across prepositions. We followed this
with a manual cleanup phase. Some senses do not
cleanly align with a single relation because the def-
initions include idiomatic or figurative usage. For
example, the sense in:7(5) of the preposition in, ac-
cording to the definition, includes both spatial and
figurative notions of the spatial sense (that is, both
in London and in a film). In such cases, we sam-
pled 20 examples from the SemEval 2007 training
set and assigned the relation label based on majority.
If sufficient examples could not be sampled, these
senses were added to the label Other, which is not
a semantically coherent category and represents the
?overflow? case.
Overall, we have 32 labels, which are listed in
Table 14. A companion publication (available on
the authors? website) provides detailed definitions
of each relation and the senses that were merged to
create each label. Since we define relations to be
groups of preposition sense labels, each sense can
be uniquely mapped to a relation label. Hence, we
can use the annotated sense data from SemEval 2007
to obtain a corpus of relation-labeled sentences.
To validate the labeling scheme, two native speak-
ers of English annotated 200 sentences from the
SemEval training corpus using only the definitions
of the labels as the annotation guidelines. We mea-
sured Cohen?s kappa coefficient (Cohen, 1960) be-
tween the annotators to be 0.75 and also between
each annotator and the original corpus to be 0.76 and
0.74 respectively.
3.2 Preposition Relation Extraction
The input to the prediction problem consists of a
preposition in a sentence and the goal is to jointly
model the following: (i) The relation expressed by
the preposition, and (ii) The arguments of the rela-
tion, namely the governor and the object.
We use sentence (2) in the introduction as our run-
ning example the following discussion. In our run-
4Note that, even though we do not consider intransitive
prepositions, the definitions of some relations in Table 1 could
be extended apply to prepositional particles such drive down
(Direction) and run about (Manner).
Relation Name Example
Activity good at boxing
Agent opened by Annie
Attribute walls of stone
Beneficiary fight for Napoleon
Cause died of cancer
Co-Participants pick one among these
Destination leaving for London
Direction drove towards the border
EndState driven to tears
Experiencer warm towards her
Instrument cut with a knife
Journey travel by road
Location living in London
Manner scream like an animal
MediumOfCommunication new show on TV
Numeric increase by 10%
ObjectOfVerb murder of the boys
Opponent/Contrast fight with him
Other all others
Participant/Accompanier steak with wine
PartWhole member of gang
PhysicalSupport lean against the wall
Possessor son of a friend
ProfessionalAspect works in publishing
Purpose tools for making it
Recipient unkind to her
Separation ousted from power
Source purchased from the shop
Species city of Prague
StartState recover from illness
Temporal arrived on Monday
Topic books on Shakespeare
Table 1: List of preposition relations
ning example, the relation label is Cause. We rep-
resent the predicted relation label by r.
Arguments The relation label crucially depends
on correctly identifying the arguments of the prepo-
sition, which are death and pneumonia in our run-
ning example. While a parser can identify the argu-
ments of a preposition, simply relying on the parser
may impose an upper limit on the accuracy of rela-
tion prediction.
We build an oracle experiment to highlight this
limitation. Table 2 shows the recall of the easy-first
dependency parser of (Goldberg and Elhadad, 2010)
on Section 23 of the Penn Treebank for identifying
the governor and object of prepositions.
We define heuristics that generate a candidate
governors and objects for a preposition. For the gov-
234
ernor, this set includes the previous verb or noun
and for the object, it includes only the next noun.
The row labeled Best(Parser, Heuristics) shows the
performance of an oracle predictor which selects the
true governor/object if present among the parser?s
prediction and the heuristics. We see that, even for
the in-domain case, if we are able to re-rank the can-
didates, we could achieve a big improvement in ar-
gument identification.
Recall
Governor Object
Parser 88.88 92.37
Best(Parser, Heuristics) 92.50 93.06
Table 2: Identifying governor and object of prepositions
in the Penn Treebank data. Here, Best(Parser, Heuris-
tics) reports the performance of an oracle that picks the
true governor and object, if present among the candidates
presented by the parser and the heuristic. This presents
an in-domain upper bound for governor and object detec-
tion. See text for further details.
To overcome erroneous parser decisions, we en-
tertain governor and object candidates proposed
both by the parser and the heuristics. In the follow-
ing discussion, we denote the chosen governor and
object by g and o respectively.
Argument types While the primary purpose of
this work is to model preposition relations and their
arguments, the relation prediction is strongly depen-
dent on the semantic type of the arguments. To il-
lustrate this, consider the following incomplete sen-
tence: The message was delivered at ? ? ? . This
preposition can express both a Temporal or a
Location relation depending on the object (for ex-
ample, noon vs. the doorstep).
(Agirre et al, 2008) shows that modeling the se-
mantic type of the arguments jointly with attachment
can improve PP attachment accuracy. In this work,
we point out that argument types should be modeled
jointly with both aspects of the problem of preposi-
tion relation labeling.
Types are an abstraction that capture common
properties of groups of entities. For example, Word-
Net provides generalizations of words in the form of
their hypernyms. In our running example, we wish
to generalize the relation label for death from pneu-
monia to include cases such as suffering from flu.
Figure 1 shows the hypernym hierarchy for the word
pneumonia. In this case, synsets in the hypernym
hierarchy, like pathological state or physical condi-
tion, would also include ailments like flu.
pneumonia
=> respiratory disease
=> disease
=> illness
=> ill health
=> pathological state
=> physical condition
=> condition
=> state
=> attribute
=> abstraction
=> entity
Figure 1: Hypernym hierarchy for the word pneumonia
We define a semantic type to be a cluster of words.
In addition to WordNet hypernyms, we also cluster
verbs, nouns and adjectives using the dependency-
based word similarity of (Lin, 1998) and treat cluster
membership as types. These are described in detail
in Section 5.1.
Relation prediction involves not only identifying
the arguments, but also selecting the right semantic
type for them, which together, help predicting the
relation label. Given an argument candidate and a
collection of possible types (given by WordNet or
the similarity based clusters), we need to select one
of the types. For example, in the WordNet case, we
need to pick one of the hypernyms in the hypernym
hierarchy. Thus, for the governor and object, we
have a set of type labels, comprised of one element
for each type category. We denote this by tg (gover-
nor type) and to (object type) respectively.
3.3 Problem definition
The input to our prediction task is a preposition in
a sentence. Our goal is to jointly model the relation
it expresses, the governor and the object of the rela-
tion and the types of each argument (both WordNet
hypernyms and cluster membership). We denote the
input by x, which consists not only of the prepo-
sition but also a set of candidates for the governor
and the object and, for each type category, the list of
types for the governor and candidate.
235
The prediction, which we denote by y, consists
of the relation r, which can be one of the valid re-
lation labels in Table 1 and the governor and object,
denoted by g and o, each of which is one of text seg-
ments proposed by the parser or the heuristics. Ad-
ditionally, y also consists of type predictions for the
governor and object, denoted by tg and to respec-
tively, each of which is a vector of labels, one for
each type category. Table 3 summarizes the nota-
tion described above. We refer to the ith element of
vectors using subscripts and use the superscript ? to
denote gold labels. Recall that we have gold labels
only for the relation labels and not for arguments and
their types.
Symbol Meaning
x Input (pre-processed sentence and
preposition)
r relation label for the preposition
g, o governor and object of the relation
tg, to vectors of type assignments for
governor and object respectively
y Full structure (r, g, o, tg, to)
Table 3: Summary of notation
4 Learning preposition relations
A key challenge in modeling preposition relations is
that our training data only annotates the relation la-
bels and not the arguments and types. In this section,
we introduce two approaches for predicting preposi-
tion relations using this data.
4.1 Feature Representation
We use the notation ?(x,y) to indicate the feature
function for an input x and the full output y. We
build ? using the features of the components of y:
1. Arguments: For g and o, which represent an
assignment to the governor and object, we de-
note the features extracted from the arguments
as ?A(x, g) and ?A(x, o) respectively.
2. Types: Given a type assignment tgi to the ith
type category of the governor, we define fea-
tures ?T (x, g, tgi ). Similarly, we define features
?T (x, o, toi ) for the types of the object.
We combine the argument and their type features to
define the features for classifying the relation, which
we denote by ?(x, g, o, tg, to):
? =
?
a?{g,o}
(
?A(x, a) +
?
i
?T (x, a, tai )
)
(1)
Section 5 describes the actual features used in our
experiments.
Observe that given the arguments and their types,
the task of predicting relations is simply a multiclass
classification problem. Thus, following the standard
convention for multiclass classification, the overall
feature representation for the relation and argument
prediction is defined by conjoining the relation r
with features for the corresponding arguments and
types, ?. This gives us the full feature representa-
tion, ?(x,y).
4.2 Model 1: Predicting only relations
The first model aims at predicting only the rela-
tion labels and not the arguments and types. This
falls into the standard multiclass classification set-
ting, where we wish to predict one of 32 labels. To
do so, we sum over all the possible assignments to
the rest of the structure and define features for the
inputs as
??(x) =
?
g,o,tg ,to
?(x, g, o, tg, to) (2)
Effectively, doing so uses all the governor and ob-
ject candidates and all their semantic types to get
a feature representation for the relation classifica-
tion problem. Once again, for a relation label r, the
overall feature representation is defined by conjoin-
ing the relation r with the features for that relation
??, which we write as ?R(x, r). Note that this sum-
mation is computationally inexpensive in our case
because the sum decomposes according to equation
(1). With a learned weight vector w, the relation
label is predicted as
r = arg max
r?
wT?R(x, r?) (3)
We use a structural SVM (Tsochantaridis et al,
2004) to train a weight vector w that predicts the re-
lation label as above. The training is parameterized
by C, which represents the tradeoff between gener-
alization and the hinge loss.
236
4.3 Model 2: Learning from partial
annotations
In the second model, even though our annotation
does not provide gold labels for arguments and
types, our goal is to predict them. At inference time,
if we had a weight vector w, we could predict the
full structure using inference as follows:
y = arg max
y?
wT?(x,y) (4)
We propose an iterative learning algorithm to learn
this weight vector.
In the following discussion, for a labeled example
(x,y?), we refer to the missing part of its structure
as h(y?). That is, h(y?) is the assignment to the
arguments of the relation and their types. We use the
notation r(y) to denote the relation label specified
by a structure y.
Our learning algorithm is closely related to re-
cently developed latent variable based frameworks
(Yu and Joachims, 2009; Chang et al, 2010a; Chang
et al, 2010b), where the supervision provides only
partial annotation. We begin by defining two addi-
tional inference procedures:
1. Latent Inference: Given a weight vector w
and a partially labeled example (x,y?), we can
?complete? the rest of the structure by infer-
ring the highest scoring assignment to the miss-
ing parts. In the algorithm, we call this pro-
cedure LatentInf(w,x,y?), which solves the
following maximization problem:
y? = arg maxy wT?(x,y), (5)
s.t. r(y) = r(y?).
2. Loss augmented inference: This is a variant
of the the standard loss augmented inference
for structural SVMs, which solves the follow-
ing maximization problem for a given x and
fully labeled y?:
arg max
y
wT?(x,y) + ?(y,y?) (6)
Here, ?(y,y?) denotes the loss function. In
the standard structural SVMs, the loss is over
the entire structure. In the Latent Structural
SVM formulation of (Yu and Joachims, 2009),
the loss is defined only over the part of the
structure with the gold label. In this work, we
use the standard Hamming loss over the entire
structure, but scale the loss for the elements of
h(y) by a parameter ? < 1. This is a gen-
eralization of the latent structural SVM, which
corresponds to the setting ? = 0. The intu-
ition behind having a non-zero ? is that in ad-
dition to penalizing the learning algorithm if it
violates the annotated part of the structure, we
also incorporate a small penalty for the rest of
the structure.
Using these two inference procedures, we define
the learning algorithm as Algorithm 1. The weight
vector is initialized using Model 1. The algorithm
then finds the best arguments and types for all ex-
amples in the training set (steps 3-5). Doing so
gives an estimate of the arguments and types for
each example, giving us ?fully labeled? structured
data. The algorithm then proceeds to use this data to
train a new weight vector using the standard struc-
tural SVM with the loss augmented inference listed
above (step 6). These two steps are repeated several
times. Note that as with the summation in Model
1, solving the inference problems described above is
computationally inexpensive.
Algorithm 1 Algorithm for learning Model 2
Input: Examples D = {xi, r(y?i )}, where exam-
ples are labeled only with the relation labels.
1: Initialize weight vector w using Model 1
2: for t = 1, 2, ? ? ? do
3: for (xi,y?i ) ? D do
4: y?i ? LatentInf(w,xi,y?i ) (Eq. 5)
5: end for
6: w ? LearnSSVM({xi, y?i}) with the loss
augmented inference of Eq. 6
7: end for
8: return w
Algorithm 1 is parameterized by C and ?. The
parameter ? controls the extent to which the hypoth-
esized labels according to the previous iteration?s
weight vector influence the learning.
237
4.4 Joint inference between preposition senses
and relations
By defining preposition relations as disjoint sets of
preposition senses, we effectively have a hierarchi-
cal relationship between senses and relations. This
suggests that joint inference can be employed be-
tween sense and relation predictions with a validity
constraint connecting the two. The idea of employ-
ing inference to combine independently trained pre-
dictors to obtain a coherent output structure has been
used for various NLP tasks in recent years, starting
with the work of (Roth and Yih, 2004; Roth and Yih,
2007).
We use the features defined by (Hovy et al, 2010),
which we write as ?s(x, s) for a given input x and
sense label s, and train a separate preposition sense
model on the SemEval data with features ?s(x, s)
using the structural SVM algorithm. Thus, we have
two weight vectors ? the one for predicting preposi-
tion relations described earlier, and the preposition
sense weight vector. At prediction time, for a given
input, we find the highest scoring joint assignment to
the relation, arguments and types and the sense, sub-
ject to the constraint that the sense and the relation
agree according to the definition of the relations.
5 Experiments and Results
The primary research goal of our experiments is to
evaluate the different models (Model 1, Model 2 and
joint relation-sense inference) for predicting prepo-
sition relations. In additional analysis experiments,
we also show that the definition of preposition rela-
tions indeed captures cross-preposition semantics by
taking advantage of shared features and also high-
light the need for going beyond the syntactic parser.
5.1 Types and Features
Types As described in Section 3, we use WordNet
hypernyms as one of the type categories. We use all
hypernyms within four levels in the hypernym hier-
archy for all senses.
The second type category is defined by word-
similarity driven clusters. We briefly describe the
clustering process here. The thesaurus of (Lin,
1998) specifies similar lexical items for a given
word along with a similarity score from 0 to 1. It
treats nouns, verbs and adjectives separately. We
use the score to cluster groups of similar words us-
ing a greedy set-covering approach. Specifically,
we randomly select a word which is not yet in a
cluster as the center of a new cluster and add all
words whose score is greater than ? to it. We re-
peat this process till all words are in some clus-
ter. A word can appear in more than one cluster
because all words similar to the cluster center are
added to the cluster. We repeat this process for
? ? {0.1, 0.125, 0.15, 0.175, 0.2, 0.25}. By increas-
ing the value of ?, the clusters become more selec-
tive and hence smaller. Table 4 shows example noun
clusters created using ? = 0.15. For a given word,
identifiers for clusters to which the word belongs
serve as type label candidates for this type category5.
Features Our argument features, denoted by ?A
in Section 4.1, are derived from the preposition
sense feature set of (Hovy et al, 2010) and extract
the following from the argument: 1. Word, part-of-
speech, lemma and capitalization indicator, 2. Con-
flated part-of-speech (one of Noun, Verb, Adjective,
Adverb, and Other), 3. Indicator for existence in
WordNet, 4. WordNet synsets for the first and all
senses, 5. WordNet lemma, lexicographer file names
and part, member and substance holonyms, 6. Roget
thesaurus divisions for the word, 7. The first and last
two and three letters, and 8. Indicators for known af-
fixes. Our type features (?T ) are simply indicators
for the type label, conjoined with the type category.
One advantage of abstracting word senses into re-
lations is that we can share features across different
prepositions. The base feature set (for both types
and arguments) defined above does not encode in-
formation about the preposition to be classified. We
do so by conjoining the features with the preposi-
tion. In addition, since the relation labels are shared
across all prepositions, we include the base features
as a shared representation between prepositions.
We consider two variants of our feature sets.
We refer to the features described above as the
typed features. In addition, we define the
typed+gen features by conjoining argument and
type features of typed with the name of the genera-
tor that proposes the argument. Recall that governor
candidates are proposed by the dependency parser,
or by the heuristics described earlier. Hence, for
5The clusters can be downloaded from the authors? website.
238
Jimmy Carter; Ronald Reagan; richard nixon; George Bush; Lyndon Johnson; Richard M. Nixon; Gerald Ford
metalwork; porcelain; handicraft; jade; bronzeware; carving; pottery; ceramic; earthenware; jewelry; stoneware; lacquerware
degradation; erosion; pollution; logging; desertification; siltation; urbanization; felling; poaching; soil erosion; depletion;
water pollution; deforestation
expert; Wall Street analyst; analyst; economist; telecommunications analyst; strategist; media analyst
fox news channel; NBC News; MSNBC; Fox News; CNBC; CNNfn; C-Span
Tuesdays; Wednesdays; weekday; Mondays; Fridays; Thursdays; sundays; Saturdays
Table 4: Examples of noun clusters generated using the set-covering approach for ? = 0.15
a governor, the typed+gen features would conjoin
the corresponding typed features with one of parser,
previous-verb, previous-noun, previous-adjective, or
previous-word.
5.2 Experimental setup and data
All our experiments are based on the Sem-
Eval 2007 data for preposition sense disambigua-
tion (Litkowski and Hargraves, 2007) comprising
word sense annotation over 16176 training and
8058 examples of prepositions labeled with their
senses. We pre-processed sentences with part-of-
speech tags using the Illinois POS tagger and de-
pendency graphs using the parser of (Goldberg and
Elhadad, 2010)6. For the experiments described be-
low, we used the relation-annotated training set to
train the models and evaluate accuracy of prediction
on the test set.
We chose the structural SVM parameter C using
five-fold cross-validation on a 1000 random exam-
ples chosen from the training set. For Model 2, we
picked ? = 0.1 using a validation set consisting of
a separate set of 1000 training examples. We ran
Algorithm 1 for 20 rounds.
Predicting the most frequent relation for a prepo-
sition gives an accuracy of 21.18%. Even though
the performance of the most-frequent relation label
is poor, it does not represent the problem?s difficulty
and is not a good baseline. To compare, for prepo-
sition senses, using features from the neighboring
words, (Ye and Baldwin, 2007) obtained an accuracy
of 69.3%, and with features designed for the prepo-
sition sense task, (Hovy et al, 2010) get up to 84.8%
accuracy for the task. Our re-implementation of the
latter system using a different set of pre-processing
tools gets an accuracy of 83.53%.
For preposition relations, our baseline system for
6We used the Curator (Clarke et al, 2012) for all pre-
processing.
relation labeling uses the typed feature set, but with-
out any type information. This produces an accuracy
of 88.01% with Model 1 and 88.64% with Model 2.
We report statistical significance of results using our
implementation of Dan Bikel?s stratified-shuffling
based statistical significance tester7.
5.3 Main results: Relation prediction
Our main result, presented in Table 5, compares the
baseline model (without types) against other sys-
tems, using both models described in Section 4.
First, we see that adding type information (typed)
improves performance over the baseline. Expand-
ing the feature space (typed+gen) gives further im-
provements. Finally, jointly predicting the relations
with preposition senses gives another improvement.
Setting AccuracyModel 1 Model 2
No types 88.01 88.64
typed 88.77 89.14
typed+gen 89.90? 89.43?
Joint typed+gen & sense 89.99? 90.26??
Table 5: Main results: Accuracy of relation labeling.
Results in bold are statistically significant (p < 0.01)
improvements over the system that is unaware of types.
Superscripts ? and ? indicate significant improvements
over typed and typed+gen respectively at p < 0.01. For
Model 2, the improvement of typed over the model with-
out types is significant at p < 0.05.
Our objective is not predicting preposition sense.
However, we observe that with Model 2, jointly pre-
dicting the sense and relations improves not only the
performance of relation identification, but via joint
inference between relations and senses also leads to
a large improvement in sense prediction accuracy.
Table 6 shows the accuracy for sense prediction. We
7http://www.cis.upenn.edu/?dbikel/software.html
239
see that while Model 1 does not lead to a significant
improvement in the accuracy, Model 2 gives an ab-
solute improvement of over 1%.
Setting Sense accuracy
Hovy (re-implementation) 83.53
Joint + Model 1 83.78
Joint + Model 2 84.78?
Table 6: Sense prediction performance. Joint inference
with Model 1, while improving relation performance,
does not help sense accuracy in comparison to our re-
implementation of the Hovy sense disambiguation sys-
tem. However, with Model 2, the improvement is statis-
tically significant at p < 0.01.
5.4 Ablation experiments
Feature sharing across prepositions In our first
analysis experiment, we seek to highlight the utility
of sharing features between different prepositions.
To do so, we compare the performance of a sys-
tem trained without shared features against the type-
independent system, which uses shared features. To
discount the influence of other factors, we use Model
1 in the typed setting without any types. Table 7
reports the accuracy of relation prediction for these
two feature sets. We observed a similar improve-
ment in performance even when type features are
added or the setting is changed to typed+gen or with
Model 2.
Setting Accuracy
Independent 87.17
+ Shared 88.01
Table 7: Comparing the effect of feature sharing across
prepositions. We see that having a shared representation
that goes across prepositions improves accuracy of rela-
tion prediction (p < 0.01).
Different argument candidate generators Our
second ablation study looks at the effect of the var-
ious argument candidate generators. Recall that in
addition to the dependency governor and object, our
models also use the previous word, the previous
noun, adjective and verb as governor candidates and
the next noun as object candidate. We refer to the
candidates generated by the parser as Parser only
and the others as Heuristics only. Table 8 compares
the performance of these two argument candidate
generators against the full set using Model 1 in both
the typed and typed+gen settings.
We see that the heuristics give a better accu-
racy than the parser based system. This is because
the heuristics often contain the governor/object pre-
dicted by the dependency. This is not always the
case, though, because using all generators gives a
slightly better performing system (not statistically
significant). In the overall system, we retain the de-
pendency parser as one of the generators in order to
capture long-range governor/object candidates that
may not be in the set selected by the heuristics.
Feature sets
Generator typed typed+gen
Parser only 87.12 87.12
Heuristics only 87.63 88.84
All 88.01 89.12
Table 8: The performance of different argument candi-
date generators. We see that considering a larger set of
candidate generators gives a big accuracy improvement.
6 Discussion
There are two key differences between Model 1 and
2. First, the former predicts only the relation label,
while the latter predicts the entire structure. Table 9
shows example predictions of Model 2 for relation
label and WordNet argument types. These examples
show how the argument types can be thought of as
an explanation for the choice of relation label.
Input Relation Hypernyms
governor object
died of pneumonia Cause experience disease
suffered from flu Cause experience disease
recovered from flu StartState change disease
Table 9: Example predictions according to Model 2. The
hypernyms column shows a representative of the synset
chosen for the WordNet types. We see that in the com-
bination of experience and disease suggests the relation
Cause while the change and disease indicate the rela-
tion StartState.
The main difference between the two models is
in the treatment of the unlabeled (or latent) parts of
the structure (namely, the arguments and the types)
during training and inference. During training, for
240
each example, Model 1 aggregates features from all
governors and objects even if they are possibly ir-
relevant, which may lead to a much bigger model in
terms of the number of active weights. On the other
hand, for Model 2, Algorithm 1 uses the single high-
est scoring prediction of the latent variables, accord-
ing to the current parameters, to refine the parame-
ters. Indeed, in our experiments, we observed that
the number of non-zero weights in the weight vec-
tor of Model 2 is much smaller than that of Model
1. For instance, in the typed setting, the weight vec-
tor for Model 1 had 2.57 million elements while that
for Model 2 had only 1.0 million weights. Similarly,
for the typed+gen setting, Model 1 had 5.41 million
non-zero elements in the weight vector while Model
2 had only 2.21 million non-zero elements.
The learning algorithm itself is a generalization
of the latent structural SVM of (Yu and Joachims,
2009). By setting ? to zero, we get the latent struc-
ture SVM. However, we found via cross-validation
that this is not the best setting of the parameter. A
theoretical understanding of the sparsity of weights
learned by the algorithm and a study of its conver-
gence properties is an avenue of future research.
7 Conclusion
We addressed the problem of modeling semantic re-
lations expressed by prepositions. We approached
this task by defining a set of preposition relations
that combine preposition senses across prepositions.
Doing so allowed us to leverage existing annotated
preposition sense data to induce a corpus for prepo-
sition labels. We modeled preposition relations in
terms of its arguments, namely the governor and ob-
ject of the preposition, and the semantic types of
the arguments. Using a generalization of the latent
structural SVM, we trained a relation, argument and
type predictor using only annotated relation labels.
This allowed us to get an accuracy of 89.43% on re-
lation prediction. By employing joint inference with
a preposition sense predictor, we further improved
the relation accuracy to 90.23%.
Acknowledgments
The authors wish to thank Martha Palmer, Nathan Schneider,
the anonymous reviewers and the editor for their valuable feed-
back. The authors gratefully acknowledge the support of the
Defense Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181. This material
is also based on research sponsored by DARPA under agree-
ment number FA8750-13-2-0008. The U.S. Government is au-
thorized to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright notation thereon. The
views and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or implied, of
DARPA, AFRL or the U.S. Government.
References
E. Agirre, T. Baldwin, and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with sense
information. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 317?325, Columbus, USA.
T. Baldwin, V. Kordoni, and A. Villavicencio. 2009.
Prepositions in applications: A survey and introduc-
tion to the special issue. Computational Linguistics,
35(2):119?149.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010a. Discriminative learning over constrained latent
representations. In Proceedings of the Annual Meet-
ing of the North American Association of Computa-
tional Linguistics (NAACL), pages 429?437, Los An-
geles, USA.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010b. Structured output learning with indirect super-
vision. In Proceedings of the International Conference
on Machine Learning (ICML), pages 199?206, Haifa,
Israel.
J. Clarke, V. Srikumar, M. Sammons, and D. Roth. 2012.
An NLP Curator (or: How I Learned to Stop Wor-
rying and Love NLP Pipelines). In Proceedings of
the International Conference on Language Resources
and Evaluation (LREC), pages 3276?3283, Istanbul,
Turkey.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46.
D. Dahlmeier, H. T. Ng, and T. Schultz. 2009. Joint
learning of preposition senses and semantic roles of
prepositional phrases. In Proceedings of the Confer-
ence on Empirical Methods for Natural Language Pro-
cessing (EMNLP), pages 450?458, Singapore.
D. Das, N. Schneider, D. Chen, and N. Smith. 2010.
Probabilistic frame-semantic parsing. In Proceedings
of Human Language Technologies: The 2010 Annual
241
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 948?
956, Los Angeles, USA.
C. Fillmore, C. Johnson, and M. Petruck. 2003. Back-
ground to FrameNet. International Journal of Lexi-
cography, 16(3):235?250.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proceedings of Human Language Technolo-
gies: The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 742?750, Los Angeles, USA.
D. Hovy, S. Tratz, and E. Hovy. 2010. What?s in a prepo-
sition? dimensions of sense disambiguation for an in-
teresting word class. In Coling 2010: Posters, pages
454?462, Beijing, China.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 768?774, Montreal, Canada.
K. Litkowski and O. Hargraves. 2005. The Preposition
Project. In ACL-SIGSEM Workshop on the Linguistic
Dimensions of Prepositions and their Use in Computa-
tional Linguistics Formalisms and Applications, pages
171?179, Colchester, UK.
K. Litkowski and O. Hargraves. 2007. SemEval-2007
Task 06: Word-Sense Disambiguation of Prepositions.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 24?
29, Prague, Czech Republic.
K. Litkowski. 2012. Proposed Next Steps for The Prepo-
sition Project. Technical Report 12-01, CL Research.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: An interim report. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation, pages 24?
31, Boston, USA.
T. O?Hara and J. Wiebe. 2009. Exploiting semantic role
resources for preposition disambiguation. Computa-
tional Linguistics, 35(2):151?184.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?106.
M. Palmer, D. Gildea, and N. Xue. 2010. Semantic Role
Labeling, volume 3. Morgan & Claypool Publishers.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks.
In Proceedings of the Annual Conference on Compu-
tational Natural Language Learning (CoNLL), pages
1?8, Boston, USA.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. Introduction to Statistical Rela-
tional Learning.
V. Srikumar and D. Roth. 2011. A joint model for
extended semantic role labeling. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Edinburgh, Scotland.
S. Tratz and D. Hovy. 2009. Disambiguation of prepo-
sition sense using linguistically motivated features. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Student Research Workshop and
Doctoral Consortium, pages 96?100, Boulder, USA.
S. Tratz. 2011. Semantically-enriched Parsing for Natu-
ral Language Understanding. Ph.D. thesis, University
of Southern California.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interde-
pendent and structured output spaces. In Proceedings
of the International Conference on Machine Learning
(ICML), pages 104?111, Banff, Canada.
P. Ye and T. Baldwin. 2006. Semantic role labeling
of prepositional phrases. ACM Transactions on Asian
Language Information Processing (TALIP), 5(3):228?
244.
P. Ye and T. Baldwin. 2007. MELB-YB: Preposition
sense disambiguation using rich semantic features.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 241?
244, Prague, Czech Republic.
C. Yu and T. Joachims. 2009. Learning structural SVMs
with latent variables. In Proceedings of the Inter-
national Conference on Machine Learning (ICML),
pages 1?8, Montreal, Canada.
B. Zapirain, E. Agirre, L. Ma`rquez, and M. Surdeanu.
2012. Selectional preferences for semantic role classi-
fication. Computational Linguistics, pages 1?33.
242
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 65?69,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Sorting out the Most Confusing English Phrasal Verbs
Yuancheng Tu
Department of Linguistics
University of Illinois
ytu@illinois.edu
Dan Roth
Department of Computer Science
University of Illinois
danr@illinois.edu
Abstract
In this paper, we investigate a full-fledged
supervised machine learning framework for
identifying English phrasal verbs in a given
context. We concentrate on those that we de-
fine as the most confusing phrasal verbs, in the
sense that they are the most commonly used
ones whose occurrence may correspond either
to a true phrasal verb or an alignment of a sim-
ple verb with a preposition.
We construct a benchmark dataset1 with 1,348
sentences from BNC, annotated via an Inter-
net crowdsourcing platform. This dataset is
further split into two groups, more idiomatic
group which consists of those that tend to be
used as a true phrasal verb and more compo-
sitional group which tends to be used either
way. We build a discriminative classifier with
easily available lexical and syntactic features
and test it over the datasets. The classifier
overall achieves 79.4% accuracy, 41.1% er-
ror deduction compared to the corpus major-
ity baseline 65%. However, it is even more
interesting to discover that the classifier learns
more from the more compositional examples
than those idiomatic ones.
1 Introduction
Phrasal verbs in English, are syntactically defined
as combinations of verbs and prepositions or parti-
cles, but semantically their meanings are generally
not the direct sum of their parts. For example, give
in means submit, yield in the sentence, Adam?s say-
ing it?s important to stand firm , not give in to ter-
rorists. Adam was not giving anything and he was
1http://cogcomp.cs.illinois.edu/page/resources/PVC Data
not in anywhere either. (Kolln and Funk, 1998) uses
the test of meaning to detect English phrasal verbs,
i.e., each phrasal verb could be replaced by a single
verb with the same general meaning, for example,
using yield to replace give in in the aforementioned
sentence. To confuse the issue even further, some
phrasal verbs, for example, give in in the follow-
ing two sentences, are used either as a true phrasal
verb (the first sentence) or not (the second sentence)
though their surface forms look cosmetically similar.
1. How many Englishmen gave in to their emo-
tions like that ?
2. It is just this denial of anything beyond what is
directly given in experience that marks Berke-
ley out as an empiricist .
This paper is targeting to build an automatic learner
which can recognize a true phrasal verb from its
orthographically identical construction with a verb
and a prepositional phrase. Similar to other types
of MultiWord Expressions (MWEs) (Sag et al,
2002), the syntactic complexity and semantic id-
iosyncrasies of phrasal verbs pose many particular
challenges in empirical Natural Language Process-
ing (NLP). Even though a few of previous works
have explored this identification problem empiri-
cally (Li et al, 2003; Kim and Baldwin, 2009) and
theoretically (Jackendoff, 2002), we argue in this pa-
per that this context sensitive identification problem
is not so easy as conceivably shown before, espe-
cially when it is used to handle those more com-
positional phrasal verbs which are empirically used
either way in the corpus as a true phrasal verb or
a simplex verb with a preposition combination. In
addition, there is still a lack of adequate resources
or benchmark datasets to identify and treat phrasal
65
verbs within a given context. This research is also
an attempt to bridge this gap by constructing a pub-
licly available dataset which focuses on some of the
most commonly used phrasal verbs within their most
confusing contexts.
Our study in this paper focuses on six of the most
frequently used verbs, take, make, have, get, do
and give and their combination with nineteen com-
mon prepositions or particles, such as on, in, up
etc. We categorize these phrasal verbs according to
their continuum of compositionality, splitting them
into two groups based on the biggest gap within
this scale, and build a discriminative learner which
uses easily available syntactic and lexical features to
analyze them comparatively. This learner achieves
79.4% overall accuracy for the whole dataset and
learns the most from the more compositional data
with 51.2% error reduction over its 46.6% baseline.
2 Related Work
Phrasal verbs in English were observed as one kind
of composition that is used frequently and consti-
tutes the greatest difficulty for language learners
more than two hundred and fifty years ago in Samuel
Johnson?s Dictionary of English Language2. They
have also been well-studied in modern linguistics
since early days (Bolinger, 1971; Kolln and Funk,
1998; Jackendoff, 2002). Careful linguistic descrip-
tions and investigations reveal a wide range of En-
glish phrasal verbs that are syntactically uniform,
but diverge largely in semantics, argument struc-
ture and lexical status. The complexity and idiosyn-
crasies of English phrasal verbs also pose a spe-
cial challenge to computational linguistics and at-
tract considerable amount of interest and investi-
gation for their extraction, disambiguation as well
as identification. Recent computational research on
English phrasal verbs have been focused on increas-
ing the coverage and scalability of phrasal verbs by
either extracting unlisted phrasal verbs from large
corpora (Villavicencio, 2003; Villavicencio, 2006),
or constructing productive lexical rules to gener-
ate new cases (Villanvicencio and Copestake, 2003).
Some other researchers follow the semantic regular-
ities of the particles associated with these phrasal
verbs and concentrate on disambiguation of phrasal
2It is written in the Preface of that dictionary.
verb semantics, such as the investigation of the most
common particle up by (Cook and Stevenson, 2006).
Research on token identification of phrasal verbs
is much less compared to the extraction. (Li et
al., 2003) describes a regular expression based sim-
ple system. Regular expression based method re-
quires human constructed regular patterns and can-
not make predictions for Out-Of-Vocabulary phrasal
verbs. Thus, it is hard to be adapted to other NLP
applications directly. (Kim and Baldwin, 2009) pro-
poses a memory-based system with post-processed
linguistic features such as selectional preferences.
Their system assumes the perfect outputs of a parser
and requires laborious human corrections to them.
The research presented in this paper differs from
these previous identification works mainly in two
aspects. First of all, our learning system is fully
automatic in the sense that no human intervention
is needed, no need to construct regular patterns or
to correct parser mistakes. Secondly, we focus our
attention on the comparison of the two groups of
phrasal verbs, the more idiomatic group and the
more compositional group. We argue that while
more idiomatic phrasal verbs may be easier to iden-
tify and can have above 90% accuracy, there is still
much room to learn for those more compostional
phrasal verbs which tend to be used either positively
or negatively depending on the given context.
3 Identification of English Phrasal Verbs
We formulate the context sensitive English phrasal
verb identification task as a supervised binary clas-
sification problem. For each target candidate within
a sentence, the classifier decides if it is a true phrasal
verb or a simplex verb with a preposition. Formally,
given a set of n labeled examples {xi, yi}ni=1, we
learn a function f : X ? Y where Y ? {?1, 1}.
The learning algorithm we use is the soft-margin
SVM with L2-loss. The learning package we use
is LIBLINEAR (Chang and Lin, 2001)3.
Three types of features are used in this discrimi-
native model. (1)Words: given the window size from
the one before to the one after the target phrase,
Words feature consists of every surface string of
all shallow chunks within that window. It can be
an n-word chunk or a single word depending on
3http://www.csie.ntu.edu.tw/?cjlin/liblinear/
66
the the chunk?s bracketing. (2)ChunkLabel: the
chunk name with the given window size, such as VP,
PP, etc. (3)ParserBigram: the bi-gram of the non-
terminal label of the parents of both the verb and
the particle. For example, from this partial tree (VP
(VB get)(PP (IN through)(NP (DT the)(NN day))),
the parent label for the verb get is VP and the par-
ent node label for the particle through is PP. Thus,
this feature value is VP-PP. Our feature extractor
is implemented in Java through a publicly available
NLP library4 via the tool called Curator (Clarke et
al., 2012). The shallow parser is publicly avail-
able (Punyakanok and Roth, 2001)5 and the parser
we use is from (Charniak and Johnson, 2005).
3.1 Data Preparation and Annotation
All sentences in our dataset are extracted from BNC
(XML Edition), a balanced synchronic corpus con-
taining 100 million words collected from various
sources of British English. We first construct a list of
phrasal verbs for the six verbs that we are interested
in from two resources, WN3.0 (Fellbaum, 1998)
and DIRECT6. Since these targeted verbs are also
commonly used in English Light Verb Constructions
(LVCs), we filter out LVCs in our list using a pub-
licly available LVC corpus (Tu and Roth, 2011). The
result list consists of a total of 245 phrasal verbs.
We then search over BNC and find sentences for all
of them. We choose the frequency threshold to be
25 and generate a list of 122 phrasal verbs. Finally
we manually pick out 23 of these phrasal verbs and
sample randomly 10% extracted sentences for each
of them for annotation.
The annotation is done through a crowdsourcing
platform7. The annotators are asked to identify true
phrasal verbs within a sentence. The reported inner-
annotator agreement is 84.5% and the gold aver-
age accuracy is 88%. These numbers indicate the
good quality of the annotation. The final corpus
consists of 1,348 sentences among which, 65% with
a true phrasal verb and 35% with a simplex verb-
preposition combination.
4http://cogcomp.cs.illinois.edu/software/edison/
5http://cogcomp.cs.illinois.edu/page/software view/Chunker
6http://u.cs.biu.ac.il/?nlp/downloads/DIRECT.html
7crowdflower.com
3.2 Dataset Splitting
Table 1 lists all verbs in the dataset. Total is the to-
tal number of sentences annotated for that phrasal
verb and Positive indicated the number of examples
which are annotated as containing the true phrasal
verb usage. In this table, the decreasing percent-
age of the true phrasal verb usage within the dataset
indicates the increasing compositionality of these
phrasal verbs. The natural division line with this
scale is the biggest percentage gap (about 10%) be-
tween make out and get at. Hence, two groups are
split over that gap. The more idiomatic group con-
sists of the first 11 verbs with 554 sentences and 91%
of these sentences include true phrasal verb usage.
This data group is more biased toward the positive
examples. The more compositional data group has
12 verbs with 794 examples and only 46.6% of them
contain true phrasal verb usage. Therefore, this data
group is more balanced with respective to positive
and negative usage of the phrase verbs.
Verb Total Positive Percent(%)
get onto 6 6 1.00
get through 61 60 0.98
get together 28 27 0.96
get on with 70 67 0.96
get down to 17 16 0.94
get by 11 10 0.91
get off 51 45 0.88
get behind 7 6 0.86
take on 212 181 0.85
get over 34 29 0.85
make out 57 48 0.84
get at 35 26 0.74
get on 142 103 0.73
take after 10 7 0.70
do up 13 8 0.62
get out 206 118 0.57
do good 8 4 0.50
make for 140 65 0.46
get it on 9 3 0.33
get about 20 6 0.30
make over 12 3 0.25
give in 118 27 0.23
have on 81 13 0.16
Total: 23 1348 878 0.65
Table 1: The top group consists of the more idiomatic
phrasal verbs with 91% of their occurrence within the
dataset to be a true phrasal verb. The second group con-
sists of those more compositional ones with only 46.6%
of their usage in the dataset to be a true phrasal verb.
67
3.3 Experimental Results and Discussion
Our results are computed via 5-cross validation. We
plot the classifier performance with respect to the
overall dataset, the more compositional group and
the more idiomatic group in Figure 1. The clas-
sifier only improves 0.6% when evaluated on the
idiomatic group. Phrasal verbs in this dataset are
more biased toward behaving like an idiom regard-
less of their contexts, thus are more likely to be cap-
tured by rules or patterns. We assume this may ex-
plain some high numbers reported in some previ-
ous works. However, our classifier is more effec-
tive over the more compositional group and reaches
73.9% accuracy, a 51.1% error deduction comparing
to its majority baseline. Phrasal verbs in this set tend
to be used equally likely as a true phrasal verb and
as a simplex verb-preposition combination, depend-
ing on their context. We argue phrasal verbs such as
these pose a real challenge for building an automatic
context sensitive phrasal verb classifier. The overall
accuracy of our preliminary classifier is about 79.4%
when it is evaluated over all examples from these
two groups.
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
Overall Compositional Idiomatic
Ac
cu
ra
cy
Data Groups
Classifier Accuracy for Different Data Groups
Comparison against their Majority Baselines Respectively
Majority Baseline
Classifier Accuracy
Figure 1: Classifier Accuracy of each data group, com-
paring with their baseline respectively. Classifier learns
the most from the more compositional group, indicated
by its biggest histogram gap.
Finally, we conduct an ablation analysis to ex-
plore the contributions of the three types of features
in our model and their accuracies with respect to
each data group are listed in Table 2 with the bold-
faced best performance. Each type of features is
used individually in the classifier. The feature type
Words is the most effective feature with respect to
the idiomatic group and the overall dataset. And the
chunk feature is more effective towards the compo-
sitional group, which may explain the linguistic in-
tuition that negative phrasal verbs usually do not be-
long to the same syntactic chunk.
Datasets
Overall Compositional Idiom.
Baseline 65.0% 46.6% 91%
Words 78.6% 70.2% 91.4%
Chunk 65.6% 70.7% 89.4%
ParserBi 64.4% 67.2% 89.4%
Table 2: Accuracies achieved by the classifier when
tested on different data groups. Features are used indi-
vidually to evaluate the effectiveness of each type.
4 Conclusion
In this paper, we build a discriminative learner to
identify English phrasal verbs in a given context.
Our contributions in this paper are threefold. We
construct a publicly available context sensitive En-
glish phrasal verb dataset with 1,348 sentences from
BNC. We split the dataset into two groups according
to their tendency toward idiosyncrasy and compo-
sitionality, and build a discriminative learner which
uses easily available syntactic and lexical features to
analyze them comparatively. We demonstrate em-
pirically that high accuracy achieved by models may
be due to the stronger idiomatic tendency of these
phrasal verbs. For many of the more ambiguous
cases, a classifier learns more from the composi-
tional examples and these phrasal verbs are shown
to be more challenging.
Acknowledgments
The authors would like to thank four annonymous
reviewers for their valuable comments. The research
in this paper was supported by the Multimodal Infor-
mation Access & Synthesis Center at UIUC, part of
CCICADA, a DHS Science and Technology Center
of Excellence and the Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opin-
ions and findings expressed in this material are those
of the authors and do not necessarily reflect the view
of DHS, DARPA, AFRL, or the US government.
68
References
D. Bolinger. 1971. The Phrasal Verb in English. Har-
vard University Press.
C. Chang and C. Lin, 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of ACL-2005.
J. Clarke, V. Srikumar, M. Sammons, and D. Roth. 2012.
An NLP curator: How I learned to stop worrying and
love NLP pipelines. In Proceedings of LREC-2012.
P. Cook and S. Stevenson. 2006. Classifying particle
semantics in English verb-particle constructions. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Proper-
ties, pages 45?53, Sydney, Australia.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
R. Jackendoff. 2002. English particle constructions, the
lexicon, and the autonomy of syntax. In N. Dehe?,
R. Jackendoff, A. McIntyre, and S. Urban, editors,
Verb-Particle Explorations, pages 67?94. Mouton de
Gruyter.
S Kim and T. Baldwin. 2009. How to pick out token
instances of English verb-particle constructions. Jour-
nal of Language Resources and Evaluation.
M. Kolln and R. Funk. 1998. Understanding English
Grammar. Allyn and Bacon.
W. Li, X. Zhang, C. Niu, Y. Jiang, and R. Srihari. 2003.
An expert lexicon approach to identifying English
phrasal verbs. In Proceedings of the 41st Annual Meet-
ing of ACL, pages 513?520.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS, pages 995?1001.
I. Sag, T. Baldwin, F. Bond, and A. Copestake. 2002.
Multiword expressions: A pain in the neck for NLP.
In Proc. of the 3rd International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing-2002), pages 1?15.
Y. Tu and D. Roth. 2011. Learning english light verb
constructions: Contextual or statistica. In Proceedings
of the ACL Workshop on Multiword Expressions: from
Parsing and Generation to the Real World.
A. Villanvicencio and A. Copestake. 2003. Verb-particle
constructions in a computational grammar of English.
In Proceedings of the 9th International Conference on
HPSG, pages 357?371.
A. Villavicencio. 2003. Verb-particle constructions and
lexical resources. In Proceedings of the ACL 2003
Workshop on Multiword Expressions: Analysis, Acqui-
sition and Treatment, pages 57?64.
A. Villavicencio, 2006. Computational Linguistics Di-
mensions of the Syntax and Semantics of Prepositions,
chapter Verb-Particel Constructions in the World Wide
Web. Springer.
69
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 81?88
Manchester, August 2008
Baby SRL: Modeling Early Language Acquisition.
Michael Connor
Department of Computer Science
University of Illinois
connor2@uiuc.edu
Yael Gertner
Beckman Institute
University of Illinois
ygertner@cyrus.psych.uiuc.edu
Cynthia Fisher
Department of Psychology
University of Illinois
cfisher@cyrus.psych.uiuc.edu
Dan Roth
Department of Computer Science
University of Illinois
danr@uiuc.edu
Abstract
A fundamental task in sentence compre-
hension is to assign semantic roles to sen-
tence constituents. The structure-mapping
account proposes that children start with
a shallow structural analysis of sentences:
children treat the number of nouns in the
sentence as a cue to its semantic predicate-
argument structure, and represent language
experience in an abstract format that per-
mits rapid generalization to new verbs. In
this paper, we tested the consequences of
these representational assumptions via ex-
periments with a system for automatic se-
mantic role labeling (SRL), trained on a
sample of child-directed speech. When
the SRL was presented with representa-
tions of sentence structure consisting sim-
ply of an ordered set of nouns, it mim-
icked experimental findings with toddlers,
including a striking error found in children.
Adding features representing the position
of the verb increased accuracy and elim-
inated the error. We show the SRL sys-
tem can use incremental knowledge gain
to switch from error-prone noun order fea-
tures to a more accurate representation,
demonstrating a possible mechanism for
this process in child development.
1 Introduction
How does the child get started in learning to in-
terpret sentences? The structure-mapping view
of early verb and syntax acquisition proposes that
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
children start with a shallow structural analysis of
sentences: children treat the number of nouns in
the sentence as a cue to its semantic predicate-
argument structure (Fisher, 1996), and represent
language experience in an abstract format that per-
mits rapid generalization to new verbs (Gertner et
al., 2006).
The structure-mapping account makes strong
predictions. First, as soon as children can identify
some nouns, they should interpret transitive and in-
transitive sentences differently, simply by assign-
ing a distinct semantic role to each noun in the sen-
tence. Second, language-specific syntactic learn-
ing should transfer rapidly to new verbs. Third,
some striking errors of interpretation can occur.
In ?Fred and Ginger danced?, an intransitive verb
is presented with two nouns. If children interpret
any two-noun sentence as if it were transitive, they
should be fooled into interpreting the order of two
nouns in such conjoined-subject intransitive sen-
tences as conveying agent-patient role information.
Experiments with young children support these
predictions. First, 21-month-olds use the number
of nouns to understand sentences containing new
verbs (Yuan et al, 2007). Second, 21-month-olds
generalize what they have learned about English
transitive word-order to sentences containing new
verbs: Children who heard ?The girl is gorping the
boy? interpreted the girl as an agent and the boy as
a patient (Gertner et al, 2006). Third, 21-month-
olds make the predicted error, treating intransitive
sentences containing two nouns as if they were
transitive: they interpret the first noun in ?The girl
and the boy are gorping? as an agent and the sec-
ond as a patient (Gertner and Fisher, 2006). This
error is short-lived. By 25 months, children add
new features to their representations of sentences,
and interpret conjoined-subject intransitives differ-
81
ently from transitives (Naigles, 1990).
These experimental results shed light on what
syntactic information children might have avail-
able for early sentence comprehension, but do not
rule out the possibility that children?s early per-
formance is based on a more complex underlying
system. In this paper, we tested the consequences
of our representational assumptions by perform-
ing experiments with a system for automatic se-
mantic role labeling (SRL), whose knowledge of
sentence structure is under our control. Com-
putational models of semantic role labeling learn
to identify, for each verb in a sentence, all con-
stituents that fill a semantic role, and to determine
their roles. We adopt the architecture proposed
by Roth and colleagues (Punyakanok et al, 2005),
limiting the classifier?s features to a set of lexical
features and shallow structural features suggested
by the structure-mapping account. Learning abil-
ity is measured by the level of SRL accuracy and,
more importantly, the types of errors made by the
system on sentences containing novel verbs. Test-
ing these predictions on the automatic SRL pro-
vides us with a demonstration that it is possible to
learn how to correctly assign semantic roles based
only on these very simple cues.
From an NLP perspective this feature study pro-
vides evidence for the efficacy of alternative, sim-
pler syntactic representations in gaining an initial
foothold on sentence interpretation. It is clear that
human learners do not begin interpreting sentences
in possession of full part-of-speech tagging, or full
parse trees. By building a model that uses shal-
low representations of sentences and mimics fea-
tures of language development in children, we can
explore the nature of initial representations of syn-
tactic structure and build more complex features
from there, further mimicking child development.
2 Learning Model
We trained a simplified SRL classifier (Baby SRL)
with sets of features derived from the structure-
mapping account. Our test used novel verbs to
mimic sentences presented in experiments with
children. Our learning task is similar to the full
SRL task (Carreras and M`arquez, 2004), except
that we classify the roles of individual words rather
than full phrases. A full automatic SRL system
(e.g. (Punyakanok et al, 2005)) typically involves
multiple stages to 1) parse the input, 2) identify ar-
guments, 3) classify those arguments, and then 4)
run inference to make sure the final labeling for the
full sentence does not violate any linguistic con-
straints. Our simplified SRL architecture (Baby
SRL) essentially replaces the first two steps with
heuristics. Rather than identifying arguments via
a learned classifier with access to a full syntac-
tic parse, the Baby SRL treats each noun in the
sentence as a candidate argument and assigns a
semantic role to it. A simple heuristic collapsed
compound or sequential nouns to their final noun:
an approximation of the head noun of the noun
phrase. For example, ?Mr. Smith? was treated
as the single noun ?Smith?. Other complex noun
phrases were not simplified in this way. Thus,
a phrase such as ?the toy on the floor? would be
treated as two separate nouns, ?toy? and ?floor?.
This represents the assumption that young children
know ?Mr. Smith? is a single name, but they do not
know all the predicating terms that may link mul-
tiple nouns into a single noun phrase. The simpli-
fied learning task of the Baby SRL implements a
key assumption of the structure-mapping account:
that at the start of multiword sentence comprehen-
sion children can tell which words in a sentence are
nouns (Waxman and Booth, 2001), and treat each
noun as a candidate argument.
Feedback is provided based on annotation in
Propbank style: in training, each noun receives the
role label of the phrase that noun is part of. Feed-
back is given at the level of the macro-role (agent,
patient, etc., labeled A0-A4 for core arguments,
and AM-* adjuncts). We also introduced a NO la-
bel for nouns that are not part of any argument.
For argument classification we use a linear clas-
sifier trained with a regularized perceptron update
rule (Grove and Roth, 2001). This learning algo-
rithm provides a simple and general linear clas-
sifier that has been demonstrated to work well in
other text classification tasks, and allows us to in-
spect the weights of key features to determine their
importance for classification. The Baby SRL does
not use inference for the final classification. In-
stead it classifies every argument independently;
thus multiple nouns can have the same role.
2.1 Training
The training data were samples of parental speech
to one child (?Eve?; (Brown, 1973), available
via Childes (MacWhinney, 2000)). We trained
on parental utterances in samples 9 through 20,
recorded at child age 21-27 months. All verb-
82
containing utterances without symbols indicating
long pauses or unintelligible words were automat-
ically parsed with the Charniak parser (Charniak,
1997) and annotated using an existing SRL sys-
tem (Punyakanok et al, 2005). In this initial pass,
sentences with parsing errors that misidentified ar-
gument boundaries were excluded. Final role la-
bels were hand-corrected using the Propbank an-
notation scheme (Kingsbury and Palmer, 2002).
The child-directed speech (CDS) training set con-
sisted of about 2200 sentences, of which a majority
had a single verb and two nouns to be labeled
1
. We
used the annotated CDS training data to train our
Baby SRL, converting labeled phrases to labeled
nouns in the manner described above.
3 Experimental Results
To evaluate the Baby SRL we tested it with sen-
tences like those used for the experiments with
children described above. All test sentences con-
tained a novel verb (?gorp?). We constructed two
test sentence templates: ?A gorps B? and ?A and B
gorp?, where A and B were replaced with nouns
that appeared more than twice in training. We
filled the A and B slots by sampling nouns that
occurred roughly equally as the first and second
of two nouns in the training data. This procedure
was adopted to avoid ?building in? the predicted er-
ror by choosing A and B nouns biased toward an
agent-patient interpretation. For each test sentence
template we built a test set of 100 sentences by ran-
domly sampling nouns in this fashion.
The test sentences with novel verbs ask whether
the classifier transfers its learning about argument
role assignment to unseen verbs. Does it as-
sume the first of two nouns in a simple transi-
tive sentence (?A gorps B?) is the agent (A0) and
the second is the patient (A1)? Does it over-
generalize this rule to two-noun intransitives (?A
and B gorp?), mimicking children?s behavior? We
used two measures of success, one to assess clas-
sification accuracy, and the other to assess the
predicted error. We used a per argument F1 for
classification accuracy, with F1 based on correct
identification of individual nouns rather than full
phrases. Here precision is defined as the propor-
tion of nouns that were given the correct label
based on the argument they belong to, and recall
is the proportion of complete arguments for which
1
Corpus available at http://L2R.cs.uiuc.edu/
?
cogcomp/data.php
some noun in that argument was correctly labeled.
The desired labeling for ?A gorps B? is A0 for the
first argument and A1 for the second; for ?A and
B gorp? both arguments should be A0. To mea-
sure predicted errors we also report the proportion
of test sentences classified with A0 first and A1
second (%A0A1). This labeling is a correct gener-
alization for the novel ?A gorps B? sentences, but
is an overgeneralization for ?A and B gorp.?
3.1 Noun Pattern
The basic feature we propose is the noun pattern
feature. We hypothesize that children use the num-
ber and order of nouns to represent argument struc-
ture. To encode this we created a feature (NPat-
tern) that indicates how many nouns there are in
the sentence and which noun the target is. For ex-
ample, in our two-noun test sentences noun A has
the feature ? N? active indicating that it is the first
noun of two. Likewise for B the feature ?N ? is ac-
tive, indicating that it is the second of two nouns.
This feature is easy to compute once nouns are
identified, and does not require fine-grained dis-
tinctions between types of nouns or any other part
of speech. Table 1 shows the initial feature pro-
gression that involves this feature. The baseline
system (feature set 1) uses lexical features only:
the target noun and the root form of the predicate.
We first tested the hypothesis that children use
the NPattern features to distinguish different noun
arguments, but only for specific verbs. The NPat-
tern&V features are conjunctions of the target verb
and the noun pattern, and these are added to the
word features to form feature set 2. Now every
example has three features active: target noun, tar-
get predicate, and a NPattern&V feature indicating
?the target is the first of two nouns and the verb
is X.? This feature does not improve results on the
novel ?A gorps B? test set, or generate the predicted
error with the ?A and B gorp? test set, because the
verb-specific NPattern&V features provide no way
to generalize to unseen verbs.
We next tested the NPattern feature alone, with-
out making it verb-specific (feature set 3). The
noun pattern feature was added to the word fea-
tures and again each example had three features ac-
tive: target noun, target predicate, and the target?s
noun-pattern feature (first of two, second of three,
etc.). The abstract NPattern feature allows the
Baby SRL to generalize to new verbs: it increases
the system?s tendency to predict that the first of two
83
CHILDES WSJ
Unbiased Noun Choice Biased Noun Choice Biased Noun Choice
A gorps B A and B gorp A gorps B A and B gorp A gorps B A and B gorp
Features F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A1
1. Words 0.59 0.38 0.46 0.38 0.80 0.65 0.53 0.65 0.57 0.31 0.37 0.31
2. NPattern&V 0.53 0.28 0.54 0.28 0.81 0.67 0.53 0.67 0.56 0.31 0.39 0.31
3. NPattern 0.83 0.65 0.33 0.65 0.96 0.92 0.46 0.92 0.67 0.44 0.37 0.44
4. NPattern + NPattern&V 0.83 0.65 0.33 0.65 0.95 0.90 0.45 0.90 0.73 0.53 0.44 0.53
5. + VPosition 0.99 0.96 0.98 0.00 1.00 1.00 0.99 0.01 0.94 0.88 0.69 0.39
Table 1: Experiments showing the efficacy of Noun Pattern features for determining agent/patient roles in
simple two-noun sentences. The novel verb test sets assess whether the Baby SRL generalizes transitive
argument prediction to unseen verbs in the case of ?A gorps B? (increasing %A0A1 and thus F1), and
overgeneralizes in the case of ?A and B gorp? (increasing %A0A1, which is an error). By varying the
sampling method for creating the test sentences we can start with a biased or unbiased lexical baseline,
demonstrating that the noun pattern features still improve over knowledge that can be contained in
typical noun usage. The simple noun pattern features are still effective at learning this pattern when
trained with more complex Wall Street Journal training data.
nouns is A0 and the second of two nouns is A1 for
verbs not seen in training. Feature set 4 includes
both the abstract, non-verb-specific NPattern fea-
ture and the verb-specific version. This feature set
preserves the ability to generalize to unseen verbs;
thus the availability of the verb-specific NPattern
features during training did not prevent the abstract
NPattern features from gathering useful informa-
tion.
3.2 Lexical Cues for Role-Labeling
Thus far, the target nouns? lexical features pro-
vided little help in role labeling, allowing us to
clearly see the contribution of the proposed sim-
ple structural features. Would our structural fea-
tures produce any improvement above a more re-
alistic lexical baseline? We created a new set of
test sentences, sampling the A nouns based on the
distribution of nouns seen as the first of two nouns
in training, and the B nouns based on the distri-
bution of nouns seen as the second of two nouns.
Given this revised sampling of nouns, the words-
only baseline is strongly biased toward A0A1 (bi-
ased results for feature set 1 in table 1). This high
baseline reflects a general property of conversa-
tion: Lexical choices provide considerable infor-
mation about semantic roles. For example, the 6
most common nouns in the Eve corpus are pro-
nouns that are strongly biased in their positions
and in their semantic roles (e.g., ?you?, ?it?). De-
spite this high baseline, however, we see the same
pattern in the unbiased and biased experiments in
table 1. The addition of the NPattern features (fea-
ture set 3) substantially improves performance on
?A gorps B? test sentences, and promotes over-
generalization errors on ?A and B gorp? sentences.
3.3 More Complex Training Data
For comparison purposes we also trained the Baby
SRL on a subset of the Propbank training data
of Wall Street Journal (WSJ) text (Kingsbury and
Palmer, 2002). To approximate the simpler sen-
tences of child-directed speech we selected only
those sentences with 8 or fewer words. This
provided a training set of about 2500 sentences,
most with a single verb and two nouns to be la-
beled. The CDS and WSJ data pose similar prob-
lems for learning abstract and verb-specific knowl-
edge. However, newspaper text differs from ca-
sual speech to children in many ways, including
vocabulary and sentence complexity. One could
argue that the WSJ corpus presents a worst-case
scenario for learning based on shallow representa-
tions of sentence structure: Full passive sentences
are more common in written corpora such as the
WSJ than in samples of conversational speech, for
example (Roland et al, 2007). As a result of such
differences, two-noun sequences are less likely to
display an A0-A1 sequence in the WSJ (0.42 A0-
A1 in 2-noun sentences) than in the CDS training
data (0.67 A0-A1). The WSJ data provides a more
demanding test of the Baby SRL.
We trained the Baby SRL on the WSJ data, and
tested it using the biased lexical choices as de-
scribed above, sampling A and B nouns for novel-
verb test sentences based on the distribution of
nouns seen as the first of two nouns in training, and
as the second of two nouns, respectively. The WSJ
training produced performance strikingly similar
to the performance resulting from CDS training
(last 4 columns of Table 1). Even in this more
complex training set, the addition of the NPattern
84
features (feature set 3) improves performance on
?A gorps B? test sentences, and promotes over-
generalization errors on ?A and B gorp? sentences.
3.4 Tests with Familiar Verbs
Features Total A0 A1 A2 A4
1. Words 0.64 0.83 0.74 0.33 0.00
2. NPattern&V 0.67 0.86 0.77 0.45 0.44
3. NPattern 0.66 0.87 0.76 0.37 0.22
4. NPattern + NPattern&V 0.68 0.87 0.80 0.47 0.44
5. + VPosition 0.70 0.88 0.83 0.50 0.50
Table 2: Testing NPattern features on full SRL task
of heldout section 8 of Eve when trained on sec-
tions 9 through 20. Each result column reflects a
per argument F1.
Learning to interpret sentences depends on bal-
ancing abstract and verb-specific structural knowl-
edge. Natural linguistic corpora, including our
CDS training data, have few verbs of very high fre-
quency and a long tail of rare verbs. Frequent verbs
occur with differing argument patterns. For exam-
ple, ?have? and ?put? are frequent in the CDS data.
?Have? nearly always occurs in simple transitive
sentences that display the canonical word order of
English (e.g., ?I have cookies?). ?Put?, in contrast,
tends to appear in non-canonical sentences that do
not display an agent-patient ordering, including
imperatives (?Put it on the floor?). To probe the
Baby SRL?s ability to learn the argument-structure
preferences of familiar verbs, we tested it on a
held-out sample of CDS from the same source
(Eve sample 8, approximately 234 labeled sen-
tences). Table 2 shows the same feature progres-
sion shown previously, with the full SRL test set.
The words-only baseline (feature set 1 in Table 2)
yields fairly accurate performance, showing that
considerable success in role assignment in these
simple sentences can be achieved based on the
argument-role biases of the target nouns and the
familiar verbs. Despite this high baseline, how-
ever, we still see the benefit of simple structural
features. Adding verb-specific (feature set 2) or
abstract NPattern features (feature set 3) improves
classification performance, and the combination of
both verb-specific and abstract NPattern features
(feature set 4) yields higher performance than ei-
ther alone. The combination of abstract NPattern
features with the verb-specific versions allows the
Baby SRL both to generalize to unseen verbs, as
seen in earlier sections, and to learn the idiosyn-
crasies of known verbs.
3.5 Verb Position
The noun pattern feature results show that the
Baby SRL can learn helpful rules for argument-
role assignment using only information about the
number and order of nouns. It also makes the error
predicted by the structure-mapping account, and
documented in children, because it has no way to
represent the difference between the ?A gorps B?
and ?A and B gorp? test sentences. At some point
the learner must develop more sophisticated syn-
tactic representations that could differentiate these
two. These could include many aspects of the sen-
tence, including noun-phrase and verb-phrase mor-
phological features, and word-order features. As a
first step in examining recovery from the predicted
error, we focused on word-order features. We did
this by adding a verb position feature (VPosition)
that specifies whether the target noun is before or
after the verb. Now simple transitive sentences in
training should support the generalization that pre-
verbal nouns tend to be agents, and post-verbal
nouns tend to be patients. In testing, the Baby
SRL?s classification of the ?A gorps B? and ?A and
B gorp? sentences should diverge.
When we add verb position information (fea-
ture set 5 in table 1 and 2), performance improves
still further for transitive sentences, both with bi-
ased and unbiased test sentences. Also, for the first
time, the A0A1 pattern is predicted less often for
?A and B gorp? sentences. This error diminished
because the classifier was able to use the verb po-
sition features to distinguish these from ?A gorps
B? sentences.
Unbiased Lexical
A gorps B A and B gorp
Features F1 %A0A1 F1 %A0A1
1. Words 0.59 0.38 0.46 0.38
3. NPattern 0.83 0.65 0.33 0.65
6. VPosition 0.99 0.95 0.97 0.00
Table 3: Verb Position vs. Noun Pattern features
alone. Verb position features yield better overall
performance, but do not replicate the error on ?A
and B gorp? sentences seen with children.
Verb position alone provides another simple ab-
stract representation of sentence structure, so it
might be proposed as an equally natural initial
representation for human learners, rather than the
noun pattern features we proposed. The VPo-
sition features should also support learning and
generalization of word-order rules for interpret-
ing transitive sentences, thus reproducing some of
85
the data from children that we reviewed above.
In table 3 we compared the words-only baseline
(set 1), words and NPattern features (set 3), and a
new feature set, words and VPosition (set 6). In
terms of correct performance on novel transitive
verbs (?A gorps B?), the VPosition features out-
perform the NPattern features. This may be partly
because the same VPosition features are used in
all sentences during training, while the NPattern
features partition sentences by number of nouns,
but is also due to the fact that the verb position
features provide a more sophisticated representa-
tion of English sentence structure. Verb position
features can distinguish transitive sentences from
imperatives containing multiple post-verbal nouns,
for example. Although verb position is ultimately
a more powerful representation of word order for
English sentences, it does not accurately reproduce
a 21-month-old?s performance on all aspects of
this task. In particular, the VPosition feature does
not support the overgeneralization of the A0A1
pattern to the ?A and B gorp? test sentences. This
suggests that children?s very early sentence com-
prehension is dominated by less sophisticated rep-
resentations of word order, akin to the NPattern
features we proposed.
3.6 Informativeness vs. Availability
In the preceding sections, we modeled increases
in syntactic knowledge by building in more so-
phisticated features. The Baby SRL escaped the
predicted error on two-noun intransitive sentences
when given access to features reflecting the posi-
tion of the target noun relative to the verb. This
imposed sequence of features is useful as a starting
point, but a more satisfying approach would be to
use the Baby SRL to explore possible reasons why
NPattern features might dominate early in acquisi-
tion, even though VPosition features are ultimately
more useful for English.
In theory, a feature might be unavailable early in
acquisition because of its computational complex-
ity. For example, lexical features are presumably
less complex than relative position features such as
NPattern and VPosition. In practice, features can
also be unavailable at first because of an informa-
tional lack. Here we suggest that NPattern features
might dominate VPosition features early in acqui-
sition because the early lexicon is dominated by
nouns, and it is easier to compute position relative
to a known word than to an unknown word. Many
studies have shown that children?s early vocabu-
lary is dominated by names for objects and peo-
ple (Gentner and Boroditsky, 2001).
-0.5
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 0  5000  10000  15000  20000
 0
 30
 60
 90
 120
 150
 180
A0
-A
1
#V
erb
s
Examples
_N
_V
Known Verbs
(a) Verb threshold = 5
-0.5
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 0  5000  10000  15000  20000
 0
 30
 60
 90
 120
 150
 180
A0
-A
1
#V
erb
s
Examples
_N
_V
Known Verbs
(b) Verb threshold = 20
-0.5
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 0  5000  10000  15000  20000
 0
 30
 60
 90
 120
 150
 180
A0
-A
1
#V
erb
s
Examples
_N
_V
Known Verbs
(c) Verb threshold = 20, +verb-specific features
Figure 1: Testing the consequences of the assump-
tion that Verb Position features are only active for
familiar verbs. The figure plots the bias of the fea-
tures ? N? and ? V? to predict A0 over A1, as the
difference between the weights of these connec-
tions in the learned network. Verb position fea-
tures win out over noun pattern features as the
verb vocabulary grows. Varying the verb familiar-
ity threshold ((a) vs. (b)) and the presence versus
absence of verb-specific versions of the structural
features ((b) vs. (c)) affects how quickly the verb
position features become dominant.
To test the consequences of this proposed infor-
86
mational bottleneck on the relative weighting of
NPattern and VPosition features during training,
we modified the Baby SRL?s training procedure
such that NPattern features were always active, but
VPosition features were active during training only
when the verb in the current example had been en-
countered a critical number of times. This repre-
sents the assumption that the child can recognize
which words in the sentence are nouns, based on
lexical familiarity or morphological context (Wax-
man and Booth, 2001), but is less likely to be able
to represent position relative to the verb without
knowing the verb well.
Figure 1 shows the tendency of the NPattern fea-
ture ? N? (first of two nouns) and the VPosition
feature ? V? (pre-verbal noun) to predict the role
A0 as opposed to A1 as the difference between
the weights of these connections in the learned net-
work. Figure 1(a) shows the results when VPosi-
tion features were active whenever the target verb
had occurred at least 5 times; in Figure 1(b) the
threshold for verb familiarity was 20. In both fig-
ures we see that the VPosition features win out
over the NPattern features as the verb vocabulary
grows. Varying the degree of verb familiarity re-
quired to accurately represent VPosition features
affects how quickly the VPosition features win
out (compare Figures 1(a) and 1(b)). Figure 1(c)
shows the same analysis with a threshold of 20,
but with verb-specific as well as abstract versions
of the NPattern and the VPosition features. In this
procedure, every example started with three fea-
tures: target noun, target predicate, NPattern, and
if the verb was known, added NPattern&V, VPo-
sition, and VPosition&V. Comparing Figures 1(b)
and 1(c), we see that the addition of verb-specific
versions of the structural features also affects the
rate at which the VPosition features come to dom-
inate the NPattern features.
Thus, in training the VPosition features become
dominant as the SRL learns to recognize more
verbs. However, the VPosition features are inac-
tive when the Baby SRL encounters the novel-verb
test sentences. Since the NPattern features are ac-
tive in test, the system generates the predicted error
until the bias of the NPattern features reaches 0.
Note in figure 1(c) that when verb-specific struc-
tural features were added, the Baby SRL never
learned to entirely discount the NPattern features
within the range of training provided. This result
is reminiscent of suggestions in the psycholinguis-
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
A0
A1
Noise
Words
+NPattern+NPattern&V
+VPosition
Figure 2: Testing the ability of simple features
to cope with varying amounts of noisy feedback.
Even with noisy feedback, the noun pattern fea-
tures support learning and generalization to new
verbs of a simple agent-patient template for un-
derstanding transitive sentences. These results are
lower than those found in table 1 due to slightly
different training assumptions.
tics literature that shallow representations of syn-
tax persist in the adult parser, alongside more so-
phisticated representations (e.g., (Ferreira, 2003)).
3.7 Noisy Training
So far, the Baby SRL has only been trained with
perfect feedback. Theories of human language ac-
quisition assume that learning to understand sen-
tences is naturally a partially-supervised task: the
child uses existing knowledge of words and syntax
to assign a meaning to a sentence; the appropriate-
ness of this meaning for the referential context pro-
vides the feedback (e.g., (Pinker, 1989)). But this
feedback must be noisy. Referential scenes pro-
vide useful but often ambiguous information about
the semantic roles of sentence participants. For ex-
ample, a participant could be construed as an agent
of fleeing or as a patient being chased. In a final
set of experiments, we examined the generaliza-
tion abilities of the Baby SRL as a function of the
integrity of semantic feedback.
We provided noisy semantic-role feedback dur-
ing training by giving a randomly-selected argu-
ment label on 0 to 100% of examples. Following
this training, we tested with the ?A gorps B? test
sentences, using the unbiased noun choices.
As shown in Figure 2, feature sets including
NPattern or VPosition features yield reasonable
performance on the novel verb test sentences up to
50% noise, and promote an A0-A1 sequence over
87
the words-only baseline even at higher noise lev-
els. Thus the proposed simple structural features
are robust to noisy feedback.
4 Conclusion
The simplified SRL classifier mimicked experi-
mental results with toddlers. We structured the
learning task to ask whether shallow representa-
tions of sentence structure provided a useful ini-
tial representation for learning to interpret sen-
tences. Given representations of the number and
order of nouns in the sentence (noun pattern fea-
tures), the Baby SRL learned to classify the first
of two nouns as an agent and the second as a pa-
tient. When provided with both verb-general and
verb-specific noun pattern features, the Baby SRL
learned to balance verb-specific and abstract syn-
tactic knowledge. By treating each noun as an
argument, it also reproduced the errors children
make. Crucially, verb-position features improved
performance when added to the noun-pattern fea-
ture, but when presented alone failed to produce
the error found with toddlers. We believe that
our model can be naturally extended to support
the case in which the arguments are noun phrases
rather than single noun words and this extension is
one of the first steps we will explore next.
Acknowledgments
We would like to thank our annotators, espe-
cially Yuancheng Tu. This research is supported
by NSF grant BCS-0620257 and NIH grant R01-
HD054448.
References
Brown, R. 1973. A First Language. Harvard Univer-
sity Press, Cambridge, MA.
Carreras, X. and L. M`arquez. 2004. Introduction to
the CoNLL-2004 shared tasks: Semantic role label-
ing. In Proceedings of CoNLL-2004, pages 89?97.
Boston, MA, USA.
Charniak, E. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. National
Conference on Artificial Intelligence.
Ferreira, F. 2003. The misinterpretation of noncanoni-
cal sentences. Cognitive Psychology, 47:164?203.
Fisher, C. 1996. Structural limits on verb mapping:
The role of analogy in children?s interpretation of
sentences. Cognitive Psychology, 31:41?81.
Gentner, D. and L. Boroditsky. 2001. Individuation,
relativity and early word learning. In Bowerman, M.
and S. C. Levinson, editors, Language acquisition
and conceptual development, pages 215?256. Cam-
bridge University Press, New York.
Gertner, Y. and C. Fisher. 2006. Predicted errors in
early verb learning. In 31st Annual Boston Univer-
sity Conference on Language Development.
Gertner, Y., C. Fisher, and J. Eisengart. 2006. Learning
words and rules: Abstract knowledge of word order
in early sentence comprehension. Psychological Sci-
ence, 17:684?691.
Grove, A. and D. Roth. 2001. Linear concepts and
hidden variables. Machine Learning, 42(1/2):123?
141.
Kingsbury, P. and M. Palmer. 2002. From Treebank to
PropBank. In Proceedings of LREC-2002, Spain.
MacWhinney, B. 2000. The CHILDES project: Tools
for analyzing talk. Third Edition. Lawrence Elrbaum
Associates, Mahwah, NJ.
Naigles, L. R. 1990. Children use syntax to learn verb
meanings. Journal of Child Language, 17:357?374.
Pinker, S. 1989. Learnability and Cognition. Cam-
bridge: MIT Press.
Punyakanok, V., D. Roth, and W. Yih. 2005. The ne-
cessity of syntactic parsing for semantic role label-
ing. In Proc. of the International Joint Conference
on Artificial Intelligence (IJCAI), pages 1117?1123.
Roland, D., F. Dick, and J. L. Elman. 2007. Fre-
quency of basic english grammatical structures: A
corpus analysis. Journal of Memory and Language,
57:348?379.
Waxman, S. R. and A. Booth. 2001. Seeing pink
elephants: Fourteen-month-olds?s interpretations of
novel nouns and adjectives. Cognitive Psychology,
43:217?242.
Yuan, S., C. Fisher, Y. Gertner, and J. Snedeker. 2007.
Participants are more than physical bodies: 21-
month-olds assign relational meaning to novel tran-
sitive verbs. In Biennial Meeting of the Society for
Research in Child Development, Boston, MA.
88
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 28?36,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Annotating ESL Errors: Challenges and Rewards
Alla Rozovskaya and Dan Roth
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,danr}@illinois.edu
Abstract
In this paper, we present a corrected and error-
tagged corpus of essays written by non-native
speakers of English. The corpus contains
63000 words and includes data by learners of
English of nine first language backgrounds.
The annotation was performed at the sentence
level and involved correcting all errors in the
sentence. Error classification includes mis-
takes in preposition and article usage, errors
in grammar, word order, and word choice. We
show an analysis of errors in the annotated
corpus by error categories and first language
backgrounds, as well as inter-annotator agree-
ment on the task.
We also describe a computer program that was
developed to facilitate and standardize the an-
notation procedure for the task. The program
allows for the annotation of various types of
mistakes and was used in the annotation of the
corpus.
1 Introduction
Work on automated methods for detecting and cor-
recting context dependent mistakes (e.g., (Golding
and Roth, 1996; Golding and Roth, 1999; Carlson
et al, 2001)) has taken an interesting turn over the
last few years, and has focused on correcting mis-
takes made by non-native speakers of English. Non-
native writers make a variety of errors in grammar
and word usage. Recently, there has been a lot of
effort on building systems for detecting mistakes in
article and preposition usage (DeFelice, 2008; Eeg-
Olofsson, 2003; Gamon et al, 2008; Han et al,
2006; Tetreault and Chodorow, 2008b). Izumi et al
(2003) consider several error types, including article
and preposition mistakes, made by Japanese learn-
ers of English, and Nagata et al (2006) focus on the
errors in mass/count noun distinctions with an ap-
plication to detecting article mistakes also made by
Japanese speakers. Article and preposition mistakes
have been shown to be very common mistakes for
learners of different first language (L1) backgrounds
(Dagneaux et al, 1998; Gamon et al, 2008; Izumi
et al, 2004; Tetreault and Chodorow, 2008a), but
there is no systematic study of a whole range of er-
rors non-native writers produce, nor is it clear what
the distribution of different types of mistakes is in
learner language.
In this paper, we describe a corpus of sentences
written by English as a Second Language (ESL)
speakers, annotated for the purposes of developing
an automated system for correcting mistakes in text.
Although the focus of the annotation were errors
in article and preposition usage, all mistakes in the
sentence have been corrected. The data for anno-
tation were taken from two sources: The Interna-
tional Corpus of Learner English (ICLE, (Granger
et al, 2002a)) and Chinese Learners of English Cor-
pus (CLEC, (Gui and Yang, 2003)). The annotated
corpus includes data from speakers of nine first lan-
guage backgrounds. To our knowledge, this is the
first corpus of non-native English text (learner cor-
pus) of fully-corrected sentences from such a diverse
group of learners1. The size of the annotated corpus
is 63000 words, or 2645 sentences. While a corpus
1Possibly, except for the Cambridge Learner Corpus
http://www.cambridge.org/elt
28
of this size may not seem significant in many natu-
ral language applications, this is in fact a large cor-
pus for this field, especially considering the effort to
correct all mistakes, as opposed to focusing on one
language phenomenon. This corpus was used in the
experiments described in the companion paper (Ro-
zovskaya and Roth, 2010).
The annotation schema that we developed was
motivated by our special interest in errors in arti-
cle and preposition usage, but also includes errors
in verbs, morphology, and noun number. The cor-
pus contains 907 article corrections and 1309 prepo-
sition corrections, in addition to annotated mistakes
of other types.
While the focus of the present paper is on anno-
tating ESL mistakes, we have several goals in mind.
First, we present the annotation procedure for the
task, including an error classification schema, anno-
tation speed, and inter-annotator agreement. Sec-
ond, we describe a computer program that we de-
veloped to facilitate the annotation of mistakes in
text. Third, having such a diverse corpus allows
us to analyze the annotated data with respect to the
source language of the learner. We show the anal-
ysis of the annotated data through an overall break-
down of error types by the writer?s first language.
We also present a detailed analysis of errors in arti-
cle and preposition usage. Finally, it should be noted
that there are currently very few annotated learner
corpora available. Consequently, systems are eval-
uated on different data sets, which makes perfor-
mance comparison impossible. The annotation of
the data presented here is available2 and, thus, can
be used by researchers who obtain access to these
respective corpora3.
The rest of the paper is organized as follows.
First, we describe previous work on the annotation
of learner corpora and statistics on ESL mistakes.
Section 3 gives a description of the annotation pro-
cedure, Section 4 presents the annotation tool that
was developed for the purpose of this project and
used in the annotation. We then present error statis-
tics based on the annotated corpus across all error
types and separately for errors in article and preposi-
tion usage. Finally, in Section 6 we describe how we
2Details about the annotation are accessible from
http://L2R.cs.uiuc.edu/?cogcomp/
3The ICLE and CLEC corpora are commercially available.
evaluate inter-annotator agreement and show agree-
ment results for the task.
2 Learner Corpora and Error Tagging
In this section, we review research in the annota-
tion and error analysis of learner corpora. For a
review of learner corpus research see, for exam-
ple, (D??az-Negrillo, 2006; Granger, 2002b; Pravec,
2002). Comparative error analysis is difficult, as
there are no standardized error-tagging schemas, but
we can get a general idea about the types of errors
prevalent with such speakers. Izumi et al (2004a)
describe a speech corpus of Japanese learners of En-
glish (NICT JLE). The corpus is corrected and anno-
tated and consists of the transcripts (2 million words)
of the audio-recordings of the English oral profi-
ciency interview test. In the NICT corpus, whose
error tag set consists of 45 tags, about 26.6% of er-
rors are determiner related, and 10% are preposition
related, which makes these two error types the most
common in the corpus (Gamon et al, 2008). The
Chinese Learners of English corpus (CLEC, (Gui
and Yang, 2003)) is a collection of essays written
by Chinese learners of beginning, intermediate, and
advanced levels. This corpus is also corrected and
error-tagged, but the tagging schema does not allow
for an easy isolation of article and preposition errors.
The International Corpus of Learner English (ICLE,
(Granger et al, 2002a)) is a corpus of argumenta-
tive essays by advanced English learners. The cor-
pus contains 2 million words of writing by European
learners from 14 mother tongue backgrounds. While
the entire corpus is not error-tagged, the French sub-
part of the corpus along with other data by French
speakers of a lower level of proficiency has been an-
notated (Dagneaux et al, 1998). The most com-
mon errors for the advanced level of proficiency
were found to be lexical errors (words) (15%), regis-
ter (10%), articles (10%), pronouns (10%), spelling
(8%) , verbs (8%).
In a study of 53 post-intermediate ESOL (mi-
grant) learners in New Zealand (Bitchener et al,
2005), the most common errors were found to be
prepositions (29%), articles (20%), and verb tense
(22%). Dalgish (1985) conducted a study of er-
rors produced by ESL students enrolled at CUNY.
It was found that across students of different first
29
languages, the most common error types among
24 different error types were errors in article us-
age (28%), vocabulary error (20-25%) (word choice
and idioms), prepositions (18%), and verb-subject
agreement (15%). He also noted that the speakers of
languages without article system made considerably
more article errors, but the breakdown of other error
types across languages was surprisingly similar.
3 Annotation
3.1 Data Selection
Data for annotation were extracted from the ICLE
corpus (Granger et al, 2002a) and CLEC (Gui and
Yang, 2003). As stated in Section 2, the ICLE con-
tains data by European speakers of advanced level
of proficiency, and the CLEC corpus contains es-
says by Chinese learners of different levels of pro-
ficiency. The annotated corpus includes sentences
written by speakers of nine languages: Bulgarian,
Chinese, Czech, French, German, Italian, Polish,
Russian, and Spanish. About half of the sentences
for annotation were selected based on their scores
with respect to a 4-gram language model built using
the English Gigaword corpus (LDC2005T12). This
was done in order to exclude sentences that would
require heavy editing and sentences with near-native
fluency, sentences with scores too high or too low.
Such sentences would be less likely to benefit from
a system on preposition/article correction. The sen-
tences for annotation were a random sample out of
the remaining 80% of the data.
To collect more data for errors in preposition us-
age, we also manually selected sentences that con-
tained such errors. This might explain why the pro-
portion of preposition errors is so high in our data.
3.2 Annotation Procedure
The annotation was performed by three native
speakers of North American English, one under-
graduate and two graduate students, specializing in
foreign languages and Linguistics, with previous ex-
perience in natural language annotation. A sentence
was presented to the annotator in the context of the
essay from which it was extracted. Essay context
can become necessary, especially for the correction
of article errors, when an article is acceptable in the
context of a sentence, but is incorrect in the context
of the essay. The annotators were also encouraged
to propose more than one correction, as long as all
of their suggestions were consistent with the essay
context.
3.3 Annotation Schema
While we were primarily interested in article and
preposition errors, the goal of the annotation was to
correct all mistakes in the sentence. Thus, our er-
ror classification schema4, though motivated by our
interest in errors in article and preposition usage,
was also intended to give us a general idea about
the types of mistakes ESL students make. A better
understanding of the nature of learners? mistakes is
important for the development of a robust automated
system that detects errors and proposes corrections.
Even when the focus of a correction system is on
one language phenomenon, we would like to have
information about all mistakes in the context: Error
information around the target article or preposition
could help us understand how noisy data affect the
performance.
But more importantly, a learner corpus with er-
ror information could demonstrate how mistakes in-
teract in a sentence. A common approach to de-
tecting and correcting context-sensitive mistakes is
to deal with each phenomenon independently, but
sometimes errors cannot be corrected in isolation.
Consider, for example, the following sentences that
are a part of the corpus that we annotated.
1. ?I should know all important aspects of English.? ? ?I should
know all of the important aspects of English.?
2. ?But some of the people thought about him as a parodist of a
rhythm-n-blues singer.? ? ?But some people considered him to
be a parodist of a rhythm-n-blues singer.?
3. ?...to be a competent avionics engineer...? ? ...?to become com-
petent avionics engineers...?
4. ?...which reflect a traditional female role and a traditional attitude
to a woman...? ? ?...which reflect a traditional female role and
a traditional attitude towards women...?
5. ?Marx lived in the epoch when there were no entertainments.?
? ?Marx lived in an era when there was no entertainment.?
In the examples above, errors interact with one an-
other. In example 1, the context requires a definite
article, and the definite article, in turn, calls for the
4Our error classification was inspired by the classification
developed for the annotation of preposition errors (Tetreault and
Chodorow, 2008a).
30
preposition ?of?. In example 2, the definite article
after ?some of? is used extraneously, and deleting it
also requires deleting preposition ?of?. Another case
of interaction is caused by a word choice error: The
writer used the verb ?thought? instead of ?consid-
ered?; replacing the verb requires also changing the
syntactic construction of the verb complement. In
examples 3 and 4, the article choice before the words
?engineer? and ?woman? depends on the number
value of those nouns. To correctly determine which
article should be used, one needs to determine first
whether the context requires a singular noun ?engi-
neer? or plural ?engineers?. Finally, in example 5,
the form of the predicate in the relative clause de-
pends on the number value of the noun ?entertain-
ment?.
For the reasons mentioned above, the annotation
involved correcting all mistakes in a sentence. The
errors that we distinguish are noun number, spelling,
verb form, and word form, in addition to article and
preposition errors . All other corrections, the major-
ity of which are lexical errors, were marked as word
replacement, word deletion, and word insertion. Ta-
ble 1 gives a description of each error type.
4 Annotation Tool
In this section, we describe a computer program that
was developed to facilitate the annotation process.
The main purpose of the program is to allow an an-
notator to easily mark the type of mistake, when cor-
recting it. In addition, the tool allows us to provide
the annotator with sufficient essay context. As de-
scribed in Section 3, sentences for annotation came
from different essays, so each new sentence was usu-
ally extracted from a new context. To ensure that
the annotators preserved the meaning of the sentence
being corrected, we needed to provide them with the
essay context. A wider context could affect the an-
notator?s decision, especially when determining the
correct article choice. The tool allowed us to effi-
ciently present to the annotator the essay context for
each target sentence.
Fig. 1 shows the program interface. The sentence
for annotation appears in the white text box and the
annotator can type corrections in the box, as if work-
ing in a word processor environment. Above and be-
low the text box we can see the context boxes, where
the rest of the essay is shown. Below the lower con-
text box, there is a list of buttons. The pink buttons
and the dark green buttons correspond to different
error types, the pink buttons are for correcting arti-
cle and preposition errors, and the dark green but-
tons ? for correcting other errors. The annotator can
indicate the type of mistake being corrected by plac-
ing the cursor after the word that contains an error
and pressing the button that corresponds to this er-
ror type. Pressing on an error button inserts a pair of
delimiters after the word. The correction can then be
entered between the delimiters. The yellow buttons
and the three buttons next to the pink ones are the
shortcuts that can be used instead of typing in arti-
cles and common preposition corrections. The but-
ton None located next to the article buttons is used
for correcting cases of articles and prepositions used
superfluously. To correct other errors, the annotator
needs to determine the type of error, insert the corre-
sponding delimiters after the word by pressing one
of the error buttons and enter the correction between
the delimiters.
The annotation rate for the three annotators varied
between 30 and 40 sentences per hour.
Table 2 shows sample sentences annotated with
the tool. The proposed corrections are located inside
the delimiters and follow the word to which the cor-
rection refers. When replacing a sequence of words,
the sequence was surrounded with curly braces. This
is useful if a sequence is a multi-word expression,
such as at last.
5 Annotation Statistics
In this section, we present the results of the anno-
tation by error type and the source language of the
writer.
Table 3 shows statistics for the annotated sen-
tences by language group and error type. Because
the sub-corpora differ in size, we show the number
of errors per hundred words. In total, the annotated
corpus contains 63000 words or 2645 sentences of
learner writing. Category punctuation was not spec-
ified in the annotation, but can be easily identified
and includes insertion, deletion, and replacement of
punctuation marks. The largest error category is
word replacement, which combines deleted, inserted
words and word substitutions. This is followed by
31
Error type Description Examples
Article error Any error involving an article ?Women were indignant at [None/the] inequality
from men.?
Preposition error Any error involving a preposition ?...to change their views [to/for] the better.?
Noun number Errors involving plural/singular
confusion of a noun
?Science is surviving by overcoming the mistakes not
by uttering the [truths/truth] .?
Verb form Errors in verb tense and verb inflec-
tions
?He [write/writes] poetry.?
Word form Correct lexeme, but wrong suffix ?It is not [simply/simple] to make professional army
.?
Spelling Error in spelling ?...if a person [commited/committed] a crime...?
Word insertion, deletion,
or replacement
Other corrections that do not fall
into any of the above categories
?There is a [probability/possibility] that today?s fan-
tasies will not be fantasies tomorrow.?
Table 1: Error classification used in annotation
Figure 1: Example of a sentence for annotation as it appears in the annotation tool window. The target sentence is
shown in the white box. The surrounding essay context is shown in the brown boxes. The buttons appear below the
boxes with text: pink buttons (for marking article and preposition errors), dark green (for marking other errors), ligh t
green (article buttons) and yellow (preposition buttons).
Annotated sentence Corrected errors
1. Television becomes their life , and in many cases it replaces their real life /lives/ noun number (life ? lives)
2. Here I ca n?t $help$ but mention that all these people were either bankers or the
Heads of companies or something of that kind @nature, kind@.
word insertion (help); word replacement (kind ? kind,
nature)
3. We exterminated *have exterminated* different kinds of animals verb form (exterminated ? have exterminated)
4. ... nearly 30000 species of plants are under the <a> serious threat of disappear-
ance |disappearing|
article replacement (the ? a); word form (disappear-
ance ? disappearing)
5. There is &a& saying that laziness is the engine of the <None> progress article insertion (a); article deletion (the)
6. ...experience teaches people to strive to <for> the <None> possible things preposition replacement (to ? for); article deletion
(the)
Table 2: Examples of sentences annotated using the annotation tool. Each type of mistake is marked using a different
set of delimiters. The corrected words are enclosed in the delimiters and follow the word to which the correction
refers. In example 2, the annotator preserved the author?s choice kind and added a better choice nature.
32
Source Total Total Errors per Corrections by Error Type
language sent. words 100 words Articles Prepo- Verb Word Noun Word Spell. Word Punc.
sitions form form number order repl.
Bulgarian 244 6197 11.9 10.3% 12.1% 3.5% 3.1% 3.0% 2.0% 5.0% 46.7% 14.2%
Chinese 468 9327 15.1 12.7% 27.2% 7.9% 3.1% 4.6% 1.4% 5.4% 26.2% 11.3%
Czech 296 6570 12.9 16.3% 10.8% 5.2% 3.4% 2.7% 3.2% 8.3% 32.5% 17.5%
French 238 5656 5.8 6.7% 17.4% 2.1% 4.0% 4.6% 3.1% 9.8% 12.5% 39.8%
German 198 5086 11.4 4.0% 13.0% 4.3% 2.8% 1.9% 2.9% 4.7% 15.4% 51.0%
Italian 243 6843 10.6 5.9% 16.6% 6.4% 1.4% 3.0% 2.4% 4.6% 20.5% 39.3%
Polish 198 4642 10.1 15.1% 16.3% 4.0% 1.3% 1.3% 2.3% 2.1% 12.3% 45.2%
Russian 464 10844 13.0 19.2% 17.8% 3.7% 2.5% 2.5% 2.1% 5.0% 28.3% 18.8%
Spanish 296 7760 15.0 11.5% 14.2% 6.0% 3.8% 2.6% 1.6% 11.9% 37.7% 10.7%
All 2645 62925 12.2 12.5% 17.1% 5.2% 2.9% 3.0% 2.2% 6.5% 28.2% 22.5%
Table 3: Error statistics on the annotated data by source language and error type
the punctuation category, which comprises 22% of
all corrections. About 12% of all errors involve ar-
ticles, and prepositions comprise 17% of all errors.
We would expect the preposition category to be less
significant if we did not specifically look for such er-
rors, when selecting sentences for annotation. Two
other common categories are spelling and verb form.
Verb form combines errors in verb conjugation and
errors in verb tense. It can be observed from the
table that there is a significantly smaller proportion
of article errors for the speakers of languages that
have articles, such as French or German. Lexical
errors (word replacement) are more common in lan-
guage groups that have a higher rate of errors per
100 words. In contrast, the proportion of punctua-
tion mistakes is higher for those learners that make
fewer errors overall (cf. French, German, Italian,
and Polish). This suggests that punctuation errors
are difficult to master, maybe because rules of punc-
tuation are not generally taught in foreign language
classes. Besides, there is a high degree of variation
in the use of punctuation even among native speak-
ers.
5.1 Statistics on Article Corrections
As stated in Section 2, article errors are one of the
most common mistakes made by non-native speak-
ers of English. This is especially true for the speak-
ers of languages that do not have articles, but for ad-
vanced French speakers this is also a very common
mistake (Dagneaux et al, 1998), suggesting that ar-
ticle usage in English is a very difficult language fea-
ture to master.
Han et al (2006) show that about 13% of noun
phrases in TOEFL essays by Chinese, Japanese, and
Russian speakers have article mistakes. They also
show that learners do not confuse articles randomly
and the most common article mistakes are omissions
and superfluous article usage. Our findings are sum-
marized in Table 4 and are very similar. We also
distinguish between the superfluous use of a and
the, we allows us to observe that most of the cases
of extraneously used articles involve article the for
all language groups. In fact, extraneous the is the
most common article mistake for the majority of
our speakers. Superfluous the is usually followed
by the omission of the and the omission of a. An-
other statistic that our table demonstrates and that
was shown previously (e.g. (Dalgish, 1985)) is that
learners whose first language does not have articles
make more article mistakes: We can see from col-
umn 3 of the table that the speakers of German,
French and Italian are three to four times less likely
to make an article mistake than the speakers of Chi-
nese and all of the Slavic languages. The only ex-
ception are Spanish speakers. It is not clear whether
the higher error rate is only due to a difference in
overall language proficiency (as is apparent from the
average number of mistakes by these speakers in Ta-
ble 3) or to other factors. Finally, the last column in
the table indicates that confusing articles with pro-
nouns is a relatively common error and on average
accounts for 10% of all article mistakes5. Current
article correction systems do not address this error
type.
5An example of such confusion is ? To pay for the crimes,
criminals are put in prison?, where the is used instead of their.
33
Source Errors Errors Article mistakes by error type
language total per 100 Miss. Miss. Extr. Extr. Confu- Mult. Other
words the a the a sion labels
Bulgarian 76 1.2 9% 25% 41% 3% 8% 1% 13%
Chinese 179 1.9 20% 12% 48% 4% 7% 2% 7%
Czech 138 2.1 29% 13% 29% 9% 7% 4% 9%
French 22 0.4 9% 14% 36% 14% 0% 23% 5%
German 23 0.5 22% 9% 22% 4% 8% 9% 26%
Italian 43 0.6 16% 40% 26% 2% 9% 0% 7%
Polish 71 1.5 37% 18% 17% 8% 11% 4% 4%
Russian 271 2.5 24% 18% 31% 6% 11% 1% 9%
Spanish 134 1.7 16% 10% 51% 7% 3% 1% 10%
All 957 1.5 22% 16% 36% 6% 8% 3% 9%
Table 4: Distribution of article mistakes by error type and source language of the writer. Confusion error type refers to
confusing articles a and the. Multiple labels denotes cases where the annotator specified more than one article choice,
one of which was used by the learner. Other refers to confusing articles with possessive and demonstrative pronouns.
5.2 Statistics on Preposition Corrections
Table 5 shows statistics on errors in preposition us-
age. Preposition mistakes are classified into three
categories: replacements, insertions, and deletions.
Unlike with article errors, the most common type
of preposition errors is confusing two prepositions.
This category accounts for more than half of all er-
rors, and the breakdown is very similar for all lan-
guage groups. The fourth category in the table, with
original, refers to the preposition usages that were
found acceptable by the annotators, but with a bet-
ter suggestion provided. We distinguish this case
as a separate category because preposition usage is
highly variable, unlike, for example, article usage.
Tetreault and Chodorow (Tetreault and Chodorow,
2008a) show that agreement between two native
speakers on a cloze test targeting prepositions is
about 76%, which demonstrates that there are many
contexts that license multiple prepositions.
6 Inter-annotator Agreement
Correcting non-native text for a variety of mistakes
is challenging and requires a number of decisions on
the part of the annotator. Human language allows for
many ways to express the same idea. Furthermore, it
is possible that the corrected sentence, even when it
does not contain clear mistakes, does not sound like
a sentence produced by a native speaker. The latter
is complicated by the fact that native speakers differ
widely with respect to what constitutes acceptable
usage (Tetreault and Chodorow, 2008a).
To date, a common approach to annotating non-
native text has been to use one rater (Gamon et al,
Source Errors Errors Mistakes by error type
language total per 100 Repl. Ins. Del. With
words orig.
Bulgarian 89 1.4 58% 22% 11% 8%
Chinese 384 4.1 52% 24% 22% 2%
Czech 91 1.4 51% 21% 24% 4%
French 57 1.0 61% 9% 12% 18%
German 75 1.5 61% 8% 16% 15%
Italian 120 1.8 57% 22% 12% 8%
Polish 77 1.7 49% 18% 16% 17%
Russian 251 2.3 53% 21% 17% 9%
Spanish 165 2.1 55% 20% 19% 6%
All 1309 2.1 54% 21% 18% 7%
Table 5: Distribution of preposition mistakes by error
type and source language of the writer. With orig refers to
prepositions judged as acceptable by the annotators, but
with a better suggestion provided.
2008; Han et al, 2006; Izumi et al, 2004; Na-
gata et al, 2006). The output of human annota-
tion is viewed as the gold standard when evaluating
an error detection system. The question of reliabil-
ity of using one rater has been raised in (Tetreault
and Chodorow, 2008a), where an extensive reliabil-
ity study of human judgments in rating preposition
usage is described. In particular, it is shown that
inter-annotator agreement on preposition correction
is low (kappa value of 0.63) and that native speakers
do not always agree on whether a specific preposi-
tion constitutes acceptable usage.
We measure agreement by asking an annotator
whether a sentence corrected by another person is
correct. After all, our goal was to make the sentence
sound native-like, without enforcing that errors are
corrected in the same way. One hundred sentences
annotated by each person were selected and the cor-
34
Agreement set Rater Judged Judged
correct incorrect
Agreement set 1 Rater #2 37 63Rater #3 59 41
Agreement set 2 Rater #1 79 21Rater #3 73 27
Agreement set 3 Rater #1 83 17Rater #2 47 53
Table 6: Annotator agreement at the sentence level. The
number next to the agreement set denotes the annotator
who corrected the sentences on the first pass. Judged cor-
rect denotes the proportion of sentences in the agreement
set that the second rater did not change. Judged incorrect
denotes the proportion of sentences, in which the second
rater made corrections.
rections were applied. This corrected set was mixed
with new sentences and given to the other two anno-
tators. In this manner, each annotator received two
hundred sentences corrected by the other two anno-
tators. For each pair of the annotators, we compute
agreement based on the 100 sentences on which they
did a second pass after the initial corrections by the
third rater. To compute agreement at the sentence
level, we assign the annotated sentences to one of
the two categories: ?correct? and ?incorrect?: A sen-
tence is considered ?correct? if a rater did not make
any corrections in it on the second pass 6. Table 6
shows for each agreement set the number of sen-
tences that were corrected on the second pass. On
average, 40.8% of the agreement set sentences be-
long to the ?incorrect? category, but the proportion
of ?incorrect? sentences varies across annotators.
We also compute agreement on the two cate-
gories, ?correct? and ?incorrect?. The agreement
and the kappa values are shown in Table 7. Agree-
ment on the sentences corrected on the second pass
varies between 56% to 78% with kappa values rang-
ing from 0.16 to 0.40. The low numbers reflect the
difficulty of the task and the variability of the na-
tive speakers? judgments about acceptable usage. In
fact, since the annotation requires looking at sev-
eral phenomena, we can expect a lower agreement,
when compared to agreement rate on one language
phenomenon. Suppose rater A disagrees with rater
B on a given phenomenon with probability 1/4,
then, when there are two phenomena, the probabil-
ity that he will disagree with at least on of them is
6We ignore punctuation corrections.
Agreement set Agreement kappa
Agreement set 1 56% 0.16
Agreement set 2 78% 0.40
Agreement set 3 60% 0.23
Table 7: Agreement at the sentence level. Agreement
shows how many sentences in each agreement set were
assigned to the same category (?correct?, ?incorrect?) for
each of the two raters.
1 ? 9/16 = 7/16. And the probability goes down
with the number of phenomena.
7 Conclusion
In this paper, we presented a corpus of essays by stu-
dents of English of nine first language backgrounds,
corrected and annotated for errors. To our knowl-
edge, this is the first fully-corrected corpus that con-
tains such diverse data. We have described an anno-
tation schema, have shown statistics on the error dis-
tribution for writers of different first language back-
grounds and inter-annotator agreement on the task.
We have also described a program that was devel-
oped to facilitate the annotation process.
While natural language annotation, especially in
the context of error correction, is a challenging and
time-consuming task, research in learner corpora
and annotation is important for the development of
robust systems for correcting and detecting errors.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments. This research is partly supported by a
grant from the U.S. Department of Education.
References
J. Bitchener, S. Young and D. Cameron. 2005. The Ef-
fect of Different Types of Corrective Feedback on ESL
Student Writing. Journal of Second Language Writ-
ing.
A. J. Carlson and J. Rosen and D. Roth. 2001. Scaling
Up Context Sensitive Text Correction. IAAI, 45?50.
M. Chodorow, J. Tetreault and N-R. Han. 2007. De-
tection of Grammatical Errors Involving Prepositions.
Proceedings of the Fourth ACL-SIGSEM Workshop on
Prepositions.
E. Dagneaux, S. Denness and S. Granger. 1998.
Computer-aided Error Analysis. System, 26:163?174.
35
G. Dalgish. 1985. Computer-assisted ESL Research.
CALICO Journal, 2(2).
G. Dalgish. 1991. Computer-Assisted Error Analysis
and Courseware Design: Applications for ESL in the
Swedish Context. CALICO Journal, 9.
R. De Felice and S. Pulman. 2008. A Classifier-Based
Approach to Preposition and Determiner Error Correc-
tion in L2 English. In Proceedings of COLING-08.
A. D??az-Negrillo and J. Ferna?ndez-Dom??nguez. 2006.
Error Tagging Systems for Learner Corpora. RESLA,
19:83-102.
J. Eeg-Olofsson and O. Knuttson. 2003. Automatic
Grammar Checking for Second Language Learners -
the Use of Prepositions. In Nodalida.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W.
Dolan, D. Belenko and L. Vanderwende. 2008. Using
Contextual Speller Techniques and Language Model-
ing for ESL Error Correction. Proceedings of IJCNLP.
A. R. Golding and D. Roth. 1996. Applying Winnow
to Context-Sensitive Spelling Correction. ICML, 182?
190.
A. R. Golding and D. Roth. 1999. A Winnow based ap-
proach to Context-Sensitive Spelling Correction. Ma-
chine Learning, 34(1-3):107?130.
S. Granger, E. Dagneaux and F. Meunier. 2002. Interna-
tional Corpus of Learner English
S. Granger. 2002. A Bird?s-eye View of Learner Cor-
pus Research. Computer Learner Corpora, Second
Language Acquisition and Foreign Language Teach-
ing, Eds. S. Granger, J. Hung and S. Petch-Tyson,
Amsterdam: John Benjamins. 3?33.
S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu
Yuliaohu. (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe. (In Chinese).
N. Han, M. Chodorow and C. Leacock. 2006. Detect-
ing Errors in English Article Usage by Non-native
Speakers. Journal of Natural Language Engineering,
12(2):115?129.
E. Izumi, K. Uchimoto, T. Saiga and H. Isahara. 2003.
Automatic Error Detection in the Japanese Leaners
English Spoken Data. ACL.
E. Izumi, K. Uchimoto and H. Isahara. 2004. The
Overview of the SST Speech Corpus of Japanese
Learner English and Evaluation through the Exper-
iment on Automatic Detection of Learners? Errors.
LREC.
E. Izumi, K. Uchimoto and H. Isahara. 2004. The
NICT JLE Corpus: Exploiting the Language Learner?s
Speech Database for Research and Education. Inter-
national Journal of the Computer, the Internet and
Management, 12(2):119?125.
R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A
Feedback-Augmented Method for Detecting Errors in
the Writing of Learners of English. ACL/COLING.
N. Pravec. 2002. Survey of learner corpora. ICAME
Journal, 26:81?114.
A. Rozovskaya and D. Roth 2010. Training Paradigms
for Correcting Errors in Grammar and Usage. In Pro-
ceedings of the NAACL-HLT, Los-Angeles, CA.
J. Tetreault and M. Chodorow. 2008. Native Judgments
of Non-Native Usage: Experiments in Preposition Er-
ror Detection. COLING Workshop on Human Judg-
ments in Computational Linguistics, Manchester, UK.
J. Tetreault and M. Chodorow. 2008. The Ups and
Downs of Preposition Error Detection in ESL Writing.
COLING, Manchester, UK.
36
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Experts? Retrieval with Multiword-Enhanced Author Topic Model
Nikhil Johri Dan Roth Yuancheng Tu
Dept. of Computer Science Dept. of Linguistics
University of Illinois at Urbana-Champaign
{njohri2,danr,ytu}@illinois.edu
Abstract
In this paper, we propose a multiword-
enhanced author topic model that clusters au-
thors with similar interests and expertise, and
apply it to an information retrieval system that
returns a ranked list of authors related to a key-
word. For example, we can retrieve Eugene
Charniak via search for statistical parsing.
The existing works on author topic model-
ing assume a ?bag-of-words? representation.
However, many semantic atomic concepts are
represented by multiwords in text documents.
This paper presents a pre-computation step as
a way to discover these multiwords in the cor-
pus automatically and tags them in the term-
document matrix. The key advantage of this
method is that it retains the simplicity and
the computational efficiency of the unigram
model. In addition to a qualitative evaluation,
we evaluate the results by using the topic mod-
els as a component in a search engine. We ex-
hibit improved retrieval scores when the docu-
ments are represented via sets of latent topics
and authors.
1 Introduction
This paper addresses the problem of searching peo-
ple with similar interests and expertise without in-
putting personal names as queries. Many existing
people search engines need people?s names to do a
?keyword? style search, using a person?s name as a
query. However, in many situations, such informa-
tion is impossible to know beforehand. Imagine a
scenario where the statistics department of a univer-
sity invited a world-wide known expert in Bayesian
statistics and machine learning to give a keynote
speech; how can the department head notify all the
people on campus who are interested without spam-
ming those who are not? Our paper proposes a solu-
tion to the aforementioned scenario by providing a
search engine which goes beyond ?keyword? search
and can retrieve such information semantically. The
department head would only need to input the do-
main keyword of the keynote speaker, i.e. Bayesian
statistics, machine learning, and all professors and
students who are interested in this topic will be
retrieved. Specifically, we propose a Multiword-
enhanced Author-Topic Model (MATM), a proba-
bilistic generative model which assumes two steps
of generation process when producing a document.
Statistical topical modeling (Blei and Lafferty,
2009a) has attracted much attention recently due to
its broad applications in machine learning, text min-
ing and information retrieval. In these models, se-
mantic topics are represented by multinomial distri-
bution over words. Typically, the content of each
topic is visualized by simply listing the words in or-
der of decreasing probability and the ?meaning? of
each topic is reflected by the top 10 to 20 words in
that list. The Author-Topic Model (ATM) (Steyvers
et al, 2004; Rosen-Zvi et al, 2004) extends the ba-
sic topical models to include author information in
which topics and authors are modeled jointly. Each
author is a multinomial distribution over topics and
each topic is a multinomial distribution over words.
Our contribution to this paper is two-fold. First
of all, our model, MATM, extends the original ATM
by adding semantically coherent multiwords into the
term-document matrix to relax the model?s ?bag-of-
10
words? assumption. Each multiword is discovered
via statistical measurement and filtered by its part of
speech pattern via an off-line way. One key advan-
tage of tagging these semantic atomic units off-line,
is the retention of the flexibility and computational
efficiency in using the simpler word exchangeable
model, while providing better interpretation of the
topics author distribution.
Secondly, to the best of our knowledge, this is
the first proposal to apply the enhanced author topic
modeling in a semantic retrieval scenario, where
searching people is associated with a set of hid-
den semantically meaningful topics instead of their
names. While current search engines cannot sup-
port interactive and exploratory search effectively,
search based on our model serves very well to an-
swer a range of exploratory queries about the doc-
ument collections by semantically linking the inter-
ests of the authors to the topics of the collection, and
ultimately to the distribution of the words in the doc-
uments.
The rest of the paper is organized as follows. We
present some related work on topic modeling, the
original author-topic model and automatic phrase
discovery methods in Sec. 2. Then our model is de-
scribed in Sec. 3. Sec. 4 presents our experiments
and the evaluation of our method on expert search.
We conclude this paper in Sec. 5 with some discus-
sion and several further developments.
2 Related Work
Author topic modeling, originally proposed
in (Steyvers et al, 2004; Rosen-Zvi et al, 2004), is
an extension of another popular topic model, Latent
Dirichlet Allocation (LDA) (Blei et al, 2003), a
probabilistic generative model that can be used to
estimate the properties of multinomial observations
via unsupervised learning. LDA represents each
document as a mixture of probabilistic topics and
each topic as a multinomial distribution over words.
The Author topic model adds an author layer over
LDA and assumes that the topic proportion of a
given document is generated by the chosen author.
Both LDA and the author topic model assume
bag-of-words representation. As shown by many
previous works (Blei et al, 2003; Steyvers et al,
2004), even such unrealistic assumption can actu-
ally lead to a reasonable topic distribution with rel-
atively simple and computationally efficient infer-
ence algorithm. However, this unigram represen-
tation also poses major handicap when interpreting
and applying the hidden topic distributions. The
proposed MATM is an effort to try to leverage this
problem in author topic modeling. There have been
some works on Ngram topic modeling over the orig-
inal LDA model (Wallach, 2006; Wang and McCal-
lum, 2005; Wang et al, 2007; Griffiths et al, 2007).
However, to the best of our knowledge, this paper
is the first to embed multiword expressions into the
author topic model.
Many of these Ngram topic models (Wang and
McCallum, 2005; Wang et al, 2007; Griffiths et
al., 2007) improves the base model by adding a new
indicator variable xi to signify if a bigram should
be generated. If xi = 1, the word wi is gener-
ated from a distribution that depends only on the
previous word to form an Ngram. Otherwise, it is
generated from a distribution only on the topic pro-
portion (Griffiths et al, 2007) or both the previous
words and the latent topic (Wang and McCallum,
2005; Wang et al, 2007). However, these complex
models not only increase the parameter size to V
times larger than the size of the original LDA model
parameters (V is the size of the vocabulary of the
document collection) 1, it also faces the problem of
choosing which word to be the topic of the potential
Ngram. In many text retrieval tasks, the humongous
size of data may prevent us using such complicated
computation on-line. However, our model retains
the computational efficiency by adding a simple tag-
ging process via pre-computation.
Another effort in the current literature to interpret
the meaning of the topics is to label the topics via
a post-processing way (Mei et al, 2007; Blei and
Lafferty, 2009b; Magatti et al, 2009). For example,
Probabilistic topic labeling (Mei et al, 2007) first
extracts a set of candidate label phrases from a refer-
ence collection and represents each candidate label-
ing phrase with a multinomial distribution of words.
Then KL divergence is used to rank the most prob-
able labels for a given topic. This method needs not
only extra reference text collection, but also facing
1LDA collocation models and topic Ngram models also have
parameters for the binomial distribution of the indicator variable
xi for each word in the vocabulary.
11
the problem of finding discriminative and high cov-
erage candidate labels. Blei and Lafferty (Blei and
Lafferty, 2009b) proposed a method to annotate each
word of the corpus by its posterior word topic distri-
bution and then cast a statistical co-occurrence anal-
ysis to extract the most significant Ngrams for each
topic and visualize the topic with these Ngrams.
However, they only applied their method to basic
LDA model.
In this paper, we applied our multiword extension
to the author topic modeling and no extra reference
corpora are needed. The MATM, with an extra pre-
computing step to add meaningful multiwords into
the term-document matrix, enables us to retain the
flexibility and computational efficiency to use the
simpler word exchangeable model, while providing
better interpretation of the topics and author distri-
bution.
3 Multiword-enhanced Author-Topic
Model
The MATM is an extension of the original ATM
(Rosen-Zvi et al, 2004; Steyvers et al, 2004) by
semantically tagging collocations or multiword ex-
pressions, which represent atomic concepts in doc-
uments in the term-document matrix of the model.
Such tagging procedure enables us to retain compu-
tational efficiency of the word-level exchangeabil-
ity of the orginal ATM while provides more sensi-
ble topic distributions and better author topic coher-
ence. The details of our model are presented in Al-
gorithm 1.
3.1 Beyond Bag-of-Words Tagging
The first for loop in Algorithm 1 is the procedure
of our multiword tagging. Commonly used ngrams,
or statistically short phrases in text retrieval, or
so-called collocations in natural language process-
ing have long been studied by linguistics in vari-
ous ways. Traditional collocation discovery meth-
ods range from frequency to mean and variance,
from statistical hypothesis testing, to mutual infor-
mation (Manning and Schtze, 1999). In this pa-
per, we use a simple statistical hypothesis testing
method, namely Pearson?s chi-square test imple-
mented in Ngram Statistic Package (Banerjee and
Pedersen, 2003), enhanced by passing the candidate
phrases through some pre-defined part of speech
patterns that are likely to be true phrases. This
very simple heuristic has been shown to improve the
counting based methods significantly (Justenson and
Katz, 1995).
The ?2 test is chosen since it does not assume any
normally distributed probabilities and the essence
of this test is to compare the observed frequencies
with the frequencies expected for independence. We
choose this simple statistic method since in many
text retrieval tasks the volume of data we see al-
ways makes it impractical to use very sophisticated
statistical computations. We also focus on nominal
phrases, such as bigram and trigram noun phrases
since they are most likely to function as semantic
atomic unit to directly represent the concepts in text
documents.
3.2 Author Topic Modeling
The last three generative procedures described in Al-
gorithm 1 jointly model the author and topic infor-
mation. This generative model is adapted directly
from (Steyvers et al, 2004). Graphically, it can be
visualized as shown in Figure 1.
Figure 1: Plate notation of our model: MATM
The four plates in Fiture 1 represent topic (T), au-
thor (A), document (D) and Words in each document
(Nd) respectively. Each author is associated with a
multinomial distribution over all topics, ~?a and each
topic is a multinomial distribution over all words, ~?t.
Each of these distribution has a symmetric Dirichlet
prior over it, ~? and ~? respectively. When generat-
ing a document, an author k is first chosen according
to a uniform distribution. Then this author chooses
the topic from his/her associated multinomial distri-
bution over topics and then generates a word from
the multinomial distribution of that topic over the
12
words.
Algorithm 1: MATM: A,T ,D,N are four
plates as shown in Fig. 1. The first for loop is the
off-line process of multiword expressions. The
rest of the algorithm is the generative process of
the author topic modeling.
Data: A,T ,D,N
for all documents d ? D do
Part-of-Speech tagging ;
Bigram extraction ;
Part-of Speech Pattern Filtering ;
Add discovered bigrams into N ;
for each author a ? A do
draw a distribution over topics:
~?a ? DirT (~?) ;
for each topic t ? T do
draw a distribution over words:
~?t ? DirN (~?) ;
for each document d ? D and k authors ? d do
for each word w ? d do
choose an author k ? uniformly;
draw a topic assignment i given the
author: zk,i|k ? Multinomial(?a) ;
draw a word from the chosen topic:
wd,k,i|zk,i ? Multinomial(?zk,i) ;
MATM includes two sets of parameters. The T
topic distribution over words, ?t which is similar to
that in LDA. However, instead of a document-topic
distribution, author topic modeling has the author-
topic distribution, ?a. Using a matrix factorization
interpretation, similar to what Steyvers, Griffiths and
Hofmann have pointed out for LDA (Steyvers and
Griffiths, 2007) and PLSI (Hofmann, 1999), a word-
author co-occurrence matrix in author topic model
can be split into two parts: a word-topic matrix ?
and a topic-author matrix ?. And the hidden topic
serves as the low dimensional representation for the
content of the document.
Although the MATM is a relatively simple model,
finding its posterior distribution over these hidden
variables is still intractable. Many efficient ap-
proximate inference algorithms have been used to
solve this problem including Gibbs sampling (Grif-
fiths and Steyvers, 2004; Steyvers and Griffiths,
2007; Griffiths et al, 2007) and mean-field vari-
ational methods (Blei et al, 2003). Gibbs sam-
pling is a special case of Markov-Chain Monte Carlo
(MCMC) sampling and often yields relatively sim-
ple algorithms for approximate inference in high di-
mensional models.
In our MATM, we use a collapsed Gibbs sam-
pler for our parameter estimation. In this Gibbs
sampler, we integrated out the hidden variables ?
and ? as shown by the delta function in equation 2.
This Dirichlet delta function with a M dimentional
symmetric Dirichlet prior is defined in Equation 1.
For the current state j, the conditional probability
of drawing the kth author Kkj and the ith topic Zij
pair, given all the hyperparameters and all the obe-
served documents and authors except the current as-
signment (the exception is denoted by the symbol
?j), is defined in Equation 2.
?M (?) =
?
(
?M
)
? (M?) (1)
P (Zij ,Kkj |Wj = w,Z?j ,K?j ,W?j , Ad, ~?, ~?)
?
?(nZ+~?)
?(nZ,?j+~?)
?(nK+~?)
?(nK,?j+~?)
= n
w
i,?j+ ~?w
?V
w=1 nwi,?j+V ~?w
nik,?j+~?i
?T
i=1 nik,?j+T ~?i
(2)
And the parameter sets ? and ? can be interpreted
as sufficient statistics on the state variables of the
Markov Chain due to the Dirichlet conjugate priors
we used for the multinomial distributions. The two
formulars are shown in Equation 3 and Equation 4 in
which nwi is defined as the number of times that the
word w is generated by topic i and nik is defined as
the number of times that topic i is generated by au-
thor k. The Gibbs sampler used in our experiments
is from the Matlab Topic Modeling Toolbox 2.
?w,i =
nwi + ~?w
?V
w=1 nwi + V ~?w
(3)
?k,i =
nik + ~?i
?T
i=1 nik + T ~?i
(4)
2http://psiexp.ss.uci.edu/research/programs data/toolbox.htm
13
4 Experiments and Analysis
In this section, we describe the empirical evaluation
of our model qualitatively and quantitatively by ap-
plying our model to a text retrieval system we call
Expert Search. This search engine is intended to re-
trieve groups of experts with similar interests and ex-
pertise by inputting only general domain key words,
such as syntactic parsing, information retrieval.
We first describe the data set, the retrieval system
and the evaluation metrics. Then we present the em-
pirical results both qualitatively and quantitatively.
4.1 Data
We crawled from ACL anthology website and col-
lected seven years of annual ACL conference papers
as our corpus. The reference section is deleted from
each paper to reduce some noisy vocabulary, such
as idiosyncratic proper names, and some coding er-
rors caused during the file format conversion pro-
cess. We applied a part of speech tagger3 to tag
the files and retain in our vocabulary only content
words, i.e., nouns, verbs, adjectives and adverbs.
The ACL anthology website explicitly lists each
paper together with its title and author information.
Therefore, the author information of each paper can
be obtained accurately without extracting from the
original paper. We transformed all pdf files to text
files and normalized all author names by eliminating
their middle name initials if they are present in the
listed names. There is a total of 1,326 papers in the
collected corpus with 2, 084 authors. Then multi-
words (in our current experiments, the bigram collo-
cations) are discovered via the ?2 statistics and part
of speech pattern filtering. These multiwords are
then added into the vocabulary to build our model.
Some basic statistics about this corpus is summa-
rized in Table 1.
Two sets of results are evaluated use the retrieval
system in our experiments: one set is based on un-
igram vocabulary and the other with the vocabulary
expanded by the multiwords.
4.2 Evaluation on Expert Search
We designed a preliminary retrieval system to eval-
uate our model. The functionality of this search is
3The tagger is from:
http://l2r.cs.uiuc.edu/?cogcomp/software.php
ACL Corpus Statistics
Year range 2003-2009
Total number of papers 1,326
Total number of authors 2,084
Total unigrams 34,012
Total unigram and multiwords 205,260
Table 1: Description of the ACL seven-year collection in
our experiments
to associate words with individual authors, i.e., we
rank the joint probability of the query words and the
target author P (W,a). This probability is marginal-
ized over all topics in the model to rank all authors
in our corpus. In addition, the model assumes that
the word and the author is conditionally indepen-
dent given the topic. Formally, we define the ranking
function of our retrieval system in Equation 5:
P (W,a) =
?
wi
?i
?
t
P (wi, a|t)P (t)
=
?
wi
?i
?
t
P (wi|t)P (a|t)P (t) (5)
W is the input query, which may contain one or
more words. If a multiword is detected within the
query, it is added into the query. The final score is
the sum of all words in this query weighted by their
inverse document frequency ?i The inverse docu-
ment frequency is defined as Equation 6.
?i =
1
DF (wi)
(6)
In our experiments, we chose ten queries which
covers several most popular research areas in com-
putational linguistics and natural language process-
ing. In our unigram model, query words are treated
token by token. However, in our multiword model,
if the query contains a multiword inside our vocabu-
lary, it is treated as an additional token to expand the
query. For each query, top 10 authors are returned
from the system. We manually label the relevance
of these 10 authors based on the papers they submit-
ted to these seven-year ACL conferences collected
in our corpus. Two evaluation metrics are used to
measure the precision of the retrieving results. First
we evaluate the precision at a given cut-off rank,
namely precision at K with K ranging from 1 to 10.
14
We also calculate the average precision (AP) for
each query and the mean average precision (MAP)
for all the 10 queries. Average precision not only
takes ranking as consideration but also emphasizes
ranking relevant documents higher. Different from
precision at K, it is sensitive to the ranking and cap-
tures some recall information since it assumes the
precision of the non-retrieved documents to be zero.
It is defined as the average of precisions computed
at the point of each of the relevant documents in the
ranked list as shown in equation 7.
AP =
?n
r=1(Precision(r)? rel(r))
?
relevant documents
(7)
Currently in our experiments, we do not have a
pool of labeled authors to do a good evaluation of
recall of our system. However, as in the web brows-
ing activity, many users only care about the first sev-
eral hits of the retrieving results and precision at K
and MAP measurements are robust measurements
for this purpose.
4.3 Results and Analysis
In this section, we first examine the qualitative re-
sults from our model and then report the evaluation
on the external expert search.
4.3.1 Qualitative Coherence Analysis
As have shown by other works on Ngram topic
modeling (Wallach, 2006; Wang et al, 2007; Grif-
fiths et al, 2007), our model also demonstrated that
embedding multiword tokens into the simple author
topic model can always achieve more coherent and
better interpretable topics. We list top 15 words
from two topics of the multiword model and uni-
gram model respectively in Table 2. Unigram topics
contain more general words which can occur in ev-
ery topic and are usually less discriminative among
topics.
Our experiments also show that embedding the
multiword tokens into the model achieves better
clustering of the authors and the coherence between
authors and topics. We demonstrate this qualita-
tively by listing two examples respectively from the
multiword models and the unigram model in Table 3.
For example, for the topic on dependency pars-
ing, unigram model missed Ryan-McDonald and the
ranking of the authors are also questionable. Further
MultiWord Model Unigram Model
TOPIC 4 Topic 51
coreference-resolution resolution
antecedent antecedent
treesubstitution-grammars pronoun
completely pronouns
pronoun is
resolution information
angry antecedents
candidate anaphor
extracted syntactic
feature semantic
pronouns coreference
model anaphora
perceptual-cooccurrence definite
certain-time model
anaphora-resolution only
TOPIC 49 Topic 95
sense sense
senses senses
word-sense disambiguation
target-word word
word-senses context
sense-disambiguation ontext
nouns ambiguous
automatically accuracy
semantic-relatedness nouns
disambiguation unsupervised
provided target
ambiguous-word predominant
concepts sample
lexical-sample automatically
nouns-verbs meaning
Table 2: Comparison of the topic interpretation from the
multiword-enhanced and the unigram models. Qualita-
tively, topics with multiwords are more interpretable.
quantitative measurement is listed in our quantita-
tive evaluation section. However, qualitatively, mul-
tiword model seems less problematic.
Some of the unfamiliar author may not be easy to
make a relevance judgment. However, if we trace
all the papers the author wrote in our collected cor-
pus, many of the authors are coherently related to the
topic. We list all the papers in our corpus for three
authors from the machine translation topic derived
from the multiword model in Table 4 to demonstrate
the coherence between the author and the related
topic. However, it is also obvious that our model
missed some real experts in the corresponding field.
15
MultiWord Model Unigram Model
Topic 63 Topic 145 Topic 23 Topic 78
Word Word Word Word
translation dependency-parsing translation dependency
machine-translation dependency-tree translations head
language-model dependency-trees bilingual dependencies
statistical-machine dependency pairs structure
translations dependency-structures language structures
phrases dependency-graph machine dependent
translation-model dependency-relation parallel order
decoding dependency-relations translated word
score order monolingual left
decoder does quality does
Author Author Author Author
Shouxun-Lin Joakim-Nivre Hua-Wu Christopher-Manning
David-Chiang Jens-Nilsson Philipp-Koehn Hisami-Suzuk
Qun-Liu David-Temperley Ming-Zhou Kenji-Sagae
Philipp-Koehn Wei-He Shouxun-Lin Jens-Nilsson
Chi-Ho-Li Elijah-Mayfield David-Chiang Jinxi-Xu
Christoph-Tillmann Valentin-Jijkoun Yajuan-Lu Joakim-Nivre
Chris-Dyer Christopher-Manning Haifeng-Wang Valentin-Jijkoun
G-Haffari Jiri-Havelka Aiti-Aw Elijah-Mayfield
Taro-Watanabe Ryan-McDonald Chris-Callison-Burch David-Temperley
Aiti-Aw Andre-Martins Franz-Och Julia-Hockenmaier
Table 3: Two examples for topic and author coherece from multiword-enhanced model and unigram model. Top 10
words and authors are listed accordingly for each model.
For example, we did not get Kevin Knight for the
machine translation topic. This may be due to the
limitation of our corpus since we only collected pa-
pers from one conference in a limited time, or be-
cause usually these experts write more divergent on
various topics.
Another observation in our experiment is that
some experts with many papers may not be ranked
at the very top by our system. However, they have
pretty high probability to associate with several top-
ics. Intuitively this makes sense, since many of these
famous experts write papers with their students in
various topics. Their scores may therefore not be as
high as authors who have fewer papers in the corpus
which are concentrated in one topic.
4.3.2 Results from Expert Search
One annotator labeled the relevance of the re-
trieval results from our expert search system. The
annotator was also given all the paper titles of each
corresponding retrieved author to help make the bi-
nary judgment. We experimented with ten queries
and retrieved the top ten authors for each query.
We first used the precision at K for evaluation. we
calculate the precision at K for both of our multi-
word model and the unigram model and the results
are listed in Table 5. It is obvious that at every rank
position, the multiword model works better than the
unigram model. In order to focus more on relevant
retrieval results, we then calculate the average preci-
sion for each query and mean average precision for
both models. The results are in Table 6.
When only comparing the mean average precision
(MAP), the multiword model works better. How-
ever, when examining the average precision of each
query within these two models, the unigram model
also works pretty well with some queries. How the
query words may interact with our model deserves
further investigation.
5 Discussion and Further Development
In this paper, we extended the existing author topic
model with multiword term-document input and ap-
plied it to the domain of expert retrieval. Although
our study is preliminary, our experiments do return
16
Author Papers from ACL(03-09)
Shouxun-Lin
Log-linear Models for Word Alignment
Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation
Tree-to-String Alignment Template for Statistical Machine Translation
Forest-to-String Statistical Translation Rules
Partial Matching Strategy for Phrase-based Statistical Machine Translation
David-Chiang
A Hierarchical Phrase-Based Model for Statistical Machine Translation
Word Sense Disambiguation Improves Statistical Machine Translation
Forest Rescoring: Faster Decoding with Integrated Language Models
Fast Consensus Decoding over Translation Forests
Philipp-Koehn
Feature-Rich Statistical Translation of Noun Phrases
Clause Restructuring for Statistical Machine Translation
Moses: Open Source Toolkit for Statistical Machine Translation
Enriching Morphologically Poor Languages for Statistical Machine Translation
A Web-Based Interactive Computer Aided Translation Tool
Topics in Statistical Machine Translation
Table 4: Papers in our ACL corpus for three authors related to the ?machine translation? topic in Table 3.
Precision@K
K Multiword Model Unigram Model
1 0.90 0.80
2 0.80 0.80
3 0.73 0.67
4 0.70 0.65
5 0.70 0.64
6 0.72 0.65
7 0.71 0.64
8 0.71 0.66
9 0.71 0.66
10 0.70 0.64
Table 5: Precision at K evaluation of the multiword-
enhanced model and the unigram model.
promising results, demonstrating the effectiveness
of our model in improving coherence in topic clus-
ters. In addition, the use of the MATM for expert
retrieval returned some useful preliminary results,
which can be further improved in a number of ways.
One immediate improvement would be an exten-
sion of our corpus. In our experiments, we consid-
ered only ACL papers from the last 7 years. If we
extend our data to cover papers from additional con-
ferences, we will be able to strengthen author-topic
associations for authors who submit papers on the
same topics to different conferences. This will also
allow more prominent authors to come to the fore-
front in our search application. Such a modifica-
Average Precision (AP)
Query Multi. Mod. Uni. Mod.
Language Model 0.79 0.58
Unsupervised Learning 1.0 0.78
Supervised Learning 0.84 0.74
Machine Translation 0.95 1.0
Semantic Role Labeling 0.81 0.57
Coreference Resolution 0.59 0.72
Hidden Markov Model 0.93 0.37
Dependency Parsing 0.75 0.94
Parsing 0.81 0.98
Transliteration 0.62 0.85
MAP: 0.81 0.75
Table 6: Average Precision (AP) for each query and Mean
Average Precision (MAP) of the multiword-enhanced
model and the unigram model.
tion would require us to further increase the model?s
computational efficiency to handle huge volumes of
data encountered in real retrieval systems.
Another further development of this paper is the
addition of citation information to the model as a
layer of supervision for the retrieval system. For in-
stance, an author who is cited frequently could have
a higher weight in our system than one who isn?t,
and could occur more prominently in query results.
Finally, we can provide a better evaluation of our
system through a measure of recall and a simple
baseline system founded on keyword search of pa-
per titles. Recall can be computed via comparison to
a set of expected prominent authors for each query.
17
Acknowledgments
The research in this paper was supported by the Mul-
timodal Information Access & Synthesis Center at
UIUC, part of CCICADA, a DHS Science and Tech-
nology Center of Excellence.
References
S. Banerjee and T. Pedersen. 2003. The design, im-
plementation, and use of the Ngram Statistic Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381.
D. Blei and J. Lafferty. 2009a. Topic models. In A. Sri-
vastava and M. Sahami, editors, Text Mining: Theory
and Applications. Taylor and Francis.
D. Blei and J. Lafferty. 2009b. Visualiz-
ing topics with multi-word expressions. In
http://arxiv.org/abs/0907.1013.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topic. In Proceedings of the National Academy of Sci-
ence.
T. Griffiths, M. Steyvers, and J. Tenenbaum. 2007. Top-
ics in semantic representation. Psychological Review.
T. Hofmann. 1999. Probabilistic latent semantic index-
ing. In Proceedings of SIGIR.
J. Justenson and S. Katz. 1995. Technical terminology:
some linguistic properties and an algorithm for inden-
tification in text. Natural Language Engineering.
D. Magatti, S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In ISDA, pages 1227?
1232.
Christopher D. Manning and Hinrich Schtze. 1999.
Foundations of Statistical Natural Language Process-
ing. Cambridge, Massachusetts.
Q. Mei, X. Shen, and C. Zhai. 2007. Automatic la-
beling of multinomial topic models. In Proceedings
of the 13th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 490?
499.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. the author-topic model for authors and docu-
ments. In Proceedings of UAI.
M. Steyvers and T. Griffiths. 2007. Probabilistic topic
models. In Handbook of Latent Semantic Analysis.
Lawrence Erlbaum Associates.
M. Steyvers, P. Smyth, and T. Griffiths. 2004. Proba-
bilistic author-topic models for information discovery.
In Proceedings of KDD.
H. Wallach. 2006. Topic modeling; beyond bag
of words. In International Conference on Machine
Learning.
X. Wang and A. McCallum. 2005. A note on topical n-
grams. Technical report, University of Massachusetts.
X. Wang, A. McCallum, and X. Wei. 2007. Topical n-
grams: Phrase and topic discoery with an application
to information retrieval. In Proceedings of ICDM.
18
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 44?52,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Object Search: Supporting Structured Queries in Web Search Engines
Kim Cuong Pham?, Nicholas Rizzolo?, Kevin Small?, Kevin Chen-Chuan Chang?, Dan Roth?
University of Illinois at Urbana-Champaign?
Department of Computer Science
{kimpham2, rizzolo, kcchang, danr}@illinois.edu
Tufts University?
Department of Computer Science
kevin.small@tufts.edu
Abstract
As the web evolves, increasing quantities of
structured information is embedded in web
pages in disparate formats. For example, a
digital camera?s description may include its
price and megapixels whereas a professor?s
description may include her name, univer-
sity, and research interests. Both types of
pages may include additional ambiguous in-
formation. General search engines (GSEs)
do not support queries over these types of
data because they ignore the web document
semantics. Conversely, describing requi-
site semantics through structured queries into
databases populated by information extraction
(IE) techniques are expensive and not easily
adaptable to new domains. This paper de-
scribes a methodology for rapidly develop-
ing search engines capable of answering struc-
tured queries over unstructured corpora by uti-
lizing machine learning to avoid explicit IE.
We empirically show that with minimum ad-
ditional human effort, our system outperforms
a GSE with respect to structured queries with
clear object semantics.
1 Introduction
General search engines (GSEs) are sufficient for
fulfilling the information needs of most queries.
However, they are often inadequate for retrieving
web pages that concisely describe real world ob-
jects as these queries require analysis of both un-
structured text and structured data contained in web
pages. For example, digital cameras with specific
brand, megapixel, zoom, and price attributes might
be found on an online shopping website, or a pro-
fessor with her name, university, department, and
research interest attributes might be found on her
homepage. Correspondingly, as the web continues
to evolve from a general text corpus into a hetero-
geneous collection of documents, targeted retrieval
strategies must be developed for satisfying these
more precise information needs. We accomplish this
by using structured queries to capture the intended
semantics of a user query and learning domain spe-
cific ranking functions to represent the hidden se-
mantics of object classes contained in web pages.
It is not uncommon for a user to want to pose an
object query on the web. For example, an online
shopper might be looking for shopping pages that
sell canon digital cameras with 5 megapixels cost-
ing no more than $300. A graduate student might
be looking for homepages of computer science pro-
fessors who work in the information retrieval area.
Such users expect to get a list web pages containing
objects they are looking for, or object pages, which
we will define more precisely in later sections.
GSEs rarely return satisfactory results when the
user has a structured query in mind for two primary
reasons. Firstly, GSEs only handle keyword queries
whereas structured queries frequently involve data
field semantics (e.g. numerical constraints) and ex-
hibit field interdependencies. Secondly, since GSEs
are domain-agnostic, they will generally rank cam-
era pages utilizing the same functions as a profes-
sor?s homepage, ignoring much of the structured in-
formation specific to particular domains.
Conversely, vertical search engines (e.g. DBLife,
cazoodle.com, Rexa.info, etc.) approach this prob-
44
lem from the information extraction (IE) perspec-
tive. Instead of searching an inverted index directly,
they first extract data records from text (Kushmer-
ick et al, 1997; McCallum et al, 2000). IE solu-
tions, even with large scale techniques (Agichtein,
2005), do not scale to the entire web and cost signif-
icantly more than GSEs. Secondly, creating domain-
specific models or wrappers require labeling training
examples and human expertise for each individual
site. Thirdly, pre-extracting information lacks flexi-
bility; decisions made during IE are irrevocable, and
at query time, users may find additional value in par-
tial or noisy records that were discarded by the IE
system.
These issues motivate our novel approach for de-
signing a GSE capable of answering complex struc-
tured queries, which we refer to as Object Search.
At a high level, we search web pages containing
structured information directly over their feature in-
dex, similarly to GSEs, adding expressivity by re-
formulating the structured query such that it can be
executed on a traditional inverted index. Thus, we
avoid the expense incurred by IE approaches when
supporting new object domains. From a techni-
cal perspective, this work describes a principled ap-
proach to customizing GSEs to answer structured
queries from any domain by proposing a composi-
tional ranking model for ranking web pages with
regards to structured queries and presenting an in-
teractive learning approach that eases the process of
training for a new domain.
2 The Object Search Problem
The Object Search problem is to find the object
pages that answer a user?s object query. An object
query belongs to an object domain. An object do-
main defines a set of object attributes. An object
query is simply a set of constraints over these at-
tributes. Thus we define an object query as a tuple
of n constraints q ? c1 ? c2 ? .. ? cn, where ci is a
constraint on attribute ai. More specifically, a con-
straint ci is defined as a set of acceptable values ?i
for attribute ai; i.e. ci = (ai ? ?i). For example, an
equality constraint such as ?the brand is Canon? can
be specified as (abrand ? {Canon}) and a numeric
range constraint such as ?the price is at most $200?
can be specified as (aprice ? [0, 200]). When the
user does not care about an attribute, the constraint
is the constant true.
Given an object query, we want a set of satis-
fying object pages. Specifically, object pages are
pages that represent exactly one inherent object on
the web. Pages that list several objects such as a
department directory page or camera listing pages
are not considered object pages because even though
they mentioned the object, they do not represent any
particular object. There is often a single object page
but there are many web pages that mention the ob-
ject.
The goal of Object Search is similar to learning to
rank problems (Liu, 2009), in that its goal is to learn
a ranking function ? : D ? Q ? R that ranks any
(document, query) pairs. This is accomplished by
learning an function over a set of relevant features.
Each feature can be modeled as a function that takes
the pair and outputs a real value ? : D ? Q ? R.
For example, a term frequency feature outputs the
number of times the query appears in the document.
We define a function ? = (?1, ?2, ...?n) that takes a
(document, query) pair and outputs a vector of fea-
tures. The original ranking function can be written
as ?(d, q) = ??(?(d, q)) where ?? : Rn ? R is the
function; i.e.:
? = ?? ? ? (1)
Despite the similarities, Object Search differs
from traditional information retrieval (IR) problems
in many respects. First, IR can answer only keyword
queries whereas an object query is structured by
keyword constraints as well as numeric constraints.
Second, Object Search results are ?focused?, in the
sense that they must contain an object, as opposed
to the broad notion of relevance in IR. Finally, since
object pages of different domains might have little
in common, we cannot apply the same ranking func-
tion for different object domains.
As a consequence, in a learning to rank problem,
the set of features ? are fixed for all query. The
major concern is learning the function ??. In Object
Search settings, we expect different ? for each ob-
ject domain. Thus, we have to derive both ? and
??.
There are a number of challenges in solving these
problems. First, we need a deeper understanding of
45
structured information embedded in web pages. In
many cases, an object attribute such as professor?s
university might appear only once in his homepage.
Thus, using a traditional bag-of-words model is of-
ten insufficient, because one cannot distinguish the
professor own university from other university men-
tioned in his homepage. Second, we will need train-
ing data to train a new ranking function for each
new object domain. Thus, we require an efficient
bootstrapping method to tackle this problem. Fi-
nally, any acceptable solution must scale to the size
of the web. This requirement poses challenges for
efficient query processing and efficient ranking via
the learned ranking function.
3 Object Search Framework
In this section, we illustrate the primary intuitions
behind our aproach for an Object Search solu-
tion. We describe its architecture, which serves
as a search engine framework to support structured
queries of any domain. The technical details of ma-
jor components are left for subsequent sections.
3.1 Intuition
The main idea behind our proposed approach is that
we develop different vertical search engines to sup-
port object queries in different domains. However,
we want to keep the cost of supporting each new
domain as small as possible. The key principles to
keep the cost small are to 1) share as much as pos-
sible between search engines of different domains
and 2) automate the process as much as possible
using machine learning techniques. To illustrate
our proposed approach, we suppose that an user is
searching the web for cameras. Her object query is
q = abrand ? {canon} ? aprice ? [0, 200].
First, we have to automatically learn a function ?
that ranks web pages given an object query as de-
scribed in Section 2. We observe web pages rele-
vant to the query and notice several salient features
such as ?the word canon appears in the title?, ?the
word canon appears near manufacturer?, ?interest-
ing words that appear include powershot, eos, ixus?,
and ?a price value appears after ?$? near the word
price or sale?. Intuitively, pages containing these
features have a much higher chance of containing
the Canon camera being searched. Given labeled
training data, we can learn a ranking function that
combines these features to produce the probability
of a page containing the desired camera object.
Furthermore, we need to answer user query at
query time. We need to be able to look up these
features efficiently from our index of the web. A
na??ve method to index the web is to store a list of
web pages that have the above features, and at query
time, union all pages that have one or more features,
aggregate the score for each web page, and return
the ranked result. There are three problems with this
method. First, these features are dependent on each
object domain; thus, the size of the index will in-
crease as the number of domains grows. Second,
each time a new domain is added, a new set of fea-
tures needs to be indexed, and we have to extract
features for every single web page again. Third, we
have to know beforehand the list of camera brands,
megapixel ranges, price ranges, etc, which is infea-
sible for most object domain.
However, we observe that the above query de-
pendent features can be computed efficiently from
a query independent index. For example, whether
?the word canon appears near manufacturer? can be
computed if we index all occurrences of the words
canon and manufacturer. Similarly, the feature ?the
word canon appears in the title? can be computed if
we index all the words from web pages? title, which
only depends on the web pages themselves. Since
the words and numbers from different parts of a web
page can be indexed independently of the object do-
main, we can share them across different domains.
Thus, we follow the first principle mentioned above.
Of course, computing query dependent features
from the domain independent index is more expen-
sive than computing it from the na??ve index above.
However, this cost is scalable to the web. As a mat-
ter of fact, these features are equivalent to ?phrase
search? features in modern search engines.
Thus, at a high level, we solve the Object Search
problem by learning a domain dependent ranking
function for each object domain. We store basic do-
main independent features of the web in our index.
At query time, we compute domain dependent fea-
tures from this index and apply the ranking function
to return a ranked list of web pages. In this paper, we
focus on the learning problems, leaving the problem
of efficient query processing for future work.
46
Figure 1: Object Search Architecture
3.2 System Architecture
The main goal of our Object Search system is to en-
able searching the web with object queries. In order
to do this, the system must address the challenges
described in Section 2. From the end-user?s point
of view, the system must promptly and accurately
return web pages for their object query. From the
developer?s point of view, the system must facilitate
building a new search engine to support his object
domain of interest. The goal of the architecture is to
orchestrate all of these requirements.
Figure 1 depicts Object Search architecture. It
shows how different components of Object Search
interact with an end-user and a developer. The end-
user can issue any object query of known domains.
Each time the system receives an object query from
the end-user, it translates the query into a domain in-
dependent feature query. Then the Query Processor
executes the feature query on the inverted index, ag-
gregates the features using learned function ??, and
returns a ranked list of web pages to the user.
The developer?s job is to define his object domain
and train a ranking function for it. He does it by
incrementally training the function. He starts by an-
notating a few web pages and running a learning al-
gorithm to produce a ranking function, which is then
used to retrieve more data for the developer to anno-
tate. The process iterates until the developer is satis-
fied with his trained ranking function for the object
domain.
More specifically, the Ranking Function Learner
module learns the function ?? and ? as mentioned in
Section 2. The Query Translator instantiates ? with
user object query q, resulting in ?(q). Recall that ?
is a set of feature functions ?i. Each ?i is a function
of a (d, q) pair such as ?term frequency of ak in title?
(ak is an attribute of the object). Thus we can instan-
tiate ?(q) by replacing ak with ?k, which is part of
the query q. For example, if ?k = {canon} in the
previous example, then ?(q) is ?term frequency of
canon in title?. Thus ?(q) becomes a query indepen-
dent feature and ?(q) becomes a feature query that
can be executed in our inverted index by the Query
Processor.
4 Learning for Structured Ranking
We now describe how we learn the domain depen-
dent ranking function ?, which is the core learn-
ing aspect of Object Search. As mentioned in the
previous section, ? differs from existing learning
to rank work due to the structure in object queries.
We exploit this structure to decompose the ranking
function into several components (Section 4.1) and
combine them using a probabilistic model. Exist-
ing learning to rank methods can then be leveraged
to rank the individual components. Section 4.2 de-
scribes how we fit individual ranking scores into our
probabilistic model by calibrating their probability.
4.1 Ranking model
As stated, ? models the joint probability distribu-
tion over the space of documents and queries ? =
P (d, q). Once estimated, this distribution can rank
documents inD according to their probability of sat-
isfying q. Since we are only interested in finding
satisfying object pages, we introduce a variable ?
which indicates if the document d is an object page.
Furthermore, we introduce n variables ?i which in-
dicate whether constraint ci in the query q is satis-
fied. The probability computed by ? is then:
P (d, q) = P (?1, . . . , ?n, d)
= P (?1, . . . , ?n, d, ?)
+P (?1, . . . , ?n, d, ?)
= P (d)P (?|d)P (?1, . . . , ?n|d, ?)
+P (d)P (?|d)P (?1, . . . , ?n|d, ?)
= P (d)P (?|d)P (?1, . . . , ?n|d, ?) (2)
47
' P (?|d)
n
?
i=1
P (?i|d, ?) (3)
Equation 2 holds because non-object pages do
not satisfy the query, thus, P (?1, . . . , ?n|d, ?) = 0.
Equation 3 holds because we assume a uniform dis-
tribution over d and conditional independence over
?i given d and ?.
Thus, the rest of the problem is estimating P (?|d)
and P (?i|d, ?). The difference between these prob-
ability estimates lies in the features we use. Since ?
depends only in d but not q, we use query indepen-
dent features. Similarly, ?i only depends on d and
ci, thus we use features depending on ci and d.
4.2 Calibrating ranking probability
In theory, we can use any learning algorithm men-
tioned in (Liu, 2009)?s survey to obtain the terms in
Equation 3. In practice, however, such learning al-
gorithms often output a ranking score that does not
estimate the probability. Thus, in order to use them
in our ranking model, we must transform that rank-
ing score into a probability.
For empirical purposes, we use the averaged Per-
ceptron (Freund and Schapire, 1999) to discrimina-
tively train each component of the factored distri-
bution independently. This algorithm requires a set
of input vectors, which we obtain by applying the
relational feature functions to the paired documents
and queries. For each constraint ci, we have a fea-
ture vector xi = ?i(d, q). The algorithm produces a
weight vector of parameterswi as output. The prob-
ability of ci being satisfied by d given that d contains
an object can then be estimated with a sigmoid func-
tion as:
P (ci|d, ?) ? P (true|?i(d, q)) ?
1
1 + exp(?wTi xi)
(4)
Similarly, to estimate P (?|d), we use a fea-
ture vector that is dependent only on d. De-
noting the function as ?0, we have P (?|d) =
P (true|?0(d, q)), which can be obtained from (4).
While the sigmoid function has performed well
empirically, probabilities it produces are not cali-
brated. For better calibrated probabilities, one can
apply Platt scaling (Platt, 1999). This method intro-
duces two parameters A and B, which can be com-
puted using maximum likelihood estimation:
P (true|?i(d, q)) ?
1
1 + exp(AwTi ?i(d, q) + B)
(5)
In contrast to the sigmoid function, Platt scaling can
also be applied to methods that give un-normalized
scores such as RankSVM (Cao et al, 2006).
Substituting (4) and (5) into (3), we see that our
final learned ranking function has the form
?(d, q) =
n
?
i=0
1
(1 + exp(AiwTi ?i(d, q) + Bi))
(6)
5 Learning Based Programming
Learning plays a crucial role in developing a new ob-
ject domain. In addition to using supervised meth-
ods to learn ?, we also exploit active learning to ac-
quire training data from unlabeled web pages. The
combination of these efforts would benefit from a
unified framework and interface to machine learn-
ing. Learning Based Programming (LBP) (Roth,
2005) is such a principled framework. In this sec-
tion, we describe how we applied and extended LBP
to provide a user friendly interface for the developer
to specify features and guide the learning process.
Section 5.1 describes how we structured our frame-
work around Learning Based Java (LBJ), an instance
of LBP. Section 5.2 extends the framework to sup-
port interactive learning.
5.1 Learning Based Java
LBP is a programming paradigm for systems whose
behaviors depend on naturally occurring data and
that require reasoning about data and concepts in
ways that are hard, if not impossible, to write explic-
itly. This is exactly our situation. Not only do we
not know how to specify a ranking function for an
object query, we might not even know exactly what
features to use. Using LBP, we can specify abstract
information sources that might contribute to deci-
sions and apply a learning operator to them, thereby
letting a learning algorithm figure out their impor-
tances in a data-driven way.
Learning Based Java (LBJ) (Rizzolo and Roth,
2007) is an implementation of LBP which we used
and extended for our purposes. The most useful
abstraction in LBJ is that of the feature generation
48
function (FGF). This allows the programmer to rea-
son in terms of feature types, rather than specifying
individual features separately, and to treat them as
native building blocks in a language for constructing
learned functions. For example, instead of specify-
ing individual features such as the phrases ?profes-
sor of?,?product description?, etc., we can specify a
higher level feature type called ?bigram?, and let an
algorithm select individual features for ranking pur-
poses.
From the programming point of view, LBJ pro-
vides a clean interface and abstracts away the te-
dium of feature extraction and learning implemen-
tations. This enabled us to build our system quickly
and shorten our development cycle.
5.2 Interactive Machine Learning
We advocate an interactive training process (Fails
and Olsen, 2003), in which the developer iteratively
improves the learner via two types of interaction
(Algorithm 1).
The first type of interaction is similar to active
learning where the learner presents unlabeled in-
stances to the developer for annotation which it be-
lieves will most positively impact learning. In rank-
ing problems, top ranked documents are presented
as they strongly influence the loss function. The
small difference from traditional active learning in
our setting is that the developer assists this process
by also providing more queries other than those en-
countered in the current training set.
The second type of interaction is feature selec-
tion. We observed that feature selection contributed
significantly in the performance of the learner espe-
cially when training data is scarce. This is because
with little training data and a huge feature space, the
learner tends to over-fit. Fortunately in web search,
the features used in ranking are in natural language
and thereby intuitive to the developer. For example,
one type of feature used in ranking the university
constraint of a professor object query is the words
surrounding the query field as in ?university of ...?
or ?... university?. If the learner only sees examples
from the University of Anystate at Anytown, then
it?s likely that Anytown will have a high weight in
addition to University and of. However, the Any-
town feature will not generalize for documents from
other universities. Having background knowledge
like this, the developer can unselect such features.
Furthermore, the fact that Anytown has a high weight
is also an indication that the developer needs to pro-
vide more examples of other universities so that the
learner can generalize (the first type of interaction).
Algorithm 1 Interactive Learning Algorithm
1: The developer uses keyword search to find and
annotate an initial training set.
2: The system presents a ranked list of features
computed from labeled data.
3: The developer adds/removes features.
4: The system learns the ranking function using se-
lected features.
5: The developer issues queries and annotates top
ranked unlabeled documents returned by the
system.
6: If performance is not satisfactory, go to step 2.
The iterative algorithm starts with zero training
data and continues until the learner?s performance
reaches a satisfactory point. At step 2, the developer
is presented with a ranked list of features. To deter-
mine which features played the biggest role in the
classifier?s decision making, we use a simple rank-
ing metric called expected entropy loss (Glover et
al., 2001). Let f represent the event that a given
feature is active. Let C be the event that the given
example is classified as true. The conditional en-
tropy of the classification distribution given that
f occurs is H(C|f) ? ?P (C|f) log(P (C|f)) ?
P (C|f) log(P (C|f) and similarly, when f does not
occur, we replace f by f . The expected entropy loss
is
L(C|f) ? H(C)? E[H(C|f)]
= H(C)? (P (f)H(C|f) +
P (f)H(C|f) (7)
The intuition here is that if the classification loses
a lot of entropy when conditioned on a particular
feature, that feature must be very discriminative and
correlated with the classification itself.
It is noted that feature selection plays two impor-
tant roles in our framework. First, it avoids over-
fitting when training data is scarce, thus increas-
ing the effectiveness of our active learning protocol.
Second, since search time depends on how many
49
domain # pages train test
homepage 22.1 11.1 11
laptop 21 10.6 10.4
camera 18 9 9
random 97.8 48.9 48.8
total 158.9 79.6 79.2
Table 1: Number of web pages (in thousands) collected
for experiment
features we use to query the web pages, keeping the
number of features small will ensure that searching
is fast enough to be useful.
6 Experimental Results
In this section we present an experiment that com-
pares Object Search with keyword search engines.
6.1 Experimental Setting
Since we are the first to tackle this problem of an-
swering structured query on the web, there is no
known dataset available for our experiment. We col-
lected the data ourselves using various sources from
the web. Then we labeled search results from differ-
ent object queries using the same annotation proce-
dure described in Section 5.
We collected URLs from two main sources: the
open directory (DMOZ) and existing search en-
gines (SE). For DMOZ, we included URLs from
relevant categories. For SE, we manually entered
queries with keywords related to professors? home-
pages, laptops, and digital cameras, and included
all returned URLs. Having collected the URLs, we
crawled their content and indexed them. Table 1
summarizes web page data we have collected.
We split the data randomly into two parts, one for
training and one for testing, and created a single in-
verted index for both of them. The developer can
only see the training documents to select features
and train ranking functions. At testing time, we ran-
domly generate object queries, and evaluate on the
testing set. Since Google?s results come not from
our corpus but the whole web, it might not be fair to
compare against our small corpus. To accommodate
this, we also added Google?s results into our testing
corpus. We believe that most ?difficult? web pages
that hurt Google?s performance would have been in-
Field Keywords Example
Laptop domain
brand laptop,notebook lenovo laptop
processor ghz, processor 2.2 ghz
price $, price $1000..1100
Professor domain
name professor, re-
search professor,
faculty
research profes-
sor scott
university university, uni-
versity of
stanford
university
Table 2: Sample keyword reformulation for Google
cluded in the top Google result. Thus, they are also
available to test ours. In the future, we plan to im-
plement a local IR engine to compare against ours
and conduct a larger scale experiment to compare to
Google.
We evaluated the experiment with two different
domains: professor and laptop. We consider home-
pages and online shopping pages as object pages for
the professor and laptop domains respectively.
For each domain, we generated 5 random object
queries with different field configurations. Since
Google does not understand structured queries, we
reformulated each structured query into a simple
keyword query. We do so by pairing the query field
with several keywords. For example, a query field
abrand ? {lenovo} can be reformulated as ?lenovo
laptop?. We tried different combinations of key-
words as shown in table 2. To deal with numbers,
we use Google?s advanced search feature that sup-
ports numeric range queries1. For example, a price
constraint aprice ? [100, 200] might be reformulated
as ?price $100..200?. Since it is too expensive to
find the best keyword formulations for every query,
we picked the combination that gives the best result
for the first Google result page (Top 10 URLs).
6.2 Result
We measure the ranking performance with average
precision. Table 3 shows the results for our search
engine (OSE) and Google. Our ranking function
outperforms Google for most queries, especially in
1A numeric range written as ?100..200? is treated as a key-
word that appears everywhere a number in the range appears
50
Qry Professor LaptopOSE Google OSE Google
1 0.92 (71) 0.90(65) 0.7 (15) 0.44 (12)
2 0.83(88) 0.91(73) 0.62 (12) 0.26 (11)
3 0.51(73) 0.66(48) 0.44 (40) 0.31 (24)
4 0.42(49) 0.3(30) 0.36 (3) 0.09 (1)
5 0.91(18) 0.2(16) 0.77 (17) 0.42 (3)
Table 3: Average precision for 5 random queries. The
number of positive documents are in brackets
the laptop domain. In the professor domain, Google
wins in two queries (?UC Berkeley professor? and
?economics professors?). This suggests that in cer-
tain cases, reformulating to keyword query is a sen-
sible approach, especially if all the fields in the ob-
ject query are keywords. Even though Google can
be used to reformulate some queries, it is not clear
how and when this will succeed. Therefore, we need
a principled solution as proposed in this paper.
7 Related Work
Many recent works propose methods for supporting
structured queries on unstructured text (Jain et al,
2007), (Cafarella et al, 2007), (Gruhl et al, 2004).
These works follow a typical extract-then-query ap-
proach, which has several problems as we discussed
in section 1. (Agichtein, 2005) proposed using sev-
eral large scale techniques. Their idea of using spe-
cialized index and search engine is similar to our
work. However those methods assumes that struc-
tured data follows some textual patterns whereas our
system can flexibly handle structured object using
textual patterns as well as web page features.
Interestingly, the approach of translating struc-
tured queries to unstructured queries has been stud-
ied in (Liu et al, 2006). The main difference is
that SEMEX relies on carefully hand-tuned heuris-
tics on open-domain SQL queries while we use ma-
chine learning to do the translation on domain spe-
cific queries.
Machine Learning approaches to rank documents
have been studied extensively in IR (Liu, 2009).
Even though much of existing works can be used to
rank individual constraints in the structured query.
We proposed an effective way to aggregate these
ranking scores. Further more, existing learning to
rank works assumed a fixed set of features, whereas,
the feature set in object search depends on object
domain. As we have shown, the effectiveness of
the ranking function depends much on the set of
features. Thus, an semi-automatic method to learn
these was proposed in section 5.
Our interactive learning protocol inherits features
from existing works in Active Learning (see (Set-
tles, 2009) for a survey). (Fails and Olsen, 2003)
coined the term ?interactive machine learning? and
showed that a learner can take advantage of user in-
teraction to quickly acquire necessary training data.
(Roth and Small, 2009) proposed another interactive
learning protocol that improves upon a relation ex-
traction task by incremetally modifying the feature
representation.
Finally, this work is related to document re-
trieval mechanisms used for question answering
tasks (Voorhees, 2001) where precise retrieval meth-
ods are necessary to find documents which con-
tain specific information for answering factoids
(Agichtein et al, 2001).
8 Conclusion
We introduces the Object Search framework that
searches the web for documents containing real-
world objects. We formalized the problem as a
learning to rank for IR problem and showed an ef-
fective method to solve it. Our approach goes be-
yond the traditional bag-of-words representation and
views each web page as a set of domain independent
features. This representation enabled us to rank web
pages with respect to object query. Our experiments
showed that, with small human effort, it is possi-
ble to create specialized search engines that out-
performs GSEs on domain specific queries. More-
over, it is possible to search the web for documents
with deeper meaning, such as those found in object
pages. Our work is a small step toward semantic
search engines by handling deeper semantic queries.
Acknowledgement
This work is supported by DARPA funding under
the Bootstrap Learning Program, MIAS, a DHS-
IDS Center for Multimodal Information Access and
Synthesis at UIUC, NSF grant NSF SoD-HCER-
0613885 and a grant from Yahoo! Inc.
51
References
Eugene Agichtein, Steve Lawrence, and Luis Gravano.
2001. Learning search engine specific query trans-
formations for question answering. In WWW ?01:
Proceedings of the 10th international conference on
World Wide Web, pages 169?178, New York, NY,
USA. ACM.
Eugene Agichtein. 2005. Scaling Information Extraction
to Large Document Collections. IEEE Data Eng. Bull,
28:3.
Michael Cafarella, Christopher Re, Dan Suciu, and Oren
Etzioni. 2007. Structured Querying of Web Text Data:
A Technical Challenge. In CIDR.
Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou Huang,
and Hsiao-Wuen Hon. 2006. Adapting Ranking SVM
to Document Retrieval. In SIGIR ?06: Proceedings of
the 29th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 186?193, New York, NY, USA. ACM.
Jerry Alan Fails and Dan R. Olsen, Jr. 2003. Interactive
machine learning. In IUI ?03: Proceedings of the 8th
international conference on Intelligent user interfaces,
pages 39?45, New York, NY, USA. ACM.
Yoav Freund and Robert E. Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Machine Learning, 37(3):277?296.
Eric J. Glover, Gary W. Flake, Steve Lawrence, Andries
Kruger, David M. Pennock, William P. Birmingham,
and C. Lee Giles. 2001. Improving Category Specific
Web Search by Learning Query Modifications. Ap-
plications and the Internet, IEEE/IPSJ International
Symposium on, 0:23.
D. Gruhl, L. Chavet, D. Gibson, J. Meyer, P. Pattanayak,
A. Tomkins, and J. Zien. 2004. How to Build a Web-
Fountain: An Architecture for Very Large Scale Text
Analytics. IBM Systems Journal.
A. Jain, A. Doan, and L. Gravano. 2007. SQL Queries
Over Unstructured Text Databases. In Data Engineer-
ing, 2007. ICDE 2007. IEEE 23rd International Con-
ference on, pages 1255?1257.
N. Kushmerick, D. Weld, and R. Doorenbos. 1997.
Wrapper Induction for Information Extraction. In IJ-
CAI, pages 729?737.
Jing Liu, Xin Dong, and Alon Halevy. 2006. Answering
Structured Queries on Unstructured Data. In WebDB.
Tie-Yan Liu. 2009. Learning to Rank for Information
Retrieval. Found. Trends Inf. Retr., 3(3):225?331.
Andrew Kachites McCallum, Kamal Nigam, Jason Ren-
nie, and Kristie Seymore. 2000. Automating the Con-
struction of Internet Portals with Machine Learning.
Information Retrieval, 3(2):127?163.
J. Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparison to regularized likelihood
methods. In In Advances in Large Margin Classifiers.
MIT Press.
N. Rizzolo and D. Roth. 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Inter-
national Conference on Semantic Computing (ICSC),
pages 597?604, Irvine, California, September. IEEE.
Dan Roth and Kevin Small. 2009. Interactive feature
space construction using semantic information. In
CoNLL ?09: Proceedings of the Thirteenth Conference
on Computational Natural Language Learning, pages
66?74, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Dan Roth. 2005. Learning Based Programming. Innova-
tions in Machine Learning: Theory and Applications.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, University
of Wisconsin-Madison.
Ellen M. Voorhees. 2001. The trec question answering
track. Nat. Lang. Eng., 7(4):361?378.
52
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 18?27,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Driving Semantic Parsing from the World?s Response
James Clarke Dan Goldwasser Ming-Wei Chang Dan Roth
Department of Computer Science
University of Illinois
Urbana, IL 61820
{clarkeje,goldwas1,mchang21,danr}@illinois.edu
Abstract
Current approaches to semantic parsing,
the task of converting text to a formal
meaning representation, rely on annotated
training data mapping sentences to logi-
cal forms. Providing this supervision is
a major bottleneck in scaling semantic
parsers. This paper presents a new learn-
ing paradigm aimed at alleviating the su-
pervision burden. We develop two novel
learning algorithms capable of predicting
complex structures which only rely on a
binary feedback signal based on the con-
text of an external world. In addition we
reformulate the semantic parsing problem
to reduce the dependency of the model on
syntactic patterns, thus allowing our parser
to scale better using less supervision. Our
results surprisingly show that without us-
ing any annotated meaning representations
learning with a weak feedback signal is ca-
pable of producing a parser that is compet-
itive with fully supervised parsers.
1 Introduction
Semantic Parsing, the process of converting text
into a formal meaning representation (MR), is one
of the key challenges in natural language process-
ing. Unlike shallow approaches for semantic in-
terpretation (e.g., semantic role labeling and in-
formation extraction) which often result in an in-
complete or ambiguous interpretation of the natu-
ral language (NL) input, the output of a semantic
parser is a complete meaning representation that
can be executed directly by a computer program.
Semantic parsing has mainly been studied in the
context of providing natural language interfaces
to computer systems. In these settings the target
meaning representation is defined by the seman-
tics of the underlying task. For example, provid-
ing access to databases: a question posed in nat-
ural language is converted into a formal database
query that can be executed to retrieve information.
Example 1 shows a NL input query and its corre-
sponding meaning representation.
Example 1 Geoquery input text and output MR
?What is the largest state that borders Texas??
largest(state(next to(const(texas))))
Previous works (Zelle and Mooney, 1996; Tang
and Mooney, 2001; Zettlemoyer and Collins,
2005; Ge and Mooney, 2005; Zettlemoyer and
Collins, 2007; Wong and Mooney, 2007) employ
machine learning techniques to construct a seman-
tic parser. The learning algorithm is given a set of
input sentences and their corresponding meaning
representations, and learns a statistical semantic
parser ? a set of rules mapping lexical items and
syntactic patterns to their meaning representation
and a score associated with each rule. Given a sen-
tence, these rules are applied recursively to derive
the most probable meaning representation. Since
semantic interpretation is limited to syntactic pat-
terns identified in the training data, the learning
algorithm requires considerable amounts of anno-
tated data to account for the syntactic variations
associated with the meaning representation. An-
notating sentences with their MR is a difficult,
time consuming task; minimizing the supervision
effort required for learning is a major challenge in
scaling semantic parsers.
This paper proposes a new model and learning
paradigm for semantic parsing aimed to alleviate
the supervision bottleneck. Following the obser-
vation that the target meaning representation is to
be executed by a computer program which in turn
provides a response or outcome; we propose a re-
sponse driven learning framework capable of ex-
ploiting feedback based on the response. The feed-
back can be viewed as a teacher judging whether
the execution of the meaning representation pro-
duced the desired response for the input sentence.
18
This type of supervision is very natural in many
situations and requires no expertise, thus can be
supplied by any user.
Continuing with Example 1, the response gen-
erated by executing a database query would be
used to provide feedback. The feedback would be
whether the generated response is the correct an-
swer for the input question or not, in this case New
Mexico is the desired response.
In response driven semantic parsing, the learner
is provided with a set of natural language sen-
tences and a feedback function that encapsulates
the teacher. The feedback function informs the
learner whether its interpretation of the input sen-
tence produces the desired response. We consider
scenarios where the feedback is provided as a bi-
nary signal, correct +1 or incorrect ?1.
This weaker form of supervision poses a chal-
lenge to conventional learning methods: semantic
parsing is in essence a structured prediction prob-
lem requiring supervision for a set of interdepen-
dent decisions, while the provided supervision is
binary, indicating the correctness of a generated
meaning representation. To bridge this difference
we propose two novel learning algorithms suited
to the response driven setting.
Furthermore, to account for the many syntac-
tic variations associated with the MR, we propose
a new model for semantic parsing that allows us
to learn effectively and generalize better. Cur-
rent semantic parsing approaches extract parsing
rules mapping NL to their MR, restricting pos-
sible interpretations to previously seen syntactic
patterns. We replace the rigid inference process
induced by the learned parsing rules with a flex-
ible framework. We model semantic interpreta-
tion as a sequence of interdependent decisions,
mapping text spans to predicates and use syntac-
tic information to determine how the meaning of
these logical fragments should be composed. We
frame this process as an Integer Linear Program-
ming (ILP) problem, a powerful and flexible in-
ference framework that allows us to inject rele-
vant domain knowledge into the inference process,
such as specific domain semantics that restrict the
space of possible interpretations.
We evaluate our learning approach and model
on the well studied Geoquery domain (Zelle and
Mooney, 1996; Tang and Mooney, 2001), a
database consisting of U.S. geographical informa-
tion, and natural language questions. Our experi-
mental results show that our model with response
driven learning can outperform existing models
trained with annotated logical forms.
The key contributions of this paper are:
Response driven learning for semantic parsing
We propose a new learning paradigm for learn-
ing semantic parsers without any annotated mean-
ing representations. The supervision for learning
comes from a binary feedback signal based a re-
sponse generated by executing a meaning repre-
sentation. This type of supervision signal is nat-
ural to produce and can be acquired from non-
expert users.
Novel training algorithms Two novel train-
ing algorithms are developed within the response
driven learning paradigm. The training algorithms
are applicable beyond semantic parsing and can be
used in situations where it is possible to obtain bi-
nary feedback for a structured learning problem.
Flexible semantic interpretation process We
propose a novel flexible semantic parsing model
that can handle previously unseen syntactic varia-
tions of the meaning representation.
2 Semantic Parsing
The goal of semantic parsing is to produce a func-
tion F : X ? Z that maps from the space natural
language input sentences, X , to the space of mean-
ing representations, Z . This type of task is usu-
ally cast as a structured output prediction problem,
where the goal is to obtain a model that assigns the
highest score to the correct meaning representa-
tion given an input sentence. However, in the task
of semantic parsing, this decision relies on identi-
fying a hidden intermediate representation (or an
alignment) that captures the way in which frag-
ments of the text correspond to the meaning repre-
sentation. Therefore, we formulate the prediction
function as follows:
z? = Fw(x) = argmax
y?Y ,z?Z
wT?(x,y, z) (1)
Where ? is a feature function that describes the
relationships between an input sentence x, align-
ment y and meaning representation z. w is the
weight vector which contains the parameters of the
model. We refer to the argmax above as the in-
ference problem. The feature function combined
with the nature of the inference problem defines
the semantic parsing model. The key to producing
19
What is the largest Texas?
largest( const(texas))))
New Mexico
x:
y:
z:
r:
that bordersstate
state( next_to(
Figure 1: Example input sentence, meaning repre-
sentation, alignment and answer for the Geoquery
domain
a semantic parser involves defining a model and a
learning algorithm to obtain w.
In order to exemplify these concepts we con-
sider the Geoquery domain. Geoquery contains a
query language for a database of U.S. geograph-
ical facts. Figure 1 illustrates concrete examples
of the terminology introduce. The input sentences
x are natural language queries about U.S. geog-
raphy. The meaning representations z are logical
forms which can be executed on the database to
obtain a response which we denote with r. The
alignment y captures the associations between x
and z.
Building a semantic parser involves defining the
model (feature function ? and inference problem)
and a learning strategy to obtain weights (w) as-
sociated with the model. We defer discussion of
our model until Section 4 and first focus on our
learning strategy.
3 Structured Learning with Binary
Feedback
Previous approaches to semantic parsing have
assumed a fully supervised setting where
a training set is available consisting of ei-
ther: input sentences and logical forms
{(xl, zl)}Nl=1 (e.g., (Zettlemoyer and Collins,
2005)) or input sentences, logical forms
and a mapping between their constituents
{(xl,yl, zl)}Nl=1 (e.g., (Ge and Mooney, 2005)).
Given such training examples a weight vector w
can be learned using structured learning methods.
Obtaining, through annotation or other means, this
form of training data is an expensive and difficult
process which presents a major bottleneck for
semantic parsing.
To reduce the burden of annotation we focus
on a new learning paradigm which uses feedback
from a teacher. The feedback signal is binary
(+1,?1) and informs the learner whether a pre-
dicted logical form z? when executed on the target
Algorithm 1 Direct Approach (Binary Learning)
Input: Sentences {xl}Nl=1,
Feedback : X ?Z ? {+1, 1},
initial weight vector w
1: Bl ? {} for all l = 1, . . . , N
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = argmaxy,z w
T?(xl,y, z)
5: f = Feedback (xl, z?)
6: add (?(xl, y?, z?)/|xl|, f) to Bl
7: end for
8: w? BinaryLearn(B) where B = ?lBl
9: until no Bl has new unique examples
10: return w
domain produces the desired response or outcome.
This is a very natural method for providing super-
vision in many situations and requires no exper-
tise. For example, a user can observe the response
and provide a judgement. The general form of
the teacher?s feedback is provided by a function
Feedback : X ? Z ? {+1,?1}.
For the Geoquery domain this amounts to
whether the logical form produces the correct re-
sponse r for the input sentence. Geoquery has the
added benefit that the teacher can be automated
if we have a dataset consisting of input sentences
and response pairs {(xl, rl)}Nl=1. Feedback eval-
uates whether a logical form produces a response
matching r:
Feedback (xl, z) =
{
+1 if execute(z) = rl
?1 otherwise
We are now ready to present our learning
with feedback algorithms that operate in situations
where input sentences, {xl}Nl=1, and a teacher
feedback mechanism, Feedback , are available. We
do not assume the availability of logical forms.
3.1 Direct Approach (Binary Learning)
In general, a weight vector can be considered
good if when used in the inference problem (Equa-
tion (1)) it scores the correct logical form and
alignment (which may be hidden) higher than all
other logical forms and alignments for a given in-
put sentence. The intuition behind the direct ap-
proach is that the feedback function can be used to
subsample the space of possible structures (align-
ments and logical forms (Y ? Z)) for a given in-
put x. The feedback mechanism indicates whether
the structure is good (+1) or bad (?1). Using this
20
intuition we can cast the problem of learning a
weight vector for Equation (1) as a binary classifi-
cation problem where we directly consider struc-
tures the feedback assigns +1 as positive examples
and those assigned ?1 as negative.
We represent the input to the binary classifier
as the feature vector ?(x,y, z) normalized by the
size of the input sentence1 |x|, and the label as the
result from Feedback (x, z).
Algorithm 1 outlines the approach in detail. The
first stage of the algorithm iterates over all the
training input sentences and computes the best
logical form z? and alignment y? by solving the in-
ference problem (line 4). The feedback function
is queried (line 5) and a training example for the
binary predictor created using the normalized fea-
ture vector from the triple containing the sentence,
alignment and logical form as input and the feed-
back as the label. This training example is added
to the working set of training examples for this in-
put sentence (line 6). All the feedback training ex-
amples are used to train a binary classifier whose
weight vector is used in the next iteration (line 8).
The algorithm repeats until no new unique training
examples are added to any of the working sets for
any input sentence. Although the number of possi-
ble training examples is very large, in practice the
algorithm is efficient and converges quickly. Note
that this approach is capable of using a wide va-
riety of linear classifiers as the base learner (line
8).
A policy is required to specify the nature of
the working set of training examples (Bl) used for
training the base classifier. This is pertinent in line
6 of the algorithm. Possible policies include: al-
lowing duplicates in the working set (i.e., Bl is
a multiset), disallowing duplicates (Bl is a set),
or only allowing one example per input sentence
(?Bl? = 1). We adopt the first approach in this
paper.2
3.2 Aggressive Approach (Structured
Learning)
There is important implicit information which
the direct approach ignores. It is implicit that
when the teacher indicates an input paired with
an alignment and logical form is good (+1 feed-
1Normalization is required to ensure that each sentence
contributes equally to the binary learning problem regardless
of the sentence?s length.
2The working set Bl for each sentence may contain multi-
ple positive examples with the same and differing alignments.
Algorithm 2 Aggressive Approach (Structured
Learning)
Input: Sentences {xl}Nl=1,
Feedback : X ?Z ? {+1, 1},
initial weight vector w
1: Sl ? ? for all l = 1, . . . , N
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = argmaxy,z w
T?(xl,y, z)
5: f = Feedback (xl, z?)
6: if f is +1 then
7: Sl ? {(xl, y?, z?)}
8: end if
9: end for
10: w? StructLearn(S,?) where S = ?lSl
11: until no Sl has changed
12: return w
back) that in order to repeat this behavior all other
competing structures should be made suboptimal
(or bad). To leverage this implicit information
we adopt a structured learning strategy in which
we consider the prediction as the optimal structure
and all others as suboptimal. This is in contrast to
the direct approach where only structures that have
explicitly received negative feedback are consid-
ered subopitmal.
When a structure is found with positive feed-
back it is added to the training pool for a struc-
tured learner. We consider this approach aggres-
sive as the structured learner implicitly considers
all other structures as being suboptimal. Negative
feedback indicates that the structure should not be
added to the training pool as it will introduce noise
into the learning process.
Algorithm 2 outlines the learning in more detail.
As before, y? and z? are predicted using the cur-
rent weight vector and feedback received (lines 4
and 5). When positive feedback is received a new
training instance for a structured learner is created
from the input sentence and prediction (line 7) this
training instance replaces any previous instance
for the input sentence. When negative feedback
is received the training pool Sl is not updated. A
weight vector is learned using a structured learner
where the training data S contains at most one ex-
ample per input sentence. In the first iteration of
the outer loop the training data S will contain very
few examples. In each subsequent iteration the
newly learned weight vector allows the algorithm
to acquire new examples. This is repeated until no
21
new examples are added or changed in S.
Like the direct approach, this learning frame-
work is makes very few assumptions about the
type of structured learner used as a base learner
(line 10).3
4 Model
Semantic parsing is the process of converting a
natural language input into a formal logic repre-
sentation. This process is performed by associat-
ing lexical items and syntactic patterns with logi-
cal fragments and composing them into a complete
formula. Existing approaches rely on extracting
a set of parsing rules, mapping text constituents
to a logical representation, from annotated train-
ing data and applying them recursively to obtain
the meaning representation. Adapting to new data
is a major limitation of these approaches as they
cannot handle inputs containing syntactic patterns
which were not observed in the training data. For
example, assume the training data produced the
following set of parsing rules:
Example 2 Typical parsing rules
(1) NP [?x.capital(x)]? capital
(2) PP [ const(texas)]? of Texas
(3) NNP [ const(texas)]? Texas
(4) NP [capital(const(texas))]?
NP[?x.capital(x)] PP [ const(texas)]
At test time the parser is given the sentences in
Example 3. Despite the lexical similarity in these
examples, the semantic parser will correctly parse
the first sentence but fail to parse the second be-
cause the lexical items belong to different a syn-
tactic category (i.e., the word Texas is not part of a
preposition phrase in the second sentence).
Example 3 Syntactic variations of the same MR
Target logical form: capital(const(texas))
Sentence 1: ?What is the capital of Texas??
Sentence 2: ?What is Texas? capital??
The ability to adapt to unseen inputs is one
of the key challenges in semantic parsing. Sev-
eral works (Zettlemoyer and Collins, 2007; Kate,
2008) have addressed this issue explicitly by man-
ually defining syntactic transformation rules that
can help the learned parser generalize better. Un-
fortunately these are only partial solutions as a
3Mistake driven algorithms that do not enforce margin
constraints may not be able to generalize using this proto-
col since they will repeat the same prediction at training time
and therefore will not update the model.
manually constructed rule set cannot cover the
many syntactic variations.
Given the previous example, we observe
that it is enough to identify that the function
capital(?) and the constant const(texas)
appear in the target MR, since there is only a single
way to compose these entities into a single formula
? capital(const(texas)).
Motivated by this observation we define our
meaning derivation process over the rules of the
MR language and use syntactic information as a
way to bias the MR construction process. That
is, our inference process considers the entire space
of meaning representations irrespective of the pat-
terns observed in the training data. This is possi-
ble as the MRs are defined by a formal language
and formal grammar.4 The syntactic information
present in the natural language is used as soft ev-
idence (features) which guides the inference pro-
cess to good meaning representations.
This formulation is a major shift from existing
approaches that rely on extracting parsing rules
from the training data. In existing approaches
the space of possible meaning representations is
constrained by the patterns in the training data
and syntactic structure of the natural language in-
put. Our formulation considers the entire space of
meaning representations and allows the model to
adapt to previously unseen data and always pro-
duce a semantic interpretation by using the pat-
terns observed in the input.
We frame our semantic interpretation process
as a constrained optimization process, maximiz-
ing the objective function defined by Equation 1
which relies on extracting lexical and syntactic
features instead of parsing rules. In the remain-
der of this section we explain the components of
our inference model.
4.1 Target Meaning Representation
Following previous work, we capture the se-
mantics of the Geoquery domain using a sub-
set of first-order logic consisting of typed con-
stants and functions. There are two types: en-
tities E in the domain and numeric values N .
Functions describe a functional relationship over
types (e.g., population : E ? N ). A com-
plete logical form is constructed through func-
tional composition; in our formalism this is per-
4This is true for all meaning representations designed to
be executed by a computer system.
22
formed by the substitution operator. For ex-
ample, given the function next to(x) and
the expression const(texas), substitution re-
places the occurrence of the free variable x, with
the expression, resulting in a new logical form:
next to(const(texas)). Due to space lim-
itations we refer the reader to (Zelle and Mooney,
1996) for a detailed description of the Geoquery
domain.
4.2 Semantic Parsing as Constrained
Optimization
Recall that the goal of semantic parsing is to pro-
duce the following function (Equation (1)):
Fw(x) = argmax
y,z
wT?(x,y, z)
However, given that y and z are complex struc-
tures it is necessary to decompose the structure
into a set of smaller decisions to facilitate efficient
inference.
In order to define our decomposition we intro-
duce additional notation: c is a constituent (or
word span) in the input sentence x and D is the
set of all function and constant symbols in the do-
main. The alignment y is defined as a set of map-
pings between constituents and symbols in the do-
main y = {(c, s)} where s ? D.
We decompose the construction of an alignment
and logical form into two types of decisions:
First-order decisions. A mapping between con-
stituents and logical symbols (functions and con-
stants).
Second-order decisions. Expressing how logi-
cal symbols are composed into a complete logical
interpretation. For example, whether next to
and state forms next to(state(?)) or
state(next to(?)).
Note that for all possible logical forms and
alignments there exists a one-to-one mapping to
these decisions.
We frame the inference problem as an Integer
Linear Programming (ILP) problem (Equation (2))
in which the first-order decisions are governed by
?cs, a binary decision variable indicating that con-
stituent c is aligned with logical symbol s. And
?cs,dt capture the second-order decisions indicat-
ing the symbol t (associated with constituent d)
is an argument to function s (associated with con-
stituent c).
Fw(x) = argmax
?,?
?
c?x
?
s?D
?cs ?wT?1(x, c, s)
+
?
c,d?x
?
s,t?D
?cs,dt ?wT?2(x, c, s, d, t) (2)
It is clear that there are dependencies between
the ?-variables and ?-variables. For example,
given that ?cs,dt is active, the corresponding ?-
variables ?cs and ?dt must also be active. In order
to ensure a consistent solution we introduce a set
of constraints on Equation (2). In addition we add
constraints which leverage the typing information
inherent in the domain to eliminate logical forms
that are invalid in the Geoquery domain. For ex-
ample, the function length only accepts river
types as input. The set of constraints are:
? A given constituent can be associated with
exactly one logical symbol.
? ?cs,dt is active if and only if ?cs and ?dt are
active.
? If ?cs,dt is active, s must be a function and
the types of s and t should be consistent.
? Functional composition is directional and
acyclic.
The flexibility of ILP has previously been advan-
tageous in natural language processing tasks (Roth
and Yih, 2007) as it allows us to easily incorporate
such constraints.
4.3 Features
The inference problem defined in Equation (2)
uses two feature functions: ?1 and ?2.
First-order decision features ?1 Determining
if a logical symbol is aligned with a specific con-
stituent depends mostly on lexical information.
Following previous work (e.g., (Zettlemoyer and
Collins, 2005)) we create a small lexicon, mapping
logical symbols to surface forms.5 This lexicon is
small and only used as a starting point. Existing
approaches rely on annotated logical forms to ex-
tend the lexicon. However, in our setting we do
not have access to annotated logical forms, instead
we rely on external knowledge to supply further
5The lexicon contains on average 1.42 words per func-
tion and 1.07 words per constant. For example the function
next to has the lexical entries: borders, next, adjacent and
the constant illinois the lexical item illinois.
23
information. We add features which measure the
lexical similarity between a constituent and a logi-
cal symbol?s surface forms (as defined by the lexi-
con). Two metrics are used: stemmed word match
and a similarity metric based on WordNet (Miller
et al, 1990) which allows our model to account
for words not in the lexicon. The WordNet met-
ric measures similarity based on synonymy, hy-
ponymy and meronymy (Do et al, 2010). In the
case where the constituent is a preposition, which
are notorious for being ambiguous, we add a fea-
ture that considers the current lexical context (one
word to the left and right) in addition to word sim-
ilarity.
Second-order decision features ?2 Determin-
ing how to compose two logical symbols relies on
syntactic information, in our model we use the de-
pendency tree (Klein and Manning, 2003) of the
input sentence. Given a second-order decision
?cs,dt, the dependency feature takes the normal-
ized distance between the head words in the con-
stituents c and d. A set of features also indicate
which logical symbols are usually composed to-
gether, without considering their alignment to text.
5 Experiments
In this section we describe our experimental setup,
which includes the details of the domain, re-
sources and parameters.
5.1 Domain and Corpus
We evaluate our system on the Geoquery domain
as described previously. The domain consists of
a database and Prolog query language for U.S.
geographical facts. The corpus contains of 880
natural language queries paired with Prolog log-
ical form queries ((x, z) pairs). We follow previ-
ous approaches and transform these queries into a
functional representation. We randomly select 250
sentences for training and 250 sentences for test-
ing.6 We refer to the training set as Response 250
(R250) indicating that each example x in this data
set has a corresponding desired database response
r. We refer the testing set as Query 250 (Q250)
where the examples only contain the natural lan-
guage queries.
6Our inference problem is less constrained than previous
approaches thus we limit the training data to 250 examples
due to scalability issues. We also prune the search space by
limiting the number of logical symbol candidates per word
(on average 13 logical symbols per word).
Precision and recall are typically used as eval-
uation metrics in semantic parsing. However, as
our model inherently has the ability to map any
input sentence into the space of meaning repre-
sentations the trade off between precision and re-
call does not exist. Thus, we report accuracy: the
percentage of meaning representations which pro-
duce the correct response. This is equivalent to
recall in previous work (Wong and Mooney, 2007;
Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007).
5.2 Resources and Parameters
Feedback Recall that our learning framework
does not require meaning representation annota-
tions. However, we do require a Feedback func-
tion that informs the learner whether a predicted
meaning representation when executed produces
the desired response for a given input sentence.
We automatically generate a set of natural lan-
guage queries and response pairs {(x, r)} by exe-
cuting the annotated logical forms on the database.
Using this data we construct an automatic feed-
back function as described in Section 3.
Domain knowledge Our learning approaches
require an initial weight vector as input. In or-
der to provide an initial starting point, we initialize
the weight vector using a similar procedure to the
one used in (Zettlemoyer and Collins, 2007) to set
weights for three features and a bias term. The
weights were developed on the training set using
the feedback function to guide our choices.
Underlying Learning Algorithms In the direct
approach the base linear classifier we use is a lin-
ear kernel Support Vector Machine with squared-
hinge loss. In the aggressive approach we de-
fine our base structured learner to be a structural
Support Vector Machine with squared-hinge loss
and use hamming distance as the distance func-
tion. We use a custom implementation to op-
timize the objective function using the Cutting-
Plane method, this allows us to parrallelize the
learning process by solving the inference problem
for multiple training examples simultaneously.
6 Results
Our experiments are designed to answer three
questions:
1. Is it possible to learn a semantic parser with-
out annotated logical forms?
24
Algorithm R250 Q250
NOLEARN 22.2 ?
DIRECT 75.2 69.2
AGGRESSIVE 82.4 73.2
SUPERVISED 87.6 80.4
Table 1: Accuracy of learned models on R250 data and
Q250 (testing) data. NOLEARN: using initialized weight
vector, DIRECT: using feedback with the direct approach,
AGGRESSIVE: using feedback with the aggressive approach,
SUPERVISED: using gold 250 logical forms for training.
Note that none of the approaches use any annotated logical
forms besides the SUPERVISED approach.
Algorithm # LF Accuracy
AGGRESSIVE ? 73.2
SUPERVISED 250 80.4
W&M 2006 ? 310 ? 60.0
W&M 2007 ? 310 ? 75.0
Z&C 2005 600 79.29
Z&C 2007 600 86.07
W&M 2007 800 86.59
Table 2: Comparison against previously published results.
Results show that with a similar number of logical forms
(# LF) for training our SUPERVISED approach outperforms
existing systems, while the AGGRESSIVE approach remains
competitive without using any logical forms.
2. How much performance do we sacrifice by
not restricting our model to parsing rules?
3. What, if any, are the differences in behaviour
between the two learning with feedback ap-
proaches?
We first compare how well our model performs
under four different learning regimes. NOLEARN
uses a manually initialized weight vector. DIRECT
and AGGRESSIVE use the two response driven
learning approaches, where a feedback function
but no logical forms are provided. As an up-
per bound we train the model using a fully SU-
PERVISED approach where the input sentences are
paired with hand annotated logical forms.
Table 1 shows the accuracy of each setup. The
model without learning (NOLEARN) gives a start-
ing point with an accuracy of 22.2%. The re-
sponse driven learning methods perform substan-
tially better than the starting point. The DIRECT
approach which uses a binary learner reaches an
accuracy of 75.2% on the R250 data and 69.2% on
the Q250 (testing) data. While the AGGRESSIVE
approach which uses a structured learner sees a
bigger improvement, reaching 82.4% and 73.2%
respectively. This is only 7% below the fully SU-
PERVISED upper bound of the model.
To answer the second question, we compare a
supervised version of our model to existing se-
mantic parsers. The results are in Table 2. Al-
though the numbers are not directly comparable
due to different splits in the data7, we can see that
with a similar number of logical forms for train-
ing our SUPERVISED approach outperforms ex-
isting systems (Wong and Mooney, 2006; Wong
and Mooney, 2007), while the AGGRESSIVE ap-
proach remains competitive without using any log-
ical forms. Our SUPERVISED model is still very
competitive with other approaches (Zettlemoyer
and Collins, 2007; Wong and Mooney, 2007),
which used considerably more annotated logical
forms in the training phase.
In order to answer the third question, we turn
our attention to the differences between the two
response driven learning approaches. The DIRECT
and AGGRESSIVE approaches use binary feedback
to learn, however they utilize the signal differently.
DIRECT uses the signal directly to learn a bi-
nary classifier capable of replicating the feedback,
whereas AGGRESSIVE learns a structured predic-
tor that can repeatedly obtain the logical forms
for which positive feedback was received. Thus,
although the AGGRESSIVE outperforms the DI-
RECT approach the concepts each approach learns
may be different. Analysis over the training data
shows that in 66.8% examples both approaches
predict a logical form that gives the correct an-
swer. While AGGRESSIVE correctly answers an
additional 16% which DIRECT gets incorrect. In
the opposite direction, DIRECT correctly answers
8.8% that AGGRESSIVE does not. Leaving only
8.4% of the examples that both approaches pre-
dict incorrect logical forms. This suggests that an
approach which combines DIRECT and AGGRES-
SIVE may be able to improve even further.
Figure 2 shows the accuracy on the entire train-
ing data (R250) at each iteration of learning. We
see that the AGGRESSIVE approach learns to cover
more of the training data and at a faster rate than
DIRECT. Note that the performance of the DI-
RECT approach drops at the first iteration. We hy-
pothesize this is due to imbalances in the binary
feedback dataset (too many negative examples) in
the first iteration.
7It is relatively difficult to compare different approaches
in the Geoquery domain given that many existing papers do
not use the same data split.
25
70 1 2 3 4 5 6
90
0
10
20
30
40
50
60
70
80
Learning Iterations
A
cc
ur
ac
y 
on
 R
es
po
ns
e 
25
0
Direct Approach
Aggressive Approach
Initialization
Figure 2: Accuracy on training set as number of learning
iterations increases.
7 Related Work
Learning to map sentences to a meaning repre-
sentation has been studied extensively in the NLP
community. Early works (Zelle and Mooney,
1996; Tang and Mooney, 2000) employed induc-
tive logic programming approaches to learn a se-
mantic parser. More recent works apply statisti-
cal learning methods to the problem. In (Ge and
Mooney, 2005; Nguyen et al, 2006), the input to
the learner consists of complete syntactic deriva-
tions for the input sentences annotated with logi-
cal expressions. Other works (Wong and Mooney,
2006; Kate and Mooney, 2006; Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Zettlemoyer and Collins, 2009) try to alleviate the
annotation effort by only taking sentence and log-
ical form pairs to train the models. Learning is
then defined over hidden patterns in the training
data that associate logical symbols with lexical
and syntactic elements.
In this work we take an additional step to-
wards alleviating the difficulty of training seman-
tic parsers and present a world response based
training protocol. Several recent works (Chen and
Mooney, 2008; Liang et al, 2009; Branavan et
al., 2009) explore using an external world context
as a supervision signal for semantic interpretation.
These works operate in settings different to ours as
they rely on an external world state that is directly
referenced by the input text. Although our frame-
work can also be applied in these settings we do
not assume that the text can be grounded in a world
state. In our experiments the input text consists of
generalized statements which describe some infor-
mation need that does not correspond directly to a
grounded world state.
Our learning framework closely follows recent
work on learning from indirect supervision. The
direct approach resembles learning a binary clas-
sifier over a latent structure (Chang et al, 2010a);
while the aggressive approach has similarities with
work that uses labeled structures and a binary
signal indicating the existence of good structures
to improve structured prediction (Chang et al,
2010b).
8 Conclusions
In this paper we tackle one of the key bottlenecks
in semantic parsing ? providing sufficient super-
vision to train a semantic parser. Our solution is
two fold, first we present a new training paradigm
for semantic parsing that relies on natural, hu-
man level supervision. Second, we suggest a new
model for semantic interpretation that does not
rely on NL syntactic parsing rules, but rather uses
the syntactic information to bias the interpretation
process. This approach allows the model to gener-
alize better and reduce the required amount of su-
pervision. We demonstrate the effectiveness of our
training paradigm and interpretation model over
the Geoquery domain, and show that our model
can outperform fully supervised systems.
Acknowledgements We are grateful to Rohit Kate and
Raymond Mooney for their help with the Geoquery dataset.
Thanks to Yee Seng Chan, Nick Rizzolo, Shankar Vembu
and the three anonymous reviewers for their insightful com-
ments. This material is based upon work supported by the
Air Force Research Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181 and by DARPA under the Bootstrap
Learning Program. Any opinions, findings, and conclusion or
recommendations expressed in this material are those of the
authors and do not necessarily reflect the views of the AFRL
or DARPA.
References
S.R.K. Branavan, H. Chen, L. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement learning for map-
ping instructions to actions. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010a. Discriminative learning over constrained la-
tent representations. In Proc. of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).
26
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010b. Structured output learning with indirect su-
pervision. In Proc. of the International Conference
on Machine Learning (ICML).
D. Chen and R. Mooney. 2008. Learning to sportscast:
a test of grounded language acquisition. In Proc. of
the International Conference on Machine Learning
(ICML).
Q. Do, D. Roth, M. Sammons, Y. Tu, and V.G. Vydis-
waran. 2010. Robust, Light-weight Approaches to
compute Lexical Similarity. Computer Science Re-
search and Technical Reports, University of Illinois.
http://hdl.handle.net/2142/15462.
R. Ge and R. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
R. Kate and R. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics (ACL).
R. Kate. 2008. Transforming meaning representation
grammars to improve semantic parsing. In Proc. of
the Annual Conference on Computational Natural
Language Learning (CoNLL).
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Proc. of the Conference on Advances in
Neural Information Processing Systems (NIPS).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL).
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. Wordnet: An on-line lexical
database. International Journal of Lexicography.
L. Nguyen, A. Shimazu, and X. Phan. 2006. Semantic
parsing with structured svm ensemble classification
models. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
L. Tang and R. Mooney. 2000. Automated construc-
tion of database interfaces: integrating statistical and
relational learning for semantic parsing. In Proc. of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP).
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming
for semantic parsing. In Proc. of the European Con-
ference on Machine Learning (ECML).
Y.-W. Wong and R. Mooney. 2006. Learning for
semantic parsing with statistical machine transla-
tion. In Proc. of the Annual Meeting of the North
American Association of Computational Linguistics
(NAACL).
Y.-W. Wong and R. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming.
In Proc. of the National Conference on Artificial In-
telligence (AAAI).
L. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proc. of
the Annual Conference in Uncertainty in Artificial
Intelligence (UAI).
L. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Pro-
cessing and on Computational Natural Language
Learning (EMNLP-CoNLL).
L. Zettlemoyer and M. Collins. 2009. Learning
context-dependent mappings from sentences to log-
ical form. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).
27
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 229?237,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Adapting Text instead of the Model: An Open Domain Approach
Gourab Kundu, Dan Roth
University of Illinois at Urbana Champaign
Urbana, IL 61801
{kundu2,danr}@illinois.edu
Abstract
Natural language systems trained on labeled
data from one domain do not perform well
on other domains. Most adaptation algorithms
proposed in the literature train a new model for
the new domain using unlabeled data. How-
ever, it is time consuming to retrain big mod-
els or pipeline systems. Moreover, the domain
of a new target sentence may not be known,
and one may not have significant amount of
unlabeled data for every new domain.
To pursue the goal of an Open Domain NLP
(train once, test anywhere), we propose ADUT
(ADaptation Using label-preserving Transfor-
mation), an approach that avoids the need for
retraining and does not require knowledge of
the new domain, or any data from it. Our ap-
proach applies simple label-preserving trans-
formations to the target text so that the trans-
formed text is more similar to the training do-
main; it then applies the existing model on
the transformed sentences and combines the
predictions to produce the desired prediction
on the target text. We instantiate ADUT for
the case of Semantic Role Labeling (SRL)
and show that it compares favorably with ap-
proaches that retrain their model on the target
domain. Specifically, this ?on the fly? adapta-
tion approach yields 13% error reduction for
a single parse system when adapting from the
news wire text to fiction.
1 Introduction
In several NLP tasks, systems trained on annotated
data from one domain perform well when tested
on the same domain but adapt poorly to other do-
mains. For example, all systems of CoNLL 2005
shared task (Carreras and M?rquez, 2005) on Se-
mantic Role Labeling showed a performance degra-
dation of almost 10% or more when tested on a dif-
ferent domain.
Most works in domain adaptation have focused
on learning a common representation across train-
ing and test domains (Blitzer et al, 2006; Daum?III,
2007; Huang and Yates, 2009). Using this represen-
tation, they retrain the model for every new domain.
But these are not Open Domain Systems since the
model needs to be retrained for every new domain.
This is very difficult for pipeline systems like SRL
where syntactic parser, shallow parser, POS tagger
and then SRL need to be retrained. Moreover, these
methods need to have a lot of unlabeled data that is
taken from the same domain, in order to learn mean-
ingful feature correspondences across training and
test domain. These approaches cannot work when
they do not have a lot of unlabeled data from the test
domain or when the test domain in itself is very di-
verse, e.g., the web.
The contribution of this paper is a new frame-
work for adaptation. We propose ADUT (ADap-
tation Using label-preserving Transformation) as a
framework in which a previously learned model can
be used on an out-of-domain example without re-
training and without looking at any labeled or unla-
beled data for the domain of the new example. The
framework transforms the test sentence to generate
sentences that have, in principle, identical labeling
but that are more like instances from the training do-
main. Consequently, it is expected that the exist-
229
ing model will make better predictions on them. All
these predictions are then combined to choose the
most probable and consistent prediction for the test
sentence.
ADUT is a general technique which can be ap-
plied to any natural language task. In this paper, we
demonstrate its usefulness on the task of semantic
role labeling (Carreras and M?rquez, 2005). Start-
ing with a system that was trained on the news text
and does not perform well on fiction, we show that
ADUT provides significant improvement on fiction,
and is competitive with the performance of algo-
rithms that were re-trained on the test domain.
The paper is organized as follows. Section 2 dis-
cusses two motivating examples. Section 3 gives a
formal definition of our adaptation framework. Sec-
tion 4 describes the transformation operators that we
applied for this task. Section 5 presents our joint in-
ference approach. Section 6 describes our semantic
role labeling system and our experimental results are
in Section 7. Section 8 describes the related works
for domain adaptation. Finally in Section 9 we con-
clude the paper with a discussion.
2 Motivating Examples
One of the key reasons for performance degradation
of an NLP tool is unseen features such as words in
the new domain that were not seen in the training
domain. But if an unknown word is replaced by a
known word without changing the labeling of the
sentence, tools perform better. For example, in the
task of syntactic parsing, the unknown word checkup
causes the Charniak parser to make a wrong co-
ordination decision on the sentence
He was discharged from the hospital af-
ter a two-day checkup and he and his par-
ents had what Mr. Mckinley described as
a ?celebration lunch? at the cafeteria on
the campus.
If we replace the word checkup with its hyper-
nym examination which appears in training data, the
parse gets corrected. Figure 1 shows both original
and corrected parse trees.
For the task of semantic role labeling, systems do
not perform well on the predicates that are infre-
quent in training domain. But if an infrequent predi-
cate is replaced with a frequent predicate from train-
ing domain such that both predicates have similar
semantic argument structure, the system performs
better. Consider the following sentence
Scotty gazed out at ugly gray slums.
The semantic role for the phrase at ugly gray slums
with respect to predicate gaze is A1. But the pred-
icate gaze appears only once in training data and
our model predicts at ugly gray slums as AM-LOC
instead of A1. But if gaze is replaced with look
which occurs 328 times in training data and has sim-
ilar argument structure (in the same VerbNet class as
gaze), the system makes the correct prediction.
3 Problem Formulation
Let the in-domain distribution be Di and out-of-
domain distribution be Do. We have a model f
trained over a set of labeled examples drawn from
Di. If Di and Do are very dissimilar, f will not per-
form well on examples drawn from Do. The prob-
lem is to get good performance from f on Do with-
out retraining f .
We define a Transformation g to be a function that
maps an example e into a set of examples E. So g :
X ? 2X where X is the entire space of examples.
In this paper, we only consider the Label-preserving
Transformations which satisfy the property that all
transformed examples in E have the same label as
input example e, i.e., ?x x ? Sk ? g(x) ? Sk
where Sk is the set of examples with label k . Let
G be a set of label-preserving transformation func-
tions. G = {g1, g2, . . ., gp}.
At evaluation time, for test example d, we will
apply G to get a set of examples T1. Let T2 = {d? ?
T1 : Di(d?) > Di(d)}. So all examples in T2 have
same label as d but have a higher probability than
d to be drawn from the in-domain distribution. So
f should perform better on examples in T2 than on
d. For each d? ? T2, f will produce scores for the
output labels. The scores will be combined subject
to constraints to produce the final output.
4 Transformation Functions
After applying a transformation function to get a
new sentence from an input sentence, we remem-
ber the mapping of segments across the original
230
a. S1
S
NP
He
VP
was VP
discharged PP
from the hospital
PP
after NP
NP
a two-day
checkup
SBAR
NP
and he and his parents
VP
had . . . campus
.
b. S1
S
S
NP
He
VP
was VP
discharged PP
from the hospital
PP
after NP
a two-day
examination
and S
NP
he and his parents
VP
had . . . campus
.
Figure 1: a. Original Parse tree b. Corrected Parse tree after replacement of unknown word checkup by examination
and transformed sentence. Thus, after annotating
the transformed sentence with SRL, we can transfer
the roles to the original sentence through this map-
ping. Transformation functions can be divided into
two categories. The first category is Transforma-
tions From List which uses external resources like
WordNet, VerbNet and Word Clusters. The second
is Learned Transformations that uses transformation
rules that have been learned from training data.
4.1 Transformation From List
I. Replacement of Predicate:
As noted in (Huang and Yates, 2010), 6.1% of the
predicates in the Brown test set do not appear in WSJ
training set and 11.8% appear at most twice. Since
the semantic roles of a sentence depend on the pred-
icate, these infrequent predicates hurt SRL perfor-
mance on new domains. Note that since all predi-
cates in PropBank are verbs, we will use the words
predicate and verb interchangeably.
We count the frequency of each predicate and its
accuracy in terms of F1 score over the training data.
If the frequency or the F1 score of the predicate in
the test sentence is below a threshold, we perturb
that predicate. We take all the verbs in the same class
of VerbNet1 as the original verb (in case the verb is
present in multiple classes, we take all the classes).
In case the verb is not present in VerbNet, we take
its synonyms from WordNet. If there is no synonym
in WordNet, we take the hypernyms.
From this collection of new verbs, we select verbs
that have a high accuracy and a high frequency in
1
http://verbs.colorado.edu/ mpalmer/projects/verbnet.html
training. We replace the original verb with each of
these new verbs and generate one new sentence for
each new verb; the sentence is retained if the parse
score for the new sentence is higher than the parse
score for the original sentence.2 VerbNet has de-
fined a set of verb-independent thematic roles and
grouped the verbs according to their usage in frames
with identical thematic roles. But PropBank anno-
tation was with respect to each verb. So the same
thematic role is labeled as different roles for dif-
ferent verbs in PropBank. For example, both warn
and advise belong to the same VerbNet class (37.9)
and take thematic roles of Recipient (person being
warned or advised) and Topic (topic of warning or
advising). But Recipient was marked as A2 for warn
and A1 for advise and Topic was marked as A1 for
warn and A2 for advise in PropBank annotation.
Semlink3 provides a mapping from the thematic role
to PropBank role for each verb. After the SRL anno-
tates the new sentence with PropBank roles for the
new verb, we map the PropBank roles of the new
verb to their corresponding thematic roles and then
map the thematic roles to the corresponding Prop-
Bank roles for the original verb.
II. Replacement and Removal of Quoted Strings:
Quoted sentences can vary a lot from one domain
to another. For example, in WSJ, quoted sentences
are like formal statements but in Brown, these are
like informal conversations. We generate the trans-
formations in the following ways:
1) We use the content of the quoted string as one
2
Parse score is the parse probability returned by Charniak or Stanford parser.
3
http://verbs.colorado.edu/semlink/
231
sentence. 2) We replace each quoted string in turn
with a simple sentence (This is good) to generate a
new sentence. 3) If a sentence has a quoted string in
the beginning, we move that quoted string after the
first NP and VP that immediately follow the quoted
string. For example, from the input sentence, ?We
just sit quiet?, he said. we generate the sentences 1)
We just sit quiet 2) ?This is good?, he said. 3) He
said, ?We just sit quiet?.
III. Replacement of Unseen Words:
A major difficulty for domain adaptation is that
some words in the new domain do not appear in the
training domain. In the Brown test set, 5% of total
words were never seen in the WSJ training set.
Given an unseen word which is not a verb, we
replace it with WordNet synonyms and hypernyms
that were seen in the training data. We used the
clusters obtained in (Liang, 2005) from running the
Brown algorithm (Brown et al, 1992) on Reuters
1996 dataset. But since this cluster was generated
automatically, it is noisy. So we chose replacements
from the Brown clusters selectively. We only replace
those words for which the POS tagger and the syn-
tactic parser predicted different tags. For each such
word, we find its cluster and select the set of words
from the cluster. We delete from this set al words
that do not take at least one part-of-speech tag that
the original word can take (from WordNet). For each
candidate synonym or hypernym or cluster member,
we get a new sentence. Finally we only keep those
sentences that have higher parse scores than the orig-
inal sentence.
IV. Sentence Split based on Stop Symbols:
We split each sentence based on stop symbols like
; and . . Each of the splitted sentences becomes one
transformation of the original sentence.
V. Sentence Simplification:
We have a set of heuristics for simplifying the
constituents of the parse tree; for example, replac-
ing an NP with its first and last word, removal of
PRN phrases etc. We apply these heuristics and gen-
erate simpler sentences until no more simplification
is possible. Examples of our heuristics are given in
Table 1.
Note that we can use composition of multiple
transformation functions as one function. A compo-
sition p1  p2(s) = ?a?p1(s)p2(a). We apply III,
IIII, IVI and VI.
Node Input Example Simplified Example Operation
NP He and she ran. He ran. replace
NP The big man ran. The man ran. replace
ADVP He ran fast. He ran. delete
PP He ran in the field. He ran. delete
PRN He ? though sick ? ran. He ran. delete
VP He walked and ran. He ran. delete
TO I want him to run. I want that he can ran. rewrite
Table 1: Examples of Simplifications (Predicate is run)
4.2 Learned Transformations
The learned model is inaccurate over verbs and roles
that are infrequent in the training data. The purpose
of the learned transformation is to transfer such a
phrase in the test sentence in place of a phrase of a
simpler sentence; this is done such that there exists
a mapping from the role of the phrase in the new
sentence to the role of the phrase in the original sen-
tence.
Phrase Representation: A phrase tuple is a 3-
tuple (t, i, h) where, t is the phrase type, i is the in-
dex, and h is the headword of the phrase. We denote
by PR the Phrase Representation of a sentence ? an
ordered list of phrase tuples. A phrase tuple corre-
sponds to a node in the tree. We only consider phrase
tuples that correspond to nodes that are (1) a sibling
of the predicate node or (2) a sibling of an ancestor
of the predicate node. Phrase tuples inPR are sorted
based on their position in the sentence. The index i
of the phrase tuple containing the predicate is taken
to be zero with the indices of the phrase tuples on
the left (right) sequentially decreasing (increasing).
Transformation Rule: We denote by Label(n, s)
the semantic role of nth phrase in the PR of the
sentence s. Let Replace(ns, nt, ss, st) be a new
sentence that results from inserting the phrase ns in
sentence ss instead of phrase nt in sentence st. We
will refer to st as target sentence and to nt as the
target phrase. Let sp be a sequence of phrase tuples
named as source pattern. If Label(ns, ss) = r1 and
Label(nt, Replace(ns, nt, ss, st)) = r2, then denote
f(r2) = r1. In this case we call the 6-tuple (st, nt,
p, sp, ns, f ) a transformation rule. We call f the
232
label correspondence function.
Example: Consider the sentence st = ?But it did
not sing." and the rule ? : (st, nt, p, sp, ns, f). Let:
nt = ?3, p = entitle,
sp = [?2, NP, ?][?1, AUX, ?][0, V, entitle][1, ?, to]
ns = ?2, f = {<A0, A2>} ? {<Ai,Ai>|i 6= 0}.
The PR of ?.st is {[?4, CC, But] [?3, NP, it]
[?2, AUX, did] [?1, RB, not] [0, VB, sing] [1, ., .]}.
Consider the input sentence ss: Mr. X was entitled
to a discount . with PR of {[?2, NP, X] [?1, AUX,
was] [0, V, entitle] [1, PP, to][2, ., .]}. Since ?.sp is
a subsequence of the PR of ss, ? will apply to the
predicate entitle of ss. The transformed sentence is:
str = Replace(?.ns, ?.nt, ss, ?.st) = But Mr. X
did not sing. with PR of {[?4, CC, But] [?3, NP,
X] [?2, AUX, did] [?1, RB, not] [0, VB, sing] [1,
., .]}. If the SRL system assigns the semantic role
of A0 to the phrase Mr. X of str, the semantic role
of Mr. X in ss can be recovered through ?.f since
?.f(A0) = A2 = Label(?2, ss).
While checking if ?.sp is a subsequence of the
PR of the input sentence, ? in each tuple of ?.sp
has to be considered a trivial match. So ? will
match the sentence He is entitled to a reward. with
PR = {[?2, NP, He] [?1, AUX, is] [0, V, entitle]
[1, PP, to][2, ., .]} but will not match the sentence
The conference was entitled a big success. with
PR = {[?2, NP, conference] [?1, AUX, was] [0,
V, entitle] [1, S, success][2, ., .]} (mismatch position
is bolded). The index of a phrase tuple cannot be ?,
only the head word or type can be ? and the rules
with more ? strings in the source pattern are more
general since they can match more sentences.
Algorithm 1 GenerateRules
1: Input: predicate v, semantic role r, Training sentences D, SRL
Model M
2: Output: set of rules R
3: R? GetInitialRules(v, r,D,M)
4: repeat
5: J ? ExpandRules(R)
6: K ? R ? J
7: sort K based on accuracy, support, size of source pattern
8: select some rules R ? K based on database coverage
9: until all rules in R have been expanded before
10: return R
The algorithm for finding rules for a semantic role
r of a predicate v is given in Algorithm 1. It is a
specific to general beam search procedure that starts
with a set of initial rules (Line 3, detail in Algorithm
2) and finds new rules from these rules (Line 5, de-
tail in Algorithm 3). In Line 7, the rules are sorted
by decreasing order of accuracy, support and number
of ? strings in the source pattern. In Line 8, a set of
rules are selected to cover all occurrences of the se-
mantic role r with the predicate v a specific number
of times. This process continues until no new rules
are found. Note that these rules need to be learned
only once and can be used for every new domain.
Algorithm 2 GetInitialRules
1: Input: predicate v, semantic role r, Training sentences D, SRL-
Model M
2: Output: Set of initial rules I
3: I ? ?
4: T ? {s ? D : length(s) <= e}
5: S ? {s ? D : s has role r for predicate v}
6: M ? Set of all semantic roles
7: for each phrase p1 in s1 ? S with gold label r for predicate v do
8: for each phrase p2 in s2 ? T labeled as a core argument do
9: if s1 6= s2 and p1 and p2 have same phrase types then
10: ? ? empty rule
11: ?.st ? s2, ?.p? v
12: ?.nt ? index of p2 in PR of s2
13: ?.ns ? index of p1 in PR of s1
14: ?.sp ? phrase tuples for phrases from p1 to v and two
phrases after v in PR of s1
15: L? ?
16: for each sentence s3 ?D with predicate v do
17: if ?.sp is a subsequence of PR of s3 then
18: x? replace(?.ns, ?.nt, s3, ?.st)
19: annotate x with SRL using M
20: r1 ? the gold standard semantic role of the
phrase with index ?.ns in PR of s3
21: r2 ? Label(?.nt, x)
22: if r2 /? L then
23: insert(r2, r1) in ?.f
24: L = L ? {r2}
25: end if
26: end if
27: end for
28: for each role j ?M ? L do
29: insert(j, j) in ?.f
30: end for
31: I ? I? {?}
32: end if
33: end for
34: end for
35: return I
The algorithm for generating initial rules for the
semantic role r of predicate v is given in Algorithm
2. Shorter sentences are preferred to be target sen-
tences(Line 4). A rule ? is created for every (p1,p2)
pair where p1, p2 are phrases, p1 has the semantic
role r in some sentence s1, p2 is labeled as a core
argument(A0 ? A5) in some sentence in T and the
phrase types of p1 and p2 in their respective parse
trees are same(Lines 7 ? 9). Every sentence s3 in
233
training corpus with predicate ?.p is a potential can-
didate for applying ? (Line 16) if ?.sp is a subse-
quence ofPR of s3(Line 17). After applying ? to s3,
a transformed sentence x is created(Line 18). Lines
20 ? 26 find the semantic role r2 of the transferred
phrase from SRL annotation of x using model M
and create a mapping from r2 to the gold standard
role r1 of the phrase in s3. L maintains the set of se-
mantic roles for which mappings have been created.
In lines 28 ? 30, all unmapped roles are mapped to
themselves.
The algorithm for creating new rules from a set
of existing rules is given in Algorithm 3. Lines 4 ?
13 generate all immediate more general neighbors of
the current rule by nullifying the headword or phrase
type element in any of the phrase tuples in its source
pattern.
Algorithm 3 ExpandRules
1: Input: a set of rules R
2: Output: a set of expanded rules E
3: E ? ?
4: for each phrase tuple c in the source pattern of r ? R do
5: if c is not the tuple for predicate then
6: create a new rule r? with all components of r
7: mark the head word of c in the source pattern of r? to ?
8: add r? to E
9: create a new rule r?? with all components of r
10: mark the phrase type of c in the source pattern of r?? to ?
11: add r?? to E
12: end if
13: end for
14: return E
5 Combination by Joint Inference
The transformation functions transform an input
sentence into a set of sentences T . From each trans-
formed sentence ti, we get a set of argument can-
didates Si. Let S =
?|T |
i=1 Si be the set of all ar-
guments. Argument classifier assigns scores for
each argument over the output labels(roles) in S
that is then converted into a probability distribu-
tion over the possible labels using the softmax func-
tion (Bishop, 1995). Note that multiple arguments
with the same span can be generated from multiple
transformed sentences.
First, we take all arguments from S with distinct
span and put them in S?. For each argument arg in
S?, we calculate scores over possible labels as the
sum over the probability distribution (over output la-
bels) of all arguments in S that have the same span
as arg divided by the number of sentences in T that
contained arg. This results in a set of arguments with
distinct spans and for each argument, a set of scores
over possible labels. Following the joint inference
procedure in (Punyakanok et al, 2008), we want to
select a label for each argument such that the total
score is maximized subject to some constraints. Let
us index the set S? as S?1:M where M = |S?|. Also
assume that each argument can take a label from a
set P . The set of arguments in S?1:M can take a set
of labels c1:M ? P 1:M . Given some constraints, the
resulting solution space is limited to a feasible set F;
the inference task is: c1:M = arg maxc1:M?F (P 1:M )
?M
i=1 score(S
?i = ci).
The constraints used are: 1) No overlapping or
embedding argument. 2) No duplicate argument for
core arguments A0-A5 and AA. 3) For C-arg, there
has to be an arg argument.
6 Experimental Setup
In this section, we discuss our experimental setup
for the semantic role labeling system. Similar to the
CoNLL 2005 shared tasks, we train our system using
sections 02-21 of the Wall Street Journal portion of
Penn TreeBank labeled with PropBank. We test our
system on an annotated Brown corpus consisting of
three sections (ck01 - ck03).
Since we need to annotate new sentences with
syntactic parse, POS tags and shallow parses, we do
not use annotations in the CoNLL distribution; in-
stead, we re-annotate the data using publicly avail-
able part of speech tagger and shallow parser1, Char-
niak 2005 parser (Charniak and Johnson, 2005) and
Stanford parser (Klein and Manning, 2003).
Our baseline SRL model is an implementation of
(Punyakanok et al, 2008) which was the top per-
forming system in CoNLL 2005 shared task. Due to
space constraints, we omit the details of the system
and refer readers to (Punyakanok et al, 2008).
7 Results
Results for ADUT using only the top parse of Char-
niak and Stanford are shown in Table 2. The Base-
line model using top Charniak parse (BaseLine-
Charniak) and top Stanford parse (BaseLine-
Stanford) score respectively 76.4 and 73.3 on the
1
http://cogcomp.cs.illinois.edu/page/software
234
WSJ test set. Since we are interested in adaptation,
we report and compare results for Brown test set
only. On this set, both ADUT-Charniak and ADUT-
Stanford significantly outperform their respective
baselines. We compare with the state-of-the-art sys-
tem of (Surdeanu et al, 2007). In (Surdeanu et
al., 2007), the authors use three models: Model
1 and 2 do sequential tagging of chunks obtained
from shallow parse and full parse. Model 3 assumes
each predicate argument maps to one syntactic con-
stituent and classifies it individually. So Model 3
matches our baseline model. ADUT-Charniak out-
performs the best individual model (Model 2) of
(Surdeanu et al, 2007) by 1.6% and Model 3 by
3.9%. We also tested another system that used clus-
ter features and word embedding features computed
following (Collobert and Weston, 2008). But we
did not see any performance improvement on Brown
over baseline.
System P R F1
BaseLine-Charniak 69.6 61.8 65.5
ADUT-Charniak 72.75 66.1 69.3
BaseLine-Stanford 70.8 56.5 62.9
ADUT-Stanford 72.5 60.0 65.7
(Surdeanu et al, 2007)(Model 2) 71.8 64.0 67.7
(Surdeanu et al, 2007)(Model 3) 72.4 59.7 65.4
Table 2: Comparing single parse system on Brown.
All state-of-the-art systems for SRL are a com-
bination of multiple systems. So we combined
ADUT-Stanford, ADUT-Charniak and another sys-
tem ADUT-Charniak-2 based on 2nd best Charniak
parse using joint inference. In Table 3, We com-
pare with (Punyakanok et al, 2008) which was the
top performing system in CoNLL 2005 shared task.
We also compare with the multi parse system of
(Toutanova et al, 2008) which uses a global joint
model using multiple parse trees. In (Surdeanu et al,
2007), the authors experimented with several com-
bination strategies. Their first combination strategy
was similar to ours where they directly combined the
outputs of different systems using constraints (de-
noted as Cons in Table 3). But their best result on
Brown set was obtained by treating the combina-
tion of multiple systems as a meta-learning problem.
They trained a new model to score candidate argu-
ments produced by individual systems before com-
bining them through constraints (denoted as LBI in
Table 3). We also compare with (Huang and Yates,
2010) where the authors retrained a SRL model us-
ing HMM features learned over unlabeled data of
WSJ and Brown.
System P R F1 Retrain
(Punyakanok et al, 2008) 73.4 62.9 67.8 ?
(Toutanova et al, 2008) NR NR 68.8 ?
(Surdeanu et al, 2007) (Cons) 78.2 62.1 69.2 ?
(Surdeanu et al, 2007) (LBI) 81.8 61.3 70.1 ?
ADUT-combined 74.3 67.0 70.5 ?
(Huang and Yates, 2010) 77.0 70.9 73.8 X
Table 3: Comparison of the multi parse system on Brown.
Table 3 shows that ADUT-Combined performs
better than (Surdeanu et al, 2007) (Cons) when in-
dividual systems have been combined similarly. We
believe that the techniques in (Surdeanu et al, 2007)
of using multiple models of different kinds (two
based on sequential tagging of chunks to capture ar-
guments whose boundaries do not match a syntac-
tic constituent) and training an additional model to
combine the outputs of individual systems are or-
thogonal to the performance improvement that we
have and applying these methods will further in-
crease the performance of our final system which is
a research direction we want to pursue in future.
We did an ablation study to determine which
transformations help and by how much. Table 4
presents results when only one transformation is ac-
tive at a time. We see that each transformation im-
proves over the baseline.
The effect of the transformation of Replacement
of Predicate on infrequent verbs is shown in Table
5. This transformation improves F1 as much as 6%
on infrequent verbs.
The running time for ADUT-Charniak on Brown
set is 8 hours compared to SRL training time of 20
hours. Average number of transformed sentences
generated by ADUT-Charniak for every sentence
from Brown is 36. The times are calculated based
on a machine with 2x 6-Core Xeon X5650 Proces-
sor with 48G memory.
235
Transformation P R F1
Baseline 69.6 61.8 65.5
Replacement of Unknown Words 70.6 62.1 66.1
Replacement of Predicate 71.2 62.8 66.8
Replacement of Quotes 71.0 63.4 67.0
Simplification 70.3 62.9 66.4
RuleTransformation 70.9 62.2 66.2
Sentence Split 70.8 62.1 66.2
Together 72.75 66.1 69.3
Table 4: Ablation Study for ADUT-Charniak
Frequency Baseline Replacement of Predicate
0 64.2 67.8
less than 3 59.7 65.1
less than 7 58.9 64.8
all predicates 65.5 66.78
Table 5: Performance on Infrequent Verbs for the Trans-
formation of Replacement of Predicate
8 Related Work
Traditional adaptation techniques like (Daum?III,
2007; Chelba and Acero, 2004; Finkel and Man-
ning, 2009; Jiang and Zhai, 2007; Blitzer et al,
2006; Huang and Yates, 2009; Ando and Zhang,
2005; Ming-wei Chang and Roth, 2010) need to re-
train the model for every new domain. In (Umansky-
Pesin et al, 2010), there was no retraining; instead,
a POS tag was predicted for every unknown word
in the new domain by considering contexts of that
word collected by web search queries. We differ
from them in that our transformations are label-
preserving; moreover, our transformations aim at
making the target text resemble the training text.
We also present an algorithm to learn transformation
rules from training data. Our application domain,
SRL, is also more complex and structured than POS
tagging.
In (McClosky et al, 2010), the task of multiple
source parser adaptation was introduced. The au-
thors trained parsing models on corpora from dif-
ferent domains and given a new text, used a linear
combination of trained models. Their approach re-
quires annotated data from multiple domains as well
as unlabeled data for the new domain, which is not
needed in our framework. In (Huang and Yates,
2010), the authors trained a HMM over the Brown
test set and the WSJ unlabeled data. They derived
features from Viterbi optimal states of single words
and spans of words and retrained their models us-
ing these features. In (Vickrey and Koller, 2008),
a large number of hand-written rules were used to
simplify the parse trees and reduce syntactic vari-
ation to overcome feature sparsity. We have sev-
eral types of transformations, and use less than 10
simplification heuristics, based on replacing larger
phrases with smaller phrases and deleting unneces-
sary parse tree nodes. There are also some methods
for unsupervised semantic role labeling (Swier and
Stevenson, 2004), (Abend et al, 2009) that easily
adapt across domains but their performances are not
comparable to supervised systems.
9 Conclusion
We presented a framework for adaptating natural
language text so that models can be used across do-
mains without modification. Our framework sup-
ports adapting to new domains without any data or
knowledge of the target domain. We showed that our
approach significantly improves SRL performance
over the state-of-the-art single parse based system
on Brown set. In the future, we would like to extend
this approach to other NLP problems and study how
combining multiple systems can further improve its
performance and robustness.
Acknowledgements This research is sponsored
by the Army Research Laboratory (ARL) under
agreement W911NF-09-2-0053 and by the Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0181. Any opinions, findings, conclusions or rec-
ommendations are those of the authors and do not
necessarily reflect the view of the ARL, the DARPA,
AFRL, or the US government.
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling . In Proceedings of the ACL.
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple labeled
236
and unlabeled data . Journal of Machine Learning Re-
search.
Christopher Bishop. 1995. Neural Networks for Pattern
recognition, chapter 6.4: Modelling conditional distri-
butions. Oxford University Press.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. D. Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational
Linguistics, 18(4):467?479.
Xavier Carreras and Llu?s M?rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling .
In Proceedings of CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL.
Ciprian Chelba and Alex Acero. 2004. Little data
can help a lot. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceedings
of ICML.
Hal Daum?III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the the Annual Meeting of the
Association of Computational Linguistics (ACL).
Jenny R. Finkel and Christopher D. Manning. 2009. Hi-
erarchical bayesian domain adaptation . In Proceed-
ings of NAACL.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling . In Proceedings of ACL.
Fei Huang and Alexander Yates. 2010. Open-domain
semantic role labeling by modeling word spans. In
Proceedings of ACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of
ACL.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Proceedings of NIPS.
Percy Liang. 2005. Semi-supervised learning for natural
language. Masters thesis, Massachusetts Institute of
Technology.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adaptation for parsing. In
Proceedings of NAACL.
Michael Connor Ming-wei Chang and Dan Roth. 2010.
The necessity of combining adaptation methods. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Mas-
sachusetts, USA.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
Mihai Surdeanu, Llu?s M?rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research, 29:105?151.
Robert S. Swier and Suzanne Stevenson. 2004. Unsuper-
vised semantic role labelling. In Proceedings of Em-
pirical Methods in Natural Language Processing.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34:161?191.
Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-
poport. 2010. A multi-domain web-based algorithm
for pos tagging of unknown words . In Proceedings of
Coling.
David Vickrey and Daphne Koller. 2008. Sentence sim-
plification for semantic role labeling. In Proceedings
of the ACL-HLT.
237
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 31?39,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Learning English Light Verb Constructions: Contextual or Statistical
Yuancheng Tu
Department of Linguistics
University of Illinois
ytu@illinois.edu
Dan Roth
Department of Computer Science
University of Illinois
danr@illinois.edu
Abstract
In this paper, we investigate a supervised ma-
chine learning framework for automatically
learning of English Light Verb Constructions
(LVCs). Our system achieves an 86.3% accu-
racy with a baseline (chance) performance of
52.2% when trained with groups of either con-
textual or statistical features. In addition, we
present an in-depth analysis of these contex-
tual and statistical features and show that the
system trained by these two types of cosmet-
ically different features reaches similar per-
formance empirically. However, in the situa-
tion where the surface structures of candidate
LVCs are identical, the system trained with
contextual features which contain information
on surrounding words performs 16.7% better.
In this study, we also construct a balanced
benchmark dataset with 2,162 sentences from
BNC for English LVCs. And this data set is
publicly available and is also a useful com-
putational resource for research on MWEs in
general.
1 Introduction
Multi-Word Expressions (MWEs) refer to various
types of linguistic units or expressions, including
idioms, noun compounds, named entities, complex
verb phrases and any other habitual collocations.
MWEs pose a particular challenge in empirical Nat-
ural Language Processing (NLP) because they al-
ways have idiosyncratic interpretations which can-
not be formulated by directly aggregating the se-
mantics of their constituents (Sag et al, 2002).
The study in this paper focuses on one special
type of MWEs, i.e., the Light Verb Constructions
(LVCs), formed from a commonly used verb and
usually a noun phrase (NP) in its direct object po-
sition, such as have a look and make an offer in
English. These complex verb predicates do not fall
clearly into the discrete binary distinction of com-
positional or non-compositional expressions. In-
stead, they stand somewhat in between and are typ-
ically semi-compositional. For example, consider
the following three candidate LVCs: take a wallet,
take a walk and take a while. These three complex
verb predicates are cosmetically very similar. But
a closer look at their semantics reveals significant
differences and each of them represents a different
class of MWEs. The first expression, take a wallet
is a literal combination of a verb and its object noun.
The last expression take a while is an idiom and its
meaning cost a long time to do something, cannot
be derived by direct integration of the literal mean-
ing of its components. Only the second expression,
take a walk is an LVC whose meaning mainly de-
rives from one of its components, namely its noun
object (walk) while the meaning of its main verb is
somewhat bleached (Butt, 2003; Kearns, 2002) and
therefore light (Jespersen, 1965).
LVCs have already been identified as one of the
major sources of problems in various NLP applica-
tions, such as automatic word alignment (Samardz?ic?
and Merlo, 2010) and semantic annotation transfer-
ence (Burchardt et al, 2009), and machine transla-
tion. These problems provide empirical grounds for
distinguishing between the bleached and full mean-
ing of a verb within a given sentence, a task that is
often difficult on the basis of surface structures since
they always exhibit identical surface properties. For
example, consider the following sentences:
31
1. He had a look of childish bewilderment on his
face.
2. I?ve arranged for you to have a look at his file
in our library.
In sentence 1, the verb have in the phrase have a
look has its full fledged meaning ?possess, own? and
therefore it is literal instead of light. However, in
sentence 2, have a look only means look and the
meaning of the verb have is impoverished and is thus
light.
In this paper, we propose an in-depth case study
on LVC recognition, in which we investigate ma-
chine learning techniques for automatically identi-
fying the impoverished meaning of a verb given a
sentence. Unlike the earlier work that has viewed all
verbs as possible light verbs (Tan et al, 2006), We
focus on a half dozen of broadly documented and
most frequently used English light verbs among the
small set of them in English.
We construct a token-based data set with a total
of 2, 162 sentences extracted from British National
Corpus (BNC)1 and build a learner with L2-loss
SVM. Our system achieves a 86.3% accuracy with
a baseline (chance) performance of 52.2%. We also
extract automatically two groups of features, statis-
tical and contextual features and present a detailed
ablation analysis of the interaction of these features.
Interestingly, the results show that the system per-
forms similarly when trained independently with ei-
ther groups of these features. And the integration
of these two types of features does not improve the
performance. However, when tested with all sen-
tences with the candidate LVCs whose surface struc-
tures are identical in both negative and positive ex-
amples, for example, the aforementioned sentence 1
(negative) and 2 (positive) with the candidate LVC
?have a look?, the system trained with contextual
features which include information on surrounding
words performs more robust and significantly better.
This analysis contributes significantly to the under-
standing of the functionality of both contextual and
statistical features and provides empirical evidence
to guide the usage of them in NLP applications.
In the rest of the paper, we first present some re-
lated work on LVCs in Sec. 2. Then we describe our
1http://www.natcorp.ox.ac.uk/XMLedition/
model including the learning algorithm and statisti-
cal and contextual features in Sec. 3. We present our
experiments and analysis in Sec. 4 and conclude our
paper in Sec. 5.
2 Related Work
LVCs have been well-studied in linguistics since
early days (Jespersen, 1965; Butt, 2003; Kearns,
2002). Recent computational research on LVCs
mainly focuses on type-based classification, i.e., sta-
tistically aggregated properties of LVCs. For exam-
ple, many works are about direct measuring of the
compositionality (Venkatapathy and Joshi, 2005),
compatibility (Barrett and Davis, 2003), acceptabil-
ity (North, 2005) and productivity (Stevenson et al,
2004) of LVCs. Other works, if related to token-
based identification, i.e., identifying idiomatic ex-
pressions within context, only consider LVCs as one
small subtype of other idiomatic expressions (Cook
et al, 2007; Fazly and Stevenson, 2006).
Previous computational works on token-based
identification differs from our work in one key as-
pect. Our work builds a learning system which sys-
tematically incorporates both informative statistical
measures and specific local contexts and does in-
depth analysis on both of them while many previ-
ous works, either totally rely on or only emphasize
on one of them. For example, the method used
in (Katz and Giesbrecht, 2006) relies primarily on
local co-occurrence lexicon to construct feature vec-
tors for each target token. On the other hand, some
other works (Fazly and Stevenson, 2007; Fazly and
Stevenson, 2006; Stevenson et al, 2004), argue that
linguistic properties, such as canonical syntactic pat-
terns of specific types of idioms, are more informa-
tive than local context.
Tan et.al. (Tan et al, 2006) propose a learning ap-
proach to identify token-based LVCs. The method is
only similar to ours in that it is a supervised frame-
work. Our model uses a different data set annotated
from BNC and the data set is larger and more bal-
anced compared to the previous data set from WSJ.
In addition, previous work assumes all verbs as po-
tential LVCs while we intentionally exclude those
verbs which linguistically never tested as light verbs,
such as buy and sell in English and only focus on
a half dozen of broadly documented English light
32
verbs, such as have, take, give, do, get and make.
The lack of common benchmark data sets for
evaluation in MWE research unfortunately makes
many works incomparable with the earlier ones. The
data set we construct in this study hopefully can
serve as a common test bed for research in LVCs
or MWEs in general.
3 Learning English LVCs
In this study, we formulate the context sensitive En-
glish LVC identification task as a supervised binary
classification problem. For each target LVC candi-
date within a sentence, the classifier decides if it is
a true LVC. Formally, given a set of n labeled ex-
amples {xi, yi}ni=1, we learn a function f : X ? Y
where Y ? {?1, 1}. The learning algorithm we use
is the classic soft-margin SVM with L2-loss which
is among the best ?off-the-shelf? supervised learn-
ing algorithms and in our experiments the algorithm
indeed gives us the best performance with the short-
est training time. The algorithm is implemented us-
ing a modeling language called Learning Based Java
(LBJ) (Rizzolo and Roth, 2010) via the LIBSVM
Java API (Chang and Lin, 2001).
Previous research has suggested that both local
contextual and statistical measures are informative
in determining the class of an MWE token. How-
ever, it is not clear to what degree these two types
of information overlap or interact. Do they contain
similar knowledge or the knowledge they provide
for LVC learning is different? Formulating a clas-
sification framework for identification enables us to
integrate all contextual and statistical measures eas-
ily through features and test their effectiveness and
interaction systematically.
We focus on two types of features: contextual and
statistical features, and analyze in-depth their inter-
action and effectiveness within the learning frame-
work. Statistical features in this study are numerical
features which are computed globally via other big
corpora rather than the training and testing data used
in the system. For example, the Cpmi and Deverbal
v/n Ratio (details in sec. 3.1) are generated from the
statistics of Google n-gram and BNC corpus respec-
tively. Since the phrase size feature is numerical and
the selection of the candidate LVCs in the data set
uses the canonical length information2, we include
it into the statistical category. Contextual features
are defined in a broader sense and consist of all local
features which are generated directly from the input
sentences, such as word features within or around
the candidate phrases. We describe the details of the
used contextual features in sec. 3.2.
Our experiments show that arbitrarily combining
statistic features within our current learning system
does not improve the performance. Instead, we pro-
vide systematic analysis for these features and ex-
plore some interesting empirical observations about
them within our learning framework.
3.1 Statistical Features
Cpmi: Collocational point-wise mutual information
is calculated from Google n-gram dataset whose n-
gram counts are generated from approximately one
trillion words of text from publicly accessible Web
pages. We use this big data set to overcome the data
sparseness problem.
Previous works (Stevenson et al, 2004; Cook et
al., 2007) show that one canonical surface syntac-
tic structure for LVCs is V + a/an Noun. For ex-
ample, in the LVC take a walk, ?take? is the verb
(V) and ?walk? is the deverbal noun. The typical
determiner in between is the indefinite article ?a?.
It is also observed that when the indefinite article
changes to definite, such as ?the?, ?this? or ?that?,
a phrase is less acceptable to be a true LVC. There-
fore, the direct collocational pmi between the verb
and the noun is derived to incorporate this intuition
as shown in the following3:
Cpmi = 2I(v, aN) ? I(v, theN)
Within this formula, I(v, aN) is the point-wise mu-
tual information between ?v?, the verb, and ?aN?,
the phrase such as ?a walk? in the aforementioned
example. Similar definition applies to I(v, theN).
PMI of a pair of elements is calculated as (Church et
al., 1991):
I(x, y) = log Nx+yf(x, y)f(x, ?)f(?, y)
2We set an empirical length constraint to the maximal length
of the noun phrase object when generating the candidates from
BNC corpus.
3The formula is directly from (Stevenson et al, 2004).
33
Nx+y is the total number of verb and a/the noun
pairs in the corpus. In our case, all trigram counts
with this pattern in N-gram data set. f(x, y) is the
frequency of x and y co-occurring as a v-a/theN pair
where f(x, ?) and f(?, y) are the frequency when
either of x and y occurs independent of each other
in the corpus. Notice these counts are not easily
available directly from search engines since many
search engines treat articles such as ?a? or ?the? as
stop words and remove them from the search query4.
Deverbal v/n Ratio: the second statistical feature
we use is related to the verb and noun usage ratio of
the noun object within a candidate LVC. The intu-
ition here is that the noun object of a candidate LVC
has a strong tendency to be used as a verb or related
to a verb via derivational morphology. For exam-
ple, in the candidate phrase ?have a look?, ?look?
can directly be used as a verb while in the phrase
?make a transmission?, ?transmission? is derivation-
ally related to the verb ?transmit?. We use fre-
quency counts gathered from British National Cor-
pus (BNC) and then calculate the ratio since BNC
encodes the lexeme for each word and is also tagged
with parts of speech. In addition, it is a large corpus
with 100 million words, thus, an ideal corpus to cal-
culate the verb-noun usage for each candidate word
in the object position.
Two other lexical resources, WordNet (Fellbaum,
1998) and NomLex (Meyers et al, 1998), are used
to identify words which can directly be used as a
noun and a verb and those that are derivational re-
lated. Specifically, WordNet is used to identify the
words which can be used as both a noun and a verb
and NomLex is used to recognize those derivation-
ally related words. And the verb usage counts of
these nouns are the frequencies of their correspond-
ing derivational verbs. For example, for the word
?transmission?, its verb usage frequency is the count
in BNC with its derivationally related verb ?trans-
mit?.
Phrase Size: the third statistical feature is the ac-
tual size of the candidate LVC phrase. Many modi-
fiers can be inserted inside the candidate phrases to
generate new candidates. For example, ?take a look?
can be expanded to ?take a close look?, ?take an ex-
4Some search engines accept ?quotation strategy? to retain
stop words in the query.
tremely close look? and the expansion is in theory
infinite. The hypothesis behind this feature is that
regular usage of LVCs tends to be short. For exam-
ple, it is observed that the canonical length in En-
glish is from 2 to 6.
3.2 Contextual Features
All features generated directly from the input sen-
tences are categorized into this group. They con-
sists of features derived directly from the candidate
phrases themselves as well as their surrounding con-
texts.
Noun Object: this is the noun head of the object
noun phrase within the candidate LVC phrase. For
example, for a verb phrase ?take a quick look?, its
noun head ?look? is the active Noun Object feature.
In our data set, there are 777 distinctive such nouns.
LV-NounObj: this is the bigram of the light verb
and the head of the noun phrase. This feature en-
codes the collocation information between the can-
didate light verb and the head noun of its object.
Levin?s Class: it is observed that members within
certain groups of verb classes are legitimate candi-
dates to form acceptable LVCs (Fazly et al, 2005).
For example, many sound emission verbs accord-
ing to Levin (Levin, 1993), such as clap, whis-
tle, and plop, can be used to generate legitimate
LVCs. Phrases such as make a clap/plop/whistle are
all highly acceptable LVCs by humans even though
some of them, such as make a plop rarely occur
within corpora. We formulate a vector for all the
256 Levin?s verb classes and turn the correspond-
ing class-bits on when the verb usage of the head
noun in a candidate LVC belongs to these classes.
We add one extra class, other, to be mapped to those
verbs which are not included in any one of these 256
Levin?s verb classes.
Other Features: we construct other local con-
textual features, for example, the part of speech of
the word immediately before the light verb (titled
posBefore) and after the whole phrase (posAfter).
We also encode the determiner within all candidate
LVCs as another lexical feature (Determiner). We
examine many other combinations of these contex-
tual features. However, only those features that con-
tribute positively to achieve the highest performance
of the classifier are listed for detailed analysis in the
next section.
34
4 Experiments and Analysis
In this section, we report in detail our experimental
settings and provide in-depth analysis on the inter-
actions among features. First, we present our mo-
tivation and methodology to generate the new data
set. Then we describe our experimental results and
analysis.
4.1 Data Preparation and Annotation
The data set is generated from BNC, a balanced syn-
chronic corpus containing 100 million words col-
lected from various sources of British English. We
begin our sentence selection process with the ex-
amination of a handful of previously investigated
verbs (Fazly and Stevenson, 2007; Butt, 2003).
Among them, we pick the 6 most frequently used
English light verbs: do, get, give, have, make and
take.
To identify potential LVCs within sentences, we
first extract all sentences where one or more of the
six verbs occur from BNC (XML Edition) and then
parse these sentences with Charniak?s parser (Char-
niak and Johnson, 2005). We focus on the ?verb
+ noun object? pattern and choose all the sentences
which have a direct NP object for the target verbs.
We then collect a total of 207, 789 sentences.
We observe that within all these chosen sentences,
the distribution of true LVCs is still low. We there-
fore use three resources to filter out trivial nega-
tive examples. Firstly, We use WordNet (Fellbaum,
1998) to identify the head noun in the object position
which can be used as both a noun and a verb. Then,
we use frequency counts gathered from BNC to fil-
ter out candidates whose verb usage is smaller than
their noun usage. Finally, we use NomLex (Meyers
et al, 1998) to recognize those head words in the
object position whose noun forms and verb forms
are derivationally related, such as transmission and
transmit. We keep all candidates whose object head
nouns are derivationlly related to a verb according
to a gold-standard word list we extract from Nom-
Lex5. With this pipeline method, we filter out ap-
proximately 55% potential negative examples. This
leaves us with 92, 415 sentences which we sample
about 4% randomly to present to annotators. This
filtering method successfully improves the recall of
5We do not count those nouns ending with er and ist
the positive examples and ensures us a corpus with
balanced examples.
A website6 is set up for annotators to annotate the
data. Each potential LVC is presented to the anno-
tator in a sentence. The annotator is asked to decide
whether this phrase within the given sentence is an
LVC and to choose an answer from one of these four
options: Yes, No, Not Sure, and Idiom.
Detailed annotation instructions and LVC exam-
ples are given on the annotation website. When fac-
ing difficult examples, the annotators are instructed
to follow a general ?replacing? principle, i.e, if the
candidate light verb within the sentence can be re-
placed by the verb usage of its direct object noun
and the meaning of the sentence does not change,
that verb is regarded as a light verb and the candidate
is an LVC. Each example is annotated by two anno-
tators and We only accept examples where both an-
notators agree on positive or negative. We generate a
total of 1, 039 positive examples and 1, 123 negative
examples. Among all these positive examples, there
are 760 distinctive LVC phrases and 911 distinctive
verb phrases with the pattern ?verb + noun object?
among negative examples. The generated data set
therefore gives the classifier the 52.2% chance base-
line if the classifier always votes the majority class
in the data set.
4.2 Evaluation Metrics
For each experiment, we evaluate the performance
with three sets of metrics. We first report the stan-
dard accuracy on the test data set. Since accuracy
is argued not to be a sufficient measure of the eval-
uation of a binary classifier (Fazly et al, 2009) and
some previous works also report F1 values for the
positive classes, we therefore choose to report the
precision, recall and F1 value for both positive and
negative classes.
True Class
+ -
Predicted Class + tp fp
- fn tn
Table 1: Confusion matrix to define true positive (tp),
true negative (tn), false positive (fp) and false negative
(fn).
6http://cogcomp.cs.illinois.edu/?ytu/test/LVCmain.html
35
Based on the classic confusion matrix as shown in
Table 1, we calculate the precision and recall for the
positive class in equation 1:
P+ = tptp + fp R
+ = tptp + fn (1)
And similarly, we use equation 2 for negative class.
And the F1 value is the harmonic mean of the preci-
sion and recall of each class.
P? = tntn + fn R
? = tntn + fp (2)
4.3 Experiments with Contextual Features
In our experiments, We aim to build a high perfor-
mance LVC classifier as well as to analyze the in-
teraction between contextual and statistical features.
We randomly sample 90% sentences for training and
the rest for testing. Our chance baseline is 52.2%,
which is the percentage of our majority class in the
data set. As shown in Table 2, the classifier reaches
an 86.3% accuracy using all contextual features de-
scribed in previous section 3.2. Interestingly, we ob-
serve that adding other statistical features actually
hurts the performance. The classifier can effectively
learn when trained with discrete contextual features.
Label Precision Recall F1
+ 86.486 84.211 85.333
- 86.154 88.189 87.160
Accuracy 86.307
Chance Baseline 52.2
Table 2: By using all our contextual features, our classi-
fier achieves overall 86.307% accuracy.
In order to examine the effectiveness of each indi-
vidual feature, we conduct an ablation analysis and
experiment to use only one of them each time. It is
shown in Table 3 that LV-NounObj is found to be the
most effective contextural feature since it boosts the
baseline system up the most, an significant increase
of 31.6%.
We then start from this most effective feature, LV-
NounObj and add one feature each step to observe
the change of the system accuracy. The results are
listed in Table 4. Other significant features are fea-
tures within the candidate LVCs themselves such as
Determiner, Noun Object and Levin?s Class related
Features Accuracy Diff(%)Baseline (chance) 52.2
LV-NounObj 83.817 +31.6
Noun Object 79.253 +27.1
Determiner 72.614 +20.4
Levin?s Class 69.295 +17.1
posBefore 53.112 +0.9
posAfter 51.037 -1.1
Table 3: Using only one feature each time. LV-NounObj
is the most effective feature. Performance gain is associ-
ated with a plus sign and otherwise a negative sign.
to the object noun. This observation agrees with pre-
vious research that the acceptance of LVCs is closely
correlated to the linguistic properties of their compo-
nents. The part of speech of the word after the phrase
seems to have negative effect on the performance.
However, experiments show that without this fea-
ture, the overall performance decreases.
Features Accuracy Diff(%)Baseline (chance) 52.2
+ LV-NounObj 83.817 +31.6
+ Noun Object 84.232 +0.4
+ Levin?s Class 84.647 +0.4
+ posBefore 84.647 0.0
+ posAfter 83.817 -0.8
+ Determiner 86.307 +2.5
Table 4: Ablation analysis for contextual features. Each
feature is added incrementally at each step. Performance
gain is associated with a plus sign otherwise a negative
sign.
4.4 Experiments with Statistical Features
When using statistical features, instead of directly
using the value, we discretize each value to a binary
feature. On the one hand, our experiments show that
this way of transformation achieves the best perfor-
mance. On the other hand, the transformation plays
an analogical role as a kernel function which maps
one dimensional non-linear separable examples into
an infinite or high dimensional space to render the
data linearly separable.
In these experiments, we use only numerical fea-
tures described in section 3.1. And it is interesting
to observe that those features achieve very similar
36
Label Precision Recall F1
+ 86.481 85.088 86.463
- 86.719 87.402 87.059
Accuracy 86.307
Table 5: Best performance achieved with statistical fea-
tures. Comparing to Table 2, the performance is similar
to that trained with all contextual features.
performance as the contextual features as shown in
Table 5.
To validate that the similar performance is not
incidental. We then separate our data into 10-fold
training and testing sets and learn independently
from each fold of these ten split. Figure 1, which
shows the comparison of accuracies for each data
fold, indicates the comparable results for each fold
of the data. Therefore, we conclude that the similar
effect achieved by training with these two groups of
features is not accidental.
 50
 60
 70
 80
 90
 100
0 1 2 3 4 5 6 7 8 9
Ac
cu
ra
cy
Ten folds in the Data Set
Accuracy of each fold using statistic or contextual features
Contextual Features
Statistic Features
Figure 1: Classifier Accuracy of each fold of all 10 fold
testing data, trained with groups of statistical features and
contextual features separately. The similar height of each
histogram indicates the similar performance over each
data separation and the similarity is not incidental.
We also conduct an ablation analysis with statis-
tical features. Similar to the ablation analyses for
contextual features, we first find that the most ef-
fective statistical feature is Cpmi, the collocational
based point-wise mutual information. Then we add
one feature at each step and show the increasing
performance in Table 6. Cpmi is shown to be a
good indicator for LVCs and this observation agrees
with many previous works on the effectiveness of
Features Accuracy Diff(%)BaseLine (chance) 52.2
+ Cpmi 83.402 +31.2
+ Deverbal v/n Ratio 85.892 +2.5
+ Phrase Size 86.307 +0.4
Table 6: Ablation analysis for statistical features. Each
feature is added incrementally at each step. Performance
gain is associated with a plus sign.
point-wise mutual information in MWE identifica-
tion tasks.
4.5 Interaction between Contextual and
Statistical Features
Experiments from our previous sections show that
two types of features which are cosmetically differ-
ent actually achieve similar performance. In the ex-
periments described in this section, we intend to do
further analysis to identify further the relations be-
tween them.
4.5.1 Situation when they are similar
Our ablation analysis shows that Cpmi and LV-
NounObj features are the most two effective features
since they boost the baseline performance up more
than 30%. We then train the classifier with them to-
gether and observe that the classifier exhibits sim-
ilar performance as the one trained with them in-
dependently as shown in Table 7. This result indi-
cates that these two types of features actually pro-
vide similar knowledge to the system and therefore
combining them together does not provide any addi-
tional new information. This observation also agrees
with the intuition that point-wise mutual informa-
tion basically provides information on word collo-
cations (Church and Hanks, 1990).
Feature Accuracy F1+ F1-
LV-NounObj 83.817 82.028 85.283
Cpmi 83.402 81.481 84.962
Cpmi+LV-NounObj 83.817 82.028 85.283
Table 7: The classifier achieves similar performance
trained jointly with Cpmi and LV-NounObj features, com-
paring with the performance trained independently.
37
4.5.2 Situation when they are different
Token-based LVC identification is a difficult task
on the basis of surface structures since they always
exhibit identical surface properties. However, can-
didate LVCs with identical surface structures in both
positive and negative examples provide an ideal test
bed for the functionality of local contextual features.
For example, consider again these two aforemen-
tioned sentences which are repeated here for refer-
ence:
1. He had a look of childish bewilderment on his
face.
2. I?ve arranged for you to have a look at his file
in our library.
The system trained only with statistic features can-
not distinguish these two examples since their type-
based statistical features are exactly the same. How-
ever, the classifier trained with local contextual fea-
tures is expected to perform better since it contains
feature information from surrounding words. To
verify our hypothesis, we extract all examples in
our data set which have this property and then se-
lect same number of positive and negative examples
from them to formulate our test set. We then train
out classifier with the rest of the data, independently
with contextual features and statistical features. As
shown in Table 8, the experiment results validate
our hypothesis and show that the classifier trained
with contextual features performs significantly bet-
ter than the one trained with statistical features. The
overall lower system results also indicate that indeed
the test set with all ambiguous examples is a much
harder test set.
One final observation is the extremely low F1
value for negative class and relatively good perfor-
mance for positive class when trained with only sta-
tistical features. This may be explained by the fact
that statistical features have stronger bias toward
predicting examples as positive and can be used as
an unsupervised metric to acquire real LVCs in cor-
pora.
5 Conclusion and Further Research
In this paper, we propose an in-depth case study on
LVC recognition, in which we build a supervised
learning system for automatically identifying LVCs
Classifier Accuracy F1+ F1-
Contextual 68.519 75.362 56.410
Statistical 51.852 88.976 27.778
Diff (%) +16.7 -13.6 +28.3
Table 8: Classifier trained with local contextual features
is more robust and significantly better than the one trained
with statistical features when the test data set consists of
all ambiguous examples.
in context. Our learning system achieves an 86.3%
accuracy with a baseline (chance) performance of
52.2% when trained with groups of either contex-
tual or statistical features. In addition, we exploit in
detail the interaction of these two groups of contex-
tual and statistical features and show that the system
trained with these two types of cosmetically differ-
ent features actually reaches similar performance in
our learning framework. However, when it comes to
the situation where the surface structures of candi-
date LVCs are identical, the system trained with con-
textual features which include information on sur-
rounding words provides better and more robust per-
formance.
In this study, we also construct a balanced bench-
mark dataset with 2,162 sentences from BNC for
token-based classification of English LVCs. And
this data set is publicly available and is also a use-
ful computational resource for research on MWEs in
general.
There are many aspects for further research of the
current study. One direction for further improve-
ment would be to include more long-distance fea-
tures, such as parse tree path, to test the sensitivity of
the LVC classifier to those features and to examine
more extensively the combination of the contextual
and statistical features. Another direction would be
to adapt our system to other MWE types and to test
if the analysis on contextual and statistical features
in this study also applies to other MWEs.
Acknowledgments
The authors would like to thank all annotators who
annotated the data via the web interface and four
annonymous reviewers for their valuable comments.
The research in this paper was supported by the Mul-
timodal Information Access & Synthesis Center at
38
UIUC, part of CCICADA, a DHS Science and Tech-
nology Center of Excellence.
References
L. Barrett and A. Davis. 2003. Diagnostics for determing
compatibility in english support verb nominalization
pairs. In Proceedings of CICLing-2003, pages 85?90.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado,
and M. Pinkal. 2009. Using framenet for seman-
tic analysis of german: annotation, representation
and automation. In Hans Boas, editor, Multilingual
FrameNets in Computational Lexicography: methods
and applications, pages 209?244. Mouton de Gruyter.
M. Butt. 2003. The light verb jungle. In Harvard Work-
ing Paper in Linguistics, volume 9, pages 1?49.
C. Chang and C. Lin, 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of ACL-2005.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1), March.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991. Us-
ing statistics in lexical analysis. In Lexical Acquisi-
tion: Exploiting On-Line Resources to Build a Lexi-
con, pages 115?164. Erlbaum.
P. Cook, A. Fazly, and S. Stevenson. 2007. Pulling their
weight: Exploiting syntactic forms for the automatic
identification of idiomatic expressions in context. In
Proceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pages 41?48, Prague,
Czech Republic, June. Association for Computational
Linguistics.
A. Fazly and S. Stevenson. 2006. Automatically con-
structing a lexicon of verb phrase idiomatic combina-
tions. In Proceedings of EACL-2006.
A. Fazly and S. Stevenson. 2007. Distinguishing sub-
types of multiword expressions using linguistically-
motivated statistical measures. In Proceedings of the
Workshop on A Broader Perspective on Multiword Ex-
pressions, pages 9?16, Prague, Czech Republic, June.
A. Fazly, R. North, and S. Stevenson. 2005. Auto-
matically distinguishing literal and figurative usages
of highly polysemous verbs. In Proceedings of the
ACL-SIGLEX Workshop on Deep Lexical Acquisition,
pages 38?47, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
A. Fazly, P. Cook, and S. Stevenson. 2009. Unsupervised
type and token identification of idiomatic expression.
Comutational Linguistics.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
O. Jespersen. 1965. A Modern English Grammar on His-
torical Principles, Part VI, Morphology. Aeorge Allen
and Unwin Ltd.
G. Katz and E. Giesbrecht. 2006. Automatic identi-
fication of non-compositional multi-word expressions
using latent semantic analysis. In Proceedings of the
Workshop on Multiword Expressions: Identifying and
Exploiting Underlying Properties, pages 12?19.
K. Kearns. 2002. Light verbs in english. In
http://www.ling.canterbury.ac.nz/documents.
B. Levin. 1993. English Verb Classes and Alternations,
A Preliminary Investigation. University of Chicago
Press.
A. Meyers, C. Macleod, R. Yangarber, R. Grishman,
L. Barrett, and R. Reeves. 1998. Using nomlex to
produce nominalization patterns for information ex-
traction. In Proceedings of COLING-ACL98 Work-
shop:the Computational Treatment of Nominals.
R. North. 2005. Computational measures of the ac-
ceptability of light verb constructions. University of
Toronto, Master Thesis.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In Proceedings of
the International Conference on Language Resources
and Evaluation (LREC).
I. Sag, T. Baldwin, F. Bond, and A. Copestake. 2002.
Multiword expressions: A pain in the neck for nlp. In
In Proc. of the 3rd International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing-2002, pages 1?15.
T. Samardz?ic? and P. Merlo. 2010. Cross-lingual vari-
ation of light verb constructions: Using parallel cor-
pora and automatic alignment for linguistic research.
In Proceedings of the 2010 Workshop on NLP and Lin-
guistics: Finding the Common Ground, pages 52?60,
Uppsala, Sweden, July.
S. Stevenson, A. Fazly, and R. North. 2004. Statistical
measures of the semi-productivity of light verb con-
structions. In Proceedings of ACL-04 workshop on
Multiword Expressions: Integrating Processing, pages
1?8.
Y. Tan, M. Kan, and H. Cui. 2006. Extending corpus-
based identification of light verb constructions using
a supervised learning framework. In Proceedings of
EACL-06 workshop on Multi-word-expressions in a
multilingual context, pages 49?56.
S. Venkatapathy and A. Joshi. 2005. Measuring the rel-
ative compositionality of verb-noun (v-n) collocations
by integrating features. In Proceedings of HLT and
EMNLP05, pages 899?906.
39
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 40?44,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Inference Protocols for Coreference Resolution
Kai-Wei Chang Rajhans Samdani
Alla Rozovskaya Nick Rizzolo
Mark Sammons Dan Roth
University of Illinois at Urbana-Champaign
{kchang10|rsamdan2|rozovska|rizzolo|mssammon|danr}@illinois.edu
Abstract
This paper presents Illinois-Coref, a system
for coreference resolution that participated
in the CoNLL-2011 shared task. We in-
vestigate two inference methods, Best-Link
and All-Link, along with their corresponding,
pairwise and structured, learning protocols.
Within these, we provide a flexible architec-
ture for incorporating linguistically-motivated
constraints, several of which we developed
and integrated. We compare and evaluate the
inference approaches and the contribution of
constraints, analyze the mistakes of the sys-
tem, and discuss the challenges of resolving
coreference for the OntoNotes-4.0 data set.
1 Introduction
The coreference resolution task is challenging, re-
quiring a human or automated reader to identify
denotative phrases (?mentions?) and link them to
an underlying set of referents. Human readers use
syntactic and semantic cues to identify and dis-
ambiguate the referring phrases; a successful auto-
mated system must replicate this behavior by linking
mentions that refer to the same underlying entity.
This paper describes Illinois-Coref, a corefer-
ence resolution system built on Learning Based
Java (Rizzolo and Roth, 2010), that participated
in the ?closed? track of the CoNLL-2011 shared
task (Pradhan et al, 2011). Building on elements
of the coreference system described in Bengtson
and Roth (2008), we design an end-to-end system
(Sec. 2) that identifies candidate mentions and then
applies one of two inference protocols, Best-Link
and All-Link (Sec. 2.3), to disambiguate and clus-
ter them. These protocols were designed to easily
incorporate domain knowledge in the form of con-
straints. In Sec. 2.4, we describe the constraints that
we develop and incorporate into the system. The
different strategies for mention detection and infer-
ence, and the integration of constraints are evaluated
in Sections 3 and 4.
2 Architecture
Illinois-Coref follows the architecture used in
Bengtson and Roth (2008). First, candidate men-
tions are detected (Sec. 2.1). Next, a pairwise
classifier is applied to each pair of mentions, gen-
erating a score that indicates their compatibility
(Sec. 2.2). Next, at inference stage, a coreference
decoder (Sec. 2.3) aggregates these scores into men-
tion clusters. The original system uses the Best-Link
approach; we also experiment with All-Link decod-
ing. This flexible decoder architecture allows lin-
guistic or knowledge-based constraints to be easily
added to the system: constraints may force mentions
to be coreferent or non-coreferent and can be option-
ally used in either of the inference protocols. We
designed and implemented several such constraints
(Sec. 2.4). Finally, since mentions that are in single-
ton clusters are not annotated in the OntoNotes-4.0
data set, we remove those as a post-processing step.
2.1 Mention Detection
Given a document, a mention detector generates a
set of mention candidates that are used by the subse-
quent components of the system. A robust mention
detector is crucial, as detection errors will propagate
to the coreference stage. As we show in Sec. 3, the
system that uses gold mentions outperforms the sys-
tem that uses predicted mentions by a large margin,
from 15% to 18% absolute difference.
40
For the ACE 2004 coreference task, a good per-
formance in mention detection is typically achieved
by training a classifier e.g., (Bengtson and Roth,
2008). However, this model is not appropriate for
the OntoNotes-4.0 data set, in which (in contrast to
the ACE 2004 corpus) singleton mentions are not
annotated: a specific noun phrase (NP) may corre-
spond to a mention in one document but will not
be a mention in another document. Therefore, we
designed a high recall (? 90%) and low precision
(? 35%) rule-based mention detection system that
includes all phrases recognized as Named Entities
(NE?s) and all phrases tagged as NPs in the syntac-
tic parse of the text. As a post-processing step, we
remove all predicted mentions that remain in single-
ton clusters after the inference stage.
The best mention detection result on the DEV set1
is 64.93% in F1 score (after coreference resolution)
and is achieved by our best inference protocol, Best-
Link with constraints.
2.2 Pairwise Mention Scoring
The basic input to our inference algorithm is a pair-
wise mention score, which indicates the compatibil-
ity score of a pair of mentions. For any two mentions
u and v, the compatibility score wuv is produced
by a pairwise scoring component that uses extracted
features ?(u, v) and linguistic constraints c:
wuv = w ? ?(u, v) + c(u, v) + t, (1)
where w is a weight vector learned from training
data, c(u, v) is a compatibility score given by the
constraints, and t is a threshold parameter (to be
tuned). We use the same features as Bengtson and
Roth (2008), with the knowledge extracted from the
OntoNotes-4.0 annotation. The exact use of the
scores and the procedure for learning weights w are
specific to the inference algorithm and are described
next.
2.3 Inference
In this section, we present our inference techniques
for coreference resolution. These clustering tech-
niques take as input a set of pairwise mention scores
over a document and aggregate them into globally
1In the shared task, the data set is split into three sets:
TRAIN, DEV, and TEST.
consistent cliques representing entities. We investi-
gate the traditional Best-Link approach and a more
intuitively appealing All-Link algorithm.
2.3.1 Best-Link
Best-Link is a popular approach to coreference
resolution. For each mention, it considers the best
mention on its left to connect to (best according
the pairwise score wuv) and creates a link between
them if the pairwise score is above some thresh-
old. Although its strategy is simple, Bengtson and
Roth (2008) show that with a careful design, it can
achieve highly competitive performance.
Inference: We give an integer linear programming
(ILP) formulation of Best-Link inference in order to
present both of our inference algorithms within the
same framework. Given a pairwise scorer w, we
can compute the compatibility scores ? wuv from
Eq. (1) ? for all mention pairs u and v. Let yuv be
a binary variable, such that yuv = 1 only if u and v
are in the same cluster. For a document d, Best-Link
solves the following ILP formulation:
argmaxy
?
u,v
wuvyuv
s.t
?
u<v
yuv ? 1 ?v,
yuw ? {0, 1}.
(2)
Eq. (2) generates a set of connected components and
all the mentions in each connected component con-
stitute an entity.
Learning: We follow the strategy in (Bengtson
and Roth, 2008, Section 2.2) to learn the pairwise
scoring function w. The scoring function is trained
on:
? Positive examples: for each mention u, we con-
struct a positive example (u, v), where v is the
closest preceding mention in u?s equivalence
class.
? Negative examples: all mention pairs (u, v),
where v is a preceding mention of u and u, v
are not in the same class.
As a result of the singleton mentions not being anno-
tated, there is an inconsistency in the sample distri-
butions in the training and inference phases. There-
fore, we apply the mention detector to the training
set, and train the classifier using the union set of gold
and predicted mentions.
41
2.3.2 All-Link
The All-Link inference approach scores a cluster-
ing of mentions by including all possible pairwise
links in the score. It is also known as correlational
clustering (Bansal et al, 2002) and has been applied
to coreference resolution in the form of supervised
clustering (Mccallum and Wellner, 2003; Finley and
Joachims, 2005).
Inference: Similar to Best-Link, for a document d,
All-Link inference finds a clustering All-Link(d;w)
by solving the following ILP problem:
argmaxy
?
u,v
wuvyuv
s.t yuw ? yuv + yvw ? 1 ?u,w, v,
yuw ? {0, 1}.
(3)
The inequality constraints in Eq. (3) enforce the
transitive closure of the clustering. The solution of
Eq. (3) is a set of cliques, and the mentions in the
same cliques corefer.
Learning: We present a structured perceptron al-
gorithm, which is similar to supervised clustering
algorithm (Finley and Joachims, 2005) to learn w.
Note that as an approximation, it is certainly pos-
sible to use the weight parameter learned by using,
say, averaged perceptron over positive and negative
links. The pseudocode is presented in Algorithm 1.
Algorithm 1 Structured Perceptron like learning al-
gorithm for All-Link inference
Given: Annotated documents D and initial
weight winit
Initialize w ? winit
for Document d in D do
Clustering y ? All-Link(d;w)
for all pairs of mentions u and v do
I1(u, v) = [u, v coreferent in D]
I2(u, v) = [y(u) = y(v)]
w ? w +
(
I1(u, v)? I2(u, v)
)
?(u, v)
end for
end for
return w
For the All-Link clustering, we drop one of the
three transitivity constraints for each triple of men-
tion variables. Similar to Pascal and Baldridge
(2009), we observe that this improves accuracy ?
the reader is referred to Pascal and Baldridge (2009)
for more details.
2.4 Constraints
The constraints in our inference algorithm are based
on the analysis of mistakes on the DEV set2. Since
the majority of errors are mistakes in recall, where
the system fails to link mentions that refer to the
same entity, we define three high precision con-
straints that improve recall on NPs with definite de-
terminers and mentions whose heads are NE?s.
The patterns used by constraints to match mention
pairs have some overlap with those used by the pair-
wise mention scorer, but their formulation as con-
straints allow us to focus on a subset of mentions
to which a certain pattern applies with high preci-
sion. For example, the constraints use a rule-based
string similarity measure that accounts for the in-
ferred semantic type of the mentions compared. Ex-
amples of mention pairs that are correctly linked by
the constraints are: Governor Bush? Bush; a cru-
cial swing state , Florida? Florida; Sony itself ?
Sony; Farmers? Los Angeles - based Farmers.
3 Experiments and Results
In this section, we present the performance of the
system on the OntoNotes-4.0 data set. A previous
experiment using an earlier version of this data can
be found in (Pradhan et al, 2007). Table 1 shows the
performance for the two inference protocols, with
and without constraints. Best-Link outperforms All-
Link for both predicted and gold mentions. Adding
constraints improves the performance slightly for
Best-Link on predicted mentions. In the other con-
figurations, the constraints either do not affect the
performance or slightly degrade it.
Table 2 shows the results obtained on TEST, using
the best system configurations found on DEV. We
report results on predicted mentions with predicted
boundaries, predicted mentions with gold bound-
aries, and when using gold mentions3.
2We provide a more detailed analysis of the errors in Sec. 4.
3Note that the gold boundaries results are different from the
gold mention results. Specifying gold mentions requires coref-
erence resolution to exclude singleton mentions. Gold bound-
aries are provided by the task organizers and also include sin-
gleton mentions.
42
Method
Pred. Mentions w/Pred. Boundaries Gold Mentions
MD MUC BCUB CEAF AVG MUC BCUB CEAF AVG
Best-Link 64.70 55.67 69.21 43.78 56.22 80.58 75.68 64.69 73.65
Best-Link W/ Const. 64.69 55.8 69.29 43.96 56.35 80.56 75.02 64.24 73.27
All-Link 63.30 54.56 68.50 42.15 55.07 77.72 73.65 59.17 70.18
All-Link W/ Const. 63.39 54.56 68.46 42.20 55.07 77.94 73.43 59.47 70.28
Table 1: The performance of the two inference protocols on both gold and predicted mentions. The systems are
trained on the TRAIN set and evaluated on the DEV set. We report the F1 scores (%) on mention detection (MD)
and coreference metrics (MUC, BCUB, CEAF). The column AVG shows the averaged scores of the three coreference
metrics.
Task MD MUC BCUB CEAF AVG
Pred. Mentions w/ Pred. Boundaries 64.88 57.15 67.14 41.94 55.96
Pred. Mentions w/ Gold Boundaries 67.92 59.79 68.65 41.42 56.62
Gold Mentions - 82.55 73.70 65.24 73.83
Table 2: The results of our submitted system on the TEST set. The system uses Best-Link decoding with constraints
on predicted mentions and Best-Link decoding without constraints on gold mentions. The systems are trained on a
collection of TRAIN and DEV sets.
4 Discussion
Most of the mistakes made by the system are due to
not linking co-referring mentions. The constraints
improve slightly the recall on a subset of mentions,
and here we show other common errors for the sys-
tem. For instance, the system fails to link the two
mentions, the Emory University hospital in Atlanta
and the hospital behind me, since each of the men-
tions has a modifier that is not part of the other men-
tion. Another common error is related to pronoun
resolution, especially when a pronoun has several
antecedents in the immediate context, appropriate in
gender, number, and animacy, as in ? E. Robert Wal-
lach was sentenced by a U.S. judge in New York to
six years in prison and fined $ 250,000 for his rack-
eteering conviction in the Wedtech scandal .?: both
E. Robert Wallach and a U.S. judge are appropri-
ate antecedents for the pronoun his. Pronoun errors
are especially important to address since 35% of the
mentions are pronouns.
The system also incorrectly links some mentions,
such as: ?The suspect said it took months to repack-
age...? (?it? cannot refer to a human); ?They see
them.? (subject and object in the same sentence are
linked); and ?Many freeway accidents occur simply
because people stay inside the car and sort out...?
(the NP the car should not be linked to any other
mention, since it does not refer to a specific entity).
5 Conclusions
We have investigated a coreference resolution sys-
tem that uses a rich set of features and two popular
types of clustering algorithm.
While the All-Link clustering seems to be capable
of taking more information into account for making
clustering decisions, as it requires each mention in
a cluster to be compatible with all other mentions in
that cluster, the Best-Link approach still outperforms
it. This raises a natural algorithmic question regard-
ing the inherent nature of clustering style most suit-
able for coreference and regarding possible ways of
infusing more knowledge into different coreference
clustering styles. Our approach accommodates in-
fusion of knowledge via constraints, and we have
demonstrated its utility in an end-to-end coreference
system.
Acknowledgments This research is supported by the Defense
Advanced Research Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL) prime contract no.
FA8750-09-C-0181 and the Army Research Laboratory (ARL) under
agreement W911NF-09-2-0053. Any opinions, findings, and conclu-
sion or recommendations expressed in this material are those of the au-
thor(s) and do not necessarily reflect the view of the DARPA, AFRL,
ARL or the US government.
43
References
N. Bansal, A. Blum, and S. Chawla. 2002. Correlation
clustering. In Proceedings of the 43rd Symposium on
Foundations of Computer Science.
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP, 10.
T. Finley and T. Joachims. 2005. Supervised cluster-
ing with support vector machines. In Proceedings
of the International Conference on Machine Learning
(ICML).
A. Mccallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).
D. Pascal and J. Baldridge. 2009. Global joint models for
coreference resolution and named entity classification.
In Procesamiento del Lenguaje Natural.
S. Pradhan, L. Ramshaw, R. Weischedel, J. MacBride,
and L. Micciulla. 2007. Unrestricted Coreference:
Identifying Entities and Events in OntoNotes. In in
Proceedings of the IEEE International Conference on
Semantic Computing (ICSC), September 17-19.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. Conll-2011 shared
task: Modeling unrestricted coreference in ontonotes.
In Proceedings of the Annual Conference on Compu-
tational Natural Language Learning (CoNLL).
N. Rizzolo and D. Roth. 2010. Learning Based Java
for Rapid Development of NLP Systems. In Proceed-
ings of the International Conference on Language Re-
sources and Evaluation (LREC), Valletta, Malta, 5.
44
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 272?280,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
The UI System in the HOO 2012 Shared Task on Error Correction
Alla Rozovskaya Mark Sammons Dan Roth
Cognitive Computation Group
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,mssammon,danr}@illinois.edu
Abstract
We describe the University of Illinois (UI) sys-
tem that participated in the Helping Our Own
(HOO) 2012 shared task, which focuses on
correcting preposition and determiner errors
made by non-native English speakers. The
task consisted of three metrics: Detection,
Recognition, and Correction, and measured
performance before and after additional revi-
sions to the test data were made. Out of 14
teams that participated, our system scored first
in Detection and Recognition and second in
Correction before the revisions; and first in
Detection and second in the other metrics af-
ter revisions. We describe our underlying ap-
proach, which relates to our previous work in
this area, and propose an improvement to the
earlier method, error inflation, which results
in significant gains in performance.
1 Introduction
The task of correcting grammar and usage mistakes
made by English as a Second Language (ESL) writ-
ers is difficult: many of these errors are context-
sensitive mistakes that confuse valid English words
and thus cannot be detected without considering the
context around the word.
Below we show examples of two common ESL
mistakes considered in this paper:
1. ?Nowadays ?*/the Internet makes us closer and closer.?
2. ?I can see at*/on the list a lot of interesting sports.?
In (1), the definite article is incorrectly omitted.
In (2), the writer uses an incorrect preposition.
This paper describes the University of Illinois sys-
tem that participated in the HOO 2012 shared task
on error detection and correction in the use of prepo-
sitions and determiners (Dale et al, 2012). Fourteen
teams took part in the the competition. The scoring
included three metrics: Detection, Recognition, and
Correction, and our team scored first or second in
each metric (see Dale et al (2012) for details).
The UI system consists of two components, a de-
terminer classifier and a preposition classifier, with
a common pre-processing step that corrects spelling
mistakes. The determiner system builds on the ideas
described in Rozovskaya and Roth (2010c). The
preposition classifier uses a combined system, build-
ing on work described in Rozovskaya and Roth
(2011) and Rozovskaya and Roth (2010b).
Both the determiner and the preposition systems
apply the method proposed in our earlier work,
which uses the error distribution of the learner data
to generate artificial errors in training data. The orig-
inal method was proposed for adding artificial er-
rors when training on native English data. In this
task, however, we apply this method when training
on annotated ESL data. Furthermore, we introduce
an improvement that is conceptually simple but very
effective and which also proved to be successful in
an earlier error correction shared task (Dale and Kil-
garriff, 2011; Rozovskaya et al, 2011). We identify
the unique characteristics of the error correction task
and analyze the limitations of existing approaches to
error correction that are due to these characteristics.
Based on this analysis, we propose the error infla-
tion method (Sect. 6.2).
In this paper, we first briefly discuss the task (Sec-
272
tion 2) and present our overall approach (Section
3. Next, we describe the spelling correction mod-
ule (Section 4). Section 5 provides an overview of
the training approaches for error correction tasks.
We present the inflation method in Section 6. Next,
we describe the determiner error correction system
(Section 7), and the preposition error correction
module (Section 8). In Section 9, we present the
performance results of our system in the competi-
tion. We conclude with a brief discussion (Section
10).
2 Task Description
The HOO 2012 shared task focuses on correcting
determiner and preposition errors made by non-
native speakers of English. These errors are some of
the most common and also some of the most difficult
for ESL learners (Leacock et al, 2010); even very
advanced learners make these mistakes (Rozovskaya
and Roth, 2010a).
The training data released by the task organizers
comes from the publicly available FCE corpus (Yan-
nakoudakis et al, 2011). The original FCE data set
contains 1244 essays written by non-native English
speakers and is corrected and error-tagged using a
detailed error classification schema. The HOO train-
ing data contains 1000 of those files.1 The test data
for the task consists of an additional set of 100 stu-
dent essays, different from the 1244 above.
Since the HOO task focuses on determiner and
preposition mistakes, only annotations marking
preposition and determiner mistakes were kept.
Note that while the other error annotations were
removed, the errors still remain in the HOO data.
More details can be found in Dale et al (2012).
3 System Overview
Our system consists of two components that address
individually article2 and preposition errors and use
the same pre-processing.
1In addition, the participating teams were allowed to use for
training the remaining 244 files of this corpus, as well as any
other data. We also use a publicly available data set of native
English, Google Web 1T corpus (Brants and Franz, 2006), in
one of our models.
2We will use the terms ?article-? and ?determiner errors? in-
terchangeably: article errors constitute the majority of deter-
miner errors, and we address only article mistakes.
The first pre-processing step is correcting spelling
errors. Since the essays were written by students of
English as a Second language, and these essays were
composed on-the-fly, they contain a large number of
spelling errors. These errors add noise to the context
around the target word (article or preposition). Good
context is crucial for robust detection and correction
of article and preposition mistakes.
After spelling errors are corrected, we run a sen-
tence splitter, part-of-speech tagger3 and shallow
parser4 (Punyakanok and Roth, 2001) on the data.
Both the article and the preposition systems use fea-
tures based on the output of these tools.
We made a 244-document subset of the FCE data
a held-out set for development. The results in Sec-
tions 7 and 8 give performance on this held-out set,
where we use the HOO data (1000 files) for train-
ing. The actual performance in the task (Section 9)
reflects the system trained on the whole set of 1244
documents.
Our article and preposition modules build on the
elements of the systems described in Rozovskaya
and Roth (2010b), Rozovskaya and Roth (2010c)
and Rozovskaya and Roth (2011). All article sys-
tems are trained using the Averaged Perceptron
(AP) algorithm (Freund and Schapire, 1999), im-
plemented within Learning Based Java (Rizzolo and
Roth, 2010). Our preposition systems combine the
AP algorithm with the Na??ve Bayes (NB) classifier
with prior parameters adapted to the learner data
(see Section 5). The AP systems are trained using
the inflation method (see Section 6.2).
We submitted 10 runs. All of our runs achieved
comparable performance. Sections 7 and 8 describe
our modules.
4 Correcting Spelling Errors
Analysis of the HOO data made clear the need for
a variety of corrections beyond the immediate scope
of the current evaluation. When a mistake occurs in
the vicinity of a target (i.e. preposition or article) er-
ror, it may result in local cues that obscure the nature
of the desired correction.
3http://cogcomp.cs.illinois.edu/page/
software view/POS
4http://cogcomp.cs.illinois.edu/page/
software view/Chunker
273
The following example illustrates such a problem:
?In my opinion your parents should be arrive in the
first party of the month becouse we could be go in
meeting with famous writer, travelled and journalist
who wrote book about Ethiopia.?
In this sample sentence, there are multiple errors
in close proximity: the misspelled word becouse; the
verb form should be arrive; the use of the word party
instead of part; the verb travelled instead of a noun
form; an incorrect preposition in (in meeting).
The context thus contains a considerable amount
of noise that is likely to negatively affect system per-
formance. To address some of these errors, we run a
standard spell-checker over the data.
We use Jazzy5, an open-source Java spell-checker.
The distribution, however, comes only with a US
English dictionary, which also has gaps in its cov-
erage of the language. The FCE corpus prefers UK
English spelling, so we use a mapping from US to
UK English6 to automatically correct the original
dictionary. We also keep the converted US spelling,
since our preposition module makes use of native
English data, where the US spelling is prevalent.
The Jazzy API allows the client to query a word,
and get a list of candidate corrections sorted in or-
der of edit distance from the original term. We
take the first suggestion and replace the original
word. The resulting substitution may be incorrect,
which may in turn mislead the downstream correc-
tion components. However, manual evaluation of
the spelling corrections suggested about 80% were
appropriate, and experimental evaluation on the cor-
pus development set indicated a modest overall im-
provement when the spell-checked documents were
used in place of the originals.
5 Training for Correction Tasks
The standard approach to correcting context-
sensitive ESL mistakes follows the methodology of
the context-sensitive spelling correction task that ad-
dresses such misspellings as their and there (Carl-
son et al, 2001; Golding and Roth, 1999; Golding
and Roth, 1996; Carlson and Fette, 2007; Banko and
Brill, 2001).
Following Rozovskaya and Roth (2010c), we dis-
5http://jazzy.sourceforge.net/
6http://www.tysto.com/articles05/q1/20050324uk-us.shtml
tinguish between two training paradigms in ESL er-
ror correction, depending on whether the author?s
original word choice is used in training as a feature.
In the standard context-sensitive spelling correction
paradigm, the decision of the classifier depends only
on the context around the author?s word, e.g. arti-
cle or preposition, and the author?s word itself is not
taken into consideration in training.
Mistakes made by non-native speakers obey cer-
tain regularities (Lee and Seneff, 2008; Rozovskaya
and Roth, 2010a). Adding knowledge about typ-
ical errors to a model significantly improves its
performance (Gamon, 2010; Rozovskaya and Roth,
2010c; Dahlmeier and Ng, 2011). Typical errors
may refer both to speakers whose first language is
L1 and to specific authors. For example, non-native
speakers whose first language does not have articles
tend to make more articles errors in English (Ro-
zovskaya and Roth, 2010a).
Since non-native speakers? mistakes are system-
atic, the author?s word choice (the source word)
carries a lot of information. Models that use the
source word in training (Han et al, 2010; Gamon,
2010; Dahlmeier and Ng, 2011) learn which errors
are typical for the learner and thus significantly out-
perform systems that only look at context. We call
these models adapted. Training adapted models re-
quires annotated data, since in native English data
the source word is always correct and thus cannot be
used by the classifier.
In this work, we use two methods of adapting a
model to typical errors that have been proposed ear-
lier. Both methods were originally developed for
models trained on native English data: they use a
small amount of annotated ESL data to generate er-
ror statistics. The artificial errors method is based
on generating artificial errors7 in correct native En-
glish training data. The method was implemented
within the Averaged Perceptron (AP) algorithm (Ro-
zovskaya and Roth, 2010c; Rozovskaya and Roth,
2010b), a discriminative learning algorithm, and this
is the algorithm that we use in this work. The NB-
priors method is a special adaptation technique for
the Na??ve Bayes algorithm (Rozovskaya and Roth,
2011). While NB-priors improves both precision
7For each task, only relevant errors are generated ? for ex-
ample, article mistakes for the article correction task.
274
and recall, the artificial errors approach suffers
from low recall due to error sparsity (Sec. 6.1).
In this work, in the preposition correction task,
we use the NB-priors method without modifications
(as described in the original paper). We use the ar-
tificial errors approach both for article and prepo-
sition error correction but with two important mod-
ifications: we train on annotated ESL data instead
of native data, and use the proposed error inflation
method (described in Section 6) to increase the error
rate in training.
6 Error Inflation
In this section, we show why AP (Freund and
Schapire, 1999), a discriminative classifier, is sen-
sitive to the error sparsity of the data, and propose
a method that addresses the problems raised by this
sensitivity.
6.1 Error Sparsity and Low Recall
The low recall of the AP algorithm is related to the
nature of the error correction tasks, which exhibit
low error rates. Even for ESL writers, over 90% of
their preposition and article usage is correct, which
makes the errors very sparse (Rozovskaya and Roth,
2010c). The low recall problem is, in fact, a special
case of a more general problem where there is one
or a small group of dominant features that are very
strongly correlated with the label. In this case, the
system tends to predict the label that matches this
feature, and tends to not predict it when that fea-
ture is absent. In error correction, which tends to
have a very skewed label distribution, this results in
very few errors being detected by the system: when
training on annotated data with naturally occurring
errors and using the source word as a feature, the
system will learn that in the majority of cases the
source word corresponds to the label, and will tend
to over-predict it, which will result in low recall.
In the artificial errors approach, errors are sim-
ulated according to real observed mistakes. Ta-
ble 1 shows a sample confusion matrix based on
preposition mistakes in the FCE corpus; we show
four rows, but the entire table contains 17 rows and
columns, one for each preposition, and each entry
shows Prob(pi|pj), the probability that the author?s
preposition is pi given that the correct preposition
is pj . The matrix also shows the preposition count
for each source and label in the data set. Given the
entire matrix and the counts, it is also possible to
generate the matrix in the other direction and obtain
Prob(pj |pi), the probability that the correct prepo-
sition is pj given that the author?s preposition is pi.
This other matrix is used for adapting NB with the
priors method.
The confusion matrix is sparse and shows that the
distribution of alternatives for each source preposi-
tion is very different from that of the others. This
strongly suggests that these errors are systematic.
Additionally, most prepositions are used correctly,
so the error rate is very low (the error rate can be
estimated by looking at the matrix diagonal in the
table; for example, the error rate for the preposition
about is lower than for into, since 94.4% of the oc-
currences of label about are correct, but only 76.8%
of label into are correct).
The artificial errors thus model the two proper-
ties that we mentioned: the confusability of differ-
ent preposition pairs and the low error rate, and the
artificial errors are similarly sparse.
6.2 The Error Inflation Method
Two extreme choices for solving the low recall prob-
lem due to error sparsity are: (1) training without the
source word feature or (2) training with this feature,
where the classifier relies on it too much. Models
trained without the source feature have very poor
precision. While the NB-priors method does have
good recall, our expectation is that with the right ap-
proach, a discriminative classifier will also improve
recall, but maintain higher precision as well.
We wish to reduce the confidence that the system
has in the source word, while preserving the knowl-
edge the model has about likely confusions and con-
texts of confused words. To accomplish this, we re-
duce the proportion of correct examples, i.e. exam-
ples where the source and the label are the same,
by some positive constant < 1.0 and distribute the
extra probability mass among the typical errors in
an appropriate proportion by generating additional
error examples. This inflates the proportion of ar-
tificial errors in the training data, and hence the er-
ror rate, while keeping the probability distribution
among likely corrections the same. Increasing the
error rate improves the recall, while the typical er-
275
Label Sources
on about into with as at by for from in of over to
(648) (700) (54) (733) (410) (880) (243) (1394) (515) (2213) (1954) (98) (1418)
on (598) 0.846 0.003 0.003 0.008 0.013 - 0.003 0.022 - 0.076 0.013 0.001 0.009
about (686) 0.004 0.944 - 0.007 - - - 0.022 0.005 0.002 0.016 0.001 -
into (55) 0.001 - 0.768 - - - 0.011 0.011 - 0.147 - - 0.053
with (710) 0.001 0.006 - 0.934 - 0.001 0.007 0.004 0.001 0.027 0.003 - 0.015
Table 1: Confusion matrix for preposition errors. Based on data from the FCE corpus for top 17 most frequent English
prepositions. The left column shows the correct preposition. Each row shows the author?s preposition choices for that label and
Prob(source|label). The sources among, between, under and within are not shown for lack of space; they all have 0 probabilities
in the matrix. The numbers next to the targets show the count of the label (or source) in the data set.
ror knowledge ensures that high precision is main-
tained. This method causes the classifier to rely on
the source feature less and increases the contribu-
tion of the features based on context. The learning
algorithm therefore finds a more optimal balance be-
tween the source feature and the context features.
Algorithm 1 shows the pseudo-code for generat-
ing training data; it takes as input training examples,
the confusion matrix CM as shown in Table 1, and
the inflation constant, and generates artificial source
features for correct training examples.8 An infla-
tion constant value of 1.0 simulates learner mistakes
without inflation. Table 2 shows the proportion of
artificial errors created in training using the inflation
method for different inflation rates.
Algorithm 1 Data Generation with Inflation
Input: Training examples E with correct sources, confusion matrix
CM , inflation constant C
Output: Training examples E with artificial errors
for Example e in E do
Initialize lab? e.label, e.source? e.label
Randomize targets ? CM [lab]
Initialize flag? False
for target t in targets do
if flag equals True then
Break
end if
if t equals lab then
Prob(t) = CM [lab][t] ? C
else
Prob(t) = 1.0?CM [lab][lab]?C1.0?CM [lab][lab] ? CM [lab][t]
end if
x? Random[0, 1]
if x < Prob(t) then
e.source? t
flag? True
end if
end for
end for
return E
8When training on native English data, all examples are cor-
rect. When training on annotated learner data, some examples
will contain naturally occurring mistakes.
Inflation rate
1.0 (Regular) 0.9 0.8 0.7 0.6 0.5
7.7% 15.1% 22.6% 30.1% 37.5% 45.0%
Table 2: Artificial errors. Proportion of generated artificial
preposition errors in training using the inflation method (based
on the FCE corpus).
7 Determiners
Table 4 shows the distribution of determiner errors
in the HOO training set. Even though the majority
of determiner errors involve article mistakes, 14% of
errors are personal and possessive pronouns.9 Most
of the determiner errors involve omitting an article.
Similar error patterns have been observed in other
ESL corpora (Rozovskaya and Roth, 2010a).
Our system focuses on article errors. Because
the majority of determiner errors are omissions, it is
very important to target this subset of mistakes. One
approach would be to consider every space as a pos-
sible article insertion point. However, this method
will likely produce a lot of noise. The standard
approach is to consider noun-phrase-initial contexts
(Han et al, 2006; Rozovskaya and Roth, 2010c).
Error type Example
Repl. 15.7% ?Can you send me the*/a letter back writing
what happened to you recently.?
Omis. 57.5% ?Nowadays ?*/the Internet makes us closer and
closer.?
Unnec. 26.8% ?One of my hobbies is the*/? photography.?
Table 4: Distribution of determiner errors in the HOO
training data.
9e.g. ?Pat apologized to me for not keeping the*/my secrets.?
276
Feature Type Description
Word n-grams wB, w2B, w3B, wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB, w2BwBwA, wBwAw2A,
wAw2Aw3A, w4Bw3Bw2BwB, w3w2BwBwA, w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4A
POS features pB, p2B, p3B , pA, p2A, p3A, pBpA, p2BpB, pAp2A, pBwB, pAwA, p2Bw2B, p2Aw2A, p2BpBpA, pBpAp2A,
pAp2Ap3A
NP1 headWord, npWords, NC, adj&headWord, adjTag&headWord, adj&NC, adjTag&NC, npTags&headWord, npTags&NC
NP2 headWord&headPOS, headNumber
wordsAfterNP headWord&wordAfterNP, npWords&wordAfterNP, headWord&2wordsAfterNP, npWords&2wordsAfterNP,
headWord&3wordsAfterNP, npWords&3wordsAfterNP
wordBeforeNP wB&fi ?i ? NP1
Verb verb, verb&fi ?i ? NP1
Preposition prep&fi ?i ? NP1
Table 3: Features used in the article error correction system. wB and wA denote the word immediately before and after
the target, respectively; and pB and pA denote the POS tag before and after the target. headWord denotes the head of the NP
complement. NC stands for noun compound and is active if second to last word in the NP is tagged as a noun. Verb features are
active if the NP is the direct object of a verb. Preposition features are active if the NP is immediately preceded by a preposition. adj
feature is active if the first word (or the second word preceded by an adverb) in the NP is an adjective. npWords and npTags denote
all words (POS tags) in the NP.
7.1 Determiner Features
The features are presented in Table 3. The model
also uses the source article as a feature.
7.2 Training the Determiner System
Model Detection Correction
AP (natural errors) 30.75 28.97
AP (inflation) 34.62 32.02
Table 5: Article development results: AP with inflation. The
performance shows the F-Score for the 244 held-out documents
of the original FCE data set. AP with inflation uses the constant
value of 0.8.
The article classifier is based on the artificial er-
rors approach (Rozovskaya and Roth, 2010c). The
original method trains a system on native English
data. The current setting is different, since the FCE
corpus contains annotated learner errors. Since the
errors are sparse, we use the error inflation method
(Section 6.2) to boost the proportion of errors in
training using the error distribution obtained from
the same training set. The effectiveness of this
method is demonstrated by the system performance:
we obtain the top or second result in every metric.
Note also that the article system does not use addi-
tional data for training.
Table 5 compares the performance of the system
trained on natural errors with the performance of the
system trained with the inflation method. We found
that any value of the inflation constant between 0.9
and 0.5 will give a boost in performance. We use
several values; the top determiner model uses the in-
flation constant of 0.8.
8 Prepositions
Table 6 shows the distribution of the three types of
preposition errors in the HOO training data. The
FCE annotation distinguishes between preposition
mistakes and errors involving the infinitive marker
to, e.g. ?He wants ?*/to go there.?, which are anno-
tated as verb errors. Since in the competition only
article and preposition annotations are kept, these
errors are not annotated, and thus we do not target
these mistakes.
Error type Example
Repl. 57.9% ?I can see at*/on the list a lot of interesting
sports.?
Omis. 24.0% ?I will be waiting ?*/for your call.?
Unnec. 18.1% ?Despite of */? being tiring , it was rewarding?
Table 6: Distribution of preposition errors in the HOO
training data.
To detect missing preposition errors, we use a set
of rules, mined from the training data, to identify
possible locations where a preposition might have
been incorrectly omitted. Below we show examples
of such contexts.
? ?I will be waiting ?*/for your call.?
? ?But now we use planes to go ?*/to far places.?
8.1 Preposition Features
All features used in the preposition module are lex-
ical: word n-grams in the 4-word window around
277
Feature Type Description
Word n-ngram features in the 4-word window
around the target
wB, w2B, w3B , wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB,
w2BwBwA, wBwAw2A, wAw2Aw3A, w4Bw3Bw2BwB, w3w2BwBwA,
w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4A
Preposition complement features compHead, wB&compHead, w2BwB&compHead
Table 7: Features used in the preposition error correction system. wB and wA denote the word immediately before and
after the target, respectively; the other features are defined similarly. compHead denotes the head of the preposition complement.
wB&compHead, w2BwB&compHead are feature conjunctions of compHead with wB and w2BwB, respectively.
the target preposition, and three features that use the
head of the preposition complement (see Table 7).
The NB-priors classifier, which is part of our model,
can only make use of the word n-gram features; it
uses n-gram features of lengths 3, 4, and 5. AP is
trained on the HOO data and uses n-grams of lengths
2, 3, and 4, the head complement features, and the
author?s preposition as a feature.
Model Detection Correction
AP (inflation) 34.64 27.51
NB-priors 38.76 26.57
Combined 41.27 29.35
Table 8: Preposition development results: performance of
individual and combined systems. The performance shows
the F-Score for the 244 held-out documents of the original FCE
data set.
8.2 Training the Preposition System
We train two systems. The first one is an AP model
trained on the FCE data with inflation (similar to
the article system). Correcting preposition errors re-
quires more data to achieve performance compara-
ble to article error correction, due to the task com-
plexity (Gamon, 2010). Moreover, given that the
development and test data are quite different,10 it
makes sense to use a model that is independent of
those, to avoid overfitting. We combine the AP
model with a model trained on native English data.
Our second system is an NB-priors classifier trained
on the the Google Web 1T 5-gram corpus (Brants
and Franz, 2006). We use training data to replace the
prior parameters of the model (see Rozovskaya and
Roth, 2011 for more detail). The NB-priors model
does not target preposition omissions.
10The data contains essays written on prompts, so that the
training data may contain several essays written on the same
prompt and thus will be very similar in content. In contrast,
we expected that the test data will likely contain essays on a
different set of prompts.
The NB-priors model outperforms the AP classi-
fier. The two models are also very different due to
the different learning algorithms and the type of the
data used in training. Our final preposition model
is thus a combination of these two, where we take
as the base the decisions of the NB-priors classifier
and add the AP model predictions for cases when
the base model does not flag a mistake. Table 8
shows the results. The combined model improves
both the detection and correction scores. Our prepo-
sition system ranked first in detection and recogni-
tion and second in correction.
Model Detection Correction
AP (natural errors) 13.50 12.73
AP (inflation) 21.31 32.02
Table 9: Preposition development results: AP with infla-
tion. The performance shows the F-Score for the 244 held-out
documents of the original FCE data set. AP with inflation uses
the constant value of 0.7.
9 Test Performance
A number of revisions were made to the test data
based on the input from the participating teams af-
ter the initial results were obtained, where each team
submitted proposed edits to correct annotation mis-
takes. We show both results.
Table 10 shows results before the revisions were
made. Row 1 shows the performance of the de-
terminer system for the three metrics. This system
achieved the best score in correction, and the second
best scores in detection and recognition. The system
is described in Section 7.2, with the exception that
the final system for the article correction is trained
on the entire FCE data set.
Table 10 (row 2) presents the results on prepo-
sition error correction. The system is described in
Section 8.2 and is a combined model of AP trained
with inflation on the FCE data set and NB-priors
model trained on the Google Web 1T corpus. The
278
Model Detection Recognition Correction
Precision Recall F-Score Precision Recall F-Score Precision Recall F-Score
Articles 40.00 37.79 38.862 38.05 35.94 36.972 35.61 33.64 34.601
Prepositions 38.21 45.34 41.471 31.05 40.25 35.061 20.36 24.15 22.092
Combined 37.22 43.71 40.201 34.23 36.64 35.391 26.39 28.26 27.292
Table 10: Performance on test before revisions. Results are shown before revisions were made to the data. The rank of the
system is shown as a superscript.
Model Detection Recognition Correction
Precision Recall F-Score Precision Recall F-Score Precision Recall F-Score
Articles 43.90 39.30 41.472 45.98 34.93 39.702 41.46 37.12 39.172
Prepositions 41.43 47.54 44.271 37.14 42.62 39.691 26.79 30.74 28.632
Combined 43.56 42.92 43.241 38.97 39.96 39.462 32.58 33.40 32.992
Table 11: Performance on test after revisions. Results are shown after revisions were made to the data. The rank of the system
is shown as a superscript.
preposition system achieved the best scores in detec-
tion and recognition, scoring second in correction.
Row 3 shows the performance of the combined
system. This system was ranked first in detection
and recognition, and second in correction.
Table 11 shows our performance after the revi-
sions were applied.
10 Discussion
The HOO 2012 shared task follows the HOO 2011
pilot shared task (Dale and Kilgarriff, 2011), where
the data was fully corrected and error-tagged and
the participants could address any types of mistakes.
The current task allows for comparison of individ-
ual systems for each error type considered. This is
important, since to date it has been difficult to com-
pare different systems due to the lack of a bench-
mark data set.
The data used for the shared task has many errors
besides the preposition and determiner errors; the
annotations for these have been removed. One un-
desirable consequence of this approach is that some
complex errors that involve either an article or a
preposition mistake but depend on other corrections
on neighboring words, e.g. a noun of a verb, may
result in ungrammatical sequences.
Clearly, the task of annotating all requisite correc-
tions is a daunting task, and it is preferable to iden-
tify subsets of these corrections that can be tackled
somewhat independently of the rest, and these more
complex cases present a problem.
To address these conflicting needs, we propose
that the scope of all ?final? corrections be marked,
without necessarily specifying all individual correc-
tions necessary to transform the original text into
correct English. Edits that plausibly require correc-
tions to their context to resolve correctly could then
be treated as out of scope, and ignored by spelling
correction systems even though in other contexts,
those same edits would be in scope.
11 Conclusion
We have demonstrated how a competitive system for
preposition and determiner error correction can be
built using techniques that address the error sparsity
of the data and the overfitting problem. We built on
our previous work and presented the error inflation
method that can be applied to the earlier proposed
artificial errors approach to boost recall. Our de-
terminer system used error inflation and trained a
model using only the annotated FCE corpus. Our
preposition system combined the FCE-trained sys-
tem with a native-data model that was adapted to
learner errors, using the NB-priors approach pro-
posed earlier. Both of the systems showed compet-
itive performance, scoring first or second in every
task ranking.
Acknowledgments
The authors thank Jeff Pasternack for his assistance and Vivek
Srikumar for helpful feedback. This research is supported by
a grant from the U.S. Department of Education and is partly
supported by the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-018.
References
M. Banko and E. Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Proc.
279
of 39th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 26?33, Toulouse,
France, July.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1.
Linguistic Data Consortium, Philadelphia, PA.
A. Carlson and I. Fette. 2007. Memory-based context-
sensitive spelling correction at web scale. In Proc. of
the IEEE International Conference on Machine Learn-
ing and Applications (ICMLA).
A. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In Proceedings of the
National Conference on Innovative Applications of Ar-
tificial Intelligence (IAAI), pages 45?50.
D. Dahlmeier and H. T. Ng. 2011. Grammatical er-
ror correction with alternating structure optimization.
In Proc. of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL), pages 915?923, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proc. of the 13th
European Workshop on Natural Language Generation
(ENLG), pages 242?249, Nancy, France.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A re-
port on the preposition and determiner error correction
shared task. In Proc. of the NAACL HLT 2012 Sev-
enth WorkshopWorkshop on Innovative Use of NLP for
Building Educational Applications, Montreal, Canada,
June. Association for Computational Linguistics.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In Proc. of the 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL), pages 163?171, Los
Angeles, California, June.
A. R. Golding and D. Roth. 1996. Applying Win-
now to context-sensitive spelling correction. In Proc.
of the International Conference on Machine Learning
(ICML), pages 182?190.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
N. Han, M. Chodorow, and C. Leacock. 2006. Detecting
errors in English article usage by non-native speakers.
Journal of Natural Language Engineering, 12(2):115?
129.
N. Han, J. Tetreault, S. Lee, and J. Ha. 2010. Us-
ing an error-annotated learner corpus to develop and
ESL/EFL error correction system. In Proc. of the Sev-
enth conference on International Language Resources
and Evaluation (LREC), Valletta, Malta, May. Euro-
pean Language Resources Association (ELRA).
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan and Claypool Publish-
ers.
J. Lee and S. Seneff. 2008. An analysis of grammatical
errors in non-native speech in English. In Proc. of the
2008 Spoken Language Technology Workshop, Goa.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 995?1001. MIT Press.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In Proceedings of
the International Conference on Language Resources
and Evaluation (LREC), Valletta, Malta, 5.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL er-
rors: Challenges and rewards. In Proc. of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 28?36,
Los Angeles, California, June. Association for Com-
putational Linguistics.
A. Rozovskaya and D. Roth. 2010b. Generating confu-
sion sets for context-sensitive error correction. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
961?970, Cambridge, MA, October. Association for
Computational Linguistics.
A. Rozovskaya and D. Roth. 2010c. Training paradigms
for correcting errors in grammar and usage. In Proc. of
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL), pages 154?
162, Los Angeles, California, June. Association for
Computational Linguistics.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks.
In Proc. of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL), pages 924?933, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task. In Proc. of the 13th European
Workshop on Natural Language Generation (ENLG).
H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011. A
new dataset and method for automatically grading esol
texts. In Proc. of the 49th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
180?189, Portland, Oregon, USA, June. Association
for Computational Linguistics.
280
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 113?117,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Illinois-Coref: The UI System in the CoNLL-2012 Shared Task
Kai-Wei Chang Rajhans Samdani Alla Rozovskaya Mark Sammons Dan Roth
University of Illinois at Urbana-Champaign
{kchang10|rsamdan2|rozovska|mssammon|danr}@illinois.edu
Abstract
The CoNLL-2012 shared task is an extension
of the last year?s coreference task. We partici-
pated in the closed track of the shared tasks in
both years. In this paper, we present the im-
provements of Illinois-Coref system from last
year. We focus on improving mention detec-
tion and pronoun coreference resolution, and
present a new learning protocol. These new
strategies boost the performance of the system
by 5% MUC F1, 0.8% BCUB F1, and 1.7%
CEAF F1 on the OntoNotes-5.0 development
set.
1 Introduction
Coreference resolution has been a popular topic of
study in recent years. In the task, a system requires
to identify denotative phrases (?mentions?) and to
cluster the mentions into equivalence classes, so that
the mentions in the same class refer to the same en-
tity in the real world.
Coreference resolution is a central task in the
Natural Language Processing research. Both the
CoNLL-2011 (Pradhan et al, 2011) and CoNLL-
2012 (Pradhan et al, 2012) shared tasks focus on
resolving coreference on the OntoNotes corpus. We
also participated in the CoNLL-2011 shared task.
Our system (Chang et al, 2011) ranked first in two
out of four scoring metrics (BCUB and BLANC),
and ranked third in the average score. This year,
we further improve the system in several respects.
In Sec. 2, we describe the Illinois-Coref system
for the CoNLL-2011 shared task, which we take as
the baseline. Then, we discuss the improvements
on mention detection (Sec. 3.1), pronoun resolu-
tion (Sec. 3.2), and learning algorithm (Sec. 3.3).
Section 4 shows experimental results and Section 5
offers a brief discussion.
2 Baseline System
We use the Illinois-Coref system from CoNLL-2011
as the basis for our current system and refer to it as
the baseline. We give a brief outline here, but fo-
cus on the innovations that we developed; a detailed
description of the last year?s system can be found in
(Chang et al, 2011).
The Illinois-Coref system uses a machine learn-
ing approach to coreference, with an inference pro-
cedure that supports straightforward inclusion of do-
main knowledge via constraints.
The system first uses heuristics based on Named
Entity recognition, syntactic parsing, and shallow
parsing to identify candidate mentions. A pair-
wise scorer w generates compatibility scores wuv
for pairs of candidate mentions u and v using ex-
tracted features ?(u, v) and linguistic constraints c.
wuv = w ? ?(u, v) + c(u, v) + t, (1)
where t is a threshold parameter (to be tuned). An
inference procedure then determines the optimal set
of links to retain, incorporating constraints that may
override the classifier prediction for a given mention
pair. A post-processing step removes mentions in
singleton clusters.
Last year, we found that a Best-Link decoding
strategy outperformed an All-Link strategy. The
Best-Link approach scans candidate mentions in a
document from left to right. At each mention, if cer-
tain conditions are satisfied, the pairwise scores of
all previous mentions are considered, together with
any constraints that apply. If one or more viable
113
links is available, the highest-scoring link is selected
and added to the set of coreference links. After the
scan is complete, the transitive closure of edges is
taken to generate the coreference clusters, each clus-
ter corresponding to a single predicted entity in the
document.
The formulation of this best-link solution is as fol-
lows. For two mentions u and v, u < v indicates
that the mention u precedes v in the document. Let
yuv be a binary variable, such that yuv = 1 only if
u and v are in the same cluster. For a document d,
Best-Link solves the following formulation:
argmaxy
?
u,v:u<v
wuvyuv
s.t
?
u<v
yuv ? 1 ?v,
yuw ? {0, 1}.
(2)
Eq. (2) generates a set of connected components
and the set of mentions in each connected compo-
nent constitute an entity. Note that we solve the
above Best-Link inference using an efficient algo-
rithm (Bengtson and Roth, 2008) which runs in time
quadratic in the number of mentions.
3 Improvements over the Baseline System
Below, we describe improvements introduced to the
baseline Illinois-Coref system.
3.1 Mention Detection
Mention detection is a crucial component of an end-
to-end coreference system, as mention detection er-
rors will propagate to the final coreference chain.
Illinois-Coref implements a high recall and low
precision rule-based system that includes all noun
phrases, pronouns and named entities as candidate
mentions. The error analysis shows that there are
two main types of errors.
Non-referential Noun Phrases. Non-referential
noun phrases are candidate noun phrases, identified
through a syntactic parser, that are unlikely to re-
fer to any entity in the real world (e.g., ?the same
time?). Note that because singleton mentions are not
annotated in the OntoNotes corpus, such phrases are
not considered as mentions. Non-referential noun
phrases are a problem, since during the coreference
stage they may be incorrectly linked to a valid men-
tion, thereby decreasing the precision of the system.
To deal with this problem, we use the training data
to count the number of times that a candidate noun
phrase happens to be a gold mention. Then, we re-
move candidate mentions that frequently appear in
the training data but never appear as gold mentions.
Relaxing this approach, we also take the predicted
head word and the words before and after the men-
tion into account. This helps remove noun phrases
headed by a preposition (e.g., the noun ?fact? in the
phrase ?in fact?). This strategy will slightly degrade
the recall of mention detection, so we tune a thresh-
old learned on the training data for the mention re-
moval.
Incorrect Mention Boundary. A lot of errors in
mention detection happen when predicting mention
boundaries. There are two main reasons for bound-
ary errors: parser mistakes and annotation incon-
sistencies. A mistake made by the parser may be
due to a wrong attachment or adding extra words
to a mention. For example, if the parser attaches
the relative clause inside of the noun phrase ?Pres-
ident Bush, who traveled to China yesterday? to a
different noun, the algorithm will predict ?President
Bush? as a mention instead of ?President Bush, who
traveled to China yesterday?; thus it will make an er-
ror, since the gold mention also includes the relative
clause. In this case, we prefer to keep the candi-
date with a larger span. On the other hand, we may
predict ?President Bush at Dayton? instead of ?Pres-
ident Bush?, if the parser incorrectly attaches the
prepositional phrase. Another example is when ex-
tra words are added, as in ?Today President Bush?.
A correct detection of mention boundaries is cru-
cial to the end-to-end coreference system. The re-
sults in (Chang et al, 2011, Section 3) show that the
baseline system can be improved from 55.96 avg F1
to 56.62 in avg F1 by using gold mention boundaries
generated from a gold annotation of the parsing tree
and the name entity tagging. However, fixing men-
tion boundaries in an end-to-end system is difficult
and requires additional knowledge. In the current
implementation, we focus on a subset of mentions
to further improve the mention detection stage of the
baseline system. Specifically, we fix mentions start-
ing with a stop word and mentions ending with a
punctuation mark. We also use training data to learn
patterns of inappropriate mention boundaries. The
mention candidates that match the patterns are re-
114
moved. This strategy is similar to the method used
to remove non-referential noun phrases.
As for annotation inconsistency, we find that in a
few documents, a punctuation mark or an apostrophe
used to mark the possessive form are inconsistently
added to the end of a mention. The problem results
in an incorrect matching between the gold and pre-
dicted mentions and downgrades the performance of
the learned model. Moreover, the incorrect mention
boundary problem also affects the training phase be-
cause our system is trained on a union set of the pre-
dicted and gold mentions. To fix this problem, in
the training phase, we perform a relaxed matching
between predicted mentions and gold mentions and
ignore the punctuation marks and mentions that start
with one of the following: adverb, verb, determiner,
and cardinal number. For example, we successfully
match the predicted mention ?now the army? to the
gold mention ?the army? and match the predicted
mention ?Sony ?s? to the gold mention ?Sony.? Note
that we cannot fix the inconsistency problem in the
test data.
3.2 Pronoun Resolution
The baseline system uses an identical model for
coreference resolution on both pronouns and non-
pronominal mentions. However, in the litera-
ture (Bengtson and Roth, 2008; Rahman and Ng,
2011; Denis and Baldridge, 2007) the features
for coreference resolution on pronouns and non-
pronouns are usually different. For example, lexi-
cal features play an important role in non-pronoun
coreference resolution, but are less important for
pronoun anaphora resolution. On the other hand,
gender features are not as important in non-pronoun
coreference resolution.
We consider training two separate classifiers with
different sets of features for pronoun and non-
pronoun coreference resolution. Then, in the decod-
ing stage, pronoun and non-pronominal mentions
use different classifiers to find the best antecedent
mention to link to. We use the same features for
non-pronoun coreference resolution, as the baseline
system. For the pronoun anaphora classifier, we use
a set of features described in (Denis and Baldridge,
2007), with some additional features. The aug-
mented feature set includes features to identify if a
pronoun or an antecedent is a speaker in the sen-
Algorithm 1 Online Latent Structured Learning for
Coreference Resolution
Loop until convergence:
For each document Dt and each v ? Dt
1. Let u? = max
u?y(v)
wT?(u, v), and
2. u? = max
u?{u<v}?{?}
wT?(u, v) + ?(u, v, y(v))
3. Let w? w + ?wT (?(u?, v)? ?(u?, v)).
tence. It also includes features to reflect the docu-
ment type. In Section 4, we will demonstrate the im-
provement of using separate classifiers for pronoun
and non-pronoun coreference resolution.
3.3 Learning Protocol for Best-Link Inference
The baseline system applies the strategy in (Bengt-
son and Roth, 2008, Section 2.2) to learn the pair-
wise scoring functionw using the Averaged Percep-
tron algorithm. The algorithm is trained on mention
pairs generated on a per-mention basis. The exam-
ples are generated for a mention v as
? Positive examples: (u, v) is used as a positive
example where u < v is the closest mention to
v in v?s cluster
? Negative examples: for all w with u < w < v,
(w, v) forms a negative example.
Although this approach is simple, it suffers from
a severe label imbalance problem. Moreover, it does
not relate well to the best-link inference, as the deci-
sion of picking the closest preceding mention seems
rather ad-hoc. For example, consider three men-
tions belonging to the same cluster: {m1: ?Presi-
dent Bush?, m2: ?he?, m3:?George Bush?}. The
baseline system always chooses the pair (m2,m3)
as a positive example because m2 is the closet men-
tion of m3. However, it is more proper to learn the
model on the positive pair (m1,m3), as it provides
more information. Since the best links are not given
but are latent in our learning problem, we use an on-
line latent structured learning algorithm (Connor et
al., 2011) to address this problem.
We consider a structured problem that takes men-
tion v and its preceding mentions {u | u < v} as
inputs. The output variables y(v) is the set of an-
tecedent mentions that co-refer with v. We define
a latent structure h(v) to be the bestlink decision
of v. It takes the value ? if v is the first mention
115
Method
Without Separating Pronouns With Separating Pronouns
MD MUC BCUB CEAF AVG MD MUC BCUB CEAF AVG
Binary Classifier (baseline) 70.53 61.63 69.26 43.03 57.97 73.24 64.57 69.78 44.95 59.76
Latent-Structured Learning 73.02 64.98 70.00 44.48 59.82 73.95 65.75 70.25 45.30 60.43
Table 1: The performance of different learning strategies for best-link decoding algorithm. We show the results
with/without using separate pronoun anaphora resolver. The systems are trained on the TRAIN set and evaluated on
the CoNLL-2012 DEV set. We report the F1 scores (%) on mention detection (MD) and coreference metrics (MUC,
BCUB, CEAF). The column AVG shows the averaged scores of the three coreference metrics.
System MD MUC BCUB CEAF AVG
Baseline 64.58 55.49 69.15 43.72 56.12
New Sys. 70.03 60.65 69.95 45.39 58.66
Table 2: The improvement of Illinois-Coref. We report
the F1 scores (%) on the DEV set from CoNLL-2011
shared task. Note that the CoNLL-2011 data set does not
include corpora of bible and of telephone conversation.
in the equivalence class, otherwise it takes values
from {u | u < v}. We define a loss function
?(h(v), v, y(v)) as
?(h(v), v, y(v)) =
{
0 h(v) ? y(v),
1 h(v) /? y(v).
We further define the feature vector ?(?, v) to be a
zero vector and ? to be the learning rate in Percep-
tron algorithm. Then, the weight vectorw in (1) can
be learned from Algorithm 1. At each step, Alg. 1
picks a mention v and finds the Best-Link decision
u? that is consistent with the gold cluster. Then, it
solves a loss-augmented inference problem to find
the best link decision u? with current model (u? = ?
if the classifier decides that v does not have coref-
erent antecedent mention). Finally, the model w is
updated by the difference between the feature vec-
tors ?(u?, v) and ?(u?, v).
Alg. 1 makes learning more coherent with infer-
ence. Furthermore, it naturally solves the data im-
balance problem. Lastly, this algorithm is fast and
converges very quickly.
4 Experiments and Results
In this section, we demonstrate the performance of
Illinois-Coref on the OntoNotes-5.0 data set. A pre-
vious experiment using an earlier version of this data
can be found in (Pradhan et al, 2007). We first show
the improvement of the mention detection system.
Then, we compare different learning protocols for
coreference resolution. Finally, we show the overall
performance improvement of Illinois-Coref system.
First, we analyze the performance of mention de-
tection before the coreference stage. Note that sin-
gleton mentions are included since it is not possible
to identify singleton mentions before running coref-
erence. They are removed in the post-processing
stage. The mention detection performance of the
end-to-end system will be discussed later in this sec-
tion. With the strategy described in Section 3.1, we
improve the F1 score for mention detection from
55.92% to 57.89%. Moreover, we improve the de-
tection performance on short named entity mentions
(name entity with less than 5 words) from 61.36 to
64.00 in F1 scores. Such mentions are more impor-
tant because they are easier to resolve in the corefer-
ence layer.
Regarding the learning algorithm, Table 1 shows
the performance of the two learning protocols
with/without separating pronoun anaphora resolver.
The results show that both strategies of using a pro-
noun classifier and training a latent structured model
with a online algorithm improve the system perfor-
mance. Combining the two strategies, the avg F1
score is improved by 2.45%.
Finally, we compare the final system with the
baseline system. We evaluate both systems on the
CoNLL-11 DEV data set, as the baseline system
is tuned on it. The results show that Illinois-Coref
achieves better scores on all the metrics. The men-
tion detection performance after coreference resolu-
tion is also significantly improved.
116
Task MD MUC BCUB CEAF AVG
English (Pred. Mentions) 74.32 66.38 69.34 44.81 60.18
English (Gold Mention Boundaries) 75.72 67.80 69.75 45.12 60.89
English (Gold Mentions) 100.00 85.74 77.46 68.46 77.22
Chinese (Pred Mentions) 47.58 37.93 63.23 35.97 45.71
Table 3: The results of our submitted system on the TEST set. The systems are trained on a collection of TRAIN and
DEV sets.
4.1 Chinese Coreference Resolution
We apply the same system to Chinese coreference
resolution. However, because the pronoun proper-
ties in Chinese are different from those in English,
we do not train separate classifiers for pronoun and
non-pronoun coreference resolution. Our Chinese
coreference resolution on Dev set achieves 37.88%
MUC, 63.37% BCUB, and 35.78% CEAF in F1
score. The performance for Chinese coreference is
not as good as the performance of the coreference
system for English. One reason for that is that we
use the same feature set for both Chinese and En-
glish systems, and the feature set is developed for
the English corpus. Studying the value of strong fea-
tures for Chinese coreference resolution system is a
potential topic for future research.
4.2 Test Results
Table 3 shows the results obtained on TEST, using
the best system configurations found on DEV. We
report results on both English and Chinese coref-
erence resolution on predicted mentions with pre-
dicted boundaries. For English coreference resolu-
tion, we also report the results when using gold men-
tions and when using gold mention boundaries1.
5 Conclusion
We described strategies for improving mention de-
tection and proposed an online latent structure al-
gorithm for coreference resolution. We also pro-
posed using separate classifiers for making Best-
Link decisions on pronoun and non-pronoun men-
tions. These strategies significantly improve the
Illinois-Coref system.
1Note that, in Ontonotes annotation, specifying gold men-
tions requires coreference resolution to exclude singleton men-
tions. Gold mention boundaries are provided by the task orga-
nizers and include singleton mentions.
Acknowledgments This research is supported by the
Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-
0181 and the Army Research Laboratory (ARL) under
agreement W911NF-09-2-0053. Any opinions, findings,
and conclusion or recommendations expressed in this ma-
terial are those of the author(s) and do not necessarily
reflect the view of the DARPA, AFRL, ARL or the US
government.
References
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference proto-
cols for coreference resolution. In CoNLL.
M. Connor, C. Fisher, and D. Roth. 2011. Online latent
structure training for language acquisition. In IJCAI.
P. Denis and J. Baldridge. 2007. A ranking approach to
pronoun resolution. In IJCAI.
S. Pradhan, L. Ramshaw, R. Weischedel, J. MacBride,
and L. Micciulla. 2007. Unrestricted Coreference:
Identifying Entities and Events in OntoNotes. In
ICSC.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. CoNLL-2011
shared task: Modeling unrestricted coreference in
OntoNotes. In CoNLL.
S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and
Y. Zhang. 2012. CoNLL-2012 shared task: Modeling
multilingual unrestricted coreference in OntoNotes. In
CoNLL.
A. Rahman and V. Ng. 2011. Narrowing the modeling
gap: a cluster-ranking approach to coreference resolu-
tion. Journal of AI Research, 40(1):469?521.
117
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 13?19,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
The University of Illinois System in the CoNLL-2013 Shared Task
Alla Rozovskaya Kai-Wei Chang Mark Sammons Dan Roth
Cognitive Computation Group
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,kchang10,mssammon,danr}@illinois.edu
Abstract
The CoNLL-2013 shared task focuses on
correcting grammatical errors in essays
written by non-native learners of English.
In this paper, we describe the University
of Illinois system that participated in the
shared task. The system consists of five
components and targets five types of com-
mon grammatical mistakes made by En-
glish as Second Language writers. We de-
scribe our underlying approach, which re-
lates to our previous work, and describe
the novel aspects of the system in more de-
tail. Out of 17 participating teams, our sys-
tem is ranked first based on both the orig-
inal annotation and on the revised annota-
tion.
1 Introduction
The task of correcting grammar and usage mis-
takes made by English as a Second Language
(ESL) writers is difficult for several reasons. First,
many of these errors are context-sensitive mistakes
that confuse valid English words and thus can-
not be detected without considering the context
around the word. Second, the relative frequency
of mistakes is quite low: for a given type of mis-
take, an ESL writer will typically make mistakes
in only a small proportion of relevant structures.
For example, determiner mistakes usually occur
in 5% to 10% of noun phrases in various anno-
tated ESL corpora (Rozovskaya and Roth, 2010a).
Third, an ESL writer may make multiple mistakes
in a single sentence, which may give misleading
local cues for individual classifiers. In the exam-
ple shown in Figure 1, the agreement error on the
verb ?tend? interacts with the noun number error
on the word ?equipments?.
Therefore , the *equipments/equipment of bio-
metric identification *tend/tends to be in-
expensive .
Figure 1: Representative ESL errors in a sample
sentence from the training data.
The CoNLL-2013 shared task (Ng et al, 2013)
focuses on the following five common mistakes
made by ESL writers:
? article/determiner
? preposition
? noun number
? subject-verb agreement
? verb form
Errors outside this target group are present in the
task corpora, but are not evaluated.
In this paper, we present a system that combines
a set of statistical models, where each model spe-
cializes in correcting one of the errors described
above. Because the individual error types have
different characteristics, we use several different
approaches. The article system builds on the el-
ements of the system described in (Rozovskaya
and Roth, 2010c). The preposition classifier uses
a combined system, building on work described
in (Rozovskaya and Roth, 2011) and (Rozovskaya
and Roth, 2010b). The remaining three models are
all Na??ve Bayes classifiers trained on the Google
Web 1T 5-gram corpus (henceforth, Google cor-
pus, (Brants and Franz, 2006)).
We first briefly discuss the task (Section 2) and
give the overview of our system (Section 3). We
then describe the error-specific components (Sec-
tions 3.1, 3.2 and 3.3). The sections describ-
ing individual components quantify their perfor-
mance on splits of the training data. In Section 4,
13
we evaluate the complete system on the training
data using 5-fold cross-validation (hereafter, ?5-
fold CV?) and in Section 5 we show the results we
obtained on test.
We close with a discussion focused on error
analysis (Section 6) and our conclusions (Sec-
tion 7).
2 Task Description
The CoNLL-2013 shared task focuses on correct-
ing five types of mistakes that are commonly made
by non-native speakers of English. The train-
ing data released by the task organizers comes
from the NUCLE corpus (Dahlmeier et al, 2013),
which contains essays written by learners of En-
glish as a foreign language and is corrected by
English teachers. The test data for the task con-
sists of an additional set of 50 student essays. Ta-
ble 1 illustrates the mistakes considered in the task
and Table 2 illustrates the distribution of these er-
rors in the released training data and the test data.
We note that the test data contains a much larger
proportion of annotated mistakes. For example,
while only 2.4% of noun phrases in the training
data have determiner errors, in the test data 10%
of noun phrases have mistakes.
Error type Percentage of errors
Training Test
Articles 2.4% 10.0%
Prepositions 2.0% 10.7%
Noun number 1.6% 6.0%
Subject-verb agreement 2.0% 5.2%
Verb form 0.8% 2.5%
Table 2: Statistics on error distribution in train-
ing and test data. Percentage denotes the erro-
neous instances with respect to the total number of
relevant instances in the data. For example, 10%
of noun phrases in the test data have determiner
errors.
Since the task focuses on five error types, only
annotations marking these mistakes were kept.
Note that while the other error annotations were
removed, the errors still remain in the data.
3 System Components
Our system consists of five components that ad-
dress individually article1, preposition, noun verb
1We will use the terms ?article-? and ?determiner errors?
interchangeably: article errors constitute the majority of de-
form and subject-verb agreement errors.
Our article and preposition modules build on the
elements of the systems described in Rozovskaya
and Roth (2010b), Rozovskaya and Roth (2010c)
and Rozovskaya and Roth (2011). The article sys-
tem is trained using the Averaged Perceptron (AP)
algorithm (Freund and Schapire, 1999), imple-
mented within Learning Based Java (Rizzolo and
Roth, 2010). The AP system is trained using the
inflation method (Rozovskaya et al, 2012). Our
preposition system is a Na??ve Bayes (NB) classi-
fier trained on the Google corpus and with prior
parameters adapted to the learner data.
The other modules ? those that correct noun and
verb errors ? are all NB models trained on the
Google corpus.
All components take as input the corpus doc-
uments preprocessed with a part-of-speech tag-
ger2 and shallow parser3 (Punyakanok and Roth,
2001). Note that the shared task data already
contains comparable pre-processing information,
in addition to other information, including depen-
dency parse and constituency parse, but we chose
to run our own pre-processing tools. The article
module uses the POS and chunker output to gen-
erate some of its features and to generate candi-
dates (likely contexts for missing articles). The
other system components use the pre-processing
tools only as part of candidate generation (e.g., to
identify all nouns in the data for the noun classi-
fier) because these components are trained on the
Google corpus and thus only employ word n-gram
features.
During development, we split the released train-
ing data into five parts. The results in Sections 3.1,
3.2, and 3.3 give performance of 5-fold CV on the
training data. In Section 4 we report the develop-
ment 5-fold CV results of the complete model and
the performance on the test data. Note that the per-
formance reported for the overall task on the test
data in Section 4 reflects the system that makes use
of the entire training corpus. It is also important to
remark that only the determiner system is trained
on the ESL data. The other models are trained on
native data, and the ESL training data is only used
to optimize the decision thresholds of the models.
terminer errors, and we address only article mistakes.
2http://cogcomp.cs.illinois.edu/page/
software view/POS
3http://cogcomp.cs.illinois.edu/page/
software view/Chunker
14
Error type Examples
Article ?It is also important to create *a/? better material that can support
*the/? buildings despite any natural disaster like earthquakes.?
Preposition ?As the number of people grows, the need *of /for habitable environ-
ment is unquestionably essential.
Noun number Some countries are having difficulties in managing a place to live for
their *citizen/citizens as they tend to get overpopulated.?
Subject-verb agreement ?Therefore , the equipments of biometric identification *tend/tends
to be inexpensive.
Verb form
?...countries with a lot of deserts can terraform their desert to increase
their habitable land and *using/use irrigation..?
?it was not *surprised/surprising to observe an increasing need for a
convenient and cost effective platform.?
Table 1: Example errors. Note that only the errors exemplifying the relevant phenomena are marked
in the table; the sentences may contain other mistakes. Errors marked as verb form include multiple
grammatical phenomena that may characterize verbs.
3.1 Determiners
There are three types of determiner error: omitting
a determiner; choosing an incorrect determiner;
and adding a spurious determiner. Even though
the majority of determiner errors involve article
mistakes, some of these errors involve personal
and possessive pronouns.4 Most of the determiner
errors, however, involve omitting an article (these
make up over 60% in the training data). Similar er-
ror patterns have been observed in other ESL cor-
pora (Rozovskaya and Roth, 2010a).
Our system focuses on article errors. The sys-
tem first extracts from the data all articles, and all
spaces at the beginning of a noun phrase where an
article is likely to be omitted (Han et al, 2006; Ro-
zovskaya and Roth, 2010c). Then we train a multi-
class classifier with features described in Table 3.
These features were used successfully in previous
tasks in error correction (Rozovskaya et al, 2012;
Rozovskaya et al, 2011).
The original word choice (the source article)
used by the writer is also used as a feature. Since
the errors are sparse, this feature causes the model
to abstain from flagging a mistake, which results
in low recall. To avoid this problem, we adopt the
approach proposed in (Rozovskaya et al, 2012),
the error inflation method, and add artificial arti-
cle errors in the training data based on the error
distribution on the training set. This method pre-
vents the source feature from dominating the con-
text features, and improves the recall of the sys-
4e.g. ?Pat apologized to me for not keeping the*/my se-
crets.?
tem.
We experimented with two types of classifiers:
Averaged Perceptron (AP) and an L1-generalized
logistic regression classifier (LR). Since the arti-
cle system is trained on the ESL data, of which
we have a limited amount, we also experimented
with adding a language model (LM) feature to the
LR learner. This feature indicates if the correc-
tion is accepted by a language model trained on
the Google corpus. The performance of each clas-
sifier on 5-fold CV on the training data is shown in
Table 4. The results show that AP performs better
than LR. We observed that adding the LM feature
improves precision but results in lower F1, so we
chose the AP classifier without the LM feature for
our final system.
Model Precision Recall F1
AP (inflation) 0.17 0.31 0.22
AP (inflation+LM) 0.26 0.15 0.19
LR (inflation) 0.17 0.29 0.22
LR (inflation+LM) 0.24 0.21 0.22
Table 4: Article development results Results on 5-fold
CV. AP With Inflation achieves the best development using an
inflation constant of 0.85. AP achieves higher performance
without using the language model feature.
3.2 Prepositions
The most common preposition errors are replace-
ments, i.e., where the author correctly recognized
the need for a preposition, but chose the wrong one
to use.
15
Feature Type Description
Word n-grams wB, w2B, w3B, wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB, w2BwBwA, wBwAw2A, wAw2Aw3A,
w4Bw3Bw2BwB, w3w2BwBwA, w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4A
POS features pB, p2B, p3B , pA, p2A, p3A, pBpA, p2BpB, pAp2A, pBwB, pAwA, p2Bw2B, p2Aw2A, p2BpBpA, pBpAp2A,
pAp2Ap3A
NP1 headWord, npWords, NC, adj&headWord, adjTag&headWord, adj&NC, adjTag&NC, npTags&headWord, npTags&NC
NP2 headWord&headPOS, headNumber
wordsAfterNP headWord&wordAfterNP, npWords&wordAfterNP, headWord&2wordsAfterNP, npWords&2wordsAfterNP, headWord&3wordsAfterNP,
npWords&3wordsAfterNP
wordBeforeNP wB&fi ?i ? NP1
Verb verb, verb&fi ?i ? NP1
Preposition prep&fi ?i ? NP1
Source the word used by the original writer
LM a binary feature assigned by a language model
Table 3: Features used in the article error correction system. wB and wA denote the word immediately before and after
the target, respectively; and pB and pA denote the POS tag before and after the target. headWord denotes the head of the NP
complement. NC stands for noun compound and is active if second to last word in the NP is tagged as a noun. Verb features are
active if the NP is the direct object of a verb. Preposition features are active if the NP is immediately preceded by a preposition.
adj feature is active if the first word (or the second word preceded by an adverb) in the NP is an adjective. npWords and npTags
denote all words (POS tags) in the NP.
3.2.1 Preposition Features
All features used in the preposition module are
lexical: word n-grams in the 4-word window
around the target preposition. The NB-priors clas-
sifier, which is part of our model, can only make
use of the word n-gram features; it uses n-gram
features of lengths 3, 4, and 5. Note that since the
NB model is trained on the Google corpus, the an-
notated ESL training data is used only to replace
the prior parameters of the model (see Rozovskaya
and Roth, 2011 for more details).
3.2.2 Training the Preposition System
Correcting preposition errors requires more data
to achieve performance comparable to article er-
ror correction due to the task complexity (Gamon,
2010). We found that training an AP model on
the ESL training data with more sophisticated fea-
tures is not as effective as training on a native En-
glish dataset of larger size. The ESL training data
contains slightly over 100K preposition examples,
which is several orders of magnitude smaller than
the Google n-gram corpus. We use the shared
task training data to replace the prior parameters
of the model (see Rozovskaya and Roth, 2011 for
more details). The NB-priors model does not tar-
get preposition omissions and insertions: it cor-
rects only preposition replacements that involve
the 12 most common English prepositions. The
task includes mistakes that cover 36 prepositions
but we found that the model performance drops
once the confusion set becomes too large. Table
5 shows the performance of the system on the 5-
fold CV on the training data, where each time the
classifier was trained on 80% of the documents.
Model Precision Recall F1
NB-priors 0.14 0.14 0.14
Table 5: Preposition results: NB with priors. Results on
5-fold CV. The model is trained on the Google corpus.
3.3 Correcting Nouns and Verbs
The three remaining types of errors ? noun num-
ber errors, subject-verb agreement, and the various
verb form mistakes ? are corrected using separate
NB models also trained on the Google corpus. We
focus here on the selection of candidates for cor-
rection, as this strongly affects performance.
3.3.1 Candidate Selection
This stage selects the set of words that are pre-
sented as input to the classifier. This is a crucial
step because it limits the performance of any sys-
tem: those errors that are missed at this stage have
no chance of being detected by the later stages.
This is also a challenging step as the class of
verbs and nouns is open, with many English verbs
and nouns being compatible with multiple parts of
speech. This problem does not arise in preposi-
tion and article error correction, where candidates
are determined by surface form (i.e. can be deter-
mined using a closed list of prepositions or arti-
cles).
We use the POS tag and the shallow parser out-
put to identify the set of candidates that are input
to the classifiers. In particular, for nouns, we col-
lect all words tagged as NN or NNS. Since pre-
processing tools are known to make more mis-
takes on ESL data than on native data, this pro-
cedure does not have a perfect result on the iden-
tification of all noun mistakes. For example, we
16
miss about 10% of noun errors due to POS/shallow
parser errors. For verbs, we compared several
candidate selection methods. Method (1) ex-
tracts all verbs heading a verb phrase, as iden-
tified by the shallow parser. Method (2) ex-
pands this set to words tagged with one of the
verb POS tags {VB,VBN,VBG,VBD,VBP,VBZ}.
However, generating candidates by selecting only
those tagged as verbs is not good enough, since the
POS tagger performance on ESL data is known to
be suboptimal (Nagata et al, 2011), especially for
verbs containing errors. For example, verbs lack-
ing agreement markers are likely to be mistagged
as nouns (Lee and Seneff, 2008). Erroneous verbs
are exactly the cases that we wish to include.
Method (3) adds words that are in the lemma list of
common English verbs compiled using the Giga-
word corpus. The last method has the highest re-
call on the candidate identification; it misses only
5% of verb errors, and also has better performance
in the complete model. We thus use this method.
3.3.2 Noun-Verb Correction Performance
Table 6 shows the performance of the systems
based on 5-fold CV on the training data. Each
model is trained individually on the Google cor-
pus, and is individually processed to optimize the
respective thresholds.
Model Precision Recall F1
Noun number 0.17 0.38 0.23
Subject-verb agr. 0.19 0.24 0.21
Verb form 0.07 0.20 0.10
Table 6: Noun, subject-verb agreement and
verb form results. Results on 5-fold CV. The
models are trained on the Google corpus.
4 Combined Model
In the previous sections, we described the individ-
ual components of the system developed to target
specific error types. The combined model includes
all of these modules, which are each applied to
examples individually: there is no pipeline, and
the individual predictions of the modules are then
pooled.
The combined system also includes a post-
processing step where we remove certain correc-
tions of noun and verb forms that we found oc-
cur quite often but are never correct. This hap-
pens when both choices ? the writer?s selection
and the correction ? are valid but the latter is ob-
served more frequently in the native training data.
For example, the phrase ?developing country? is
changed to ?developed country? even though both
are legitimate English expressions. If a correction
is frequently proposed but always results in a false
alarm, we add it to a list of changes that is ignored
when we generate the system output. When we
generate the output on Test set, 8 unique pairs of
such changes are ignored (36 pairs of changes in
total).
We now show the combined results on the train-
ing data by conducting 5-fold CV, where we add
one component at a time. Table 8 shows that the
recall and the F1 scores improve when each com-
ponent is added to the system. The final system
achieves an F1 score of 0.21 on the training data
in 5-fold CV.
Model Precision Recall F1
Articles 0.16 0.12 0.14
+Prepositions 0.16 0.14 0.15
+Noun number 0.17 0.23 0.20
+Subject-verb agr. 0.18 0.25 0.21
+Verb form (All) 0.18 0.27 0.21
Table 7: Results on 5-fold CV on the training
data. The article model is trained on the ESL
data using AP. The other models are trained on the
Google corpus. The last line shows the results,
when all of the five modules are included.
5 Test Results
The previous section showed the performance of
the system on the training data. In this section,
we show the results on the test set. As previously,
the performance improves when each component
is added into the final system. However, we also
note that the precision is much higher while the
recall is only slightly lower. We attribute this in-
creased precision to the observed differences in
the percentage of annotated errors in training vs.
test (see Section 3) and hypothesize that the train-
ing data may contain additional relevant errors that
were not included in the annotation.
Besides the original official annotations an-
nounced by the organizers, another set of anno-
tations is offered based on the combination of re-
vised official annotations and accepted alternative
annotations proposed by participants. We show in
Table 8 when our system is scored based on the
17
revised annotations, both the precision and the re-
call are higher. Our system achieves the highest
scores out of 17 participating teams based on both
the original and revised annotations.
Model Precision Recall F1
Scores based on the original annotations
Articles 0.48 0.11 0.18
+Prepositions 0.45 0.12 0.19
+Noun number 0.48 0.21 0.29
+Subject-verb agr. 0.48 0.22 0.30
+Verb form (All) 0.46 0.23 0.31
Scores based on the revised annotations
All 0.62 0.32 0.42
Table 8: Results on Test. The article model is
trained on the ESL data using AP. The other mod-
els are trained on the Google corpus. All denotes
the results of the complete model that includes all
of the five modules.
6 Discussion and Error Analysis
Here, we present some interesting errors that our
system makes.
6.1 Error Analysis
Incorrect verb form correction: Safety is one of
the crucial problems that many countries and com-
panies *concerned/concerns.
Here, the phrasing requires multiple changes;
to maintain the same word order, this correction
would be needed in tandem with the insertion of
the auxiliary ?have? to create a passive construc-
tion.
Incorrect determiner insertion: In this era,
Engineering designs can help to provide more
habitable accommodation by designing a stronger
material so it?s possible to create a taller and safer
building, a better and efficient sanitation system
to prevent *?/ the disease, and also by designing
a way to change the condition of the inhabitable
environment.
This example requires a model of discourse at
the level of recognizing when a specific disease
is a focus of the text, rather than disease in gen-
eral. The use of a singular construction ?a taller
and safer building? in this context is somewhat un-
conventional and potentially makes this distinction
even harder to detect.
Incorrect verb number correction:
One current human *need/needs that should
be given priority is the search for renewable re-
sources.
This appears to be the result of the system
heuristics intended to mitigate POS tagging errors
on ESL text, where the word ?need? is considered
as a candidate verb rather thana noun; this results
in an incorrect change to make the ?verb? agree in
number with the phrase ?one human?.
Incorrect determiner deletion: This had
shown that the engineering design process is es-
sential in solving problems and it ensures that the
problem is thoroughly looked into and ensure that
the engineers are generating ideas that target the
main problem, *the/? depletion and harmful fuel.
In this example, local context may suggest a list
structure, but the wider context indicates that the
comma represents an appositive structure.
6.2 Discussion
Note that the presence of multiple errors can have
very negative effects on preprocessing. For exam-
ple, when an incorrect verb form is used that re-
sults in a word form commonly used as a noun,
the outputs of the parsers tend to be incorrect. This
limits the potential of rule-based approaches.
Machine learning approaches, on the other
hand, require sufficient examples of each error
type to allow robust statistical modeling of contex-
tual features. Given the general sparsity of ESL
errors, together with the additional noise intro-
duced into more sophisticated preprocessing com-
ponents by errors with overlapping contexts, it ap-
pears hard to leverage these more sophisticated
tools to generate features for machine learning ap-
proaches. This motivates our use of just POS and
shallow parse analysis, together with language-
modeling approaches that can use counts derived
from very large native corpora, to provide robust
inputs for machine learning algorithms.
The interaction between errors suggests that
constraints could be used to improve results by en-
suring, for example, that verb number, noun num-
ber, and noun phrase determiner are consistent.
This is more difficult than it may first appear for
two reasons. First, the noun that is the subject
of the verb under consideration may be relatively
distant in the sentence (due to the presence of in-
tervening relative clauses, for example). Second,
the constraint only limits the possible correction
options: the correct number for the noun in fo-
18
cus may depend on the form used in the preceding
sentences ? for example, to distinguish between a
general statement about some type of entity, and a
statement about a specific entity.
These observations suggest that achieving very
high performance in the task of grammar correc-
tion requires sophisticated modeling of deep struc-
ture in natural language documents.
7 Conclusion
We have described our system that participated in
the shared task on grammatical error correction
and ranked first out of 17 participating teams. We
built specialized models for the five types of mis-
takes that are the focus of the competition. We
have also presented error analysis of the system
output and discussed possible directions for future
work.
Acknowledgments
This material is based on research sponsored by DARPA under agreement num-
ber FA8750-13-2-0008. The U.S. Government is authorized to reproduce and
distribute reprints for Governmental purposes notwithstanding any copyright
notation thereon. The views and conclusions contained herein are those of the
authors and should not be interpreted as necessarily representing the official
policies or endorsements, either expressed or implied, of DARPA or the U.S.
Government. This research is also supported by a grant from the U.S. Depart-
ment of Education and by the DARPA Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-018.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium, Philadelphia, PA.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL
HLT 2013 Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?
171, Los Angeles, California, June.
N. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Journal of Natural Language Engineer-
ing, 12(2):115?129.
J. Lee and S. Seneff. 2008. Correcting misuse of verb
forms. In ACL, pages 174?182, Columbus, Ohio,
June. Association for Computational Linguistics.
R. Nagata, E. Whittaker, and V. Sheinman. 2011. Cre-
ating a manually error-tagged and shallow-parsed
learner corpus. In ACL, pages 1210?1219, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task
on grammatical error correction. In Proc. of the
Seventeenth Conference on Computational Natural
Language Learning. Association for Computational
Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS.
N. Rizzolo and D. Roth. 2010. Learning Based Java
for Rapid Development of NLP Systems. In LREC.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of
the NAACL Workshop on Innovative Use of NLP for
Building Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Generating con-
fusion sets for context-sensitive error correction. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
A. Rozovskaya and D. Roth. 2010c. Training
paradigms for correcting errors in grammar and us-
age. In NAACL.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task.
A. Rozovskaya, M. Sammons, and D. Roth. 2012. The
UI system in the hoo 2012 shared task on error cor-
rection.
19
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 34?42,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
The Illinois-Columbia System in the CoNLL-2014 Shared Task
Alla Rozovskaya
1
Kai-Wei Chang
2
Mark Sammons
2
Dan Roth
2
Nizar Habash
1
1
Center for Computational Learning Systems, Columbia University
{alla,habash}@ccls.columbia.edu
2
Cognitive Computation Group, University of Illinois at Urbana-Champaign
{kchang10,mssammon,danr}@illinois.edu
Abstract
The CoNLL-2014 shared task is an ex-
tension of last year?s shared task and fo-
cuses on correcting grammatical errors in
essays written by non-native learners of
English. In this paper, we describe the
Illinois-Columbia system that participated
in the shared task. Our system ranked sec-
ond on the original annotations and first on
the revised annotations.
The core of the system is based on the
University of Illinois model that placed
first in the CoNLL-2013 shared task. This
baseline model has been improved and ex-
panded for this year?s competition in sev-
eral respects. We describe our underly-
ing approach, which relates to our previ-
ous work, and describe the novel aspects
of the system in more detail.
1 Introduction
The topic of text correction has seen a lot of inter-
est in the past several years, with a focus on cor-
recting grammatical errors made by English as a
Second Language (ESL) learners. ESL error cor-
rection is an important problem since most writers
of English are not native English speakers. The in-
creased interest in this topic can be seen not only
from the number of papers published on the topic
but also from the three competitions devoted to
grammatical error correction for non-native writ-
ers that have recently taken place: HOO-2011
(Dale and Kilgarriff, 2011), HOO-2012 (Dale et
al., 2012), and the CoNLL-2013 shared task (Ng
et al., 2013).
In all three shared tasks, the participating sys-
tems performed at a level that is considered ex-
tremely low compared to performance obtained in
other areas of NLP: even the best systems attained
F1 scores in the range of 20-30 points.
The key reason that text correction is a diffi-
cult task is that even for non-native English speak-
ers, writing accuracy is very high, as errors are
very sparse. Even for some of the most com-
mon types of errors, such as article and preposi-
tion usage, the majority of the words in these cate-
gories (over 90%) are used correctly. For instance,
in the CoNLL training data, only 2% of preposi-
tions are incorrectly used. Because errors are so
sparse, it is more difficult for a system to identify a
mistake accurately and without introducing many
false alarms.
The CoNLL-2014 shared task (Ng et al., 2014)
is an extension of the CoNLL-2013 shared task
(Ng et al., 2013). Both competitions make use
of essays written by ESL learners at the National
University of Singapore. However, while the first
one focused on five kinds of mistakes that are com-
monly made by ESL writers ? article, preposition,
noun number, verb agreement, and verb form ?
this year?s competition covers all errors occurring
in the data. Errors outside the target group were
present in the task corpora last year as well, but
were not evaluated.
Our system extends the one developed by the
University of Illinois (Rozovskaya et al., 2013)
that placed first in the CoNLL-2013 competition.
For this year?s shared task, the system has been
extended and improved in several respects: we ex-
tended the set of errors addressed by the system,
developed a general approach for improving the
error-specific models, and added a joint inference
component to address interaction among errors.
See Rozovskaya and Roth (2013) for more detail.
We briefly discuss the task (Section 2) and give
an overview of the baseline Illinois system (Sec-
tion 3). Section 4 presents the novel aspects of the
system. In Section 5, we evaluate the complete
system on the development data and show the re-
sults obtained on test. We offer error analysis and a
brief discussion in Section 6. Section 7 concludes.
34
Error type Rel. freq. Examples
Article (ArtOrDet) 14.98% *?/The government should help encourage *the/?
breakthroughs as well as *a/? complete medication
system .
Wrong collocation (Wci) 11.94% Some people started to *think/wonder if electronic
products can replace human beings for better perfor-
mances .
Local redundancy (Rloc-) 10.52% Some solutions *{as examples}/? would be to design
plants/fertilizers that give higher yield ...
Noun number (Nn) 8.49% There are many reports around the internet and on
newspaper stating that some users ? *iPhone/iPhones
exploded .
Verb tense (Vt) 7.21% Through the thousands of years , most Chinese scholars
*are/{have been} greatly affected by Confucianism .
Orthography/punctuation (Mec) 6.88% Even British Prime Minister , Gordon Brown *?/, has
urged that all cars in *britain/Britain to be green by
2020 .
Preposition (Prep) 5.43% I do not agree *on/with this argument that surveillance
technology should not be used to track people .
Word form (Wform) 4.87% On the other hand , the application of surveillance tech-
nology serves as a warning to the *murders/murderers
and they might not commit more murder .
Subject-verb agreement (SVA) 3.44% However , tracking people *are/is difficult and different
from tracking goods .
Verb form (Vform) 3.25% Travelers survive in desert thanks to GPS
*guide/guiding them .
Tone (Wtone) 1.29% Hence , as technology especially in the medical field
continues to get developed and updated , people {do
n?t}/{do not} risk their lives anymore .
Table 1: Example errors. In the parentheses, the error codes used in the shared task are shown. Note
that only the errors exemplifying the relevant phenomena are marked in the table; the sentences may
contain other mistakes. Errors marked as verb form include multiple grammatical phenomena that may
characterize verbs. Our system addresses all of the error types except ?Wrong Collocation? and ?Local
Redundancy?.
2 Task Description
Both the training and the test data of the CoNLL-
2014 shared task consist of essays written by stu-
dents at the National University of Singapore. The
training data contains 1.2 million words from the
NUCLE corpus (Dahlmeier et al., 2013) corrected
by English teachers, and an additional set of about
30,000 words that was released last year as a test
set for the CoNLL-2013 shared task. We use last
year?s test data as a development set; the results in
the subsequent sections are reported on this subset.
The CoNLL corpus error tagset includes 28 er-
ror categories. Table 1 illustrates the most com-
mon error categories in the training data; errors are
marked with an asterisk, and ? denotes a missing
word. Our system targets all of these, with the ex-
ception of collocation and local redundancy errors.
Among the less commonly occurring error types,
our system addresses tone (style) errors; these are
illustrated in the table.
It should be noted that the proportion of erro-
neous instances is several times higher in the de-
velopment data than in the training data for all of
the error categories. For example, while only 2.4%
of noun phrases in the training data have deter-
miner errors, in the development data 10% of noun
phrases have determiner errors.
35
?Hence, the environmental *factor/factors also
*contributes/contribute to various difficulties,
*included/including problems in nuclear tech-
nology.?
Error type Confusion set
Noun number {factor, factors}
Verb Agreement {contribute, contributes}
Verb Form
{included, including,
includes, include}
Table 2: Sample confusion sets for noun num-
ber, verb agreement, and verb form.
3 The Baseline System
In this section, we briefly describe the Univer-
sity of Illinois system (henceforth Illinois; in the
overview paper of the shared task the system is re-
ferred to as UI) that achieved the best result in the
CoNLL-2013 shared task and which we use as our
baseline model. For a complete description, we
refer the reader to Rozovskaya et al. (2013).
The Illinois system implements five
independently-trained machine-learning clas-
sifiers that follow the popular approach to ESL
error correction borrowed from the context-
sensitive spelling correction task (Golding and
Roth, 1999; Carlson et al., 2001). A confusion
set is defined as a list of confusable words.
Each occurrence of a confusable word in text is
represented as a vector of features derived from a
context window around the target. The problem
is cast as a multi-class classification task and a
classifier is trained on native or learner data. At
prediction time, the model selects the most likely
candidate from the confusion set.
The confusion set for prepositions includes the
top 12 most frequent English prepositions (this
year, we extend the confusion set and also target
extraneous preposition usage). The article confu-
sion set is as follows: {a, the, ?}.
1
The confu-
sion sets for noun, agreement, and form modules
depend on the target word and include its morpho-
logical variants. Table 2 shows sample confusion
sets for noun, agreement, and form errors.
Each classifier takes as input the corpus doc-
uments preprocessed with a part-of-speech tag-
1
? denotes noun-phrase-initial contexts where an article
is likely to have been omitted. The variants ?a? and ?an? are
conflated and are restored later.
ger
2
and shallow parser
3
(Punyakanok and Roth,
2001). The other system components use the pre-
processing tools only as part of candidate genera-
tion (e.g., to identify all nouns in the data for the
noun classifier).
The choice of learning algorithm for each clas-
sifier is motivated by earlier findings showing
that discriminative classifiers outperform other
machine-learning methods on error correction
tasks (Rozovskaya and Roth, 2011). Thus, the
classifiers trained on the learner data make use of
a discriminative model. Because the Google cor-
pus does not contain complete sentences but only
n-gram counts of length up to five, training a dis-
criminative model is not desirable, and we thus use
NB (details in Rozovskaya and Roth (2011)).
The article classifier is a discriminative model
that draws on the state-of-the-art approach de-
scribed in Rozovskaya et al. (2012). The model
makes use of the Averaged Perceptron (AP) algo-
rithm (Freund and Schapire, 1996) and is trained
on the training data of the shared task with rich
features. The article module uses the POS and
chunker output to generate some of its features and
candidates (likely contexts for missing articles).
The original word choice (the source article)
used by the writer is also used as a feature. Since
the errors are sparse, this feature causes the model
to abstain from flagging mistakes, resulting in low
recall. To avoid this problem, we adopt the ap-
proach proposed in Rozovskaya et al. (2012), the
error inflation method, and add artificial article er-
rors to the training data based on the error distribu-
tion on the training set. This method prevents the
source feature from dominating the context fea-
tures, and improves the recall of the system.
The other classifiers in the baseline system ?
noun number, verb agreement, verb form, and
preposition ? are trained on native English data,
the Google Web 1T 5-gram corpus (henceforth,
Google, (Brants and Franz, 2006)) with the Na??ve
Bayes (NB) algorithm. All models use word n-
gram features derived from the 4-word window
around the target word. In the preposition model,
priors for preposition preferences are learned from
the shared task training data (Rozovskaya and
Roth, 2011).
The modules targeting verb agreement and
2
http://cogcomp.cs.illinois.edu/page/
software view/POS
3
http://cogcomp.cs.illinois.edu/page/
software view/Chunker
36
verb form mistakes draw on the linguistically-
motivated approach to correcting verb errors pro-
posed in Rozovskaya et. al (2014).
4 The CoNLL-2014 System
The system in the CoNLL-2014 shared task is im-
proved in three ways: 1) Additional error-specific
classifiers: word form, orthography/punctuation,
and style; 2) Model combination; and 3) Joint in-
ference to address interacting errors. Table 3 sum-
marizes the Illinois and the Illinois-Columbia sys-
tems.
4.1 Targeting Additional Errors
The Illinois-Columbia system implements several
new classifiers to address word form, orthography
and punctuation, and style errors (Table 1).
4.1.1 Word Form Errors
Word form (Wform) errors are grammatical er-
rors that involve confusing words that share a
base form but differ in derivational morphology,
e.g. ?use? and ?usage? (see also Table 1). Con-
fusion sets for word form errors thus should in-
clude words that differ derivationally but share the
same base form. In contrast to verb form errors
where confusion sets specify all possible inflec-
tional forms for a given verb, here, the associated
parts-of-speech may vary more widely. An ex-
ample of a confusion set is {technique, technical,
technology, technological}.
Because word form errors encompass a wide
range of misuse, one approach is to consider ev-
ery word as an error candidate. We follow a more
conservative method and only attempt to correct
those words that occurred in the training data and
were tagged as word form errors (we cleaned up
that list by removing noisy annotations).
A further challenge in addressing word form er-
rors is generating confusion sets. We found that
about 45% of corrections for word form errors in
the development data are covered by the confusion
sets from the training data for the same word. We
thus derive the confusion sets using the training
data. Specifically, for every source word that is
tagged as a word form error in the training data,
the confusion set includes all labels to which that
word is mapped in the training data. In addition,
plural and singular forms are added for all words
tagged as nouns, and inflectional forms are added
for words tagged as verbs. For more detail on
correcting verb errors, we refer the reader to Ro-
zovskaya et al. (2014).
4.1.2 Orthography and Punctuation Errors
The Mec error category includes errors in
spelling, context-sensitive spelling, capitalization,
and punctuation. Our system addresses punctua-
tion errors and capitalization errors.
To correct capitalization errors, we collected
words that are always capitalized in the train-
ing and development data when not occurring
sentence-initially.
The punctuation classifier includes two mod-
ules: a learned component targets missing and
extraneous comma usage and is an AP classifier
trained on the learner data with error inflation.
A second, pattern-based component, complements
the AP model: it inserts missing commas by using
a set of patterns that overwhelmingly prefer the us-
age of a comma, e.g. when a sentence starts with
the word ?hence?. The patterns are learned auto-
matically over the training data: specifically, us-
ing a sliding window of three words on each side,
we compiled a list of word n-gram contexts that
are strongly associated with the usage of a comma.
This list is then used to insert missing commas in
the test data.
4.1.3 Style Errors
The style (Wtone) errors marked in the corpus are
diverse, and the annotations are often not consis-
tent. We constructed a pattern-based system to
deal with two types of style errors that are com-
monly annotated. The first type of style edit avoids
using contractions of negated auxiliary verbs. For
example, it changes ?do n?t? to ?do not?. We use a
pattern-based classifier to identify such errors and
replace the contractions. The second type of style
edit encourages the use of a semi-colon to join
two independent clauses when a conjunctive ad-
verb is used. For example, it edits ?[clause], how-
ever, [clause]? to ?[clause]; however, [clause]?. To
identify such errors, we use a part-of-speech tag-
ger to recognize conjunctive adverbs signifying in-
dependent clauses: if two clauses are joined by the
pattern ?, [conjunctive adverb],?, we will replace it
with ?; [conjunctive adverb],?.
4.2 Modules not Included in the Final System
In addition to the modules described above, we at-
tempted to address two other common error cate-
gories: spelling errors and collocation errors. We
37
Illinois
Classifiers Training data Algorithm
Article Learner AP with inflation
Preposition Native NB-priors
Noun number Native NB
Verb agreement Native NB
Verb form Native NB
Illinois-Columbia
Classifiers Training data Algorithm
Article Learner and native AP with infl. (learner) and NB-priors (native)
Preposition Learner and native AP with infl. (learner) and NB-priors (native)
Noun number Learner and native AP with infl. (learner) and NB (native)
Verb agreement Native AP with infl. (learner) and NB (native)
Verb form Native NB-priors
Word form Native NB-priors
Orthography/punctuation Learner AP and pattern-based
Style Learner Pattern-based
Model combination Section 4.3
Global inference Section 4.4
Table 3: The baseline (Illinois) system vs. the Illinois-Columbia system. AP stands for Averaged
Perceptron, and NB stands for the Na??ve Bayes algorithm.
describe these below even though they were not
included in the final system.
Regular spelling errors are noticeable but not
very frequent, and a number are not marked in
the corpus (for example, the word ?dictronary? in-
stead of ?dictionary? is not tagged as an error). We
used an open source package ? ?Jazzy?
4
? to at-
tempt to automatically correct these errors to im-
prove context signals for other modules. However,
there are often multiple similar words that can be
proposed as corrections, and Jazzy uses phonetic
guidelines that sometimes lead to unintuitive pro-
posals (such as ?doctrinaire? for ?dictronary?). It
would be possible to extend the system with a filter
on candidate answers that uses n-grams or some
other context model to choose better candidates,
but the relatively small number of such errors lim-
its the potential impact of such a system.
Collocation errors are the second most common
error category accounting for 11.94% of all errors
in the training data (Table 1). We tried using the
Illinois context-sensitive spelling system
5
to de-
tect these errors, but this system requires prede-
fined confusion sets to detect possible errors and
to propose valid corrections. The coverage of the
pre-existing confusion sets was poor ? the system
4
http://jazzy.sourceforge.net
5
http://cogcomp.cs.illinois.edu/cssc/
could potentially correct only 2.5% of collocation
errors ? and it is difficult to generate new con-
fusion sets that generalize well, which requires a
great deal of annotated training data. The sys-
tem performance was relatively poor because it
proposed many spurious corrections: we believe
this is due to the relatively limited context it uses,
which makes it particularly susceptible to making
mistakes when there are multiple errors in close
proximity.
4.3 Model Combination
Model combination is another key extension of the
Illinois system.
In the Illinois-Columbia system, article, prepo-
sition, noun, and verb agreement errors are each
addressed via a model that combines error predic-
tions made by a classifier trained on the learner
data with the AP algorithm and those made by
the NB model trained on the Google corpus. The
AP classifiers all make use of richer sets of fea-
tures than the native-trained classifiers: the article,
noun number, and preposition classifiers employ
features that use POS information, while the verb
agreement classifier also makes use of dependency
features extracted using a parser (de Marneffe et
al., 2008). For more detail on the features used
in the agreement module, we refer the reader to
38
Rozovskaya et al. (2014). Finally, all of the AP
models use the source word of the author as a fea-
ture and, similar to the article AP classifier (Sec-
tion 3), implement the error inflation method. The
combined model generates a union of corrections
produced by the components.
We found that for every error type, the com-
bined model is superior to each of the single classi-
fiers, as it combines the advantages of both of the
classifiers so that they complement one another.
In particular, while each of the learner and native
components have similar precision, since the pre-
dictions made differ, the recall of the combined
model improves.
4.4 Joint Inference
One of the mistakes typical for Illinois system
were inconsistent predictions. Inconsistent predic-
tions occur when the classifiers address grammat-
ical phenomena that interact at the sentence level,
e.g. noun number and verb agreement. To ad-
dress this problem, the Illinois-Columbia system
makes use of global inference via an Integer Lin-
ear Programming formulation (Rozovskaya and
Roth, 2013). Note that Rozovskaya and Roth
(2013) also describe a joint learning model that
performs better than the joint inference approach.
However, the joint learning model is based on
training a joint model on the Google corpus, and
is not as strong as the individually-trained classi-
fiers of the Illinois-Columbia system that combine
predictions from two components ? NB classifiers
trained on the native data from the Google corpus
and AP models trained on the learner data (Sec-
tion 4.3).
5 Experimental Results
In Sections 3 and 4, we described the individual
system components that address different types of
errors. In this section, we show how the system
improves when each component is added into the
system. In this year?s competition, systems are
compared using F0.5 measure instead of F1. This
is because in error correction good precision is
more important than having a high recall, and the
F0.5 reflects that by weighing precision twice as
much as recall. System output is scored with the
M2 scorer (Dahlmeier and Ng, 2012).
Table 4 reports performance results of each in-
dividual classifier. In the final system, the arti-
cle, preposition, noun number, and verb agree-
Model P R F0.5
Articles (AP) 38.97 8.85 23.19
Articles (NB-priors) 47.34 6.01 19.93
Articles (Comb.) 38.73 10.93 25.67
Prep. (AP) 34.00 0.5 2.35
Prep. (NB-priors) 33.33 0.79 3.61
Prep. (Comb.) 30.06 1.17 5.13
Noun number (NB) 44.74 5.48 18.39
Noun number (AP) 82.35 0.41 2.01
Noun number (Comb.) 45.02 5.57 18.63
Verb agr. (AP) 38.56 1.23 5.46
Verb agr. (NB) 63.41 0.76 3.64
Verb agr. (Comb.) 41.09 1.55 6.75
Verb form (NB-priors) 59.26 1.41 6.42
Word form (NB-priors) 57.54 3.02 12.48
Mec (AP; patterns) 48.48 0.47 2.26
Style (patterns) 84.62 0.64 3.13
Table 4: Performance of classifiers targeting
specific errors.
Model P R F0.5
The baseline (Illinois) system
Articles 38.97 8.85 23.19
+Prepositions 39.24 9.35 23.93
+Noun number 42.13 14.83 30.79
+Subject-verb agr. 42.25 16.06 31.86
+Verb form 43.19 17.20 33.17
Model Combination
+Model combination 42.72 20.19 34.92
Additional Classifiers
+Word form 43.39 21.54 36.07
+Mec 43.70 22.04 36.52
+Style 44.22 21.54 37.09
Joint Inference
+Joint Inference 44.28 22.57 37.13
Table 5: Results on the development data. The
top part of the table shows the performance of the
baseline (Illinois) system from last year.
P R F0.5
Scores based on the original annotations
41.78 24.88 36.79
Scores based on the revised annotations
52.44 29.89 45.57
Table 6: Results on Test.
39
ment classifiers use combined models, each con-
sisting of a classifier trained on the learner data
and a classifier trained on native data. We report
performance of each such component separately
and when they are combined. The results show
that combining models boosts the performance of
each classifier: for example, the performance of
the article classifier improves by more than 2 F0.5
points. It should be noted that results are com-
puted with respect to all errors present in the data.
For this reason, recall is low.
Next, in Table 5, we show the contribution of
the novel components over the baseline system on
the development set. As described in Section 3,
the baseline Illinois system consists of five indi-
vidual components; their performance is shown in
the top part of the table. Note that although for the
development set we make use of last year?s test
set, these results are not comparable to the perfor-
mance results reported in last year?s competition
that used the F1 measure. Overall, the baseline
system achieves an F0.5 score of 33.17 on the de-
velopment set.
Then, by applying the model combination tech-
nique introduced in Section 4.3, the performance
is improved to 34.92. By adding modules to tar-
get three additional error types, the overall perfor-
mance becomes 37.09. Finally, the joint inference
technique (see Section 4.4) slightly improves the
performance further. The final system achieves an
F0.5 score of 37.13.
Table 6 shows the results on the test set provided
by the organizers. As was done previously, the
organizers also offered another set of annotations
based on the combination of revised official anno-
tations and accepted alternative annotations pro-
posed by participants. Performance results on this
set are also shown in Table 6.
6 Discussion and Error Analysis
Here, we present some interesting errors that our
system makes on the development set and discuss
our observations on the competition. We analyze
both the false positive errors and those cases that
are missed by our system.
6.1 Error Analysis
Stylistic preference Surveillance technology
such as RFID (radio-frequency identification) is
one type of examples that has currently been im-
plemented.
Here, our system proposes a change to plural
for the noun ?technology?. The gold standard
solution instead proposes a large number of cor-
rections throughout that work with the choice of
the singular ?technology?. However, using the
plural ?technologies? as proposed by the Illinois-
Columbia system is quite acceptable, and a com-
parable number of corrections would make the rest
of the sentence compatible. Note also that the
gold standard proposes the use of commas around
the phrase ?such as RFID (radio-frequency iden-
tification)?, which could also be omitted based on
stylistic considerations alone.
Word choice The high accuracy in utiliz-
ing surveillance technology eliminates the
*amount/number of disagreements among people.
The use of ?amount? versus ?number? depends
on the noun to which the term attaches. This could
conceivably be achieved by using a rule and word
list, but many such rules would be needed and each
would have relatively low coverage. Our system
does not detect this error.
Presence of multiple errors Not only the details
of location will be provided, but also may lead to
find out the root of this kind of children trading
agency and it helps to prevent more this kind of
tragedy to happen on any family.
The writer has made numerous errors in this
sentence. To determine the correct preposition in
the marked location requires at least the preced-
ing verb phrase to be corrected to ?from happen-
ing?; the extraneous ?more? after ?prevent? in turn
makes the verb phrase correction more unlikely as
it perturbs the contextual clues that a system might
learn to make that correction. Our system pro-
poses a different preposition ? ?in? ? that is better
than the original in the local context, but which is
not correct in the wider context.
Locally coherent, globally incorrect People?s
lives become from increasingly convenient to al-
most luxury, thanks to the implementation of in-
creasingly technology available for the Man?s life.
In this example, the system proposes to delete
the preposition ?from?. This correctiom improves
the local coherency of the sentence. However, the
resulting construction is not consistent with ?to al-
most luxury?, suggesting a more complex correc-
tion (changing the word ?become? to ?are going?).
40
Cascading NLP errors In this, I mean that we
can input this device implant into an animal or
birds species, for us to track their movements and
actions relating to our human research that can
bring us to a new regime.
The word ?implant? in the example sentence
has been identified as a verb by the system and
not a noun due to the unusual use as part of the
phrase ?device implant?. As a result, the system
incorrectly proposes the verb form correction ?im-
planted?.
6.2 Discussion
The error analysis suggests that there are three sig-
nificant challenges to developing a better gram-
mar correction system for the CoNLL-2014 shared
task: identifying candidate errors; modeling the
context of possible errors widely enough to cap-
ture long-distance cues where necessary; and
modeling stylistic preferences involving word
choice, selection of plural or singular, standards
for punctuation, use of a definite or indefinite arti-
cle (or no article at all), and so on. For ESL writ-
ers, the tendency for multiple errors to be made in
close proximity means that global decisions must
be made about sets of possible mistakes, and a sys-
tem must therefore have a quite sophisticated ab-
stract model to generate the basis for consistent
sets of corrections to be proposed.
7 Conclusion
We have described our system that participated in
the shared task on grammatical error correction.
The system builds on the elements of the Illinois
system that participated in last year?s shared task.
We extended and improved the Illinois system in
three key dimensions, which we presented and
evaluated in this paper. We have also presented
error analysis of the system output and discussed
possible directions for future work.
Acknowledgments
This material is based on research sponsored by DARPA un-
der agreement number FA8750-13-2-0008. The U.S. Gov-
ernment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright nota-
tion thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as necessar-
ily representing the official policies or endorsements, either
expressed or implied, of DARPA or the U.S. Government.
This research is also supported by a grant from the U.S. De-
partment of Education and by the DARPA Machine Reading
Program under Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-018. The first and last authors
were partially funded by grant NPRP-4-1058-1-168 from the
Qatar National Research Fund (a member of the Qatar Foun-
dation). The statements made herein are solely the responsi-
bility of the authors.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium, Philadelphia, PA.
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In IAAI.
D. Dahlmeier and H.T. Ng. 2012. Better evaluation
for grammatical error correction. In NAACL, pages
568?572, Montr?eal, Canada, June. Association for
Computational Linguistics.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL
HLT 2013 Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A
report on the preposition and determiner error cor-
rection shared task. In Proc. of the NAACL HLT
2012 Seventh Workshop on Innovative Use of NLP
for Building Educational Applications, Montreal,
Canada, June. Association for Computational Lin-
guistics.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In ACL.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proc. 13th
International Conference on Machine Learning.
A. R. Golding and D. Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning.
H.T. Ng, S.M. Wu, Y. Wu, C. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task
on grammatical error correction. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning: Shared Task, pages 1?12,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
41
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
shared task on grammatical error correction. In Pro-
ceedings of the Eighteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
Baltimore, Maryland, USA, June. Association for
Computational Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL.
A. Rozovskaya and D. Roth. 2013. Joint learning
and inference for grammatical error correction. In
EMNLP, 10.
A. Rozovskaya, M. Sammons, and D. Roth. 2012.
The UI system in the HOO 2012 shared task on er-
ror correction. In Proc. of the Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL) Workshop
on Innovative Use of NLP for Building Educational
Applications.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system
in the CoNLL-2013 shared task. In CoNLL Shared
Task.
A. Rozovskaya, D. Roth, and V. Srikumar. 2014. Cor-
recting grammatical verb errors. In EACL.
42
