Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 99?108, Dublin, Ireland, August 23-29 2014.
Multi-Objective Search Results Clustering
Sudipta Acharya Sriparna Saha
Indian Institute of Technology Patna
Kurji, Patna, Bihar, India
{sudipta.pcs13,sriparna}@iitp.ac.in
Jose G. Moreno Ga
?
el Dias
Normandie University - CNRS GREYC
Caen, France
first.last@unicaen.fr
Abstract
Most web search results clustering (SRC) strategies have predominantly studied the definition of
adapted representation spaces to the detriment of new clustering techniques to improve perfor-
mance. In this paper, we define SRC as a multi-objective optimization (MOO) problem to take
advantage of most recent works in clustering. In particular, we define two objective functions
(compactness and separability), which are simultaneously optimized using a MOO-based simu-
lated annealing technique called AMOSA. The proposed algorithm is able to automatically detect
the number of clusters for any query and outperforms all state-of-the-art text-based solutions in
terms of F
?
-measure and F
b
3-measure over two gold standard data sets.
1 Introduction
Web search results clustering (SRC), also known as post-retrieval clustering or ephemeral clustering has
received much attention for the past twenty years for easing up user?s effort in web browsing. The key
idea behind SRC systems is to return some meaningful labeled clusters from a set of web documents (or
web snippets) retrieved from a search engine for a given query.
Recently, SRC strategies have been focusing on the introduction of external (exogenous) knowledge to
better capture semantics between documents (Scaiella et al., 2012; Marco and Navigli, 2013). Although
this research direction has evidenced competitive results, the proposed clustering techniques are based
on a single cluster quality measure, which must reflect alone the goodness of a given partitioning. These
techniques are usually referred to as single objective optimizations (SOO).
In this paper, we hypothesize that improved clustering can be achieved by defining different objective
functions over well-known data representations. As such, our study aims to focus on new clustering
issues for SRC instead of defining new representation spaces.
Recent studies (Maulik et al., 2011) have shown that clustering can be defined as a multi-objective
optimization (MOO) problem. Within the context of SRC, we propose to define two objective functions
(compactness and separability), which are simultaneously optimized using a MOO-based simulated an-
nealing technique called AMOSA (Bandyopadhyay et al., 2008).
In order to draw conclusive remarks, we present an exhaustive evaluation where our MOO algorithm
(MOO-clus) is compared to the most competitive text-based (endogenous) SRC algorithms: STC (Zamir
and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Romano, 2010) and
GK-means (Moreno et al., 2013). Experiments are run over two different gold standard data sets (ODP-
239 and MORESQUE) for two clustering evaluation metrics (F
?
-measure and F
b
3
-measure). Results
show that MOO-clus outperforms all text-based solutions and approaches performances of knowledge
driven strategies (Scaiella et al., 2012). In this paper, our main contributions are:
? The first
1
attempt to solve SRC by defining multiple objective functions,
? A new MOO clustering algorithm for SRC, which automatically determines the number of clusters,
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/.
1
As far as we know.
99
? An exhaustive evaluation of SRC algorithms with recent data sets and evaluation metrics over the
most competitive state-of-the-art text-based SRC algorithms.
2 Related Work
2.1 SRC Algorithms
One of the most cited SRC solutions is the Suffix Tree Clustering (STC) algorithm proposed by (Zamir
and Etzioni, 1998). They propose a monothetic clustering technique, which merges base clusters with
high string overlap based on web snippets represented as compact tries. Their evaluation shows improve-
ments over agglomerative hierarchical clustering, K-Means, Buckshot, Fractionation and Single-Pass
algorithms, and is still a hard baseline to beat (Moreno and Dias, 2014).
Later, (Osinski and Weiss, 2005) proposed a polythetic solution called LINGO based on the same
string representation as of (Zamir and Etzioni, 1998). They first extract frequent phrases based on suffix-
arrays and match group descriptions with topics obtained with latent semantic analysis. Documents are
then assigned straightforwardly to their corresponding groups. Their evaluation does not allow conclu-
sive remarks but they propose an open source implementation, which is an important contribution.
More recently, (Carpineto and Romano, 2010) showed that the characteristics of the outputs returned
by SRC algorithms suggest the adoption of a meta clustering approach. The underlying idea is that dif-
ferent SOO solutions lead to complementary results that must be combined. So, they introduce a novel
criterion to measure the concordance of two partitions of objects into different clusters based on the infor-
mation content associated to the series of decisions made by the partitions on single pairs of objects. The
results of OPTIMSRC demonstrate that meta clustering is superior over individual clustering techniques.
The latest work, exclusively based on endogenous information (i.e. web snippets returned by the
search engine), is proposed by (Moreno et al., 2013). They adapt the K-means algorithm to a third-order
similarity measure and propose a stopping criterion to automatically determine the ?optimal? number of
clusters. Experiments are run over two gold standard data sets, ODP-239 (Carpineto and Romano, 2010)
and MORESQUE (Navigli and Crisafulli, 2010), and show improved results over all state-of-the-art
text-based SRC techniques so far.
A great deal of works have also proposed to include exogenous information to solve the SRC problem.
One important work is proposed by (Scaiella et al., 2012) who use Wikipedia articles to build a bipartite
graph and apply spectral clustering over it to discover relevant clusters. More recently, (Marco and
Navigli, 2013) proposed to include word sense induction based on the Web1T corpus (Brants and Franz,
2006) to improve SRC. In this paper, we exclusively focus on endogenous solutions.
2.2 MOO-based Clustering
Many works have been proposed where the problem of clustering is posed as one of multi-objective op-
timization (Deb, 2009; Maulik et al., 2011). One important work is proposed by (Handl and Knowles,
2007) who define a multi-objective clustering technique with automatic K-determination called MOCK.
Their algorithm outperforms several standard single-objective clustering algorithms (K-means, agglom-
erative hierarchical clustering and ensemble clustering) on artificial data sets.
In parallel, a multi-objective evolutionary algorithm for fuzzy clustering is proposed by (Bandyopad-
hyay et al., 2007) for clustering gene expressions. Here, two objectives are simultaneously optimized.
The first one is the objective function optimized in the fuzzy C-means algorithm (Bezdek, 1981) and the
other one is the Xie-Beni index (Xie and Beni, 1991).
Later, (Mukhopadhyay and Maulik, 2009) proposed a novel approach that combines the multi-
objective fuzzy clustering method of (Bandyopadhyay et al., 2007) with a Support Vector Machines
(SVM) classifier. Performance results are provided for remote sensing data.
As far as we know, within text applications, (Morik et al., 2012) is the first work, which formulates
text clustering a multi-objective optimization problem. In particular, they express desired properties
of frequent termset clustering in terms of multiple conflicting objective functions. The optimization is
solved by a genetic algorithm and the result is a set of Pareto-optimal solutions. Note that this effort is
100
defined for large text colllections with high dimensional data, which is contradictory to the specific task
of SRC (Carpineto et al., 2009)
2
.
2.3 Our Motivation
Recent works have focused on the introduction of external (exogenous) knowledge to solve the SRC
task. However, this research direction higly depends on existing resources, which are not available for a
great deal of languages. Moreover, (Carpineto and Romano, 2010) has suggested an interesting research
direction, which has still remained unexplored. Indeed, (Carpineto and Romano, 2010) showed that meta
clustering leads to improved results in the context of text-based (endogenous) SRC. This suggests that
better clustering can be obtained by combining different SOO solutions. However, their algorithm is
casted to a SOO problem of the concordance between the clustering combination and a meta partition.
As a consequence, we hypothesize that improved performances can be obtained by defining the SRC
task as a MOO clustering problem. For that purpose, we (1) take advantage of the recent advances in the
field of multi-objective clustering (Saha and Bandyopadhyay, 2010), (2) define new objective functions
in a non euclidean space and (3) adapt a MOO-based simulated annealing technique called AMOSA
(Bandyopadhyay et al., 2008) to take into account third-order similarity metrics (Moreno et al., 2013).
3 Clustering as a MOO Problem
3.1 Formal Definition of MOO Clustering
Multi-objective optimization can be formally stated as finding the vector x
?
= [x
?
1
, x
?
2
, . . . , x
?
n
]
T
of
decision variables that simultaneously optimize M objective function values {f
1
(x), f
2
(x), . . . , f
M
(x)}
while satisfying user-defined constraints, if any.
An important concept in MOO is that of domination. Within the context of a maximization prob-
lem, a solution x
i
is said to dominate x
j
if ?k ? 1, 2, . . . ,M, f
k
(x
i
) ? f
k
(x
j
) and ?k ?
1, 2, . . . ,M, such that f
k
(x
i
) > f
k
(x
j
).
Among a set of solutions R, the non-dominated set of solutions R
?
are those that are not dominated by
any member of the set R and is called the globally Pareto-optimal set or Pareto front. In general, a MOO
algorithm outputs a set of solutions not dominated by any solution encountered by it. These notions can
be illustrated by considering an optimization problem with two objective functions (f
1
and f
2
) with six
different solutions, as shown in Figure 1. Here target is to maximize both objective functions f
1
and f
2
.
1
3
4
5
Pareto Front
2
6
f1(maximize)
f2(maximize)
Figure 1: Example of dominance and Pareto optimal front.
In this example, solutions 3, 4 and 5 dominate all the other three solutions 1, 2 and 6. Solutions 3, 4
and 5 are nondominating to each other. Because 3 is better than 4 w.r.t. function f
1
, but 4 is better than
3 w.r.t. f
2
. Similarly 4 is better than 5 w.r.t. f
1
but 5 is better than 4 w.r.t. f
2
. The same happens for
solutions 3 and 5. So, the Pareto front is made of solutions 3, 4 and 5.
Within the specific context of clustering, two objective functions are usually defined, which must be
optimized simultaneously. These functions are based on two intrinsic properties of the data space and
are defined as follows.
2
SRC is usually referred to as text clustering in the ?small?: i.e. small list of short text documents.
101
Compactness: This objective function measures the proximity among the various elements of a given
cluster and must be maximized.
Separability: This objective function measures the similarity between two cluster centroids and must
be minimized.
3.2 AMOSA Optimization Strategy
Clustering is viewed as a search problem, where optimal partitions satisfying the given set of objective
functions must be discovered. As such, an optimization strategy must be defined. Here, we propose to
use archived multi-objective simulated annealing (AMOSA) proposed by (Bandyopadhyay et al., 2008).
AMOSA incorporates the concept of an archive where the non-dominated solutions seen so far are stored.
Two limits are kept on the size of the archive: a hard limit denoted by HL and a soft limit denoted by
SL. Given ? > 1, the algorithm begins with the initialization of a number (? ? SL) of solutions each of
which representing a state in the search space. Thereafter, the non-dominated solutions are determined
and stored in the archive.
Then, one point is randomly selected from the archive. This is taken as the current point, or the initial
solution, at temperature T = T
max
. The current point is perturbed/mutated to generate a new solution
named new-pt and its objective functions are computed. The domination status of the new-pt is checked
w.r.t. the current point and the solutions in the archive. Based on domination status, different cases may
arise: (i) accept the new-pt, (ii) accept the current-pt or (iii) accept a solution from the archive. In case
of overflow of the archive, its size is reduced to HL.
The process is repeated iter times for each temperature that is annealed with a cooling rate of ? (<1)
till the minimum temperature T
min
is attained. The process thereafter stops and the archive contains the
final non-dominated solutions i.e. the Pareto front.
4 SRC as MOO Problem: MOO-clus
4.1 Archive Initialization
As we follow an endogenous approach, only the information returned by a search engine is used. In
particular, we only deal with web snippets and each one is represented as a word feature vector. So, our
solution called MOO-clus starts its execution after initializing the archive with some random solutions
as archive members. Here, a particular solution refers to a complete assignment of web snippets (or data
points) in several clusters. So, the first step is to represent a solution compatible with AMOSA, which
represents each individual solution as a string. In order to encode the clustering problem in the form of
a string, a center-based representation is used. Note that the use of a string representation facilitates the
definition of individuals and mutation functions (Bandyopadhyay et al., 2008).
Let us assume that the archive member i represents the centroids of K
i
clusters and the number of
tokens in a centroid is p
3
, then the archive member (or string) has length l
i
where l
i
= p?K
i
. To initialize
the number of centroids K
i
encoded in the string i, a random value between 2 and K
max
is chosen and
each K
i
cluster centroid is initialized by randomly generated tokens from the global vocabulary.
4.2 Assignment of Web Snippets
As for any classical clustering algorithms, web snippets (or data points) must be assigned to their respec-
tive clusters. In MOO-clus, this assignment is computed as in (Moreno et al., 2013), to take advantage
of recent advances in similarity measures. For two word feature vectors d
i
and d
j
, their similarity is
evaluated by the similarity of their constituents as defined in Equation 1.
S(d
i
, d
j
) =
1
?d
i
??d
j
?
?d
i
?
?
r=1
?d
j
?
?
b=1
SCP (w
r
i
, w
b
j
), with SCP (w
1
, w
2
) =
P (w
1
, w
2
)
2
P (w
1
) ? P (w
2
)
(1)
3
A centroid is represented by a p word feature vector (w
1
k
, w
2
k
, w
3
k
, . . . , w
p
k
).
102
Here, w
r
i
(resp. w
b
j
) corresponds to the token at the r
th
(resp. b
th
) position of the word feature vector d
i
(resp. d
j
). ?d
i
? and ?d
j
? respectively denote the total number of tokens in word feature vectors d
i
and
d
j
. SCP (w
r
i
, w
b
j
) is the Symmetric Conditional Probability (da Silva et al., 1999) where P (., .) is the
joint probability of two tokens (w
1
and w
2
) appearing in the same word feature vector and P (.) is the
marginal probability of any token appearing in a word feature vector.
Note that each cluster centroid is a word feature vector of varying number of tokens. Thus, Equation 2
is used to assign any data point (web snippet) d
j
to a cluster twhose centroid has the maximum similarity
value to d
j
.
t = argmax
k=1,...K
S(d
j
,m
pi
k
) (2)
K denotes the total number of clusters, d
j
is the j
th
web snippet, m
pi
k
is the centroid of the k
th
cluster pi
k
and S(d
j
,m
pi
k
) denotes similarity measurement between the point d
j
and cluster centroid
m
pi
k
defined in Equation 1.
4.3 Definition of Objective Functions
A string i represents a set of centroids to which web snippets can be assigned as seen in Section 4.2. As a
consequence, each string i corresponds to a candidate partition of the data space. Now, in order to verify
the domination of different solutions over other ones, objective functions must be defined. Compactness
and separability are usually used in MOO clustering solutions. Here, compactness can be defined as the
informational density of each cluster. This can be straightforwardly formulated as in Equation 3.
Compactness =
K
?
k=1
?
d
i
?pi
k
S(d
i
,m
pi
k
) (3)
Note that if tokens in a particular cluster are very similar to the cluster centroid then the corresponding
Compactness value would be maximized. Here our target is to form good clusters whose compactness
in terms of similarity should be maximum.
The second objective function is cluster separability, which measures the dissimilarity between two
cluster centroids. Indeed, the purpose of any clustering algorithm is to obtain compact similar typed
clusters, which are dissimilar to each other. Here, we define separability as the minimization of the
summation of similarities between each pair of cluster centroids. This is defined in Equation 4, where
m
pi
k
and m
pi
o
are the centroids of clusters pi
k
and pi
o
, respectively.
Separability =
K
?
k=1
K
?
o=k+1
S(m
pi
k
,m
pi
o
) (4)
Finally, for a particular string, the following objectives {Compactness,
1
Separability
} are maximized
using the search capability of AMOSA.
4.4 Search Operators
In MOO-clus, AMOSA is used as the optimization strategy. For that purpose, three different types of
mutation operations have been defined to suit the framework.
Mutation 1: This mutation operation is used to update the cluster center representation. Each token of
cluster centroid is replaced by one token from the global vocabulary according to highest SCP similarity.
This is applied individually to all tokens of a particular centroid if it is selected for mutation.
Mutation 2: This mutation operation is used to reduce the size of the string by 1. We randomly select a
cluster centroid and thereafter all the tokens of this centroid are deleted from the string.
Mutation 3: This mutation is for increasing the size of string by 1 i.e. one new centroid is inserted in
the string. For that purpose, we randomly choose p number of tokens from the global vocabulary and
add it to the string.
103
Let be a string < w
1
w
2
w
3
w
4
w
5
w
6
> representing three cluster centroids (w
1
, w
2
), (w
3
, w
4
) and
(w
5
, w
6
)
4
. For mutation 1, let position 2 be selected randomly. Each token of the word vector (w
3
, w
4
)
will be changed by some token from the global vocabulary using SCP. Then, after change, the string
will look like < w
1
w
2
w
new
3
w
new
4
w
5
w
6
>. If mutation 2 is selected, a centroid will be removed from
the string. Let centroid 3 be selected for deletion. The new string will look like < w
1
w
2
w
3
w
4
>.
In case of mutation 3, a new centroid will be added to the string. A new cluster centroid is generated
choosing p=2 number of tokens from the global vocabulary. Let the randomly generated new clus-
ter centroid to be added to the string be (w
7
, w
8
). After inclusion of this centroid, the string will be
< w
1
w
2
w
3
w
4
w
5
w
6
w
7
w
8
>. In our experiments, we have associated equal probability to each of
these mutation operations. Thus, each mutation is applied in 33% cases of the cases.
5 Experimental Setup
5.1 Datasets
The main gold standards used for the evaluation of SRC algorithms are ODP-239 and MORESQUE
5
.
In ODP-239 (Carpineto and Romano, 2010), each document is represented by a title and a web snip-
pet and the subtopics are chosen from the top levels of DMOZ
6
. On the other hand, the subtopics in
MORESQUE (Navigli and Crisafulli, 2010) follow a more natural distribution as they are defined based
on the disambiguation pages of Wikipedia. As such, the subtopics cover most of the query-related senses.
However, not all queries are Wikipedia related or ambiguous (e.g. ?Olympic Games?, which Wikipedia
entry is not ambiguous, although there are many events related to this topic). As a consequence, it is
clear that different results can be obtained from one data set to another. A quick summary of both data
sets is presented in Table 1.
# of # of Subtopics # of
Dataset queries Avg / Min / Max Snippets
ODP-239 239 10 / 10 / 10 25580
MORESQUE 114 6.7 / 2 / 38 11402
Table 1: SRC gold standard data sets.
5.2 Evaluation Metrics
A successful SRC systemmust evidence high quality level clustering. Each query subtopic should ideally
be represented by a unique cluster containing all the relevant web pages inside. However, determining a
unique and complete metric to evaluate the performance of a clustering algorithm is still an open problem
(Amig?o et al., 2013).
In this paper, we propose to use the F
b
3
-measure (Amig?o et al., 2009) to explore the Pareto front.
In particular, F
b
3
has been defined to evaluate cluster homogeneity, completeness, rag-bag and size-vs-
quantity constraints. F
b
3
is a function of Precision
b
3
(P
b
3
) and Recall
b
3
(R
b
3
). All metrics are defined
in Equation 5
F
b
3
=
2 ? P
b
3
?R
b
3
P
b
3
+ R
b
3
, P
b
3
=
1
N
K
?
i=1
?
d
j
?pi
i
1
|pi
i
|
?
d
l
?pi
i
g
?
(d
j
, d
l
), R
b
3
=
1
N
K
?
i=1
?
d
j
?pi
?
i
1
|pi
?
i
|
?
d
l
?pi
?
i
g(d
j
, d
l
) (5)
where pi
i
is i
th
cluster, pi
?
i
is the gold standard of the category i, and g
?
(., .) and g(., .) are defined as
follows:
g
?
(d
i
, d
j
) =
{
1 ? ?l : d
i
? pi
?
l
? d
j
? pi
?
l
0 otherwise
and g(d
i
, d
j
) =
{
1 ? ?l : d
i
? pi
l
? d
j
? pi
l
0 otherwise
.
4
with p=2.
5
AMBIENT has received less attention since the creation of ODP-239.
6
http://www.dmoz.org [Last access: 14/03/2014].
104
Most SRC studies have also used the F
?
-measure (F
?
), which is defined in Equation 6.
F
?
=
(?
2
+ 1) ? P ?R
?
2
? P + R
, P =
TP
TP + FP
, R =
TP
TP + FN
(6)
where
TP =
K
?
i=1
?
d
j
?pi
?
i
?
d
l
? pi
?
i
l 6= j
g(d
i
, d
j
), FP =
K
?
i=1
?
d
j
?pi
i
?
d
l
? pi
i
l 6= j
(1? g
?
(d
i
, d
j
)), FN =
K
?
i=1
?
d
j
?pi
?
i
?
d
l
? pi
?
i
l 6= j
(1? g(d
i
, d
j
)).
6 Results and Discussion
In this evaluation, we used the open source framework GATE (Cunningham et al., 2013) without stop-
word removal for web snippet tokenization
7
. We executed MOO-clus over ODP-239 and MORESQUE.
The parameters of MOO-clus are: T
min
= 0.01, T
max
= 100, ? = 0.85, HL = 10, SL = 20 and
iter = 15. Note that, they have been determined after conducting a thorough sensitivity study. A first
set of experiments have been conducted for different p values of tokens present in the centroid, namely
in the range 2 to 5 in order to understand the behavior of MOO-clus w.r.t. centroid size
8
. Note that the
partition with maximum F
b
3 is choosen for each size of p
9
. Overall results are shown in Table 2.
MORESQUE ODP-239
MOO-clus MOO-clus
2 3 4 5 2 3 4 5
F
b
3
0.477 0.491 0.497 0.502 0.478 0.481 0.484 0.481
F
1
0.661 0.666 0.675 0.658 0.379 0.379 0.384 0.381
F
2
0.750 0.768 0.764 0.742 0.534 0.536 0.537 0.535
F
5
0.831 0.862 0.846 0.820 0.717 0.720 0.716 0.715
Table 2: Evaluation results of MOO-clus over MORESQUE and ODP239 data sets.
Results show that for MORESQUE, MOO-clus obtains the highest F
b
3 value for p=5. In particular,
performance increases for higher values of p. For ODP-239, best results are reported for p=4, but evi-
dence less sensitivity to the number of words in the centroids. Indeed, a marginal difference is obtained
between all runs. In terms of F
?
, the same behaviour is obtained for ODP-239. But, for MORESQUE,
best results are provided for smaller values of p, namely p=3.
Two important comments must be pointed at. In the first place, F
b
3
shows a steady behaviour compared
to F
?
when the data set changes. The conclusions drawn in (Amig?o et al., 2009) reporting the superiority
of F
b
3
over F
?
seem to be verified for the specific case of SRC. In the second place, MOO-clus evidences
a marginal sensitivity to different p values. Indeed, for ODP-239, changing p between 2 and 5 words has
a negligible impact on F
b
3 . The figures show a different behaviour for MORESQUE but this can easily
be explained. In MORESQUE, less queries are provided for test and the number of reference clusters
varies between 2 and 38, with a majority of queries containing very few clusters (the average cluster size
is 6.7). As such, small clustering errors may result in high deviations in the evaluation metrics. So, p
can be seen as a non influent parameter for clustering purposes. In fact, increasing the value of p may
exclusively allow a more descriptive power for cluster labeling.
We also compared MOO-clus to the current state-of-the-art text-based (endogenous) SRC algorithms:
STC (Zamir and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Ro-
mano, 2010), Bisecting Incremental K-means (BIK), GK-means (Moreno et al., 2013) and the combi-
nation STC-LINGO (Moreno and Dias, 2014). The results are illustrated in Table 3 where we provide
values for all the metrics for open source implementations and reported values in the literature for the
7
Note that keeping stop words is a challenging task as most methodologies withdraw these elements as they are hard to
handle. This decision is supported by the fact that we aim to produce as much as possible language-independent solutions.
8
Note that to ease the user effort in searching for information, the cluster label must be small and expressive. Typical
configurations range between 3 to 5 to include multiword expressions.
9
F
?
metrics are calculated over the partition with highest F
b
3
value.
105
other experiments i.e. OPTIMSRC, GK-means and STC-LINGO. In particular, the Min (resp. Max)
column refers to the worst (resp. best) performance when varying p, the size of the centroid.
The results of Table 3 clearly show the performance improvements of our proposed methodology over
existing text-based techniques for both data sets and most evaluation metrics. For ODP-239, MOO-clus
attains the highest values with respect to F
1
, F
2
, F
5
and F
b
3
metrics against all existing endogenous algo-
rithms. For MORESQUE, our algorithm reaches highest performance over all state-of-the-art algorithms
for F
1
and F
b
3 metrics but marginally fails for F
2
and F
5
against GK-means.
MOO-clus SOO SRC Combination of SOO SRC
Min Max GK-means STC LINGO BIK OPTIMSRC STC-LINGO
MORESQUE F
1
0.658 0.675 0.665 0.455 0.326 0.317 N/A 0.561
F
2
0.742 0.768 0.770 0.392 0.260 0.269 N/A N/A
F
5
0.820 0.862 0.872 0.370 0.237 0.255 N/A N/A
F
b
3
0.477 0.502 0.482 0.460 0.399 0.315 N/A 0.498
ODP-239 F
1
0.379 0.384 0.366 0.324 0.273 0.200 0.313 0.362
F
2
0.534 0.537 0.416 0.319 0.167 0.173 0.341 N/A
F
5
0.715 0.720 0.462 0.322 0.153 0.165 0.380 N/A
F
b
3
0.478 0.484 0.452 0.403 0.346 0.307 N/A 0.425
Table 3: Comparative results with respect to F
?
and F
b
3 metrics over the ODP-239 and MORESQUE
datasets obtained by different SRC techniques.
It is important to notice that OPTIMSRC and STC-LINGO can be viewed as a combination of different
SRC SOO solutions but still casted to a SOO solution. These previous results report interesting issues
for SRC and confort the idea that the combination of different objective functions may lead to enhanced
SRC algorithms. But, MOO-clus is capable to find better partitions than OPTIMSRC and STC-LINGO
for all data sets and all evaluation metrics as reported in Table 3.
It is important to notice that the MOO-clus provides a set of partitions with automatic definition of
the number of clusters. So, defining one unique solution is an important issue for SRC. So far, we have
provided results for the best partition evaluated by F
b
3
. However, deeper analysis of all the partitions
on the Pareto front must be endeavoured. Results are reported for F
b
3
only as all other metrics behave
correspondingly and are reported in Table 4.
MORESQUE ODP-239
2 3 4 5 2 3 4 5
Min 0.428 0.464 0.464 0.462 0.396 0.401 0.403 0.408
Max 0.477 0.491 0.497 0.502 0.478 0.481 0.484 0.481
Avg. 0.454 0.479 0.482 0.486 0.443 0.447 0.448 0.449
Table 4: F
b
3 evaluation results of the Pareto front.
Figures show the validity of each individual solution of the Pareto front. In the worst case, MOO-clus
produces similar results compared to the hard baseline STC. On average, it reaches the results of GK-
means and the highest performance values can be found on the Pareto front. The correct identification
of the best partition is still an open issue and can be compared to the automatic selection of K clusters,
which is a hard task as shown in recent studies (Scaiella et al., 2012; Marco and Navigli, 2013).
7 Conclusions
In this paper, we proposed the first attempt
10
to define the SRC task as a multi-objective problem. For that
purpose, we defined two objective functions, which are simultaneously optimized through the archived
multi-objective simulated annealing framework called AMOSA. A correct definition of the task allowed
to take advantage of the most recent advances in terms of endogenous SRC algorithms as well as the most
powerful techniques for multi-objective clustering. The performance of MOO-clus has been evaluated
over two gold standard data sets, ODP-239 andMORESQUE for different evaluation metrics, F
1
and F
b
3 .
10
As far as we know.
106
Results showed that our proposal steadily outperforms all existing state-of-the-art text-based endogenous
SRC algorithms and approaches recent knowledge-driven exogenous strategies (Scaiella et al., 2012),
which reach F
1
=0.413 for ODP-239
11
.
As future works, we propose to use MOO clustering in a strict meta learning way, where any labeled-
based SOO solution is defined by specific Compactness and Separability functions. Another research
direction is the definition of the Dual representation proposed by (Moreno et al., 2014) as a MOO prob-
lem. Finally, new objective functions can be defined to measure the quality of the labels, which may
integrate meaningful multiword expressions or named entities.
Acknowledgement
We would like to thank the CNRS to provide Sriparna Saha with a 6 months internship at the GREYC
Laboratory of the Normandie University.
References
Enrique Amig?o, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Information Retrieval, 12(4):461?486.
Enrique Amig?o, Julio Gonzalo, and Felisa Verdejo. 2013. A general evaluation measure for document organization
tasks. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 643?652.
Sanghamitra Bandyopadhyay, Anirban Mukhopadhyay, and Ujjwal Maulik. 2007. An improved algorithm for
clustering gene expression data. Bioinformatics, 23(21):2859?2865.
Sanghamitra Bandyopadhyay, Sriparna Saha, Ujjwal Maulik, and Kalyanmoy Deb. 2008. A simulated annealing-
based multiobjective optimization algorithm: Amosa. In IEEE transactions on evolutionary computation, pages
269?283.
James C. Bezdek. 1981. Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum, New York.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram.
Claudio Carpineto and Giovanni Romano. 2010. Optimal meta search results clustering. In 33rd International
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 170?177.
Claudio Carpineto, Stanislaw Osinski, Giovanni Romano, and Dawid Weiss. 2009. A survey of web clustering
engines. ACM Computing Surveys, 41(3):1?38.
Hamish Cunningham, Valentin Tablan, Angus Roberts, and Kalina Bontcheva. 2013. Getting more out of
biomedical documents with gate?s full lifecycle open source text analytics. PLoS Computational Biology,
9(2):e1002854.
Joaquim Ferreira da Silva, Ga?el Dias, Sylvie Guillor?e, and Jos?e Gabriel Pereira Lopes. 1999. Using localmaxs
algorithm for the extraction of contiguous and non-contiguous multiword lexical units. In Proceedings of 9th
Portuguese Conference in Artificial Intelligence (EPIA), pages 113?132.
Kalyanmoy Deb. 2009. Multi-Objective Optimization Using Evolutionary Algorithms. Wiley.
Julia Handl and Joshua Knowles. 2007. An evolutionary approach to multiobjective clustering. IEEE Transactions
on Evolutionary Computation, 11:56?76.
Antonio D. Marco and Roberto Navigli. 2013. Clustering and diversifying web search results with graph-based
word sense induction. Computational Linguistics, 39(4):1?43.
Ujjwal Maulik, Sanghamitra Bandyopadhyay, and Anirban Mukhopadhyay. 2011. Multiobjective Genetic Algo-
rithms for Clustering - Applications in Data Mining and Bioinformatics. Springer.
Jos?e G. Moreno and Ga?el Dias. 2014. Easy web search results clustering: When baselines can reach state-of-
the-art algorithms. In Proceedings of the 14th Conference of the European Chapter of the Association for
Computational Linguistics (EACL), pages 1?5.
11
Note that results of (Marco and Navigli, 2013) are not reported in this paper as the authors do not use the standard versions
of MORESQUE and do not provide experiments for ODP-239.
107
Jos?e G. Moreno, Ga?el Dias, and Guillaume Cleuziou. 2013. Post-retrieval clustering using third-order similarity
measures. In 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 153?158.
Jos?e G. Moreno, Ga?el Dias, and Guillaume Cleuziou. 2014. Query log driven web search results clustering. In
Proceedings of the 37th Annual ACM SIGIR Conference (SIGIR).
Katharina Morik, Andreas Kaspari, Michael Wurst, and Marcin Skirzynsk. 2012. Multi-objective frequent termset
clustering. Knowledge Information Systems, 30(3):715?738.
Anirban Mukhopadhyay and Ujjwal Maulik. 2009. Unsupervised pixel classification in satellite imagery using
multiobjective fuzzy clustering combined with SVM classifier. IEEE Transactions on Geoscience and Remote
Sensing, pages 1132?1138.
Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing word senses to improve web search result clustering. In
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
116?126.
Stanislaw Osinski and Dawid Weiss. 2005. A concept-driven algorithm for clustering search results. IEEE
Intelligent Systems, 20(3):48?54.
Sriparna Saha and Sanghamitra Bandyopadhyay. 2010. A symmetry based multiobjective clustering technique for
automatic evolution of clusters. Pattern Recognition, 43(3):738?751.
Ugo Scaiella, Paolo Ferragina, Andrea Marino, and Massimiliano Ciaramita. 2012. Topical clustering of search
results. In 5th ACM International Conference on Web Search and Data Mining (WSDM), pages 223?232.
Xuanli L. Xie and Gerardo Beni. 1991. A validity measure for fuzzy clustering. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 13:841?847.
Oren Zamir and Oren Etzioni. 1998. Web document clustering: A feasibility demonstration. In 21st Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages
46?54.
108
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 314?318,
Dublin, Ireland, August 23-24, 2014.
IITP: A Supervised Approach for Disorder Mention Detection and
Disambiguation
Utpal Kumar Sikdar, Asif Ekbal and Sriparna Saha
Department of Computer Science and Engineering
Indian Institute of Technology Patna, India
{utpal.sikdar,asif,sriparna}@iitp.ac.in
Abstract
In this paper we briefly describe our super-
vised machine learning approach for dis-
order mention detection system that we
submitted as part of our participation in
the SemEval-2014 Shared task. The main
goal of this task is to build a system that
automatically identifies mentions of clini-
cal conditions from the clinical texts. The
main challenge lies due in the fact that the
same mention of concept may be repre-
sented in many surface forms. We develop
the system based on the supervised ma-
chine learning algorithms, namely Condi-
tional Random Field and Support Vector
Machine. One appealing characteristics of
our system is that most of the features for
learning are extracted automatically from
the given training or test datasets with-
out using deep domain specific resources
and/or tools. We submitted three runs, and
best performing system is based on Condi-
tional Random Field. For task A, it shows
the precision, recall and F-measure values
of 50.00%, 47.90% and 48.90%, respec-
tively under the strict matching criterion.
When the matching criterion is relaxed, it
shows the precision, recall and F-measure
of 81.50%, 79.70% and 80.60%, respec-
tively. For task B, we obtain the accuracies
of 33.30% and 69.60% for the relaxed and
strict matches, respectively.
1 Introduction
The SemEval-2014 Shared Task 7 is concerned
with the analysis of clinical texts, particularly for
disorder mention detection and disambiguation.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
The purpose of this task is to enhance current
research in Natural Language Processing (NLP)
methods used in the clinical domain. The task is
a continuation of the CLEF/eHealth ShARe 2013
Shared Task. In particular there were two specific
tasks, viz. (i). Task A: To identify disorder men-
tions from biomedicine domain and (ii) Task B:
To classify each mention with respect to the Uni-
fied Medical Language System (UMLS) Concept
Unique Identifier (CUI). The task is challenging
in the sense that the same mention of concept may
be represented in many surface forms and men-
tion may appear in the different parts of texts.
Some systems (Cogley et al., 2013; Zuccon et al.,
2013; Tang et al., 2013; Cogley et al., 2013) are
available for disorder mention detection. Look-
ing at the challenges and resources available at
our hand we planned to adapt our existing system
(Sikdar et al., 2013) for disorder mention detec-
tion. The original architecture was conceptualized
as part of our participation in the BioCreative-IV
Track-2 Shared Task on Chemical Compound and
Drug Name Recognition. Although our submit-
ted system for SemEval-14 shared task is in line
with BioCreative-IV
1
, it has many different fea-
tures and characteristics.
We develop three systems (e.g. Model-1:
sikdar.run-0, Model-2: sikdar.run-1 and Model-
3: sikdar.run-2) based on the popular supervised
machine learning algorithms, namely Conditional
Random Field (CRF) (Lafferty et al., 2001) and
Support Vector Machine (SVM) (Cortes and Vap-
nik, 1995; Joachims, 1999). The models were de-
veloped by varying the features and feature tem-
plates. A baseline model is constructed by us-
ing the UMLS MetaMap
2
tool. During testing
we merge the development set with the train-
ing set. Evaluation results on test data with the
benchmark set up show the F-measure values of
1
www.biocreative.org/tasks/biocreative-iv/chemdner/
2
http://mmtx.nlm.nih.gov/
314
48.90%, 46.50% and 46.50%, respectively under
the strict criterion. Under relaxed matching cri-
terion the models show the F-measure values of
80.60%, 78.20% and 79.60%, respectively. Our
submission for Task-B is simple in nature where
we consider only those mentions that are also pre-
dicted in the baseline model, i.e. only the com-
mon CUIs are considered. It shows the accuracies
of 33.30%, 31.90% and 33.20%, respectively un-
der strict matching criterion; and 69.60%, 69.60%
and 69.10%, respectively under the relaxed match-
ing criterion.
2 Method
Our method for disorder mention detection from
clinical text is based on the supervised machine
learning algorithms, namely CRF and SVM. The
key focus was to develop a system that could be
easily adapted to other domains and applications.
We submitted three runs defined as below:
Model-1:sikdar.run-0: This is based on CRF,
and makes use of the features as mentioned below.
Model-2:sikdar.run-1: This model is built by
training a SVM classifier with the same set of
features as CRF.
Model-3:sikdar.run-2: This model is constructed
by defining a heuristics that looks at the outputs
of both the models. For given instance, if one of
the models predicts it to belong to the category
of candidate disorder mention then this is given
more priority in assigning the class. We observed
performance improvement on the development set
with this heuristic.
We identify and implement different features,
mostly without using any deep domain knowledge
or domain-specific external resources and/or tools.
The features that are used to train the classifiers are
briefly described below:
? Context words: Surrounding words carry ef-
fective information to identify disorder men-
tion. In our case we consider the previous
three and next three words as the features.
? MetaMapmatch: MetaMap is a widely used
tool that maps biomedical mention to the
UMLS CUI
3
. In UMLS, there are 11 seman-
tic types denoting disorders. These are Con-
genital Abnormality, Acquired Abnormality,
Injury or Poisoning, Pathologic Function,
3
http://www.nlm.nih.gov/research/umls/
Disease or Syndrome, Mental or Behavioral
Dysfunction, Cell or Molecular Dysfunction,
Experimental Model of Disease, Anatomical
Abnormality, Neoplastic Process and Signs
and Symptoms. The training set is passed
through the MetaMap, and then we prepare a
list of mentions that belong to the UMLS se-
mantic types. A feature is thereafter defined
that takes a value of 1 if the current token ap-
pears in the list; otherwise the value becomes
0.
? Part-of-Speech (PoS) Information: In this
work, we use PoS information of the current
token as the feature. PoS information was
extracted from the GENIA tagger
4
V2.0.2,
which is a freely available resource.
? Root words: Stems or root words, which
are extracted form GENIA tagger V2.0.2, are
used as the feature.
? Chunk information: We use GENIA tagger
V2.0.2 to extract the chunk information. It
helps to identify the boundaries of disorder
mentions.
? Initial capital: The feature is set to true if the
first character of the current token is a capital
letter.
? All capital: The feature is set to true if all the
letters of the current token are capitalized.
? Stop words: A feature is defined that is set
to one if the current token appears in the list
of stop words.
? Word normalization: Word shapes refer to
the mapping of each word to their equiva-
lence classes. Each capitalized character of
the word is replaced by ?A?, small characters
are replaced by ?a? and digits are replaced by
?0?.
? Word suffix and prefix: These features in-
dicate the fixed-length character sequences
(here 4) stripped either from the end (suffix)
or beginning positions of words. This is use-
ful in the sense that disorder mentions share
some common sub-strings.
4
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger
315
? Unknown word: This feature is imple-
mented depending upon whether the current
token was found during training or not. For
the training set this has been set randomly.
? Word length: If the length of a token is more
than a predefined threshold (here 5) then it is
most likely a disorder mention. This feature
is defined with the observation that very short
words are most probably not disorder men-
tions.
? Alpha digit: If the current token contains
digit character(s), then the feature is set to
true otherwise false.
? Informative words: This feature is devel-
oped from the training dataset. The words or
the sequence of words that precede and fol-
low the disorder mentions could be useful for
mention detection. The most frequently oc-
curring words that appear within the context
of w
i+2
i?2
= w
i?2
. . . w
i+2
of w
i
are extracted
from the training data. Two different lists are
prepared, one for the informative words that
precede the mentions and the other contains
the informative words that follow the men-
tions. Thereafter we define two features that
fire for the words of these lists.
? Disorder mention prefix and suffix: We ex-
tract most frequently occurring prefixes and
suffixes of length 2 from the disorder men-
tions present in the training data. We pre-
pare two lists containing the prefix and suffix
sub-sequences (of length two) that appear at
least 10 times in the training set. We define
two features that go on/off depending upon
whether the current word contains any sub-
sequence present in the lists.
? Dynamic information: The feature is ex-
tracted from the output label(s) of the previ-
ous token(s). The feature value is determined
at run time.
3 Experimental Results
3.1 Datasets
In SemEval-2014 Shared task 7, three types of
data were provided- training, development and
test. Training data contains four different types
of notes- discharge, ecg, echo and radiology. De-
velopment data consists of notes of three different
domains, viz. discharge, echo and radiology. But
the test set contains only the discharge notes. For
a given document, the start and end indices are
mentioned for the disorder mentions. There are
199, 99 and 133 documents in the training, devel-
opment and test set, respectively.
3.2 Results and Analysis
We use a regular expression based simple pattern
(e.g. dot and space) matching techniques for the
sentence splitting and tokenization. We use C
++
based CRF
++
package
5
for CRF experiments. We
set the default values of the following parame-
ters (a). the hyper-parameter of CRF. With larger
value, CRF tends to overfit to the given training
data; (b). parameter which sets the cut-off thresh-
old for the features (default value is 1). CRF uses
only those features, having more than the cut-off
threshold in the given training data.
In case of SVM we used YamCha
6
toolkit
along with TinySVM
7
. We use the polynomial
kernel function of degree two. In order to denote
the boundaries of a multi-word disorder mention
properly we use the standard BIO encoding
scheme, where B, I and O represent the beginning,
intermediate and outside, respectively, for a
multi-word token. Please note that the mentions
are not continuous, i.e. they could appear at the
various positions of the text. For example, in the
sentence The left atrium is moderately dilated,
there is a single mention left atrium dilated. Its
BIO format is represented in Table 1.
Token Tag
The O
left B-Men
atrium I-Men
is O
moderately O
dilated I-Men
. O
Table 1: An example of BIO representation.
Experiments are conducted on the benchmark
setup as provided by the competition organizer. At
first we train our system using the training set and
evaluate using the development set in order to de-
5
http://crfpp.sourceforge.net
6
http://chasen-org/ taku/software/yamcha/
7
http://chasen.org/ taku/software/TinySVM/
316
System Strict Relaxed
P R F P R F
Baseline 19.9 29.0 23.6 44.9 63.0 52.4
Model-1 52.5 43.0 47.3 86.2 72.6 78.8
Model-2 49.3 41.0 44.8 82.8 70.6 76.2
Model-3 46.7 44.0 45.3 81.2 77.5 79.3
Table 2: Results on development set for Task A.
System Strict Relaxed
Accuracy Accuracy
Baseline 24.6 85.1
Model-1 31.2 72.5
Model-2 29.9 73.0
Model-3 31.8 72.4
Table 3: Results on development set for Task B.
termine the best configuration. We define a base-
line model by passing the development set to the
UMLS MetaMap tool. Its results along with the
baseline model are reported in Table 2 for Task A.
Evaluation shows that our proposed system per-
forms reasonably better compared to the baseline
model. It is also to be noted that Model-1 performs
better compared to the other two submitted mod-
els for the strict matching, but for relaxed evalu-
ation, Model-3 performs better than Model-1 and
Model-2. Under strict matching criterion, Model-
1 achieves 2.7% and 5.0% increments in precision
over the second and third models, respectively.
For relaxed matching, Model-3 achieves 4.9% and
6.9% increments in recall over the first and sec-
ond models, respectively. Results on the develop-
ment set for Task-B are reported in Table 3. Please
note that although our system performs better than
the baseline in terms of strict matching, it does not
show better accuracy under relaxed matching cri-
terion. This is because our system for Task-B is
developed by considering only those mentions that
lie in the intersection of baseline and CRF models.
As a result many mentions are missed. During fi-
nal submissions we merged development sets with
the respective training sets, and perform evalua-
tion on the test sets. We report our results on the
test sets in Table 4 and Table 5 for Task-A and
Task-B, respectively.
We carefully analyze the results and find that
most of the errors encountered because of the dis-
contiguous mentions. Different components of a
mention may be mapped to the different concepts.
In our system we treat two mentions as a single
System Strict Relaxed
P R F P R F
Model-1 50.0 47.9 48.9 81.5 79.7 80.6
Model-2 47.3 45.8 46.5 78.9 77.6 78.2
Model-3 45.0 48.1 46.5 76.9 82.6 79.6
Table 4: Evaluation results on test set for Task A.
System Strict Relaxed
Accuracy Accuracy
Model-1 33.3 69.6
Model-2 31.9 69.6
Model-3 33.2 69.1
Table 5: Results of Task B for the test set.
unit if they have some shared tokens. For exam-
ple, the sentence ?She also notes new sharp pain in
left shoulder blade/back area? contains two differ-
ent mentions, viz.?pain shoulder blade? and ?pain
back?. Here shared word of these two mentions
is ?pain?, but we consider these two mentions as
a single unit such as ?pain shoulder blade back?.
This contributes largely to the errors that our sys-
tem faces for the first task. For the second task,
we miss a number of mentions, and this can be
captured if we directly match the system identified
mentions to the entire UMLS database.
4 Conclusion
In this paper we report on our works as part of our
participation in the SemEval-2014 shared task re-
lated to clinical text mining. We submitted three
runs for both the tasks, viz. disorder mention de-
tection and disambiguation. Our submitted runs
for the first task are based on CRF and SVM. We
make use of a set of features that are not very
domain-specific. The system developed for the
second task is very simple and is based on UMLS
Meta Map tool.
There are many avenues for future research:
identification of more features for the first task;
use of some domain-specific resources and/or
tools for the first task; use of entire UMLS the-
saurus for mapping the disorder mentions; use
of some machine learning techniques for disam-
biguation. We also plan to investigate how sys-
tematic feature selection, ensemble learning and
machine learning optimization have impact on dis-
order mention detection and disambiguation.
317
References
James Cogley, Nicola Stokes, and Joe Carthy. 2013.
Medical Disorder Recognition with Structural Sup-
port Vector Machines. In Proceedings of CLEF.
Corinna Cortes and Vladimir Vapnik. 1995. Support
Vector Networks. Machine Learning, 20:273?297.
Thorsten Joachims, 1999. Making Large Scale SVM
Learning Practical, pages 169?184. MIT Press,
Cambridge, MA, USA.
John Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In ICML, pages 282?289.
Utpal Kumar Sikdar, Asif Ekbal, and Sriparna Saha.
2013. Domain-independent Model for Chemical
Compound and Drug Name Recognition. Proceed-
ings of the Fourth BioCreative Challenge Evaluation
Workshop, vol. 2:158?161.
Buzhou Tang, Yonghui Wu, M. Jiang, J. C. Denny, and
Hua Xu. 2013. Recognizing and Encoding Disorder
Concepts in Clinical Text using Machine Learning
and Vector Space Model. In Proceedings of CLEF.
Guido Zuccon, A. Holloway, B. Koopman, and
A. Nguyen. 2013. Identify Disorders in Health
Records using Conditional Random Fields and
Metamap. In Proceedings of CLEF.
318
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 61?65,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Multi-metric optimization for coreference: The UniTN / IITP / Essex
submission to the 2011 CONLL Shared Task
Olga Uryupina? Sriparna Saha? Asif Ekbal? Massimo Poesio??
?University of Trento
?Indian Institute of Technology Patna
? University of Essex
uryupina@gmail.com, sriparna@iitp.ac.in,
asif@iitp.ac.in, massimo.poesio@unitn.it
Abstract
Because there is no generally accepted met-
ric for measuring the performance of anaphora
resolution systems, a combination of met-
rics was proposed to evaluate submissions to
the 2011 CONLL Shared Task (Pradhan et
al., 2011). We investigate therefore Multi-
objective function Optimization (MOO) tech-
niques based on Genetic Algorithms to opti-
mize models according to multiple metrics si-
multaneously.
1 Introduction
Many evaluation metrics have been proposed for
anaphora resolution (Vilain et al, 1995; Bagga and
Baldwin, 1998; Doddington et al, 2000; Luo, 2005;
Recasens and Hovy, 2011). Each of these metrics
seems to capture some genuine intuition about the
the task, so that, unlike in other areas of HLT, none
has really taken over. This makes it difficult to com-
pare systems, as dramatically demonstrated by the
results of the Coreference Task at SEMEVAL 2010
(Recasens et al, 2010). It was therefore wise of the
CONLL organizers to use a basket of metrics to as-
sess performance instead of a single one.
This situation suggests using methods to opti-
mize systems according to more than one metric
at once. And as it happens, techniques for doing
just that have been developed in the area of Ge-
netic Algorithms?so-called multi-objective opti-
mization techniques (MOO) (Deb, 2001). The key
idea of our submission is to use MOO techniques
to optimize our anaphora resolution system accord-
ing to three metrics simultaneously: the MUC scorer
(a member of what one might call the ?link-based?
cluster of metrics) and the two CEAF metrics (rep-
resentative of the ?entity-based? cluster). In a pre-
vious study (Saha et al, 2011), we show that our
MOO-based approach yields more robust results than
single-objective optimization.
We test two types of optimization: feature se-
lection and architecture?whether to learn a single
model for all types of anaphors, or to learn sepa-
rate models for pronouns and for other nominals.
We also discuss how the default mention extraction
techniques of the system we used for this submis-
sion, BART (Versley et al, 2008), were modified to
handle the all-mention annotation in the OntoNotes
corpus.
In this paper, we first briefly provide some back-
ground on optimization for anaphora resolution, on
genetic algorithms, and on the method for multi-
objective optimization we used, Non-Dominated
Sorting Genetic Algorithm II (Deb et al, 2002). Af-
ter that we discuss our experiments, and present our
results.
2 Background
2.1 Optimization for Anaphora Resolution
There have only been few attempts at optimization
for anaphora resolution, and with a few exceptions,
this was done by hand.
The first systematic attempt at automatic opti-
mization of anaphora resolution we are aware of was
carried out by Hoste (2005), who used genetic algo-
rithms for automatic optimization of both feature se-
lection and of learning parameters, also considering
61
two different machine learners, TimBL and Ripper.
Her results suggest that such techniques yield im-
provements on the MUC-6/7 datasets. Recasens and
Hovy (2009) carried out an investigation of feature
selection for Spanish using the ANCORA corpus.
A form of multi-objective optimization was ap-
plied to coreference by Munson et al (2005). Mun-
son et al (2005) did not propose to train models so
as to simultaneously optimize according to multi-
ple metrics; instead, they used ensemble selection to
learn to choose among previously trained models the
best model for each example. Their general conclu-
sion was negative, stating that ?ensemble selection
seems too unreliable for use in NLP?, but they did
see some improvements for coreference.
2.2 Genetic Algorithms
Genetic algorithms (GAs) (Goldberg, 1989) are ran-
domized search and optimization techniques guided
by the principles of evolution and natural genetics.
In GAs the parameters of the search space are en-
coded in the form of strings called chromosomes. A
collection of such strings is called a population. An
objective or fitness function is associated with each
chromosome that represents the degree of goodness
of that chromosome. A few of the chromosomes are
selected on the basis of the principle of survival of
the fittest, and assigned a number of copies that go
into the mating pool. Biologically inspired opera-
tors like crossover and mutation are applied on these
chromosomes to yield a new generation of strings.
The processes of selection, crossover and mutation
continues for a fixed number of generations or till a
termination condition is satisfied.
2.3 Multi-objective Optimization
Multi-objective optimization (MOO) can be formally
stated as follows (Deb, 2001). Find the vectors
x? = [x?1, x?2, . . . , x?n]T of decision variables that si-
multaneously optimize the M objective values
{f1(x), f2(x), . . . , fM (x)}
while satisfying the constraints, if any.
An important concept in MOO is that of dom-
ination. In the context of a maximization prob-
lem, a solution xi is said to dominate xj if
?k ? 1, 2, . . . ,M, fk(xi) ? fk(xj) and ?k ?
1, 2, . . . ,M, such that fk(xi) > fk(xj).
Genetic algorithms are known to be more effec-
tive for solving MOO than classical methods such as
weighted metrics, goal programming (Deb, 2001),
because of their population-based nature. A particu-
larly popular genetic algorithm of this type is NSGA-
II (Deb et al, 2002), which we used for our runs.
3 Using MOO for Optimization in
Anaphora Resolution
We used multi-objective optimization techniques for
feature selection and for identifying the optimal ar-
chitecture for the CONLL data. In this section we
briefly discuss each aspect of the methodology.
3.1 The BART System
For our experiments, we use BART (Versley et al,
2008), a modular toolkit for anaphora resolution that
supports state-of-the-art statistical approaches to the
task and enables efficient feature engineering. BART
comes with a set of already implemented features,
along with the possibility to design new ones. It
also implements different models of anaphora reso-
lution, allowing the choice between single and split
classifiers that we explore in our runs, as well as
between mention-pair and entity-mention, and be-
tween best-first and ranking. It also has interfaces
to different machine learners (MaxEnt, SVM, de-
cision trees). It is thus ideally suited for experi-
menting with feature selection and other aspects of
optimization. However, considering all the param-
eters, it was unfeasible to run an optimization on
the amount of data available on CONLL; we fo-
cused therefore on feature selection and the choice
between single and split classifiers. We considered
42 features, including 7 classifying mention type, 8
for string matching of different subparts and differ-
ent levels of exactness, 2 for aliasing, 4 for agree-
ment, 12 for syntactic information including also
binding constraints, 3 encoding salience, 1 encod-
ing patterns extracted from the Web, 3 for proximity,
and 2 for 1st and 2nd person pronouns. Again be-
cause of time considerations, we used decision trees
as implemented in Weka as our classification model
instead of maximum-entropy or SVMs. Finally, we
used a simple mention-pair model without ranking
as in (Soon et al, 2001).
62
3.2 Mention detection
BART supports several solutions to the mention
detection (MD) task. The users can input pre-
computed mentions, thus, experimenting with gold
boundaries or system boundaries computed by ex-
ternal modules (e.g., CARAFE). BART also has
a built-in mention extraction module, computing
boundaries heuristically from the output of a parser.
For the CoNLL shared task, we use the BART
internal MD module, as it corresponds better to
the mention detection guidelines of the OntoNotes
dataset. We have further adjusted this module to im-
prove the MD accuracy. The process of mention de-
tection involves two steps.
First, we create a list of candidate mentions by
merging basic NP chunks with named entities. NP
chunks are computed from the parse trees provided
in the CoNLL distribution, Named entities are ex-
tracted with the Stanford NER tool (Finkel et al,
2005). For each candidate mention, we store it mini-
mal and maximal span. The former is used for com-
puting feature values (e.g., for string matching); it
corresponds to either the basic NP chunk or the NE,
depending on the mention type. The latter is used
for alignment with CoNLL mentions; it is computed
by climbing up the parse tree.
This procedure, combined with the perfect (gold)
coreference resolution, gives us an F-score of
91.56% for the mention detection task on the
CoNLL development set1.
At the second step, we aim at discarding men-
tions that are unlikely to participate in corefer-
ence chains. We have identified several groups of
such mentions: erroneous (?[uh]?), (parts of) multi-
word expressions (?for [example]?), web addresses,
emails (?[http://conll.bbn.com]?), time/date expres-
sions (?two times [a year]?), non-referring pronouns
(?[there]?,?[nobody]?), pronouns that are unlikely
to participate in a chain (?[somebody]?, ?[that]?),
time/date expressions that are unlikely to participate
in a chain (?[this time]?), and expletive ?it?.
Our experiments on the development data show
that the first five groups can be reliably identified
and safely discarded from the processing: even with
1Note that, due to the fact that OntoNotes guidelines exclude
singleton mentions, it is impossible to evaluate the MD compo-
nent independently from coreference resolution.
the perfect resolution, we observe virtually no per-
formance loss (the F-score for our MD module with
the gold coreference resolution remains at 91.45%
once we discard mentions from groups 1-5).
The remaining groups are more problematic:
when we eliminate such mentions, we see perfor-
mance drops with the gold resolution. The exact im-
pact of discarding those mentions can only be as-
sessed once we have trained the classifier.
In practice, we have performed our optimization
experiments, selected the best classifier and then
have done additional runs to fine-tune the mention
detection module.
3.3 Using NSGA-II
Chromosome Representation of Feature and Ar-
chitecture Parameters We used chromosomes of
length 43, each binary gene encoding whether or not
to use a particular feature in constructing the classi-
fier, plus one gene set to 1 to use a split classifier, 0
to use a single classifier for all types of anaphors.
Fitness Computation and Mutations For fitness
computation, the following procedure is executed.
1. Suppose there are N number of features
present in a particular chromosome (i.e., there
are total N number of 1?s in that chromosome).
2. Construct the coreference resolution system
(i.e., BART) with only these N features.
3. This coreference system is evaluated on the de-
velopment data. The recall, precision and F-
measure values of three metrics are calculated.
For MOO, the objective functions corresponding to
a particular chromosome are F1 = F-measureMUC
(for the MUC metric), F2 = F-measure?3 (for CEAF
using the ?3 entity alignment function (Luo, 2005))
and F3 = F-measure?4 (for CEAF using the ?3
entity alignment function). The objective is to:
max[F1, F2, F3]: i.e., these three objective func-
tions are simultaneously optimized using the search
capability of NSGA-II.
We use crowded binary tournament selection as
in NSGA-II, followed by conventional crossover and
mutation for the MOO based optimization. The
most characteristic part of NSGA-II is its elitism op-
eration, where the non-dominated solutions (Deb,
63
2001) among the parent and child populations are
propagated to the next generation. The near-Pareto-
optimal strings of the last generation provide the dif-
ferent solutions to the feature selection problem.
Genetic Algorithms Parameters Using the
CONLL development set, we set the following pa-
rameter values for MOO (i.e., NSGA-II): population
size=20, number of generations=20, probability of
mutation=0.1 and probability of crossover=0.9.
3.4 Running the Optimization
Considering the size of the OntoNotes corpus, it
would be very time-consuming to run an optimiza-
tion experiment on the whole dataset. We have
therefore split the data into 3 sub-samples and per-
formed separate MOO experiments on each one.
The MOO approach provides a set of non-
dominated solutions on the final Pareto optimal
front. All the solutions are equally important from
the algorithmic point of view. We have collected sets
of chromosomes for each sub-sample and evaluated
them on the whole train/development set, picking
the solution with the highest FINAL2 score for our
CoNLL submission.
4 Results
4.1 Development set
Table 1 compares the performance level obtained
using all the features with that of loose re-
implementations of the systems proposed by Soon
et al (2001) and Ng and Cardie (2002), commonly
used as baselines. Our reimplementation of the Ng
& Cardie model uses only a subset of features.
The results in Table 1 show that our system with
a rich feature set does not outperform simpler base-
lines (and, in fact, yields poorer results). A similar
trend has been observed by Ng and Cardie (2002),
where the improvement was only possible after man-
ual feature selection.
The last line of Table 1 shows the performance
level of the best chromosome found through the
MOO technique. As it can be seen, it outperforms
all the baselines according to all the measures, lead-
ing to an improvement of 2-5 percentage points in
the FINAL score.
2The FINAL score is an average of FMUC , FB3 and
FCEAF E .
This suggests that automatic feature selection is
essential to improve performance ? i.e., that an effi-
cient coreference resolution system should combine
rich linguistic feature sets with automatic feature se-
lection mechanisms.
4.2 Test set
We have re-trained our best solution on the com-
bined train and development set, running it on the
test data. This system has showed the following per-
formance in the official evaluation (open track): the
FINAL score of 54.32, FMUC = 57.53%, FB3 =
65.18%, FCEAFE = 40.16%.
5 Conclusion
Our results on the development set suggest that a
linguistically-rich system for coreference resolution
might benefit a lot from feature selection. In partic-
ular, we have investigated Non-Dominated Sorting
Genetic Algorithm II (Deb et al, 2002) for multi-
objective optimization.
In subsequent work, we plan to expand the opti-
mization technique to consider also learning param-
eters optimization, classifier selection, and learning
model selection.
Acknowledgments
This work was in part supported by the Provincia
di Trento Grande Progetto LiveMemories, in part by
an Erasmus Mundus scholarship for Asif Ekbal and
Sriparna Saha.
64
Features FMUC FCEAFE FB3 FINAL
following Soon et al (2001) 54.12 41.08 66.67 53.42
-*-, with splitting 53.81 41.03 66.70 53.31
following Ng & Cardie (2002) 52.97 42.40 66.18 53.31
-*-, with splitting 53.28 40.46 66.03 52.72
All features 50.18 38.54 63.79 50.33
-*-, with splitting 50.19 39.47 65.38 51.16
Optimized feature set (splitting) 57.05 42.61 67.46 55.15
Table 1: Performance on the development set
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Proc. of the LREC workshop on
Linguistic Coreference, pages 563?566, Granada.
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and
T. Meyarivan. 2002. A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation, 6(2):181?197.
Kalyanmoy Deb. 2001. Multi-objective Optimization
Using Evolutionary Algorithms. John Wiley and Sons,
Ltd, England.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassell, and R. Weischedel. 2000. The auto-
matic content extraction (ACE) program?tasks, data,
and evaluation. In Proc. of LREC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
363?370.
D. E. Goldberg. 1989. Genetic Algorithms in Search,
Optimization and Machine Learning. Addison-
Wesley, New York.
Veronique Hoste. 2005. Optimization Issues in Ma-
chine Learning of Coreference Resolution. Ph.D. the-
sis, Antwerp University.
X. Luo. 2005. On coreference resolution performance
metrics. In Proc. NAACL / EMNLP, Vancouver.
Art Munson, Claire Cardie, and Rich Caruana. 2005.
Optimizing to arbitrary NLP metrics using ensem-
ble selection. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 539?546.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting on Association
for C omputational Linguistics, pages 104?111.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
M. Recasens and E. Hovy. 2009. A deeper look into fea-
tures for coreference resolution. In S. Lalitha Devi,
A. Branco, and R. Mitkov, editors, Anaphora Pro-
cessing and Applications (DAARC 2009, number 5847
in LNAI, pages 29?42, Berlin / Heidelberg. Springer-
Verlag.
M. Recasens and E. Hovy. 2011. Blanc: Implement-
ing the rand index for coreference evaluation. Natural
Language Engineering.
M. Recasens, L. Ma`rquez, E. Sapena, M. A. Mart??,
M. Taule?, V. Hoste, M. Poesio, and Y. Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proc. SEMEVAL 2010, Uppsala.
Sriparna Saha, Massimo Poesio, Asif Ekbal, and Olga
Uryupina. 2011. Single and multi-objective optimiza-
tion for feature selection in anaphora resolution. Sub-
mitted.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistic, 27(4):521?544.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: a modular toolkit for coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies, pages 9?12.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proc. of the Sixth Message Under-
standing Conference, pages 45?52.
65
