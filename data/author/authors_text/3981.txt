Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 455?462,
New York, June 2006. c?2006 Association for Computational Linguistics
Paraphrasing for Automatic Evaluation
David Kauchak
Department of Computer Science
University of California, San Diego
dkauchak@cs.ucsd.edu
Regina Barzilay
CSAIL
Massachusetts Institute of Technology
regina@csail.mit.edu
Abstract
This paper studies the impact of para-
phrases on the accuracy of automatic eval-
uation. Given a reference sentence and a
machine-generated sentence, we seek to
find a paraphrase of the reference sen-
tence that is closer in wording to the ma-
chine output than the original reference.
We apply our paraphrasing method in the
context of machine translation evaluation.
Our experiments show that the use of
a paraphrased synthetic reference refines
the accuracy of automatic evaluation. We
also found a strong connection between
the quality of automatic paraphrases as
judged by humans and their contribution
to automatic evaluation.
1 Introduction
The use of automatic methods for evaluating
machine-generated text is quickly becoming main-
stream in natural language processing. The most
notable examples in this category include measures
such as BLEU and ROUGE which drive research
in the machine translation and text summarization
communities. These methods assess the quality of
a machine-generated output by considering its simi-
larity to a reference text written by a human. Ideally,
the similarity would reflect the semantic proximity
between the two. In practice, this comparison breaks
down to n-gram overlap between the reference and
the machine output.
1a. However, Israel?s reply failed to completely
clear the U.S. suspicions.
1b. However, Israeli answer unable to fully
remove the doubts.
Table 1: A reference sentence and corresponding
machine translation from the NIST 2004 MT eval-
uation.
Consider the human-written translation and the
machine translation of the same Chinese sentence
shown in Table 1. While the two translations con-
vey the same meaning, they share only auxiliary
words. Clearly, any measure based on word over-
lap will penalize a system for generating such a sen-
tence. The question is whether such cases are com-
mon phenomena or infrequent exceptions. Empiri-
cal evidence supports the former. Analyzing 10,728
reference translation pairs1 used in the NIST 2004
machine translation evaluation, we found that only
21 (less than 0.2%) of them are identical. Moreover,
60% of the pairs differ in at least 11 words. These
statistics suggest that without accounting for para-
phrases, automatic evaluation measures may never
reach the accuracy of human evaluation.
As a solution to this problem, researchers use
multiple references to refine automatic evaluation.
Papineni et al (2002) shows that expanding the
number of references reduces the gap between au-
tomatic and human evaluation. However, very few
human annotated sets are augmented with multiple
references and those that are available are relatively
1Each pair included different translations of the same sen-
tence, produced by two human translators.
455
small in size. Moreover, access to several references
does not guarantee that the references will include
the same words that appear in machine-generated
sentences.
In this paper, we explore the use of paraphras-
ing methods for refinement of automatic evalua-
tion techniques. Given a reference sentence and a
machine-generated sentence, we seek to find a para-
phrase of the reference sentence that is closer in
wording to the machine output than the original ref-
erence. For instance, given the pair of sentences in
Table 1, we automatically transform the reference
sentence (1a.) into
However, Israel?s answer failed to com-
pletely remove the U.S. suspicions.
Thus, among many possible paraphrases of the
reference, we are interested only in those that use
words appearing in the system output. Our para-
phrasing algorithm is based on the substitute in con-
text strategy. First, the algorithm identifies pairs of
words from the reference and the system output that
could potentially form paraphrases. We select these
candidates using existing lexico-semantic resources
such as WordNet. Next, the algorithm tests whether
the candidate paraphrase is admissible in the con-
text of the reference sentence. Since even synonyms
cannot be substituted in any context (Edmonds and
Hirst, 2002), this filtering step is necessary. We pre-
dict whether a word is appropriate in a new context
by analyzing its distributional properties in a large
body of text. Finally, paraphrases that pass the filter-
ing stage are used to rewrite the reference sentence.
We apply our paraphrasing method in the context
of machine translation evaluation. Using this strat-
egy, we generate a new sentence for every pair of
human and machine translated sentences. This syn-
thetic reference then replaces the original human ref-
erence in automatic evaluation.
The key findings of our work are as follows:
(1) Automatically generated paraphrases im-
prove the accuracy of the automatic evaluation
methods. Our experiments show that evaluation
based on paraphrased references gives a better ap-
proximation of human judgments than evaluation
that uses original references.
(2) The quality of automatic paraphrases de-
termines their contribution to automatic evalua-
tion. By analyzing several paraphrasing resources,
we found that the accuracy and coverage of a para-
phrasing method correlate with its utility for auto-
matic MT evaluation.
Our results suggest that researchers may find it
useful to augment standard measures such as BLEU
and ROUGE with paraphrasing information thereby
taking more semantic knowledge into account.
In the following section, we provide an overview
of existing work on automatic paraphrasing. We
then describe our paraphrasing algorithm and ex-
plain how it can be used in an automatic evaluation
setting. Next, we present our experimental frame-
work and data and conclude by presenting and dis-
cussing our results.
2 Related Work
Automatic Paraphrasing and Entailment Our
work is closely related to research in automatic para-
phrasing, in particular, to sentence level paraphras-
ing (Barzilay and Lee, 2003; Pang et al, 2003; Quirk
et al, 2004). Most of these approaches learn para-
phrases from a parallel or comparable monolingual
corpora. Instances of such corpora include multiple
English translations of the same source text writ-
ten in a foreign language, and different news arti-
cles about the same event. For example, Pang et
al. (2003) expand a set of reference translations us-
ing syntactic alignment, and generate new reference
sentences that could be used in automatic evaluation.
Our approach differs from traditional work on au-
tomatic paraphrasing in goal and methodology. Un-
like previous approaches, we are not aiming to pro-
duce any paraphrase of a given sentence since para-
phrases induced from a parallel corpus do not nec-
essarily produce a rewriting that makes a reference
closer to the system output. Thus, we focus on
words that appear in the system output and aim to
determine whether they can be used to rewrite a ref-
erence sentence.
Our work also has interesting connections with
research on automatic textual entailment (Dagan et
al., 2005), where the goal is to determine whether
a given sentence can be inferred from text. While
we are not assessing an inference relation between
a reference and a system output, the two tasks
face similar challenges. Methods for entailment
456
recognition extensively rely on lexico-semantic re-
sources (Haghighi et al, 2005; Harabagiu et al,
2001), and we believe that our method for contex-
tual substitution can be beneficial in that context.
Automatic Evaluation Measures A variety of au-
tomatic evaluation methods have been recently pro-
posed in the machine translation community (NIST,
2002; Melamed et al, 2003; Papineni et al, 2002).
All these metrics compute n-gram overlap between
a reference and a system output, but measure the
overlap in different ways. Our method for reference
paraphrasing can be combined with any of these
metrics. In this paper, we report experiments with
BLEU due to its wide use in the machine translation
community.
Recently, researchers have explored additional
knowledge sources that could enhance automatic
evaluation. Examples of such knowledge sources in-
clude stemming and TF-IDF weighting (Babych and
Hartley, 2004; Banerjee and Lavie, 2005). Our work
complements these approaches: we focus on the im-
pact of paraphrases, and study their contribution to
the accuracy of automatic evaluation.
3 Methods
The input to our method consists of a reference sen-
tence R = r1 . . . rm and a system-generated sen-
tence W = w1 . . . wp whose words form the sets R
and W respectively. The output of the model is a
synthetic reference sentence SRW that preserves the
meaning of R and has maximal word overlap with
W . We generate such a sentence by substituting
words from R with contextually equivalent words
from W .
Our algorithm first selects pairs of candidate word
paraphrases, and then checks the likelihood of their
substitution in the context of the reference sentence.
Candidate Selection We assume that words from
the reference sentence that already occur in the sys-
tem generated sentence should not be considered
for substitution. Therefore, we focus on unmatched
pairs of the form {(r, w)|r ? R?W, w ? W?R}.
From this pool, we select candidate pairs whose
members exhibit high semantic proximity. In our
experiments we compute semantic similarity us-
ing WordNet, a large-scale lexico-semantic resource
employed in many NLP applications for similar pur-
2a. It is hard to believe that such tremendous
changes have taken place for those people and
lands that I have never stopped missing while
living abroad.
2b. For someone born here but has been
sentimentally attached to a foreign country
far from home, it is difficult to believe
this kind of changes.
Table 2: A reference sentence and a corresponding
machine translation. Candidate paraphrases are in
bold.
poses. We consider a pair as a substitution candidate
if its members are synonyms in WordNet.
Applying this step to the two sentences in Table 2,
we obtain two candidate pairs (home, place) and
(difficult, hard).
Contextual Substitution The next step is to de-
termine for each candidate pair (ri, wj) whether
wj is a valid substitution for ri in the context of
r1 . . . ri?12ri+1 . . . rm. This filtering step is essen-
tial because synonyms are not universally substi-
tutable2 . Consider the candidate pair (home, place)
from our example (see Table 2). Words home and
place are paraphrases in the sense of ?habitat?, but
in the reference sentence ?place? occurs in a differ-
ent sense, being part of the collocation ?take place?.
In this case, the pair (home, place) cannot be used
to rewrite the reference sentence.
We formulate contextual substitution as a
binary classification task: given a context
r1 . . . ri?12ri+1 . . . rm, we aim to predict whether
wj can occur in this context at position i. For
each candidate word wj we train a classifier that
models contextual preferences of wj . To train such
a classifier, we collect a large corpus of sentences
that contain the word wj and an equal number of
randomly extracted sentences that do not contain
this word. The former category forms positive
instances, while the latter represents the negative.
For the negative examples, a random position in
a sentence is selected for extracting the context.
This corpus is acquired automatically, and does not
require any manual annotations.
2This can explain why previous attempts to use WordNet for
generating sentence-level paraphrases (Barzilay and Lee, 2003;
Quirk et al, 2004) were unsuccessful.
457
We represent context by n-grams and local col-
locations, features typically used in supervised
word sense disambiguation. Both n-grams and
collocations exclude the word wj . An n-gram
is a sequence of n adjacent words appearing in
r1 . . . ri?12ri+1 . . . rm. A local collocation also
takes into account the position of an n-gram with
respect to the target word. To compute local colloca-
tions for a word at position i, we extract all n-grams
(n = 1 . . . 4) beginning at position i ? 2 and ending
at position i + 2. To make these position dependent,
we prepend each of them with the length and starting
position.
Once the classifier3 for wj is trained, we ap-
ply it to the context r1 . . . ri?12ri+1 . . . rm. For
positive predictions, we rewrite the string as
r1 . . . ri?1wjri+1 . . . rm. In this formulation, all
substitutions are tested independently.
For the example from Table 2, only the pair
(difficult, hard) passes this filter, and thus the sys-
tem produces the following synthetic reference:
For someone born here but has been senti-
mentally attached to a foreign country far
from home, it is hard to believe this kind
of changes.
The synthetic reference keeps the meaning of the
original reference, but has a higher word overlap
with the system output.
One of the implications of this design is the need
to develop a large number of classifiers to test con-
textual substitutions. For each word to be inserted
into a reference sentence, we need to train a sepa-
rate classifier. In practice, this requirement is not a
significant burden. The training is done off-line and
only once, and testing for contextual substitution is
instantaneous. Moreover, the first filtering step ef-
fectively reduces the number of potential candidates.
For example, to apply this approach to the 71,520
sentence pairs from the MT evaluation set (described
in Section 4.1.2), we had to train 2,380 classifiers.
We also discovered that the key to the success of
this approach is the size of the corpus used for train-
ing contextual classifiers. We derived training cor-
pora from the English Gigaword corpus, and the av-
erage size of a corpus for one classifier is 255,000
3In our experiments, we used the publicly available BoosT-
exter classifier (Schapire and Singer, 2000) for this task.
sentences. We do not attempt to substitute any words
that have less that 10,000 appearances in the Giga-
word corpus.
4 Experiments
Our primary goal is to investigate the impact of
machine-generated paraphrases on the accuracy of
automatic evaluation. We focus on automatic evalu-
ation of machine translation due to the availability of
human annotated data in that domain. The hypoth-
esis is that by using a synthetic reference transla-
tion, automatic measures approximate better human
evaluation. In section 4.2, we test this hypothesis
by comparing the performance of BLEU scores with
and without synthetic references.
Our secondary goal is to study the relationship
between the quality of paraphrases and their con-
tribution to the performance of automatic machine
translation evaluation. In section 4.3, we present a
manual evaluation of several paraphrasing methods
and show a close connection between intrinsic and
extrinsic assessments of these methods.
4.1 Experimental Set-Up
We begin by describing relevant background infor-
mation, including the BLEU evaluation method, the
test data set, and the alternative paraphrasing meth-
ods considered in our experiments.
4.1.1 BLEU
BLEU is the basic evaluation measure that we use
in our experiments. It is the geometric average of
the n-gram precisions of candidate sentences with
respect to the corresponding reference sentences,
times a brevity penalty. The BLEU score is com-
puted as follows:
BLEU = BP ? 4
?
?
?
?
4
?
n=1
pn
BP = min(1, e1?r/c),
where pn is the n-gram precision, c is the cardinality
of the set of candidate sentences and r is the size of
the smallest set of reference sentences.
To augment BLEU evaluation with paraphrasing
information, we substitute each reference with the
corresponding synthetic reference.
458
4.1.2 Data
We use the Chinese portion of the 2004 NIST
MT dataset. This portion contains 200 Chinese doc-
uments, subdivided into a total of 1788 segments.
Each segment is translated by ten machine transla-
tion systems and by four human translators. A quar-
ter of the machine-translated segments are scored by
human evaluators on a one-to-five scale along two
dimensions: adequacy and fluency. We use only ad-
equacy scores, which measure how well content is
preserved in the translation.
4.1.3 Alternative Paraphrasing Techniques
To investigate the effect of paraphrase quality on
automatic evaluation, we consider two alternative
paraphrasing resources: Latent Semantic Analysis
(LSA), and Brown clustering (Brown et al, 1992).
These techniques are widely used in NLP applica-
tions, including language modeling, information ex-
traction, and dialogue processing (Haghighi et al,
2005; Serafin and Eugenio, 2004; Miller et al,
2004). Both techniques are based on distributional
similarity. The Brown clustering is computed by
considering mutual information between adjacent
words. LSA is a dimensionality reduction technique
that projects a word co-occurrence matrix to lower
dimensions. This lower dimensional representation
is then used with standard similarity measures to
cluster the data. Two words are considered to be a
paraphrase pair if they appear in the same cluster.
We construct 1000 clusters employing the Brown
method on 112 million words from the North Amer-
ican New York Times corpus. We keep the top 20
most frequent words for each cluster as paraphrases.
To generate LSA paraphrases, we used the Infomap
software4 on a 34 million word collection of arti-
cles from the American News Text corpus. We used
the default parameter settings: a 20,000 word vocab-
ulary, the 1000 most frequent words (minus a stop-
list) for features, a 15 word context window on either
side of a word, a 100 feature reduced representation,
and the 20 most similar words as paraphrases.
While we experimented with several parameter
settings for LSA and Brown methods, we do not
claim that the selected settings are necessarily opti-
mal. However, these methods present sensible com-
4http://infomap-nlp.sourceforge.net
Method 1 reference 2 references
BLEU 0.9657 0.9743
WordNet 0.9674 0.9763
ContextWN 0.9677 0.9764
LSA 0.9652 0.9736
Brown 0.9662 0.9744
Table 4: Pearson adequacy correlation scores for
rewriting using one and two references, averaged
over ten runs.
Method vs. BLEU vs. ContextWN
WordNet // 44
ContextWN // -
LSA X 44
Brown // 4
Table 5: Paired t-test significance for all methods
compared to BLEU as well as our method for one
reference. Two triangles indicates significant at the
99% confidence level, one triangle at the 95% con-
fidence level and X not significant. Triangles point
towards the better method.
parison points for understanding the relationship be-
tween paraphrase quality and its impact on auto-
matic evaluation.
Table 3 shows synthetic references produced by
the different paraphrasing methods.
4.2 Impact of Paraphrases on Machine
Translation Evaluation
The standard way to analyze the performance of an
evaluation metric in machine translation is to com-
pute the Pearson correlation between the automatic
metric and human scores (Papineni et al, 2002;
Koehn, 2004; Lin and Och, 2004; Stent et al, 2005).
Pearson correlation estimates how linearly depen-
dent two sets of values are. The Pearson correlation
values range from 1, when the scores are perfectly
linearly correlated, to -1, in the case of inversely cor-
related scores.
To calculate the Pearson correlation, we create
a document by concatenating 300 segments. This
strategy is commonly used in MT evaluation, be-
cause of BLEU?s well-known problems with docu-
ments of small size (Papineni et al, 2002; Koehn,
2004). For each of the ten MT system translations,
459
Reference: The monthly magazine ?Choices? has won the deep trust of the residents. The current
Internet edition of ?Choices? will give full play to its functions and will help
consumers get quick access to market information.
System: The public has a lot of faith in the ?Choice? monthly magazine and the Council is now
working on a web version. This will enhance the magazine?s function and help consumer
to acquire more up-to-date market information.
WordNet The monthly magazine ?Choices? has won the deep faith of the residents. The current
Internet version of ?Choices? will give full play to its functions and will help
consumers acquire quick access to market information.
ContextWN The monthly magazine ?Choices? has won the deep trust of the residents. The current
Internet version of ?Choices? will give full play to its functions and will help
consumers acquire quick access to market information.
LSA The monthly magazine ?Choice? has won the deep trust of the residents. The current
web edition of ?Choice? will give full play to its functions and will help
consumer get quick access to market information.
Brown The monthly magazine ?Choices? has won the deep trust of the residents. The current
Internet version of ?Choices? will give full play to its functions and will help
consumers get quick access to market information.
Table 3: Sample of paraphrasings produced by each method based on the corresponding system translation.
Paraphrased words are in bold and filtered words underlined.
the evaluation metric score is calculated on the docu-
ment and the corresponding human adequacy score
is calculated as the average human score over the
segments. The Pearson correlation is calculated over
these ten pairs (Papineni et al, 2002; Stent et al,
2005). This process is repeated for ten different
documents created by the same process. Finally, a
paired t-test is calculated over these ten different cor-
relation scores to compute statistical significance.
Table 4 shows Pearson correlation scores for
BLEU and the four paraphrased augmentations,
averaged over ten runs.5 In all ten tests, our
method based on contextual rewriting (ContextWN)
improves the correlation with human scores over
BLEU. Moreover, in nine out of ten tests Contex-
tWN outperforms the method based on WordNet.
The results of statistical significance testing are sum-
marized in Table 5. All the paraphrasing methods
except LSA, exhibit higher correlation with human
scores than plain BLEU. Our method significantly
outperforms BLEU, and all the other paraphrase-
based metrics. This consistent improvement con-
firms the importance of contextual filtering.
5Depending on the experimental setup, correlation values
can vary widely. Our scores fall within the range of previous
researchers (Papineni et al, 2002; Lin and Och, 2004).
The third column in Table 4 shows that auto-
matic paraphrasing continues to improve correlation
scores even when two human references are para-
phrased using our method.
4.3 Evaluation of Paraphrase Quality
In the last section, we saw significant variations
in MT evaluation performance when different para-
phrasing methods were used to generate a synthetic
reference. In this section, we examine the correla-
tion between the quality of automatically generated
paraphrases and their contribution to automatic eval-
uation. We analyze how the substitution frequency
and the accuracy of those substitutions contributes
to a method?s performance.
We compute the substitution frequency of an au-
tomatic paraphrasing method by counting the num-
ber of words it rewrites in a set of reference sen-
tences. Table 6 shows the substitution frequency and
the corresponding BLEU score. The substitution
frequency varies greatly across different methods ?
LSA is by far the most prolific rewriter, while Brown
produces very few substitutions. As expected, the
more paraphrases identified, the higher the BLEU
score for the method. However, this increase does
460
Method Score Substitutions
BLEU 0.0913 -
WordNet 0.0969 994
ContextWN 0.0962 742
LSA 0.992 2080
Brown 0.921 117
Table 6: Scores and the number of substitutions
made for all 1788 segments, averaged over the dif-
ferent MT system translations
Method Judge 1 Judge 2 Kappa
accuracy accuracy
WordNet 63.5% 62.5% 0.74
ContextWN 75% 76.0% 0.69
LSA 30% 31.5% 0.73
Brown 56% 56% 0.72
Table 7: Accuracy scores by two human judges as
well as the Kappa coefficient of agreement.
not translate into better evaluation performance. For
instance, our contextual filtering method removes
approximately a quarter of the paraphrases sug-
gested by WordNet and yields a better evaluation
measure. These results suggest that the substitu-
tion frequency cannot predict the utility value of the
paraphrasing method.
Accuracy measures the correctness of the pro-
posed substitutions in the context of a reference sen-
tence. To evaluate the accuracy of different para-
phrasing methods, we randomly extracted 200 para-
phrasing examples from each method. A paraphrase
example consists of a reference sentence, a refer-
ence word to be paraphrased and a proposed para-
phrase of that reference (that actually occurred in a
corresponding system translation). The judge was
instructed to mark a substitution as correct only if
the substitution was both semantically and grammat-
ically correct in the context of the original reference
sentence.
Paraphrases produced by the four methods were
judged by two native English speakers. The pairs
were presented in random order, and the judges were
not told which system produced a given pair. We
employ a commonly used measure, Kappa, to as-
sess agreement between the judges. We found that
negative positive
filtered 40 27
non-filtered 33 100
Table 8: Confusion matrix for the context filtering
method on a random sample of 200 examples la-
beled by the first judge.
on all the four sets the Kappa value was around 0.7,
which corresponds to substantial agreement (Landis
and Koch, 1977).
As Table 7 shows, the ranking between the ac-
curacy of the different paraphrasing methods mir-
rors the ranking of the corresponding MT evalua-
tion methods shown in Table 4. The paraphrasing
method with the highest accuracy, ContextWN, con-
tributes most significantly to the evaluation perfor-
mance of BLEU. Interestingly, even methods with
moderate accuracy, i.e. 63% for WordNet, have a
positive influence on the BLEU metric. At the same
time, poor paraphrasing accuracy, such as LSA with
30%, does hurt the performance of automatic evalu-
ation.
To further understand the contribution of contex-
tual filtering, we compare the substitutions made by
WordNet and ContextWN on the same set of sen-
tences. Among the 200 paraphrases proposed by
WordNet, 73 (36.5%) were identified as incorrect by
human judges. As the confusion matrix in Table 8
shows, 40 (54.5%) were eliminated during the filter-
ing step. At the same time, the filtering erroneously
eliminates 27 positive examples (21%). Even at this
level of false negatives, the filtering has an overall
positive effect.
5 Conclusion and Future Work
This paper presents a comprehensive study of the
impact of paraphrases on the accuracy of automatic
evaluation. We found a strong connection between
the quality of automatic paraphrases as judged by
humans and their contribution to automatic evalua-
tion. These results have two important implications:
(1) refining standard measures such as BLEU with
paraphrase information moves the automatic evalu-
ation closer to human evaluation and (2) applying
paraphrases to MT evaluation provides a task-based
assessment for paraphrasing accuracy.
461
We also introduce a novel paraphrasing method
based on contextual substitution. By posing the
paraphrasing problem as a discriminative task, we
can incorporate a wide range of features that im-
prove the paraphrasing accuracy. Our experiments
show improvement of the accuracy of WordNet
paraphrasing and we believe that this method can
similarly benefit other approaches that use lexico-
semantic resources to obtain paraphrases.
Our ultimate goal is to develop a contextual filter-
ing method that does not require candidate selection
based on a lexico-semantic resource. One source of
possible improvement lies in exploring more power-
ful learning frameworks and more sophisticated lin-
guistic representations. Incorporating syntactic de-
pendencies and class-based features into the context
representation could also increase the accuracy and
the coverage of the method. Our current method
only implements rewriting at the word level. In the
future, we would like to incorporate substitutions at
the level of phrases and syntactic trees.
Acknowledgments
The authors acknowledge the support of the Na-
tional Science Foundation (Barzilay; CAREER
grant IIS-0448168) and DARPA (Kauchak; grant
HR0011-06-C-0023). Thanks to Michael Collins,
Charles Elkan, Yoong Keok Lee, Philip Koehn, Igor
Malioutov, Ben Snyder and the anonymous review-
ers for helpful comments and suggestions. Any
opinions, findings and conclusions expressed in this
material are those of the author(s) and do not neces-
sarily reflect the views of DARPA or NSF.
References
B. Babych, A. Hartley. 2004. Extending the BLEU
evaluation method with frequency weightings. In Pro-
ceedings of the ACL, 621?628.
S. Banerjee, A. Lavie. 2005. METEOR: An automatic
metric for MT evaluation with improved correlation
with human judgments. In Proceedings of the ACL
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and/or Summarization, 65?72.
R. Barzilay, L. Lee. 2003. Learning to paraphrase: An
unsupervised approach using multiple-sequence align-
ment. In Proceedings of NAACL-HLT, 16?23.
P. F. Brown, P. V. deSouza, R. L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
I. Dagan, O. Glickman, B. Magnini, eds. 2005. The PAS-
CAL recognizing textual entailment challenge, 2005.
P. Edmonds, G. Hirst. 2002. Near synonymy and lexical
choice. Computational Linguistics, 28(2):105?144.
A. Haghighi, A. Ng, C. Manning. 2005. Robust tex-
tual inference via graph matching. In Proceedings of
NAACL-HLT, 387?394.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihal-
cea, M. Surdeanu, R. Bunescu, R. Girju, V. Rus,
P. Morarescu. 2001. The role of lexico-semantic feed-
back in open-domain textual question-answering. In
Proceedings of ACL, 274?291.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of EMNLP,
388?395.
J. R. Landis, G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159?174.
C. Lin, F. Och. 2004. ORANGE: a method for evaluating
automatic evaluation metrics for machine translation.
In Proceedings of COLING, 501?507.
I. D. Melamed, R. Green, J. P. Turian. 2003. Precision
and recall of machine translation. In Proceedings of
NAACL-HLT, 61?63.
S. Miller, J. Guinness, A. Zamanian. 2004. Name tag-
ging with word clusters and discriminative training. In
Proceedings of HLT-NAACL, 337?342.
NIST. 2002. Automatic evaluation of machine trans-
lation quality using n-gram co-occurrence statistics,
2002.
B. Pang, K. Knight, D. Marcu. 2003. Syntax-based
alignment of multiple translations: Extracting para-
phrases and generating new sentences. In Proceedings
of NAACL-HLT, 102?209.
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2002. BLEU:
a method for automatic evaluation of machine transla-
tion. In Proceedings of the ACL, 311?318.
C. Quirk, C. Brockett, W. Dolan. 2004. Monolingual
machine translation for paraphrase generation. In Pro-
ceedings of EMNLP, 142?149.
R. E. Schapire, Y. Singer. 2000. Boostexter: A boosting-
based system for text categorization. Machine Learn-
ing, 39(2/3):135?168.
R. Serafin, B. D. Eugenio. 2004. FLSA: Extending la-
tent semantic analysis with features for dialogue act
classification. In Proceedings of the ACL, 692?699.
A. Stent, M. Marge, M. Singhai. 2005. Evaluating eval-
uation methods for generation in the presense of vari-
ation. In Proceedings of CICLING, 341?351.
462
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 32?39,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Feature-Based Segmentation of Narrative Documents
David Kauchak
Palo Alto Research Center and
University of California, San Diego
San Diego, CA 92093
dkauchak@cs.ucsd.edu
Francine Chen
Palo Alto Research Center
3333 Coyote Hill Rd.
Palo Alto, CA 94304
fchen@parc.com
Abstract
In this paper we examine topic segmen-
tation of narrative documents, which are
characterized by long passages of text
with few headings. We first present results
suggesting that previous topic segmenta-
tion approaches are not appropriate for
narrative text. We then present a feature-
based method that combines features from
diverse sources as well as learned features.
Applied to narrative books and encyclope-
dia articles, our method shows results that
are significantly better than previous seg-
mentation approaches. An analysis of in-
dividual features is also provided and the
benefit of generalization using outside re-
sources is shown.
1 Introduction
Many long text documents, such as magazine arti-
cles, narrative books and news articles contain few
section headings. The number of books in narrative
style that are available in digital form is rapidly in-
creasing through projects such as Project Gutenberg
and the Million Book Project at Carnegie Mellon
University. Access to these collections is becom-
ing easier with directories such as the Online Books
Page at the University of Pennsylvania.
As text analysis and retrieval moves from retrieval
of documents to retrieval of document passages, the
ability to segment documents into smaller, coherent
regions enables more precise retrieval of meaningful
portions of text (Hearst, 1994) and improved ques-
tion answering. Segmentation also has applications
in other areas of information access, including docu-
ment navigation (Choi, 2000), anaphora and ellipsis
resolution, and text summarization (Kozima, 1993).
Research projects on text segmentation have fo-
cused on broadcast news stories (Beeferman et al,
1999), expository texts (Hearst, 1994) and synthetic
texts (Li and Yamanishi, 2000; Brants et al, 2002).
Broadcast news stories contain cues that are indica-
tive of a new story, such as ?coming up?, or phrases
that introduce a reporter, which are not applicable to
written text. In expository texts and synthetic texts,
there is repetition of terms within a topical segment,
so that the similarity of ?blocks? of text is a useful
indicator of topic change. Synthetic texts are created
by concatenating stories, and exhibit stronger topic
changes than the subtopic changes within a docu-
ment; consequently, algorithms based on the simi-
larity of text blocks work well on these texts.
In contrast to these earlier works, we present a
method for segmenting narrative documents. In this
domain there is little repetition of words and the seg-
mentation cues are weaker than in broadcast news
stories, resulting in poor performance from previous
methods.
We present a feature-based approach, where the
features are more strongly engineered using linguis-
tic knowledge than in earlier approaches. The key to
most feature-based approaches, particularly in NLP
tasks where there is a broad range of possible feature
sources, is identifying appropriate features. Select-
ing features in this domain presents a number of in-
teresting challenges. First, features used in previous
methods are not sufficient for solving this problem.
We explore a number of different sources of infor-
mation for extracting features, many previously un-
used. Second, the sparse nature of text and the high
32
cost of obtaining training data requires generaliza-
tion using outside resources. Finally, we incorporate
features from non-traditional resources such as lexi-
cal chains where features must be extracted from the
underlying knowledge representation.
2 Previous Approaches
Previous topic segmentation methods fall into three
groups: similarity based, lexical chain based, and
feature based. In this section we give a brief
overview of each of these groups.
2.1 Similarity-based
One popular method is to generate similarities be-
tween blocks of text (such as blocks of words,
sentences or paragraphs) and then identify section
boundaries where dips in the similarities occur.
The cosine similarity measure between term vec-
tors is used by Hearst (1994) to define the simi-
larity between blocks. She notes that the largest
dips in similarity correspond to defined boundaries.
Brants et al (2002) learn a PLSA model using EM
to smooth the term vectors. The model is parame-
terized by introducing a latent variable, representing
the possible ?topics?. They show good performance
on a number of different synthetic data sets.
Kozima and Furugori (1994) use another similar-
ity metric they call ?lexical cohesion?. The ?cohe-
siveness? of a pair of words is calculated by spread-
ing activation on a semantic network as well as word
frequency. They showed that dips in lexical cohe-
sion plots had some correlation with human subject
boundary decisions on one short story.
2.2 Lexical Chains
Semantic networks define relationships be-
tween words such as synonymy, specializa-
tion/generalization and part/whole. Stokes et al
(2002) use these relationships to construct lexical
chains. A lexical chain is a sequence of lexicograph-
ically related word occurrences where every word
occurs within a set distance from the previous word.
A boundary is identified where a large numbers of
lexical chains begin and end. They showed that
lexical chains were useful for determining the text
structure on a set of magazine articles, though they
did not provide empirical results.
2.3 Feature-based
Beeferman et al (1999) use an exponential model
and generate features using a maximum entropy se-
lection criterion. Most features learned are cue-
based features that identify a boundary based on the
occurrence of words or phrases. They also include a
feature that measures the difference in performance
of a ?long range? vs. ?short range? model. When
the short range model outperforms the long range
model, this indicates a boundary. Their method per-
formed well on a number of broadcast news data
sets, including the CNN data set from TDT 1997.
Reynar (1999) describes a maximum entropy
model that combines hand selected features, includ-
ing: broadcast news domain cues, number of content
word bigrams, number of named entities, number of
content words that are WordNet synonyms in the left
and right regions, percentage of content words in the
right segment that are first uses, whether pronouns
occur in the first five words, and whether a word
frequency based algorithm predicts a boundary. He
found that for the HUB-4 corpus, which is composed
of transcribed broadcasts, that the combined feature
model performed better than TextTiling.
Mochizuki et al (1998) use a combination of lin-
guistic cues to segment Japanese text. Although a
number of cues do not apply to English (e.g., top-
ical markers), they also use anaphoric expressions
and lexical chains as cues. Their study was small,
but did indicate that lexical chains are a useful cue
in some domains.
These studies indicate that a combination of fea-
tures can be useful for segmentation. However,
Mochizuki et al (1998) analyzed Japanese texts, and
Reynar (1999) and Beeferman et al (1999) evalu-
ated on broadcast news stories, which have many
cues that narrative texts do not. Beeferman et al
(1999) also evaluated on concatenated Wall Street
Journal articles, which have stronger topic changes
than within a document. In our work, we examine
the use of linguistic features for segmentation of nar-
rative text in English.
3 Properties of Narrative Text
Characterizing data set properties is the first step
towards deriving useful features. The approaches
in the previous section performed well on broad-
33
Table 1: Previous approaches evaluated on narrative
data from Biohazard
Word Sent. Window
Model Error Error Diff
random 0.486 0.490 0.541
TextTiling 0.481 0.497 0.526
PLSA 0.480 0.521 0.559
cast news, expository and synthetic data sets. Many
properties of these documents are not shared by nar-
rative documents. These properties include: 1) cue
phrases, such as ?welcome back? and ?joining us?
that feature-based methods used in broadcast news,
2) strong topic shifts, as in synthetic documents cre-
ated by concatenating newswire articles, and 3) large
data sets such that the training data and testing data
appeared to come from similar distributions.
In this paper we examine two narrative-style
books: Biohazard by Ken Alibek and The Demon
in the Freezer by Richard Preston. These books are
segmented by the author into sections. We manu-
ally examined these author identified boundaries and
they are reasonable. We take these sections as true
locations of segment boundaries. We split Biohaz-
ard into three parts, two for experimentation (exp1
and exp2) and the third as a holdout for testing. De-
mon in the Freezer was reserved for testing. Biohaz-
ard contains 213 true and 5858 possible boundaries.
Demon has 119 true and 4466 possible boundaries.
Locations between sentences are considered possi-
ble boundaries and were determined automatically.
We present an analysis of properties of the book
Biohazard by Ken Alibek as an exemplar of nar-
rative documents (for this section, test=exp1 and
train=exp2). These properties are different from pre-
vious expository data sets and will result in poor per-
formance for the algorithms mentioned in Section 2.
These properties help guide us in deriving features
that may be useful for segmenting narrative text.
Vocabulary The book contains a single topic with a
number of sub-topics. These changing topics, com-
bined with the varied use of words for narrative doc-
uments, results in many unseen terms in the test set.
25% of the content words in the test set do not oc-
cur in the training set and a third of the words in the
test set occur two times or less in the training set.
This causes problems for those methods that learn
a model of the training data such as Brants et al
(2002) and Beeferman et al (1999) because, with-
out outside resources, the information in the training
data is not sufficient to generalize to the test set.
Boundary words Many feature-based methods rely
on cues at the boundaries (Beeferman et al, 1999;
Reynar, 1999). 474 content terms occur in the first
sentence of boundaries in the training set. Of these
terms, 103 occur at the boundaries of the test set.
However, of those terms that occur signicantly at
a training set boundary (where significant is de-
termined by a likelihood-ratio test with a signifi-
cance level of 0.1), only 9 occur at test boundaries.
No words occur significantly at a training boundary
AND also significantly at a test boundary.
Segment similarity Table 1 shows that two
similarity-based methods that perform well on syn-
thetic and expository text perform poorly (i.e., on
par with random) on Biohazard. The poor perfor-
mance occurs because block similarities provide lit-
tle information about the actual segment boundaries
on this data set. We examined the average similarity
for two adjacent regions within a segment versus the
average similarity for two adjacent regions that cross
a segment boundary. If the similarity scores were
useful, the within segment scores would be higher
than across segment scores. Similarities were gener-
ated using the PLSA model, averaging over multiple
models with between 8 and 20 latent classes. The
average similarity score within a segment was 0.903
with a standard deviation of 0.074 and the average
score across a segment boundary was 0.914 with a
standard deviation of 0.041. In this case, the across
boundary similarity is actually higher. Similar val-
ues were observed for the cosine similarities used by
the TextTiling algorithm, as well as with other num-
bers of latent topics for the PLSA model. For all
cases examined, there was little difference between
inter-segment similarity and across-boundary simi-
larity, and there was always a large standard devia-
tion.
Lexical chains Lexical chains were identified as
synonyms (and exact matches) occurring within
a distance of one-twentieth the average segment
length and with a maximum chain length equal to
the average segment length (other values were ex-
34
amined with similar results). Stokes et al (2002)
suggest that high concentrations of lexical chain be-
ginnings and endings are indicative of a boundary
location. On the narrative data, of the 219 over-
all chains, only 2 begin at a boundary and only 1
ends at a boundary. A more general heuristic iden-
tifies boundaries where there is an increase in the
number of chains beginning and ending near a possi-
ble boundary while also minimizing chains that span
boundaries. Even this heuristic does not appear in-
dicative on this data set. Over 20% of the chains
actually cross segment boundaries. We also mea-
sured the average distance from a boundary and the
nearest beginning and ending of a chain if a chain
begins/ends within that segment. If the chains are a
good feature, then these should be relatively small.
The average segment length is 185 words, but the
average distance to the closest beginning chain is 39
words away and closest ending chain is 36 words
away. Given an average of 4 chains per segment,
the beginning and ending of chains were not concen-
trated near boundary locations in our narrative data,
and therefore not indicative of boundaries.
4 Feature-Based Segmentation
We pose the problem of segmentation as a classifi-
cation problem. Sentences are automatically iden-
tified and each boundary between sentences is a
possible segmentation point. In the classification
framework, each segmentation point becomes an ex-
ample. We examine both support vector machines
(SVMlight (Joachims, 1999)) and boosted decision
stumps (Weka (Witten and Frank, 2000)) for our
learning algorithm. SVMs have shown good per-
formance on a variety of problems, including nat-
ural language tasks (Cristianini and Shawe-Taylor,
2000), but require careful feature selection. Classifi-
cation using boosted decisions stumps can be a help-
ful tool for analyzing the usefulness of individual
features. Examining multiple classification meth-
ods helps avoid focusing on the biases of a particular
learning method.
4.1 Example Reweighting
One problem with formulating the segmentation
problem as a classification problem is that there are
many more negative than positive examples. To dis-
courage the learning algorithm from classifying all
results as negative and to instead focus on the posi-
tive examples, the training data must be reweighted.
We set the weight of positive vs. negative exam-
ples so that the number of boundaries after testing
agrees with the expected number of segments based
on the training data. This is done by iteratively ad-
justing the weighting factor while re-training and re-
testing until the predicted number of segments on the
test set is approximately the expected number. The
expected number of segments is the number of sen-
tences in the test set divided by the number of sen-
tences per segment in the training data. This value
can also be weighted based on prior knowledge.
4.2 Preprocessing
A number of preprocessing steps are applied to the
books to help increase the informativeness of the
texts. The book texts were obtained using OCR
methods with human correction. The text is pre-
processed by tokenizing, removing stop words, and
stemming using the Inxight LinguistiX morpholog-
ical analyzer. Paragraphs are identified using for-
matting information. Sentences are identified using
the TnT tokenizer and parts of speech with the TnT
part of speech tagger (Brants, 2000) with the stan-
dard English Wall Street Journal n-grams. Named
entities are identified using finite state technology
(Beesley and Karttunen, 2003) to identify various
entities including: person, location, disease and or-
ganization. Many of these preprocessing steps help
provide salient features for use during segmentation.
4.3 Engineered Features
Segmenting narrative documents raises a number of
interesting challenges. First, labeling data is ex-
tremely time consuming. Therefore, outside re-
sources are required to better generalize from the
training data. WordNet is used to identify words that
are similar and tend to occur at boundaries for the
?word group? feature. Second, some sources of in-
formation, in particular entity chains, do not fit into
the standard feature based paradigm. This requires
extracting features from the underlying information
source. Extracting these features represents a trade-
off between information content and generalizabil-
ity. In the case of entity chains, we extract features
that characterize the occurrence distribution of the
35
entity chains. Finally, the ?word groups? and ?entity
groups? feature groups generate candidate features
and a selection process is required to select useful
features. We found that a likelihood ratio test for sig-
nificance worked well for identifying those features
that would be useful for classification. Throughout
this section, when we use the term ?significant? we
are referring to significant with respect to the likeli-
hood ratio test (with a significance level of 0.1).
We selected features both a priori and dynami-
cally during training (i.e., word groups and entity
groups are selected dynamically). Feature selection
has been used by previous segmentation methods
(Beeferman et al, 1999) as a way of adapting bet-
ter to the data. In our approach, knowledge about
the task is used more strongly in defining the fea-
ture types, and the selection of features is performed
prior to the classification step. We also used mutual
information, statistical tests of significance and clas-
sification performance on a development data set to
identify useful features.
Word groups In Section 3 we showed that there are
not consistent cue phrases at boundaries. To general-
ize better, we identify word groups that occur signif-
icantly at boundaries. A word group is all words that
have the same parent in the WordNet hierarchy. A
binary feature is used for each learned group based
on the occurrence of at least one of the words in the
group. Groups found include months, days, tempo-
ral phrases, military rankings and country names.
Entity groups For each entity group (i.e. named
entities such as person, city, or disease tagged by the
named entity extractor) that occurs significantly at
a boundary, a feature indicating whether or not an
entity of that group occurs in the sentence is used.
Full name The named entity extraction system
tags persons named in the document. A rough
co-reference resolution was performed by group-
ing together references that share at least one to-
ken (e.g., ?General Yury Tikhonovich Kalinin? and
?Kalinin?). The full name of a person is the longest
reference of a group referring to the same person.
This feature indicates whether or not the sentence
contains a full name.
Entity chains Word relationships work well when
the documents have disjoint topics; however, when
topics are similar, words tend to relate too easily. We
propose a more stringent chaining method called en-
tity chains. Entity chains are constructed in the same
fashion as lexical chains, except we consider named
entities. Two entities are considered related (i.e. in
the same chain) if they refer to the same entity. We
construct entity chains and extract features that char-
acterize these chains: How many chains start/end at
this sentence? How many chains cross over this sen-
tence/previous sentence/next sentence? Distance to
the nearest dip/peak in the number of chains? Size
of that dip/peak?
Pronoun Does the sentence contain a pronoun?
Does the sentence contain a pronoun within 5 words
of the beginning of the sentence?
Numbers During training, the patterns of numbers
that occur significantly at boundaries are selected.
Patterns considered are any number and any number
with a specified length. The feature then checks if
that pattern appears in the sentence. A commonly
found pattern is the number pattern of length 4,
which often refers to a year.
Conversation Is this sentence part of a conversa-
tion, i.e. does this sentence contain ?direct speech??
This is determined by tracking beginning and end-
ing quotes. Quoted regions and single sentences be-
tween two quoted regions are considered part of a
conversation.
Paragraph Is this the beginning of a paragraph?
5 Experiments
In this section, we examine a number of narra-
tive segmentation tasks with different segmentation
methods. The only data used during development
was the first two thirds from Biohazard (exp1 and
exp2). All other data sets were only examined after
the algorithm was developed and were used for test-
ing purposes. Unless stated otherwise, results for the
feature based method are using the SVM classifier.1
5.1 Evaluation Measures
We use three segmentation evaluation metrics that
have been recently developed to account for ?close
but not exact? placement of hypothesized bound-
aries: word error probability, sentence error prob-
ability, and WindowDiff. Word error probability
1SVM and boosted decision stump performance is similar.
For brevity, only SVM results are shown for most results.
36
Table 2: Experiments with Biohazard
Word Sent. Window Sent err
Error Error Diff improv
Biohazard
random (sent.) 0.488 0.485 0.539 ??-
random (para.) 0.481 0.477 0.531 (base)
Biohazard
exp1 ? holdout 0.367 0.357 0.427 25%
exp2 ? holdout 0.344 0.325 0.395 32%
3x cross validtn. 0.355 0.332 0.404 24%
Train Biohazard
Test Demon 0.387 0.364 0.473 25%
(Beeferman et al, 1999) estimates the probability
that a randomly chosen pair of words k words apart
is incorrectly classified, i.e. a false positive or false
negative of being in the same segment. In contrast to
the standard classification measures of precision and
recall, which would consider a ?close? hypothesized
boundary (e.g., off by one sentence) to be incorrect,
word error probability gently penalizes ?close? hy-
pothesized boundaries. We also compute the sen-
tence error probability, which estimates the proba-
bility that a randomly chosen pair of sentences s sen-
tences apart is incorrectly classified. k and s are cho-
sen to be half the average length of a section in the
test data. WindowDiff (Pevzner and Hearst, 2002)
uses a sliding window over the data and measures
the difference between the number of hypothesized
boundaries and the actual boundaries within the win-
dow. This metric handles several criticisms of the
word error probability metric.
5.2 Segmenting Narrative Books
Table 2 shows the results of the SVM-segmenter on
Biohazard and Demon in the Freezer. A baseline
performance for segmentation algorithms is whether
the algorithm performs better than naive segment-
ing algorithms: choose no boundaries, choose all
boundaries and choose randomly. Choosing all
boundaries results in word and sentence error proba-
bilities of approximately 55%. Choosing no bound-
aries is about 45%. Table 2 also shows the results
for random placement of the correct number of seg-
ments. Both random boundaries at sentence loca-
tions and random boundaries at paragraph locations
are shown (values shown are the averages of 500
random runs). Similar results were obtained for ran-
dom segmentation of the Demon data.
Table 3: Performance on Groliers articles
Word Sent. Window
Error Error Diff
random 0.482 0.483 0.532
TextTile 0.407 0.412 0.479
PLSA 0.420 0.435 0.507
features (stumps) 0.387 0.400 0.495
features (SVM) 0.385 0.398 0.503
For Biohazard the holdout set was not used dur-
ing development. When trained on either of the de-
velopment thirds of the text (i.e., exp1 or exp2) and
tested on the test set, a substantial improvement is
seen over random. 3-fold cross validation was done
by training on two-thirds of the data and testing on
the other third. Recalling from Table 1 that both
PLSA and TextTiling result in performance simi-
lar to random even when given the correct number
of segments, we note that all of the single train/test
splits performed better than any of the naive algo-
rithms and previous methods examined.
To examine the ability of our algorithm to perform
on unseen data, we trained on the entire Biohaz-
ard book and tested on Demon in the Freezer. Per-
formance on Demon in the Freezer is only slightly
worse than the Biohazard results and is still much
better than the baseline algorithms as well as previ-
ous methods. This is encouraging since Demon was
not used during development, is written by a differ-
ent author and has a segment length distribution that
is different than Biohazard (average segment length
of 30 vs. 18 in Biohazard).
5.3 Segmenting Articles
Unfortunately, obtaining a large number of narrative
books with meaningful labeled segmentation is dif-
ficult. To evaluate our algorithm on a larger data set
as well as a wider variety of styles similar to narra-
tive documents, we also examine 1000 articles from
Groliers Encyclopedia that contain subsections de-
noted by major and minor headings, which we con-
sider to be the true segment boundaries. The articles
contained 8,922 true and 102,116 possible bound-
aries. We randomly split the articles in half, and
perform two-fold cross-validation as recommended
by Dietterich (1998). Using 500 articles from one
half of the pair for testing, 50 articles are randomly
selected from the other half for training. We used
37
Table 4: Ave. human performance (Hearst, 1994)
Word Sent. Window
Error (%) Error (%) Diff (%)
Sequoia 0.275 0.272 0.351
Earth 0.219 0.221 0.268
Quantum 0.179 0167 0.316
Magellan 0.147 0.147 0.157
a subset of only 50 articles due to the high cost of
labeling data. Each split yields two test sets of 500
articles and two training sets. This procedure of two-
fold cross-validation is performed five times, for a
total of 10 training and 10 corresponding test sets.
Significance is then evaluated using the t-test.
The results for segmenting Groliers Encyclope-
dia articles are given in Table 3. We compare
the performance of different segmentation models:
two feature-based models (SVMs, boosted deci-
sion stumps), two similarity-based models (PLSA-
based segmentation, TextTiling), and randomly se-
lecting segmentation points. All segmentation sys-
tems are given the estimated number of segmenta-
tion points based based on the training data. The
feature based approaches are significantly2 better
than either PLSA, TextTiling or random segmenta-
tion. For our selected features, boosted stump per-
formance is similar to using an SVM, which rein-
forces our intuition that the selected features (and
not just classification method) are appropriate for
this problem.
Table 1 indicates that the previous TextTiling and
PLSA-based approaches perform close to random
on narrative text. Our experiments show a perfor-
mance improvement of >24% by our feature-based
system, and significant improvement over other
methods on the Groliers data. Hearst (1994) ex-
amined the task of identifying the paragraph bound-
aries in expository text. We provide analysis of this
data set here to emphasize that identifying segments
in natural text is a difficult problem and since cur-
rent evaluation methods were not used when this
data was initially presented. Human performance
on this task is in the 15%-35% error rate. Hearst
asked seven human judges to label the paragraph
2For both SVM and stumps at a level of 0.005 us-
ing a t-test except SVM TextTile-WindowDiff (at 0.05)
and stumps TextTile-WindowDiff and SVM/stumps PLSA-
WindowDiff (not significantly different)
Table 5: Feature occurrences at boundary and non-
boundary locations
boundary non-boundary
Paragraph 74 621
Entity groups 44 407
Word groups 39 505
Numbers 16 59
Full name 2 109
Conversation 0 510
Pronoun 8 742
Pronoun ? 5 1 330
boundaries of four different texts. Since no ground
truth was available, true boundaries were identified
by those boundaries that had a majority vote as a
boundary. Table 4 shows the average human perfor-
mance for each text. We show these results not for
direct comparison with our methods, but to highlight
that even human segmentation on a related task does
not achieve particularly low error rates.
5.4 Analysis of Features
The top section of Table 5 shows features that are
intuitively hypothesized to be positively correlated
with boundaries and the bottom section shows nega-
tively correlated. For this analysis, exp1 from Alibek
was used for training and the holdout set for testing.
There are 74 actual boundaries and 2086 possibly
locations. Two features have perfect recall: para-
graph and conversation. Every true section bound-
ary is at a paragraph and no section boundaries are
within conversation regions. Both the word group
and entity group features have good correlation with
boundary locations and also generalized well to the
training data by occurring in over half of the positive
test examples.
The benefit of generalization using outside re-
sources can be seen by comparing the boundary
words found using word groups versus those found
only in the training set as in Section 3. Using word
groups triples the number of significant words found
in the training set that occur in the test set. Also, the
number of shared words that occur significantly in
both the training and test set goes from none to 9.
More importantly, significant words occur in 37 of
the test segments instead of none without the groups.
38
6 Discussion and Summary
Based on properties of narrative text, we proposed
and investigated a set of features for segmenting nar-
rative text. We posed the problem of segmentation
as a feature-based classification problem, which pre-
sented a number of challenges: many different fea-
ture sources, generalization from outside resources
for sparse data, and feature extraction from non-
traditional information sources.
Feature selection and analyzing feature interac-
tion is crucial for this type of application. The para-
graph feature has perfect recall in that all boundaries
occur at paragraph boundaries. Surprisingly, for cer-
tain train/test splits of the data, the performance of
the algorithm was actually better without the para-
graph feature than with it. We hypothesize that the
noisiness of the data is causing the classifier to learn
incorrect correlations.
In addition to feature selection issues, posing the
problem as a classification problem loses the se-
quential nature of the data. This can produce very
unlikely segment lengths, such as a single sentence.
We alleviated this by selecting features that capture
properties of the sequence. For example, the entity
chains features represent some of this type of infor-
mation. However, models for complex sequential
data should be examined as possible better methods.
We evaluated our algorithm on two books and
encyclopedia articles, observing significantly bet-
ter performance than randomly selecting the correct
number of segmentation points, as well as two pop-
ular, previous approaches, PLSA and TextTiling.
Acknowledgments
We thank Marti Hearst for the human subject perfor-
mance data and the anonymous reviewers for their
very helpful comments. Funded in part by the Ad-
vanced Research and Development Activity NIMD
program (MDA904-03-C-0404).
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34:177?210.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI Publications, Palo Alto, CA.
Thorsten Brants, Francine Chen, and Ioannis Tsochan-
taridis. 2002. Topic-based document segmentation
with probabilistic latent semantic analysis. In Pro-
ceedings of CIKM, pg. 211?218.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech
tagger. In Proceedings of the Applied NLP Confer-
ence.
Freddy Choi. 2000. Improving the efficiency of speech
interfaces for text navigation. In Proceedings of IEEE
Colloquium: Speech and Language Processing for
Disabled and Elderly People.
Nello Cristianini and John Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines. Cambridge
University Press.
Thomas Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895?1923.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Meeting of ACL, pg. 9?16.
Thorsten Joachims, 1999. Advances in Kernel Methods -
Support Vector Learning, chapter Making large-Scale
SVM Learning Practical. MIT-Press.
Hideki Kozima and Teiji Furugori. 1994. Segmenting
narrative text into coherent scenes. In Literary and
Linguistic Computing, volume 9, pg. 13?19.
Hideki Kozima. 1993. Text segmentation based on sim-
ilarity between words. In Meeting of ACL, pg. 286?
288.
Hang Li and Kenji Yamanishi. 2000. Topic analysis us-
ing a finite mixture model. In Proceedings of Joint
SIGDAT Conference of EMNLP and Very Large Cor-
pora, pg. 35?44.
Hajime Mochizuki, Takeo Honda, and Manabu Okumura.
1998. Text segmentation with multiple surface lin-
guistic cues. In COLING-ACL, pg. 881?885.
Lev Pevzner and Marti Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, pg. 19?36.
Jeffrey Reynar. 1999. Statistical models for topic seg-
mentation. In Proceedings of ACL, pg. 357?364.
Nicola Stokes, Joe Carthy, and Alex Smeaton. 2002.
Segmenting broadcast news streams using lexical
chains. In Proceedings of Starting AI Researchers
Symposium, (STAIRS 2002), pg. 145?154.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical machine learning tools with Java implemen-
tations. Morgan Kaufmann.
39
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 665?669,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Simple English Wikipedia: A New Text Simplification Task
William Coster
Computer Science Department
Pomona College
Claremont, CA 91711
wpc02009@pomona.edu
David Kauchak
Computer Science Department
Pomona College
Claremont, CA 91711
dkauchak@cs.pomona.edu
Abstract
In this paper we examine the task of sentence
simplification which aims to reduce the read-
ing complexity of a sentence by incorporat-
ing more accessible vocabulary and sentence
structure. We introduce a new data set that
pairs English Wikipedia with Simple English
Wikipedia and is orders of magnitude larger
than any previously examined for sentence
simplification. The data contains the full range
of simplification operations including reword-
ing, reordering, insertion and deletion. We
provide an analysis of this corpus as well as
preliminary results using a phrase-based trans-
lation approach for simplification.
1 Introduction
The task of text simplification aims to reduce the
complexity of text while maintaining the content
(Chandrasekar and Srinivas, 1997; Carroll et al,
1998; Feng, 2008). In this paper, we explore the
sentence simplification problem: given a sentence,
the goal is to produce an equivalent sentence where
the vocabulary and sentence structure are simpler.
Text simplification has a number of important ap-
plications. Simplification techniques can be used to
make text resources available to a broader range of
readers, including children, language learners, the
elderly, the hearing impaired and people with apha-
sia or cognitive disabilities (Carroll et al, 1998;
Feng, 2008). As a preprocessing step, simplification
can improve the performance of NLP tasks, includ-
ing parsing, semantic role labeling, machine transla-
tion and summarization (Miwa et al, 2010; Jonnala-
gadda et al, 2009; Vickrey and Koller, 2008; Chan-
drasekar and Srinivas, 1997). Finally, models for
text simplification are similar to models for sentence
compression; advances in simplification can bene-
fit compression, which has applications in mobile
devices, summarization and captioning (Knight and
Marcu, 2002; McDonald, 2006; Galley and McKe-
own, 2007; Nomoto, 2009; Cohn and Lapata, 2009).
One of the key challenges for text simplification
is data availability. The small amount of simplifi-
cation data currently available has prevented the ap-
plication of data-driven techniques like those used
in other text-to-text translation areas (Och and Ney,
2004; Chiang, 2010). Most prior techniques for
text simplification have involved either hand-crafted
rules (Vickrey and Koller, 2008; Feng, 2008) or
learned within a very restricted rule space (Chan-
drasekar and Srinivas, 1997).
We have generated a data set consisting of 137K
aligned simplified/unsimplified sentence pairs by
pairing documents, then sentences from English
Wikipedia1 with corresponding documents and sen-
tences from Simple English Wikipedia2. Simple En-
glish Wikipedia contains articles aimed at children
and English language learners and contains similar
content to English Wikipedia but with simpler vo-
cabulary and grammar.
Figure 1 shows example sentence simplifications
from the data set. Like machine translation and other
text-to-text domains, text simplification involves the
full range of transformation operations including
deletion, rewording, reordering and insertion.
1http://en.wikipedia.org/
2http://simple.wikipedia.org
665
a. Normal: As Isolde arrives at his side, Tristan dies with her name on his lips.
Simple: As Isolde arrives at his side, Tristan dies while speaking her name.
b. Normal: Alfonso Perez Munoz, usually referred to as Alfonso, is a
former Spanish footballer, in the striker position.
Simple: Alfonso Perez is a former Spanish football player.
c. Normal: Endemic types or species are especially likely to develop on islands
because of their geographical isolation.
Simple: Endemic types are most likely to develop on islands because
they are isolated.
d. Normal: The reverse process, producing electrical energy from mechanical,
energy, is accomplished by a generator or dynamo.
Simple: A dynamo or an electric generator does the reverse: it changes
mechanical movement into electric energy.
Figure 1: Example sentence simplifications extracted from Wikipedia. Normal refers to a sentence in an English
Wikipedia article and Simple to a corresponding sentence in Simple English Wikipedia.
2 Previous Data
Wikipedia and Simple English Wikipedia have both
received some recent attention as a useful resource
for text simplification and the related task of text
compression. Yamangil and Nelken (2008) examine
the history logs of English Wikipedia to learn sen-
tence compression rules. Yatskar et al (2010) learn
a set of candidate phrase simplification rules based
on edits identified in the revision histories of both
Simple English Wikipedia and English Wikipedia.
However, they only provide a list of the top phrasal
simplifications and do not utilize them in an end-
to-end simplification system. Finally, Napoles and
Dredze (2010) provide an analysis of the differences
between documents in English Wikipedia and Sim-
ple English Wikipedia, though they do not view the
data set as a parallel corpus.
Although the simplification problem shares some
characteristics with the text compression problem,
existing text compression data sets are small and
contain a restricted set of possible transformations
(often only deletion). Knight and Marcu (2002) in-
troduced the Zipf-Davis corpus which contains 1K
sentence pairs. Cohn and Lapata (2009) manually
generated two parallel corpora from news stories to-
taling 3K sentence pairs. Finally, Nomoto (2009)
generated a data set based on RSS feeds containing
2K sentence pairs.
3 Simplification Corpus Generation
We generated a parallel simplification corpus by
aligning sentences between English Wikipedia and
Simple English Wikipedia. We obtained complete
copies of English Wikipedia and Simple English
Wikipedia in May 2010. We first paired the articles
by title, then removed all article pairs where either
article: contained only a single line, was flagged as a
stub, was flagged as a disambiguation page or was a
meta-page about Wikipedia. After pairing and filter-
ing, 10,588 aligned, content article pairs remained
(a 90% reduction from the original 110K Simple En-
glish Wikipedia articles). Throughout the rest of this
paper we will refer to unsimplified text from English
Wikipedia as normal and to the simplified version
from Simple English Wikipedia as simple.
To generate aligned sentence pairs from the
aligned document pairs we followed an approach
similar to those utilized in previous monolingual
alignment problems (Barzilay and Elhadad, 2003;
Nelken and Shieber, 2006). Paragraphs were iden-
tified based on formatting information available in
the articles. Each simple paragraph was then aligned
to every normal paragraph where the TF-IDF, co-
sine similarity was over a threshold or 0.5. We ini-
tially investigated the paragraph clustering prepro-
cessing step in (Barzilay and Elhadad, 2003), but
did not find a qualitative difference and opted for the
simpler similarity-based alignment approach, which
does not require manual annotation.
666
For each aligned paragraph pair (i.e. a simple
paragraph and one or more normal paragraphs), we
then used a dynamic programming approach to find
that best global sentence alignment following Barzi-
lay and Elhadad (2003). Specifically, given n nor-
mal sentences to align to m simple sentences, we
find a(n,m) using the following recurrence:
a(i, j) =
max
?
?
?
?
?
?
?
?
?
a(i, j ? 1)? skip penalty
a(i? 1, j)? skip penalty
a(i? 1, j ? 1) + sim(i, j)
a(i? 1, j ? 2) + sim(i, j) + sim(i, j ? 1)
a(i? 2, j ? 1) + sim(i, j) + sim(i? 1, j)
a(i? 2, j ? 2) + sim(i, j ? 1) + sim(i? 1, j)
where each line above corresponds to a sentence
alignment operation: skip the simple sentence, skip
the normal sentence, align one normal to one sim-
ple, align one normal to two simple, align two nor-
mal to one simple and align two normal to two sim-
ple. sim(i, j) is the similarity between the ith nor-
mal sentence and the jth simple sentence and was
calculated using TF-IDF, cosine similarity. We set
skip penalty = 0.0001 manually.
Barzilay and Elhadad (2003) further discourage
aligning dissimilar sentences by including a ?mis-
match penalty? in the similarity measure. Instead,
we included a filtering step removing all sentence
pairs with a normalized similarity below a threshold
of 0.5. We found this approach to be more intuitive
and allowed us to compare the effects of differing
levels of similarity in the training set. Our choice of
threshold is high enough to ensure that most align-
ments are correct, but low enough to allow for vari-
ation in the paired sentences. In the future, we hope
to explore other similarity techniques that will pair
sentences with even larger variation.
4 Corpus Analysis
From the 10K article pairs, we extracted 75K
aligned paragraphs. From these, we extracted the
final set of 137K aligned sentence pairs. To evaluate
the quality of the aligned sentences, we asked two
human evaluators to independently judge whether or
not the aligned sentences were correctly aligned on
a random sample of 100 sentence pairs. They then
were asked to reach a consensus about correctness.
91/100 were identified as correct, though many of
the remaining 9 also had some partial content over-
lap. We also repeated the experiment using only
those sentences with a similarity above 0.75 (rather
than 0.50 in the original data). This reduced the
number of pairs from 137K to 90K, but the eval-
uators identified 98/100 as correct. The analysis
throughout the rest of the section is for threshold
of 0.5, though similar results were also seen for the
threshold of 0.75.
Although the average simple article contained ap-
proximately 40 sentences, we extracted an average
of 14 aligned sentence pairs per article. Qualita-
tively, it is rare to find a simple article that is a direct
translation of the normal article, that is, a simple ar-
ticle that was generated by only making sentence-
level changes to the normal document. However,
there is a strong relationship between the two data
sets: 27% of our aligned sentences were identical
between simple and normal. We left these identical
sentence pairs in our data set since not all sentences
need to be simplified and it is important for any sim-
plification algorithm to be able to handle this case.
Much of the content without direct correspon-
dence is removed during paragraph alignment. 65%
of the simple paragraphs do not align to a normal
paragraphs and are ignored. On top of this, within
aligned paragraphs, there are a large number of sen-
tences that do not align. Table 1 shows the propor-
tion of the different sentence level alignment opera-
tions in our data set. On both the simple and normal
sides there are many sentences that do not align.
Operation %
skip simple 27%
skip normal 23%
one normal to one simple 37%
one normal to two simple 8%
two normal to one simple 5%
Table 1: Frequency of sentence-level alignment opera-
tions based on our learned sentence alignment. No 2-to-2
alignments were found in the data.
To better understand how sentences are trans-
formed from normal to simple sentences we learned
a word alignment using GIZA++ (Och and Ney,
2003). Based on this word alignment, we calcu-
lated the percentage of sentences that included: re-
667
wordings ? a normal word is changed to a different
simple word, deletions ? a normal word is deleted,
reorderings ? non-monotonic alignment, splits ? a
normal words is split into multiple simple words,
and merges ? multiple normal words are condensed
to a single simple word.
Transformation %
rewordings 65%
deletions 47%
reorders 34%
merges 31%
splits 27%
Table 2: Percentage of sentence pairs that contained
word-level operations based on the induced word align-
ment. Splits and merges are from the perspective of
words in the normal sentence. These are not mutually
exclusive events.
Table 2 shows the percentage of each of these phe-
nomena occurring in the sentence pairs. All of the
different operations occur frequently in the data set
with rewordings being particularly prevalent.
5 Sentence-level Text Simplification
To understand the usefulness of this data we ran
preliminary experiments to learn a sentence-level
simplification system. We view the problem of
text simplification as an English-to-English transla-
tion problem. Motivated by the importance of lex-
ical changes, we used Moses, a phrase-based ma-
chine translation system (Och and Ney, 2004).3 We
trained Moses on 124K pairs from the data set and
the n-gram language model on the simple side of this
data. We trained the hyper-parameters of the log-
linear model on a 500 sentence pair development set.
We compared the trained system to a baseline of
not doing any simplification (NONE). We evaluated
the two approaches on a test set of 1300 sentence
pairs. Since there is currently no standard for au-
tomatically evaluating sentence simplification, we
used three different automatic measures that have
been used in related domains: BLEU, which has
been used extensively in machine translation (Pap-
ineni et al, 2002), and word-level F1 and simple
string accuracy (SSA) which have been suggested
3We also experimented with T3 (Cohn and Lapata, 2009)
but the results were poor and are not presented here.
System BLEU word-F1 SSA
NONE 0.5937 0.5967 0.6179
Moses 0.5987 0.6076 0.6224
Moses-Oracle 0.6317 0.6661 0.6550
Table 3: Test scores for the baseline (NONE), Moses and
Moses-Oracle.
for text compression (Clarke and Lapata, 2006). All
three of these measures have been shown to correlate
with human judgements in their respective domains.
Table 3 shows the results of our initial test. All
differences are statistically significant at p = 0.01,
measured using bootstrap resampling with 100 sam-
ples (Koehn, 2004). Although the baseline does well
(recall that over a quarter of the sentence pairs in
the data set are identical) the phrase-based approach
does obtain a statistically significant improvement.
To understand the the limits of the phrase-based
model for text simplification, we generated an n-
best list of the 1000 most-likely simplifications for
each test sentence. We then greedily picked the sim-
plification from this n-best list that had the highest
sentence-level BLEU score based on the test exam-
ples, labeled Moses-Oracle in Table 3. The large
difference between Moses and Moses-Oracle indi-
cates possible room for improvement utilizing better
parameter estimation or n-best list reranking tech-
niques (Och et al, 2004; Ge and Mooney, 2006).
6 Conclusion
We have described a new text simplification data set
generated from aligning sentences in Simple English
Wikipedia with sentences in English Wikipedia. The
data set is orders of magnitude larger than any cur-
rently available for text simplification or for the re-
lated field of text compression and is publicly avail-
able.4 We provided preliminary text simplification
results using Moses, a phrase-based translation sys-
tem, and saw a statistically significant improvement
of 0.005 BLEU over the baseline of no simplifica-
tion and showed that further improvement of up to
0.034 BLEU may be possible based on the oracle
results. In the future, we hope to explore alignment
techniques more tailored to simplification as well as
applications of this data to text simplification.
4http://www.cs.pomona.edu/?dkauchak/simplification/
668
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP.
John Carroll, Gido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of English newspaper text to assist aphasic readers. In
Proceedings of AAAI Workshop on Integrating AI and
Assistive Technology.
Raman Chandrasekar and Bangalore Srinivas. 1997. Au-
tomatic induction of rules for text simplification. In
Knowledge Based Systems.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of ACL.
James Clarke and Mirella Lapata. 2006. Models for
sentence compression: A comparison across domains,
training requirements and evaluation measures. In
Proceedings of ACL.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research.
Lijun Feng. 2008. Text simplification: A survey. CUNY
Technical Report.
Michel Galley and Kathleen McKeown. 2007. Lexical-
ized Markov grammars for sentence compression. In
Proceedings of HLT/NAACL.
Ruifang Ge and Raymond Mooney. 2006. Discrimina-
tive reranking for semantic parsing. In Proceedings of
COLING.
Siddhartha Jonnalagadda, Luis Tari, Jorg Hakenberg,
Chitta Baral, and Graciela Gonzalez. 2009. To-
wards effective sentence simplification for automatic
processing of biomedical text. In Proceedings of
HLT/NAACL.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Kevin Knight and Daniel Marcu. 2002. Summarization
beyond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
Makoto Miwa, Rune Saetre, Yusuke Miyao, and Jun?ichi
Tsujii. 2010. Entity-focused sentence simplication for
relation extraction. In Proceedings of COLING.
Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple Wikipedia: A cogitation in ascertaining
abecedarian language. In Proceedings of HLT/NAACL
Workshop on Computation Linguistics and Writing.
Rani Nelken and Stuart Shieber. 2006. Towards robust
context-sensitive sentence alignment for monolingual
corpora. In Proceedings of AMTA.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. In Informa-
tion Processing and Management.
Tadashi Nomoto. 2008. A generic sentence trimmer with
CRFs. In Proceedings of HLT/NAACL.
Tadashi Nomoto. 2009. A comparison of model free ver-
sus model intensive approaches to sentence compres-
sion. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Com-
putational Linguistics.
Franz Josef Och, Kenji Yamada, Stanford U, Alex Fraser,
Daniel Gildea, and Viren Jain. 2004. A smorgasbord
of features for statistical machine translation. In Pro-
ceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL.
Emily Pitler. 2010. Methods for sentence compression.
Technical Report MS-CIS-10-20, University of Penn-
sylvania.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
David Vickrey and Daphne Koller. 2008. Sentence sim-
plification for semantic role labeling. In Proceedings
of ACL.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia revision histories for improving sentence
compression. In ACL.
Mark Yatskar, Bo Pang, Critian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifications
from Wikipedia. In HLT/NAACL Short Papers.
669
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1537?1546,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improving Text Simplification Language Modeling
Using Unsimplified Text Data
David Kauchak
Middlebury College
Middlebury, VT 05753
dkauchak@middlebury.edu
Abstract
In this paper we examine language mod-
eling for text simplification. Unlike some
text-to-text translation tasks, text simplifi-
cation is a monolingual translation task al-
lowing for text in both the input and out-
put domain to be used for training the lan-
guage model. We explore the relation-
ship between normal English and simpli-
fied English and compare language mod-
els trained on varying amounts of text
from each. We evaluate the models intrin-
sically with perplexity and extrinsically
on the lexical simplification task from Se-
mEval 2012. We find that a combined
model using both simplified and normal
English data achieves a 23% improvement
in perplexity and a 24% improvement on
the lexical simplification task over a model
trained only on simple data. Post-hoc anal-
ysis shows that the additional unsimplified
data provides better coverage for unseen
and rare n-grams.
1 Introduction
An important component of many text-to-text
translation systems is the language model which
predicts the likelihood of a text sequence being
produced in the output language. In some problem
domains, such as machine translation, the trans-
lation is between two distinct languages and the
language model can only be trained on data in
the output language. However, some problem do-
mains (e.g. text compression, text simplification
and summarization) can be viewed as monolingual
translation tasks, translating between text varia-
tions within a single language. In these monolin-
gual problems, text could be used from both the
input and output domain to train a language model.
In this paper, we investigate this possibility for text
simplification where both simplified English text
and normal English text are available for training
a simple English language model.
Table 1 shows the n-gram overlap proportions
in a sentence aligned data set of 137K sentence
pairs from aligning Simple English Wikipedia and
English Wikipedia articles (Coster and Kauchak,
2011a).1 The data highlights two conflicting
views: does the benefit of additional data out-
weigh the problem of the source of the data?
Throughout the rest of this paper we refer to
sentences/articles/text from English Wikipedia as
normal and sentences/articles/text from Simple
English Wikipedia as simple.
On the one hand, there is a strong correspon-
dence between the simple and normal data. At the
word level 96% of the simple words are found in
the normal corpus and even for n-grams as large as
5, more than half of the n-grams can be found in
the normal text. In addition, the normal text does
represent English text and contains many n-grams
not seen in the simple corpus. This extra informa-
tion may help with data sparsity, providing better
estimates for rare and unseen n-grams.
On the other hand, there is still only modest
overlap between the sentences for longer n-grams,
particularly given that the corpus is sentence-
aligned and that 27% of the sentence pairs in
this aligned data set are identical. If the word
distributions were very similar between simple
and normal text, then the overlap proportions be-
tween the two languages would be similar re-
gardless of which direction the comparison is
made. Instead, we see that the normal text has
more varied language and contains more n-grams.
Previous research has also shown other differ-
ences between simple and normal data sources
that could impact language model performance
including average number of syllables, reading
1http://www.cs.middlebury.edu/?dkauchak/simplification
1537
n-gram size: 1 2 3 4 5
simple in normal 0.96 0.80 0.68 0.61 0.55
normal in simple 0.87 0.68 0.58 0.51 0.46
Table 1: The proportion of n-grams that overlap
in a corpus of 137K sentence-aligned pairs from
Simple English Wikipedia and English Wikipedia.
complexity, and grammatical complexity (Napoles
and Dredze, 2010; Zhu et al, 2010; Coster and
Kauchak, 2011b). In addition, for some monolin-
gual translation domains, it has been argued that it
is not appropriate to train a language model using
data from the input domain (Turner and Charniak,
2005).
Although this question arises in other monolin-
gual translation domains, text simplification rep-
resents an ideal problem area for analysis. First,
simplified text data is available in reasonable
quantities. Simple English Wikipedia contains
more than 60K articles written in simplified En-
glish. This is not the case for all monolingual
translation tasks (Knight and Marcu, 2002; Cohn
and Lapata, 2009). Second, the quantity of sim-
ple text data available is still limited. After pre-
processing, the 60K articles represents less than
half a million sentences which is orders of mag-
nitude smaller than the amount of normal English
data available (for example the English Gigaword
corpus (David Graff, 2003)). Finally, many recent
text simplification systems have utilized language
models trained only on simplified data (Zhu et al,
2010; Woodsend and Lapata, 2011; Coster and
Kauchak, 2011a; Wubben et al, 2012); improve-
ments in simple language modeling could translate
into improvements for these systems.
2 Related Work
If we view the normal data as out-of-domain data,
then the problem of combining simple and nor-
mal data is similar to the language model do-
main adaption problem (Suzuki and Gao, 2005),
in particular cross-domain adaptation (Bellegarda,
2004) where a domain-specific model is improved
by incorporating additional general data. Adapta-
tion techniques have been shown to improve lan-
guage modeling performance based on perplexity
(Rosenfeld, 1996) and in application areas such
as speech transcription (Bacchiani and Roark,
2003) and machine translation (Zhao et al, 2004),
though no previous research has examined the lan-
guage model domain adaptation problem for text
simplification. Pan and Yang (2010) provide a sur-
vey on the related problem of domain adaptation
for machine learning (also referred to as ?transfer
learning?), which utilizes similar techniques. In
this paper, we explore some basic adaptation tech-
niques, however this paper is not a comparison of
domain adaptation techniques for language mod-
eling. Our goal is more general: to examine the
relationship between simple and normal data and
determine whether normal data is helpful. Previ-
ous domain adaptation research is complementary
to our experiments and could be explored in the
future for additional performance improvements.
Simple language models play a role in a va-
riety of text simplification applications. Many
recent statistical simplification techniques build
upon models from machine translation and uti-
lize a simple language model during simplifica-
tion/decoding both in English (Zhu et al, 2010;
Woodsend and Lapata, 2011; Coster and Kauchak,
2011a; Wubben et al, 2012) and in other lan-
guages (Specia, 2010). Simple English language
models have also been used as predictive features
in other simplification sub-problems such as lexi-
cal simplification (Specia et al, 2012) and predict-
ing text simplicity (Eickhoff et al, 2010).
Due to data scarcity, little research has been
done on language modeling in other monolin-
gual translation domains. For text compression,
most systems are trained on uncompressed data
since the largest text compression data sets con-
tain only a few thousand sentences (Knight and
Marcu, 2002; Galley and McKeown, 2007; Cohn
and Lapata, 2009; Nomoto, 2009). Similarly for
summarization, systems that have employed lan-
guage models trained only on unsummarized text
(Banko et al, 2000; Daume and Marcu, 2002).
3 Corpus
We collected a data set from English Wikipedia
and Simple English Wikipedia with the former
representing normal English and the latter sim-
ple English. Simple English Wikipedia has been
previously used for many text simplification ap-
proaches (Zhu et al, 2010; Yatskar et al, 2010;
Biran et al, 2011; Coster and Kauchak, 2011a;
Woodsend and Lapata, 2011; Wubben et al, 2012)
and has been shown to be simpler than normal En-
glish Wikipedia by both automatic measures and
human perception (Coster and Kauchak, 2011b;
1538
simple normal
sentences 385K 2540K
words 7.15M 64.7M
vocab size 78K 307K
Table 2: Summary counts for the simple-normal
article aligned data set consisting of 60K article
pairs.
Woodsend and Lapata, 2011). We downloaded all
articles from Simple English Wikipedia then re-
moved stubs, navigation pages and any article that
consisted of a single sentence, resulting in 60K
simple articles.
To partially normalize for content and source
differences we generated a document aligned cor-
pus for our experiments. We extracted the cor-
responding 60K normal articles from English
Wikipedia based on the article title to represent the
normal data. We held out 2K article pairs for use
as a testing set in our experiments. The extracted
data set is available for download online.2
Table 2 shows count statistics for the collected
data set. Although the simple and normal data
contain the same number of articles, because nor-
mal articles tend to be longer and contain more
content, the normal side is an order of magnitude
larger.
4 Language Model Evaluation:
Perplexity
To analyze the impact of data source on simple
English language modeling, we trained language
models on varying amounts of simple data, nor-
mal data, and a combination of the two. For our
first task, we evaluated these language models us-
ing perplexity based on how well they modeled the
simple side of the held-out data.
4.1 Experimental Setup
We used trigram language models with interpo-
lated Kneser-Kney discounting trained using the
SRI language modeling toolkit (Stolcke, 2002). To
ensure comparability, all models were closed vo-
cabulary with the same vocabulary set based on
the words that occurred in the simple side of the
training corpus, though similar results were seen
for other vocabulary choices. We generated differ-
ent models by varying the size and type of training
2http://www.cs.middlebury.edu/?dkauchak/simplification
 100
 150
 200
 250
 300
 350
0.5M 1M 1.5M 2M 2.5M 3M
pe
rp
le
xi
ty
total number of sentences
simple-only
normal-only
simple-ALL+normal
Figure 1: Language model perplexities on the
held-out test data for models trained on increasing
amounts of data.
data:
- simple-only: simple sentences only
- normal-only: normal sentences only
- simple-X+normal: X simple sentences com-
bined with a varying number of normal sen-
tences
To evaluate the language models we calculated
the model perplexity (Chen et al, 1998) on the
simple side of the held-out data. The test set con-
sisted of 2K simple English articles with 7,799
simple sentences and 179K words. Perplexity
measures how likely a model finds a test set, with
lower scores indicating better performance.
4.2 Perplexity Results
Figure 1 shows the language model perplexi-
ties for the three types of models for increasing
amounts of training data. As expected, when
trained on the same amount of data, the language
models trained on simple data perform signifi-
cantly better than language models trained on nor-
mal data. In addition, as we increase the amount of
data, the simple-only model improves more than
the normal-only model.
However, the results also show that the normal
data does have some benefit. The perplexity for
the simple-ALL+normal model, which starts with
all available simple data, continues to improve as
normal data is added resulting in a 23% improve-
ment over the model trained with only simple data
(from a perplexity of 129 down to 100). Even
by itself the normal data does have value. The
normal-only model achieves a slightly better per-
plexity than the simple-only model, though only
by utilizing an order of magnitude more data.
1539
 100
 150
 200
 250
 300
0.5M 1M 1.5M 2M 2.5M
pe
rp
le
xi
ty
number of additional normal sentences
simple-50k+normal
simple-100k+normal
simple-150k+normal
simple-200k+normal
simple-250k+normal
simple-300k+normal
simple-350k+normal
Figure 2: Language model perplexities for com-
bined simple-normal models. Each line represents
a model trained on a different amount of simple
data as normal data is added.
To better understand how the amount of sim-
ple and normal data impacts perplexity, Figure 2
shows perplexity scores for models trained on
varying amounts of simple data as we add increas-
ing amounts of normal data. We again see that
normal data is beneficial; regardless of the amount
of simple data, adding normal data improves per-
plexity. This improvement is most beneficial when
simple data is limited. Models trained on less
simple data achieved larger performance increases
than those models trained on more simple data.
Figure 2 also shows again that simple data
is more valuable than normal data. For ex-
ample, the simple-only model trained on 250K
sentences achieves a perplexity of approximately
150. To achieve this same perplexity level start-
ing with 200K simple sentences requires an ad-
ditional 300K normal sentences, or starting with
100K simple sentences an additional 850K normal
sentences.
4.3 Language Model Adaptation
In the experiments above, we generated the lan-
guage models by treating the simple and normal
data as one combined corpus. This approach has
the benefit of simplicity, however, better perfor-
mance for combining related corpora has been
seen by domain adaptation techniques which com-
bine the data in more structured ways (Bacchiani
and Roark, 2003). Our goal for this paper is not
to explore domain adaptation techniques, but to
determine if normal data is useful for the simple
language modeling task. However, to provide an-
other dimension for comparison and to understand
 95
 100
 105
 110
 115
 120
 125
 130
simple-only 0.2 0.4 0.6 0.8 normal-only
pe
rp
le
xi
ty
lambda
Figure 3: Perplexity scores for a linearly interpo-
lated model between the simple-only model and
the normal-only model for varying lambda values.
if domain adaptation techniques may be useful, we
also investigated a linearly interpolated language
model.
A linearly interpolated language model com-
bines the probabilities of two or more language
models as a weighted sum. In our case, the in-
terpolated model combines the simple model esti-
mate, ps(wi|wi?2, wi?1), and the normal model esti-
mate, pn(wi|wi?2, wi?1), linearly (Jelinek and Mer-
cer, 1980; Hsu, 2007):
pinterpolated(wi|wi?2, wi?1) =
? pn(wi|wi?2, wi?1) + (1? ?) ps(wi|wi?2, wi?1)
where 0 ? ? ? 1.
Figure 3 shows perplexity scores for vary-
ing lambda values ranging from the simple-only
model on the left with ? = 0 to the normal-only
model on the right with ? = 1. As with the pre-
vious experiments, adding normal data improves
improves perplexity. In fact, with a lambda of
0.5 (equal weight between the models) the per-
formance is slightly better than the aggregate ap-
proaches above with a perplexity of 98. The re-
sults also highlight the balance between simple
and normal data; normal data is not as good as
simple data and adding too much of it can cause
the results to degrade.
5 Language Model Evaluation:
Lexical Simplification
Currently, no automated methods exist for eval-
uating sentence-level or document-level text sim-
plification systems and manual evaluation is time-
consuming, expensive and has not been vali-
dated. Because of these evaluation challenges, we
chose to evaluate the language models extrinsi-
1540
Word: tight
Context: With the physical market as tight as it has been in memory, silver could fly at any time.
Candidates: constricted, pressurised, low, high-strung, tight
Human ranking: tight, low, constricted, pressurised, high-strung
Figure 4: A lexical substitution example from the SemEval 2012 data set.
cally based on the lexical simplification task from
SemEval 2012 (Specia et al, 2012).
Lexical simplification is a sub-problem of the
general text simplification problem (Chandrasekar
and Srinivas, 1997); a sentence is simplified by
substituting words or phrases in the sentence with
?simpler? variations. Lexical simplification ap-
proaches have been shown to improve the read-
ability of texts (Urano, 2000; Leroy et al, 2012),
are useful in domains such as medical texts where
major content changes are restricted, and they may
be useful as a pre- or post-processing step for gen-
eral simplification systems.
5.1 Experimental Setup
Examples from the lexical simplification data set
from SemEval 2012 consist of three parts: w, the
word to be simplified; s1, ..., si?1, w, si+1, ..., sn,
a sentence containing the word; and, r1, r2, ..., rm,
a list of candidate simplifications for w. The goal
of the task is to rank the candidate simplifications
according to their simplicity in the context of the
sentence. Figure 4 shows an example from the
data set. The data set contains a development set
of 300 examples and a test set of 1710 examples.3
For our experiments, we evaluated the models on
the test set.
Given a language model p(?) and a lexical sim-
plification example, we ranked the list of candi-
dates based on the probability the language model
assigns to the sentence with the candidate simplifi-
cation inserted in context. Specifically, we scored
each candidate simplification rj by
p(s1... si?1 rj si+1... sn)
and then ranked them based on this score. For ex-
ample, to calculate the ranking for the example in
Figure 4 we calculate the probability of each of:
With the physical market as constricted as it has been ...
With the physical market as pressurised as it has been ...
With the physical market as low as it has been ...
With the physical market as high-strung as it has been ...
With the physical market as tight as it has been ...
with the language model and then rank them by
their probability. We do not suggest this as a com-
3http://www.cs.york.ac.uk/semeval-2012/task1/
 0.24
 0.26
 0.28
 0.3
 0.32
 0.34
 0.36
0.5M 1M 1.5M 2M 2.5M 3M
ka
pp
a 
ra
nk
 s
co
re
total number of sentences
simple-only
normal-only
simple-ALL+normal
Figure 5: Kappa rank scores for the models trained
on increasing amounts of data.
plete lexical substitution system, but it was a com-
mon feature for many of the submitted systems, it
performs well relative to the other systems, and it
allows for a concrete comparison between the lan-
guage models on a simplification task.
To evaluate the rankings, we use the metric from
the SemEval 2012 task, the Cohen?s kappa coeffi-
cient (Landis and Koch, 1977) between the system
ranking and the human ranking, which we denote
the ?kappa rank score?. See Specia et al (2012)
for the full details of how the evaluation metric is
calculated.
We use the same setup for training the language
models as in the perplexity experiments except
the models are open vocabulary instead of closed.
Open vocabulary models allow for the language
models to better utilize the varying amounts of
data and since the lexical simplification problem
only requires a comparison of probabilities within
a given model to produce the final ranking, we do
not need the closed vocabulary requirement.
5.2 Lexical Simplification Results
Figure 5 shows the kappa rank scores for the
simple-only, normal-only and combined models.
As with the perplexity results, for similar amounts
of data the simple-only model performs better than
the normal-only model. We also again see that the
performance difference between the two models
grows as the amount of data increases. However,
1541
 0.24
 0.26
 0.28
 0.3
 0.32
 0.34
 0.36
0.5M 1M 1.5M 2M 2.5M
ka
pp
a 
ra
nk
 s
co
re
number of additional normal sentences
normal-only
simple-100k+normal
simple-150k+normal
simple-200k+normal
simple-250k+normal
simple-300k+normal
simple-350k+normal
simple-ALL+normal
Figure 6: Kappa rank scores for models trained
with varying amounts of simple data combined
with increasing amounts of normal data.
unlike the perplexity results, simply appending ad-
ditional normal data to the entire simple data set
does not improve the performance of the lexical
simplifier.
To determine if additional normal data im-
proves the performance for models trained on
smaller amounts of simple data, Figure 6 shows
the kappa rank scores for models trained on differ-
ent amounts of simple data as additional normal
data is added. For smaller amounts of simple data
adding normal data does improve the kappa rank
score. For example, a language model trained with
100K simple sentences achieves a score of 0.246
and is improved by almost 40% to 0.344 by adding
all of the additional normal data. Even the perfor-
mance of a model trained with 300K simple sen-
tences is increased by 3% (0.01 improvement in
kappa rank score) by adding normal data.
5.3 Language Model Adaptation
The results in the previous section show that
adding normal data to a simple data set can im-
prove the lexical simplifier if the amount of simple
data is limited. To investigate this benefit further,
we again generated linearly interpolated language
models between the simple-only model and the
normal-only model. Figure 7 shows results for the
same experimental design as Figure 6 with vary-
ing amounts of simple and normal data, however,
rather than appending the normal data we trained
the models separately and created a linearly inter-
polated model as described in Section 4.3. The
best lambda was chosen based on a linear search
optimized on the SemEval 2012 development set.
For all starting amounts of simple data, interpo-
 0.24
 0.27
 0.3
 0.33
 0.36
 0.39
 0.42
0.5M 1M 1.5M 2M 2.5M
ka
pp
a 
ra
nk
 s
co
re
number of additional normal sentences
simple-100k
simple-150k
simple-200k
simple-250k
simple-300k
simple-350k
simple-ALL
Figure 7: Kappa rank scores for linearly inter-
polated models between simple-only and normal-
only models trained with varying amounts of sim-
ple and normal data.
lating the simple model with the normal model re-
sults in a large increase in the kappa rank score.
Combining the model trained on all the simple
data with the model trained on all the normal data
achieves a score of 0.419, an improvement of 23%
over the model trained on only simple data. Al-
though our goal was not to create the best lexical
simplification system, this approach would have
ranked 6th out of 11 submitted systems in the
SemEval 2012 competition (Specia et al, 2012).
Interestingly, although the performance of the
simple-only models varied based on the amount of
simple data, when these models are interpolated
with a model trained on normal data, the perfor-
mance tended to converge. This behavior is also
seen in Figure 6, though to a lesser extent. This
may indicate that for some tasks like lexical sim-
plification, only a modest amount of simple data is
required when combining with additional normal
data to achieve reasonable performance.
6 Why Does Unsimplified Data Help?
For both the perplexity experiments and the lexi-
cal simplification experiments, utilizing additional
normal data resulted in large performance im-
provements; using all of the simple data available,
performance is still significantly improved when
combined with normal data. In this section, we
investigate why the additional normal data is ben-
eficial for simple language modeling.
6.1 More n-grams
Intuitively, adding normal data provides additional
English data to train on. Most language models are
1542
Perplexity test data Lexical simplification
simple normal % inc. simple normal % inc.
1-grams 0.85 0.93 9.4% 0.74 0.78 6.2%
2-grams 0.66 0.82 24% 0.34 0.54 56%
3-grams 0.39 0.57 46% 0.088 0.19 117%
Table 3: Proportion of n-grams in the test sets that
occur in the simple and normal training data sets.
trained using a smoothed version of the maximum
likelihood estimate for an n-gram. For trigrams,
this is:
p(a|bc) = count(abc)count(bc)
where count(?) is the number of times the n-gram
occurs in the training corpus. For interpolated
and backoff n-gram models, these counts are
smoothed based on the probabilities of lower or-
der n-gram models, which are in-turn calculated
based on counts from the corpus.
We hypothesize that the key benefit of addi-
tional normal data is access to more n-gram counts
and therefore better probability estimation, partic-
ularly for n-grams in the simple corpus that are
unseen or have low frequency. For n-grams that
have never been seen before, the normal data pro-
vides some estimate from English text. This is
particularly important for unigrams (i.e. words)
since there is no lower order model to gain infor-
mation from and most language models assume a
uniform prior on unseen words, treating them all
equally. For n-grams that have been seen but are
rare, the additional normal data can help provide
better probability estimates. Because frequencies
tend to follow a Zipfian distribution, these rare
n-grams make up a large portion of n-grams in
real data (Ha et al, 2003).
To partially validate this hypothesis, we exam-
ined the n-gram overlap between the n-grams in
the training data and the n-grams in the test sets
from the two tasks. Table 3 shows the percentage
of unigrams, bigrams and trigrams from the two
test sets that are found in the simple and normal
training data.
For all n-gram sizes the normal data contained
more test set n-grams than the simple data. Even
at the unigram level, the normal data contained
significantly more of the test set unigrams than the
simple data. On the perplexity data set, the 9.4%
increase in word occurrence between the simple
and normal data set represents an over 50% reduc-
tion in the number of out of vocabulary words. For
Perplexity test data Lexical simplification
simple + % inc. over simple + % inc. over
normal normal normal normal
1-grams 0.93 0.2% 0.78 0.0%
2-grams 0.83 0.8% 0.54 1.1%
3-grams 0.58 2.5% 0.20 2.6%
Table 4: Proportion of n-grams in the test sets that
occur in the combination of both the simple and
normal data.
larger n-grams, the difference between the simple
and normal data sets are even more pronounced.
On the lexical simplification data the normal data
contained more than twice as many test trigrams
as the simple data. These additional n-grams al-
low for better probability estimates on the test data
and therefore better performance on the two tasks.
6.2 The Role of Normal Data
Estimation of rare events is one component of lan-
guage model performance, but other factors also
impact performance. Table 4 shows the test set
n-gram overlap on the combined data set of simple
and normal data. Because the simple and normal
data come from the same content areas, the simple
data provides little additional coverage if the nor-
mal data is already used. For example, adding the
simple data to the normal data only increases the
number of seen unigrams by 0.2%, representing
only about 600 new words. However, the exper-
iments above showed the combined models per-
formed much better than models trained only on
normal data.
This discrepancy highlights the key problem
with normal data: it is out-of-domain data. While
it shares some characteristics with the simple data,
it represents a different distribution over the lan-
guage. To make this discrepancy more explicit,
we created a sentence aligned data set by align-
ing the simple and normal articles using the ap-
proach from Coster and Kauchak (2011b). This
approach has been previously used for aligning
English Wikipedia and Simple English Wikipedia
with reasonable accuracy. The resulting data set
contains 150K aligned simple-normal sentence
pairs.
Figure 8 shows the perplexity scores for lan-
guage models trained on this data set. Because
the data is aligned and therefore similar, we see
the perplexity curves run parallel to each other as
more data is added. However, even though these
1543
 100
 150
 200
 250
 300
25K 50K 75K 100K 125K 150K
pe
rp
le
xi
ty
number of sentences
simple-only-aligned
normal-only-aligned
Figure 8: Language model perplexities for mod-
els trained on increasing data sizes for a simple-
normal sentence aligned data set.
sentences represent the same content, the language
use is different between simple and normal and the
normal data performs consistently worse.
6.3 A Balance Between Simple and Normal
Examining the optimal lambda values for the lin-
early interpolated models also helps understand
the role of the normal data. On the perplexity task,
the best perplexity results were obtained with a
lambda of 0.5, or an equal weighting between the
simple and normal models. Even though the nor-
mal data contained six times as many sentences
and nine times as many words, the best model-
ing performance balanced the quality of the simple
model with the coverage of the normal model.
For the simplification task, the optimal lambda
value determined on the development set was 0.98,
with a very strong bias towards the simple model.
Only when the simple model did not provide dif-
ferentiation between lexical choices will the nor-
mal model play a role in selecting the candidates.
For the lexical simplification task, the role of the
normal model is even more clear: to handle rare
occurrences not covered by the simple model and
to smooth the simple model estimates.
7 Conclusions and Future Work
In the experiments above we have shown that on
two different tasks utilizing additional normal data
improves the performance of simple English lan-
guage models. On the perplexity task, the com-
bined model achieved a performance improvement
of 23% over the simple-only model and on the
lexical simplification task, the combined model
achieved a 24% improvement. These improve-
ments are achieved over a simple-only model that
uses all simple English data currently available in
this domain.
For both tasks, the best improvements were
seen when using language model adaptation tech-
niques, however, the adaptation results also indi-
cated that the role of normal data is partially task
dependent. On the perplexity task, the best results
were achieved with an equal weighting between
the simple-only and normal-only model. How-
ever, on the lexical simplification task, the best
results were achieved with a very strong bias to-
wards the simple-only model. For other simplifi-
cation tasks, the optimal parameters will need to
be investigated.
For many of the experiments, combining a
smaller amount of simple data (50K-100K sen-
tences) with normal data achieved results that were
similar to larger simple data set sizes. For ex-
ample, on the lexical simplification task, when
using a linearly interpolated model, the model
combining 100K simple sentences with all the
normal data achieved comparable results to the
model combining all the simple sentences with all
the normal data. This is encouraging for other
monolingual domains such as text compression
or text simplification in non-English languages
where less data is available.
There are still a number of open research ques-
tions related to simple language modeling. First,
further experiments with larger normal data sets
are required to understand the limits of adding
out-of-domain data. Second, we have only uti-
lized data from Wikipedia for normal text. Many
other text sources are available and the impact of
not only size, but also of domain should be in-
vestigated. Third, it still needs to be determined
how language model performance will impact
sentence-level and document-level simplification
approaches. In machine translation, improved
language models have resulted in significant im-
provements in translation performance (Brants et
al., 2007). Finally, in this paper we only in-
vestigated linearly interpolated language models.
Many other domain adaptations techniques exist
and may produce language models with better per-
formance.
1544
References
Michiel Bacchiani and Brian Roark. 2003. Unsuper-
vised language model adaptation. In Proceedings of
ICASSP.
Michele Banko, Vibhu Mittal, and Michael Witbrock.
2000. Headline generation based on statistical trans-
lation. In Proceedings of ACL.
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: Review and perspectives. Speech
Communication.
Or Biran, Samuel Brody, and Noem?ie Elhadad. 2011.
Putting it simply: A context-aware approach to lexi-
cal simplification. In Proceedings of ACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP.
Raman Chandrasekar and Bangalore Srinivas. 1997.
Automatic induction of rules for text simplification.
Knowledge Based Systems.
Stanley Chen, Douglas Beeferman, and Ronald Rosen-
feld. 1998. Evaluation metrics for language mod-
els. In DARPA Broadcast News Transcription and
Understanding Workshop.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research.
William Coster and David Kauchak. 2011a. Learning
to simplify sentences using Wikipedia. In Proceed-
ings of Text-To-Text Generation.
William Coster and David Kauchak. 2011b. Simple
English Wikipedia: A new text simplification task.
In Proceedings of ACL.
Hal Daume and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings
of ACL.
Christopher Cieri David Graff. 2003. En-
glish gigaword. http://www.ldc.
upenn.edu/Catalog/CatalogEntry.
jsp?catalogId=LDC2003T05.
Carsten Eickhoff, Pavel Serdyukov, and Arjen P.
de Vries. 2010. Web page classification on child
suitability. In Proceedings of CIKM.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Proceedings of HLT-NAACL.
Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F. J.
Smith. 2003. Extension of Zipf?s law to word and
character n-grams for English and Chinese. Compu-
tational Linguistics and Chinese Language Process-
ing.
Bo-June Hsu. 2007. Generalized linear interpolation
of language models. In IEEE Workshop on ASRU.
Frederick Jelinek and Robert Mercer. 1980. Interpo-
lated estimation of markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
ter Recognition in Practice.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics.
Gondy Leroy, James E. Endicott, Obay Mouradi, David
Kauchak, and Melissa Just. 2012. Improving per-
ceived and actual text difficulty for health informa-
tion consumers using semi-automated methods. In
American Medical Informatics Association (AMIA)
Fall Symposium.
Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple Wikipedia: A cogitation in ascertain-
ing abecedarian language. In Proceedings of
HLT/NAACL Workshop on Computation Linguistics
and Writing.
Tadashi Nomoto. 2009. A comparison of model free
versus model intensive approaches to sentence com-
pression. In Proceedings of EMNLP.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering.
Ronald Rosenfeld. 1996. A maximum entropy ap-
proach to adaptive statistical language modeling.
Computer, Speech and Language.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-
cea. 2012. Semeval-2012 task 1: English lexical
simplification. In Joint Conference on Lexical and
Computerational Semantics (*SEM).
Lucia Specia. 2010. Translating from complex to sim-
plified sentences. In Proceedings of Computational
Processing of the Portuguese Language.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Proceedings of ICSLP.
Hisami Suzuki and Jianfeng Gao. 2005. A compara-
tive study on language model adaptation techniques.
In Proceedings of EMNLP.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL.
Ken Urano. 2000. Lexical simplification and elabo-
ration: Sentence comprehension and incidental vo-
cabulary acquisition. Master?s thesis, University of
Hawaii.
1545
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of EMNLP.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of ACL.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Proceedings of NAACL.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of COLING.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of ICCL.
1546
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 458?463,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Learning a Lexical Simplifier Using Wikipedia
Colby Horn, Cathryn Manduca and David Kauchak
Computer Science Department
Middlebury College
{chorn,cmanduca,dkauchak}@middlebury.edu
Abstract
In this paper we introduce a new lexical
simplification approach. We extract over
30K candidate lexical simplifications by
identifying aligned words in a sentence-
aligned corpus of English Wikipedia with
Simple English Wikipedia. To apply these
rules, we learn a feature-based ranker us-
ing SVM
rank
trained on a set of labeled
simplifications collected using Amazon?s
Mechanical Turk. Using human simplifi-
cations for evaluation, we achieve a preci-
sion of 76% with changes in 86% of the
examples.
1 Introduction
Text simplification is aimed at reducing the read-
ing and grammatical complexity of text while re-
taining the meaning (Chandrasekar and Srinivas,
1997). Text simplification techniques have a broad
range of applications centered around increasing
data availability to both targeted audiences, such
as children, language learners, and people with
cognitive disabilities, as well as to general readers
in technical domains such as health and medicine
(Feng, 2008).
Simplifying a text can require a wide range
of transformation operations including lexical
changes, syntactic changes, sentence splitting,
deletion and elaboration (Coster and Kauchak,
2011; Zhu et al, 2010). In this paper, we ex-
amine a restricted version of the text simplifica-
tion problem, lexical simplification, where text is
simplified by substituting words or phrases with
simpler variants. Even with this restriction, lexi-
cal simplification techniques have been shown to
positively impact the simplicity of text and to im-
prove reader understanding and information reten-
tion (Leroy et al, 2013). Additionally, restrict-
ing the set of transformation operations allows for
more straightforward evaluation than the general
simplification problem (Specia et al, 2012).
Most lexical simplification techniques rely on
transformation rules that change a word or phrase
into a simpler variant with similar meaning (Bi-
ran et al, 2011; Specia et al, 2012; Yatskar et
al., 2010). Two main challenges exist for this type
of approach. First, the lexical focus of the trans-
formation rules makes generalization difficult; a
large number of transformation rules is required to
achieve reasonable coverage and impact. Second,
rules do not apply in all contexts and care must be
taken when performing lexical transformations to
ensure local cohesion, grammaticality and, most
importantly, the preservation of the original mean-
ing.
In this paper, we address both of these issues.
We leverage a data set of 137K aligned sentence
pairs between English Wikipedia and Simple En-
glish Wikipedia to learn simplification rules. Pre-
vious approaches have used unaligned versions of
Simple English Wikipedia to learn rules (Biran et
al., 2011; Yatskar et al, 2010), however, by using
the aligned version we are able to learn a much
larger rule set.
To apply lexical simplification rules to a new
sentence, a decision must be made about which, if
any, transformations should be applied. Previous
approaches have used similarity measures (Biran
et al, 2011) and feature-based approaches (Specia
et al, 2012) to make this decision. We take the lat-
ter approach and train a supervised model to rank
candidate transformations.
2 Problem Setup
We learn lexical simplification rules that consist
of a word to be simplified and a list of candidate
simplifications:
w ? c
1
, c
2
, ..., c
m
Consider the two aligned sentence pairs in Table
458
The first school was established in 1857.
The first school was started in 1857.
The district was established in 1993 by merging
the former districts of Bernau and Eberswalde.
The district was made in 1993 by joining the
old districts of Bernau and Eberswalde.
Table 1: Two aligned sentence pairs. The bottom
sentence is a human simplified version of the top
sentence. Bold words are candidate lexical simpli-
fications.
1. The bottom sentence of each pair is a simpli-
fied variant of the top sentence. By identifying
aligned words within the aligned sentences, can-
didate lexical simplifications can be learned. The
bold words show two such examples, though other
candidates exist in the bottom pair. By examining
aligned sentence pairs we can learn a simplifica-
tion rule. For example, we might learn:
established? began,made, settled, started
Given a sentence s
1
, s
2
, ..., s
n
, a simplification
rule applies if the left hand side of the rule can be
found in the sentence (s
i
= w, for some i). If a
rule applies, then a decision must be made about
which, if any, of the candidate simplifications
should be substituted for the word w to simplify
the sentence. For example, if we were attempting
to simplify the sentence
The ACL was established in 1962.
using the simplification rule above, some of the
simplification options would not apply because
of grammatical constraints, e.g. began, while
others would not apply for semantic reasons, e.g.
settled. This does not mean that these are not
good simplifications for established since in other
contexts, they might be appropriate. For example,
in the sentence
The researcher established a new paper
writing routine.
began is a reasonable option.
3 Learning a Lexical Simplifier
We break the learning problem into two steps: 1)
learn a set of simplification rules and 2) learn a
ranking function for determining the best simpli-
fication candidate when a rule applies. Each of
these steps are outlined below.
3.1 Rule Extraction
To extract the set of simplification rules, we use
a sentence-aligned data set of English Wikipedia
sentences (referred to as normal) aligned to Sim-
ple English Wikipedia sentences (referred to as
simple) (Coster and Kauchak, 2011). The data set
contains 137K such aligned sentence pairs.
Given a normal sentence and the corresponding
aligned simple sentence, candidate simplifications
are extracted by identifying a word in the simple
sentence that corresponds to a different word in the
normal sentence. To identify such pairs, we au-
tomatically induce a word alignment between the
normal and simple sentence pairs using GIZA++
(Och and Ney, 2000). Words that are aligned are
considered as possible candidates for extraction.
Due to errors in the sentence and word alignment
processes, not all words that are aligned are actu-
ally equivalent lexical variants. We apply the fol-
lowing filters to reduce such spurious alignments:
? We remove any pairs where the normal word
occurs in a stoplist. Stoplist words tend to be
simple already and stoplist words that are be-
ing changed are likely either bad alignments
or are not simplifications.
? We require that the part of speeches (POS)
of the two words be the same. The parts of
speech were calculated based on a full parse
of the sentences using the Berkeley parser
(Petrov and Klein, 2007).
? We remove any candidates where the POS
is labeled as a proper noun. In most cases,
proper nouns should not be simplified.
All other aligned word pairs are extracted. To
generate the simplification rules, we collect all
candidate simplifications (simple words) that are
aligned to the same normal word.
As mentioned before, one of the biggest chal-
lenges for lexical simplification systems is gen-
eralizability. To improve the generalizability of
the extracted rules, we add morphological variants
of the words in the rules. For nouns, we include
both singular and plural variants. For verbs, we
expand to all inflection variants. The morpholog-
ical changes are generated using MorphAdorner
(Burns, 2013) and are applied symmetrically: any
change to the normal word is also applied to the
corresponding simplification candidates.
459
3.2 Lexical Simplification as a Ranking
Problem
A lexical simplification example consists of three
parts: 1) a sentence, s
1
, s
2
, ..., s
n
, 2), a word in
that sentence, s
i
, and 3) a list of candidate sim-
plifications for s
i
, c
1
, c
2
, ..., c
m
. A labeled exam-
ple is an example where the rank of the candidate
simplifications has been specified. Given a set of
labeled examples, the goal is to learn a ranking
function that, given an unlabeled example (exam-
ple without the candidate simplifications ranked),
specifies a ranking of the candidates.
To learn this function, features are extracted
from a set of labeled lexical simplification exam-
ples. These labeled examples are then used to train
a ranking function. We use SVM
rank
(Joachims,
2006), which uses a linear support vector machine.
Besides deciding which of the candidates is
most applicable in the context of the sentence,
even if a rule applies, we must also decide if
any simplification should occur. For example,
there may be an instance where none of the can-
didate simplifications are appropriate in this con-
text. Rather than viewing this as a separate prob-
lem, we incorporate this decision into the ranking
problem by adding w as a candidate simplifica-
tion. For each rule, w ? c
1
, c
2
, ..., c
m
we add one
additional candidate simplification which does not
change the sentence, w ? c
1
, c
2
, ..., c
m
, w. If w is
ranked as the most likely candidate by the ranking
algorithm, then the word is not simplified.
3.2.1 Features
The role of the features is to capture information
about the applicability of the word in the context
of the sentence as well as the simplicity of the
word. Many features have been suggested previ-
ously for use in determining the simplicity of a
word (Specia et al, 2012) and for determining if
a word is contextually relevant (Biran et al, 2011;
McCarthy and Navigli, 2007). Our goal for this
paper is not feature exploration, but to examine
the usefulness of a general framework for feature-
based ranking for lexical simplification. The fea-
tures below represent a first pass at candidate fea-
tures, but many others could be explored.
Candidate Probability
p(c
i
|w): in the sentence-aligned Wikipedia data,
when w is aligned to some candidate simplifica-
tion, what proportion of the time is that candidate
c
i
.
Frequency
The frequency of a word has been shown to cor-
relate with the word?s simplicity and with peo-
ple?s knowledge of that word (Leroy and Kauchak,
2013). We measured a candidate simplification?s
frequency in two corpora: 1) Simple English
Wikipedia and 2) the web, as measured by the un-
igram frequency from the Google n-gram corpus
(Brants and Franz, 2006).
Language Models
n-gram language models capture how likely a par-
ticular sequence is and can help identify candidate
simplifications that are not appropriate in the con-
text of the sentence. We included features from
four different language models trained on four dif-
ferent corpora: 1) Simple English Wikipedia, 2)
English Wikipedia, 3) Google n-gram corpus and
4) a linearly interpolated model between 1) and
2) with ? = 0.5, i.e. an even blending. We
used the SRI language modeling toolkit (Stolcke,
2002) with Kneser-Kney smoothing. All models
were trigram language models except the Google
n-gram model, which was a 5-gram model.
Context Frequency
As another measure of the applicability of a can-
didate in the context of the sentence, we also cal-
culate the frequency in the Google n-grams of the
candidate simplification in the context of the sen-
tence with context windows of one and two words.
If the word to be substituted is at position i in the
sentence (w = s
i
), then the one word window
frequency for simplification c
j
is the trigram fre-
quency of s
i?1
c
j
s
i+1
and the two word window
the 5-gram frequency of s
i?2
s
i?1
c
j
s
i+1
s
i+2
.
4 Data
For training and evaluation of the models, we col-
lected human labelings of 500 lexical simplifica-
tion examples using Amazon?s Mechanical Turk
(MTurk)
1
. MTurk has been used extensively for
annotating and evaluating NLP tasks and has been
shown to provide data that is as reliable as other
forms of human annotation (Callison-Burch and
Dredze, 2010; Zaidan and Callison-Burch, 2011).
Figure 1 shows an example of the task we asked
annotators to do. Given a sentence and a word
to be simplified, the task is to suggest a simpler
variant of that word that is appropriate in the con-
text of the sentence. Candidate sentences were se-
1
https://www.mturk.com/
460
Enter a simpler word that could be substituted for the red, bold word in the sentence. A simpler
word is one that would be understood by more people or people with a lower reading level (e.g.
children).
Food is procured with its suckers and then crushed using its tough ?beak? of chitin.
Figure 1: Example task setup on MTurk soliciting lexical simplifications from annotators.
lected from the sentence-aligned Wikipedia cor-
pus where a word in the normal sentence is be-
ing simplified to a different word in the simple
sentence, as identified by the automatically in-
duced word alignment. The normal sentence and
the aligned word were then selected for annota-
tion. These examples represent words that other
people (those that wrote/edited the Simple En-
glish Wikipedia page) decided were difficult and
required simplification.
We randomly selected 500 such sentences and
collected candidate simplifications from 50 people
per sentence, for a total of 25,000 annotations. To
participate in the annotation process, we required
that the MTurk workers live in the U.S. (for En-
glish proficiency) and had at least a 95% accep-
tance rate on previous tasks.
The simplifications suggested by the annotators
were then tallied and the resulting list of simpli-
fications with frequencies provides a ranking for
training the candidate ranker. Table 2 shows the
ranked list of annotations collected for the exam-
ple in Figure 1. This data set is available online.
2
Since these examples were selected from En-
glish Wikipedia they, and the corresponding
aligned Simple English Wikipedia sentences, were
removed from all resources used during both the
rule extraction and the training of the ranker.
5 Experiments
5.1 Other Approaches
We compared our lexical simplification approach
(rank-simplify) to two other approaches. To un-
derstand the benefit of the feature-based ranking
algorithm, we compared against a simplifier that
uses the same rule set, but ranks the candidates
only based on their frequency in Simple English
Wikipedia (frequency). This is similar to base-
lines used in previous work (Biran et al, 2011).
To understand how our extracted rules com-
pared to the rules extracted by Biran et al, we
2
http://www.cs.middlebury.edu/
?
dkauchak/simplification/
used their rules with our ranking approach (rank-
Biran). Their approach also extracts rules from
a corpus of English Wikipedia and Simple En-
glish Wikipedia, however, they do not utilize a
sentence-aligned version and instead rely on con-
text similarity measures to extract their rules.
5.2 Evaluation
We used the 500 ranked simplification examples to
train and evaluate our approach. We employed 10-
fold cross validation for all experiments, training
on 450 examples and testing on 50.
We evaluated the models with four different
metrics:
precision: Of the words that the system changed,
what percentage were found in any of the human
annotations.
precision@k: Of the words that the system
changed, what percentage were found in the top
k human annotations, where the annotations were
ranked by response frequency. For example, if we
were calculating the precision@1 for the example
in Table 2, only ?obtained? would be considered
correct.
accuracy: The percentage of the test examples
where the system made a change to one of the
annotations suggested by the human annotators.
Note that unlike precision, if the system does not
suggest a change to a word that was simplified it
still gets penalized.
changed: The percentage of the test examples
where the system suggested some change (even if
it wasn?t a ?correct? change).
5.3 Results
Table 3 shows the precision, accuracy and percent
changed for the three systems. Based on all three
metrics, our system achieves the best results. Al-
though the rules generated by Biran et al have rea-
sonable precision, they suffer from a lack of cov-
erage, only making changes on about 5% of the
461
word frequency word frequency word frequency
obtained 17 made 2 secured 1
gathered 9 created 1 found 1
gotten 8 processed 1 attained 1
grabbed 4 received 1 procured 1
acquired 2 collected 1 aquired 1
Table 2: Candidate simplifications generated using MTurk for the examples in Figure 1. The frequency
is the number of annotators that suggested that simplification.
precision accuracy changed
frequency 53.9% 46.1% 84.9%
rank-Biran 71.4% 3.4% 5.2%
rank-simplify 76.1% 66.3% 86.3%
Table 3: Precision, accuracy and percent changed
for the three systems, averaged over the 10 folds.
examples. For our approach, the extracted rules
had very good coverage, applying in over 85% of
the examples.
This difference in coverage can be partially at-
tributed to the number of rules learned. We learned
simplifications for 14,478 words with an average
of 2.25 candidate simplifications per word. In con-
trast, the rules from Biran et al only had simpli-
fications for 3,598 words with an average of 1.18
simplifications per word.
The precision of both of the approaches that
utilized the SVM candidate ranking were sig-
nificantly better than the frequency-based ap-
proach. To better understand the types of sug-
gestions made by the systems, Figure 2 shows the
precision@k for increasing k. On average, over
the 500 examples we collected, people suggested
12 different simplifications, though this varied de-
pending on the word in question and the sentence.
As such, at around k=12, the precision@k of most
of the systems has almost reached the final preci-
sion. However, even at k = 5, which only counts
correct an answer in the top 5 human suggested
results, our system still achieved a precision of
around 67%.
6 Future Work
In this paper we have introduced a new rule ex-
traction algorithm and a new feature-based rank-
ing approach for applying these rules in the con-
text of different sentences. The number of rules
learned is an order of magnitude larger than any
previous lexical simplification approach and the
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0  5  10  15  20  25  30  35  40  45  50
pr
ec
isi
on
k
rank-simplify
rank-Biran
frequency
Figure 2: Precision@k for varying k for the three
different approaches averaged over the 10 folds.
quality of the resulting simplifications after apply-
ing these rules is better than previous approaches.
Many avenues exist for improvement and for
better understanding how well the current ap-
proach works. First, we have only explored a
small set of possible features in the ranking algo-
rithm. Additional improvements could be seen by
incorporating a broader feature set. Second, more
analysis needs to be done to understand the quality
of the produced simplifications and their impact on
the simplicity of the resulting sentences. Third, the
experiments above assume that the word to be sim-
plified has already been identified in the sentence.
This identification step also needs to be explored
to implement a sentence-level simplifier using our
approach. Fourth, the ranking algorithm can be
applied to most simplification rules (e.g. we ap-
plied the ranking approach to the rules obtained
by Biran et al (2011)). We hope to explore other
approaches for increasing the rule set by incorpo-
rating other rule sources and other rule extraction
techniques.
462
References
Or Biran, Samuel Brody, and Noe ?mie Elhadad. 2011.
Putting it simply: A context-aware approach to lexi-
cal simplification. In Proceedings of ACL.
Thorsten Brants and Alex Franz. 2006. Web 1T
5-gram version 1. Linguistic Data Consortium,
Philadelphia.
Philip R. Burns. 2013. Morphadorner v2: A Java li-
brary for the morphological adornment of english
language texts.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with Amazon?s Me-
chanical Turk. In Proceedings of NAACL-HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
Raman Chandrasekar and Bangalore Srinivas. 1997.
Automatic induction of rules for text simplification.
In Knowledge Based Systems.
William Coster and David Kauchak. 2011. Simple En-
glish Wikipedia: A new text simplification task. In
Proceedings of ACL.
Lijun Feng. 2008. Text simplification: A survey.
CUNY Technical Report.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of KDD.
Gondy Leroy and David Kauchak. 2013. The effect
of word familiarity on actual and perceived text dif-
ficulty. Journal of American Medical Informatics
Association.
Gondy Leroy, James E. Endicott, David Kauchak,
Obay Mouradi, and Melissa Just. 2013. User evalu-
ation of the effects of a text simplification algorithm
using term familiarity on perception, understanding,
learning, and information retention. Journal of Med-
ical Internet Research (JMIR).
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of SEMEVAL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HTL-
NAACL.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-
cea. 2012. Semeval-2012 task 1: English lexical
simplification. In Joint Conference on Lexical and
Computerational Semantics (*SEM).
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Statistical Language Pro-
cessing.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In NAACL/HLT.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proceedings of ACL.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of ICCL.
463
Workshop on Monolingual Text-To-Text Generation, pages 1?9,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1?9,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Learning to Simplify Sentences Using Wikipedia
William Coster
Computer Science Department
Pomona College
wpc02009@pomona.edu
David Kauchak
Computer Science Department
Pomona College
dkauchak@cs.pomona.edu
Abstract
In this paper we examine the sentence sim-
plification problem as an English-to-English
translation problem, utilizing a corpus of
137K aligned sentence pairs extracted by
aligning English Wikipedia and Simple En-
glish Wikipedia. This data set contains the
full range of transformation operations includ-
ing rewording, reordering, insertion and dele-
tion. We introduce a new translation model
for text simplification that extends a phrase-
based machine translation approach to include
phrasal deletion. Evaluated based on three
metrics that compare against a human refer-
ence (BLEU, word-F1 and SSA) our new ap-
proach performs significantly better than two
text compression techniques (including T3)
and the phrase-based translation system with-
out deletion.
1 Introduction
In this paper we examine the sentence simplifica-
tion problem: given an English sentence we aim to
produce a simplified version of that sentence with
simpler vocabulary and sentence structure while
preserving the main ideas in the original sentence
(Feng, 2008). The definition what a ?simple? sen-
tence is can vary and represents a spectrum of com-
plexity and readability. For concreteness, we use
Simple English Wikipedia1 as our archetype of sim-
plified English. Simple English Wikipedia arti-
cles represent a simplified version of traditional En-
glish Wikipedia articles. The main Simple English
1http://simple.wikipedia.org
Wikipedia page outlines general guidelines for cre-
ating simple articles:
? Use Basic English vocabulary and shorter sen-
tences. This allows people to understand nor-
mally complex terms or phrases.
? Simple does not mean short. Writing in Simple
English means that simple words are used. It
does not mean readers want basic information.
Articles do not have to be short to be simple;
expand articles, add details, but use basic vo-
cabulary.
The data set we examine contains aligned sen-
tence pairs of English Wikipedia2 with Simple En-
glish Wikipedia (Coster and Kauchak, 2011; Zhu
et al, 2010). We view the simplification problem
as an English-to-English translation problem: given
aligned sentence pairs consisting of a normal, un-
simplified sentence and a simplified version of that
sentence, the goal is to learn a sentence simplifica-
tion system to ?translate? from normal English to
simplified English. This setup has been successfully
employed in a number of text-to-text applications in-
cluding machine translation (Och and Ney, 2003),
paraphrasing (Wubben et al, 2010) and text com-
pression (Knight and Marcu, 2002; Cohn and Lap-
ata, 2009).
Table 1 shows example sentence pairs from the
aligned data set. One of the challenges of text sim-
plification is that, unlike text compression where the
emphasis is often on word deletion, text simplifica-
2http://en.wikipedia.org/
1
a. Normal: Greene agreed that she could earn more by breaking away from 20th Century Fox.
Simple: Greene agreed that she could earn more by leaving 20th Century Fox.
b. Normal: The crust and underlying relatively rigid mantle make up the lithosphere.
Simple: The crust and mantle make up the lithosphere.
c. Normal: They established themselves here and called that port Menestheus?s port.
Simple: They called the port Menestheus?s port.
d. Normal: Heat engines are often confused with the cycles they attempt to mimic.
Simple: Real heat engines are often confused with the ideal engines or cycles they attempt
to mimic.
e. Normal: In 1962 , Steinbeck received the Nobel Prize for Literature.
Simple: Steinbeck won the Nobel Prize in Literature in 1962.
Table 1: Example aligned sentences from English Wikipedia and Simple English Wikipedia. Normal refers an English
Wikipedia sentence and Simple to a corresponding Simple English Wikipedia sentence.
tion involves the full range of transformation opera-
tions:
deletion: ?underlying relatively rigid? in b., ?es-
tablished themselves here and? in c. and the comma
in d.
rewording: ?breaking away from? ? ?leaving? in
a. and ?received? ? ?won? in e.
reordering: in e. ?in 1962? moves from the be-
ginning of the sentence to the end.
insertion: ?ideal engines or? in d.
Motivated by the need to model all of these dif-
ferent transformations, we chose to extend a statis-
tical phrase-based translation system (Koehn et al,
2007). In particular, we added phrasal deletion to the
probabilistic translation model. This addition broad-
ens the deletion capabilities of the system since the
base model only allows for deletion within a phrase.
As Kauchak and Coster (2011) point out, deletion is
a frequently occurring phenomena in the simplifica-
tion data.
There are a number of benefits of text simplifica-
tion research. Much of the current text data avail-
able including Wikipedia, news articles and most
web pages are written with an average adult reader
as the target audience. Text simplification can make
this data available to a broader range of audiences in-
cluding children, language learners, the elderly, the
hearing impaired and people with aphasia or cogni-
tive disabilities (Feng, 2008; Carroll et al, 1998).
Text simplification has also been shown to improve
the performance of other natural language process-
ing applications including semantic role labeling
(Vickrey and Koller, 2008) and relation extraction
(Miwa et al, 2010).
2 Previous Work
Most previous work in the area of sentence simpli-
fication has not been from a data-driven perspec-
tive. Feng (2008) gives a good historical overview
of prior text simplification systems including early
rule-based approaches (Chandrasekar and Srinivas,
1997; Carroll et al, 1998; Canning et al, 2000) and
a number of commercial approaches. Vickrey and
Koller (2008) and Miwa et al (2010) employ text
simplification as a preprocessing step, though both
use manually generated rules.
Our work extends recent work by Zhu et al
(2010) that also examines Wikipedia/Simple En-
glish Wikipedia as a data-driven, sentence simpli-
fication task. They propose a probabilistic, syntax-
based approach to the problem and compare against
a baseline of no simplification and a phrase-based
translation approach. They show improvements
with their approach on target-side only metrics in-
cluding Flesch readability and n-gram language
model perplexity, but fail to show improvements for
their approach on evaluation metrics that compare
against a human reference simplification. In con-
trast, our approach achieves statistically significant
improvements for three different metrics that com-
pare against human references.
Sentence simplification is closely related to the
2
problem of sentence compression, another English-
to-English translation task. Knight and Marcu
(2002) were one of the first to formalize text
compression as a data-driven problem and pro-
posed a probabilistic, noisy-channel model and de-
cision tree-based model for compression. Galley
and McKeown (2007) show improvements to the
noisy-channel approach based on rule lexicaliza-
tion and rule Markovization. Recently, a number
of approaches to text compression have been pro-
posed that score transformation rules discrimina-
tively based on support vector machines (McDonald,
2006; Cohn and Lapata, 2009) and conditional ran-
dom fields (Nomoto, 2007; Nomoto, 2008) instead
of using maximum likelihood estimation. With the
exception of Cohn and Lapata (2009), all of these
text compression approaches make the simplifying
assumption that the compression process happens
only via word deletion. We provide comparisons
with some of these systems, however, for text sim-
plification where lexical changes and reordering are
frequent, most of these techniques are not appropri-
ate.
Our proposed approach builds upon approaches
employed in machine translation (MT). We intro-
duce a variant of a phrase-based machine translation
system (Och and Ney, 2003; Koehn et al, 2007) for
text simplification. Although MT systems that em-
ploy syntactic or hierarchical information have re-
cently shown improvements over phrase-based ap-
proaches (Chiang, 2010), our initial investigation
with syntactically driven approaches showed poorer
performance on the text simplification task and were
less robust to noise in the training data.
Both English Wikipedia and Simple English
Wikipedia have received recent analysis as a pos-
sible corpus by for both sentence compression and
simplification. Yamangil and Nelken (2008) exam-
ine the history logs of English Wikipedia to learn
sentence compression rules. Yatskar et al (2010)
learn a set of candidate phrase simplification rules
based on edit changes identified in both Wikipedias
revision histories, though they only provide a list
of the top phrasal rules and do not utilize them in
an end-to-end simplification system. Napoles and
Dredze (2010) provide an analysis of the differences
between documents in English Wikipedia and Sim-
ple English Wikipedia, though they do not view the
data set as a parallel corpus.
3 Text Simplification Corpus
Few data sets exist for text simplification and data
sets for the related task of sentence compression
are small, containing no more than a few thousand
aligned sentence pairs (Knight and Marcu, 2002;
Cohn and Lapata, 2009; Nomoto, 2009). For this pa-
per, we utilized a sentence-aligned corpus generated
by aligning English Wikipedia with Simple English
Wikipedia resulting in 137K aligned sentence pairs.
This data set is larger than any previously examined
for sentence simplification and orders of magnitude
larger than those previously examined for sentence
compression.
We give a brief overview of the corpus generation
process here. For more details and an analysis of the
data set, see (Coster and Kauchak, 2011). Through-
out this article we will refer to English Wikipedia
articles/sentences as normal and Simple English
Wikipedia articles as simple.
We aligned the normal and simple articles at the
document level based on exact match of the title and
then removed all article pairs that were stubs, dis-
ambiguation pages, meta-pages or only contained a
single line. Following a similar approach to pre-
vious monolingual alignment techniques (Barzilay
and Elhadad, 2003; Nelken and Shieber, 2006), we
then aligned each simple paragraph to any normal
paragraph that had a normalized TF-IDF cosine sim-
ilarity above a set threshold. These aligned para-
graphs were then aligned at the sentence level using
a dynamic programming approach, picking the best
sentence-level alignment from a combination of the
following sentence-level alignments:
? normal sentence inserted
? normal sentence deleted
? one normal sentence to one simple sentence
? two normal sentences to one simple sentence
? one normal sentence to two simple sentence
Following Nelken and Shieber (2006), we used TF-
IDF cosine similarity to measure the similarity be-
tween aligned sentences and only kept aligned sen-
tence pairs with a similarity threshold above 0.5. We
3
found this thresholding approach to be more intu-
itive than trying to adjust a skip (insertion or dele-
tion) penalty, which has also been proposed (Barzi-
lay and Elhadad, 2003).
4 Simplification Model
Given training data consisting of aligned normal-
simple sentence pairs, we aim to produce a trans-
lation system that takes as input a normal English
sentence and produces a simplified version of that
sentence. Motivated by the large number and im-
portance of lexical changes in the data set, we chose
to use a statistical phrase-based translation system.
We utilized a modified version of Moses, which was
originally developed for machine translation (Koehn
et al, 2007).
Moses employs a log-linear model, which can be
viewed as an extension of the noisy channel model
and combines a phrase-based translation model, an
n-gram language model, as well as a number of other
models/feature functions to identify the best transla-
tion/simplification. The key component of Moses
is the phrase-based translation model which decom-
poses the probability calculation of a normal sen-
tence simplifying to a simple sentence as the product
of individual phrase translations:
p(simple|normal) =
m?
i=1
p(s?i|n?i)
where each s?i is a phrase (one or more contigu-
ous words) in the simple sentence and s?1, s?2, ..., s?m
exactly cover the simple sentence. n?i are simi-
larly defined over the normal sentence. p(s?i|n?i)
denotes the probability of a normal phrase being
translated/simplified to the corresponding simpli-
fied phrase. These phrasal probabilities are ex-
tracted from the sentence pairs based on an EM-
learned word alignment using GIZA++ (Och and
Ney, 2000).
Phrase-based models in machine translation of-
ten require that both phrases in the phrasal prob-
abilities contain one or more words, since phrasal
deletion/insertion is rare and can complicate the de-
coding process. For text simplification, however,
phrasal deletion commonly occurs: 47% of the sen-
tence pairs contain deletions (Coster and Kauchak,
2011). To model this deletion, we relax the restric-
tion that the simple phrase must be non-empty and
include in the translation model probabilistic phrasal
deletion rules of the form p(NULL|n?i) allowing for
phrases to be deleted during simplification.
To learn these phrasal deletions within Moses,
we modify the original word alignment output from
GIZA++ before learning the phrase table entries in
two ways:
1. If one or more contiguous normal words are
unaligned in the original alignment, we align
them to NULL appropriately inserted on the
simple side
2. If a set of normal words N all align to a single
simple word s and there exists an n ? N where
n = s then for all n? ? N : n? 6= n we align
them to NULL.
This second modification has two main benefits.
Frequently, if a word occurs in both the normal and
simple sentence and it is aligned to itself, no other
words should be aligned to that word. As others
have noted, this type of spurious alignment is partic-
ularly prevalent with function words, which tend to
occur in many different contexts (Chen et al, 2009).
Second, even in situations where it may be appro-
priate for multiple words to align to a single word
(for example, in compound nouns, such as President
Obama ? Obama), removing the alignment of the
extra words, allows us to delete those words in other
contexts. We lose some specificity with this adap-
tation because some deletions can now occur inde-
pendent of context, however, empirically this modi-
fication provides more benefit than hindrance for the
model. We conjecture that the language model helps
avoid these problematic cases.
Table 2 shows excerpts from an example sentence
pair before the alignment alteration and after. In the
original alignment ?, aka Rodi? is unaligned. Af-
ter the alignment processing, the unaligned phrase
is mapped to NULL allowing for the possibility of
learning a phrasal deletion entry in the phrase table.
We also modified the decoder to appropriately han-
dle NULL mappings during the translation process.
Table 3 shows a sample of the phrasal deletion
rules learned. These rules and probabilities were
learned by the original phrase-table generation code
4
Normal: Sergio Rodriguez Garcia , aka Rodri , is a spanish footballer ...
Simple: Sergio Rodriguez Garcia is a spanish football player ...
Modified Simple: Sergio Rodriguez Garcia NULL is a spanish football player ...
Table 2: Example output from the alignment modification step to capture phrasal deletion. Words that are vertically
aligned are aligned in the word alignment.
Phrase-table entry prob
, ? NULL 0.057
the ? NULL 0.033
of the ? NULL 0.0015
or ? NULL 0.0014
however , ? NULL 0.00095
the city of ? NULL 0.00034
generally ? NULL 0.00033
approximately ? NULL 0.00025
, however , ? NULL 0.00022
, etc ? NULL 0.00013
Table 3: Example phrase-table entries learned from the
data and their associated probability.
of Moses after the word alignment was modified.
The highest probability rules tend to delete punctua-
tion and function words, however, other phrases also
appeared. 0.5% of the rules learned during training
are deletion rules.
5 Experiments
We compared five different approaches on the text
simplification task:
none: Does no simplification. Outputs the normal,
unsimplified sentence.
K & M: Noisy-channel sentence compression sys-
tem described in Knight and Marcu (2002).
T3: Synchronous tree substitution grammar,
trained discriminatively (Cohn and Lapata, 2009).
Moses: Phrase-based, machine translation ap-
proach (Koehn et al, 2007).
Moses+Del: Our approach described in Section 4
which is a phrase-based approach with the addition
of phrasal deletion.
From the aligned data set of 137K sentence pairs,
we used 124K for training and 1,300 for testing
with the remaining 12K sentences used during de-
velopment. We trained the n-gram language model
used by the last four systems on the simple side of
the training data.3 T3 requires parsed data which
we generated using the Stanford parser (Klein and
Manning, 2003). Both Moses and Moses+Del were
trained using the default Moses parameters and we
used the last 500 sentence pairs from the training set
to optimize the hyper-parameters of the log-linear
model for both Moses variants. T3 was run with the
default parameters.
Due to runtime and memory issues, we were un-
able to run T3 on the full data set.4 We therefore
present results for T3 trained on the largest train-
ing set that completed successfully, the first 30K
sentence pairs. This still represents a significantly
larger training set than T3 has been run on previ-
ously. For comparison, we also provide results be-
low for Moses+Del trained on the same 30K sen-
tences.
5.1 Evaluation
Since there is no standard way of evaluating text
simplification, we provide results for three different
automatic methods, all of which compare the sys-
tem?s output to a reference simplification. We used
BLEU (Papineni et al, 2002), which is the weighted
mean of n-gram precisions with a penalty for brevity.
It has been used extensively in machine translation
and has been shown to correlate well with human
performance judgements.
We also adopt two automatic measures that have
been used to evaluate text compression that com-
pare the system?s output to a reference translation
3See (Turner and Charniak, 2005) for a discussion of prob-
lems that can occur for text compression when using a language
model trained on data from the uncompressed side.
4On 30K sentences T3 took 4 days to train. On the full data
set, we ran T3 for a week and at that point the discriminative
training was using over 100GB of memory and we terminated
the run.
5
System BLEU word-F1 SSA
none 0.5937 0.5967 0.6179
K & M 0.4352 0.4352 0.4871
T3* 0.2437 0.2190 0.3651
Moses 0.5987 0.6076 0.6224
Moses+Del 0.6046 0.6149 0.6259
Table 4: Performance of the five approaches on the test
data. All differences in performance are statistically sig-
nificant. * - T3 was only trained on 30K sentence pairs
for performance reasons.
(Clarke and Lapata, 2006): simple string accuracy
measure (a normalized version of edit distance, ab-
breviated SSA) and F1 score calculated over words.
We calculated F1 over words instead of grammatical
relations (subject, direct/indirect object, etc.) since
finding the relation correspondence between the sys-
tem output and the reference is a non-trivial task for
simplification data where reordering, insertions and
lexical changes can occur. Clarke and Lapata (2006)
showed a moderate correlation with human judge-
ment for SSA and a strong correlation for the F1
measure.
To measure whether the difference between sys-
tem performance is statistically significant, we use
bootstrap resampling with 100 samples with the t-
test (Koehn, 2004).
5.2 Results
Table 4 shows the results on the test set for the dif-
ferent evaluation measures. All three of the evalu-
ation metrics rank the five systems in the same or-
der with Moses+Del performing best. All differ-
ences between the systems are statistically signifi-
cant for all metrics at the p = 0.01 level. One of the
challenges for the sentence simplification problem
is that, like sentence compression, not making any
changes to the system produces reasonable results
(contrast this with machine translation). In the test
set, 30% of the simple sentences were the same as
the corresponding normal sentence. Because of this,
we see that not making any changes (none) performs
fairly well. It is, however, important to leave these
sentences in the test set, since not all sentences need
simplification and systems should be able to handle
these sentences appropriately.
Both of the text compression systems perform
poorly on the text simplification task with results
that are significantly worse than doing nothing. Both
of these systems tended to bias towards modifying
the sentences (T3 modified 77% of the sentences and
K & M 96%). For K & M, the poor results are not
surprising since the model only allows for deletion
operations and is more tailored to the compression
task. Although T3 does allow for the full range of
simplification operations, it was often overly aggres-
sive about deletion, for example T3 simplified:
There was also a proposal for an extension
from Victoria to Fulham Broadway station
on the district line , but this was not in-
cluded in the bill .
to ?it included .? Overall, the output of T3 aver-
aged 13 words per sentence, which is significantly
lower than the gold standard?s 21 words per sen-
tence. T3 also suffered to a lesser extent from inap-
propriately inserting words/phrases, which other re-
searchers have also noted (Nomoto, 2009). Some of
these issues were a results of T3?s inability to cope
with noise in the test data, both in the text or the
parses.
Both Moses and Moses+Del perform better than
the text compression systems as well as the baseline
system, none. If we remove those sentences in the
test set where the simple sentence is the same as the
normal sentence and only examine those sentences
where a simplification should occur, the difference
between the phrase-based approaches and none is
even more significant with BLEU scores of 0.4560,
0.4723 and 0.4752, for none, Moses and Moses+Del
respectively.
If we compare Moses and Moses+Del, the ad-
dition of phrasal deletion results in a statistically
significant improvement. The phrasal deletion was
a common operation in the simplifications made
by Moses+Del; in 8.5% of the test sentences,
Moses+Del deleted at least one phrase. To better un-
derstand this performance difference, Table 5 shows
the BLEU scores for sentences where each respec-
tive system made a change (i.e. the output simpli-
fication is different than the input). In both cases,
when the systems make simplifications on sentences
that should be simplified, we see large gains in the
output over doing nothing. While Moses improves
over the baseline of doing nothing by 0.047 BLEU,
6
BLEU
System Case none output
Moses
correct change 0.4431 0.4901
incorrect change 1 0.8625
Moses+Del
correct change 0.4087 0.4788
incorrect change 1 0.8706
Table 5: BLEU scores for Moses and Moses+Del on sen-
tences where the system made a change. ?correct change?
shows the score where a change was made by the system
as well as in the reference and ?incorrect change? where
a change was made by the system, but not the reference.
we see an even larger gain by Moses+Del with a dif-
ference of 0.07 BLEU.
For completeness, we also trained Moses+Del on
the same 30K sentences used to train the T3 sys-
tem.5 Using this training data, Moses+Del achieved
a BLEU score of 0.5952. This is less than the score
achieved when using the full training data, but is sig-
nificantly better than T3 and still represents a small
improvement over none.
Table 6 shows example simplifications made by
Moses+Del. In many of the examples we see phrasal
deletion during the simplification process. The out-
put also contains a number of reasonable lexical
changes, for example in a, d and e. Example b
contains reordering and e shows an example of a
split being performed where the normal sentence is
turned into two simplified sentences. This is not un-
common in the data, but can be challenging to model
for current syntactic approaches. The examples also
highlight some of the common issues with the ap-
proach. Examples a and f are not grammatically cor-
rect and the simplification in f does not preserve the
original meaning of the text. As an aside, the normal
sentence of example d also contains an omission er-
ror following ?as? due to preprocessing of the data,
resulting from ill-formed xml in the articles.
5.3 Oracle
In the previous section, we looked at the perfor-
mance of the systems based on the best translations
suggested by the systems. For many approaches, we
can also generate an n-best list of possible transla-
tions. We examined the simplifications in this n-
5To be completely consistent with T3, we used the first
29,700 pairs for training and the last 300 for parameter tuning.
BLEU
System original oracle
Moses 0.5987 0.6317
Moses+Del 0.6046 0.6421
Table 7: BLEU score for the original system versus the
best possible ?oracle? translations generated by greedily
selecting the best translation from an n-best list based on
the reference simplification.
best list to measure the potential benefit of reranking
techniques, which have proved successful in many
NLP applications (Och et al, 2004; Ge and Mooney,
2006), and to understand how well the underlying
model captures the phenomena exhibited in the data.
For both of the phrase-based approaches, we gener-
ated an n-best list of size 1000 for each sentence in
the test set. Using these n-best lists, we generated
an ?oracle? simplification of the test set by greed-
ily selecting for each test sentence the simplification
in the n-best list with the best sentence-level BLEU
score.
Table 7 shows the BLEU scores for the original
system output and the system?s oracle output. In all
cases, there is a large difference between the sys-
tem?s current output and the oracle output, suggest-
ing that utilizing some reranking technique could be
useful. Also, we again see the benefit of the phrasal
deletion rules. The addition of the phrasal dele-
tion rule gives the system an additional dimension
of flexibility, resulting in a more varied n-best list
and an overall higher oracle BLEU score.
6 Conclusions and Future Work
In this paper, we have explored a variety of ap-
proaches for learning to simplify sentences from
Wikipedia. In contrast to prior work in the related
field of sentence compression where deletion plays
the dominant role, the simplification task we exam-
ined has the full range of text-to-text operations in-
cluding lexical changes, reordering, insertions and
deletions.
We implemented a modified phrase-based sim-
plification approach that incorporates phrasal dele-
tion. Our approach performs significantly better
than two different text compression approaches, in-
cluding T3, and better than previous approaches on
a similar data set (Zhu et al, 2010). We also showed
7
a. normal: Critical reception for The Wild has been negative.
simplified: Reviews for The Wild has been negative.
b. normal: Bauska is a town in Bauska county , in the Zemgale region of southern Latvia .
simplified: Bauska is a town in Bauska county , in the region of Zemgale .
c. normal: LaBalme is a commune in the Ain department in eastern France .
simplified: LaBalme is a commune .
d. normal: Shadow of the Colossus , released in Japan as , is a Japanese-developed action-
adventure video game developed and published by Sony computer entertainment
for the Playstation 2.
simplified: Shadow of the Colossus is a Japanese-developed action-adventure video game
made by Sony computer entertainment for the Playstation 2.
e. normal: Nicolas Anelka is a French footballer who currently plays as a striker for Chelsea
in the English premier league .
simplified: Nicolas Anelka is a French football player . He plays for Chelsea .
f. normal: Each edge of a tesseract is of the same length.
simplified: Same edge of the same length.
Table 6: Example simplifications. ?normal? is the the unsimplified input sentence and ?simplified? the simplification
made by Moses+Del.
that the incorporation of phrasal deletion into the
simplification process results in statistically signif-
icant improvements over a traditional phrase-based
approach.
While we obtained positive results using a phrase-
based approach, we still believe that incorporating
some additional hierarchical structure will help the
simplification process, particularly since one of the
goals of simplification is to reduce the grammatical
complexity of the sentence. Also, as seen in some
of the examples above, the phrase-based model can
produce output that is not grammatically correct.
Though T3 did not perform well, many other syntax-
based models exists that have been successful in ma-
chine translation.
There are a number of research questions moti-
vated by this work in related areas including the scal-
ability of discriminative trained rule sets, the impact
of the language model training source (simple vs.
normal English), document-level simplification and
applications of text simplification. Our hope is that
this new simplification task will spur a variety of re-
lated research inquiries.
Acknowledgments
We?d like to thank Dan Feblowitz for his insights
and discussions, and for generating the results for
the K & M implementation.
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP.
Yvonne Canning, John Tait, Jackie Archibald, and Ros
Crawley. 2000. Cohesive generation of syntactically
simplified newspaper text. In Proceedings of TSD.
John Carroll, Gido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of English newspaper text to assist aphasic readers. In
Proceedings of AAAI Workshop on Integrating AI and
Assistive Technology.
Raman Chandrasekar and Bangalore Srinivas. 1997. Au-
tomatic induction of rules for text simplification. In
Knowledge Based Systems.
Yu Chen, Martin Kay, and Andreas Eisele. 2009. Inter-
secting multilingual data for faster and better statistical
translations. In Proceedings of HLT/NAACL.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of ACL.
James Clarke and Mirella Lapata. 2006. Models for
sentence compression: A comparison across domains,
training requirements and evaluation measures. In
Proceedings of ACL.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research.
8
Will Coster and David Kauchak. 2011. Simple English
Wikipedia: A new simplification task. In Proceedings
of ACL (Short Paper).
Lijun Feng. 2008. Text simplification: A survey. CUNY
Technical Report.
Michel Galley and Kathleen McKeown. 2007. Lexical-
ized Markov grammars for sentence compression. In
Proceedings of HLT/NAACL.
Ruifang Ge and Raymond Mooney. 2006. Discrimina-
tive reranking for semantic parsing. In Proceedings of
COLING.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Kevin Knight and Daniel Marcu. 2002. Summarization
beyond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
Makoto Miwa, Rune Saetre, Yusuke Miyao, and Jun?ichi
Tsujii. 2010. Entity-focused sentence simplication for
relation extraction. In Proceedings of COLING.
Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple Wikipedia: A cogitation in ascertaining
abecedarian language. In Proceedings of HLT/NAACL
Workshop on Computation Linguistics and Writing.
Rani Nelken and Stuart Shieber. 2006. Towards robust
context-sensitive sentence alignment for monolingual
corpora. In Proceedings of AMTA.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. In Informa-
tion Processing and Management.
Tadashi Nomoto. 2008. A generic sentence trimmer with
CRFs. In Proceedings of HLT/NAACL.
Tadashi Nomoto. 2009. A comparison of model free ver-
sus model intensive approaches to sentence compres-
sion. In Proceedings of EMNLP.
F.J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Kenji Yamada, Stanford U, Alex Fraser,
Daniel Gildea, and Viren Jain. 2004. A smorgasbord
of features for statistical machine translation. In Pro-
ceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
David Vickrey and Daphne Koller. 2008. Sentence sim-
plification for semantic role labeling. In Proceedings
of ACL.
S. Wubben, A. van den Bosch, and E. Krahmer. 2010.
Paraphrase generation as monolingual translation:
Data and evaluation. In Proceedings of the Interna-
tional Workshop on Natural Language Generation.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia revision histories for improving sentence
compression. In ACL.
Mark Yatskar, Bo Pang, Critian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifica-
tions from Wikipedia. In Proceedings of HLT/NAACL
(Short Paper).
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of COLING.
9
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 1?10,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Sentence Simplification as Tree Transduction
Dan Feblowitz
Computer Science Department
Pomona College
Claremont, CA
djf02007@mymail.pomona.edu
David Kauchak
Computer Science Department
Middlebury College
Middlebury, VT
dkauchak@middlebury.edu
Abstract
In this paper, we introduce a syntax-based
sentence simplifier that models simplifi-
cation using a probabilistic synchronous
tree substitution grammar (STSG). To im-
prove the STSG model specificity we uti-
lize a multi-level backoff model with addi-
tional syntactic annotations that allow for
better discrimination over previous STSG
formulations. We compare our approach
to T3 (Cohn and Lapata, 2009), a re-
cent STSG implementation, as well as
two state-of-the-art phrase-based sentence
simplifiers on a corpus of aligned sen-
tences from English and Simple English
Wikipedia. Our new approach performs
significantly better than T3, similarly to
human simplifications for both simplicity
and fluency, and better than the phrase-
based simplifiers for most of the evalua-
tion metrics.
1 Introduction
Text simplification is aimed at reducing the read-
ing and grammatical complexity of text while re-
taining the meaning. Text simplification has ap-
plications for children, language learners, people
with disabilities (Carroll et al, 1998; Feng, 2008)
and in technical domains such as medicine (El-
hadad, 2006), and can be beneficial as a prepro-
cessing step for other NLP applications (Vickrey
and Koller, 2008; Miwa et al, 2010). In this paper
we introduce a new probabilistic model for sen-
tence simplification using synchronous tree sub-
stitution grammars (STSG).
Synchronous grammars can be viewed as simul-
taneously generating a pair of recursively related
strings or trees (Chiang, 2006). STSG grammar
rules contain pairs of tree fragments called ele-
mentary trees (Eisner, 2003; Cohn and Lapata,
2009; Yamangil and Shieber, 2010). The leaves
of an elementary tree can be either terminal, lex-
ical nodes or aligned nonterminals (also referred
to as variables or frontier nodes). Because ele-
mentary trees may have any number of internal
nodes structured in any way STSGs allow for more
complicated derivations not expressible with other
synchronous grammars.
To simplify an existing tree, an STSG gram-
mar is used as a tree transducer. Figure 1 shows
some example simplification STSG rules written
in transductive form. As a transducer the gram-
mar rules take an elementary tree and rewrite it as
the tree on the right-hand side of the rule. For ex-
ample, the first rule in Figure 1 would make the
transformation
S
VP1VP
ADVP
RB
occasionally
MD
may
NP0
S
VP1NP0,
,
ADVP
RB
sometimes
changing ?may occasionally? to ?sometimes ,? and
moving the noun phrase from the beginning of the
sentence to after the comma. The indices on the
nonterminals indicate alignment and transduction
continues recursively on these aligned nontermi-
nals until no nonterminals remain. In the example
above, transduction would continue down the tree
on the NP and VP subtrees. A probabilistic STSG
has a probability associated with each rule.
One of the key challenges in learning an STSG
from an aligned corpus is determining the right
level of specificity for the rules: too general and
they can be applied in inappropriate contexts; too
specific, and the rules do not apply in enough con-
texts. Previous work on STSG learning has regu-
lated the rule specificity based on elementary tree
depth (Cohn and Lapata, 2009), however, this ap-
proach has not worked well for text simplifica-
1
S(NP0 VP(MD(may) ADVP(RB(occasionally))) VP1) ? S(ADVP(RB(sometimes)) ,(,) NP0 VP1)
NP(NNS0) ? NP(NNS0)
NP(JJ0 NNS1) ? NP(JJ0 NNS1)
VP(VB0 PP(IN(in) NP1)) ? VP(VB0 NP1)
VB(assemble), ? VB(join)
JJ(small) ? JJ(small)
NNS(packs) ? NNS(packs)
NNS(jackals) ? NNS(jackals)
Figure 1: Example STSG rules representing the maximally general set for the aligned trees in Figure 2.
The rules are written in transductive form. Aligned nonterminals are indicated by indices.
tion (Coster and Kauchak, 2011a). In this pa-
per, we take a different approach and augment the
grammar with additional information to increase
the specificity of the rules (Galley and McKeown,
2007). We combine varying levels of grammar
augmentation into a single probabilistic backoff
model (Yamangil and Nelken, 2008). This ap-
proach creates a model that uses specific rules
when the context has been previously seen in the
training data and more general rules when the con-
text has not been seen.
2 Related Work
Our formulation is most closely related to the T3
model (Cohn and Lapata, 2009), which is also
based on the STSG formalism. T3 was devel-
oped for the related problem of text compression,
though it supports the full range of transforma-
tion operations required for simplification. We use
a modified version of their constituent alignment
and rule extraction algorithms to extract the ba-
sic STSG rules with three key changes. First, T3
modulates the rule specificity based on elemen-
tary tree depth, while we use additional grammar
annotations combined via a backoff model allow-
ing for a broader range of context discrimination.
Second, we learn a probabilistic model while T3
learns the rule scores discriminatively. T3?s dis-
criminative training is computationally prohibitive
for even modest sized training sets and a proba-
bilistic model can be combined with other proba-
bilities in a meaningful way. Third, our implemen-
tation outputs an n-best list which we then rerank
based on a trained log-linear model to select the
final candidate.
Zhu et al (2010) suggest a probabilistic, syntax-
based approach to text simplification. Unlike the
STSG formalism, which handles all of the trans-
formation operations required for sentence simpli-
fication in a unified framework, their model uses
a combination of hand-crafted components, each
designed to handle a different transformation op-
eration. Because of this model rigidity, their sys-
tem performed poorly on evaluation metrics that
take into account the content and relative to other
simplification systems (Wubben et al, 2012).
Woodsend and Lapata (2011) introduce a quasi-
synchronous grammar formulation and pose the
simplification problem as an integer linear pro-
gram. Their model has similar representational ca-
pacity to an STSG, though the learned models tend
to be much more constrained, consisting of <1000
rules. With this limited rule set, it is impossible
to model all of the possible lexical substitutions
or to handle simplifications that are strongly con-
text dependent. This quasi-synchronous grammar
approach performed better than Zhu et al (2010)
in a recent comparison, but still performed worse
than recent phrase-based approaches (Wubben et
al., 2012).
A number of other approaches exist that use
Simple English Wikipedia to learn a simplifica-
tion model. Yatskar et al (2010) and Biran et
al. (2011) learn lexical simplifications, but do not
tackle the more general simplification problem.
Coster and Kauchak (2011a) and Wubben et al
(2012) use a modified phrase-based model based
on a machine translation framework. We compare
against both of these systems. Qualitatively, we
find that phrasal models do not have the represen-
tative power of syntax-based approaches and tend
to only make small changes when simplifying.
Finally, there are a few early rule-based sim-
plification systems (Chandrasekar and Srinivas,
1997; Carroll et al, 1998) that provide motivation
for recent syntactic approaches. Feng (2008) pro-
vides a good overview of these.
3 Probabilistic Tree-to-Tree
Transduction
We model text simplification as tree-to-tree trans-
duction with a probabilistic STSG acquired from
2
S1
VP
VP4
PP6
NP6
NNS8
packs
JJ7
small
IN
in
VB5
assemble
ADVP
RB
occasionally
MD
may
NP2
NNS3
jackals
S1
VP4
NP6
NNS8
packs
JJ7
small
VB5
join
NP2
NNS3
jackals
,
,
ADVP
RB
sometimes
Figure 2: An example pair of constituent aligned trees generated by the constituent alignment algorithm.
Aligned constituents are indicated with a shared index number (e.g. NP2 is aligned to NP2).
a parsed, sentence-aligned corpus between normal
and simplified sentences. To learn the grammar,
we first align tree constituents based on an in-
duced word alignment then extract grammar rules
that are consistent with the constituent alignment.
To improve the specificity of the grammar we
augment the original rules with additional lexi-
cal and positional information. To simplify a sen-
tence based on the learned grammar, we generate
a finite-state transducer (May and Knight, 2006)
and use the transducer to generate an n-best list
of simplifications. We then rerank the n-best list
of simplifications using a trained log-linear model
and output the highest scoring simplification. The
subsections below look at each of these steps in
more detail. Throughout the rest of this paper, we
will refer to the unsimplified text/trees as normal
and the simplified variants as simple.
3.1 Rule Extraction
Given a corpus of pairs of trees representing nor-
mal and simplified sentences, the first step is to
extract a set of basic STSG production rules from
each tree pair. We used a modified version of the
algorithm presented by Cohn and Lapata (2009).
Due to space constraints, we only present here
a brief summary of the algorithm along with our
modifications to the original algorithm. See Cohn
and Lapata (2009) for more details.
Word-level alignments are learned using
Giza++ (Och and Ney, 2000) then tree nodes (i.e.
constituents) are aligned if: there exists at least
one pair of nodes below them that is aligned and
all nodes below them are either aligned to a node
under the other constituent or unaligned. Given
the constituent alignment, we then extract the
STSG production rules. Because STSG rules can
have arbitrary depth, there are often many possible
sets of rules that could be extracted from a pair
of trees.1 Following Cohn and Lapata (2009)
we extract the maximally general rule set from
an aligned pair of input trees that is consistent
with the alignment: the set of rules capable of
synchronously deriving the original aligned tree
pair consisting of rules with the smallest depth.
Figure 2 shows an example tree pair that has
been constituent aligned and Figure 1 shows the
extracted STSG rules.
We modify the constituent alignment algorithm
from Cohn and Lapata (2009) by adding the re-
quirement that if node b with parent a are both
aligned to node z and its parent y, we only align
the pairs (a, y) and (b, z), i.e. align the children
and align the parents. This eliminates a common
occurrence where too many associations are made
between a pair of preterminal nodes and their chil-
dren. For example, for the sentences shown in Fig-
ure 2 the word alignment contains ?assemble?
aligned to ?join?. Under the original definition
four aligned pairs would be generated:
VB
assemble
VB
join
but only two under our revised definition:
VB
assemble
VB
join
This revised algorithm reduces the size of the
alignment, decreasing the number of cases which
must be checked during grammar extraction while
preserving the intuitive correspondence.
1There is always at least one set of rules that can generate
a tree pair consisting of the entire trees.
3
3.2 Grammar Generation
During the production rule extraction process, we
select the production rules that are most general.
More general rules allow the resulting transducer
to handle more potential inputs, but can also re-
sult in unwanted transformations. When generat-
ing the grammar, this problem can be mitigated by
also adding more specific rules.
Previous approaches have modulated rule speci-
ficity by incorporating rules of varying depth in
addition to the maximally general rule set (Cohn
and Lapata, 2009), though this approach can be
problematic. Consider the aligned subtrees rooted
at nodes (VP4, VP4) in Figure 2. An STSG learn-
ing algorithm that controls rule specificity based
on depth must choose between generating the rule:
VP(VB0 PP(IN(in) NP1)) ? VP(VB0 NP1)
which drops the preposition, or a deeper rule that
includes the lexical leaves such as:
VP(VB(assemble) PP(IN(in) NP1))? VP(VB(join) NP1)
or
VP(VB(assemble) PP(IN(in) NP(JJ0 NNS1))) ?
VP(VB(join) NP(JJ0 NNS1))
If either of the latter rule forms is chosen, the
applicability is strongly restricted because of the
specificity and lexical requirement. If the former
rule is chosen and we apply this rule we could
make the following inappropriate transformation:
VP
PP
NP
NN
cafeteria
DT
the
IN
in
VB
eat
VP
NP
NN
cafeteria
DT
the
VB
eat
simplifying ?eat in the cafeteria? to ?eat the cafe-
teria?.
We adopt a different approach to increase the
rule specificity. We augment the production rules
and resulting grammar with several parse tree an-
notations shown previously to improve SCFG-
based sentence compression (Galley and McKe-
own, 2007) as well as parsing (Collins, 1999): par-
ent annotation, head-lexicalization, and annotation
with the part of speech of the head word.
Following Yamangil and Nelken (2008), we
learn four different models and combine them into
a single backoff model. Each model level in-
creases specificity by adding additional rule anno-
tations. Model 1 contains only the original pro-
duction rules. Model 2 adds parent annotation,
Model 3 adds the head child part of speech and
Model 4 adds head child lexicalization. The head
child was determined using the set of rules from
Collins (1999). Figure 3 shows the four different
model representations for the VP rule above.
3.3 Probability Estimation
We train each of the four models individually us-
ing maximum likelihood estimation over the train-
ing corpus, specifically:
p(s|n) =
count(s ? n)
count(n)
where s and n are tree fragments with that level?s
annotation representing the right and left sides of
the rule respectively.
During simplification, we start with the most
specific rules, i.e. Model 4. If a tree fragment
was not observed in the training data at that model
level, we repeatedly try a model level simpler until
a model is found with the tree fragment (Yamangil
and Nelken, 2008). We then use the probability
distribution given by that model. A tree fragment
only matches at a particular level if all of the anno-
tation attributes match for all constituents. If none
of the models contain a given tree fragment we in-
troduce a rule that copies the tree fragment with
probability one.
Two types of out-of-vocabulary problems can
occur and the strategy of adding copy rules pro-
vides robustness against both. In the first, an input
contains a tree fragment whose structure has never
been seen in training. In this case, copy rules allow
the structure to be reproduced, leaving the system
to make more informed changes lower down in the
tree. In the second, the input contains an unknown
word. This only affects transduction at the leaves
of the tree since at the lower backoff levels nodes
are not annotated with words. Adding copy rules
allows the program to retain, replace, or delete un-
seen words based only on the probabilities of rules
higher up for which it does have estimates. In both
cases, the added copy rules make sure that any in-
put tree will have an output.
3.4 Decoding and Reranking
Given a parsed sentence to simplify and the prob-
abilistic STSG grammar, the last step is to find the
most likely transduction (i.e. simplification) of the
input tree based on the grammar. To accomplish
this, we convert the STSG grammar into an equiv-
alent finite tree-to-tree transducer: each STSG
4
Model 1: VP (VB0 PP (IN(in) NP1))? VP (VB0 NP1)
Model 2: VP?VP (VB?VP0 PP?VP (IN?PP (in) NP?PP1))? VP?S (VB?VP0 NP?VP1)
Model 3: VP[VB]?VP (VB?VP0 PP[NNS]?VP (IN?PP (in) NP[NNS]?PP1))?
VP[VB]?S (VB?VP0 NP[NNS]?VP1)
Model 4: VP[VB-assemble]?VP (VB[assemble]?VP0 PP[NNS-packs]?VP (IN[in]?PP (in) NP[NNS-packs]?PP1))?
VP[VB-join]?S (VB[join]?VP0 NP[NNS-packs]?VP1)
Figure 3: The four levels of rule augmentation for an example rule ranging from Model 1 with no
additional annotations to Model 4 with all annotations. The head child and head child part of speech are
shown in square brackets and the parent constituent is annotated with ?.
grammar rule represents a state transition and is
weighted with the grammar rule?s probability. We
then use the Tiburon tree automata package (May
and Knight, 2006) to apply the transducer to the
parsed sentence. This yields a weighted regular
tree grammar that generates every output tree that
can result from rewriting the input tree using the
transducer. The probability of each output tree in
this grammar is equal to the product of the proba-
bilities of all rewrite rules used to produce it.
Using this output regular tree grammar and
Tiburon, we generate the 10,000 most probable
output trees for the input parsed sentence. We
then rerank this candidate list based on a log-linear
combination of features:
- The simplification probability based on the
STSG backoff model.
- The probability of the output tree?s yield, as
given by an n-gram language model trained on
the simple side of the training corpus using the
IRSTLM Toolkit (Federico et al, 2008).
- The probability of the sequence of the part of
speech tags in the output tree, as given by an n-
gram model trained on the part of speech tags of
the simple side of the training corpus.
- A two-sided length penalty decreasing the score
of output sentences whose length, normalized by
the length of the input, deviates from the training
corpus mean, found empirically to be 0.85.
The first feature represents the simplification like-
lihood based on the STSG grammar described
above. The next two features ensure that outputs
are well-formed according to the language used
in Simple English Wikipedia. Finally, the length
penalty is used to prevent both over-deletion and
over-insertion of out-of-source phrases. In addi-
tion, the length feature mean could be reduced or
increased to encourage shorter or longer simplifi-
cations if desired.
The weights of the log-linear model are opti-
mized using random-restart hill-climbing search
(Russell and Norvig, 2003) to maximize BLEU
(Papineni et al, 2002) on a development set.2
4 Experiment Setup
To train and evaluate the systems we used the data
set from Coster and Kauchak (2011b) consisting
of 137K aligned sentence pairs between Simple
English Wikipedia and English Wikipedia. The
sentences were parsed using the Berkeley Parser
(Petrov and Klein, 2007) and the word alignments
determined using Giza++ (Och and Ney, 2000).
We used 123K sentence pairs for training, 12K for
development and 1,358 for testing.
We compared our system (SimpleTT ? simple
tree transducer) to three other simplification ap-
proaches:
T3: Another STSG-based approach (Cohn and La-
pata, 2009). Our approach shares similar con-
stituent alignment and rule extraction algorithms,
but our approach differs in that it is generative
instead of discriminative, and T3 increases rule
specificity by increasing rule depth, while we em-
ploy a backoff model based on grammar augmen-
tation. In addition, we employ n-best reranking
based on a log-linear model that incorporates a
number of additional features.
The code for T3 was obtained from the au-
thors.3 Due to performance limitations, T3 was
only trained on 30K sentence pairs. T3 was run on
the full training data for two weeks, but it never
terminated and required over 100GB of memory.
The slow algorithmic step is the discriminative
training, which cannot be easily parallelized. T3
was tested for increasing amounts of data up to
2BLEU was chosen since it has been used successfully in
the related field of machine translation, though this approach
is agnostic to evaluation measure.
3http://staffwww.dcs.shef.ac.uk/
people/T.Cohn/t3/
5
30K training pairs and the results on the automatic
evaluation measures did not improve.
Moses-Diff: A phrase-based approach based on
the Moses machine translation system (Koehn et
al., 2007) that selects the simplification from the
10-best output list that is most different from the
input sentence (Wubben et al, 2012). Moses-Diff
has been shown to perform better than a number
of recent syntactic systems including Zhu et al
(2010) and Woodsend and Lapata (2011).
Moses-Del: A phrase-based approach also based
on Moses which incorporates phrasal deletion
(Coster and Kauchak, 2011b). The code was ob-
tained from the authors.
For an additional data point to understand the
benefit of the grammar augmentation, we also
evaluated a deletion-only system previously used
for text compression and a variant of that sys-
tem that included the grammar augmentation de-
scribed above. K&M is a synchronous context
free grammar-based approach (Knight and Marcu,
2002) and augm-K&M adds the grammar aug-
mentation along with the four backoff levels.
There are currently no standard evaluation met-
rics for text simplification. Following previous
work (Zhu et al, 2010; Coster and Kauchak,
2011b; Woodsend and Lapata, 2011; Wubben
et al, 2012) we evaluated the systems using
automatic metrics to analyze different system
characteristics and human evaluations to judge the
system quality.
Automatic Evaluation
- BLEU (Papineni et al, 2002): BLEU measures
the similarity between the system output and a
human reference and has been used successfully
in machine translation. Higher BLEU scores are
better, indicating an output that is more similar
to the human reference simplification.
- Oracle BLEU: For each test sentence we gener-
ate the 1000-best output list and greedily select
the entry with the highest sentence-level BLEU
score. We then calculate the BLEU score over
the entire test set for all such greedily selected
sentences. The oracle score provides an analy-
sis of the generation capacity of the model and
gives an estimate of the upper bound on the
BLEU score attainable through reranking.
- Length ratio: The ratio of the length of the orig-
inal, unsimplified sentence and the system sim-
plified sentence.
Human Evaluation
Following previous work (Woodsend and Lapata,
2011; Wubben et al, 2012) we had humans judge
the three simplification systems and the human
simplifications from Simple English Wikipedia
(denoted SimpleWiki)4 based on three metrics:
simplicity, fluency and adequacy. Simplicity mea-
sures how simple the output is, fluency measures
the quality of the language and grammatical cor-
rectness of the output, and adequacy measures
how well the content is preserved. For the flu-
ency experiments, the human evaluators were just
shown the system output. For simplicity and ade-
quacy, in addition to the system output, the orig-
inal, unsimplified sentence was also shown. All
metrics were scored on a 5-point Likert scale with
higher indicating better.
We used Amazon?s Mechanical Turk (MTurk)5
to collect the human judgements. MTurk has been
used by many NLP researchers, has been shown
to provide results similar to other human annota-
tors and allows for a large population of annotators
to be utilized (Callison-Burch and Dredze, 2010;
Gelas et al, 2011; Zaidan and Callison-Burch,
2011).
We randomly selected 100 sentences from the
test set where all three systems made some change
to the input sentence. We chose sentences where
all three systems made a change to focus on the
quality of the simplifications made by the systems.
For each sentence we collected scores from 10
judges, for each of the systems, for each of the
three evaluation metrics (a total of 100*10*3*3 =
9000 annotations). The scores from the 10 judges
were averaged to give a single score for each sen-
tence and metric. Judges were required to be
within the U.S. and have a prior acceptance rate
of 95% or higher.
5 Results
Automatic evaluation
Table 1 shows the results of the automatic eval-
uation metrics. SimpleTT performs significantly
better than T3, the other STSG-based model, and
obtains the second highest BLEU score behind
only Moses-Del. SimpleTT has the highest oracle
BLEU score, indicating that the syntactic model of
SimpleTT allows for more diverse simplifications
4T3 was not included in the human evaluation due to the
very poor quality of the output based on both the automatic
measures and based on a manual review of the output.
5https://www.mturk.com/
6
System BLEU Oracle Length
Ratio
SimpleTT 0.564 0.663 0.849
Moses-Diff 0.543 ?? 0.960
Moses-Del 0.605 0.642 0.991
T3 0.244 ??? 0.581
K&M 0.406 0.602 0.676
augm-K&M 0.498 0.609 0.826
corpus mean ? ? 0.85
Table 1: Automatic evaluation scores for all sys-
tems tested and the mean values from the training
corpus. ?Moses-Diff uses the n-best list to choose
candidates and therefore is not amenable to oracle
scoring. ??T3 only outputs the single best simpli-
fication.
than the phrase-based models and may be more
amenable to future reranking techniques. Sim-
pleTT also closely matches the in-corpus mean
of the length ratio seen by human simplifications,
though this can be partially explained by the length
penalty in the log-linear model.
Moses-Del obtains the highest BLEU score, but
accomplishes this with only small changes to the
input sentence: the length of the simplified sen-
tences are only slightly different from the original
(a length ratio of 0.99). Moses-Diff has the low-
est BLEU score of the three simplification systems
and while it makes larger changes than Moses-
Del it still makes much smaller changes than Sim-
pleTT and the human simplifications.
T3 had significant problems with over-deleting
content as indicated by the low length ratio which
resulted in a very low BLEU score. This issue
has been previously noted by others when using
T3 for text compression (Nomoto, 2009; Marsi et
al., 2010).
The two deletion-only systems performed
worse than the three simplification systems. Com-
paring the two systems shows the benefit of the
grammar augmentation: augm-K&M has a signif-
icantly higher BLEU score than K&M and also
avoided the over-deletion that occurred in the orig-
inal K&M system. The additional specificity of
the rules allowed the model to make better deci-
sions for which content to delete.
Human evaluation
Table 2 shows the human judgement scores for
the simplification approaches for the three differ-
ent metrics averaged over the 100 sentences and
Table 3 shows the pairwise statistical significance
calculations between each system based on a two-
simplicity fluency adequacy
SimpleWiki 3.45 3.93 3.42
SimpleTT 3.55 3.80 3.09
Moses-Diff 3.07 3.64 3.91
Moses-Del 3.19 3.74 3.86
Table 2: Human evaluation scores on a 5-point
Likert scale averaged over 100 sentences.
tailed paired t-test. Overall, SimpleTT performed
well with simplicity and fluency scores that were
comparable to the human simplifications. Sim-
pleTT was too aggressive at removing content, re-
sulting in lower adequacy scores. This phenom-
ena was also seen in the human simplifications and
may be able to be corrected in future variations by
adjusting the sentence length target.
The human evaluations highlight the trade-off
between the simplicity of the output and the
amount of content preserved. For simplicity, Sim-
pleTT and the human simplifications performed
significantly better than both the phrase-based sys-
tems. However, simplicity does come with a cost;
both SimpleTT and the human simplifications re-
duced the length of the sentences by 15% on aver-
age. This content reduction resulted in lower ad-
equacy than the phrase-based systems. A similar
trade-off has been previously shown for text com-
pression, balancing content versus the amount of
compression (Napoles et al, 2011).
For fluency, SimpleTT again scored similarly to
the human simplifications. SimpleTT performed
significantly better than Moses-Diff and slightly
better than Moses-Del, though the difference was
not statistically significant.
As an aside, Moses-Del performs slightly bet-
ter than Moses-Diff overall. They perform simi-
larly on adequacy and Moses-Del performs better
on simplicity and Moses-Diff performs worse rel-
ative to the other systems on fluency.
Qualitative observations
SimpleTT tended to simplify by deleting prepo-
sitional, adjective, and adverbial phrases, and by
truncating conjunctive phrases to one of their con-
juncts. This often resulted in outputs that were
syntactically well-formed with only minor infor-
mation loss, for example, it converts
?The Haiti national football team is the na-
tional team of Haiti and is controlled by the
Fe?de?ration Hat??enne de Football.?
to
7
Simplicity
SimpleWiki Moses-Diff Moses-Del
SimpleTT ??? ???
SimpleWiki ??? ???
Moses-Diff ?
Fluency
SimpleWiki Moses-Diff Moses-Del
SimpleTT ?
SimpleWiki ??? ?
Moses-Diff
Adequacy
SimpleWiki Moses-Diff Moses-Del
SimpleTT ?? ??? ???
SimpleWiki ??? ???
Moses-Diff
Table 3: Pairwise statistical significance test re-
sults between systems for the human evaluations
based on a paired t-test. The number of arrows de-
notes significance with one, two and three arrows
indicating p < 0.05, p < 0.01 and p < 0.001
respectively. The direction of the arrow points to-
wards the system that performed better.
?The Haiti national football team is the na-
tional football team of Haiti.?
which only differs from the human reference by
one word.
SimpleTT also produces a number of interesting
lexical and phrasal substitutions, including:
football striker ? football player
football defender ? football player
in order to ? to
known as ? called
member ? part
T3, on the other hand, tended to over-delete con-
tent, for example simplifying:
?In earlier times, they frequently lived on the
outskirts of communities, generally in squalor.?
to just
?A lived?.
As we saw in the automatic evaluation results,
the phrase-based systems tended to make fewer
changes to the input and those changes it did make
tended to be more minor. Moses-Diff was more
aggressive about making changes, though it was
more prone to errors since the simplifications cho-
sen were more distant from the input sentence than
other options in the n-best list.
6 Conclusions and Future work
In this paper, we have introduced a new prob-
abilistic STSG approach for sentence simplifica-
tion, SimpleTT. We improve upon previous STSG
approaches by: 1) making the model probabilistic
instead of discriminative, allowing for an efficient,
unified framework that can be easily interpreted
and combined with other information sources, 2)
increasing the model specificity using four levels
of grammar annotations combined into a single
model, and 3) incorporating n-best list reranking
combining the model score, language model prob-
abilities and additional features to choose the fi-
nal output. SimpleTT performs significantly better
than previous STSG formulations for text simpli-
fication. In addition, our approach was rated by
human judges similarly to human simplifications
in both simplicity and fluency and it scored bet-
ter than two state-of-the-art phrase-based sentence
simplification systems along many automatic and
human evaluation metrics.
There are a number of possible directions for
extending the capabilities of SimpleTT and related
systems. First, while some sentence splitting can
occur in SimpleTT due to sentence split and merge
examples in the training data, SimpleTT does not
explicitly model this. Sentence splitting could be
incorporated as another probabilistic component
in the model (Zhu et al, 2010). Second, in this
work, like many previous researchers, we assume
Simple English Wikipedia as our target simplic-
ity level. However, the difficulty of Simple En-
glish Wikipedia varies across articles and there are
many domains where the desired simplicity varies
depending on the target consumer. In the future,
we plan to explore how varying algorithm param-
eters (for example the length target) affects the
simplicity level of the output. Third, one of the
benefits of SimpleTT and other probabilistic sys-
tems is they can generate an n-best list of can-
didate simplifications. Better reranking of output
sentences could close this gap across all these sys-
tems, without requiring deep changes to the under-
lying model.
References
Or Biran, Samuel Brody, and Noem?ie Elhadad. 2011.
Putting it simply: A context-aware approach to lexi-
cal simplification. In Proceedings of ACL.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
8
ing speech and language data with Amazon?s Me-
chanical Turk. In Proceedings of NAACL-HLT
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
John Carroll, Gido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplifica-
tion of English newspaper text to assist aphasic read-
ers. In Proceedings of AAAI Workshop on Integrat-
ing AI and Assistive Technology.
Raman Chandrasekar and Bangalore Srinivas. 1997.
Automatic induction of rules for text simplification.
In Knowledge Based Systems.
David Chiang. 2006. An introduction to synchronous
grammars. Part of a tutorial given at ACL.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Review.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
William Coster and David Kauchak. 2011a. Learning
to simplify sentences using Wikipedia. In Proceed-
ings of the Workshop on Monolingual Text-To-Text
Generation.
William Coster and David Kauchak. 2011b. Simple
English Wikipedia: A new text simplification task.
In Proceedings of ACL.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Noemie Elhadad. 2006. Comprehending technical
texts: predicting and defining unfamiliar terms. In
Proceedings of AMIA.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: An open source toolkit for
handling large scale language models. In Proceed-
ings of Interspeech, Brisbane, Australia.
Lijun Feng. 2008. Text simplification: A survey.
CUNY Technical Report.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Proceedings of HLT-NAACL.
Hadrien Gelas, Solomon Teferra Abate, Laurent Be-
sacier, and Francois Pellegrino. 2011. Evaluation of
crowdsourcing transcriptions for African languages.
In Interspeech.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL.
Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Wal-
ter Daelemans. 2010. On the limits of sentence
compression by deletion. In Empirical Methods in
NLG.
Jonathan May and Kevin Knight. 2006. Tiburon: A
weighted tree automata toolkit. In Proceedings of
CIAA.
Makoto Miwa, Rune Saetre, Yusuke Miyao, and
Jun?ichi Tsujii. 2010. Entity-focused sentence sim-
plification for relation extraction. In Proceedings of
COLING.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation.
Tadashi Nomoto. 2009. A comparison of model free
versus model intensive approaches to sentence com-
pression. In Proceedings of EMNLP.
Franz Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of ACL.
Kishore Papineni, Kishore Papineni, Salim Roukos,
Salim Roukos, Todd Ward, Todd Ward, Wei jing
Zhu, and Wei jing Zhu. 2002. BLEU: A method
for automatic evaluation of machine translation. In
Proceedings of ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HTL-
NAACL.
Stuart Russell and Peter Norvig. 2003. Artificial intel-
ligence: A modern approach.
David Vickrey and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of ACL.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of EMNLP.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of ACL.
Elif Yamangil and Rani Nelken. 2008. Mining
wikipedia revision histories for improving sentence
compression. In Proceedings of HLT-NAACL.
9
Elif Yamangil and Stuart Shieber. 2010. Bayesian syn-
chronous tree-substitution grammar induction and
its application to sentence compression. In Proceed-
ings of ACL.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simpli-
fications from Wikipedia. In Proceedings of HLT-
NAACL.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proceedings of ACL.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of ICCL.
10
