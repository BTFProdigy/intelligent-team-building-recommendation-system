Domain Knowledge Engineering  
Based on Encyclopedias and the Web Text*
SUI Zhifang 
Institute of 
Computational 
Linguistics,
Peking University 
szf@pku.edu.cn
CUI Gaoying
Institute of 
Computational 
Linguistics,
Peking University 
cuigy@pku.edu.cn
DING Wansong 
Institute of 
Computational 
Linguistics,
Peking University 
dws@pku.edu.cn
ZHANG Qinlong
Institute of 
Computational 
Linguistics,
Peking University 
zql@pku.edu.cn
                                                          
*  This  research was funded by 973 Natural Basic Research Program of China 2004CB318102 and Natural Sciences 
Foundation of Beijing 4052019 
Abstract
Domain knowledge is the fundamental 
resources required by all intelligent 
information processing systems. With 
the upsurge of new technology and new 
products in various domains, the 
manual construction and updating of 
domain knowledge base can hardly 
meet the real needs of application 
systems, in terms of coverage or 
effectiveness. 
Based on natural language text analysis, 
this paper intends to draw a basic 
framework for the construction of 
domain knowledge base. Using 
encyclopedia resources and text 
information resources on the Web, we 
focus on the method of constructing 
domain knowledge base through 
technologies in natural language text 
analysis and machine learning. 
Moreover, an open network platform 
will be developed, through which 
common users can work with domain 
experts to contribute domain 
knowledge.
The technology can be applied to the 
construction and updating of domain 
knowledge base for intelligent 
information processing, and it can also 
provide help for the knowledge 
updating of encyclopedias. 
Keywords: Domain knowledge base, 
Natural language text analysis, machine 
learning, encyclopedia, open platform 
1 Introduction 
Domain knowledge is the indispensable resource 
for an intelligent information processing system. 
With the XSsurge of technology, more and more 
new technology, new product and new 
techniques come into being. The manual 
construction and updating domain knowledge 
base can hardly meet the real needs of 
application systems, in terms of coverage or 
effectiveness. In order to improve the robust of 
an information system, we need to study a 
computer-aided method to solve the bottleneck 
of domain knowledge acquisition.  
The Institute of Computational Linguistics, 
Peking University, now cooperates with 
Encyclopedia of China Publishing Hall on the 
project of human-machine interactive 
encyclopedia knowledge engineering. We want 
to study how to exploit, use and update 
encyclopedia resource properly. We will use the 
technique of natural language processing, 
machine learning and text mining to acquire 
domain knowledge semi-automatically from 
1
both encyclopedia and the Web text. 
Furthermore, we set up an open platform for 
domain knowledge acquisition, so that common 
network users and domain experts can work 
together to contribute new domain knowledge. 
Based on this technology, we can build domain 
knowledge base on each domain. This 
technology can be used as an important method 
of constructing and updating the domain 
knowledge base for the intelligent information 
retrieval and extraction systems. In the same 
time, it can also provide help for the knowledge 
updating of encyclopedias. 
2 Related works 
The researches on knowledge acquisition can be 
divided into three parts: artificial construction, 
semi-automatic construction and automatic 
construction. 
Artificial methods are usually used in 
constructing the common sense knowledge base, 
such as CYC[1], WordNet[2], EuroWordNet[3], 
HowNet[4], and CCD[5] etc. That?s because 
common sense is steady comparatively and it 
can not be affected by the task, also it can be 
reused by various kinds of system when 
constructed. For instance, since the WordNet 
was established in 1985, it had been widely used 
in IR, Text categorization, QA system etc. 
Similarly, the HowNet is being used in many 
Chinese information procession systems. It?s 
worthy of large-scale devotion for long-time 
using.
On the contrary, domain knowledge is tied 
with some concrete domain. Once the 
application domain changed, we need to re-
construct the domain knowledge base. 
Furthermore, domain knowledge updates 
continually, so that the domain knowledge base 
should be updates frequently. So it?s unrealistic 
to construct the domain knowledge base 
manually.  
In the way of constructing domain knowledge 
base, the semi-automatic method is mainly used. 
[6][7][8][9][10] established the platform of 
human-computer interactively working for the 
construction of domain knowledge base. They 
use various kinds of text processing and 
language analysis tools, which have the 
functions of morphological analysis, partial 
syntactic analysis, partial semantic analysis, 
with the mode of online cooperation, helping 
knowledge engineers or domain experts to find 
the domain concepts and the relations among 
them. The acquired knowledge can be added 
into the domain knowledge base. All these 
methods try to use pattern matching or various 
layers of NLP technology to acquire domain 
knowledge from large-scale free text. Free text 
is easy to get, however, it comes from different 
kinds of domain, including complicated 
language phenomenon hence is hard to 
understand. It?s difficult to extract knowledge 
reliably using current technology of NLP and 
machine learning from such free text. If there 
not exists a pre-defined basic domain knowledge 
architecture, it is difficult to acquire the 
concepts and the relations relative to the domain. 
Also, among the above-mentioned methods, the 
construction of domain knowledge base depends 
on the expert?s point of view and opinions. 
However, it?s very difficult to let experts to 
construct the ?real-time? domain architecture 
objectively and roundly and hence express it 
clearly in the given time. 
3 Domain Knowledge Engineering 
Based on Encyclopedias and the Web 
text
In this paper, we propose a technology of 
domain knowledge engineering based on 
encyclopedias and the web text. (ncyclopedia is 
the embodiment of the systematization and 
centralization of existed domain knowledge. The 
knowledge has been compiled and modified by 
many experts. Compared with free text, there are 
more canonical and NLP technologies can be 
used comparatively easily to extract knowledge 
from it. Since the knowledge in encyclopedia is 
more systematic, we can easily construct the 
basic frame of domain knowledge. So we will 
use NLP technology and machine learning 
method to construct the kernel of domain 
knowledge based on the analysis of the 
encyclopedia. Then based on the kernel of 
domain knowledge base, we can extract domain 
knowledge from other text resources. 
There exist some researches on extracting 
knowledge from the encyclopedia [11] [12] [13] 
[14]. These researches use the encyclopedias as 
the only source to acquire knowledge. However, 
with the high-speed improvement in each 
2
domain, there is severe knowledge lag in 
encyclopedias. So it is inadequate to use 
encyclopedias as the only source for knowledge 
acquisition. We need to learn more domain 
knowledge from other text resource besides 
encyclopedias. 
With the surge of Internet, information in it is 
increasing exponentially. Abundant knowledge 
lies in this huge Web resource. If we can extract 
knowledge from the Web, we could update and 
expand domain knowledge base most efficiently. 
Standing in the computational linguistics? point, 
we focus on retrieving information from the 
content rather than from the structure of the 
Web.
This paper studies the technology of domain 
knowledge engineering. Using encyclopedia 
resource and text information resource on the 
Web, we focus on the method of constructing 
domain knowledge base through technologies in 
natural language text analysis and machine 
learning. Moreover, an open network platform 
will be developed, through which common users 
can work together with domain experts to 
contribute domain knowledge. 
4 Strategy and Research Plan 
4.1 Learning the style of knowledge-
dense text from encyclopedias 
The compilation of encyclopedias always 
follows some specified compilatory model. 
Encyclopedias have the relatively formal diction 
and different compilatory model for different 
kinds of entries. Because of that, the 
paraphrasable text in encyclopedias has clearer 
model to express the relation among concepts in 
most cases. For instance, ?X is a kind of ?Y?, 
?X is composed of A, B, C and D?, ?A, B and C 
make up D?, ?X can be divided into A, B and C?. 
Through recognizing the terms and partial 
parsing for the paraphrasable text in the 
encyclopedia, we could learn the patterns, which 
express the relations among concepts. 
Furthermore, we could learn the styles of 
knowledge dense text based on those patterns. 
Next step, we will follow the styles and combine 
the HTML target set to acquire more knowledge 
dense text fragments from the web. Based on 
such knowledge-dense text, some deeper natural 
language processing technologies could be used 
to extract domain knowledge reliably. 
4.2 Automatic extraction of terms 
Through analyzing the characters and expression 
forms of Chinese terms, we learn term 
knowledge from large-scale domain corpus and 
term bank. Using natural language processing 
method combing rule and statistics, we can 
automatically extract Chinese terms from corpus. 
A term is a kind of phrases, whose 
components are close related. Further more, it 
has strong domain feature. The close relation of 
the components in a term can be captured 
through calculating the static association rate 
between the words that compose a term 
candidate. The linguistic feature can be captured 
through analysis the grammatical structural 
information of the terms. While the domain 
feature of a term can be captured through the 
domain component that has the possibility of 
composing a term. For example, ?movable 
terminal? and ?social economy? are both 
composed by the close related components. 
While, the former is a term in the domain of 
information science and technology, and the 
latter is just a common phrase instead of a term. 
The reason lies in that the former has the domain 
feature comes from one of its components? 
terminal?, while the latter has not the domain 
feature.
We will use the above characteristic and 
representation forms of a term to perform 
automatic term extraction. The system of 
automatic term extraction includes two phases: 
learning stage and application stage. 
In the stage of learning, we use a series of 
machine learning methods to get various kinds 
of integrated knowledge for automatic term 
extraction from a large-scale corpus and a term 
bank. These knowledge includes the inner 
structural knowledge of terms, the statistical 
domain features of term component, the 
statistical mutual information between the 
components of terms, the outer environment 
features of terms and the distinct text-level 
features of term recognition etc... In the stage of 
application, through an efficient model, we use 
all these various types of knowledge into 
automatic term extraction. 
3
4.3 Design and implementation of 
partially analysis technology 
oriented for knowledge-dense text 
The knowledge-dense text fragments (including 
encyclopedia and the Web text segments whose 
style is similar to encyclopedias) is relatively 
simple. Therefore, it?s possible to implement 
deeper analysis on it using the natural language 
processing technology. 
We use comprehensive language knowledge 
and statistical technique together to design 
language analysis method oriented for 
knowledge-dense text. We will use the 
comprehensive language knowledge resource, 
such as The Grammatical Dictionary-base of 
Contemporary Chinese, Chinese Semantic 
Dictionary, Chinese Concept Dictionary (CCD), 
and Termbank of Information Technology, 
which was developed by Institute of 
Computational Linguistics (ICL) of Peking 
University. Moreover, we will design a natural 
language partial parsing and understanding 
technology combining statistic technology base 
on the 80,000,000 words IT corpus. Concretely, 
on the base of the developed software such as 
word segmentation, POS tagging, term 
extraction and identification, skeletal 
dependency analysis of sentence, we will 
combine semantic restrict information with 
syntax rules, so that during syntax analysis we 
can get the semantic restrict information 
between syntax components at the same time. 
We will label semantic roles for predicate-head 
and its central valency components using 
Chinese Semantic Dictionary. So we can get 
shallow case frames of sentences after natural 
language partial parsing and understanding for 
text sentences. And domain knowledge will be 
extracted in the later stage from this analysis 
result.
4.4 Establishing the basic knowledge 
description frame based on the 
encyclopedia
The knowledge of encyclopedia is relatively 
systematic, mature and intensive. On this 
foundation, it is easier to set up a basic domain 
knowledge base which includes the kernel of 
domain knowledge. In the encyclopedia, every 
subject is described by attributes, and different 
subjects are organized hierarchically. 
Figure1: An example of the fragments of 
domain knowledge framework 
For example as showed in Figure1, aiming at 
the subject ?Input equipment? in the domain of 
computer hardware, the encyclopedia describes 
the basic knowledge around the subject from 
many of point of views such as components, 
function, classification etc, which we call 
attributes here. On the other hand, ?Input 
equipment?, ?Output equipment?, ?Terminal 
unit? constitute the subject ?computer I/O 
equipment?; furthermore, ?Input equipment?, 
?Computer storage equipment?, ?Network 
equipment? are also components of the 
?Computer hardware?. This paper will make the 
?classification + Attribute? as the basic 
knowledge description method for constructing 
the basic domain knowledge base. When the 
basic knowledge description method is set up, 
we take every entry in the encyclopedia as a 
subject, through analyzing the correlative 
sentence and recognizing the key terms in the 
paraphrase text and the relations among the 
terms, we can describe the basic knowledge on 
this subject. In the next step, we may couple 
several subjects in the same domain gradually in 
order to construct the basic domain knowledge 
base in this domain. 
4.5 Using bootstrapping method to 
expand domain knowledge base 
The structure of the web text is incompact, and 
the diction is not canonical enough. However, 
the web text is easy to get and contains a great 
amount of new knowledge. So based on the 
4
basic domain knowledge base, we can select the 
knowledge dense text fragments from the web 
resource as the source to acquire more new 
knowledge.
We collect language patterns, which are 
known showing some kind of domain 
knowledge from encyclopedia. Using the 
language patterns as the seed set, we could learn 
more language patterns from the web text using 
boot strapping machine learning method.  Using 
the expanded seed set, we could learn more 
language patterns from the larger text. This 
technique can expand domain knowledge base 
iteratively. 
The system structure is as Figure2. 
Figure2: The system structure for domain 
knowledge acquisition. 
4.6 Developing open platform for 
domain knowledge collecting 
With the rapid development of Internet, people 
could communicate and collaborate without face 
to face. They can share work and collaborate 
through the web. So we constructed an open 
human-computer interactive platform to call on 
domain knowledge experts and spacious 
common network users to collaborate together 
and contribute new domain knowledge. This 
platform could also assist the experts of 
encyclopedia in editing and managing new 
domain knowledge. 
5 Current result 
5.1 Automatic extraction of terms 
We have exploited a term extraction system, 
including term extraction, human-computer 
interactive updating etc. The system is made up 
of basic source layer, learning layer, application 
layer and service layer. 
We select the texts from 16 representative 
Chinese journals in the field of science and 
technology to construct the testing set.  
The Principles for Testing 
First of all, we manually tagged the terms in the 
testing texts. The principles we used in term 
tagging is very strict, that is, we only tag the 
longest terms in the texts, while ignore any of 
the term fragments in a longest term. For 
example, for the word sequence ??? ?? ?
?  (Interface Technology Specification)?, we 
only select ??? ?? ??  (Interface 
Technology Specification)? as a term, while 
ignore ??? ??  (Interface Technology?, 
although it may be also a term in other context.  
Similarly, for word sequence ??? ?? ?
? (Digital Television Signal)?, we only select 
??????? (Digital Television Signal)? as 
a term, while ignore ??? ??  (Digital 
Television)?.
The above testing principles may result in a 
great decrease of the precision and recall of term 
recognition. However, through these principles, 
we can find more problems existed in the term 
recognition algorithm.   
The Testing Results 
Based on the above testing principles, we get the 
precision and recall of term recognition as Table 
1.
RECALL PRECISION THE JOURNALS 
OF THE 
TESTING
TEXTS
% %
Semi-Conductor 
Technology (1999-
01)
65.6 55.1
Telecom Science 
(1998-01) 
52.9 60.4
Computer and the 
Peripheral
Equipments (1999-
01)
52.9 71.2
5
The Research and 
Progress of Solid 
Electronics (2000-
01)
57.6 62.4
Compute-Aided 
Design and (1999-
01)
60.0 54.6
Computer 
Engineering (1999-
04)
65.1 73.4
Computer 
Application (1999-
01)
57.2 68.9
Automatic
Measure and 
Control (1998-02) 
51 59.5
Control Theory 
and Application 
(1999-01) 
49.4 64.1
Software (1998-01) 52.6 54.1
Micro-Electronics 
(1999-01) 
65.6 55.0
Wireless
Communication 
Technology (2000-
01)
57.7 69.6
Remote Sensing 
(1999-01) 
67.1 62.1
System Emulation 
(1999-01) 
64.9 75.4
Motional
Communication 
(1999-01) 
61 51.3
Chinese Cable 
Television (2000-
01)
60.0 57.0
AVERAGE 57.8 62.2
Table 1: Testing Result 
There is no unique standard for term?s 
determination. What is a term? What is a 
common word? What is a term fragment? It is 
difficult to give an objective and unique 
standard that is operable for computers. 
Therefore, what the automatic term recognition 
system find can only be taken as the term 
candidates attached with the confidences. We 
still need the human terminology experts to give 
a final confirmation of the terms.  
Our software includes human-computer 
interactive updating interface besides automatic 
term extraction. The interactive updating 
interface is as Figure3: 
Figure3: The interactive updating interface after 
term extraction 
5.2 Set up the basic database of 
encyclopedia
A lot of key concepts in the encyclopedia are 
well-marked with hyperlinks, titles, bookmarks 
and other Html tags according to different kinds 
of information respectively in the paraphrasable 
text. Using the information supplied by ?China 
encyclopedia? e-press, we put encyclopedia 
subject information, relationship between 
subjects and term hierarchy into database to 
form an encyclopedia database for a primary 
domain knowledge base. 
The core structure of encyclopedia database is 
presented as (main entry, relation term, 
relationship). Main entry is the entry that is 
listed in the encyclopedia. Relation terms are the 
hyperlink, bookmark, subtitle and so on. The 
relationship between main term and these 
elements now is null that need to be added with 
human assistance. 
For example, the paraphrasable text of term 
?frequency divider? is showed in the database as 
Table2.
Main entry Relation term Relationship
Frequency 
divider
Crystal 
oscillator
Unknown
Frequency 
divider
Impulse 
frequency 
divider
Unknown
Frequency 
divider
Trigger Unknown
Frequency Regenerate Unknown
6
divider frequency 
divider
Frequency 
divider
Trigger Unknown
Frequency 
divider
Regenerate
frequency 
divider
Unknown
Table2: the database fragment for the term 
?frequency divider? 
5.3 Attribute relation template 
extraction
attribute relation type template example 
definition xxx?/??/??/?
??/???/???/
??
substitutable name 
mark
xxx?/??/? xxx
country xxx????xxx?
nationality xxx?/??
native place or home 
place
???/?? xxx/xxx
?
experience ??? xxx?? xxx
??
literature ??/??/??/??
xxx/??? xxx/??
? xxx/???? xxx
working experience xxx?? xxx?xxx?
? xxx?? xxx??
? xxx??? xxx?
??? xxx???
xxx???? xxx
achievement and 
influence
??? xxx/? xxx?
?/xxx???
Table3: The examples of the attribute 
relation template of human entry 
We have semi-automatically extracted several 
attribute relation templates for human entries 
from encyclopedia text. The attribute relation 
template of human entry examples are as Table3. 
5.4 Open platform for domain 
knowledge collection
We design the open platform for domain 
knowledge collection using ASP.NET network 
programming technology. We establish 
interactive working relation among domain 
knowledge engineers, domain experts and 
common users through the platform. The 
functions including: 
z Domain knowledge requirement collection: 
on-line collection of new term entries of 
current domain, which are needed by the 
users.
z Domain knowledge supply collection: on-
line collection of more detailed attribute 
information of the new terms. 
z On-line management: system 
administrators manage new term 
information, which were submitted on line 
by the users. 
The interface of the platform is as Figure4. 

Figure4: The interface of the open platform for 
domain knowledge collection 
6 Conclusion
The construction of domain knowledge base is a 
kind of high intelligent knowledge engineering. 
Since there is still have big gap between current 
level of technological development and real 
need, it is unrealistic to build domain knowledge 
base using automatic method or manual method 
only. However, in the human-computer 
interaction process, how to sufficiently absorb 
the knowledge resource which human being has 
already mastered and use it to supervise 
7
automatic acquisition of new knowledge? How 
to call together knowledge engineers, domain 
experts and common network users and realize 
multi-member collaboration during the updating 
and extending process of domain knowledge 
base? These are the key problems to be settled in 
the knowledge engineering domain. This paper 
tries to do some exploration on these aspects. 
References 
[1] http://www.opencyc.org/
[2] http://www.cogsci.princeton.edu/~wn
[3] http://www.illc.uva.nl/EuroWordNet/
[4] http://www.keenage.com
[5] Yu Jiangshen, Liu Yang, Yu Shiwen, The 
specification of the Chinese Concept Dictionary, 
Journal of Chinese Language and Computing, 
Vol.13, 2003.  
[6]A.Maedche, S.Staab, Semi-Automatic 
Engineering of Ontologies from text, Proceedings 
of International Conference on Software 
Engineering and Knowledge Engineering (SEKE' 
2000), Chicago, IL, USA, 2000 
[7] Szpakowicz,S., Semi-automatic acquisition of 
conceptual structure from technical texts, 
International journal of Man-machine Studies, 
33(4),385-397,1990 
[8] Biebow, B., Szulman, S., TERMINAE: a 
linguistic-based tool for the building of domain 
ontology. In Dieter fensel, Rudi Studer (eds.), 
Knowledge Acquisition, Modeling and 
Management, pp.49-66, 1999 
[9] Lapalut, S., How to handle multiple expertise 
from several experts: a general text clustering 
approach. In F. Maurer (Ed.), Proc. 2nd Knowledge 
Engineering Forum (KEF?96), Karlsruhe, Jan., 
1996. 
[10] Mikheev, A., Finch, S., A workbench for 
acquisition of ontological knowledge from natural 
text. In proc. Of the 7th conference of the 
European Chapter for Computational Linguistics 
(EACL?95), Dublin, Ireland, pp. 194-201, 1995 
[11] Richard Hull, Fernando Gomez, Automatic 
acquisition of biographic knowledge from 
encyclopedic texts, ExpertSystems with 
Applications, 16(1999), pp.261-270, 1999 
[12] Fernando Gomez, Richard Hull, Carlos Segami, 
1994, Acquiring Knowledge from Encyclopedic 
Texts, Proceedings of the 4th ACL Conference on 
Applied Natural Language Processing, Stuttgart, 
Germany, 1994 
[13] Song Rou, Xu Yong, An Experiment on 
Knowledge Extraction from an Encyclopedia 
Based on Lexicon Semantics, pp.101-112, 2002 
[14] Gu Fang, Cao Cungen, Biological Knowledge 
Acquisition from the Electronic Encyclopedia of 
China, Proceedings of ICYCS?2001, pp.1199-
1203, 2001 
8
Preliminary Chinese Term Classification 
for Ontology Construction 
Gaoying Cui, Qin Lu, Wenjie Li 
Department of Computing, 
Hong Kong Polytechnic University 
{csgycui, csluqin, cswjli}@comp.polyu.edu.hk 
 
 
Abstract 
An ontology can be seen as a representa-
tion of concepts in a specific domain. Ac-
cordingly, ontology construction can be re-
garded as the process of organizing these 
concepts. If the terms which are used to la-
bel the concepts are classified before build-
ing an ontology, the work of ontology con-
struction can proceed much more easily. 
Part-of-speech (PoS) tags usually carry 
some linguistic information of terms, so 
PoS tagging can be seen as a kind of pre-
liminary classification to help constructing 
concept nodes in ontology because features 
or attributes related to concepts of different 
PoS types may be different. This paper pre-
sents a simple approach to tag domain 
terms for the convenience of ontology con-
struction, referred to as Term PoS (TPoS) 
Tagging. The proposed approach makes 
use of segmentation and tagging results 
from a general PoS tagging software to pre-
dict tags for extracted domain specific 
terms. This approach needs no training and 
no context information. The experimental 
results show that the proposed approach 
achieves a precision of 95.41% for ex-
tracted terms and can be easily applied to 
different domains. Comparing with some 
existing approaches, our approach shows 
that for some specific tasks, simple method 
can obtain very good performance and is 
thus a better choice. 
Keywords: ontology construction, part-of-
speech (PoS) tagging, Term PoS (TPoS) 
tagging. 
1 Introduction 
Ontology construction has two main issues 
including the acquisition of domain concepts and 
the acquisition of appropriate taxonomies of these 
concepts. These concepts are labeled by the terms 
used in the domain which are described by 
different attributes. Since domain specific terms 
(terminology) are labels of concepts among other 
things, terminology extraction is the first and the 
foremost important step of domain concept 
acquisition. Most of the existing algorithms in 
Chinese terminology extraction only produce a list 
of terms without much linguistic information or 
classification information (Yun Li and Qiangjun 
Wang, 2001; Yan He et al, 2006; Feng Zhang et 
al., 2006). This fact makes it difficult in ontology 
construction as the fundamental features of these 
terms are missing. The acquisition of taxonomies is 
in fact the process of organizing domain specific 
concepts. These concepts in an ontology should be 
defined using a subclass hierarchy by assigning 
and defining properties and by defining 
relationship between concepts etc. (Van Rees, R., 
2003). These methods are all concept descriptions. 
The linguistic information associated with domain 
terms such as PoS tags and semantic classification 
information of terms can also make up for the 
concept related features which are associated with 
concept labels. Terms with different PoS tags 
usually carry different semantic information. For 
example, a noun is usually a word naming a thing 
or an object. A verb is usually a word denoting an 
action, occurrence or state of existence, which are 
all associated with a time and a place. Thus in 
ontology construction, noun nodes and verb nodes 
should be described using different attributes with 
different discriminating characters. With this 
information, extracted terms can then be classified 
The 6th Workshop on Asian Languae Resources, 2008
17
accordingly to help in ontology construction and 
retrieval work. Thus PoS tags can help identify the 
different features needed for concept representation 
in domain ontology construction.  
It should be pointed out that Term PoS (TPoS) 
tagging is different from the general PoS tagging 
tasks. It is designed to do PoS tagging for a given 
list of terms extracted from some terminology ex-
traction algorithms such as those presented in 
(Luning Ji et al, 2007). The granularity of general 
PoS tagging is smaller than what is targeted in this 
paper because terms representing domain specific 
concepts are more likely to be compound words 
and sometimes even phrases, such as ?????
? ?(file manager),  ????? ?(description of 
concurrency), etc.. Even though current general 
word segmentation and PoS tagging can achieve 
precision of 99.6% and 97.58%, respectively 
(Huaping Zhang et al, 2003),   its performance for 
domain specific corpus is much less satisfactory 
(Luning Ji et al, 2007), which is why terminology 
extraction algorithms need to be developed. 
In this paper, a very simple but effective method 
is proposed for TPoS tagging which needs no train-
ing process or even context information. This 
method is based on the assumption that every term 
has a headword. For a given list of domain specific 
terms which are segmented and each word in the 
term already has a PoS tag, the TPoS tagging algo-
rithm then identifies the position of the headword 
and take the tag of the headword as the tag of the 
term. Experiments show that this method is quite 
effective in giving good precision and minimal 
computing time. 
The remaining of this paper is organized as fol-
lows. Section 2 reviews the related work. Section 3 
gives the observations to the task and correspond-
ing corpus, then presents our method for TPOS 
tagging. Section 4 gives the evaluation details and 
discussions on the proposed method and reference 
methods. Section 5 concludes this paper. 
2 Related Work 
Although TPoS tagging is different from general 
PoS tagging, the general POS tagging methods are 
worthy of referencing. There are a lot of existing 
POS tagging researches which can be classified 
into following categories in general. Natural ideas 
of solving this problem were to make use of the 
information from the words themselves. A number 
of features based on prefixes and suffixes and 
spelling cues like capitalization were adopted in 
these researches (Mikheev, A, 1997; Brants, Thor-
sten, 2000; Mikheev, A, 1996). Mikheev presented 
a technique for automatically acquiring rules to 
guess possible POS tags for unknown words using 
their starting and ending segments (Mikheev, A, 
1997). Through an unsupervised process of rule 
acquisition, three complementary sets of word-
guessing rules would be induced from a general 
purpose lexicon and a raw corpus: prefix morpho-
logical rules, suffix morphological rules and end-
ing-guessing rules (Mikheev, A, 1996). Brants 
used the linear interpolation of fixed length suffix 
model for word handling in his POS tagger, named 
TnT. For example, an English word ending in the 
suffix ?able was very likely to be an adjective 
(Brants, Thorsten, 2000). 
Some existing methods are based on the analysis 
of word morphology. They exploited more features 
besides morphology or took morphology as sup-
plementary means (Toutanova et al, 2003; Huihsin 
Tseng et al, 2005; Samuelsson, Christer, 1993). 
Toutanova et al demonstrated the use of both pre-
ceding and following tag contexts via a depend-
ency network representation and made use of some 
additional features such as lexical features includ-
ing jointly conditioning on multiple consecutive 
words and other fine-grained modeling of word 
features (Toutanova et al, 2003). Huihisin et al 
proposed a variety of morphological word features, 
such as the tag sequence features from both left 
and right side of the current word for POS tagging 
and implemented them in a Maximum Entropy 
Markov model (Huihsin Tseng et al, 2005). 
Samuelsson used n-grams of letter sequences end-
ing and starting each word as word features. The 
main goal of using Bayesian inference was to in-
vestigate the influence of various information 
sources, and ways of combining them, on the abil-
ity to assign lexical categories to words. The 
Bayesian inference was used to find the tag as-
signment T with highest probability P(T|M, S) 
given morphology M (word form) and syntactic 
context S (neighboring tags) (Samuelsson, Christer, 
1993). 
Other researchers inclined to regard this POS 
tagging work as a multi-class classification prob-
lem. Many methods used in machine learning, such 
The 6th Workshop on Asian Languae Resources, 2008
18
as Decision Tree, Support Vector Machines (SVM) 
and k-Nearest-Neighbors (k-NN), were used for 
guessing possible POS tags of words (G. Orphanos 
and D. Christodoulakis, 1999; Nakagawa T, 2001; 
Maosong Sun et al, 2000). Orphanos and Christo-
doulakis presented a POS tagger for Modern Greek 
and focused on a data-driven approach for the in-
duction of decision trees used as disambiguation or 
guessing devices (G. Orphanos and D. Christodou-
lakis, 1999). The system was based on a high-
coverage lexicon and a tagged corpus capable of 
showing off the behavior of all POS ambiguity 
schemes and characteristics of words. Support 
Vector Machine is a widely used (or effective) 
classification approach for solving two-class pat-
tern recognition problems. Selecting appropriate 
features and training effective classifiers are the 
main points of SVM method. Nakagawa et al used 
substrings and surrounding context as features and 
achieve high accuracy in POS tag prediction (Na-
kagawa T, 2001). Furthermore, Sun et alpresented 
a POS identification algorithm based on k-nearest-
neighbors (k-NN) strategy for Chinese word POS 
tagging. With the auxiliary information such as 
existing tagged lexicon, the algorithm can find out 
k nearest words which were mostly similar with the 
word need tagging (Maosong Sun et al, 2000). 
3 Algorithm Design 
As pointed out earlier, TPoS tagging is different 
from the general PoS tagging tasks. In this paper, it 
is assumed that a terminology extraction algorithm 
has already obtained the PoS tags of individual 
words. For example, in the segmented and tagged 
sentence ????/n ??/n ?/v ?/f ?/w ??/n 
??/d ?/v ???/a ??/n ?/f ??/v ?/w?(In 
computer graphics, objects are usually represented 
as polygonal meshes.), the term ??????? 
(polygonal meshes) has been segmented into two 
individual words and tagged as ???? /a? 
(polygonal /a) and ??? /n? (meshes /n). The 
terminology extraction algorithm would identify 
these two words  ????/a? and ???/n? as a 
single term in a specific domain. The proposed 
algorithm is to determine the PoS of this single 
term ??????? (polygonal meshes), thus the 
algorithm is referred to as TPoS tagging. It can be 
seen that the general purpose PoS tagging and term 
PoS tagging assign tags at different granularity. In 
principle, the context information of terms can help 
TPoS tagging and the individual PoS tags may be 
good choices as classification features. 
The proposed TPoS tagging algorithm consists 
of two modules. The first module is a terminology 
extraction preprocessing module. The second 
module carries out the TPoS tag assignment. In the 
terminology extraction module, if the result of ter-
minology extraction algorithm is a list of terms 
without PoS tags, a general purpose segmenter 
called ICTCLAS1 will be used to give PoS tags to 
all individual words. ICTCLAS is developed by 
Chinese Academy of Science, the precision of 
which is 97.58% on tagging general words 
(Huaping Zhang et al, 2003). Then the output of 
this module is a list of terms, referred to as Term-
List, using algorithms such as the method de-
scribed in (Luning Ji et al, 2007). 
In this paper, two simple schemes for the term 
PoS tag assignment module are proposed. The first 
scheme is called the blind assignment scheme. It 
simply assigns the noun tag to every term in the 
TermList. This is based on the assumption that 
most of the terms in a specific domain represent 
certain concepts that are most likely to be nouns. 
Result from this blind assignment scheme can be 
considered as the baseline or the worse case sce-
nario. Even in general domain, it is observed that 
nouns are in the majority of Chinese words with 
more than 50% among all different PoS tags (Hui 
Wang, 2006).  
The second scheme is called head-word-driven 
assignment scheme. Theoretically, it will take the 
tag of the head word of one term as the tag of the 
whole term. But here it simply takes the tag of the 
last word in a term. This is based on the assump-
tion that each term has a headword which in most 
cases is the last word in a term (Hui Wang, 2006). 
One additional experiment has been done to verify 
this assumption. A manually annotated Chinese 
shallow Treebank in general domain is used for the 
statistic work (Ruifeng Xu et al, 2005). There are 
9 different structures of Chinese phrases, (Yunfang 
Wu et al, 2003), but only 3 of them do not have 
their head words in the tail, which are about 6.56% 
from all phrases. Following the examples earlier, 
   
1 Copyright ? Institute of Computing Technology, Chinese 
Academy of Sciences 
The 6th Workshop on Asian Languae Resources, 2008
19
the term ????/a ??/n? (polygonal /a meshes 
/n) will be assigned the tag ?/n? because the last 
word is labeled ?/n?. 
There are a lot of semanteme tags at the end of a 
term. For example, ?/ng? presents single character 
postfix of a noun. But it would be improper if a 
term is tagged as ?/ng?. For example, the term ??
?? ? (decision-making machine) contains two 
segments as  listed with two components ???/n? 
and ??/ng?. It is obvious that ????/ng? is in-
appropriate. Thus the head-word-driven assign-
ment scheme also includes some rules to correct 
this kind of problems. As will be discussed in the 
experiment, the current result of TPoS tagging is 
based on 2 simple induction rules applied in this 
algorithm. 
4 Experiments and Discussions 
The domain corpus used in this work contains 
16 papers selected from different Chinese IT jour-
nals between 1998 and 2000 with over 1,500,000 
numbers of characters. They cover topics in IT, 
such as electronics, software engineering, telecom, 
and wireless communication. The same corpus is 
used by the terminology extraction algorithm de-
veloped in (Luning Ji et al, 2007). In the domain 
of IT, two TermLists are used for the experiment. 
TermList1 is a manually collected and verified 
term list from the selected corpus containing a total 
of 3,343 terms. TermList1 is also referred to as the 
standard answer set to the corpus for evaluation 
purposes. TermList2 is produced by running the 
terminology extraction algorithm in (Van Rees, R, 
2003). TermList2 contains 2,660 items out of 
which 929 of them are verified as terminology and 
1,731 items are not considered terminology ac-
cording to the standard answer above. 
To verify the validity of the proposed method to 
different domains, a term list containing 366 legal 
terms obtained from Google searching results for 
??????? ?(complete dictionary of legal 
terms) is selected for comparison, which is named 
TermList3. 
4.1 Experiment on the Blind Assignment 
Scheme 
The first experiment is designed to examine the 
proportion of nouns in TermList1 and TermList3, 
to validate of the assumption of the blind assign-
ment scheme. In first part of this experiment, all 
the 3,343 terms in TermList1 are tagged as nouns. 
The result shows that the precision of the blind 
assignment scheme is between 78.79% and 84.77%. 
The reason for the range is that there are about 200 
terms in TermList1 which can be considered either 
as nouns, gerunds, or even verbs without reference 
to context. For example, the term ???????
?? (?remote access of local area network? or ?re-
mote access to local area network?) and the term 
???? (polarization or polarize), can be consid-
ered either as nouns if they are regarded as courses 
of events or as verbs if they refer to the actions for 
completing certain work. The specific type is de-
pendent on the context which is not provided with-
out the use of a corpus. However, the experiment 
result does show that in a specific domain, there is 
a much higher percentage of terms that are nouns 
than other tags in general (Hui Wang, 2006). As to 
TermList3, the precision of blind assignment is 
between 65.57% and 70.77% (19 mixed ones). 
TermList2 is the result of a terminology extraction 
algorithm and there are non-term items in the ex-
traction result, so the blind assignment scheme is 
not applied on TermList2. The blue colored bars 
(lighter color) in Figure 1 shows the result of 
TermList1 and TermList3 using the blind assign-
ment scheme which gives the two worst result 
compared to our proposed approach to be dis-
cussed in Section 4.2 
4.2 Experiments on the Head-Word-Driven 
Assignment Scheme 
The experiment in this section was designed to 
validate the proposed head-word-driven assign-
ment scheme. The same experiment is conducted 
on the three term lists respectively, as shown in 
Figure 1 in purple color (darker color). The preci-
sion for assigning TPoS tags to TermList1 is 
93.45%.  By taking the result from a terminology 
extraction algorithm without regards to its potential 
error propagations, the precision of the head-word-
driven assignment scheme for TermList2 is 
94.32%. For TermList3, the precision of PoS tag 
assignment is 90.71%. By comparing to the blind 
assignment scheme, this algorithm has reasonably 
good performance for all three term list with 
precision of over 90%. It also gives 8.7% and 
19.9% improvement for TermList1 and TermList3, 
respectively, as compared to the blind assignment 
The 6th Workshop on Asian Languae Resources, 2008
20
scheme, a reasonably good improvement without a 
heavy cost.  However, there are some abnormali-
ties in these results. Supposedly, TermList1 is a 
hand verified term list in the IT domain and thus its 
result should have less noise and thus should per-
form better than TermList2, which is not the case 
as shown in Figure 1. 
Figure 1 Performance of the Two Assignment 
Schemes on the Three Term Lists 
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
T1 T2 T3
Term Lists
Pr
ec
isi
on
blind assignment
head-driven
assignment
 
By further analyzing the error result, for exam-
ple for TermList1, among these 3,343 terms, about 
219 were given improper tags, such as the term ??
??? (Graphics). In this example, two individual 
words, ???/n? and ??/v?, form a term. So the 
output was ????/v? for taking the tag of the last 
segment. It was a wrong tag because the whole 
term was a noun. In fact, the error is caused by the 
general word PoS tagging algorithm because with-
out context, the most likely tagging of ???, a se-
manteme, is a verb. This kind of errors in se-
manteme tagging appeared in the results of all 
three term lists with 169 from TermList1, 29 from 
TermList2 and 12 from TermList3, respectively. 
This was a kind of errors which can be corrected 
by applying some simple induction rules. For ex-
ample, for all semantemes with multiple tags (in-
cluding noun as in the example), the rule can be 
?tagging terms with noun suffixes as nouns?. For 
example, terms ???/n ?/q? (reform-through-
labor camp) and ????/n ??/n ?/v? (com-
puter graphics) were given different tags using the 
head-word-driven assignment scheme. They were 
assigned as: ????/q? and ???????/v? 
which can be corrected by this rule.   Another kind 
of mistake is related to the suffix tags such as ?/ng? 
(noun suffix) and ?/vg?(verb suffix). For examples, 
???/n ??/n ?/ng? (intellectual property tri-
bunal) and ???/n ?/vg? (data set) will be tagged 
as ?????? /ng? and ???? /vg?, respec-
tively, which are obviously wrong. So, the simple 
rule of ?tagging terms with ?/ng? and ?/vg? to ?/n? 
is applied. The performance of TPoS tag assign-
ment after applying these two fine tuning induction 
rules are shown in Table 1 below. 
Table 1 Influence of Induction Rules on Different 
Term Lists 
Term 
Lists 
Precision 
of tagging
Precision 
after add-
ing induc-
tion rule 
Improve-
ment 
Percentage
TermList1 93.45% 97.03% 3.83% 
TermList2 94.32% 95.41% 1.16% 
TermList3 90.71% 93.99% 3.62% 
It is obvious that with the use of fine tuning us-
ing induction rules, the results are much better. In 
fact the result for TermList1 reached 97.03% 
which is quite close to PoS tagging of general do-
main data. The abnormality also disappeared as the 
performance of TermList1 has the best result. The 
improvement to TermList2 (1.16%) is not as obvi-
ous as that for TermList1 and TermList3, which 
are 3.83% and 3.62%, respectively. This, however, 
is reasonable as TermList2 is produced directly 
from a terminology extraction algorithm using a 
corpus, thus, the results are noisier. 
Further analysis is then conducted on the result 
of TermList2 to examine the influence of non-term 
items to this term list. The non-term items are 
items that are general words or items cannot be 
considered as terminology according to the stan-
dard answer sheet. For example, neither of the 
terms ???? (problem) and ??????? (pat-
tern training is) were considered as terms because 
the former was a general word, and the latter 
should be considered as a fragment rather than a 
word. In fact, in 2,660 items extracted by the algo-
rithm as terminology, only 929 of them are indeed 
terminology (34.92%), and rest of them do not 
qualify as domain specific terms. The result of this 
analysis is listed in Table 2. 
The 6th Workshop on Asian Languae Resources, 2008
21
Table 2 Data Distribution Analysis on TermList2 
Without Induction 
Rules 
Induction Rules 
Applied  
correct 
terms precision 
correct 
terms precision 
Terms 
(929)  879 94.62%  898 96.66%
Non-terms 
(1,731) 1,630 94.17% 1,640 94.74% 
Total 
(2,660) 2,509 94.32% 2,538 95.41% 
Results show that 31 and 50 from the 929 cor-
rect terms were assigned improper PoS tags using 
the proposed algorithm with and without the induc-
tions rules, respectively. That is, the precisions for 
correct data are comparable to that of TermList1 
(93.45% and 97.03%, respectively). For the non-
terms, 91 items and 101 items from 1,731 items 
were assigned improper tags with and without the 
induction rules, respectively. Even though the pre-
cisions for terms and non-terms without using the 
induction rules are quite the same (94.62% vs. 
94.17%), the improvement for the non-terms using 
the induction rules are much less impressive than 
that for the terms. This is the reason for the rela-
tively less impressed performance of induction 
rules for TermList2.  It is interesting to know that, 
even though the performance of the terminology 
extraction algorithm is quite poor with precision of 
only around 35% (929 out of 2,666 terms), it does 
not affect too much on the performance of the 
TPoS proposed in this paper. This is mainly be-
cause the items extracted are still legitimate words, 
compounds, or phrases which are not necessarily 
domain specific. 
The proposed algorithm in this paper use mini-
mum resources. They need no training process and 
even no context information. But the performance 
of the proposed algorithm is still quite good and 
can be directly used as a preparation work for do-
main ontology construction because of its presion 
of over 95%. Other PoS tagging algorithms reach 
good performance in processing general words. 
For example, a k-nearest-neighbors strategy to 
identify possible PoS tags for Chinese words can 
reach 90.25% for general word PoS tagging 
(Maosong Sun et al, 2000). Another method based 
on SVM method on English corpus can reach 
96.9% in PoS tagging known and unknown words 
(Nakagawa T, 2001). These results show that pro-
posed method in this paper is comparable to these 
general PoS tagging algorithms in magnitude. Of 
course, one main reason of this fact is the differ-
ence in its objectives. The proposed method is for 
the PoS tagging of domain specific terms which 
have much less ambiguity than tagging of general 
text. Domain specific terms are more likely to be 
nouns and there are some rules in the word-
formation patterns while general PoS tagging algo-
rithms usually need training process in which large 
manually labeled corpora would be involved. Ex-
periment results also show that this simple method 
can be applied to data in different domains. 
5 Conclusion and Future Work 
In this paper, a simple but effective method for 
assigning PoS tags to domain specific terms was 
presented. This is a preliminary classification work 
on terms. It needs no training process and not even 
context information. Yet it obtains a relatively 
good result. The method itself is not domain de-
pendent, thus it is applicable to different domains. 
Results show that in certain applications, a simple 
method may be more effective under similar cir-
cumstances. The algorithm can still be investigated 
over the use of more induction rules. Some context 
information, statistics of word/tag usage can also 
be explored. 
Acknowledgments 
This project is partially supported by CERG 
grants (PolyU 5190/04E and PolyU 5225/05E) and 
B-Q941 (Acquisition of New Domain Specific 
Concepts and Ontology Update). 
References 
Yun Li, Qiangjun Wang. 2001. Automatic Term Extrac-
tion in the Field of Information Technology. In the 
proceedings of The Conference of 20th Anniversary 
for Chinese Information Processing Society of China.  
Yan He, Zhifang Sui, Huiming Duan, and Shiwen Yu. 
2006. Term Mining Combining Term Component 
Bank. In Computer Engineering and Applications. 
Vol.42 No.33,4--7. 
Feng Zhang, Xiaozhong Fan, and Yun Xu. 2006. Chi-
nese Term Extraction Based on PAT Tree. Journal of 
Beijing Institute of Technology. Vol. 15, No. 2. 
Van Rees, R. 2003. Clarity in the Usage of the Terms 
Ontology, Taxonomy and Classification. CIB73. 
The 6th Workshop on Asian Languae Resources, 2008
22
Mikheev, A. 1997. Automatic Rule Induction. for Un-
known Word Guessing. In Computational Lingusitics 
Vol. 23(3), ACL.  
Toutanova, Kristina, Dan Klein, Christopher Manning, 
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network. 
In proceedings of HLT-NAACL. 
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005.Morphological Features Help POS Tag-
ging of Unknown Words across Language Varieties. 
In proceedings of the Fourth SIGHAN Workshop on 
Chinese Language Processing. 
Samuelsson, Christer. 1993. Morphological Tagging 
Based Entirely on Bayesian Inference. In proceedings 
of NCCL 9. 
Brants, Thorsten. 2000. TnT: A Statistical Part-of-
Speech Tagger. In proceedings of ANLP 6. 
G. Orphanos, and D. Christodoulakis. 1999. POS Dis-
ambiguation and Unknown Word Guessing with De-
cision Trees. In proceedings of EACL?99, 134--141. 
H Schmid. 1994. Probabilistic Part-of-Speech Tagging 
Using Decision Trees. In proceedings of International 
Conference on New Methods in Language Processing.  
Maosong Sun, Dayang Shen, and Changning Huang. 
1997. CSeg& Tag1.0: a practical word segmenter 
and POS tagger for Chinese texts. In proceedings of 
the fifth conference on applied natural language 
processing.  
Ying Liu. 2002. Analysing Chinese with Rule-based 
Method Combined with Statistic-based Method. In 
Computer Engineering and Applications, Vol.7. 
Mikheev, A. 1996. Unsupervised Learning of Word-
Category Guessing Rules. In proceedings of ACL-96.  
Nakagawa T, Kudoh T, and Matsumoto Y. 2001. Un-
known Word Guessing and Part-of-Speech Tagging 
Using Support Vector Machines. In proceedings of 
NLPPRS 6, 325--331. 
Maosong Sun, Zhengping Zuo, and B K, TSOU. 2000. 
Part-of-Speech Identification for Unknown Chinese 
Words Based on K-Nearest-Neighbors Strategy. In 
Chinese Journal of Computers. Vol.23 No.2: 166--
170. 
Luning Ji, Mantai Sum, Qin Lu, Wenjie Li, Yirong 
Chen. 2007. Chinese Terminology Extraction using 
Window-based Contextual Information. In proceed-
ings of CICLING. 
Huaping Zhang et al 2003. HHMM-based Chinese 
Lexical Analyzer ICTCLAS. Second SIGHAN work-
shop affiliated with 41th ACL, 184--187. Sapporo 
Japan. 
Hui Wang. Last checked: 2007-08-04. Statistical studies 
on Chinese vocabulary (???????? ). 
http://www.huayuqiao.org/articles/wanghui/wanghui
06.doc. The date of publication is unknown from the 
online source. 
Ruifeng Xu, Qin Lu, Yin Li and Wanyin Li. 2005. The 
Design and Construction of the PolyU Shallow Tree-
bank. International Journal of Computational Lin-
guistics and Chinese Language Processing, V.10 N.3. 
Yunfang Wu, Baobao Chang and Weidong Zhan. 2003. 
Building Chinese-English Bilingual Phrase Database. 
Page 41-45, Vol. 4. 
The 6th Workshop on Asian Languae Resources, 2008
23
The 6th Workshop on Asian Languae Resources, 2008
24
