Proceedings of NAACL HLT 2007, Companion Volume, pages 201?204,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Three-step Deterministic Parser for Chinese Dependency Parsing 
 
Kun Yu Sadao Kurohashi Hao Liu 
Graduate School of Informatics Graduate School of Informatics Graduate School of Information 
Science and Technology 
Kyoto University Kyoto University The University of Tokyo 
kunyu@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp liuhao@kc.t.u-tokyo.ac.jp 
 
Abstract 
  This paper presents a three-step dependency 
parser to parse Chinese deterministically. By divid-
ing a sentence into several parts and parsing them 
separately, it aims to reduce the error propagation 
coming from the greedy characteristic of determi-
nistic parsing. Experimental results showed that 
compared with the deterministic parser which 
parsed a sentence in sequence, the proposed parser 
achieved extremely significant improvement on 
dependency accuracy.  
1 Introduction 
Recently, as an attractive alternative to probabilistic 
parsing, deterministic parsing (Yamada and Matsumoto, 
2003; Nivre and Scholz, 2004) has drawn great attention 
with its high efficiency, simplicity and good accuracy 
comparable to the state-of-the-art generative probabilis-
tic models. The basic idea of deterministic parsing is 
using a greedy parsing algorithm that approximates a 
globally optimal solution by making a sequence of lo-
cally optimal choices (Hall et al, 2006). This greedy 
idea guarantees the simplicity and efficiency, but at the 
same time it also suffers from the error propagation 
from the previous parsing choices to the left decisions.  
For example, given a Chinese sentence, which means 
Paternity test is a test that gets personal identity 
through DNA analysis, and it brings proof for finding 
lost children, the correct dependency tree is shown by 
solid line  (see Figure 1). But, if word ??(through) is 
incorrectly parsed as depending on word ?(is) (shown 
by dotted line), this error will result in the incorrect 
parse of word??(a test) as depending on word ??
(brings) (shown by dotted line).  
This problem exists not only in Chinese, but also in 
other languages. Some efforts have been done to solve 
this problem. Cheng et al (2005) used a root finder to 
divide one sentence into two parts by the root word and 
parsed them separately. But the two-part division is not 
enough when a sentence is composed of several coordi-
nating sub-sentences. Chang et al (2006) applied a 
pipeline framework in their dependency parser to make 
the local predictions more robust. While it did not show 
great help for stopping the error propagation between 
different parsing stages.  
 
Figure 1. Dependency tree of a sentence  (word sequence is top-down) 
This paper focuses on resolving this issue for Chi-
nese. After analyzing the dependency structure of sen-
tences in Penn Chinese Treebank 5.1 (Xue et al, 2002), 
we found an interesting phenomenon: if we define a 
main-root as the head of a sentence, and define a sub-
sentence as a sequence of words separated by punctua-
tions, and the head1 of these words is the child of main-
root or main-root itself, then the punctuations that de-
pend on main-root can be a separator of sub-sentences.  
For example, in the example sentence there are three 
punctuations marked as PU_A, PU_B and PU_C, in 
which PU_B and PU_C depends on main-root but 
PU_A depends on word ??(gets). According to our 
observation, PU_B and PU_C can be used for segment-
ing this sentence into two sub-sentences A and B (cir-
cled by dotted line in Figure 2), where the sub-root of A 
is main-root and the sub-root of B depends on main-root.  
This phenomenon gives us a useful clue: if we divide 
a sentence by the punctuations whose head is main-root, 
then the divided sub-sentences are basically independ-
ent of each other, which means we can parse them sepa-
rately. The shortening of sentence length and the recog-
nition of sentence structure guarantee the robustness of 
deterministic parsing. The independent parsing of each 
sub-sentence also prevents the error-propagation. In 
                                                 
1
 The head of sub-sentence is defined as a sub-root. 
201
addition, because the sub-root depends on main-root or 
is main-root itself, it is easy to combine the dependency 
structure of each sub-sentence to create the final de-
pendency tree. 
 
Figure 2. A segmentation of the sentence in Figure 1 
Based on above analyses, this paper proposes a three-
step deterministic dependency parser for Chinese, which 
works as: 
Step1(Sentence Segmentation): Segmenting a sen-
tence into sub-sentences by punctuations (sub-sentences 
do not contain the punctuations for segmentation); 
Step2(Sub-sentence Parsing): Parsing each sub-
sentence deterministically; 
Step3(Parsing Combination): Finding main-root 
among all the sub-roots, then combining the dependency 
structure of sub-sentences by making main-root as the 
head of both the left sub-roots and the punctuations for 
sentence segmentation. 
2 Sentence Segmentation 
As mentioned in section 1, the punctuations depending 
on main-root can be used to segment a sentence into 
several sub-sentences, whose sub-root depends on main-
root or is main-root. But by analysis, we found only 
several punctuations were used as separator commonly. 
To ensure the accuracy of sentence segmentation, we 
first define the punctuations which are possible for seg-
mentation as valid punctuation, which includes comma, 
period, colon, semicolon, question mark, exclamatory 
mark and ellipsis. Then the task in step 1 is to find 
punctuations which are able to segment a sentence from 
all the valid punctuations in a sentence, and use them to 
divide the sentence into two or more sub-sentences. 
We define a classifier (called as sentence seg-
menter) to classify the valid punctuations in a sentence 
to be good or bad for sentence segmentation. SVM (Se-
bastiani, 2002) is selected as classification model for its 
robustness to over-fitting and high performance.  
Table 1 shows the binary features defined for sen-
tence segmentation. We use a lexicon consisting of all 
the words in Penn Chinese Treebank 5.1 to lexicalize 
word features. For example, if word ? (for) is the 
27150th word in the lexicon, then feature Word1 of 
PU_B (see Figure 2) is ?27150:1?. The pos-tag features 
are got in the same way by a pos-tag list containing 33 
pos-tags, which follow the definition in Penn Chinese 
Treebank. Such method is also used to get word and 
pos-tag features in other modules. 
Table 1. Features for sentence segmenter 
Feature Description 
Wordn/Posn word/pos-tag in different position, n=-2,-1,0,1,2 
Word_left/ 
Pos_left 
word/pos-tag between the first left valid punctua-
tion and current punctuation 
Word_right/ 
Pos_right 
word/pos-tag between current punctuation and 
the first right valid punctuation 
#Word_left/ 
#Word_right 
if the number of words between the first left/right 
valid punctuation and current punctuation is 
higher than 2, set as 1; otherwise set as 0 
V_left/ 
V_right 
if there is a verb between the first left/right valid 
punctuation and current punctuation, set as 1; 
otherwise set as 0 
N_leftFirst/ 
N_rightFirst 
if the left/right neighbor word is a noun, set as 1; 
otherwise set as 0 
P_rightFirst/ 
CS_rightFirst 
if the right neighbor word is a preposi-
tion/subordinating conjunction, set as 1; other-
wise set as 0 
3 Sub-sentence Parsing  
3.1 Parsing Algorithm 
The parsing algorithm in step 2 is a shift-reduce parser 
based on (Yamada and Matsumoto, 2003). We call it as 
sub-sentence parser. 
Two stacks P and U are defined, where stack P keeps 
the words under consideration and stack U remains all 
the unparsed words. All the dependency relations cre-
ated by the parser are stored in queue A.  
At start, stack P and queue A are empty and stack U 
contains all the words. Then word on the top of stack U 
is pushed into stack P, and a trained classifier finds 
probable action for word pair <p,u> on the top of the 
two stacks. After that, according to different actions, 
dependency relations are created and pushed into queue 
A, and the elements in the two stacks move at the same 
time. Parser stops when stack U is empty and the de-
pendency tree can be drawn according to the relations 
stored in queue A.  
Four actions are defined for word pair <p, u>: 
LEFT: if word p modifies word u, then push pu 
into A and push u into P. 
RIGHT: if word u modifies word p, then push up 
into A and pop p. 
REDUCE: if there is no word u? (u??U and u??u) 
which modifies p, and word next to p in stack P is p?s 
head, then pop p. 
SHIFT: if there is no dependency relation between p 
and u, and word next to p in stack P is not p?s head, then 
push u into stack P. 
202
We construct a classifier for each action separately, 
and classify each word pair by all the classifiers. Then 
the action with the highest classification score is se-
lected. SVM is used as the classifier, and One vs. All 
strategy (Berger, 1999) is applied for its good efficiency 
to extend binary classifier to multi-class classifier. 
3.2 Features 
Features are crucial to this step. First, we define some 
features based on local context (see Flocal in Table 2), 
which are often used in other deterministic parsers 
(Yamada and Matsumoto, 2003; Nivre et al, 2006). 
Then, to get top-down information, we add some global 
features (see Fglobal in Table 2). All the features are bi-
nary features, except that Distance is normalized be-
tween 0-1 by the length of sub-sentence.  
Before parsing, we use a root finder (i.e. the sub-
sentence root finder introduced in Section 4) to get 
Rootn feature, and develop a baseNP chunker to get 
BaseNPn feature. In the baseNP chunker, IOB represen-
tation is applied for each word, where B means the word 
is the beginning of a baseNP, I means the word is inside 
of a baseNP, and O means the word is outside of a 
baseNP. Tagging is performed by SVM with One vs. All 
strategy. Features used in baseNP chunking are current 
word, surrounding words and their corresponding pos-
tags. Window size is 5. 
Table 2. Features for sub-sentence parser 
Feature Description 
Wordn/ 
Posn 
word/pos-tag in different position, 
n= P0, P1, P2, U0, U1, U2 (Pi/Ui mean 
the ith position from top in stack P/U) 
Word_childn/ 
Pos_childn 
the word/pos-tag of the children of 
Wordn, n= P0, P1, P2, U0, U1, U2 
Local 
Feature 
(Flocal) 
Distance distance between p and u in sentence 
Rootn 
if Wordn is the sub-root of this sub-
sentence, set as 1; otherwise set as 0 
Global 
Feature 
(Fglobal) BaseNPn baseNP tag of Wordn 
Table 3. Features for sentence/sub-sentence root finder 
Feature Description 
Wordn/Posn words in different position, n=-2,-1,0,1,2 
Word_left/Pos_left wordn/posn where n<-2 
Word_right/Pos_right wordn/posn where n>2 
#Word_left/ 
#Word_right 
if the number of words between the 
start/end of sentence and current word is 
higher than 2, set as 1; otherwise set as 0 
V_left/V_right 
if there is a verb between the start/end of 
sentence and current word, set as 1; oth-
erwise set as 0 
Nounn/Verbn/Adjn 
if the word in different position is a 
noun/verb/adjective, set as 1; otherwise 
set as 0. n=-2,-1,1,2 
Dec_right if the word next to current word in right 
side is ?(of), set as 1; otherwise set as 0 
CC_left 
if there is a coordinating conjunction 
between the start of sentence and current 
word, set as 1; otherwise set as 0 
BaseNPn baseNP tag of Wordn 
4 Parsing Combination 
A root finder is developed to find main-root for parsing 
combination. We call it as sentence root finder. We 
also retrain the same module to find the sub-root in step 
2, and call it as sub-sentence root finder. 
We define the root finding problem as a classification 
problem. A classifier, where we still select SVM, is 
trained to classify each word to be root or not. Then the 
word with the highest classification score is chosen as 
root. All the binary features for root finding are listed in 
Table 3. Here the baseNP chunker introduced in section 
3.2 is used to get the BaseNPn feature. 
5 Experimental Results 
5.1 Data Set and Experimental Setting 
We use Penn Chinese Treebank 5.1 as data set. To 
transfer the phrase structure into dependency structure, 
head rules are defined based on Xia?s head percolation 
table (Xia and Palmer, 2001). 16,984 sentences and 
1,292 sentences are used for training and testing. The 
same training data is also used to train the sentence 
segmenter, the baseNP chunker, the sub-sentence root 
finder, and the sentence root finder. During both train-
ing and testing, the gold-standard word segmentation 
and pos-tag are applied. 
TinySVM is selected as a SVM toolkit. We use a 
polynomial kernel and set the degree as 2 in all the ex-
periments.  
5.2 Three-step Parsing vs. One-step Parsing 
First, we evaluated the dependency accuracy and root 
accuracy of both three-step parsing and one-step parsing. 
Three-step parsing is the proposed parser and one-step 
parsing means parsing a sentence in sequence (i.e. only 
using step 2). Local and global features are used in both 
of them. 
Results (see Table 4) showed that because of the 
shortening of sentence length and the prevention of er-
ror propagation three-step parsing got 2.14% increase 
on dependency accuracy compared with one-step pars-
ing. Based on McNemar?s test (Gillick and Cox, 1989), 
this improvement was considered extremely statistically 
significant (p<0.0001).  In addition, the proposed parser 
got 1.01% increase on root accuracy.  
Table 4. Parsing result of three-step and one-step parsing 
Parsing Strategy Dep.Accu. (%) 
Root Accu. 
(%) 
Avg. Parsing 
Time (sec.) 
One-step Parsing 82.12 74.92 22.13 
Three-step Parsing 84.26 (+2.14) 
75.93 
(+1.01) 
24.27 
(+2.14) 
Then we tested the average parsing time for each sen-
tence to verify the efficiency of proposed parser. The 
average sentence length is 21.68 words. Results (see 
Table 4) showed that compared with one-step parsing, 
the proposed parser only used 2.14 more seconds aver-
203
agely when parsing one sentence, which did not affect 
efficiency greatly. 
To verify the effectiveness of proposed parser on 
complex sentences, which contain two or more sub-
sentences according to our definition, we selected 665 
such sentences from testing data set and did evaluation 
again. Results (see Table 5) proved that our parser 
outperformed one-step parsing successfully.  
Table 5. Parsing result of complex sentence 
Parsing Strategy Dep.Accu. (%) Root Accu. (%) 
One-step Parsing 82.56 78.95 
Three-step Parsing 84.94 (+2.38) 79.25 (+0.30) 
5.3 Comparison with Others? Work 
At last, we compare the proposed parser with Nivre?s 
parser (Hall et al, 2006). We use the same head rules 
for dependency transformation as what were used in 
Nivre?s work. We also used the same training (section 
1-9) and testing (section 0) data and retrained all the 
modules. Results showed that the proposed parser 
achieved 84.50% dependency accuracy, which was 
0.20% higher than Nivre?s parser (84.30%).  
6 Discussion 
In the proposed parser, we used five modules: sentence 
segmenter (step1); sub-sentence root finder (step2); 
baseNP chunker (step2&3); sub-sentence parser (step2); 
and sentence root finder (step3).  
The robustness of the modules will affect parsing ac-
curacy. Thus we evaluated each module separately. Re-
sults (see Table 6) showed that all the modules got rea-
sonable accuracy except for the sentence root finder. 
Considering about this, in step 3 we found main-root 
only from the sub-roots created by step 2. Because the 
sub-sentence parser used in step 2 had good accuracy, it 
could provide relatively correct candidates for main-root 
finding. Therefore it helped decrease the influence of 
the poor sentence root finding to the proposed parser. 
Table 6. Evaluation result of each module 
Module F-score(%) Dep.Accu(%) 
Sentence Segmenter (M1) 88.04 --- 
Sub-sentence Root Finder (M2) 88.73 --- 
BaseNP Chunker (M3) 89.25 --- 
Sub-sentence Parser (M4) --- 85.56 
Sentence Root Finder (M5) 78.01 --- 
Then we evaluated the proposed parser assuming us-
ing gold-standard modules (except for sub-sentence 
parser) to check the contribution of each module to 
parsing. Results (see Table 7) showed that (1) the accu-
racy of current sentence segmenter was acceptable be-
cause only small increase on dependency accuracy and 
root accuracy was got by using gold-standard sentence 
segmentation; (2) the correct recognition of baseNP 
could help improve dependency accuracy but gave a 
little contribution to root accuracy; (3) the accuracy of 
both sub-sentence root finder and sentence root finder 
was most crucial to parsing. Therefore improving the 
two root finders is an important task in our future work. 
Table 7. Parsing result with gold-standard modules 
Gold-standard Module Dep.Accu(%) Root.Accu(%) 
w/o 84.26 75.93 
M1 84.51 76.24 
M1+M2 86.57 80.34 
M1+M2+M3 88.63 80.57 
M1+M2+M3+M5 91.25 91.02 
7 Conclusion and Future Work 
We propose a three-step deterministic dependency 
parser for parsing Chinese. It aims to solve the error 
propagation problem by dividing a sentence into inde-
pendent parts and parsing them separately. Results 
based on Penn Chinese Treebank 5.1 showed that com-
pared with the deterministic parser which parsed a sen-
tence in sequence, the proposed parser achieved ex-
tremely significant increase on dependency accuracy. 
Currently, the proposed parser is designed only for 
Chinese. But we believe it can be easily adapted to other 
languages because no language-limited information is 
used. We will try this work in the future. In addition, 
improving sub-sentence root finder and sentence root 
finder will also be considered in the future. 
Acknowledgement 
We would like to thank Dr. Daisuke Kawahara and Dr. Eiji Aramaki 
for their helpful discussions. We also thank the three anonymous 
reviewers for their valuable comments. 
Reference 
A.Berger. Error-correcting output coding for text classification. 1999. 
In Proceedings of the IJCAI-99 Workshop on Machine Learning 
for Information Filtering. 
M.Chang, Q.Do and D.Roth. 2006. A Pipeline Framework for De-
pendency Parsing. In Proceedings of Coling-ACL 2006. 
Y.Cheng, M.Asahara and Y.Matsumoto. 2005. Chinese Deterministic 
Dependency Analyzer: Examining Effects of Global Features and 
Root Node Finder. In Proceedings of IJCNLP 2005.  
L.Gillick and S.J.Cox. 1989. Some Statistical Issues in the Compari-
son of Speech Recognition Algorithms. In Proceedings of ICASSP.  
J.Hall, J.Nivre and J.Nilsson. 2006. Discriminative Classifiers for 
Deterministic Dependency Parsing. In Proceedings of Coling-ACL 
2006. pp. 316-323. 
J.Nivre and M.Scholz. 2004. Deterministic Dependency Parsing of 
English Text. In Proceedings of Coling 2004. pp. 64-70. 
F.Sebastiani. 2002. Machine learning in automated text categorization. 
ACM Computing Surveys, 34(1): 1-47. 
F.Xia and M.Palmer. 2001. Converting Dependency Structures to 
Phrase Structures. In HLT-2001. 
N.Xue, F.Chiou and M.Palmer. 2002. Building a Large-Scale Anno-
tated Chinese Corpus. In Proceedings of COLING 2002. 
H.Yamada and Y.Matsumoto. 2003. Statistical Dependency Analysis 
with Support Vector Machines. In Proceedings of IWPT. 2003. 
204
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 146?149,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Word Segmentation and Named Entity Recognition by 
Character Tagging 
 
Kun Yu1      Sadao Kurohashi2     Hao Liu1     Toshiaki Nakazawa1 
 Graduate School of Information Science and Technology, The University of 
Tokyo, Tokyo, Japan, 113-86561 
Graduate School of Informatics, Kyoto University, Kyoto, Japan, 606-85012 
 {kunyu, liuhao, nakazawa}@kc.t.u-tokyo.ac.jp1 
kuro@i.kyoto-u.ac.jp2 
  
Abstract 
This paper describes our word segmenta-
tion system and named entity recognition 
(NER) system for participating in the 
third SIGHAN Bakeoff. Both of them are 
based on character tagging, but use dif-
ferent tag sets and different features. 
Evaluation results show that our word 
segmentation system achieved 93.3% and 
94.7% F-score in UPUC and MSRA open 
tests, and our NER system got 70.84% 
and 81.32% F-score in LDC and MSRA 
open tests. 
1 Introduction 
Dealing with word segmentation as character 
tagging showed good results in last SIGHAN 
Bakeoff (J.K.Low et al,2005). It is good at un-
known word identification, but only using char-
acter-level features sometimes makes mistakes 
when identifying known words (T.Nakagawa, 
2004). Researchers use word-level features 
(J.K.Low et al,2005) to solve this problem. 
Based on this idea, we develop a word segmenta-
tion system based on character-tagging, which 
also combine character-level and word-level fea-
tures. In addition, a character-based NER module 
and a rule-based factoid identification module 
are developed for post-processing.  
Named entity recognition based on character-
tagging has shown better accuracy than word-
based methods (H.Jing et al,2003). But the small 
window of text makes it difficult to recognize the 
named entities with many characters, such as 
organization names (H.Jing et al,2003). Consid-
ering about this, we developed a NER system 
based on character-tagging, which combines 
word-level and character-level features together. 
In addition, in-NE probability is defined in this 
system to remove incorrect named entities and 
create new named entities as post-processing. 
2 Character Tagging for Word 
Segmentation and NER 
2.1 Basic Model 
We look both word segmentation and NER as 
character tagging, which is to find the tag se-
quence T* with the highest probability given a 
sequence of characters S=c1c2?cn.  
)|(maxarg* STPT
T
=  (1) 
Then we assume that the tagging of one char-
acter is independent of each other, and modify 
formula 1 as 
?
=
=
=
=
=
n
i
ii
tttT
nn
tttT
ctP
ccctttPT
n
n
1...
2121
...
*
)|(maxarg     
)...|...(maxarg
21
21
 (2) 
 Beam search (n=3) (Ratnaparkhi,1996) is ap-
plied for tag sequence searching, but we only 
search the valid sequences to ensure the validity 
of searching result. SVM is selected as the basic 
classification model for tagging because of its 
robustness to over-fitting and high performance 
(Sebastiani, 2002). To simplify the calculation, 
the output of SVM is regarded as P(ti|ci). 
2.2 Tag Definition 
Four tags ?B, I, E, S? are defined for the word 
segmentation system, in which ?B? means the 
character is the beginning of one word, ?I? means 
the character is inside one word, ?E? means the 
character is at the end of one word and ?S? means 
the character is one word by itself. 
For the NER system, different tag sets are de-
fined for different corpuses. Table 1 shows the 
146
tag set defined for MSRA corpus. It is the prod-
uct of Segment-Tag set and NE-Tag set, because 
not only named entities but also words are seg-
mented in this corpus. Here NE-Tag ?O? means 
the character does not belong to any named enti-
ties. For LDC corpus, because there is no seg-
mentation information, we delete NE-Tag ?O? 
but add tag ?NONE? to indicate the character 
does not belong to any named entities (Table 2). 
Table 1 Tags of NER for MSRA corpus 
Segment-Tag NE-Tag 
B, I, E, S ? PER, LOC, ORG, O 
Table 2 Tags of NER for LDC corpus 
Segment Tag NE Tag 
B, I, E, S ? PER, LOC, ORG, GPE + NONE 
2.3 Feature Definition 
First, some features based on characters are 
defined for the two tasks, which are: 
(a) Cn (n=-2,-1,0,1,2) 
(b) Pu(C0) 
Feature Cn (n=-2,-1,0,1,2) mean the Chinese 
characters appearing in different positions (the 
current character and two characters to its left 
and right), and they are binary features. A char-
acter list, which contains all the characters in the 
lexicon introduced later, is used to identify them.
 
Besides of that, feature Pu(C0) means whether C0 
is in a punctuation character list. It is also binary 
feature and all the punctuations in the punctua-
tion character list come from Penn Chinese Tree-
bank 5.1 (N.Xue et al,2002). 
In addition, we define some word-level fea-
tures based on a lexicon to enlarge the window 
size of text in the two tasks, which are:  
(c) Wn (n=-1,0,1) 
Feature Wn (n=-1,0,1) mean the lexicon words 
in different positions (the word containing C0 
and one word to its left and right) and they are 
also binary features. We select all the possible 
words in the lexicon that satisfy the requirements, 
not like only selecting the longest one in 
(J.K.Low et al,2005). To create the lexicon, we 
use following steps. First, a lexicon from NICT 
(National Institute of Information and Communi-
cations Technology, Japan) is used as the basic 
lexicon, which is extracted from Peking Univer-
sity Corpus of the second SIGHAN Bakeoff 
(T.Emerson, 2005), Penn Chinese Treebank 4.0 
(N.Xue et al,2002), a Chinese-to-English Word-
list1  and part of NICT corpus (K.Uchimoto et 
al.,2004; Y.J.Zhang et al,2005). Then, all the 
words containing digits and letters are removed 
                                                 
1
 http://projects.ldc.upenn.edu/Chinese/  
from this lexicon. At last, all the punctuations in 
Penn Chinese Treebank 5.1 (N.Xue et al,2002) 
and all the words in the training data of UPUC 
and MSRA corpuses are added into the lexicon.  
Besides of above features, some extra features 
are defined only for NER task. 
First, we add some character-based features to 
improve the accuracy of person name recognition, 
which are CNn (n=-2,-1,0,1,2). They mean 
whether C
 n (n=-2,-1,0,1,2) belong to a Chinese 
surname list. All of them are binary features. The 
Chinese surname list contains the most famous 
100 Chinese surnames, such as ?, ?, ?, ? 
(Zhao, Qian, Sun, Li). 
Then, we add some word-based features to 
help identify the organization name, which are 
WORGn (n=-1,0,1). They mean whether W n (n= 
-1,0,1) belong to an organization suffix list. All 
of them are also binary features. The organiza-
tion suffix list is created by extracting the last 
word from all the organization names in the 
training data of both MSRA and LDC corpuses. 
3 Post-processing 
Besides of the basic model, a NER module 
and a factoid identification module are developed 
in our word segmentation system for post-
processing. In addition, we define in-NE prob-
ability to delete the incorrect named entities and 
identify new named entities in the post-
processing phrase of our NER system. 
3.1 Named Entity Recognition for Word 
Segmentation 
In this module, if two or more segments in the 
outputs of basic model are recognized as one 
named entity, we combine them as one segment.  
This module uses the same basic NER model 
as what we introduced in the previous section. 
But it only identifies person and location names, 
because organization names often contain more 
than one word. In addition, to keep the high ac-
curacy of person name recognition, the features 
about organization suffixes are not used here.  
3.2 Factoid Identification for Word Seg-
mentation 
Rules are used to identify the following fac-
toids among the segments from the basic word 
segmentation model:  
NUMBER: Integer, decimal, Chinese number 
PERCENT: Percentage and fraction 
DATE: Date 
FOREIGN: English words 
147
Table 3 shows some rules defined here. 
Table 3 Some Rules for Factoid Identification 
Factoid Rule 
NUMBER If previous segment ends with DIGIT and current 
segment starts with DIGIT, then combine them. 
PERCENT If previous segment is composed of DIGIT and 
current segment equals ?%?, then combine them. 
DATE 
If previous segment is composed of DIGIT and 
current segment is in the list of ??, ?, ?, ? 
(Year, Month, Day, Day)?, then combine them. 
FOREIGN Combine the consequent letters as one segment. 
(DIGIT means both Arabic and Chinese numerals) 
3.3 NER Deletion and Creation 
In-word probability has been used in unknown 
word identification successfully (H.Q.Li et al, 
2004). Accordingly, we define in-NE probability 
to help delete and create named entities (NE). 
Formula 3 shows the definition of in-NE prob-
ability for character sequence cici+1?ci+n. Here ?# 
of cici+1?ci+n as NE? is defined as TimeInNE and 
the occurrence of cici+1?ci+n in different type of 
NE is treated differently. 
data in testing ... of #
NE as ... of #)...(
1
1
1
niii
niii
niiiInNE
ccc
ccc
cccP
++
++
++ =
 (3) 
Then, we use some criteria to delete the incor-
rect NE and create new possible NE, in which 
different thresholds are set for different tasks. 
Criterion 1: If PInNE(cici+1?ci+n) of one NE 
type is lower than TDel, and TimeInNE(cici+1?ci+n) 
of the same NE type is also lower than TTime, then 
delete this type of NE composed of cici+1?ci+n.  
Criterion 2: If PInNE(cici+1?ci+n) of one NE 
type is higher than TCre, and in other places the 
character sequence cici+1?ci+n does not belong to 
any NE, then create a new NE containing 
cici+1?ci+n with this NE type.  
4 Evaluation Results and Discussion 
4.1 Evaluation Setting 
SVMlight (T.Joachims, 1999) was used as 
SVM tool. In addition, we used the MSRA train-
ing corpus of NER task in this Bakeoff to train 
our NER post-processing module. 
4.2 Results of Word Segmentation 
We attended the open track of word segmenta-
tion task for two corpuses: UPUC and MSRA. 
Table 4 shows the evaluation results. 
Table 4 Results of Word Segmentation Task (in percentage %) 
Corpus Pre. Rec. F-score Roov Riv 
UPUC 94.4 92.2 93.3 68.0 97.0 
MSRA 94.0 95.3 94.7 50.3 96.9 
The F-score of our word segmentation system 
in UPUC corpus ranked 4th (same as that of the 
3rd group) among all the 8 participants. And it 
was only 1.1% lower than the highest one and 
0.2% lower than the second one. It showed that 
our character-tagging approach was feasible. But 
the F-score of MSRA corpus was only higher 
than one participant in all the 10 groups (the 
highest one was 97.9%). Error analysis shows 
that there are two main reasons.  
First, in MSRA corpus, they tend to segment 
one organization name as one word, such as ?
?????(China Chamber of Commerce in 
USA). But our basic segmentation model seg-
mented such word into several words, e.g. ??/
??/??(USA/China/Chamber of Commerce), 
and our post-processing NER module does not 
consider about organization names.  
Second, our factoid identification rule did not 
combine the consequent DATE factoids into one 
word, but they are combined in MSRA corpus. 
For example, our system segmented the word?
? 9?? (9 o?clock in the evening) into three 
parts ??/9 ?/? (Evening/9 o?clock/Exact). 
This error can be solved by revising the rules for 
factoid identification. 
Besides of that, we also found although our 
large lexicon helped identify the known word 
successfully, it also decreased the recall of OOV 
words (our Riv of UPUC corpus ranked 2nd, with 
only 0.6% decrease than the highest one, but 
Roov ranked 4th, with 8.8% decrease than the 
highest one). The large size of this lexicon is 
looked as the main reason.  
Our lexicon contains 221,407 words, in which 
6,400 words are single-character words. It made 
our system easy to segment one word into sev-
eral words, for example word ??? (Economy 
Group) in UPUC corpus was segmented into?
?  (Economy) and ? (Group). Moreover, the 
large size of this lexicon also brought errors of 
combining two words into one word if the word 
was in the lexicon. For example, words ? (Only) 
and ? (Have) in MSRA corpus were identified 
as one word because there existed the word?? 
(Only) in our lexicon. We will reduce our lexi-
con to a reasonable size to solve these problems. 
4.3 Results of NER 
We also attended the open track of NER task 
for both LDC corpus and MSRA corpus. Table 5 
and Table 6 give the evaluation results.  
There were only 3 participants in the open 
track of LDC corpus and our group got the best 
F-score. In addition, among all the 11 partici-
pants for MSRA corpus, our system ranked 6th 
148
by F-score. It showed the validity of our charac-
ter-tagging method for NER. But for location 
name (LOC) in LDC corpus, both the precision 
and recall of our NER system were very low. It 
was because there were too few location names 
in the training data (there were only 476 LOC in 
the training data, but 5648 PER, 5190 ORG and 
9545 GPE in the same data set). 
Table 5 Results of NER Task for LDC corpus (in percentage %) 
 PER LOC ORG GPE Overall 
Pre. 83.29 58.52 61.48 78.66 76.16 
Rec. 66.93 18.87 45.19 79.94 66.21 
F-score 74.22 28.57 52.09 79.30 70.84 
Table 6 Results of NER Task for MSRA corpus (in percentage %) 
 PER LOC ORG Overall 
Pre. 90.76 85.62 73.90 84.68 
Rec. 76.13 85.41 65.74 78.22 
F-score 82.80 85.52 69.58 81.32 
Besides of that, error analysis shows there are 
four types of main errors in the NER results. 
First, some organization names were very long 
and can be divided into several words, in which 
parts of them can also be looked as named enti-
ties. In such case, our system only recognized the 
small parts as named entities. For example,  ??
???????????  (Fei Zhengqing 
Eastern Asia Research Center of Harvard Univ.) 
was an organization name. But our system rec-
ognized it as????(Harvard Univ.)/ORG+?
? ? (Fei Zheng Qing)/PER+ ? ? (Eastern 
Asia)/LOC+ ????(Research Center)/ORG. 
Adding more context features may be useful to 
resolve this issue. 
In addition, our system was not good at recog-
nizing foreign person names, such as ??? 
(Riordan), and abbreviations, such as ?? (Los 
Angeles), if they seldom or never appeared in 
training corpus. It is because the use of the large 
lexicon decreased the unknown word identifica-
tion ability of our NER system simultaneously. 
Third, the in-NE probability used in post-
processing is helpful to identify named entities 
which cannot be recognized by the basic model. 
But it also recognized some words which can 
only be regarded as named entities in the local 
context incorrectly. For example, our system 
recognized?? (Najing) as GPE in ?????
? (Send to Najing for remedy) in LDC corpus. 
We will consider about adding the in-NE prob-
ability as one feature into the basic model to 
solve this problem. 
At last, in LDC corpus, they combine the at-
tributive of one named entity (especially person 
and organization names) with the named entity 
together. But our system only recognized the 
named entity by itself. For example, our system 
only recognized ??? (Liu Gui Fang) as PER 
in the reference person name ???????? 
(Liu Gui Fang who does not know the inside). 
5 Conclusion and Future Work 
Through the participation of the third 
SIGHAN Bakeoff, we found that tagging charac-
ters with both character-level and word-level fea-
tures was effective for both word segmentation 
and NER. While, this work is only our 
preliminary attempt and there are still many 
works needed to do in the future, such as the 
control of lexicon size, the use of extra 
knowledge (e.g. pos-tag), the feature definition, 
and so on. In addition, our word segmentation 
system only combined the NER module as post-
processing, which resulted in that lots of infor-
mation from NER module cannot be used by the 
basic model. We will consider about combining 
the NER and factoid identification modules into 
the basic word segmentation model by defining 
new tag sets in our future work. 
Acknowledgement 
We would like to thank Dr. Kiyotaka Uchi-
moto for providing the NICT lexicon. 
Reference 
T.Emerson. 2005. The Second International Chinese Word Seg-
mentation Bakeoff. In the 4th SIGHAN Workshop. pp. 123-133. 
H.Jing et al 2003. HowtogetaChineseName(Entity): Segmentation 
and Combination Issues. In EMNLP 2003. pp. 200-207. 
T.Joachims. 1999. Making large-scale SVM learning practical. 
Advances in Kernel Methods - Support Vector Learning. MIT-
Press. 
H.Q.Li et al 2004. The Use of SVM for Chinese New Word Identi-
fication. In IJCNLP 2004. pp. 723-732. 
J.K.Low et al 2005. A Maximum Entropy Approach to Chinese 
Word Segmentation. In the 4th SIGHAN Workshop. pp. 161-164. 
T.Nakagawa. 2004. Chinese and Japanese Word Segmentation 
Using Word-level and Character-level Information. In COLING 
2004. pp. 466-472. 
A.Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-
Speech Tagging. In EMNLP 1996. 
F.Sebastiani. 2002. Machine learning in automated text categoriza-
tion. ACM Computing Surveys. 34(1): 1-47. 
K.Uchimoto et al 2004. Multilingual Aligned Parallel Treebank 
Corpus Reflecting Contextual Information and its Applications. 
In Proceedings of the MLR 2004. pp. 63-70. 
N.Xue et al 2002. Building a Large-Scale Annotated Chinese Cor-
pus. In COLING 2002. 
Y.J.Zhang et al 2005. Building an Annotated Japanese-Chinese 
Parallel Corpus ? A part of NICT Multilingual Corpora. In Pro-
ceedings of the MT SummitX. pp. 71-78. 
149
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 569?573,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Effective Selection of Translation Model Training Data 
Le Liu  Yu Hong*  Hao Liu  Xing Wang  Jianmin Yao 
School of Computer Science & Technology, Soochow University, China 
{20124227052, hongy, 20134227035, 20114227047, jyao}@suda.edu.cn 
 
Abstract 
Data selection has been demonstrated to 
be an effective approach to addressing 
the lack of high-quality bitext for statisti-
cal machine translation in the domain of 
interest. Most current data selection 
methods solely use language models 
trained on a small scale in-domain data to 
select domain-relevant sentence pairs 
from general-domain parallel corpus. By 
contrast, we argue that the relevance be-
tween a sentence pair and target domain 
can be better evaluated by the combina-
tion of language model and translation 
model. In this paper, we study and exper-
iment with novel methods that apply 
translation models into domain-relevant 
data selection. The results show that our 
methods outperform previous methods. 
When the selected sentence pairs are 
evaluated on an end-to-end MT task, our 
methods can increase the translation per-
formance by 3 BLEU points.* 
1 Introduction 
Statistical machine translation depends heavily 
on large scale parallel corpora. The corpora are 
necessary priori knowledge for training effective 
translation model. However, domain-specific 
machine translation has few parallel corpora for 
translation model training in the domain of inter-
est. For this, an effective approach is to automat-
ically select and expand domain-specific sen-
tence pairs from large scale general-domain par-
allel corpus. The approach is named Data Selec-
tion. Current data selection methods mostly use 
language models trained on small scale in-
domain data to measure domain relevance and 
select domain-relevant parallel sentence pairs to 
expand training corpora. Related work in litera-
ture has proven that the expanded corpora can 
substantially improve the performance of ma-
                                                 
* Corresponding author 
chine translation (Duh et al, 2010; Haddow and 
Koehn, 2012). 
However, the methods are still far from satis-
factory for real application for the following rea-
sons: 
? There isn?t ready-made domain-specific 
parallel bitext. So it?s necessary for data se-
lection to have significant capability in min-
ing parallel bitext in those assorted free texts. 
But the existing methods seldom ensure 
parallelism in the target domain while se-
lecting domain-relevant bitext. 
? Available domain-relevant bitext needs keep 
high domain-relevance at both the sides of 
source and target language. But it?s difficult 
for current method to maintain two-sided 
domain-relevance when we aim at enhanc-
ing parallelism of bitext.   
In a word, current data selection methods can?t 
well maintain both parallelism and domain-
relevance of bitext. To overcome the problem, 
we first propose the method combining transla-
tion model with language model in data selection. 
The language model measures the domain-
specific generation probability of sentences, be-
ing used to select domain-relevant sentences at 
both sides of source and target language. Mean-
while, the translation model measures the trans-
lation probability of sentence pair, being used to 
verify the parallelism of the selected domain-
relevant bitext. 
2 Related Work 
The existing data selection methods are mostly 
based on language model. Yasuda et al (2008) 
and Foster et al (2010) ranked the sentence pairs 
in the general-domain corpus according to the 
perplexity scores of sentences, which are com-
puted with respect to in-domain language models. 
Axelrod et al (2011) improved the perplexity-
based approach and proposed bilingual cross-
entropy difference as a ranking function with in- 
and general- domain language models. Duh et al 
(2013) employed the method of (Axelrod et al, 
569
2011) and further explored neural language mod-
el for data selection rather than the conventional 
n-gram language model. Although previous 
works in data selection (Duh et al, 2013; Koehn 
and Haddow, 2012; Axelrod et al, 2011; Foster 
et al, 2010; Yasuda et al, 2008) have gained 
good performance, the methods which only 
adopt language models to score the sentence 
pairs are sub-optimal. The reason is that a sen-
tence pair contains a source language sentence 
and a target language sentence, while the existing 
methods are incapable of evaluating the mutual 
translation probability of sentence pair in the tar-
get domain. Thus, we propose novel methods 
which are based on translation model and lan-
guage model for data selection. 
3 Training Data Selection Methods 
We present three data selection methods for 
ranking and selecting domain-relevant sentence 
pairs from general-domain corpus, with an eye 
towards improving domain-specific translation 
model performance. These methods are based on 
language model and translation model, which are 
trained on small in-domain parallel data.  
3.1 Data Selection with Translation Model 
Translation model is a key component in statisti-
cal machine translation. It is commonly used to 
translate the source language sentence into the 
target language sentence. However, in this paper, 
we adopt the translation model to evaluate the 
translation probability of sentence pair and de-
velop a simple but effective variant of translation 
model to rank the sentence pairs in the general-
domain corpus. The formulations are detailed as 
below: 
 (   )  
 
(    )
  
? ?  (     )
  
   
  
       (1) 
  ? (   )
  
       (2) 
Where  (   ) is the translation model, which is 
IBM Model 1 in this paper, it represents the 
translation probability of target language sen-
tence   conditioned on source language sentence 
 .    and    are the number of words in sentence 
  and  respectively.  (     )  is the translation 
probability of word    conditioned on word   and 
is estimated from the small in-domain parallel 
data. The parameter   is a constant and is as-
signed with the value of 1.0.   is the length-
normalized IBM Model 1, which is used to score 
general-domain sentence pairs. The sentence pair 
with higher score is more likely to be generated 
by in-domain translation model, thus, it is more 
relevant to the in-domain corpus and will be re-
mained to expand the training data.  
3.2 Data Selection by Combining Transla-
tion and Language model  
As described in section 1, the existing data selec-
tion methods which only adopt language model 
to score sentence pairs are unable to measure the 
mutual translation probability of sentence pairs. 
To solve the problem, we develop the second 
data selection method, which is based on the 
combination of translation model and language 
model. Our method and ranking function are 
formulated as follows: 
   (   )   (   )   ( )        (3) 
    ? (   )
  
 ? ( )
  
             (4) 
Where  (   ) is a joint probability of sentence   
and   according to the translation model  (   ) 
and language model  ( ), whose parameters are 
estimated from the small in-domain text.   is the 
improved ranking function and used to score the 
sentence pairs with the length-normalized trans-
lation model  (   )and language model  ( ). 
The sentence pair with higher score is more simi-
lar to in-domain corpus, and will be picked out.  
3.3 Data Selection by Bidirectionally   
Combining Translation and Language 
Models  
As presented in subsection 3.2, the method com-
bines translation model and language model to 
rank the sentence pairs in the general-domain 
corpus. However, it does not evaluate the inverse 
translation probability of sentence pair and the 
probability of target language sentence. Thus, we 
take bidirectional scores into account and simply 
sum the scores in both directions.  
  ? (   )
  
 ? ( )
  
 ? (   )
  
 ? ( )
  
 
 (5) 
Again, the sentence pairs with higher scores are 
presumed to be better and will be selected to in-
corporate into the domain-specific training data. 
This approach makes full use of two translation 
models and two language models for sentence 
pairs ranking. 
570
4 Experiments 
4.1 Corpora 
We conduct our experiments on the Spoken Lan-
guage Translation English-to-Chinese task. Two 
corpora are needed for the data selection. The in-
domain data is collected from CWMT09, which 
consists of spoken dialogues in a travel setting, 
containing approximately 50,000 parallel sen-
tence pairs in English and Chinese. Our general-
domain corpus mined from the Internet contains 
16 million sentence pairs. Both the in- and gen-
eral- domain corpora are identically tokenized (in 
English) and segmented (in Chinese)1. The de-
tails of corpora are listed in Table 1. Additionally, 
we evaluate our work on the 2004 test set of 
?863? Spoken Language Translation task (?863? 
SLT), which consists of 400 English sentences 
with 4 Chinese reference translations for each. 
Meanwhile, the 2005 test set of ?863? SLT task, 
which contains 456 English sentences with 4 ref-
erences each, is used as the development set to 
tune our systems.  
Bilingual Cor-
pus 
#sentence #token 
Eng Chn Eng Chn 
In-domain 50K 50K 360K 310K 
General-domain 16M 16M 3933M 3602M 
Table 1. Data statics 
4.2 System settings 
We use the NiuTrans 2  toolkit which adopts 
GIZA++ (Och and Ney, 2003) and MERT (Och, 
2003) to train and tune the machine translation 
system. As NiuTrans integrates the mainstream 
translation engine, we select hierarchical phrase-
based engine (Chiang, 2007) to extract the trans-
lation rules and carry out our experiments. 
Moreover, in the decoding process, we use the 
NiuTrans decoder to produce the best outputs, 
and score them with the widely used NIST mt-
eval131a3  tool. This tool scores the outputs in 
several criterions, while the case-insensitive 
BLEU-4 (Papineni et al, 2002) is used as the 
evaluation for the machine translation system. 
4.3 Translation and Language models 
Our work relies on the use of in-domain lan-
guage models and translation models to rank the 
sentence pairs from the general-domain bilingual 
training set. Here, we employ ngram language 
                                                 
1http://www.nlplab.com/NiuPlan/NiuTrans.YourData.ch.html 
2http://www.nlplab.com/NiuPlan/NiuTrans.ch.html#download 
3 http://ww.itl.nist.gov/iad/mig/tools 
model and IBM Model 1 for data selection. Thus, 
we use the SRI Language Modeling Toolkit 
(Stolcke, 2002) to train the in-domain 4-gram 
language model with interpolated modified 
Kneser-Ney discounting (Chen and Goodman, 
1998). The language model is only used to score 
the general-domain sentences. Meanwhile, we 
use the language model training scripts integrat-
ed in the NiuTrans toolkit to train another 4-gram 
language model, which is used in MT tuning and 
decoding. Additionally, we adopt GIZA++ to get 
the word alignment of in-domain parallel data 
and form the word translation probability table. 
This table will be used to compute the translation 
probability of general-domain sentence pairs.  
4.4 Baseline Systems 
As described above, by using the NiuTrans 
toolkit, we have built two baseline systems to 
fulfill ?863? SLT task in our experiments. The 
In-domain baseline trained on spoken language 
corpus has 1.05 million rules in its hierarchical-
phrase table. While, the General-domain baseline 
trained on 16 million sentence pairs has a hierar-
chical phrase table containing 1.7 billion transla-
tion rules. These two baseline systems are 
equipped with the same language model which is 
trained on large-scale monolingual target lan-
guage corpus. The BLEU scores of the In-
domain and General-domain baseline system are 
listed in Table 2.  
Corpus 
Hierarchical 
phrase 
Dev Test 
In-domain 1.05M 15.01 21.99 
General-domain 1747M 27.72 34.62 
Table 2. Translation performances of In-domain and 
General-domain baseline systems 
The results show that General-domain system 
trained on a larger amount of bilingual resources 
outperforms the system trained on the in-domain 
corpus by over 12 BLEU points. The reason is 
that large scale parallel corpus maintains more 
bilingual knowledge and language phenomenon, 
while small in-domain corpus encounters data 
sparse problem, which degrades the translation 
performance. However, the performance of Gen-
eral-domain baseline can be improved further. 
We use our three methods to refine the general-
domain corpus and improve the translation per-
formance in the domain of interest. Thus, we 
build several contrasting systems trained on re-
fined training data selected by the following dif-
ferent methods.  
571
? Ngram: Data selection by 4-gram LMs with 
Kneser-Ney smoothing. (Axelrod et al, 
2011) 
? Neural net: Data selection by Recurrent 
Neural LM, with the RNNLM Tookit. (Duh 
et al, 2013) 
? Translation Model (TM): Data selection 
with translation model: IBM Model 1. 
? Translation model and Language Model 
(TM+LM): Data selection by combining 4-
gram LMs with Kneser-Ney smoothing and 
IBM model 1(equal weight).  
? Bidirectional TM+LM: Data selection by 
bidirectionally combining translation and 
language models (equal weight).  
4.5 Results of Training Data Selection 
We adopt five methods for extracting domain-
relevant parallel data from general-domain cor-
pus. Using the scoring methods, we rank the sen-
tence pairs of the general-domain corpus and 
select only the top N = {50k, 100k, 200k, 400k, 
600k, 800k, 1000k} sentence pairs as refined 
training data. New MT systems are then trained 
on these small refined training data. Figure 1 
shows the performances of systems trained on 
selected corpora from the general-domain corpus. 
The horizontal coordinate represents the number 
of selected sentence pairs and vertical coordinate 
is the BLEU scores of MT systems.  
 
Figure 1. Results of the systems trained on only a sub-
set of the general-domain parallel corpus. 
From Figure 1, we conclude that these five da-
ta selection methods are effective for domain-
specific translation. When top 600k sentence 
pairs are picked out from general-domain corpus 
to train machine translation systems, the systems 
perform higher than the General-domain baseline 
trained on 16 million parallel data. The results 
indicate that more training data for translation 
model is not always better. When the domain-
specific bilingual resources are deficient, the 
domain-relevant sentence pairs will play an im-
portant role in improving the translation perfor-
mance.  
Additionally, it turns out that our methods 
(TM, TM+LM and Bidirectional TM+LM) are 
indeed more effective in selecting domain-
relevant sentence pairs. In the end-to-end SMT 
evaluation, TM selects top 600k sentence pairs 
of general-domain corpus, but increases the 
translation performance by 2.7 BLEU points. 
Meanwhile, the TM+LM and Bidirectional 
TM+LM have gained 3.66 and 3.56 BLEU point 
improvements compared against the general-
domain baseline system. Compared with the 
mainstream methods (Ngram and Neural net), 
our methods increase translation performance by 
nearly 3 BLEU points, when the top 600k sen-
tence pairs are picked out. Although, in the fig-
ure 1, our three methods are not performing bet-
ter than the existing methods in all cases, their 
overall performances are relatively higher. We 
therefore believe that combining in-domain 
translation model and language model to score 
the sentence pairs is well-suited for domain-
relevant sentence pair selection. Furthermore, we 
observe that the overall performance of our 
methods is gradually improved. This is because 
our methods are combining more statistical char-
acteristics of in-domain data in ranking and se-
lecting sentence pairs. The results have proven 
the effectiveness of our methods again. 
5 Conclusion 
We present three novel methods for translation 
model training data selection, which are based on 
the translation model and language model. Com-
pared with the methods which only employ lan-
guage model for data selection, we observe that 
our methods are able to select high-quality do-
main-relevant sentence pairs and improve the 
translation performance by nearly 3 BLEU points. 
In addition, our methods make full use of the 
limited in-domain data and are easily implement-
ed. In the future, we are interested in applying 
20.00
22.00
24.00
26.00
28.00
30.00
32.00
34.00
36.00
38.00
40.00
0 200 400 600 800 1000
Axelord et al(2011) Duh et al(2013)
TM TM+LM
Bidirectional TM+LM
572
our methods into domain adaptation task of sta-
tistical machine translation in model level. 
Acknowledgments 
This research work has been sponsored by two 
NSFC grants, No.61373097 and No.61272259, 
and one National Science Foundation of Suzhou 
(Grants No. SH201212).  
Reference 
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language 
Processing, pages 355?362, Edinburgh, Scotland, 
UK, July. Association for Computational Linguis-
tics. 
Peter F.Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: Pa-
rameter estimation. Computational linguistics, 
1993, 19(2): 263-311. 
Stanley Chen and Joshua Goodman. 1998. An Empir-
ical Study of Smoothing Techniques for Language 
Modeling. Technical Report 10-98, Computer Sci-
ence Group, Harvard University.  
Moore Robert C, Lewis William. 2010. Intelligent 
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Pa-
pers. Association for Computational Linguistics, 
2010: 220-224. 
Chiang David. A hierarchical phrase-based model for 
statistical machine translation. 2005. In Proceed-
ings of the 43rd Annual Meeting on Association for 
Computational Linguistics, pages: 263-270. Asso-
ciation for Computational Linguistics. 
Kevin Duh, Graham Neubig, Katsuhito Sudoh and  
Hajime Tsukada. Adaptation Data Selection using 
Neural Language Models: Experiments in Machine 
Translation. In Proceedings of the 51st Annual 
Meeting of the Association for Computational Lin-
guistics, pages 678-683, Sofia, Bulgaria, August 4-
9 2013.  
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 
2010.Analysis of translation model adaptation for 
statistical machine translation. In Proceedings of 
the International Workshop on Spoken Language 
Translation (IWSLT) - Technical Papers Track.  
George Foster, Cyril Goutte, and Roland Kuhn. 2010. 
Discriminative Instance Weighting for Domain 
Adaptation in Statistical Machine Translation. Em-
pirical Methods in Natural Language Processing. 
Barry Haddow and Philipp Koehn. 2012. Analysing 
the effect of out-of-domain data on smt systems. In 
Proceedings of the Seventh Workshop on Statistical 
Machine Translation, pages 422?432, Montreal, 
Canada, June. Association for Computational Lin-
guistics. 
Och, Franz Josef, and Hermann Ney. A systematic 
comparison of various statistical alignment models. 
Computational linguistics 29.1 (2003): 19-51.  
Och, Franz Josef. Minimum error rate training in sta-
tistical machine translation. Proceedings of the 41st 
Annual Meeting on Association for Computational 
Linguistics-Volume 1. Association for Computa-
tional Linguistics, 2003.  
Philipp Koehn and Barry Haddow. 2012. Towards 
effective use of training data in statistical machine 
translation. In WMT. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: A method for auto-
matic evaluation of machine translation. In ACL.  
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. Spoken Language Pro-
cessing. 
Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li. 
NiuTrans: an open source toolkit for phrase-based 
and syntax-based machine translation. In Proceed-
ings of the ACL 2012 System Demonstrations. As-
sociation for Computational Linguistics, 2012: 19-
24. 
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, 
and Eiichiro Sumita. 2008. Method of selecting 
training data to build a compact and efficient trans-
lation model. International Joint Conference on 
Natural Language Processing.  
573
