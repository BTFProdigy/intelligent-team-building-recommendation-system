A Multilingual News Summarizer 
Ilsin-Hsi Chen 
l)epartlnent of Computer Science and 
Information Engineering 
National Taiwan University 
Taipei, TAIWAN, R.O.C. 
hh chen @csie.ntu.edu.tw 
Chuan-Jie Lin 
Defmltment of Computer Science and 
Information Engineering 
National Taiwan University 
Taipei, TAIWAN, R.O.C. 
cjli n @ nlg2.csie.ntu.edu.tw 
Abstract 
Huge multilingual news articles are reported 
and disseminated on the Internet. ltow to 
extract the kcy information and savc the 
reading time is a crucial issue. This paper 
proposes architecture of multilingual news 
sumlnarizer, including monolingual and 
multilingual clustering, similarity measure 
among lneaningful ullits, and presentation of 
summarization results. Translation anlong 
news stories, idiosyncrasy among languages, 
itnplicit information, and user preference are 
addressed. 
Introduction 
Today many web sites on the lnternet provide 
online news services. Multilingual news articles 
are reported periodically, and across geographic 
barrier to disseminate to readers. Readers can 
access the news stories conveniently, but it takes 
much time l'or people to read all tile news. This 
paper will present a personal news secretariat that 
helps on-line readers absorb news information 
from multiple sources in different languages. 
Such a news secretariat eliminates the redundant 
information in tile news articles, reorganizes tile 
news for readers, and helps them resolve the 
language barriers. 
Reorganization of news is sonic sort of 
document summarization, which creates a short 
version of original document. Recently, many 
papers touch on single document summarization 
(ltovy and Marcu, 1998a). Only a few touch on 
multiple document sulnmarization (Chen and 
Huang, 1999; Mani and Bloedorn, 1997; Radev 
and McKeown, 1998) and multilingual document 
summarization (Hovy and Marcu, 1998b). For 
multilingual multiple news summarization, several 
issues have to be addressed: 
(1) Translation among news stories in 
different languages 
The basic idea in multiple doculnent 
sulnmarizations i  to identify which paris of news 
articles present similar reports. Because the 
news stories are in different languages, seine kind 
of Iranslation is required, e.g., term translation. 
Besides the problem of translation ambiguity, 
different news sites often use difl'erent names to 
refer tile same entity. The translation o1' named 
entities, which are usually ttnknown words, is 
another probleln. 
(2) Idiosyncrasy among languages 
1)ifferent languages have their own specific 
features. For example, a Chinese sentence is 
composed of characters without word boundary. 
Word segmentation is indispensable for Chinese. 
Besides, Chinese writers often assign l~unctuation 
ntarks at randonl, how to determine a mealfingful 
unit for similarity checking is a crucial issue. 
Thus seine tasks may be done for specific 
languages during SUlnmarization. 
(3) hnplicit information in news reports 
Some information is ilnplicit in news stories. 
For example, the name of a country is usually not 
mentioned in a news article reporting an event that 
happened in that country. On the contrary, the 
country name is important in foreign news. 
Besides, time zone is used to specify date/time 
implicitly in the news. 
(4) User preference 
When users want to read documents in their 
familiar languages, news fragments in some 
159 
9 ANt r::: 
Nl.lnlnlary N illllmal'y Sullllllary ,~ 11111111 al'y 
ltlr l 'venl 1 for l ivent 2 for l ivem 3 for Event m 
Figure 1. Architecture of 
Our Multilingual Sunmmrization System 
languages are preferred to those in other languages. 
Even machine translation should be introduced to 
translate news fragments. Besides, if a user 
prefers the news from tile view of his country, or 
more precisely, of some news sites, we should 
meet his need. 
Figure 1 shows the architecture of a 
multilingual summarization system, which is used 
to sulnmarize the news from multiple sources in 
different languages. It is composed of three 
m~tior components: several monolingual news 
clusterers, a multilingual news clusterer, and a 
news summarizer. Tile monolingual news 
clusterer receives a news stream from multiple on.- 
line newspapers in its respective language, and 
directs them into several output news streams by 
using events. The multilingual news clusterer 
then matches and merges the news streams of the 
same event but in different languages in a cluster. 
The news summarizer summarizes the news 
stories for each event. 
The possible tasks for each component 
depend on the languages used. Some major tasks 
of a monolingual clusterer are listed below. 
(1) identifying word boundaries for Chinese 
and Japanese sentences, 
(2) Extracting named entities like people, 
place, organization, time, date and monetary 
expressions, 
(3) Clustering news streams based on 
predefined topic set and named entities. 
The task for the multilingual clusterer is to 
align the news clusters in the same topic set, but in 
different languages. It is similar to document 
alignment in comparable corpus. Named entities 
are also useful cues. 
The major tasks for the news summarizer are 
shown as follows. 
(1) Partitioning a news story into several 
meaningful units (MUs), 
(2) Linking the lneaningful units, denoting 
the salne thing, from different news reports, 
(3) Displaying the summarization results 
under the consideration of language type users 
prefer, information decay and views of reporters. 
1. Clustering 
1.1 Monolingual Clustering 
We adopt a two-level approach to cluster the 
news t)o111 multiple sources. At first, news is 
classified on the basis of a predefined topic set. 
Then, tile news articles in the same topic set are 
partitioned into several clusters according to 
named emities. Classification is necessary. Oil 
tile one hand, a famous person may appear in 
many kinds of news stories. For example, 
President Clinton may make a public speech 
(political news), join an international meeting 
(international news), or even just show up in the 
opening of a baseball game (sports news). On 
the other hand, a common name is flequently seen 
but denotes different persons. Classification 
reduces the ambiguity introduced by famous 
persons and/or common names. 
An event in a news story is characterized by 
five basic entities such as people, affairs, time, 
places and things. These entities form important 
cues during clustering. Systems for named entity 
extraction in a famous lnessage understanding 
competition (MUC, 1998) demonstrate promising 
performances for English, Japanese and Chinese. 
In our multilingual summarization system, we 
focus on English and Chinese. Gazetteer 
approach is adopted to deal with English news 
articles. Comparatively, Chinese news articles 
are segmented at first. Then, several types of 
inforlnation fiom character, sentence and text 
levels are employed to extract Chinese named 
160 
entities. These tasks are similar to tile 
approaches ill tile papers (Chen and Lee, 1996; 
Chen, el al., 1998a). 
1.2 Multilingual Clustering 
Tile multilingual clusterer takes input from 
the lnonolingual clusterers, and determines which 
news clusters ill which languages talk about tile 
same story. Recall that a news cluster consists of 
several news articles reporting tile same event, and 
one news cluster exists lbr one event ariel 
monolingual clustering. Ill this way, there is at 
most one corresponding news cluster ill another 
language. Therefore, the main task of the 
multilingual news clusterer is to lind tile 
matchings among tile clusters ill different 
languages. Figure 2 shows an example, ill 
Topic !, cluster cHl is aligned to c itr, and cluster 
Cil 2 is aligned to c.ili. Clusters cii~z arid cjl 2 are 
left unaligned. That means the denoted events 
arc reported ill only one language. 
Similarity of two clusters is measured based 
on verbs, named entities, and the other nouns. 
Because Chinese words are less anibiguolls tMn 
English ones (Chen, Bian anti Lin, 1999), we 
translate nouns and verbs in the Chinese news 
articles into English. If a word Ms more than 
one translation, we select high fl-equent English 
translation. For tile named enlities not listed ill 
tile lexicon, name transliteration similar to tile 
algoritlnn (Chen, el al., 1998b) is introduced for 
matching in non-alpMbetic (e.g., Clfinese) and 
alphabetic languages (e.g., English). 
Alignment is made under the same topic. A 
news chlster c i is aligried to another cluster cj if 
their similarity is above a threshold, and is tile 
highest between q and the other clusters. If tile 
similarity of q and the other clusters is less than a 
given threshold, ci is not aligned. It is possible 
because local news is reported only ill tile 
restricted areas. 
2. Similarity Analysis 
2.1 Meauingful Units 
The basic idea during smnmarization is to tell 
which parts of the news articles are similar in the 
same event. The basic unit tbr similarity 
measure may be a paragraph or a sentence. For 
Language l., 
Language 1i 
Topic I Topic i 
I 
Topic I Topic t 
Figure 2. Matching among tile Clusters 
in Two Languages 
tile t'ormer, text segmentation is necessary for 
documents without paragraph markers (Chcn and 
Chen, 1995). For the latter, text segmentation is 
necessary ibr languages like Chinese. Unlike 
English writers, Chinese writers often assign 
punctuat ion marks at random (Chen, 1994). 
Thus the sentence boundary is not clear. 
Consider the following Chinese example (C l): 
(Central News Agency, 1999.12.02) 
(Although they were undeterred by mass arrests 
and a police crackdown, anti free-trade protesters 
still marched on downtown Seattle today. The 
protesters, carrying signs and chanting, opposed 
lhc global trade liberalization being worked on at 
a meeting of h+ade lninisters flom tile World Trade 
Organi zat ion.) 
It is composed of four sentence segments 
separated by commas. 11' a sentence segment is 
regarded as a unit for similarity checking, it may 
contain too little information. On tile contrary, if 
a sentence is regarded as a unit, it may contain too 
much M'ormation. Here we consider a 
meaningful unit (MU) as a basic unit for 
measurement. A MU is composed of several 
sentence segments and denotes a complete 
meaning. We will find two MUs shown as 
follows for (C 1): 
(Although they were undeterred by mass arrests 
and a police crackdown, anti free-trade protesters 
still marched on downtown Seattle today.) 
161 
-~.~ .e- ~ 4-/5,-- " 5, 
(The protesters, carrying signs and chanting, 
opposed the global trade liberalization being 
worked on at a meeting of trade ministers fl'om the 
World Trade Organization.) 
In our summarization system, an English 
sentence itself is an MU. Comparatively, it is a 
little harder to identify Chinese MUs. Three 
kinds of linguistic kuowledge- punctuation marks, 
linking elements and topic chaius, are proposed. 
(1) Punctuation marks 
There are fourteen marks in Chinese (Yang, 1981). 
Only period, question mark, exclamation mark, 
comma, semicolon and caesura mark are 
employed. The former three are sentence 
terminators, and the latter three are segment 
separators. 
(2) Linking elements 
There are three kinds of linking elements (Li and 
Thompson, 1981): forward-linking elements, 
backward-linking elements, and couple-linking 
elements. A segment with a forward-linking 
(backward-linking) elemeut is linked with its next 
(previous) segment. A couple-linking element is 
a pair of words that exist in two segments. 
Apparently, these two segments are joined 
together. Examples (C4)-(C6) show each ldnd of 
linkings. 
(C4) T~,~:-~,,..,-~ ' q~&d# g#~ ? 
(After school, I wanted to see a movie.) 
(I wanted to see a movie, but I couldn't get a 
ticket.) 
(C6) N -h&a-~ ~. ~'; g ' ,~a.rx & a_~-.-24 "~d 
:V4 o 
(Because I couldfft get a ticket, (so) 1 didn't 
see a movie.) 
(3) Topic chains 
The topic of a clausal segment is usually deleted 
under the identity with a topic in its preceding 
segment. The result of such a deleting process is 
a topic chain. We employ part of speech 
information to predict if a subject of a verb is 
missing. If it does, we postulate that it must 
appear in the previous segment and the two 
segments are connected to form a larger unit. 
Consider example (C1). The word "f'$ @" 
(although) is a forward linking element. Thus 
the first two segments are connected together (C2). 
The last segment does not have ally subject, so 
that it is connected to the previous one by topic 
chain (C3). In summary, two MUs are formed. 
2.2 Similarity Model 
Tile next step is to find the similarity among 
MUs in the news articles reporting the same event, 
and to link the similar MUs together. We 
analyze the news stories within the same language, 
and then the news stories among different 
languages. The key idea is similar at these two 
steps. That is, predicate argument structure 
forms the kernel of a sentence, thus verbs and 
nouns are regarded as important cues for similarity 
measures. The difference between these two 
steps is that we have to translate nouns and verbs 
in one language into another language. The 
approach of select-high-frequent translation and 
name transliteration shown in Section 1.2 is 
adopted here too. Consider (MUI) - (MU3). 
The former two are in Chinese and the last one is 
in English. They denote a similar event 
"Seattle's Curfew Hours". Each noun (verb) is 
enclosed by parentheses and assigned an index. 
There are 9 common terms between (MUI) and 
(MU2); 10 common terms between (MUI) and 
(MU3); and 8 common terms between (MU2) and 
(MU3). Note the time zones used in (MU2) and 
(MUI) are different, so are (MU2) and (MU3). 
(MU 1 ) .g (1 ~-J 5J~ N )(2 ~ ~ )(3 ~'~ ~ )(4 ~ )~I ~- ) ~'~ (5 
2-~-(11~)~%) ( 12"qc2 }\]~)(13~\]~)(14>J" ;~v)"~ ' kl5 ;~ 
G}I~"(16.T-.~")(,7,{g'a~) 1" (,s'?O" fl" '~) o 
(Chinatimes, 1999.12.02) 
(MU2) (, N~IflN)(2~-~v)(4Gdff), ~(5"1~)(6~}.~/N,~)(7 
(Formosa Television, 1999.12.02) 
(MU3) GSeattle) (2Mayor) (2sPaul) (3Schell) has 
Gdeclared) a (sState) of (scivil) (Temergency) and 
(13imposed) a (m7 p.m.) to (267:30 a.m) ((2v10 p.m.) 
EST - (2s10:30 a.m.) EST) (,4curfew) on 
(2,downtown) (29areas) of the (30city). 
(Reuters) 
162 
(s2) 
(s3) 
($4) 
once .  
(ss) 
Several strategies lnay be considered in 
similarity measure: 
(SI) Nouns in one MU are matched to nouns in 
another MU, so are verbs. 
The operations in (1) are exact matches. 
Thesauri are employed tu-ing matching. 
Each term specified in (S 1) is matched only 
Tile order of llOUllS and verbs in MU is not 
considered. 
($6) 'File order of nouns and verbs in MU is 
critical, but it is relaxed within a window. 
(S7) When continuous terms are matched, an 
extra score is added. 
($8) When tile object o1: transitive verbs arc not 
matched, a score is subtracled. 
($9) When date/time xpressions and monetary 
and percentage xpressions are matched, an extra 
score is added. 
l;ive models shown below are collstrtlcted 
under different combinations of tile strategies 
specified in tile above. 
(M t) (S 1)+($3)+($4)+($5) 
(M2) (S 1)+($3)+($4)+($6) 
(M3) (S 1)+(S3)+(S4)+(S5)+($7)+($8) 
(M4) (S 1)+($3)+($4)+($5)+($7)+($8)+($9) 
(M5) (S I )+($2)+($4)+($5)+($7)+($8)+($9) 
3. Experiments 
3.1 l ' reparat ion el'Testing Corpus 
Six events selected from Central l)aily News, 
China I)aily Newspaper, China Times Interactive, 
and FTV News Online in Taiwan arc used to 
lneasure tile performance of each lnodel. They 
are shown as follows: 
(1) military service: 6 articles 
(2) construction permit: 4 articles 
(3) landslide in Shah Jr: 6 articles 
(4) Buslfs sons: 4 articles 
(5) Typhoon Babis: 3 articles 
(6) stabilization fund: 5 articles 
The news events are selected from different 
editions, including social edition, economic 
edition, international edition, political edition, etc. 
An annotator eads all tile news articles, and 
connects tile MUs that discuss the same story. 
Because each MU is assigned a unique ID, the 
links among MUs form the answer keys for the 
performance evaluation. 
Table I. Perf iwmance of Similarity of MUs 
Model 
M I 
M2 
M 3 
M4 
M5 
Precision Rate 
0.5000 
0.4871 
0.5080 
0.5164 
0.5243 
Recall Rate 
0.5434 
0.3905 
(/.5888 
0.6198 
0.5579 
3.2 Resulls 
Traditional precision and recall are computed. 
Table 1 lists the perfornmnce of these five models. 
M I is regarded as a baseline model. M2 is 
different l'ronl M1 in that the matching order of 
nouns itl\](l verbs are kept conditionally. It tries to 
consider the subject-verl>object sequence. The 
experiment shows that tile performance is worse. 
The major reason is that we c~ltl express the same 
meaning using different syntactic structures. 
Movement ransformation may affect tile order of 
sulkiest-verb-object. Thus in M3 we give up the 
order criterion, but we add an extra score when 
continuous terms are matched, l ind  subtract some 
score when tile object of a transitive verb is not 
matched. Compared with M1, the precision is a 
little higher, and tile recall is improved about 4.5%. 
If we further consider some special named entities 
such as date/time xpressions and monetary and 
percentage expressions in M4, tile recall is 
increased about 7.6% at no expense of precision. 
M5 tries Io estimate tile function of tile thesauri. 
It uses exact matching. Tile precision is a little 
higher but the recall is decreased abollt G% 
compared with M4. 
Several m~\ior errors affect tile overall 
performance. Using nouns and verbs to find the 
similar MUs is not always workable. Tile same 
meaning may not be expressed in terms of the 
same words or synonymous words. Besides, we 
can use different format to express monetary and 
percentage xpressions. Word segmentation is
another source of errors. Two sentences 
denoting tile similar meaning may be segmented 
differently clue to tile segmentation strategies. 
Unknown words generate many single-character 
words. After tagging, these words tend to be 
nOUllS and verbs, which are used in computing tile 
scores for similarity measure. Thus errors may 
be introduced. 
163 
4. Presentation Model 
Two models, i.e., focusing inodel and 
browsing model, are proposed to display the 
sumlnarization results. In the focusing model, a 
SUlnlnarization is presented by voting fi'om 
reporters. For each event, a reporter ecords a 
news story from his own viewpoint. Recall that 
a news article is composed of several MUs. 
Those MUs that are similar in a specific event are 
COlnmon focuses of different reporters. In other 
words, they are worthy of reading. In the current 
ilnplementation, the MUs that are reported more 
than once are our target. For readability, the 
original sentences that cover the MUs are selected. 
For each set of similar MUs, the longest sentence 
in user-preferred language is displayed. The 
display order of the selected sentences is 
determined by relative position in the original 
news articles. 
In the browsing lnodel, the news articles are 
listed by information decay. The first news 
article is shown to the user in its whole content. 
In the latter shown news articles, the MUs 
denoting the inforlnation mentioned before are 
shadowed (or eliminated), so that the reader can 
focus on the new information. The alnount of 
information in a news article is lneasured in terms 
of the number of MUs, so that the article that 
contains lnore MUs is displayed before the others. 
For readability, a sentence is a display unit. In 
this model, users can read both the COlnmon views 
and different views of reporters. It saves the 
reading time by listing the colnlno11 view only 
once. 
5. Evaluation of Sumnmrization Results 
The same six events specified in Section 3.1 
are used to measure the performance of the two 
summarization models. Three kinds of metrics 
are considered - say, the document reduction rate, 
the reading-tilne reduction rate, and the 
inforlnation carried. The higher the document 
reduction rate is, the more time the reader may 
save, but the higher possibility the ilnportant 
information may be lost. Tables 2 and 3 list the 
document reduction rates for focusing and 
browsing summarization, respectively. Only 
focuses are displayed in focusing sutnmarization, 
Table 2. Reduction Rates for 
Focusing Summarization 
Event Name l)ocl,en Sum Len Sum/l)oc \[ Reduction 
mililary service 7658 2402 0.3137 68.63% 
construction permit 4182 1226 0.2932 70.68% 
laMslide in Shah ,It" 5491 1823 0.3320 66.80% 
Busies sons 6186 924 0.1494 85.06% 
Typhoon Babis 4068 1460 0.3589 64. I 1% 
stabilization ftmd 8434 2243 0.2659 73.41% 
Average 36019 10078 0.2798 72.02% 
Table 3. Reduction Rates 
for Browsing Summarization 
Event Name Doc Len Sum Len + Sum/l)oc Reduction 
military service 7658 2716 0.3547 64.53% 
construclion permit 4182 2916 0.6973 30.27% 
landslide in Shah Jr 5491 2946 0.5365 46.35% 
Buslfs sons 6186 5098 0.8241 17.59% 
Typhoon Babis 4068 2270 0.5580 44.20% 
stabilization fund 8434 4299 0.5(197 49.03% 
Average 36(/19 20245 0 .5621 43.79% 
Table 4. Assessors' Evaluation 
Event Name Document Question- Reading-Time 
Reduction Answering Reduction 
Rate Correct Rate Rate 
military service 64.53% 10(1% 45.24% 
3/I.27% 33.33% 33.54% construction permit 
landslide in Shah J|" 46.35% 80% I 10.28% 
gush's oils 17.59% 100% I 36.49% 
Typhoon Babis 44.20% 100% 35.10% 
stabilization fund 49.03% 100% 18.49~ 
Average 43.79% 88.46% 3(/.86% 
so that the average doculnent reduction rate is 
higher than that of browsing summarization. 
Besides the document reduction rate, we also 
measure the correct rate of question-answering, 
and reading-time reduction rate. Assessors read 
the highlight parts only in the browsing 
summarization, and answer 3 to 5 questions. 
Table 4 lists the evaluation results of the six 
events. The average doculnent reduction rate is 
43.79%. On the average, the summary saves 
30.86% of reading time. While reading the 
summary only, the correct rate of question- 
answering task is 88.46%. 
Conclusion 
This paper sketches architecture for 
multilingual news summarizer. In multilingual 
clustering, lnatching all pairs of news clusters in 
all languages is time-exhaustive. Because only 
English and Chinese news articles are considered 
in this paper, it is not a problem. In general, an 
164 
effective way is to predefine a sequence of 
language pairs according to the degree of 
translation ambiguity. The hmguage pair of less 
ambiguity is tried first. 
To discuss which fi'agments of multilingual 
news stories denote the salne things, this paper 
defines the concept of MUs. Punctuation marks, 
linking elements and topic chains are cues to 
identify MUs for Chinese. Select-high-frequent 
English translation and name transliteration are 
adopted to transhtte Chinese MUs into L;nglish. 
Five models are proposed to link the similar MUs 
together. Different formats used in time, date 
and monetary expressions, e.g., implicit time zone, 
affect the performance of linking. It should be 
studied in the fllture. 
In presentation o1' summarization results, the 
information decay strategy helps reduce the 
redundancy, and the user can get al the 
information provided by the news sites. 
However, the news sequence is not presented 
according to the importance. The user may quit 
reading and miss the information not shown yet. 
The voting strategy from reporters gives a shorter 
summarization in terlnS of user-preferred 
languages. However, it also misses some unique 
information reported only by one site. A hybrid 
strategy should be developed in the future to meet 
all the requirements. 
References 
Chen, H.H. (1994) "The Contextual Analysis of 
Chinese Sentences with Punctuation Marks," Litelwl 
aud Linguistic Computing, Oxford University Press, 
9(4), 1994, pp. 281-289. 
Chen, H.H; et al (1998a) "Descriplion of the NTU 
System Used for MET2." Proceedings of 7 a' 
Message Undel:s'tanding Conference, 1998. 
Chen, H.H.; et al (1998b) "Proper Name Translation in 
Cross-Language Information Retrieval," Proceedings 
of COLING-A CL98, 1998, pp. 232-236. 
Chen, H.H.; Bian, G.W. and Lin, W.C. (1999) 
"Resolving Translation Ambiguily and Target 
Polysemy in Cross-Language Inli)rmation Retriewd," 
PJweeedings of 37 'l' Auroral Meeting of the 
Association./'or Conqmtational Linguistics, 1999, pp. 
215-222. 
Chert, K.H. and Chert, H.H. (1995)"A Corpus-Based 
Approach to Text Partition," Pivceedings of 
International Col!/'erenee of Recent Advances on 
Natural Language Processing, Tzigov Chark, 
Bulgaria, 1995, pp. 152-160. 
Chen, ILH. and Huang, S.J. (1999) "A Sunnnarization 
System for Chinese News from Multiple Sources," 
Proceedings of 4 't' International Workshop on 
lqformation Retrieval with Asia l~nguages, 1999, pp. 
1-7. 
Chert, H.H. and Lee, J.C. (1996) "Identification and 
Classification of Proper Nouns in Chinese Texts," 
Proceedings" of 16th International Conference on 
Computational Linguistics, 1996, pp. 222-229. 
Hovy, E. and Mareu, D. (1998a) Automated Text 
SmmnaHzation, Tutorial in 17 'h ACL attd 36 '~' 
COLING, Montreal, Quebec, Canada, 1998. 
Hovy, E. and Marcu, D. (1998b) Multilingual Text 
Summarization, Tutorial in AMTA-98, 1998. 
IA, C.N. and Thompson, S.A. (1981) Mandarin 
Chinese: A Functional Re\[erence Giwmmar, 
University of California Press, 1981. 
Mani, I. and Bloedorn, E. (1997) "Multi-documenl 
Summarizalion by Graph Search and Matching," 
Proceedings of the Fourteenth National Con.lisrence 
oil Arti/icial Intelligence, Providence, RI, pp. 622- 
628. 
MUC (1998) Ptvceedings of 7 ~1' Message 
{hMet:s'tanding Cot!ferenc.e, http://www.muc.saic. 
corn/proceedings/proceedings index.broil. 
P, adev, I).P,. and McKeown, K.R. (1998)"Generating 
Natural Language Summaries from Multiple On-Line 
Sources," Computational Linguistics, Vol. 24, No. 3, 
pp. 469-500. 
Yang, Y. (1981) The Research on lhmetuation Marks, 
Tian-iian Publishing Company, ltong Kong, 1981. 
165 
Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 16?23,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Question Pre-Processing in a QA System on Internet 
Discussion Groups 
 
Chuan-Jie Lin and Chun-Hung Cho 
Department of Computer Science and Engineering 
National Taiwan Ocean University 
No 2, Rd Pei-Ning, Keelung 202, Taiwan, R.O.C 
cjlin@mail.ntou.edu.tw; futurehero@seed.net.tw 
 
Abstract 
This paper proposes methods to 
pre-process questions in the postings 
before a QA system can find answers in a 
discussion group in the Internet.  
Pre-processing includes garbage text 
removal and question segmentation.  
Garbage keywords are collected and 
different length thresholds are assigned to 
them for garbage text identification.  
Interrogative forms and question types 
are used to segment questions.  The best 
performance on the test set achieves 
92.57% accuracy in garbage text removal 
and 85.87% accuracy in question 
segmentation, respectively. 
1 Introduction 
Question answering has been a hot research topic 
in recent years.  Large scale QA evaluation 
projects (e.g. TREC QA-Track1, QA@CLEF2, 
and NTCIR 3  QAC and CLQA Tracks) are 
helpful to the developments of question 
answering. 
However, real automatic QA services are not 
ready in the Internet.  One popular way for 
Internet users to ask questions and get answers is 
to visit discussion groups, such as Usenet 
newsgroups 4  or Yahoo! Answers 5 .  Each 
discussion group focuses on one topic so that 
users can easily find one to post their questions. 
There are two ways a user can try to find 
                                                     
1 http://trec.nist.gov/data/qa.html 
2 http://clef-qa.itc.it/ 
3 http://research.nii.ac.jp/ntcir/index-en.html 
4 Now they can be accessed via Google Groups: 
http://groups.google.com/ 
5 http://answers.yahoo.com/ 
answers.  You can post your question in a 
related discussion group and wait for other users 
to provide answers.  Some discussion groups 
provide search toolbars so that you can search 
your question first to see if there are similar 
postings asking the same question.  In Yahoo! 
Answers, you can also judge answers offered by 
other users and mark the best one. 
Postings in discussion groups are good 
materials to develop a FAQ-style QA system in 
the Internet.  By finding questions in the 
discussion groups similar to a new posting, 
responses to these questions can provide answers 
or relevant information. 
But without pre-processing, measuring 
similarity with original texts will arise some 
problems: 
1. Some phrases such as ?many thanks? or 
?help me please? are not part of a 
question.  These kinds of phrases will 
introduce noise and harm matching 
performance. 
2. Quite often there is more than one 
question in one posting.  If the question 
which is most similar to the user's 
question appears in an existed posting 
together with other different questions, it 
will get a lower similarity score than the 
one it is supposed to have because of 
other questions. 
Therefore, inappropriate phrases should be 
removed and different questions in one posting 
should be separated before question comparison. 
There is no research focusing on this topic.  
FAQ finders (Lai et al, 2002; Lytinen and 
Tomuro, 2002; Burke, 1997) are closely related 
to this topic.  However, there are differences 
between them.  First of all, questions in a FAQ 
set are often written in perfect grammar without 
16
garbage text.  Second, questions are often 
paired with answers separately.  I.e. there is 
often one question in one QA pair. 
There were some research groups who 
divided questions into segments.  Soricut and 
Brill (2004) chunked questions and used them as 
queries to search engines.  Saquete et al (2004) 
focused on decomposition of a complex question 
into several sub-questions.  In this paper, 
question segmentation is to identify different 
questions posed in one posting. 
2 Garbage Text Removal 
2.1 Garbage Texts 
Articles in discussion groups are colloquial.  
Users often write articles as if they are talking to 
other users.  For this reason, phrases expressing 
appreciation, begging, or emotions of writers are 
often seen in the postings.  For example: 
?? powerpoint ?? ?????? 1 ?
??? access ???????????? 
?? 2 
(About Powerpoint, I?d like to ask1, how to 
put the whole window seen in Access onto a 
slide?  Thank you2!) 
The phrases ???????? (?I?d like to ask?) 
and ???? (?Thank you?) are unimportant to the 
question itself. 
These phrases often contain content words, 
not stop words, and thus are hard to be 
distinguished with the real questions.  If these 
phrases are not removed, it can happen that two 
questions are judged ?similar? because one of 
these phrases appears in both questions. 
A phrase which contributes no information 
about a question is called garbage text in this 
paper and should be removed beforehand in 
order to reduce noise.  The term theme text is 
used to refer to the remaining text. 
After examining real querying postings, 
some characteristics of garbage texts are 
observed: 
1. Some words strongly suggest themselves 
being in a garbage text, such as ?thank? 
in ?thank you so much?, or ?help? in 
?who can help me?. 
2. Some words appear in both theme texts 
and garbage texts, hence ambiguity 
arises.  For example: 
?????? (Any expert please help) 
?????? (Flash Expert) 
The first phrase is a garbage text, while 
the second phrase is a product name.  
The word ?expert? suggests an existence 
of a garbage text but not in all cases. 
Because punctuation marks are not reliable in 
Chinese, we use sentence fragment as the unit to 
be processed.  A sentence fragment is defined 
to be a fragment of text segmented by commas, 
periods, question marks, exclamation marks, or 
space marks.  A space mark can be a boundary 
of a sentence fragment only when both 
characters preceding and following the space 
mark are not the English letters, digits, or 
punctuation marks. 
2.2 Strategies to Remove Garbage Texts 
Frequent terms seen in garbage texts are 
collected as garbage keywords and grouped into 
classes according to their meanings and usages.  
Table 1 gives some examples of classes of 
garbage keywords collected from the training set. 
Class Garbage Keywords 
Please ????, ??,????? 
Thanks ??,??,??,??? 
Help ??,??,????,??? 
Urgent ??,??,??,?? 
Table 1. Some Classes of Garbage Keywords 
To handle ambiguity, this paper proposes a 
length information strategy to determine garbage 
texts as follows: 
If a sentence fragment contains a garbage 
keyword and the length of the fragment after 
removing the garbage keyword is less than a 
threshold, the whole fragment will be judged as a 
garbage text.  Otherwise, only the garbage 
keyword itself is judged as garbage text if it is 
never in an ambiguous case. 
Different length thresholds are assigned to 
different classes of garbage keywords.  If more 
than one garbage keyword occurring in a 
fragment, discard all the keywords first, and then 
compare the length of the remaining fragment 
with the maximal threshold among the ones 
corresponding to these garbage keywords. 
In order to increase the coverage of garbage 
keywords, other linguistic resources are used to 
expand the list of garbage keywords.  
Synonyms in Tongyici Cilin (?????), a 
17
thesaurus of Chinese words, are added into the 
list.  More garbage keywords are added by 
common knowledge. 
3 Question Segmentation 
When a user posts an article in a discussion 
group, he may pose more than one question at 
one time.  For example, in the following 
posting: 
Office 2003 ? XP????????? ?
? ? ? ? ? ? ? ? ? ? ? ?
???????????? 
(Office 2003 and XP ?  What are the 
differences between them? Which 
version is newer? What is the latest 
version???????????) 
there are 3 questions submitted at a time.  If a 
new user wants to know the latest version of 
Office, responses to the previous posting will 
give answers. 
Table 2 lists the statistics of number of 
questions in the training set.  The first column is 
the number of questions in one posting.  The 
second and the third columns are the number and 
the percentage of postings which contain such 
number of questions, respectively. 
Q# Post# Perc (%) 
1 494 56.98 
2 259 29.87 
3 82 9.46 
4 22 2.54 
5 4 0.46 
? 6 6 0.69 
? 2 373 43.02 
Total 867 100.00 
Table 2. Statistics of Number of Questions 
in Postings  
As we can see in Table 2, nearly half (43.02%) of 
the postings contain two or more questions.  
That is why question segmentation is necessary. 
3.1 Characteristics of Questions in a Posting 
Several characteristics of question texts in 
postings were found in real discussion groups: 
1. Some people use ??? (question mark) at 
the end of a question while some people 
do not.  In Chinese, some people even 
separate sentences only by spaces 
instead of punctuation marks.  (Note 
that there is no space mark between 
words in Chinese text.) 
2. Questions are usually in interrogative 
form.  Either interrogatives or question 
marks appear in the questions. 
3. One question may occur repeatedly in 
the same posting.  It is often the case 
that a question appears both in the title 
and in the content.  Sometimes a user 
repeats a sentence several times to show 
his anxiety. 
4. One question may be expressed in 
different ways in the same posting.  The 
sentences may be similar.  For example: 
A: Office2000????????12?
??? 
B: Office2000????????12?
??? 
(Can the clipboard of Office2000 only 
keep 12 items?) 
???? and ???? are synonyms in the 
meaning of ?keep?. 
Dissimilar sentences may also refer 
to the same question.  For example, 
(1) How to use automatic text 
wrapping in Excel? 
(2) If I want to put two or more lines in 
one cell, what can I do? 
(3) How to use it? 
These three sentences ask the same 
question: ?How to use automatic text 
wrapping in Excel??  The second 
sentence makes a detailed description of 
what he wants to do.  Topic of the third 
sentence is the same as the first sentence 
hence is omitted.  Topic ellipsis is quite 
often seen in Chinese. 
5. Some users will give examples to 
explain the questions.  These sentences 
often start with phrases like ?for 
example? or ?such as?. 
3.2 Strategies to Separate Questions 
According to the observations in Section 3.1, 
several strategies are proposed to separate 
questions: 
 
18
(1) Separating by Question Mark (???) 
It is the simplest method.  We use it as a 
baseline strategy. 
(2) Identifying Questions by Interrogative 
Forms 
Questions are usually in interrogative forms 
including subject inversion (?is he??, ?does 
it??), using interrogatives (?who is??), or a 
declarative sentence attached with a question 
mark (?Office2000 is better??).  Only the 
third form requires a question mark.  The 
first two forms can specify themselves as 
questions by text only. Moreover, there are 
particles in Chinese indicating a question as 
well, such as ??? or ???. 
If a sentence fragment is in interrogative 
form, it will be judged as a question and 
separated from the others.  A fragment not 
in interrogative form is merged with the 
nearest question fragment preceding it (or 
following it if no preceding one).  Note that 
garbage texts have been removed before 
question separation. 
(3) Merging or Removing Similar Sentences 
If two sentence fragments are exactly the 
same, one of them will be removed.  If two 
sentence fragments are similar, they are 
merged into one question fragment. 
Similarity is measured by the Dice 
coefficient (Dice, 1945) using weights of 
common words in the two sentence 
fragments.  The similarity of two sentence 
fragments X and Y is defined as follows: 
( ) ( )( ) ( )??
?
??
??
+
?
=
YtXw
YXk
tWtwWt
kWt
YXSim
2
,  (1) 
where Wt(w) is the weight of a word w.  In 
Equation 1, k is one of the words appearing 
in both X and Y.  Fragments with similarity 
higher than a threshold are merged together. 
The weight of a word is designed as the 
weight of its part-of-speech as listed in Table 
3.  Nouns and verbs have higher weights, 
while adverbs and particles have lower 
weights.  Note that foreign words are 
assigned a rather high weight, because names 
of software products such as ?Office? or 
?Oracle? are often written in English, which 
are foreign words with respect to Chinese. 
POS Weight
Vt (Transitive Verb), 
FW (Foreign Word) 100 
N (Noun) 90 
Vi (Intransitive Verb) 80 
A (Adjective) 40 
ADV (Adverb), ASP (Tense), 
C (Connective), DET (Determiner), 
P (Preposition), T (Particle) 
0 
Table 3. Weights of Part-of-Speeches 
Before computing similarity, word 
segmentation is performed to identify words 
in Chinese text.  After that, a part-of-speech 
tagger is used to obtain POS information of 
each word. 
(4) Merging Questions with the Same Type 
The information of question type has been 
widely adopted in QA systems (Zhang and 
Lee, 2003; Hovy et al, 2002; Harabagiu et 
al., 2001).  Question type often refers to the 
possible type of its answer, such as a person 
name, a location name, or a temporal 
expression.  The question types used in this 
paper are PERSON, LOCATION, REASON, 
QUANTITY, TEMPORAL, COMPARISON, 
DEFINITION, METHOD, SELECTION, 
YESNO, and OTHER.  Rules to determine 
question types are created manually. 
This strategy tries to merge two question 
fragments of the same question type.  This 
paper proposes two features to determine the 
threshold to merge two question fragments: 
length and sum of term weights of a fragment.  
Length is measured in characters and term 
weights are designed as in Table 3. 
Merging algorithm is as follows: if the 
feature value of a question fragment is 
smaller than a threshold, it will be merged 
into the preceding question fragment (or the 
following fragment if no preceding one).  
This strategy applies recursively until no 
question fragment has a feature value lower 
than the threshold. 
(5) Merging Example Fragments 
If a fragment starts with a phrase such as ?for 
example? or ?such as?, it will be merged into 
its preceding question fragment. 
19
4 Experiments 
4.1 Experimental Data 
All the experimental data were collected from 
Yahoo! Knowledge+ (Yahoo!???? +) 6 , 
discussion groups similar to Yahoo! Answers but 
using Chinese instead of English. 
Three discussion groups, ?Business 
Application? (????), ?Website Building? 
(????), and ?Image Processing? (????), 
were selected to collect querying postings.  The 
reason that we chose these three discussion 
groups was their moderate growing rates.  We 
could collect enough amount of querying 
postings published in the same period of time. 
The following kinds of postings were not 
selected as our experimental data: 
1. No questions inside 
2. Full of algorithms or program codes 
3. Full of emoticons or Martian texts (??
?, a funny term used in Chinese to refer 
to a writing style that uses words with 
similar pronunciation to replace the 
original text) 
4. Redundant postings 
Totally 598 querying postings were collected as 
the training set and 269 postings as the test set.  
The real numbers of postings collected from each 
group are listed in Table 4, where ?BA?, ?WB?, 
and ?IP? stand for ?Business Application?, 
?Website Building?, and ?Image Processing?, 
respectively. 
Group BA WB IP 
Training Set 198 207 193 
Test Set 101 69 99 
Table 4. Numbers of Postings in the Data Set 
Two persons were asked to mark garbage texts 
and separate questions in the whole data set.  If 
a conflicting case occurred, a third person (who 
was one of the authors of this paper) would solve 
the inconsistency. 
4.2 Garbage Texts Removal 
The first factor examined in garbage text 
removal is the length threshold.  Table 5 lists 
the experimental results on the training set and 
                                                     
6 http://tw.knowledge.yahoo.com/ 
Table 6 on the test set.  All garbage keywords 
are collected from the training set. 
Eight experiments were conducted to use 
different values as length thresholds.  The 
strategy Lenk sets the length threshold to be k 
characters (no matter in Chinese or English).  
Hence, Len0 is one baseline strategy which 
removes only the garbage keyword itself.  LenS 
is the other baseline strategy which removes the 
whole sentence fragment where a garbage 
keyword appears. 
The strategy Heu uses different length 
thresholds for different classes of garbage 
keywords.  The thresholds are heuristic values 
after observing many examples in the training 
set. 
Accuracy is defined as the percentage of 
successful removal.  In one posting, if all real 
garbage texts are correctly removed and no other 
text is wrongly deleted, it counts one successful 
removal. 
Strategy Accuracy (%) 
Len0 64.21 
LenS 27.59 
Len1 73.91 
Len2 78.43 
Len3 80.60 
Len4 78.26 
Len5 71.91 
Heu 99.67 
HeuExp 99.67 
Table 5. Accuracy of Garbage Text Removal 
with Different Length Thresholds (Training) 
Strategy Accuracy (%) 
Len0 62.08 
LenS 24.54 
Len1 69.52 
Len2 75.09 
Len3 75.46 
Len4 71.75 
Len5 65.80 
Heu 87.73 
HeuExp 92.57 
Table 6. Accuracy of Garbage Text Removal 
with Different Length Thresholds (Test Set) 
As we can see in both tables, the two baseline 
strategies are poorer than any other strategy.  It 
means that length threshold is useful to decide 
garbage existence. 
Heu is the best strategy (99.67% on the 
training set and 87.73% on the test set).  Len3 is 
20
the best strategy (80.60% on the training set and 
75.49% on the test set) among Lenk, but it is far 
worse than Heu.  We can conclude that the 
length threshold should be assigned individually 
for each class of garbage words.  If it is 
assigned carefully, the performance of garbage 
removal will be good. 
The second factor is the expansion of 
garbage keywords.  The strategy HeuExp is the 
same as Heu except that the list of garbage 
keywords was expanded as described in Section 
2.2. 
Comparing the last two rows in Table 6, 
HeuExp strategy improves the performance from 
87.73% to 92.57%.  It shows that a small 
amount of postings can provide good coverage of 
garbage keywords after keyword expansion by 
using available linguistic resources. 
The results of HeuExp and Heu on the 
training set are the same.  It makes sense 
because the expanded list suggests garbage 
existence in the training set no more than the 
original list does. 
4.3 Question Segmentation 
Overall Strategies 
Six experiments were conducted to see the 
performance of different strategies for question 
segmentation.  The strategies used in each 
experiment are: 
Baseline: using only ??? (question mark) to 
separate questions 
SameS: removing repeated sentence 
fragments then separating by ??? 
Interrg: after removing repeated sentence 
fragments, separating questions which 
are in interrogative forms 
SimlrS: following the strategy Interrg, 
removing or merging similar sentence 
fragments of the same question type 
ForInst: following the strategy SimlrS, 
merging a sentence fragment beginning 
with ?for instance? and alike with its 
preceding question fragment 
SameQT: following the strategy ForInst, 
merging question fragments of the same 
question type without considering 
similarity 
Table 7 and Table 8 depict the results of the six 
experiments on the training set and the test set, 
respectively.  The second column in each table 
lists the accuracy which is defined as the 
percentage of postings which are separated into 
the same number of questions as manually 
tagged.  The third column gives the number of 
postings which are correctly separated.  The 
fourth and the fifth columns contain the numbers 
of postings which are separated into more and 
fewer questions, respectively. 
Strategy Acc (%) Same More Fewer
Baseline 50.67 303 213 82
SameS 59.03 353 156 89
Interrg 64.88 388 204 6
SimlrS 75.08 449 141 8
ForInst 75.75 453 137 8
SameQT 88.29 528 13 57
Table 7. Accuracy of Question Segmentation 
by Different Strategies (Training Set) 
Strategy Acc (%) Same More Fewer
Baseline 54.28 146 84 39
SameS 65.43 176 54 39
Interrg 65.43 176 93 0
SimlrS 74.35 200 68 1
ForInst 74.35 200 68 1
SameQT 85.87 231 16 22
Table 8. Accuracy of Question Segmentation 
by Different Strategies (Test Set) 
As we can see in Table 7, performance is 
improved gradually after adding new strategies.  
SameQT achieves the best performance with 
88.29% accuracy.  Same conclusion could also 
be made by the results on the test set.  SameQT 
is the best one with 85.87% accuracy. 
In Table 7, Baseline achieves only 50.67% 
accuracy.  That matches our observations: (1) 
one question is often stated many times by 
sentences ended with question marks in one 
posting (as 213 postings were separated into 
more questions); (2) some users do not use ??? in 
writing (as 82 postings were separated into fewer 
questions). 
SameS greatly reduces the cases (57 postings) 
of separation into more questions by removing 
repeated sentences. 
On the other hand, Interrg greatly reduces the 
cases (76 postings) of separation into fewer 
questions.  Many question sentences without 
question marks were successfully captured by 
detecting the interrogative forms. 
SimlrS also improves a lot (successfully 
reducing number of questions separated in 63 
postings).  But ForInst only improves a little.  
It is more common to express one question 
several times in different way than giving 
21
examples. 
SameQT achieves the best performance, 
which means that question type is a good strategy.  
Different ways to express a question are usually 
in the same question type.  Comparing with 
SimlrS which also considers sentence fragments 
in the same question type, more improvement 
comes from the successful merging of fragments 
with topic ellipses, co-references, or paraphrases.  
However, there may be other questions in the 
same question type which are wrongly merged 
together (as 49 failures in the training set). 
Considering the results on the test set, Interrg 
does not improve the overall performance 
comparing to SameS because the improvement 
equals the drop.  ForInst does not improve 
either.  It seems that giving examples is not 
common in the discussion groups. 
Thresholds in SameQT 
In the strategy SameQT, two features, length and 
sum of term weights, are used to determine 
thresholds to merge question fragments as 
mentioned in Section 3.2.  In order to decide 
which feature is better and which threshold value 
should be set, two experiments were conducted. 
LenThr Acc (%) LenThr Acc (%)
0 75.75 9 85.62 
3 76.25 10 86.62 
4 78.60 15 88.29 
5 81.94 20 88.13 
6 84.95 30 88.63 
7 85.79 40 88.29 
8 86.29 ? 88.29 
Table 9. Accuracy of Question Segmentation 
with Different Length Thresholds 
70
75
80
85
90
0 3 4 5 6 7 8 9 10 15 20 30 40 ?
Length Threshold
(%)
 
Figure 1. Accuracy of Question Segmentation 
with Different Length Thresholds 
Table 9 depicts the experimental results of using 
length of sentence fragments as merging 
threshold.  The column ?LenThr? lists different 
settings of length threshold and the column 
?Acc? gives the accuracy. 
The performance is gradually improved as 
the value of length threshold increases.  The 
best one is LenThr=30 with 88.63% accuracy.  
However, ?Always Merging? (LenThr=?) 
achieves 88.29% accuracy, which is also 
acceptable comparing to the best performance.  
Fig 1 shows the curve of accuracy against length 
threshold. 
Table 10 presents the experimental results of 
using sum of term weights as merging thresold.  
The column ?WgtThr? lists different settings of 
length threshold and the column ?Acc? gives the 
accuracy. 
The performance is also gradually improved 
as the value of weight threshold increases.  
When WgtThr is set to be 500, 700, or 900, the 
performance is the best, with 88.46% accuracy.  
But the same as the threshold settings of length 
feature, the best one does not outperform 
?Always Merging? strategy (WgtThr=?, 88.29% 
accuracy) too much.  Fig 2 shows the curve of 
accuracy against similarity threshold. 
WgtThr Acc (%) WgtThr Acc (%)
0 75.75 350 87.29 
50 77.93 400 88.13 
100 83.11 450 88.29 
150 85.28 500 88.46 
200 86.29 700 88.46 
250 86.79 900 88.46 
300 87.46 ? 88.29 
Table 10. Accuracy of Question Segmentation 
with Different Weight Thresholds 
70
75
80
85
90
0 50 100 150 200 250 300 350 400 450 500 700 900 ?
Weight Threshold(%)
 
Figure 2. Accuracy of Question Segmentation 
with Different Weight Thresholds 
From the results of above experiments, we can 
see that although using length feature with a 
22
threshold LenThr=30 achieves the best 
performance, ?Always Merging? is more 
welcome for a online system because no feature 
extraction or computation is needed with only a 
little sacrifice of performance.  Hence we 
choose ?Always Merging? as merging strategy in 
SameQT. 
5 Conclusion and Future Work 
This paper proposes question pre-processing 
methods for a FQA-style QA system on 
discussion groups in the Internet.  For a posting 
already existing or being submitted to a 
discussion group, garbage texts in it are removed 
first, and then different questions in it are 
identified so that they can be compared with 
other questions individually. 
An expanded list of garbage keywords is 
used to detect garbage texts.  If there is a 
garbage keyword appearing in a sentence 
fragment and the fragment has a length shorter 
than a threshold corresponding to the class of the 
garbage keyword, the fragment will be judged as 
a garbage text.  This method achieves 92.57% 
accuracy on the test set.  It means that a small 
set is sufficient to collect all classes of garbage 
keywords. 
In question segmentation, sentence fragments 
in interrogative forms are considered as question 
fragments.  Besides, repeated fragments are 
removed and fragments of the same question 
types are merged into one fragment.  The 
overall accuracy is 85.87% on the test set. 
In the future, performance of a QA system 
with or without question pre-processing will be 
evaluated to verify its value. 
New methods to create the list of garbage 
keywords more robotically should be studied, as 
well as the automatic assignments of the length 
thresholds of classes of garbage keywords. 
New feature should be discovered in the 
future in order to segment questions more 
accurately. 
Although the strategies and the thresholds are 
developed according to experimental data in 
Chinese, we can see that many of them are 
language-independent or can be adapted with not 
too much effort. 
Reference 
Burke, Robin, Kristian Hammond, Vladimir 
Kulyukin, Steven Lytinen, Noriko Tomuro, 
and Scott Schoenberg (1997) ?Natural 
language processing in the FAQFinder 
system: Results and prospects,? Proceedings 
of the 1997 AAAI Spring Symposium on 
Natural Language Processing for the World 
Wide Web, pp. 17-26. 
Dice, Lee R. (1945) ?Measures of the amount of 
ecologic association between species,? 
Journal of Ecology, Vol. 26, pp. 297-302. 
Harabagiu, Sanda, Dan Moldovan, Marius Pa?ca, 
Rada Mihalcea, Mihai Surdeanu, R?zvan 
Bunescu, Roxana G?rju, Vasile Rus, and Paul 
Mor?rescu (2001) ?The Role of 
Lexico-Semantic Feedback in Open-Domain 
Textual Question-Answering,? Proceedings 
of ACL-EACL 2001, pp. 274-281. 
Hovy, Eduard, Ulf Hermjakob, and Chin-Yew 
Lin (2002) ?The Use of External Knowledge 
in Factoid QA,? Proceedings of TREC-10, pp. 
644-652. 
Lai, Yu-Sheng, Kuao-Ann Fung, and 
Chung-Hsien Wu (2002) ?FAQ Mining via 
List Detection,? Proceedings of the COLING 
Workshop on Multilingual Summarization 
and Question Answering. 
Lytinen, Steven and Noriko Tomuro (2002) ?The 
use of question types to match questions in 
FAQFinder,? Proceedings of the 2002 AAAI 
Spring Symposium on Mining Answers from 
Texts and Knowledge Bases, pp. 46-53. 
Saquete, Estela, Patricio Martinez-Barco, Rafael 
Munoz, and Jose Luis Vicedo Gonzalez 
(2004) ?Splitting Complex Temporal 
Questions for Question Answering Systems,? 
Proceedings of ACL 2004, pp. 566-573. 
Soricut, Radu and Eric Brill (2004) ?Automatic 
Question Answering: Beyond the Factoid,? 
Proceedings of HLT-NAACL 2004, pp. 57-64. 
Zhang, Dell and Wee Sun Lee (2003) ?Question 
Classification using Support Vector 
Machines,? Proceedings of SIGIR 2003, pp. 
26-32. 
 
23
