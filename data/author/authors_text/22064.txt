Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604?614,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Name-aware Machine Translation
Haibo Li? Jing Zheng? Heng Ji? Qi Li? Wen Wang?
? Computer Science Department and Linguistics Department
Queens College and Graduate Center, City University of New York
New York, NY, USA 10016
{lihaibo.c, hengjicuny, liqiearth}@gmail.com
? Speech Technology & Research Laboratory
SRI International
Menlo Park, CA, USA 94025
{zj, wwang}@speech.sri.com
Abstract
We propose a Name-aware Machine
Translation (MT) approach which can
tightly integrate name processing into MT
model, by jointly annotating parallel cor-
pora, extracting name-aware translation
grammar and rules, adding name phrase
table and name translation driven decod-
ing. Additionally, we also propose a new
MT metric to appropriately evaluate the
translation quality of informative words,
by assigning different weights to differ-
ent words according to their importance
values in a document. Experiments on
Chinese-English translation demonstrated
the effectiveness of our approach on en-
hancing the quality of overall translation,
name translation and word alignment over
a high-quality MT baseline1.
1 Introduction
A shrinking fraction of the world?s Web pages are
written in English, therefore the ability to access
pages across a range of languages is becoming in-
creasingly important. This need can be addressed
in part by cross-lingual information access tasks
such as entity linking (McNamee et al, 2011; Cas-
sidy et al, 2012), event extraction (Hakkani-Tur
et al, 2007), slot filling (Snover et al, 2011) and
question answering (Parton et al, 2009; Parton
and McKeown, 2010). A key bottleneck of high-
quality cross-lingual information access lies in the
performance of Machine Translation (MT). Tradi-
tional MT approaches focus on the fluency and
accuracy of the overall translation but fall short
in their ability to translate certain content word-
s including critical information, especially names.
1Some of the resources and open source programs devel-
oped in this work are made freely available for research pur-
pose at http://nlp.cs.qc.cuny.edu/NAMT.tgz
A typical statistical MT system can only trans-
late 60% person names correctly (Ji et al, 2009).
Incorrect segmentation and translation of names
which often carry central meanings of a sentence
can also yield incorrect translation of long con-
texts. Names have been largely neglected in the
prior MT research due to the following reasons:
? The current dominant automatic MT scoring
metrics (such as Bilingual Evaluation Under-
study (BLEU) (Papineni et al, 2002)) treat
all words equally, but names have relative low
frequency in text (about 6% in newswire and
only 3% in web documents) and thus are vast-
ly outnumbered by function words and com-
mon nouns, etc..
? Name translations pose a greater complexity
because the set of names is open and highly
dynamic. It is also important to acknowledge
that there are many fundamental differences
between the translation of names and other
tokens, depending on whether a name is ren-
dered phonetically, semantically, or a mixture
of both (Ji et al, 2009).
? The artificial settings of assigning low
weights to information translation (compared
to overall word translation) in some large-
scale government evaluations have discour-
aged MT developers to spend time and ex-
plore resources to tackle this problem.
We propose a novel Name-aware MT (NAMT)
approach which can tightly integrate name pro-
cessing into the training and decoding processes of
an end-to-end MT pipeline, and a new name-aware
metric to evaluate MT which can assign different
weights to different tokens according to their im-
portance values in a document. Compared to pre-
vious methods, the novel contributions of our ap-
proach are:
1. Tightly integrate joint bilingual name tag-
ging into MT training by coordinating tagged
604
names in parallel corpora, updating word seg-
mentation, word alignment and grammar ex-
traction (Section 3.1).
2. Tightly integrate name tagging and transla-
tion into MT decoding via name-aware gram-
mar (Section 3.2).
3. Optimize name translation and context trans-
lation simultaneously and conduct name
translation driven decoding with language
model (LM) based selection (Section 3.2).
4. Propose a new MT evaluation metric which
can discriminate names and non-informative
words (Section 4).
2 Baseline MT
As our baseline, we apply a high-performing
Chinese-English MT system (Zheng, 2008; Zheng
et al, 2009) based on hierarchical phrase-based
translation framework (Chiang, 2005). It is based
on a weighted synchronous context-free grammar
(SCFG). All SCFG rules are associated with a set
of features that are used to compute derivation
probabilities. The features include:
? Relative frequency in two directions P (?|?)
andP (?|?), estimating the likelihoods of one
side of the rule r: X ?< ?, ? > translating
into the other side, where ? and ? are strings
of terminals and non-terminals in the source
side and target side. Non-terminals in ? and
? are in one-to-one correspondence.
? Lexical weights in two directions: Pw(?|?)
andPw(?|?), estimating likelihoods of word-
s in one side of the rule r: X ?< ?, ? >
translating into the other side (Koehn et al,
2003).
? Phrase penalty: a penalty exp(1) for a rule
with no non-terminal being used in deriva-
tion.
? Rule penalty: a penalty exp(1) for a rule
with at least one non-terminal being used in
derivation.
? Glue rule penalty: a penalty exp(1) if a glue
rule used in derivation.
? Translation length: number of words in trans-
lation output.
Our previous work showed that combining mul-
tiple LMs trained from different sources can lead
to significant improvement. The LM used for de-
coding is a log-linear combination of four word
n-gram LMs which are built on different English
corpora (details described in section 5.1), with
the LM weights optimized on a development set
and determined by minimum error rate training
(MERT), to estimate the probability of a word giv-
en the preceding words. All four LMs were trained
using modified Kneser-Ney smoothing algorithm
(Chen and Goodman, 1996) and converted into
Bloom filter LMs (Talbot and Brants, 2008) sup-
porting memory map.
The scaling factors for all features are optimized
by minimum error rate training algorithm to max-
imize BLEU score (Och, 2003). Given an input
sentence in the source language, translation into
the target language is cast as a search problem,
where the goal is to find the highest-probability
derivation that generates the source-side sentence,
using the rules in our SCFG. The source-side
derivation corresponds to a synchronous target-
side derivation and the terminal yield of this target-
side derivation is the output of the system. We em-
ploy our CKY-style chart decoder, named SRInter-
p, to solve the search problem.
3 Name-aware MT
We tightly integrate name processing into the
above baseline to construct a NAMT model. Fig-
ure 1 depicts the general procedure.
3.1 Training
This basic training process of NAMT requires us
to apply a bilingual name tagger to annotate par-
allel training corpora. Traditional name tagging
approaches for single languages cannot address
this requirement because they were all built on da-
ta and resources which are specific to each lan-
guage without using any cross-lingual features.
In addition, due to separate decoding processes
the results on parallel data may not be consistent
across languages. We developed a bilingual joint
name tagger (Li et al, 2012) based on condition-
al random fields that incorporates both monolin-
gual and cross-lingual features and conducts join-
t inference, so that name tagging from two lan-
guages can mutually enhance each other and there-
fore inconsistent results can be corrected simulta-
neously. This joint name tagger achieved 86.3%
bilingual pair F-measure with manual alignment
and 84.4% bilingual pair F-measure with automat-
ic alignment as reported in (Li et al, 2012). Given
a parallel sentence pair we first apply Giza++ (Och
and Ney, 2003) to align words, and apply this join-
605
Dec
odin
g
Hier
arch
ical 
Phra
sed-
base
d M
T
Tran
slate
d Te
xt
Tran
slate
Bi-te
xt 
Data Sou
rce Text
Join
t
Nam
e Ta
gger
Sou
rce L
angu
age 
Nam
e Ta
gger
Nam
e Tr
ansl
ator
Trai
ning
Nam
e Pa
ir M
iner
Extr
act s
ourc
e lan
guag
e na
mes
 
and
 add
 the
m to
 dict
iona
ries 
for 
sour
ce la
ngu
age 
nam
e ta
gger
 E
xtra
ct n
ame
 pair
s an
d 
add
 the
m to
tran
slati
on 
dict
iona
ry
Extr
act a
nd a
dd 
nam
e pa
irs t
o
phra
se ta
ble
GIZA
++
Rule
 Extr
acto
r
Extr
act S
CFG
 rule
s wi
th 
com
bina
tion
 of n
ame
-rep
lace
d 
data
 and
 orig
inal 
bi-te
xt d
ata
Rep
lace
 nam
es w
ith 
non
-term
inals
 and
 
com
bine
 with
 the
 
orig
inal 
para
llel d
ata
Figure 1: Architecture of Name-aware Machine Translation System.
t bilingual name tagger to extract three types of
names: (Person (PER), Organization (ORG) and
Geo-political entities (GPE)) from both the source
side and the target side. We pair two entities from
two languages, if they have the same entity type
and are mapped together by word alignment. We
ignore two kinds of names: multi-word names
with conflicting boundaries in two languages and
names only identified in one side of a parallel sen-
tence.
We built a NAMT system from such name-
tagged parallel corpora. First, we replace tagged
name pairs with their entity types, and then
use Giza++ and symmetrization heuristics to re-
generate word alignment. Since the name tags ap-
pear very frequently, the existence of such tags
yields improvement in word alignment quality.
The re-aligned parallel corpora are used to train
our NAMT system based on SCFG. Since the joint
name tagger ensures that each tagged source name
has a corresponding translation on the target side
(and vice versa), we can extract SCFG rules by
treating the tagged names as non-terminals.
However, the original parallel corpora contain
many high-frequency names, which can already be
handled well by the baseline MT. Some of these
names carry special meanings that may influence
translations of the neighboring words, and thus re-
placing them with non-terminals can lead to infor-
mation loss and weaken the translation model. To
address this issue, we merged the name-replaced
parallel data with the original parallel data and ex-
tract grammars from the combined corpus. For ex-
ample, given the following sentence pair:
? -???e???e????? .
? China appeals to world for non involvement
in Angola conflict .
after name tagging it becomes
? GPE??e???e GPE?? .
? GPE appeals to world for non involvement in
GPE conflict .
Both sentence pairs are kept in the combined data
to build the translation model.
3.2 Decoding
During decoding phase, we extract names with
the baseline monolingual name tagger described
in (Li et al, 2012) from a source document. It-
s performance is comparable to the best report-
ed results on Chinese name tagging on Automat-
ic Content Extraction (ACE) data (Ji and Grish-
man, 2006; Florian et al, 2006; Zitouni and Flo-
rian, 2008; Nguyen et al, 2010). Then we ap-
ply a state-of-the-art name translation system (Ji
et al, 2009) to translate names into the target lan-
guage. The name translation system is composed
of the following steps: (1) Dictionary matching
based on 150,041 name translation pairs; (2) Sta-
tistical name transliteration based on a structured
perceptron model and a character based MT mod-
el (Dayne and Shahram, 2007); (3) Context infor-
mation extraction based re-ranking.
In our NAMT framework, we add the following
extensions to name translation.
We developed a name origin classifier based on
Chinese last name list (446 name characters) and
name structure parsing features to distinguish Chi-
nese person names and foreign person names (Ji,
2009), so that pinyin conversion is applied for Chi-
nese names while name transliteration is applied
only for foreign names. This classifier works rea-
sonably well in most cases (about 92% classifica-
tion accuracy), except when a common Chinese
last name appears as the first character of a foreign
606
name, such as ?1?? which can be translated ei-
ther as ?Jolie? or ?Zhu Li?.
For those names with fewer than five instances
in the training data, we use the name translation
system to provide translations; for the rest of the
names, we leave them to the baseline MT mod-
el to handle. The joint bilingual name tagger was
also exploited to mine bilingual name translation
pairs from parallel training corpora. The mapping
score between a Chinese name and an English
name was computed by the number of aligned to-
kens. A name pair is extracted if the mapping
score is the highest among all combinations and
the name types on both sides are identical. It is
necessary to incorporate word alignment as addi-
tional constraints because the order of names is of-
ten changed after translation. Finally, the extract-
ed 9,963 unique name translation pairs were also
used to create an additional name phrase table for
NAMT. Manual evaluation on 2,000 name pairs
showed the accuracy is 86%.
The non-terminals in SCFG rules are rewritten
to the extracted names during decoding, therefore
allow unseen names in the test data to be trans-
lated. Finally, based on LMs, our decoder ex-
ploits the dynamically created phrase table from
name translation, competing with originally ex-
tracted rules, to find the best translation for the
input sentence.
4 Name-aware MT Evaluation
Traditional MT evaluation metrics such as
BLEU (Papineni et al, 2002) and Translation Ed-
it Rate (TER) (Snover et al, 2006) assign the
same weights to all tokens equally. For exam-
ple, incorrect translations of ?the? and ?Bush? will
receive the same penalty. However, for cross-
lingual information processing applications, we
should acknowledge that certain informationally
critical words are more important than other com-
mon words. In order to properly evaluate the trans-
lation quality of NAMT methods, we propose to
modify the BLEU metric so that they can dynam-
ically assign more weights to names during evalu-
ation.
BLEU considers the correspondence between a
system translation and a human translation:
BLEU = BP ? exp
( N?
n=1
wn log pn
)
(1)
where BP is brevity penalty defined as follows:
BP =
{
1 if c > r,
e(1?r/c) if c ? r. (2)
where wn is a set of positive weights summing to
one and usually uniformly set as wn = 1/N , c is
the length of the system translation and r is the
length of reference translation, and pn is modified
n-gram precision defined as:
pn =
?
C?Candidates
?
n-gram?C
Countclip(n-gram)
?
C??Candidates
?
n-gram??C?
Countclip(n-gram?)
(3)
where C and C ? are translation candidates in the
candidate sentence set, if a source sentence is
translated to many candidate sentences.
As in BLEU metric, we first count the maxi-
mum number of times an n-gram occurs in any s-
ingle reference translation. The total count of each
candidate n-gram is clipped at sentence level by it-
s maximum reference count. Then we add up the
weights of clipped n-grams and divide them by the
total weight of all n-grams.
Based on BLEU score, we design a name-aware
BLEU metric as follows. Depending on whether a
token t is contained in a name in reference trans-
lation, we assign a weight weightt to t as follows:
weightt ={
1? e?tf(t,d)?idf(t,D), if t never appears in names
1 + PEZ , if t occurs in name(s)
(4)
where PE is the sum of penalties of non-name
tokens and Z is the number of tokens within all
names:
PE =
?
t never appears in names
e?tf(t,d)?idf(t,D) (5)
In this paper, the tf ? idf score is computed at sen-
tence level, therefore, D is the sentence set and
each d ? D is a sentence.
The weight of an n-gram in reference translation
is the sum of weights of all tokens it contains.
weightngram =
?
t?ngram
weightt (6)
Next, we compute the weighted modified n-
gram precision Countweight?clip(n-gram) as fol-
lows:
Countweight?clip(n-gram) =?
if the ngrami is correctly translated
weightngrami (7)
607
The Countclip(n-gram) in the equation 3 is
substituted with aboveCountweight?clip(n-gram).
When we sum up the total weight of all n-grams of
a candidate translation, some n-grams may contain
tokens which do not exist in reference translation.
We assign the lowest weight of tokens in reference
translation to these rare tokens.
We also add an item, name penalty NP , to
penalize the output sentences which contain too
many or too few names:
NP = e?(uv?1)2/2? (8)
where u is the number of name tokens in system
translation and v is the number of name tokens in
reference translation.
Finally the name-aware BLEU score is defined
as:
BLEUNA = BP ?NP ? exp
( N?
n=1
wn logwpn
)
(9)
This new metric can also be applied to evalu-
ate MT approaches which emphasize other types
of facts such as events, by simply replacing name
tokens by other fact tokens.
5 Experiments
In this section we present the experimental results
of NAMT compared to the baseline MT.
5.1 Data Set
We used a large Chinese-English MT training cor-
pus from various sources and genres (including
newswire, web text, broadcast news and broadcast
conversations) for our experiments. We also used
some translation lexicon data and Wikipedia trans-
lations. The majority of the data sets were col-
lected or made available by LDC for U.S. DARPA
Translingual Information Detection, Extraction
and Summarization (TIDES) program, Global Au-
tonomous Language Exploitation (GALE) pro-
gram, Broad Operational Language Translation
(BOLT) program and National Institute of Stan-
dards and Technology (NIST) MT evaluations.
The training corpus includes 1,686,458 sentence
pairs. The joint name tagger extracted 1,890,335
name pairs (295,087 Persons, 1,269,056 Geo-
political entities and 326,192 Organizations).
Four LMs, denoted LM1, LM2, LM3, and
LM4, were trained from different English cor-
pora. LM1 is a 7-gram LM trained on the tar-
get side of Chinese-English and Egyptian Arabic-
English parallel text, English monolingual discus-
sion forums data R1-R4 released in BOLT Phase
1 (LDC2012E04, LDC2012E16, LDC2012E21,
LDC2012E54), and English Gigaword Fifth Edi-
tion (LDC2011T07). LM2 is a 7-gram LM trained
only on the English monolingual discussion fo-
rums data listed above. LM3 is a 4-gram LM
trained on the web genre among the target side
of all parallel text (i.e., web text from pre-BOLT
parallel text and BOLT released discussion fo-
rum parallel text). LM4 is a 4-gram LM trained
on the English broadcast news and conversation
transcripts released under the DARPA GALE pro-
gram. Note that for LM4 training data, some tran-
scripts were quick transcripts and quick rich tran-
scripts released by LDC, and some were generated
by running flexible alignment of closed captions or
speech recognition output from LDC on the audio
data (Venkataraman et al, 2004).
In order to demonstrate the effectiveness and
generality of our approach, we evaluated our ap-
proach on seven test sets from multiple genres and
domains. We asked four annotators to annotate
names in four reference translations of each sen-
tence and an expert annotator to adjudicate result-
s. The detailed statistics and name distribution of
each test data set is shown in Table 1. The per-
centage of names occurred fewer than 5 times in
training data are listed in the brackets in the last
column of the table.
5.2 Overall Performance
Besides the new name-aware MT metric, we also
adopt two traditional metrics, TER to evaluate the
overall translation performance and Named Entity
Weak Accuracy (NEWA) (Hermjakob et al, 2008)
to evaluate the name translation performance.
TER measures the amount of edits required to
change a system output into one of the reference
translations. Specifically:
TER = # of editsaverage # of reference words (10)
Possible edits include insertion, substitution dele-
tion and shifts of words.
The NEWA metric is defined as follows. Us-
ing a manually assembled name variant table, we
also support the matching of name variants (e.g.,
?World Health Organization? and ?WHO?).
NEWA = Count # of correctly translated namesCount # of names in references (11)
608
Corpus Genre Sentence # Word # Token # GPE(%) PER(%) ORG(%) All namesin source in reference (% occurred < 5)
BOLT 1 forum 1,200 20,968 24,193 875(82.9) 90(8.5) 91(8.6) 1,056 (51.4)
BOLT 2 forum 1,283 23,707 25,759 815(73.7) 141(12.8) 149(13.5) 1,105 (65.9)
BOLT 3 forum 2,000 38,595 42,519 1,664(80.4) 204(9.8) 204(9.8) 2,072 (47.4)
BOLT 4 forum 1,918 41,759 47,755 1,852(80.0) 348(25.0) 113(5.0) 2,313 (53.3)
BOLT 5 blog 950 23,930 26,875 352(42.5) 235(28.3) 242(29.2) 829 (55.3)
NIST2006 news&blog 1,664 38,442 45,914 1,660(58.2) 568(19.9) 625(21.9) 2,853 (73.1)
NIST2008 news&blog 1,357 32,646 37,315 700(47.9) 367(25.1) 395(27.0) 1,462 (72.0)
Table 1: Statistics and Name Distribution of Test Data Sets.
Metric System BOLT 1 BOLT 2 BOLT 3 BOLT 4 BOLT 5 NIST2006 NIST2008
BLEU
Baseline 14.2 14.0 17.3 15.6 15.3 35.5 29.3
NPhrase 14.1 14.4 17.1 15.4 15.3 35.4 29.3
NAMT 14.2 14.6 16.9 15.7 15.5 36.3 30.0
Name-aware BLEU
Baseline 18.2 17.9 18.6 17.6 18.3 36.1 31.7
NPhrase 18.1 18.8 18.5 18.1 18.0 35.8 31.8
NAMT 18.4 19.5 19.7 18.2 18.9 39.4 33.1
TER
Baseline 70.6 71.0 69.4 70.3 67.1 58.7 61.0
NPhrase 70.6 70.4 69.4 70.4 67.1 58.7 60.9
NAMT 70.3 70.2 69.2 70.1 66.6 57.7 60.5
NEWA
All
Baseline 69.7 70.1 73.9 72.3 60.6 66.5 60.4
NPhrase 69.8 71.1 73.8 72.5 60.6 68.3 61.9
NAMT 71.4 72.0 77.7 75.1 62.7 72.9 63.2
GPE
Baseline 72.8 78.4 80.0 78.7 81.3 79.2 76.0
NPhrase 73.6 79.3 79.2 78.9 82.3 82.6 79.5
NAMT 74.2 80.2 82.8 80.4 79.3 85.5 79.3
PER
Baseline 53.3 44.7 45.1 49.4 48.9 54.2 51.2
NPhrase 52.2 45.4 48.9 48.5 47.6 55.1 50.9
NAMT 55.6 45.4 58.8 55.2 56.2 60.0 52.3
ORG
Baseline 56.0 49.0 52.9 38.1 41.7 44.0 41.3
NPhrase 50.5 50.3 54.4 40.7 41.3 42.2 40.7
NAMT 60.4 52.3 55.4 41.6 45.0 51.0 44.8
Table 2: Translation Performance (%).
For better comparison with NAMT, besides the
original baseline, we develop the other baseline
system by adding name translation table into the
phrase table (NPhrase).
Table 2 presents the performance of overal-
l translation and name translation. We can see
that except for the BOLT3 data set with BLEU
metric, our NAMT approach consistently outper-
formed the baseline system for all data sets with
all metrics, and provided up to 23.6% relative er-
ror reduction on name translation. According to
Wilcoxon Matched-Pairs Signed-Ranks Test, the
improvement is not significant with BLEU metric,
but is significant at 98% confidence level with all
of the other metrics. The gains are more signifi-
cant for formal genres than informal genres main-
ly because most of the training data for name tag-
ging and name translation were from newswire.
Furthermore, using external name translation table
only did not improve translation quality in most
test sets except for BOLT2. Therefore, it is im-
portant to use name-replaced corpora for rule ex-
traction to fully take advantage of improved word
alignment.
Many errors from the baseline MT approach oc-
curred because some parts of out-of-vocabulary
names were mistakenly segmented into common
words. For example, the baseline MT system mis-
takenly translated a person name ?Y?? (Sun
Honglei)? into ?Sun red thunder?. In informal
genres such as discussion forums and web blogs,
even common names often appear in rare form-
s due to misspelling or morphing. For example,
?e8l (Obama)? was mistakenly translated into
?Ma Olympic?. Such errors can be compounded
when word re-ordering was applied. For example,
the following sentence: ????????/:
'J/iy (Guo Meimei?s strength real-
ly is formidable, I really admire her)? was mis-
takenly translated into ?Guo the strength of the
America and the America also really strong , ah
, really admire her? by the baseline MT system
because the person name ???? (Guomeimei)?
was mistakenly segmented into three words ??
(Guo)?, ?? (the America)? and ?? (the Ameri-
ca)?. But our NAMT approach successfully iden-
tified and translated this name and also generated
better overall translation: ?Guo Meimei ?s power
is also really strong , ah , really admire her?.
609
B L E U N a m e - a w a r eB L E U024681 01 2
1 41 61 82 0Score A u t o m a t i c  M e t r i c s H u m .  1  H u m .  2 H u m .  30 . 00 . 51 . 01 . 52 . 02 . 5
3 . 03 . 54 . 0 b a s e l i n e N A M T Score H u m a n  E v a l u a t i o n
Figure 2: Scores based on Automatic Metrics and Human
Evaluation.
5.3 Name-aware BLEU vs The Human
Evaluation
In order to investigate the correlation between
name-aware BLEU scores and human judgment
results, we asked three bi-lingual speakers to judge
our translation output from the baseline system
and the NAMT system, on a Chinese subset of 250
sentences (each sentence has two corresponding
translations from baseline and NAMT) extracted
randomly from 7 test corpora. The annotators rat-
ed each translation from 1 (very bad) to 5 (very
good) and made their judgments based on whether
the translation is understandable and conveys the
same meaning.
We computed the name-aware BLEU scores on
the subset and also the aggregated average scores
from human judgments. Figure 2 shows that
NAMT consistently achieved higher scores with
both name-aware BLEU metric and human judge-
ment. Furthermore, we calculated three Pearson
product-moment correlation coefficients between
human judgment scores and name-aware BLEU s-
cores of these two MT systems. Give the sample
size and the correlation coefficient value, the high
significance value of 0.99 indicates that name-
aware BLEU tracks human judgment well.
5.4 Word Alignment
It is also important to investigate the impact of our
NAMT approach on improving word alignmen-
t. We conducted the experiment on the Chinese-
English Parallel Treebank (Li et al, 2010) with
ground-truth word alignment. The detailed pro-
cedure following NAMT framework is as follows:
(1) Ran the joint bilingual name tagger; (2) Re-
placed each name string with its name type (PER,
ORG or GPE), and ran Giza++ on the replaced
sentences; (3) Ran Giza++ on the words within
Words Method P R F 
Baseline Giza++ 69.8 47.8 56.7 
Joint Name 
Tagging 
70.4 48.1 57.1 
 
Overall 
Words 
Ground-truth 
Name Tagging 
(Upper-bound) 
71.3 48.9 58.0 
Baseline Giza++ 86.0 31.4 46.0 Words 
Within 
Names 
Joint Name 
Tagging 
77.6 37.2 50.3 
 
 
 
 
 
 
 
 
 
 
Table 3: Impact of Joint Bilingual Name Tagging on Word
Alignment (%).
each name pair. (4) Merged (2) and (3) to pro-
duce the final word alignment results. In order to
compare with the upper-bound gains, we also mea-
sured the performance of applying ground-truth
name tagging with the above procedures.
The experiment results are shown in Table 3.
For the words within names, our approach provid-
ed significant gains by enhancing F-measure from
46.0% to 50.3%. Only 10.6% words are within
names, therefore the upper-bound gains on over-
all word alignment is only 1.3%. Our joint name
tagging approach achieved 0.4% (statistically sig-
nificant) improvement over the baseline. In Fig-
ure 3 we categorized the sentences according to
the percentage of name words in each sentence and
measured the improvement for each category. We
can clearly see that as the sentences include more
names, the gains achieved by our approach tend to
be greater.
5.5 Remaining Error Analysis
Although the proposed model has significantly en-
hanced translation quality, some challenges re-
main. We analyze some major sources of the re-
maining errors as follows.
1. Name Structure Parsing.
We found that the gains of our NAMT approach
were mainly achieved for names with one or two
components. When the name structure becomes
too complicated to parse, name tagging and name
translation are likely to produce errors, especially
for long nested organizations. For example, ??0
???b?@? (Anti-malfeasance Bureau of
Gutian County Procuratorate) consists of a nested
organization name with a GPE as modifier: ??
0???b? (Gutian County Procuratorate) and
an ORG name: ??@? (Anti-malfeasance Bu-
reau).
2. Name abbreviation tagging and translation.
Some organization abbreviations are also dif-
ficult to extract because our name taggers have
610
0~10 10~20 20~30 30~40 >40
-0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
F-Measure Gains in Overall Word Alignment (%)
#name tokens/#all tokens(%)
 Baseline Giza++
 Joint Name Tagging
 Ground-truth Name Tagging (Upper-bound)
Figure 3: Word alignment gains according to the percentage
of name words in each sentence.
not incorporated any coreference resolution tech-
niques. For example, without knowing that ?FAW?
refers to ?First Automotive Works? in ?FAW has
also utilized the capital market to directly fi-
nance, and now owns three domestic listed compa-
nies?, our system mistakenly labeled it as a GPE.
The same challenge exists in name alignment and
translation (for example, ? i (Min Ge)? refer-
s to ? -??Zi}?XProceedings of the 25th International Conference on Computational Linguistics, pages 74?81,
Dublin, Ireland, August 23-29 2014.
Cross-media Cross-genre Information Ranking Multi-media Information
Networks
Tongtao Zhang
Rensselaer Polytechnic Institute
zhangt13@rpi.edu
Haibo Li
Nuance
lihaibo.c@gmail.com
Hongzhao Huang
R.P.I.
huangh9@rpi.edu
Heng Ji
R.P.I.
jih@rpi.edu
Min-Hsuan Tsai
mtsai2@illinois.edu
Shen-Fu Tsai
University of Illinois at Urbana-Champaign
stsai8@illinois.edu
Thomas Huang
huang@ifp.uiuc.edu
Abstract
Current web technology has brought us a scenario that information about a certain topic is widely dis-
persed in data from different domains and data modalities, such as texts and images from news and social
media. Automatic extraction of the most informative and important multimedia summary (e.g. a ranked
list of inter-connected texts and images) from massive amounts of cross-media and cross-genre data can
significantly save users? time and effort that is consumed in browsing. In this paper, we propose a novel
method to address this new task based on automatically constructed Multi-media Information Networks
(MiNets) by incorporating cross-genre knowledge and inferring implicit similarity across texts and im-
ages. The facts from MiNets are exploited in a novel random walk-based algorithm to iteratively propagate
ranking scores across multiple data modalities. Experimental results demonstrated the effectiveness of our
MiNets-based approach and the power of cross-media cross-genre inference.
1 Introduction
Recent development on web technology ? especially on fast connection and large-scale storage systems ? has
enabled social and news media to fulfill their jobs more efficiently in time and depth. However, such development
also raises some problems such as overwhelming social media information and distracting news media contents.
In emergent scenarios such as facing an incoming disaster (e.g., Hurricane Irene in 2011 or Sandy in 2012), tweets
and news are often repeatedly spread and forwarded in certain circles and contents are often overlapped by each
other. However, browsing these messages and pages is almost unpleasant and inefficient. Therefore, an automatic
summarization on piles of tweets and news is always necessary and welcomed, among which ranking is the most
intuitive way to inform the users about the most informative content.
A passive solution is prompting the users to add more key words when typing the search query as most search
engines do. However, without prior knowledge or due to the word limit, it is never trivial for the users to establish
a satisfied ranking list for topics which attract more public attention. Recent changes on some Google Search have
integrated image search and adopted some heterogenous content analysis, nevertheless, the connection between
image and the keywords are still arbitrarily determined by the users, thus it is still far from optimal.
Active solutions which attempt to summarize information only focused on single data modalities. For example,
Zanzotto et al. (2011) provided a comprehensive comparison about summarization methods for tweets. Zhao et al.
(2011) developed a context-sensitive topical PageRank (Brin and Page, 1998) method to extract topical key phrase
from Twitter as a way to summarize twitter content. As a new prospective, Feng and Lapata (2010) used LDA to
annotate images, but this does not firmly integrate the information across different data types. Huang et al. (2012)
presented a tweet ranking approach but only focused on single data modality (i.e., text).
Other conventional solutions towards analyzing the relationship or links between the instances have long been
proposed and applied, such as PageRank (Brin and Page, 1998)and VisualRank (Jing and Baluja, 2008). The
former is excessively used in heterogeneous networks (i.e., webpages and resources) but they are mainly based on
linkage itself. VisualRank, which is based on PageRank, is a content-based linkage method but is confined with
homogeneous networks.
Above all, our goal is to integrate cross-media inference and create the linkage among the information extracted
from those heterogenous data. Our novel Multi-media Information Networks (MiNets) representation initializes
our idea about a basic ontology of the ranking system.
The main contribution of this work is to fill in the domain gaps across different network genres and bridge them
in a principled method. In this work, we manage to discover the hidden links or structures between the heteroge-
neous networks in different genres. We combine joint inference to resolve information conflicts across multi-genre
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
74
networks. We can also effectively measure, share and transfer complementary information and knowledge across
multi-genre networks using structured correspondence.
The work is presented in sections as follows. We firstly introduce an overview of our system in Section 2.
Detailed approaches in information extraction and constructing meta-information network are then followed in
Section 3. Measurement across the multimedia information are proposed in Section 4 and 5. In Section 6 we
demonstrate the results and performance gain.
2 Approach Overview
Within the context of an event where users generate a vast amount of multi-media messages in forms of tweets and
images, we aim to provide a ranked subset of the most informative ones. Given a set of tweets T = {t
1
, ..., t
n
},
and a set of images P = {p
1
, ..., p
m
} as input, our approach provides ordered lists of the most informative tweets
or images (a.k.a objects) so that the informativeness of an object in position i is higher than or equal to that of an
object in position i + 1. We consider the degree of informativeness of a certain object as the extent to which it
provides valuable information to people who are involved in or tracking the event in question.
During emergent events, there are tight correlations between social media and web documents. Important infor-
mation shared in social media tends to be posted in web documents. Therefore we also integrate information in a
formal genre such as web documents to enhance the ranking quality of tweets and images. It consists of two main
sub-tasks:
? Multimedia Information Network (MiNet) Construction:
Construct MiNet from cross-media and cross-genre information (i.e. tweets, images, sentences of web doc-
uments). Given a set of tweets and images on a specific topic as input, the formal genre web documents and
images from the embedded URLs in those tweets are retrieved. Afterwards, a set of sentences and images are
extracted from the web documents. Then we exploit advanced text Information Extraction and image Concept
Extraction techniques to extract meta-information and construct the meta-information network. Together with
three sets of heterogeneous input data, MiNet is constructed.
? MiNet-Based Information Ranking: Rank the tweets and images. By extending and adapting Tri-
HITS (Huang et al., 2012), we propose EN-Tri-HITs, an random walk-based propagation algorithm which
iteratively propagate ranking scores for sentences, tweets, and images across MiNet to refine the tweet and
image rankings.
3 Meta-information Network
When integrating information from different data modalities, meta-information network plays a pivotal role for
representing interesting concepts and relations between them. We automatically construct the initial informa-
tion networks using our state-of-the-art information extraction and image concept extraction techniques. A meta-
information network is a heterogeneous network including a set of ?information graphs? which is formally defined
as: G = {G
i
: G
i
= (V
i
, E
i
)}, where V
i
is the collection of concept nodes, and E
i
is the collection of edges
linking one concept to the other. An example is depicted in Figure 1. The meta-information network contains
human knowledge pertaining to the target domain that could improve the performance of text process and image
analysis. In this paper, we first construct meta-information networks separately from texts and images, and then
fuse and enrich them through effective cross-media linking methods.
3.1 Information Extraction from Texts
Extracting salient types of facts for a meta-information network is challenging. In this paper we tackle this problem
from two angles to balance the trade-off between quality and granularity/annotation cost. On one hand, to reveal
deep semantics in meta-information network, we focus on achieving high-quality extraction for pre-defined fine-
grained types such as those in NIST Automatic Content Extraction (ACE)
1
. For example, a ?Person/Individual?
node may include attributes such as ?Birth-Place?, and a ?Organization/Employee? node may include attributes
such as ?City-of-Headquarter?. These two nodes may be connected via a ?Employment/End-Position? link.
We apply an Information Extraction (IE) system (Li et al., 2013) to extract entities, relations and events defined
in ACE2005. There are 7 types of entities, 18 types of relations and 33 types of events. This system is based
on a joint framework using structured perceptron with efficient beam-search and incorporating diverse lexical,
syntactic, semantic and ontological features. We convert the IE output into the graph structured representation of
meta-information network by mapping each entity as a node, and link entity nodes by semantic relations or events
they are involved. For example, the relations between entities are naturally mapped to links in the meta-information
network, such as the ?employment? relation between ?Bill Read? and ?Hurricane Center?. In addition, if an event
1
http://www.itl.nist.gov/iad/894.01/tests/ace/
75
flood
Multimedia information Networks
Meta-information Networks
ORG
flooding from Irene killed one person Puerto RicoBill Read Hurricane Center said
Employment
Speaker
Agent Subject
Clause Subject
PlaceAgent
Victim
storm
Wikipedia
Verifiedby wiki Verifiedby wiki Verifiedby wiki
Verified Entity Concepts
Tweets Sentences of web documents Images
Contents Structured information
PER GPEEvent TriggerPredicate Predicate Noun phrase Noun phrase
Example:SRL
IE concept
type
Conceptfrom image Concept from image
Figure 1: An example of meta-information network. Sentence: ?Bill Read, Hurricane Center director, said that
flooding from Irene killed at least one person in Puerto Rico?
argument is an entity, we also add an ?Event Argument? link between the event trigger and the entity, such as the
link between ?Irene? and ?killed?.
On the other hand, in order to enrich the meta-information network, we extract more coarse-grained salient
fact types based on Semantic Role Labeling (SRL) (Pradhan et al., 2008). For example, given the sentence ?In
North Carolina, 10 counties are being evacuated.?, the ?evacuation? event is not included in ACE. However, the
SRL system can successfully detect the predicate (?evacuated?) and its semantic roles (?10 counties? and ?North
Carolina?). These argument heads and predicates are added into the meta-information network as vertices, and
edges are added between each predicate-argument pairs.
We merge entity mentions across tweets and web documents based on a cross-document entity clustering system
described in (Chen and Ji, 2011). Moreover, for the same type of nodes from the SRL system, we also merge them
by string matching across documents.
3.2 Concept Extraction from Images
We also developed a concept modeling approach by extending the similar framework in previous work (Tsai et al.,
2012), Probabilistic Logical Tree (PLT), to extract semantic concepts from images. PLT integrates the logical and
statistical inferences in a unifying framework where the existing primitive concepts are connected into a potentially
unlimited vocabulary of high-level concepts by basic logical operations. In contrast, most existing image concept
extraction algorithms either only learn a flat correlative concept structure, or a simple hierarchical structure without
logical connections.
With an efficient statistical learning algorithm, the complex concepts in upper level of PLT are modeled upon
some logically connected primitive concepts. This statistical learning approach is very flexible, where each concept
in PLT can be modeled from distinctive feature spaces with the most suitable feature descriptors (e.g., visual
features such as color and shape for scenery concepts).
For our case study on ?Hurricane Irene? scenario, we apply this algorithm to extract the hierarchical concept
trees with roots ?flood? or ?storm? from all the images in web documents whose URLs are contained in tweets.
The main problem is the classifications of the concepts such that it may be properly be placed onto an ontology.
In order to enrich the hierarchy, we seek to classify these linkages through the use of the semi-structured and
structured data that exists on Wikipedia. We use pattern matching to extract is-a relations from the first paragraphs
of Wikipedia articles. For example, starting from our initial concept ?Hurricane Irene?, we can find its is-a relation
with ?Tropical Cyclone?, and then climb up one more level to ?Storm? where we can further mine lower concepts
such as ?Tornado? and ?Snow Storm?.
76
4 Multi-media Information Networks
A Multimedia Information Network (MINet) is a structured collection made up of a set of multimedia documents
(e.g., texts and images) and links between these documents. Each link corresponds to a specific relationship
between nodes, such as hyperlinks between web documents or similarity links between tweets. In this paper, we
construct our MINet based on two forms of contents from different domains: tweets, web documents (plain texts)
and images.
4.1 Within-media Linking
4.1.1 Text-Text Similarity
Taking web document for example, we construct the meta-information network G = {G
i
: G
i
= (V
i
, E
i
)} for all
web documents D, in which each web document d
i
? D corresponds to G
i
. Given the meta-information network
G, we compute the weight of each vertex v
j
? V
i
as weight
v
j
=
nf(v
j
,d)
AV E(D)
,
where nf(v
j
, d) is the mention number of node v
j
appearing in a document d and AV E(D) is the average
number of mentions in a document d, which is defined as AV E(D) =
?
d?D
concept mentions in d
|D|
.
Similarly, we define the weight of each link e
k
? E
i
as weight
e
k
=
nf(e
k
,d)
AV E(D)
, where nf(e
k
, d) is the mention
number of the node e
k
in a document d and AV E(D) is the average number of mentions in a document d, which
is defined as AV E(D) =
?
d?D
relation mentions in d
|D|
.
If two edges share the same type and link nodes corresponding to the same tokens, we consider them as two
mentions involved in a relation. Based on the weight of each concept mention and relation mention, we count their
frequencies and transform them into vectors. Finally, we compute cosine similarity between every two vectors.
4.1.2 Image-Image Similarity
We extract Histogram of Oriented Gradients (HOG) features (Dalal and Triggs, 2005) from patches in images and
apply Hierarchical Gaussianization (Zhou et al., 2009) to those HOG feature vectors. We learn a Gaussian mixture
model (GMM) to obtain the statistics of the patches of an image by adapting the distribution of the extracted HOG
features from these image patches and each image is represented by a super-vector. Based on the obtained image
representation, the image-image similarity is simply a cosine similarity between two HG super-vectors.
4.2 Cross-media Linking
In order to obtain cross-media similarity, we propose a method based on transfer learning technique (Qi et al.,
2012). Given a set of m points [p
1
, p
2
, . . . , p
m
] in the source (image) domain P , a set of n points [t
1
, t
2
, . . . , t
n
]
in the target (text) domain T , and a set of N corresponding pairs C = {(p
a
i
, t
b
i
}
N
i=1
in these two domains, we aim
to find a cross-media similarity function:
G(p, t) = `((Up)
T
(V t)) = `(p
T
St)), (1)
where U and V are the linear embedding of P and T , respectively. S = U
T
V is the cross-domain similarity
matrix and `(?) =
1
1+e
??
is the logistic sigmoid function.
The key to S in Equation 1 is to solve the optimization problem blow:
min
S
?
L
s
(S) + ?
?
L
d
(S) + ?
?
?(S), (2)
where
?
L
s
(S) =
?
(x,y)?C
log(1 + exp(?p
T
St)), and
?
?(S) = ?S?
?
is the nuclear norm that is the surrogate of
the matrix rank. Also, we have
?
L
d
(S) =
1
2
?
K
P
(p, p
?
)d
T
(p, p
?
) +
1
2
?
K
T
(t, t
?
)d
P
(t, t
?
),
where K(?, ?) is the similarity matrix among the points in a single domain and d(?, ?) defines the distance between
two points due to the transfer.
Taking one step further, we have
?
L
d
(S) = tr(L
T
Q
T
(S)
T
K
P
Q
P
(S)) + tr(L
X
Q
P
(S)
T
K
T
Q
T
(S)),
where L
P
and L
T
are the Laplacian matrices for K
P
and K
T
, respectively.
To solve the optimization problem (2) with nuclear norm regularization we follow the proximal gradient method
(Toh and Yun, 2010) with the following gradients:
5
?
L
s
(S) = P (J
C
?H)P
T
,5
?
L
d
(S) = P ((K
P
Q
P
L
T
+ L
P
Q
T
K
T
) ?H)P
T
(3)
77
JC
is an m ? n matrix with its (i, j)-th entry 1 if (p
i
, t
j
) ? C, otherwise 0. H is also an m ? n matrix whose
(i, j)-th entry where H
ij
= `
?
(p
T
i
St
j
).
Hence we have
5
?
L(S) = P (G ?H)T
T
, (4)
where G = J
C
+ ?K
P
Q
P
L
T
+ ?L
P
Q
T
K
T
. With the gradient in (4), one can solve the problem (2) using the
proximal gradient method.
5 MiNet-Based Information Ranking: EN-Tri-HITs
5.1 Initializing Ranking Scores
1 Input: A set of tweets (T ), and images (P ) and web documents (W ) on a given topic.
2 Output: Ranking scores (S
t
) for T and (S
p
) for P .
1: Use TextRank to compute initial ranking scores S
0
p
for P ,
S
0
t
for T and S
0
w
for W ;
2: Construct multimedia information networks across P , T
and W ;
3: k ? 0, diff ? 10e6;
4: while k < MaxIteration and diff > MinThreshold do
5: Use Eq. (5) (6) and (7) to compute S
k+1
p
, S
k+1
t
and S
k+1
w
;
6: Normalize S
k+1
p
, S
k+1
t
and S
k+1
w
;
7: diff ? max(
?
(|S
k+1
t
? S
k
t
|),
?
(|S
k+1
p
? S
k
p
|));
8: k ? k + 1
9: end while
Algorithm 1: EN-Tri-HITS: Random walk on multimedia information networks
Graph-based ranking algorithms have been widely used to analyze relations between vertices in graphs. In
this paper, we adapted PageRank (Brin and Page, 1998; Mihalcea and Tarau, 2004; Jing and Baluja, 2008) to
compute initial ranking scores in tweet-only and image-only networks where edges between tweets or images are
determined by their cosine similarity.
The ranking score is computed as follows:
S(V
i
) = (1? d) + d ?
?
V
j
?In(V
i
)
w
ji
?
V
k
?Out(V
j
)
w
jk
S(V
j
),
where V
i
is a vertex with S(V
i
) as its ranking score; In(V
i
) and Out(V
i
) are the incoming edge set and outgoing
edge set of V
i
, respectively; w
ij
is the weight for the edge between two vertices V
i
and V
j
. An edge links two
vertices that represent text units when their cosine similarity of shared content exceeds or equals to a predefined
threshold ?
t
.
5.2 Random Walk on Multimedia Information Networks
We introduce a novel algorithm to incorporate both initial ranking scores and global evidence from multimedia
information networks. It propagates ranking scores across MiNets iteratively. Our algorithm is a natural extension
of Tri-HITS (Huang et al., 2012) based on the mutual reinforcement to boost linked objects.
By extending Tri-HITS, we develop enhanced Tri-HITS (EN-Tri-HITs) to handle multimedia information net-
works with three types of objects: Tweets (T ), sentences of web documents (W ) and images (P ). EN-Tri-HITs
is able to handle more complicated network structure with more links. Given the similarity matrices M
tw
(be-
tween tweets and sentences of web documents), M
wp
(between sentences of web documents and images) andM
tp
(between tweets and images), and initial ranking scores of S
0
(p), S
0
(t) and S
0
(w), we aim to refine the initial
ranking scores and obtain the final ranking scores S(w), S(t) and S(p). Starting from images S(p), the update
process considers both the initial score S
0
(p) and the propagation from connected tweets S(t) and web documents
S(w), which can be expressed as:
?
S
w
(p
j
) =
?
i?W
m
wp
ij
S(w
i
),
?
S
t
(p
j
) =
?
k?T
m
tp
kj
S(t
k
),
S(p
j
) = (1? ?
wp
? ?
tp
)S
0
(p
j
) + ?
wp
?
S
w
(p
j
)
?
j
?
S
w
(p
j
)
+ ?
tp
?
S
t
(p
j
)
?
j
?
S
t
(p
j
)
, (5)
78
Set ID Tweets
Web Doc
Images
(Sentences)
1 1171 41(1272) 183
2 1116 47(1634) 265
3 1184 69(1639) 346
All 3471 157(4545) 794
Table 1: Data Statistics: Numbers of each item in
the dataset.
word word word word
+IE +SRL +IE+SRL
I+W 0.545 0.539 0.521 0.583
I+T 0.422 0.436 0.407 0.489
I+W+T 0.526 0.513 0.492 0.541
Table 2: NDCG@5 of Images. The image ranking
baseline performance is 0.421. I stands for Image;
W Web Documents; T Tweets
where ?
wp
, ?
tp
? [0, 1] (?
wp
+ ?
tp
? 1) are the parameters to balance between initial and propagated ranking
scores. Similar to Tri-HITS, EN-Tri-HITS normalizes the propagated ranking scores
?
S
w
(p
i
) and
?
S
t
(p
i
).
Similarly, we define the propagations from images and web documents to tweets as follows:
?
S
p
(t
k
) =
?
i?P
m
pt
ik
S(p
i
),
?
S
w
(t
k
) =
?
j?W
m
wt
jk
S(w
j
),
S(t
k
) = (1? ?
wt
? ?
pt
)S
0
(t
k
) + ?
wt
?
S
p
(t
k
)
?
k
?
S
p
(t
k
)
+ ?
pt
?
S
w
(t
k
)
?
k
?
S
w
(t
k
)
, (6)
where M
pt
is the transpose of M
tp
, ?
pt
and ?
wt
are parameters to balance between initial and propagated ranking
scores.
Each sentence of web documents S(w
j
) may be influenced by the propagation from both tweets and images:
?
S
t
(w
i
) =
?
k?T
m
tw
ki
S(t
k
),
?
S
p
(w
i
) =
?
i?P
m
pw
ji
S(p
j
),
S(w
i
) = (1? ?
tw
? ?
pw
)S
0
(w
i
) + ?
tw
?
S
t
(w
i
)
?
i
?
S
t
(w
i
)
+ ?
pw
?
S
p
(w
i
)
?
i
?
S
p
(w
i
)
, (7)
where M
pw
is the transpose of M
wp
, ?
tw
and ?
pw
are parameters to balance between initial and propagated
ranking scores.
Algorithm 1 summarizes En-Tri-HITS.
6 Experiments
6.1 Data and Scoring Metric
Currently there are no information ranking related benchmark data sets publicly available, therefore we build our
own data set and network ontology.
We crawled 3471 tweets during a three-hour period and extracted key phrases from these tweets, then we use
the key phrases as image search queries. The image search queries are submitted to Bing Image Search API and
we take the top 10 images for each query. We extract a 512-d GIST feature from each image for meta information
training. For image similarity metrics, we resize images to a maximum of 240 ? 240 and segmented into patches
with three different sizes (16, 25 and 31) by a 6-pixel step size. A 128-d Histogram of Oriented Gradients (HOG)
feature is extracted from each patch and followed by a PCA dimension reduction to 80-d. The size of dimension
of the final feature vector for each image is 42,496.
We create the ground truth based on human assessment of informativeness on a 5-star likert scale, with grade 5
as the most informative and 1 as the least informative. Table 1 presents an overview on our data sets. We conduct
3-fold cross-validation for our experiments.
To evaluate tweet ranking, we use nDCG as our evaluation metric (J?arvelin and Kek?al?ainen, 2002), which
considers both the informativeness and the position of a tweet:
nDCG(?, k) =
1
|?|
|?|
?
i=1
DCG
ik
IDCG
ik
, DCG
ik
=
k
?
j=1
2
rel
ij
? 1
log(1 + j)
,
where ? is the set of documents in the test set, with each document corresponding to an hour of tweets in our case,
rel
ij
is the human-annotated label for the tweet j in the document i, and IDCG
ik
is the DCG score of the ideal
ranking. The average nDCG score for the top k tweets is: Avg@k =
?
k
i=1
nDCG(?, i)/k. To favor diversity of
top ranked tweets, redundant tweets are penalized to lower down the final score.
79
6.2 Impact of Cross-media Inference
Table 2 and Figure 2 present the image ranking results. The results indicate that methods integrating heterogeneous
networks outperform the baseline of image ranking (0.421). When web documents are aligned with images (row
1), the ranking quality improves significantly, proving that web documents can help detect informative images by
adding support from text media of formal genre. However, the text media of informal genre, such as tweets, almost
cannot help improve the ranking performance.
1 2 3 4 5 6 7 8 9 1 00 . 20 . 30 . 40 . 50 . 60 . 7
0 . 80 . 91 . 0NDCG@N N
 I m a g e + T w e e t + W e b  D o c + W e b  D o c & T w e e t
Figure 2: NDCG@n score of Images with Various n
word word word word
+IE +SRL +IE+SRL
T 0.675 0.691 0.697 0.700
T+W 0.766 0.771 0.757 0.809
T+I 0.675 0.691 0.667 0.700
T+W+I 0.722 0.771 0.757 0.809
Table 3: NDCG@5 of Tweets
6.3 Impact of Cross-genre Inference
Methods that integrate heterogeneous networks after filtering, outperform the baseline TextRank, as shown in
Table 3. When tweets are aligned with web documents, the ranking quality improves significantly, proving that
web documents can help infer informative tweets by adding support from a formal genre. The fact that tweets with
low initial ranking scores are aligned with web documents helps promote their ranking positions. For example,
the ranking of the tweet ?Hurricane Irene: City by City Forecasts http://t.co/x1t122A? is improved compared to
TextRank, benefitting from the fact that 10 retrieved web documents are about this topic.
6.4 Remaining Error Analysis
Enhanced Tri-HITS shows encouraging improvements in ranking quality with respect to a state-of-the-art model
such as TextRank. However, there are still some issues to be addressed for further improvements.
(i) Long tweets preferred. We tracked tweets containing the keywords ?Hurricane? and ?Irene?. Using such a
query might also return tweets that are not related to the event being followed. This may occur either because
the terms are ambiguous, or because of spam being injected into trending conversations to make it visible. For
example, the tweet ?Hurricane Kitty: http://t.co/cdIexE3? is an advertisement, which is not topically related to
Irene.
(ii) Deep semantic analysis of the content, especially for images. We rely on distinct terms to refer to the
same concept. More extensive semantic analyses of text can help identify those terms, possibly enhancing the
propagation process. For example, we can explore existing text dictionaries such as WordNet (Miller, 1995) to
mine synonym/hypernym/hyponym relations, and Brown clusters (Brown et al., 1992) to mine other types of
relations in order to enrich the concepts extracted from images.
7 Conclusion and Future Work
In this paper, we propose a comprehensive information ranking approach which facilitates measurement on cross-
media/cross-genre informativeness based on a novel multi-media information network representation MiNet. We
establish links via information extraction method from text and images and verification with Wikipedia. In ad-
dition, we propose similarity measurement on intra-media and cross-media using transfer learning techniques.
We also introduce a novel En-Tri-Hits algorithm to evaluate the ranking scores across MiNet. Experiments have
demonstrated that our cross-media/cross-genre ranking method is able to significantly boost the performance of
multi-media tweet ranking. In the future, we aim to focus on enhancing the quality of concept extraction by
exploiting cross-media inference that goes beyond simple fusion.
Acknowledgement
This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-
09-2-0053 (NS-CTA), U.S. NSF CAREER Award under Grant IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the ?Deep Exploration and Filtering of Text? (DEFT) Program, IBM Faculty award and RPI faculty
start-up grant. The views and conclusions contained in this document are those of the authors and should not
be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The
80
U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any
copyright notation here on.
References
Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. Computer
Networks, 30(1-7):107?117.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18:467?479.
Zheng Chen and Heng Ji. 2011. Collaborative ranking: A case study on entity linking. In Proc. EMNLP2011.
Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human detection. In In CVPR, pages
886?893.
Yansong Feng and Mirella Lapata. 2010. Topic models for image annotation and text illustration. In Human
Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 831?839, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hongzhao Huang, Arkaitz Zubiaga, Heng Ji, Hongbo Deng, Dong Wang, Hieu Le, Tarek Abdelzaher, Jiawei Han,
Alice Leung, John Hancock, and Clare Voss. 2012. Tweet ranking based on heterogeneous networks. In Proc.
COLING 2012, pages 1239?1256, Mumbai, India. The COLING 2012 Organizing Committee.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2002. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf.
Syst., 20(4):422?446, October.
Yushi Jing and Shumeet Baluja. 2008. Visualrank: Applying pagerank to large-scale image search. IEEE Trans.
Pattern Anal. Mach. Intell., 30(11):1877?1890.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In
Proc. ACL2013, pages 73?82.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing order into texts. In Proceedings of EMNLP, volume 4.
Barcelona: ACL.
George A. Miller. 1995. Wordnet: A lexical database for english. COMMUNICATIONS OF THE ACM, 38:39?41.
Sameer Pradhan, Wayne Ward, and James H. Martin. 2008. Towards robust semantic role labeling. In Computa-
tional Linguistics Special Issue on Semantic Role Labeling, volume 34, pages 289?310.
Guo-Jun Qi, Charu C. Aggarwal, and Thomas S. Huang. 2012. Transfer learning of distance metrics by cross-
domain metric sampling across heterogeneous spaces. In SDM, pages 528?539.
Kim-Chuan Toh and Sangwoon Yun. 2010. An accelerated proximal gradient algorithm for nuclear norm regular-
ized linear least squares problems. Pacific Journal of Optimization.
Shen-Fu Tsai, Henry Hao Tang, Feng Tang, and Thomas S. Huang. 2012. Ontological inference framework
with joint ontology construction and learning for image understanding. In IEEE International Conference on
Multimedia and Expo (ICME) 2012.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and Kostas Tsioutsiouliklis. 2011. Linguistic redundancy in
twitter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11,
pages 659?669, Stroudsburg, PA, USA. Association for Computational Linguistics.
Wayne X. Zhao, Jing Jiang, Jing He, Yang Song, Palakorn Achananuparp, Ee P. Lim, and Xiaoming Li. 2011.
Topical keyphrase extraction from Twitter. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies - Volume 1, HLT ?11, pages 379?388, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Xi Zhou, Na Cui, Zhen Li, Feng Liang, and Thomas S. Huang. 2009. Hierarchical gaussianization for image
classification. In ICCV, pages 1971?1977.
81
