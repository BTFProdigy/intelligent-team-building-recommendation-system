Word sense disambiguation criteria: a systematic study 
Laurent AUDIBERT 
DELIC, Universit? de Provence 
29 Av. Robert SCHUMAN ? 13621 
Aix-en-Provence Cedex 1, FRANCE 
laurent.audibert@up.univ-aix.fr 
 
Abstract 
This article describes the results of a systematic in-
depth study of the criteria used for word sense 
disambiguation. Our study is based on 60 target 
words: 20 nouns, 20 adjectives and 20 verbs. Our 
results are not always in line with some practices in 
the field. For example, we show that omitting non-
content words decreases performance and that 
bigrams yield better results than unigrams. 
1 Introduction 
The task of word sense disambiguation (WSD) is 
to identify the correct sense of a word in context. 
WSD is usually performed by matching 
information from the context in which the word 
occurs with disambiguation knowledge source. Our 
approach uses supervised machine-learning 
techniques to automatically acquire such 
disambiguation knowledge from sense-tagged 
corpora. At present, this type of approach is widely 
used and seems to yield the best results (Kilgarriff, 
Rosenzweig, 2000; Ng, 1997b). 
Information conveyed by context words 
(morphological form) is enriched with further 
annotations: part-of-speech tag, lemma, etc. Each 
individual piece of information is called a feature. 
A good feature should capture an important source 
of knowledge that is critical in determining the 
sense of the word to be disambiguated. The choice 
of the appropriate set of features is an important 
issue for WSD (Bruce, Wiebe, Perdersen, 1996; 
Ng, Zelle, 1997; Pedersen, 2001). Thus, this paper 
describes the results of a systematic and in-depth 
study of homogenous criteria (i.e. set of features) 
that can be used for WSD. 
2 Methodology 
2.1 Corpus 
The corpus we worked on is composed of 
different types of texts and comprises 6 468 522 
words. It was put together within the framework of 
the SyntSem project that aims at producing a 
French corpus which is morphologically and 
syntactically tagged, lemmatised and that 
comprises a light syntactical tagging as well as a 
lexical tagging of 60 target words selected for their 
strongly polysemous nature (V?ronis, 1998)1. 
These 60 target words are evenly divided into 20 
nouns, 20 adjectives and 20 verbs, having a total of 
53 796 occurrences in the corpus. 
The inadequacy of standard dictionaries 
(V?ronis, 2001) and computational lexicons 
(Palmer, 1998) for natural language processing is 
presently one of the major difficulties encountered 
in word sense disambiguation. For instance, by 
using these dictionaries, the inter-annotator 
agreement may sometimes reach only 57% (Ng, 
Lee, 1996) or may simply be equivalent to random 
sense allocation (V?ronis, 1998). To overcome this 
weakness, a dictionary more specific to natural 
language processing is being developed in our 
team (Reymond, 2002). It has been used to tag the 
occurrences of the 60 target words of the SyntSem 
corpus.  
Table 8 in the appendix gives quantitative 
information for each target word. The number of 
senses per word may be very large for it includes 
idioms and phrasal verbs such as: ? mettre sur 
pied ?, ? mettre ? pied ?, ? pied de nez ?, etc. 
A general agreement seems to emerge according 
to which morpho-syntatic disambiguation and 
sense disambiguation can be disentangled 
(Kilgarriff, 1997; Ng, Zelle, 1997). We have 
entrusted the part-of-speech tagging of our corpus 
to the Cordial software (developed by Synapse 
D?veloppement company) as it offers 
lemmatisation and part-of-speech tagging of a 
satisfactory accuracy (Valli, V?ronis, 1999). 
mform lemma ems cgems sense
mettre mettre VINF VINF 1.12.7
fin fin NCFS NCOM  
? ? PREP PREP  
la le DETDFS DET  
pratique pratique NCFS NCOM  
des de DETDPIG DET  
d?tentions d?tention NCFP NCOM 1 
Table 1: SyntSem tagged corpus extract. 
                                                     
1 These words are those used in the French part of the 
Senseval-1 evaluation exercice (Segond, 2000) but the 
corpus and dictionary are different in the present study. 
Table 1 displays an extract of the SyntSem 
corpus. It shows all the tags of each word. We use 
the information provided by these tags in our 
lexical disambiguation criteria. 
2.2 Criteria 
The aim of our study is to evaluate a large 
variety of homogenous criteria (i.e. set of features). 
The name of each criterion specifies its nature and 
takes the following form [par1|par2|par3|par4]. 
Parameter par1 indicates whether the criterion 
takes into account unigrams (par1=1gr), bigrams 
(par1=2gr) or trigrams (par1=3gr), knowing that 
an n-gram represents the juxtaposition of n words. 
Parameter par2 indicates which word tag is 
considered: morphological form (par2=mform), 
lemma (par2=lemma), part-of-speech (par2=ems) 
or coarse-grained part-of-speech (par2=cgems). 
Parameter par3 indicates if we take into account 
word positions (par3=position), if we only 
distinguish left from right context (par3=leftright), 
or if we simply consider unordered set of 
surrounding words (par3=unordered). Lastly, 
parameter par4 shows whether the criterion takes 
into account all the words (par4=all) or content 
words only (par3=content). We call these criteria 
?homogeneous criteria? as the four parameters 
together determine the nature of all pieces of 
contextual evidence selected by the criterion. 
For contexts within a range of ?1 to ? 8 words, 
the combination of all parameters generates 576 
(3?4?3?2?8) distinct criteria. We have 
systematically evaluated each one of these criteria 
as well as other criteria in order to answer specific 
questions and to validate or invalidate certain 
hypothesis. 
Within the framework of this study, we have 
developed an application used to model these 
criteria and to further apply them to the corpus in 
order to generate feature vectors used by our 
classifiers (Audibert, 2001). 
2.3 Classifiers 
We have selected two complementary classifiers. 
We have chosen the Na?ve-Bayes classifier (NB) 
for its simplicity and widespread use, as well as for 
its well-known state-of-the-art accuracy on 
supervised WSD (Domingos, Pazzani, 1997; 
Mooney, 1996; Ng, 1997a). The NB classifier 
assumes the features are independent given the 
sense. During classification, it chooses the sense 
with the highest posterior probability. We have 
also selected a decision list classifier (DL) which is 
similar to the classifier used by (Yarowsky, 1994) 
for words having two senses, and extended for 
more senses by (Golding, 1995). DL classifier is 
further developed in (Audibert, 2003). In DL, 
features are sorted in order of decreasing strength, 
where strength reflects feature reliability for 
decision-making. The DL classifier distinguishes 
itself clearly from the NB classifier as it does not 
combine the features, but bases its classifications 
solely on the single most reliable feature identified 
in the target context selected by the criteria. We 
will make a use of this decision-making 
transparency several times in this article. Some 
other advantages of DL classifier are its significant 
simplicity and its ease of implementation. 
Both of the classifiers we used require 
probability estimates. Given the data-sparseness, 
we have to deal with zero or low frequency counts. 
For this reason, we have decided to use m-
estimation (Cussens, 1993) rather than classical 
estimations of probabilities or Laplace (?add one?) 
smoothing. 
When a classifier is not able to disambiguate a 
target word, which is very rare, it selects the most 
frequent sense from the training data. Thus, all 
occurrences are tagged. As in this case precision 
equals the recall, the present article speaks in terms 
of precision only. 
To evaluate a criterion in the corpus, we use a k-
fold cross-validation method (in accordance with 
the common use, in our experiment, k=10). Despite 
the fact that this method takes much computing 
time, it enables the evaluation of the criterion in 
the whole corpus. 
Throughout the tests, the two classifiers have 
generally obtained comparable accuracy, although 
the NB classifier has almost systematically 
outperformed the DL classifier. 
3 Results 
3.1 Best criteria precision 
Table 2 displays for each of the target words 
studied the optimal context size and the 
disambiguation precision obtained by the best 
unigram, bigram and trigram-based criteria. 
This table points out that best criteria take into 
account all words in the context. Section 3.2 will 
concentrate on feature reliability according to their 
part-of-speech. Then, section 3.2 will deal with the 
impact of different feature selections based on 
 Nouns Adjectives Verbs 
Criterion P% S P% S P% S 
[1gr|lemma| 
ordered|all] 81.9 ?2 76.8 ?1 71.8 ?3
[2gr|lemma| 
leftright|all] 83.6 ?4 77.9 ?3 74.0 ?4
[3gr|lemma| 
leftright|all] 82.3 ?5 72.7 ?3 71.2 ?5
Table 2: Optimal context size (S) and criteria 
precision (P%) using NB classifier. 
features part-of-speech.  Nouns Adjectives Verbs 
 P% U% P% U% P% U%
NCOM 93.0 12.7 93.7 25.3 87.7 26.0
DET 73.8 30.2 69.8 21.3 48.1 12.7
PREP 78.3 24.2 61.4 15.2 62.9 17.4
ADJ 94.9 13.7 80.3 2.2 65.5 3.2
ADV 57.0 1.1 79.2 9.6 60.3 5.7
PROPE 63.6 0.6 67.0 2.6 65.9 11.4
PCTFORTE 72.1 3.00 69.0 4.2 80.9 2.9
VCON 67.9 1.5 53.9 2.4 54.4 3.0
SUB 78.6 0.6 58.1 1.8 79.9 2.7
VINF 90.2 0.9 80.7 0.7 87.2 2.9
NPRO 86.8 0.3 92.0 0.6 81.8 1.2
VPAR 89.7 0.3 50.0 0.2 81.0 0.9
PRODE 100 0.0 35.0 0.2 68.6 0.8
Table 3: Precision (P%) and usage proportion 
(U%) by coarse-grained part-of-speech of most 
reliable contextual evidences using DL classifier. 
 
 
 
Figure 1: space distribution by part-of-speech of 
most reliable pieces of contextual evidence used for 
disambiguation with DL classifier. 
According to the Table 2, the optimal context 
size comprises ?1 to ?5 words. Further 
developments of the context optimality will be 
made in section 3.4.  
Surprisingly, Table 2 outlines the fact that the 
criterion that obtains the best precision is based on 
bigrams and not on unigrams. This subject is dealt 
with in section 3.5. 
3.2 The most reliable parts-of-speech 
In this section, we aim at learning the part-of-
speech and the space distribution of all pieces of 
contextual evidence used for disambiguation. 
To this end, we use the DL classifier because it 
bases its classifications solely on the most reliable 
piece of evidence identified by the criteria. Thus, 
DL classifier enables us to learn which is the part-
of-speech and the space distribution of this 
indicator by using the criterion 
[1gr|mform|ordered|all]. Table 3 and graphics 
presented in Figure 1 synthesize this study results. 
Table 3 enables to bring out the following results 
(we quote between brackets and in order the main 
results obtained for the nouns, the adjectives and 
the verbs): 
? common nouns (NCOM) obtain one of the best 
precisions (93.0%; 93.7%; 87.8%) and represent 
one of the most used indicators (12.7%; 25.3%; 
26%) for the three term categories; 
? adjectives (ADJ) represent good indicators for 
nouns (p=94.9%) and adjectives (p=80.3%), but 
they are especially useful for nouns since they 
are used in 13.7% of the cases against 2.2% only 
for the adjectives; 
? adverbs (ADV) are mainly useful for adjectives; 
their precision reaches 79.2% and they are used 
in 9.6% of the cases; 
? verbs in the infinitive (VINF) are very reliable 
indicators for the three parts-of-speech (90.2%; 
80.6%; 87.2%), but they are rarely used as they 
are not very often encountered in the context 
(0.9%; 0.7%; 2.9%); 
? conjugated verbs (VCON) obtain poor precision 
(67.9%; 53.9%; 54.4%). 
Figure 1 graphics show the space distribution of 
the main parts-of-speech of the indicators used to 
disambiguate each one of the three term categories. 
The dissymmetric shape of verbs, and more 
precisely, the strong prevalence of indicators 
located in position +1, +2, +3, makes us believe 
that disambiguating verbs is done more 
accordingly to their object than to their subject 
since as a rule the main form encountered is 
subject?verb?object. 
Table 4 summarizes these graphs. Our results 
and those of (Yarowsky, 1993) agree in many 
respects, although his study applies only to pseudo-
words having only two ?senses?: 
?  ?Adjectives derive almost all of their 
disambiguating information from the nouns they 
modify?; 
? ?Nouns are best disambiguated by directly 
adjacent adjectives or nouns?; 
? ?Verbs derive more disambiguating information 
from their objects than from their subjects?. 
3.3 The importance of stop-words 
 Nouns Adjectives Verbs 
Unigrams 0.3% 2.5% 6.9%
Bigrams 2.7% 3.4% 13.5%
Trigrams 12.4% 15.9% 20.2%
Table 5: precision decrease when omitting non-
content words using DL classifier. 
Many studies do not consider all the words of 
the context (El-B?ze, Loupy, Marteau, 1998; 
Mooney, 1996; Ng, Lee, 2002). The assumption 
according to which content words represent the 
most reliable indicators underlies the choice to use 
only content words based criteria. This seems to be 
obvious, but it is not confirmed in Table 2. Table 5 
shows the average decrease of the precision of the 
content words based criteria 
([par1|par2|par3|content]) compared to the same 
criteria based on all words ([par1|par2|par3|all]). 
This table shows that the decrease is low when the 
criteria are based on unigrams and are used to 
disambiguate nouns, but it can become very high 
in the other cases, and in particular for verbs 
disambiguation. 
 Table 3 informs us about the disambiguation 
precision according to the coarse-grained part-of-
speech tag. This table shows that using content 
words only is probably not the most appropriate 
feature selection (for example inflected verbs are 
not relevant). We have therefore chosen to try out a 
more subtle selection (we refer to it by 
par4=selected) by restraining to the most reliable 
parts-of-speech according to Table 3: 
? For nouns, we use indicators having the 
following coarse-grained part-of-speech tagging: 
NCOM, PREP, ADJ, SUB, VINF, NPRO, VPAR 
or PRODE; 
 Most reliable contextual evidences 
 NCOM ADJ ADV DET PREP 
Nouns -2, +2 +1  -1 -1, +1 
Adject. -1, +1  -1 -1, -2 +1 
Verbs +2, +3   +1, +2 +1 
Table 4: space distribution of most reliable 
pieces of contextual evidence used for 
disambiguation with DL classifier. 
 Nouns Adj. Verbs
[1gr|mform|ordered|all] 81.5 75.7 71.0 
[1gr|mform|ordered|content] 78.9 71.6 59.5 
[1gr|mform|ordered|selected] 79.2 71.5 66.3 
Table 6: precision with and without feature 
selections using NB classifier. 
? For adjectives, we use indicators having the 
following coarse-grained part-of-speech tagging: 
NCOM, DET, ADJ, ADV, VINF or NPRO; 
? For verbs, we use indicators having the following 
coarse-grained part-of-speech tagging: NCOM, 
ADJ, PROPE, PCTFORTE, SUB, VINF, NPRO, 
VPAR or PRODE. 
Table 6 gives a comparison of the precision 
reached by the following 3 criteria: 
? [1gr|mform|ordered|all], 
? [1gr|mform|ordered|content], 
? [1gr|mform|ordered|selected]. 
We observe that this subtler selection lowers the 
disambiguation precision too. We assume then that 
all words, whatever their part-of-speech, contribute 
to the disambiguation. 
3.4 Optimal context 
3.4.1 Size and symmetry 
We tested up to ?10 000 word contexts. 
However, the best precision is always obtained for 
short contexts ranging from ?1 to ?5 words. These 
results are similar to those obtained by many other 
researches (El-B?ze, et al, 1998; Yarowsky, 1993; 
2000). 
Optimal context size is criteria, target part-of-
speech and n-gram size dependent. In particular, it 
increases with the n-gram size. 
Table 7 shows, for all the criteria we examined, 
the average size of the optimal context by the n-
gram size and by the part-of-speech. 
The main indicators used to disambiguate nouns 
and adjectives surround roughly symmetrically the 
word we want to disambiguate. On the contrary, 
indicators for verbs tend to be mainly situated after 
the verb. Therefore, a non-symmetrical context 
shifted forward by a word proves to be more 
appropriate. Our experiments show that the use of 
this shifted context improves the precision of the 
verbs disambiguation by 0.75% in average. 
 Nouns Adjectives Verbs 
Unigrams 1.5 1.1 1.8 
Bigrams 2.4 2 2.8 
Trigrams 3.1 3.4 3.8 
Table 7: optimal context size using both 
classifiers. 
3.4.2 Do n-grams have to contain the target 
word? 
The lemma being unique for a given word, if 
only lemmas are considered, an n-gram which is 
adjacent to the target word contains precisely the 
same information as the same n-gram to which the 
target word is added in order to compose a (n+1)-
gram. The n-gram that is situated next to the word 
to disambiguate can thus be assimilated to the 
(n+1)-gram which contains it. Consequently, the 
question becomes: do n-grams have to contain the 
word to disambiguate or at least to be adjacent to 
it? Several studies set themselves this constraint 
probably because n-grams are used to capture fixed 
constructions containing the word to disambiguate. 
Table 2 shows that the optimal context size for 
best bigram or trigram-based criteria does not fit 
this constraint. The relevant n-grams do not 
necessarily contain the target word and are not 
necessarily adjacent to it. For example, for nouns 
and verbs, the ?4 words context is the optimal 
context size of the bigram-based criteria which 
obtains the best disambiguation precision. This 
criterion produces some bigrams separated from 
the target word by one or two words. However, 
this single observation cannot enable us to abandon 
the constraint in terms of containing or being 
adjacent to the target word. Indeed, the bigram 
increasing distance may help locating an 
information which could be captured by the joint 
use of one or several larger n-grams. We have thus 
evaluated a combination of criteria in which all n-
grams contain the target word in a context up to ?4 
words: 
? [2gr|lemma|leftright|all] with context size of ?1 
words; 
? [3gr|lemma|leftright|all] with context size of ?2 
words; 
? [4gr|lemma|leftright|all] with context size of ?3 
words; 
? [5gr|lemma|leftright|all] with context size of ?4 
words. 
This combination leads to a disambiguating 
precision of 74.3%, which is lower than the one 
obtained using the criteria 
[2gr|lemma|leftright|all] alone with a ?4 words 
context. This experiment confirms that 
constraining the context to contain the word to 
disambiguate, or at least to be adjacent to it, 
decreases disambiguation accuracy. Consequently, 
nothing justifies this constraint on criteria. 
3.5 Surprising bigrams accuracy 
Contrary to all expectations, Table 2 shows that 
the best unigram-based criterion 
([1gr|lemma|ordered|all]) is definitely less 
accurate than the best bigram-based criterion 
([2gr|lemma|leftright|all]). However, in practice, 
bigrams and trigrams are seldom used alone. When 
used, they are taken in conjunction with unigrams 
which are supposed to convey the most reliable 
piece of information. 
Why does the criterion [2gr|lemma|leftright|all] 
work so well? First, since this criterion is a 
juxtaposition of lemmas, among the features 
generated by this criterion, the left and the right 
unigrams are to be found, even if these unigrams 
are disguised as bigrams (cf. section 3.4.2). As 
these pieces of contextual evidence are certainly 
the most important ones (cf. section 3.4), it makes 
sense that this bigram-based criterion obtains good 
results. 
Second, in a context of a higher size, the 
juxtaposition of two words seems more relevant 
than one isolated word. For example, to 
disambiguate the word utile, the bigram pour_le is 
relevant, whereas the single unigrams pour and le 
are not of much help. 
Lastly, sometimes, the presence of a unigram 
noncontiguous to the target word can be sufficient 
to solve the ambiguity. But using bigram-based 
criteria does not necessarily lose the piece of 
information conveyed by unigram-based criteria. 
For example, a determiner, a preposition or an 
apostrophe often precedes a common noun. The 
lemmatisation variability of this determiner, this 
preposition or this apostrophe is low for a given 
common noun located at a given distance from a 
given target word. Therefore, the piece of 
information brought out by the juxtaposition of the 
noun and the preceding word is often very similar 
to the piece of information provided by the noun 
alone. 
4 Conclusion 
We have described here the results of a 
systematic and in-depth research on WSD criteria. 
This may be the first research of this extent carried 
out within a unified framework. This study enabled 
us to confirm certain results stated in the field 
literature such as: 
? importance of short contexts; 
? importance of adjacent noun or adverb for 
adjective disambiguation; 
? importance of adjacent adjective, or noun in a 
very short context for noun disambiguation; 
? importance of the noun in the area after the verb 
and use of dissymmetrical contexts for verb 
disambiguation. 
We have also obtained more original results, not 
always in line with some practices in the field such 
as: 
? importance of stop-words whose withdrawal 
decreases the performance almost systematically; 
? better results obtained by bigrams taken alone 
than unigrams alone; 
? unnecessary constraint of including or be 
adjacent to the target word. 
Disambiguation accuracy could probably be 
improved by the study of other sources of 
information useful in disambiguation, such as: 
? criteria based on binary syntactic relations (noun-
noun, noun-verb, adjective-noun, etc.) to capture 
information which can be absent from short 
contexts;  
? the use of thesauri or other sources of 
information to carry out generalizations on 
context words to overcome data sparseness 
problem; 
? topical text information; 
? selectional restrictions. 
This preliminary study focuses on homogenous 
criteria (for example: lemmas located from ?2 to 
+2 position). To improve the disambiguation 
accuracy, we have to look for heterogeneous 
criteria by gathering the most relevant pieces of 
contextual evidence not necessarily of the same 
type (for example: lemma in position ?2, part-of-
speech in position ?1, morphological form of 
target word and lemma in position +2). This 
feature selection leads to a combinatorial explosion 
that can be solved by genetic algorithms 
(Daelemans, Hoste, Meulder, Naudts, 2003). 
 
 
References  
Audibert L. (2001), LoX: Outil Polyvalent pour 
l'Exploration de Corpus Annot?s, 5?me 
Rencontre des ?tudiants Chercheurs en 
Informatique  pour le Traitement Automatique 
des Langues (RECITAL-2001), 411-419. 
Audibert L. (2003), Etude des Crit?res de 
D?sambigu?sation S?mantique Automatique: 
R?sultats sur les Cooccurrences, 10?me 
conf?rence sur le Traitement Automatique des 
Langues Naturelles (TALN-2003), 35-44. 
Bruce R., Wiebe J., Perdersen T. (1996), The 
Measure of a Model, 1st Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-1996), 101-112. 
Cussens J. (1993), Bayes and Pseudo-Bayes 
Estimates of Conditional Probability and their 
Reliability, 6th European Conference on 
Machine Learning (ECML-1993), 136-152. 
Daelemans W., Hoste V., Meulder F. D., Naudts B. 
(2003), Combined Optimization of Feature 
Selection and Algorithm Parameter Interaction in 
Machine Learning of Language, 14th European 
Conference on Machine Learning (ECML-2003), 
84-95. 
Domingos P., Pazzani M. (1997), Beyond 
Independence: Conditions for the Optimality of 
the Simple Bayesian Classifier, Machine 
Learning, 29: 103-130. 
El-B?ze M., Loupy C. d., Marteau P.-F. (1998), 
WSD Based on Three Short Context Methods, 
SENSEVAL Workshop, in press. 
Golding A. R. (1995), A Bayesion Hybrid Method 
for Context-Sensitive Spelling Correction, 3th 
Workshop on Very Large Corpora, 39-53. 
Kilgarriff A. (1997), Evaluating Word Sense 
Disambiguation Programs: Progress Report, 
Speech and Language Technology (SALT-1997) 
Workshop on Evaluation in Speech and 
Language Technology, 114-120. 
Kilgarriff A., Rosenzweig J. (2000), English 
Senseval: Report and Results, 2nd International 
Conference on Language Resources and 
Evaluation (LREC-2000), 3: 1239-1244. 
Mooney R. J. (1996), Comparative Experiments on 
Disambiguating Word Senses: an Illustration of 
the Role of Bias in Machine Learning, 1st 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP-1996), 82-91. 
Ng H. T. (1997a), Exemplar-Based Word Sense 
Disambiguation: Some Recent Improvements, 
2nd Conference on Empirical Methods in 
Natural Language Processing (EMNLP-1997), 
208-213. 
Ng H. T. (1997b), Getting Serious About Word 
Sense Disambiguation, Association for 
Computational Linguistics Special Interest 
Group on the Lexicon (ACL-SIGLEX-1997): 
Workshop "Tagging Text with Lexical 
Semantics: Why, What, and How ?" 1-7. 
Ng H. T., Lee Y. K. (1996), Integrating Multiple 
Knowledge Sources to Disambiguate Word 
Sense: An Exemplar-Based Approach, 34th 
Annual Meeting of the Society for Computational 
Linguistics, 40-47. 
Ng H. T., Lee Y. K. (2002), An Empirical 
Evaluation of Knowledge Sources and Learning 
Algorithms for Word Sense Disambiguation, 7th 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP-2002), 41-48. 
Ng H. T., Zelle J. (1997), Corpus-Based 
Approaches to Semantic Interpretation in Natural 
Language Processing, Artificial Intelligence 
Magazine - Special Issue on Natural Language 
Processing, 18: 45-64. 
Palmer M. (1998), Are WordNet Sense 
Distinctions Appropriate for Computational 
Lexicons, Association for Computational 
Linguistics Special Interest Group on the 
Lexicon (ACL-SIGLEX-1998): Senseval, in 
press. 
Pedersen T. (2001), Machine Learning with 
Lexical Features: the Duluth Approach to 
Senseval-2, 2nd International Workshop on 
Evaluating Word Sense Disambiguation Systems 
(Senseval-2), 139-142. 
Reymond D. (2002), M?thodologie pour la 
Cr?ation d'un Dictionnaire Distributionnel dans 
une Perspective d'?tiquetage Lexical Semi-
Automatique, 6?me Rencontre des ?tudiants 
Chercheurs en Informatique pour le Traitement 
Automatique des Langues (RECITAL-2002), 
405-414. 
Segond F. (2000), Framework and Results for 
French, Computers and the Humanities, 34: 49-
60. 
Valli A., V?ronis J. (1999), ?tiquetage 
Grammatical de Corpus Oraux: Probl?mes et 
Perpectives, Revue Fran?aise de Linguistique 
Appliqu?e, 4: 113-133. 
V?ronis J. (1998), A Study of Polysemy 
Judgements and Inter-Annotator Agreement, 
Programme and Advanced Papers of the 
Senseval-1 Workshop, 2-4. 
V?ronis J. (2001), Sense Tagging: Does It Make 
Sense ?, Corpus Linguistics Conference, 
http://www.up.univ-mrs.fr/~veronis/pdf/2001-
lancaster-sense.pdf. 
Yarowsky D. (1993), One Sense Per Collocation, 
ARPA Workshop on Human Language 
Technology, 266-271. 
Yarowsky D. (1994), A Comparision of Corpus-
Based Techniques for Restoring Accents in 
Spanish and French Text, 2nd Annual Workshop 
on Very Large Text Corpora, 19-32. 
Yarowsky D. (2000), Hierarchical Decision List 
for Word Sense Disambiguation, Computers and 
the Humanities, 34: 179-186. 
 
Appendix 
Nouns F S H MFS 
barrage 92 5 1.2 76.1%
chef 1133 11 1.5 76.0%
communication 1703 13 2.4 40.6%
compagnie 412 12 1.6 71.4%
concentration 246 6 2 45.1%
constitution 422 6 1.6 50.0%
degr? 507 18 2.5 58.6%
d?tention 112 2 0.9 72.3%
?conomie 930 10 2.2 49.1%
formation 1528 9 1.7 63.8%
lancement 138 5 1 79.7%
observation 572 3 0.7 86.0%
observation 572 3 0.7 86.0%
organe 366 6 2.2 38.3%
passage 600 19 2.7 37.0%
pied 960 62 3.5 37.6%
restauration 104 5 1.8 43.3%
solution 880 4 0.4 93.3%
station 266 8 2.6 32.0%
suspension 110 5 1.5 61.8%
vol 278 10 2.2 40.3%
Average 568 14.2 1.9 57.3%
     
Adjectives F S H MFS 
biologique 475 4 0.5 89.9%
clair 557 20 3.1 29.3%
correct 116 5 1.8 53.4%
courant 170 4 0.6 90.0%
exceptionnel 226 3 1.4 53.1%
frais 184 18 3.1 36.4%
haut 1017 29 3.5 25.0%
historique 620 3 0.7 87.4%
plein 844 35 4 17.1%
populaire 457 5 2 47.9%
r?gulier 181 11 2.5 32.6%
sain 129 10 2.4 40.3%
secondaire 195 5 1.7 53.8%
sensible 425 11 2.6 29.9%
simple 1051 14 2.1 41.3%
strict 220 9 2.2 45.5%
s?r 645 14 2.6 45.9%
traditionnel 447 2 0.5 89.5%
utile 359 9 2.4 42.9%
vaste 368 6 2.1 42.4%
Average 434 14.1 2.3 46.4%
     
Verbs F S H MFS 
arr?ter 916 15 3 23.9%
comprendre 2145 13 2.8 32.6%
conclure 727 16 2.4 45.5%
conduire 1093 15 2.3 38.2%
conna?tre 1635 16 2.2 40.1%
couvrir 543 22 3.3 33.3%
entrer 1258 39 3.7 26.6%
exercer 698 8 1.5 59.5%
importer 576 8 2.6 27.6%
mettre 5246 140 3.7 42.2%
ouvrir 919 41 3.8 26.0%
parvenir 654 8 2.3 36.7%
passer 2556 84 4.5 15.8%
porter 2347 59 4 29.4%
poursuivre 978 16 2.7 36.2%
pr?senter 2142 18 2.6 40.1%
rendre 1990 27 2.9 46.4%
r?pondre 2529 9 1 78.3%
tirer 1002 47 3.9 28.9%
venir 3797 33 3.2 24.9%
Average 1688 47.4 3.1 37.2%
Table 8: target word frequency (F), average 
number of senses (S), sense repartition entropy (H) 
and base-line accuracy (Most Frequent 
Sense: MFS). 
 
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 139?143,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Ontology-based semantic annotation: an automatic hybrid rule-based
method
Sondes Bannour, Laurent Audibert and Henry Soldano
LIPN, UMR 7030 CNRS
Universite? Paris 13, Sorbonne Paris Cite?, F-93430, Villetaneuse, France
firstname.lastname@lipn.univ-paris13.fr
Abstract
In the perspective of annotating a text with
respect to an ontology, we have partici-
pated in the subtask 1 of the BB BioNLP-
ST whose aim is to detect, in the text,
Bacteria Habitats and associate to them
one or several categories from the Onto-
Biotope ontology provided for the task.
We have used a rule-based machine learn-
ing algorithm (WHISK) combined with a
rule-based automatic ontology projection
method and a rote learning technique. The
combination of these three sources of rules
leads to good results with a SER measure
close to the winner and a best F-measure.
1 Introduction
Ontology-based semantic annotation consists in
linking fragments of a text to elements of a do-
main ontology enabling the interpretation and the
automatic exploitation of the texts content. Many
systems annotate texts with respect to an ontology
(Dill et al, 2003). Some of them use machine-
learning techniques to automate the annotation
process (Ciravegna, 2000).
On one side, machine-learning techniques de-
pend strongly on the amount and quality of pro-
vided training data sets and do not use information
available in the ontology. On the other side, using
the ontology to project its elements onto the text
depends strongly on the richness of the ontology
and may neglect important information available
in texts.
Our participation in the subtask 1 (entity de-
tection and categorization) of the BB BioNLP-
ST leverages the provided OntoBiotope ontology
and the training and development data sets pre-
processed using our annotation platform based on
UIMA (Ferrucci and Lally, 2004) (section 2). We
first tested, on the development set, a rule-based
machine-learning algorithm (WHISK (Soderland
et al, 1999)) that used training set examples (sec-
tion 3). Its results are limited because of the weak-
nesses of training data (section 4). We, then, com-
puted a rule-based automatic ontology projection
method consisting in retrieving from the text field
information content provided by the ontology (eg.
name of the concept). Thanks to the wealth of
the OntoBiotope ontology, this method gave good
results (section 5) that have been improved by
adding a rote learning technique that uses train-
ing examples and some filtering techniques (sec-
tion 6). Finally, we combined our method with
WHISK results, which slightly improved the F-
measure (section 7) on the development data.
2 TextMarker and data preprocessing
In a rule-based information extraction or seman-
tic annotation system, annotation rules are usually
written by a domain expert. However, these rules
can be learned using a rule-based learning algo-
rithm. The TextRuler system (Kluegl et al, 2009)
is a framework for semi-automatic development
of rule-based information extraction applications
that contains some implementations of such algo-
rithms ((LP)2 (Ciravegna, 2001; Ciravegna, 2003),
WHISK (Soderland et al, 1999), RAPIER (Califf
and Mooney, 2003), BWI (Freitag and Kushmer-
ick, 2000) and WIEN (Kushmerick et al, 1997)).
TextRuler is based on Apache UIMA TextMarker
which is a rule-based script language.
TextMarker is roughly similar to JAPE (Cun-
ningham et al, 2000), but based on UIMA (Fer-
rucci and Lally, 2004) rather than GATE (Cun-
ningham, 2002). According to some users ex-
periences, it is even more complete than JAPE.
Here is an example that gives an idea about how
to write and use TextMarker rules: Given an
UIMA type system that contains the types SPACE
(whitespace) and Lemma (with a feature ?lemma?
containing the lemmatized form of the matched
139
word), the following rule can be used to recognize
the term ?human body? in whatever form it ap-
pears in the text (singular, plural, uppercase, low-
ercase):
Lemma{FEATURE("lemma","human")}
SPACE Lemma{FEATURE("lemma","body")
--> MARK(Habitat, 1, 2, 3)};
This rule allows the creation of an annotation
called ?Habitat? that covers the three matched pat-
terns of the condition part of the rule.
To be able to use TextMarker, we have used our
annotation platform based on UIMA to preprocess
data with:
? Tokenisation, lemmatisation, sentence split-
ting and PoS-tagging of input data using
BioC (Smith et al, 2004; Liu et al, 2012).
? Term extraction using BioYatea (Golik et
al., 2013), a term extractor adapted to the
biomedical domain.
? Bacteria Habitat annotation to train learning
algorithms using annotation files provided in
this task (.a2).
For simplicity reasons, we do not take into ac-
count discontinuous annotations. We consider a
discontinuous annotation as the smallest segment
that include all fragments.
3 Rule Learning using WHISK
?In the subtask 1 of the BB BioNLP-ST, par-
ticipants must detect the boundaries of Bacteria
Habitat entities and, for each entity, assign one
or several concepts of the OntoBiotope ontology.?
Should we decompose the task into two subtasks
like it is suggested in the task formulation : (1) en-
tity detection and (2) categorization ? To answer
this question, we have conducted two experiments.
? Learning the root concept Habitat without as-
signing a Category to matched terms.
? Learning Bacteria Categories directly: each
Habitat Category is learned independently.
For the two experiments we considered only
Categories that have more than two examples in
the training set to train WHISK. Results are shown
in Table 1:
Experiment Precision Recall F-measure
Habitats learning 76.9% 24.5% 37.2%
Categories learning 77.3% 24% 36.6%
Table 1: Habitats learning vs Categories learning
WHISK gives an acceptable precision but a
low recall (the explanation is provided in sec-
tion 4) for both experiments. There is no big
difference between the two experiments? results:
WHISK doesn?t generalize over Habitats Cate-
gories. Learning Habitat Categories seems to be
the easier and safer way to use WHISK in this task.
4 Weaknesses of training examples
explain poor rule learning results
Training Development Total
Nb. Concepts: 333 274 491
Nb. Habitat: 934 611 1545
Nb. Annotation: 948 626 1574
Nb. C. 1 Instance: 182 179 272
Nb. C. 2 Instances: 66 41 86
Nb. C. > 2 Instances: 27 15 133
Number of concepts in ontology: 1756
Table 2: Figures on provided data
A close look at data samples helps understand
why the WHISK algorithm did not obtain good re-
sults. Table 2 exhibits some figures on training and
development data:
? 158 of the 274 concepts (58%) present in the
development data do not appear in the train-
ing data.
? Concepts present in sample data account for
19% of the ontology for the training data,
16% for the development data and 28% for
their combination.
? Obviously, it is difficult for a machine learn-
ing algorithm to learn (i.e. generalize) on
only one instance. This is the case for 55%
(272) of the concepts considering both the
training and the development sample data.
? If we consider that at least 3 instances are
needed to apply a machine learning algo-
rithm, only 27% of concepts present in the
training or development data are concerned.
This means that the ontology coverage is less
than 8%.
The conclusion is that training data are too
small to lead to a high performance recall for a
machine learning algorithm based exclusively on
these data.
5 The wealth of the ontology helps build
an efficient ontology-based rule set
The BB BioNLP-ST?s subtask 1 provides the On-
toBiotope ontology used to tag samples. For ex-
140
ample, the information provided by the ontology
for the concept MBTO:00001516 is
[Term]
id: MBTO:00001516
name: microorganism
exact_synonym: "microbe" [TyDI:23602]
related_synonym: "microbial" [TyDI:23603]
is_a: MBTO:00000297 ! living organism
Text segments tagged with this concept in ex-
amples are : microbe, microbial, microbes,
microorganisms, harmless stomach bugs.
One can notice that the name, exact synonym
and related synonym field information provided
by the ontology can help identify these segments.
If this strategy works, it will be a very robust one
because it is not sample dependent and it is ap-
plicable for all the 1756 concepts present in the
ontology.
The main idea is to directly search and tag in
the corpus the information provided by the con-
tent of fields name, exact synonym and related-
synonym of the ontology. Of course, projecting
them directly on samples raises inflection issues.
Our corpus provides two levels of lemmatisation
to avoid inflection problems: one from BioC and
the other from BioYaTeA. Our experiments show
that using the two of them in conjunction with the
token level (without any normalisation of words)
provides the best results. For example, the rules to
project name field of MBTO:00001516 are:
Token{REGEXP("?microorganism$")
-> MARKONCE(MBTO:00001516,1)} ;
Lemma{FEATURE("lemma","microorganism$")
-> MARKONCE(MBTO:00001516,1)} ;
Term{FEATURE("lemma","microorganism$")
-> MARKONCE(MBTO:00001516,1)} ;
Table 3 provides results obtained on develop-
ment data. We have also used training data to gen-
erate rote learning rules introduced in the next sec-
tion.
Rule set name Precision Recall F-measure
name: 67.4% 61.2% 64.2%
exact synonym: 61.2% 4.2% 7.8%
related synonym: 26.6% 5.9% 9.7%
rote learning: 63.6% 50.2% 56.1%
all together: 58.9% 73.8% 65.5%
Table 3: Performances of some sets of rules
6 Improving ontology-based rules
Rote learning rules
Results obtained for name and exact synonym
rules in Table 3 are very encouraging. We can
apply the same strategy of automatic rule genera-
tion from training data to text segments covered by
training examples. Projection rules are generated,
as described in section 5, for each example seg-
ment using the associated concept?s name as the
rule conclusion. This is a kind of rote learning.
Of course, we use an appropriate normalised ver-
sion of example segment to produce appropriate
rules based on BioC lemmatisation and BioYaTeA
lemmatisation1. For example, rote learning rules
for the segment harmless stomach bugs tagged
as MBTO:00000297 in trainning data are:
Token{REGEXP("?harmless$")}
Token{REGEXP("?stomach$")}
Token{REGEXP("?bugs$")
-> MARKONCE(MBTO:00001516,1,3)} ;
Lemma{FEATURE("lemma","harmless")}
Lemma{FEATURE("lemma","stomach")}
Lemma{FEATURE("lemma","bug")
-> MARKONCE(MBTO00001516,1,3)} ;
Rule sets filtering
Rule set name Precision Recall F-measure
name: 87.6% 55.1% 67.6%
exact synonym: 94.4% 2.7% 5.3%
related synonym: 71.4% 2.4% 4.6%
rote learning: 75.8% 44% 55.8%
all together: 80.9% 63.4% 71.1%
all together bis: 81.4% 63.4% 71.2%
Table 4: Performances of sets of filtered rules
A detailed analysis shows that our strategy
works well on the majority of concepts, but pro-
duces poor results for some concepts. To over-
come this limitation, we have adopted a strategy
consisting in filtering (deleting) rules that produce
lots of erroneous matches. More precisely, we
have deleted rules that match at least one time and
that conclude on a concept that obtains both a pre-
cision less or equal to 0.66 and a F-measure less or
equal to 0.66. This filtering is computed on train-
ing data. Table 4 shows performances on develop-
ment data obtained by filtered versions of rules of
table 3.
Rule sets combination
Our goal is to maximise the F-measure. F-
measure in table 4 for exact synonym and
related synonym rules is worse than in table 3 be-
cause of the decrease of the recall. But the com-
bination of the four simple rule sets allows to re-
cover some of the lost recall. The significative im-
1The information from BioYaTeA exists only for seg-
ments identified as a term.
141
provement of precision finally leads to an overall
improvement of the F-measure (all together in ta-
ble 4). Removing either one of the four sets of
rules that constitute the all together set of rules
from table 4 leads systematically to a decrease of
the F-measure.
Embedded rules removing
We have noticed a phenomenon that decreases pre-
cision and that can be corrected when combining
ontology-based sets of rules with the rote learn-
ing set of rules. To illustrate it, the name of the
concept MBTO:00002027 is plant. Among exam-
ples tagged with this concept, we can find healthy
plants. The name rule set matches on plants
and tags it with MBTO:00002027 (which is a mis-
take), while the rote learning rule set matches on
healthy plants and tags it with MBTO:00002027.
It is possible to correct this problem by a simple
rule that unmarks such embedded rules:
MBTO:00002027{ PARTOFNEQ( MBTO:00002027 )
-> UNMARK( MBTO:00002027 ) } ;
We have generated such a rule systematically for
all the concepts of the ontology to remove a few
mistakes (all together bis set of rules in table 4).
7 Adding Learned rules
Finally, we have completed the all together bis
set of filtered rules with the rules produced by the
WHISK algorithm. The difference between all to-
gether bis + whisk set of rules and the submitted
set of rules is that, by mistake, the last one did not
contain the related synonym rule set.
It is important to mention that all rules may ap-
ply simultaneously. There is also no execution or-
der between them except for rules that remove em-
bedded ones which must be applied at the end of
the rules set but before WHISK rules.
Rule set name Precision Recall F-measure
all together bis: 81.4% 63.4% 71.2%
all[...] + whisk: 79.1% 65% 71.4%
submitted: 79.3% 64.4% 71.1%
Table 5: Performances of final sets of rules on dev
data
Table 5 summarises performances achieved by
our final rule sets. Precision, Recall and F-
measure are computed on the development data
with rules based on the training data.
Table 6 summarises performances on test data
with the evaluator?s measures achieved by our fi-
nal rule sets based on training plus development
data.
Rule set name Precision Recall F1 SER
all together bis: 66.5% 61.4% 63.9% 42.5%
all[...] + WHISK: 61.4% 64.4% 62.9% 46.0%
submitted: 60.8% 60.8% 60.8% 48.7%
IRISA-TexMex (winner): 48% 72% 57% 46%
Table 6: Performances of final sets of rules on test
data
The subtask 1 of the BB BioNLP-ST ranks
competitors using the SER measure that must be
as close as possible to 0. We are quite close to the
winner with a SER of 48.7% against 46%. Our
F-measure (60.8%) is even better than the win-
ner?s F-measure (57%). Without our mistake, we
would have been placed equal first with a far bet-
ter F-measure (62.9%). We can also notice that
the WHISK rule set contribution is negative while
it was not the case on the developement data.
8 Conclusion and perspectives
Given the wealth of the OntoBiotope ontology
provided for subtask 1 of the BB BioNLP-ST, we
have decided to use a method that consists in iden-
tifying Bacteria Habitats using information avail-
able in this ontology. The method we have used is
rule-based and allows the automatic establishment
of a set of rules, written in the TextMarker lan-
guage, that match every ontology element (Habitat
Category) with its exact name, exact synonyms or
related synonyms in the text. As expected, this
method has achieved good results improved by
adding a rote learning technique based on train-
ing examples and filtering techniques that elimi-
nate categories that don?t perform well on the de-
velopment set.
The WHISK algorithm was also used to learn
Bacteria Habitats Categories. It gives a good pre-
cision but a low recall because of the poverty
of training data. Its combination with the ontol-
ogy projection method improves the recall and F-
measure in developement data but not in the final
test data.
The combination of these sources of rules leads
to good results with a SER measure close to the
winner and a best F-measure.
Actually, due to implementation limitations,
WHISK rules are essentially based on the Token
level (inflected form) of the corpus. Improvements
can be made by ameliorating this implementation
142
considering the lemmatized form of words, their
postags and also terms extracted by a term extrac-
tor. There is also another way of improvement
that consists in taking into account the is a rela-
tion of the ontology, both on WHISK rule set and
on ontology-based projection rules. Last, a closer
look at false positive and false negative errors can
lead to some improvements.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme funded by OSEO, French State agency for
innovation.
References
Mary Elaine Califf and Raymond J. Mooney. 2003.
Bottom-up relational learning of pattern matching
rules for information extraction. J. Mach. Learn.
Res., 4:177?210, December.
Fabio Ciravegna. 2000. Learning to tag for infor-
mation extraction from text. In Proceedings of the
ECAI-2000 Workshop on Machine Learning for In-
formation Extraction.
Fabio Ciravegna. 2001. (lp)2, an adaptive algorithm
for information extraction from web-related texts.
In In Proceedings of the IJCAI-2001 Workshop on
Adaptive Text Extraction and Mining.
Fabio Ciravegna. 2003. (lp)2: Rule induction for
information extraction using linguistic constraints.
Technical report.
Hamish Cunningham, Diana Maynard and Valentin
Tablan. 2000. JAPE: a Java Annotation Pat-
terns Engine (Second Edition). Technical report, of
Sheffield, Department of Computer Science.
Hamish Cunningham. 2002. Gate, a general architec-
ture for text engineering. Computers and the Hu-
manities, 36(2):223?254.
Stephen Dill, Nadav Eiron, David Gibson, Daniel
Gruhl, R. Guha, Anant Jhingran, Tapas Kanungo,
Kevin S. Mccurley, Sridhar Rajagopalan, Andrew
Tomkins, John A. Tomlin, and Jason Y. Zien. 2003.
A case for automated large scale semantic annota-
tions. Journal of Web Semantics, 1:115?132.
David Ferrucci and Adam Lally. 2004. Uima: an
architectural approach to unstructured information
processing in the corporate research environment.
Nat. Lang. Eng., 10:327?348.
Dayne Freitag and Nicholas Kushmerick. 2000.
Boosted wrapper induction. pages 577?583. AAAI
Press.
Wiktoria Golik, Robert Bossy, Zorana Ratkovic, and
Ne?dellec Claire. 2013. Improving Term Extraction
with Linguistic Analysis in the Biomedical Domain.
Proceedings of the 14th International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLing13), Special Issue of the journal
Research in Computing Science, pages 24?30.
Peter Kluegl, Martin Atzmueller, Tobias Hermann,
and Frank Puppe. 2009. A framework for semi-
automatic development of rule-based information
extraction applications. In Proc. LWA 2009 (KDML
- Special Track on Knowledge Discovery and Ma-
chine Learning), pages 56?59.
Nicholas Kushmerick, Daniel S. Weld and Robert
Doorenbos. 1997. Wrapper induction for informa-
tion extraction. In Proc. Int. Joint Conf. Artificial
Intelligence.
Haibin Liu, Tom Christiansen, William A. Baumgart-
ner, and Karin Verspoor. 2012. BioLemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of biomedical semantics,
3(1):3+.
Stephen Soderland, Claire Cardie, and Raymond
Mooney. 1999. Learning information extraction
rules for semi-structured and free text. In Machine
Learning, pages 233?272.
Lawrence H. Smith, Thomas C. Rindflesch and W.
John Wilbur. 2004. MedPost: a part-of-speech
tagger for bioMedical text. Bioinformatics (Oxford,
England), 20(14):2320?2321, September.
143
