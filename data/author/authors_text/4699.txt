Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 216?220, New York City, June 2006. c?2006 Association for Computational Linguistics
Multilingual Dependency Analysis with a Two-Stage Discriminative Parser
Ryan McDonald Kevin Lerman Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA
{ryantm,klerman,pereira}@cis.upenn.edu
Abstract
We present a two-stage multilingual de-
pendency parser and evaluate it on 13
diverse languages. The first stage is
based on the unlabeled dependency pars-
ing models described by McDonald and
Pereira (2006) augmented with morpho-
logical features for a subset of the lan-
guages. The second stage takes the out-
put from the first and labels all the edges
in the dependency graph with appropri-
ate syntactic categories using a globally
trained sequence classifier over compo-
nents of the graph. We report results on
the CoNLL-X shared task (Buchholz et
al., 2006) data sets and present an error
analysis.
1 Introduction
Often in language processing we require a deep syn-
tactic representation of a sentence in order to assist
further processing. With the availability of resources
such as the Penn WSJ Treebank, much of the fo-
cus in the parsing community had been on producing
syntactic representations based on phrase-structure.
However, recently their has been a revived interest
in parsing models that produce dependency graph
representations of sentences, which model words
and their arguments through directed edges (Hud-
son, 1984; Mel?c?uk, 1988). This interest has gener-
ally come about due to the computationally efficient
and flexible nature of dependency graphs and their
ability to easily model non-projectivity in freer-word
order languages. Nivre (2005) gives an introduction
to dependency representations of sentences and re-
cent developments in dependency parsing strategies.
Dependency graphs also encode much of the deep
syntactic information needed for further process-
ing. This has been shown through their success-
ful use in many standard natural language process-
ing tasks, including machine translation (Ding and
Palmer, 2005), sentence compression (McDonald,
2006), and textual inference (Haghighi et al, 2005).
In this paper we describe a two-stage discrimi-
native parsing approach consisting of an unlabeled
parser and a subsequent edge labeler. We evaluate
this parser on a diverse set of 13 languages using
data provided by the CoNLL-X shared-task organiz-
ers (Buchholz et al, 2006; Hajic? et al, 2004; Simov
et al, 2005; Simov and Osenova, 2003; Chen et al,
2003; Bo?hmova? et al, 2003; Kromann, 2003; van
der Beek et al, 2002; Brants et al, 2002; Kawata
and Bartels, 2000; Afonso et al, 2002; Dz?eroski et
al., 2006; Civit Torruella and Mart?? Anton??n, 2002;
Nilsson et al, 2005; Oflazer et al, 2003; Atalay et
al., 2003). The results are promising and show the
language independence of our system under the as-
sumption of a labeled dependency corpus in the tar-
get language.
For the remainder of this paper, we denote by
x = x1, . . . xn a sentence with n words and by
y a corresponding dependency graph. A depen-
dency graph is represented by a set of ordered pairs
(i, j) ? y in which xj is a dependent and xi is the
corresponding head. Each edge can be assigned a la-
bel l(i,j) from a finite set L of predefined labels. We
216
assume that all dependency graphs are trees but may
be non-projective, both of which are true in the data
sets we use.
2 Stage 1: Unlabeled Parsing
The first stage of our system creates an unlabeled
parse y for an input sentence x. This system is
primarily based on the parsing models described
by McDonald and Pereira (2006). That work ex-
tends the maximum spanning tree dependency pars-
ing framework (McDonald et al, 2005a; McDonald
et al, 2005b) to incorporate features over multiple
edges in the dependency graph. An exact projec-
tive and an approximate non-projective parsing al-
gorithm are presented, since it is shown that non-
projective dependency parsing becomes NP-hard
when features are extended beyond a single edge.
That system uses MIRA, an online large-margin
learning algorithm, to compute model parameters.
Its power lies in the ability to define a rich set of fea-
tures over parsing decisions, as well as surface level
features relative to these decisions. For instance, the
system of McDonald et al (2005a) incorporates fea-
tures over the part of speech of words occurring be-
tween and around a possible head-dependent rela-
tion. These features are highly important to over-
all accuracy since they eliminate unlikely scenarios
such as a preposition modifying a noun not directly
to its left, or a noun modifying a verb with another
verb occurring between them.
We augmented this model to incorporate morpho-
logical features derived from each token. Consider a
proposed dependency of a dependent xj on the head
xi, each with morphological features Mj and Mi re-
spectively. We then add to the representation of the
edge: Mi as head features, Mj as dependent fea-
tures, and also each conjunction of a feature from
both sets. These features play the obvious role of
explicitly modeling consistencies and commonali-
ties between a head and its dependents in terms of
attributes like gender, case, or number. Not all data
sets in our experiments include morphological fea-
tures, so we use them only when available.
3 Stage 2: Label Classification
The second stage takes the output parse y for sen-
tence x and classifies each edge (i, j) ? y with a
particular label l(i,j). Ideally one would like to make
all parsing and labeling decisions jointly so that the
shared knowledge of both decisions will help resolve
any ambiguities. However, the parser is fundamen-
tally limited by the scope of local factorizations that
make inference tractable. In our case this means
we are forced only to consider features over single
edges or pairs of edges. However, in a two stage
system we can incorporate features over the entire
output of the unlabeled parser since that structure is
fixed as input. The simplest labeler would be to take
as input an edge (i, j) ? y for sentence x and find
the label with highest score,
l(i,j) = argmax
l
s(l, (i, j),y,x)
Doing this for each edge in the tree would pro-
duce the final output. Such a model could easily be
trained using the provided training data for each lan-
guage. However, it might be advantageous to know
the labels of other nearby edges. For instance, if we
consider a head xi with dependents xj1 , . . . , xjM , it
is often the case that many of these dependencies
will have correlated labels. To model this we treat
the labeling of the edges (i, j1), . . . , (i, jM ) as a se-
quence labeling problem,
(l(i,j1), . . . , l(i,jM )) = l? = argmax
l?
s(l?, i,y,x)
We use a first-order Markov factorization of the
score
l? = argmax
l?
M
?
m=2
s(l(i,jm), l(i,jm?1), i,y,x)
in which each factor is the score of labeling the adja-
cent edges (i, jm) and (i, jm?1) in the tree y. We at-
tempted higher-order Markov factorizations but they
did not improve performance uniformly across lan-
guages and training became significantly slower.
For score functions, we use simple dot products
between high dimensional feature representations
and a weight vector
s(l(i,jm), l(i,jm?1), i,y,x) =
w ? f(l(i,jm), l(i,jm?1), i,y,x)
Assuming we have an appropriate feature repre-
sentation, we can find the highest scoring label se-
quence with Viterbi?s algorithm. We use the MIRA
217
online learner to set the weights (Crammer and
Singer, 2003; McDonald et al, 2005a) since we
found it trained quickly and provide good perfor-
mance. Furthermore, it made the system homoge-
neous in terms of learning algorithms since that is
what is used to train our unlabeled parser (McDon-
ald and Pereira, 2006). Of course, we have to define
a set of suitable features. We used the following:
? Edge Features: Word/pre-suffix/part-of-speech
(POS)/morphological feature identity of the head and the
dependent (affix lengths 2 and 3). Does the head and its
dependent share a prefix/suffix? Attachment direction.
What morphological features do head and dependent
have the same value for? Is the dependent the first/last
word in the sentence?
? Sibling Features: Word/POS/pre-suffix/morphological
feature identity of the dependent?s nearest left/right sib-
lings in the tree (siblings are words with same parent in
the tree). Do any of the dependent?s siblings share its
POS?
? Context Features: POS tag of each intervening word be-
tween head and dependent. Do any of the words between
the head and the dependent have a parent other than the
head? Are any of the words between the head and the de-
pendent not a descendant of the head (i.e. non-projective
edge)?
? Non-local: How many children does the dependent have?
What morphological features do the grandparent and the
dependent have identical values? Is this the left/right-
most dependent for the head? Is this the first dependent
to the left/right of the head?
Various conjunctions of these were included
based on performance on held-out data. Note that
many of these features are beyond the scope of the
edge based factorizations of the unlabeled parser.
Thus a joint model of parsing and labeling could not
easily include them without some form of re-ranking
or approximate parameter estimation.
4 Results
We trained models for all 13 languages provided
by the CoNLL organizers (Buchholz et al, 2006).
Based on performance from a held-out section of the
training data, we used non-projective parsing algo-
rithms for Czech, Danish, Dutch, German, Japanese,
Portuguese and Slovene, and projective parsing al-
gorithms for Arabic, Bulgarian, Chinese, Spanish,
Swedish and Turkish. Furthermore, for Arabic and
Spanish, we used lemmas instead of inflected word
DATA SET UA LA
ARABIC 79.3 66.9
BULGARIAN 92.0 87.6
CHINESE 91.1 85.9
CZECH 87.3 80.2
DANISH 90.6 84.8
DUTCH 83.6 79.2
GERMAN 90.4 87.3
JAPANESE 92.8 90.7
PORTUGUESE 91.4 86.8
SLOVENE 83.2 73.4
SPANISH 86.1 82.3
SWEDISH 88.9 82.5
TURKISH 74.7 63.2
AVERAGE 87.0 80.8
Table 1: Dependency accuracy on 13 languages.
Unlabeled (UA) and Labeled Accuracy (LA).
forms, again based on performance on held-out
data1.
Results on the test set are given in Table 1. Per-
formance is measured through unlabeled accuracy,
which is the percentage of words that modify the
correct head in the dependency graph, and labeled
accuracy, which is the percentage of words that
modify the correct head and label the dependency
edge correctly in the graph. These results show that
the discriminative spanning tree parsing framework
(McDonald et al, 2005b; McDonald and Pereira,
2006) is easily adapted across all these languages.
Only Arabic, Turkish and Slovene have parsing ac-
curacies significantly below 80%, and these lan-
guages have relatively small training sets and/or are
highly inflected with little to no word order con-
straints. Furthermore, these results show that a two-
stage system can achieve a relatively high perfor-
mance. In fact, for every language our models per-
form significantly higher than the average perfor-
mance for all the systems reported in Buchholz et
al. (2006).
For the remainder of the paper we provide a gen-
eral error analysis across a wide set of languages
plus a detailed error analysis of Spanish and Arabic.
5 General Error Analysis
Our system has several components, including the
ability to produce non-projective edges, sequential
1Using the non-projective parser for all languages does not
effect performance significantly. Similarly, using the inflected
word form instead of the lemma for all languages does not
change performance significantly.
218
SYSTEM UA LA
N+S+M 86.3 79.7
P+S+M 85.6 79.2
N+S+B 85.5 78.6
N+A+M 86.3 79.4
P+A+B 84.8 77.7
Table 2: Error analysis of parser components av-
eraged over Arabic, Bulgarian, Danish, Dutch,
Japanese, Portuguese, Slovene, Spanish, Swedish
and Turkish. N/P: Allow non-projective/Force pro-
jective, S/A: Sequential labeling/Atomic labeling,
M/B: Include morphology features/No morphology
features.
assignment of edge labels instead of individual as-
signment, and a rich feature set that incorporates
morphological properties when available. The bene-
fit of each of these is shown in Table 2. These results
report the average labeled and unlabeled precision
for the 10 languages with the smallest training sets.
This allowed us to train new models quickly.
Table 2 shows that each component of our system
does not change performance significantly (rows 2-
4 versus row 1). However, if we only allow projec-
tive parses, do not use morphological features and
label edges with a simple atomic classifier, the over-
all drop in performance becomes significant (row
5 versus row 1). Allowing non-projective parses
helped with freer word order languages like Dutch
(78.8%/74.7% to 83.6%/79.2%, unlabeled/labeled
accuracy). Including rich morphology features natu-
rally helped with highly inflected languages, in par-
ticular Spanish, Arabic, Turkish, Slovene and to a
lesser extent Dutch and Portuguese. Derived mor-
phological features improved accuracy in all these
languages by 1-3% absolute.
Sequential classification of labels had very lit-
tle effect on overall labeled accuracy (79.4% to
79.7%)2. The major contribution was in helping to
distinguish subjects, objects and other dependents
of main verbs, which is the most common label-
ing error. This is not surprising since these edge
labels typically are the most correlated (i.e., if you
already know which noun dependent is the subject,
then it should be easy to find the object). For in-
stance, sequential labeling improves the labeling of
2This difference was much larger for experiments in which
gold standard unlabeled dependencies are used.
objects from 81.7%/75.6% to 84.2%/81.3% (la-
beled precision/recall) and the labeling of subjects
from 86.8%/88.2% to 90.5%/90.4% for Swedish.
Similar improvements are common across all lan-
guages, though not as dramatic. Even with this im-
provement, the labeling of verb dependents remains
the highest source of error.
6 Detailed Analysis
6.1 Spanish
Although overall unlabeled accuracy is 86%, most
verbs and some conjunctions attach to their head
words with much lower accuracy: 69% for main
verbs, 75% for the verb ser, and 65% for coor-
dinating conjunctions. These words form 17% of
the test corpus. Other high-frequency word classes
with relatively low attachment accuracy are preposi-
tions (80%), adverbs (82%) and subordinating con-
junctions (80%), for a total of another 23% of the
test corpus. These weaknesses are not surprising,
since these decisions encode the more global as-
pects of sentence structure: arrangement of clauses
and adverbial dependents in multi-clause sentences,
and prepositional phrase attachment. In a prelimi-
nary test of this hypothesis, we looked at all of the
sentences from a development set in which a main
verb is incorrectly attached. We confirmed that the
main clause is often misidentified in multi-clause
sentences, or that one of several conjoined clauses
is incorrectly taken as the main clause. To test this
further, we added features to count the number of
commas and conjunctions between a dependent verb
and its candidate head. Unlabeled accuracy for all
verbs increases from 71% to 73% and for all con-
junctions from 71% to 74%. Unfortunately, accu-
racy for other word types decreases somewhat, re-
sulting in no significant net accuracy change. Nev-
ertheless, this very preliminary experiment suggests
that wider-range features may be useful in improv-
ing the recognition of overall sentence structure.
Another common verb attachment error is a
switch between head and dependent verb in phrasal
verb forms like dejan intrigar or qiero decir, possi-
bly because the non-finite verb in these cases is often
a main verb in training sentences. We need to look
more carefully at verb features that may be useful
here, in particular features that distinguish finite and
219
non-finite forms.
In doing this preliminary analysis, we noticed
some inconsistencies in the reference dependency
structures. For example, in the test sentence Lo
que decia Mae West de si misma podr??amos decirlo
tambie?n los hombres:..., decia?s head is given as de-
cirlo, although the main verbs of relative clauses are
normally dependent on what the relative modifies, in
this case the article Lo.
6.2 Arabic
A quick look at unlabeled attachment accuracies in-
dicate that errors in Arabic parsing are the most
common across all languages: prepositions (62%),
conjunctions (69%) and to a lesser extent verbs
(73%). Similarly, for labeled accuracy, the hard-
est edges to label are for dependents of verbs, i.e.,
subjects, objects and adverbials. Note the differ-
ence in error between the unlabeled parser and the
edge labeler: the former makes mistakes on edges
into prepositions, conjunctions and verbs, and the
latter makes mistakes on edges into nouns (sub-
ject/objects). Each stage by itself is relatively ac-
curate (unlabeled accuracy is 79% and labeling ac-
curacy3 is also 79%), but since there is very little
overlap in the kinds of errors each makes, overall la-
beled accuracy drops to 67%. This drop is not nearly
as significant for other languages.
Another source of potential error is that the aver-
age sentence length of Arabic is much higher than
other languages (around 37 words/sentence). How-
ever, if we only look at performance for sentences
of length less than 30, the labeled accuracy is still
only 71%. The fact that Arabic has only 1500 train-
ing instances might also be problematic. For exam-
ple if we train on 200, 400, 800 and the full training
set, labeled accuracies are 54%, 60%, 62% and 67%.
Clearly adding more data is improving performance.
However, when compared to the performance of
Slovene (1500 training instances) and Spanish (3300
instances), it appears that Arabic parsing is lagging.
7 Conclusions
We have presented results showing that the spanning
tree dependency parsing framework of McDonald et
3Labeling accuracy is the percentage of words that correctly
label the dependency between the head that they modify, even
if the right head was not identified.
al. (McDonald et al, 2005b; McDonald and Pereira,
2006) generalizes well to languages other than En-
glish. In the future we plan to extend these mod-
els in two ways. First, we plan on examining the
performance difference between two-staged depen-
dency parsing (as presented here) and joint parsing
plus labeling. It is our hypothesis that for languages
with fine-grained label sets, joint parsing and label-
ing will improve performance. Second, we plan on
integrating any available morphological features in
a more principled manner. The current system sim-
ply includes all morphological bi-gram features. It
is our hope that a better morphological feature set
will help with both unlabeled parsing and labeling
for highly inflected languages.
References
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. SIGNLL.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. ACL.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust
textual inference via graph matching. In Proc. HTL-
EMNLP.
R. Hudson. 1984. Word Grammar. Blackwell.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT-EMNLP.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proc. EACL.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
J. Nivre. 2005. Dependency grammar and dependency
parsing. Technical Report MSI report 05133, Va?xjo?
University: School of Mathematics and Systems Engi-
neering.
220
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 473?480
Manchester, August 2008
Reading the Markets:
Forecasting Public Opinion of Political Candidates by News Analysis
Kevin Lerman
Dept. of Computer Science
Columbia University
New York, NY USA
klerman@cs.columbia.edu
Ari Gilder and Mark Dredze
Dept. of CIS
University of Pennsylvania
Philadelphia, PA USA
agilder@alumni.upenn.edu
mdredze@cis.upenn.edu
Fernando Pereira
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA USA
pereira@google.com
Abstract
Media reporting shapes public opinion
which can in turn influence events, partic-
ularly in political elections, in which can-
didates both respond to and shape public
perception of their campaigns. We use
computational linguistics to automatically
predict the impact of news on public per-
ception of political candidates. Our sys-
tem uses daily newspaper articles to pre-
dict shifts in public opinion as reflected
in prediction markets. We discuss various
types of features designed for this problem.
The news system improves market predic-
tion over baseline market systems.
1 Introduction
The mass media can affect world events by sway-
ing public opinion, officials and decision makers.
Financial investors who evaluate the economic per-
formance of a company can be swayed by positive
and negative perceptions about the company in the
media, directly impacting its economic position.
The same is true of politics, where a candidate?s
performance is impacted by media influenced pub-
lic perception. Computational linguistics can dis-
cover such signals in the news. For example, De-
vitt and Ahmad (2007) gave a computable metric
of polarity in financial news text consistent with
human judgments. Koppel and Shtrimberg (2004)
used a daily news analysis to predict financial mar-
ket performance, though predictions could not be
used for future investment decisions. Recently,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
a study conducted of the 2007 French presiden-
tial election showed a correlation between the fre-
quency of a candidate?s name in the news and elec-
toral success (V?eronis, 2007).
This work forecasts day-to-day changes in pub-
lic perception of political candidates from daily
news. Measuring daily public perception with
polls is problematic since they are conducted by a
variety of organizations at different intervals and
are not easily comparable. Instead, we rely on
daily measurements from prediction markets.
We present a computational system that uses
both external linguistic information and internal
market indicators to forecast public opinion mea-
sured by prediction markets. We use features from
syntactic dependency parses of the news and a
user-defined set of market entities. Successive
news days are compared to determine the novel
component of each day?s news resulting in features
for a machine learning system. A combination sys-
tem uses this information as well as predictions
from internal market forces to model prediction
markets better than several baselines. Results show
that news articles can be mined to predict changes
in public opinion.
Opinion forecasting differs from that of opin-
ion analysis, such as extracting opinions, evaluat-
ing sentiment, and extracting predictions (Kim and
Hovy, 2007). Contrary to these tasks, our system
receives objective news, not subjective opinions,
and learns what events will impact public opinion.
For example, ?oil prices rose? is a fact but will
likely shape opinions. This work analyzes news
(cause) to predict future opinions (effect). This af-
fects the structure of our task: we consider a time-
series setting since we must use past data to predict
future opinions, rather than analyzing opinions in
batch across the whole dataset.
473
We begin with an introduction to prediction
markets. Several methods for new feature extrac-
tion are explored as well as market history base-
lines. Systems are evaluated on prediction markets
from the 2004 US Presidential election. We close
with a discussion of related and future work.
2 Prediction Markets
Prediction markets, such as TradeSports and the
Iowa Electronic Markets
1
, provide a setting sim-
ilar to financial markets wherein shares represent
not companies or commodities, but an outcome
of a sporting, financial or political event. For ex-
ample, during the 2004 US Presidential election,
one could purchase a share of ?George W. Bush
to win the 2004 US Presidential election? or ?John
Kerry to win the 2004 US Presidential election.?
A pay-out of $1 is awarded to winning sharehold-
ers at market?s end, e.g. Bush wins the election.
In the interim, price fluctuations driven by supply
and demand indicate the perception of the event?s
likelihood, which indicates public opinion of an
event. Several studies show the accuracy of predic-
tion markets in predicting future events (Wolfers
and Zitzewitz, 2004; Servan-Schreiber et al, 2004;
Pennock et al, 2000), such as the success of up-
coming movies (Jank and Foutz, 2007), political
stock markets (Forsythe et al, 1999) and sports
betting markets (Williams, 1999).
Market investors rely on daily news reports to
dictate investment actions. If something positive
happens for Bush (e.g. Saddam Hussein is cap-
tured), Bush will appear more likely to win, so
demand increases for ?Bush to win? shares, and
the price rises. Likewise, if something negative for
Bush occurs (e.g. casualties in Iraq increase), peo-
ple will think he is less likely to win, sell their
shares, and the price drops. Therefore, predic-
tion markets can be seen as rapid response indi-
cators of public mood concerning political candi-
dates. Market-internal factors, such as general in-
vestor mood and market history, also affect price.
For instance, a positive news story for a candidate
may have less impact if investors dislike the can-
didate. Explaining market behavior requires mod-
eling news information external to the market and
internal trends to the market.
This work uses the 2004 US Presidential elec-
tion markets from Iowa Electronic Markets. Each
market provides a daily average price, which indi-
1
www.tradesports.com, www.biz.uiowa.edu/iem/
cates the overall market sentiment for a candidate
on a given day. The goal of the prediction system
is to predict the price direction for the next day (up
or down) given all available information up to the
current day: previous days? market pricing/volume
information and the morning news. Market his-
tory represents information internal to the market:
if an investor has no knowledge of external events,
what is the most likely direction for the market?
This information can capture general trends and
volatility of the market. The daily news is the ex-
ternal information that influences the market. This
provides information, independent of any internal
market effects to which investors will respond. A
learning system for each information source is de-
veloped and combined to explain market behavior.
The following sections describe these systems.
3 External Information: News
Changes in market price are likely responses to
current events reported in the news. Investors read
the morning paper and act based on perceptions of
events. Can a system with access to this same in-
formation make good investment decisions?
Our system operates in an iterative (online) fash-
ion. On each day (round) the news for that day is
used to construct a new instance. A logistic re-
gression classifier is trained on all previous days
and the resulting classifier predicts the price move-
ment of the new instance. The system either prof-
its or loses money according to this prediction. It
then receives the actual price movement and labels
the instance accordingly (up or down). This set-
ting is straightforward; the difficulty is in choosing
a good feature representation for the classifier. We
now explore several representation techniques.
3.1 Bag-of-Words Features
The prediction task can be treated as a document
classification problem, where the document is the
day?s news and the label is the direction of the mar-
ket. Document classification systems typically rely
on bag-of-words features, where each feature indi-
cates the number of occurrences of a word in the
document. The news for a given day is represented
by a normalized unit length vector of counts, ex-
cluding common stop words and features that oc-
cur fewer than 20 times in our corpus.
474
3.2 News Focus Features
Simple bag-of-words features may not capture rel-
evant news information. Public opinion is influ-
enced by new events ? a change in focus. The day
after a debate, most papers may declare Bush the
winner, yielding a rise in the price of a ?Bush to
win? share. However, while the debate may be
discussed for several days after the event, public
opinion of Bush will probably not continue to rise
on old news. Changes in public opinion should
reflect changes in daily news coverage. Instead of
constructing features for a single day, they can rep-
resent differences between two days of news cov-
erage, i.e. the novelty of the coverage. Given the
counts of feature i on day t as c
t
i
, where feature i
may be the unigram ?scandal,? and the set of fea-
tures on day t as C
t
, the fraction of news focus for
each feature is f
t
i
=
c
t
i
|C
t
|
. The news focus change
(?) for feature i on day t is defined as,
?f
t
i
= log
(
f
t
i
1
3
(f
t?1
i
+ f
t?2
i
+ f
t?3
i
)
)
, (1)
where the numerator is the focus of news on fea-
ture i today and the denominator is the average
focus over the previous three days. The resulting
value captures the change in focus on day t, where
a value greater than 0 means increased focus and a
value less than 0 decreased focus. Feature counts
were smoothed by adding a constant (10).
3.3 Entity Features
As shown by Wiebe et al (2005), it is important to
know not only what is being said but about whom it
is said. The term ?victorious? by itself is meaning-
less when discussing an election ? meaning comes
from the subject. Similarly, the word ?scandal?
is bad for a candidate but good for the opponent.
Subjects can often be determined by proximity. If
the word ?scandal? and Bush are mentioned in the
same sentence, this is likely to be bad for Bush. A
small set of entities relevant to a market can be de-
fined a priori to give context to features. For exam-
ple, the entities ?Bush,? ?Kerry? and ?Iraq? were
known to be relevant before the general election.
Kim and Hovy (2007) make a similar assumption.
News is filtered for sentences that mention ex-
actly one of these entities. Such sentences are
likely about that entity, and the extracted features
are conjunctions of the word and the entity. For ex-
ample, the sentence ?Bush is facing another scan-
dal? produces the feature ?bush-scandal? instead
of just ?scandal.?
2
Context disambiguation comes
at a high cost: about 70% of all sentences do not
contain any predefined entities and about 7% con-
tain more than one entity. These likely relevant
sentences are unfortunately discarded, although
future work could reduce the number of discarded
sentences using coreference resolution.
3.4 Dependency Features
While entity features are helpful they cannot pro-
cess multiple entity sentences, nearly a quarter of
the entity sentences. These sentences may be the
most helpful since they indicate entity interactions.
Consider the following three example sentences:
? Bush defeated Kerry in the debate.
? Kerry defeated Bush in the debate.
? Kerry, a senator from Massachusetts, de-
feated President Bush in last night?s debate.
Obviously, the first two sentences have very dif-
ferent meanings for each candidate?s campaign.
However, representations considered so far do not
differentiate between these sentences, nor would
any heuristic using proximity to an entity.
3
Effec-
tive features rely on the proper identification of the
subject and object of ?defeated.? Longer n-grams,
which would be very sparse, would succeed for the
first two sentences but not the third.
To capture these interactions, features were ex-
tracted from dependency parses of the news ar-
ticles. Sentences were part of speech tagged
(Toutanova et al, 2003), parsed with a depen-
dency parser and labeled with grammatical func-
tion labels (McDonald et al, 2006). The result-
ing parses encode dependencies for each sentence,
where word relationships are expressed as parent-
child links. The parse for the third sentence above
indicates that ?Kerry? is the subject of ?defeated,?
and ?Bush? is the object. Features are extracted
from parse trees containing the pre-defined enti-
ties (section 3.3), using the parent, grandparent,
aunts, nieces, children, and siblings of any in-
stances of the pre-defined entities we observe. Fea-
tures are conjoined indicators of the node?s lexical
entry, part of speech tag and dependency relation
2
Other methods can identify the subject of sentiment ex-
pressions, but our text is objective news. Therefore, we em-
ploy this approximate method.
3
Several failed heuristics were tried, such as associating
each word to an entity within a fixed window in the sentence
or the closer entity if two were in the window.
475
Feature Good For
Kerry? plan? the Kerry
poll? showed? Bush Bush
won? Kerry
4
Kerry
agenda? ?s? Bush Kerry
Kerry? spokesperson? campaign Bush
Table 1: Simplified examples of features from the
general election market. Arrows point from parent
to child. Features also include the word?s depen-
dency relation labels and parts of speech.
label. For aunts, nieces, and children, the com-
mon ancestor is used, and in the case of grand-
parent, the intervening parent is included. Each
of these conjunctions includes the discovered en-
tity and back-off features are included by remov-
ing some of the other information. Note that be-
sides extracting more precise information from the
news text, this handles sentences with multiple en-
tities, since it associates parts of a sentence with
different entities. In practice, we use this in con-
junction with News Focus. Useful features from
the general election market are in table 1. Note
that they capture events and not opinions. For ex-
ample, the last feature indicates that a statement by
the Kerry campaign was good for Bush, possibly
because Kerry was reacting to criticism.
4 Internal Information: Market History
News cannot explain all market trends. Momen-
tum in the market, market inefficiencies, and slow
news days can affect share price. A candidate who
does well will likely continue to do well unless
new events occur. Learning general market behav-
ior can help explain these price movements.
For each day t, we create an instance using fea-
tures for the price and volume at day t ? 1 and
the price and volume change between days t ? 1
and t ? 2. We train using a ridge regression
5
on
all previous days (labeled with their actual price
movements) to forecast the movement for day t,
which we convert into a binary value: up or down.
4
This feature matches phrases like ?Kerry won [the de-
bate]? and ?[something] won Kerry [support]?
5
This outperformed more sophisticated algorithms, in-
cluding the logistic regression used earlier. This may be due
to the fact that many market history features (e.g. previous
price movements) are very similar in nature to the future price
movements being predicted.
5 Combined System
Since both news and internal market information
are important for modeling market behavior, each
one cannot be evaluated in isolation. For example,
a successful news system may learn to spot impor-
tant events for a candidate, but cannot explain the
price movements of a slow news day. A combina-
tion of the market history system and news features
is needed to model the markets.
Expert algorithms for combining prediction sys-
tems have been well studied. However, experi-
ments with the popular weighted majority algo-
rithm (Littlestone and Warmuth, 1989) yielded
poor performance since it attempts to learn the
optimal balance between systems while our set-
ting has rapidly shifting quality between few ex-
perts with little data for learning. Instead, a sim-
ple heuristic was used to select the best perform-
ing predictor on each day. We compare the 3-
day prediction accuracy (measured in total earn-
ings) for each system (news and market history)
to determine the current best system. The use of
a small window allows rapid change in systems.
When neither system has a better 3-day accuracy
the combined system will only predict if the two
systems agree and abstain otherwise. This strategy
measures how accurately a news system can ac-
count for price movements when non-news move-
ments are accounted for by market history. The
combined system improved over individual evalu-
ations of each system on every market
6
.
6 Evaluation
Daily pricing information was obtained from the
Iowa Electronic Markets for the 2004 US Presi-
dential election for six Democratic primary con-
tenders (Clark, Clinton, Dean, Gephardt, Kerry
and Lieberman) and two general election candi-
dates (Bush and Kerry). Market length varied as
some candidates entered the race later than others:
the DNC markets for Clinton, Gephardt, Kerry,
and Lieberman were each 332 days long, while
Dean?s was 130 days and Clark?s 106. The general
election market for Bush was 153 days long, while
Kerry?s was 142.
7
The price delta for each day
was taken as the difference between the average
6
This outperformed a single model built over all features,
perhaps due to the differing natures of the feature types we
used.
7
The first 11 days of the Kerry general election market
were removed due to strange price fluctuations in the data.
476
price between the previous and current day. Mar-
ket data also included the daily volume that was
used as a market history feature. Entities selected
for each market were the names of all candidates
involved in the election and ?Iraq.?
News articles covering the election were ob-
tained from Factiva
8
, an online news archive run
by Dow Jones. Since the system must make a pre-
diction at the beginning of each day, only articles
from daily newspapers released early in the morn-
ing were included. The corpus contained approxi-
mately 50 articles per day over a span of 3 months
to almost a year, depending on the market.
9
While most classification systems are evaluated
by measuring their accuracy on cross-validation
experiments, both the method and the metric are
unsuitable to our task. A decision for a given day
must be made with knowledge of only the previ-
ous days, ruling out cross validation. In fact, we
observed improved results when the system was
allowed access to future articles through cross-
validation. Further, raw prediction accuracy is not
a suitable metric for evaluation because it ignores
the magnitude in price shifts each day. A sys-
tem should be rewarded proportional to the signif-
icance of the day?s market change.
To address these issues we used a chronological
evaluation where systems were rewarded for cor-
rect predictions in proportion to the magnitude of
that day?s shift, i.e. the ability to profit from the
market. This metric is analogous to weighted accu-
racy. On each day, the system is provided with all
available morning news and market history from
which an instance is created using one of the fea-
ture schemes described above. We then predict
whether the market price will rise or fall and the
system either earns or loses the price change for
that day if it was right or wrong respectively. The
system then learns the correct price movement and
the process is repeated for the next day.
10
Sys-
tems that correctly forecast public opinions from
the news will make more money. In economic
terms, this is equivalent to buying or short-selling a
single share of the market and then selling or cov-
ering the short at the end of the day.
11
Scores were
8
http://www.factiva.com/
9
While 50 articles may not seem like much, humans read
far less text before making investment decisions.
10
This scheme is called ?online learning? for which a
whole class of algorithms apply. We used batch algorithms
since training happens only once per day.
11
More complex investment schemes are possible than
what has been described here. We choose a simple scheme
Market History Baseline
DNC Clark 20 13
Clinton 38 -8
Dean 23 24
Gephardt 8 1
Kerry -6 6
Lieberman 3 2
General Kerry 2 15
Bush 21 20
Average (% omniscience) 13.6 9.1
Table 2: Results using history features for predic-
tion compared with a baseline system that invests
according to the previous day?s result.
normalized for comparison across markets using
the maximum profit obtainable by an omniscient
system that always predicts correctly.
Baseline systems for both news and market his-
tory are included. The news baseline follows the
spirit of a study of the French presidential elec-
tion (V?eronis, 2007), which showed that candidate
mentions correlate to electoral success. Attempts
to follow this method directly ? predicting mar-
ket movement based on raw candidate mentions ?
did very poorly. Instead, we trained our learning
system with features representing daily mention
counts of each entity. For a market history base-
line, we make a simple assumption about market
behavior: the current market trend will continue,
predict today?s behavior for tomorrow.
There were too many features to learn in the
short duration of the markets so only features that
appeared at least 20 times were included, reduc-
ing bag-of-words features from 88.8k to 28.3k and
parsing features from 1150k to 15.9k. A real world
system could use online feature selection.
6.1 Results
First, we establish performance without news in-
formation by testing the market history system
alone. Table 2 shows the profit of the history pre-
diction and baseline systems. While learning beats
the rule based system on average, both earn im-
pressive profits considering that random trading
would break even. These results corroborate the
inefficient market observation of Pennock et al
(2000). Additionally, the general election markets
sometimes both increased or decreased, an impos-
sible result in an efficient zero-sum market.
to make the evaluation more transparent.
477
Figure 1: Results for the different news features and combined system across five markets. Bottom
bars can be compared to evaluate news components and combined with the stacked black bars (history
system) give combined performance. The average performance (far right) shows improved performance
from each news system over the market history system.
During initial news evaluations with the com-
bined system, the primary election markets did ei-
ther very poorly or quite well. The news predic-
tion component lost money for Clinton, Gephardt,
and Lieberman while Clark, Dean and Kerry all
made money. Readers familiar with the 2004 elec-
tion will immediately see the difference between
the groups. The first three candidates were minor
contenders for the nomination and were not news-
makers. Hillary Clinton never even declared her
candidacy. The average number of mentions per
day for these candidates in our data was 20. In con-
trast, the second group were all major contenders
for the nomination and an average mention of 94 in
our data. Clearly, the news system can only do well
when it observes news that effects the market. The
system does well on both general election markets
where the average candidate mention per day was
503. Since the Clinton, Gephardt and Lieberman
campaigns were not newsworthy, they are omitted
from the results.
Results for news based prediction systems are
shown in figure 1. The figure shows the profit
made from both news features (bottom bars) and
market history (top black bars) when evaluated as
a combined system. Bottom bars can be compared
to evaluate news systems and each is combined
with its top bar to indicate total performance. Neg-
ative bars indicate negative earnings (i.e. weighted
accuracy below 50%). Averages across all mar-
kets for the news systems and the market history
system are shown on the right. In each market,
the baseline news system makes a small profit, but
the overall performance of the combined system is
worse than the market history system alone, show-
ing that the news baseline is ineffective. However,
all news features improve over the market history
system; news information helps to explain market
behaviors. Additionally, each more advanced set
of news features improves, with dependency fea-
tures yielding the best system in a majority of mar-
kets. The dependency system was able to learn
more complex interactions between words in news
articles. As an example, the system learns that
when Kerry is the subject of ?accused? his price in-
creases but decreased when he is the object. Sim-
ilarly, when ?Bush? is the subject of ?plans? (i.e.
Bush is making plans), his price increased. But
when he appears as a modifier of the plural noun
?plans? (comments about Bush policies), his price
falls. Earning profit indicates that our systems
were able to correctly forecast changes in public
opinion from objective news text.
The combined system proved an effective way
of modeling the market with both information
sources. Figure 2 shows the profits of the depen-
dency news system, the market history system, and
the combined system?s profits and decision on two
segments from the Kerry DNC market. In the first
segment, the history system predicts a downward
trend in the market (increasing profit) and the sec-
ond segment shows the final days of the market,
where Kerry was winning primaries and the news
system correctly predicted a market increase.
V?eronis (2007) observed a connection between
electoral success and candidate mentions in news
media. The average daily mentions in the general
election was 520 for Bush (election winner) and
478
485 for Kerry. However, for the three major DNC
candidates, Dean had 183, Clark 56 and Kerry
(election winner) had the least at 43. Most Kerry
articles occurred towards the end of the race when
it was clear he would win, while early articles fo-
cused on the early leader Dean. Also, news activity
did not indicate market movement direction; me-
dian candidate mentions for a positive market day
was 210 and 192 for a negative day.
Dependency news system accuracy was corre-
lated with news activity. On days when the news
component was correct ? although not always cho-
sen ? there were 226 median candidate mentions
compared to 156 for incorrect days. Additionally,
the system was more successful at predicting neg-
ative days. While days for which it was incorrect
the market moved up or down equally, when it was
correct and selected it predicted buy 42% of the
time and sell 58%, indicating that the system bet-
ter tracked negative news impacts.
7 Related Work
Many studies have examined the effects of news on
financial markets. Koppel and Shtrimberg (2004)
found a low correlation between news and the
stock market, likely because of the extreme effi-
ciency of the stock market (Gid?ofalvi, 2001). Two
studies reported success but worked with a very
small time granularity (10 minutes) (Lavrenko et
al., 2000; Mittermayer and Knolmayer, 2006). It
appears that neither system accounts for the time-
series nature of news during learning, instead us-
ing cross-validation experiments which is unsuit-
able for evaluation of time-series data. Our own
preliminary cross-validation experiments yielded
much better results than chronological evaluation
since the system trains using future information,
and with much more training data than is actu-
ally available for most days. Recent work has ex-
amined prediction market behavior and underlying
principles (Serrano-Padial, 2007).
12
Pennock et
al. (2000) found that prediction markets are some-
what efficient and some have theorized that news
could predict these markets, which we have con-
firmed (Debnath et al, 2003; Pennock et al, 2001;
Servan-Schreiber et al, 2004).
Others have explored the concurrent modeling
of text corpora and time series, such as using stock
market data and language modeling to identify
12
For a sample of the literature on prediction markets, see
the proceedings of the recent Prediction Market workshops
(http://betforgood.com/events/pm2007/index.html).
Figure 2: Two selections from the Kerry DNC mar-
ket showing profits over time (days) for depen-
dency news, history and combined systems. Each
day?s chosen system is indicated by the bottom
stripe as red (upper) for news, blue (lower) for his-
tory, and black for ties.
influential news stories (Lavrenko et al, 2000).
Hurst and Nigam (2004) combined syntactic and
semantic information for text polarity extraction.
Our task is related to but distinct from sentiment
analysis, which focuses on judgments in opin-
ions and, recently, predictions given by opinions.
Specifically, Kim and Hovy (2007) identify which
political candidate is predicted to win by an opin-
ion posted on a message board and aggregate opin-
ions to correctly predict an election result. While
the domain and some techniques are similar to our
own, we deal with fundamentally different prob-
lems. We do not consider opinions but instead ana-
lyze objective news to learn events that will impact
opinions. Opinions express subjective statements
about elections whereas news reports events. We
use public opinion as a measure of an events im-
pact. Additionally, they use generalized features
similar to our own identification of entities by re-
placing (a larger set of) known entities with gen-
eralized terms. In contrast, we use syntactic struc-
tures to create generalized ngram features. Note
that our features (table 1) do not indicate opinions
in contrast to the Kim and Hovy features. Finally,
Kim and Hovy had a batch setting to predict elec-
tion winners while we have a time-series setting
that tracked daily public opinion of candidates.
8 Conclusion and Future Work
We have presented a system for forecasting public
opinion about political candidates using news me-
479
dia. Our results indicate that computational sys-
tems can process media reports and learn which
events impact political candidates. Additionally,
the system does better when the candidate appears
more frequently and for negative events. A news
source analysis could reveal which outlets most in-
fluence public opinion. A feature analysis could
reveal which events trigger public reactions. While
these results and analyses have significance for po-
litical analysis they could extend to other genres,
such as financial markets. We have shown that fea-
ture extraction using syntactic parses can general-
ize typical bag-of-word features and improve per-
formance, a non-trivial result as dependency parses
contain significant errors and can limit the selec-
tion of words. Also, combining the internal mar-
ket baseline with a news system improved perfor-
mance, suggesting that forecasting future public
opinions requires a combination of new informa-
tion and continuing trends, neither of which can be
captured by the other.
References
Debnath, S., D. M. Pennock, C. L. Giles, and
S. Lawrence. 2003. Information incorporation in
online in-game sports betting markets. In Electronic
Commerce.
Devitt, Ann and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Association for Computational
Linguistics (ACL).
Forsythe, R., T.A. Rietz, , and T.W. Ross. 1999.
Wishes, expectations, and actions: A survey on price
formation in election stock markets. Journal of Eco-
nomic Behavior and Organization, 39:83?110.
Gid?ofalvi, G. 2001. Using news articles to predict
stock price movements. Technical report, Univ. of
California San Diego, San Diego.
Hurst, Matthew and Kamal Nigam. 2004. Retrieving
topical sentiments from online document collections.
In Document Recognition and Retrieval XI.
Jank, Wolfgang and Natasha Foutz. 2007. Using vir-
tual stock exchanges to forecast box-office revenue
via functional shape analysis. In The Prediction
Markets Workshop at Electronic Commerce.
Kim, Soo-Min and Eduard Hovy. 2007. Crystal: Ana-
lyzing predictive opinions on the web. In Empirical
Methods in Natural Language Processing (EMNLP).
Koppel, M. and I. Shtrimberg. 2004. Good news or
bad news? let the market decide. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Lavrenko, V., M. Schmill, D. Lawrie, P. Ogilvie,
D. Jensen, and J. Allan. 2000. Mining of concur-
rent text and time series. In KDD.
Littlestone, Nick and Manfred K. Warmuth. 1989. The
weighted majority algorithm. In IEEE Symposium
on Foundations of Computer Science.
McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency parsing with a two-stage dis-
criminative parser. In Conference on Natural Lan-
guage Learning (CoNLL).
Mittermayer, M. and G. Knolmayer. 2006. News-
CATS: A news categorization and trading system. In
International Conference in Data Mining.
Pennock, D. M., S. Lawrence, C. L. Giles, and F. A.
Nielsen. 2000. The power of play: Efficiency and
forecast accuracy in web market games. Technical
Report 2000-168, NEC Research Institute.
Pennock, D. M., S. Lawrence, F. A. Nielsen, and C. L.
Giles. 2001. Extracting collective probabilistic fore-
casts from web games. In KDD.
Serrano-Padial, Ricardo. 2007. Strategic foundations
of prediction markets and the efficient markets hy-
pothesis. In The Prediction Markets Workshop at
Electronic Commerce.
Servan-Schreiber, E., J. Wolfers, D. M. Pennock, and
B. Galebach. 2004. Prediction markets: Does
money matter? Electronic Markets, 14.
Toutanova, K., D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL.
V?eronis, Jean. 2007. La presse a fait mieux que les
sondeurs. http://aixtal.blogspot.com/2007/04/2007-
la-presse-fait-mieux-que-les.html.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. LREC, 39:165?210.
Williams, L.V. 1999. Information efficiency in betting
markets: A survey. Bulletin of Economic Research,
51:1?30.
Wolfers, J. and E. Zitzewitz. 2004. Prediction markets.
Journal of Economic Perspectives, 18(2):107?126.
480
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 514?522,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Sentiment Summarization: Evaluating and Learning User Preferences
Kevin Lerman
Columbia University
New York, NY
klerman@cs.columbia.edu
Sasha Blair-Goldensohn
Google, Inc.
New York, NY
sasha@google.com
Ryan McDonald
Google, Inc.
New York, NY
ryanmcd@google.com
Abstract
We present the results of a large-scale,
end-to-end human evaluation of various
sentiment summarization models. The
evaluation shows that users have a strong
preference for summarizers that model
sentiment over non-sentiment baselines,
but have no broad overall preference be-
tween any of the sentiment-based models.
However, an analysis of the human judg-
ments suggests that there are identifiable
situations where one summarizer is gener-
ally preferred over the others. We exploit
this fact to build a new summarizer by
training a ranking SVM model over the set
of human preference judgments that were
collected during the evaluation, which re-
sults in a 30% relative reduction in error
over the previous best summarizer.
1 Introduction
The growth of the Internet as a commerce
medium, and particularly the Web 2.0 phe-
nomenon of user-generated content, have resulted
in the proliferation of massive numbers of product,
service and merchant reviews. While this means
that users have plenty of information on which to
base their purchasing decisions, in practice this is
often too much information for a user to absorb.
To alleviate this information overload, research on
systems that automatically aggregate and summa-
rize opinions have been gaining interest (Hu and
Liu, 2004a; Hu and Liu, 2004b; Gamon et al,
2005; Popescu and Etzioni, 2005; Carenini et al,
2005; Carenini et al, 2006; Zhuang et al, 2006;
Blair-Goldensohn et al, 2008).
Evaluating these systems has been a challenge,
however, due to the number of human judgments
required to draw meaningful conclusions. Of-
ten systems are evaluated piecemeal, selecting
pieces that can be evaluated easily and automati-
cally (Blair-Goldensohn et al, 2008). While this
technique produces meaningful evaluations of the
selected components, other components remain
untested, and the overall effectiveness of the entire
system as a whole remains unknown. When sys-
tems are evaluated end-to-end by human judges,
the studies are often small, consisting of only a
handful of judges and data points (Carenini et
al., 2006). Furthermore, automated summariza-
tion metrics like ROUGE (Lin and Hovy, 2003)
are non-trivial to adapt to this domain as they re-
quire human curated outputs.
We present the results of a large-scale, end-to-
end human evaluation of three sentiment summa-
rization models applied to user reviews of con-
sumer products. The evaluation shows that there
is no significant difference in rater preference be-
tween any of the sentiment summarizers, but that
raters do prefer sentiment summarizers over non-
sentiment baselines. This indicates that even sim-
ple sentiment summarizers provide users utility.
An analysis of the rater judgments also indicates
that there are identifiable situations where one sen-
timent summarizer is generally preferred over the
others. We attempt to learn these preferences by
training a ranking SVM that exploits the set of
preference judgments collected during the evalu-
ation. Experiments show that the ranking SVM
summarizer?s cross-validation error decreases by
as much as 30% over the previous best model.
Human evaluations of text summarization have
been undertaken in the past. McKeown et al
(2005) presented a task-driven evaluation in the
news domain in order to understand the utility of
different systems. Also in the news domain, the
Document Understanding Conference1 has run a
number of multi-document and query-driven sum-
marization shared-tasks that have used a wide
1http://duc.nist.gov/
514
iPod Shuffle: 4/5 stars
?In final analysis the iPod Shuffle is a decent player that offers a sleek
compact form factor an excessively simple user interface and a low
price? ... ?It?s not good for carrying a lot of music but for a little bit of
music you can quickly grab and go with this nice little toy? ... ?Mine came
in a nice bright orange color that makes it easy to locate.?
Figure 1: An example summary.
range of automatic and human-based evaluation
criteria. This year, the new Text Analysis Con-
ference2 is running a shared-task that contains an
opinion component. The goal of that evaluation is
to summarize answers to opinion questions about
entities mentioned in blogs.
Our work most closely resembles the evalua-
tions in Carenini et al (2006, 2008). Carenini et
al. (2006) had raters evaluate extractive and ab-
stractive summarization systems. Mirroring our
results, they show that both extractive and abstrac-
tive summarization outperform a baseline, but that
overall, humans have no preference between the
two. Again mirroring our results, their analysis in-
dicates that even though there is no overall differ-
ence, there are situations where one system gener-
ally outperforms the other. In particular, Carenini
and Cheung (2008) show that an entity?s contro-
versiality, e.g., mid-range star rating, is correlated
with which summary has highest value.
The study presented here differs from Carenini
et al in many respects: First, our evaluation is
over different extractive summarization systems in
an attempt to understand what model properties
are correlated with human preference irrespective
of presentation; Secondly, our evaluation is on a
larger scale including hundreds of judgments by
hundreds of raters; Finally, we take a major next
step and show that it is possible to automatically
learn significantly improved models by leveraging
data collected in a large-scale evaluation.
2 Sentiment Summarization
A standard setting for sentiment summarization
assumes a set of documents D = {d1, . . . , dm}
that contain opinions about some entity of interest.
The goal of the system is to generate a summary S
of that entity that is representative of the average
opinion and speaks to its important aspects. An
example summary is given in figure 1. For sim-
plicity we assume that all opinions in D are about
the entity being summarized. When this assump-
tion fails, one can parse opinions at a finer-level
2http://www.nist.gov/tac/
(Jindal and Liu, 2006; Stoyanov and Cardie, 2008)
In this study, we look at an extractive summa-
rization setting where S is built by extracting rep-
resentative bits of text from the set D, subject to
pre-specified length constraints. Specifically, as-
sume each document di is segmented into can-
didate text excerpts. For ease of discussion we
will assume all excerpts are sentences, but in prac-
tice they can be phrases or multi-sentence groups.
Viewed this way, D is a set of candidate sentences
for our summary, D = {s1, . . . , sn}, and summa-
rization becomes the following optimization:
argmax
S?D
L(S) s.t.: LENGTH(S) ? K (1)
where L is some score over possible summaries,
LENGTH(S) is the length of the summary and K
is the pre-specified length constraint. The defini-
tion of L will be the subject of much of this sec-
tion and it is precisely different forms of L that
will be compared in our evaluation. The nature of
LENGTH is specific to the particular use case.
Solving equation 1 is typically NP-hard, even
under relatively strong independence assumptions
between the sentences selected for the summary
(McDonald, 2007). In cases where solving L is
non-trivial we use an approximate hill climbing
technique. First we randomly initialize the sum-
mary S to length ?K. Then we greedily in-
sert/delete/swap sentences in and out of the sum-
mary to maximize L(S) while maintaining the
bound on length. We run this procedure until no
operation leads to a higher scoring summary. In
all our experiments convergence was quick, even
when employing random restarts.
Alternate formulations of sentiment summa-
rization are possible, including aspect-based sum-
marization (Hu and Liu, 2004a), abstractive sum-
marization (Carenini et al, 2006) or related tasks
such as opinion attribution (Choi et al, 2005). We
choose a purely extractive formulation as it makes
it easier to develop baselines and allows raters to
compare summaries with a simple, consistent pre-
sentation format.
2.1 Definitions
Before delving into the details of the summariza-
tion models we must first define some useful func-
tions. The first is the sentiment polarity func-
tion that maps a lexical item t, e.g., word or short
phrase, to a real-valued score,
LEX-SENT(t) ? [?1, 1]
515
The LEX-SENT function maps items with positive
polarity to higher values and items with negative
polarity to lower values. To build this function we
constructed large sentiment lexicons by seeding a
semantic word graph induced from WordNet with
positive and negative examples and then propagat-
ing this score out across the graph with a decaying
confidence. This method is common among sen-
timent analysis systems (Hu and Liu, 2004a; Kim
and Hovy, 2004; Blair-Goldensohn et al, 2008).
In particular, we use the lexicons that were created
and evaluated by Blair-Goldensohn et al (2008).
Next we define sentiment intensity,
INTENSITY(s) =
?
t?s
|LEX-SENT(t)|
which simply measures the magnitude of senti-
ment in a sentence. INTENSITY can be viewed as a
measure of subjectiveness irrespective of polarity.
A central function in all our systems is a sen-
tences normalized sentiment,
SENT(s) =
?
t?s LEX-SENT(t)
?+ INTENSITY(s)
This function measures the (signed) ratio of lexical
sentiment to intensity in a sentence. Sentences that
only contain lexical items of the same polarity will
have high absolute normalized sentiment, whereas
sentences with mixed polarity items or no polar-
ity items will have a normalized sentiment near
zero. We include the constant ? in the denomi-
nator so that SENT gives higher absolute scores to
sentences containing many strong sentiment items
of the same polarity over sentences with a small
number of weak items of the same polarity.
Most sentiment summarizers assume that as in-
put, a system is given an overall rating of the en-
tity it is attempting to summarize, R ? [?1, 1],
where a higher rating indicates a more favorable
opinion. This rating may be obtained directly from
user provided information (e.g., star ratings) or au-
tomatically derived by averaging the SENT func-
tion over all sentences in D. Using R, we can de-
fine a mismatch function between the sentiment of
a summary and the known sentiment of the entity,
MISMATCH(S) = (R?
1
|S|
?
si?S
SENT(si))
2
Summaries with a higher mismatch are those
whose sentiment disagrees most with R.
Another key input many sentiment summarizers
assume is a list of salient entity aspects, which are
specific properties of an entity that people tend to
rate when expressing their opinion. For example,
aspects of a digital camera could include picture
quality, battery life, size, color, value, etc. Find-
ing such aspects is a challenging research problem
that has been addressed in a number of ways (Hu
and Liu, 2004b; Gamon et al, 2005; Carenini et
al., 2005; Zhuang et al, 2006; Branavan et al,
2008; Blair-Goldensohn et al, 2008; Titov and
McDonald, 2008b; Titov and McDonald, 2008a).
We denote the set of aspects for an entity as A and
each aspect as a ? A. Furthermore, we assume
that given A it is possible to determine whether
some sentence s ? D mentions an aspect in A.
For our experiments we use a hybrid supervised-
unsupervised method for finding aspects as de-
scribed and evaluated in Blair-Goldensohn et al
(2008).
Having defined what an aspect is, we next de-
fine a summary diversity function over aspects,
DIVERSITY(S) =
?
a?A
COVERAGE(a)
where COVERAGE(a) ? R is a function that
weights how well the aspect is covered in the
summary and is proportional to the importance of
the aspect as some aspects are more important to
cover than others, e.g., ?picture quality? versus
?strap? for digital cameras. The diversity func-
tion rewards summaries that cover many important
aspects and plays the redundancy reducing role
that is common in most extractive summarization
frameworks (Goldstein et al, 2000).
2.2 Systems
For our evaluation we developed three extractive
sentiment summarization systems. Each system
models increasingly complex objectives.
2.2.1 Sentiment Match (SM)
The first system that we look at attempts to ex-
tract sentences so that the average sentiment of the
summary is as close as possible to the entity level
sentiment R, which was previously defined in sec-
tion 2.1. In this case L can be simply defined as,
L(S) = ?MISMATCH(S)
Thus, the model prefers summaries with average
sentiment as close as possible to the average sen-
timent across all the reviews.
516
There is an obvious problem with this model.
For entities that have a mediocre rating, i.e., R ?
0, the model could prefer a summary that only
contains sentences with no opinion whatsoever.
There are two ways to alleviate this problem. The
first is to include the INTENSITY function into L,
L(S) = ? ? INTENSITY(S)? ? ? MISMATCH(S)
Where the coefficients allow one to trade-off sen-
timent intensity versus sentiment mismatch.
The second method, and the one we chose based
on initial experiments, was to address the problem
at inference time. This is done by prohibiting the
algorithm from including a given positive or nega-
tive sentence in the summary if another more pos-
itive/negative sentence is not included. Thus the
summary is forced to consist of only the most pos-
itive and most negative sentences, the exact mix
being dependent upon the overall star rating.
2.2.2 Sentiment Match + Aspect Coverage
(SMAC)
The SM model extracts sentences for the summary
without regard to the content of each sentence rel-
ative to the others in the summary. This is in con-
trast to standard summarization models that look
to promote sentence diversity in order to cover as
many important topics as possible (Goldstein et
al., 2000). The sentiment match + aspect cov-
erage system (SMAC) attempts to model diver-
sity by building a summary that trades-off max-
imally covering important aspects with matching
the overall sentiment of the entity. The model does
this through the following linear score,
L(S) = ? ? INTENSITY(S)? ? ? MISMATCH(S)
+? ? DIVERSITY(S)
This score function rewards summaries for be-
ing highly subjective (INTENSITY), reflecting the
overall product rating (MISMATCH), and covering
a variety of product aspects (DIVERSITY). The co-
efficients were set by inspection.
This system has its roots in event-based summa-
rization (Filatova and Hatzivassiloglou, 2004) for
the news domain. In that work an optimization
problem was developed that attempted to maxi-
mize summary informativeness while covering as
many (weighted) sub-events as possible.
2.2.3 Sentiment-Aspect Match (SAM)
Because the SMAC model only utilizes an entity?s
overall sentiment when calculating MISMATCH, it
is susceptible to degenerate solutions. Consider a
product with aspects A and B, where reviewers
overwhelmingly like A and dislike B, resulting in
an overall SENT close to zero. If the SMAC model
finds a very negative sentence describing A and
a very positive sentence describing B, it will as-
sign that summary a high score, as the summary
has high intensity, has little overall mismatch, and
covers both aspects. However, in actuality, the
summary is entirely misleading.
To address this issue, we constructed the
sentiment-aspect match model (SAM), which not
only attempts to cover important aspects, but cover
them with appropriate sentiment. There are many
ways one might design a model to do this, includ-
ing linear combinations of functions similar to the
SMAC model. However, we decided to employ a
probabilistic approach as it provided performance
benefits based on development data experiments.
Under the SAM model, each sentence is treated as
a bag of aspects and their corresponding mentions?
sentiments. For a given sentence s, we define As
as the set of aspects mentioned within it. For a
given aspect a ? As, we denote SENT(as) as the
sentiment associated with the textual mention of a
in s. The probability of a sentence is defined as,
p(s) = p(a1, . . . , an, SENT(a1s), . . . , SENT(a
n
s ))
which can be re-written as,
?
a?As
p(a, SENT(as)) =
?
a?As
p(a)p(SENT(as)|a)
if we assume aspect mentions are generated inde-
pendently of one another. Thus we need to esti-
mate both p(a) and p(SENT(as)|a). The probabil-
ity of seeing an aspect, p(a), is simply set to the
maximum likelihood estimates over the data set
D. Furthermore, we assume that p(SENT(as)|a)
is normal about the mean sentiment for the as-
pect ?a with a constant standard deviation, ?a.
The mean and standard deviation are estimated
straight-forwardly using the data set D. Note that
the number of parameters our system must es-
timate is very small. For every possible aspect
a ? A we need three values: p(a), ?a, and ?a.
Since |A| is typically small ? on the order of 5-10
? it is not difficult to estimate these models even
from small sets of data.
Having constructed this model, one logical ap-
proach to summarization would be to select sen-
tences for the summary that have highest proba-
bility under the model trained on D. We found,
517
however, that this produced very redundant sum-
maries ? if one aspect is particularly prevalent in
a product?s reviews, this approach will select all
sentences about that aspect, and discuss nothing
else. To combat this we developed a technique that
scores the summary as a whole, rather than by in-
dividual components. First, denote SAM(D) as the
previously described model learned over the set of
entity documents D. Next, denote SAM(S) as an
identical model, but learned over a candidate sum-
mary S, i.e., given a summary S, compute p(a),
ma, and ?a for all a ? A using only the sentences
from S. We can then measure the difference be-
tween these models using KL-divergence:
L(S) = ?KL(SAM(D), SAM(S))
In our case we have 1 + |A| distributions ? p(a),
and p(?|a) for all a ? A ? so we just sum the KL-
divergence of each. The key property of the SAM
system is that it naturally builds summaries where
important aspects are discussed with appropriate
sentiment, since it is precisely these aspects that
will contribute the most to the KL-divergence. It
is important to note that the short length of a can-
didate summary S can make estimates in SAM(S)
rather crude. But we only care about finding the
?best? of a set of crude models, not about finding
one that is ?good? in absolute terms. Between the
few parameters we must learn and the specific way
we use these models, we generally get models use-
ful for our purposes.
Alternatively we could have simply incorpo-
rated the DIVERSITY measure into the objec-
tive function or used an inference algorithm that
specifically accounts for redundancy, e.g., maxi-
mal marginal relevance (Goldstein et al, 2000).
However, we found that this solution was well
grounded and required no tuning of coefficients.
Initial experiments indicated that the SAM sys-
tem, as described above, frequently returned sen-
tences with low intensity when important aspects
had luke-warm sentiment. To combat this we re-
moved low intensity sentences from consideration,
which had the effect of encouraging important
luke-warm aspects to mentioned multiple times in
order to balance the overall sentiment.
Though the particulars of this model are unique,
fundamentally it is closest to the work of Hu and
Liu (2004a) and Carenini et al (2006).
3 Experiments
We evaluated summary performance for reviews
of consumer electronics. In this setting an entity
to be summarized is one particular product, D is
a set of user reviews about that product, and R is
the normalized aggregate star ratings left by users.
We gathered reviews for 165 electronics products
from several online review aggregators. The prod-
ucts covered a variety of electronics, such as MP3
players, digital cameras, printers, wireless routers,
and video game systems. Each product had a min-
imum of four reviews and up to a maximum of
nearly 3000. The mean number of reviews per
product was 148, and the median was 70. We
ran each of our algorithms over the review corpus
and generated summaries for each product with
K = 650. All summaries were roughly equal
length to avoid length-based rater bias3. In total
we ran four experiments for a combined number of
1980 rater judgments (plus additional judgments
during the development phase of this study).
Our initial set of experiments were over the
three opinion-based summarization systems: SM,
SMAC, and SAM. We ran three experiments com-
paring SMAC to SM, SAM to SM, and SAM to
SMAC. In each experiment two summaries of the
same product were placed side-by-side in a ran-
dom order. Raters were also shown an overall rat-
ing, R, for each product (these ratings are often
provided in a form such as ?3.5 of 5 stars?). The
two summaries on either side were shown below
this information with links to the full text of the
reviews for the raters to explore.
Raters were asked to express their preference
for one summary over the other. For two sum-
maries SA and SB they could answer,
1. No preference
2. Strongly preferred SA (or SB)
3. Preferred SA (or SB)
4. Slightly preferred SA (or SB)
Raters were free to choose any rating, but were
specifically instructed that their rating should ac-
count for a summaries representativeness of the
overall set of reviews. Raters were also asked
to provide a brief comment justifying their rat-
ing. Over 100 raters participated in each study,
and each comparison was evaluated by three raters
with no rater making more than five judgments.
3In particular our systems each extracted four text ex-
cerpts of roughly 160-165 characters.
518
Comparison (A v B) Agreement (%) No Preference (%) Preferred A (%) Preferred B (%) Mean Numeric
SM v SMAC 65.4 6.0 52.0 42.0 0.01
SAM v SM 69.3 16.8 46.0 37.2 0.01
SAM v SMAC? 73.9 11.5 51.6 36.9 0.08
SMAC v LT? 64.1 4.1 70.4 25.5 0.24
Table 1: Results of side-by-side experiments. Agreement is the percentage of items for which all raters
agreed on a positive/negative/no-preference rating. No Preference is the percentage of agreement items
in which the raters had no preference. Preferred A/B is the percentage of agreement items in which the
raters preferred either A or B respectively. Mean Numeric is the average of the numeric ratings (converted
from discreet preference decisions) indicating on average the raters preferred system A over B on a scale
of -1 to 1. Positive scores indicate a preference for system A. ? significant at a 95% confidence interval
for the mean numeric score.
We chose to have raters leave pairwise prefer-
ences, rather than evaluate each candidate sum-
mary in isolation, because raters can make a pref-
erence decisions more quickly than a valuation
judgment, which allowed for collection of more
data points. Furthermore, there is evidence that
rater agreement is much higher in preference deci-
sions than in value judgments (Ariely et al, 2008).
Results are shown in the first three rows of ta-
ble 1. The first column of the table indicates the
experiment that was run. The second column indi-
cates the percentage of judgments for which the
raters were in agreement. Agreement here is a
weak agreement, where three raters are defined to
be in agreement if they all gave a no preference rat-
ing, or if there was a preference rating, but no two
preferences conflicted. The next three columns in-
dicate the percentage of judgments for each pref-
erence category, grouped here into three coarse as-
signments. The final column indicates a numeric
average for the experiment. This was calculated
by converting users ratings to a scale of 1 (strongly
preferred SA) to -1 (strongly preferred SB) at 0.33
intervals. Table 1 shows only results for items in
which the raters had agreement in order to draw
reliable conclusions, though the results change lit-
tle when all items are taken into account.
Ultimately, the results indicate that none of the
sentiment summarizers are strongly preferred over
any other. Only the SAM v SMAC model has a
difference that can be considered statistically sig-
nificant. In terms of order we might conclude that
SAM is the most preferred, followed by SM, fol-
lowed by SMAC. However, the slight differences
make any such conclusions tenuous at best. This
leads one to wonder whether raters even require
any complex modeling when summarizing opin-
ions. To test this we took the lowest scoring model
overall, SMAC, and compared it to a leading text
baseline (LT) that simply selects the first sentence
from a ranked list of reviews until the length con-
straint is violated. The results are given in the last
row of 1. Here there is a clear distinction as raters
preferred SMAC to LT, indicating that they did
find usefulness in systems that modeled aspects
and sentiment. However, there are still 25.5%
of agreement items where the raters did choose a
simple leading text baseline.
4 Analysis
Looking more closely at the results we observed
that, even though raters did not strongly prefer
any one sentiment-aware summarizer over another
overall, they mostly did express preferences be-
tween systems on individual pairs of comparisons.
For example, in the SAM vs SM experiment, only
16.8% of the comparisons yielded a ?no prefer-
ence? judgment from all three raters ? by far the
highest percentage of any experiment. This left
83.2% ?slight preference? or higher judgments.
With this in mind we began examining the com-
ments left by raters throughout all our experi-
ments, including a set of additional experiments
used during development of the systems. We ob-
served several trends: 1) Raters tended to pre-
fer summaries with lists, e.g., pros-cons lists; 2)
Raters often did not like text without sentiment,
hence the dislike of the leading text system where
there is no guarantee that the first sentence will
have any sentiment; 3) Raters disliked overly gen-
eral comments, e.g., ?The product was good?.
These statements carry no additional information
over a product?s overall star rating; 4) Raters did
recognize (and strongly disliked) when the overall
sentiment of the summary was inconsistent with
the star rating; 5) Raters tended to prefer different
519
systems depending on what the star rating was. In
particular, the SMAC system was generally pre-
ferred for products with neutral overall ratings,
whereas the SAM system is preferred for products
with ratings at the extremes. We hypothesize that
SAM?s low performance on neutral rated products
is because the system suffers from the dual imper-
atives of selecting high intensity snippets and of
selecting snippets that individually reflect partic-
ular sentiment polarities. When the desired senti-
ment polarity is neutral, it is difficult to find a snip-
pet with lots of sentiment, whose overall polarity
is still neutral, thus SAM may either ignore that
aspect or include multiple mentions of that aspect
at the expense of others; 6) Raters also preferred
summaries with grammatically fluent text, which
benefitted the leading text baseline.
These observations suggest that we could build
a new system that takes into account all these
factors (weighted accordingly) or we could build
a rule-based meta-classifier that selects a single
summary from the four systems described in this
paper based on the global characteristics of each.
The problem with the former is that it will require
hand-tuning of coefficients for many different sig-
nals that are all, for the most part, weakly corre-
lated to summary quality. The problem with the
latter is inefficiency, i.e., it will require the main-
tenance and output of all four systems. In the next
section we explore an alternate method that lever-
ages the data gathered in the evaluation to auto-
matically learn a new model. This approach is
beneficial as it will allow any coefficients to be au-
tomatically tuned and will result in a single model
that can be used to build new summaries.
5 Summarization with Ranking SVMs
Besides allowing us to assess the relative perfor-
mance of our summarizers, our evaluation pro-
duced several hundred points of empirical data in-
dicating which among two summaries raters pre-
fer. In this section we explore how to build im-
proved summarizers with this data by learning
preference ranking SVMs, which are designed to
learn relative to a set of preference judgments
(Joachims, 2002).
A ranking SVM typically assumes as input a set
of queries and associated partial ordering on the
items returned by the query. The training data is
defined as pairs of points, T = {(xki , x
k
j )t}
|T |
t=1,
where each pair indicates that the ith item is pre-
ferred over the jth item for the kth query. Each
input point xki ? R
m is a feature vector repre-
senting the properties of that particular item rel-
ative to the query. The goal is to learn a scoring
function s(xki ) ? R such that s(x
k
i ) > s(x
k
j ) if
(xki , x
k
j ) ? T . In other words, a ranking SVM
learns a scoring function whose induced ranking
over data points respects all preferences in the
training data. The most straight-forward scoring
function, and the one used here, is a linear classi-
fier, s(xki ) = w ? x
k
i , making the goal of learning
to find an appropriate weight vector w ? Rm.
In its simplest form, the ranking SVM opti-
mization problem can be written as the following
quadratic programming problem,
min
1
2
||w||2 s.t.: ?(xki , x
k
j ) ? T ,
s(xki )? s(x
k
j ) ? PREF(x
k
i , x
k
j )
where PREF(xki , x
k
j ) ? R is a function indicating
to what degree item xki is preferred over x
k
j (and
serves as the margin of the classifier). This opti-
mization is well studied and can be solved with a
wide variety of techniques. In our experiments we
used the SVM-light software package4.
Our summarization evaluation provides us with
precisely a large collection of preference points
over different summaries for different product
queries. Thus, we naturally have a training set T
where each query is analogous to a specific prod-
uct of interest and training points are two possi-
ble summarizations produced by two different sys-
tems with corresponding rater preferences. As-
suming an appropriate choice of feature represen-
tation it is straight-forward to then train the model
on our data using standard techniques for SVMs.
To train and test the model we compiled 1906
pairs of summary comparisons, each judged by
three different raters. These pairs were extracted
from the four experiments described in section 3
as well as the additional experiments we ran dur-
ing development. For each pair of summaries
(Ski , S
k
j ) (for some product query indexed by k),
we recorded how many raters preferred each of the
items as vki and v
k
j respectively, i.e., v
k
i is the num-
ber of the three raters who preferred summary Si
over Sj for product k. Note that vki + v
k
j does not
necessarily equal 3 since some raters expressed no
preference between them. We set the loss function
PREF(Ski , S
k
j ) = v
k
i ? v
k
j , which in some cases
4http://svmlight.joachims.org/
520
could be zero, but never negative since the pairs
are ordered. Note that this training set includes all
data points, even those in which raters disagreed.
This is important as the model can still learn from
these points as the margin function PREF encodes
the fact that these judgments are less certain.
We used a variety of features for a candidate
summary: how much capitalization, punctuation,
pros-cons, and (unique) aspects a summary had;
the overall intensity, sentiment, min sentence sen-
timent, and max sentence sentiment in the sum-
mary; the overall ratingR of the product; and con-
junctions of these. Note that none of these fea-
tures encode which system produced the summary
or which experiment it was drawn from. This is
important, as it allows the model to be used as
standalone scoring function, i.e., we can set L to
the learned linear classifier s(S). Alternatively
we could have included features like what system
was the summary produced from. This would have
helped the model learn things like the SMAC sys-
tem is typically preferred for products with mid-
range overall ratings. Such a model could only be
used to rank the outputs of other summarizers and
cannot be used standalone.
We evaluated the trained model by measuring
its accuracy on predicting a single preference pre-
diction, i.e., given pairs of summaries (Ski , S
k
j ),
how accurate is the model at predicting that Si is
preferred to Sj for product query k? We measured
10-fold cross-validation accuracy on the subset of
the data for which the raters were in agreement.
We measure accuracy for both weak agreement
cases (at least one rater indicated a preference and
the other two raters were in agreement or had no
preference) and strong agreement cases (all three
raters indicated the same preference). We ignored
pairs in which all three raters made a no preference
judgment as both summaries can can be consid-
ered equally valid. Furthermore, we ignored pairs
in which two raters indicated conflicting prefer-
ences as there is no gold standard for such cases.
Results are given in table 2. We compare the
ranking SVM summarizer to a baseline system
that always selects the overall-better-performing
summarization system from the experiment that
the given datapoint was drawn from, e.g., for all
the data points drawn from the SAM versus SMAC
experiment, the baseline always chooses the SAM
summary as its preference. Note that in most ex-
periments the two systems emerged in a statistical
Preference Prediction Accuracy
Weak Agr. Strong Agr.
Baseline 54.3% 56.9%
Ranking SVM 61.8% 69.9%
Table 2: Accuracies for learned summarizers.
tie, so this baseline performs only slightly better
than chance. Table 2 clearly shows that the rank-
ing SVM can predict preference accuracy much
better than chance, and much better than that ob-
tained by using only one summarizer (a reduction
in error of 30% for strong agreement cases).
We can thus conclude that the data gathered
in human preference evaluation experiments, such
as the one presented here, have a beneficial sec-
ondary use as training data for constructing a new
and more accurate summarizer. This raises an
interesting line of future research: can we iter-
ate this process to build even better summariz-
ers? That is, can we use this trained summarizer
(and variants of it) to generate more examples for
raters to judge, and then use that data to learn even
more powerful summarizers, which in turn could
be used to generate even more training judgments,
etc. This could be accomplished using Mechani-
cal Turk5 or another framework for gathering large
quantities of cheap annotations.
6 Conclusions
We have presented the results of a large-scale eval-
uation of different sentiment summarization algo-
rithms. In doing so, we explored different ways
of using sentiment and aspect information. Our
results indicated that humans prefer sentiment in-
formed summaries over a simple baseline. This
shows the usefulness of modeling sentiment and
aspects when summarizing opinions. However,
the evaluations also show no strong preference be-
tween different sentiment summarizers. A detailed
analysis of the results led us to take the next step
in this line of research ? leveraging preference
data gathered in human evaluations to automati-
cally learn new summarization models. These new
learned models show large improvements in pref-
erence prediction accuracy over the previous sin-
gle best model.
Acknowledgements: The authors would like to
thank Kerry Hannan, Raj Krishnan, Kristen Parton
and Leo Velikovich for insightful discussions.
5http://www.mturk.com
521
References
D. Ariely, G. Loewenstein, and D. Prelec. 2008. Co-
herent arbitrariness: Stable demand curves without
stable preferences. The Quarterly Journal of Eco-
nomics, 118:73105.
S. Blair-Goldensohn, K. Hannan, R. McDonald,
T. Neylon, G.A. Reis, and J. Reynar. 2008. Building
a sentiment summarizer for local service reviews. In
WWW Workshop on NLP in the Information Explo-
sion Era.
S.R.K. Branavan, H. Chen, J. Eisenstein, and R. Barzi-
lay. 2008. Learning document-level semantic prop-
erties from free-text annotations. In Proceedings of
the Annual Conference of the Association for Com-
putational Linguistics (ACL).
G. Carenini and J. Cheung. 2008. Extractive vs. nlg-
based abstractive summarization of evaluative text:
The effect of corpus controversiality. In Interna-
tional Conference on Natural Language Generation
(INLG).
G. Carenini, R.T. Ng, and E. Zwart. 2005. Extract-
ing knowledge from evaluative text. In Proceedings
of the International Conference on Knowledge Cap-
ture.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-
document summarization of evaluative text. In Pro-
ceedings of the Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL).
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005.
Identifying sources of opinions with conditional ran-
dom fields and extraction patterns. In Proceedings
the Joint Conference on Human Language Technol-
ogy and Empirical Methods in Natural Language
Processing (HLT-EMNLP).
E. Filatova and V. Hatzivassiloglou. 2004. A formal
model for information selection in multi-sentence
text extraction. In Proceedings of the International
Conference on Computational Linguistics (COL-
ING).
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free
text. In Proceedings of the 6th International Sympo-
sium on Intelligent Data Analysis (IDA).
J. Goldstein, V. Mittal, J. Carbonell, and
M. Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In Proceedings
of the ANLP/NAACL Workshop on Automatic
Summarization.
M. Hu and B. Liu. 2004a. Mining and summariz-
ing customer reviews. In Proceedings of the Inter-
national Conference on Knowledge Discovery and
Data Mining (KDD).
M. Hu and B. Liu. 2004b. Mining opinion features in
customer reviews. In Proceedings of National Con-
ference on Artificial Intelligence (AAAI).
N. Jindal and B. Liu. 2006. Mining comprative sen-
tences and relations. In Proceedings of 21st Na-
tional Conference on Artificial Intelligence (AAAI).
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD).
S.M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of Conference on
Computational Linguistics (COLING).
C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram cooccurrence statistics.
In Proceedings of the Conference on Human Lan-
guage Technologies and the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL).
R. McDonald. 2007. A Study of Global Inference
Algorithms in Multi-document Summarization. In
Proceedings of the European Conference on Infor-
mation Retrieval (ECIR).
K. McKeown, R.J. Passonneau, D.K. Elson,
A. Nenkova, and J. Hirschberg. 2005. Do
Summaries Help? A Task-Based Evaluation of
Multi-Document Summarization. In Proceedings
of the ACM SIGIR Conference on Research and
Development in Information Retrieval.
A.M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Proceedings of
the Conference on Computational Linguistics (COL-
ING).
I. Titov and R. McDonald. 2008a. A joint model of
text and aspect ratings. In Proceedings of the An-
nual Conference of the Association for Computa-
tional Linguistics (ACL).
I. Titov and R. McDonald. 2008b. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the Annual World Wide Web Conference
(WWW).
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie re-
view mining and summarization. In Proceedings
of the International Conference on Information and
Knowledge Management (CIKM).
522
Proceedings of NAACL HLT 2009: Short Papers, pages 113?116,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Contrastive Summarization: An Experiment with Consumer Reviews
Kevin Lerman
Columbia University
New York, NY
klerman@cs.columbia.edu
Ryan McDonald
Google Inc.
New York, NY
ryanmcd@google.com
Abstract
Contrastive summarization is the problem of
jointly generating summaries for two entities
in order to highlight their differences. In this
paper we present an investigation into con-
trastive summarization through an implemen-
tation and evaluation of a contrastive opinion
summarizer in the consumer reviews domain.
1 Introduction
Automatic summarization has historically focused
on summarizing events, a task embodied in the
series of Document Understanding Conferences1.
However, there has also been work on entity-centric
summarization, which aims to produce summaries
from text collections that are relevant to a particu-
lar entity of interest, e.g., product, person, company,
etc. A well-known example of this is from the opin-
ion mining community where there has been a num-
ber of studies on summarizing the expressed senti-
ment towards entities (cf. Hu and Liu (2006)). An-
other recent example of entity-centric summariza-
tion is the work of Filippova et al (2009) to produce
company-specific financial report summaries.
In this study we investigate a variation of entity-
centric summarization where the goal is not to sum-
marize information about a single entity, but pairs
of entities. Specifically, our aim is to jointly gen-
erate two summaries that highlight differences be-
tween the entities ? a task we call contrastive sum-
marization. An obvious application comes from the
consumer reviews domain, where a person consider-
ing a purchase wishes to see the differences in opin-
ion about the top candidates without reading all the
reviews for each product. Other applications include
1http://duc.nist.gov/
contrasting financial news about related companies
or comparing platforms of political candidates.
Contrastive summarization has many points of
comparison in the NLP, IR and Data-Mining liter-
ature. Jindal and Liu (2006) introduce techniques
to find and analyze explicit comparison sentences,
but this assumes that such sentences exist. In con-
trastive summarization, there is no assumption that
two entities have been explicitly compared. The
goal is to automatically generate the comparisons
based on the data. In the IR community, Sun et
al. (2006) explores retrieval systems that align query
results to highlight points of commonality and dif-
ference. In contrast, we attempt to identify con-
trasts from the data, and then generate summaries
that highlight them. The novelty detection task of
determining whether a new text in a collection con-
tains information distinct from that already gathered
is also related (Soboroff and Harman, 2005). The
primary difference here is that contrastive summa-
rization aims to extract information from one col-
lection not present in the other in addition to infor-
mation present in both collections that highlights a
difference between the entities.
This paper describes a contrastive summarization
experiment where the goal is to generate contrasting
opinion summaries of two products based on con-
sumer reviews of each. We look at model design
choices, describe an implementation of a contrastive
summarizer, and provide an evaluation demonstrat-
ing a significant improvement in the usefulness of
contrastive summaries versus summaries generated
by single-product opinion summarizers.
2 Single-Product Opinion Summarization
As input we assume a set of relevant text excerpts
(typically sentences), T = {t1, . . . , tm}, which con-
113
tain opinions about some product of interest. The
goal of opinion summarization2 is to select some
number of text excerpts to form a summary S of
the product so that S is representative of the aver-
age opinion and speaks to its important aspects (also
proportional to opinion), which we can formalize as:
S = argmax
S?T
L(S) s.t. LENGTH(S) ? K
where L is some score over possible summaries that
embodies what a user might desire in an opinion
summary, LENGTH(S) is the length of the summary
and K is a pre-specified length constraint.
We assume the existence of standard sentiment
analysis tools to provide the information used in the
scoring function L. First, we assume the tools can
assign a sentiment score from -1 (negative) to 1 (pos-
itive) to an arbitrary span of text. Second, we as-
sume that we can extract a set of aspects that the text
is discussing (e.g, ?The sound was crystal clear? is
about the aspect sound quality). We refer the reader
to abundance of literature on sentiment analysis for
more details on how such tools can be constructed
(cf. Pang and Lee (2008)). For this study, we use
the tools described and evaluated in Lerman et al
(2009). We note however, that the subject of this
discussion is not the tools themselves, but their use.
The single product opinion summarizer we con-
sider is the Sentiment Aspect Match model (SAM)
described and evaluated in (Lerman et al, 2009).
Underlying SAM is the assumption that opinions
can be described by a bag-of-aspects generative pro-
cess where each aspect is generated independently
and the sentiment associated with the aspect is gen-
erated conditioned on its identity,
p(t) = ?
a?At
p(a)p(SENT(at)|a)
where At is a set of aspects that are mentioned in
text excerpt t, p(a) is the probability of seeing aspect
a, and SENT(at) ? [?1, 1] is the sentiment associ-
ated with aspect a in t. The SAM model sets p(a)
through the maximum likelihood estimates over T
and assumes p(SENT(at)|a) is normally distributed
with a mean and variance also estimated from T . We
2We focus on text-only opinion summaries as opposed to
those based on numeric ratings (Hu and Liu, 2006).
denote SAM(T ) as the model learned using the entire
set of candidate text excerpts T .
The SAM summarizer scores each potential sum-
mary, S, by learning another model SAM(S) based
on the text excerpts used to construct S. We can then
measure the distance between a model learned over
the full set T and a summary S by summing the KL-
divergence between their learned probability distri-
butions. In our case we have 1 + |AT | distributions
? p(a), and p(?|a) for all a ? AT . We then define L:
L(S) = ?KL(SAM(T ), SAM(S))
That is, the SAM summarizer prefers summaries
whose induced model is close to the model induced
for all the opinions about the product of interest.
Thus, a good summary should (1) mention aspects in
roughly the same proportion that they are mentioned
in the full set of opinions and (2) mention aspects
with sentiment also in proportion to what is observed
in the full opinion set. A high scoring summary is
found by initializing a summary with random sen-
tences and hill-climbing by replacing sentences one
at a time until convergence.
We chose to use the SAM model for our exper-
iment for two reasons. First, Lerman et al (2009)
showed that among a set of different opinion sum-
marizers, SAM was rated highest in a user study.
Secondly, as we will show in the next section, the
SAM summarization model can be naturally ex-
tended to produce contrastive summaries.
3 Constrastive Summarization
When jointly generating pairs of summaries, we at-
tempt to highlight differences between two products.
These differences can take multiple forms. Clearly,
two products can have different prevailing sentiment
scores with respect to an aspect (e.g. ?Product X has
great image quality? vs ?Product Y?s image quality
is terrible?). Reviews of different products can also
emphasize different aspects. Perhaps one product?s
screen is particularly good or bad, but another?s is
not particularly noteworthy ? or perhaps the other
product simply doesn?t have a screen. Regardless of
sentiment, reviews of the first product will empha-
size the screen quality aspect more than those of the
second, indicating that our summary should as well.
114
Tx Ty
Sx Sy
Tx Ty
Sx Sy
Tx Ty
Sx Sy
(a) (b) (c)
Figure 1: (a) Non-joint model: Generates summaries for
two products independently. (b) Joint model: Summaries
attempt to look like text they are drawn from, but contrast
each-other. (c) Joint model: Like (b), except summaries
contrast text that the other summary is drawn from.
As input to our contrastive summarizer we assume
two products, call them x and y as well as two corre-
sponding candidate sets of opinions, Tx and Ty, re-
spectively. As output, a contrastive summarizer will
produce two summaries ? Sx for product x and Sy
for product y ? so that the summaries highlight the
differences in opinion between the two products.
What might a contrastive summarizer look like on
a high-level? Figure 1 presents some options. The
first example (1a) shows a system where each sum-
mary is generated independently, i.e., running the
SAM model on each product separately without re-
gard to the other. This procedure may provide some
useful contrastive information, but any such infor-
mation will be present incidentally. To make the
summaries specifically contrast each other, we can
modify our system by explicitly modeling the fact
that we want summaries Sx and Sy to contrast. In
the SAM model this is trivial as we can simply add a
term to the scoring function L that attempts to maxi-
mize the KL-divergence between the two summaries
induced models SAM(Sx) and SAM(Sy).
This approach is graphically depicted in figure 1b,
where the system attempts to produce summaries
that are maximally similar to the opinion set they are
drawn from and minimally similar from each other.
However, some obvious degenerate solutions arise
if we chose to model our system this way. Consider
two products, x and y, for which all opinions dis-
cuss two aspects a and b with identical frequency
and sentiment polarity. Furthermore, several opin-
ions of x and y discuss an aspect c, but with oppo-
site sentiment polarity. Suppose we have to build
contrastive summaries and only have enough space
to cover a single aspect. The highest scoring con-
trastive pair of summaries would consist of one for x
that mentions a exclusively, and one for y that men-
tions b exclusively ? these summaries each mention
a promiment aspect of their product, and have no
overlap with each other. However, they provide a
false contrast because they each attempt to contrast
the other summary, rather than the other product.
Better would be for both to cover aspect c.
To remedy this, we reward summaries that in-
stead have a high KL-divergence with respect to the
other product?s full model SAM(T ) as depicted in
Figure 1c. Under this setup, the degenerate solution
described above is no longer appealing, as both sum-
maries have the same KL-divergence with respect to
the other product as they do to their own product.
The fact that the summaries themselves are dissim-
ilar is irrelevant. Comparing the summaries only to
the products? full language models prevents us from
rewarding summaries that convey a false contrast be-
tween the products under comparison. Specifically,
we now optimize the following joint summary score:
L(Sx, Sy) = ?KL(SAM(Tx), SAM(Sx))
?KL(SAM(Ty), SAM(Sy))
+KL(SAM(Tx), SAM(Sy))
+KL(SAM(Ty), SAM(Sx))
Note that we could additionally model divergence
between the two summaries (i.e., merging models in
figures 1b and c), but such modeling is redundant.
Furthermore, by not explicitly modeling divergence
between the two summaries we simplify the search
space as each summary can be constructed without
knowledge of the content of the second summary.
4 The Experiment
Our experiments focused on consumer electronics.
In this setting an entity to be summarized is one spe-
cific product and T is a set of segmented user re-
views about that product. We gathered reviews for
56 electronics products from several sources such as
CNet, Epinions, and PriceGrabber. The products
covered 15 categories of electronics products, in-
cluding MP3 players, digital cameras, laptops, GPS
systems, and more. Each had at least four reviews,
and the mean number of reviews per product was 70.
We manually grouped the products into cate-
gories (MP3 players, cameras, printers, GPS sys-
115
System As Received Consolidated
SAM 1.85 ? 0.05 1.82 ? 0.05
SAM + contrastive 1.76 ? 0.05 1.68 ? 0.05
Table 1: Mean rater scores for contrastive summaries by
system. Scores range from 0-3 and lower is better.
tems, headphones, computers, and others), and gen-
erated contrastive summaries for each pair of prod-
ucts in the same category using 2 different algo-
rithms: (1) The SAM algorithm for each product in-
dividually (figure 1a) and (2) The SAM algorithm
with our adaptation for contrastive summarization
(figure 1c). Summaries were generated using K =
650, which typically consisted of 4 text excerpts of
roughly 160 characters. This allowed us to compare
different summaries without worrying about the ef-
fects of summary length on the ratings. In all, we
gathered 178 contrastive summaries (89 per system)
to be evaluated by raters and each summary was
evaluated by 3 random raters resulting in 534 rat-
ings. The raters were 55 everyday internet users
that signed-up for the experiment and were assigned
roughly 10 random ratings each. Raters were shown
two products and their contrastive summaries, and
were asked to list 1-3 differences between the prod-
ucts as seen in the two summaries. They were also
asked to read the products? reviews to help ensure
that the differences observed were not simply arti-
facts of the summarizer but in fact are reflected in
actual opinions. Finally, raters were asked to rate
the helpfulness of the summaries in identifying these
distinctions, rating each with an integer score from
0 (?extremely useful?) to 3 (?not useful?).
Upon examining the results, we found that raters
had a hard time finding a meaningful distinction be-
tween the two middle ratings of 1 and 2 (?useful?
and ?somewhat useful?). We therefore present two
sets of results: one with the scores as received from
raters, and another with all 1 and 2 votes consol-
idated into a single class of votes with numerical
score 1.5. Table 1 gives the average scores per sys-
tem, lower scores indicating superior performance.
5 Analysis and Conclusions
The scores indicate that the addition of the con-
trastive term to the SAM model improves helpful-
ness, however both models roughly have average
System 2+ raters All 3 raters
SAM 0.8 0.2
SAM + contrastive 2.0 0.6
Table 2: Average number of points of contrast per com-
parison observed by multiple raters, by system. Raters
were asked to list up to 3. Higher is better.
scores in the somewhat-useful to useful range. The
difference becomes more pronounced when look-
ing at the consolidated scores. The natural question
arises: does the relatively small increase in helpful-
ness reflect that the contrastive summarizer is doing
a poor job? Or does it indicate that users only find
slightly more utility in contrastive information in
this domain? We inspected comments left by raters
in an attempt to answer this. Roughly 80% of raters
were able to find at least two points of contrast in
summaries generated by the SAM+contrastive ver-
sus 40% for summaries generated by the simple
SAM model. We then examined the consistency
of rater comments, i.e., to what degree did differ-
ent raters identify the same points of contrast from a
specific comparison? We report the results in table 2.
Note that by this metric in particular, the contrastive
summarizer outperforms its the single-product sum-
marizer by significant margins and provides a strong
argument that the contrastive model is doing its job.
Acknowledgements: The Google sentiment analy-
sis team for insightful discussions and suggestions.
References
K. Filippova, M. Surdeanu, M. Ciaramita, and
H. Zaragoza. 2009. Company-oriented extractive
summarization of financial news. In Proc. EACL.
M. Hu and B. Liu. 2006. Opinion extraction and sum-
marization on the web. In Proc. AAAI.
N. Jindal and B. Liu. 2006. Mining comparative sen-
tences and relations. In Proc. AAAI.
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan Mc-
Donald. 2009. Sentiment summarization: Evaluating
and learning user preferences. In Proc. EACL.
B. Pang and L. Lee. 2008. Opinion mining and sentiment
analysis. Now Publishers.
I. Soboroff and D. Harman. 2005. Novelty detection:
The TREC experience. In Proc. HLT/EMNLP.
Sun, Wang, Shen, Zeng, and Chen. 2006. CWS: A Com-
parative Web search System. In Proc. WWW.
116
