French-English Terminology Extraction from
Comparable Corpora
Be?atrice Daille and Emmanuel Morin
University of Nantes, LINA - FRE CNRS 2729,
2, rue de la Houssinie`re - BP 92208, 44322 Nantes Cedex 3, France
{beatrice.daille, emmanuel.morin}@univ-nantes.fr
Abstract. This article presents a method of extracting bilingual lexica
composed of single-word terms (SWTs) and multi-word terms (MWTs)
from comparable corpora of a technical domain. First, this method ex-
tracts MWTs in each language, and then uses statistical methods to
align single words and MWTs by exploiting the term contexts. After ex-
plaining the difficulties involved in aligning MWTs and specifying our
approach, we show the adopted process for bilingual terminology ex-
traction and the resources used in our experiments. Finally, we evaluate
our approach and demonstrate its significance, particularly in relation to
non-compositional MWT alignment.
1 Introduction
Traditional research into the automatic compilation of bilingual dictionaries from
corpora exploits parallel texts, i.e. a text and its translation [17]. From sentence-
to-sentence aligned corpora, symbolic [2], statistical [11], or combined [7] tech-
niques are used for word and expression alignments.
The use of parallel corpora raises two problems:
? as a parallel corpus is a pair of translated texts, the vocabulary appearing
in the translated text is highly influenced by the source text, especially for
technical domains;
? such corpora are difficult to obtain for paired languages not involving
English.
New methods try to exploit comparable corpora: texts that are of the same text
type and on the same subject without a source text-target text relationship. The
main studies concentrate on finding in such corpora translation candidates for
one-item words. For example, the French SWT manteau is translated in English
by mantle in the domain of forestry, shield in the domain of marine activities,
and by coat in the domain of clothing. The method is based on lexical context
analysis and relies on the simple observation that a word and its translation tend
to appear in the same lexical contexts. Thus, for our three possible translations
of manteau, three different lexical contexts are encountered which are expressed
below by English lexical units:
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 707?718, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
708 B. Daille and E. Morin
? manteau/mantle : vegetation, forest, wood. . .
? manteau/shield : boat, sea, shipbuilding. . .
? manteau/coat : cloth, cold, wear. . .
These contexts can be represented by vectors, and each vector element represents
a word which occurs within the window of the word to be translated. Translation
is obtained by comparing the source context vector to each translation candidate
vector after having translated each element of the source vector with a general
dictionary. This method is known as the ?direct context-vector approach?. Using
this method, [10] extracts English-Chinese one-item candidate translations from
two years of English and Chinese newspaper articles by matching the context
vector with 76% precision on the first 20 candidates. From English-German
newspaper corpora of 85 million words, [14] improves the precision to 89% on the
first one-item 10 candidates using the same techniques. [4] obtain 50% precision
on the first one-item 10 candidates from a French/English corpus of 1.2 million
words. [1] adapted this approach to deal with many-to-many word translations.
In extracting English-Chinese nominal phrases belonging to general domains
from the web, they obtain a precision of 91% on the first 3 candidates.
Some improvements have been proposed by [9] to avoid the insufficient cov-
erage of bilingual dictionary and thus not to get context vectors with too many
elements that are not translated. This method is called ?similarity-vector ap-
proach?: it associates to the word to be translated the context vectors of the
nearest lexical units that are in the bilingual dictionary. With this method, they
obtain for one-item French-English words 43% and 51% precision on the ten and
twenty first candidates applied on a medical corpus of 100 000 words (respec-
tively 44% and 57% with the direct method) and 79% and 84% precision on the
ten and twenty first candidates applied on a social science corpus of 8 millions
words (respectively 35% and 42% with the direct method).
If the results obtained in the field of bilingual lexicon extraction from compa-
rable corpora are promising, they only cover either bilingual single words from
general or specialised corpora, or bilingual nominal phrases from general corpora.
Our goal is to find translation for multi-word terms (MWTs) from specialised
comparable corpora.
If MWTs are more representative of domain specialities than single-word
terms (SWTs), pinpointing their translations poses specific problems:
? SWTs and MWTs are not always translated by a term of the same length. For
example, the French MWT peuplement forestier (2 content words) is trans-
lated into English as the SWT crop and the French term essence d?ombre (2
content words) as shade tolerant species (3 content words). This well-known
problem, referred to as ?fertility?, is seldom taken into account in bilingual
lexicon extraction, a word-to-word assumption being generally adopted.
? When a MWT is translated into a MWT of the same length, the target
sequence is not typically composed of the translation of its parts [13]. For
example, the French term plantation e?nerge?tique is translated into English as
fuel plantation where fuel is not the translation of e?nerge?tique. This property
is referred to as ?non-compositionality?.
French-English Terminology Extraction from Comparable Corpora 709
? A MWT could appear in texts under different forms reflecting either syn-
tactic, morphological or semantic variations [12],[5]. Term variations should
be taken into account in the translation process. For example, the French
sequences ame?nagement de la fore?t and ame?nagement forestier refer to the
same MWT and are both translated into the same English term: forest man-
agement.
We propose tackling these three problems, fertility, non-compositionality, and
variations, by using both linguistic and statistical methods. First, MWTs are
identified in both the source and target language using a monolingual term
extraction program. Second, a statistical alignment algorithm is used to link
MWTs in the source language to single words and MWTs in the target language.
Our alignment algorithm extracts the words and MWT contexts and proposes
translations by comparing source and target words and MWT contexts.
2 Extraction Process
We present in this section the bilingual extraction process which is composed of
two steps:
1. Identification in source and target languages of MWTs and their variations;
2. Alignment of theses MWTs using a method close to the ?similarity-vector
approach?.
2.1 MWT Identification
MWTs are extracted using a terminology extraction program available for French
and English: ACABIT 1. This program is open source and one of its character-
istics is to take into account variants of MWTs (graphical, inflectional, syntac-
tic, and morphosyntactic)[6]. It does not need any external linguistic resources
and is domain-independent. ACABIT applies on a corpus with the following
pre-processing:
? tokenisation and sentence segmentation;
? part-of-speech and lemma tagging.
First, ACABIT carries out shallow parsing: it scans the corpus, counts and
extracts strings whose tag sequences characterise patterns of MWTs or one of
their variants. The different occurrences referring to a MWT or one of its variants
are grouped and constitute an unique candidate MWT. Thus the candidate
MWT produit forestier ?forest product? appears under the following forms:
1 http://www.sciences.univ-nantes.fr/info/perso/permanents/daille/ and LINUX
Mandrake release
710 B. Daille and E. Morin
? base form: produit forestier ;
? graphical variant: produit fo-restier, pro-duit forestier ;
? inflexional variant: produits forestiers ;
? syntatic variant: modification: produit non forestier, produit alimentaire
forestier, produit fini d?origine forestie?re, produit ligneux non forestier ;
? syntactic variant: coordination: produit halieutique et forestier, produit
agricole ou forestier, le produit et le service forestier.
The MWT candidates produit de la fore?t, produit agroforestier, non-produit agro-
forestier, and sous-produit forestier, sous-produit de la fore?t have also been iden-
tified.
Second, ACABIT performs semantic grouping thanks to the following
operations:
Merging of two MWTs. Two MWT candidates are merged if they are syn-
onymic variants obtained by derivation or conversion. Such variants in-
clude a relational adjective: either a denominal adjective, i.e. morphologi-
cally derived from a noun thanks to a suffix, such as fore?t/forestier ?forest?,
or an adjective having a noun usage such as mathe?matique ?mathemati-
cal/mathematics?.
Dissociation of some MWT variants. Syntactical variants that induce se-
mantic discrepancies are retrieved from the set of the candidate variants
and new MWT candidates are created. Modification variants with the in-
sertion of an adverb of negation denoting an antonymy link such as produit
non forestier ?non forest product? and produit forestier ?forest product?, or
insertion of a relational adjectives denoting an hyperonymy link such as pro-
duit alimentaire forestier ?food forest product? with produit forestier ?forest
product? [6].
Grouping of MWTs. All MWT candidates linked by derivational morphol-
ogy or by variations inducing semantic variations are clustered. For exam-
ple, the following MWT candidates constitutes a cluster of MWTs: produit
forestier/produit de la fore?t, produit non forestier, non-produit agroforestier,
produit agroforestier, sous-produit forestier/sous-produit de la fore?t, produit
alimentaire forestier andproduit forestier.
In the following steps, we do not consider a unique sequence reflecting a
candidate MWT but a set of sequences. We consider only term variants that
are grouped under a unique MWT. This grouping of term variations could be
interpreted as a terminology normalisation in the same way as lemmatisation at
the morphological level.
2.2 MWT Alignment
The goal of this step, which adapts the similarity vector-based approach defined
for single words by [9] to MWTs, is to align source MWTs with target single
words, SWTs or MWTs. From now on, we will refer to lexical units as words,
SWTs or MWTs.
French-English Terminology Extraction from Comparable Corpora 711
Context Vectors. First, we collect all the lexical units in the context of each
lexical unit i and count their occurrence frequency in a window of n sentences
around i. For each lexical unit i of the source and the target language, we obtain a
context vector vi which gathers the set of co-occurrence units j associated with
the number of times that j and i occur together occij . We normalise context
vectors using an association score such as Mutual Information or Log-likelihood.
(cf. equations 1 and 2 and table 1). In order to reduce the arity of context vectors,
we keep only the co-occurrences with the highest association scores.
Table 1. Contingency table
j ?j
i a = occ(i, j) b = occ(i, ?j)
?i c = occ(?i, j) d = occ(?i, ?j)
MI(i, j) = log
a
(a + b)(a + c)
(1)
?(i, j) = a log(a) + b log(b) + c log(c) + d log(d)
+(a + b + c + d) log(a + b + c + d) ? (a + b) log(a + b)
?(a + c) log(a + c) ? (b + d) log(b + d) ? (c + d) log(c + d)
(2)
Similarity Vectors. For each lexical unit k to be translated, we identify the
lexical units which the context vectors are similar to vk thanks to a vector
distance measure such as Cosine [15] or Jaccard [16] (cf. equations 3 and 4).
From now, we call ?similarity vector? of the unit k a vector that contains all the
lexical units which the context vectors are similar to vk. To each unit l of the
similarity vector vk, we associate a similarity score similvkvl between vl and vk.
In order to reduce the arity of similarity vectors, we keep only the lexical units
with the highest similarity scores. Up to now, similarity vectors have only been
built for the source language.
similvkvl =
?
t assoc
l
t assoc
k
t
?
?
t assoc
l
t
2
assockt
2
(3)
similvkvl =
?
t min(assoc
l
t, assoc
k
t )
?
t assoc
l
t
2
+
?
t assoc
k
t
2 ?
?
t assoc
l
t assoc
k
t
(4)
Translation of the Similarity Vectors. Using a bilingual dictionary, we
translate the lexical units of the similarity vector and identify their context
vectors in the target language. Figure 1 illustrates this translation process.
Depending the nature of the lexical unit, two different treatments are
carried out:
712 B. Daille and E. Morin
TRANSLATION
SOURCE LANGUAGE TARGET LANGUAGE
close vector
close vector
close vector
close vector
close vector
Candidate translations
average vector
average context vectorsimilarity vectors
context vectorscontext vector of the lexical unit to be translated
MWT to be translated
context vector of the candidate translations
Fig. 1. Transfer procedure of similarity vectors from source to target language
Translation of a SWT. If the bilingual dictionary provides several transla-
tions for a word belonging to the similarity vector, we generate as many
target context vectors as possible translations. Then, we calculate the union
of these vectors to obtain only one target context vector.
Translation of a MWT. If the translation of the parts of the MWT are found
in the bilingual dictionary, we generate as many target context vectors as
translated combinations identified by ACABIT and calculate their union.
When it is not possible to translate all the parts of a MWT, or when the
translated combinations are not identified by ACABIT, the MWT is not
taken into account in the translation process.
Finding the MWT Translations. We calculate the barycentre of all the
target context vectors obtained in the preceding step in order to propose a
target average vector. The candidate translations of a lexical unit are the tar-
get lexical units closest to the target average vector according to vector
distance.
3 Resources Presentation
We present in this section the different resources used for our experiments:
3.1 Comparable Corpus
Our comparable corpus has been built from the Unasylva electronic international
journal published by FAO2 and representing 4 million words. This journal deals
2 http://www.fao.org/forestry/foris/webview/forestry2/
French-English Terminology Extraction from Comparable Corpora 713
with forests and forest industries and is available in English, French and Spanish.
In order to constitute a comparable corpus, we only select texts which are not
the translation of each other.
3.2 Bilingual Dictionary
Our bilingual dictionary has been built from lexical resources on the Web. It
contains 22,300 French single words belonging to the general language with an
average of 1.6 translation per entry.
3.3 Reference Bilingual Terminology
The evaluation of our bilingual terminology extraction method has been done
from a reference bilingual terminology. This reference list has been built from
three different terminological resources:
1. a bilingual glossary of the terminology of silviculture3. It contains 700 terms
of which 70% are MWTs.
2. the Eurosilvasur multilingual lexicon4. It contains 2,800 terms of which 66%
are MWTs.
3. the multilingual AGROVOC thesaurus5. It contains 15,000 index terms of
which 47% are MWTs.
These three terminological resources are complementary, the glossary being the
most specialised, the thesaurus the least. From these resources, we automatically
select 300 terms with the constraint that each French term should appear at least
5 times in our corpus. These terms are divided into three sub-lists:
? [list 1] 100 French SWTs of which the translation is an English SWT. Of
course, this translation is not given by our bilingual dictionary.
? [list 2] 100 French MWTs of which the translation could be an English SWT
or a MWT. In the case of MWTs, the translation could not be obtained by
the translation of the MWT?s parts.
? [list 3] 100 MWT of which the translation is an English MWT. The transla-
tion of these MWTs is obtained by the translation of their parts.
This reference list contains a majority of terms with low frequency (cf.
Table 2). Two main reasons explain this fact: on the one hand, the different
resources which have been used to build this reference list are either specific or
generic; on the other hand, our corpus covers several domains linked to forestry
and does not constitute a highly specialised resource.
3 http://nfdp.ccfm.org/silviterm/silvi f/silvitermintrof.htm
4 http://www.eurosilvasur.net/francais/lexique.php
5 http://www.fao.org/agrovoc/
714 B. Daille and E. Morin
Table 2. Frequency in the corpus of the French terms belonging to the reference list
# occ. < 50 ? 100 ? 1 000 > 1 000
[list 1] 50 21 18 11
[list 2] 54 21 25 0
[list 3] 51 18 29 2
4 Evaluation
We present now the evaluation of the bilingual terminology extraction. We have
to deal with 55 013 SWTs and MWTs, but only 7 352 SWTs and 6 769 MWTs
appear both in the reference bilingual terminology and in the corpus.
4.1 Parameter Estimation
Several parameters appear in the extraction process presented in Section 2. The
most interesting results have been obtained with the following values:
? Size of the context window is 3 sentences around the lexical unit to be
translated;
? Context vectors are built only with one-item words to increase representa-
tivity. For example, the context vector of the French term de?bardage ?hauling?
includes the MWT tracteur a` chenille ?crawler tractor? which is more dis-
criminating than its parts, tracteur or chenille. But including MWTs into
context vectors increases the vectorial space dimension and reduces the rep-
resentativity of the terms appearing both in the corpus and the reference
bilingual terminology. The term de?bardage ?hauling? has a frequency of 544
as a SWT and only a frequency of 144 as part of a MWT as it appears in
several MWTs. The context vector size are limited to the first 100 values of
the Log-likelihood association score.
? Similarity vectors are the first 30 values of Cosine distance measure.
? Finding translations is done with Cosine distance measure.
4.2 Result Analysis
Table 3 gives the results obtained with our experiments. For each sublist, we
give the number of translations found (NBtrans), and the average and standard
deviation position for the translations in the ranked list of candidate translations
(AV Gpos, STDDEVpos).
We note that translations of MWTs belonging to [list 3] which are composi-
tionally translated are well-identified and often appear in the first 20 candidate
translations. The translations belonging to [lists 1 and 2 ] are not always found
and, when they are, they seldom appear in the first 20 candidate translations.
The examination of the candidate translations of a MWT regardless of the
list to which it belongs shows that they share the same semantic field (cf. table 5).
French-English Terminology Extraction from Comparable Corpora 715
Table 3. Bilingual terminology extraction results
NBtrans AV Gpos STDDEVpos
|list 1] 56 32.9 23,7
[list 2] 63 30.7 26,7
[list 3] 89 3.8 7,9
Table 4. Bilingual MWT extraction with parameter combination
NBtrans AV Gpos STDDEVpos Top 10 Top 20
|list 1] 59 16.2 15.9 41 51
[list 2] 63 14.8 22.3 45 55
[list 3] 89 2.4 3.7 87 88
Table 5. Exemples of candidate translations obtained for 3 terms belonging to [list 2]
degre? de humidite? gaz a` effet de serre papeterie
(# occ. 41) (# occ. 33) (# occ. 178)
humidity carbon newsprint
saturation carbon cycle paper production
aridity atmosphere raw material
evaporation greenhouse gas mill
saturation deficit greenhouse pulp mill
rate of evaporation global carbon raw
atmospheric humidity atmospheric carbon manufacture
water vapor emission paper mill
joint sink manufacturing
dry carbon dioxide capacity
hot fossil fuel printing
rainy fossil paper manufacture
temperature carbon pool factory
moisture control mitigate paperboard
meyer global warming fiberboard
party climate change bagasse
atmospheric atmospheric paper-making
dryness dioxide board
monsoon sequestration material supply
joint meeting quantity of carbon paper pulp
As noted above, our results differ widely according the chosen parameter values.
Because of time constraints, we cannot evaluate all the possible values of all the
different parameters, but manual examination of the candidate translations for
a few different configurations shows:
716 B. Daille and E. Morin
? Some good translations obtained for one parameter configuration are not
found for another, and, inversely, some terms which are not translated in
the first configuration could be correctly translated by another. So, it is
difficult to choose the best configuration, especially for [lists 1 and 2].
? More precisely, for a given term, the first candidate translations are different
for different configurations. For example, for the French MWT pa?te a` papier
(paper pulp), the first 50 candidate translations of 20 different configurations
have only 30 items in common.
? The right translation appears in different positions for different configura-
tions.
In order to identify more correct translations, we decided to take into account
the different results proposed by different configurations by fusing the first 20
candidate translations proposed by each configuration. The different configura-
tions concern the size of the context and similarity vectors, and the association
and similarity measures. The results obtained and presented in Table 4 show a
slight improvement in the position of the correct translations among the set of
candidate translations.
The results for [list 3] are still very satisfactory. The results for [list 1] improve,
but remain a little below the results obtained by [8] who obtained 43% and 51%
for the first 10 and 20 candidates respectively for a 100,000-word medical corpus,
and 79% and 84% for a multi-domain 8 million word corpus.
4.3 Comment
In a general way, it is difficult to compare our experiments to previous ones
[3],[8] as the corpora are different. Indeed, our comparable corpus covers several
domains belonging to forestry, and does not constitute a very specialised re-
source on the contrary of the medical corpus of [3] built thanks to the key words
?symptoms, pathological status?. Moreover, half of the terms of the reference
bilingual terminological database have a frequency of less than 50 occurrences in
the corpus that lead to non-discriminating context vectors. [8] use for their ex-
periments a social sciences corpora of 8 millions words and a reference bilingual
terminological database of 180 words with high frequencies in the corpus: from
100 to 1000. Our automatic evaluation is also more constrained than manual
evaluation. For example, our reference list gives haulage road as the transla-
tion of piste de de?bardage. In our candidate translation list, haulage road is not
present. We find an acceptable translation, skid trail, in the first 20 candidates,
but this is never considered valid by our automatic evaluation.
Our results for MWTs are better than those for single words. The method seems
promising, especially for MWTs for which translation is not compositional.
5 Conclusion
In this paper, we proposed and evaluated a combined method for bilingual MWT
extraction from comparable corpora which takes into account three main char-
acteristics of MWT translation: fertility, non-compositionality, and variation
French-English Terminology Extraction from Comparable Corpora 717
clustering. We first extracted monolingually MWTs and clustered synonymic
variants. Secondly, we aligned them using a statistical method adapted from
similarity-vector approach for single words which exploits the context of these
MWTs. This combined approach for MWTs gives satisfactory results compared
to those for single word. It also allows us to obtain non compositional translations
of MWTs. Our further works will concentrate on the interaction parameters, the
combining of the source-to-target and target-to-source alignment results, and
the handling of non-synonymic term variations.
Acknowledgements
We are particularly grateful to Samuel Dufour-Kowalski, who undertook the
computer programs. This work has also benefited from his comments.
References
1. Cao, Y., Li, H.: Base Noun Phrase Translation Using Web Data and the EM
Algorithm. In: Proceeding of the 19th International Conference on Computational
Linguistics (COLING?02), Tapei, Taiwan (2002) 127?133
2. Carl, M., Langlais, P.: An intelligent Terminology Database as a pre-processor
for Statistical Machine Translation. In Chien, L.F., Daille, B., Kageura, L., Nak-
agawa, H., eds.: Proceeding of the COLING 2002 2nd International Workshop on
Computational Terminology (COMPUTERM?02), Tapei, Taiwan (2002) 15?21
3. Chiao, Y.C.: Extraction lexicale bilingue a` partir de textes me?dicaux comparables :
application a` la recherche d?information translangue. PhD thesis, Universite? Pierre
et Marie Curie, Paris VI (2004)
4. Chiao, Y.C., Zweigenbaum, P.: Looking for candidate translational equivalents in
specialized, comparable corpora. In: Proceedings of the 19th International Confer-
ence on Computational Linguistics (COLING?02), Tapei, Taiwan (2002) 1208?1212
5. Daille, B.:. Conceptual Structuring through Term Variations. In Bond, F.,
Korhonen, A., MacCarthy, D., Villacicencio A., eds.: Proceedings of the ACL
2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment
(2003) 9?16
6. Daille, B.: Terminology Mining. In Pazienza, M., ed.: Information Extraction in
the Web Era. Springer (2003) 29?44
7. Daille, B., Gaussier, E., Lange?, J.-M..: Towards Automatic Extraction of Monolin-
gual and Bilingual Terminology. Proceedings of the 15th International Conference
on Computational Linguistics (COLING?94) 1 (1994) 515?521
8. De?jean, H., Sadat, F., Gaussier, E.: An approach based on multilingual thesauri
and model combination for bilingual lexicon extraction. In: Proceedings of the
19th International Conference on Computational Linguistics (COLING?02). (2002)
218?224
9. De?jean, H., Gaussier, E.: Une nouvelle approche a` l?extraction de lexiques bilingues
a` partir de corpus comparables. Lexicometrica, Alignement lexical dans les corpus
multilingues (2002) 1?22
10. Fung, P.: A Statistical View on Bilingual Lexicon Extraction: From Parallel Cor-
pora to Non-parallel Corpora. In Farwell, D., Gerber, L., Hovy, E., eds.: Pro-
ceedings of the 3rd Conference of the Association for Machine Translation in the
Americas (AMTA?98), Springer (1998) 1?16
718 B. Daille and E. Morin
11. Gaussier, E., Lange?, J.M.: Mode`les statistiques pour l?extraction de lexiques
bilingues. Traitement Automatique des Langues (TAL) 36 (1995) 133?155
12. Jacquemin, C.: Spotting and Discovering Terms through Natural Language Pro-
cessing. Cambridge: MIT Press (2001)
13. Melamed, I.D.: Empirical Methods for Exploiting Parallel Texts. MIT Press (2001)
14. Rapp, R.: Automatic Identification of Word Translations from Unrelated English
and German Corpora. In: Proceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL?99). (1999) 519?526
15. Salton, G., Lesk, M.E.: Computer Evaluation of Indexing and Text Processing.
Journal of the Association for Computational Machinery 15 (1968) 8?36
16. Tanimoto, T.T.: An elementary mathematical theory of classification. Technical
report, IBM Research (1958)
17. Veronis, J., ed.: Parallel Text Processing. Kluwer Academic Publishers (2000)
An Effective Compositional Model for Lexical Alignment
Be?atrice Daille Emmanuel Morin
Universit?e de Nantes, LINA - FRE CNRS 2729
2, rue de la Houssiniere, BP 92208
F-44322 Nantes cedex 03
 
beatrice.daille,emmanuel.morin  @univ-nantes.fr
Abstract
The automatic compilation of bilingual dic-
tionaries from comparable corpora has been
successful for single-word terms (SWTs),
but remains disappointing for multi-word
terms (MWTs). One of the main problems is
the insufficient coverage of the bilingual dic-
tionary. Using the compositional translation
method improved the results, but still shows
some limits for MWTs of different syntac-
tic structures. In this paper, we propose to
bridge the gap between syntactic structures
through morphological links. The results
show a significant improvement in the com-
positional translation of MWTs that demon-
strate the efficiency of the morphologically
based-method for lexical alignment.
1 Introduction
Current research in the automatic compilation of
bilingual dictionaries from corpora uses of compara-
ble corpora. Comparable corpora gather texts shar-
ing common features (domain, topic, genre, dis-
course) without having a source text-target text re-
lationship. They are considered by human transla-
tors more trustworthy than parallel corpora (Bowker
and Pearson, 2002). Moreover, they are available for
any written languages and not only for pairs of lan-
guages involving English. The compilation of spe-
cialized dictionaries should take into account multi-
word terms (MWTs) that are more precise and spe-
cific to a particular scientific domain than single-
word terms (SWTs). The standard approach is based
on lexical context analysis and relies on the simple
observation that a SWT or a MWT and its trans-
lation tend to appear in the same lexical contexts.
Correct results are obtained for SWTs with an ac-
curacy of about 80% for the top 10-20 proposed
candidates using large comparable corpora (Fung,
1998; Rapp, 1999; Chiao and Zweigenbaum, 2002)
or 60% using small comparable corpora (De?jean
and Gaussier, 2002). In comparison, the results ob-
tained for MWTs are disappointing. For instance,
(Morin et al, 2007) have achieved 30% and 42%
precision for the top 10 and top 20 candidates in a
0.84 million-word French-Japanese corpus. These
results could be explained by the low frequency of
MWTs compared to SWTs, by the lack of paral-
lelism between the source and the target MWT ex-
traction systems, and by the low performance of the
alignment program. For SWTs, the process is in
two steps: looking in a dictionary, and if no direct
translation is available, starting the contextual anal-
ysis. Looking in the dictionary gives low results for
MWTs: 1% compared to 30% for French and 20%
for Japanese SWTs (Morin and Daille, 2006). To ex-
tend the coverage of the bilingual dictionary, an in-
termediate step is added between looking in the dic-
tionary and the contextual analysis that will propose
several translation candidates to compare with the
target MWTs. These candidate translations are ob-
tained thanks to a compositional translation method
(Melamed, 1997; Grefenstette, 1999). This method
reveals some limits when MWTs in the source and
the target languages do not share the same syntactic
patterns.
In this paper, we put forward an extended compo-
95
sitional method that bridges the gap between MWTs
of different syntactic structures through morpho-
logical links. We experiment within this method
of French-Japanese lexical alignment, using multi-
lingual terminology mining chain made up of two
terminology extraction systems; one in each lan-
guage, and an alignment program. The term extrac-
tion systems are publicly available and both extract
MWTs. The alignment program makes use of the
direct context-vector approach (Fung, 1998; Rapp,
1999). The results show an improvement of 33% in
the translation of MWTs that demonstrate the effi-
ciency of the morphologically based-method for lex-
ical alignment.
2 Multilingual terminology mining chain
Taking a comparable corpora as input, the multi-
lingual terminology mining chain outputs a list of
single- and multi-word candidate terms along with
their candidate translations (see Figure 1). This
chain performs a contextual analysis that adapts the
direct context-vector approach (Rapp, 1995; Fung
and McKeown, 1997) for SWTs to MWTs. It con-
sists of the following five steps:
1. For each language, the documents are cleaned,
tokenized, tagged and lemmatized. For French,
Brill?s POS tagger1 and the FLEM lemmatiser2
are used, and for Japanese, ChaSen3. We then
extract the MWTs and their variations using
the ACABIT terminology extraction system avail-
able for French4 (Daille, 2003), English and
Japanese5 (Takeuchi et al, 2004). (From now
on, we will refer to lexical units as words,
SWTs or MWTs).
2. We collect all the lexical units in the context of
each lexical unit  and count their occurrence
frequency in a window of  words around  .
For each lexical unit  of the source and the
target languages, we obtain a context vector
1http://www.atilf.fr/winbrill/
2http://www.univ-nancy2.fr/pers/namer/
3http://chasen-legacy.sourceforge.jp/
4http://www.sciences.univ-nantes.fr/
info/perso/permanents/daille/ and release for
Mandriva Linux.
5http://cl.cs.okayama-u.ac.jp/rsc/
jacabit/
 which gathers the set of co-occurrence units
 associated with the number of times that 
and  occur together 	
	

 . In order to iden-
tify specific words in the lexical context and
to reduce word-frequency effects, we normal-
ize context vectors using an association score
such as Mutual Information (Fano, 1961) or
Log-likelihood (Dunning, 1993).
3. Using a bilingual dictionary, we translate the
lexical units of the source context vector. If the
bilingual dictionary provides several transla-
tions for a lexical unit, we consider all of them
but weigh the different translations by their fre-
quency in the target language.
4. For a lexical unit to be translated, we com-
pute the similarity between the translated con-
text vector and all target vectors through vector
distance measures such as Cosine (Salton and
Lesk, 1968) or Jaccard (Tanimoto, 1958).
5. The candidate translations of a lexical unit are
the target lexical units closest to the translated
context vector according to vector distance.
In this approach, the translation of the lexical units
of the context vectors (step 3 of the previous ap-
proach), which depends on the coverage of the bilin-
gual dictionary vis-a`-vis the corpus, is the most im-
portant step: the greater the number of elements
translated in the context vector, the more discrim-
inating the context vector in selecting translations
in the target language. Since the lexical units re-
fer to SWTs and MWTs, the dictionary must con-
tain many entries which occur in the corpus. For
SWTs, combining a general bilingual dictionary
with a specialized bilingual dictionary or a multi-
lingual thesaurus to translate context vectors ensures
that much of their elements will be translated (Chiao
and Zweigenbaum, 2002; De?jean et al, 2002). For a
MWT to be translated, steps 3 to 5 could be avoided
thanks to a compositional method that will propose
several translation candidates to directly compare
with the target MWTs identified in step 1. More-
over, the compositional method is useful in step 3
to compensate for the bilingual dictionary when the
multi-word units of the context vector are not di-
rectly translated.
96
dictionary
bilingual
Japanese documents French documents
terminology
extraction
terminology
extraction
lexical context
extraction
lexical context
extraction
process
translated
terms to be
translations
candidate
haversting
lexical alignment
The Web
documents
Figure 1: Architecture of the multilingual terminology mining chain
3 Default compositional method
In order to increase the coverage of the dictionary for
MWTs that could not be directly translated, we gen-
erated possible translations by using a default com-
positional method (Melamed, 1997; Grefenstette,
1999).
For each element of the MWT found in the bilin-
gual dictionary, we generated all the translated com-
binations identified by the terminology extraction
system. For example, for the French MWT fatigue
chronique (chronic fatigue), there are four Japanese
translations for fatigue (fatigue) ?  ,  ,  ,
 ? and two translations for chronique (chronic)
? ffProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 664?671,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Bilingual Terminology Mining ? Using Brain, not brawn comparable
corpora
E. Morin, B. Daille
Universit? de Nantes
LINA FRE CNRS 2729
2, rue de la Houssini?re
BP 92208
F-44322 Nantes Cedex 03
{morin-e,daille-b}@
univ-nantes.fr
K. Takeuchi
Okayama University
3-1-1, Tsushimanaka
Okayama-shi, Okayama,
700-8530, Japan
koichi@
cl.it.okayama-u.ac.jp
K. Kageura
Graduate School of Education
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo, 113-0033, Japan
kyo@p.u-tokyo.ac.jp
Abstract
Current research in text mining favours the
quantity of texts over their quality. But for
bilingual terminology mining, and for many
language pairs, large comparable corpora
are not available. More importantly, as terms
are defined vis-?-vis a specific domain with
a restricted register, it is expected that the
quality rather than the quantity of the corpus
matters more in terminology mining. Our
hypothesis, therefore, is that the quality of
the corpus is more important than the quan-
tity and ensures the quality of the acquired
terminological resources. We show how im-
portant the type of discourse is as a charac-
teristic of the comparable corpus.
1 Introduction
Two main approaches exist for compiling corpora:
?Big is beautiful? or ?Insecurity in large collec-
tions?. Text mining research commonly adopts the
first approach and favors data quantity over qual-
ity. This is normally justified on the one hand by
the need for large amounts of data in order to make
use of statistic or stochastic methods (Manning and
Sch?tze, 1999), and on the other by the lack of oper-
ational methods to automatize the building of a cor-
pus answering to selected criteria, such as domain,
register, media, style or discourse.
For lexical alignment from comparable corpora,
good results on single words can be obtained from
large corpora ? several millions words ? the accu-
racy of proposed translation is about 80% for the top
10-20 candidates (Fung, 1998; Rapp, 1999; Chiao
and Zweigenbaum, 2002). (Cao and Li, 2002) have
achieved 91% accuracy for the top three candidates
using the Web as a comparable corpus. But for spe-
cific domains, and many pairs of languages, such
huge corpora are not available. More importantly,
as terms are defined vis-?-vis a specific domain with
a restricted register, it is expected that the quality
rather than the quantity of the corpus matters more in
terminology mining. For terminology mining, there-
fore, our hypothesis is that the quality of the corpora
is more important than the quantity and that this en-
sures the quality of the acquired terminological re-
sources.
Comparable corpora are ?sets of texts in different
languages, that are not translations of each other?
(Bowker and Pearson, 2002, p. 93). The term com-
parable is used to indicate that these texts share
some characteristics or features: topic, period, me-
dia, author, register (Biber, 1994), discourse... This
corpus comparability is discussed by lexical align-
ment researchers but never demonstrated: it is of-
ten reduced to a specific domain, such as the med-
ical (Chiao and Zweigenbaum, 2002) or financial
domains (Fung, 1998), or to a register, such as
newspaper articles (Fung, 1998). For terminology
664
mining, the comparability of the corpus should be
based on the domain or the sub-domaine, but also
on the type of discourse. Indeed, discourse acts
semantically upon the lexical units. For a defined
topic, some terms are specific to one discourse or
another. For example, for French, within the sub-
domain of obesity in the domain of medicine, we
find the term exc?s de poids (overweight) only in-
side texts sharing a popular science discourse, and
the synonym exc?s pond?ral (overweight) only in
scientific discourse. In order to evaluate how impor-
tant the discourse criterion is for building bilingual
terminological lists, we carried out experiments on
French-Japanese comparable corpora in the domain
of medicine, more precisely on the topic of diabetes
and nutrition, using texts collected from the Web and
manually selected and classified into two discourse
categories: one contains only scientific documents
and the other contains both scientific and popular
science documents.
We used a state-of-the-art multilingual terminol-
ogy mining chain composed of two term extraction
programs, one in each language, and an alignment
program. The term extraction programs are pub-
licly available and both extract multi-word terms
that are more precise and specific to a particular sci-
entific domain than single word terms. The align-
ment program makes use of the direct context-vector
approach (Fung, 1998; Peters and Picchi, 1998;
Rapp, 1999) slightly modified to handle both single-
and multi-word terms. We evaluated the candidate
translations of multi-word terms using a reference
list compiled from publicly available resources. We
found that taking discourse type into account re-
sulted in candidate translations of a better quality
even when the corpus size is reduced by half. Thus,
even using a state-of-the-art alignment method well-
known as data greedy, we reached the conclusion
that the quantity of data is not sufficient to obtain
a terminological list of high quality and that a real
comparability of corpora is required.
2 Multilingual terminology mining chain
Taking as input a comparable corpora, the multilin-
gual terminology chain outputs a list of single- and
multi-word candidate terms along with their candi-
date translations. Its architecture is summarized in
Figure 1 and comprises term extraction and align-
ment programs.
2.1 Term extraction programs
The terminology extraction programs are avail-
able for both French1 (Daille, 2003) and Japanese2
(Takeuchi et al, 2004). The terminological units
that are extracted are multi-word terms whose syn-
tactic patterns correspond either to a canonical or a
variation structure. The patterns are expressed us-
ing part-of-speech tags: for French, Brill?s POS tag-
ger3 and the FLEM lemmatiser4 are utilised, and for
Japanese, CHASEN5. For French, the main patterns
are N N, N Prep N et N Adj and for Japanese, N N,
N Suff, Adj N and Pref N. The variants handled are
morphological for both languages, syntactical only
for French, and compounding only for Japanese. We
consider as a morphological variant a morphological
modification of one of the components of the base
form, as a syntactical variant the insertion of another
word into the components of the base form, and as
a compounding variant the agglutination of another
word to one of the components of the base form. For
example, in French, the candidate MWT s?cr?tion
d?insuline (insulin secretion) appears in the follow-
ing forms:
  base form of N Prep N pattern: s?cr?tion
d?insuline (insulin secretion);
  inflexional variant: s?cr?tions d?insuline (in-
sulin secretions);
  syntactic variant (insertion inside the base
form of a modifier): s?cr?tion pancr?atique
d?insuline (pancreatic insulin secretion);
  syntactic variant (expansion coordination of
base form): secr?tion de peptide et d?insuline
(insulin and peptide secretion).
The MWT candidates secr?tion insulinique (insulin
secretion) and hypers?cr?tion insulinique (insulin
1http://www.sciences.univ-nantes.fr/
info/perso/permanents/daille/ and release
LINUX.
2http://research.nii.ac.jp/~koichi/
study/hotal/
3http://www.atilf.fr/winbrill/
4http://www.univ-nancy2.fr/pers/namer/
5http://chasen.org/$\sim$taku/software/
mecab/
665
WEB
dictionary
bilingual
Japanese documents French documents
terminology
extraction
terminology
extraction
lexical context
extraction
lexical context
extraction
process
translated
terms to be
translations
candidate
haversting
documents
lexical alignment
Figure 1: Architecture of the multilingual terminology mining chain
hypersecretion) have also been identified and lead
together with s?cr?tion d?insuline (insulin secretion)
to a cluster of semantically linked MWTs.
In Japanese, the MWT
 
. 
	
6 (in-
sulin secretion) appears in the following forms:
  base form of NN pattern:   /N  . 
	 /N  (insulin secretion);
  compounding variant (agglutination of a
word at the end of the base form):  

/N  . 	 /N  .  /N  (insulin secretion
ability)
At present, the Japanese term extraction program
does not cluster terms.
2.2 Term alignment
The lexical alignment program adapts the direct
context-vector approach proposed by (Fung, 1998)
for single-word terms (SWTs) to multi-word terms
(MWTs). It aligns source MWTs with target single
6For all Japanese examples, we explicitly segment the com-
pound into its component parts through the use of the ?.? sym-
bol.
words, SWTs or MWTs. From now on, we will refer
to lexical units as words, SWTs or MWTs.
2.2.1 Implementation of the direct
context-vector method
Our implementation of the direct context-vector
method consists of the following 4 steps:
1. We collect all the lexical units in the context of
each lexical unit  and count their occurrence
frequency in a window of  words around  .
For each lexical unit  of the source and the
target language, we obtain a context vector Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 55?63,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Compilation of Specialized Comparable Corpora in French and Japanese
Lorraine Goeuriot, Emmanuel Morin and B?atrice Daille
LINA - Universit? de Nantes
France
firstname.lastname@univ-nantes.fr
Abstract
We present in this paper the development
of a specialized comparable corpora com-
pilation tool, for which quality would be
close to a manually compiled corpus. The
comparability is based on three levels: do-
main, topic and type of discourse. Domain
and topic can be filtered with the keywords
used through web search. But the detec-
tion of the type of discourse needs a wide
linguistic analysis. The first step of our
work is to automate the detection of the
type of discourse that can be found in a
scientific domain (science and popular sci-
ence) in French and Japanese languages.
First, a contrastive stylistic analysis of the
two types of discourse is done on both lan-
guages. This analysis leads to the creation
of a reusable, generic and robust typology.
Machine learning algorithms are then ap-
plied to the typology, using shallow pars-
ing. We obtain good results, with an av-
erage precision of 80% and an average re-
call of 70% that demonstrate the efficiency
of this typology. This classification tool
is then inserted in a corpus compilation
tool which is a text collection treatment
chain realized through IBM UIMA system.
Starting from two specialized web docu-
ments collection in French and Japanese,
this tool creates the corresponding corpus.
1 Introduction
Comparable corpora are sets of texts in differ-
ent languages, that are not translations, but share
some characteristics (Bowker and Pearson, 2002).
They represent useful resources from which are
extracted multilingual terminologies (D?jean et
al., 2002) or multilingual lexicons (Fung and Yee,
1998). Comparable corpora are also used in
contrastive multilingual studies framework (Peters
and Picchi, 1997), they constitute a precious re-
source for translators (Laviosa, 1998) and teachers
(Zanettin, 1998), as they provide a way to observe
languages in use.
Their compilation is easier than parallel corpora
compilation, because translated resources are rare
and there is a lack of resources when the languages
involved do not include English. Furthermore, the
amount of multilingual documents available on the
Web ensures the possibility of automatically com-
piling them. Nevertheless, this task can not be
summarized to a simple collection of documents
sharing vocabulary. It is necessary to respect the
common characteristics of texts in corpora, es-
tablished before the compilation, according to the
corpus finality (McEnery and Xiao, 2007). Many
works are about compilation of corpora from the
Web (Baroni and Kilgarriff, 2006) but none, in our
knowledge, focuses on compilation of compara-
ble corpora, which has to satisfy many constraints.
We fix three comparability levels: domain, topic
and type of discourse. Our goal is to automate
recognition of these comparability levels in docu-
ments, in order to include them into a corpus. We
work on Web documents on specialized scientific
domains in French and Japanese languages. As
document topics can be filtered with keywords in
the Web search (Chakrabarti et al, 1999), we fo-
cus in this paper on automatic recognition of types
of discourse that can be found in scientific docu-
ments: science and popular science. This classi-
fication tool is then inserted in a specialized com-
parable corpora compilation tool, which is devel-
opped through the Unstructured Information Man-
55
agement Architecture (UIMA) (Ferrucci and Lally,
2004).
This paper is structured as follows. After an in-
troduction of related works in section 2, stylistic
analysis of our corpus will be presented in sec-
tion 3. This analysis will lead to the creation of
a typology of scientific and popular science dis-
course type in specifialized domains. The appli-
cation of learning algorithms to the typology will
be described in section 4, and the results will be
presented in section 5. We will show that our ty-
pology, based on linguistically motivated features,
can characterize science and popular science dis-
courses in French and Japanese documents, and
that the use of our three comparablility levels can
improve corpora comparability. Finally, we de-
scribe the development of the corpus compilation
tool.
2 Background
?A comparable corpus can be defined as a corpus
containing components that are collected using
the same sampling frame and similar balance and
representativeness? (McEnery and Xiao, 2007, p.
20). Comparability is ensured using character-
istics which can refer to the text creation con-
text (period, author...), or to the text itself (topic,
genre...). The choice of the common characteris-
tics, which define the content of corpora, affects
the degree of comparability, notion used to quan-
tify how two corpora can be comparable. The
choice of these characteristics depends on the fi-
nality of the corpus. Among papers on comparable
corpora, we distinguish two types of works, which
induces different choices:
? General language works, where texts of cor-
pora usually share a domain and a period.
Fung and Yee (1998) used a corpus composed
of newspaper in English and Chinese on a
specific period to extract words translations,
using IR and NLP methods. Rapp (1999)
used a English / German corpus, composed of
documents coming from newspapers as well
as scientific papers to study alignment meth-
ods and bilingual lexicon extraction from
non-parallel corpora (which can be consid-
ered as comparable);
? Specialized language works, where choice of
criteria is various. D?jean et al (2002) used a
corpus composed of scientific abstracts from
Medline, a medical portal, in English and
German. Thus they used documents sharing a
domain and a genre to extract bilingual termi-
nology. Chiao (2002) used a corpus of docu-
ments of medical domain on a specific topic
to work on the extraction of specialized ter-
minologies.
In general language works, documents of compa-
rable corpora often share characteristics like do-
main or topic. As they are usually extracted from
newspapers, it is important to limit them to a cer-
tain period to guarantee their comparability.
In specialized corpora, first levels of compara-
bility can be achieved with the domain and the
topic. Moreover, several communicative settings
appear in specialized language (Bowker and Pear-
son, 2002): expert-expert, expert-initiate, relative
expert to the uninitiated, teacher-pupil. Malrieu
and Rastier (2002) specify several levels of tex-
tual classification, each of which corresponding to
a certain granularity. The first level is discourse,
defined as a set of utterances from a enunciator
characterized by a global topical unit (Ducrot and
Todorov, 1972). The second level is genre, de-
fined as text categories distinguished by matured
speakers. For example, to literary discourse corre-
spond several genres: drama, poetry, prose. . . In-
spired by these communicative settings and tex-
tual categories, we choose to distinguish two com-
municative settings or type of discourse in spe-
cialized domains: science (texts written by ex-
perts to experts) and popular science (texts written
to non-experts, by experts, semi-experts or non-
experts). This comparability level, the type of dis-
course, reflects the context of production or usage
of the documents, and guarantees a lexical homo-
geneity in corpora (Bowker and Pearson, 2002, p.
27). Furthermore, Morin et al (2007) proved that
comparable corpora sharing a topic and a type of
discourse are well adapted for multilingual termi-
nologies extraction.
Our goal is to create a tool to compile compa-
rable corpora in French and Japanese which docu-
ments are extracted from the Web. We investigate
automatic categorization of documents according
to their type of discourse. This categorization is
based on a typology of elements characterizing
these types of discourse. To this end, we carry
out a stylistic and contrastive analysis (Karlgren,
1998). This analysis aims to highlight linguis-
tically motivated features through several dimen-
56
sions (structural, modal and lexical), whose com-
bination characterizes scientific or popular science
discourse. A specialized comparable corpus can
be compiled from a single type of discourse docu-
ment collection through several steps. Last part of
this paper focuses on the automation of these steps
using the IBM Unstructured Information Manage-
ment Architecture (UIMA).
3 Analysis of Types of Discourse
The recognition of types of discourse is based
on a stylistic analysis adapted from a deductive
and contrastive method, which purpose is to raise
discriminant and linguistically motivated features
characterizing these two types of discourse. Main
difficulty here is to find relevant features which fit
every language involved. These features, gathered
in a typology, will be used to adapt machine learn-
ing algorithms to compilation of corpora. This
typology thus needs to be robust, generic and
reusable in other languages and domains. Gener-
icity is ensured by a broad typology composed of
features covering a wide range of documents char-
acteristics, while robustness is guaranteed with
operational (computable) features and treatment
adaptable to Web documents as well as texts.
Sinclair (1996) distinguishes two levels of anal-
ysis in his report on text typologies: external level,
characterizing the context of creation of the docu-
ment; and internal level, corresponding to linguis-
tic characteristics of document. Because our cor-
pora are composed of documents extracted from
the Web, we consider external level features as
all the features related to the creation of docu-
ments and their structure (non-linguistic features)
and call them structural features. Stylistic analy-
sis raises several granularity levels among linguis-
tic characteristics of the texts. We thus distinguish
two levels in the internal dimension. Firstly, in
order to distinguish between scientific and pop-
ular science documents, we need to consider the
speaker in his speech: the modality. Secondly, sci-
entific discourse can be characterized by vocabu-
lary, word length and other lexical features. There-
fore our typology is based on three analysis levels:
structural, modal and lexical.
3.1 Structural Dimension
When documents are extracted from the Web, the
structure and the context of creation of the doc-
uments should be considered. In the framework
Feature French Japanese
URL pattern ?
Document?s format ? ?
Meta tags ? ?
Title tag ? ?
Pages layout ? ?
Pages background ? ?
Images ? ?
Links ? ?
Paragraphs ? ?
Item lists ? ?
Number of sentences ? ?
Typography ? ?
Document?s length ? ?
Table 1: Structural dimension features
of Web documents classification, several elements
bring useful information: pictures, videos and
other multimedia contents (Asirvatham and Ravi,
2001); meta-information, title and HTML struc-
ture (Riboni, 2002). While those information are
not often used in comparable corpora, they can be
used to classify them. Table 1 shows structural
features.
3.2 Modal Dimension
The degree of specialization required by the recip-
ient or reader is characterized by the relation built
in the utterance between the speaker or author and
the recipient or reader1. The tone and linguistic
elements in texts define this relation. The modal-
isation is an interpretation of the author?s attitude
toward the content of his/her assertion. Modali-
sation is characterized by many textual markers:
verbs, adverbs, politeness forms, etc. Presence of
the speaker and his position towards his speech
are quite different in scientific and popular science
discourse. Thus we think modalisation markers
can be relevant. For example, the speaker directly
speaks to the reader in some popular science doc-
uments: ?By eating well, you?ll also help to pre-
vent diabetes problems that can occur later in life,
like heart disease?. Whereas a scientific document
would have a neutral tone: ?Obesity plays a cen-
tral role in the insulin resistance syndrome, which
includes hyperinsulinemia, [. . . ] and an increased
risk of atherosclerotic cardiovascular disease?.
Most of the modal theories are language de-
pendent, and use description phenomena that are
specific to each language. Conversely, the theory
exposed in (Charaudeau, 1992) is rather indepen-
1Since we work on a scientific domain, we will consider
the speaker as the author of texts, and the recipient as the
reader.
57
dent of the language and operational for French
and Japanese (Ishimaru, 2006). According to Cha-
raudeau (1992, p.572), modalisation clarifies the
position of the speaker with respect to his reader,
to himself and to his speech. Modalisation is com-
posed of locutive acts, particular positions of the
author in his speech, and each locutive act is char-
acterized by modalities. We kept in his theory two
locutive acts involving the author:
Allocutive act: the author gets the reader in-
volved in the speech (ex.: ?You have to do
this.?);
Elocutive act: the author is involved in his own
speech, he reveals his position regarding his
speech (ex.: ?I would like to do this.?).
Each of these acts are then divided into several
modalities. These modalities are presented in ta-
ble 2 with English examples. Some of the modali-
ties are not used in a language or another, because
they are not frequent or too ambiguous.
3.3 Lexical Dimension
Biber (1988) uses lexical information to observe
variations between texts, especially between gen-
res and types of texts. Karlgren (1998) also use
lexical information to characterize text genres, and
use them to observe stylistic variations among
texts. Thus, we assume that lexical information
is relevant in the distinction between science and
popular science discourse. Firstly, because a spe-
cialized vocabulary is a principal characteristic of
specialized domain texts (Bowker and Pearson,
2002, p. 26). Secondly, because scientific docu-
ments contain more complex lexical units, nomi-
nal compounds or nominal sentences than popular
science documents (Sager, 1990).
Table 3 presents the lexical dimension features.
Note that these features show a higher language
dependency than other dimension features.
4 Automatic Classification by Type of
Discourse
The process of documents classification can be di-
vided into three steps: document indexing, classi-
fier learning and classifier evaluation (Sebastiani,
2002). Document indexing consists in building
a compact representation of documents that can
be interpreted by a classifier. In our case, each
document di is represented as a vector of fea-
tures weight: ~di = {w1i, . . . , wni} where n is the
Feature French Japanese
Specialized vocabulary ? ?
Numerals ? ?
Units of measurement ? ?
Words length ?
Bibliography ? ?
Bibliographic quotes ? ?
Punctuation ? ?
Sentences end ?
Brackets ? ?
Other alphabets (latin, ?
hiragana, katakana)
Symbols ?
Table 3: Lexical dimension features
Dimension Method
Structural Pattern matching
Modal Lexical and lexico-syntactic patterns
Lexical Lexical patterns
Table 4: Markers detection methods
number of features of the typology and wij is the
weight of the jth feature in the ith document. Each
feature weight is normalized, dividing the weight
by the total. Documents indexing is characterized
by our typology (section 3) and features imple-
mentation.
4.1 Features Implementation
In order to get a fast classification system, we priv-
ileged for the implementation of our typology fea-
tures shallow parsing such as lexical markers and
lexico-syntactic patterns (method for each dimen-
sion is detailed in table 4).
Structural Features We used 12 structural fea-
tures introduced in section 3.1. Most of these fea-
tures are achieved through pattern matching. For
example, URL patterns can determine is the docu-
ment belongs to websites such as hospital (http:
//www.chu-***.fr) or universities websites
(http://www.univ-***.fr), etc. As for
paragraphs, images, links, etc., one simple search
of HTML tags was made.
Modal Features Locutor presence markers in
a text can be implicit or ambiguous. We fo-
cused here on simple markers of his presence in
order to avoid noise in our results (high preci-
sion but weak recall). Thus we don?t recognize
all modal markers in a text but those recognized
are correct. There are pronouns which are spe-
cific to the speech act: for instance, for the eloc-
utive act, the French pronouns je (I) and nous
(we), and the Japanese pronouns? (I),?? (we)
58
Feature Example French Japanese
Allocutive modality
Allocutive personal pronouns You ?
Injunction modality Don?t do this ? ?
Authorization modality You can do this ?
Judgement modality Congratulations for doing it! ?
Suggestion modality You should do this ? ?
Interrogation modality When do you arrive? ? ?
Interjection modality How are you, Sir? ?
Request modality Please, do this ? ?
Elocutive modality
Elocutive personal I, we ? ?
Noticing modality We notice that he left ? ?
Knowledge modality I know that he left ? ?
Opinion modality I think he left ? ?
Will modality I would like him to leave ? ?
Promise modality I promise to be here ? ?
Declaration modality I affirm he left ?
Appreciation modality I like this ?
Commitment modality We have to do this ?
Possibility modality I can inform them ?
Table 2: Modal dimension features
and ?? (we). The modalities are also com-
puted with lexical markers. For example, the
modality of knowledge can be detected in French
with verbs like savoir, conna?tre (know), and in
Japanese with the verb ?? (know), with po-
lite form ?????? and with neutral form
?????.
Lexical Features Some of our lexical criteria
are specific to the scientific documents, like bib-
liographies and bibliographic quotations, special-
ized vocabulary or the measurement units. To
measure the terminological density (proportion of
specialized vocabulary in the text) in French, we
evaluate terms with stems of Greek-Latin (Namer
and Baud, 2007) and suffix characters of rela-
tional adjectives that are particularly frequent in
scientific domains (Daille, 2000). We listed about
50 stems such as inter-, auto- or nano-, and the
10 relational suffixes such such as -ique or -al.
For Japanese, we listed prefix characteristics of
names of disease or symptoms (??? (congen-
ital), ???(hereditary), etc.). These stems can
be found in both type of discourse, but not in the
same proportions. Specialized terms are used in
both type of discourse in different ways. For ex-
ample, the term ?ovarectomie? (ovarectomy) can
be frequent in a scientific document and used once
in a popular science documents to explain it and
then replaced by ?ablation des ovaires? (ovary ab-
lation). Sentences end are specific ending particles
used in japanese, for example the particle? is of-
ten used at the end of an interrogative sentence.
4.2 Learning Algorithms
Classifier learning is a process which observes fea-
tures weight of documents classified in a class
c or c and determine characteristics that a new
document should have to be classified in one of
these two classes 2. Given a document indexing,
there are some well-known algorithms that can
achieve this process (neural network, Bayes clas-
sifiers, SVM, etc.) of which Sebastiani (2002) car-
ried out a research about the assemblage and com-
parison. Applied to a Reuters newswires corpus,
these techniques showed variable performances in
the usage level of supervised or unsupervised ap-
proaches, of the size of the corpus, of the number
of categories, etc. We decided to use SVMlight
(Joachims, 2002) and C4.5 (Quinlan, 1993), since
both of them seem to be the most appropriate to
our data (small corpora, binary classification, less
than 100 features).
5 Experiments
In this section, we describe the two comparable
corpora used and present the two experiments car-
ried out with each of them. The first compara-
ble corpus is used to train the classifier in order
to learn a classification model based on our typol-
ogy (i.e. training task). The second comparable
corpus is used to evaluate the impact of the clas-
sification model when applied on new documents
(i.e. evaluation task).
2This is the binary case. See (Sebastiani, 2002) for other
cases.
59
5.1 Comparable Corpora
The corpora used in our experiments are both
composed of French and Japanese documents har-
vested from the Web. The documents were taken
from the medical domain, within the topic of di-
abetes and nutrition for training task, and breast
cancer for the evaluation task. Document harvest-
ing was carried out with a domain-based search
and a manual selection. Documents topic is fil-
tered using keywords reflecting the specialized
domain: for example alimentation, diab?te and
ob?sit? 3 for French part and ??? and ?? 4
for the Japanese part of the training task corpus.
Those keywords are directly related to the topic or
they can be synonyms (found on thesaurus) or se-
mantically linked terms (found in Web documents
collected). Then the documents were manually se-
lected by native speakers of each language who are
not domain specialists, and classified with respect
to their type of discourse: science (SC) or pop-
ular science (PS). Manual classification is based
on the following heuristics, to decide their type of
discourse:
? A scientific document is written by special-
ists to specialists.
? We distinguish two levels of popular science:
texts written by specialists for the general
public and texts written by the general pub-
lic for the general public. Without distinction
of these last two levels, we privileged doc-
uments written by specialists, assuming that
they may be richer in content and vocabulary
(for example advices from a doctor would be
richer and longer than forum discussions).
Our manual classification is based on the two
previous heuristics, and endorsed by several em-
pirical elements: website?s origin, vocabulary
used, etc. The classification of ambiguous docu-
ments has been validated by linguists. A few doc-
uments for which it was difficult to decide on the
type of discourse, such as those written by peo-
ple whose specialist status was not clear, were not
retained.
We thus created two comparable corpora:
? [DIAB_CP] related to the topic of diabetes
and nutrition and used to train the classifier.
3nutrition, diabetes, and obesity
4diabetes and overweight
? [BC_CP] related to the topic of breast cancer
and used to evaluate the effectiveness of the
classifier.
Table 5 shows the main features of each compa-
rable corpora: the number of documents, and the
number of words5 for each language and each type
of discourse.
# docs # words
[DIAB_CP]
FR SC 65 425,781PS 183 267,885
JP SC 119 234,857PS 419 572,430
[BC_CP]
FR SC 50 443,741PS 42 71,980
JP SC 48 211,122PS 51 123,277
Table 5: Basic data on each comparable corpora
5.2 Results
We present in this section two classification tasks:
? the first one consists in training and test-
ing classifiers with [DIAB_CP], using N-fold
cross validation method that consists in divid-
ing the corpus into n sub-samples of the same
size (we fix N = 5). Results are for 5 parti-
tioning on average;
? the second one consists in testing on [BC_CP]
the best classifier learned on [DIAB_CP], in
order to evaluate its impact on new docu-
ments.
Tables 6 and 7 show results of these two tasks.
On both table we present precision and recall
metrics with the two learning systems used. On
table 6, we can see that the results concerning
the French documents are quite satisfactory alto-
gether, with a recall on average of 87%, and a pre-
cision on average of 90% as for the classifier C4.5
(more than 215 documents are well classified from
248 French documents of [DIAB_CP]). The re-
sults of the classification in Japanese are also good
with the classifier C.4.5. More than 90% of doc-
uments are correctly classified, and the precision
reaches on average 80%. Some of the lower results
can be explained, especially in Japanese by the
high range of document genres in the corpus (re-
search papers, newspapers, scientific magazines,
recipes, job offers, forum discussions. . . ).
5For Japanese, the number of words is the number of oc-
currences recognized by ChaSen (Matsumoto et al, 1999)
60
French Japanese
Prec. Rec. Prec. Rec.
SC 1.00 0.36 0.70 0.65
svm
l
PS 0.80 1,00 0.72 0.80
SC 0.89 0.80 0.76 0.96
c4
.5
PS 0.91 0.94 0.95 0.99
Table 6: Precision and recall for each language,
each classifier, on [DIAB_CP]
Table 7 shows results on [BC_CP]. In general,
we note a decrease of the results with [BC_CP],
although results are still satisfactory. French doc-
uments are well classified whatever the classifier
is, with a precision higher than 75% and a recall
higher than 75%, which represent more than 70
well classified documents on 92. Japanese docu-
ments are well classified too, with 76% precision
and 77% recall on average, with 23 documents
wrong classified on 99. This classification model
is effective when it is applied to a different medi-
cal topic. This classification model seems efficient
to recognize scientific discourse from popular sci-
ence one in French and Japanese documents on a
particular topic.
French Japanese
Prec. Rec. Prec. Rec.
SC 0.92 0.53 0.90 0.61
svm
l
PS 0.64 0.95 0.66 0.98
SC 0.70 0.92 0.76 0.70
c4
.5
PS 0.87 0.56 0.75 0.80
Table 7: Precision and recall for each language,
each classifier, on [BC_CP]
6 Comparable Corpora Compilation
Tool
Compilation of a corpus, whatever type it is, is
composed of several steps.
1. Corpus Specifications: they must be defined
by the creator or user of the corpus. It in-
cludes decisions on its type, languages in-
volved, resources from which are extracted
documents, its size, etc. In the case of spe-
cialized comparable corpora, specifications
concern languages involved, size, resources
and documents domain, theme and type of
discourse. This step depends on the applica-
tive goals of the corpus and has to be done
carefully.
2. Documents Selection and Collection:
according to the resource, size and other
corpus criteria chosen during the first step,
documents are collected.
3. Documents Normalization and Annotation:
cleaning and linguistic treatments are applied
to documents in order to convert them into
raw texts and annotated texts.
4. Corpus Documentation: compilation of a
corpus that can be used in a durable way
must include this step. Documentation
of the corpus includes information about
the compilation (creator, date, method,
resources, etc.) and information about the
corpus documents. Text Encoding Initiative
(TEI) standard has been created in order to
conserve in an uniformed way this kind of
information in a corpus 6.
A corpus quality highly depends on the first two
steps. Moreover, these steps are directly linked to
the creator use of the corpus. The first step must
be realized by the user to create an relevant corpus.
Although second step can be computerizable (Ro-
gelio Nazar and Cabr?, 2008), we choose to keep
it manual in order to guarantee corpus quality. We
decided to work on a system which realizes the
last steps, i.e. normalization, annotation and docu-
mentation, starting from a collection of documents
selected by a user.
Our tool has been developed on Unstructured
Information Management Architecture (UIMA)
that has been created by IBM Research Divi-
sion (Ferrucci and Lally, 2004). Unstructured
data (texts, images, etc.) collections can be eas-
ily treated on this platform and many libraries are
available. Our tool starts with a web documents or
texts collection and is composed of several com-
ponents realizing each part of the creation of the
corpus:
1. the collection is loaded and documents are
converted to texts (with conversion tools
from pdf or html to text mainly);
2. all texts are cleaned and normalized (noise
from the conversion is cleaned, all texts are
converted into the same encoding, etc.);
6http://www.tei-c.org/index.xml
61
3. a pre-syntactic treatment is applied on texts
(segmentation mainly) to prepare them for
the following step;
4. morphologic and morpho-syntactic tagging
tools are applied on the texts (Brill tagger
(Brill, 1994) and Flemm lemmer (Namer,
2000) for French texts, Chasen (Matsumoto
et al, 1999) for Japanese);
5. texts are classified according to their type
of discourse: we use here the most efficient
SVMlight classifier. In fact, two corpus are
created, on for each type of discourse, then
the user can choose one of them. A vecto-
rial representation of each document is com-
puted, then these vectors are classified with
the classifier selected.
6. documentation is produced for the corpus, a
certain amount of information are included
and they can be easily completed by the user.
In reality, this tool is more a compilation assis-
tant than a compilator. It facilitates the compila-
tion task: the user is in charge of the most im-
portant part of the compilation, but the technical
part (treatment of each document) is realized by
the system. This guarantee a high quality in the
corpus.
7 Conclusion
This article has described a first attempt of com-
piling smart comparable corpora. The quality is
close to a manually collected corpus, and the high
degree of comparability is guaranteed by a com-
mon domain and topic, but also by a same type of
discourse. In order to detect automatically some of
the comparability levels, we carried out a stylistic
and contrastive analysis and elaborated a typology
for the characterization of scientific and popular
science types of discourse on the Web. This typol-
ogy is based on three aspects of Web documents:
the structural aspect, the modal aspect and lexi-
cal aspect. From the modality part, this distinction
is operational even on linguistically distant lan-
guages, as we proved by the validation on French
and Japanese. Our typology, implemented using
SVMlight and C4.5 learning algorithms brought
satisfactory results of classification, not only on
the training corpus but also on an evaluation cor-
pus, since we obtained a precision on average of
80% and a recall of 70%. This classifier has then
been included into a tool to assist specialized com-
parable corpora compilation. Starting from a Web
documents collection selected by the user, this
tool realizes cleaning, normalization and linguis-
tic treatment of each document and ?physically?
creates the corpus.
This tool is a first attempt and can be improved.
In a first time, we would like to assist the selection
and collection of documents, which could be real-
ized through the tool. Moreover, we would like to
investigate needs of comparable corpora users in
order to adapt our tool. Finally, others languages
could be added to the system, which represents a
quite time-consuming task: a classifier would have
to be created so all the linguistic analysis and clas-
sification tasks would have to be done again for
other languages.
Acknowledgement
This research program has been funded by the
French National Research Agency (ANR) through
the C-mantic project (ANR-07-MDCO-002-01)
2008-2010. We thank Yukie Nakao for the
japanese corpus and linguistic resources.
References
Arul Prakash Asirvatham and Kranthi Kumar Ravi.
2001. Web page classification based on document
structure. IEEE National Convention.
Marco Baroni and Adam Kilgarriff. 2006. Large
linguistically-processed web corpora for multiple
languages. In EACL?06, pages 87?90. The Associa-
tion for Computer Linguistics.
Douglas Biber. 1988. Variation across Speech and
Writing. Cambridge University Press.
Lynne Bowker and Jennifer Pearson. 2002. Working
with Specialized Language: A Practical Guide to
Using Corpora. London/New York, Routeledge.
Eric Brill. 1994. Some advances in transformation-
based part of speech tagging. In Proceedings of the
12th National Conference on Artificial Intelligence
(AAAI?94), pages 722?727, Seattle, WA, USA.
Soumen Chakrabarti, Martin van den Berg, and Byron
Dom. 1999. Focused crawling: a new approach
to topic-specific Web resource discovery. Computer
Networks (Amsterdam, Netherlands: 1999), 31(11?
16):1623?1640.
Patrick Charaudeau. 1992. Grammaire du sens et de
l?expression. Hachette.
62
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In COLING?02,
pages 1208?1212, Tapei, Taiwan.
B?atrice Daille. 2000. Morphological rule induction
for terminology acquisition. In COLING?00, pages
215?221, Sarrbrucken, Germany.
Herv? D?jean, ?ric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In COLING?02.
Oswald Ducrot and Tzvetan Todorov. 1972. Diction-
naire encyclop?dique des sciences du langage. ?di-
tions du Seuil.
David Ferrucci and Adam Lally. 2004. Uima: An
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10:327?348.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, com-
parable texts. In Christian Boitet and Pete White-
lock, editors, COLING?98, volume 1, pages 414?
420, Montreal, Quebec, Canada.
Kumiko Ishimaru. 2006. Comparative study on the
discourse of advertisement in France and Japan:
beauty products. Ph.D. thesis, Osaka University,
Japan.
Thorsten Joachims. 2002. Learning to Classify Text
using Support Vector Machines. Kluwer Academic
Publishers.
Jussi Karlgren, 1998. Natural Language Information
Retrieval, chapter Stylistic Experiments in Informa-
tion Retrieval. Tomek, Kluwer.
Sarah Laviosa. 1998. Corpus-based approaches to
contrastive linguistics and translation studies. Meta,
43(4):474?479.
Denise Malrieu and Francois Rastier. 2002. Genres et
variations morphosyntaxiques. Traitement Automa-
tique des Langues (TAL), 42(2):548?577.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
and Yoshitaka Hirano. 1999. Japanese Morpho-
logical Analysis System ChaSen 2.0 Users Manual.
Technical report, Nara Institute of Science and Tech-
nology (NAIST).
Anthony McEnery and Zhonghua Xiao. 2007. Par-
allel and comparable corpora: What is happening?
In Gunilla Anderman and Margaret Rogers, editors,
Incorporating Corpora: The Linguist and the Trans-
lator. Clevedon: Multilingual Matters.
Fiammetta Namer and Robert Baud. 2007. Defin-
ing and relating biomedical terms: Towards a cross-
language morphosemantics-based system. Interna-
tional Journal of Medical Informatics, 76(2-3):226?
233.
Fiametta Namer. 2000. Flemm : Un analyseur flexion-
nel du fran?ais ? base de r?gles. Traitement Automa-
tique des Langues (TAL), 41(2):523?548.
Carol Peters and Eugenio Picchi. 1997. Using lin-
guistic tools and resources in cross-language re-
trieval. In David Hull and Doug Oard, editors,
Cross-Language Text and Speech Retrieval. Papers
from the 1997 AAAI Spring Symposium, Technical
Report SS-97-05, pages 179?188.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Fran-
cisco, CA, USA.
Reinhard Rapp. 1999. Automatic Identification of
Word Translations from Unrelated English and Ger-
man Corpora. In ACL?99, pages 519?526, College
Park, Maryland, USA.
Daniele Riboni. 2002. Feature selection for web
page classification. In Hassan Shafazand and A Min
Tjoa, editors, Proceedings of the 1st EurAsian Con-
ference on Advances in Information and Communi-
cation Technology (EURASIA-ICT), pages 473?478,
Shiraz, Iran. Springer.
Jorge Vivaldi Rogelio Nazar and Teresa Cabr?. 2008.
A suite to compile and analyze an lsp corpus. In
Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odjik, Stelios Piperidis,
and Daniel Tapias, editors, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European
Language Resources Association (ELRA).
J. C. Sager. 1990. A Pratical Course in Terminology
Processing. John Benjamins, Amsterdam.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing Sur-
veys, 34(1):1?47.
John Sinclair. 1996. Preliminary recommendations on
text typology. Technical report, EAGLES (Expert
Advisory Group on Language Engineering Stan-
dards).
Federico Zanettin. 1998. Bilingual comparable
corpora and the training of translators. Meta,
43(4):616?630.
63
Proceedings of NAACL-HLT 2013, pages 298?305,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression
Florian Boudin and Emmanuel Morin
LINA - UMR CNRS 6241, Universite? de Nantes, France
{florian.boudin,emmanuel.morin}@univ-nantes.fr
Abstract
Multi-Sentence Compression (MSC) is the
task of generating a short single sentence sum-
mary from a cluster of related sentences. This
paper presents an N-best reranking method
based on keyphrase extraction. Compression
candidates generated by a word graph-based
MSC approach are reranked according to the
number and relevance of keyphrases they con-
tain. Both manual and automatic evaluations
were performed using a dataset made of clus-
ters of newswire sentences. Results show that
the proposed method significantly improves
the informativity of the generated compres-
sions.
1 Introduction
Multi-Sentence Compression (MSC) can be broadly
described as the task of generating a short single sen-
tence summary from a cluster of related sentences.
It has recently attracted much attention, mostly be-
cause of its relevance to single or multi-document
extractive summarization. A standard way to gen-
erate summaries consists in ranking sentences by
importance, cluster them by similarity and select a
sentence from the top ranked clusters (Wang et al,
2008). One difficulty is then to generate concise,
non-redundant summaries. Selected sentences al-
most always contain additional information specific
to the documents from which they came, leading to
readability issues in the summary.
Sentence Compression (SC), i.e. the task of
summarizing a sentence while retaining most of
the informational content and remaining grammat-
ical (Jing, 2000), is a straightforward solution to this
problem. Another solution would be to create, for
each cluster of related sentences, a concise and flu-
ent fusion of information, reflecting facts common
to all sentences. Originally defined as sentence fu-
sion (Barzilay and McKeown, 2005), MSC is a text-
to-text generation process in which a novel sentence
is produced as a result of summarizing common in-
formation across a set of similar sentences.
Most of the previous MSC approaches rely on
syntactic parsers for producing grammatical com-
pressions, e.g. (Filippova and Strube, 2008; El-
sner and Santhanam, 2011). Recently, (Filippova,
2010) proposed a word graph-based approach which
only requires a Part-Of-Speech (POS) tagger and a
list of stopwords. The key assumption behind her
approach is that redundancy within the set of related
sentences provides a reliable way of generating in-
formative and grammatical sentences. Although this
approach seemingly works well, 48% to 60% of the
generated sentences are missing important informa-
tion about the set of related sentences. In this study,
we aim at producing more informative sentences by
maximizing the range of topics they cover.
Keyphrases are words that capture the main top-
ics of a document. Extracting keyphrases can benefit
various Natural Language Processing tasks such as
summarization, information retrieval and question-
answering (Kim et al, 2010). In summarization,
keyphrases provide semantic metadata that represent
the content of a document. Sentences containing the
most relevant keyphrases are used to generate the
summary (D?Avanzo and Magnini, 2005). In the
same way, we hypothesize that keyphrases can be
used to better generate sentences that convey the gist
298
of the set of related sentences.
In this paper, we present a reranking method
of N-best multi-sentence compressions based on
keyphrase extraction and describe a series of experi-
ments conducted on a manually constructed evalua-
tion corpus. More precisely, the main contributions
of our work are as follows:
? We extend Filippova (2010)?s word graph-
based MSC approach to produce well-
punctuated and more informative compres-
sions.
? We investigate the use of automatic Machine
Translation (MT) and summarization evalua-
tion metrics to evaluate MSC performance.
? We introduce a French evaluation dataset made
of 40 sets of related sentences along with refer-
ence compressions composed by humans.
The rest of this paper is organized as follows. We
first briefly review the previous work, followed by
a description of the method we propose. Next, we
give the details of the evaluation dataset we have
constructed and present our experiments and results.
Lastly, we conclude with a discussion and directions
for further work.
2 Related work
2.1 Multi-sentence compression
MSC have received much attention recently and
many different approaches have been proposed. The
pioneering work of (Barzilay and McKeown, 2005)
introduced the framework used by many subsequent
works: input sentences are represented by depen-
dency trees, some words are aligned to merge the
trees into a lattice, and the lattice is linearized using
tree traversal to produce fusion sentences. (Filip-
pova and Strube, 2008) cast MSC as an integer linear
program, and show promising results for German.
Later, (Elsner and Santhanam, 2011) proposed a su-
pervised approach trained on examples of manually
fused sentences.
Previously described approaches require the use
of a syntactic parser to control the grammatical-
ity of the output. As an alternative, several word
graph-based approaches that only require a POS
tagger were proposed. The key assumption is
that redundancy provides a reliable way of gen-
erating grammatical sentences. First, a directed
word graph is constructed from the set of input sen-
tences in which nodes represent unique words, de-
fined as word and POS tuples, and edges express
the original structure of sentences (i.e. word order-
ing). Sentence compressions are obtained by find-
ing commonly used paths in the graph. Word graph-
based MSC approaches were used in different tasks,
such as guided microblog summarization (Sharifi
et al, 2010), opinion summarization (Ganesan et
al., 2010) and newswire summarization (Filippova,
2010).
2.2 Keyphrase extraction
Keyphrases are words that are representative of the
main content of documents. Extracting keyphrases
can benefit various Natural Language Processing
tasks such as summarization, information retrieval
and question-answering (Kim et al, 2010). Previ-
ous works fall into two categories: supervised and
unsupervised methods. The idea behind supervised
methods is to recast keyphrase extraction as a binary
classification task. A model is trained using anno-
tated data to determine whether a given phrase is a
keyphrase or not (Frank et al, 1999; Turney, 2000).
Unsupervised approaches proposed so far have in-
volved a number of techniques, including language
modeling (Tomokiyo and Hurst, 2003), graph-based
ranking (Mihalcea and Tarau, 2004; Wan and Xiao,
2008) and clustering (Liu et al, 2009). While super-
vised approaches have generally proven more suc-
cessful, the need for training data and the bias to-
wards the domain on which they are trained remain
two critical issues.
3 Method
In this section, we first describe Filippova (2010)?s
word graph-based MSC approach. Then, we present
the keyphrase extraction approach we use and our
method for reranking generated compressions.
3.1 Description of Filippova?s approach
Let G = (V, E) be a directed graph with the set
of vertices (nodes) V and a set of directed edges E,
where E is a subset of V ? V . Given a set of re-
lated sentences S = {s1, s2, ..., sn}, a word graph
is constructed by iteratively adding sentences to it.
299
world
's
pinta
tortoise
island
the
last kind
has
to
be
known
as
giant
away
-end-
his
lonesome
george
a
died
the
believed
of
passed
-start-
the
...
...
(3)
(2)
Figure 1: Word graph constructed from the set of related sentences, a possible compression path is also given.
Figure 1 is an illustration of the word graph con-
structed from the following sentences. For clarity,
edge weights are omitted and italicized fragments
from the sentences are replaced with dots.
1. Lonesome George, the world?s last Pinta Island
giant tortoise, has passed away.
2. The giant tortoise known as Lonesome George
died Sunday at the Galapagos National Park in
Ecuador.
3. He was only about a hundred years old, but
the last known giant Pinta tortoise, Lonesome
George, has passed away.
4. Lonesome George, a giant tortoise believed to
be the last of his kind, has died.
At the first step, the graph simply represents one
sentence plus the start and end symbols (?start? and
?end? in Figure 1). A node is added to G for each
word in the sentence, and words adjacent in the sen-
tence are connected with directed edges. A word
from the following sentences is mapped onto an ex-
isting node in the graph if they have the same lower-
cased word form and POS and that no word from this
sentence has already been mapped onto this node. A
new node is created if there is no suitable candidate
in the graph.
Words are added to the graph in the following or-
der:
i. non-stopwords for which no candidate exists in
the graph or for which an unambiguous map-
ping is possible;
ii. non-stopwords for which there are either sev-
eral possible candidates in the graph or which
occur more than once in the sentence;
iii. stopwords.
For the last two groups of words where mapping
is ambiguous (i.e. there are two or more nodes in
the graph that refer to the same word/POS tuple),
the immediate context (the preceding and following
words in the sentence and the neighboring nodes in
the graph) or the frequency (i.e. the node which has
words mapped onto it) are used to select the candi-
date node. We use the stopword list included in nltk1
extended with temporal nouns (e.g. monday, yester-
day).
In Filippova?s approach, punctuation marks are
excluded. To generate well-punctuated compres-
sions, we simply added a fourth step for adding
punctuation marks in the graph. When mapping is
ambiguous, we select the candidate which has the
same immediate context.
Once the words from a sentence are added to the
graph, words adjacent in the sentence are connected
with directed edges. Edge weights are calculated us-
ing the weighting function defined in Equation 1.
1http://nltk.org/
300
w(i, j) =
cohesion(i, j)
freq(i)? freq(j)
(1)
cohesion(i, j) =
freq(i) + freq(j)
?
s?S d(s, i, j)?1
(2)
where freq(i) is the number of words mapped to the
node i. The function d(s, i, j) refers to the distance
between the offset positions of words i and j in sen-
tence s.
The purpose of this function is two fold: i. to
generate a grammatical compression, links between
words which appear often in this order are favored
(see Equation 2); ii. to generate an informative com-
pression, the weight of edges connecting salient
nodes is decreased.
A K-shortest paths algorithm is then used to find
the 50 shortest paths from start to end nodes in the
graph. Paths shorter than eight words or that do not
contain a verb are filtered. The remaining paths are
reranked by normalizing the total path weight over
its length. The path which has the lightest average
edge weight is then considered as the best compres-
sion.
3.2 Reranking paths using keyphrases
The main difficulty of MSC is to generate sentences
that are both informative and grammatically correct.
Here, redundancy within the set of input sentences
is used to identify important words and salient links
between words. Although this approach seemingly
works well, important information is missing in 48%
to 60% of the generated sentences (Filippova, 2010).
One of the reasons for this is that node salience
is estimated only with the frequency measure. To
tackle this issue, we propose to rerank the N-best list
of compressions using keyphrases extracted from
the set of related sentences. Intuitively, an infor-
mative sentence should contain the most relevant
keyphrases. We propose to rerank generated com-
pressions according to the number and relevance of
keyphrases they contain.
An unsupervised method based on (Wan and
Xiao, 2008) is used to extract keyphrases from each
set of related sentences. This method is based on
the assumption that a word recommends other co-
occurring words, and the strength of the recommen-
dation is recursively computed based on the im-
portance of the words making the recommendation.
Keyphrase extraction can be divided into two steps.
First, a weighted graph is constructed from the set
of related sentences, in which nodes represent words
defined as word and POS tuples. Two nodes (words)
are connected if their corresponding lexical units co-
occur within a sentence. Edge weights are the num-
ber of times two words co-occur. TextRank (Mihal-
cea and Tarau, 2004), a graph-based ranking algo-
rithm that takes into account edge weights, is ap-
plied for computing a salience score for each node.
The score for node Vi is initialized with a default
value and is computed in an iterative manner until
convergence using this equation:
S(Vi) = (1?d)+d?
?
Vj?adj(Vi)
wji
?
Vk?adj(Vi)wjk
S(Vi)
where adj(Vi) denotes the neighbors of Vi and d is
the damping factor set to 0.85.
The second step consists in generating and scor-
ing keyphrase candidates. Sequences of adja-
cent words satisfying a specific syntactic pattern
are collapsed into multi-word phrases. We use
(ADJ)*(NPP|NC)+(ADJ)* for French, in which
ADJ are adjectives, NPP are proper nouns and NC
are common nouns.
The score of a candidate keyphrase k is computed
by summing the salience scores of the words it con-
tains normalized by its length + 1 to favor longer
n-grams (see equation 3).
score(k) =
?
w?k TextRank(w)
length(k) + 1
(3)
The small vocabulary size as well as the high
redundancy within the set of related sentences are
two factors that make keyphrase extraction easier
to achieve. On the other hand, a large number
of the generated keyphrases are redundant. Some
keyphrases may be contained within larger ones,
e.g. giant tortoise and Pinta Island giant tortoise. To
solve this problem, generated keyphrases are clus-
tered using word overlap. For each cluster, we then
select the keyphrase with the highest score. This fil-
tering process enables the generation of a smaller
subset of keyphrases while having a better coverage
of the cluster content.
301
Reranking techniques can suffer from the limited
scope of the N-best list, which may rule out many
potentially good candidates. For this reason, we use
a larger number of paths than the one in (Filippova,
2010). Accordingly, the K-shortest paths algorithm
is used to find the 200 shortest paths. We rerank the
paths by normalizing the total path weight over its
length multiplied by the sum of keyphrase scores it
contains. The score of a sentence compression c is
given by:
score(c) =
?
i,j?path(c)w(i,j)
length(c)?
?
k?c score(k)
(4)
4 Experimental settings
4.1 Construction of the evaluation dataset
To our knowledge, there is no dataset available to
evaluate MSC in an automatic way. The perfor-
mance of the previously described approaches was
assessed by human judges. In this work, we intro-
duce a new evaluation dataset made of 40 sets of re-
lated sentences along with reference compressions
composed by human assessors. The purpose of this
dataset is to investigate the use of existing automatic
evaluation metrics for the MSC task.
Similar to (Filippova, 2010), we collected news
articles presented in clusters on the French edition of
Google News2 over a period of three months. Clus-
ters composed of at least 20 news articles and con-
taining one single prevailing event were manually
selected. To obtain the sets of related sentences, we
extracted the first sentences from each article in the
cluster, removing duplicates. Leading sentences in
news articles are known to provide a good summary
of the article content and are used as a baseline in
summarization (Dang, 2005).
The resulting dataset contains 618 sentences (33
tokens on average) spread over 40 clusters. The
number of sentences within each cluster is on av-
erage 15, with a minimum of 7 and a maximum of
36. The word redundancy rate within the dataset,
computed as the number of unique words over the
number of words for each cluster, is 38.8%.
Three reference compressions were manually
composed for each set of sentences. Human an-
notators, all native French speakers, were asked to
2http://news.google.fr
carefully read the set of sentences, extract the most
salient facts and generate a sentence (compression)
that summarize the set of sentences. Annotators
were also told to introduce as little new vocabu-
lary as possible in their compressions. The purpose
of this guideline is to reduce the number of possi-
ble mismatches, as existing evaluation metrics are
based on n-gram comparison. Reference compres-
sions have a compression rate of 60%.
4.2 Automatic evaluation
The use of automatic methods for evaluating
machine-generated text has gradually become the
mainstream in Computational Linguistics. Well
known examples are the ROUGE (Lin, 2004) and
BLEU (Papineni et al, 2002) evaluation metrics used
in the summarization and MT communities. These
metrics assess the quality of a system output by com-
puting its similarity to one or more human-generated
references.
Prior work in sentence compression use the F1
measure over grammatical relations to evaluate can-
didate compressions (Riezler et al, 2003). It was
shown to correlate significantly with human judg-
ments (Clarke and Lapata, 2006) and behave sim-
ilarly to BLEU (Unno et al, 2006). However,
this metric is not entirely reliable as it depends on
parser accuracy and the type of dependency relations
used (Napoles et al, 2011). In this work, the fol-
lowing evaluation measures are considered relevant:
BLEU3, ROUGE-1 (unigrams), ROUGE-2 (bigrams)
and ROUGE-SU4 (bigrams with skip distance up to
4 words)4. ROUGE measures are computed using
stopword removal and French stemming 5.
4.3 Manual evaluation
The quality of the generated compressions was as-
sessed in an experiment with human raters. Two as-
pects were considered: grammaticality and informa-
tivity. Following previous work (Barzilay and McK-
eown, 2005), we asked raters to assess grammati-
cality on a 3-points scale: perfect (2 pts), if the com-
pression is a complete grammatical sentence; almost
3ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a.pl
4We use the version 1.5.5 of the ROUGE package available
from http://www.berouge.com
5http://snowball.tartarus.org/
302
(1 pt), if it requires minor editing, e.g. one mistake
in articles, agreement or punctuation; ungrammati-
cal (0 pts), if it is none of the above. Raters were ex-
plicitly asked to ignore lack of capitalization while
evaluating grammaticality.
Informativity is evaluated according to the 3-
points scale defined in (Filippova, 2010): perfect (2
pts), if the compression conveys the gist of the main
event and is more or less like the summary the per-
son would produce himself; related (1 pt), if it is
related to the the main theme but misses something
important; unrelated (0 pts), if the compression is
not related to the main theme.
Three raters, all native French speakers, were
hired to assess the generated compressions.
5 Results
To evaluate the effectiveness of our method, we
compare the compressions generated with Filip-
pova?s approach (denoted as baseline) against the
ones obtained by reranking paths using keyphrases
(denoted as KeyRank). We evaluated the agreement
between the three raters using Fleiss?s kappa (Art-
stein and Poesio, 2008). The ? value is 0.56 which
denotes a moderate agreement.
Table 1 presents the average grammaticality and
informativity scores. Results achieved by the base-
line are consistent with the ones presented in (Fil-
ippova, 2010). We observe a significant improve-
ment in informativity for KeyRank. Grammaticality
scores are, however, slightly decreased. One reason
for that is the reranking we added to the shortest path
method that outputs longer compressions. The aver-
age length for our method is nevertheless drastically
shorter than the average length of the input sentences
(19 vs. 33 tokens). This corresponds to a compres-
sion rate (58%) that is close to the one observed on
reference compressions (60%).
Table 2 shows the distributions over the three
scores for both grammaticality and informativity.
We observe that 97.5% of the compressions gener-
ated with KeyRank are related to the main theme
of the cluster, and 62.5% convey the very gist of
it without missing any important information. This
represents an absolute increase of 19.2% over the
baseline. Although our reranking method has lower
grammaticality scores, 65% of the generated sen-
Method Gram. Info.
Length
CompR
Avg. Std.Dev.
Baseline 1.63 1.33 16.3 4.8 50%
KeyRank 1.53 1.60? 19 6.1 58%
Table 1: Average ratings over all clusters and raters along
with average compression length (in tokens), standard de-
viation and corresponding compression rate (? indicates
significance at the 0.01 level using Student?s t-test).
tences are perfectly grammatical.
Method
Gram. Info.
0 1 2 0 1 2
Baseline 9.2% 18.3% 72.5% 10.0% 46.7% 43.3%
KeyRank 11.7% 23.3% 65.0% 2.5% 35.0% 62.5%
Table 2: Distribution over possible manual ratings for
grammaticality and informativity. Ratings are expressed
on a scale of 0 to 2.
Table 3 shows the performance of the baseline
and our reranking method in terms of ROUGE and
BLEU scores. KeyRank significantly outperforms
the baseline according to the different ROUGE met-
rics. This indicates an improvement in informativity
for the compressions generated using our method.
We observe a large but not significant increase in
BLEU scores. The slightly decreased grammatical-
ity scores could be a reason for this. BLEU is essen-
tially a precision metric, and it measures how well a
compression candidate overlaps with multiple refer-
ences. Longer n-grams used by BLEU6 tend to score
for grammaticality rather than content.
Metric Baseline KeyRank
ROUGE-1 0.57441 0.65677?
ROUGE-2 0.39212 0.44140?
ROUGE-SU4 0.37004 0.43443?
BLEU 0.61560 0.65770
Table 3: Automatic evaluation scores (? and ? indicate
significance at the 0.01 and 0.001 levels respectively us-
ing Student?s t-test)
To assess the effectiveness of automatic evalua-
6BLEU measures are computed using 4-grams.
303
tion metrics, we compute the Pearson?s correlation
coefficient between ROUGE and BLEU scores and
averaged manual ratings. According to Table 4, re-
sults show medium to strong correlation between
ROUGE scores and informativity ratings. On the
other hand, BLEU scores better correlate with gram-
maticality ratings. Overall, automatic evaluation
metrics are not highly correlated with manual rat-
ings. One reason for that may be that the manual
score assignments are arbitrary (i.e. 0, 1, 2), and that
a score of one is in fact closer to two than to zero.
Results suggest that automatic metrics do give an in-
dication of the compression quality, but can not re-
place manual evaluation.
Metric Gram. Info.
ROUGE-1 0.402 0.591
ROUGE-2 0.432 0.494
ROUGE-SU4 0.386 0.542
BLEU 0.444 0.401
Table 4: Pearson correlation coefficients for automatic
metrics vs. average human ratings.
6 Conclusion
This paper presented a multi-sentence compres-
sion approach that uses keyphrases to generate
more informative compressions. We extended Fil-
ippova (2010)?s word graph-based MSC approach
by adding a re-reranking step that favors compres-
sions that contain the most relevant keyphrases of
the input sentence set. An implementation of the
proposed multi-sentence compression approach is
available for download7. We constructed an eval-
uation dataset made of 40 sets of related sentences
along with reference compressions composed by hu-
mans. This dataset is freely available for download8.
We performed both manual and automatic evalua-
tions and showed that our method significantly im-
proves the informativity of the generated compres-
sions. We also investigated the correlation between
manual and automatic evaluation metrics and found
that ROUGE and BLEU have a medium correlation
with manual ratings.
7https://github.com/boudinfl/takahe
8https://github.com/boudinfl/lina-msc
In future work, we intend to examine how gram-
maticality of the generated compressions can be en-
hanced. Similar to the work of Hasan et al (2006) in
the Machine Translation field, we plan to experiment
with high order POS language models reranking.
Acknowledgments
The authors would like to thank Sebastia?n Pen?a Sal-
darriaga and Ophe?lie Lacroix for helpful comments
on this work. We thank the anonymous reviewers for
their useful comments. This work was supported by
the French Agence Nationale de la Recherche under
grant ANR-12-CORD-0027 and by the French Re-
gion Pays de Loire in the context of the DEPART
project (http://www.projetdepart.org/).
References
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Computational Linguis-
tics, 34(4):555?596.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
James Clarke and Mirella Lapata. 2006. Models for
sentence compression: A comparison across domains,
training requirements and evaluation measures. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 377?384, Sydney, Australia, July. Association
for Computational Linguistics.
Hoa Trang Dang. 2005. Overview of duc 2005. In Pro-
ceedings of the Document Understanding Conference.
Ernesto D?Avanzo and Bernardo Magnini. 2005. A
keyphrase-based approach to summarization: the lake
system at duc-2005. In Proceedings of the Document
Understanding Conference.
Micha Elsner and Deepak Santhanam. 2011. Learning to
fuse disparate sentences. In Proceedings of the Work-
shop on Monolingual Text-To-Text Generation, pages
54?63, Portland, Oregon, June. Association for Com-
putational Linguistics.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 177?185, Hon-
olulu, Hawaii, October. Association for Computational
Linguistics.
Katja Filippova. 2010. Multi-Sentence Compression:
Finding Shortest Paths in Word Graphs. In Proceed-
304
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 322?330,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: A Graph Based Approach to Abstrac-
tive Summarization of Highly Redundant Opinions. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 340?
348, Beijing, China, August. Coling 2010 Organizing
Committee.
S. Hasan, O. Bender, and H. Ney. 2006. Reranking trans-
lation hypotheses using structural properties. In Pro-
ceedings of the EACL Workshop on Learning Struc-
tured Information in Natural Language Applications,
pages 41?48.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the Sixth
Conference on Applied Natural Language Processing,
pages 310?315, Seattle, Washington, USA, April. As-
sociation for Computational Linguistics.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-
thy Baldwin. 2010. Semeval-2010 task 5 : Automatic
keyphrase extraction from scientific articles. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, pages 21?26, Uppsala, Sweden, July.
Association for Computational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun.
2009. Clustering to find exemplar terms for keyphrase
extraction. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, pages 257?266, Singapore, August. Association
for Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 404?411,
Barcelona, Spain, July. Association for Computational
Linguistics.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence compres-
sion: Pitfalls and suggested remedies. In Proceedings
of the Workshop on Monolingual Text-To-Text Gener-
ation, pages 91?97, Portland, Oregon, June. Associa-
tion for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 118?125. Association for Computational Lin-
guistics.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing Microblogs Automatically. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 685?
688, Los Angeles, California, June. Association for
Computational Linguistics.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment, pages 33?40, Sapporo, Japan, July. Association
for Computational Linguistics.
Peter D. Turney. 2000. Learning algorithms
for keyphrase extraction. Information Retrieval,
2(4):303?336.
Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and
Jun?ichi Tsujii. 2006. Trimming cfg parse trees
for sentence compression using machine learning ap-
proaches. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 850?857,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Xiaojun Wan and Jianguo Xiao. 2008. Collabrank:
Towards a collaborative approach to single-document
keyphrase extraction. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 969?976, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
Dingding Wang, Tao Li, Shenghuo Zhu, and Chris Ding.
2008. Multi-document Summarization via Sentence-
Level Semantic Analysis and Symmetric Matrix Fac-
torization. In Proceedings of the 31st annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ?08, pages
307?314, New York, NY, USA. ACM.
305
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1284?1293,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Looking at Unbalanced Specialized Comparable Corpora
for Bilingual Lexicon Extraction
Emmanuel Morin and Amir Hazem
Universit?e de Nantes, LINA UMR CNRS 6241
2 rue de la houssini`ere, BP 92208, 44322 Nantes Cedex 03, France
{emmanuel.morin,amir.hazem}@univ-nantes.fr
Abstract
The main work in bilingual lexicon ex-
traction from comparable corpora is based
on the implicit hypothesis that corpora are
balanced. However, the historical context-
based projection method dedicated to this
task is relatively insensitive to the sizes
of each part of the comparable corpus.
Within this context, we have carried out
a study on the influence of unbalanced
specialized comparable corpora on the
quality of bilingual terminology extraction
through different experiments. Moreover,
we have introduced a regression model
that boosts the observations of word co-
occurrences used in the context-based pro-
jection method. Our results show that the
use of unbalanced specialized comparable
corpora induces a significant gain in the
quality of extracted lexicons.
1 Introduction
The bilingual lexicon extraction task from bilin-
gual corpora was initially addressed by using par-
allel corpora (i.e. a corpus that contains source
texts and their translation). However, despite
good results in the compilation of bilingual lex-
icons, parallel corpora are scarce resources, es-
pecially for technical domains and for language
pairs not involving English. For these reasons,
research in bilingual lexicon extraction has fo-
cused on another kind of bilingual corpora com-
prised of texts sharing common features such as
domain, genre, sampling period, etc. without hav-
ing a source text/target text relationship (McEnery
and Xiao, 2007). These corpora, well known now
as comparable corpora, have also initially been
introduced as non-parallel corpora (Fung, 1995;
Rapp, 1995), and non-aligned corpora (Tanaka
and Iwasaki, 1996). According to Fung and Che-
ung (2004), who range bilingual corpora from par-
allel corpora to quasi-comparable corpora going
through comparable corpora, there is a continuum
from parallel to comparable corpora (i.e. a kind of
filiation).
The bilingual lexicon extraction task from com-
parable corpora inherits this filiation. For instance,
the historical context-based projection method
(Fung, 1995; Rapp, 1995), known as the standard
approach, dedicated to this task seems implicitly
to lead to work with balanced comparable corpora
in the same way as for parallel corpora (i.e. each
part of the corpus is composed of the same amount
of data).
In this paper we want to show that the assump-
tion that comparable corpora should be balanced
for bilingual lexicon extraction task is unfounded.
Moreover, this assumption is prejudicial for spe-
cialized comparable corpora, especially when in-
volving the English language for which many doc-
uments are available due the prevailing position
of this language as a standard for international
scientific publications. Within this context, our
main contribution consists in a re-reading of the
standard approach putting emphasis on the un-
founded assumption of the balance of the spe-
cialized comparable corpora. In specialized do-
mains, the comparable corpora are traditionally of
small size (around 1 million words) in comparison
with comparable corpus-based general language
(up to 100 million words). Consequently, the ob-
servations of word co-occurrences which is the ba-
sis of the standard approach are unreliable. To
make them more reliable, our second contribution
is to contrast different regression models in order
to boost the observations of word co-occurrences.
This strategy allows to improve the quality of ex-
tracted bilingual lexicons from comparable cor-
pora.
1284
2 Bilingual Lexicon Extraction
In this section, we first describe the standard ap-
proach that deals with the task of bilingual lexi-
con extraction from comparable corpora. We then
present an extension of this approach based on re-
gression models. Finally, we discuss works related
to this study.
2.1 Standard Approach
The main work in bilingual lexicon extraction
from comparable corpora is based on lexical con-
text analysis and relies on the simple observation
that a word and its translation tend to appear in
the same lexical contexts. The basis of this obser-
vation consists in the identification of ?first-order
affinities? for each source and target language:
?First-order affinities describe what other words
are likely to be found in the immediate vicinity
of a given word? (Grefenstette, 1994, p. 279).
These affinities can be represented by context vec-
tors, and each vector element represents a word
which occurs within the window of the word to
be translated (e.g. a seven-word window approxi-
mates syntactic dependencies). In order to empha-
size significant words in the context vector and to
reduce word-frequency effects, the context vectors
are normalized according to an association mea-
sure. Then, the translation is obtained by compar-
ing the source context vector to each translation
candidate vector after having translated each ele-
ment of the source vector with a general dictio-
nary.
The implementation of the standard approach
can be carried out by applying the following
three steps (Rapp, 1999; Chiao and Zweigenbaum,
2002; D?ejean et al, 2002; Morin et al, 2007;
Laroche and Langlais, 2010, among others):
Computing context vectors We collect all the
words in the context of each word i and count
their occurrence frequency in a window of
n words around i. For each word i of the
source and the target languages, we obtain
a context vector v
i
which gathers the set of
co-occurrence words j associated with the
number of times that j and i occur together
cooc(i, j). In order to identify specific words
in the lexical context and to reduce word-
frequency effects, we normalize context vec-
tors using an association score such as Mu-
tual Information, Log-likelihood, or the dis-
counted log-odds (LO) (Evert, 2005) (see
equation 1 and Table 1 where N = a + b +
c + d).
Transferring context vectors Using a bilingual
dictionary, we translate the elements of the
source context vector. If the bilingual dictio-
nary provides several translations for an ele-
ment, we consider all of them but weight the
different translations according to their fre-
quency in the target language.
Finding candidate translations For a word to be
translated, we compute the similarity be-
tween the translated context vector and all
target vectors through vector distance mea-
sures such as Jaccard or Cosine (see equa-
tion 2 where assoc
i
j
stands for ?association
score?, v
k
is the transferred context vector of
the word k to translate, and v
l
is the con-
text vector of the word l in the target lan-
guage). Finally, the candidate translations of
a word are the target words ranked following
the similarity score.
j ?j
i a = cooc(i, j) b = cooc(i,?j)
?i c = cooc(?i, j) d = cooc(?i,?j)
Table 1: Contingency table
LO(i, j) = log
(a +
1
2
) ? (d +
1
2
)
(b +
1
2
) ? (c +
1
2
)
(1)
Cosine
v
k
v
l
=
?
t
assoc
l
t
assoc
k
t
?
?
t
assoc
l
t
2
?
?
t
assoc
k
t
2
(2)
This approach is sensitive to the choice of pa-
rameters such as the size of the context, the choice
of the association and similarity measures. The
most complete study about the influence of these
parameters on the quality of word alignment has
been carried out by Laroche and Langlais (2010).
The standard approach is used by most re-
searchers so far (Rapp, 1995; Fung, 1998; Pe-
ters and Picchi, 1998; Rapp, 1999; Chiao and
Zweigenbaum, 2002; D?ejean et al, 2002; Gaussier
et al, 2004; Morin et al, 2007; Laroche and
Langlais, 2010; Prochasson and Fung, 2011;
1285
References Domain Languages Source/Target Sizes
Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words
Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data
Rapp (1999) Newspaper GE/EN 135/163 million words
Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words
D?ejean et al (2002) Medical GE/EN 100,000/100,000 words
Morin et al (2007) Medical FR/JP 693,666/807,287 words
Otero (2007) European Parliament SP/EN 14/17 million words
Ismail and Manandhar (2010) European Parliament EN/SP 500,000/500,000 sentences
Bouamor et al (2013) Financial FR/EN 402,486/756,840 words
- Medical FR/EN 396,524/524,805 words
Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction
Bouamor et al, 2013, among others) with the im-
plicit hypothesis that comparable corpora are bal-
anced. As McEnery and Xiao (2007, p. 21) ob-
serve, a specialized comparable corpus is built
as balanced by analogy with a parallel corpus:
?Therefore, in relation to parallel corpora, it is
more likely for comparable corpora to be designed
as general balanced corpora.?. For instance, Ta-
ble 2 describes the comparable corpora used in the
main work dedicated to bilingual lexicon extrac-
tion for which the ratio between the size of the
source and the target texts is comprised between
1 and 1.8.
In fact, the assumption that words which have
the same meaning in different languages should
have the same lexical context distributions does
not involve working with balanced comparable
corpora. To our knowledge, no attention
1
has
been paid to the problem of using unbalanced
comparable corpora for bilingual lexicon extrac-
tion. Since the context vectors are computed from
each part of the comparable corpus rather than
through the parts of the comparable corpora, the
standard approach is relatively insensitive to dif-
ferences in corpus sizes. The only precaution for
using the standard approach with unbalanced cor-
pora is to normalize the association measure (for
instance, this can be done by dividing each entry
of a given context vector by the sum of its associ-
ation scores).
2.2 Prediction Model
Since comparable corpora are usually small in spe-
cialized domains (see Table 2), the discrimina-
1
We only found mention of this aspect in Diab and Finch
(2000, p. 1501) ?In principle, we do not have to have the
same size corpora in order for the approach to work?.
tive power of context vectors (i.e. the observa-
tions of word co-occurrences) is reduced. One
way to deal with this problem is to re-estimate
co-occurrence counts by a prediction function
(Hazem and Morin, 2013). This consists in as-
signing to each observed co-occurrence count of
a small comparable corpora, a new value learned
beforehand from a large training corpus.
In order to make co-occurrence counts more
discriminant and in the same way as Hazem
and Morin (2013), one strategy consists in ad-
dressing this problem through regression: given
training corpora of small and large size (abun-
dant in the general domain), we predict word co-
occurrence counts in order to make them more
reliable. We then apply the resulting regression
function to each word co-occurrence count as a
pre-processing step of the standard approach. Our
work differs from Hazem and Morin (2013) in two
ways. First, while they experienced the linear re-
gression model, we propose to contrast different
regression models. Second, we apply regression
to unbalanced comparable corpora and study the
impact of prediction when applied to the source
texts, the target texts and both source and target
texts of the used comparable corpora.
We use regression analysis to describe the rela-
tionship between word co-occurrence counts in a
large corpus (the response variable) and word co-
occurrence counts in a small corpus (the predictor
variable). As most regression models have already
been described in great detail (Christensen, 1997;
Agresti, 2007), the derivation of most models is
only briefly introduced in this work.
As we can not claim that the prediction of word
co-occurrence counts is a linear problem, we con-
sider in addition to the simple linear regression
1286
model (Lin), a generalized linear model which is
the logistic regression model (Logit) and non lin-
ear regression models such as polynomial regres-
sion model (Poly
n
) of order n. Given an input
vector x ? R
m
, where x
1
,...,x
m
represent fea-
tures, we find a prediction y? ? R
m
for the co-
occurrence count of a couple of words y ? R us-
ing one of the regression models presented below:
y?
Lin
= ?
0
+ ?
1
x (3)
y?
Logit
=
1
1 + exp(?(?
0
+ ?
1
x))
(4)
y?
Poly
n
= ?
0
+ ?
1
x + ?
2
x
2
+ ... + ?
n
x
n
(5)
where ?
i
are the parameters to estimate.
Let us denote by f the regression function and
by cooc(w
i
, w
j
) the co-occurrence count of the
words w
i
and w
j
. The resulting predicted value of
cooc(w
i
, w
j
), noted ?cooc(w
i
, w
j
) is given by the
following equation:
?cooc(w
i
, w
j
) = f(cooc(w
i
, w
j
)) (6)
2.3 Related Work
In the past few years, several contributions have
been proposed to improve each step of the stan-
dard approach.
Prochasson et al (2009) enhance the represen-
tativeness of the context vector by strengthening
the context words that happen to be transliterated
words and scientific compound words in the target
language. Ismail and Manandhar (2010) also sug-
gest that context vectors should be based on the
most important contextually relevant words (in-
domain terms), and thus propose a method for fil-
tering the noise of the context vectors. In another
way, Rubino and Linar`es (2011) improve the con-
text words based on the hypothesis that a word and
its candidate translations share thematic similari-
ties. Yu and Tsujii (2009) and Otero (2007) pro-
pose, for their part, to replace the window-based
method by a syntax-based method in order to im-
prove the representation of the lexical context.
To improve the transfer context vectors step,
and increase the number of elements of translated
context vectors, Chiao and Zweigenbaum (2003)
and Morin and Prochasson (2011) combine a stan-
dard general language dictionary with a special-
ized dictionary, whereas D?ejean et al (2002) use
the hierarchical properties of a specialized the-
saurus. Koehn and Knight (2002) automatically
induce the initial seed bilingual dictionary by us-
ing identical spelling features such as cognates
and similar contexts. As regards the problem of
words ambiguities, Bouamor et al (2013) carried
out word sense disambiguation process only in
the target language whereas Gaussier et al (2004)
solve the problem through the source and target
languages by using approaches based on CCA
(Canonical Correlation Analysis) and multilingual
PLSA (Probabilistic Latent Semantic Analysis).
The rank of candidate translations can be im-
proved by integrating different heuristics. For in-
stance, Chiao and Zweigenbaum (2002) introduce
a heuristic based on word distribution symme-
try. From the ranked list of candidate translations,
the standard approach is applied in the reverse
direction to find the source counterparts of the
first target candidate translations. And then only
the target candidate translations that had the ini-
tial source word among the first reverse candidate
translations are kept. Laroche and Langlais (2010)
suggest a heuristic based on the graphic similarity
between source and target terms. Here, candidate
translations which are cognates of the word to be
translated are ranked first among the list of trans-
lation candidates.
3 Linguistic Resources
In this section, we outline the different textual re-
sources used for our experiments: the comparable
corpora, the bilingual dictionary and the terminol-
ogy reference lists.
3.1 Specialized Comparable Corpora
For our experiments, we used two specialized
French/English comparable corpora:
Breast cancer corpus This comparable corpus is
composed of documents collected from the
Elsevier website
2
. The documents were taken
from the medical domain within the sub-
domain of ?breast cancer?. We have auto-
matically selected the documents published
between 2001 and 2008 where the title or the
keywords contain the term cancer du sein in
French and breast cancer in English. We col-
lected 130 French documents (about 530,000
words) and 1,640 English documents (about
2
http://www.elsevier.com
1287
7.4 million words). We split the English doc-
uments into 14 parts each containing about
530,000 words.
Diabetes corpus The documents making up the
French part of the comparable corpus have
been craweled from the web using three
keywords: diab`ete (diabetes), alimentation
(food), and ob?esit?e (obesity). After a man-
ual selection, we only kept the documents
which were relative to the medical domain.
As a result, 65 French documents were ex-
tracted (about 257,000 words). The English
part has been extracted from the medical
website PubMed
3
using the keywords: dia-
betes, nutrition and feeding. We only kept
the free fulltext available documents. As a re-
sult, 2,339 English documents were extracted
(about 3,5 million words). We also split the
English documents into 14 parts each con-
taining about 250,000 words.
The French and English documents were then
normalised through the following linguistic pre-
processing steps: tokenisation, part-of-speech tag-
ging, and lemmatisation. These steps were car-
ried out using the TTC TermSuite
4
that applies
the same method to several languages including
French and English. Finally, the function words
were removed and the words occurring less than
twice in the French part and in each English part
were discarded. Table 3 shows the number of dis-
tinct words (# words) after these steps. It also
indicates the comparability degree in percentage
(comp.) between the French part and each English
part of each comparable corpus. The comparabil-
ity measure (Li and Gaussier, 2010) is based on
the expectation of finding the translation for each
word in the corpus and gives a good idea about
how two corpora are comparable. We can notice
that all the comparable corpora have a high degree
of comparability with a better comparability of the
breast cancer corpora as opposed to the diabetes
corpora. In the remainder of this article, [breast
cancer corpus i] for instance stands for the breast
cancer comparable corpus composed of the unique
French part and the English part i (i ? [1, 14]).
3.2 Bilingual Dictionary
The bilingual dictionary used in our experiments
is the French/English dictionary ELRA-M0033
3
http://www.ncbi.nlm.nih.gov/pubmed/
4
http://code.google.com/p/ttc-project
Breast cancer Diabetes
# words (comp.) # words (comp.)
French
Part 1 7,376 4,982
English
Part 1 8,214 (79.2) 5,181 (75.2)
Part 2 7,788 (78.8) 5,446 (75.9)
Part 3 8,370 (78.8) 5,610 (76.6)
Part 4 7,992 (79.3) 5,426 (74.8)
Part 5 7,958 (78.7) 5,610 (75.0)
Part 6 8,230 (79.1) 5,719 (73.6)
Part 7 8,035 (78.3) 5,362 (75.6)
Part 8 8,008 (78.8) 5,432 (74.6)
Part 9 8,334 (79.6) 5,398 (74.2)
Part 10 7,978 (79.1) 5,059 (75.6)
Part 11 8,373 (79.4) 5,264 (74.9)
Part 12 8,065 (78.9) 4,644 (73.4)
Part 13 7,847 (80.0) 5,369 (74.8)
Part 14 8,457 (78.9) 5,669 (74.8)
Table 3: Number of distinct words (# words) and
degree of comparability (comp.) for each compa-
rable corpora
available from the ELRA catalogue
5
. This re-
source is a general language dictionary which con-
tains only a few terms related to the medical do-
main.
3.3 Terminology Reference Lists
To evaluate the quality of terminology extrac-
tion, we built a bilingual terminology reference
list for each comparable corpus. We selected
all French/English single words from the UMLS
6
meta-thesaurus. We kept only i) the French sin-
gle words which occur more than four times in the
French part and ii) the English single words which
occur more than four times in each English part
i
7
. As a result of filtering, 169 French/English
single words were extracted for the breast can-
cer corpus and 244 French/English single words
were extracted for the diabetes corpus. It should
be noted that the evaluation of terminology ex-
traction using specialized comparable corpora of-
5
http://www.elra.info/
6
http://www.nlm.nih.gov/research/umls
7
The threshold sets to four is required to build a bilin-
gual terminology reference list composed of about a hundred
words. This value is very low to obtain representative context
vectors. For instance, Prochasson and Fung (2011) showed
that the standard approach is not relevant for infrequent words
(since the context vectors are very unrepresentative i.e. poor
in information).
1288
Breast cancer corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Balanced 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1
Unbalanced 26.1 31.9 34.7 36.0 37.7 36.4 36.6 37.2 39.8 40.5 40.6 42.3 40.9 41.6
Diabetes corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Balanced 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3
Unbalanced 13.6 17.5 18.9 21.2 23.4 23.8 24.8 24.7 24.7 24.4 24.8 25.2 26.0 24.9
Table 4: Results (MAP %) of the standard approach using the balanced and unbalanced comparable
corpora
ten relies on lists of a small size: 95 single
words in Chiao and Zweigenbaum (2002), 100 in
Morin et al (2007), 125 and 79 in Bouamor et
al. (2013).
4 Experiments and Results
In this section, we present experiments to evaluate
the influence of comparable corpus size and pre-
diction models on the quality of bilingual termi-
nology extraction.
We present the results obtained for the terms be-
longing to the reference list for English to French
direction measured in terms of the Mean Average
Precision (MAP) (Manning et al, 2008) as fol-
lows:
MAP (Ref) =
1
|Ref |
|Ref |
?
i=1
1
r
i
(7)
where |Ref | is the number of terms of the refer-
ence list and r
i
the rank of the correct candidate
translation i.
4.1 Standard Approach Evaluation
In order to evaluate the influence of corpus size on
the bilingual terminology extraction task, two ex-
periments have been carried out using the standard
approach. We first performed an experiment using
each comparable corpus independently of the oth-
ers (we refer to these corpora as balanced corpora).
We then conducted a second experiment where we
varied the size of the English part of the compara-
ble corpus, from 530,000 to 7.4 million words for
the breast cancer corpus in 530,000 words steps,
and from 250,000 to 3.5 million words for the di-
abetes corpus in 250,000 words steps (we refer to
these corpora as unbalanced corpora). In the ex-
periments reported here, the size of the context
window w was set to 3 (i.e. a seven-word window
that approximates syntactic dependencies), the re-
tained association and similarity measures were
the discounted log-odds and the Cosine (see Sec-
tion 2.1). The results shown were those that give
the best performance for the comparable corpora
used individually.
Table 4 shows the results of the standard ap-
proach on the balanced and the unbalanced breast
cancer and diabetes comparable corpora. Each
column corresponds to the English part i (i ?
[1, 14]) of a given comparable corpus. The first
line presents the results for each individual com-
parable corpus and the second line presents the re-
sults for the cumulative comparable corpus. For
instance, the column 3 indicates theMAP obtained
by using a comparable corpus that is composed i)
only of [breast cancer corpus 3] (MAP of 21.0%),
and ii) of [breast cancer corpus 1, 2 and 3] (MAP
of 34.7%).
As a preliminary remark, we can notice that the
results differ noticeably according to the compa-
rable corpus used individually (MAP variation be-
tween 21.0% and 29.6% for the breast cancer cor-
pora and between 10.5% and 16.5% for the dia-
betes corpora). We can also note that the MAP
of all the unbalanced comparable corpora is al-
ways higher than any individual comparable cor-
pus. Overall, starting with a MAP of 26.1% as
provided by the balanced [breast cancer corpus 1],
we are able to increase it to 42.3% with the un-
balanced [breast cancer corpus 12] (the variation
observed for some unbalanced corpora such as
[diabetes corpus 12, 13 and 14] can be explained
by the fact that adding more data in the source
language increases the error rate of the translation
phase of the standard approach, which leads to the
introduction of additional noise in the translated
context vectors).
1289
Balanced breast cancer corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
No prediction 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1
Source
pred
26.5 26.0 23.0 30.0 25.4 30.1 28.3 29.4 32.1 24.9 24.4 30.5 30.1 29.0
Target
pred
19.5 20.0 17.2 23.4 19.9 23.1 21.4 21.6 24.1 19.3 18.1 26.6 24.3 22.6
Source
pred
+ Target
pred
23.9 21.9 20.5 25.8 23.5 25.3 24.1 26.1 27.4 22.5 21.0 25.6 28.5 24.6
Balanced diabetes corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
No prediction 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3
Source
pred
13.9 14.3 12.6 15.5 14.9 10.9 17.6 11.1 14.0 14.2 16.4 13.3 13.5 15.7
Target
pred
09.8 09.0 08.3 11.9 10.1 08.0 15.9 07.3 10.8 10.0 10.1 08.8 10.8 10.2
Source
pred
+ Target
pred
10.9 11.0 09.0 13.6 11.8 08.6 15.4 07.7 12.8 11.5 11.9 10.5 11.7 11.8
Table 5: Results (MAP %) of the standard approach using the Lin regression model on the balanced
breast cancer and diabetes corpora (comparison of predicting the source side, the target side and both
sides of the comparable corpora)
4.2 Prediction Evaluation
The aim of this experiment is two-fold: first, we
want to evaluate the usefulness of predicting word
co-occurrence counts and second, we want to find
out whether it is more appropriate to apply predic-
tion to the source side, the target side or both sides
of the bilingual comparable corpora.
Breast cancer Diabetes
No prediction 29.6 16.5
Lin 30.5 17.6
Poly
2
30.6 17.5
Poly
3
30.4 17.6
Logit 22.3 13.6
Table 6: Results (MAP %) of the standard ap-
proach using different regression models on the
balanced breast cancer and diabetes corpora
4.2.1 Regression Models Comparison
We contrast the prediction models presented in
Section 2.2 to findout which is the most appropri-
ate model to use as a pre-processing step of the
standard approach. We chose the balanced corpora
where the standard approach has shown the best
results in the previous experiment, namely [breast
cancer corpus 12] and [diabetes corpus 7].
Table 6 shows a comparison between the
standard approach without prediction noted No
prediction and the standard approach with pre-
diction models. We contrast the simple linear re-
gression model (Lin) with the second and the third
order polynomial regressions (Poly
2
and Poly
3
)
and the logistic regression model (Logit). We
can notice that except for the Logit model, all the
regression models outperform the baseline (No
prediction). Also, as we can see, the results
obtained with the linear and polynomial regres-
sions are very close. This suggests that both linear
and polynomial regressions are suitable as a pre-
processing step of the standard approach, while
the logistic regression seems to be inappropriate
according to the results shown in Table 6.
That said, the gain of regression models is not
significant. This may be due to the regression pa-
rameters that have been learned from a training
corpus of the general domain. Another reason that
could explain these results is the prediction pro-
cess. We applied the same regression function
to all co-occurrence counts while learning mod-
els for low and high frequencies should have been
more appropriate. In the light of the above results,
we believe that prediction can be beneficial to our
task.
4.2.2 Source versus Target Prediction
Table 5 shows a comparison between the standard
approach without prediction noted No prediction
and the standard approach based on the predic-
tion of the source side noted Source
pred
, the tar-
get side noted Target
pred
and both sides noted
Source
pred
+Target
pred
. If prediction can not re-
place a large amount of data, it aims at increasing
co-occurrence counts as if large amounts of data
were at our disposal. In this case, applying pre-
diction to the source side may simulate a config-
uration of using unbalanced comparable corpora
where the source side is n times bigger than the
target side. Predicting the target side only, may
1290
1 2 3 4 5 6 7 8 9 10 11 12 13 140
5
10
15
20
25
30
35
40
45
50
[English-i]-French breast cancer corpus
M
AP
(%
)
Balanced
Balanced+Prediction
Unbalanced
Unbalanced+Prediction
(a)
1 2 3 4 5 6 7 8 9 10 11 12 13 140
5
10
15
20
25
30
35
[English-i]-French diabetes corpus
M
AP
(%
)
Balanced
Balanced+Prediction
Unbalanced
Unbalanced+Prediction
(b)
Figure 1: Results (MAP %) of the standard approach using the best configurations of the prediction
models (Lin for Balanced + Prediction and Poly
2
for Unbalanced + Prediction) on the breast
cancer and the diabetes corpora
leads us to the opposite configuration where the
target side is n times bigger than the source side.
Finally, predicting both sides may simulate a large
comparable corpora on both sides. In this experi-
ment, we chose to use the linear regression model
(Lin) for the prediction part. That said, the other
regression models have shown the same behavior
as Lin.
We can see that the best results are obtained by
the Source
pred
approach for both comparable cor-
pora. We can also notice that predicting the tar-
get side and both sides of the comparable corpora
degrades the results. It is not surprising that pre-
dicting the target side only leads to lower results,
since it is well known that a better characterization
of a word to translate (given from the source side)
leads to better results. We can deduce from Ta-
ble 5 that source prediction is the most appropriate
configuration to improve the quality of extracted
lexicons. This configuration which simulates the
use of unbalanced corpora leads us to think that
using prediction with unbalanced comparable cor-
pora should also increase the performance of the
standard approach. This assumption is evaluated
in the next Subsection.
4.3 Predicting Unbalanced Corpora
In this last experiment we contrast the standard
approach applied to the balanced and unbalanced
corpora noted Balanced and Unbalanced with
the standard approach combined with the predic-
tion model noted Balanced + Prediction and
Unbalanced + Prediction.
Figure 1(a) illustrates the results of the exper-
iments conducted on the breast cancer corpus.
We can see that the Unbalanced approach sig-
nificantly outperforms the baseline (Balanced).
The big difference between the Balanced and
the Unbalanced approaches would indicate that
the latter is optimal. We can also notice that the
prediction model applied to the balanced corpus
(Balanced + Prediction) slightly outperforms
the baseline while the Unbalanced+Prediction
approach significantly outperforms the three other
approaches (moreover the variation observed with
the Unbalanced approach are lower than the
Unbalanced + Prediction approach). Overall,
the prediction increases the performance of the
standard approach especially for unbalanced cor-
pora.
The results of the experiments conducted on
the diabetes corpus are shown in Figure 1(b). As
for the previous experiment, we can see that the
Unbalanced approach significantly outperforms
the Balanced approach. This confirms the unbal-
anced hypothesis and would motivate the use of
unbalanced corpora when they are available. We
can also notice that the Balanced + Prediction
approach slightly outperforms the baseline while
the Unbalanced+Prediction approach gives the
best results. Here also, the prediction increases the
performance of the standard approach especially
for unbalanced corpora. It is clear that in addi-
tion to the benefit of using unbalanced comparable
1291
corpora, prediction shows a positive impact on the
performance of the standard approach.
5 Conclusion
In this paper, we have studied how an unbalanced
specialized comparable corpus could influence the
quality of the bilingual lexicon extraction. This as-
pect represents a significant interest when working
with specialized comparable corpora for which the
quantity of the data collected may differ depend-
ing on the languages involved, especially when in-
volving the English language as many scientific
documents are available. More precisely, our dif-
ferent experiments show that using an unbalanced
specialized comparable corpus always improves
the quality of word translations. Thus, the MAP
goes up from 29.6% (best result on the balanced
corpora) to 42.3% (best result on the unbalanced
corpora) in the breast cancer domain, and from
16.5% to 26.0% in the diabetes domain. Addition-
ally, these results can be improved by using a pre-
diction model of the word co-occurrence counts.
Here, the MAP goes up from 42.3% (best result
on the unbalanced corpora) to 46.9% (best result
on the unbalanced corpora with prediction) in the
breast cancer domain, and from 26.0% to 29.8%
in the diabetes domain. We hope that this study
will pave the way for using specialized unbalanced
comparable corpora for bilingual lexicon extrac-
tion.
Acknowledgments
This work is supported by the French National Re-
search Agency under grant ANR-12-CORD-0020.
References
Alan Agresti. 2007. An Introduction to Categorical
Data Analysis (2nd ed.). Wiley & Sons, Inc., Hobo-
ken, New Jersey.
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2013. Context vector disambigua-
tion for bilingual lexicon extraction from compa-
rable corpora. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL?13), pages 759?764, Sofia, Bulgaria.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING?02), pages 1208?1212, Tapei,
Taiwan.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003.
The Effect of a General Lexicon in Corpus-Based
Identification of French-English Medical Word
Translations. In The New Navigators: from Profes-
sionals to Patients, Actes Medical Informatics Eu-
rope, pages 397?402.
Ronald Christensen. 1997. Log-Linear Models and
Logistic Regression. Springer-Verlag, Berlin.
Herv?e D?ejean, Fatia Sadat, and
?
Eric Gaussier. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In Proceedings of the 19th International Conference
on Computational Linguistics (COLING?02), pages
218?224, Tapei, Taiwan.
Mona T. Diab and Steve Finch. 2000. A Statistical
Word-Level Translation Model for Comparable Cor-
pora. In Proceedings of the 6th International Con-
ference on Computer-Assisted Information Retrieval
(RIAO?00), pages 1500?1501, Paris, France.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
Universit?at Stuttgart, Germany.
Pascale Fung and Percy Cheung. 2004. Multi-
level bootstrapping for extracting parallel sentences
from a quasi-comparable corpus. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics (COLING?04), pages 1051?1057,
Geneva, Switzerland.
Pascale Fung and Kathleen McKeown. 1997. Finding
Terminology Translations from Non-parallel Cor-
pora. In Proceedings of the 5th Annual Workshop
on Very Large Corpora (VLC?97), pages 192?202,
Hong Kong.
Pascale Fung. 1995. Compiling Bilingual Lexicon
Entries from a non-Parallel English-Chinese Cor-
pus. In Proceedings of the 3rd Annual Workshop
on Very Large Corpora (VLC?95), pages 173?183,
Cambridge, MA, USA.
Pascale Fung. 1998. A Statistical View on Bilin-
gual Lexicon Extraction: From Parallel Corpora to
Non-parallel Corpora. In David Farwell, Laurie
Gerber, and Eduard Hovy, editors, Proceedings of
the 3rd Conference of the Association for Machine
Translation in the Americas (AMTA?98), pages 1?
16, Langhorne, PA, USA.
?
Eric Gaussier, Jean-Michel Renders, Irena Matveeva,
Cyril Goutte, and Herv?e D?ejean. 2004. A
Geometric View on Bilingual Lexicon Extraction
from Comparable Corpora. In Proceedings of the
42nd Annual Meeting of the Association for Com-
putational Linguistics (ACL?04), pages 526?533,
Barcelona, Spain.
Gregory Grefenstette. 1994. Corpus-Derived First,
Second and Third-Order Word Affinities. In Pro-
ceedings of the 6th Congress of the European As-
sociation for Lexicography (EURALEX?94), pages
279?290, Amsterdam, The Netherlands.
1292
Amir Hazem and Emmanuel Morin. 2013. Word
co-occurrence counts prediction for bilingual ter-
minology extraction from comparable corpora. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing (IJCNLP?13),
pages 1392?1400, Nagoya, Japan.
Azniah Ismail and Suresh Manandhar. 2010. Bilingual
lexicon extraction from comparable corpora using
in-domain terms. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING?10), pages 481?489, Beijing, China.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition (ULA?02), pages 9?16,
Philadelphia, PA, USA.
Audrey Laroche and Philippe Langlais. 2010. Revis-
iting Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora. In
Proceedings of the 23rd International Conference
on Computational Linguistics (COLING?10), pages
617?625, Beijing, China.
Bo Li and
?
Eric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (COLING?10), pages 644?652, Beijing, China.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schtze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Anthony McEnery and Zhonghua Xiao. 2007. Paral-
lel and comparable corpora: What are they up to?
In Gunilla Anderman and Margaret Rogers, editors,
Incorporating Corpora: Translation and the Lin-
guist, Multilingual Matters, chapter 2, pages 18?31.
Clevedon, UK.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora (BUCC?11), pages 27?34, Portland,
OR, USA.
Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual Terminology
Mining ? Using Brain, not brawn comparable cor-
pora. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?07), pages 664?671, Prague, Czech Republic.
Pablo Gamallo Otero. 2007. Learning bilingual lexi-
cons from comparable english and spanish corpora.
In Proceedings of the 11th Conference on Machine
Translation Summit (MT Summit XI), pages 191?
198, Copenhagen, Denmark.
Carol Peters and Eugenio Picchi. 1998. Cross-
language information retrieval: A system for com-
parable corpus querying. In Gregory Grefenstette,
editor, Cross-language information retrieval, chap-
ter 7, pages 81?90. Kluwer Academic Publishers.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
Word Translation Extraction from Aligned Compa-
rable Documents. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics (ACL?11), pages 1327?1335, Portland, OR,
USA.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexicon
extraction from small comparable corpora. In Pro-
ceedings of the 12th Conference on Machine Trans-
lation Summit (MT Summit XII), pages 284?291, Ot-
tawa, Canada.
Reinhard Rapp. 1995. Identify Word Translations in
Non-Parallel Texts. In Proceedings of the 35th An-
nual Meeting of the Association for Computational
Linguistics (ACL?95), pages 320?322, Boston, MA,
USA.
Reinhard Rapp. 1999. Automatic Identification of
Word Translations from Unrelated English and Ger-
man Corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics (ACL?99), pages 519?526, College Park,
MD, USA.
Rapha?el Rubino and Georges Linar`es. 2011. A multi-
view approach for term translation spotting. In Pro-
ceedings of the 12th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing?11), pages 29?40, Tokyo, Japan.
Kumiko Tanaka and Hideya Iwasaki. 1996. Extraction
of Lexical Translations from Non-Aligned Corpora.
In Proceedings of the 16th International Conference
on Computational Linguistics (COLING?96), pages
580?585, Copenhagen, Denmark.
Kun Yu and Junichi Tsujii. 2009. Extracting bilin-
gual dictionary from comparable corpora with de-
pendency heterogeneity. In Proceedings of the
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT?09),
pages 121?124, Boulder, CO, USA.
1293
Bilingual Lexicon Extraction from Comparable Corpora Enhanced with
Parallel Corpora
Emmanuel Morin and Emmanuel Prochasson
Universite? de Nantes, LINA - UMR CNRS 6241
2 rue de la Houssinie`re, BP 92208
44322 Nantes Cedex 03
{emmanuel.morin,emmanuel.prochasson}@univ-nantes.fr
Abstract
In this article, we present a simple and ef-
fective approach for extracting bilingual lex-
icon from comparable corpora enhanced with
parallel corpora. We make use of structural
characteristics of the documents comprising
the comparable corpus to extract parallel sen-
tences with a high degree of quality. We then
use state-of-the-art techniques to build a spe-
cialized bilingual lexicon from these sentences
and evaluate the contribution of this lexicon
when added to the comparable corpus-based
alignment technique. Finally, the value of this
approach is demonstrated by the improvement
of translation accuracy for medical words.
1 Introduction
Bilingual lexicons are important resources of many
applications of natural language processing such
as cross-language information retrieval or machine
translation. These lexicons are traditionally ex-
tracted from bilingual corpora.
In this area, the main work involves parallel cor-
pora, i.e. a corpus that contains source texts and their
translations. From sentence-to-sentence aligned cor-
pora, symbolic (Carl and Langlais, 2002), statistical
(Daille et al, 1994), or hybrid techniques (Gaussier
and Lange?, 1995) are used for word and expression
alignments. However, despite good results in the
compilation of bilingual lexicons, parallel corpora
are rather scarce resources, especially for technical
domains and for language pairs not involving En-
glish. For instance, current resources of parallel cor-
pora are built from the proceedings of international
institutions such as the European Union (11 lan-
guages) or the United Nations (6 languages), bilin-
gual countries such as Canada (English and French
languages), or bilingual regions such as Hong Kong
(Chinese and English languages).
For these reasons, research in bilingual lexicon
extraction is focused on another kind of bilingual
corpora. These corpora, known as comparable cor-
pora, are comprised of texts sharing common fea-
tures such as domain, genre, register, sampling pe-
riod, etc. without having a source text-target text
relationship. Although the building of comparable
corpora is easier than the building of parallel cor-
pora, the results obtained thus far on comparable
corpora are contrasted. For instance, good results
are obtained from large corpora ? several million
words ? for which the accuracy of the proposed
translation is between 76% (Fung, 1998) and 89%
(Rapp, 1999) for the first 20 candidates. (Cao and
Li, 2002) have achieved 91% accuracy for the top
three candidates using the Web as a comparable cor-
pus. But for technical domains, for which large
corpora are not available, the results obtained, even
though encouraging, are not completely satisfactory
yet. For instance, (De?jean et al, 2002) obtained a
precision of 44% and 57% for the first 10 and 20
candidates in a 100,000-word medical corpus, and
35% and 42% in a multi-domain 8 million-word
corpus. For French/English single words, (Chiao
and Zweigenbaum, 2002) using a medical corpus
of 1.2 million words, obtained a precision of about
50% and 60% for the top 10 and top 20 candidates.
(Morin et al, 2007) obtained a precision of 51%
and 60% for the top 10 and 20 candidates in a 1.5
27
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 27?34,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
million-word French-Japanese diabetes corpus.
The above work in bilingual lexicon extraction
from comparable corpora relies on the assumption
that words which have the same meaning in different
languages tend to appear in the same lexical contexts
(Fung, 1998; Rapp, 1999). Based on this assump-
tion, a standard approach consists of building con-
text vectors for each word of the source and target
languages. The candidate translations for a partic-
ular word are obtained by comparing the translated
source context vector with all target context vectors.
In this approach, the translation of the words of the
source context vectors depends on the coverage of
the bilingual dictionary vis-a`-vis the corpus. This
aspect can be a potential problem if too few corpus
words are found in the bilingual dictionary (Chiao
and Zweigenbaum, 2003; De?jean et al, 2002).
In this article, we want to show how this prob-
lem can be partially circumvented by combining a
general bilingual dictionary with a specialized bilin-
gual dictionary based on a parallel corpus extracted
through mining of the comparable corpus. In the
same way that recent works in Statistical Machine
Translation (SMT) mines comparable corpora to dis-
cover parallel sentences (Resnik and Smith, 2003;
Yang and Li, 2003; Munteanu and Marcu, 2005;
Abdul-Rauf and Schwenk, 2009, among others),
this work contributes to the bridging of the gap be-
tween comparable and parallel corpora by offering
a framework for bilingual lexicon extraction from
comparable corpus with the help of parallel corpus-
based pairs of terms.
The remainder of this article is organized as fol-
lows. In Section 2, we first present the method for
bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora and the associ-
ated system architecture. We then quantify and anal-
yse in Section 3 the performance improvement of
our method on a medical comparable corpora when
used to extract specialized bilingual lexicon. Fi-
nally, in Section 4, we discuss the present study and
present our conclusions.
2 System Architecture
The overall architecture of the system for lexical
alignment is shown in Figure 1 and comprises par-
allel corpus- and comparable corpus-based align-
ments. Starting from a comparable corpus harvested
from the web, we first propose to extract parallel
sentences based on the structural characteristics of
the documents harvested. These parallel sentences
are then used to build a bilingual lexicon through
a tool dedicated to bilingual lexicon extraction. Fi-
nally, this bilingual lexicon is used to perform the
comparable corpus-based alignment. For a word to
be translated, the output of the system is a ranked
list of candidate translations.
2.1 Extracting Parallel Sentences from
Comparable Corpora
Parallel sentence extraction from comparable cor-
pora has been studied by a number of researchers
(Ma and Liberman, 1999; Chen and Nie, 2000;
Resnik and Smith, 2003; Yang and Li, 2003; Fung
and Cheung, 2004; Munteanu and Marcu, 2005;
Abdul-Rauf and Schwenk, 2009, among others) and
several systems have been developed such as BITS
(Bilingual Internet Test Search) (Ma and Liberman,
1999), PTMiner (Parallel Text Miner) (Chen and
Nie, 2000), and STRAND (Structural Translation
Recognition for Acquiring Natural Data) (Resnik
and Smith, 2003). Their work relies on the observa-
tion that a collection of texts in different languages
composed independently and based on sharing com-
mon features such as content, domain, genre, regis-
ter, sampling period, etc. contains probably some
sentences with a source text-target text relation-
ship. Based on this observation, dynamic program-
ming (Yang and Li, 2003), similarity measures such
as Cosine (Fung and Cheung, 2004) or word and
translation error ratios (Abdul-Rauf and Schwenk,
2009), or maximum entropy classifier (Munteanu
and Marcu, 2005) are used for discovering parallel
sentences.
Although our purpose is similar to these works,
the amount of data required by these techniques
makes them ineffective when applied to specialized
comparable corpora used to discover parallel sen-
tences. In addition, the focus of this paper is not to
propose a new technique for this task but to study
how parallel sentences extracted from a compara-
ble corpus can improve the quality of the candidate
translations. For theses reasons, we propose to make
use of structural characteristics of the documents
comprising the comparable corpus to extract auto-
28
bilingual
extraction
parallel sentences
 source documents  target documents
candidate
translations
terms to be
documents
harversting
The Web
extraction
lexical alignment
dictionary
bilingual
lexicon
process
extraction
lexical contexts lexical contexts
extraction
bilingual lexicons
parallel corpus?based alignment
comparable corpus?based alignment
translated
Figure 1: Overview of the system for lexical alignment
matically parallel sentences.
In fact, specialized comparable corpora are gener-
ally constructed via the consultation of specialized
Web portals. For instance, (Chiao and Zweigen-
baum, 2002) use CISMeF1 for building the French
part of their comparable corpora and CliniWeb2 for
the English part, and (De?jean and Gaussier, 2002)
use documents extracted from MEDLINE3 to build a
German/English comparable corpus. Consequently,
the documents collected through these portals are
often scientific papers. Moreover, when the lan-
guage of these papers is not the English, the paper
usually comprises an abstract, keywords and title in
the native language and their translations in the En-
glish language. These characteristics of scientific
paper is useful for the efficient extraction of parallel
sentences or word translations from the documents
forming a specialized comparable corpus for which
one part will inevitably be in English.
In this study, the documents comprising the
French/English specialized comparable corpus were
1http://www.chu-rouen.fr/cismef/
2http://www.ohsu.edu/cliniweb/
3http://www.ncbi.nlm.nih.gov/PubMed
taken from the medical domain within the sub-
domain of ?breast cancer?. These documents have
been automatically selected from the Elsevier web-
site4 among the articles published between 2001 and
2008 for which the title or the keywords of the arti-
cles contain the multi-word term ?cancer du sein? in
French and ?breast cancer? in English. We thus col-
lected 130 documents in French and 118 in English
and about 530,000 words for each language. Since
the 130 French documents previously collected are
scientific papers, each document contains a French
abstract which is accompanied by its English trans-
lation. We exploit this structural characteristic of the
French documents in order to build a small special-
ized parallel corpus directly correlated to the sub-
domain of ?breast cancer? involved in the compara-
ble corpus.
2.2 Parallel Corpus-Based Alignment
We use the Uplug5 collection of tools for alignment
(Tiedemann, 2003) to extract translations from our
4http://www.elsevier.com
5http://stp.ling.uu.se/cgi-bin/joerg/
Uplug
29
specialized parallel corpus. The output of such a tool
is a list of aligned parts of sentences, that has to be
post-process and filtered in our case. We clean the
alignment with a simple yet efficient method in order
to obtain only word translations. We associate every
source word from a source sequence with every tar-
get word from the target sequence. As an example,
uplug efficiently aligns the English word breast can-
cer with the French word cancer du sein (the data
are described in Section 3.1). We obtain the follow-
ing lexical alignment:
? cancer (fr) ? (en) breast, cancer
? du (fr) ? (en) breast, cancer
? sein (fr) ? (en) breast, cancer
With more occurrences of the French word can-
cer, we are able to align it with the English words
{breast, cancer, cancer, cancer, the, of, breast, can-
cer}. We can then filter such a list by counting the
translation candidates. In the previous example, we
obtain: cancer (fr) ? breast/2, the /1, of/1, can-
cer/4. The English word cancer is here the best
match for the French word cancer. In many cases,
only one alignment is obtained. For example, there
is only one occurrence of the French word chromo-
some, aligned with the English word chromosome.
In order to filter translation candidates, we keep
1:1 candidates if their frequencies are comparable
in the original corpus. We keep the most frequent
translation candidates (in the previous example, can-
cer) if their frequencies in the corpus are also com-
parable. This in-corpus frequency constraint is use-
ful for discarding candidates that appear in many
alignments (such as functional words). The criterion
for frequency acceptability is:
min(f1, f2)/max(f1, f2) > 2/3
with f1 and f2 the frequency of words to be aligned
in the parallel corpus.
By this way, we build a French/English special-
ized bilingual lexicon from the parallel corpus. This
lexicon, called breast cancer dictionary (BC dictio-
nary) in the remainder of this article, is composed of
549 French/English single words.
2.3 Comparable Corpus-Based Alignment
The comparable corpus-based alignment relies on
the simple observation that a word and its translation
tend to appear in the same lexical contexts. Based
on this observation, the alignment method, known as
the standard approach, builds context vectors in the
source and the target languages where each vector
element represents a word which occurs within the
window of the word to be translated (for instance a
seven-word window approximates syntactic depen-
dencies). In order to emphasize significant words
in the context vector and to reduce word-frequency
effects, the context vectors are normalized accord-
ing to association measures. Then, the translation is
obtained by comparing the source context vector to
each translation candidate vector after having trans-
lated each element of the source vector with a gen-
eral dictionary.
The implementation of this approach can be car-
ried out by applying the four following steps (Fung,
1998; Rapp, 1999):
1. We collect all the lexical units in the context of
each lexical unit i and count their occurrence
frequency in a window of n words around i.
For each lexical unit i of the source and the
target languages, we obtain a context vector vi
which gathers the set of co-occurrence units j
associated with the number of times that j and
i occur together occ(i, j). In order to iden-
tify specific words in the lexical context and to
reduce word-frequency effects, we normalize
context vectors using an association score such
as Mutual Information (MI) or Log-likelihood,
as shown in equations 1 and 2 and in Table 1
(where N = a + b + c + d).
2. Using a bilingual dictionary, we translate the
lexical units of the source context vector. If the
bilingual dictionary provides several transla-
tions for a lexical unit, we consider all of them
but weight the different translations according
to their frequency in the target language.
3. For a lexical unit to be translated, we com-
pute the similarity between the translated con-
text vector and all target vectors through vector
distance measures such as Cosine or Weighted
30
Jaccard (WJ) (see equations 3 and 4 where
associj stands for ?association score?).
4. The candidate translations of a lexical unit are
the target lexical units ranked following the
similarity score.
j ?j
i a = occ(i, j) b = occ(i,?j)
?i c = occ(?i, j) d = occ(?i,?j)
Table 1: Contingency table
MI(i, j) = log a(a + b)(a + c) (1)
?(i, j) = a log(a) + b log(b) + c log(c)
+d log(d) + (N) log(N)
?(a + b) log(a + b)
?(a + c) log(a + c)
?(b + d) log(b + d)
?(c + d) log(c + d)
(2)
Cosinevkvl =
?
t assoclt assockt??
t assoclt
2
??
t assockt
2
(3)
WJvkvl =
?
t min(assoclt, assockt )?
t max(assoclt, assockt )
(4)
This approach is sensitive to the choice of param-
eters such as the size of the context, the choice of
the association and similarity measures. The most
complete study about the influence of these param-
eters on the quality of bilingual alignment has been
carried out by Laroche and Langlais (2010).
3 Experiments and Results
In the previous section, we have introduced our com-
parable corpus and described the method dedicated
to bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In this sec-
tion, we then quantify and analyse the performance
improvement of our method on a medical compara-
ble corpus when used to extract specialized bilingual
lexicon.
3.1 Experimental Test bed
The documents comprising the French/English spe-
cialized comparable corpus have been normalised
through the following linguistic pre-processing
steps: tokenisation, part-of-speech tagging, and lem-
matisation. Next, the function words were removed
and the words occurring less than twice in the
French and the English parts were discarded. Fi-
nally, the comparable corpus comprised about 7,400
distinct words in French and 8,200 in English.
In this study, we used four types of bilingual dic-
tionary: i) the Wiktionary6 free-content multilin-
gual dictionary, ii) the ELRA-M00337 professional
French/English bilingual dictionary, iii) the MeSH8
metha-thesaurus, and iv) the BC dictionary (see Sec-
tion 2.2). Table 2 shows the main features of the dic-
tionaries, namely: the number of distinct French sin-
gle words in the dictionary (# SWs dico.), the num-
ber of distinct French single words in the dictionary
after projection on the French part of the compara-
ble corpus (# SWs corpus), and the number of trans-
lations per entry in the dictionary (# TPE). For in-
stance, 42% of the French context vectors could be
translated with the Wiktionary (3,099/7,400).
Table 2: Main features of the French/English dictionaries
Name # SWs # SWs # TPE
dict. corpus
Wiktionary 20,317 3,099 1.8
ELRA 50,330 4,567 2.8
MeSH 18,972 833 1.6
BC 549 549 1.0
In bilingual terminology extraction from special-
ized comparable corpora, the terminology refer-
ence list required to evaluate the performance of
the alignment programs are often composed of 100
single-word terms (SWTs) (180 SWTs in (De?jean
and Gaussier, 2002), 95 SWTs in (Chiao and
Zweigenbaum, 2002), and 100 SWTs in (Daille and
Morin, 2005)). To build our reference list, we se-
lected 400 French/English SWTs from the UMLS9
6http://www.wiktionary.org/
7http://www.elra.info/
8http://www.ncbi.nlm.nih.gov/mesh
9http://www.nlm.nih.gov/research/umls
31
meta-thesaurus and the Grand dictionnaire termi-
nologique10. We kept only the French/English pair
of SWTs which occur more than five times in each
part of the comparable corpus. As a result of filter-
ing, 122 French/English SWTs were extracted.
3.2 Experimental Results
In order to evaluate the influence of the parallel
corpus-based bilingual lexicon induced from the
comparable corpus on the quality of comparable cor-
pus based-bilingual terminology extraction, four ex-
periments were carried out. For each experiment,
we change the bilingual dictionary required for the
translation phase of the standard approach (see Sec-
tion 2.3):
1. The first experiment uses only the Wiktionary.
Since the coverage of the Wiktionary from the
comparable corpus is small (see Table 2), the
results obtained with this dictionary yield a
lower boundary.
2. The second experiment uses the Wiktionary
added to the BC dictionary. This experiment
attempts to verify the hypothesis of this study.
3. The third experiment uses the Wiktionary
added to the MeSH thesaurus. This experiment
attempts to determine whether a specialised
dictionary (in this case the MeSH) would be
more suitable than a specialized bilingual dic-
tionary (in this case the BC dictionary) directly
extracted from the corpus.
4. The last experiment uses only the ELRA dic-
tionary. Since the coverage of the ELRA dic-
tionary from the comparable corpus is the best
(see Table 2), the results obtained with this one
yield a higher boundary.
Table 3 shows the coverage of the four bilin-
gual lexical resources involved in the previous ex-
periments in the comparable corpus. The first col-
umn indicates the number of single words belong-
ing to a dictionary found in the comparable cor-
pus (# SWs corpus). The other column indicates
the coverage of each dictionary in the ELRA dic-
tionary (Coverage ELRA). Here, 98.9% of the sin-
gle words belonging to the Wiktionary are included
10http://www.granddictionnaire.com/
in the ELRA dictionary whereas less than 95% of
the single words belonging to the Wiktionary+BC
and Wiktionary+MeSH dictionaries are included in
the ELRA dictionary. Moreover, the MeSH and BC
dictionaries are two rather distinct specialized re-
sources since they have only 117 single words in
common.
Table 3: Coverage of the bilingual lexical resources in the
comparable corpus
Name # SWs Coverage
corpus ELRA
Wiktionary 3,099 98.8%
Wiktionary + BC 3,326 94.8%
Wiktionary + MeSH 3,465 94.9%
ELRA 4,567 100%
In the experiments reported here, the size of the
context window n was set to 3 (i.e. a seven-word
window), the association measure was the Mutual
Information and the distance measure the Cosine
(see Section 2.3). Other combinations of parameters
were assessed but the previous parameters turned out
to give the best performance.
Figure 2 summarises the results obtained for the
four experiments for the terms belonging to the ref-
erence list according to the French to English direc-
tion. As one could expect, the precision of the re-
sult obtained with the ELRA dictionary is the best
and the precision obtained with the Wiktionary is the
lowest. For instance, the ELRA dictionary improves
the precision of the Wiktionary by about 14 points
for the Top 10 and 9 points for the top 20. These
results confirm that the coverage of the dictionary is
an important factor in the quality of the results ob-
tained. Now, when you add the BC dictionary to
the Wiktionary, the results obtained are also much
better than those obtained with the Wiktionary alone
and very similar to those obtained with the ELRA
dictionary alone (without taking into account the top
5). This result suggests that a standard general lan-
guage dictionary enriched with a small specialized
dictionary can replace a large general language dic-
tionary.
Furthermore, this combination is more interesting
than the combination of the MeSH dictionary with
32
the Wiktionary. Since the BC dictionary is induced
from the corpus, this dictionary is directly correlated
to the theme of breast cancer involved in the cor-
pus. Consequently the BC dictionary is more suit-
able than the MeSH dictionary i) even if the MeSH
dictionary specializes in the medical domain and ii)
even if more words in the comparable corpus are
found in the MeSH dictionary than in the BC dic-
tionary.
This last observation should make us relativize the
claim: the greater the number of context vector el-
ements that are translated, the more discriminating
the context vector will be for selecting translations
in the target language. We must also take into ac-
count the specificity of the context vector elements
in accordance with the thematic of the documents
making up the corpus studied in order to improve
bilingual lexicon extraction from specialized com-
parable corpora.
5 10 15 20
10
20
30
40
50
60
70
80
Top
P
?
?
?
?
?
bc
bc
bc
bc
bc
ut
ut
ut
ut
ut
rs
rs
rs
rs
rs
Wiktionary ?
ELRA rs
Wiktionary + BC ut
Wiktionary + MeSH bc
Figure 2: Precision of translations found according to the
rank
4 Conclusion and Discussion
In this article, we have shown how the quality of
bilingual lexicon extraction from comparable cor-
pora could be improved with a small specialized
bilingual lexicon induced through parallel sentences
included in the comparable corpus. We have eval-
uated the performance improvement of our method
on a French/English comparable corpus within the
sub-domain of breast cancer in the medical domain.
Our experimental results show that this simple bilin-
gual lexicon, when combined with a general dic-
tionary, helps improve the accuracy of single word
alignments by about 14 points for the Top 10 and 9
points for the top 20. Even though we focus here
on one structural characteristic (i.e. the abstracts)
of the documents comprising the comparable corpus
to discover parallel sentences and induced bilingual
lexicon, the method could be easily applied to other
comparable corpora for which a bilingual dictionary
can be extracted by using other characteristics such
as the presence of parallel segments or paraphrases
in the documents making up the comparable corpus.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Communitys
Seventh Framework Programme (FP7/2007-2013)
under Grant Agreement no 248005.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL?09), pages 16?23, Athens,
Greece.
Yunbo Cao and Hang Li. 2002. Base Noun Phrase Trans-
lation Using Web Data and the EM Algorithm. In
Proceedings of the 19th International Conference on
Computational Linguistics (COLING?02), pages 127?
133, Tapei, Taiwan.
Michael Carl and Philippe Langlais. 2002. An Intelligent
Terminology Database as a Pre-processor for Statisti-
cal Machine Translation. In L.-F. Chien, B. Daille,
K. Kageura, and H. Nakagawa, editors, Proceed-
ings of the 2nd International Workshop on Computa-
tional Terminology (COMPUTERM?02), pages 15?21,
Tapei, Taiwan.
Jiang Chen and Jian-Yun Nie. 2000. Parallel Web Text
Mining for Cross-Language Information Retrieval. In
Proceedings of Recherche d?Information Assiste?e par
Ordinateur (RIAO?00), pages 62?77, Paris, France.
33
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING?02), pages 1208?1212, Tapei, Tai-
wan.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The
effect of a general lexicon in corpus-based identifica-
tion of French-English medical word translations. In
R. Baud, M. Fieschi, P. Le Beux, and P. Ruch, editors,
The New Navigators: from Professionals to Patients,
Actes Medical Informatics Europe, volume 95 of Stud-
ies in Health Technology and Informatics, pages 397?
402.
Be?atrice Daille and Emmanuel Morin. 2005. French-
English Terminology Extraction from Comparable
Corpora. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing (IJ-
CLNP?05), pages 707?718, Jeju Island, Korea.
Be?atrice Daille, ?Eric Gaussier, and Jean-Marc Lange?.
1994. Towards Automatic Extraction of Monolingual
and Bilingual Terminology. In Proceedings of the 15th
International Conference on Computational Linguis-
tics (COLING?94), volume I, pages 515?521, Kyoto,
Japan.
Herve? De?jean and ?Eric Gaussier. 2002. Une nouvelle ap-
proche a` l?extraction de lexiques bilingues a` partir de
corpus comparables. Lexicometrica, Alignement lexi-
cal dans les corpus multilingues, pages 1?22.
Herve? De?jean, Fatia Sadat, and ?Eric Gaussier. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of the 19th International Conference on
Computational Linguistics (COLING?02), pages 218?
224, Tapei, Taiwan.
Pascale Fung and Percy Cheung. 2004. Mining Very-
Non-Parallel Corpora: Parallel Sentence and Lexicon
Extraction via Bootstrapping and EM. In D. Lin and
D. Wu, editors, Proceedings of Empirical Methods
on Natural Language Processing (EMNLP?04), pages
57?63, Barcelona, Spain.
Pascale Fung. 1998. A Statistical View on Bilin-
gual Lexicon Extraction: From Parallel Corpora to
Non-parallel Corpora. In D. Farwell, L. Gerber, and
E. Hovy, editors, Proceedings of the 3rd Conference of
the Association for Machine Translation in the Ameri-
cas (AMTA?98), pages 1?16, Langhorne, PA, USA.
?Eric Gaussier and Jean-Marc Lange?. 1995. Mode`les
statistiques pour l?extraction de lexiques bilingues.
Traitement Automatique des Langues (TAL), 36(1?
2):133?155.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora. In
Proceedings of the 23rd International Conference on
Computational Linguistics (COLING?10), pages 617?
625, Beijing, China.
Xiaoyi Ma and Mark Y. Liberman. 1999. Bits: A
Method for Bilingual Text Search over the Web. In
Proceedings of Machine Translation Summit VII, Kent
Ridge Digital Labs, National University of Singapore.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual Terminology Mining ?
Using Brain, not brawn comparable corpora. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL?07), pages 664?
671, Prague, Czech Republic.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Exploit-
ing Non-Parallel Corpora. Computational Linguistics,
31(4):477?504.
Reinhard Rapp. 1999. Automatic Identification of Word
Translations from Unrelated English and German Cor-
pora. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics (ACL?99),
pages 519?526, College Park, MD, USA.
Philip Resnik and Noah A. Smith. 2003. The Web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
J. Tiedemann. 2003. Recycling Translations - Extraction
of Lexical Data from Parallel Corpora and their Appli-
cation in Natural Language Processing. Ph.D. thesis,
Studia Linguistica Upsaliensia 1.
Christopher C. Yang and Kar Wing Li. 2003. Au-
tomatic construction of English/Chinese parallel cor-
pora. Journal of the American Society for Information
Science and Technology, 54(8):730?742.
34
Bilingual Lexicon Extraction from Comparable Corpora as Metasearch
Amir Hazem and Emmanuel Morin
Universite? de Nantes,
LINA - UMR CNRS 6241
2 rue de la Houssinie`re,
BP 92208 44322 Nantes Cedex 03
amir.hazem@univ-nantes.fr
emmanuel.morin@univ-nantes.fr
Sebastian Pen?a Saldarriaga
1100 rue Notre-Dame Ouest,
Montre?al, Que?bec,
Canada H3C 1K3
spena@synchromedia.ca
Abstract
In this article we present a novel way of look-
ing at the problem of automatic acquisition
of pairs of translationally equivalent words
from comparable corpora. We first present
the standard and extended approaches tradi-
tionally dedicated to this task. We then re-
interpret the extended method, and motivate a
novel model to reformulate this approach in-
spired by the metasearch engines in informa-
tion retrieval. The empirical results show that
performances of our model are always better
than the baseline obtained with the extended
approach and also competitive with the stan-
dard approach.
1 Introduction
Bilingual lexicon extraction from comparable cor-
pora has received considerable attention since the
1990s (Rapp, 1995; Fung, 1998; Fung and Lo,
1998; Peters and Picchi, 1998; Rapp, 1999; Chiao
and Zweigenbaum, 2002a; De?jean et al, 2002;
Gaussier et al, 2004; Morin et al, 2007; Laroche
and Langlais, 2010, among others). This attention
has been motivated by the scarcity of parallel cor-
pora, especially for countries with only one official
language and for language pairs not involving En-
glish. Furthermore, as a parallel corpus is com-
prised of a pair of texts (a source text and a translated
text), the vocabulary appearing in the translated text
is highly influenced by the source text, especially in
technical domains. Consequently, comparable cor-
pora are considered by human translators to be more
trustworthy than parallel corpora (Bowker and Pear-
son, 2002). Comparable corpora are clearly of use
in the enrichment of bilingual dictionaries and the-
sauri (Chiao and Zweigenbaum, 2002b; De?jean et
al., 2002), and in the improvement of cross-language
information retrieval (Peters and Picchi, 1998).
According to (Fung, 1998), bilingual lexicon
extraction from comparable corpora can be ap-
proached as a problem of information retrieval (IR).
In this representation, the query would be the word
to be translated, and the documents to be found
would be the candidate translations of this word. In
the same way that as documents found, the candi-
date translations are ranked according to their rele-
vance (i.e. a document that best matches the query).
More precisely, in the standard approach dedicated
to bilingual lexicon extraction from comparable cor-
pora, a word to be translated is represented by a
vector context composed of the words that appear
in its lexical context. The candidate translations
for a word are obtained by comparing the translated
source context vector with the target context vectors
through a general bilingual dictionary. Using this
approach, good results on single word terms (SWTs)
can be obtained from large corpora of several million
words, with an accuracy of about 80% for the top 10-
20 proposed candidates (Fung and McKeown, 1997;
Rapp, 1999). Cao and Li (2002) have achieved 91%
accuracy for the top three candidates using the Web
as a comparable corpus. Results drop to 60% for
SWTs using specialized small size language cor-
pora (Chiao and Zweigenbaum, 2002a; De?jean and
Gaussier, 2002; Morin et al, 2007).
In order to avoid the insufficient coverage of the
bilingual dictionary required for the translation of
source context vectors, an extended approach has
35
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 35?43,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
been proposed (De?jean et al, 2002; Daille and
Morin, 2005). This approach can be seen as a query
reformulation process in IR for which similar words
are substituted for the word to be translated. These
similar words share the same lexical environments
as the word to be translated without appearing with
it. With the extended approach, (De?jean et al, 2002)
obtained for single French-English words 43% and
51% precision out of the ten and twenty first candi-
dates applied to a medical corpus of 100 000 words
(respectively 44% and 57% with the standard ap-
proach) and 79% and 84% precision on the ten and
twenty first candidates applied to a social science
corpus of 8 million words (respectively 35% and
42% with the standard approach). Within this con-
text, we want to show how metasearch engines can
be used for bilingual lexicon extraction from spe-
cialized comparable corpora. In particular, we will
focus on the use of different strategies to take full
advantage of similar words.
The remainder of this paper is organized as fol-
lows. Section 2 presents the standard and extended
approaches based on lexical context vectors dedi-
cated to word alignment from comparable corpora.
Section 3 describes our metasearch approach that
can be viewed as the combination of different search
engines. Section 4 describes the different linguistic
resources used in our experiments and evaluates the
contribution of the metasearch approach on the qual-
ity of bilingual terminology extraction through dif-
ferent experiments. Finally, Section 5 presents our
conclusions.
2 Related Work
In this section, we first describe the standard ap-
proach dedicated to word alignment from compara-
ble corpora. We then present an extension of this
approach.
2.1 Standard Approach
The main work in bilingual lexicon extraction from
comparable corpora is based on lexical context anal-
ysis and relies on the simple observation that a word
and its translation tend to appear in the same lexi-
cal contexts. The basis of this observation consists
in the identification of first-order affinities for each
source and target language: First-order affinities de-
scribe what other words are likely to be found in
the immediate vicinity of a given word (Grefenstette,
1994a, p. 279). These affinities can be represented
by context vectors, and each vector element repre-
sents a word which occurs within the window of
the word to be translated (for instance a seven-word
window approximates syntactical dependencies).
The implementation of this approach can be car-
ried out by applying the following four steps (Rapp,
1995; Fung and McKeown, 1997):
Context characterization
All the lexical units in the context of each lexical
unit i are collected, and their frequency in a window
of n words around i extracted. For each lexical unit
i of the source and the target languages, we obtain a
context vector i where each entry, ij , of the vector is
given by a function of the co-occurrences of units j
and i. Usually, association measures such as the mu-
tual information (Fano, 1961) or the log-likelihood
(Dunning, 1993) are used to define vector entries.
Vector transfer
The lexical units of the context vector i are trans-
lated using a bilingual dictionary. Whenever the
bilingual dictionary provides several translations for
a lexical unit, all the entries are considered but
weighted according to their frequency in the target
language. Lexical units with no entry in the dictio-
nary are discarded.
Target language vector matching
A similarity measure, sim(i, t), is used to score
each lexical unit, t, in the target language with re-
spect to the translated context vector, i. Usual mea-
sures of vector similarity include the cosine similar-
ity (Salton and Lesk, 1968) or the weighted jaccard
index (WJ) (Grefenstette, 1994b) for instance.
Candidate translation
The candidate translations of a lexical unit are the
target lexical units ranked following the similarity
score.
2.2 Extended Approach
The main shortcoming of the standard approach is
that its performance greatly relies on the coverage of
the bilingual dictionary. When the context vectors
36
mot Identify k
similar vectors
Source language Target language
mot word
mot word
mot word
mot word
mot word
...
...
word
word
word
Match vectors
in target language ...
Bilingual dictionary
Figure 1: Illustration of the extended approach.
are well translated, the translation retrieval rate in
the target language improves.
Although, the coverage of the bilingual dictionary
can be extended by using specialized dictionaries
or multilingual thesauri (Chiao and Zweigenbaum,
2003; De?jean et al, 2002), translation of context
vectors remains the core of the approach.
In order to be less dependent on the coverage
of the bilingual dictionary, De?jean and Gaussier
(2002) have proposed an extension to the standard
approach. The basic intuition of this approach is
that words sharing the same meaning will share
the same environments. The approach is based
on the identification of second-order affinities in
the source language: Second-order affinities show
which words share the same environments. Words
sharing second-order affinities need never appear
together themselves, but their environments are sim-
ilar (Grefenstette, 1994a, p. 280).
Generally speaking, a bilingual dictionary is a
bridge between two languages established by its en-
tries. The extended approach is based on this ob-
servation and avoids explicit translation of vectors
as shown in Figure 1. The implementation of this
extended approach can be carried out in four steps
where the first and last steps are identical to the stan-
dard approach (De?jean and Gaussier, 2002; Daille
and Morin, 2005):
Reformulation in the target language
For a lexical unit i to be translated, we identify
the k-nearest lexical units (k nlu), among the dic-
tionary entries corresponding to words in the source
language, according to sim(i, s). Each nlu is trans-
lated via the bilingual dictionary, and the vector in
the target language, s, corresponding to the transla-
tion is selected. If the bilingual dictionary provides
several translations for a given unit, s is given by
the union of the vectors corresponding to the trans-
lations. It is worth noting that the context vectors are
not translated directly, thus reducing the influence of
the dictionary.
Vector matching against reformulations
The similarity measure, sim(s, t), is used to score
each lexical unit, t, in the target language with re-
spect to the k nlu. The final score assigned to each
unit, t, in the target language is given by:
sim(i, t) = ?
s?kNLU
sim(i, s)? sim(s, t) (1)
An alternate scoring function has been proposed
by Daille and Morin (2005). The authors computed
the centroid vector of the k nlu, then scored target
units with respect to the centroid.
3 The Metasearch Approach
3.1 Motivations
The approach proposed by De?jean and Gaussier
(2002) implicitly introduces the problem of select-
ing a good k. Generally, the best choice of k depends
on the data. Although several heuristic techniques,
like cross-validation, can be used to select a good
value of k, it is usually defined empirically.
The application of the extended approach (EA) to
our data showed that the method is unstable with
respect to k. In fact, for values of k over 20, the
precision drops significantly. Furthermore, we can-
not ensure result stability within particular ranges of
37
values. Therefore, the value of k should be carefully
tuned.
Starting from the intuition that each nearest lexi-
cal unit (nlu) contributes to the characterization of a
lexical unit to be translated, our proposition aims at
providing an algorithm that gives a better precision
while ensuring higher stability with respect to the
number of nlu. Pushing the analogy of IR style ap-
proaches (Fung and Lo, 1998) a step further, we pro-
pose a novel way of looking at the problem of word
translation from comparable corpora that is concep-
tually simple: a metasearch problem.
In information retrieval, metasearch is the prob-
lem of combining different ranked lists, returned
by multiple search engines in response to a given
query, in such a way as to optimize the performance
of the combined ranking (Aslam and Montague,
2001). Since the k nlu result in k distinct rankings,
metasearch provides an appropriate framework for
exploiting information conveyed by the rankings.
In our model, we consider each list of a given nlu
as a response of a search engine independently from
the others. After collecting all the lists of the se-
lected nlu?s, we combine them to obtain the final
similarity score. It is worth noting that all the lists
are normalized to maximize in such a way the con-
tribution of each nlu. A good candidate is the one
that obtains the highest similarity score which is cal-
culated with respect to the selected k. If a given can-
didate has a high frequency in the corpus, it may be
similar not only to the selected nearest lexical units
(k), but also to other lexical units of the dictionary. If
the candidate is close to the selected nlu?s and also
close to other lexical units, we consider it as a po-
tential noise (the more neighbours a candidate has,
the more it?s likely to be considered as noise). We
thus weight the similarity score of a candidate by
taking into account this information. We compare
the distribution of the candidate with the k nlu and
also with all its neighbours. This leads us to sup-
pose that a good candidate should be closer to the
selected nlu?s than the rest of its neighbours, if it?s
not the case there is more chances for this candidate
to be a wrong translation.
3.2 Proposed Approach
In the following we will describe our extension
to the method proposed by De?jean and Gaussier
(2002). The notational conventions adopted are re-
viewed in Table 1. Elaborations of definitions will
be given when the notation is introduced. In all our
experiments both terms and lexical units are single
words.
Symbol Definition
l a list of a given lexical unit.
k the number of selected nearest lex-
ical units (lists).
freq(w, k) the number of lists (k) in which a
term appears.
n all the neighbours of a given term.
u all the lexical units of the dictio-
nary.
wl a term of a given list l.
s(wl) the score of the term w in the list l.
maxl the maximum score of a given list
l.
maxAll the maximum score of all the lists.
snorm(wl) the normalized score of term w in
the list l.
s(w) the final score of a term w.
?w the regulation parameter of the
term w.
Table 1: Notational conventions.
The first step of our method is to collect each
list of each nlu. The size of the list has its impor-
tance because it determines how many candidates
are close to a given nlu. We noticed from our ex-
periments that, if we choose lists with small sizes,
we should lose information and if we choose lists
with large sizes, we could keep more information
than necessary and this should be a potential noise,
so we consider that a good size of each list should
be between 100 and 200 terms according to our ex-
periments.
After collecting the lists, the second step is to nor-
malize the scores. Let us consider the equation 2 :
snorm(wl) = s(wl)? maxlmaxAll (2)
We justify this by a rationale derived from two
observations. First, scores in different rankings are
compatible since they are based on the same simi-
larity measure (i.e., on the same scale). The second
observations follows from the first: if max (l) 
38
max (m), then the system is more confident about
the scores of the list l than m.
Using scores as fusion criteria, we compute the
similarity score of a candidate by summing its scores
from each list of the selected nlu?s :
s(w) = ?w ?
?k
l=1 snorm(wl)?n
l=1 snorm(wl)
(3)
the weight ? is given by :
?w = freq(w, k)? (u? (k ? freq(w, k)))(u? freq(w, n)) (4)
The aim of this parameter is to give more con-
fidence to a term that occurs more often with the
selected nearest neighbours (k) than the rest of its
neighbours. We can not affirm that the best candi-
date is the one that follows this idea, but we can nev-
ertheless suppose that candidates that appear with a
high number of lexical units are less confident and
have higher chances to be wrong candidates (we can
consider those candidates as noise). So, ? allows us
to regulate the similarity score, it is used as a confi-
dent weight or a regulation parameter. We will refer
to this model as the multiple source (MS) model. We
also use our model without using ? and refer to it by
(LC), this allows us to show the impact of ? in our
results.
4 Experiments and Results
4.1 Linguistic Resources
We have selected the documents from the Elsevier
website1 in order to obtain a French-English spe-
cialized comparable corpus. The documents were
taken from the medical domain within the sub-
domain of ?breast cancer?. We have automatically
selected the documents published between 2001 and
2008 where the title or the keywords contain the
term ?cancer du sein? in French and ?breast can-
cer? in English. We thus collected 130 documents
in French and 118 in English and about 530,000
words for each language. The documents compris-
ing the French/English specialized comparable cor-
pus have been normalized through the following lin-
guistic pre-processing steps: tokenisation, part-of-
1www.elsevier.com
speech tagging, and lemmatisation. Next, the func-
tion words were removed and the words occurring
less than twice (i.e. hapax) in the French and the
English parts were discarded. Finally, the compara-
ble corpus comprised about 7,400 distinct words in
French and 8,200 in English.
The French-English bilingual dictionary required
for the translation phase was composed of dictionar-
ies that are freely available on the Web. It contains,
after linguistic pre-processing steps, 22,300 French
single words belonging to the general language with
an average of 1.6 translations per entry.
In bilingual terminology extraction from special-
ized comparable corpora, the terminology refer-
ence list required to evaluate the performance of
the alignment programs are often composed of 100
single-word terms (SWTs) (180 SWTs in (De?jean
and Gaussier, 2002), 95 SWTs in (Chiao and
Zweigenbaum, 2002a), and 100 SWTs in (Daille
and Morin, 2005)). To build our reference list,
we selected 400 French/English SWTs from the
UMLS2 meta-thesaurus and the Grand dictionnaire
terminologique3. We kept only the French/English
pair of SWTs which occur more than five times in
each part of the comparable corpus. As a result of
filtering, 122 French/English SWTs were extracted.
4.2 Experimental Setup
Three major parameters need to be set to the ex-
tended approach, namely the similarity measure, the
association measure defining the entry vectors and
the size of the window used to build the context vec-
tors. Laroche and Langlais (2010) carried out a com-
plete study about the influence of these parameters
on the quality of bilingual alignment.
As similarity measure, we chose to use the
weighted jaccard index:
sim(i, j) =
?
t min (it, jt)?
t max (it, jt)
(5)
The entries of the context vectors were deter-
mined by the log-likelihood (Dunning, 1993), and
we used a seven-word window since it approximates
syntactic dependencies. Other combinations of pa-
rameters were assessed but the previous parameters
turned out to give the best performance.
2http://www.nlm.nih.gov/research/umls
3http://www.granddictionnaire.com/
39
4.3 Results
To evaluate the performance of our method, we use
as a baseline, the extended approach (EA) proposed
by De?jean and Gaussier (2002). We compare this
baseline to the two metasearch strategies defined
in Section 3: the metasearch model without the
regulation parameter ? (LC); and the one which is
weighted by theta (MS). We also provide results ob-
tained with the standard approach (SA).
We first investigate the stability of the metasearch
strategies with respect to the number of nlu consid-
ered. Figure 2 show the precision at Top 20 as a
function of k.
1 10 20 30 40 50 600
10
20
30
40
50
60
70
Number of NLU
Pre
cisi
on
att
op
20
LC
MS
EA
SA
Figure 2: Precision at top 20 as a function of the number
of nlu.
In order to evaluate the contribution of the param-
eter ?, we chose to evaluate the metasearch method
starting from k = 4, this explains why the precision
is extremely low for low values of k. We further
considered that less than four occurrences of a term
in the whole lexical units lists can be considered as
noise. On the other side, we started from k = 1 for
the extended approach since it makes no use of the
parameter ?. Figure 2 shows that extended approach
reaches its best performance at k = 7 with a preci-
sion of 40.98%. Then, after k = 15 the precision
starts steadily decreasing as the value of k increases.
The metasearch strategy based only on similarity
scores shows better results than the baseline. For
every value of k ? 10, the LC model outperform the
extended approach. The best precision (48.36%) is
obtained at k = 14, and the curve corresponding to
the LC model remains above the baseline regardless
of the increasing value of the parameter k. The curve
corresponding to the MS model is always above the
(EA) for every value of k ? 10. The MS model
consistently improves the precision, and achieves its
best performance (60.65%) at k = 21.
We can notice from Figure 2 that the LC and MS
models outperform the baseline (EA). More impor-
tantly, these models exhibit a better stability of the
precision with respect to the k-nearest lexical units.
Although the performance decrease as the value of k
increases, it does not decrease as fast as in the base-
line approach.
For the sake of comparability, we also provide
results obtained with the standard approach (SA)
(56.55%) represented by a straight line as it is not
dependent on k. As we can see, the metasearch
approach (MS) outperforms the standard approach
for values of k bertween 20 and 30 and for greater
values of k the precision remains more or less al-
most the same as the standard approach (SA). Thus,
the metasearch model (MS) can be considered as a
competitive approach regarding to its results as it is
shown in the figure 2.
Finally, Figure 3 shows the contribution of each
nlu taken independently from the others. This con-
firms our intuition that each nlu contribute to the
characterization of a lexical unit to be translated, and
supports our idea that their combination can improve
the performances.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
5
10
15
20
25
lexical units
Prec
isio
nat
top
20
Figure 3: Precision at top 20 for each of the 20 nlu. The
precision is computed by taking the each nlu indepen-
dently from the others.
Figure 3 shows the top 20 of each nlu. Notice
40
that the nlu are ordered from the most similar to the
lexical unit to be translated to the less similar, and
that each one of the nearest lexical units contains
information that it is worth taking into account.
Although each nlu can only translate few terms,
by using the metasearch idea we are able to improve
the retrieval of translation equivalents. The main
idea of the metasearch paradigm is to take into ac-
count the information conveyed by all the k nlu, us-
ing either similarity scores, their behaviour with all
the neighbours, in order to improve the performance
of the alignment process.
Although significant improvements can be ob-
tained with the metasearch models (comparatively
to the EA and SA approach), especially concerning
precision stability with respect to the k nlu, we be-
lieve that we need to address the estimation of k be-
forehand. Rather than fixing the same k for all the
units to be translated, there is the possibility to adapt
an optimal value of k to each lexical unit, according
to some criteria which have to be determined.
Approachs Top 5 Top 10 Top 15 Top 20
SA 37.70 45.08 52.45 56.55
EA 21.31 31.14 36.88 40.98
MS 40.98 54.91 56.55 60.65
Table 2: Precision(%) at top 5, 10, 15, 20 for SA, EA and
MS.
Finally, we present in table 2 a comparison be-
tween SA, EA and MS for the top 5, 10, 15 and 20.
By choosing the best configuration of each method,
we can note that our method outperforms the others
in each top. In addition, for the top 10 our preci-
sion is very close to the precision of the standard ap-
proach (SA) at the top 20. we consider these results
as encouraging for future work.
4.4 Discussion
Our experiments show that the parameter k remains
the core of both EA and MS approaches. A good
selection of the nearest lexical units of a term guar-
antee to find the good translation. It is important to
say that EA and MS which are based on the k nlu?s
depends on the coverage of the terms to be trans-
lated. Indeed, these approaches face three cases :
firstly, if the frequency of the word to be translated
is high and the frequency of the good translation in
the target language is low, this means that the nearest
lexical units of the candidate word and its translation
are unbalanced. This leads us to face a lot of noise
because of the high frequency of the source word
that is over-represented by its nlu?s comparing to the
target word which is under-represented. Secondly,
we consider the inverse situation, which is: low fre-
quency of the source word and high frequency of
the target translation, here as well, we have both the
source and the target words that are unbalanced re-
garding to the selected nearest lexical units. The
third case, represents more or less the same distri-
bution of the frequencies of source candidate and
target good translation. This can be considered as
the most appropriate case to find the good transla-
tion by applying the approaches based on the nlu?s
(EA or MS). Our experiments show that our method
works well in all the cases by using the parameter ?
which regulate the similarity score by taken into ac-
count the distribution of the candidate according to
both : selected nlu?s and all its neighbours. In re-
sume, words to be translated as represented in case
one and two give more difficulties to be translated
because of their unbalanced distribution which leads
to an unbalanced nlu?s. Future works should con-
firm the possibility to adapt an optimal value of k
to each candidate to be translated, according to its
distribution with respect to its neighbours.
5 Conclusion
We have presented a novel way of looking at the
problem of bilingual lexical extraction from compa-
rable corpora based on the idea of metasearch en-
gines. We believe that our model is simple and
sound. Regarding the empirical results of our propo-
sition, performances of the multiple source model
on our dataset was better than the baseline proposed
by De?jean and Gaussier (2002), and also outper-
forms the standard approach for a certain range of
k. We believe that the most significant result is that
a new approach to finding single word translations
has been shown to be competitive. We hope that
this new paradigm can lead to insights that would
be unclear in other models. Preliminary tests in this
perspective show that using an appropriate value of
k for each word can improve the performance of the
lexical extraction process. Dealing with this prob-
41
lem is an interesting line for future research.
6 Acknowledgments
The research leading to these results has received
funding from the French National Research Agency
under grant ANR-08-CORD-013.
References
Javed A. Aslam and Mark Montague. 2001. Models for
Metasearch. In SIGIR ?01, proceedings of the 24th
Annual SIGIR Conference, pages 276?284.
Lynne Bowker and Jennifer Pearson. 2002. Working
with Specialized Language: A Practical Guide to Us-
ing Corpora. Routledge, London/New York.
Yunbo Cao and Hang Li. 2002. Base Noun Phrase Trans-
lation Using Web Data and the EM Algorithm. In
Proceedings of the 19th International Conference on
Computational Linguistics (COLING?02), pages 127?
133, Tapei, Taiwan.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002a.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING?02), pages 1208?1212, Tapei, Tai-
wan.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002b.
Looking for French-English Translations in Compara-
ble Medical Corpora. Journal of the American Society
for Information Science, 8:150?154.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The
Effect of a General Lexicon in Corpus-Based Identifi-
cation of French-English Medical Word Translations.
In Robert Baud, Marius Fieschi, Pierre Le Beux, and
Patrick Ruch, editors, The New Navigators: from Pro-
fessionals to Patients, Actes Medical Informatics Eu-
rope, volume 95 of Studies in Health Technology and
Informatics, pages 397?402, Amsterdam. IOS Press.
Be?atrice Daille and Emmanuel Morin. 2005. French-
English Terminology Extraction from Comparable
Corpora. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing (IJ-
CLNP?05), pages 707?718, Jeju Island, Korea.
Herve? De?jean and E?ric Gaussier. 2002. Une nouvelle ap-
proche a` l?extraction de lexiques bilingues a` partir de
corpus comparables. Lexicometrica, Alignement lexi-
cal dans les corpus multilingues, pages 1?22.
Herve? De?jean, Fatia Sadat, and E?ric Gaussier. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of the 19th International Conference on
Computational Linguistics (COLING?02), pages 218?
224, Tapei, Taiwan.
Ted Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguis-
tics, 19(1):61?74.
Robert M. Fano. 1961. Transmission of Information:
A Statistical Theory of Communications. MIT Press,
Cambridge, MA, USA.
Pascale Fung and Yuen Yee Lo. 1998. An ir approach for
translating new words from nonparallel, comparable
texts. In Proceedings of the 17th international con-
ference on Computational linguistics (COLING?98),
pages 414?420.
Pascale Fung and Kathleen McKeown. 1997. Find-
ing Terminology Translations from Non-parallel Cor-
pora. In Proceedings of the 5th Annual Workshop on
Very Large Corpora (VLC?97), pages 192?202, Hong
Kong.
Pascale Fung. 1998. A Statistical View on Bilingual Lex-
icon Extraction: From ParallelCorpora to Non-parallel
Corpora. In David Farwell, Laurie Gerber, and Eduard
Hovy, editors, Proceedings of the 3rd Conference of
the Association for Machine Translation in the Ameri-
cas (AMTA?98), pages 1?16, Langhorne, PA, USA.
E?ric Gaussier, Jean-Michel Renders, Irena Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A Geomet-
ric View on Bilingual Lexicon Extraction from Com-
parable Corpora. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?04), pages 526?533, Barcelona, Spain.
Gregory Grefenstette. 1994a. Corpus-Derived First, Sec-
ond and Third-Order Word Affinities. In Proceedings
of the 6th Congress of the European Association for
Lexicography (EURALEX?94), pages 279?290, Ams-
terdam, The Netherlands.
Gregory Grefenstette. 1994b. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publisher,
Boston, MA, USA.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora. In
Proceedings of the 23rd International Conference on
Computational Linguistics (COLING?10), pages 617?
625, Beijing, China.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual Terminology Mining ?
Using Brain, not brawn comparable corpora. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL?07), pages 664?
671, Prague, Czech Republic.
Carol Peters and Eugenio Picchi. 1998. Cross-language
information retrieval: A system for comparable cor-
pus querying. In Gregory Grefenstette, editor, Cross-
language information retrieval, chapter 7, pages 81?
90. Kluwer Academic Publishers.
42
Reinhard Rapp. 1995. Identify Word Translations in
Non-Parallel Texts. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?95), pages 320?322, Boston, MA, USA.
Reinhard Rapp. 1999. Automatic Identification of Word
Translations from Unrelated English and German Cor-
pora. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics (ACL?99),
pages 519?526, College Park, MD, USA.
Gerard Salton and Michael E. Lesk. 1968. Computer
evaluation of indexing and text processing. Jour-
nal of the Association for Computational Machinery,
15(1):8?36.
43
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 24?33,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
A Comparison of Smoothing Techniques for Bilingual Lexicon Extraction
from Comparable Corpora
Amir Hazem and Emmanuel Morin
Laboratore d?Informatique de Nantes-Atlantique (LINA)
Universite? de Nantes, 44322 Nantes Cedex 3, France
{Amir.Hazem, Emmanuel.Morin}@univ-nantes.fr
Abstract
Smoothing is a central issue in lan-
guage modeling and a prior step in dif-
ferent natural language processing (NLP)
tasks. However, less attention has been
given to it for bilingual lexicon extrac-
tion from comparable corpora. If a first
work to improve the extraction of low
frequency words showed significant im-
provement while using distance-based av-
eraging (Pekar et al, 2006), no investi-
gation of the many smoothing techniques
has been carried out so far. In this pa-
per, we present a study of some widely-
used smoothing algorithms for language
n-gram modeling (Laplace, Good-Turing,
Kneser-Ney...). Our main contribution is
to investigate how the different smoothing
techniques affect the performance of the
standard approach (Fung, 1998) tradition-
ally used for bilingual lexicon extraction.
We show that using smoothing as a pre-
processing step of the standard approach
increases its performance significantly.
1 Introduction
Cooccurrences play an important role in many
corpus based approaches in the field of natural-
language processing (Dagan et al, 1993). They
represent the observable evidence that can be
distilled from a corpus and are employed for a
variety of applications such as machine transla-
tion (Brown et al, 1992), information retrieval
(Maarek and Smadja, 1989), word sense disam-
biguation (Brown et al, 1991), etc. In bilingual
lexicon extraction from comparable corpora,
frequency counts for word pairs often serve as
a basis for distributional methods, such as the
standard approach (Fung, 1998) which compares
the cooccurrence profile of a given source word, a
vector of association scores for its translated cooc-
currences (Fano, 1961; Dunning, 1993), with the
profiles of all words of the target language. The
distance between two such vectors is interpreted
as an indicator of their semantic similarity and
their translational relation. If using association
measures to extract word translation equivalents
has shown a better performance than using a
raw cooccurrence model, the latter remains the
core of any statistical generalisation (Evert, 2005).
As has been known, words and other type-rich
linguistic populations do not contain instances
of all types in the population, even the largest
samples (Zipf, 1949; Evert and Baroni, 2007).
Therefore, the number and distribution of types
in the available sample are not reliable estimators
(Evert and Baroni, 2007), especially for small
comparable corpora. The literature suggests two
major approaches for solving the data sparseness
problem: smoothing and class-based methods.
Smoothing techniques (Good, 1953) are often
used to better estimate probabilities when there
is insufficient data to estimate probabilities ac-
curately. They tend to make distributions more
uniform, by adjusting low probabilities such as
zero probabilities upward, and high probabilities
downward. Generally, smoothing methods not
only prevent zero probabilities, but they also
attempt to improve the accuracy of the model as a
whole (Chen and Goodman, 1999). Class-based
models (Pereira et al, 1993) use classes of similar
words to distinguish between unseen cooccur-
rences. The relationship between given words is
modeled by analogy with other words that are
in some sense similar to the given ones. Hence,
class-based models provide an alternative to the
independence assumption on the cooccurrence
of given words w1 and w2: the more frequent
w2 is, the higher estimate of P (w2|w1) will be,
regardless of w1.
24
Starting from the observation that smoothing es-
timates ignore the expected degree of association
between words (assign the same estimate for all
unseen cooccurrences) and that class-based mod-
els may not structure and generalize word cooc-
currence to class cooccurrence patterns without
losing too much information, (Dagan et al, 1993)
proposed an alternative to these latter approaches
to estimate the probabilities of unseen cooccur-
rences. They presented a method that makes
analogies between each specific unseen cooccur-
rence and other cooccurrences that contain similar
words. The analogies are based on the assump-
tion that similar word cooccurrences have simi-
lar values of mutual information. Their method
has shown significant improvement for both: word
sense disambiguation in machine translation and
data recovery tasks. (Pekar et al, 2006) em-
ployed the nearest neighbor variety of the previ-
ous approach to extract translation equivalents for
low frequency words from comparable corpora.
They used a distance-based averaging technique
for smoothing (Dagan et al, 1999). Their method
yielded a significant improvement in relation to
low frequency words.
Starting from the assumption that smoothing
improves the accuracy of the model as a whole
(Chen and Goodman, 1999), we believe that
smoothed context vectors should lead to bet-
ter performance for bilingual terminology extrac-
tion from comparable corpora. In this work we
carry out an empirical comparison of the most
widely-used smoothing techniques, including ad-
ditive smoothing (Lidstone, 1920), Good-Turing
estimate (Good, 1953), Jelinek-Mercer (Mercer,
1980), Katz (Katz, 1987) and kneser-Ney smooth-
ing (Kneser and Ney, 1995). Unlike (Pekar et al,
2006), the present work does not investigate un-
seen words. We only concentrate on observed
cooccurrences. We believe it constitutes the most
systematic comparison made so far with differ-
ent smoothing techniques for aligning translation
equivalents from comparable corpora. We show
that using smoothing as a pre-processing step of
the standard approach, leads to significant im-
provement even without considering unseen cooc-
currences.
In the remainder of this paper, we present in
Section 2, the different smoothing techniques. The
steps of the standard approach and our extended
method are then described in Section 3. Section
4 describes the experimental setup and our re-
sources. Section 5 presents the experiments and
comments on several results. We finally discuss
the results in Section 6 and conclude in Section 7.
2 Smoothing Techniques
Smoothing describes techniques for adjusting the
maximum likelihood estimate of probabilities to
reduce more accurate probabilities. The smooth-
ing techniques tend to make distributions more
uniform. In this section we present the most
widely used techniques.
2.1 Additive Smoothing
The Laplace estimator or the additive smoothing
(Lidstone, 1920; Johnson, 1932; Jeffreys, 1948)
is one of the simplest types of smoothing. Its
principle is to estimate probabilities P assuming
that each unseen word type actually occurred once.
Then, if we have N events and V possible words
instead of :
P(w) = occ(w)N (1)
We estimate:
Paddone(w) =
occ(w) + 1
N + V (2)
Applying Laplace estimation to word?s cooc-
currence suppose that : if two words cooccur to-
gether n times in a corpus, they can cooccur to-
gether (n + 1) times. According to the maximum
likelihood estimation (MLE):
P(wi+1|wi) =
C(wi,wi+1)
C(wi)
(3)
Laplace smoothing:
P?(wi+1|wi) =
C(wi,wi+1) + 1
C(wi) + V
(4)
Several disadvantages emanate from this
method:
1. The probability of frequent n-grams is under-
estimated.
2. The probability of rare or unseen n-grams is
overestimated.
25
3. All the unseen n-grams are smoothed in the
same way.
4. Too much probability mass is shifted towards
unseen n-grams.
One improvement is to use smaller added count
following the equation below:
P?(wi+1|wi) =
? + C(wi,wi+1)
?|V|+ C(wi)
(5)
with ? ?]0, 1].
2.2 Good-Turing Estimator
The Good-Turing estimator (Good, 1953) pro-
vides another way to smooth probabilities. It states
that for any n-gram that occurs r times, we should
pretend that it occurs r? times. The Good-Turing
estimators use the count of things you have seen
once to help estimate the count of things you have
never seen. In order to compute the frequency of
words, we need to compute Nc, the number of
events that occur c times (assuming that all items
are binomially distributed). Let Nr be the num-
ber of items that occur r times. Nr can be used to
provide a better estimate of r, given the binomial
distribution. The adjusted frequency r? is then:
r? = (r + 1)Nr+1Nr
(6)
2.3 Jelinek-Mercer Smoothing
As one alternative to missing n-grams, useful in-
formation can be provided by the corresponding
(n-1)-gram probability estimate. A simple method
for combining the information from lower-order
n-gram in estimating higher-order probabilities is
linear interpolation (Mercer, 1980). The equation
of linear interpolation is given below:
Pint(wi+1|wi) = ?P(wi+1|wi) + (1? ?)P(wi) (7)
? is the confidence weight for the longer n-
gram. In general, ? is learned from a held-out
corpus. It is useful to interpolate higher-order n-
gram models with lower-order n-gram models, be-
cause when there is insufficient data to estimate a
probability in the higher order model, the lower-
order model can often provide useful information.
Instead of the cooccurrence counts, we used the
Good-Turing estimator in the linear interpolation
as follows:
c?int(wi+1|wi) = ?c
?(wi+1|wi) + (1? ?)P(wi) (8)
2.4 Katz Smoothing
(Katz, 1987) extends the intuitions of Good-
Turing estimate by adding the combination of
higher-order models with lower-order models. For
a bigram wii?1 with count r = c(wii?1), its cor-
rected count is given by the equation:
ckatz(w
i
i?1) =
{
r? if r > 0
?(wi?1)PML(wi) if r = 0 (9)
and:
?(wi?1) =
1?
?
wi:c(w
i
i?1)>0
Pkatz(wii?1)
1?
?
wi:c(w
i
i?1)>0
PML(wi?1)
(10)
According to (Katz, 1987), the general dis-
counted estimate c? of Good-Turing is not used forall counts c. Large counts where c > k for somethreshold k are assumed to be reliable. (Katz,1987) suggests k = 5. Thus, we define c? = cfor c > k, and:
c? =
(c + 1)Nc+1Nc ? c
(k+1)Nk+1
N1
1?
(k+1)Nk+1
N1
(11)
2.5 Kneser-Ney Smoothing
Kneser-Ney have introduced an extension of ab-
solute discounting (Kneser and Ney, 1995). The
estimate of the higher-order distribution is created
by subtracting a fixed discount D from each non-
zero count. The difference with the absolute dis-
counting smoothing resides in the estimate of the
lower-order distribution as shown in the following
equation:
r =
?
?
?
?
?
Max(c(wii?n+1)?D,0)
?
wi
c(wii?n+1)
if c(wii?n+1) > 0
?(wi?1i?n+1)Pkn(wi|w
i?1
i?n+2) if c(wii?n+1) = 0
(12)
where r = Pkn(wi|wi?1i?n+1) and ?(wi?1i?n+1) is
chosen to make the distribution sum to 1 (Chen
and Goodman, 1999).
3 Methods
In this section we first introduce the different steps
of the standard approach, then we present our ex-
tended approach that makes use of smoothing as a
new step in the process of the standard approach.
26
3.1 Standard Approach
The main idea for identifying translations of terms
in comparable corpora relies on the distributional
hypothesis 1 that has been extended to the bilin-
gual scenario (Fung, 1998; Rapp, 1999). If many
variants of the standard approach have been pro-
posed (Chiao and Zweigenbaum, 2002; Herve?
De?jean and Gaussier, 2002; Morin et al, 2007;
Gamallo, 2008)[among others], they mainly differ
in the way they implement each step and define its
parameters. The standard approach can be carried
out as follows:
Step 1 For a source word to translate wsi , we first
build its context vector vwsi . The vector vwsicontains all the words that cooccur with wsi
within windows of n words. Lets denote by
cooc(wsi , wsj ) the cooccurrence value of wsi
and a given word of its context wsj . The pro-
cess of building context vectors is repeated
for all the words of the target language.
Step 2 An association measure such as the point-
wise mutual information (Fano, 1961), the
log-likelihood (Dunning, 1993) or the dis-
counted odds-ratio (Laroche and Langlais,
2010) is used to score the strength of corre-
lation between a word and all the words of its
context vector.
Step 3 The context vector vwsi is projected intothe target language vtwsi . Each wordwsj of vwsiis translated with the help of a bilingual dic-
tionary D. If wsj is not present in D, wsj is
discarded. Whenever the bilingual dictionary
provides several translations for a word, all
the entries are considered but weighted ac-
cording to their frequency in the target lan-
guage (Morin et al, 2007).
Step 4 A similarity measure is used to score each
target word wti , in the target language with
respect to the translated context vector, vtwsi .Usual measures of vector similarity include
the cosine similarity (Salton and Lesk, 1968)
or the weighted Jaccard index (WJ) (Grefen-
stette, 1994) for instance. The candidate
translations of the word wsi are the target
words ranked following the similarity score.
1words with similar meaning tend to occur in similar con-
texts
3.2 Extended Approach
We aim at investigating the impact of differ-
ent smoothing techniques for the task of bilin-
gual terminology extraction from comparable cor-
pora. Starting from the assumption that word
cooccurrences are not reliable especially for small
corpora (Zipf, 1949; Evert and Baroni, 2007)
and that smoothing is usually used to counter-
act this problem, we apply smoothing as a pre-
processing step of the standard approach. Each
cooc(wsi , wsj ) is smoothed according to the tech-
niques described in Section 2. The smoothed
cooccurrence cooc?(wsi , wsj ) is then used for cal-
culating the association measure between wsi and
wsj and so on (steps 2, 3 and 4 of the standard ap-
proach are unchanged). We chose not to study
the prediction of unseen cooccurrences. The lat-
ter has been carried out successfully by (Pekar
et al, 2006). We concentrate on the evaluation
of smoothing techniques of known cooccurrences
and their effect according to different association
and similarity measures.
4 Experimental Setup
In order to evaluate the smoothing techniques, sev-
eral resources and parameters are needed. We
present hereafter the experiment data and the pa-
rameters of the standard approach.
4.1 Corpus Data
The experiments have been carried out on two
English-French comparable corpora. A special-
ized corpus of 530,000 words from the medical
domain within the sub-domain of ?breast cancer?
and a specialize corpus from the domain of ?wind-
energy? of 300,000 words. The two bilingual cor-
pora have been normalized through the follow-
ing linguistic pre-processing steps: tokenization,
part-of-speech tagging, and lemmatization. The
function words have been removed and the words
occurring once (i.e. hapax) in the French and
the English parts have been discarded. For the
breast cancer corpus, we have selected the doc-
uments from the Elsevier website2 in order to
obtain an English-French specialized comparable
corpora. We have automatically selected the doc-
uments published between 2001 and 2008 where
the title or the keywords contain the term ?cancer
du sein? in French and ?breast cancer? in English.
We collected 130 documents in French and 118 in
2www.elsevier.com
27
English. As summarised in Table 1, The compara-
ble corpora comprised about 6631 distinct words
in French and 8221 in English. For the wind en-
ergy corpus, we used the Babook crawler (Groc,
2011) to collect documents in French and English
from the web. We could only obtain 50 documents
in French and 65 in English. As the documents
were collected from different websites according
to some keywords of the domain, this corpus is
more noisy and not well structured compared to
the breast cancer corpus. The wind-energy corpus
comprised about 5606 distinct words in French
and 6081 in English.
Breast cancer Wind energy
TokensS 527,705 307,996
TokensT 531,626 314,551
|S| 8,221 6,081
|T | 6,631 5,606
Table 1: Corpus size
4.2 Dictionary
In our experiments we used the French-English
bilingual dictionary ELRA-M0033 of about
200,000 entries3. It contains, after linguistic pre-
processing steps and projection on both corpora
fewer than 4000 single words. The details are
given in Table 2.
Breast cancer Wind energy
|ELRAS | 3,573 3,459
|ELRAT | 3,670 3,326
Table 2: Dictionary coverage
4.3 Reference Lists
In bilingual terminology extraction from special-
ized comparable corpora, the terminology refer-
ence list required to evaluate the performance
of the alignment programs is often composed of
100 single-word terms (SWTs) (180 SWTs in
(Herve? De?jean and Gaussier, 2002), 95 SWTs in
(Chiao and Zweigenbaum, 2002), and 100 SWTs
in (Daille and Morin, 2005)). To build our ref-
erence lists, we selected only the French/English
pair of SWTs which occur more than five times in
each part of the comparable corpus. As a result
3ELRA dictionary has been created by Sciper in the Tech-
nolangue/Euradic project
of filtering, 321 French/English SWTs were ex-
tracted (from the UMLS4 meta-thesaurus.) for the
breast cancer corpus, and 100 pairs for the wind-
energy corpus.
4.4 Evaluation Measure
Three major parameters need to be set to the
standard approach, namely the similarity measure,
the association measure defining the entry vec-
tors and the size of the window used to build the
context vectors. (Laroche and Langlais, 2010)
carried out a complete study of the influence of
these parameters on the quality of bilingual align-
ment. As a similarity measure, we chose to use
Weighted Jaccard Index (Grefenstette, 1994) and
Cosine similarity (Salton and Lesk, 1968). The en-
tries of the context vectors were determined by the
log-likelihood (Dunning, 1993), mutual informa-
tion (Fano, 1961) and the discounted Odds-ratio
(Laroche and Langlais, 2010). We also chose a 7-
window size. Other combinations of parameters
were assessed but the previous parameters turned
out to give the best performance. We note that
?Top k? means that the correct translation of a
given word is present in the k first candidates of
the list returned by the standard approach. We use
also the mean average precision MAP (Manning
et al, 2008) which represents the quality of the
system.
MAP (Q) = 1
|Q|
|Q|?
i=1
1
mi
k?
mi=1
P (Rik) (13)
where |Q| is the number of terms to be trans-
lated, mi is the number of reference translations
for the ith term (always 1 in our case), and P (Rik)
is 0 if the reference translation is not found for the
ith term or 1/r if it is (r is the rank of the reference
translation in the translation candidates).
4.5 Baseline
The baseline in our experiments is the standard
approach (Fung, 1998) without any smoothing of
the data. The standard approach is often used for
comparison (Pekar et al, 2006; Gamallo, 2008;
Prochasson and Morin, 2009), etc.
4.6 Training Data Set
Some smoothing techniques such as the Good-
Turing estimators need a large training corpus to
4http://www.nlm.nih.gov/research/umls
28
estimate the adjusted cooccurrences. For that pur-
pose, we chose a training general corpus of 10 mil-
lion words. We selected the documents published
in 1994 from the ?Los Angeles Times/Le Monde?
newspapers.
5 Experiments and Results
We conducted a set of three experiments on two
specialized comparable corpora. We carried out a
comparison between the standard approach (SA)
and the smoothing techniques presented in Sec-
tion 2 namely : additive smoothing (Add1), Good-
Turing smoothing (GT), the Jelinek-Mercer tech-
nique (JM), the Katz-Backoff (Katz) and kneser-
Ney smoothing (Kney). Experiment 1 shows the
results for the breast cancer corpus. Experiment 2
shows the results for the wind energy corpus and
finally experiment 3 presents a comparison of the
best configurations on both corpora.
5.1 Experiment 1
Table 3 shows the results of the experiments on
the breast cancer corpus. The first observation
concerns the standard approach (SA). The best
results are obtained using the Log-Jac parame-
ters with a MAP = 27.9%. We can also no-
tice that for this configuration, only the Addi-
tive smoothing significantly improves the perfor-
mance of the standard approach with a MAP =
30.6%. The other smoothing techniques even de-
grade the results. The second observation con-
cerns the Odds-Cos parameters where none of
the smoothing techniques significantly improved
the performance of the baseline (SA). Although
Good-Turing and Katz-Backoff smoothing give
slightly better results with respectively a MAP =
25.2 % and MAP = 25.3 %, these results are not
significant. The most notable result concerns the
PMI-COS parameters. We can notice that four of
the five smoothing techniques improve the perfor-
mance of the baseline. The best smoothing is the
Jelinek-Mercer technique which reaches a MAP =
29.5% and improves the Top1 precision of 6% and
the Top10 precision of 10.3%.
5.2 Experiment 2
Table 4 shows the results of the experiments on
the wind energy corpus. Generally the results
exhibit the same behaviour as the previous ex-
periment. The best results of the standard ap-
proach are obtained using the Log-Jac parameters
SA Add1 GT JM Katz Kney
P1 15.5 17.1 18.7 21.5 18.7 05.3
PM
I-C
osP5 31.1 32.7 32.0 38.3 33.9 13.4
P10 34.5 37.0 37.0 44.8 38.0 15.2
MAP 22.6 24.8 25.6 29.5 25.9 09.1
P1 15.8 16.1 16.8 14.6 17.1 09.0
Od
ds-
Co
s
P5 34.8 33.6 34.2 33.0 33.9 19.6
P10 40.4 41.7 39.8 38.3 40.1 25.2
MAP 24.8 24.4 25.2 23.3 25.3 14.1
P1 20.2 22.4 14.6 14.6 14.6 16.2
Lo
g-J
acP5 35.8 40.5 27.7 26.7 26.7 29.9
P10 42.6 44.2 34.2 33.3 33.0 33.9
MAP 27.9 30.6 21.4 21.2 21.2 22.9
Table 3: Results of the experiments on the ?Breast
cancer? corpus (except the Odds-Cos configura-
tion, the improvements indicate a significance at
the 0.05 level using the Student t-test).
SA Add1 GT JM Katz Kney
P1 07.0 14.0 14.0 21.0 16.0 09.0
PM
I-C
osP5 27.0 32.0 31.0 37.0 30.0 17.0
P10 37.0 42.0 43.0 51.0 44.0 28.0
MAP 17.8 23.6 22.9 30.1 24.2 14.1
P1 12.0 17.0 12.0 12.0 12.0 06.0
Od
ds-
Co
s
P5 31.0 35.0 31.0 32.0 28.0 16.0
P10 38.0 44.0 36.0 39.0 35.0 21.0
MAP 21.8 26.5 19.8 20.8 19.7 11.1
P1 17.0 22.0 13.0 13.0 13.0 14.0
Lo
g-J
acP5 36.0 38.0 27.0 27.0 27.0 29.0
P10 42.0 50.0 37.0 38.0 38.0 39.0
MAP 25.7 29.7 20.5 21.3 21.3 22.9
Table 4: Results of the experiments on the ?Wind
Energy? corpus (except the Odds-Cos configura-
tion, the improvements indicate a significance at
the 0.05 level using the Student t-test).
with a MAP = 25.7%. Here also, only the Ad-
ditive smoothing significantly improves the per-
formance of the standard approach with a MAP
= 39.7%. The other smoothing techniques also
degrade the results. About the Odds-Cos param-
eters, except the additive smoothing, here again
none of the smoothing techniques significantly im-
proved the performance of the baseline. Finally
the most remarkable result still concerns the PMI-
COS parameters where the same four of the five
smoothing techniques improve the performance of
the baseline. The best smoothing is the Jelinek-
Mercer technique which reaches a MAP = 30.1%
and improves the Top1 and and the Top10 preci-
sions by 14.0%.
29
5.3 Experiment 3
In this experiment, we would like to investigate
whether the smoothing techniques are more effi-
cient for frequent translation equivalents or less
frequent ones. For that purpose, we split the breast
cancer reference list of 321 entries into two sets
of translation pairs. A set of 133 frequent pairs
named : High-test set and a set of 188 less fre-
quent pairs called Low-test set. The initial refer-
ence list of 321 pairs is the Full-test set. We con-
sider frequent pairs those of a frequency higher
than 100. We chose to analyse the two configu-
rations that provided the best performance that is :
Log-Jac and Pmi-Cos parameters according to the
Full-test, High-test and Low-test sets.
Figure 1 shows the results using the Log-
Jac configuration. We can see that the additive
smoothing always outperforms the standard ap-
proach for all the test sets. The other smoothing
techniques are always under the baseline and be-
have approximately the same way. Figure 2 shows
the results using the PMI-COS configuration. We
can see that except the Kneser-Ney smoothing, all
the smoothing techniques outperform the standard
approach for all the test sets. We can also notice
that the Jelinek-Mercer smoothing improves more
notably the High-test set.
6 Discussion
Smoothing techniques are often evaluated on their
ability to predict unseen n-grams. In our exper-
iments we only focused on smoothing observed
cooccurrences of context vectors. Hence, the pre-
vious evaluations of smoothing techniques may
not always be consistent with our findings. This
is for example the case for the additive smooth-
ing technique. The latter which is described as
a poor estimator in statistical NLP, turns out to
perform well when associated with the Log-Jac
parameters. This is because we did not consider
unseen cooccurences which are over estimated by
the Add-one smoothing. Obviously, we can imag-
ine that adding one to all unobserved cooccur-
rences would not make sense and would certainly
degrade the results. Except the add-one smooth-
ing, none of the other algorithms reached good
results when associated to the Log-Jac configu-
ration. This is certainly related to the properties
of the log-likelihood association measure. Addi-
tive smoothing has been used to address the prob-
lem of rare words aligning to too many words
(Moore, 2004). At each iteration of the standard
Expectation-Maximization (EM) procedure all the
translation probability estimates are smoothed by
adding virtual counts to uniform probability dis-
tribution over all target words. Here also additive
smoothing has shown interesting results. Accord-
ing to these findings, we can consider the addi-
tive smoothing as an appropriate technique for our
task.
Concerning the Odds-Cos parameters, although
there have been slight improvements in the add-
one algorithm, smoothing techniques have shown
disappointing results. Here again the Odds-ratio
association measure seems to be incompatible
with re-estimating small cooccurrences. More in-
vestigations are certainly needed to highlight the
reasons for this poor performance. It seems that
smoothing techniques based on discounting does
not fit well with association measures based on
contingency table. The most noticeable improve-
ment concerns the PMI-Cos configurations. Ex-
cept Kneser-Ney smoothing, all the other tech-
niques showed better performance than the stan-
dard approach. According to the results, point-
wise mutual information performs better with
smoothing techniques especially with the linear
interpolation of Jelinek-Mercer method that com-
bines high-order (cooccurrences) and low-order
(unigrams) counts of the Good-Turing estima-
tions. Jelinek-Mercer smoothing counteracts the
disadvantage of the point-wise mutual information
which consists of over estimating less frequent
words. This latter weakness is corrected first by
the Good-Turing estimators and then by consider-
ing the low order counts. The best performance
was obtained with ? = 0.5.
Smoothing techniques attempt to improve the
accuracy of the model as a whole. This particu-
larity has been confirmed by the third experiment
where we noticed the smoothing improvements for
both reference lists, that is the High-test and Low-
test sets. This latter experiment has shown that
smoothing observed cooccurrences is useful for all
frequency ranges. The difference of precision be-
tween the two test lists can be explained by the fact
that less frequent words are harder to translate.
In statistical NLP, smoothing techniques for n-
gram models have been addressed in a number
of studies (Chen and Goodman, 1999). The ad-
30
1 5 10 15 20 25 30 35 40 45 5010
20
30
40
50
60
70
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(a)
1 5 10 15 20 25 30 35 40 45 5020
30
40
50
60
70
80
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(b)
1 5 10 15 20 25 30 35 40 45 500
10
20
30
40
50
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(c)
Figure 1: A set of three figures on the breast cancer corpus for the Log-Jac configuration : (a) Full-test
set ; (b) High-test set; and (c) Low-test set.
1 5 10 15 20 25 30 35 40 45 500
10
20
30
40
50
60
70
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(a)
1 5 10 15 20 25 30 35 40 45 500
10
20
30
40
50
60
70
80
90
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(b)
1 5 10 15 20 25 30 35 40 45 500
10
20
30
40
50
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(c)
Figure 2: A set of three figures on the breast cancer corpus for the PMI-COS configuration : (a) Full-test
set ; (b) High-test set; and (c) Low-test set.
ditive smoothing that performs rather poorly has
shown good results in our evaluation. The Good-
Turing estimate which is not used in isolation
forms the basis of later techniques such as Back-
off or Jelinek-Mercer smoothing, two techniques
that generally work well. The good performance
of Katz and JM on the PMI-Cos configura-
tion was expected. The reason is that these two
methods have used the Good-Turing estimators
which also achieved good performances in our
experiments. Concerning the Kneser-Ney algo-
rithm, surprisingly this performed poorly in our
experiments while it is known to be one of the
best smoothing techniques. Discounting a fixed
amount in all counts of observed cooccurrences
degrades the results in our data set. We also im-
plemented the modified Knener-ney method (not
presented in this paper) but this also performed
poorly. We conclude that discounting is not an
appropriate method for observed cooccurrences.
Especially for point-wise mutual information that
over-estimates low frequencies, hense, discount-
ing low cooccurrences will increase this over-
estimation.
7 Conclusion
In this paper, we have described and compared
the most widely-used smoothing techniques for
the task of bilingual lexicon extraction from com-
parable corpora. Regarding the empirical results
of our proposition, performance of smoothing on
our dataset was better than the baseline for the
Add-One smoothing combined with the Log-Jac
parameters and all smoothing techniques except
the Kneser-ney for the Pmi-Cos parameters. Our
findings thus lend support to the hypothesis that
a re-estimation process of word cooccurrence in a
small specialized comparable corpora is an appro-
priate way to improve the accuracy of the standard
approach.
31
Acknowledgments
The research leading to these results has re-
ceived funding from the French National Research
Agency under grant ANR-12-CORD-0020.
References
Brown, P. F., Pietra, S. D., Pietra, V. J. D., and Mercer,
R. L. (1991). Word-sense disambiguation using sta-
tistical methods. In Proceedings of the 29th Annual
Meeting of the Association for Computational Lin-
guistics (ACL?91), pages 264?270, California, USA.
Brown, P. F., Pietra, V. J. D., de Souza, P. V., Lai, J. C.,
and Mercer, R. L. (1992). Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
Chen, S. F. and Goodman, J. (1999). An empirical
study of smoothing techniques for language model-
ing. Computer Speech & Language, 13(4):359?393.
Chiao, Y.-C. and Zweigenbaum, P. (2002). Look-
ing for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING?02), pages 1208?1212, Tapei,
Taiwan.
Dagan, I., Lee, L., and Pereira, F. C. N. (1999).
Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Dagan, I., Marcus, S., and Markovitch, S. (1993). Con-
textual word similarity and estimation from sparse
data. In Proceedings of the 31ST Annual Meet-
ing of the Association for Computational Linguistics
(ACL?93), pages 164?171, Ohio, USA.
Daille, B. and Morin, E. (2005). French-English Ter-
minology Extraction from Comparable Corpora. In
Proceedings of the 2nd International Joint Confer-
ence on Natural Language Processing (IJCLNP?05),
pages 707?718, Jeju Island, Korea.
Dunning, T. (1993). Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational
Linguistics, 19(1):61?74.
Evert, S. (2005). The statistics of word cooccurrences :
word pairs and collocations. PhD thesis, University
of Stuttgart, Holzgartenstr. 16, 70174 Stuttgart.
Evert, S. and Baroni, M. (2007). zipfr: Word frequency
modeling in r. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics (ACL?07), Prague, Czech Republic.
Fano, R. M. (1961). Transmission of Information: A
Statistical Theory of Communications. MIT Press,
Cambridge, MA, USA.
Fung, P. (1998). A Statistical View on Bilingual Lex-
icon Extraction: From Parallel Corpora to Non-
parallel Corpora. In Proceedings of the 3rd Confer-
ence of the Association for Machine Translation in
the Americas (AMTA?98), pages 1?16, Langhorne,
PA, USA.
Gamallo, O. (2008). Evaluating two different meth-
ods for the task of extracting bilingual lexicons from
comparable corpora. In Proceedings of LREC 2008
Workshop on Comparable Corpora (LREC?08),
pages 19?26, Marrakech, Marroco.
Good, I. J. (1953). The population frequencies of
species and the estimation of population parameters.
Biometrika, 40:16?264.
Grefenstette, G. (1994). Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publisher,
Boston, MA, USA.
Groc, C. D. (2011). Babouk : Focused Web Crawl-
ing for Corpus Compilation and Automatic Termi-
nology Extraction. In Proceedings of The IEEE-
WICACM International Conferences on Web Intel-
ligence, pages 497?498, Lyon, France.
Herve? De?jean and Gaussier, E?. (2002). Une nouvelle
approche a` l?extraction de lexiques bilingues a` partir
de corpus comparables. Lexicometrica, Alignement
lexical dans les corpus multilingues, pages 1?22.
Jeffreys, H. (1948). Theory of Probability. Clarendon
Press, Oxford. 2nd edn Section 3.23.
Johnson, W. (1932). Probability: the deductive and in-
ductive problems. Mind, 41(164):409?423.
Katz, S. M. (1987). Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech and Signal Processing, 35(3):400?401.
Kneser, R. and Ney, H. (1995). Improved backing-
off for M-gram language modeling. In Proceedings
of the 20th International Conference on Acoustics,
Speech, and Signal Processing (ICASSP?95), pages
181?184, Michigan, USA.
Laroche, A. and Langlais, P. (2010). Revisit-
ing Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora. In
Proceedings of the 23rd International Conference
on Computational Linguistics (COLING?10), pages
617?625, Beijing, China.
Lidstone, G. J. (1920). Note on the general case of the
bayes-laplace formula for inductive or a posteriori
probabilities. Transactions of the Faculty of Actuar-
ies, 8:182?192.
Maarek, Y. S. and Smadja, F. A. (1989). Full text
indexing based on lexical relations an application:
Software libraries. In SIGIR, pages 198?206, Mas-
sachusetts, USA.
32
Manning, D. C., Raghavan, P., and Schu?tze, H. (2008).
Introduction to information retrieval. Cambridge
University Press.
Mercer, L. ; Jelinek, F. (1980). Interpolated estimation
of markov source parameters from sparse data. In
Workshop on pattern recognition in Practice, Ams-
terdam, The Netherlands.
Moore, R. C. (2004). Improving ibm word alignment
model 1. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?04), pages 518?525, Barcelona, Spain.
Morin, E., Daille, B., Takeuchi, K., and Kageura, K.
(2007). Bilingual Terminology Mining ? Using
Brain, not brawn comparable corpora. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL?07), pages 664?
671, Prague, Czech Republic.
Pekar, V., Mitkov, R., Blagoev, D., and Mulloni,
A. (2006). Finding translations for low-frequency
words in comparable corpora. Machine Translation,
20(4):247?266.
Pereira, F. C. N., Tishby, N., and Lee, L. (1993). Dis-
tributional clustering of english words. In Proceed-
ings of the 31ST Annual Meeting of the Association
for Computational Linguistics (ACL?93), pages 183?
190, Ohio, USA.
Prochasson, E. and Morin, E. (2009). Anchor points
for bilingual extraction from small specialized com-
parable corpora. TAL, 50(1):283?304.
Rapp, R. (1999). Automatic Identification of Word
Translations from Unrelated English and German
Corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?99), pages 519?526, College Park, MD, USA.
Salton, G. and Lesk, M. E. (1968). Computer evalua-
tion of indexing and text processing. Journal of the
Association for Computational Machinery, 15(1):8?
36.
Zipf, G. K. (1949). Human Behaviour and the Princi-
ple of Least Effort: an Introduction to Human Ecol-
ogy. Addison-Wesley.
33
