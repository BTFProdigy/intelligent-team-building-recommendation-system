LANGUAGE IDENTIFICATION 
IN 
UNKNOWN SIGNALS 
Contact author: John Elliott, jre@scs.leeds.ac.uk 
Co-authors: Eric Atwell, eric@scs.leeds.ac.uk 
Bill Whyte, billw@scs.leeds.ac.uk 
Organisation: Centre for Computer Analysis of Language and Speech, 
School of Computer Studies, University of Leeds, Leeds, Yorkshire, LS2 9JT England 
Abstract 
This paper describes algorithms and software developed to characterise and detect generic 
intelligent language-like features iu an input signal, using Natural Language Learning 
techniques: looking for characteristic statistical "language-signatures" in test corpora. As a 
first step towards such species-independent language-detection, we present a suite of 
programs to analyse digital representations of a range of data, and use the results to 
extrapolate whether or not there are language-like structures which distiuguish this data from 
other sources, such as nmsic, images, and white noise. We assume that generic species- 
independent commuuication can be detected by concentrating on localised patterns and 
rhythms, identifying segments at the level of characters, words and phrases, without 
necessarily having to "understand" the content. 
We assume that a language-like signal will be encoded symbolically, i.e. some kind of 
character-stream. Our language-detection algorithm for symbolic input uses a number of 
statistical clues: data compression ratio, "chunking" to find character bit-length and 
boundaries, and matching against a Zipfian type-token distribution for "letters" and "words". 
We do not claim extensive (let alne exhaustive) empirical evidence that our language- 
detection clues are "correct"; the only real test will come when the Search for Extra- 
Terrestrial Intelligence finds true alien signals. If and when true SETI signals are found, the 
first step to interpretation is to identify the language-like f atures, using techniques like the 
above. Our current research goal is to apply Natural Language Learning techniques to the 
identification of "higher-level" grammatical nd semantic structure in a linguistic signal. 
Introduction 
A useful thought experiment is to imagine 
eavesdropping on a signal from outer 
space. How can you decide that it is a 
message between intelligent life forms, 
without dialogue with the source? What is 
special about the language signal that 
separates it fiom non-language? What 
special 'zone' in the signal universe does 
language occupy? Is it, indeed, separable 
from other senti-structured sources, such 
as DNA and music (fig 1). 
Solving this problem might not only be 
useful in the event of detecting such 
signals fiom space, but also, by 
deliberately ignoring preconceptions based 
011 human texts, may provide us with some 
better understanding of what language 
really is. 
The Signal Universe 
ttowever, we ueed to start somewhere, and 
our initial investigations - which this paper 
summarises make some basic 
assumptions (which we would hope to 
relax in later research). Namely, that 
identifiable script will be a serial string, 
possessing a hierarchy of elements broadly 
equivalent to 'characters,' 'words', and 
1021 
'spaces', and possess something akin to  
human grammar .  
Identifying the 'Character Set' 
In 'real' decoding of unknown scripts it is 
accepted that identifying the correct set of 
discrete symbols is no mean feat 
(Chadwick 1967). To make life simple for 
ourselves we assume a digital signal with a 
fixed number of bits per character. Very 
different techniques are required to deal 
with audio or analogue equivalent 
waveforms (Elliott & Atweli 99, 00). We 
have reason to believe that the following 
method can be modified to relax this 
constraint, but this needs to be tested 
further. 
The task then reduces to trying to identify 
the number of bits per character. 
Suppose the probability of a bit is P~. Then 
the message ntropy of a string of length 
N will be given by: 
E = SUM \[PI In Pi\]; i =I,N 
If the signal contains merely a set of 
random digits, the expected wflue of this 
fnnctiou will rise monotonically as N 
increases. However, if the string contains a 
set of symbols of fixed length representing 
a character set used for communication, it 
is likely to show some decrease in entropy 
when analysed in blocks of this length, 
because the signal is 'less random' when 
thus blocked. Of course, we need to 
analyse blocks that begin and end at 
character boundaries. We simply carry out 
the measurements in sliding windows 
along the data. In figure 2 below, we see 
what happeus when we. apply this to 
samples of 8-bit ASCII text: 
i 
Enlropy Figure 2 
Lallgtlage 
{ I 
I 
4 5 6 7 8 9 
Bit Length 
Entropy profile as an indicalor of character bit-length 
We notice a clear drop, as predicted, for a 
bit length of 8.Modest progress though it 
may be, it is not unreasonable to assume 
that the first piece of evidence for the 
presence of language-like sO'ucture, 
would be the identification of a low- 
entropy, character set within the signal. 
Identifying 'Words' 
Again, work by crytopaleologists suggests 
that, once the character set has been found, 
the separation into word-like units, is not 
trivial and again we cheat, slightly: we 
assume that the language possesses 
something akin to a 'space' character. 
Taking our entropy measurement 
described above as a way of separating 
characters, we now try to identify the one, 
which represents 'space'. It is not 
unreasonable to believe that, in a word- 
based language, it is likely to be one of the 
most frequently used characters. 
Using a uumber of texts in a variety of 
languages, we first identified the top three 
most used characters. For each of these we 
hypothesised in turn that it represented 
'space'. This then allowed us to segment 
the signal into words-like units ('words' 
t'o1" simplicity). We coukl then compute the 
frequency distribution of words as a 
function of word length, for each of the 
three candidate 'space' characters (fig 3). 
Figure 3: Candidate word-lcnglh dishibulions 
using the 3 most frequent characters. 
400 
35O 
o ~ 3oo 
: ~ 250 
g 200 
i , ,  ~ 150 
i 1 O0 
l 50  
I 0 
I 
I 
It can be seen that  one 'separator' 
candidate (unsurprisingly, in fact, the most 
frequent character of all) results in a very 
varied distribution of word lengths. This is 
an interesting distribution, which, on the 
right hand side of the peak, approximately 
follows the well-known 'law' according to 
Zipf (Zipf, 1949), which predicts this 
1022 
behaviour o i l  the grounds of minimum 
efl'ort in a communication act. 
To ascertain whether the word-length 
frequency distribution holds for hmguage 
in general, nmltiple salnples from 20 
different hmguages fi'om Indo-European, 
Bantu, Semitic, Finno-Ugrian and Malayo- 
Polynesian groups were analysed (fig 4). 
Word lenglh dislribulions in mulliplc samples fl'om 
lndo-Eurol~ean, Semitic, l:inno-Ugrian, and Malayo- 
I'olynesian language groups 
20.00 :' 
15.oo 
10.00 
o 
5.00 
i:: ::y-: . . 
0 .00  :v  '1 ' ,~ , '  i - :  'i ' '~' -r ..... , 
? - "~  I'~ 0 03 r,D 03 C',l LO 
Od Cxl Figure 4 Word length 
Using statistical measures of signil'icance, 
it was found that most groups fell well 
within 5% limits - only two individual 
hmguages were near exceeding these 
limits -- of the proposed Human language 
word-length profile shown in fig 5. 
Figure 5: Itulnan language word-lengfl~ 
frequency distribution profile 
10.00  
t~ 5.00 f r l  
\[H . . . . . . . . . .  i 0.00  ~ , ~ ~ , 
Word l ength  l 
i 
Zipf's law is a strong indication of 
language-like behaviour. It can be used 
to segment the signal ptvvided a 'space' 
character exists. 
However, we shotdd not assume Zipf to be 
an infifllible language detector. Other 
natural phenomena such as molecular 
distribution in yeast DNA possess 
characteristics of power laws. Analyses of 
protein length distributions also display 
Poisson distributions where the number of 
proteins is plotted against the lengths of 
amino acids (Jenson 1998). 
Identifying 'Phrases' 
Although alien brains may be more oi less 
powerlhl than ours (Norris 1999), it is 
reasonable to assume that all intelligent 
Doblem solvers are subject to the same 
ultimate constraints of computatioual 
power and storage and their symbol 
systems will reflect his. 
Thus, language must use small sets of 
rules to generate a vast world of 
implications and consequences. Perhaps 
its most ilnportant single device is the use 
of embedded clauses and phrases (Minsky 
1984), with which to represent an 
expression or description, however 
complex, as a single component of another 
description. 
In serial languages, this appears to be 
achieved by clustering words into 'chunks' 
(phrases, sentences) of information, which 
are more-or-less consistent and self- 
contained elements of thought. 
lVurthermore, in human language at least, 
these 'chunks' tend to consist of contelzl 
terms, which describe what the chunk is 
'about' and .fimctional terms, which 
attribute references aud context by which 
tile content erms convey their information 
unambiguously. 'King' is usually a 
content erm; 'of' and 'the' are functional. 
We use 'term' rather than word, because 
many languages make far less use of full 
words for l'unctional operations than does 
English: in Latin the transformation 'rex' 
('king') to 'regis' (of the king) is one such 
example. 
Functional terms in a language tend to be 
short, probably attributable to the principle 
of least effort, as they are used frequently. 
A further distinguishing characteristic of 
functional and content terms is that 
different texts will often wtry in their 
content but tend to share a common 
linguistic structure and therefore make 
similar use of functional terms. That is, the 
probability distribution of content terms 
will vary from text to text, but the 
distribution of ftmction terms will not. 
Using English text, which had been 
enciphered using a simple substitution 
cipher (to avoid cheating), we identified 
1023 
across a variety of texts, the most common 
words, with least inter-text variation. 
These we call 'candidate function words'. 
Now, suppose these words occurred at 
random in the signal: we would expect o 
see the spacing between them to be merely 
a ftmction of their individual probabilities 
of occurrence. Analysing this statistically 
(as a Poisson distribution) or simply 
simulate it practically, we find that there 
are a non-insignificant number of cases 
wherein there are very large gaps (of the 
order of several tens of words) between 
successive occurrences. Compare this with 
the results from our analysis (fig 6). 
I\[ll,,. 
3 4 5 6 7 8 9 
Figure 6 
Function 
Word 
separation 
in 
English. 
Number of words between candidate flmclional words 
initial findings show that the frequency 
distribution of these lengths of text - our 
candidate phrases - follow a Zipfian 
distribution curve and rarely exceed 
lengths of more than eight. 
We might conclude from this, that our 
brains tend to "chunk' linguistic 
information into phrase-like structures of  
the order of seven or so word units long. 
Interestingly enough, this fits in well with 
human cognition theory (Ally & Bacon 
1991), which states that out: short-term 
mental capacity operates well only up to 7 
(+ or -  2) pieces of information, but any 
causal connection between this and our 
results must be considered highly 
speculative at this stage! 
Directions for Future Research 
We are familiar with parts of speech 
(commonly, 'nouns', verbs' etc) in 
language. Identification of patterns 
indicative of these would be further 
evidence of language-like characteristics 
and, by allowing us to group together the 
numerous word tokens in any language 
into smaller, more manageable collections 
would facilitate statistical analysis. Some 
attempts have been made in the past to use 
n-gram probabilities in order to define 
word classes or 'parts of speech' 
(Charniak 1993). 
In our own work we have begun the 
development of tools that measure the 
correlation profile between pairs of words, 
as a precursor to deducing general 
principles for 'typing' and clustering into 
syntactico-semantic classes. 
Correlation profile for word pair 
P(wl,w2 ) P(w2,wl) 
The figure 7 above shows the results for 
the relationship between a pair of 
unknown (because of tile substitution 
cipher approach) content and functional 
words, so identified by looking at their 
cross-corpus statistics as described above. 
It can be seen that the functional word has 
a very high probability of preceding the 
content word but has 11o instance of 
directly following it. At least 
metaphorically, the graph can be 
considered to show the 'binding force' 
between the two words varying with their 
separation. We are looking at how this 
metaphor might be used in order to 
describe language as a molecular structure, 
whose 'inter-molecular forces' can be 
related to part-of-speech interaction and 
the development of potential semantic 
categories for the unknown language. 
So far we have mainly been working with 
English, but we have begun to look at 
1024 
languages which represent their flmctional 
relationships by internal changes to words 
or by the addition of prefixes or suffixes. 
Although the process for separating into 
functional and content terms is more 
complex, we believe the fundamental 
results should be consistent. This will be 
one test of the theories presented above. 
In general, we realise that testing our 
hmguage detection algorithms will be a 
significant issue. We do not have 
examples that we know to be definitely 
from non-hulnan, but intelligent origins, 
and we need to look extensively at signals 
of non-intelligent origin which may mimic 
some of the language characteristics 
described above. This will form a 
significant part of our fllture work and we 
welcome discussion and suggestions. 
Conclusion 
Language in its written format has proved 
to be a rich source for a variety ot' 
statistical analyses - some more conclusive 
than others - which when combined, give a 
comprehensive algorithm for identifying 
the presence of language-like systems. 
Analysis stages include compression, 
entropy profile, type-token distribution, 
word-length Zipfian analysis, finding a 
fiequency distribution signature by 
successive chunking, stemming, cohesion 
analysis, phrase-length fiequency 
distribution and pattern comparison across 
smnples. 
REFERENCES 
Ally & Bacon, Cognitive Psychology, 
(third edition), Solso, Massachusetts, 
USA, 1991. 
Baldi, P., & Brunak, S., Bioinformatics - 
The Machine Learning Approach, MIT 
press, Cmnbridge Massaclmsetts, 1998. 
Chadwick, J. Tim Decipherment of Linear 
B, Cmnbridge University Press, 1967. 
Charniak E., Statistical language learning 
Bradford/MIT Press, Cambridge. 1993. 
Elliott, J & Atwell, E, Language in 
signals: the detection of generic species- 
independent intelligent language features 
in symbolic and oral communications, 
Proceedings of the 50 th International 
Astronautical Congress, paper IAA-99- 
IAA.9.1.08, International Astronautical 
Federation, Paris, 1999. 
Elliott, J & Atweli, E., Is anybody out 
there?: the detection of intelligent and 
generic language-like f atures, Journal of 
the British Interplanetary Society, Vo153 
No 1 &2. 
Elliott, J, Decoding the Martian 
Chronicles, MSc project report, School of 
Comlmter Studies, University of Leeds 
1999. 
Hughes J & Atwell E., The automated 
ewtluation of inferred word classifications 
in Proceedings of the European 
Conference on Artificial Intelligence 
(ECAI'94), pp550-554, John Wiley, 
Chichester. 1994. 
Jenson, H. Sell' Organised Criticality, 
Cambridge University Press, 1998. 
Minsky, M., Why Intelligent Aliens will 
be Intelligible, Cambridge University 
Press, 1984. 
Norris, R,. How old is ET?, Proceedings of 
50th International Ashonautical Congress, 
paper 1AA-99-IAA.9.1.04, International 
Astronautical Federation, Paris. 1999. 
Zipf, G. K., Human Behaviour and The 
Principle of Least Effort, Addison Wesley 
Press, New York, 1949 (1965 reprint) 
1025 
Coling 2008: Companion volume ? Posters and Demonstrations, pages 107?110
Manchester, August 2008
 
Comparative Evaluation of Arabic Language Morphological         
Analysers and  Stemmers  
Majdi Sawalha  
School of Computing, 
University of Leeds, 
Leeds LS2 9JT, UK. 
sawalha@comp.leeds.ac.uk 
Eric Atwell 
School of Computing, 
University of Leeds, 
Leeds LS2 9JT, UK. 
eric@comp.leeds.ac.uk 
Abstract   
Arabic morphological analysers and 
stemming algorithms have become a 
popular area of research. Many 
computational linguists have designed 
and developed algorithms to solve the 
problem of morphology and stemming. 
Each researcher proposed his own gold 
standard, testing methodology and 
accuracy measurements to test and 
compute the accuracy of his algorithm. 
Therefore, we cannot make comparisons 
between these algorithms. In this paper 
we have accomplished two tasks. First, 
we proposed four different fair and 
precise accuracy measurements and two 
1000-word gold standards taken from the 
Holy Qur?an and from the Corpus of 
Contemporary Arabic. Second, we 
combined the results from the 
morphological analysers and stemming 
algorithms by voting after running them 
on the sample documents. The evalua-
tion of the algorithms shows that Arabic 
morphology is still a challenge.  
 
1 Three Stemming Algorithms 
 
We selected three stemming algorithms for 
which we had ready access to the implementation 
and/or results. 
Shereen Khoja Stemmer : We obtained a Java 
version of Shereen Khoja?s stemmer 
(Khoja,1999). Khoja?s stemmer removes the 
longest suffix and the longest prefix. It then 
matches the remaining word with verbal and 
                                                 
 
 ? 2008. Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
1
  Tim Buckwalter web site: http://www.qamus.org 
noun patterns, to extract the root. The stemmer 
makes use of several linguistic data files such as 
a list of all diacritic characters, punctuation char-
acters, definite articles, and 168 stop words (Lar-
key & Connell 2001).  
Tim Buckwalter Morphological analyzer:  
Tim Buckwalter developed a morphological ana-
lyzer for Arabic. Buckwalter compiled a single 
lexicon of all prefixes and a corresponding uni-
fied lexicon for suffixes instead of compiling 
numerous lexicons of prefixes and suffix mor-
phemes. He included short vowels and diacritics 
in the lexicons1. 
Tri-literal Root Extraction Algorithm : Al-
Shalabi, Kanaan and Al-Serhan developed a root 
extraction algorithm which does not use any dic-
tionary. It depends on assigning weights for a 
word?s letters multiplied by the letter?s position, 
Consonants were assigned a weight of zero and 
different weights were assigned to the letters 
grouped in the word ?	
? where all affixes 
are formed by combinations of these letters. The 
algorithm selects the letters with the lowest 
weights as root letters (Al-Shalabi et al 2003). 
 
2 Our Approach: Reuse Others? Work 
 
The reuse of existing components is an estab-
lished principle in software engineering. We pro-
cured results from several candidate systems, and 
then developed a program to allow ?voting? on 
the analysis of each word: for each word, exam-
ine the set of candidate analyses. Where all sys-
tems were in agreement, the common analysis is 
copied; but where contributing systems disagree 
on the analysis; take the ?majority vote?, the 
analysis given by most systems. If there is a tie, 
take the result produced by the system with the 
highest accuracy (Atwell & Roberts, 2007). 
 
3 Experiments and Results 
Experiments are done by executing the three 
stemming algorithms, discussed above, on a ran-
107
 domly selected chapter number 29 of the Qur?an 
?Souraht Al-Ankaboot? ?The Spider? in Eng 
lish see figure 1; and a newspaper text taken 
from the Corpus of Contemporary Arabic devel-
oped at the University of Leeds, UK. We se-
lected the test document from the politics, sports 
and economics section, taken from newspaper 
articles, see figure 2 (Al-Sulaiti & Atwell, 2006). 
Each test document contains about 1000 words.  
We manually extracted the roots of the test 
documents? words to compare results from dif-
ferent stemming systems. Roots extracted have 
been checked by Arabic Language scholars who 
are experts in the Arabic Language. 
     Table 1 shows a detailed analysis been done 
for the sample test documents, the Qur?an corpus 
as one unit, and a daily newspaper of contempo-
rary Arabic test document, taken from Al-Rai   
  
Figure 1: Sample from Gold Standard first 
document taken from Chapter 29 of the Qur?an. 
   daily newspaper published in Jordan. The 
analysis also shows that function words such as 
?? ?fi? ?in?, ?? ?min? ?from?, ?? 
?Ala? ?on? and ??? ?Allah? ?GOD? are the 
most frequent words in any Arabic text. On the 
other hand, non functional words with high fre-
quency such as ???? ?Al-Jami?at? ?Uni-
versities? and ??? ?Al-Kuwait? ?Kuwait? 
gives a general idea about the main topic of the 
article. 
     Simple tokenization is applied for the text of 
the gold standard documents. This will ensure 
that test documents can be used to test any 
stemming algorithm smoothly and correctly.  
 
4     Four Accuracy measurements 
In order to fairly compare between different 
stemming algorithms we applied four different 
 
 
Figure 2: Sample from Gold Standard document 
taken from the Corpus of Contemporary Arabic. 
Table 1: Summary of detailed analysis. 
 Qur?an Corpus Gold Standard 
First Document  
Chapter 29 of 
the Qur?an 
Gold Standard    
Second Document 
?Corpus of Con-
temporary Arabic? 
Al-Rai daily 
Newspaper Test 
Document 
Total  
number of 
Tokens 
77,789 987 1005 977 
Word Types 19,278 616 710 678 
Top 10 To-
kens 
Token Freq. Token Freq. Token Freq. Token Freq. 
1   ?   1179   ? 21  35  39 
2   ?  872   	??    17  21  16 
3   ?  832   ?  14  12  13 
4    	??  ?   808   	??    12 	12 ? 	10 ? 
5    ??  652   ? ? ?  12 	11 ? 	9 ? 
6   ? ? ?  640  	 ? 10 ?? 12 ? 	8 ? 
7  ? ? ? 605    	??  ?   11 "!? 10 ?	8 ? 
8   	?? ?   464   ?  8 	7 ?? 8 ? 
9 499 ? ? ?   	??    8 &%? 8 ??	7 ? 
10  ? (?  ? 416 ? ( ?  ? ?  8  7 	%+ 7 
 ,   ?  ? ? 	0  ? / & .  ?	?? ? ?  ? ?    ? ? ? ?       2   ?   	? 3   ? ? 0   	?? ? ? ?  4   ??? ?  ? ? ? ?  ?    ? ??  ?   ?
     ! 5 	 7 6 ) ? ?    	?   	?     ! 5    0 8   (      	4 6      ? 	?  ? ?? ???   ?? ? ?  ?  ?    ? ?? ??? ? ? ?? ?  ? ?  ?   ?? ? ? ??
   4 (  : ?  &  & 5 ; ?  ? ?	?  ?   !     	? > 5   ? ?  / & .  ?  	? ?   ?  ? ??? ?  ? ?  ?>   ?  ?? ? ? ?  ?  ? ? ? ? ? ?  ??  ? ? 
    ?   ? ? ?  ? ? ? ?   @?  ? ? 	   	?  A + ?  ? B    	4 ? ? 	? ?   , +   ? ? ? ?  ? ?   ? ? ?  ?? ?     ??  ? ? ? ?  ?  
    D 	   	2 &   ? ?  ?  	? 6   "    ) B   6 !  05   ?   + ? 	?  E5  &	?F ?? ? ?    ? ? ? ? ?? ? ?  ? ?  ? ?? ? ?  ?  ? ? ?  ?? ? ?  ?    
  0 8    2 , ?   	?   ? 	 G	? ?    3  ? ?    	5   ? ?  	 ! 	?   ? ? ? ?  ? >? ?? ?  ? ?      ?? ? ?   ??  ?  ?  ? ?  ? ? ? ? ? ?
 ; 5 :?>?   5 7 ?  ? ?   !  ? ? ( ?  	8 0  ? / &   ?   I "  	 8 0  ? J ??  ? ? ?  ?? ? ?   ? ?  ?  ? ? ? ? ? ? ?? ? ??? ? ? ? 
   <  K 	  L 5 	    <  ? , N  	  6    / &   ? ? ? + ? 6 ?? 	?  >  ? &) B 	?? ? ? ? ? ? ?  ?  ? ? ? ? ?? ?  ? ?  ?  ??  ?? ? ? ? ? ? ? ? ?  ?  ?? 
 ?   ! J  0  ?   < 0  ;  ) P   0  ! + ,    	 ?   8 ! Q J     0  ?  ?? ? ? ? ? ?  ? ?  ? ?>??? ? ? ? ? ? ?  ??  ? ? ? ? ?  ?? ? ??   ??    	??  ?  
   5 ? 	 G	8 0    ?   S 6  	  ? ? 	 G	? ?    3?  ? ?  ? ?     ? ? ? ?? ? ?? ?  ? ?      ?? ? ?   ??    
 
T<+U?? T;:V	 5,?X 6 (? 	?? T	!	4 ?:  
  T;:U? ? Y!<  T45 APJ T2(4? ?	? ?? ?
 T	!	 ??,J >?	66?  ?+ T+ 6??  ,?<
 .Q4	? A[  ,N	?5? ?	"66 	? AN	?? ?<
 ]	 >? ,??	? ?  ]) ??? ,U?
 ,U? AXU? ]	4? >27 ?X	?? ??,U6? ?!
 A`:)?> ? 	"66? ?	? T5)	? T,4	5?  ??	
 ?%a? A`:? ??!	0 ?b)?7%? ?	? ??cGJc?
  ,N< ?,b	? ?7?? ? ,N? T2e	?` ?I+5 ?<
 5?	??> ,N< 66+ ?) 
 
108
 accuracy measurements. Each time we ran the 
experiment, a comparison of the results with the 
gold standard was performed.  
The first experiment was done by comparing 
each root extracted using the three stemming al-
gorithms with the roots of words in the gold 
standard. 
 
The second experiment excludes from the words? 
list stop words. The third experiment compares 
all word-type roots to the gold standard?s roots. 
Finally, word-type roots excluding the stop 
words are compared to the gold standard?s roots. 
Tables   4-7 show the accuracy rates resulting 
from the four different accuracy measurements. 
 
Table 2: Tokens Accuracy of stemming algo-
rithms after testing on Qur?an gold standard  
Number of Tokens including Stop words 
(978 tokens) 
Stemming 
Algorithm 
Errors Fault 
Rate 
Accuracy 
Khoja stemmer 311 31.8% 68.2% 
Tim Buckwalter 
morph. Analyzer 
419 42.8% 57.16% 
Tri-literal Root 
algorithm 
394 40.3% 59.71% 
Ex.1 434 44.4% 55.6% Voting 
algorithm Ex.2 405 41.4% 58.6% 
Number of Tokens excluding  Stop words (554 
tokens) 
Khoja stemmer 209 37.73% 62.27% 
Tim Buckwalter 
morph. Analyzer 
123                 22.2% 77.8% 
Tri-literal Root 
algorithm 
279 50.36% 49.64% 
Ex.1 266 48.0% 52.0% Voting 
algorithm Ex.2 229 41.3% 58.7% 
Table 3: Word type Accuracy of stemming al-
gorithms  after testing on Qur?an gold standard  
Number of Word Types including Stop words 
(616 word types) 
Stemming Al-
gorithm 
Errors Fault 
Rate 
Accuracy 
Khoja stemmer 224 36.36% 63.64% 
Tim Buckwalter 
morph. Analyzer 
267 43.34% 56.66% 
Tri-literal Root 
algorithm 
266 43.18% 56.82% 
Ex.1 242 39.3% 60.7% Voting 
algorithm Ex.2 219 35.6% 64.4% 
Number of Word types excluding  Stop words 
( 451 word types) 
Khoja stemmer 155 34.37% 65.63% 
Tim Buckwalter 
morph. Analyzer 
251 55.65% 44.34% 
Tri-literal Root 
algorithm 
214 47.45% 52.55% 
Ex.1 174 38.6% 61.4% Voting 
algorithm Ex.2 151 33.5% 66.5% 
Table 4: Token Accuracy of stemming algo-
rithms.  Tested on newspaper gold standard 
Number of Tokens including Stop words(1005 
tokens) 
Stemming Al-
gorithm 
Errors Fault 
Rate 
Accuracy 
Khoja stemmer 231 22.99% 77.01% 
Tim Buckwalter 
morph. Analyzer 
596 59.30% 40.70% 
Tri-literal Root 
algorithm 
234 23.28% 76.72% 
Ex.1 303 30.15% 69.85% Voting 
algorithm Ex.2 266 26.47% 73.53% 
Number of Tokens excluding  Stop words (766 
tokens) 
Khoja stemmer 212 27.7% 72.3% 
Tim Buckwalter 
morph. Analyzer 
431 60.70% 39.30% 
Tri-literal Root 
algorithm 
253 35.63% 64.37% 
Ex.1 303 39.56% 60.44% Voting 
algorithm Ex.2 266 34.73% 65.27% 
Table 5: Word type Accuracy of stemming al-
gorithms.  Tested on newspaper gold standard 
Number of Word Types including Stop words 
(710 word types) 
Stemming Al-
gorithm 
Errors Fault 
Rate 
Accuracy 
Khoja stemmer 232 32.68% 67.32% 
Tim Buckwalter 
morph. Analyzer 
431 60.70% 39.30% 
Tri-literal Root 
algorithm 
253 35.63% 64.37% 
Ex.1 248 34.93% 65.07% Voting 
algorithm Ex.2 215 30.28% 69.71% 
Number of Word types excluding  Stop words 
( 640 word types) 
Khoja stemmer 184 28.75% 71.25% 
Tim Buckwalter 
morph. Analyzer 
423 66.09% 33.91% 
Tri-literal Root 
algorithm 
224 35.00% 65.00% 
Ex.1 252 39.4% 60.6% Voting 
algorithm Ex.2 195 30.5% 69.5% 
109
 Experiments are done for results generated from 
the three stemming algorithms after executing 
them on both gold standard documents.   
      The output analysis of the stemming algo-
rithms is considered as input for the ?voting? 
program. The program reads in these files, token-
izes them, and stores the words and the roots ex-
tracted by each stemming algorithm in temporary 
lists to be used by the voting procedures.  
     The temporary lists work as a bag of words 
that contains all the result analysis of the stem-
ming algorithms. Khoja and the tri-literal stem-
ming algorithms generate only one result analy-
sis for each input word, while Tim Buckwalter 
morphological analyzer generates one or more 
result analysis. These roots are ranked in best- 
first order according to accuracy measurement 
done before. Khoja stemmer results are inserted 
to the list first then the results from tri-literal 
stemming algorithm and finally the results of 
Tim Buckwalter morphological analyzer. 
    After the construction of the lists of all words 
and their roots, a majority voting procedure is 
applied to it to select the most common root 
among the list. If the systems disagree on the 
analysis, the voting algorithm selects ?Majority 
Vote? root as the root of the word. If there is a 
tie, where each stemming algorithm generates a 
different root analysis then the voting algorithm 
selects the root by two ways. Firstly, it simply 
selects the root randomly from the list using the 
FreqDist() Python function in experiment 1. 
Secondly, In experiment 2, the algorithm selects 
the root generated from the highest accuracy 
stemming algorithm which is simply placed in 
the first position of the list as the root of the word 
are inserted to the list using the best-first in terms 
of accuracy strategy.  
     After the voting algorithm, the selected root is 
compared to the gold standard. Tables 2-5 show 
the result of the voting algorithm which achieves 
promising accuracy results of slightly better than 
the best stemming algorithm in experiment 2 and 
a similar accuracy rates for the best stemming 
algorithms in experiment 1.  
 
5  Conclusions 
 
In this paper, we compared between three stem-
ming algorithms; Shereen Khoja?s stemmer, Tim 
Buckwalter?s morphological analyzer and the 
Tri-literal root extraction algorithm.  
     Results of the stemming algorithms are com-
pared with the gold standard using four different 
accuracy measurements. The four accuracy 
measurements show the same accuracy rank for 
the stemming algorithms: the Khoja stemmer 
achieves the highest accuracy then the tri-literal 
root extraction algorithm and finally the Buck-
walter morphological analyzer.  
     The voting algorithm achieves about 62% 
average accuracy rate for Qur?an text and about 
70% average accuracy for newspaper text. The 
results show that the stemming algorithms used 
in the experiments work better on newspaper text 
than Quran text, not unexpectedly as they were 
originally designed for stemming newspaper text.  
   All stemming algorithms involved in the ex-
periments agreed and generate correct analysis 
for simple roots that do not require detailed 
analysis. So, more detailed analysis and en-
hancements are recommended as future work. 
   Most stemming algorithms are designed for 
information retrieval systems where accuracy of 
the stemmers is not important issue. On the other 
hand, accuracy is vital for natural language proc-
essing. The accuracy rates show that the best al-
gorithm failed to achieve accuracy rate of more 
than 75%. This proves that more research is re-
quired. We can not rely on such stemming algo-
rithms for doing further research as Part-of-
Speech tagging and then Parsing because errors 
from the stemming algorithms will propagate to 
such systems.  
    Our experiments are limited to the three 
stemming algorithms. Other algorithms are not 
available freely on the web, and we have been 
unable so far to acquire them from the authors. 
We hope Arabic NLP researchers can cooperate 
further in open-source development of resources. 
 
References 
Al-Shalabi, R., Kanaan, G., & Al-Serhan, H. 
(2003, December). New approach for extract-
ing Arabic roots. Paper presented at the Inter-
national Arab Conference on Information 
Technology (ACIT?2003), Egypt. 
Al-Sulaiti, Latifa; Atwell, Eric 2006. The design 
of a corpus of contemporary Arabic. Interna-
tional Journal of Corpus Linguistics, vol. 11, 
pp. 135-171. 2006. 
Atwell, Eric and Roberts, Andy, 2007. CHEAT: 
combinatory hybrid elementary analysis of text 
in: Proceedings of Corpus Linguistics 2007. 
Khoja, Shereen, 1999. Stemming Arabic Text. 
http://zeus.cs.pacificu.edu/shereen/research.htm 
Larkey Leah. S. and Connell Margrate. E. 2001. 
Arabic information retrieval at UMass. In Pro-
ceedings of TREC 2001, Gaithersburg: NIST, 
2001. 
110
In: Proceedings of CoNLL-2000 and LLL-2000, pages 25-30, Lisbon, Portugal, 2000. 
Increasing our Ignorance of Language: Identifying Language 
Structure in an Unknown 'Signal' 
J ohn  E l l i o t t  and Er i c  A twe l l  and Bi l l  Whyte  
Centre for Computer  Analysis of Language and Speech, School of Computer  Studies 
University of Leeds, Leeds, Yorkshire, LS2 9JT England 
{ j re ,  e r i c ,  b i l lw}?scs . leeds .ac .uk  
Abst rac t  
This paper describes algorithms and software 
developed to characterise and detect generic 
intelligent language-like features in an input 
signal, using natural language learning tech- 
niques: looking for characteristic statistical 
"language-signatures" in test corpora. As 
a first step towards such species-independent 
language-detection, we present a suite of pro- 
grams to analyse digital representations of a 
range of data, and use the results to extrap- 
olate whether or not there are language-like 
structures which distinguish this data from 
other sources, such as music, images, and white 
noise. Outside our own immediate NLP sphere, 
generic communication techniques are of par- 
ticular interest in the astronautical community, 
where two sessions are dedicated to SETI at 
their annual International conference with top- 
ics ranging from detecting ET technology to the 
ethics and logistics of message construction (E1- 
liott and Atwell, 1999; Ollongren, 2000; Vakoch, 
2000). 
1 In t roduct ion  
A useful thought experiment is to imagine 
eavesdropping on a signal from outer space. 
How can you decide that it is a message be- 
tween intelligent life forms? We need a 'lan- 
guage detector': or, to put it more accu- 
rately, something that separates language from 
non-language. But what is special about the 
language signal that separates it from non- 
language? Is it, indeed, separable? 
The problem goal is to separate language 
from non-language without dialogue, and learn 
something about the structure of language in 
the passing. The language may not be human 
(animals, aliens, computers...), the perceptual 
space can be unknown, and we cannot assume 
human language structure but must begin some- 
where. We need to approach the language signal 
from a naive viewpoint, in effect, increasing our 
ignorance and assuming as little as possible. 
Given this standpoint, an informal descrip- 
tion of 'language' might include that it: 
? has structure at several interrelated levels 
? is not random 
? has grammar 
? has letters/characters, words, phrases and 
sentences 
? has parts of speech 
? is recursive 
? has a theme with variations 
? is aperiodic but evolving 
? is generative 
? has transformation rules 
? is designed for communication 
? has Zipfian type-token distributions at sev- 
eral levels 
Language as a 'signal' 
? has some signalling elements (a 'script') 
? has a hierarchy of signalling elements? 
('Words', 'phrases' etc.) 
? is serial? 
? is correlated across a distance of several sig- 
nalling elements applying at various levels 
in the hierarchy 
? is usually not truly periodic 
? is quasi-stationary? 
? is non-ergodic? 
We assume that a language-like signal will be 
encoded symbolically, i.e. with some kind of 
character-stream. Our language-detection al- 
gorithm for symbolic input uses a number of 
25 
statistical clues such as entropy, "chunking" to 
find character bit-length and boundaries, and 
matching against a Zipfian type-token distribu- 
tion for "letters" and "words". 
2 Ident i fy ing  S t ructure  and  the  
'Character  Set '  
The initial task, given an incoming bit-stream, 
is to identify if a language-like structure ex- 
ists and if detected what are the unique pat- 
terns/symbols, which constitute its 'character 
set'. A visualisation of the alternative possible 
byte-lengths i gleaned by plotting the entropy 
calculated for a range of possible byte-lengths 
(fig 1). 
In 'real' decoding of unknown scripts it is ac- 
cepted that identifying the correct set of dis- 
crete symbols is no mean feat (Chadwick, 1967). 
To make life simple for ourselves we assume 
a digital signal with a fixed number of bits 
per character. Very different echniques are re- 
quired to deal with audio or analogue quivalent 
waveforms (Elliott and Atwell, 2000; Elliott and 
Atwell, 1999). We have reason to believe that 
the following method can be modified to relax 
this constraint, but this needs to be tested fur- 
ther. The task then reduces to trying to iden- 
tify the number of bits per character. Given the 
probability of a bit is Pi; the message ntropy 
of a string of length N will be given by the first 
order measure: 
E = SUM\[P i lnP i \ ] ; i  = 1, N 
If the signal contains merely a set of random dig- 
its, the expected value of this function will rise 
monotonically as N increases. However, if the 
string contains a set of symbols of fixed length 
representing a character set used for commu- 
nication, it is likely to show some decrease in 
entropy when analysed in blocks of this length, 
because the signal is 'less random' when thus 
blocked. Of course, we need to analyse blocks 
that begin and end at character boundaries. We 
simply carry out the measurements in sliding 
windows along the data. In figure 1, we see 
what happens when we applied this to samples 
of 8-bit ASCII text. We notice a clear drop, 
as predicted, for a bit length of 8. Modest 
progress though it may be, it is not unreason- 
able to assume that the first piece of ev- 
idence for the presence of language- l ike 
s t ruc ture ,  wou ld  be the  ident i f icat ion of  a 
low-entropy,  character  set w i th in  the  sig- 
nal. 
The next task, still below the stages normally 
tackled by NLL researchers, is to chunk the in- 
coming character-stream into words. Looking 
at a range of (admittedly human language) text, 
if the text includes a space-like word-separator 
character, this will be the most frequent charac- 
ter. So, a plausible hypothesis would be that the 
most frequent character is a word-separator1; 
then plot type-token frequency distributions for 
words, and for word-lengths. If the distribu- 
tions are Zipfian, and there are no significant 
'outliers' (very large gaps between 'spaces' sig- 
nifying very long words) then we have evidence 
corroborating our space hypothesis; this also 
corroborates our byte-length ypothesis, since 
the two are interdependent. 
3 Ident i fy ing  'Words '  
Again, work by crytopaleologists suggests that, 
once the character set has been found, the sep- 
aration into word-like units, is not trivial and 
again we cheat, slightly: we assume that the 
language possesses something akin to a 'space' 
character. Taking our entropy measurement de- 
scribed above as a way of separating characters, 
we now try to identify which character epre- 
sents 'space'. It is not unreasonable to believe 
that, in a word-based language, it is likely to be 
one of the most frequently used characters. 
Using a number of texts in a variety of lan- 
guages, we first identified the top three most 
used characters. For each of these we hy- 
pothesised in turn that it represented 'space'. 
This then allowed us to segment the signal into 
words-like units ('words' for simplicity). We 
could then compute the frequency distribution 
of words as a function of word length, for each 
of the three candidate 'space' characters (fig 2). 
It can be seen that one 'separator' candidate 
(unsurprisingly, in fact, the most frequent char- 
acter of all) results in a very varied distribu- 
tion of word lengths. This is an interesting 
distribution, which, on the right hand side of 
the peak, approximately follows the well-known 
'law' according to Zipf (1949), which predicts 
this behaviour on the grounds of minimum ef- 
1Work is currently progressing on techniques for un- 
supervised word separation without spaces. 
26 
fort in a communication act. Conversely, re- 
sults obtained similar to the 'flatter' distribu- 
tions above, when using the most frequent char- 
acter, is likely to indicate the absence of word 
separators in the signal. 
To ascertain whether the word-length fre- 
quency distribution holds for language in gen- 
eral, multiple samples from 20 different lan- 
guages from Indo-European, Bantu, Semitic, 
Finno-Ugrian and Malayo-Polynesian groups 
were analysed (fig 3). Using statistical measures 
of significance, it was found that most groups 
fell well within 5- only two individual languages 
were near exceeding these limits - of the pro- 
posed Human language word-length profile (E1- 
liott et al, 2000). 
Zipf 's  law is a s t rong  ind icat ion  of  
language- l ike behav iour .  I t  can be used 
to segment  the  signal p rov ided  a 'space'  
character  exists. However, we should not 
assume Zipf to be an infallible language detec- 
tor. Natural phenomena such as molecular dis- 
tribution in yeast DNA possess characteristics 
of power laws (Jenson, 1998). Nevertheless, it 
is worth noting, that such non-language posses- 
sors of power law characteristics generally dis- 
play distribution ranges far greater than lan- 
guage with long repeats far from each other 
(Baldi and Brunak, 1998); characteristics de- 
tectable at this level or at least higher order 
entropic evaluation. 
4 Ident i fy ing  'Phrase- l i ke '  chunks  
Having detected a signal which satisfies cri- 
teria indicating language-like structures at a 
physical evel (Elliott and Atwell, 2000; Elliott 
and Atwell, 1999), second stage analysis is re- 
quired to begin the process of identifying inter- 
nal grammatical components, which constitute 
the basic building blocks of the symbol system. 
With the use of embedded clauses and phrases, 
humans are able to represent an expression or 
description, however complex, as a single com- 
ponent of another description. This allows us to 
build up complex structures far beyond our oth- 
erwise restrictive cognitive capabilities (Minsky, 
1984). Without committing ourselves to a for- 
mal phrase structure approach, (in the Chom- 
skian sense) or even to a less formal 'chunk- 
ing' of language (Sparkle Project, 2000), it is 
this universal hierarchical structure, evident in 
all human languages and believed necessary for 
any advanced communicator ,  that constitutes 
the next phase in our signal analysis (Elliott and 
Atwell, 2000). It is f rom these 'discovered' ba- 
sic syntactic units that analysis of behavioural 
trends and inter-relationships amongst  termi- 
nals and non-terminals alike can begin to unlock 
the encoded internal grammatical  structure and 
indicate candidate parts of speech. To  do this, 
we  make use of a particular feature common to 
many known languages, the 'function' words, 
which occur in corpora with approximately the 
same statistics. These tend to act as bound- 
aries to fairly self-contained semantic/syntactic 
'chunks.' They  can be identified in corpora by 
their usually high frequency of occurrence and 
cross-corpora invariance, as opposed to 'con- 
tent' words which are usually less frequent and 
much more  context dependent. 
Now suppose the function words arrived in a 
text independent of the other words, then they 
would have a Poisson distribution, with some 
long tails (distance between successive function 
words.) But  this is NOT what  happens. In- 
stead, there is empirical evidence that function 
word  separation is constrained to within short 
limits, with very few more  than nine words 
apart (see fig 4). We conjecture that this is 
highly suggestive of chunking. 
5 C lus ter ing  in to  
syntact i co -semant ic  lasses 
Unlike traditional natural language process- 
ing, a solution cannot be assisted using vast 
amounts of training data with well-documented 
'legal' syntax and semantic interpretation or 
known statistical behaviour of speech cate- 
gories. Therefore, at this stage we are endeav- 
ouring to extract the syntactic elements with- 
out a 'Rossetta' stone and by making as few as- 
sumptions as possible. Given this, a generic sys- 
tem is required to facilitate the analysis of be- 
havioural trends amongst selected pairs of ter- 
minals and non-terminals alike, regardless of the 
target language. 
Therefore, an intermediate r search goal is to 
apply Natural Language Learning techniques to 
the identification of "higher-level" lexical and 
grammatical patterns and structure in a lin- 
guistic signal. We have begun the development 
of tools to visualise the correlation profiles be- 
27 
tween pairs of words or parts of speech, as a pre- 
cursor to deducing eneral principles for 'typing' 
and clustering into syntactico-semantic lexical 
classes. Linguists have long known that collo- 
cation and combinational patterns are charac- 
teristic features of natural anguages, which set 
them apart (Sinclair, 1991). Speech and lan- 
guage technology researchers have used word- 
bigram and n-gram models in speech recogni- 
tion, and variants of PoS-bigram models for 
Part-of-Speech tagging. In general, these mod- 
els focus on immediate neighbouring words, but 
pairs of words may have bonds despite sepa- 
ration by intervening words; this is more rele- 
vant in semantic analysis, eg Wilson and Rayson 
(1993), Demetriou (1997). We sought to in- 
vestigate possible bonding between type tokens 
(i.e., pairs of words or between parts of speech 
tags) at a range of separations, by mapping the 
correlation profile between a pair of words or 
tags. This can be computed for given word-pair 
type (wl,w2) by recording each word-pair token 
(wl,w2,d) in a corpus, where d is the distance or 
number of intervening words. The distribution 
of these word-pair tokens can be visualised by 
plotting d (distance between wl and w2) against 
frequency (how many (wl,w2,d) tokens found at 
this distance). Distance can be negative, mean- 
ing that w2 occurred be/ore wl and for any size 
window (i.e., 2 to n). In other words, we postu- 
late that it might be possible to deduce part-of- 
speech membership and, indeed, identify a set 
of part-of-speech classes, using the joint proba- 
bility of words themselves. But is this possible? 
One test would be to take an already tagged 
corpus and see if the parts-of-speech did indeed 
fall into separable clusters. 
Using a five thousand-word extract from the 
LOB corpus (Johansson et al, 1986) to test this 
tool, a number of parts-of-speech pairings were 
analysed for their cohesive profiles. The arbi- 
trary figure of five thousand was chosen, as it 
both represents a sample large enough to re- 
flect trends seen in samples much larger (with- 
out loosing any valuable data) and a sample 
size, which we see as at least plausible when 
analysing ancient or extra-terrestrial l nguages 
where data is at a premium. 
Figure 5 shows the results for the relationship 
between a pair of content and function words, so 
identified by looking at their cross-corpus statis- 
tics. It can be seen that the function word has a 
high probability of preceding the content word 
but has no instance of directly following it. At 
least metaphorically, the graph can be consid- 
ered to show the 'binding force' between the two 
words varying with their separation. We are 
looking at how this metaphor might be used in 
order to describe language as a molecular struc- 
ture, whose 'inter-molecular forces' can be re- 
lated to part-of-speech interaction and the de- 
velopment of potential semantic ategories for 
the unknown language. 
Examining language in such a manner also 
lends itself to summarising ('compressing') the 
behaviour to its more notable features when 
forming profiles. Figure 6 depicts a 3D repre- 
sentation of results obtained from profiling VB- 
tags with six other major syntactic ategories; 
figure 7 shows the main syntactic behavioural 
features found for the co-occurrence of some of 
the major syntactic lasses ranging over the cho- 
sen window of ten words. 
Such a tool may also be useful in other areas, 
such a lexico-grammatical analysis or tagging 
of corpora. Data-oriented approaches to cor- 
pus annotation use statistical n-grams and/or 
constraint-based models; n-grams or constraints 
with wider windows can improve error-rates, 
by examining the topology of the annotation- 
combination space. Such information could be 
used to guide development of Constraint Gram- 
mars. The English Constraint Grammar de- 
scribed in (1995) includes constraint rules up 
to 4 words either side of the current word (see 
Table 16, p352); the peaks and troughs in the 
visualisation tool might be used to find candi- 
date patterns for such long-distance constraints. 
Our research topic NLL4SETI (Natural Lan- 
guage Learning for the Search for Extra- 
Terrestrial Intelligence) is distinctive in that - 
it is potentially a VERY useful application of 
unsupervised NLL; - it starts from more ba- 
sic assumptions than most NLL research: we 
do not assume tokenisation i to characters and 
words, and have no tagged/parsed training cor- 
pus; - it focuses on utilising statistical distri- 
butional universals of language which are com- 
putable and diagnostic; - this focus has led us 
to develop distributional visualisation tools to 
explore type/token combination distributions; - 
the goal is NOT learning algorithms which anal- 
28 
yse/annotate human language in a way which 
human experts would approve of (eg phrase- 
chunking corresponding to a human linguist's 
parsing of English text); but algorithms which 
recognise language-like structuring in a poten- 
tially much wider range of digital data sets. 
6 Summary  and  fu ture  
deve lopments  
To summarise, our achievements to date include 
- a method for splitting a binary digit-stream 
into characters, by using entropy to diagnose 
byte-length; - a method for tokenising unknown 
character-streams into words of language; - 
an approach to chunking words into phrase- 
like sub-sequences, by assuming high-frequency 
function words act as phrase-delimiters; - a vi- 
sualisation tool for exploring word-combination 
patterns, where word-pairs need not be imme- 
diate neighbours but characteristically combine 
despite several intervening words. 
So far, our approaches have involved working 
with languages with which we are most familiar 
and, to a certain extent, making use of linguistic 
'knowns' such as pre-tagged corpora. It is early 
days yet and we make no apology for this initial 
approach. However, we feel that by deliberately 
reducing our dependence on prior knowledge 
('increasing our ignorance of language') and by 
treating language as a 'signal', we might be con- 
tributing a novel approach to natural anguage 
processing which might ultimately lead to a bet- 
ter, more fundamental understanding of what 
distinguishes language from the rest of the sig- 
nal universe. 
Re ferences  
P. Baldi and S. Brunak. 1998. Bioinformatics - The 
Machine Learning Approach. MIT press, Cam- 
bridge Massachusetts. 
J. Chadwick. 1967. The Decipherment ofLinear B. 
Cambridge University Press. 
George Demetriou. 1997. PhD thesis. School of 
Computer Studies, University of Leeds. 
John Elliott and Eric Atwell. 1999. Language insig- 
nals: the detection of generic species-independent 
intelligent language f atures in symbolic and oral 
communications. In Proceedings of the 50th In- 
ternational Astronautical Congress. paper IAA- 
99-IAA.9.1.08, International Astronautical Feder- 
ation, Paris. 
John Elliott and Eric Atwell. 2000. Is there any- 
body out there?: The detection of intelligent 
and generic language-like f atures. Journal of the 
British Interplanetary Society, 53:1/2:13-22. 
John Elliott, Eric Atwell, and Bill Whyte. 2000. 
Language identification in unknown signals. 
In Proceedings of COLING'2000 International 
Conference on Computational Linguistics. Saar- 
bruecken. 
H. Jenson. 1998. Self Organised Criticality. Cam- 
bridge University Press. 
Stig Johansson, Eric Atwell, Roger Garside, 
and Geoffrey Leech. 1986. The Tagged LOB 
corpus: users' manual. Bergen University, 
Norway: ICAME, The Norwegian Comput- 
ing Centre for the Humanities. Available 
from http://www.hit.uib.no/icame/lobman/lob- 
cont.html. 
Fred Karlsson, Atro Voutilainen, Juha Heikkila, 
and Arto Anttila. 1995. Constraint Grammar: 
a language-independent system for parsing unre- 
stricted text. Berlin: Mouton de Gruyter. 
Geoffrey Leech, Roger Garside, and Eric Atwell. 
1983. The automatic grammatical tagging of the 
lob corpus. ICAME Journal, 7:13-33. 
Christopher Manning and Hinrich Schutze. 1999. 
Foundations of Statistical Natural Language Pro- 
cessing. Cambridge: MIT Press. 
M. Minsky. 1984. Why Intelligent Aliens will be In- 
telligible. Cambridge University Press. 
Alexander Ollongren. 2000. Large-size message con- 
struction for eti. In Proceedings of the 50th In- 
ternational Astronautical Congress. paper IAA- 
99-IAA.9.1.09, International Astronautical Feder- 
ation, Paris. 
Sparkle Project. 2000. http://www.ilc.pi.cnr.it/spa 
rkle/wp 1-prefinal/node25.html. 
John Sinclair. 1991. Corpus, concordance, colloca- 
tion describing English language. Oxford Univer- 
sity Press. 
Doug Vakoch. 2000. Communicating scientifi- 
cally formulated spiritual principles in interstel- 
lar messages. In Proceedings of the 50th Inter- 
national Astronautical Congress. paper IAA-99- 
IAA.9.1.10, International Astronautical Federa- 
tion, Paris. 
Andrew Wilson and Paul Rayson. 1993. The au- 
tomatic ontent analysis of spoken discourse. In 
C. Souter and E. Atwell, editors, Corpus based 
computational linguistics. Rodopi, Amsterdam. 
G.K. Zipf. 1949. Human Behaviour and The Prin- 
ciple of Least Effort. Addison Wesley Press, New 
York. (1965 reprint). 
29 
I 
Entropy 
8 
7 
6 
5 
4 
3 
2 
1 
Figure 1 
Language 
I I I I I I 
4 5 6 7 8 9 
400 
350 
"~" 300 
~ 250 
20O 
150 
IO0 
5O 
O 
Figure 2: Candidate word-length distributions 
usin~ the 3 most freouent charaelers. 
Entropy profile as an indicator of character bit-length 
MulUple samples from Indo-European, Bantu, Semitic, Fin no- 
Ug rian. and M~ayo-Polyne$ian l guage groups 
25.00 
20.00 
=o 15.oo -?  ~" 
, , .  5.00 ~,t:;~ ',~%~,~ 
Figure 3 Word length 
F~quency  
Fig are 4 
Function 
Word 
separation i
English. 
I[ll,,. 
3 4 5 6 7 8 9 
Number of words between candidate functional words 
i l aa  - . - - t  . . . . . . . . . . . . . . . . . .  
foa l  .....q . . . . . . . . . . . . . . . . .  
9t~ . -4  . . . . . . . . . . . . . . . . . . . . . . . . .  
J too  ,.  , .  ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6a l  . . . . .4  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
l 
~ to  :- . . . . . . . . . . . . .  4 
. . . . . .  \[ 
Correlation profile tbr word pair 
i Figure 5 
I 
....... ............................. 1................................... 
. .  I 
t 
........................ i ................. i ... 
. . . . . . . . . . .  t . . . . .  i- 
ill .............. i J~, ......... 
P(w I !'unctional~w2 coalcn:) I 
tmquen~ 
Figure 6: VB-tag profile 
Figure 7 Cnoun 
Cnoun \[3, ~,3 
Jj 13. 
Rb Z, Xs 
Prep 6", )~2 
Cc 6", ~3:4 
Vb X2 
Art 13" 
Jj Rb Prep Cc Vb Art 
6" ~'2 13" 13', ~6 6, ~'2 6, ~'2 
\[3 6, ~,5,9 ~'2 ~2,4 6 6, ~,3 
~7 \[3 13" 6, ~9 13 ~2 
~2 6*, ~7 6, ~'3 ~'3 Z*,~9 \[3 
\[3 13, 26 ~,4 Z ~,5 13" 
k2 13 13" 6, z9 z 13. 
13" 6, ~,3,8 Z, ~,2 Z* Z Z, ~4 
Key: Z = Zero bigram - or at offset specified - occurrences. 6 = Very weak bonding - near zero - at 
bigram occurrences. \[3 = Strong bonding at bigram co-occurrences. * = Indicates opposing cohesive 
trend when P.O.S. reversed. Xn = High peak beyond bigram at offset distance of  'n'. @ = Flat 
distribution across offsets - bigram bonding evident. 
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 89?96,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Different measurements metrics to evaluate a chatbot system 
 
Bayan Abu Shawar 
IT department 
Arab Open University 
[add] 
b_shawar@arabou-jo.edu.jo
Eric Atwell 
School of Computing 
University of Leeds 
LS2 9JT, Leeds-UK 
eric@comp .leeds.ac.uk 
 
Abstract 
A chatbot is a software system, which can 
interact or ?chat? with a human user in 
natural language such as English. For the 
annual Loebner Prize contest, rival chat-
bots have been assessed in terms of ability 
to fool a judge in a restricted chat session. 
We are investigating methods to train and 
adapt a chatbot to a specific user?s lan-
guage use or application, via a user-
supplied training corpus. We advocate 
open-ended trials by real users, such as an 
example Afrikaans chatbot for Afrikaans-
speaking researchers and students in 
South Africa. This is evaluated in terms of 
?glass box? dialogue efficiency metrics, 
and ?black box? dialogue quality metrics 
and user satisfaction feedback. The other 
examples presented in this paper are the 
Qur'an and the FAQchat prototypes.  Our 
general conclusion is that evaluation 
should be adapted to the application and 
to user needs. 
1 Introduction 
?Before there were computers, we could distin-
guish persons from non-persons on the basis of an 
ability to participate in conversations. But now, we 
have hybrids operating between person and non 
persons with whom we can talk in ordinary lan-
guage.? (Colby 1999a). Human machine conversa-
tion as a technology integrates different areas 
where the core is the language, and the computa-
tional methodologies facilitate communication be-
tween users and computers using natural language. 
A related term to machine conversation is the 
chatbot, a conversational agent that interacts with 
users turn by turn using natural language. Different 
chatbots or human-computer dialogue systems 
have been developed using text communication 
such as Eliza (Weizenbaum 1966), PARRY (Colby 
1999b), CONVERSE (Batacharia etc 1999), 
ALICE1. Chatbots have been used in different do-
mains such as: customer service, education, web 
site help, and for fun.  
Different mechanisms are used to evaluate 
Spoken Dialogue Systems (SLDs), ranging from 
glass box evaluation that evaluates individual 
components, to black box evaluation that evaluates 
the system as a whole McTear (2002). For exam-
ple, glass box evaluation was applied on the 
(Hirschman 1995) ARPA Spoken Language sys-
tem, and it shows that the error rate for sentence 
understanding was much lower than that for sen-
tence recognition. On the other hand black box 
evaluation evaluates the system as a whole based 
on user satisfaction and acceptance. The black box 
approach evaluates the performance of the system 
in terms of achieving its task, the cost of achieving 
the task in terms of time taken and number of 
turns, and measures the quality of the interaction, 
normally summarised by the term ?user satisfac-
tion?, which indicates whether the user ? gets the 
information s/he wants, is s/he comfortable with 
the system, and gets the information within accept-
able elapsed time, etc.? (Maier et al1996). 
The Loebner prize2 competition has been used 
to evaluate machine conversation chatbots. The 
Loebner Prize is a Turing test, which evaluates the 
ability of the machine to fool people that they are 
talking to human. In essence, judges are allowed a 
short chat (10 to 15 minutes) with each chatbot, 
and asked to rank them in terms of ?naturalness?.  
ALICE (Abu Shawar and Atwell 2003) is the 
Artificial Linguistic Internet Computer Entity, first 
                                                          
1 http://www.alicebot.org/ 
2 http://www.loebner.net/Prizef/loebner-prize.html 
89
implemented by Wallace in 1995. ALICE knowl-
edge about English conversation patterns is stored 
in AIML files. AIML, or Artificial Intelligence 
Mark-up Language, is a derivative of Extensible 
Mark-up Language (XML). It was developed by 
Wallace and the Alicebot free software community 
during 1995-2000 to enable people to input dia-
logue pattern knowledge into chatbots based on the 
A.L.I.C.E. open-source software technology. 
In this paper we present other methods to 
evaluate the chatbot systems. ALICE chtabot sys-
tem was used for this purpose, where a Java pro-
gram has been developed to read from a corpus 
and convert the text to the AIML format. The Cor-
pus of Spoken Afrikaans (Korpus Gesproke Afri-
kaans, KGA), the corpus of the holy book of Islam 
(Qur?an), and the FAQ of the School of Computing 
at University of Leeds3 were used to produce two 
KGA prototype, the Qur?an prototype and the 
FAQchat one consequently.  
Section 2 presents Loebner Prize contest, sec-
tion 3 illustrates the ALICE/AIMLE architecture. 
The evaluation techniques of the KGA prototype, 
the Qur?an prototype, and the FAQchat prototype 
are discussed in sections 4, 5, and 6 consequently. 
The conclusion is presented in section 7. 
2 The Loebner Prize Competition 
The story began with the ?imitation game? which 
was presented in Alan Turing?s paper ?Can Ma-
chine think?? (Turing 1950).  The imitation game 
has a human observer who tries to guess the sex of 
two players, one of which is a man and the other is 
a woman, but while screened from being able to 
tell which is which by voice, or appearance. Turing 
suggested putting a machine in the place of one of 
the humans and essentially playing the same game. 
If the observer can not tell which is the machine 
and which is the human, this can be taken as strong 
evidence that the machine can think.  
Turing?s proposal provided the inspiration for 
the Loebner Prize competition, which was an at-
tempt to implement the Turing test. The first con-
test organized by Dr. Robert Epstein was held on 
1991, in Boston?s Computer Museum. In this in-
carnation the test was known as the Loebner con-
test, as Dr. Hugh Loebner pledged a $100,000 
grand prize for the first computer program to pass 
                                                          
3 http://www.comp.leeds.ac.uk 
the test. At the beginning it was decided to limit 
the topic, in order to limit the amount of language 
the contestant programs must be able to cope with, 
and to limit the tenor. Ten agents were used, 6 
were computer programs. Ten judges would con-
verse with the agents for fifteen minutes and rank 
the terminals in order from the apparently least 
human to most human. The computer with the 
highest median rank wins that year?s prize. Joseph 
Weintraub won the first, second and third Loebner 
Prize in 1991, 1992, and 1993 for his chatbots, PC 
Therapist, PC Professor, which discusses men ver-
sus women, and PC Politician, which discusses 
Liberals versus Conservatives. In 1994 Thomas 
Whalen (Whalen 2003) won the prize for his pro-
gram TIPS, which provides information on a par-
ticular topic. TIPS provides ways to store, 
organize, and search the important parts of sen-
tences collected and analysed during system tests. 
However there are sceptics who doubt the ef-
fectiveness of the Turing Test and/or the Loebner 
Competition. Block, who thought that ?the Turing 
test is a sorely inadequate test of intelligence be-
cause it relies solely on the ability to fool people?; 
and Shieber (1994), who argued that intelligence is 
not determinable simply by surface behavior. 
Shieber claimed the reason that Turing chose natu-
ral language as the behavioral definition of human 
intelligence is ?exactly its open-ended, free-
wheeling nature?, which was lost when the topic 
was restricted during the Loebner Prize.  Epstein 
(1992) admitted that they have trouble with the 
topic restriction, and they agreed ?every fifth year 
or so ? we would hold an open-ended test - one 
with no topic restriction.? They decided that the 
winner of a restricted test would receive a small 
cash prize while the one who wins the unrestricted 
test would receive the full $100,000. 
Loebner in his responses to these arguments be-
lieved that unrestricted test is simpler, less expen-
sive and the best way to conduct the Turing Test. 
Loebner presented three goals when constructing 
the Loebner Prize (Loebner 1994):  
? ?No one was doing anything about the 
Turing Test, not AI.? The initial Loebner 
Prize contest was the first time that the 
Turing Test had ever been formally tried. 
? Increasing the public understanding of AI 
is a laudable goal of Loebner Prize. ?I be-
lieve that this contest will advance AI and 
90
serve as a tool to measure the state of the 
art.? 
? Performing a social experiment. 
 
The first open-ended implementation of the 
Turing Test was applied in the 1995 contest, and 
the prize was granted to Weintraub for the fourth 
time. For more details to see other winners over 
years are found in the Loebner Webpage4.  
In this paper, we advocate alternative evalua-
tion methods, more appropriate to practical infor-
mation systems applications. We have investigated 
methods to train and adapt ALICE to a specific 
user?s language use or application, via a user-
supplied training corpus. Our evaluation takes ac-
count of open-ended trials by real users, rather than 
controlled 10-minute trials. 
3 The ALICE/AIML chatbot architecture 
AIML consists of data objects called AIML ob-
jects, which are made up of units called topics and 
categories. The topic is an optional top-level ele-
ment; it has a name attribute and a set of categories 
related to that topic. Categories are the basic units 
of knowledge in AIML. Each category is a rule for 
matching an input and converting to an output, and 
consists of a pattern, which matches against the 
user input, and a template, which is used in gener-
ating the Alice chatbot answer. The format struc-
ture of AIML is shown in figure 1. 
 
< aiml version=?1.0? > 
< topic name=? the topic? > 
 
<category> 
<pattern>PATTERN</pattern>  
<that>THAT</that> 
<template>Template</template> 
</category> 
       .. 
       .. 
</topic> 
</aiml> 
The <that> tag is optional and means that the cur-
rent pattern depends on a  previous bot output. 
Figure 1. AIML format 
 
                                                          
4 http://www.loebner.net/Prizef/loebner-prize.html 
 
The AIML pattern is simple, consisting only of 
words, spaces, and the wildcard symbols _ and *. 
The words may consist of letters and numerals, but 
no other characters. Words are separated by a sin-
gle space, and the wildcard characters function like 
words. The pattern language is case invariant. The 
idea of the pattern matching technique is based on 
finding the best, longest, pattern match. Three 
types of AIML categories are used: atomic cate-
gory, are those with patterns that do not have wild-
card symbols, _ and   *; default categories are 
those with patterns having wildcard symbols * or 
_. The wildcard symbols match any input but can 
differ in their alphabetical order. For example, 
given input ?hello robot?, if ALICE does not find a 
category with exact matching atomic pattern, then 
it will try to find a category with a default pattern; 
The third type, recursive categories are those with 
templates having <srai> and <sr> tags, which refer 
to simply recursive artificial intelligence and sym-
bolic reduction. Recursive categories have many 
applications: symbolic reduction that reduces com-
plex grammatical forms to simpler ones; divide 
and conquer that splits an input into two or more 
subparts, and combines the responses to each; and 
dealing with synonyms by mapping different ways 
of saying the same thing to the same reply. 
The knowledge bases of almost all chatbots are 
edited manually which restricts users to specific 
languages and domains. We developed a Java pro-
gram to read a text from a machine readable text 
(corpus) and convert it to AIML format. The chat-
bot-training-program was built to be general, the 
generality in this respect implies, no restrictions on 
specific language, domain, or structure. Different 
languages were tested: English, Arabic, Afrikaans, 
French, and Spanish. We also trained with a range 
of different corpus genres and structures, includ-
ing: dialogue, monologue, and structured text 
found in the Qur?an, and FAQ websites.  
The chatbot-training-program is composed of 
four phases as follows: 
? Reading module which reads the dialogue 
text from the basic corpus and inserts it 
into a list. 
? Text reprocessing module, where all cor-
pus and linguistic annotations such as 
overlapping, fillers and others are filtered. 
? Converter module, where the pre-
processed text is passed to the converter to 
consider the first turn as a pattern and the 
91
second as a template. All punctuation is 
removed from the patterns, and the pat-
terns are transformed to upper case. 
? Producing the AIML files by copying the 
generated categories from the list to the 
AIML file. 
An example of a sequence of two utter-
ances from an English spoken corpus is: 
 
<u who=F72PS002> 
<s n="32"><w ITJ>Hello<c PUN>. 
</u> 
<u who=PS000> 
<s n="33"><w ITJ>Hello <w NP0>Donald<c 
PUN>. 
</u> 
After the reading and the text processing 
phase, the text becomes: 
 
F72PS002: Hello 
PS000: Hello Donald 
The corresponding AIML atomic category that 
is generated from the converter modules looks like: 
<category> 
<pattern>HELLO</pattern> 
<template>Hello Donald</template> 
</category> 
As a result different prototypes were developed, 
in each prototype, different machine-learning tech-
niques were used and a new chatbot was tested. 
The machine learning techniques ranged from a 
primitive simple technique like single word match-
ing to more complicated ones like matching the 
least frequent words. Building atomic categories 
and comparing the input with all atomic patterns to 
find a match is an instance based learning tech-
nique. However, the learning approach does not 
stop at this level, but it improved the matching 
process by using the most significant words (least 
frequent word). This increases the ability of find-
ing a nearest match by extending the knowledge 
base which is used during the matching process. 
Three prototypes will be discussed in this paper as 
listed below:  
?  The KGA prototype that is trained by a 
corpus of spoken Afrikaans. In this proto-
type two learning approaches were 
adopted. The first word and the most sig-
nificant word (least frequent word) ap-
proach;  
? The Qur?an prototype that is trained by the 
holy book of Islam (Qur?an): where in ad-
dition to the first word approach, two sig-
nificant word approaches (least frequent 
words) were used, and the system was 
adapted to deal with the Arabic language 
and the non-conversational nature of 
Qur?an as shown in section 5; 
? The FAQchat prototype that is used in the 
FAQ of the School of Computing at Uni-
versity of Leeds. The same learning tech-
niques were used, where the question 
represents the pattern and the answer rep-
resents the template. Instead of chatting for 
just 10 minutes as suggested by the Loeb-
ner Prize, we advocate alternative evalua-
tion methods more attuned to and 
appropriate to practical information sys-
tems applications. Our evaluation takes ac-
count of open-ended trials by real users, 
rather than artificial 10-minute trials as il-
lustrated in the following sections.  
The aim of the different evaluations method-
ologies is as follows: 
? Evaluate the success of the learning tech-
niques in giving answers, based on dia-
logue efficiency, quality and users? 
satisfaction applied on the KGA. 
? Evaluate the ability to use the chatbot as a 
tool to access an information source, and a 
useful application for this, which was ap-
plied on the Qur'an corpus. 
? Evaluate the ability of using the chatbot as 
an information retrieval system by com-
paring it with a search engine, which was 
applied on FAQchat. 
4 Evaluation of the KGA prototype 
We developed two versions of the ALICE that 
speaks Afrikaans language, Afrikaana that speaks 
only Afrikaans and AVRA that speaks English and 
Afrikaans; this was inspired by our observation 
that the Korpus Gesproke Afrikaans actually in-
cludes some English, as Afrikaans speakers are 
generally bilingual and ?code-switch? comfortably. 
We mounted prototypes of the chatbots on web-
sites using Pandorabot service5, and encouraged 
                                                          
5 http://www.pandorabots.com/pandora    
 
92
open-ended testing and feedback from remote us-
ers in South Africa; this allowed us to refine the 
system more effectively.   
We adopted three evaluation metrics: 
? Dialogue efficiency in terms of matching 
type. 
? Dialogue quality metrics based on re-
sponse type. 
? Users' satisfaction assessment based on an 
open-ended request for feedback. 
4.1 Dialogue efficiency metric 
We measured the efficiency of 4 sample dia-
logues in terms of atomic match, first word match, 
most significant match, and no match. We wanted 
to measure the efficiency of the adopted learning 
mechanisms to see if they increase the ability to 
find answers to general user input as shown in ta-
ble 1.  
Matching Type D1 D2 D3 D4 
Atomic 1 3 6 3 
First word 9 15 23 4 
Most significant 13 2 19 9 
No match 0 1 3 1 
Number of turns 23 21 51 17 
Table 1. Response type frequency 
 
The frequency of each type in each dialogue 
generated between the user and the Afrikaans 
chatbot was calculated; in Figure 2, these absolute 
frequencies are normalised to relative probabilities.   
No significant test was applied, this approach to 
evaluation via dialogue efficiency metrics illus-
trates that the first word and the most significant 
approach increase the ability to generate answers 
to users and let the conversation continue.  
Figure 2. Dialogue efficiency: Response Type 
Relative Frequencies  
4.2 Dialogue quality metric 
In order to measure the quality of each re-
sponse, we wanted to classify responses according 
to an independent human evaluation of ?reason-
ableness?: reasonable reply, weird but understand-
able, or nonsensical reply. We gave the transcript 
to an Afrikaans-speaking teacher and asked her to 
mark each response according to these classes. The 
number of turns in each dialogue and the frequen-
cies of each response type were estimated. Figure 3 
shows the frequencies normalised to relative prob-
abilities of each of the three categories for each 
sample dialogue. For this evaluator, it seems that 
?nonsensical? responses are more likely than rea-
sonable or understandable but weird answers. 
4.3 Users' satisfaction 
The first prototypes were based only on literal 
pattern matching against corpus utterances: we had 
not implemented the first word approach and least-
frequent word approach to add ?wildcard? default 
categories. Our Afrikaans-speaking evaluators 
found these first prototypes disappointing and frus-
trating: it turned out that few of their attempts at 
conversation found exact matches in the training 
corpus, so Afrikaana replied with a default ?ja? 
most of the time. However, expanding the AIML 
pattern matching using the first-word and least-
frequent-word approaches yielded more favorable 
feedback. Our evaluators found the conversations 
less repetitive and more interesting. We measure 
user satisfaction based on this kind of informal 
user feed back. 
 
Response Types
0.00
0.20
0.40
0.60
0.80
1.00
Di
alo
gu
e 
1
Di
alo
gu
e 
2
Di
alo
gu
e 
3
Di
alo
gu
e 
4
R
ep
et
io
n
 (
%
)
reasonable
Weird
Non sensical
 
Matching Types
0
0.2
0.4
0.6
0.8
Di
alo
gu
1
Di
alo
gu
e 2
Di
alo
gu
e 3
Di
alo
gu
e 4
re
pe
tit
io
n 
(%
) Atomic
First word
Most
significant
Match
nothing
Figure 3.  The quality of the Dialogue: Response 
type relative probabilities 
93
5 Evaluation of the Qur'an prototype 
In this prototype a parallel corpus of Eng-
lish/Arabic of the holy book of Islam was used, the 
aim of the Qur?an prototype is to explore the prob-
lem of using the Arabic language and of using a 
text which is not conversational in its nature like 
the Qur?an. The Qur?an is composed of 114 soora 
(chapters), and each soora is composed of different 
number of verses. The same learning technique as 
the KGA prototype were applied, where in this 
case if an input was a whole verse, the response 
will be the next verse of the same soora; or if an 
input was a question or a statement, the output will 
be all verses which seems appropriate based on the 
significant word. To measure the quality of the 
answers of the Qur?an chatbot version, the follow-
ing approach was applied: 
1. Random sentences from Islamic sites were 
selected and used as inputs of the Eng-
lish/Arabic version of the Qur?an. 
2. The resulting transcripts which have 67 
turns were given to 5 Muslims and 6 non-
Muslims students, who were asked to label 
each turn in terms of: 
? Related (R), in case the answer was correct 
and in the same topic as the input. 
? Partially related (PR), in case the answer 
was not correct, but in the same topic. 
? Not related (NR), in case the answer was 
not correct and in a different topic. 
Proportions of each label and each class of us-
ers (Muslims and non-Muslims) were calculated as 
the total number over number of users times num-
ber of turns. Four out of the 67 turns returned no 
answers, therefore actually 63 turns were used as 
presented in figure 4. 
In the transcripts used, more than half of the re-
sults were not related to their inputs. A small dif-
ference can be noticed between Muslims and non-
Muslims proportions. Approximately one half of 
answers in the sample were not related from non-
Muslims? point of view, whereas this figure is 58% 
from the Muslims? perspective. Explanation for 
this includes: 
? The different interpretation of the answers. 
The Qur?an uses traditional Arabic lan-
guage, which is sometimes difficult to un-
derstand without knowing the meaning of 
some words, and the historical story be-
hind each verse. 
? The English translation of the Qur?an is 
not enough to judge if the verse is related 
or not, especially given that non-Muslims 
do not have the background knowledge of 
the Qur?an. 
Using chatting to access the Qur?an looks like 
the use of a standard Qur?an search tool. In fact it 
is totally different; a searching tool usually 
matches words not statements. For example, if the 
input is: ?How shall I pray?? using chatting: the 
robot will give you all ayyas where the word 
?pray? is found because it is the most significant 
word. However, using a search tool6 will not give 
you any match.  If the input was just the word 
?pray?, using chatting will give you the same an-
swer as the previous, and the searching tool will 
provide all ayyas that have ?pray? as a string or 
substring, so words such as: ?praying, prayed, etc.? 
will match.  
Another important difference is that in the 
search tool there is a link between any word and 
the document it is in, but in the chatting system 
there is a link just for the most significant words, 
so if it happened that the input statement involves a 
significant word(s), a match will be found, other-
wise the chatbot answer will be: ?I have no answer 
for that?.  
 
Answer types
0%
10%
20%
30%
40%
50%
60%
70%
Related Partialy
Related 
Not related
Answers
P
ro
p
o
rt
io
n
Muslims
Non Muslims
Overall
 
Figure4. The Qur?an proportion of each answer 
type denoted by users 
6 Evaluation of the FAQchat prototype 
To evaluate FAQchat, an interface was built, 
which has a box to accept the user input, and a but-
ton to send this to the system. The outcomes ap-
                                                          
6 http://www.islamicity.com/QuranSearch/ 
94
pear in two columns: one holds the FAQchat an-
swers, and the other holds the Google answers af-
ter filtering Google to the FAQ database only. 
Google allows search to be restricted to a given 
URL, but this still yields all matches from the 
whole SoC website (http://www.comp.leeds.ac.uk) 
so a Perl script was required to exclude matches 
not from the FAQ sub-pages. 
An evaluation sheet was prepared which con-
tains 15 information-seeking tasks or questions on 
a range of different topics related to the FAQ data-
base. The tasks were suggested by a range of users 
including SoC staff and research students to cover 
the three possibilities where the FAQchat could 
find a direct answer, links to more than one possi-
ble answer, and where the FAQchat could not find 
any answer. In order not to restrict users to these 
tasks, and not to be biased to specific topics, the 
evaluation sheet included spaces for users to try 5 
additional tasks or questions of their own choosing. 
Users were free to decide exactly what input-string 
to give to FAQchat to find an answer: they were 
not required to type questions verbatim; users were 
free to try more than once: if no appropriate an-
swer was found; users could reformulate the query. 
The evaluation sheet was distributed among 21 
members of the staff and students. Users were 
asked to try using the system, and state whether 
they were able to find answers using the FAQchat 
responses, or using the Google responses; and 
which of the two they preferred and why. 
Twenty-one users tried the system; nine mem-
bers of the staff and the rest were postgraduates. 
The analysis was tackled in two directions: the 
preference and the number of matches found per 
question and per user. 
Which tool do you prefer?
0%
10%
20%
30%
40%
50%
60%
FAQchat Google
Tool
A
ve
ar
ge
 p
er
ce
nt
ag
e 
nu
m
be
r Staff
Student
Total
6.1 Number of matches per question 
The number of evaluators who managed to find 
answers by FAQchat and Google was counted, for 
each question. 
Results in table 2 shows that 68% overall of our 
sample of users managed to find answers using the 
FAQchat while 46% found it by Google. Since 
there is no specific format to ask the question, 
there are cases where some users could find an-
swers while others could not. The success in find-
ing answers is based on the way the questions were 
presented to FAQchat. 
 
Users 
/Tool 
Mean of users find-
ing answers 
Proportion of find-
ing answers  
 FAQchat Google FAQchat Google 
Staff 5.53 3.87 61% 43% 
Student 8.8 5.87 73% 49% 
Overall 14.3 9.73 68% 46% 
Table 2: Proportion of users finding answers 
 
Of the overall sample, the staff outcome shows 
that 61% were able to find answers by FAQchat 
where 73% of students managed to do so; students 
were more successful than staff. 
6.2 The preferred tool per each question 
For each question, users were asked to state 
which tool they preferred to use to find the answer. 
The proportion of users who preferred each tool 
was calculated. Results in figure 5 shows that 51% 
of the staff, 41% of the students, and 47% overall 
preferred using FAQchat against 11% who pre-
ferred the Google.  
 
Figure5. Proportion of preferred tool 
6.3 Number of matches and preference found 
per user 
The number of answers each user had found 
was counted. The proportions found were the 
same. The evaluation sheet ended with an open 
section inviting general feedback. The following is 
a summary of the feedback we obtained:  
? Both staff and students preferred using the 
FAQchat for two main reasons: 
1. The ability to give direct answers some-
times while Google only gives links. 
2. The number of links returned by the 
FAQchat is less than those returned by 
Google for some questions, which saves 
time browsing/searching. 
95
? Users who preferred Google justified their 
preference for two reasons: 
1. Prior familiarity with using Google. 
2. FAQchat seemed harder to steer with care-
fully chosen keywords, but more often did 
well on the first try. This happens because 
FAQchat gives answers if the keyword 
matches a significant word. The same will 
occur if you reformulate the question and 
the FAQchat matches the same word. 
However Google may give different an-
swers in this case. 
To test reliability of these results, the t=Test 
were applied, the outcomes ensure the previous 
results. 
7 Conclusion 
The Loebner Prize Competition has been used 
to evaluate the ability of chatbots to fool people 
that they are speaking to humans. Comparing the 
dialogues generated from ALICE, which won the 
Loebner Prize with real human dialogues, shows 
that ALICE tries to use explicit dialogue-act lin-
guistic expressions more than usual to re enforce 
the impression that users are speaking to human.  
Our general conclusion is that we should NOT 
adopt an evaluation methodology just because a 
standard has been established, such as the Loebner 
Prize evaluation methodology adopted by most 
chatbot developers. Instead, evaluation should be 
adapted to the application and to user needs. If the 
chatbot is meant to be adapted to provide a specific 
service for users, then the best evaluation is based 
on whether it achieves that service or task 
References 
Abu Shawar B and Atwell E. 2003. Using dialogue 
corpora to retrain a chatbot system. In Proceedings of 
the Corpus Linguistics 2003 conference, Lancaster 
University, UK, pp681-690. 
Batacharia, B., Levy, D., Catizone R., Krotov A. and 
Wilks, Y. 1999. CONVERSE: a conversational com-
panion. In Wilks, Y. (ed.), Machine Conversations. 
Kluwer, Boston/Drdrecht/London, pp. 205-215. 
Colby, K. 1999a. Comments on human-computer con-
versation. In Wilks, Y. (ed.), Machine Conversations. 
Kluwer, Boston/Drdrecht/London, pp. 5-8. 
Colby, K. 1999b.  Human-computer conversation in a 
cognitive therapy program. In Wilks, Y. (ed.), Ma-
chine Conversations. Kluwer, Bos-
ton/Drdrecht/London, pp. 9-19. 
Epstein R. 1992. Can Machines Think?. AI magazine, 
Vol 13, No. 2, pp80-95 
Garner R. 1994. The idea of RED, [Online], 
http://www.alma.gq.nu/docs/ideafred_garner.htm 
Hirschman L. 1995. The Roles of language processing 
in a spoken language interface. In Voice Communi-
cation Between Humans and Machines, D. Roe and J. 
Wilpon (Eds), National Academy Press Washinton, 
DC, pp217-237. 
Hutchens, J. 1996. How to pass the Turing test by 
cheating. [Onlin], http://ciips.ee.uwa.edu.au/Papers/, 
1996 
Hutchens, T., Alder, M. 1998. Introducing MegaHAL. 
[Online], 
http://cnts.uia.ac.be/conll98/pdf/271274hu.pdf 
Loebner H. 1994. In Response to lessons from a re-
stricted Turing Test. [Online], 
http://www.loebner.net/Prizef/In-response.html 
Maier E, Mast M, and LuperFoy S. 1996. Overview. 
In Elisabeth Maier, Marion Mast, and Susan Luper-
Foy (Eds), Dialogue Processing in Spoken Language 
Systems, , Springer, Berlin, pp1-13. 
McTear M. 2002. Spoken dialogue technology: ena-
bling the conversational user interface. ACM Com-
puting Surveys. Vol. 34, No. 1, pp. 90-169. 
Shieber S. 1994. Lessons from a Restricted Turing 
Test. Communications of the Association for Com-
puting Machinery, Vol 37, No. 6, pp70-78 
Turing A. 1950. Computing Machinery and intelli-
gence. Mind 59, 236, 433-460. 
Weizenbaum, J.  1966. ELIZA-A computer program 
for the study of natural language communication be-
tween man and machine. Communications of the 
ACM. Vol. 10, No. 8, pp. 36-45. 
Whalen T.  2003. My experience with 1994 Loebner 
competition, [Online], 
http://hps.elte.hu/~gk/Loebner/story94.htm 
96
Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 25?31
Manchester, August 2008
ProPOSEL: a human-oriented prosody and PoS English lexicon for 
machine learning and NLP 
 
Claire Brierley 
School of Games Computing & Creative 
Technologies 
University of Bolton 
Deane Road 
BOLTON 
BL3 5AB 
cb5@bolton.ac.uk 
Eric Atwell 
School of Computing 
University of Leeds 
LEEDS 
LS2 9JT 
 
 
eric@comp.leeds.ac.uk 
 
 
 
Abstract 
ProPOSEL is a prosody and PoS English lexicon, 
purpose-built to integrate and leverage domain 
knowledge from several well-established lexical 
resources for machine learning and NLP applica-
tions. The lexicon of 104049 separate entries is 
in accessible text file format, is human and ma-
chine-readable, and is intended for open source 
distribution with the Natural Language ToolKit. 
It is therefore supported by Python software tools 
which transform ProPOSEL into a Python dic-
tionary or associative array of linguistic concepts 
mapped to compound lookup keys. Users can 
also conduct searches on a subset of the lexicon 
and access entries by word class, phonetic tran-
scription, syllable count and lexical stress pat-
tern. ProPOSEL caters for a range of different 
cognitive aspects of the lexicon?.   
1 Introduction  
ProPOSEL (Brierley and Atwell, 2008) is a pros-
ody and part-of-speech (PoS) English lexicon 
which merges information from respected elec-
tronic dictionaries and databases, and which is 
purpose-built for linkage with corpora; for popu-
lating tokenized corpus text with a priori linguis-
tic knowledge; for machine learning tasks which 
involve the prosodic-syntactic chunking of text; 
and for open source distribution with NLTK - the 
Python-based Natural Language Toolkit (Bird et 
al, 2007a).   
                                                 
?
 ? 2008. Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
A pronunciation lexicon like ProPOSEL is an 
integral part of the front-end natural language 
processing (NLP) module in a generic text-to-
speech (TTS) synthesis system and constitutes a 
natural way of giving such a system phonetic, 
prosodic and morpho-syntactic insights into input 
text. For English, three such resources, originally 
developed for automatic speech recognition  
(ASR) and listing words and their phonetic tran-
scriptions, are widely used: CELEX-2 (Baayen et 
al, 1996); PRONLEX (Kingsbury et al 1997); 
and CMU, the Carnegie-Mellon Pronouncing 
Dictionary (Carnegie-Mellon University, 1998). 
The latter is used in Edinburgh?s state of the art 
Festival speech synthesis system (Black et al 
1999) and is included as one of the datasets in 
NLTK. 
The starting point for ProPOSEL is CUVPlus1 
(Pedler, 2002), a computer-usable and human-
readable dictionary of inflected forms which 
uniquely identifies word class for each entry via 
C5 PoS tags, the syntactic annotation scheme 
used in the BNC or British National Corpus 
(Burnard, 2000). CUVPlus is an updated version 
of CUV2 (Mitton, 1992), an electronic dictionary 
in accessible text file format which in turn de-
rives from the traditional paper-based Oxford 
Advanced Learner?s Dictionary of Current Eng-
lish (Hornby, 1974). 
Recently, lexica for thirteen world lan-
guages, including US-English, have been 
created via the European-funded LC-STAR 
project (Hartinkainen et al 2003) to address the 
shortage of language resources in the form of 
wide coverage lexica with detailed morpho-
syntactic information that meet the needs of 
ASR, TTS and speech-to-speech translation 
(SST) applications. The incorporation of C5 PoS-
tags in CUVPlus provides this kind of detail and 
                                                 
1
 http://ota.ahds.ac.uk/textinfo/2469.html 
25
distinguishes this lexicon from other paper-based 
and electronic English dictionaries, including 
CELEX-2, PRONLEX and CMU; it also facili-
tates linkage with machine-readable corpora like 
the BNC.  
However, CUVPlus entries compact PoS vari-
ants for a given word form into a single field as 
in the following example where burning is classi-
fied as an adjective, a present participle and a 
noun in Table 1: 
 
burning|AJ0:14,VVG:14,NN1:2| 
Table 1: Sample from CUVPlus record structure 
showing PoS variants for the word form burning 
 
An early operation during ProPOSEL build was 
therefore to introduce one-to-one mappings of 
word form to word class, as defined by C5, to 
facilitate their use as compound lookup keys 
when the lexicon is transformed into a Python 
dictionary or associative array (?4). 
2 ProPOSEL: a repository of phonetic, 
syntactic and prosodic concepts  
The current revised version of ProPOSEL2 is a 
text file of 104049 separate entries, each com-
prising 15 pipe-separated fields arranged as fol-
lows: 
(1) word form; (2) BNC C5 tag; (3) CUV2 capi-
talisation flag alert for word forms which start 
with a capital letter; (4) SAM-PA phonetic tran-
scription; (5) CUV2 tag and frequency rating; (6) 
C5 tag and BNC frequency rating; (7) syllable 
count; (8) lexical stress pattern; (9) Penn Tree-
bank tag(s); (10) default content or function word 
tag; (11) LOB tag(s); (12) C7 tag(s); (13) DISC 
stressed and syllabified phonetic transcription; 
(14) stressed and unstressed values mapped to 
DISC syllable transcriptions; (15) consonant-
vowel [CV] pattern. 
 
sunniest|AJS|0|'sVnIIst|Os%|AJS:0|3|100|JJS 
|C|JJT|JJT|'sV-nI-Ist|'sV:1 nI:0 Ist:0| 
[CV][CV][VCC] 
Table 2: Example entry from ProPOSEL textfile 
 
Table 2 shows an example entry showing all 
fields; subsequent illustrative examples include 
only a subset of fields. For an explanation of 
fields 3 to 7, the reader is referred to Pedler 
                                                 
2
 April 2008 
(2002) and Mitton (1992). A full account of Pro-
POSEL build is planned for a subsequent paper, 
where phonology fields in source lexica (CU-
VPlus, CELEX-2 and CMU) and new phonology 
fields in the prosody and PoS English lexicon 
will be discussed in detail. The rationale for 
fields displaying syllable count, lexical stress 
pattern and CFP status is summarised here in 
section 3. 
Four major PoS tagging schemes have been 
included in ProPOSEL to facilitate linkage with 
several widely used speech corpora: C5 (field 2) 
with the BNC as mentioned; Penn Treebank 
(field 9) with Treebank-3 (Marcus et al 1999); 
LOB (Johansson et al 1986) (field 11) with 
MARSEC (Roach et al 1993); and C7 (field 12) 
with the 2 million-word BNC Sampler Corpus. 
The lookup mechanism described in section 4 
where a match is sought between (token, tag) 
tuples in incoming corpus text and ProPOSEL?s 
compound dictionary keys, also in the form of 
(token, tag) tuples, is possible for all four syntac-
tic annotation schemes represented in the lexi-
con.  
3 Accessing the lexicon through sound, 
syllables and rhythmic structure  
One field of particular significance for Pro-
POSEL?s target application of prosodic phrase 
break prediction (?3) is field (8) for lexical stress 
patterns, symbolic representations of the rhyth-
mic structure of word forms via a string of num-
bers. Thus the pattern for the word form 
,objec?tivity - with secondary stress on the first 
syllable and primary stress on the third syllable - 
is 20100. For some homographs, this lexical 
stress pattern can fluctuate depending on part-of-
speech category and meaning. The wordform 
present is a case in point, as demonstrated by 
fields 1, 2, 4, 7, 8 and 10 for all its entries in 
ProPOSEL shown in Table 3: 
present | AJ0 | ?preznt | 2 | 10 | C | 
present | NN1 | ?preznt | 2 | 10 | C | 
present | VVI | prI?zent | 2 | 01 | C | 
present | VVB | prI?zent | 2 | 01 | C | 
Table 3: Rhythmic structure for the homograph 
present is inverted when it functions as a verb 
 
Two well established phonetic transcription 
schemes are also represented in ProPOSEL: the 
original SAM-PA transcriptions in field 4 and 
DISC stressed and syllabified transcriptions in 
fields 13 and 14 which, unlike SAM-PA and the 
International Phonetic Alphabet (IPA), use a sin-
gle character to represent dipthongs: /p8R/ for 
pair, for example. 
26
 Phonology fields in ProPOSEL constitute a range 
of access routes for users. As an illustration, a 
search for like candidates to the verb obliterate 
might focus on structure and sound: verbs of 4 
syllables (fields 2 and 7), with vowel reduction 
on the first syllable (fields 8 or 14), and primary 
stress on the second syllable (again, a choice of 
fields as users may wish to use the SAM-PA 
phonetic transcriptions). This filter retrieves 
sixty-seven candidates - most but not all of them 
end in /eIt/ - and includes one oddity among 
the examples in Table 4. Further examples of live 
filtered searches are presented in section 5. 
Table 4: Sample of 8 candidate verbs retrieved 
which share requested phonological features with 
the template verb: obliterate  
 
4 ProPOSEL: domain knowledge for 
machine learning  
As previously stated, the rationale for ProPOSEL 
was to integrate information from different dic-
tionaries and databases into one lexicon, custom-
ised for language engineering tasks which in-
volve the prosodic-syntactic chunking of text. 
One such task is automated phrase break predic-
tion: the classification of junctures (whitespaces) 
between words in the input text as either breaks 
(the minority class) or non-breaks. Typically, the 
machine learner is trained on PoS-tagged and 
boundary-annotated text - the speech corpus or 
gold standard - and then tested on an unseen ref-
erence dataset, minus the boundary tags, from the 
same corpus. Finally, it is evaluated by counting 
how many of the original boundary locations 
have been recaptured or predicted by the model.   
Phrase break classifiers have been trained on 
additional text-based features besides PoS tags. 
The CFP status of a token - is it a content word 
(e.g. nouns or adjectives) or function word (e.g. 
prepositions or articles) or punctuation mark? - 
has proved to be a very effective attribute in both 
deterministic and probabilistic models (Liberman 
and Church, 1992; Busser et al 2001) and there-
fore, a default content-word/function-word tag is 
assigned to each entry in ProPOSEL in field (10). 
It is anticipated that further research will suggest 
modifications to this default status when the CFP 
attribute interacts with other text-based features. 
Syllable counts - field (7) in ProPOSEL - have 
already been used successfully in phrase break 
models for English (Atterer and Klein, 2002). 
However, they assume uniformity in terms of 
duration of syllables whereas we know that in 
connected speech, an indefinite number of un-
stressed syllables are packed into the gap be-
tween one stress pulse (Mortimer, 1985) and an-
other, English being a stress-timed language. A 
lexical stress pattern, where syllables are 
weighted 0, 1 or 2, has therefore been included in 
fields (8) and (14) for entries in ProPOSEL be-
cause of its potential as a classificatory feature in 
the machine learning task of phrase break predic-
tion.   
The thematic programme for PASCAL 3  in 
2008 focuses on approaches to supplementing 
raw training data (e.g. the speech corpus) with a 
priori knowledge (e.g. the lexicon) to improve 
performance in machine learning. The prosody-
syntax interface is notoriously complex. Planned 
research into the phrase break prediction task 
will attempt to incorporate a dictionary-derived 
feature such as lexical stress (field 8 in Pro-
POSEL) into a data-driven model to explore this 
interface more fully. 
5 Implementing ProPOSEL as a Python 
dictionary  
The Python programming language has a dic-
tionary mapping object with entries in the form 
of (key, value) pairs. Each key must be unique 
and immutable (e.g. a string or tuple), while the 
values can be any type (e.g. a list). This data 
structure can be exploited by transforming Pro-
POSEL into a live Python dictionary, where the 
recommended access strategy is via compound 
keys (word form and C5 PoS tag) which 
uniquely identify each lexical entry. Thus, using 
a sample of 4 entries to represent ProPOSEL and 
version 0.8 of NLTK, we can use the code in 
Listing 1 (?next page) to convert this mini lexi-
con into the new formalism. The Python diction-
ary method returns an as yet unsorted dictionary, 
where the data structure itself is represented by 
                                                 
3
 Pattern Analysis, Statistical Modelling and Compu-
tational Learning research network 
http://www.cs.man.ac.uk/~neill/thematic08.html 
 
('affiliate', "@'fIlIeIt") 
('caparison', "k@'p&rIs@n") 
('corroborate', "k@'r0b@reIt") 
('manipulate', "m@'nIpjUleIt") 
('originate', "@'rIdZIneIt")  
('perpetuate', "p@'petSUeIt")  
('subordinate', "s@'bOdIneIt") 
('vociferate', "v@'sIf@reIt") 
27
squigs { } and where key : value pairs are sepa-
rated by a colon. Table 5 displays the output 
from Listing 1 (below), demonstrating how mul-
tiple values representing a series of linguistic 
observations on syllable count, lexical stress pat-
tern and content/function word status have now 
been mapped to compound keys (cf. Bird et al 
2007b, chapter 6; Martelli et al 2005 pp. 173-5). 
{ 
('cascaded', 'VVD') : ['3', '010', 'C'], 
('cascaded', 'VVN') : ['3', '010', 'C'], 
('cascading', 'VVG') : ['3', '010', 'C'], 
('cascading', 'AJ0') : ['3', '010', 'C'] 
} 
Table 5: Output from Listing 1  
 
from nltk.book import * # In NLTK 0.9, the import statement would be: import nltk, re, pprint 
lexicon = """ 
cascaded|VVD|0|k&?skeIdId|Ic%,Id%|VVD:1|3|010|VBD|C|VVD|VBD 
cascaded|VVN|0|k&?skeIdId|Ic%,Id%|VVN:0|3|010|VBN|C|VVN,VVNK|VBN 
cascading|VVG|0|k&?skeIdIN|Ib%|VVG:1|3|010|VBG|C|VVG,VVGK|VBG 
cascading|AJ0|0|k&?skeIdIN|Ib%|AJ0:0|3|010|JJ|C|JJ,JK|JJ,JJB,JNP 
""" 
lexicon = [line.split(?|?) for line in list(tokenize.line(lexicon))] 
lexKeys = [(index[0], index[1]) for index in lexicon] 
lexValues = [[index[6], index[7], index[9]] for index in lexicon] 
proPOSEL = dict(zip(lexKeys, lexValues)) 
Listing 1: Code snippet using Python list comprehensions and built-ins to transform the prosody-PoS 
English Lexicon into an associative array 
 
 
    For linkage with corpora and for annotating 
a corpus with the prior knowledge of phonol-
ogy contained in ProPOSEL, a match is sought 
between incoming corpus text in the familiar 
(token, tag) format and the dictionary keys 
(?Table 5). Thus intersection enables corpus 
text to accumulate additional values which 
have the potential to become features for ma-
chine learning tasks. This lookup mechanism 
is relatively straightforward for corpora tagged 
with C5, the basic tagset used in the BNC. For 
corpora tagged with alternative schemes (i.e. 
Penn, LOB, and C7), incoming tokens and 
tags can either be matched against word forms 
and PoS tokens in the corresponding tagset 
field in the lexicon, or C5 tags can be ap-
pended to each item in the input text such that 
lookup can proceed in the normal way. 
6 Filtered searches and having fun 
with ProPOSEL  
ProPOSEL will be supported by a tutorial, of-
fering a range of Python software compatible 
with NLTK, to enable users to prepare the text 
file for NLP; to implement ProPOSEL as a 
Python dictionary; to cross-reference linguistic 
data in the lexicon and corpus text; and to cus-
tomise searches via multiple criteria.  
The previous section demonstrated how 
fine-grained grammatical distinctions in the 
PoS tag field(s) in ProPOSEL are integral to 
linkage with corpora. It also demonstrated how 
an electronic dictionary in the form of a simple 
text file can be reconceived and reconstituted 
as a computational data structure known as an 
associative memory or array. When Pro-
POSEL is thus transformed, filtered searches 
can be performed on the text itself. 
Brierley and Atwell (ibid.) present auto-
matic corpus annotations achieved via inter-
section of two parallel iterables: ProPOSEL?s 
keys and a LOB-tagged corpus extract (this is 
a short extract of 153 tokens just for demon-
stration) which also carries equivalent C5 tags 
generated from the lexicon. A successful 
match between C5 tags in both lists results in a 
corpus sequence object where word tokens and 
syntactic annotations have now been comple-
mented with prosodic information from se-
lected fields in ProPOSEL, as in Table 6: 
 
[["aren't", 'BER+XNOT', 'VBB+XX0', 
['1', '1', 'CF', "'#nt:1"]] 
Table 6: Entry index of length 3, with word 
token mapped to LOB and C5 tags plus sylla-
ble count, lexical stress pattern, CFP status and 
syllable-stress mapping  
 
The corpus sequence object can now be 
queried. Suppose, for instance, we wanted to 
find all bi-syllabic prepositions and particles in 
28
this extract. By specifying part-of-speech and 
syllable count, we unearth just one candidate 
matching our search criteria, as shown in Ta-
ble 7: 
 
['between', 'IN', 'PRP', ['2', '01', 
'F', "bI:0 'twin:1"]] 
Table 7: There is one candidate in the 153 
word extract which meets the condition: PoS 
equals preposition or particle and syllable 
count is 2  
 
It is not always necessary to transform Pro-
POSEL into a Python dictionary, however. 
Users can also read in the lexicon textfile, ap-
ply Python?s splitlines() method to process the 
text as a list of lines, and then apply the split() 
method, with the pipe field separator as argu-
ment, to tokenize each field. Listing 2 presents 
this much more succinctly:  
 
lexicon = open(?filepath?, ?rU?).read() 
lexicon = lexicon.splitlines() 
lexicon = [line.split('|') for line in 
lexicon] 
 
Listing2: Reading in ProPOSEL as a nested 
structure   
 
Users can then perform a search on a de-
fined subset of the lexicon. For example, users 
may wish to retrieve all entries with seven syl-
lables from the lexicon.  As well as returning 
items like: industrialisation, operating-theatre, 
and radioactivity, Listing 3 discovers the 
rather intriguing sir roger de coverley! 
  
for index in lexicon: 
if index[6] == '7': # look in the subset 
print index[0] # return word form(s) 
 
Listing 3: Searching a subset of the lexicon  
 
Another illustration would be finding words 
which rhyme. If we wanted to find all the 
words which rhyme with corpus in the lexi-
con, we could search field (4), for example, 
the SAM-PA phonetic transcriptions, for simi-
lar strings to /'kOp@s/. One way of doing 
this would be to compile a regular expression, 
substituting the metacharacter . for the ?c? in 
corpus and then seek a match in the SAM-PA 
field4. We might also look for minimal pairs, 
replacing the phoneme /s/ with the phoneme 
/z/ as in /'.Op@z/. Retaining the apostrophe 
as diacritic for primary stress before the wild-
card here imitates the lexical stress pattern for 
corpus and is part of the rhyme. It transpires 
there is only one candidate which rhymes with 
corpus in the lexicon and two half rhymes. 
Listing4 gives us porpoise /'pOp@s/ and then 
paupers /'pOp@z/ and torpors /'tOp@z/. 
  
p1 = re.compile("'.Op@s") 
p2 = re.compile("'.Op@z") 
sampa = [index[3] for index in lexicon]  
rhymes1 = p1.findall(' '.join(sampa)) 
rhymes2 = p2.findall(' '.join(sampa)) 
 
Listing 4: Using regular expressions to retrieve 
bi-syllabic words with primary stress on the 
first syllable that rhyme with corpus 
 
7 Cognitive Aspects of the Lexicon 
ProPOSEL and associated access tools are 
presented to the CogALex workshop audience 
to illustrate our approach to enhancing the 
structure, indexing and entry points of elec-
tronic dictionaries. As the Call for Papers 
notes, ?Access strategies vary with the task 
(text understanding vs. text production) and 
the knowledge available at the moment of con-
sultation (word, concept, sound). Unlike read-
ers who look for meanings, writers start from 
them, searching for the corresponding words. 
While paper dictionaries are static, permitting 
only limited strategies for accessing informa-
tion, their electronic counterparts promise dy-
namic, proactive search via multiple criteria 
(meaning, sound, related word) and via diverse 
access routes. ? The goal of this workshop is 
to perform the groundwork for the next gen-
eration of electronic dictionaries, that is, to 
study the possibility of integrating the differ-
ent resources ?? ProPOSEL integrates a range 
of different resources, and enables a variety of 
access strategies, with consultation based on 
various combinations of partial syntactic and 
prosodic knowledge of the target words. It ad-
dresses the main themes of the workshop: 
                                                 
4
 Note that Python lists start at index 0, hence in 
Listing 4, the SAM-PA field is at position [3] in the 
inner list of tokenized list fields for each entry. 
29
7.1 Conceptual input of a dictionary user 
Human users of electronic dictionaries can 
start from partial concepts or patterns when 
they are generating a message or looking for a 
(target) word. Other papers in the workshop 
focus on semantic cues, such as conceptual 
primitives, semantically related words, some 
type of partial definition, something like syn-
sets etc; but speakers/writers may also be 
searching for a word which matches syntactic, 
phonetic or prosodic partial patterns, for ex-
ample seeking a matching rhythm or rhyme. 
 
7.2 Access, navigation and search strategies 
The Call for Papers notes that ?we would 
like to be able to access entries by word form 
but also by meaning and sounds (syllables) 
?Even if input is given in an incomplete, im-
precise or degraded form.? Meaning is clearly 
the main focus of many lexicography re-
searchers, but access by sound, rhythm, pros-
ody, and also syntactic similarity may also 
prove useful complementary strategies for 
some users. 
 
7.3 Indexing words and organizing the lexi-
con 
Another key issue for discussion in the Call 
for Papers is robust yet flexible organization of 
lexical resources: ?Indexing must robustly al-
low for multiple ways of navigation and ac-
cess? ?. By building on and integrating with 
Python and the NLTK Natural Language Tool 
Kit, ProPOSEL can be accessed by other NLP 
tools or via the standard Python interface for 
direct browsing and search. ProPOSEL is also 
a potential exemplar for lexical entry stan-
dardization. Many lexicographers focus on 
standardization of semantics or definitions, but 
standardization of syntactic, phonetic and pro-
sodic information is also an issue. Our prag-
matic approach is to integrate lexical entries 
from a range of resources into a standardized 
Python dictionary format.  
 
7.4 NLP Applications 
We initially developed ProPOSEL in the con-
text of research in linking lexical, syntactic 
and prosodic markup in English corpus text, 
and specifically as a resource for prosodic 
phrase break prediction (Brierley and Atwell, 
2007a,b,c).  The software developed within the 
NLTK architecture has been able to utilize 
existing NLTK tools for PoS-tagging, phrase-
chunking and partial parsing; in turn, other 
researchers in these fields may want to use the 
syntactic information in ProPOSEL in their 
future NLP applications, particularly in re-
search which attempts to compare or map be-
tween alternative tagsets or labeling systems, 
eg (Nancarrow and Atwell 2007), Atwell and 
Roberts 2006), (Atwell et al2000), (Teufel 
1995).  
8 Conclusions 
The English lexicon presented in this paper, - a 
revised version to that reported in (Brierley 
and Atwell, 2008), - is an assembly of domain 
knowledge of phonology and syntax from sev-
eral widely used lexical resources. Linkage 
with corpora is facilitated by the inclusion of 
four variant PoS tagging schemes in the lexi-
con and by re-thinking and reconstituting the 
lexicon as a Python dictionary or associative 
array. A successful match between (token, tag) 
pairings in input text and new linguistic anno-
tations mapped to ProPOSEL?s compound 
keys will in turn embed a priori knowledge 
from the lexicon in data-driven models derived 
from a corpus and enhance performance in 
machine learning. The lexicon is also human-
oriented (de Schryver, 2003). ProPOSEL?s 
software tools are compatible with NLTK and 
enable users to define and search a subset of 
the lexicon and access entries by word class, 
phonetic transcription, syllable count and 
rhythmic structure.  ProPOSEL was initially 
developed as a language engineering resource 
for use in our own research, but in the process 
of development we have also addressed sev-
eral more general issues relating to cognitive 
aspects of the lexicon: the partial patterns in 
the mind of a dictionary user; the need for ac-
cess and search by sound, rhythm, prosody, 
and also syntactic similarity; robust and stan-
dardised organization of lexical entries from 
different sources; and ease of integration into 
NLP applications. 
References 
Atterer M., and E. Klein. 2002. Integrating Lin-
guistic and Performance-Based Constraints for 
Assigning Phrase Breaks. In Proceedings of 
Coling 2002:29-35. 
Atwell, E., G. Demetriou, J. Hughes, A. Schriffin, 
C. Souter, S. Wilcock. 2000. A comparative 
evaluation of modern English corpus grammati-
cal annotation schemes. ICAME Journal, vol. 
24, pp. 7-23. 
30
Atwell, E. and A. Roberts. 2006. Combinatory hy-
brid elementary analysis of text. In Kurimo, M, 
Creutz, M & Lagus, K (editors) Proceedings of 
the PASCAL Challenge Workshop on Unsuper-
vised Segmentation of Words into Morphemes. 
Venice. 
Baayen, R. H., R. Piepenbrock, and L. Gulikers 
1996. CELEX2 Linguistic Data Consortium, 
Philadelphia 
Bird, S., E. Loper, and E. Klein 2007a. NLTK-lite 
0.8 beta [June 2007] Available online from: 
http://nltk.sourceforge.net/index.php/Main_Page 
(accessed: 21/06/07). 
Bird, S., E. Klein, and E. Loper 2007b. Natural 
Language Processing Available online from: 
http://nltk.sourceforge.net/index.php/Book (ac-
cessed: 21/09/07). 
Black A.W., P. Taylor, and R. Caley. 1999. The 
Festival Speech Synthesis System: System 
Documentation Festival version 1.4 Available 
online from: 
http://www.cstr.ed.ac.uk/projects/festival/manua
l/festival_toc.html (Accessed: 07/03/08) 
Brierley, C. and E. Atwell. 2007a. Corpus-based 
evaluation of prosodic phrase break prediction 
in: Proceedings of Corpus Linguistics 2007, 
Birmingham University. 
Brierley, C. and E. Atwell. 2007b. An approach for 
detecting prosodic phrase boundaries in spoken 
English. ACM Crossroads journal, vol. 14.1. 
Brierley, C. and E. Atwell. 2007c. Prosodic phrase 
break prediction: problems in the evaluation of 
models against a gold standard. Traitement 
Automatique des Langues, vol. 48.1. 
Brierley, C. and E. Atwell. 2008 ProPOSEL: a 
Prosody and POS English Lexicon for Language 
Engineering. In Proceedings of LREC?08 Lan-
guage Resources and Evaluation Conference, 
Marrakech, Morocco. May 2008.   
Burnard, L. (ed.) 2000. Reference Guide for the 
British National Corpus (World Edition) Avail-
able online from: 
http://www.natcorp.ox.ac.uk/docs/userManual/ 
(accessed: 20/05/07). 
Busser, B. W. Daelemans, and A. van den Bosch 
2001. Predicting phrase breaks with memory-
based learning. 4th ISCA Tutorial and Research 
Workshop on Speech Synthesis. Edinburgh, 
2001.  
Carnegie-Mellon University 1998. The CMU Pro-
nouncing Dictionary (v. 0.6) Available online 
from: http://www.speech.cs.cmu.edu/cgi-
bin/cmudict (accessed: 21/06/07). 
Hartinkainen, E., G. Maltese, A. Moreno, S. Sham-
mass, U. Ziegenhain 2003. Large Lexica for 
Speech-to-Speech Translation: frm specification 
to creation. EUROSPEECH-2003:1529-1532. 
Hornby, A.S. 1974. Oxford Advanced Learner?s 
Dictionary of Current English (third edition) 
Oxford: Oxford University Press  
Johansson, S; Atwell, E S; Garside, R; Leech, G. 
1986. The Tagged LOB Corpus - User Manual, 
160pp, Bergen, Norwegian Computing Centre 
for the Humanities.  
Kingsbury, P., S. Strassel, C. McLemore, and R. 
MacIntyre 1997. CALLHOME American Eng-
lish Lexicon (PRONLEX) Linguistic Data Con-
sortium, Philadelphia 
 Liberman, M.Y., and K.W. Church 1992. Text 
Analysis and Word Pronunciation in Text-to-
Speech Synthesis. In Furui, S., and Sondhi, 
M.M., (eds.) Advances in Speech Signal Proc-
essing New York, Marcel Dekker, Inc. 
Marcus, M.P., B. Santorini, M.A. Marcinkiewicz, 
and A. Taylor 1999. TREEBANK-3 Linguistic 
Data Consortium, Philadelphia 
Martelli, A., A. Martelli Ravenscroft, and D. 
Ascher 2005. Python Cookbook (second edition) 
Sebastopol: O?Reilly Media, Inc. 
Mitton, R. 1992. A description of a computer-
usable dictionary file based on the Oxford Ad-
vanced Learner?s Dictionary of Current English 
Available online and accessed (22/03/08) from: 
http://comp.lin.msu.edu/stabler-
notes/1850/ascii_0710-2.txt 
Mortimer, C. 1985. Elements of Pronunciation. 
Cambridge: Cambridge University Press 
Nancarrow, O. and E. Atwell. 2007.A comparative 
study of the tagging of adverbs in modern Eng-
lish corpora Proceedings of Corpus Linguistics 
2007. Birmingham University. 
Pedler, J. 2002. CUVPlus [Electronic Resource] 
Oxford Text Archive Available online from: 
http://ota.ahds.ac.uk/textinfo/2469.html (ac-
cessed: 21/06/07) 
Roach P., G. Knowles, T. Varadi and S.C. Arn-
field. 1993. Marsec: A machine-readable spoken 
English corpus Journal of the International 
Phonetic Association, vol. 23, no. 1, pp. 47?53. 
Schryver, G. M. de. 2003. Lexicographers? Dreams 
in the Electronic-Dictionary Age. International 
Journal of Lexicography 2003 16(2):143-199 
Teufel, S.. 1995. A support tool for tagset mapping. 
Proceedings of SIGDAT 1995. Workshop in co-
operation with EACL 95, Dublin 
31
