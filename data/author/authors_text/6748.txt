ASSIST: Automated semantic assistance for translators
Serge Sharoff, Bogdan Babych
Centre for Translation Studies
University of Leeds, LS2 9JT UK
{s.sharoff,b.babych}@leeds.ac.uk
Paul Rayson, Olga Mudraya, Scott Piao
UCREL, Computing Department
Lancaster University, LA1 4WA, UK
{p.rayson,o.moudraia,s.piao}@lancs.ac.uk
Abstract
The problem we address in this paper is
that of providing contextual examples of
translation equivalents for words from the
general lexicon using comparable corpora
and semantic annotation that is uniform
for the source and target languages. For
a sentence, phrase or a query expression in
the source language the tool detects the se-
mantic type of the situation in question and
gives examples of similar contexts from
the target language corpus.
1 Introduction
It is widely acknowledged that human transla-
tors can benefit from a wide range of applications
in computational linguistics, including Machine
Translation (Carl and Way, 2003), Translation
Memory (Planas and Furuse, 2000), etc. There
have been recent research on tools detecting trans-
lation equivalents for technical vocabulary in a re-
stricted domain, e.g. (Dagan and Church, 1997;
Bennison and Bowker, 2000). The methodology
in this case is based on extraction of terminology
(both single and multiword units) and alignment
of extracted terms using linguistic and/or statisti-
cal techniques (D?jean et al, 2002).
In this project we concentrate on words from the
general lexicon instead of terminology. The ratio-
nale for this focus is related to the fact that trans-
lation of terms is (should be) stable, while gen-
eral words can vary significantly in their transla-
tion. It is important to populate the terminologi-
cal database with terms that are missed in dictio-
naries or specific to a problem domain. However,
once the translation of a term in a domain has been
identified, stored in a dictionary and learned by
the translator, the process of translation can go on
without consulting a dictionary or a corpus.
In contrast, words from the general lexicon ex-
hibit polysemy, which is reflected differently in
the target language, thus causing the dependency
of their translation on corresponding context. It
also happens quite frequently that such variation
is not captured by dictionaries. Novice translators
tend to rely on dictionaries and use direct trans-
lation equivalents whenever they are available. In
the end they produce translations that look awk-
ward and do not deliver the meaning intended by
the original text.
Parallel corpora consisting of original texts
aligned with their translations offer the possibility
to search for examples of translations in their con-
text. In this respect they provide a useful supple-
ment to decontextualised translation equivalents
listed in dictionaries. However, parallel corpora
are not representative: millions of pages of orig-
inal texts are produced daily by native speakers
in major languages, such as English, while trans-
lations are produced by a small community of
trained translators from a small subset of source
texts. The imbalance between original texts and
translations is also reflected in the size of parallel
corpora, which are simply too small for variations
in translation of moderately frequent words. For
instance, frustrate occurs 631 times in 100 million
words of the BNC, i.e. this gives in average about
6 uses in a typical parallel corpus of one million
words.
2 System design
2.1 The research hypothesis
Our research hypothesis is that translators can be
assisted by software which suggests contextual ex-
139
amples in the target language that are semantically
and syntactically related to a selected example in
the source language. To enable greater coverage
we will exploit comparable rather than parallel
corpora.
Our research hypothesis leads us to a number of
research questions:
? Which semantic and syntactic contextual fea-
tures of the selected example in the source
language are important?
? How do we find similar contextual examples
in the target language?
? How do we sort the suggested target lan-
guage contextual examples in order to max-
imise their usefulness?
In order to restrict the research to what is
achievable within the scope of this project, we are
focussing on translation from English to Russian
using a comparable corpus of British and Rus-
sian newspaper texts. Newspapers cover a large
set of clearly identifiable topics that are compara-
ble across languages and cultures. In this project,
we have collected a 200-million-word corpus of
four major British newspapers and a 70-million-
word corpus of three major Russian newspapers
for roughly the same time span (2003-2004).1
In our proposed method, contexts of uses of En-
glish expressions defined by keywords are com-
pared to similar Russian expressions, using se-
mantic classes such as persons, places and insti-
tutions. For instance, the word agreement in the
example the parties were frustratingly close to
an agreement = ??????? ???? ?? ???????? ??????
? ?????????? ?????????? belongs to a seman-
tic class that also includes arrangement, contract,
deal, treaty. In the result, the search for collo-
cates of ??????? (close) in the context of agree-
ment words in Russian gives a short list of mod-
ifiers, which also includes the target: ?? ????????
??????.
2.2 Semantic taggers
In this project, we are porting the Lancaster En-
glish Semantic Tagger (EST) to the Russian lan-
guage. We have reused the existing semantic field
taxonomy of the Lancaster UCREL semantic anal-
ysis system (USAS), and applied it to Russian. We
1Russian newspapers are significantly shorter than their
British counterparts.
have also reused the existing software framework
developed during the construction of a Finnish Se-
mantic Tagger (L?fberg et al, 2005); the main ad-
justments and modifications required for Finnish
were to cope with the Unicode character set (UTF-
8) and word compounding.
USAS-EST is a software system for automatic
semantic analysis of text that was designed at
Lancaster University (Rayson et al, 2004). The
semantic tagset used by USAS was originally
loosely based on Tom McArthur?s Longman Lexi-
con of Contemporary English (McArthur, 1981).
It has a multi-tier structure with 21 major dis-
course fields, subdivided into 232 sub-categories.2
In the ASSIST project, we have been working on
both improving the existing EST and developing a
parallel tool for Russian - Russian Semantic Tag-
ger (RST). We have found that the USAS semantic
categories were compatible with the semantic cat-
egorizations of objects and phenomena in Russian,
as in the following example:3
poor JJ I1.1- A5.1- N5- E4.1- X9.1-
?????? A I1.1- A6.3- N5- O4.2- E4.1-
However, we needed a tool for analysing the
complex morpho-syntactic structure of Russian
words. Unlike English, Russian is a highly in-
flected language: generally, what is expressed in
English through phrases or syntactic structures
is expressed in Russian via morphological in-
flections, especially case endings and affixation.
For this purpose, we adopted a Russian morpho-
syntactic analyser Mystem that identifies word
forms, lemmas and morphological characteristics
for each word. Mystem is used as the equivalent
of the CLAWS part-of-speech (POS) tagger in the
USAS framework. Furthermore, we adopted the
Unicode UTF-8 encoding scheme to cope with the
Cyrillic alphabet. Despite these modifications, the
architecture of the RST software mirrors that of
the EST components in general.
The main lexical resources of the RST include
a single-word lexicon and a lexicon of multi-word
expressions (MWEs). We are building the Russian
lexical resources by exploiting both dictionaries
and corpora. We use readily available resources,
e.g. lists of proper names, which are then se-
2For the full tagset, see http://www.comp.lancs.
ac.uk/ucrel/usas/
3I1.1- = Money: lack; A5.1- = Evaluation: bad; N5- =
Quantities: little; E4.1- = Unhappy; X9.1- = Ability, intel-
ligence: poor; A6.3- = Comparing: little variety; O4.2- =
Judgement of appearance: bad
140
mantically classified. To bootstrap the system, we
have hand-tagged the 3,000 most frequent Russian
words based on a large newspaper corpus. Subse-
quently, the lexicons will be further expanded by
feeding texts from various sources into the RST
and classifying words that remain unmatched. In
addition, we will experiment with semi-automatic
lexicon construction using an existing machine-
readable English-Russian bilingual dictionary to
populate the Russian lexicon by mapping words
from each of the semantic fields in the English lex-
icon in turn. We aim at coverage of around 30,000
single lexical items and up to 9,000 MWEs, com-
pared to the EST which currently contains 54,727
single lexical items and 18,814 MWEs.
2.3 The user interface
The interface is powered by IMS Corpus Work-
bench (Christ, 1994) and is designed to be used in
the day-to-day workflow of novice and practising
translators, so the syntax of the CWB query lan-
guage has been simplified to adapt it to the needs
of the target user community.
The interface implements a search model for
finding translation equivalents in monolingual
comparable corpora, which integrates a number of
statistical and rule-based techniques for extending
search space, translating words and multiword ex-
pressions into the target language and restricting
the number of returned candidates in order to max-
imise precision and recall of relevant translation
equivalents. In the proposed search model queries
can be expanded by generating lists of collocations
for a given word or phrase, by generating sim-
ilarity classes4 or by manual selection of words
in concordances. Transfer between the source
language and target language is done via lookup
in a bilingual dictionary or via UCREL seman-
tic codes, which are common for concepts in both
languages. The search space is further restricted
by applying knowledge-based and statistical fil-
ters (such as part-of-speech and semantic class fil-
ters, IDF filter, etc), by testing the co-occurrence
of members of different similarity classes or by
manually selecting the presented variants. These
procedures are elementary building blocks that are
used in designing different search strategies effi-
cient for different types of translation equivalents
4Simclasses consist of words sharing collocates and are
computed using Singular Value Decomposition, as used by
(Rapp, 2004), e.g. Paris and Strasbourg are produced for
Brussels, or bus, tram and driver for passenger.
and contexts.
The core functionality of the system is intended
to be self-explanatory and to have a shallow learn-
ing curve: in many cases default search parame-
ters work well, so it is sufficient to input a word
or an expression in the source language in or-
der to get back a useful list of translation equiv-
alents, which can be manually checked by a trans-
lator to identify the most suitable solution for a
given context. For example, the word combina-
tion frustrated passenger is not found in the ma-
jor English-Russian dictionaries, while none of the
candidate translations of frustrated are suitable in
this context. The default search strategy for this
phrase is to generate the similarity class for En-
glish words frustrate, passenger, produce all pos-
sible translations using a dictionary and to test co-
occurrence of the resulting Russian words in target
language corpora. This returns a list of 32 Rus-
sian phrases, which follow the pattern of ?annoyed
/ impatient / unhappy + commuter / passenger /
driver?. Among other examples the list includes
an appropriate translation ??????????? ????????
(?unsatisfied passenger?).
The following example demonstrates the sys-
tem?s ability to find equivalents when there is
a reliable context to identify terms in the two
languages. Recent political developments in
Russia produced a new expression ?????????????
?????????? (?representative of president?), which
is as yet too novel to be listed in dictionaries.
However, the system can help to identify the peo-
ple that perform this duty, translate their names
to English and extract the set of collocates that
frequently appear around their names in British
newspapers, including Putin?s personal envoy and
Putin?s regional representative, even if no specific
term has been established for this purpose in the
British media.
As words cannot be translated in isolation and
their potential translation equivalents also often
consist of several words, the system detects not
only single-word collocates, but also multiword
expressions. For instance, the set of Russian
collocates of ?????????? (bureaucracy) includes
???????? (Brussels), which offers a straightfor-
ward translation into English and has such mul-
tiword collocates as red tape, which is a suitable
contextual translation for ??????????.
More experienced users can modify default pa-
rameters and try alternative strategies, construct
141
their own search paths from available basic build-
ing blocks and store them for future use. Stored
strategies comprise several elementary stages but
are executed in one go, although intermediate re-
sults can also be accessed via the ?history? frame.
Several search paths can be tried in parallel and
displayed together, so an optimal strategy for a
given class of phrases can be more easily identi-
fied.
Unlike Machine Translation, the system does
not translate texts. The main thrust of the sys-
tem lies in its ability to find several target language
examples that are relevant to the source language
expression. In some cases this results in sugges-
tions that can be directly used for translating the
source example, while in other cases the system
provides hints for the translator about the range of
target language expressions beyond what is avail-
able in bilingual dictionaries. Even if the preci-
sion of the current version is not satisfactory for an
MT system (2-3 suitable translations out of 30-50
suggested examples), human translators are able
to skim through the suggested set to find what is
relevant for the given translation task.
3 Conclusions
The set of tools is now under further development.
This involves an extension of the English seman-
tic tagger, development of the Russian tagger with
the target lexical coverage of 90% of source texts,
designing the procedure for retrieval of semanti-
cally similar situations and completing the user in-
terface. Identification of semantically similar sit-
uations can be improved by the use of segment-
matching algorithms as employed in Example-
Based MT and translation memories (Planas and
Furuse, 2000; Carl and Way, 2003).
There are two main applications of the pro-
posed methodology. One concerns training trans-
lators and advanced foreign language (FL) learn-
ers to make them aware of the variety of transla-
tion equivalents beyond the set offered by the dic-
tionary. The other application pertains to the de-
velopment of tools for practising translators. Al-
though the Russian language is not typologically
close to English and uses another writing system
which does not allow easy identification of cog-
nates, Russian and English belong to the same
Indo-European family and the contents of Rus-
sian and English newspapers reflect the same set
of topics. Nevertheless, the application of this
research need not be restricted to the English-
Russian pair only. The methodology for multilin-
gual processing of monolingual comparable cor-
pora, first tested in this project, will provide a
blueprint for the development of similar tools for
other language combinations.
Acknowledgments
The project is supported by two EPSRC grants:
EP/C004574 for Lancaster, EP/C005902 for Leeds.
References
Peter Bennison and Lynne Bowker. 2000. Designing a
tool for exploiting bilingual comparable corpora. In
Proceedings of LREC 2000, Athens, Greece.
Michael Carl and Andy Way, editors. 2003. Re-
cent advances in example-based machine transla-
tion. Kluwer, Dordrecht.
Oliver Christ. 1994. A modular and flexible archi-
tecture for an integrated corpus query system. In
COMPLEX?94, Budapest.
Ido Dagan and Kenneth Church. 1997. Ter-
might: Coordinating humans and machines in bilin-
gual terminology acquisition. Machine Translation,
12(1/2):89?107.
Herv? D?jean, ?ric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In COLING 2002.
Laura L?fberg, Scott Piao, Paul Rayson, Jukka-Pekka
Juntunen, Asko Nyk?nen, and Krista Varantola.
2005. A semantic tagger for the Finnish language.
In Proceedings of the Corpus Linguistics 2005 con-
ference.
Tom McArthur. 1981. Longman Lexicon of Contem-
porary English. Longman.
Emmanuel Planas and Osamu Furuse. 2000. Multi-
level similar segment matching algorithm for trans-
lation memories and example-based machine trans-
lation. In COLING, 18th International Conference
on Computational Linguistics, pages 621?627.
Reinhard Rapp. 2004. A freely available automatically
generated thesaurus of related words. In Proceed-
ings of LREC 2004, pages 395?398.
Paul Rayson, Dawn Archer, Scott Piao, and Tony
McEnery. 2004. The UCREL semantic analysis
system. In Proceedings of the workshop on Be-
yond Named Entity Recognition Semantic labelling
for NLP tasks in association with LREC 2004, pages
7?12.
142
Comparing Corpora using Frequency Profiling 
Paul RAYSON 
Computing Deparanent, 
Lancaster University 
Lancaster, UK, 
paul@comp.lancs.ac.uk 
Roger GARSlDE 
Computing Department, 
Lancaster University 
Lancaster, UK, 
rgg@comp.lancs.ac.uk 
Abstract 
This paper describes a method of comparing 
corpora which uses frequency profiling. The 
method can be used to discover key words in 
the corpora which differentiate one corpus 
from another. Using annotated corpora, it 
can be applied to discover key grammatical 
or word-sense categories. This can be used 
as a quick way in to find the differences 
between the corpora and is shown to have 
applications in the study of social 
differentiation in the use of English 
vocabulary, profiling of learner English and 
document analysis in the software 
engineering process. 
1 Introduct ion 
Corpus-based techniques have increasingly been 
used to compare language usage in recent years. 
One of the largest early studies was the 
comparison of one million words of American 
English (the Brown corpus) with one million 
words of British English (the LOB corpus) by 
Hofland and Johansson (1982). A difference 
coefficient defined by Yule (1944) showed the 
relative frequency of a word in the two corpora. 
A statistical goodness-of-fit test, the Chi-squared 
test, was also used to compare word frequencies 
across the two corpora. They noted any resulting 
chi-squared values which indicated that a 
statistically significant difference at the 5%, 1%, 
or 0.1% level had been detected between the 
frequency of a word in American English and in 
British English. The null hypothesis of the test is 
that there is no difference between the observed 
frequencies. 
More recently, this size of corpus comparison 
is becoming the standard even for postgraduate 
studies with the increasing availability of 
corpora and reasoning that one million words 
gives sufficient evidence for higher frequency 
words. However, with the production of large 
corpora such as the British National Corpus 
(BNC) containing one hundred million words 
(Aston & Burnard, 1998), frequency 
comparisons are available across millions of 
words of text. There are two main types of 
corpus comparison: 
? comparison of a sample corpus to a large(r) 
corpus 
? comparison of two (roughly-) equal sized 
corpora 
In the first type, we refer to the large(r) corpus 
as a horrnative' corpus since it provides a text 
norm (or standard) against which we can 
compare. These two main types of comparison 
can be extended to the comparison of more than 
two corpora. For example, we may compare one 
normative corpus to several smaller corpora at 
the same time, or compare three or more equal 
sized corpora to each other. In general, however, 
this makes the results more difficult o interpret. 
There are also a number of issues which need 
to be considered when comparing two (or more) 
corpora: 
? representativeness 
? homogeneity within the corpora 
? comparability ofthe corpora 
? reliability of statsfical tests (for different sized 
corpora nd other factors) 
Representativeness (Biber, 1993) is a 
particularly important attribute for a normative 
corpus when comparing a sample corpus to a 
large normative corpus (such as the BNC) which 
contains ections from many different text types 
and domains. To be representative a corpus 
should contain samples of  all major text types 
(Leech, 1993) and if possible in some way 
proportional to their usage in ~very day 
language' (Clear, 1992). This first type of 
comparison is intended to discow~r features in 
the sample corpus with significantly different 
usage (i.e. frequency) to that found in ~eneral' 
language. 
The second type of comparison is one that 
views corpora as equals (as in the Brown and 
LOB comparison). It aims to discover features in 
the corpora that distinguish one tiom another. 
Homogeneity within each of the corpora is 
important here since we may find that the results 
reflect sections within one of the corpora which 
are unlike other sections in either of the corpora 
under consideration (Kilgarriff 1997). 
Comparability is of interest too, since the 
corpora should have been sampled for in the 
same way. In other words, the corpora should 
have been built using the same stratified 
sampling method and with, if  possible, 
randornised methods of sample selection. This is 
the case with Brown and LOB, since LOB was 
designed to be comparable tothe Brown corpus. 
The final issue, which has been addressed 
elsewhere, is the one regarding the reliability of 
the statistical tests in relation to the size of the 
corpora under consideration. Kilgarriff (1996) 
points out that in the Brown versus LOB 
comparison many eomrnon words are marked as 
having significant chi-squared values, and that 
because words are not selected at random in 
language we will always see a large number of 
differences in two such text collections. He 
selects the Mann-Whitney test that: uses ranks of 
frequency data rather than the frequency values 
themselves tocompute the statistic. However, he 
observes that even with the new test 60% of 
words are marked as significant. Ignoring the 
actual frequency of occurrence as in the Mann- 
Whitney test discards most of the evidence we 
have about he distribution of  words. The test is 
often used when comparing ordinal rating scales 
(Oakes 1998: 17). 
Dunning (1993) reports that we should not rely 
on the assumption of a normal distribution when 
performing statistical text analysis and suggests 
that parametric analysis based on the binomial or 
multinomial distributions i  a better alternative 
for smaller texts. The chi-squared value becomes 
unreliable when the expected frequency is less 
than 5 and possibly overestimates with high 
frequency words and when comparing a 
relatively small corpus to a much larger one. He 
proposes the log-likelihood ratio as an 
alternative to Pearson~ chi-squared test. For this 
reason, we chose to use the log-likelihood ratio 
in our work as described in the next section. In 
fact, Cressie and Read (1984) show that 
Pearson~ X 2 (chi-squared) and the likelihood 
ratio G 2 (Dunning~ log-likelihood) are two 
statistics in a continuum defined by the power- 
divergence family of statistics. They go on to 
describe this family in later work (1988, 1989) 
where they also make reference to the long and 
continuing discussion of the normal and chi- 
squared approximations for X 2 and G 2. 
We have applied the goodness-of-fit test for 
comparison of linguistically annotated corpora. 
The frequency distributions of part-of-speech 
and semantic tags are sharply different to words. 
In these comparisons, we are unlikely to observe 
rare events such as tags occurring once. 
However, much higher frequencies will occur 
and so the log-likelihood test is less likely to 
overestimate significance in these cases. 
2 Methodology 
The method is fairly simple and straightforward 
to apply. Given two corpora we wish to 
compare, we produce a frequency list for each 
corpus. Normally, this would be a word 
frequency list, but as described above and as 
with examples in the following application 
section, it can be a part-of-speech (POS) or 
semantic tag frequency list. However, let us 
assume for now that we are performing a 
comparison at the word levee For each word in 
the two frequency lists we calculate the log- 
likelihood (henceforth LL) statistic. This is 
performed by constructing a contingency table 
as in Table 1. 
i The application of this technique to POS or 
semantic tag frequency lists is achieved by 
constructing the contingency table with tag rather 
than word frequencies. 
Table 1 Contigency table for word frequencies 
CORPUS CORPUS TOTAL 
ONE TWO 
Freq a b a+b 
of word 
Freq c-a d-b c+d-a-b 
of other 
words 
TOTAL c d c+d 
Note that the value ~' corresponds to the 
number of words in corpus one, and ~' 
corresponds to the number of words in corpus 
two (1'4 values). The values ~' and b'are called 
the observed values (O). We need to calculate 
the expected values (E) according to the 
following formula: 
E i = 
i 
i 
In our case N1 = c, and N2 = d. So, for this 
word, E1 = c*(a+b) / (c+d) and E2 = d*(a+b) /
(c+d). The calculation for the expected values 
takes account of the size of the two corpora, so 
we do not need to normalise the figures before 
applying the formula. We can then calculate the 
log-likehood value according to this formula: 
-21n A = 2~ Oi In~-~ 
This equates to calculating LL as follows: 
LL = 2*((a*log (a/E1)) + (b*log (b/E2))) 
The word frequency list is then sorted by the 
resulting LL values. This gives the effect of 
placing the largest LL value at the top of the list 
representing the word which has the most 
significant relative frequency difference between 
the two corpora. In this way, we can see the 
words most indicative (or characteristic) of one 
corpus, as compared to the other corpus, at the 
top of the list. The words which appear with 
roughly similar relative frequencies in the two 
corpora ppear lower down the list. Note that we 
do not use the hypothesis-test by comparing the 
LL values to a chi-squared distribution table. As 
Kilgarriff & Rose (1998) note, even Pearson~ 
X 2 is suitable without the hypothesis-testing 
link: Given the non-random nature of words in 
a text, we are always likely to find frequencies 
of words which differ across any two texts, and 
the higher the frequencies, the more information 
the statistical test has to work with. Hence, it is 
at this point that the researcher must intervene 
and qualitatively examine examples of the 
significant words highlighted by this technique. 
We are not proposing a completely automated 
approach. 
3 Applications 
This method has already been applied to study 
social differentiation in the use of English 
vocabulary and profiling of learner English. In 
Rayson et al(1997), selective quantitative 
analyses of the demographically sampled spoken 
English component of the BNC were carried out. 
This is a subcorpus of circa 4.5 million words, in 
which speakers and respondents are identified 
by such factors as gender, age, social group and 
geographical region. Using the method, a 
comparison was performed of the vocabulary of 
speakers, highlighting those differences which 
are marked by a very high value of significant 
difference between different sectors of the 
corpus according to gender, age and social 
group. 
In Granger and Rayson (1998), two similar- 
sized corpora of native and non-native writing 
were compared at the lexical level. The corpora 
were analysed by a part-of-speech tagger, and 
this permitted a comparison at the major word- 
class level. The patterns of significant overuse 
and underuse for POS categories demonstrated 
that the learner data displayed many of the 
stylistic features of spoken rather than written 
English. 
The same technique has more recently been 
applied to compare corpora analysed at the 
semantic level in a systems engineering domain 
and this is the main focus of this section. The 
motivation for this work is that despite natural 
language's well-documented shortcomings as a 
medium for precise technical description, its use 
in software-intensive systems engineering 
remains inescapable. This poses many problems 
for engineers who must derive problem 
understanding and synthesise precise solution 
descriptions from free text. This is true both for 
the largely unstructured textual descriptions 
from which system requirements are derived, 
and for more formal documents, such as 
standards, which impose requirements on system 
development processes. We describe an 
experiment that has been carried out in the 
REVERE project (Rayson et al 2000) to 
investigate the use of probabilistic natural 
language processing techniques to provide 
systems engineering support. 
The target documents are field reports of a 
series of ethnographic studies at an air traffic 
conlxol (ATC) centre. This formed part of a 
study of ATC as an example of a system that 
supports collaborative user tasks (Bentley et al 
1992). The documents consist of both the 
verbatim transcripts of the ethnographerb 
observations and interviews with controllers, 
and of reports compiled by the ethnographer for 
later analysis by a multi-disciplinary team of 
social scientists and systems engineers. The field 
reports form an interesting study because they 
exhibit many characteristics typical of 
documents een by a systems engineer. The 
volume of the information is fairly high (103 
pages) and the documents are not structured in a 
way designed to help the extraction of 
requirements ( ay around business processes or 
system architecture). 
The text is analysed by a part-of-speech tagger, 
CLAWS (Garside and Smith, 1997), and a 
semantic analyser (Rayson and Wilson, 1996) 
which assigns semantic tags that represent the 
semantic field (word-sense) of words from a 
lexicon of single words and an idiom list of 
multi-word combinations (e.g. ~ a rule). These 
resources contain approximately 52,000 words 
and idioms. 
The normative corpus that we used was a 2.3 
million-word subset of the BNC derived from 
the transcripts of spoken English. Using this 
.corpus, the most over-represented sernanfie 
categories in the ATC field reports are shown in 
Table 2. The log-likelihood test is applied as 
described in the previous ection and represents 
the semantic tag's frequency deviation from the 
normative corpus. The higher the figure, the 
greater the deviation. 
Table 2. Over-represented categories in ATC 
field reports 
Log- Tag Word sense (examples 
likelihood from the text) 
3366 $7.1 power, organising 
(bontroller; ~hief) 
2578 M5 flying (lalane; Hight; 
t~irport) 
988 02 general objects (~trip; 
holder; tack) 
643 03 electrical equipment 
(radar; blip) 
535 Y1 science and technology 
('PH) 
449 W3 geographical terms 
(Pole Hill; Dish Sea) 
432 Q1.2 paper documents and 
writing (~vriting; 
~,vritten; hotes) 
372 N3.7 measurement (length; 
height; l:listance; 
levels; '1000ft) 
318 L1 life and living things 
(live) 
310 A 10 indicating actions 
(l~ointing', indicating; 
tlisplay) 
306 X4.2 mental objects 
(~ysterns; tlpproach; 
haode; tactical; 
larocedure) 
290 A4.1 kinds, groups (Sector; 
Sectors) 
With the exception of Y I (an anomaly caused 
by an interviewees initials being mistaken for 
the PH unit of acidity), all of these semantic 
categories include important objects, roles, 
functions, etc. in the ATC domain. The 
frequency with which some of these occur, such 
as M5 (flying), are uusurprising. Others are 
more revealing about the domain of ATC. 
Figure 1 shows some of the occurrences of the 
semantic category 02 (general objects). The 
important information extracted here is the 
importance of Mrips' (formally, 1light strips). 
These are small pieces of cardboard with printed 
flight details that are the most fundamental 
artefact used by the air traffic controllers to 
manage their air space. Examination of other 
words in this category also shows that flight 
4 
!i tO mqt"ll~ " 1250L' i  n red m a , t r ip  
'he :T..sle o f  I lm . . .  &:lU0t; Tht ,  ~t r lp  
~te? l  I~, 'the ~ pr in ted  tn  box 
~on prtn, t~ l  tn hot ' 6 ' o f  the strip 
=rr ' tw l  t i l e  over th=tbeo~n ( box 
iviousllJ only aA~'ozla,~te- :some .s'lwips 
~l  t tne  neor the call$tfln on a ~trtp  
much I~msier . lhermere  1.6 ~t r i~  
! re tor t  1.6 s t r ips  in one oF h i ,  melts 
i*Y , .thot ta lk ing  aml us ing on input 
~hat t~llctr~l md u~inl; an t r lmt  device 
: /Arawtt: the nicl~ l~hina ~,~:  ~"?rins 
, d '~ i l t t  o t  = time t r l s tm 
~ tomrds  ' ~l'le b0r t~ Of ~e~ :
? II " o f  the s t r ip  ( ~ te l  
( ~ le f t  } S t r lpssee~d br  
' A " ) ' th is  ~ (:l~'tcusl~ on ly  
~mett out of, pos i t ion  , and 2\[ got  
t;o tndtcote =nur.~,~L ~meed . < 
? oF ht= ~lm . .dBb. A ; 
dev ice  s ight  =t~o be ? but that  
retort  a im be , but  ~ the pr  
i=t the i r  F lex ib i l i ty  . ,~uot :  o 
Figure 1. Browsing the semantic ategory 02 
strips are held in tacks' to organise them 
according to (for example) aircraft time-of- 
arrival. 
Similarly, browsing the context for Q1.2 
(paper documents and writing) would allow us 
to discover that controllers annotate flight strips 
'to record deviations from flight plans, and L1 
(life, living things) would reveal that some strips 
are live; that is, they refer to aircraft currently 
traversing the contxoller's sector. Notice also that 
the semantic categories' deviation from the 
normative corpus can also be expected to reveal 
domain roles (actors). In this example, the 
frequency of $7.1 (power, organising) shows the 
importance of the roles of ~ontrollers' and 
~hiefs'. 
Using the frequency profiling method does not 
automate the task of identifying abstractions, 
much less does it produce fully formed 
requirements that can be pasted into a 
specification document. Instead, it helps the 
engineer quickly isolate potentially significant 
domain abstractions that require closer analysis. 
4 Conclusions 
reliability of the statistical tests (LL, Pearson~ 
X 2 and others) under the effects of corpus size, 
ratio of the corpora being compared and word 
(or tag) frequency. 
We do not propose a completely automated 
approach. The tools suggest a group of key items 
by decreasing order of significance which 
distinguish one corpus from another. It is then 
that the researcher should investigate 
occurrences of the significant items in the 
corpora using standard corpus techniques uch 
as KWIC (key-word in context). The reasons 
behind their significance can be discovered and 
explanations ought for the patterns displayed. 
By this process, we can compare the corpora 
under investigation and make hypotheses about 
the language use that they represent. 
Acknowledgements 
Our thanks go to Geoffrey Leech and the 
anonymous reviewers who commented on 
earlier versions of this paper. The REVERE 
project is supported under the EPSRC Systems 
Engineering for Business Process Change 
(SEBPC) programme, project number 
GR/MO4846. 
This paper has described a method of comparing 
corpora which uses frequency profiling. The 
method has been shown to discover key items in 
the corpora which differentiate one corpus from 
another. It has been applied at the word level, 
part-of-speech tag level, and semantic tag level. 
It can be used as a quick way in to find the 
differences between the corpora and is shown to 
have applications in the study of social 
differentiation i  the use of English vocabulary: 
profiling of learner English and document 
analysis in the software ngineering process. 
Future directions in which we aim to research 
include a more precise specification of the 
References 
Aston, G. and Burnard, L. (1998). The BNC 
Handbook: Exploring the British National Corpus 
with SARA, Edinburgh University Press. 
Bentley IL, Rodden T., Sawyer P., Sommerville I, 
Hughes J., Randall D., Shapiro D. (1992). 
Ethnographically-informed systems design for air 
traffic control, In Proceedings of Computer- 
Supported Cooperative Work (CSCW) '92, 
Toronto, November 1992. 
Biber, D. (1993). Representativeness in Corpus 
Design. Literary and Linguistic Computing, 8, 
Issue 4, Oxford University Press, pp. 243-257. 
Clear, J. (1992). Corpus sampling. In G. Leitner (ed.) 
New directions in English lang~aage corpora~ 
Mouton-de-Gruyter, Berlin, pp. 21 - 31. 
Cressie, N. and Read, T. R. C. (1984) Multinomial 
Goodness-of-Fit Tests. Journal of the Royal 
Statistical Society. Series B (Methodological), Vol. 
46, No. 3, pp. 440 - 464. 
Cressie, N. and Read, T. R. C. (1989). Pearson~ X 2 
and the Loglikelihood Ratio Statistic G2: A 
comparative review. International Statistical 
Review, 57, 1, Belfast University Press, N.I., pp. 
19--43. 
Dunning, T. (1993). Accurate Methods for the 
Statistics of Surprise and Coincidence. 
Computational Linguistics, 19, 1, March 1993, pp. 
61-74. 
Garside, R. and Smith, N. (199"7). A Hybrid 
Grammatical Tagger: CLAtVS4, in Garside, R., 
Leech, G., and McEnery, A. (eds.) Corpus 
Annotation: Linguistic Information from Computer 
Text Corpora, Longman, London. 
Granger, S. and Rayson, P. (1998). Automatic 
profiling of learner texts. In S. Granger (ed.) 
Learner English on Computer. Longman, London 
and New York, pp. 119-131. 
Hofland, K. and Johansson, S. (1982). Word 
frequencies in British and American English. The 
Norwegian Computing Centre for the Humanities, 
Bergen, Norway. 
Kilgarriff, A. (1996) Why chi-square doesn't work; 
and an improved LOB-Brown comparison. ALLC- 
ACH Conference, June 1996, Bergen, Norway. 
Kilgarriff, A. (1997). Using word frequency lists to 
measure corpus homogeneity and similarity 
between corpora. Proceedings 5th ACL workshop 
on very large corpora. Beijing and Hong Kong. 
Kilgarriff, A. and Rose, T. (1998). Measures for 
corpus similarity and homogeneity. In proceedings 
of the 3 m conference on Empirical Methods in 
Natural Language Processing, Granada, Spain, pp. 
46 - 52. 
Leech, G. (1993). 100 million words of English: a 
description of the background, nature and 
prospects of the British National Corpus project. 
English Today 33, Vol. 9, No. 1, Cambridge 
University Press. 
Oakes, M. P. (1998). Statistics for Corpus 
Linguistics. Edinburgh University Press, 
Edinburgh. 
Rayson, P., and Wilson, A. (1996). The ACAMRIT 
semantic tagging system: progress report, In L. J. 
Evett, and T. G. Rose (eds.) Language Engineering 
for Document Analysis and Recognition, LEDAR, 
AISB96 Workshop proceedings, pp 13-20. 
Brighton, England. 
Rayson, P., Leech, G., and Hodges, M. (1997). Social 
differentiation in the use of English vocabulary: 
some analyses of the conversational component of 
the British National Corpus. International Journal 
of Corpus Linguistics. 2 (1). pp. 133 - 152. John 
Benjamins, Amsterdam/Philadelphia. 
Rayson, P., Garside, R., and Sawyer, P. (2000). 
Assisting requirements engineering with semantic 
document analysis. In Proceedings of RIAO 2000 
(Recherche d'Inforrnafions Assisfie par Ordinateur, 
Computer-Assisted Information Retrieval) 
International Conference, Coll~ge de France, Paris, 
France, April 12-14, 2000. C.I.D., Paris, pp. 1363 - 
1371. 
Read, T. R. C. and Cressie, N. A. C. (1988). 
Goodness-of-fit s atistics for discrete multivariate 
data. Springer series in statistics. Springer-Vedag, 
New York. 
Yule, G. (1944). The Statistical Study of Literary 
Vocabulary. Cambridge University Press. 
Extracting Multiword Expressions with A Semantic Tagger 
Scott S. L. Piao 
Dept. of Linguistics and MEL 
Lancaster University 
 s.piao@lancaster.ac.uk 
Paul Rayson 
Computing Department 
Lancaster University 
 paul@comp.lancs.ac.uk 
Dawn Archer 
Dept. of Linguistics and MEL 
Lancaster University 
d.archer@lancaster.ac.uk
Andrew Wilson 
Dept. of Linguistics and MEL 
Lancaster University 
 eiaaw@exchange.lancs.ac.uk 
Tony McEnery 
Dept. of Linguistics and MEL 
Lancaster University 
 amcenery@lancaster.ac.uk 
Abstract 
Automatic extraction of multiword 
expressions (MWE) presents a tough 
challenge for the NLP community 
and corpus linguistics. Although 
various statistically driven or knowl-
edge-based approaches have been 
proposed and tested, efficient MWE 
extraction still remains an unsolved 
issue. In this paper, we present our 
research work in which we tested 
approaching the MWE issue using a 
semantic field annotator. We use an 
English semantic tagger (USAS) de-
veloped at Lancaster University to 
identify multiword units which de-
pict single semantic concepts. The 
Meter Corpus (Gaizauskas et al, 
2001; Clough et al, 2002) built in 
Sheffield was used to evaluate our 
approach. In our evaluation, this ap-
proach extracted a total of 4,195 
MWE candidates, of which, after 
manual checking, 3,792 were ac-
cepted as valid MWEs, producing a 
precision of 90.39% and an esti-
mated recall of 39.38%. Of the ac-
cepted MWEs, 68.22% or 2,587 are 
low frequency terms, occurring only 
once or twice in the corpus. These 
results show that our approach pro-
vides a practical solution to MWE 
extraction. 
1 Introduction 
2 
Automatic extraction of Multiword ex-
pressions (MWE) is an important issue in the 
NLP community and corpus linguistics. An 
efficient tool for MWE extraction can be use-
ful to numerous areas, including terminology 
extraction, machine translation, bilin-
gual/multilingual MWE alignment, automatic 
interpretation and generation of language. A 
number of approaches have been suggested 
and tested to address this problem. However, 
efficient extraction of MWEs still remains an 
unsolved issue, to the extent that Sag et al 
(2001b) call it ?a pain in the neck of NLP?. 
In this paper, we present our work in 
which we approach the issue of MWE extrac-
tion by using a semantic field annotator. Spe-
cifically, we use the UCREL Semantic 
Analysis System (henceforth USAS), devel-
oped at Lancaster University to identify mul-
tiword units that depict single semantic 
concepts, i.e. multiword expressions. We have 
drawn from the Meter Corpus (Gaizauskas et 
al., 2001; Clough et al, 2002) a collection of 
British newspaper reports on court stories to 
evaluate our approach. Our experiment shows 
that it is efficient in identifying MWEs, in 
particular MWEs of low frequencies. In the 
following sections, we describe this approach 
to MWE extraction and its evaluation. 
Related Works 
Generally speaking, approaches to MWE 
extraction proposed so far can be divided into 
three categories: a) statistical approaches 
based on frequency and co-occurrence affin-
ity, b) knowledge?based or symbolic ap-
proaches using parsers, lexicons and language 
filters, and c) hybrid approaches combining 
different methods (Smadja 1993; Dagan and 
Church 1994; Daille 1995; McEnery et al 
1997; Wu 1997; Wermter et al 1997; Mi-
chiels and Dufour 1998; Merkel and Anders-
son 2000; Piao and McEnery 2001; Sag et al 
2001a, 2001b; Biber et al 2003). 
In practice, most statistical approaches use 
linguistic filters to collect candidate MWEs. 
Such approaches include Dagan and Church?s 
(1994) Termight Tool. In this tool, they first 
collect candidate nominal terms with a POS 
syntactic pattern filter, then use concordances 
to identify frequently co-occurring multiword 
units. In his Xtract system, Smadja (1993) 
first extracted significant pairs of words that 
consistently co-occur within a single syntactic 
structure using statistical scores called dis-
tance, strength and spread, and then exam-
ined concordances of the bi-grams to find 
longer frequent multiword units. Similarly, 
Merkel and Andersson (2000) compared fre-
quency-based and entropy based algorithms, 
each of which was combined with a language 
filter. They reported that the entropy-based 
algorithm produced better results. 
One of the main problems facing statistical 
approaches, however, is that they are unable 
to deal with low-frequency MWEs. In fact, 
the majority of the words in most corpora 
have low frequencies, occurring only once or 
twice. This means that a major part of true 
multiword expressions are left out by statisti-
cal approaches. Lexical resources and parsers 
are used to obtain better coverage of the lexi-
con in MWE extraction. For example, Wu 
(1997) used an English-Chinese bilingual 
parser based on stochastic transduction 
grammars to identify terms, including multi-
word expressions. In their DEFI Project, Mi-
chiels and Dufour (1998) used dictionaries to 
identify English and French multiword ex-
pressions and their translations in the other 
language. Wehrli (1998) employed a genera-
tive grammar framework to identify com-
pounds and idioms in their ITS-2 MT 
English-French system. Sag et al (2001b) 
introduced Head-driven Phrase Structure 
Grammar for analyzing MWEs. Like pure 
statistical approaches, purely knowledge-
based symbolic approaches also face prob-
lems. They are language dependent and not 
flexible enough to cope with complex struc-
tures of MWEs. As Sag et al (2001b) sug-
gest, it is important to find the right balance 
between symbolic and statistical approaches. 
In this paper, we propose a new approach 
to MWEs extraction using semantic field in-
formation. In this approach, multiword units 
depicting single semantic concepts are recog-
nized using the Lancaster USAS semantic 
tagger. We describe that system and the algo-
rithms used for identifying single and multi-
word units in the following section. 
3 
                                                          
Lancaster Semantic tagger 
The USAS system has been in develop-
ment at Lancaster University since 1990 1 . 
Based on POS annotation provided by the 
CLAWS tagger (Garside and Smith, 1997), 
USAS assigns a set of semantic tags to each 
item in running text and then attempts to dis-
ambiguate the tags in order to choose the 
most likely candidate in each context. Items 
can be single words or multiword expressions. 
The semantic tags indicate semantic fields 
which group together word senses that are 
related by virtue of their being connected at 
some level of generality with the same mental 
concept. The groups include not only syno-
nyms and antonyms but also hypernyms and 
hyponyms. 
The initial tagset was loosely based on 
Tom McArthur's Longman Lexicon of Con-
temporary English (McArthur, 1981) as this 
appeared to offer the most appropriate thesau-
rus type classification of word senses for this 
kind of analysis. The tagset has since been 
considerably revised in the light of practical 
tagging problems met in the course of the re-
search. The revised tagset is arranged in a 
hierarchy with 21 major discourse fields ex-
panding into 232 category labels. The follow-
ing list shows the 21 labels at the top level of 
the hierarchy (for the full tagset, see website: 
http://www.comp.lancs.ac.uk/ucrel/usas). 
 
1 This work is continuing to be supported by the Bene-
dict project, EU project IST-2001-34237. 
A general and abstract terms 
B the body and the individual 
C arts and crafts 
E emotion 
F food and farming 
G government and the public domain 
H architecture, buildings, houses and the 
home 
I money and commerce in industry 
K entertainment, sports and games 
L life and living things 
M movement, location, travel and trans-
port 
N numbers and measurement 
O substances, materials, objects and 
equipment 
P education 
Q linguistic actions, states and processes 
S social actions, states and processes 
T time 
W the world and our environment 
X psychological actions, states and 
processes 
Y science and technology 
Z names and grammatical words 
 
Currently, the lexicon contains just over 
37,000 words and the template list contains 
over 16,000 multiword units. These resources 
were created manually by extending and ex-
panding dictionaries from the CLAWS tagger 
with observations from large text corpora. 
Generally, only the base form of nouns and 
verbs are stored in the lexicon and a lemmati-
sation procedure is used for look-up. How-
ever, the base form is not sufficient in some 
cases. Stubbs (1996: 40) observes that ?mean-
ing is not constant across the inflected forms 
of a lemma?, and Tognini-Bonelli (2001: 92) 
notes that lemma variants have different 
senses. 
In the USAS lexicon, each entry consists 
of a word with one POS tag and one or more 
semantic tags assigned to it. At present, in 
cases where a word has more than one syntac-
tic tag, it is duplicated (i.e. each syntactic tag 
is given a separate entry).  
The semantic tags for each entry in the 
lexicon are arranged in approximate rank fre-
quency order to assist in manual post editing, 
and to allow for gross automatic selection of 
the common tag, subject to weighting by do-
main of discourse. 
In the multi-word-unit list, each template 
consists of a pattern of words and part-of-
speech tags. The semantic tags for each tem-
plate are arranged in rank frequency order in 
the same way as the lexicon. Various types of 
multiword expressions are included: phrasal 
verbs (e.g. stubbed out), noun phrases (e.g. ski 
boots), proper names (e.g. United States), true 
idioms (e.g. life of Riley).  
Figure 1 below shows samples of the actual 
templates used to identify these MWUs. Each 
of these example templates has only one se-
mantic tag associated with it, listed on the 
right-hand end of the template. However, the 
second example (ski boot) combines the 
clothing (B5) and sports (K5.1) fields into one 
tag. The pattern on the left of each template 
consists of a sequence of words joined to POS 
tags with the underscore character. The words 
and POS fields can include the asterisk wild-
card character to allow for inflectional vari-
ants and to write more powerful templates 
with wider coverage. USAS templates can 
match discontinuous MWUs, and this is illus-
trated by the first example, which includes 
optional intervening POS items marked 
within curly brackets. Thus this template can 
match stubbed out and stubbed the cigarette 
out. ?Np? is used to match simple noun 
phrases identified with a noun-phrase chun-
ker. 
 
stub*_* {Np/P*/R*} out_RP    O4.6- 
ski_NN1 boot*_NN*          B5/K5.1 
United_* States_N*              Z2 
life_NN1 of_IO Riley_NP1        K1 
 
Figure 1 Sample of USAS multiword templates 
 
As in the case of grammatical tagging, the 
task of semantic tagging subdivides broadly 
into two phases: Phase I (Tag assignment): 
attaching a set of potential semantic tags to 
each lexical unit and Phase II (Tag disam-
biguation): selecting the contextually appro-
priate semantic tag from the set provided by 
Phase I. USAS makes use of seven major 
techniques or sources of information in phase 
II. We will list these only briefly here, since 
they are described in more detail elsewhere 
(Garside and Rayson, 1997). 
  
1. POS tag. Some senses can be elimi-
nated by prior POS tagging. The CLAWS 
part-of-speech tagger is run prior to semantic 
tagging. 
2. General likelihood ranking for single-
word and MWU tags. In the lexicon and 
MWU list senses are ranked in terms of fre-
quency, even though at present such ranking 
is derived from limited or unverified sources 
such as frequency-based dictionaries, past 
tagging experience and intuition.  
3. Overlapping MWU resolution. Nor-
mally, semantic multi-word units take priority 
over single word tagging, but in some cases a 
set of templates will produce overlapping 
candidate taggings for the same set of words. 
A set of heuristics is applied to enable the 
most likely template to be treated as the pre-
ferred one for tag assignment.  
4. Domain of discourse. Knowledge of 
the current domain or topic of discourse is 
used to alter rank ordering of semantic tags in 
the lexicon and template list for a particular 
domain.  
5. Text-based disambiguation. It has 
been claimed (by Gale et al 1992) on the ba-
sis of corpus analysis that to a very large ex-
tent a word keeps the same meaning 
throughout a text. 
6. Contextual rules. The template 
mechanism is also used in identifying regular 
contexts in which a word is constrained to 
occur in a particular sense.  
7. Local probabilistic disambiguation. It 
is generally supposed that the correct seman-
tic tag for a given word is substantially de-
termined by the local surrounding context.  
 
After automatic tag assignment has been 
carried out, manual post-editing can take 
place, if desired, to ensure that each word and 
idiom carries the correct semantic classifica-
tion. 
From these seven disambiguation meth-
ods, our main interest in this paper is the third 
technique of overlapping MWU resolution. 
When more than one template match overlaps 
in a sentence, the following heuristics are ap-
plied in sequence: 
 
1. Prefer longer templates over shorter 
templates 
2. For templates of the same length, pre-
fer shorter span matches over longer 
span matches (a longer span indicates 
more intervening items for discon-
tinuous templates) 
3. If the templates do not apply to the 
same sequence of words, prefer the 
one that begins earlier in the sentence 
4. For templates matching the same se-
quence of words, prefer the one 
which contains the more fully defined 
template pattern (with fewer wild-
cards in the word fields) 
5. Prefer templates with a more fully de-
fined first word in the template 
6. Prefer templates with fewer wildcards 
in the POS tags 
 
These six rules were found to differentiate 
in all cases of overlapping MWU templates. 
Cases which failed to be differentiated indi-
cated that two (or more) templates in our 
MWU list were in fact identical, apart from 
the semantic tags and required merging to-
gether. 
4 Experiment of MWE extraction 
In order to test our approach of extracting 
MWEs using semantic information, we first 
tagged the newspaper part of the METER 
Corpus with the USAS tagger. We then col-
lected the multiword units assigned as a single 
semantic unit. Finally, we manually checked 
the results. 
The Meter Corpus chosen as the test data 
is a collection of court reports from the Brit-
ish Press Association (PA) and some leading 
British newspapers (Gaizauskas 2001; Clough 
et al, 2002). In our experiment, we used the 
newspaper part of the corpus containing 774 
articles with more than 250,000 words. It pro-
vides a homogeneous corpus (in the sense that 
the reports come from a restricted domain of 
court events) and is thus a good source from 
which to extract domain-specific MWEs. 
Another reason for choosing this corpus is 
that it has not been used in training the USAS 
system. As an open test, we assume the re-
sults of the experiment should reflect true ca-
pability of our approach for real-life 
applications. 
The current USAS tagger may assign mul-
tiple possible semantic tags for a term when it 
fails to disambiguate between them. As men-
tioned previously, the first one denotes the 
most likely semantic field of the term. There-
fore, in our experiment we chose the first tag 
when such situations arose. 
A major problem we faced in our experi-
ment is the definition of a MWE. Although it 
has been several years since people started to 
work on MWE extraction, we found that there 
is, as yet, no available ?clear-cut? definition 
for MWEs. We noticed various possible defi-
nitions have been suggested for MWE/MWU. 
For example, Smadja (1993) suggests a basic 
characteristic of collocations and multiword 
units is recurrent, domain-dependent and co-
hesive lexical clusters. Sag et el. (2001b) sug-
gest that MWEs can roughly be defined as 
?idiosyncratic interpretations that cross word 
boundaries (or spaces)?. Biber et al (2003) 
describe MWEs as lexical bundles, which 
they go on to define as combinations of words 
that can be repeated frequently and tend to be 
used frequently by many different speak-
ers/writers within a register. 
Although it is not difficult to interpret 
these deifications in theory, things became 
much more complicated when we undertook 
our practical checking of the MWE candi-
dates. Quite often, we experienced disagree-
ment between us about whether or not to 
accept a MWE candidate as a good one. In 
practice, we generally followed Biber et al?s 
definition, i.e. accept a candidate MWE as a 
good one if it can repeatedly co-occur in the 
corpus. 
Another difficulty we experienced relates 
to estimating recall. Because the MWEs in the 
METER Corpus are not marked-up, we could 
not automatically calculate the number of 
MWEs contained in the corpus. Conse-
quently, we had to manually estimate this fig-
ure. Obviously it is not practical to manually 
check though the whole corpus within the 
limited time allowed. Therefore, we had to 
estimate the recall on a sample of the corpus, 
as will be described in the following section. 
5 Evaluation 
In this section, we analyze the results of 
the MWE extraction in detail for a full 
evaluation of our approach to MWE extrac-
tion. 
Overall, after we processed the test corpus, 
the USAS tagger extracted 4,195 MWE can-
didates from the test corpus. After manually 
checking through the candidates, we selected 
3,792 as good MWEs, resulting in overall 
precision of 90.39%. 
As we explained earlier, due to the diffi-
culty of obtaining the total number of true 
MWEs in the entire test corpus, we had to 
estimate recall of the MWE extraction on a 
sample corpus. In detail, we first randomly 
selected fifty texts containing 14,711 words 
from the test corpus, then manually marked-
up good MWEs in the sample texts, finally 
counted the number of the marked-up MWUs. 
As a result, 1,511 good MWEs were found in 
the sample. Since the number of automatically 
extracted good MWEs in the sample is 595, 
the recall on the sample is calculated as fol-
lows: 
Recall=(595?1511)?100%=39.38%. 
Considering the homogenous feature of 
the test data, we assume this local recall is 
roughly approximate to the global recall of 
the test corpus. 
To analyze the performance of USAS in 
respect to the different semantic field catego-
ries, we divided candidates according to the 
assigned semantic tag, and calculated the pre-
cision for each of them. Table 1 lists these 
precisions, sorting the semantic fields by the 
number of MWE candidates (refer to section 
3 for definitions of the twenty-one main se-
mantic field categories). As shown in this ta-
ble, the USAS semantic tagger obtained 
precisions between 91.23% to 100.00% for 
each semantic field except for the field of 
?names and grammatical words? denoted by 
Z. As Z was the biggest field (containing 
45.39% of the total MWEs and 43.12% of the 
accepted MWEs), we examined these MWEs 
more closely. We discovered that numerous 
pairs of words are tagged as person names 
(Z1) and geographical names (Z2) by mistake, 
e.g. Blackfriars crown (tagged as Z1), stabbed 
Constance (tagged as Z2) etc. 
 
Semantic 
field 
Total 
MWEs 
Accepted 
MWEs 
Precision 
Z 1,904  1,635 85.87% 
T 497  459 92.35% 
A 351  328 93.44% 
M 254  241 94.88% 
N 227  211 92.95% 
S 180  177 98.33% 
B 131  128 97.71% 
G 118  110 93.22% 
X 114  104 91.23% 
I 74  72 97.30% 
Q 67  63 94.03% 
E 58  53 91.38% 
H 53  52 98.11% 
K 48  45 93.75% 
P 39  37 94.87% 
O 32  29 90.63% 
F 24  24 100.00% 
L 11  11 100.00% 
Y 6  6 100.00% 
C 5  5 100.00% 
W 2  2 100.00% 
Total 4,195 3,792 90.39% 
 
Table 1: Precisions for different semantic catego-
ries 
 
Another possible factor that affects the 
performance of the USAS tagger is the length 
of the MWEs. To observe the performance of 
our approach from this perspective, we 
grouped the MWEs by their lengths, and then 
checked precision for each of the categories. 
Table 2 shows the results (once again, they 
are sorted in descending order by MWE 
lengths). As we might expect, the number of 
MWEs decreases as the length increases. In 
fact, bi-grams alone constitute 80.52% and 
81.88% of the candidate and accepted MWEs 
respectively. The precision also showed a 
generally increasing trend as the MWE length 
increases, but with a major divergence of tri-
grams. One main type of error occurred on tri-
grams is that those with the structure of 
CIW(capital-initial word) + conjunction + 
CIW tend to be tagged as Z2 (geographical 
name). The table shows relatively high preci-
sion for longer MWEs, reaching 100% for 6-
grams. Because the longest MWEs extracted 
have six words, no longer MWEs could be 
examined.  
 
MWE 
length 
Total 
MWEs 
Accepted 
MWEs 
Precision 
2 3,378 3,105 91.92% 
3 700 575 82.14% 
4 95 91 95.44% 
5 18 17 94.44% 
6 4 4 100.00% 
Total 4,195 3,792 90.39% 
 
Table 2: Precisions for MWEs of different lengths 
 
As discussed earlier, purely statistical al-
gorithms of MWE extraction generally filter 
out candidates of low frequencies. However, 
such low-frequency terms in fact form major 
part of MWEs in most corpora. In our study, 
we attempted to investigate the possibility of 
extracting low frequency MWEs by using 
semantic field annotation. We divided MWEs 
into different frequency groups, then checked 
precision for each of the categories. Table 3 
shows the results, which are sorted by the 
candidate MWE frequencies. As we expected, 
69.46% of the candidate MWEs and 68.22% 
of the accepted MWEs occur in the corpus 
only once or twice. This means that, with a 
frequency filter of Min(f)=3, a purely statisti-
cal algorithm would exclude more than half of 
the candidates from the process. 
 
Freq. of 
MWE 
Total 
number 
Accepted 
MWEs 
Precision 
1 2,164  1,892 87.43% 
2 750  695 92.67% 
3 - 4 616 570 92.53% 
5 - 7 357 345 96.64% 
8 - 20 253 238 94.07% 
21 - 117 55 52 94.55% 
Total 4,195 3,792 90.39% 
 
Table 3: Precisions for MWEs with different fre-
quencies 
 
Table 3 also displays an interesting rela-
tionship between the precisions and the fre-
quencies. Generally, we would expect better 
precisions for MWEs of higher frequencies, 
as higher co-occurrence frequencies are ex-
pected to reflect stronger affinity between the 
words within the MWEs. By and large, 
slightly higher precisions were obtained for 
the latter groups of higher frequencies (5?7, 
8-20 and 21-117) than those for the preceding 
lower frequency groups, i.e. 94.07%-96.64% 
versus 87.43%-92.67%. Nevertheless, for the 
latter three groups of the higher frequencies 
(5-7, 8-20 and 21?117) the precision did not 
increase as the frequency increases, as we 
initially expected. 
When we made a closer examination of 
the error MWEs in this frequency range, we 
found that some frequent domain-specific 
terms are misclassified by the USAS tagger. 
For example, since the texts in the test corpus 
are newspaper reports of court stories, many 
law courts (e.g. Manchester crown court, 
Norwich crown court) are frequently men-
tioned throughout the corpus, causing high 
frequencies of such terms (f=20 and f=31 re-
spectively). Unfortunately, the templates used 
in the USAS tagger did not capture them as 
complete terms. Rather, fragments were as-
signed a Z1 person name tag (e.g. Manchester 
crown). A solution to this type of problem is 
to improve the multiword unit templates used 
in the USAS tagger. Other possible solutions 
may include incorporating a statistical algo-
rithm to help detect boundaries of complete 
MWEs. 
When we examined the error distribution 
within the semantic fields more closely, we 
found that most errors occurred within the Z 
and T categories (refer to Table 1). The errors 
occurring in these semantic field categories 
and their sub-divisions make up 76.18% of 
the total errors (403). Table 4 shows the error 
distribution across 14 sub-divisions (for defi-
nitions of these subdivisions, see: website: 
http://www.comp.lancs.ac.uk/ucrel/usas). No-
tice that the majority of errors are from four 
semantic sub-categories: Z1, Z2, Z3 and T1.3. 
Notice, also, that the first two of these ac-
count for 60.55% of the total errors. This 
shows that the main cause of the errors in the 
USAS tool is the algorithm and lexical entries 
used for identifying names - personal and 
geographical and, to a lesser extent, the algo-
rithm and lexical entries for identifying peri-
ods of time. If these components of the USAS 
can be improved, a much higher precision can 
be expected. 
In sum, our evaluation shows that our se-
mantic approach to MWE extraction is effi-
cient in identifying MWEs, in particular those 
of lower frequencies. In addition, a reasona-
bly wide lexical coverage is obtained, as indi-
cated by the recall of 39.38%, which is 
important for terminology building. Our ap-
proach provides a practical way for extracting 
MWEs on a large scale, which we envisage 
can be useful for both linguistic research and 
practical NLP applications. 
 
Stag  Err. Stag  Err. 
Z1:person names 119 T1.1.1:time-past 1 
Z2:geog. names 125 T1.1.2:time-present 1 
Z3:other names 16 T1.2:time-momentary 8 
Z4:discourse bin 3 T1.3:time-period 23 
Z5:gram. bin 2 T2:time-begin/end 2 
Z8:pronouns etc. 2 T3:time-age 1 
Z99:unmatched 2 T4:time-early/late 2 
 
Table 4: Errors for some semantic sub-divisions 
6 Conclusion 
In this paper, we have shown that it is a 
practical way to extract MWEs using seman-
tic field information. Since MWEs are lexical 
units carrying single semantic concepts, it is 
reasonable to consider the issue of MWE ex-
traction as an issue of identifying word bun-
dles depicting single semantic units. The main 
difficulty facing such an approach is that very 
few reliable automatic tools available for 
identifying lexical semantic units. However, a 
semantic field annotator, USAS, has been 
built in Lancaster University. Although it was 
not built aiming to the MWE extraction, we 
thought it might be very well suited for this 
purpose. Our experiment shows that the 
USAS tagger is indeed an efficient tool for 
MWE extraction. 
Nevertheless, the current semantic tagger 
does not provide a complete solution to the 
problem. During our experiment, we found 
that not all of the multiword units it collects 
are valid MWEs. An efficient algorithm is 
needed for distinguishing between free word 
combinations and relatively fixed, closely 
affiliated word bundles. 
References 
Douglas Biber, Susan Conrad and Viviana Cortes. 
2003. Lexical bundles in speech and writing: an 
initial taxonomy. In A. Wilson, P. Rayson and 
T. McEnery (eds.) Corpus Linguistics by the 
Lune: a festschrift for Geoffrey Leech, pp. 71-
92.  Peter Lang, Frankfurt. 
Paul Clough, Robert Gaizauskas and S. L. Piao. 
2002. Building and annotating a corpus for the 
study of journalistic text reuse. In Proceedings 
of the 3rd International Conference on Lan-
guage Resources and Evaluation (LREC-2002), 
pp. 1678-1691. Los Palmas de Gran Canaria, 
Spain. 
Ido Dagan, and Ken Church. 1994. Termight: 
identifying and translating technical terminol-
ogy. In Proceedings of the 4th Conference on 
Applied Natural Language Processing, pp. 34-
40. Stuttgard, German. 
B?atrice Daille. 1995. Combined approach for 
terminology extraction: lexical statistics and 
linguistic filtering. Technical paper. UCREL, 
Lancaster University. 
Robert Gaizauskas, Jonathan Foster, Yorick 
Wilks, John Arundel, Paul Clough and Scott 
Piao. 2001. The METER corpus: a corpus for 
analysing journalistic text reuse. In the Pro-
ceedings of the Corpus Linguistics 2001, pp: 
214-223. Lancaster, UK. 
William Gale, Kenneth Church, and David 
Yarowsky. 1992. One sense per discourse. In 
Proceedings of the 4th DARPA Speech and 
Natural Language Workshop, pp 233-7. 
Roger Garside and Nick Smith. 1997. A hybrid 
grammatical tagger: CLAWS4. In R. Garside, 
G. Leech and A. McEnery (eds.), Corpus 
Annotation: Linguistic Information from 
Computer Text Corpora, pp. 102-121. Long-
man, London. Roger Garside and Paul Rayson. 1997. Higher-
level annotation tools. In. Roger Garside, Geof-
frey Leech, and Tony McEnery (eds.) Corpus 
Annotation: Linguistic Information from Com-
puter Text Corpora, pp. 179 - 193. Longman, 
London.  
Tom McArthur. 1981. Longman Lexicon of 
Contemporary English. Longman, London. 
Tony McEnery, Lang? Jean-Marc, Oakes Michael 
and V?ronis Jean. 1997. The exploitation of 
multilingual annotated corpora for term extrac-
tion. In Garside Roger, Leech Geoffrey and 
McEnery Anthony (eds), Corpus annotation --- 
linguistic information from computer text cor-
pora, pp 220-230. London & New York, Long-
man. 
Magnus Merkel and Mikael Andersson. 2000. 
Knowledge-lite extraction of multi-word units 
with language filters and entropy thresholds. In 
Proceedings of 2000 Conference User-Oriented 
Content-Based Text and Image Handling 
(RIAO'00), pages 737--746, Paris, France. 
Archibald Michiels and Nicolas Dufour.1998. 
DEFI, a tool for automatic multi-word unit rec-
ognition, meaning assignment and translation 
selection. In Proceedings of the First Interna-
tional Conference on Language Resources & 
Evaluation, pp. 1179-1186. Granada, Spain. 
Scott Songlin Piao and Tony McEnery. 2001. 
Multi-word unit alignment in English-Chinese 
parallel corpora. In the Proceedings of the Cor-
pus Linguistics 2001, pp. 466-475. Lancaster, 
UK. 
Ivan A. Sag, Francis Bond, Ann Copestake and 
Dan Flickinger. 2001a. Multiword Expressions. 
LinGO Working Paper No. 2001-01. Stanford 
University, CA. 
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann 
Copestake and Dan Flickinger. 2001b. Multi-
word Expressions: A Pain in the Neck for NLP. 
LinGO Working Paper No. 2001-03. Stanford 
University, CA. 
Frank Smadja. 1993. Retrieving collocations from 
text: Xtract. Computational Linguistics 
19(1):143-177. 
Michael Stubbs. 1996. Text and corpus analysis: 
computer-assisted studies of language and cul-
ture. Blackwell, Oxford. 
Elena Tognini-Bonelli. 2001. Corpus linguistics at 
work. Benjamins, The Netherlands. 
Eric Wehrli. 1998. Translating idioms. In Pro-
ceedings of COLING-ACL ?98, Montreal, Can-
ada, Vol. 2, pp. 1388-1392. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics 23(3): 377-
401. 
 
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 2?11,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Measuring MWE Compositionality Using Semantic Annotation 
Scott S. L. Piao1, Paul Rayson1, Olga Mudraya2, Andrew Wilson2 and Roger Garside1 
 
1Computing Department 
Lancaster University 
Lancaster, UK 
{s.piao, p.rayson, r.garside}@lancaster.ac.uk 
2Dept. of Linguistics and EL 
Lancaster University 
Lancaster, UK 
{o.mudraya, a.wilson}@lancaster.ac.uk 
 
 
Abstract 
This paper reports on an experiment in 
which we explore a new approach to the 
automatic measurement of multi-word 
expression (MWE) compositionality. We 
propose an algorithm which ranks MWEs 
by their compositionality relative to a 
semantic field taxonomy based on the 
Lancaster English semantic lexicon (Piao 
et al, 2005a). The semantic information 
provided by the lexicon is used for meas-
uring the semantic distance between a 
MWE and its constituent words. The al-
gorithm is evaluated both on 89 manually 
ranked MWEs and on McCarthy et als 
(2003) manually ranked phrasal verbs. 
We compared the output of our tool with 
human judgments using Spearman?s 
rank-order correlation coefficient. Our 
evaluation shows that the automatic rank-
ing of the majority of our test data 
(86.52%) has strong to moderate correla-
tion with the manual ranking while wide 
discrepancy is found for a small number 
of MWEs. Our algorithm also obtained a 
correlation of 0.3544 with manual rank-
ing on McCarthy et als test data, which 
is comparable or better than most of the 
measures they tested. This experiment 
demonstrates that a semantic lexicon can 
assist in MWE compositionality meas-
urement in addition to statistical algo-
rithms. 
1 Introduction 
Over the past few years, compositionality and 
decomposability of MWEs have become impor-
tant issues in NLP research. Lin (1999) argues 
that ?non-compositional expressions need to be 
treated differently than other phrases in many 
statistical or corpus?based NLP methods?. Com-
positionality means that ?the meaning of the 
whole can be strictly predicted from the meaning 
of the parts? (Manning & Sch?tze, 2000). On the 
other hand, decomposability is a metric of the 
degree to which the meaning of a MWE can be 
assigned to its parts (Nunberg, 1994; Riehemann, 
2001; Sag et al, 2002). These two concepts are 
closely related. Venkatapathy and Joshi (2005) 
suggest that ?an expression is likely to be rela-
tively more compositional if it is decomposable?. 
While there exist various definitions for 
MWEs, they are generally defined as cohesive 
lexemes that cross word boundaries (Sag et al, 
2002; Copestake et al, 2002; Calzolari et al, 
2002; Baldwin et al, 2003), which include 
nominal compounds, phrasal verbs, idioms, col-
locations etc. Compositionality is a critical crite-
rion cutting across different definitions for ex-
tracting and classifying MWEs. While semantics 
of certain types of MWEs are non-compositional, 
like idioms ?kick the bucket? and ?hot dog?, 
some others can have highly compositional se-
mantics like the expressions ?traffic light? and 
?audio tape?. 
Automatic measurement of MWE composi-
tionality can have a number of applications. One 
of the often quoted applications is for machine 
translation (Melamed, 1997; Hwang & Sasaki, 
2005), in which non-compositional MWEs need 
special treatment. For instance, the translation of 
a highly compositional MWE can possibly be 
inferred from the translations of its constituent 
words, whereas it is impossible for non-
compositional MWEs, for which we need to 
identify the translation equivalent for the MWEs 
as a whole. 
In this paper, we explore a new method of 
automatically estimating the compositionality of 
MWEs using lexical semantic information, 
sourced from the Lancaster semantic lexicon 
(Piao et al, 2005a) that is employed in the 
USAS1 tagger (Rayson et al, 2004). This is a 
                                                 
1 UCREL Semantic Analysis System 
2
large lexical resource which contains nearly 
55,000 single-word entries and over 18,800 
MWE entries. In this lexicon, each MWE2 and 
the words it contains are mapped to their poten-
tial semantic categories using a semantic field 
taxonomy of 232 categories. An evaluation of 
lexical coverage on the BNC corpus showed that 
the lexical coverage of this lexicon reaches 
98.49% for modern English (Piao et al, 2004).  
Such a large-scale semantic lexical resource al-
lows us to examine the semantics of many 
MWEs and their constituent words conveniently 
without resorting to large corpus data. Our ex-
periment demonstrates that such a lexical re-
source provides an additional approach for auto-
matically estimating the compositionality of 
MWEs. 
One may question the necessity of measuring 
compositionality of manually selected MWEs. 
The truth is, even if the semantic lexicon under 
consideration was compiled manually, it does not 
exclusively consist of non-compositional MWEs 
like idioms. Built for practical discourse analysis, 
it contains many MWEs which are highly com-
positional but depict certain entities or semantic 
concepts. This research forms part of a larger 
effort to extend lexical resources for semantic 
tagging. Techniques are described elsewhere 
(e.g. Piao et al, 2005b) for finding new candi-
date MWE from corpora. The next stage of the 
work is to semi-automatically classify these can-
didates using an existing semantic field taxon-
omy and, to assist this task, we need to investi-
gate patterns of compositionality. 
2 Related Work  
In recent years, various approaches have been 
proposed to the analysis of MWE compositional-
ity. Many of the suggested approaches employ 
statistical algorithms. 
One of the earliest studies in this area was re-
ported by Lin (1999) who assumes that ?non-
compositional phrases have a significantly dif-
ferent mutual information value than the phrases 
that are similar to their literal meanings? and 
proposed to identify non-compositional MWEs 
in a corpus based on distributional characteristics 
of MWEs. Bannard et al (2003) tested tech-
niques using statistical models to infer the mean-
ing of verb-particle constructions (VPCs), focus-
                                                 
2 In this lexicon, many MWEs are encoded as templates, 
such as driv*_* {Np/P*/J*/R*} mad_JJ,  which represent 
variational forms of a single MWE, For further details, see 
Rayson et al, 2004.  
ing on prepositional particles. They tested four 
methods over four compositional classification 
tasks, reporting that, on all tasks, at least one of 
the four methods offers an improvement in preci-
sion over the baseline they used. 
McCarthy et al (2003) suggested that compo-
sitional phrasal verbs should have similar 
neighbours as for their simplex verbs. They 
tested various measures using the nearest 
neighbours of phrasal verbs and their simplex 
counterparts, and reported that some of the 
measures produced results which show signifi-
cant correlation with human judgments. Baldwin 
et al (2003) proposed a LSA-based model for 
measuring the decomposability of MWEs by ex-
amining the similarity between them and their 
constituent words, with higher similarity indicat-
ing the greater decomposability.  They evaluated 
their model on English noun-noun compounds 
and verb-particles by examining the correlation 
of the results with similarities and hyponymy 
values in WordNet. They reported that the LSA 
technique performs better on the low-frequency 
items than on more frequent items. Venkatapathy 
and Joshi (2005) measured relative composition-
ality of collocations having verb-noun pattern 
using a SVM (Support Vector Machine) based 
ranking function. They integrated seven various 
collocational and contextual features using their 
ranking function, and evaluated it against manu-
ally ranked test data. They reported that the SVM 
based method produces significantly better re-
sults compared to methods based on individual 
features. 
The approaches mentioned above invariably 
depend on a variety of statistical contextual in-
formation extracted from large corpus data. In-
evitably, such statistical information can be af-
fected by various uncontrollable ?noise?, and 
hence there is a limitation to purely statistical 
approaches. 
In this paper, we contend that a manually 
compiled semantic lexical resource can have an 
important part to play in measuring the composi-
tionality of MWEs. While any approach based on 
a specific lexical resource may lack generality, it 
can complement purely statistical approaches by 
importing human expert knowledge into the 
process. Particularly, if such a resource has a 
high lexical coverage, which is true in our case, 
it becomes much more useful for dealing with 
general English. It should be emphasized that we 
propose our semantic lexical-based approach not 
as a substitute for the statistical approaches. 
3
Rather we propose it as a potential complement 
to them.   
In the following sections, we describe our ex-
periment and explore this approach to the issue 
of automatic estimation of MWE compositional-
ity. 
3 Measuring MWE compositionality 
with semantic field information 
In this section, we propose an algorithm for 
automatically measuring MWE compositionality 
based on the Lancaster semantic lexicon. In this 
lexicon, the semantic field of each word and 
MWE is encoded in the form of semantic tags. 
We contend that the compositionality of a MWE 
can be estimated by measuring the distance be-
tween semantic fields of an MWE and its con-
stituent words based on the semantic field infor-
mation available from the lexicon. 
The lexicon employs a taxonomy containing 
21 major semantic fields which are further di-
vided into 232 sub-categories. 3  Tags are de-
signed to denote the semantic fields using letters 
and digits. For instance, tag N3.2 denotes the 
category of {SIZE} and Q4.1 denotes {media: 
Newspapers}. Each entry in the lexicon maps a 
word or MWE to its potential semantic field 
category/ies. More often than not, a lexical item 
is mapped to multiple semantic categories, re-
flecting its potential multiple senses. In such 
cases, the tags are arranged by the order of like-
lihood of meanings, with the most prominent one 
at the head of the list. For example, the word 
?mass? is mapped to tags N5, N3.5, S9, S5 and 
B2, which denote its potential semantic fields of 
{QUANTITIES},  {MEASUREMENT: 
WEIGHT}, {RELIGION AND SUPERNATU-
RAL}, {GROUPS AND AFFILIATION} and 
{HEALTH AND DISEASE}. 
 The lexicon provides direct access to the se-
mantic field information for large number of 
MWEs and their constituent words. Furthermore, 
the lexicon was analysed and classified manually 
by a team of linguists based on the analysis of 
corpus data and consultation of printed and elec-
tronic corpus-based dictionaries, ensuring a high 
level of consistency and accuracy of the semantic 
analysis.  
In our context, we interpret the task of measur-
ing the compositionality of MWEs as examining 
the distance between the semantic tag of a MWE 
and the semantic tags of its constituent words. 
                                                 
3 For the complete semantic tagset, see website: 
http://www.comp.lancs.ac.uk/ucrel/usas/ 
Given a MWE M and its constituent words wi (i 
= 1, .., n), the compositionality D can be meas-
ured by multiplying the semantic distance SD 
between M and each of its constituent words wi. 
In practice, the square root of the product is used 
as the score in order to reduce the range of actual 
D-scores, as shown below: 
 
(1)   ?
=
=
n
i
iwMSDMD
1
),()(  
 
where D-score ranges between [0, 1], with 1 in-
dicating the strongest compositionality and 0 the 
weakest compositionality. 
In the semantic lexicon, as the semantic in-
formation of function words is limited, they are 
classified into a single grammatical bin (denoted 
by tag Z5). In our algorithm, they are excluded 
from the measuring process by using a stop word 
list. Therefore, only the content constituent 
words are involved in measuring the composi-
tionality. Although function words may form an 
important part of many MWEs, such as phrasal 
verbs, because our algorithm solely relies on se-
mantic field information, we assume they can be 
ignored.  
 The semantic distance between a MWE and 
any of its constituent words is calculated by 
quantifying the similarity between their semantic 
field categories. In detail, if the MWE and a con-
stituent word do not share any of the major 21 
semantic domains, the SD is assigned a small 
value ?.4 If they do, three possible cases are con-
sidered: 
 
Case a. If they share the same tag, and the con-
stituent word has only one tag, then SD 
is one. 
Case b. If they share a tag or tags, but the con-
stituent words have multiple candidate 
tags, then SD is weighted using a vari-
able ? based on the position of the 
matched tag in the candidate list as well 
as the number of candidate tags. 
Case c. If they share a major category, but their 
tags fall into different sub-categories 
(denoted by the trailing digits following 
a letter), SD is further weighted using a 
                                                 
4 We avoid using zero here in order to avoid producing se-
mantic distance of zero indiscriminately when any one of 
the constituent words produces zero distance regardless of 
other constituent words. 
4
variable ? which reflects the difference 
of the sub-categories. 
With respect to weight ?, suppose L is the 
number of candidate tags of the constituent word 
under consideration, N is the position of the spe-
cific tag in the candidate list (the position starts 
from the top with N=1), then the weight ? is cal-
culated as 
 
(2)  
2
1
L
NL +?=? , 
 
where N=1, 2, ?, n and N<=L. Ranging between 
[1, 0), ? takes into account both the location of 
the matched tag in the candidate tag list and the 
number of candidate tags. This weight penalises 
the words having more candidate semantic tags 
by giving a lower value for their higher degree of 
ambiguity. As either L or N increases, the ?-
value decreases.       
Regarding the case c), where the tags share the 
same head letter but different digit codes, i.e. 
they are from the same major category but in 
different sub-categories, the weight ? is calcu-
lated based on the number of sub-categories they 
share. As we mentioned earlier, a semantic tag 
consists of an initial letter and some trailing dig-
its divided by points, e.g. S1.1.2 {RECIPROC-
ITY}, S1.1.3 {PARTICIPATION}, S1.1.4 {DE-
SERVE} etc. If we let T1, T2 be a pair of semantic 
tags with the same initial letters, which have ki 
and kj trailing digit codes (denoting the number 
of sub-division layers) respectively and share n 
digit codes from the left, or from the top layer, 
then ? is calculated as follows: 
 
(3)   
k
n=? ; 
(4)   . ),max( ji kkk =
 
where ? ranges between (0, 1). In fact, the cur-
rent USAS taxonomy allows only the maximum 
three layers of sub-division, therefore ? has one 
of three possible scores: 0.500 (1/2), 0.333 (1/3) 
and 0.666 (2/3). In order to avoid producing zero 
scores, if the pair of tags do not share any digit 
codes except the head letter, then n is given a 
small value of 0.5. 
Combining all of the weighting scores, the 
semantic distance SD in formula (1) is calculated 
as follows: 
 
(5)  ( )
??
?
?
??
?
?
?
=
?
?
=
=
.  then   c), if
;  then   b), if
1;  then   a), if
;   then   matches,   tagno if
,
1
1
n
i
ii
n
i
iiwMSD
??
?
?
 
where ? is given a small value of 0.001 for our 
experiment5. 
Some MWEs and single words in the lexicon 
are assigned with combined semantic categories 
which are considered to be inseparable, as shown 
below: 
petrol_NN1 station_NN1 M3/H1 
where the slash means that this MWE falls under 
the categories of M3 {VEHICLES AND TRANS-
PORTS ON LAND} and H1 {ARCHITECTURE 
AND KINDS OF HOUSES AND BUILDINGS} 
at the same time. For such cases, criss-cross 
comparisons between all possible tag pairs are 
carried out in order to find the optimal match 
between the tags of the MWE and its constituent 
words. 
By way of further explanation, the word 
?brush? as a verb has candidate semantic tags of 
B4 {CLEANING AND PERSONAL CARE} and 
A1.1.1 {GENERAL ACTION, MAKING} etc. On 
the other hand, the phrasal verb ?brush down? 
may fall under either B4 category with the sense 
of cleaning or G2.2 category {ETHICS} with the 
sense of reprimand. When we apply our algo-
rithm to it, we get the D-score of 1.0000 for the 
sense of cleaning, indicating a high degree of 
compositionality, whereas we get a low D-score 
of 0.0032 for the sense of reprimand, indicating 
a low degree of compositionality. Note that the 
word ?down? in this MWE is filtered out as it is 
a functional word. 
The above example has only a single constitu-
ent content word. In practice, many MWEs have 
more complex structures than this example. In 
order to test the performance of our algorithm, 
we compared its output against human judgments 
of compositionality, as reported in the following 
section. 
4 Manually Ranking MWEs for 
Evaluation 
In order to evaluate the performance of our 
tool against human judgment, we prepared a list 
                                                 
5 As long as ? is small enough, it does not affect the ranking 
of D-scores. 
5
of 89 MWEs6 and asked human raters to rank 
them via a website. The list includes six MWEs 
with multiple senses, and these were treated as 
separate MWE. The Lancaster MWE lexicon has 
been compiled manually by expert linguists, 
therefore we assume that every item in this lexi-
con is a true MWE, although we acknowledge 
that some errors may exist. 
Following McCarthy et al?s approach, we 
asked the human raters to assign each MWE a 
number ranging between 0 (opaque) and 10 
(fully compositional). Both native and non-native 
speakers are involved, but only the data from 
native speakers are used in this evaluation. As a 
result, three groups of raters were involved in the 
experiment.  Group 1 (6 people) rated MWEs 
with indexes of 1-30, Group 2 (4 people) rated 
MWEs with indexes of 31-59 and Group 3 (five 
people) rated MWEs with indexes of 6-89. 
In order to test the level of agreement between 
the raters, we used the procedures provided in 
the 'irr' package for R (Gamer, 2005). With this 
tool, the average intraclass correlation coefficient 
(ICC) was calculated for each group of raters 
using a two-way agreement model (Shrout & 
Fleiss, 1979). As a result, all ICCs exceeded 0.7 
and were significant at the 95% confidence level, 
indicating an acceptable level of agreement be-
tween raters. For Group 1, the ICC was 0.894 
(95% ci = 0.807 < ICC < 0.948), for Group 2 it 
was 0.9 (95% ci=0.783<ICC<0.956) and for 
Group 3 it was 0.886 (95% ci =  0.762 < ICC < 
0.948). 
Based on this test, we conclude that the man-
ual ranking of the MWEs is reliable and is suit-
able to be used in our evaluation. Source data for 
the human judgements is available from our 
website in spreadsheet form7. 
5 Evaluation 
In our evaluation, we focused on testing the 
performance of the D-score against human rat-
ers? judgment on ranking different MWEs by 
their degree of compositionality, as well as dis-
tinguishing the different degrees of composition-
ality for each sense in the case of multiple tags.  
The first step of the evaluation was to imple-
ment the algorithm in a program and run the tool 
on the 89 test MWEs we prepared. Fig. 1 illus-
trates the D-score distribution in a bar chart. As 
shown by the chart, the algorithm produces a 
widely dispersed distribution of D-scores across 
                                                 
6 Selected at random from the Lancaster semantic lexicon. 
7 http://ucrel.lancs.ac.uk/projects/assist/ 
the sample MWEs, ranging from 0.000032 to 
1.000000. For example, the tool assigned the 
score of 1.0 to the FOOD sense and 0.001 to the 
THIEF senses of ?tea leaf? successfully distin-
guishing the different degrees of compositional-
ity of these two senses. 
 
MWE Compositionality Distribution
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86
89 MWEs
D
-s
co
re
 
 
Fig 1: D-score distribution across 89 sample 
MWEs 
 
As shown in Fig. 1, some MWEs share the 
same scores, reflecting the limitation of the num-
ber of ranks that our algorithm can produce as 
well as the limited amount of semantic informa-
tion available from a lexicon. Nonetheless, the 
algorithm produced 45 different scores which 
ranked the MWEs into 45 groups (see the steps 
in the figure). Compared to the eleven scores 
used by the human raters, this provides a fine-
grained ranking of the compositionality.   
The primary issue in our evaluation is the ex-
tent to which the automatic ranking of the MWEs 
correlates with the manual ranking of them. As 
described in the previous section, we created a 
list of 89 manually ranked MWEs for this pur-
pose. Since we are mainly interested in the ranks 
rather than the actual scores, we examined the 
correlation between the automatic and manual 
rankings using Spearman?s correlation coeffi-
cient. (For the full ranking list, see Appendix). 
In the manually created list, each MWE was 
ranked by 3-6 human raters. In order to create a 
unified single test data of human ranking, we 
calculated the average of the human ranks for 
each MWE. For example, if two human raters 
give ranks 3 and 4 to a MWE, then its rank is 
(3+4)/2=3.5. Next, the MWEs are sorted by the 
averaged ranks in descending order to obtain the 
combined ranks of the MWEs. Finally, we sorted 
the MWEs by the D-score in the same way to 
obtain a parallel list of automatic ranks. For the 
calculation of Spearman?s correlation coefficient, 
if n MWEs are tied to a score (either D-score or 
the average manual ranks), their ranks were ad-
6
justed by dividing the sum of their ranks by the 
number of MWEs involved. Fig. 2 illustrates the 
correspondence between the adjusted automatic 
and manual rankings. 
 
Auto vs. Manual Ranks Comparison
(n=89, rho=0.2572)
0
20
40
60
80
100
0 20 40 60 80 100
auto ranks
m
an
ua
l r
an
ks
 
 
Fig. 2: Scatterplot of automatic vs. manual 
ranking. 
 
As shown in Fig. 2, the overall correlation seems 
quite weak. In the automatic ranking, quite a few 
MWEs are tied up to three ranks, illustrated by 
the vertically aligned points. The precise correla-
tion between the automatic and manual rankings 
was calculated using the function provided in R 
for Windows 2.2.1.  Spearman's rank correlation 
(rho) for these data was 0.2572 (p=0.01495), 
indicating a significant though rather weak posi-
tive relationship. 
In order to find the factors causing this weak 
correlation, we tested the correlation for those 
MWEs whose rank differences were less than 20, 
30, 40 and 50 respectively. We are interested to 
find out how many of them fall under each of the 
categories and which of their features affected 
the performance of the algorithm. As a result, we 
found 43, 54, 66 and 77 MWEs fall under these 
categories respectively, which yield different 
correlation scores, as shown in Table 1.  
 
numb of 
MWEs 
Percent 
(%) 
Rank 
diff 
rho-
score 
Sig. 
43 48.31 <20 0.9149 P<0.001 
54 60.67 <30 0.8321 P<0.001 
66 74.16 <40 0.7016 P<0.001 
77 86.52 <50 0.5084 P<0.001 
89 (total) 100.00 <=73 0.2572 P<0.02 
 
Table 1: Correlation coefficients corresponding 
different rank differences. 
 
As we expected, the rho decreases as the rank 
difference increases, but all of the four categories 
containing a total of 77 MWEs (86.52%) show 
reasonably high correlations, with the minimum 
score of 0.5084. 8 In particular, 66 of them 
(74.16%), whose ranking differences are less 
than 40, demonstrate a strong correlation with 
rho-score 0.7016, as illustrated by Fig. 3 
 
ScatterPlot of Auto vs. Man Ranks for 66 MWEs
(rank_diff < 40)
0
20
40
60
80
100
0 20 40 60 80 10auto ranks
m
an
 r
an
ks
0
 
 
Fig 3: ScatterPlot for 66 MWEs (rank_diff < 
40) which shows a strong correlation 
 
Our manual examination shows that the algo-
rithm generally pushes the highly compositional 
and non-compositional MWEs towards opposite 
ends of the spectrum of the D-score. For example, 
those assigned with score 1 include ?aid worker?, 
?audio tape? and ?unemployment figure?. On the 
other hand, MWEs such as ?tea leaf? (meaning 
thief), ?kick the bucket? and ?hot dog? are given 
a low score of 0.001. We assume these two 
groups of MWEs are generally treated as highly 
compositional and opaque MWEs respectively. 
However, the algorithm could be improved. A 
major problem found is that the algorithm pun-
ishes longer MWEs which contain function 
words. For example, ?make an appearance? is 
scored 0.000114 by the algorithm, but when the 
article ?an? is removed, it gets a higher score 
0.003608. Similarly, when the preposition ?up? 
is removed from ?keep up appearances?, it gets 
0.014907 compared to the original 0.000471, 
which would push up their rank much higher. To 
address this problem, the algorithm needs to be 
refined to minimise the impact of the function 
words to the scoring process. 
Our analysis also reveals that 12 MWEs with 
rank differences (between automatic and manual 
ranking) greater than 50 results in a degraded 
overall correlation. Table 2 lists these words, in 
which the higher ranks indicate higher composi-
tionality.  
 
                                                 
8 Salkind (2004: 88) suggests that r-score ranges 0.4~0.6, 
0.6~0.8 and 0.8~1.0 indicate moderate, strong and very 
strong correlations respectively. 
7
MWE Sem. Tag9 Auto 
rank 
Manual 
rank 
plough into A9- 53.5 3 
Bloody Mary F2 53.5 2 
pillow fight K6 26 80.5 
lollipop lady M3/S2 70 15 
cradle snatcher S3.2/T3/S2 73.5 17.5 
go bananas X5.2+++ 65 8.5 
make an appearance S1.1.3+ 2 58.5 
keep up appearances A8/S1.1.1 4 61 
sandwich course P1 69 11.5 
go bananas B2-/X1 68 10 
Eskimo roll M4 71.5 5 
in other words Z4 12.5 83 
 
Table 2: Twelve MWEs having rank differences 
greater than 50. 
 
Let us take ?pillow fight? as an example. The 
whole expression is given the semantic tag K6, 
whereas neither ?pillow? nor ?fight? as individ-
ual word is given this tag. In the lexicon, ?pil-
low? is classified as H5 {FURNITURE AND 
HOUSEHOLD FITTINGS} and ?fight? is as-
signed to four semantic categories including S8- 
{HINDERING}, X8+ {HELPING}, E3- {VIO-
LENT/ANGRY}, and K5.1 {SPORTS}. For this 
reason, the automatic score of this MWE is as 
low as 0.003953 on the scale of [0, 1]. On the 
contrary, human raters judged the meaning of 
this expression to be fairly transparent, giving it 
a high score of 8.5 on the scale of [0, 10]. Similar 
contrasts occurred with the majority of the 
MWEs with rank differences greater than 50, 
which are responsible for weakening the overall 
correlation. 
Another interesting case we noticed is the 
MWE ?pass away?. This MWE has two major 
senses in the semantic lexicon L1- {DIE} and 
T2- {END} which were ranked separately. Re-
markably, they were ranked in the opposite order 
by human raters and the algorithm. Human raters 
felt that the sense DIE is less idiomatic, or more 
compositional, than END, while the algorithm 
indicated otherwise. The explanation of this 
again lies in the semantic classification of the 
lexicon, where ?pass? as a single word contains 
the sense T2- but not L1-. Consequently, the 
automatic score for ?pass away? with the sense 
                                                 
                                                
9 Semantic tags occurring in Table 2: A8 (seem), A9 (giving 
possession), B2 (health and disease), F2 (drink), K6 (chil-
dren?s games and toys), M3 (land transport), M4 (swim-
ming), P1 (education), S1.1.1 (social actions), S1.1.3 (par-
ticipation), S2 (people), S3.2 (relationship), T3 (time: age), 
X1 (psychological actions), X5.2 (excited), Z4 (discourse 
bin) 
L1- is much lower (0.001) than that with the 
sense of T2- (0.007071). 
In order to evaluate our algorithm in compari-
son with previous work, we also tested it on the 
manual ranking list created by McCarthy et al
(2003).10 We found that 79 of the 116 phrasal 
verbs in that list are included in the Lancaster 
semantic lexicon. We applied our algorithm on 
those 79 items to compare the automatic ranks 
against the average manual ranks using the 
Spearman?s rank correlation coefficient (rho). As 
a result, we obtained rho=0.3544 with signifi-
cance level of p=0.001357. This result is compa-
rable with or better than most measures reported 
by McCarthy et al(2003). 
6 Discussion 
The algorithm we propose in this paper is dif-
ferent from previous proposed statistical methods 
in that it employs a semantic lexical resource in 
which the semantic field information is directly 
accessible for both MWEs and their constituent 
words. Often, typical statistical algorithms meas-
ure the semantic distance between MWEs and 
their constituent words by comparing their con-
texts comprising co-occurrence words in near 
context extracted from large corpora, such as 
Baldwin et als algorithm (2003). 
When we consider the definition of the com-
positionality as the extent to which the meaning 
of the MWE can be guessed based on that of its 
constituent words, a semantic lexical resource 
which maps MWEs and words to their semantic 
features provides a practical way of measuring 
the MWE compositionality. The Lancaster se-
mantic lexicon is one such lexical resource 
which allows us to have direct access to semantic 
field information of large number of MWE and 
single words. Our experiment demonstrates the 
potential value of such semantic lexical resources 
for the automatic measurement of MWE compo-
sitionality. Compared to statistical algorithms 
which can be affected by a variety of un-
controllable factors, such as size and domain of 
corpora, etc., an expert-compiled semantic lexi-
cal resource can provide much more reliable and 
?clean? lexical semantic information. 
However, we do not suggest that algorithms 
based on semantic lexical resources can substi-
tute corpus-based statistical algorithms. Rather, 
we suggest it as a complement to existing statis-
tical algorithms. As the errors of our algorithm 
 
10This list is available at website: 
http://mwe.stanford.edu/resources/  
8
reveal, the semantic information provided by the 
lexicon alone may not be rich enough for a very 
fine-grained distinction of MWE compositional-
ity. In order to obtain better results, this algo-
rithm needs to be combined with statistical tech-
niques. 
A limitation of our approach is language-
dependency. In order to port our algorithm to 
languages other than English, one needs to build 
similar semantic lexicon in those languages. 
However, similar semantic lexical resources are 
already under construction for some other lan-
guages, including Finnish and Russian (L?fberg 
et al, 2005; Sharoff et al, 2006), which will al-
low us to port our algorithm to those languages. 
7 Conclusion 
In this paper, we explored an algorithm based 
on a semantic lexicon for automatically measur-
ing the compositionality of MWEs. In our 
evaluation, the output of this algorithm showed 
moderate correlation with a manual ranking. We 
claim that semantic lexical resources provide 
another approach for automatically measuring 
MWE compositionality in addition to the exist-
ing statistical algorithms. Although our results 
are not yet conclusive due to the moderate scale 
of the test data, our evaluation demonstrates the 
potential of lexicon-based approaches for the 
task of compositional analysis. We foresee, by 
combining our approach with statistical algo-
rithms, that further improvement can be ex-
pected. 
8 Acknowledgement 
The work reported in this paper was carried 
out within the UK-EPSRC-funded ASSIST Pro-
ject (Ref. EP/C004574). 
References  
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, 
and Dominic Widdows. 2003. An Empirical Model 
of Multiword Expression Compositionality. In 
Proc. of the ACL-2003 Workshop on Multiword 
Expressions: Analysis, Acquisition and Treatment,  
pages 89-96, Sapporo, Japan. 
Colin Bannard, Timothy Baldwin, and Alex Las-
carides. 2003. A statistical approach to the seman-
tics of verb-particles. In Proc. of the ACL2003 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, pages 65?72, Sapporo. 
Nicoletta Calzolari, Charles Fillmore, Ralph Grish-
man, Nancy Ide, Alessandro Lenci, Catherine 
MacLeod, and Antonio Zampolli. 2002. Towards 
best practice for multiword expressions in compu-
tational lexicons. In Proc. of the Third Interna-
tional Conference on Language Resources and 
Evaluation (LREC 2002), pages 1934?1940, Las 
Palmas, Canary Islands. 
Ann Copestake, Fabre Lambeau, Aline Villavicencio, 
Francis Bond, Timothy Baldwin, Ivan A. Sag, and 
Dan Flickinger. 2002. Multiword expressions: Lin-
guistic precision and reusability. In Proc. of the 
Third International Conference on Language Re-
sources and Evaluation (LREC 2002), pages 1941?
1947, Las Palmas, Canary Islands. 
Matthias  Gamer. 2005. The irr Package: Various Co-
efficients of Interrater Reliability and Agreement. 
Version 0.61 of 11 October 2005.  Available from:   
cran.r-project.org/src/contrib/Descriptions/irr.html 
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of the 37th Annual 
Meeting of the ACL, pages 317?324, College Park, 
USA. 
Laura L?fberg, Scott Piao, Paul Rayson, Jukka-Pekka 
Juntunen, Asko Nyk?nen, and Krista Varantola. 
2005. A semantic tagger for the Finnish language. 
In Proc. of the Corpus Linguistics 2005 conference, 
Birmingham, UK. 
Christopher D. Manning and Hinrich Sch?tze. 2000. 
Foundations of Statistical Natural Language Proc-
essing. The MIT Press, Cambridge, Massachusetts. 
Diana McCarthy, Bill Keller, and John Carroll. 2003. 
Detecting a continuum of compositionality in 
phrasal verbs. In Proc. of the ACL-2003 Workshop 
on Multiword Expressions: Analysis, Acquisition 
and Treatment, pages 73?80, Sapporo, Japan. 
Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc. 
of the 2nd Conference on Empirical Methods in 
Natural Language Processing , Providence, USA. 
Geoffrey Nunberg, Ivan A. Sag, and Tom Wasow. 
1994. Idioms. Language, 70: 491?538. 
Scott S.L. Piao, Paul Rayson, Dawn Archer and Tony 
McEnery. 2004. Evaluating Lexical Resources for 
a Semantic Tagger. In Proc. of LREC-04, pages 
499?502, Lisbon, Portugal. 
Scott S.L. Piao, Dawn Archer, Olga Mudraya, Paul 
Rayson, Roger Garside, Tony McEnery and An-
drew Wilson. 2005a. A Large Semantic Lexicon 
for Corpus Annotation. In Proc. of the Corpus Lin-
guistics Conference 2005, Birmingham, UK. 
Scott S.L. Piao., Paul Rayson, Dawn Archer, Tony 
McEnery. 2005b. Comparing and combining a se-
mantic tagger and a statistical tool for MWE ex-
traction. Computer Speech and Language, 19, 4: 
378?397. 
9
Paul Rayson, Dawn Archer, Scott Piao, and Tony 
McEnery. 2004. The UCREL Semantic Analysis 
System. In Proc. of LREC-04 Workshop: Beyond 
Named Entity Recognition Semantic Labeling for 
NLP Tasks, pages 7?12, Lisbon, Portugal. 
Susanne Riehemann. 2001. A Constructional Ap-
proach to Idioms and Word Formation. Ph.D. the-
sis, Stanford University, Stanford. 
 Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann 
Copestake, and Dan Flickinger. 2002. Multiword 
Expressions: A Pain in the Neck for NLP. In Proc. 
of the 3rd International Conference on Intelligent 
Text Processing and Computational Linguistics 
(CICLing-2002), pages 1?15, Mexico City, Mexico. 
Neil J. Salkind. 2004. Statistics for People Who Hate 
Statistics. Sage: Thousand Oakes, US. 
Serge Sharoff, Bogdan Babych, Paul Rayson, Olga 
Mudraya and Scott Piao. 2006. ASSIST: Auto-
mated semantic assistance for translators. Proceed-
ings of EACL 2006, pages 139?142, Trento, Italy. 
Patrick E. Shrout and Joseph L. Fleiss. 1979. Intra-
class Correlations: Uses in Assessing Rater Reli-
ability. Psychological Bulletin (2), 420?428. 
Sriram Venkatapathy and Aravind K. Joshi. 2005. 
Measuring the relative compositionality of verb-
noun (V-N) collocations by integrating features. In 
Proc. of Human Language Technology Conference 
and Conference on Empirical Methods in Natural 
Language Processing (HLT/EMNLP 2005), pages 
899?906, Vancouver, Canada. 
 
Appendix: Manual vs. Automatic Ranks 
of Sample MWEs 
The table below shows the human and auto-
matic rankings of 89 sample MWEs. The MWEs 
are sorted in ascending order by manual average 
ranks. The top items are supposed to be the most 
compositional ones. For example, according to 
the manual ranking, facial expression is the most 
compositional MWE while tea leaf is the most 
opaque one. This table also shows that some 
MWEs are tied up with the same ranks. For the 
definitions of the full semantic tagset, see web-
site http://www.comp.lancs.ac.uk/ucrel/usas/. 
 
MWE Tag Sem tag Man 
rank 
Auto. 
rank 
facial expression B1 1 9 
aid worker S8/S2 2 4 
audio tape K3 3.5 4 
leisure activities K1 3.5 36.5 
advance warning T4/Q2.2 5 36.5 
living space H2 6 51 
in other words Z4 7 77.5 
unemployment fig-
ures 
I3.1/N5 8 4 
camera angle Q4.3 9.5 45 
pillow fight K6 9.5 64 
youth club S5/T3 11.5 4 
petrol station M3/H1 11.5 36.5 
palm tree L3 13 9 
rule book G2.1/Q4.1 14 4 
ball boy K5.1/S2.2 15 13 
goal keeper K5.1/S2 16.5 4 
kick in E3- 16.5 36.5 
ventilation shaft H2 18 47 
directory enquiries Q1.3 19 14 
phone box Q1.3/H1 21 18.5 
lose balance M1 21 53 
bend the rules A1.7 21 54.5 
big nose X7/X2.4 23 67 
quantity control N5/A1.7 24 11.5 
act of God S9 25 36.5 
air bag A15/M3 26 62.5 
mind stretching A12 27 59 
plain clothes B5 28 36.5 
keep up appearances A8/S1.1.1 29 86 
examining board P1 30 23 
open mind X6 31.5 49 
make an appearance S1.1.3+ 31.5 88 
cable television Q4.3 33 15 
king size N3.2 34 36.5 
action point X7 35 61 
keep tight rein on A1.7 36 28 
noughts and crosses K5.2 37 77.5 
tea leaf L3/F2 38 4 
single minded X5.1 39.5 77.5 
window dressing I2.2 39.5 77.5 
street girl G1.2/S5 42 36.5 
just over the horizon S3.2/S2.1 42 60 
pressure group T1.1.3 42 16.5 
air proof O4.1 44.5 57.5 
heart of gold S1.2.2 44.5 77.5 
lose heart X5.2 46 26 
food for thought X2.1/X5.1 47 89 
play part S8 48 68 
look down on S1.2.3 49 77.5 
arm twisting Q2.2 50 36.5 
take into account A1.8 51 69 
kidney bean F1 52 9 
come alive A3+ 53 52 
break new ground T3/T2 54 54 
make up to S1.1.2 55 65 
by virtue of C1 56.5 36.5 
snap shot A2.2 56.5 27 
pass away L1- 58 77.5 
long face E4.1 59 77.5 
bossy boots S1.2.3/S2 60 77.5 
plough into M1/A1.1.2 61 11.5 
kick in T2+ 62 50 
animal magnetism S1.2 63 55.5 
sixth former P1/S2 64 77.5 
pull the strings S7.1 65 62.5 
couch potato A1.1.1/S2 66 77.5 
think tank S5/X2.1 67 36.5 
come alive X5.2+ 68 24 
hot dog F1 69 77.5 
cheap shot G2.2-/Q2.2 70 66 
10
rock and roll K2 71 48 
bright as a button S3.2/T3/S2 72.5 87 
cradle snatcher X9.1+ 72.5 16.5 
alpha wave B1 74 77.5 
lollipop lady M3/S2 75 20 
pass away X5.2+ 76.5 57.5 
plough into T2- 76.5 36.5 
piece of cake P1 78.5 77.5 
sandwich course A12 78.5 21 
go bananas B2-/X1 80 22 
go bananas X5.2+++ 81.5 36.5 
go bananas E3- 81.5 25 
kick the bucket L1 83 77.5 
on the wagon F2 84 36.5 
Eskimo roll M4 85 18.5 
acid house K2 86 46 
plough into A9- 87 36.5 
Bloody Mary F2 88 36.5 
tea leaf G2.1-/S2mf 89 77.5 
 
11
Annotated web as corpus 
 
Paul Rayson 
Computing Department, 
Lancaster University, UK 
p.rayson@lancs.ac.uk
James Walkerdine 
Computing Department, 
Lancaster University, UK 
j.walkerdine@lancs.ac.uk 
William H. Fletcher 
United States Naval  
Academy, USA 
fletcher@usna.edu 
Adam Kilgarriff 
Lexical Computing Ltd., UK 
adam@lexmasterclass.com 
 
Abstract 
This paper presents a proposal to facili-
tate the use of the annotated web as cor-
pus by alleviating the annotation bottle-
neck for corpus data drawn from the web. 
We describe a framework for large-scale 
distributed corpus annotation using peer-
to-peer (P2P) technology to meet this 
need. We also propose to annotate a large 
reference corpus in order to evaluate this 
framework. This will allow us to investi-
gate the affordances offered by distrib-
uted techniques to ensure replicability of 
linguistic research based on web-derived 
corpora. 
1 Introduction 
Linguistic annotation of corpora contributes cru-
cially to the study of language at several levels: 
morphology, syntax, semantics, and discourse. 
Its significance is reflected both in the growing 
interest in annotation software for word sense 
tagging (Edmonds and Kilgarriff, 2002) and in 
the long-standing use of part-of-speech taggers, 
parsers and morphological analysers for data 
from English and many other languages. 
Linguists, lexicographers, social scientists and 
other researchers are using ever larger amounts 
of corpus data in their studies. In corpus linguis-
tics the progression has been from the 1 million-
word Brown and LOB corpora of the 1960s, to 
the 100 million-word British National Corpus of 
the 1990s. In lexicography this progression is 
paralleled, for example, by Collins Dictionaries? 
initial 10 million word corpus growing to their 
current corpus of around 600 million words. In 
addition, the requirement for mega- and even 
giga-corpora1 extends to other applications, such 
as lexical frequency studies, neologism research, 
and statistical natural language processing where 
models of sparse data are built. The motivation 
for increasingly large data sets remains the same. 
Due to the Zipfian nature of word frequencies, 
around half the word types in a corpus occur 
only once, so tremendous increases in corpus 
size are required both to ensure inclusion of es-
sential word and phrase types and to increase the 
chances of multiple occurrences of a given type.  
In corpus linguistics building such mega-
corpora is beyond the scope of individual re-
searchers, and they are not easily accessible 
(Kennedy, 1998: 56) unless the web is used as a 
corpus (Kilgarriff and Grefenstette, 2003). In-
creasingly, corpus researchers are tapping the 
Web to overcome the sparse data problem (Kel-
ler et al, 2002). This topic generated intense in-
terest at workshops held at the University of Hei-
delberg (October 2004), University of Bologna 
(January 2005), University of Birmingham (July 
2005) and now in Trento in April 2006. In addi-
tion, the advantages of using linguistically anno-
tated data over raw data are well documented 
(Mair, 2005; Granger and Rayson, 1998). As the 
size of a corpus increases, a near linear increase 
in computing power is required to annotate the 
text. Although processing power is steadily 
growing, it has already become impractical for a 
single computer to annotate a mega-corpus.  
Creating a large-scale annotated corpus from 
the web requires a way to overcome the limita-
tions on processing power. We propose distrib-
uted techniques to alleviate the limitations on the 
                                                 
1 See, for example, those distributed by the Linguistic 
Data Consortium: http://www.ldc.upenn.edu/ 
27
volume of data that can be tagged by a single 
processor. The task of annotating the data will be 
shared by computers at collaborating institutions 
around the world, taking advantage of processing 
power and bandwidth that would otherwise go 
unused. Such large-scale parallel processing re-
moves the workload bottleneck imposed by a 
server based structure. This allows for tagging a 
greater amount of textual data in a given amount 
of time while permitting other users to use the 
system simultaneously. Vast amounts of data can 
be analysed with distributed techniques. The fea-
sibility of this approach has been demonstrated 
by the SETI@home project2. 
The framework we propose can incorporate 
other annotation or analysis systems, for exam-
ple, lemmatisation, frequency profiling, or shal-
low parsing. To realise and evaluate the frame-
work, it will be developed for a peer-to-peer 
(P2P) network and deployed along with an exist-
ing lexicographic toolset, the Sketch Engine. A 
P2P approach allows for a low cost implementa-
tion that draws upon available resources (existing 
user PCs). As a case study for evaluation, we 
plan to collect a large reference corpus from the 
web to be hosted on servers from Lexical Com-
puting Ltd. We can evaluate annotation speed 
gains of our approach comparatively against the 
single server version by utilising processing 
power in computer labs at Lancaster University 
and the United States Naval Academy (USNA) 
and we will call for volunteers from the corpus 
community to be involved in the evaluation as 
well.  
A key aspect of our case study research will be 
to investigate extending corpus collection to new 
document types. Most web-derived corpora have 
exploited raw text or HTML pages, so efforts 
have focussed on boilerplate removal and clean-
up of these formats with tools like Hyppia-BTE, 
Tidy and Parcels 3  (Baroni and Sharoff, 2005). 
Other document formats such as Adobe PDF and 
MS-Word have been neglected due to the extra 
conversion and clean-up problems they entail. 
By excluding PDF documents, web-derived cor-
pora are less representative of certain genres 
such as academic writing. 
                                                 
                                                
2 http://setiathome.ssl.berkeley.edu/ 
3 http://www.smi.ucd.ie/hyppia/, 
http://parcels.sourceforge.net and 
http://tidy.sourceforge.net. 
2 Related Work  
The vast majority of previous work on corpus 
annotation has utilised either manual coding or 
automated software tagging systems, or else a 
semi-automatic combination of the two ap-
proaches e.g. automated tagging followed by 
manual correction. In most cases a stand-alone 
system or client-server approach has been taken 
by annotation software using batch processing 
techniques to tag corpora. Only a handful of 
web-based or email services (CLAWS4, Amal-
gam5, Connexor6) are available, for example, in 
the application of part-of-speech tags to corpora. 
Existing tagging systems are ?small scale? and 
typically impose some limitation to prevent over-
load (e.g. restricted access or document size). 
Larger systems to support multiple document 
tagging processes would require resources that 
cannot be realistically provided by existing sin-
gle-server systems. This corpus annotation bot-
tleneck becomes even more problematic for vo-
luminous data sets drawn from the web. The use 
of the web as a corpus for teaching and research 
on language has been proposed a number of 
times (Kilgarriff, 2001; Robb, 2003; Rundell, 
2000; Fletcher, 2001, 2004b) and received a spe-
cial issue of the journal Computational Linguis-
tics (Kilgarriff and Grefenstette, 2003). Studies 
have used several different methods to mine web 
data. Turney (2001) extracts word co-occurrence 
probabilities from unlabelled text collected from 
a web crawler. Baroni and Bernardini (2004) 
built a corpus by iteratively searching Google for 
a small set of seed terms. Prototypes of Internet 
search engines for linguists, corpus linguists and 
lexicographers have been proposed: WebCorp 
(Kehoe and Renouf, 2002), KWiCFinder 
(Fletcher, 2004a) and the Linguist?s Search En-
gine (Kilgarriff, 2003; Resnik and Elkiss, 2003). 
A key concern in corpus linguistics and related 
disciplines is verifiability and replicability of the 
results of studies. Word frequency counts in 
internet search engines are inconsistent and unre-
liable (Veronis, 2005). Tools based on static cor-
pora do not suffer from this problem, e.g. 
BNCweb7, developed at the University of Zurich, 
and View 8  (Variation in English Words and 
Phrases, developed at Brigham Young University) 
 
4 http://www.comp.lancs.ac.uk/ucrel/claws/trial.html 
5 http://www.comp.leeds.ac.uk/amalgam/amalgam/ 
amalghome.htm 
6 http://www.connexor.com 
7 http://homepage.mac.com/bncweb/home.html 
8 http://view.byu.edu/ 
28
are both based on the British National Corpus. 
Both BNCweb and View enable access to anno-
tated corpora and facilitate searching on part-of-
speech tags. In addition, PIE9 (Phrases in Eng-
lish), developed at USNA, which performs 
searches on n-grams (based on words, parts-of-
speech and characters), is currently restricted to 
the British National Corpus as well, although 
other static corpora are being added to its data-
base. In contrast, little progress has been made 
toward annotating sizable sample corpora from 
the web. 
?Real-time? linguistic analysis of web data at 
the syntactic level has been piloted by the Lin-
guist?s Search Engine (LSE). Using this tool, 
linguists can either perform syntactic searches 
via parse trees on a pre-analysed web collection 
of around three million sentences from the Inter-
net Archive (www.archive.org) or build their 
own collections from AltaVista search engine 
results. The second method pushes the new col-
lection onto a queue for the LSE annotator to 
analyse. A new collection does not become 
available for analysis until the LSE completes 
the annotation process, which may entail signifi-
cant delay with multiple users of the LSE server. 
The Gsearch system (Corley et al, 2001) also 
selects sentences by syntactic criteria from large 
on-line text collections. Gsearch annotates cor-
pora with a fast chart parser to obviate the need 
for corpora with pre-existing syntactic mark-up. 
In contrast, the Sketch Engine system to assist 
lexicographers to construct dictionary entries 
requires large pre-annotated corpora. A word 
sketch is an automatic one-page corpus-derived 
summary of a word's grammatical and colloca-
tional behaviour. Word Sketches were first used 
to prepare the Macmillan English Dictionary for 
Advanced Learners (2002, edited by Michael 
Rundell). They have also served as the starting 
point for high-accuracy Word Sense Disam-
biguation. More recently, the Sketch Engine was 
used to develop the new edition of the Oxford 
Thesaurus of English (2004, edited by Maurice 
Waite). 
Parallelising or distributing processing has 
been suggested before. Clark and Curran?s (2004) 
work is in parallelising an implementation of 
log-linear parsing on the Wall Street Journal 
Corpus, whereas we focus on part-of-speech tag-
ging of a far larger and more varied web corpus, 
a technique more widely considered a prerequi-
site for corpus linguistics research. Curran (2003) 
                                                 
9 http://pie.usna.edu/ 
suggested distributed processing in terms of web 
services but only to ?allow components devel-
oped by different researchers in different loca-
tions to be composed to build larger systems? 
and not for parallel processing. Most signifi-
cantly, previous investigations have not exam-
ined three essential questions: how to apply dis-
tributed techniques to vast quantities of corpus 
data derived from the web, how to ensure that 
web-derived corpora are representative, and how 
to provide verifiability and replicability. These 
core foci of our work represent crucial innova-
tions lacking in prior research. In particular, rep-
resentativeness and replicability are key research 
concerns to enhance the reliability of web data 
for corpora. 
In the areas of Natural Language Processing 
(NLP) and computational linguistics, proposals 
have been made for using the computational Grid 
for data-intensive NLP and text-mining for e-
Science (Carroll et al, 2005; Hughes et al 2004). 
While such an approach promises much in terms 
of emerging infrastructure, we wish to exploit 
existing computing infrastructure that is more 
accessible to linguists via a P2P approach. In 
simple terms, P2P is a technology that takes ad-
vantage of the resources and services available at 
the edge of the Internet (Shirky, 2001). Better 
known for file-sharing and Instant Messenger 
applications, P2P has increasingly been applied 
in distributed computational systems. Examples 
include SETI@home (looking for radio evidence 
of extraterrestrial life), ClimatePrediction.net 
(studying climate change), Predictor@home (in-
vestigating protein-related diseases) and Ein-
stein@home (searching for gravitational signals).  
A key advantage of P2P systems is that they 
are lightweight and geared to personal computing 
where informal groups provide unused process-
ing power to solve a common problem. Typically, 
P2P systems draw upon the resources that al-
ready exist on a network (e.g. home or work 
PCs), thus keeping the cost to resource ratio low. 
For example the fastest supercomputer cost over 
$110 million to develop and has a peak perform-
ance of 12.3 TFLOPS (trillions of floating-point 
operations per second). In contrast, a typical day 
for the SETI@home project involved a perform-
ance of over 20 TFLOPS, yet cost only $700,000 
to develop; processing power is donated by user 
PCs. This high yield for low start-up cost makes 
it ideal for cheaply developing effective compu-
tational systems to realise, deploy and evaluate 
our framework. The deployment of computa-
tional based P2P systems is supported by archi-
29
tectures such as BOINC10, which provide a plat-
form on which volunteer based distributed com-
puting systems can be built. Lancaster's own P2P 
Application Framework (Walkerdine et al, sub-
mitted) also supports higher-level P2P applica-
tion development and can be adapted to make 
use of the BOINC architecture.  
3 Research hypothesis and aims 
Our research hypothesis is that distributed com-
putational techniques can alleviate the annotation 
bottleneck for processing corpus data from the 
web. This leads us to a number of research ques-
tions: 
? How can corpus data from the web be di-
vided into units for processing via distrib-
uted techniques? 
? Which corpus annotation techniques are 
suitable for distributed processing? 
? Can distributed techniques assist in corpus 
clean-up and conversion to allow inclu-
sion of a wider variety of genres and to 
support more representative corpora? 
In the early stages of our proposed research, 
we are focussing on grammatical word-class 
analysis (part-of-speech tagging) of web-derived 
corpora of English and aspects of corpus clean-
up and conversion. Clarifying copyright issues 
and exploring models for legal dissemination of 
corpora compiled from web data are key objec-
tives of this stage of the investigation as well. 
4 Methodology 
The initial focus of the work will be to develop 
the framework for distributed corpus annotation. 
Since existing solutions have been centralised in 
nature, we first must examine the consequences 
that a distributed approach has for corpus annota-
tion and identify issues to address. 
A key concern will be handling web pages 
within the framework, as it is essential to mini-
mise the amount of data communicated between 
peers. Unlike the other distributed analytical sys-
tems mentioned above, the size of text document 
and analysis time is largely proportional for cor-
pora annotation. This places limitations on work 
unit size and distribution strategies. In particular, 
three areas will be investigated: 
? Mechanisms for crawling/discovery of a 
web corpus domain - how to identify 
pages to include in a web corpus. Also 
                                                 
10 BOINC, Berkeley Open Infrastructure for Network 
Computing. http://boinc.berkeley.edu. 
investigate appropriate criteria for han-
dling pages which are created or modi-
fied dynamically.  
? Mechanisms to generate work units for 
distributed computation - how to split 
the corpus into work units and reduce the 
communication / computation time ratio 
that is crucial for such systems to be ef-
fective. 
? Mechanisms to support the distribution 
of work units and collection of results - 
how to handle load balancing. What data 
should be sent to peers and how is the 
processed information handled and ma-
nipulated? What mechanisms should be 
in place to ensure correctness of results?  
How can abuse be prevented and secu-
rity concerns of collaborating institutions 
be addressed?  BOINC already provides 
a good platform for this, and these as-
pects will be investigated within the pro-
ject. 
Analysis of existing distributed computation 
systems will help to inform the design of the 
framework and tackle some of these issues. Fi-
nally, the framework will also cater for three 
common strategies for corpus annotation: 
? Site based corpus annotation - in which 
the user can specify a web site to anno-
tate 
? Domain based corpus annotation - in 
which the user specifies a content do-
main (with the use of keywords) to  an-
notate 
? Crawler based corpus annotation - more 
general web based corpus annotation in 
which crawlers are used to locate web 
pages 
From a computational linguistic view, the 
framework will also need to take into account the 
granularity of the unit (for example, POS tagging 
requires sentence-units, but anaphoric annotation 
needs paragraphs or larger). Secondly, we need 
to investigate techniques for identifying identical 
documents, virtually identical documents and 
highly repetitive documents, such as those pio-
neered by Fletcher (2004b) and shingling tech-
niques described by Chakrabarti (2002).  
The second stage of our work will involve im-
plementing the framework within a P2P envi-
ronment. We have already developed a prototype 
of an object-oriented application environment to 
support P2P system development using JXTA 
(Sun's P2P API). We have designed this envi-
ronment so that specific application functionality 
30
can be captured within plug-ins that can then in-
tegrate with the environment and utilise its func-
tionality. This system has been successfully 
tested with the development of plug-ins support-
ing instant messaging, distributed video encoding 
(Hughes and Walkerdine, 2005), distributed vir-
tual worlds (Hughes et al, 2005) and digital li-
brary management (Walkerdine and Rayson, 
2004). It is our intention to implement our dis-
tributed corpus annotation framework as a plug-
in. This will involve implementing new func-
tionality and integrating this with our existing 
annotation tools (such as CLAWS11). The devel-
opment environment is also flexible enough to 
utilise the BOINC platform, and such support 
will be built into it. 
Using the P2P Application Framework as a 
basis for the development secures several advan-
tages. First, it reduces development time by al-
lowing the developer to reuse existing function-
ality; secondly, it already supports essential as-
pects such as system security; and thirdly, it has 
already been used successfully to deploy compa-
rable P2P applications. A lightweight version of 
the application framework will be bundled with 
the corpus annotation plug-in, and this will then 
be made publicly available for download in 
open-source and executable formats. We envis-
age our end-users will come from a variety of 
disciplines such as language engineering and lin-
guistics. For the less-technical users, the proto-
type will be packaged as a screensaver or instant 
messaging client to facilitate deployment. 
5 Evaluation 
We will evaluate the framework and prototype 
developed by applying it as a pre-processor step 
for the Sketch Engine system. The Sketch Engine 
requires a large well-balanced corpus which has 
been part-of-speech tagged and shallow parsed to 
find subjects, objects, heads, and modifiers. We 
will use the existing non-distributed processing 
tools on the Sketch Engine as a baseline for a 
comparative evaluation of the AWAC frame-
work instantiation by utilising processing power 
and bandwidth in learning labs at Lancaster Uni-
versity and USNA during off hours. 
We will explore techniques to make the result-
ing annotated web corpus data available in static 
form to enable replication and verification of 
corpus studies based on such data. The initial 
solution will be to store the resulting reference 
                                                 
                                                
11 http://www.comp.lancs.ac.uk/ucrel/claws/ 
corpus in the Sketch Engine. We will also inves-
tigate whether the distributed environment un-
derlying our approach offers a solution to the 
problem of reproducibility in web-based corpus 
studies based in general. Current practise else-
where includes the distribution of URL lists, but 
given the dynamic nature of the web, this is not 
sufficiently robust. Other solutions such as com-
plete caching of the corpora are not typically 
adopted due to legal concerns over copyright and 
redistribution of web data, issues considered at 
length by Fletcher (2004a). Other requirements 
for reference corpora such as retrieval and stor-
age of metadata for web pages are beyond the 
scope of what we propose here. 
To improve the representative nature of web-
derived corpora, we will research techniques to 
enable the importing of additional document 
types such as PDF. We will reuse and extend 
techniques implemented in the collection, encod-
ing and annotation of the PERC Corpus of Pro-
fessional English12. A majority of this corpus has 
been collected by conversion of on-line academic 
journal articles from PDF to XML with a combi-
nation of semi-automatic tools and techniques 
(including Adobe Acrobat version 6). Basic is-
sues such as character encoding, table/figure ex-
traction and maintaining text flow around em-
bedded images need to be dealt with before an-
notation processing can begin. We will compara-
tively evaluate our techniques against others such 
as pdf2txt, and Multivalent PDF ExtractText13. 
Part of the evaluation will be to collect and anno-
tate a sample corpus. We aim to collect a corpus 
from the web that is comparable to the BNC in 
content and annotation. This corpus will be 
tagged using the P2P framework. It will form a 
test-bed for the framework and we will utilise the 
non-distributed annotation system on the Sketch 
Engine as a baseline for comparison and evalua-
tion. To evaluate text conversion and clean-up 
routines for PDF documents, we will use a 5-
million-word gold-standard sub-corpus extracted 
 
12 The Corpus of Professional English (CPE) is a ma-
jor research project of PERC (the Professional Eng-
lish Research Consortium) currently underway that, 
when finished, will consist of a 100-million-word 
computerised database of English used by profession-
als in science, engineering, technology and other 
fields. Lancaster University and Shogakukan Inc. are 
PERC Member Institutions. For more details, see 
http://www.perc21.org/ 
13 http://multivalent.sourceforge.net/ 
31
from the PERC Corpus of Professional 
English14.  
                                                
6 Conclusion 
Future work includes an analysis of the balance 
between computational and bandwidth require-
ments. It is essential in distributing the corpus 
annotation to achieve small amounts of data 
transmission in return for large computational 
gains for each work-unit.  
In this paper, we have discussed the require-
ment for annotation of web-derived corpus data. 
Currently, a bottleneck exists in the tagging of 
web-derived corpus data due to the voluminous 
amount of corpus processing involved. Our pro-
posal is to construct a framework for large-scale 
distributed corpus annotation using existing peer-
to-peer technology. We have presented the chal-
lenges that lie ahead for such an approach. Work 
is now underway to address the clean-up of PDF 
data for inclusion into corpora downloaded from 
the web. 
Acknowledgements 
We wish to thank the anonymous reviewers who 
commented our paper. We are grateful to Shoga-
kukan Inc. (Tokyo, Japan) for supporting re-
search at Lancaster University into the process of 
conversion and clean-up of PDF to text, and to 
the Professional English Research Consortium 
for the provision of the gold-standard corpus for 
our evaluation. 
References  
Baroni, M. and Bernardini, S. (2004). BootCaT: 
Bootstrapping Corpora and Terms from the Web. 
In Proceedings of LREC2004, Lisbon, pp. 1313-
1316. 
Baroni, M. and Sharoff, S. (2005). Creating special-
ized and general corpora using automated search 
engine queries. Web as Corpus Workshop, Bir-
mingham University, UK, 14th July 2005. 
Carroll, J., R. Evans and E. Klein (2005) Supporting 
text mining for e-Science: the challenges for Grid-
enabled natural language processing. In Workshop 
on Text Mining, e-Research And Grid-enabled 
Language Technology at the Fourth UK e-Science 
Programme All Hands Meeting (AHM2005), Not-
tingham, UK. 
 
14 This corpus has already been manually re-typed at 
Shogakukan Inc. from PDF originals downloaded 
from the web. 
Chakrabarti, S. (2002) Mining the Web: Discovering 
Knowledge from Hypertext Data. Morgan Kauf-
mann. 
Clark, S. and Curran, J. R.. (2004). Parsing the wsj 
using ccg and log-linear models. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL ?04). 
Corley, S., Corley, M., Keller, F., Crocker, M., & 
Trewin, S. (2001). Finding Syntactic Structure in 
Unparsed Corpora: The Gsearch Corpus Query 
System. Computers and the Humanities, 35, 81-94. 
Curran, J.R. (2003). Blueprint for a High Performance 
NLP Infrastructure. In Proc. of Workshop on Soft-
ware Engineering and Architecture of Language 
Technology Systems (SEALTS) Edmonton, Canada, 
2003, pp. 40 ? 45. 
Edmonds, P and Kilgarriff, A. (2002). Introduction to 
the special issue on evaluating word sense disam-
biguation systems. Journal of Natural Language 
Engineering, 8 (2), pp. 279-291. 
Fletcher, W. H. (2001).  Concordancing the Web with 
KWiCFinder.  Third North American Symposium 
on Corpus Linguistics and Language Teaching, 
Boston, MA, 23-25 March 2001.  
Fletcher, W. H. (2004a). Facilitating the compilation 
and dissemination of ad-hoc Web corpora. In G. 
Aston, S. Bernardini and D. Stewart (eds.), Cor-
pora and Language Learners, pp. 271 ? 300, John 
Benjamins, Amsterdam. 
Fletcher, W. H. (2004b). Making the Web More Use-
ful as a Source for Linguistic Corpora. In Ulla 
Connor and Thomas A. Upton (eds.) Applied Cor-
pus Linguistics. A Multidimensional Perspective. 
Rodopi, Amsterdam, pp. 191 ? 205. 
Granger, S., and Rayson, P. (1998). Automatic profil-
ing of learner texts. In S. Granger (ed.) Learner 
English on Computer. Longman, London and New 
York, pp. 119-131. 
Hughes, B, Bird, S., Haejoong, K., and Klein, E. 
(2004). Experiments with data-intensive NLP on a 
computational grid. Proceedings of the Interna-
tional Workshop on Human Language Technology. 
http://eprints.unimelb.edu.au/archive/00000503/. 
Hughes, D., Gilleade, K., Walkerdine, J. and Mariani, 
J., Exploiting P2P in the Creation of Game Worlds. 
In the proceedings of ACM GDTW 2005, Liver-
pool, UK, 8th-9th November, 2005. 
Hughes, D. and Walkerdine, J. (2005), Distributed 
Video Encoding Over A Peer-to-Peer Network. In 
the proceedings of PREP 2005, Lancaster, UK, 
30th March - 1st April, 2005 
Kehoe, A. and Renouf, A. (2002) WebCorp: Applying 
the Web to Linguistics and Linguistics to the Web. 
32
World Wide Web 2002 Conference, Honolulu, Ha-
waii. 
Keller, F., Lapata, M. and Ourioupina, O. (2002). 
Using the Web to Overcome Data Sparseness. Pro-
ceedings of the Conference on Empirical Methods 
in Natural Language Processing, Philadelphia, 
July 2002, pp. 230-237. 
Kennedy, G. (1998). An introduction to corpus lin-
guistics. Longman, London. 
Kilgarriff, A. (2001). Web as corpus. In Proceedings 
of Corpus Linguistics 2001, Lancaster University, 
29 March - 2 April 2001, pp. 342 ? 344. 
Kilgarriff, A. (2003). Linguistic Search Engine. In 
proceedings of Workshop on Shallow Processing of 
Large Corpora (SProLaC 2003), Lancaster Uni-
versity, 28 - 31 March 2003, pp. 53 ? 58. 
Kilgarriff, A. and Grefenstette, G (2003). Introduction 
to the Special Issue on the Web as Corpus. Compu-
tational Linguistics, 29: 3, pp. 333-347. 
Mair, C. (2005). The corpus-based study of language 
change in progress: The extra value of tagged cor-
pora. Presentation at the AAACL/ICAME Confer-
ence, Ann Arbor, May 2005. 
Resnik, P. and Elkiss, A. (2003) The Linguist's Search 
Engine: Getting Started Guide. Technical Report: 
LAMP-TR-108/CS-TR-4541/UMIACS-TR-2003-
109, University of Maryland, College Park, No-
vember 2003. 
Robb, T. (2003) Google as a Corpus Tool? In ETJ 
Journal, Volume 4, number 1, Spring 2003. 
Rundell, M. (2000). "The biggest corpus of all", Hu-
manising Language Teaching. 2:3; May 2000. 
Shirky, C. (2001) Listening to Napster, in Peer-to-
Peer: Harnessing the power of Disruptive Tech-
nologies, O'Reilly. 
Turney, P. (2001). Word Sense Disambiguation by 
Web Mining for Word Co-occurrence Probabili-
ties. In proceedings of SENSEVAL-3, Barcelona, 
Spain, July 2004 pp. 239-242. 
Veronis, J. (2005). Web: Google's missing pages: 
mystery solved? 
http://aixtal.blogspot.com/2005/02/web-googles-
missing-pages-mystery.html (accessed April 28, 
2005). 
Walkerdine, J., Gilleade, K., Hughes, D., Rayson, P., 
Simms, J., Mariani, J., and Sommerville, I. A 
Framework for P2P Application Development. Pa-
per submitted to Software Practice and Experience. 
Walkerdine, J. and Rayson, P. (2004) P2P-4-DL: 
Digital Library over Peer-to-Peer. In Caronni G., 
Weiler N., Shahmehri N. (eds.) Proceedings of 
Fourth IEEE International Conference on Peer-to-
Peer Computing (PSP2004) 25-27 August 2004, 
Zurich, Switzerland. IEEE Computer Society 
Press, pp. 264-265. 
 
 
33
 34
Automatic Extraction of Chinese Multiword Expressions with a Statis-
tical Tool 
Scott S.L. Piao1 
s.piao@lancaster.ac.uk 
Guangfan Sun2 
morgan2001_sun@sohu.com 
Paul Rayson1 
paul@comp.lancs.ac.uk 
Qi Yuan2 
yq@trans.ccidnet.com 
 
1UCREL 
Computing Department 
Lancaster University 
Lancaster, UK 
2CIPOL 
China Centre for Information Industry De-
velopment (CCID) 
Beijing, China 
 
Abstract 
In this paper, we report on our experi-
ment to extract Chinese multiword ex-
pressions from corpus resources as part 
of a larger research effort to improve a 
machine translation (MT) system. For ex-
isting MT systems, the issue of multi-
word expression (MWE) identification 
and accurate interpretation from source to 
target language remains an unsolved 
problem. Our initial test on the Chinese-
to-English translation functions of 
Systran and CCID?s Huan-Yu-Tong MT 
systems reveal that, where MWEs are in-
volved, MT tools suffer in terms of both 
comprehensibility and adequacy of the 
translated texts. For MT systems to be-
come of further practical use, they need 
to be enhanced with MWE processing 
capability. As part of our study towards 
this goal, we test and evaluate a statistical 
tool, which was developed for English, 
for identifying and extracting Chinese 
MWEs. In our evaluation, the tool 
achieved precisions ranging from 61.16% 
to 93.96% for different types of MWEs.  
Such results demonstrate that it is feasi-
ble to automatically identify many Chi-
nese MWEs using our tool, although it 
needs further improvement. 
1 Introduction 
In real-life human communication, meaning is 
often conveyed by word groups, or meaning 
groups, rather than by single words. Very often, 
it is difficult to interpret human speech word by 
word. Consequently, for an MT system, it is im-
portant to identify and interpret accurate meaning 
of such word groups, or multiword expressions 
(MWE hereafter), in a source language and in-
terpret them accurately in a target language. 
However, accurate identification and interpreta-
tion of MWEs still remains an unsolved problem 
in MT research. 
In this paper, we present our experiment on 
identifying Chinese MWEs using a statistical 
tool for MT purposes. Here, by multiword ex-
pressions, we refer to word groups whose con-
stituent words have strong collocational relations 
and which can be translated in the target lan-
guage into stable translation equivalents, either 
single words or MWEs, e.g. noun phrases, 
prepositional phrases etc. They may include 
technical terminology in specific domains as well 
as more general fixed expressions and idioms. 
Our observations found that existing Chinese-
English MT systems cannot satisfactorily trans-
late MWEs, although some may employ a ma-
chine-readable bilingual dictionary of idioms. 
Whereas highly compositional MWEs may pose 
a trivial challenge to human speakers for inter-
pretation, they present a tough challenge for fully 
automatic MT systems to produce even remotely 
fluent translations. Therefore, in our context, we 
expand the concept of MWE to include those 
compositional ones which have relatively stable 
identifiable patterns of translations in the target 
language. 
By way of illustration of the challenge, we ex-
perimented with simple Chinese sentences con-
taining some commonly-used MWEs in 
SYSTRAN (http://www.systransoft.com/) and 
Huan-Yu-Tong (HYT henceforth) of CCID 
(China Centre for Information Industry Devel-
opment) (Sun, 2004). The former is one of the 
most efficient MT systems today, claiming to be 
?the leading provider of the world?s most scal-
able and modular translation architecture?, while 
the latter is one of the most successful MT sys-
tems in China. Table 1 shows the result, where 
SL and TL denote source and target languages 
respectively.. As shown by the samples, such 
17
highly sophisticated MT tools still struggle to 
produce adequate English sentences.. 
 
Chinese 
Sentences 
English 
(Systran) 
English 
(HYT) 
??????
???  ??
???? 
This afternoon 
can practice a 
ball game? I 
hope not to be 
able. 
Can practise a 
ball game this 
afternoon? I hope 
can not. 
??????
??????
???? 
You may not 
such do, let us 
pay respec-
tively each. 
You cannot do 
like that, and let 
us make it Dutch. 
??????
??????
??????
????? 
Perhaps does 
not have the 
means to let 
you sit shares a 
table, did you 
mind sits sepa-
rately? 
Perhaps no way 
out(ly) let you sit 
with table, are 
you situated be-
tween not mind 
to separate to sit? 
??????
??? 
Selects the 
milk coffee 
which ices. 
Ice breasts coffee 
take is selected. 
??????
??????
?? 
Good, I want 
the beer, again 
comes to select 
the coffee. 
Alright, I want 
beer, and take the 
coffee of order-
ing again. 
 
 
Table 1: Samples of Chinese-to-English transla-
tions of Systran and HYT. 
 
Ignoring the eccentric English syntactic struc-
tures these tools produced, we focus on the trans-
lations of Chinese MWEs (see the italic charac-
ters in the Table 1) which have straightforward 
expression equivalents in English. For example, 
in this context,  ???? can be translated into 
?hope not?, ??? into ?go Dutch?, ?? into 
?together? or ?at the same table?, ??? into 
?white coffee? or ?coffee with milk?, ??? 
into ?want some more (in addition to something 
already ordered)?. While these Chinese MWEs 
are highly compositional ones, when they are 
translated word by word, we see verbose and 
awkward translations (for correct translations, 
see the appendix). 
To solve such problems, we need algorithms 
and tools for identifying MWEs in the source 
language (Chinese in this case) and to accurately 
map them to their adequate translation equiva-
lents in the target language (English in our case) 
that are appropriate for given contexts. In the 
previous examples, an MT tool should be able to 
identify the Chinese MWE ???  and either 
provide the literal translation of ?pay for each? or 
map it to the more idomatic expressions of ?go 
Dutch?. 
Obviously, it would involve a wide range of 
issues and techniques for a satisfactory solution 
to this problem. In this paper, we focus on the 
sub-issue of automatically recognising and ex-
tracting Chinese MWEs. Specifically, we test 
and evaluate a statistical tool for automatic 
MWE extraction in Chinese corpus data. As the 
results of our experiment demonstrate, the tool is 
capable of identifying many MWEs with little 
language-specific knowledge. Coupled with an 
MT system, such a tool could be useful for ad-
dressing the MWE issue. 
2 Related Work  
The issue of MWE processing has attracted 
much attention from the Natural Language Proc-
essing (NLP) community, including Smadja, 
1993; Dagan and Church, 1994; Daille, 1995; 
1995; McEnery et al, 1997; Wu, 1997; Michiels 
and Dufour, 1998; Maynard and Ananiadou, 
2000; Merkel and Andersson, 2000; Piao and 
McEnery, 2001; Sag et al, 2001; Tanaka and 
Baldwin, 2003;  Dias, 2003; Baldwin et al, 
2003; Nivre and Nilsson, 2004 Pereira et al. 
2004; Piao et al, 2005. Study in this area covers 
a wide range of sub-issues, including MWE iden-
tification and extraction from monolingual and 
multilingual corpora, classification of MWEs 
according to a variety of viewpoints such as 
types, compositionality and alignment of MWEs 
across different languages. However studies in 
this area on Chinese language are limited. 
A number of approaches have been suggested, 
including rule-based and statistical approaches, 
and have achieved success to various extents. 
Despite this research, however, MWE processing 
still presents a tough challenge, and it has been 
receiving increasing attention, as exemplified by 
recent MWE-related ACL workshops. 
Directly related to our work is the develop-
ment of a statistical MWE tool at Lancaster for 
searching and identifying English MWEs in run-
ning text (Piao et al, 2003, 2005). Trained on 
corpus data in a given domain or genre, this tool 
can automatically identify MWEs in running text 
or extract MWEs from corpus data from the 
similar domain/genre (see further information 
about this tool in section 3.1). It has been tested 
and compared with an English semantic tagger 
(Rayson et al, 2004) and was found to be effi-
cient in identifying domain-specific MWEs in 
English corpora, and complementary to the se-
18
mantic tagger which relies on a large manually 
compiled lexicon. 
Other directly related work includes the de-
velopment of the HYT MT system at CCID in 
Beijing, China. It has been under development 
since 1991 (Sun, 2004) and it is one of the most 
successful MT systems in China. However, being 
a mainly rule-based system, its performance de-
grades when processing texts from domains pre-
viously unknown to its knowledge database. Re-
cently a corpus-based approach has been adopted 
for its improvement, and efforts are being made 
to improve its capability of processing MWEs.  
Our main interest in this study is in the appli-
cation of a MWE identification tool to the im-
provement of MT system. As far as we know, 
there has not been a satisfactory solution to the 
efficient handling of Chinese MWEs in MT sys-
tems, and our experiment contributes to a deeper 
understanding of this problem.  
3 Automatic Identification and extrac-
tion of Chinese MWEs  
In order to test the feasibility of automatic 
identification and extraction of Chinese MWEs 
on a large scale, we used an existing statistical 
tool built for English and a Chinese corpus built 
at CCID. A CCID tool is used for tokenizing and 
POS-tagging the Chinese corpus. The result was 
thoroughly manually checked by Chinese experts 
at CCID. In this paper, we aim to evaluate this 
existing tool from two perspectives a) its per-
formance on MWE extraction, and b) its per-
formance on a language other than English. In 
the following sections, we describe our experi-
ment in detail and discuss main issues that arose 
during the course of our experiment. 
3.1 MWE extraction tool 
The tool we used for the experiment exploits 
statistical collocational information between 
near-context words (Piao et al, 2005). It first 
collects collocates within a given scanning win-
dow, and then searches for MWEs using the col-
locational information as a statistical dictionary. 
As the collocational information can be extracted 
on the fly from the corpus to be processed for a 
reasonably large corpus, this process is fully 
automatic. To search for MWEs in a small cor-
pus, such as a few sentences, the tool needs to be 
trained on other corpus data in advance. 
With regards to the statistical measure of col-
location, the option of several formulae are 
available, including mutual information and log 
likelihood, etc. Our past experience shows that 
log-likelihood provides an efficient metric for 
corpus data of moderate sizes. Therefore it is 
used in our experiment. It is calculated as fol-
lows (Scott, 2001). 
For a given pair of words X and Y and a search 
window W, let a be the number of windows in 
which X and Y co-occur, let b be the number of 
windows in which only X occurs, let c be the 
number of windows in which only Y occurs, and 
let d be the number of windows in which none of 
them occurs, then 
 
G2 = 2 (alna + blnb + clnc + dlnd - (a+b)ln(a+b)  
        - (a+c)ln(a+c) - (b+d)ln(b+d)  
        - (c+d)ln(c+d)) + (a+b+c+d)ln(a+b+c+d)) 
 
In addition to the log-likelihood, the t-score is 
used to filter out insignificant co-occurrence 
word pairs (Fung and Church, 1994), which is 
calculated as follows: 
 
),(
1
)()(),(
ba
baba
WWprob
M
WprobWprobWWprob
t
?=  
 
In order to filter out weak collocates, a thresh-
old is often used, i.e. in the stage of collocation 
extraction, any pairs of items producing word 
affinity scores lower than a given threshold are 
excluded from the MWE searching process. Fur-
thermore, in order to avoid the noise caused by 
functional words and some extremely frequent 
words, a stop word list is used to filter such 
words out from the process. 
If the corpus data is POS-tagged, some simple 
POS patterns can be used to filter certain syntac-
tic patterns from the candidates. It can either be 
implemented as an internal part of the process, or 
as a post-process. In our case, such pattern filters 
are mostly applied to the output of the MWE 
searching tool in order to allow the tool to be 
language-independent as much as possible. 
Consequently, for our experiment, the major 
adjustment to the tool was to add a Chinese stop 
word list. Because the tool is based on Unicode, 
the stop words of different languages can be kept 
in a single file, avoiding any need for adjusting 
the program itself. Unless different languages 
involved happen to share words with the same 
form, this practice is safe and reliable. In our par-
ticular case, because we are dealing with English 
and Chinese, which use widely different charac-
ters, such a practice performs well. 
19
Another language-specific adjustment needed 
was to use a Chinese POS-pattern filter for se-
lecting various patterns of the candidate MWEs 
(see Table 6). As pointed out previously, it was 
implemented as a simple pattern-matching pro-
gram that is separate from the MWE tool itself, 
hence minimizing the modification needed for 
porting the tool from English to Chinese lan-
guage. 
A major advantage of this tool is its capability 
of identifying MWEs of various lengths which 
are generally representative of the given topic or 
domain. Furthermore, for English it was found 
effective in extracting domain-specific multi-
word terms and expressions which are not in-
cluded in manually compiled lexicons and dic-
tionaries. Indeed, due to the open-ended nature 
of such MWEs, any manually compiled lexicons, 
however large they may be, are unlikely to cover 
them exhaustively. It is also efficient in finding 
newly emerging MWEs, particularly technical 
terms, that reflect the changes in the real world. 
3.2 Experiment 
In this experiment, our main aim was to exam-
ine the feasibility of practical application of the 
MWE tool as a component of an MT system, 
therefore we used test data from some domains 
in which translation services are in strong de-
mand. We selected Chinese corpus data of ap-
proximately 696,000 tokenised words (including 
punctuation marks) which cover the topics of 
food, transportation, tourism, sports (including 
the Olympics) and business. 
In our experiment, we processed the texts 
from different topics together. These topics are 
related to each other under the themes of enter-
tainment and business. Therefore we assume, by 
mixing the data together, we could examine the 
performance of the MWE tool in processing data 
from a broad range of related domains. We ex-
pect that the different features of texts from dif-
ferent domains will have a certain impact on the 
result, but the examination of such impact is be-
yond the scope of this paper. 
As mentioned earlier, the Chinese word to-
keniser and POS tagger used in our experiment 
has been developed at CCID. It is an efficient 
tool running with accuracy of 98% for word to-
kenisation and 95% for POS annotation. It em-
ploys a part-of-speech tagset of 15 categories 
shown in Table 2. Although it is not a finely 
grained tagset, it meets the need for creating POS 
pattern filters for MWE extraction.  
 
N Name 
V Verb 
A Adjective 
F Adverb 
R Pronoun 
I Preposition 
J Conjunction 
U Number 
S classifier (measure word)  
G Auxiliary verb 
E Accessory word 
L directional noun 
P Punctuation 
H Onomatopoeia 
X Subject-predicate phrase 
  
Table 2: CCID Chinese tagset 
 
Since function words are found to cause noise 
in the process of MWE identification, a Chinese 
stop list was collected. First, a word frequency 
list was extracted. Next, the top items were con-
sidered and we selected 70 closed class words for 
the stop word list. When the program searches 
for MWEs, such words are ignored. 
The threshold of word affinity strength is an-
other issue to be addressed. In this experiment, 
we used log-likelihood to measure the strength of 
collocation between word pairs. Generally the 
log-likelihood score of 6.6 (p < 0.01 or 99% con-
fidence) is recommended as the threshold (Ray-
son et al, 2004), but it was found to produce too 
many false candidates in our case. Based on our 
initial trials, we used a higher threshold of 30, 
i.e. any word pairs producing log-likelihood 
score less than this value are ignored in the 
MWE searching process. Furthermore, for the 
sake of the reliability of the statistical score, 
when extracting collocates, a frequency threshold 
of five was used to filter out low-frequency 
words, i.e. word pairs with frequencies less than 
five were ignored. 
An interesting issue for us in this experiment 
is the impact of the length of collocation search-
ing window on the MWE identification. For this 
purpose, we tested two search window lengths 2 
and 3, and compared the results obtained by us-
ing them. Our initial hypothesis was that the 
shorter window length may produce higher pre-
cision while the longer window length may sacri-
fice precision but boost the MWE coverage. 
The output of the tool was manually checked 
by Chinese experts at CCID, including cross 
checking to guarantee the reliability of the re-
sults. There were some MWE candidates on 
which disagreements arose. In such cases, the 
20
candidate was counted as false. Furthermore, in 
order to estimate the recall, experts manually 
identified MWEs in the whole test corpus, so that 
the output of the automatic tool could be com-
pared against it.  In the following section, we 
present a detailed report on our evaluation of the 
MWE tool. 
3.3 Evaluation 
We first evaluated the overall precision of the 
tool. A total of 7,142 MWE candidates (types) 
were obtained for window lengths of 2, of which 
4,915 were accepted as true MWEs, resulting in 
a precision of 68.82%. On the other hand, a total 
of 8,123 MWE candidates (types) were obtained 
for window lengths of 3, of which 4,968 were 
accepted as true MWEs, resulting in a precision 
of 61.16%. This result is in agreement with our 
hypothesis that shorter search window length 
tends to produce higher precision. 
Next, we estimated the recall based on the 
manually analysed data. When we compared the 
accepted MWEs from the automatic result 
against the manually collected ones, we found 
that the experts tend to mark longer MWEs, 
which often contain the items identified by the 
automatic tool. For example, the manually 
marked MWE ?? ?? ?? ?? (develop-
ment plan for the tennis sport) contains shorter 
MWEs ?? ?? (tennis sport) and?? ?? 
(development plan) which were identified by the 
tool separately. So we decided to take the partial 
matches into account when we estimate the re-
call. We found that a total 14,045 MWEs were 
manually identified and, when the search win-
dow length was set to two and three, 1,988 and 
2,044 of them match the automatic output, pro-
ducing recalls of 14.15% and 14.55% respectively. 
It should be noted that many of the manually ac-
cepted MWEs from the automatic output were 
not found in the manual MWE collection. This 
discrepancy was likely caused by the manual 
analysis being carried out independently of the 
automatic tool, resulting in a lower recall than 
expected. Table 3 lists the precisions and recalls. 
 
Window length = 2 Window length = 3 
Precision Recall Precision Recall 
68.82% 14.15% 61.16% 14.55% 
 
Table 3: Overall precisions and recalls 
 
Furthermore, we evaluated the performance of 
the MWE tool from two aspects: frequency and 
MWE pattern. 
Generally speaking, statistical algorithms 
work better on items of higher frequency as it 
depends on the collocational information. How-
ever, our tool does not select MWEs directly 
from the collocates. Rather, it uses the colloca-
tional information as a statistical dictionary and 
searches for word sequences whose constituent 
words have significantly strong collocational 
bonds between them. As a result, it is capable of 
identifying many low-frequency MWEs. Table 4 
lists the breakdown of the precision for five fre-
quency bands (window length = 2). 
 
Freq Candidates True MWEs Precision 
>= 100 17 9 52.94% 
10 ~ 99 846 646 76.36% 
3 ~ 9 2,873 2,178 75.81% 
2 949 608 64.07% 
1 2,457 1,474 59.99% 
Total 7,142 4,915 68.82% 
 
Table 4: Breakdown of precision for frequencies 
(window length = 2). 
 
As shown in the table above, the highest preci-
sions were obtained for the frequency range be-
tween 3 and 99. However, 2,082 of the accepted 
MWEs have frequencies of one or two, account-
ing for 42.36% of the total accepted MWEs. 
Such a result demonstrates again that our tool is 
capable of identifying low-frequency items. An 
interesting result is for the top frequency band 
(greater than 100). Against our general assump-
tion that higher frequency brings higher preci-
sion, we saw the lowest precision in the table for 
this band. Our manual examination reveals this 
was caused by the high frequency numbers, such 
as ?one? or ?two? in the expressions ???? 
(a/one) and ???? ( a kind of). This type of ex-
pression were classified as uninteresting candi-
dates in the manual checking, resulting in higher 
error rates for the high frequency band. 
When we carry out a parallel evaluation for 
the case of searching window length of 3, we see 
a similar distribution of precision across the fre-
quency bands except that the lowest frequency 
band has the lowest precision, as shown by Table 
5. When we compare this table against Table 4, 
we can see, for all of the frequency bands except 
the top one, that the precision drops as the search 
window increases. This further supports our ear-
lier assumption that wider searching window 
tends to reduce the precision. 
 
 
21
Freq candidates true MWEs Precision 
>= 100 17 9 52.94% 
10 ~ 99 831 597 71.84% 
3 ~ 9 3,093 2,221 71.81% 
2 1,157 669 57.82% 
1 3,025 1,472 48.66% 
Total 8,123 4,968 61.16% 
 
Table 5: Breakdown of precision for frequencies 
(window length = 3). 
 
In fact, not only the top frequency band, much 
of the errors of the total output were found to be 
caused by the numbers that frequently occur in 
the test data, e.g. ?_U ?_S (one), ?_U ?_S 
(two) etc. When a POS filter was used to filter 
them out, for the window length 2, we obtained a 
total 5,660 candidates, of which 4,386 were ac-
cepted as true MWEs, producing a precision of 
77.49%. Similarly for the window length 3, a 
total of 6,526 candidates were extracted in this 
way and 4,685 of them were accepted as true 
MWEs, yielding a precision of 71.79%. 
Another factor affecting the performance of 
the tool is the type of MWEs. In order to exam-
ine the potential impact of MWE types to the 
performance of the tool, we used filters to select 
MWEs of the following three patterns: 
1) AN: Adjective + noun structure; 
2) NN: Noun + noun Structure; 
3) FV: Adverb + Verb. 
Table 6 lists the precision for each of the 
MWE types and for search window lengths of 2 
and 3. 
 
Search window length = 2 
Pattern Candidate True MWEs Precision 
A+N 236 221 93.64% 
N+N 644 589 91.46% 
F+V 345 321 93.04% 
total 1,225 1,131 92.33% 
Search window length = 3 
Pattern Candidate True MWEs Precision 
A+N 259 233 89.96% 
N+N 712 635 89.19% 
F+V 381 358 93.96% 
Total 1,352 1,226 90.68% 
 
Table 6: Precisions for three types of MWEs 
 
As shown in the table, the MWE tool achieved 
high precisions above 91% when we use a search 
window of two words. Even when the search 
window expands to three words, the tool still 
obtained precision around 90%. In particular, the 
tool is efficient for the verb phrase type. Such a 
result demonstrates that, when we constrain the 
search algorithm to some specific types of 
MWEs, we can obtain higher precisions. While 
one may argue that rule-based parser can do the 
same work, it must be noted that we are not in-
terested in all grammatical phrases, but those 
which reflect the features of the given domain. 
This is achieved by combining statistical word 
collocation measures, a searching strategy and 
simple POS pattern filters. 
Another interesting finding in our experiment 
is that our tool extracted clauses, such as???
?? (What would you like to drink?) and??
???? (Would you like a drink first?). The 
clauses occur only once or twice in the entire test 
data, but were recognized by the tool because of 
the strong collocational bond between their con-
stituent words. The significance of such per-
formance is that such clauses are typical expres-
sions which are frequently used in real-life con-
versation in the contexts of the canteen, tourism 
etc. Such a function of our tool may have practi-
cal usage in automatically collecting longer typi-
cal expressions for the given domains. 
4 Discussion 
As our experiment demonstrates, our tool pro-
vides a practical means of identifying and ex-
tracting domain specific MWEs with a minimum 
amount of linguistic knowledge. This becomes 
important in multilingual tasks in which it can be 
costly and time consuming to build comprehen-
sive rules for several languages. In particular, it 
is capable of detecting MWEs of various lengths, 
sometimes whole clauses, which are often typical 
of the given domains of the corpus data. For ex-
ample, in our experiment, the tool successfully 
identified several daily used long expressions in 
the domain of food and tourism. MT systems 
often suffer when translating conversation. An 
efficient MWE tool can potentially alleviate the 
problem by extracting typical clauses used in 
daily life and mapping them to adequate transla-
tions in the target language. 
 Despite the flexibility of the statistical tool, 
however, there is a limit to its performance in 
terms of precision. While it is quite efficient in 
providing MWE candidates, its output has to be 
either verified by human or refined by using lin-
guistic rules. In our particular case, we improved 
the precision of our tool by employing simple 
POS pattern filters. Another limitation of this 
tool is that currently it can only recognise con-
tinuous MWEs. A more flexible searching algo-
22
rithm is needed to identify discontinuous MWEs, 
which are important for NLP tasks. 
Besides the technical problem, a major unre-
solved issue we face is what constitutes MWEs. 
Despite agreement on the core MWE types, such 
as idioms and highly idiosyncratic expressions, 
like ?? (Cheng-Yu) in Chinese, it is difficult to 
reach agreement on less fixed expressions. 
We contend that MWEs may have different 
definitions for different research purposes. For 
example, for dictionary compilation, lexicogra-
phers tend to constrain MWEs to highly non-
compositional expressions (Moon, 1998: 18). 
This is because monolingual dictionary users can 
easily understand compositional MWEs and 
there is no need to include them in a dictionary 
for native speakers. For lexicon compilation 
aimed at practical NLP tasks, however, we may 
apply a looser definition of MWEs. For example, 
in the Lancaster semantic lexicon (Rayson et al, 
2004), compositional word groups such as 
?youth club? are considered as MWEs alongside 
non-compositional expressions such as ?food for 
thought? as they depict single semantic units or 
concepts. Furthermore, for the MT research 
community whose primary concern is cross-
language interpretation, any multiword units that 
have stable translation equivalent(s) in a target 
language can be of interest. 
As we discussed earlier, a highly idiomatic 
expression in a language can be translated into a 
highly compositional expression in another lan-
guage, and vice versa. In such situations, it can 
be more practically useful to identify and map 
translation equivalents between the source and 
target languages regardless of their level of com-
positionality.  
Finally, the long Chinese clauses identified by 
the tool can potentially be useful for the im-
provement of MT systems. In fact, most of them 
are colloquial expressions in daily conversation, 
and many such Chinese expressions are difficult 
to parse syntactically. It may be more feasible to 
identify such expressions and map them as a 
whole to English equivalent expressions. The 
same may apply to technical terms, jargon and 
slang. In our experiment, our tool demonstrated 
its capability of detecting such expressions, and 
will prove useful in this regard. 
5 Conclusion 
In this paper, we have reported on our experi-
ment of automatic extraction of Chinese MWEs 
using a statistical tool originally developed for 
English. Our statistical tool produced encourag-
ing results, although further improvement is 
needed to become practically applicable for MT 
system in terms of recall. Indeed, for some con-
strained types of MWEs, high precisions above 
90% have been achieved. This shows, enhanced 
with some linguistic filters, it can provide a prac-
tically useful tool for identifying and extracting 
MWEs. Furthermore, in our experiment, our tool 
demonstrated its capability of multilingual proc-
essing. With only minor adjustment, it can be 
ported to other languages. Meanwhile, further 
study is needed for a fuller understanding of the 
factors affecting the performance of statistical 
tools, including the text styles and topic/domains 
of the texts, etc.   
Acknowledgement 
This work was supported by the National Natural 
Science Foundation of China (grant no. 
60520130297) and the British Academy (grant 
no. SG-42140). 
References 
Biber, D., Conrad, S., Cortes, V., 2003. Lexical bun-
dles in speech and writing: an initial taxonomy. In: 
Wilson, A., Rayson P., McEnery, T. (Eds.), Corpus 
Linguistics by the Lune: A Festschrift for Geoffrey 
Leech. Peter Lang, Frankfurt. pp. 71-92.   
Baldwin, T., Bannard, C., Tanaka, T. and Widdows, 
D. 2003 An Empirical Model of Multiword Ex-
pression Decomposability, In Proceedings of the 
ACL-2003 Workshop on Multiword Expressions: 
Analysis, Acquisition and Treatment, Sapporo, Ja-
pan, pp. 89?96. 
Dagan, I., Church, K., 1994. Termight: identifying 
and translating technical terminology. In: Proceed-
ings of the 4th Conference on Applied Natural 
Language Processing, Stuttgart, German. pp. 34-
40. 
Daille, B., 1995. Combined approach for terminology 
extraction: lexical statistics and linguistic filtering. 
Technical paper 5, UCREL, Lancaster University. 
Dias, G., 2003. Multiword unit hybrid extraction. In: 
Proceedings of the Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment, at 
ACL'03, Sapporo, Japan. pp. 41-48. 
Dunning, T., 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational Lin-
guistics 19 (1), 61-74. 
Fung, P., Church, K., 1994. K-vec: a new approach 
for aligning parallel texts. In: Proceedings of COL-
ING '94, Kyoto, Japan. pp. 1996-2001. 
23
Maynard, D., Ananiadou, S., 2000. Trucks: a model 
for automatic multiword term recognition. Journal 
of Natural Language Processing 8 (1), 101-126. 
McEnery, T., Lange, J. M., Oakes, M., Vernonis, J.., 
1997. The exploitation of multilingual annotated 
corpora for term extraction. In: Garside, R., Leech, 
G., McEnery, A. (Eds.), Corpus Annotation --- 
Linguistic Information from Computer Text Cor-
pora. Longman,  London & New York. pp 220-
230. 
Merkel, M., Andersson, M., 2000. Knowledge-lite 
extraction of multi-word units with language filters 
and entropy thresholds. In: Proceedings of 2000 
Conference User-Oriented Content-Based Text and 
Image Handling (RIAO'00), Paris, France. pp. 737-
746. 
Michiels, A., Dufour, N., 1998. DEFI, a tool for 
automatic multi-word unit recognition, meaning 
assignment and translation selection. In: Proceed-
ings of the First International Conference on Lan-
guage Resources & Evaluation, Granada, Spain. 
pp. 1179-1186. 
Moon, R. 1998. Fixed expressions and idioms in Eng-
lish: a corpus-based approach. Clarendon Press: 
Oxford. 
Nivre, J., Nilsson, J., 2004. Multiword units in syntac-
tic parsing. In: Proceedings of LREC-04 Workshop 
on Methodologies & Evaluation of Multiword 
Units in Real-world Applications, Lisbon, Portugal. 
pp. 37-46. 
Pereira, R., Crocker, P., Dias, G., 2004. A parallel 
multikey quicksort algorithm for mining multiword 
units. In: Proceedings of LREC-04 Workshop on 
Methodologies & Evaluation of Multiword Units in 
Real-world Applications, Lisbon, Portugal. pp. 17-
23. 
Piao, S. L., Rayson, P., Archer, D. and McEnery, T. 
2005. Comparing and Combining A Semantic Tag-
ger and A Statistical Tool for MWE Extraction. 
Computer Speech & Language Volume 19, Issue 4,  
pp. 378-397. 
Piao, S.L , Rayson, P., Archer, D., Wilson, A. and 
McEnery, T. 2003. Extracting multiword expres-
sions with a semantic tagger. In Proceedings of the 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, at ACL'03, Sapporo, 
Japan, pp. 49-56. 
Piao, S., McEnery, T., 2001. Multi-word unit align-
ment in English-Chinese parallel corpora. In: Pro-
ceedings of the Corpus Linguistics 2001, Lancas-
ter, UK. pp. 466-475. 
Rayson, P., Archer, D., Piao, S. L., McEnery, T. 
2004. The UCREL semantic analysis system. In 
proceedings of the workshop on Beyond Named 
Entity Recognition Semantic labelling for NLP 
tasks in association with LREC 2004, Lisbon, Por-
tugal, pp. 7-12. 
Rayson, P., Berridge, D. and Francis, B. 2004. Ex-
tending the Cochran rule for the comparison of 
word frequencies between corpora. In Proceedings 
of the 7th International Conference on Statistical 
analysis of textual data (JADT 2004), Louvain-la-
Neuve, Belgium. pp. 926-936. 
Sag, I., Baldwin, T., Bond, F., Copestake, A., Dan, F., 
2001. Multiword expressions: a pain in the neck 
for NLP. LinGO Working Paper No. 2001-03, 
Stanford University, CA. 
Scott, M., 2001. Mapping key words to problem and 
solution. In: Scott, M., Thompson, G. (Eds.), Pat-
terns of Text: in Honour of Michael Hoey. Benja-
mins, Amsterdam. pp. 109 ? 127. 
Smadja, F., 1993. Retrieving collocations from text: 
Xtract. Computational Linguistics 19 (1), 143-177. 
Sun, G. 2004. Design of an Interlingua-Based Chi-
nese-English Machine Translation System. In Pro-
ceedings of the 5th China-Korea Joint Symposium 
on Oriental Language Processing and Pattern Rec-
ognition, Qingdao, China. pp. 129-134. 
Tanaka, T., Baldwin, T., 2003. Noun-noun compound 
machine translation: a feasibility study on shallow 
processing. In: Proceedings of the ACL-03 Work-
shop on Multiword Expressions: Analysis, Acquisi-
tion and Treatment, Sapporo, Japan. pp. 17-24. 
Wu, D., 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics 23 (3), 377-401. 
Appendix: English translations of the 
sample Chinese sentences 
1. ????????? ?????? 
Tran: Do we have (football) training this af-
ternoon? I hope not. 
2. ???????????????? 
Tran: You can?t do that. Let?s go Dutch. 
3. ?????????????????
?????? 
Tran: I am afraid I can?t arrange for you to sit 
at the same table. Would you mind if 
you sit separately? 
4. ????????? 
Tran: I?d like iced white coffee (please). 
5. ?????????????? 
Tran: OK, I want beer and some coffee 
(please). 
24
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 64?71,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Using a Keyness Metric for Single and Multi Document Summarisation
Mahmoud El-Haj
School of Computing and
Communications
Lancaster University
United Kingdom
m.el-haj@lancaster.ac.uk
Paul Rayson
School of Computing and
Communications
Lancaster University
United Kingdom
p.rayson@lancaster.ac.uk
Abstract
In this paper we show the results of
our participation in the MultiLing 2013
summarisation tasks. We participated
with single-document and multi-document
corpus-based summarisers for both Ara-
bic and English languages. The sum-
marisers used word frequency lists and
log likelihood calculations to generate sin-
gle and multi document summaries. The
single and multi summaries generated by
our systems were evaluated by Arabic
and English native speaker participants
and by different automatic evaluation met-
rics, ROUGE, AutoSummENG, MeMoG
and NPowER. We compare our results to
other systems that participated in the same
tracks on both Arabic and English lan-
guages. Our single-document summaris-
ers performed particularly well in the auto-
matic evaluation with our English single-
document summariser performing better
on average than the results of the other
participants. Our Arabic multi-document
summariser performed well in the human
evaluation ranking second.
1 Introduction
Systems that can automatically summarise docu-
ments are becoming ever more desirable with the
increasing volume of information available on the
Web. Automatic text summarisation is the process
of producing a shortened version of a text by the
use of computers. For example, reducing a text
document or a group of related documents into a
shorter version of sentences or paragraphs using
automated tools and techniques.
The summary should convey the key contri-
butions of the text. In other words, only key
sentences should appear in the summary and the
process of defining those sentences is highly de-
pendent on the summarisation method used. In
automatic summarisation there are two main ap-
proaches that are broadly used, extractive and ab-
stractive. The first method, the extractive sum-
marisation, extracts, up to a certain limit, the
key sentences or paragraphs from the text and or-
ders them in a way that will produce a coherent
summary. The extracted units differ from one
summariser to another. Most summarisers use
sentences rather than larger units such as para-
graphs. Extractive summarisation methods are
the focus method on automatic text summarisa-
tion. The other method, abstractive summarisa-
tion, involves more language dependent tools and
Natural Language Generation (NLG) technology.
In our work we used extractive single and multi-
document Arabic and English summarisers.
A successful summarisation approach needs a
good guide to find the most important sentences
that are relevant to a certain criterion. Therefore,
the proposed methods should work on extracting
the most important sentences from a set of related
articles.
In this paper we present the results of our par-
ticipation to the MultiLing 2013 summarisation
tasks. MultiLing 2013 was built upon the Text
Analysis Conference (TAC) MultiLing Pilot task
of 2011 (Giannakopoulos et al, 2011). MultiL-
ing 2013 this year asked for participants to run
their summarisers on different languages having a
corpus and gold standard summaries in the same
seven languages (Arabic, Czech, English, French,
Greek, Hebrew or Hindi) of TAC 2011 with a
50% increase to the corpora size. It also intro-
duced three new languages (Chinese, Romanian
and Spanish). MultiLing 2013 this year intro-
duced a new single-document summarisation pilot
for 40 languages including the above mentioned
languages (in our case Arabic and English).
In this paper we introduce the results of our
64
single-document and multi-document summaris-
ers at the MultiLing 2013 summarisation tasks.
We used a language independent corpus-based
word frequency technique and the log-likelihood
statistic to extract sentences with the maximum
sum of log likelihood. The output summary is ex-
pected to be no more than 250 words.
2 Related Work
2.1 Automatic Summarisation
Work on automatic summarisation dates back
more than 50 years, with a focus on the English
language (Luhn, 1958). The work on Arabic au-
tomatic summarisation is more recent and still not
on par with the research on English and other Eu-
ropean languages. Early work on Arabic summari-
sation started less than 10 years ago (Conroy et al,
2006; Douzidia and Lapalme, 2004).
Over time, there have been various approaches
to automatic text summarisation. These ap-
proaches include single-document and multi-
document summarisation. Both single-document
and multi-document summarisation use the sum-
marisation methods mentioned earlier, i.e. ex-
tractive or abstractive. Summarising a text could
be dependent on input information such as a user
query or it could be generic where no user query
is used.
The approach of single-document summarisa-
tion relies on the idea of producing a summary
for a single document. The main factor in single-
document summarisation is to identify the most
important (informative) parts of a document. Early
work on single-document summarisation was the
work by Luhn (1958). In his work he looked
for sentences containing keywords that are most
frequent in a text. The sentences with highly
weighted keywords were selected. The work by
Luhn highlighted the need for features that reflect
the importance of a certain sentence in a text. Bax-
endale (1958) showed the importance of sentence-
position in a text, which is understood to be one
of the earliest extracted features in automatic text
summarisation. They took a sample of 200 para-
graphs and found that in 80% of the paragraphs
the most important sentence was the first one.
Multi-document summarisation produces a sin-
gle summary of a set of documents. The docu-
ments are assumed to be about the same genre and
topic. The analysis in this area is performed typi-
cally at either the sentence or document level.
2.2 Corpus-based and Word Frequency in
Summarisation
Corpus-based techniques are mainly used to com-
pare corpora for linguistic analysis (Rayson and
Garside, 2000; Rayson et al, 2004). There are
two main types of corpora comparisons, 1) com-
paring a sample corpus with a larger standard
corpus (Scott, 2000). 2) comparing two corpora
of equal size (Granger, 1998). In our work we
adopted the first approach, where we used a much
larger reference corpus. The first word list is the
frequency list of all the words in the document (or
group of documents) to be summarised which is
compared to the word frequency list of a much
larger standard corpus. We do that for both Ara-
bic and English texts. Word frequency has been
proven as an important feature when determining
a sentence?s importance (Li et al, 2006). Nenkova
and Vanderwende (2005) studies the impact of fre-
quency on summarisation. In their work they in-
vestigated the association between words that ap-
pear frequently in a document (group of related
documents), and the likelihood that they will be
selected by a human summariser to be included in
a summary. Taking the top performing summaris-
ers at the DUC 20031 they computed how many of
the top frequency words from the input documents
appeared in the system summaries. They found the
following: 1) Words with high frequency in the
input documents are very likely to appear in the
human summaries. 2) The automatic summaris-
ers include less of these high frequency words.
These two findings by Nenkova and Vanderwende
(2005) tell us two important facts. Firstly, it con-
firms that word frequency is an important factor
that impacts humans? decisions on which content
to include in the summary. Secondly, the overlap
between human and system summaries can be im-
proved by including more of the high frequency
words in the generated system summaries. Based
on Nenkova?s study we expand the work on word
frequency by comparing word frequency lists of
different corpora in a way to select sentences with
the maximum sum of log likelihood ratio. The log-
likelihood calculation favours words whose fre-
quencies are unexpectedly high in a document.
2.3 Statistical Summarisation
The use of statistical approaches (e.g. log-
likelihood) in text summarisation is a common
1http://duc.nist.gov/duc2003/tasks.html
65
technique, especially when building a language in-
dependent text summariser.
Morita et al (2011) introduced what they called
?query-snowball?, a method for query-oriented
extractive multi-document summarisation. They
worked on closing the gap between the query and
the relevant sentences. They formulated the sum-
marisation problem based on word pairs as a max-
imum cover problem with Knapsack Constraints
(MCKP), which is an optimisation problem that
maximises the total score of words covered by a
summary within a certain length limit.
Knight and Marcu (2000) used the Expectation
Maximisation (EM) algorithm to compress sen-
tences for an abstractive text summarisation sys-
tem. EM is an iterative method for finding Maxi-
mum Likelihood (ML) or Maximum A Posteriori
(MAP) estimates of parameters in statistical mod-
els. In their summariser, EM was used in the sen-
tences compression process to shorten many sen-
tences into one by compressing a syntactic parse
tree of a sentence in order to produce a shorter but
maximally grammatical version. Similarly, Mad-
nani et al (2007) performed multi-document sum-
marisation by generating compressed versions of
source sentences as summary candidates and used
weighted features of these candidates to construct
summaries.
Hennig (2009) introduced a query-based la-
tent Semantic Analysis (LSA) automatic text sum-
mariser. It finds statistical semantic relationships
between the extracted sentences rather than word
by word matching relations (Hofmann, 1999).
The summariser selects sentences with the highest
likelihood score.
In our work we used log-likelihood to select
sentences with the maximum sum of log likeli-
hood scores, unlike the traditional method of mea-
suring cosine similarity overlap between articles
or sentences to indicate importance (Luhn, 1958;
Barzilay et al, 2001; Radev et al, 2004). The
main advantage of our approach is that the auto-
matic summariser does not need to compare sen-
tences in a document with an initial one (e.g. first
sentence or a query). Our approach works by cal-
culating the keyness (or log-likelihood) score for
each token (word) in a sentence, then picks, to a
limit of 250 words, the sentences with the highest
sum of the tokens? log-likelihood scores.
To the best of our knowledge the use of corpus-
based frequency list to calculate the log-likelihood
score for text summarisation has not been reported
for the Arabic language.
3 Dataset and Evaluation Metrics
3.1 Test Collection
The test collection for the MultiLing 2013 is avail-
able in the previously mentioned languages.2 The
dataset is based on WikiNews texts.3 The source
documents contain no meta-data or tags and are
represented as UTF8 plain text les. The multi-
document dataset of each language contains (100-
150) articles divided into 10 or 15 reference sets,
each contains 10 related articles discussing the
same topic. The original language of the dataset
is English. The organisers of the tasks were re-
sponsible for translating the corpus into differ-
ent languages by having native speaker partici-
pants for each of the 10 languages. In addi-
tion to the news articles the dataset alo provides
human-generated multi-document gold standard
summaries. The single-document dataset contains
single documents for 40 language (30 documents
each) discussing various topics and collected from
Wikipedia.4
3.2 Evaluation
Evaluating the quality and consistency of a gen-
erated summary has proven to be a difficult prob-
lem (Fiszman et al, 2009). This is mainly because
there is no obvious ideal, objective summary. Two
classes of metrics have been developed: form met-
rics and content metrics. Form metrics focus on
grammaticality, overall text coherence, and organ-
isation. They are usually measured on a point
scale (Brandow et al, 1995). Content metrics are
more difficult to measure. Typically, system out-
put is compared sentence by sentence or unit by
unit to one or more human-generated ideal sum-
maries. As with information retrieval, the per-
centage of information presented in the system?s
summary (precision) and the percentage of impor-
tant information omitted from the summary (re-
call) can be assessed. There are various mod-
els for system evaluation that may help in solving
this problem. This include automatic evaluations
(e.g. ROUGE and AutoSummENG), and human-
performed evaluations. For the MultiLing 2013
task, the summaries generated by the participants
2http://multiling.iit.demokritos.gr/file/all
3http://www.wikinews.org/
4http://www.wikipedia.org/
66
were evaluated automatically based on human-
generated model summaries provided by fluent
speakers of each corresponding language (native
speakers in the general case). The models used
were, ROUGE variations (ROUGE1, ROUGE2,
ROUGE-SU4) (Lin, 2004), the MeMoG varia-
tion (Giannakopoulos and Karkaletsis, 2011) of
AutoSummENG (Giannakopoulos et al, 2008)
and NPowER (Giannakopoulos and Karkaletsis,
2013). ROUGE was not used to evaluate the
single-document summaries.
The summaries were also evaluated manually
by human participants. For the manual evalua-
tion the human evaluators were provided with the
following guidelines: Each summary is to be as-
signed an integer grade from 1 to 5, related to the
overall responsiveness of the summary. We con-
sider a text to be worth a 5, if it appears to cover
all the important aspects of the corresponding doc-
ument set using fluent, readable language. A text
should be assigned a 1, if it is either unreadable,
nonsensical, or contains only trivial information
from the document set. We consider the content
and the quality of the language to be equally im-
portant in the grading.
Note, the human evaluation results for the En-
glish language are not included in this paper as by
the time of writing the results were not yet pub-
lished. We only report the human evaluation re-
sults of the Arabic multi-document summaries.
4 Corpus-based Summarisation
Our summarisation approach is a corpus-based
where we use word frequency lists to compare cor-
pora and calculate the log likelihood score for each
word in the list. The compared corpora include
standard Arabic and English corpora in addition
to the Arabic and English summarisation datasets
provided by MultiLing 2013 for the single and
multi-document summarisation tasks. The subsec-
tions below describe the creation of the word lists
and the standard corpora we used for the compar-
ison process.
4.1 Word Frequencies
We used a simple methodology to generate the
word frequency lists for the Arabic and English
summarisation datasets provided by MultiLing
2013. The datasets used in our experiments were
single-document and multi-document documents
in English and Arabic. For the multi-document
(a) Arabic Sample (b) English Sample
Figure 1: Arabic and English Word Frequency List
Sample
dataset we counted the word frequency for all the
documents in a reference set (group of related arti-
cles), each set contains on average 10 related arti-
cles. The single-document dataset was straightfor-
ward, we calculated word frequencies for all the
words in each document. Figure 1 shows a sam-
ple of random words and their frequencies for both
Arabic and English languages. The sample was se-
lected from the MultiLing dataset word frequency
lists. As shown in the figure we did not eliminate
the stop-words, we treat them as normal words.
4.2 Standard Corpora
In our work we compared the word frequency list
of the summarisation dataset against the larger
Arabic and English standard corpora. For each
of the standard corpora we had a list of word fre-
quencies (up to 5, 000 words) for both Arabic and
English using the frequency dictionary of Ara-
bic (Buckwalter and Parkinson, 2011) and the Cor-
pus of Contemporary American English (COCA)
top 5,000 words (Davies, 2010).
The frequency dictionary of Arabic provides a
list of the 5,000 most frequently used words in
Modern Standard Arabic (MSA) in addition to
several of the most widely spoken Arabic dialects.
The list was created based on a 30-million-word
corpus of Arabic including written and spoken ma-
terial from all around the Arab world. The Ara-
bic summarisation dataset provided by MultiL-
ing 2013 was also written using MSA. The cor-
pus of contemporary American English COCA is
a freely searchable 450-million-word corpus con-
taining text in American English of different num-
ber of genres. To be consistent with the Arabic
67
word frequency list, we used the top 5000 words
from the 450 million word COCA corpus.
5 Summarisation Methodology
In our experiments we used generic single-
document and multi-document extractive sum-
marisers that have been implemented for both
Arabic and English (using identical processing
pipelines for both languages). Summaries were
created by selecting sentences from a single doc-
ument or set of related documents. The following
subsections show the methods used in our exper-
iments, the actual summarisation process and the
experimental setup.
5.1 Calculating Log-Likelihood
We begin the summarisation process by calculat-
ing the log likelihood score for each word in the
word frequency lists (see Section 4.1) using the
same methodology described in (Rayson and Gar-
side, 2000). This was performed by constructing a
contingency table as in Table 1.
Corpus
One
Corpus
Two
Total
Frequency
of Word
a b a+b
Frequency
of other
words
c-a d-b c+d-a-b
Total c d c+d
Table 1: Contingency Table
The values c and d correspond to the number of
words in corpus one and corpus two respectively.
Where a and b are the observed values (O). For
each corpus we calculated the expected value E
using the following formula:
Ei =
Ni
?
i
Oi
?
i
Ni
Ni is the total frequency in corpus i (i in our
case takes the values 1 (c) and 2 (d) for the Multi-
Ling Arabic Summaries dataset and the frequency
dictionary of Arabic (or MultiLing English Sum-
maries dataset and COCA corpus) respectively.
The log-likelihood can be calculated as follows:
LL = 2 ? ((a ? ln(
a
E1
)) + (b ? ln(
b
E2
)))
5.2 Summarisation Process
We used the same processing pipeline for both the
single-document and multi-document summaris-
ers. For each word in the MultiLing summari-
sation dataset (Arabic and English) we calculated
the log likelihood scores using the calculations de-
scribed in Section 5.1. We summed up the log
likelihood scores for each sentence in the dataset
and we picked the sentences (up to 250 word limit)
with the highest sum of log likelihood scores. The
main difference between the single-document and
multi-document summarisers is that we treat the
set of related documents in the multiling dataset
as one document.
6 Single-Document Summarisation Task
MultiLing 2013 this year introduced a new single-
document summarisation pilot for 40 languages
including (Arabic, Czech, English, French, Greek,
Hebrew, Hindi, Spanish, Chinese, Romanian
...etc). In our case we participated in two lan-
guages only, English and Arabic.
The pilot aim was to measure the ability of au-
tomated systems to apply single document sum-
marisation, in the context of Wikipedia texts.
Given a single encyclopedic entry, with several
sections/subsections, describing a specific subject,
the pilot guidelines asked the participating sys-
tems to provide a summary covering the main
points of the entry (similarly to the lead section of
a Wikipedia page). The MultiLing 2013 single-
document summaries dataset consisted of (non-
parallel) documents in the above mentioned lan-
guages.
For the English language, there were 7 partici-
pants (peers) including a baseline system (ID5).
The Arabic language had 6 participants including
the same baseline system.
7 Multi-Document Summarisation Task
The Multi-document summarisation task required
the participants to generate a single, fluent, rep-
resentative summary from a set of documents de-
scribing an event sequence. The language of the
document set was within a given range of lan-
guages and all documents in a set shared the same
language. The task guidelines required the output
summary to be of the same language as its source
documents. The output summary should be 250
words at most.
68
The set of documents were available in 10 lan-
guages (Arabic, Czech, English, French, Greek,
Hebrew, Hindi, Spanish, Chinese and Romanian).
In our case we participated using the Arabic and
English set of documents only.
For the English language, there were 10 partic-
ipants (peers) including a baseline (ID6) and a
topline (ID61) systems. The Arabic language had
10 participants as well, including the same base-
line and topline systems.
The baseline summariser sorted sentences based
on their cosine similarity to the centroid of a clus-
ter. Then starts adding sentences to the summary,
until it either reaches 250 words, or it hits the end
of the document. In the second case, it continues
with the next document in the sorted list.
The topline summariser used information from
the model summaries (i.e. cheats). First, it split all
source documents into sentences. Then it used a
genetic algorithm to generate summaries that have
a vector with maximal cosine similarity to the cen-
troid vector of the model summary texts.
8 Results and Discussion
Our single-document summarisers, both English
and Arabic, performed particularly well in the au-
tomatic evaluation. Ranking first and second re-
spectively.
Tables 2 and 3 illustrate the AutoSummEng
(AutoSumm), MeMoG and NPowER results and
the ranking of our English and Arabic single-
document summarisers (System ID2).
System AutoSumm MeMoG NPowER
ID2 0.136 0.136 1.685
ID41 0.129 0.129 1.661
ID42 0.127 0.127 1.656
ID3 0.127 0.127 1.654
ID1 0.124 0.124 1.647
ID4 0.123 0.123 1.641
ID5 0.040 0.040 1.367
Table 2: English Automatic Evaluation Scores
(single-document)
The evaluation scores of our single-document
summarisers confirm with (Li et al, 2006) and
(Nenkova and Vanderwende, 2005) findings, were
they found that word frequency is an important
feature when determining sentences importance
and that words with high frequency in the input
System AutoSumm MeMoG NPowER
ID3 0.092 0.092 1.538
ID2 0.087 0.087 1.524
ID41 0.055 0.055 1.418
ID42 0.055 0.055 1.416
ID4 0.053 0.053 1.411
ID5 0.025 0.025 1.317
Table 3: Arabic Automatic Evaluation Scores
(single-document)
System Score
ID6 3.711
ID3 3.578
ID2 3.578
ID4 3.489
ID1 3.467
ID11 3.333
ID21 3.111
ID51 2.778
ID5 2.711
ID61 2.489
Table 4: Arabic Manual Evaluation Scores (multi-
document)
documents are very likely to appear in the hu-
man summaries, which explains the high correla-
tion between our single-document and the human
(model) summaries as illustrated in the evalua-
tion scores (Tables 2 and 3). The single-document
summaries were evaluated automatically only.
Our Arabic multi-document summariser per-
formed well in the human evaluation ranking sec-
ond jointly with System ID2. Table 4 shows the
average scores of the human evaluation process,
our system is referred to as ID3. On the other
hand, we did not perform well in the automatic
evaluation of the multi-document summarisation
task for both English and Arabic. Our systems did
not perform better than the baseline. The auto-
matic evaluation results placed our Arabic and En-
glish summariser further down in the ranked lists
of systems compared to the human assessment.
This is an area for future work as this seems to
suggest that the automatic evaluation metrics are
not necessarily in line with human judgements.
The low automatic evaluation scores are due
to two main reasons. First, we treated the set
of related documents (multi-documents) as a sin-
gle big document (See Section 5.2), this penalised
69
our summaries as selecting the sentences with the
maximum sum of log likelihood score lead to
many important sentences being overlooked. This
can be solved by running the summariser on each
document to suggest candidate sentences and then
selecting the top sentence(s) of each document to
generate the final summary. Second, we did not
work on eliminating redundancies. Finally, the
log-likelihood score might be improved by the in-
clusion of a dispersion score or weighting to exam-
ine the evenness of the spread of each word across
all the documents.
9 Conclusion
In this paper we presented the results of our par-
ticipation in the MultiLing 2013 summarisation
task. We submitted results for single-document
and multi-document summarisation in two lan-
guages, English and Arabic. We applied a corpus-
based summariser that used corpus-based word
frequency lists. We used a list of the 5,000 most
frequently used words in Modern Standard Ara-
bic (MSA) and English. Using the frequency dic-
tionary of Arabic and the corpus of contemporary
American English (COCA).
Based on the automatic evaluation scores, we
found that our approach appears to work very well
for Arabic and English single-document summari-
sation. According to the human evaluation scores
the approach could potentially work for Arabic
multi-document summarisation as well. We be-
lieve that the approach could still work well for
multi-document summarisation following the sug-
gested solutions in Section 8.
References
R. Barzilay, N. Elhadad, and K. McKeown. 2001. Sen-
tence Ordering in Multidocument Summarization.
In Proceedings of the First International Conference
on Human Language Technology Research, HLT?01,
pages 1?7, Stroudsburg, PA, USA. Association for
Computational Linguistics.
P. Baxendale. 1958. Machine-made index for technical
literature: an experiment. IBM Journal of Research
and Development, 2(4):354?361.
R. Brandow, K. Mitze, and Lisa F. Rau. 1995.
Automatic Condensation of Electronic Publications
by Sentence Selection. Inf. Process. Manage.,
31(5):675?685.
T. Buckwalter and D. Parkinson. 2011. A Frequency
Dictionary of Arabic: Core Vocabulary for Learn-
ers. Routledge, London, United Kingdom.
J. Conroy, J. Schlesinger, D. O?Leary, and J. Goldstein.
2006. Back to Basics: CLASSY 2006. In Pro-
ceedings of the 6th Document Understanding Con-
ferences. DUC.
M. Davies. 2010. The Corpus of Contemporary Amer-
ican English as the First Reliable Monitor Corpus
of English. Literary and Linguistic Computing,
25:447?464.
F. Douzidia and G. Lapalme. 2004. Lakhas, an Ara-
bic Summarising System. In Proceedings of the 4th
Document Understanding Conferences , pages 128?
135. DUC.
M. Fiszman, D. Demner-Fushman, H. Kilicoglu, and
T. Rindflesch. 2009. Automatic Summarization
of MEDLINE Citations for Evidence-based Medical
Treatment: A Topic-oriented Evaluation. Jouranl of
Biomedical Informatics, 42(5):801?813.
G. Giannakopoulos and V. Karkaletsis. 2011. Au-
toSummENG and MeMoG in Evaluating Guided
Summaries. In The Proceedings of the Text Analysis
Conference, MD, USA. TAC.
G. Giannakopoulos and V. Karkaletsis. 2013. Sum-
mary evaluation: Together we stand npower-ed. In
Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, volume 7817 of
Lecture Notes in Computer Science, pages 436?450.
Springer Berlin Heidelberg.
G. Giannakopoulos, V. Karkaletsis, G. Vouros, and
P. Stamatopoulos. 2008. Summarization Sys-
tem Evaluation Revisited: N?Gram Graphs. ACM
Transactions on Speech and Language Processing
(TSLP), 5(3):1?39.
G. Giannakopoulos, M. El-Haj, B. Favre, M. Litvak,
J. Steinberger, and V. Varma. 2011. TAC 2011 Mul-
tiLing Pilot Overview. In Text Analysis Conference
(TAC) 2011, MultiLing Summarisation Pilot, Mary-
land, USA. TAC.
S. Granger. 1998. The computer learner corpus: A
versatile new source of data for SLA research. pages
3?18.
L. Hennig. 2009. Topic-based multi-document sum-
marization with probabilistic latent semantic analy-
sis. In Proceedings of the International Conference
RANLP-2009, pages 144?149, Borovets, Bulgaria,
September. Association for Computational Linguis-
tics.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ?99,
pages 50?57, New York, NY, USA. ACM.
K. Knight and D. Marcu. 2000. Statistics-Based Sum-
marization ? Step One: Sentence Compression. In
Proceedings of the Seventeenth National Conference
on Artificial Intelligence and Twelfth Conference
70
on Innovative Applications of Artificial Intelligence,
pages 703?710, Menlo Park, CA. AAAI Press.
W. Li, B. Li, and M. Wu. 2006. Query Focus Guided
Sentence Selection Strategy.
C. Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proceedings of the
Workshop on Text Summarization Branches Out
(WAS 2004), pages 25?26. WAS 2004).
H. Luhn. 1958. The Automatic Creation of Literature
Abstracts. IBM Journal of Research and Develop-
ment, 2(2):159?165.
N. Madnani, D. Zajic, B. Dorr, N. Ayan, and J. Lin.
2007. Multiple Alternative Sentence Compressions
for Automatic Text Summarization. In Proceedings
of the 7th Document Understanding Conference at
NLT/NAACL, page 26. DUC.
H. Morita, T. Sakai, and M. Okumura. 2011. Query
Snowball: A Co-occurrence-based Approach to
Multi-document Summarization for Question An-
swering. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers
- Volume 2, HLT?11, pages 223?229, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-
101.
D. Radev, H. Jing, M. Sty, and D. Tam. 2004.
Centroid-based Summarization of Multiple Docu-
ments. Information Processing and Management,
40:919?938.
P. Rayson and R. Garside. 2000. Comparing corpora
using frequency profiling. In Proceedings of the
workshop on Comparing corpora - Volume 9, WCC
?00, pages 1?6, Stroudsburg, PA, USA.
P. Rayson, D. Berridge, and B. Francis. 2004. Ex-
tending the cochran rule for the comparison of word
frequencies between corpora. In Proceedings of the
7th International Conference on Statistical analysis
of textual data (JADT 2004, pages 926?936.
M. Scott. 2000. Focusing on the text and its key
words. In Burnard, L. and McEnery, T. (eds.) Re-
thinking language pedagogy from a corpus perspec-
tive: papers from the third international conference
on teaching and language corpora, pages 103?121.
71
