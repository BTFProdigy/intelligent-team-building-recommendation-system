Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 1?11,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Working with a small dataset - semi-supervised dependency parsing for Irish
Teresa Lynn1,2, Jennifer Foster1, Mark Dras2 and Josef van Genabith1
1NCLT/CNGL, Dublin City University, Ireland
2Department of Computing, Macquarie University, Sydney, Australia
1{tlynn,jfoster,josef}@computing.dcu.ie
2{teresa.lynn,mark.dras}@mq.edu.au,
Abstract
We present a number of semi-supervised pars-
ing experiments on the Irish language carried
out using a small seed set of manually parsed
trees and a larger, yet still relatively small, set
of unlabelled sentences. We take two pop-
ular dependency parsers ? one graph-based
and one transition-based ? and compare re-
sults for both. Results show that using semi-
supervised learning in the form of self-training
and co-training yields only very modest im-
provements in parsing accuracy. We also try
to use morphological information in a targeted
way and fail to see any improvements.
1 Introduction
Developing a data-driven statistical parser relies on
the availability of a parsed corpus for the language
in question. In the case of Irish, the only parsed
corpus available to date is a dependency treebank,
which is currently under development and still rel-
atively small, with only 803 gold-annotated trees
(Lynn et al, 2012a). As treebank development is
a labour- and time-intensive process, in this study
we evaluate various approaches to bootstrapping a
statistical parser with a set of unlabelled sentences
to ascertain how accurate parsing output can be
at this time. We carry out a number of differ-
ent semi-supervised bootstrapping experiments us-
ing self-training, co-training and sample-selection-
based co-training. Our studies differ from previous
similar experiments as our data is taken from a work-
in-progress treebank. Thus, aside from the current
small treebank which is used for training the initial
seed model and for testing, there is no additional
gold-labelled data available to us to directly com-
pare supervised and semi-supervised approaches us-
ing training sets of comparable sizes.
In the last decade, data-driven dependency pars-
ing has come to fore, with two main approaches
dominating ? transition-based and graph-based. In
classic transition-based dependency parsing, the
training phase consists of learning the correct parser
action to take given the input string and the parse
history, and the parsing phase consists of the greedy
application of parser actions as dictated by the
learned model. In contrast, graph-based depen-
dency parsing involves the non-deterministic con-
struction of a parse tree by predicting the maximum-
spanning-tree in the digraph for the input sentence.
In our study, we employ Malt (Nivre et al, 2006),
a transition-based dependency parsing system, and
Mate (Bohnet, 2010), a graph-based parser.
In line with similar experiments carried out on
English (Steedman et al, 2003), we find that co-
training is more effective than self-training. Co-
training Malt on the output of Mate proves to be the
most effective method for improving Malt?s perfor-
mance on the limited data available for Irish. Yet, the
improvement is relatively small (0.6% over the base-
line for LAS, 0.3% for UAS) for the best co-trained
model. The best Mate results are achieved through a
non-iterative agreement-based co-training approach,
in which Mate is trained on trees produced by Malt
which exhibit a minimum agreement of 85% with
Mate (LAS increase of 1.2% and UAS of 1.4%).
The semi-supervised parsing experiments do not
explicitly take into account the morphosyntactic
properties of the Irish language. In order to examine
the effect of this type of information during parsing,
we carry out some orthogonal experiments where we
1
reduce word forms to lemmas and introduce mor-
phological features in certain cases. These changes
do not bring about an increase in parsing accuracy.
The paper is organised as follows. Section 2 is
an overview of Irish morphology. In Section 3 our
previous work carried out on the development of an
Irish dependency treebank is discussed followed in
Section 4 by a description of some of our prior pars-
ing results. Section 5 describes the self-training, co-
training and sample-selection-based co-training ex-
periments, Section 6 presents the preliminary pars-
ing experiments involving morphological features,
and, finally, Section 7 discusses our future work.
2 Irish as a morphologically rich language
Irish is a Celtic language of the Indo-European lan-
guage family. It has a VSO word order and is rich in
morphology. The following provides an overview of
the type of morphology present in the Irish language.
It is not a comprehensive summary as the rules gov-
erning morphological changes are too extensive and
at times too complex to document here.
Inflection in Irish mainly occurs through suffixa-
tion, but initial mutation through lenition and eclip-
sis is also common (Christian-Brothers, 1988). A
prominent feature of Irish (also of Scottish and
Manx), which influences inflection, is the existence
of two sets of consonants, referred to as ?broad? and
?slender? consonants (O? Siadhail, 1989). Conso-
nants can be slenderised by accompanying the con-
sonant with a slender vowel, either e or i. Broaden-
ing occurs through the use of broad vowels; a, o or
u. For example, buail ?to hit? becomes ag bualadh
?hitting? in the verbal noun form. In general, there
needs to be vowel harmony (slender or broad) be-
tween stem endings and the initial vowel in a suffix.
A process known as syncopation also occurs
when words with more than one syllable have a
vowel-initial suffix added. For example imir ?to
play? inflects as imr??m ?I play?.
Nouns While Old Irish employed several gram-
matical cases, Modern Irish uses only three: Nomi-
native, Genitive and Vocative. The nominative form
is sometimes regarded as the ?common form? as it is
now also used to account for accusative and dative
forms. Nouns in Irish are divided into five classes, or
declensions, depending on the manner in which the
genitive case is formed. In addition, there are two
grammatical genders in Irish - masculine and fem-
inine. Case, declension and gender are expressed
through noun inflection. For example, pa?ipe?ar ?pa-
per? is a masculine noun in the first declension. Both
lenition and slenderisation are used to form the geni-
tive singular form: pha?ipe?ir. In addition, possessive
adjectives cause noun inflection through lenition,
eclipsis and prefixation. For example, teach ?house?,
mo theach ?my house?, a?r dteach ?our house?; ainm
?name?, a hainm ?her name?.
Verbs Verbs can incorporate their subject, inflect-
ing for person and number through suffixation. Such
forms are referred to as synthetic verb forms. In
addition, verb tense is often indicated through var-
ious combinations of initial mutation, syncopation
and suffixation. For example, scr??obh ?write? can in-
flect as scr??obhaim ?I write?. The past tense of the
verb tug ?give? is thug ?gave?. Lenition occurs af-
ter the negative particle n??. For example, tugaim ?I
give?; n?? thugaim ?I do not give?; n??or thug me? ?I
did not give?. Eclipsis occurs following clitics such
as interrogative particles (an, nach); complementis-
ers (go, nach); and relativisers (a, nach) (Stenson,
1981). For example, an dtugann se?? ?does he give??;
nach dtugann se?? ?does he not give??.
Adjectives In general, adjectives follow nouns and
agree in number, gender and case. Depending on
the noun they modify, adjectives can also inflect.
Christian-Brothers (1988) note eight declensions of
adjectives. They can decline for genitive singular
masculine, genitive singular feminine and nomina-
tive plural. For example, bacach ?lame? inflects as
bacaigh (Gen.Sg.Masc), baca?? (Gen.Fem.Sg) and
bacacha (Nom.PL). Comparative adjectives are also
formed through inflection. For example, la?idir
?strong?, n??os la?idre ?stronger?; de?anach ?late?, is
de?ana?? ?latest?.
Prepositions Irish has simple and compound
prepositions. Most of the simple prepositions can
inflect for person and number (known as preposi-
tional pronouns or pronominal prepositions), thus
including a nominal element. For example, com-
pare bh?? se? ag labhairt le fear ?he was speaking
with a man? with bh?? se? ag labhairt leis ?he was
speaking with him?. These forms are used quite fre-
2
quently, not only with regular prepositional attach-
ment where pronominal prepositions operate as ar-
guments of verbs or modifiers of nouns and verbs,
but also in idiomatic use where they express emo-
tions and states, e.g. ta? bro?n orm (lit. ?be-worry-
on me?) ?I am worried? or ta? su?il agam (lit. ?be-
expectation-with me?) ?I hope?. Noted by Greene
(1966) as a noun-centered language, nouns are of-
ten used to convey the meaning that verbs often
would. Pronominal prepositions are often used in
these types of structures. For example, bhain me?
geit aisti (lit. extracted-I-shock-from her) ?I fright-
ened her?; bhain me? mo cho?ta d??om (lit. extracted-I-
coat-from me) ?I took off my coat?; bhain me? u?sa?id
as (lit. extracted-I-use-from it) ?I used it?; bhain
me? triail astu (lit. extracted-I-attempt-from them)?I
tried them?.
Derivational morphology There are also some
instances of derivational morphology in Irish. U??
Dhonnchadha (2009) notes that all verb stems and
agentive nouns can inflect to become verbal nouns.
Verbal adjectives are also derived from verb stems
through suffixation. For example, the verb du?n
?close? undergoes suffixation to become du?nadh
?closing? (verbal noun) and du?nta ?closed? (verbal
adjective). An emphatic suffix -sa/-se (both broad
and slender form) can attach to nouns or pronouns.
It can also be attached to any verb that has been in-
flected for person and number and also to pronom-
inal prepositions. For example mo thuairim ?my
opinion??mo thuairimse ?my opinion; tu? ?you?(sg)
? tusa ?you?; cloisim ?I hear?? cloisimse ?I hear?;
liom ?with me?? liomsa ?with me?. In addition, the
diminutive suffix -??n can attach to all nouns to form
a derived diminutive form. The rules of slenderisa-
tion apply here also. For example, buachaill ?boy?
becomes buachaill??n ?little boy?, and tamall ?while?
becomes tamaill??n ?short while?.
3 The Irish Dependency Treebank
Irish is the official language of Ireland, yet English
is the primary language for everyday use. Irish is
therefore considered an EU minority language and
is lacking in linguistic resources that can be used to
develop NLP applications (Judge et al, 2012).
Recently, in efforts to address this issue, we have
begun work on the development of a dependency
treebank for Irish (Lynn et al, 2012a). The treebank
has been built upon a gold standard 3,000 sentence
POS-tagged corpus1 developed by U?? Dhonnchadha
(2009). Our labelling scheme is based on an ?LFG-
inspired? dependency scheme developed for English
by C?etinog?lu et al (2010). This scheme was adopted
with the aim of identifying functional roles while
at the same time circumventing outstanding, unre-
solved issues in Irish theoretical syntax.2 The Irish
labelling scheme has 47 dependency labels in the la-
bel set. The treebank is in the CoNLL format with
the following fields: ID, FORM, LEMMA, CPOSTAG,
POSTAG, HEAD and DEPREL. The coarse-grained
part of speech of a word is marked by the la-
bel CPOSTAG, and POSTAG marks the fine-grained
part of speech for that word. For example, prepo-
sitions are tagged with the CPOSTAG Prep and
one of the following POSTAGs: Simple: ar ?on?,
Compound: i ndiaidh ?after?, Possessive: ina
?in its?, Article: sa ?in the?.
At an earlier stage of the treebank?s develop-
ment, we carried out on an inter-annotator agree-
ment (IAA) study. The study involved four stages.
(i) The first experiment (IAA-1) involved the as-
sessment of annotator agreement following the in-
troduction of a second annotator. The results re-
ported a Kappa score of 0.79, LAS of 74.4% and
UAS of 85.2% (Lynn et al, 2012a). (ii) We then
held three workshops that involved thorough anal-
ysis of the output of IAA-1, highlighting disagree-
ments between annotators, gaps in the annotation
guide, shortcomings of the labelling scheme and lin-
guistic issues not yet addressed. (iii) The annotation
guide, labelling scheme and treebank were updated
accordingly, addressing the highlighted issues. (iv)
Finally, a second inter-annotator agreement exper-
iment (IAA-2) was carried out presenting a Kappa
score of 0.85, LAS of 79.2% and UAS of 87.8%
(Lynn et al, 2012b).
We found that the IAA study was valuable in the
development of the treebank, as it resulted in im-
1A tagged, randomised subset of the NCII, (New Corpus for
Ireland - Irish http://corpas.focloir.ie/), comprised of text from
books, news data, websites, periodicals, official and government
documents.
2For example there are disagreements over the existence of
a VP in Irish and whether the language has a VSO or an under-
lying SVO structure.
3
provement of the quality of the labelling scheme,
the annotation guide and the linguistic analysis of
the Irish language. Our updated labelling scheme
is now hierarchical, allowing for a choice between
working with fine-grained or coarse-grained labels.
The scheme has now been finalised. A full list of
the labels can be found in Lynn et al (2012b). The
treebank currently contains 803 gold-standard trees.
4 Preliminary Parsing Experiments
In our previous work (Lynn et al, 2012a), we car-
ried out some preliminary parsing experiments with
MaltParser and 10-fold cross-validation using 300
gold-standard trees. We started out with the fea-
ture template used by C?etinog?lu et al (2010) and ex-
amined the effect of omitting LEMMA, WORDFORM,
POSTAG and CPOSTAG features and combinations
of these, concluding that it was best to include all
four types of information. Our final LAS and UAS
scores were 63.3% and 73.1% respectively. Follow-
ing the changes we made to the labelling scheme
as a result of the second IAA study (described
above), we re-ran the same parsing experiments on
the newly updated seed set of 300 sentences - the
LAS increased to 66.5% and the UAS to 76.3%
(Lynn et al, 2012b).
In order to speed up the treebank creation, we also
applied an active learning approach to bootstrapping
the annotation process. This work is also reported in
Lynn et al (2012b). The process involved training a
MaltParser model on a small subset of the treebank
data, and iteratively, parsing a new set of sentences,
selecting a 50-sentence subset to hand-correct, and
adding these new gold sentences to the training set.
We compared a passive setup, in which the parses
that were selected for correction were chosen at ran-
dom, to an active setup, in which the parses that
were selected for correction were chosen based on
the level of disagreement between two parsers (Malt
and Mate). The active approach to annotation re-
sulted in superior parsing results to the passive ap-
proach (67.2% versus 68.1% LAS) but the differ-
ence was not statistically significant.
5 Semi-Supervised Parsing Experiments
In order to alleviate data sparsity issues brought
about by our lack of training material, we experi-
ment with automatically expanding our training set
using well known semi-supervised techniques.
5.1 Self-Training
5.1.1 Related Work
Self-training, the process of training a system on
its own output, has a long and chequered history in
parsing. Early experiments by Charniak (1997) con-
cluded that self-training is ineffective because mis-
takes made by the parser are magnified rather than
smoothed during the self-training process. The self-
training experiments of Steedman et al (2003) also
yielded disappointing results. Reichart and Rap-
paport (2007) found, on the other hand, that self-
training could be effective if the seed training set
was very small. McClosky et al (2006) also re-
port positive results from self-training, but the self-
training protocol that they use cannot be considered
to be pure self-training as the first-stage Charniak
parser (Charniak, 2000) is retrained on the output of
the two-stage parser (Charniak and Johnson, 2005)
They later show that the extra information brought
by the discriminative reranking phase is a factor
in the success of their procedure (McClosky et al,
2008). Sagae (2010) reports positive self-training re-
sults even without the reranking phase in a domain
adaptation scenario, as do Huang and Harper (2009)
who employ self-training with a PCFG-LA parser.
5.1.2 Experimental Setup
The labelled data available to us for this experi-
ment comprises the 803 gold standard trees referred
to in Section 3. This small treebank includes the
150-tree development set and 150-tree test set used
in experiments by Lynn et al (2012b). We use the
same development and test sets for this study. As
for the remaining 503 trees, we remove any trees
that have more than 200 tokens. The motivation for
this is two-fold: (i) we had difficulties training Mate
parser with long sentences due to memory resource
issues, and (ii) in keeping with the findings of Lynn
et al (2012b), the large trees were sentences from
legislative text that were difficult to analyse for au-
tomatic parsers and human annotators. This leaves
us with 500 gold-standard trees as our seed training
data set.
For our unlabelled data, we take the next 1945
sentences from the gold standard 3,000-sentence
4
A is a parser.
M iA is a model of A at step i.
P iA is a set of trees produced using M
i
A.
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA is labelled training data for A at step i.
Initialise:
L0A ? L.
M0A ? Train(A,L
0
A)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
Li+1A ? L
i
A + P
i
A
M i+1A ? Train(A,L
i+1
A )
end for
Figure 1: Self-training algorithm
POS-tagged corpus referred to in Section 3. When
we remove sentences with more than 200 tokens, we
are left with 1938 sentences in our unlabelled set.
The main algorithm for self-training is given in
Figure 1. We carry out two separate experiments
using this algorithm. In the first experiment we use
Malt. In the second experiment, we substitute Mate
for Malt.3
The steps are as follows: Initialisation involves
training the parser on a labelled seed set of 500 gold
standard trees (L0A), resulting in a baseline parsing
model: M iA. We divide the set of gold POS-tagged
sentences (U ) into 6 sets, each containing 323 sen-
tences U i. For each of the six iterations in this ex-
periment i = [1?6], we parse U i. Each time, the set
of newly parsed sentences (PA) is added to the train-
ing set LiA to make a larger training set of L
i+1
A . A
new parsing model (M i+1A ) is then induced by train-
ing with the new training set.
5.1.3 Results
The results of our self-training experiments are
presented in Figure 2. The best Malt model was
trained on 2115 trees, at the 5th iteration (70.2%
LAS). UAS scores did not increase over the baseline
(79.1%). The improvement in LAS over the baseline
is not statistically significant. The best Mate model
was trained on 1792 trees, at the 4th iteration (71.2%
3Versions used: Maltparser v1.7 (stacklazy parsing algo-
rithm); Mate tools v3.3 (graph-based parser)
Figure 2: Self-Training Results on the Development Set
LAS, 79.2% UAS). The improvement over the base-
line is not statistically significant.
5.2 Co-Training
5.2.1 Related Work
Co-training involves training a system on the out-
put of a different system. Co-training has found
more success in parsing than self-training, and it
is not difficult to see why this might be the case
as it can be viewed as a method for combining the
benefits of individual parsing systems. Steedman
et al (2003) directly compare co-training and self-
training and find that co-training outperforms self-
training. Sagae and Tsujii (2007) successfully em-
ploy co-training in the domain adaption track of the
CoNLL 2007 shared task on dependency parsing.
5.2.2 Experimental Setup
In this and all subsequent experiments, we use
both the same training data and unlabelled data that
we refer to in Section 5.1.2.
Our co-training algorithm is given in Figure 3 and
it is the same as the algorithm provided by Steedman
et al (2003). Again, our experiments are carried out
using Malt and Mate. This time, the experiments are
run concurrently as each parser is bootstrapped from
the other parser?s output.
5
A and B are two different parsers.
M iA and M
i
B are models of A and B at step i.
P iA and P
i
B are a sets of trees produced using M
i
A and M
i
B .
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA and L
i
B are labelled training data for A and B at step i.
Initialise:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
P iB ? Parse(U
i , M iB)
Li+1A ? L
i
A + P
i
B
Li+1B ? L
i
B + P
i
A
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
end for
Figure 3: Co-training algorithm
The steps are as follows: Initialisation involves
training both parsers on a labelled seed set of 500
gold standard trees (L0A and L
0
B), resulting in two
separate baseline parsing models: M iA (Malt) and
M iB (Mate). We divide the set of gold POS-tagged
sentences (U ) into 6 sets, each containing 323 sen-
tences U i. For each of the six iterations in this ex-
periment i = [1? 6], we use Malt and Mate to parse
U i. This time, the set of newly parsed sentences P iB
(Mate output) is added to the training set LiA to make
a larger training set of Li+1A (Malt training set). Con-
versely, the set of newly parsed sentences P iA (Malt
output) is added to the training set LiB to make a
larger training set of Li+1B (Mate training set). Two
new parsing models (M i+1A and M
i+1
B ) are then in-
duced by training Malt and Mate respectively with
their new training sets.
5.2.3 Results
The results of our co-training experiment are pre-
sented in Figure 4. The best Malt model was trained
on 2438 trees, at the final iteration (71.0% LAS
and 79.8% UAS). The improvement in UAS over
the baseline is statistically significant. Mate?s best
model was trained on 823 trees on the second iter-
ation (71.4% LAS and 79.9% UAS). The improve-
ment over the baseline is not statistically significant.
Figure 4: Co-Training Results on the Development Set
5.3 Sample-Selection-Based Co-Training
5.3.1 Related Work
Sample selection involves choosing training items
for use in a particular task based on some criteria
which approximates their accuracy in the absence of
a label or reference. In the context of parsing, Re-
hbein (2011) chooses additional sentences to add to
the parser?s training set based on their similarity to
the existing training set ? the idea here is that sen-
tences that are similar to training data are likely to
have been parsed properly and so are ?safe? to add
to the training set. In their parser co-training experi-
ments, Steedman et al (2003) sample training items
based on the confidence of the individual parsers (as
approximated by parse probability).
In Active Learning research, the Query By Com-
mittee selection method (Seung et al, 1992) is used
to choose items for annotation ? if a committee of
two or more systems disagrees on an item, this is ev-
idence that the item needs to be prioritised for man-
ual correction (see for example Lynn et al (2012b)).
Steedman et al (2003) discuss a sample selection
approach based on differences between parsers ? if
parser A and parser B disagree on an analysis, parser
A can be improved by being retrained on parser B?s
analysis, and vice versa. In contrast, Ravi et al
(2008) show that parser agreement is a strong in-
6
dicator of parse quality, and in parser domain adap-
tation, Sagae and Tsujii (2007) and Le Roux et al
(2012) use agreement between parsers to choose
which automatically parsed target domain items to
add to the training set.
Sample selection can be used with both self-
training and co-training. We restrict our attention
to co-training since our previous experiments have
demonstrated that it has more potential than self-
training. In the following set of experiments, we ex-
plore the role of both parser agreement and parser
disagreement in sample selection in co-training.
5.3.2 Agreement-Based Co-Training
Experimental Setup The main algorithm for
agreement-based co-training is given in Figure 5.
Again, Malt and Mate are used. However, this algo-
rithm differs from the co-training algorithm in Fig-
ure 3 in that rather than adding the full set of 323
newly parsed trees (P iA and P
i
B) to the training set
at each iteration, selected subsets of these trees (P iA?
and P iB?) are added instead. To define these subsets,
we identify the trees that have 85% or higher agree-
ment between the two parser output sets. As a re-
sult, the number of trees in the subsets differ at each
iteration. For iteration 1, 89 trees reach the agree-
ment threshold; iteration 2, 93 trees; iteration 3, 117
trees; iteration 4, 122 trees; iteration 5, 131 trees;
iteration 6, 114 trees. The number of trees in the
training sets is much smaller compared with those
in the experiments of Section 5.2.
Results The results for agreement-based co-
training are presented in Figure 6. Malt?s best
model was trained on 1166 trees at the final iteration
(71.0% LAS and 79.8% UAS). Mate?s best model
was trained on 1052 trees at the 5th iteration (71.5%
LAS and 79.7% UAS). Neither result represents a
statistically significant improvement over the base-
line.
5.3.3 Disagreement-based Co-Training
Experimental Setup This experiment uses the
same sample selection algorithm we used for
agreement-based co-training (Figure 5). For this ex-
periment, however, the way in which the subsets
of trees (P iA? and P
i
B?) are selected differs. This
time we choose the trees that have 70% or higher
disagreement between the two parser output sets.
A and B are two different parsers.
M iA and M
i
B are models of A and B at step i.
P iA and P
i
B are a sets of trees produced using M
i
A and M
i
B .
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA and L
i
B are labelled training data for A and B at step i.
Initialise:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
P iB ? Parse(U
i , M iB)
P iA? ? a subset of X trees from P
i
A
P iB ? ? a subset of X trees from P
i
B
Li+1A ? L
i
A + P
i
B ?
Li+1B ? L
i
B + P
i
A?
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
end for
Figure 5: Sample selection Co-training algorithm
Again, the number of trees in the subsets differ at
each iteration. For iteration 1, 91 trees reach the dis-
agreement threshold; iteration 2, 93 trees; iteration
3, 73 trees; iteration 4, 74 trees; iteration 5, 68 trees;
iteration 6, 71 trees.
Results The results for our disagreement-based
co-training experiment are shown in Figure 7. The
best Malt model was trained with 831 trees at the
4th iteration (70.8% LAS and 79.8% UAS). Mate?s
best models were trained on (i) 684 trees on the 2nd
iteration (71.0% LAS) and (ii) 899 trees on the 5th
iteration (79.4% UAS). Neither improvement over
the baseline is statistically significant.
5.3.4 Non-Iterative Agreement-based
Co-Training
In this section, we explore what happens when
we add the additional training data at once rather
than over several iterations. Rather than testing this
idea with all our previous setups, we choose sample-
selection-based co-training where agreement be-
tween parsers is the criterion for selecting additional
training data.
Experimental Setup Again, we also follow the
algorithm for agreement-based co-training as pre-
sented in Figure 5. However, two different ap-
7
Figure 6: Agreement-based Co-Training Results on the
Development Set
proaches are taken this time, involving only one it-
eration in each. For the first experiment (ACT1a),
the subsets of trees (P iA? and P
i
B?) that are added to
the training data are chosen based on an agreement
threshold of 85% between parsers, and are taken
from the full set of unlabelled data (where U i = U ),
comprising 1938 trees. In this instance, the subset
consists of 603 trees, making a final training set of
1103 trees.
For the second experiment (ACT1b), only trees
meeting a parser agreement threshold of 100% are
added to the training data. 253 trees (P iA? and P
i
B?)
out of 1938 trees (U i = U ) meet this threshold. The
final training set consists of 753 trees.
Results ACT1a proved to be the most accurate
parsing model for Mate overall. The addition of
603 trees that met the agreement threshold of 85%
increased the LAS and UAS scores over the base-
line by 1.0% and 1.3% to 71.8 and 80.4 respec-
tively. This improvement is statistically significant.
Malt showed a LAS improvement of 0.93% and
a UAS improvement of 0.42% (71.0% LAS and
79.6% UAS). The LAS improvement over the base-
line is statistically significant.
The increases for ACT1b, where 100% agreement
trees are added, are less pronounced and are not sta-
Figure 7: Disagreement-based Co-Training Results on
the Development Set
tistically significant. Results showed a 0.5% LAS
and 0.2% UAS increase over the baseline with Malt,
based on the 100% agreement threshold (adding 235
trees). Mate performs at 0.5% above the LAS base-
line and 0.1% above the UAS baseline.
5.4 Analysis
We perform an error analysis for the Malt and Mate
baseline, self-trained and co-trained models on the
development set. We observe the following trends:
? All Malt and Mate parsing models confuse the
subj and obj labels. A few possible rea-
sons for this stand out: (i) It is difficult for
the parser to discriminate between analytic verb
forms and synthetic verb forms. For example,
in the phrase pho?sfainn thusa ?I would marry
you?, pho?sfainn is a synthetic form of the verb
po?s ?marry? that has been inflected with the in-
corporated pronoun ?I?. Not recognising this,
the parser decided that it is an intransitive verb,
taking ?thusa?, the emphatic form of the pro-
noun tu? ?you?, as its subject instead of object.
(ii) Possibly due to a VSO word order, when
the parser is dealing with relative phrases, it
can be difficult to ascertain whether the follow-
ing noun is the subject or object. For example,
an chail??n a chonaic me? inne? ?the girl whom
8
I saw yesterday/ the girl who saw me yester-
day?.4 (iii) There is no passive verb form in
Irish. The autonomous form is most closely
linked with passive use and is used when the
agent is not known or mentioned. A ?hidden?
or understood subject is incorporated into the
verbform. Casadh eochair i nglas ?a key was
turned in a lock? (lit. somebody turned a key
in a lock). In this sentence, eochair ?key? is the
object.
? For both parsers, there is some confusion be-
tween the labelling of obl and padjunct,
both of which mark the attachment between
verbs and prepositions. Overall, Malt?s con-
fusion decreases over the 6 iterations of self-
training, but Mate begins to incorrectly choose
padjunct over obl instead. Mixed results
are obtained using the various variants of co-
training.
? Mate handles coordination better than Malt.5 It
is not surprising then that co-training Malt us-
ing Mate parses improves Malt?s coordination
handling whereas the opposite is the case when
co-training Mate on Malt parses, demonstrat-
ing that co-training can both eliminate and in-
troduce errors.
? Other examples of how Mate helps Malt during
co-training is in the distinction between top
and comp relations, between vparticle
and relparticle, and in the analysis of
xcomps.
? Distinguishing between relative and cleft par-
ticles is a frequent error for Mate, and there-
fore Malt also begins to make this kind of error
when co-trained using Mate. Mate improves
using sample-selection-based co-training with
Malt.
? The sample-selection-based co-training vari-
ants show broadly similar trends to the basic
co-training.
4Naturally ambiguous Irish sentences like this require con-
text for disambiguation.
5Nivre and McDonald (2007) make a similar observation
when they compare the errors made by graph and transition
based dependency parsers.
Parsing Models LAS UAS
Development Set
Malt Baseline: 70.0 79.1
Malt Best (co-train) : 71.0 80.2
Mate Baseline: 70.8 79.1
Mate Best (85% threshold ACT1a): 71.8 80.4
Test Set
Malt Baseline: 70.2 79.5
Malt Best (co-train) : 70.8 79.8
Mate Baseline: 71.9 80.1
Mate Best (85% threshold ACT1a): 73.1 81.5
Table 1: Results for best performing models
5.5 Test Set Results
The best performing parsing model for Malt on
the development set is in the final iteration of the
basic co-training approach in Section 5.2. The
best performing parsing model for Mate on the de-
velopment set is the non-iterative 85% threshold
agreement-based co-training approach described in
Section 5.3.4. The test set results for these opti-
mal development set configurations are also shown
in Table 1. The baseline model for Malt obtains
a LAS of 70.2%, the final co-training iteration a
LAS of 70.8%. The baseline model for Mate ob-
tains a LAS of 71.9%, and the non-iterative 85%
agreement-based co-trained model obtains a LAS of
73.1%.
6 Parsing Experiments Using
Morphological Features
As well as the size of the dataset, data sparsity is
also confounded by the number of possible inflected
forms for a given root form. With this in mind,
and following on from the discussion in Section 5.4,
we carry out further parsing experiments in an at-
tempt to make better use of morphological informa-
tion during parsing. We attack this in two ways: by
reducing certain words to their lemmas and by in-
cluding morphological information in the optional
FEATS (features) field. The reasoning behind re-
ducing certain word forms to lemmas is to further
reduce the differences between inflected forms of
the same word, and the reasoning behind including
morphological information is to make more explicit
the similarity between two different word forms in-
flected in the same way. All experiments are car-
9
Parsing Models (Malt) LAS UAS
Baseline: 70.0 79.1
Lemma (Pron Prep): 69.7 78.9
Lemma + Pron Prep Morph Features: 69.6 78.9
Form + Pron Prep Morph Features: 69.8 79.1
Verb Morph Features: 70.0 79.1
Table 2: Results with morphological features on the de-
velopment set
ried out with MaltParser and our seed training set
of 500 gold trees. We focus on two phenomena:
prepositional pronouns or pronominal prepositions
(see Section 2) and verbs with incorporated subjects
(see Section 2 and Section 5.4).
In the first experiment, we include extra mor-
phological information for pronominal prepositions.
We ran three parsing experiments: (i) replacing the
value of the surface form (FORM) of pronominal
prepositions with their lemma form (LEMMA), for
example agam?ag, (ii) including morphological in-
formation for pronominal prepositions in the FEATS
column. For example, in the case of agam ?at me?,
we include Per=1P|Num=Sg, (iii) we combine
both approaches of reverting to lemma form and also
including the morphological features. The results
are given in Table 2.
In the second experiment, we include morpholog-
ical features for verbs with incorporated subjects:
imperative verb forms, synthetic verb forms and au-
tonomous verb forms such as those outlined in Sec-
tion 5.4. For each instance of these verb types, we
included incorpSubj=true in the FEATS col-
umn. The results are also given in Table 2.
The experiments on the pronominal prepositions
show a drop in parsing accuracy while the experi-
ments carried out using verb morphological infor-
mation showed no change in parsing accuracy.6 In
the case of inflected prepositions, perhaps we have
not seen any improvement because we have not fo-
cused on a phenomenon which is critical for parsing.
More experimentation is necessary.
7 Concluding Remarks
We have presented two sets of experiments which
aim to improve dependency parsing performance for
6Although the total number of correct attachments are the
same, the parser output is different.
a minority language with a very small treebank. In
the first set of experiments, the main focus of the pa-
per, we tried to overcome the limited treebank size
by increasing the parsers? training sets using auto-
matically parsed sentences. While we do manage
to achieve statistically significant improvements in
some settings, it is clear from the results that the
gains in parser accuracy through semi-supervised
bootstrapping methods are fairly modest. Yet, in the
absence of more gold labelled data, it is difficult to
know now whether we would achieve similar or im-
proved results by adding the same amount of gold
training data. This type of analysis will be interest-
ing at a later date when the unlabelled trees used in
these experiments are eventually annotated and cor-
rected manually.
The second set of experiments tries to mitigate
some of the data sparseness issues by exploiting
morphological characteristics of the language. Un-
fortunately, we do not see any improvements but we
may get different results if we repeat these experi-
ments using the larger semi-supervised training sets
from the first set of experiments.
There are many directions this parsing research
could take us in the future. Our unlabelled data con-
sisted of sentences annotated with gold POS tags.
In the future we would like to take advantage of
the fully unlabelled, untagged data in the New Cor-
pus for Ireland ? Irish, which consists of 30 million
words. We would also like to experiment with a fully
unsupervised parser using this dataset. Our Malt fea-
ture models are manually optimised ? it would be in-
teresting to experiment with optimising them using
MaltOptimizer (Ballesteros, 2012). An additional
avenue of research would be to exploit the hierar-
chical nature of the dependency scheme to arrive at
more flexible way of measuring agreement or dis-
agreement in sample selection.
Acknowledgements
We thank the three anonymous reviewers for their
helpful feedback. This work is supported by Sci-
ence Foundation Ireland (Grant No. 07/CE/I1142)
as part of the Centre for Next Generation Localisa-
tion (www.cngl.ie) at Dublin City University.
10
References
Miguel Ballesteros. 2012. Maltoptimizer: A sys-
tem for maltparser optimization. In Proceedings of
the Eighth International Conference on Linguistic Re-
sources and Evaluation (LREC), pages 2757?2763, Is-
tanbul, Turkey.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING.
O?zlem C?etinog?lu, Jennifer Foster, Joakim Nivre, Deirdre
Hogan, Aoife Cahill, and Josef van Genabith. 2010.
LFG without c-structures. In Proceedings of the 9th
International Workshop on Treebanks and Linguistic
Theories.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd ACL.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of AAAI.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics (NAACL-00).
Christian-Brothers. 1988. New Irish Grammar. Dublin:
C J Fallon.
David Greene. 1966. The Irish Language. Dublin: The
Three Candles.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proceedings of EMNLP.
John Judge, Ailbhe N?? Chasaide, Rose N?? Dhubhda,
Kevin P. Scannell, and Elaine U?? Dhonnchadha. 2012.
The Irish Language in the Digital Age. Springer Pub-
lishing Company, Incorporated.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
soul Samed Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 systems for the sancl 2012 shared task.
In Working Notes of SANCL.
Teresa Lynn, O?zlem C?etinog?lu, Jennifer Foster, Elaine U??
Dhonnchadha, Mark Dras, and Josef van Genabith.
2012a. Irish treebanking and parsing. In Proceedings
of the Eight International Conference on Language
Resources and Evaluation, pages 1939?1946.
Teresa Lynn, Jennifer Foster, Mark Dras, and Elaine U??
Dhonnchadha. 2012b. Active learning and the Irish
treebank. In Proceeedings of the Australasian Lan-
guage Technology Workshop (ALTA), pages 23?32.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City, USA, June. Association for Computational
Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. When is self-training effective for parsing? In
Proceedings of COLING.
Joakim Nivre and Ryan McDonald. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL, Prague, Czech
Republic.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC2006).
M??chea?l O? Siadhail. 1989. Modern Irish: Grammatical
structure and dialectal variation. Cambridge: Cam-
bridge University Press.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Au-
tomatic prediction of parser accuracy. In Proceedings
of EMNLP, Hawaii.
Ines Rehbein. 2011. Data point selection for self-
training. In Proceedings of the Second Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2011), Dublin, Ireland.
Roi Reichart and Ari Rappaport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In Proceedings of
ACL.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL shared task
session of EMNLP-CoNLL.
Kenji Sagae. 2010. Self-training without reranking for
parser domain adapation and its impact on semantic
role labelling. In Proceedings of the ACL Workshop
on Domain Adaptation for NLP.
Sebastian Seung, Manfred Opper, and Haim Sompolin-
sky. 1992. Query by committee. In Proceedings
of the Fifth Annual ACM Workshop on Computational
Learning Theory.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the tenth conference on European chapter
of the Association for Computational Linguistics - Vol-
ume 1, EACL ?03, pages 331?338, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nancy Stenson. 1981. Studies in Irish Syntax. Tu?bingen:
Gunter Narr Verlag.
Elaine U?? Dhonnchadha. 2009. Part-of-Speech Tagging
and Partial Parsing for Irish using Finite-State Trans-
ducers and Constraint Grammar. Ph.D. thesis, Dublin
City University.
11
Proceedings of the First Celtic Language Technology Workshop, pages 41?49,
Dublin, Ireland, August 23 2014.
Cross-lingual Transfer Parsing for Low-Resourced Languages: An Irish
Case Study
Teresa Lynn
1,2
, Jennifer Foster
1
, Mark Dras
2
and Lamia Tounsi
1
1
CNGL, School of Computing, Dublin City University, Ireland
2
Department of Computing, Macquarie University, Sydney, Australia
1
{tlynn,jfoster,ltounsi}@computing.dcu.ie
2
{teresa.lynn,mark.dras}@mq.edu.au
Abstract
We present a study of cross-lingual direct transfer parsing for the Irish language. Firstly we
discuss mapping of the annotation scheme of the Irish Dependency Treebank to a universal de-
pendency scheme. We explain our dependency label mapping choices and the structural changes
required in the Irish Dependency Treebank. We then experiment with the universally annotated
treebanks of ten languages from four language family groups to assess which languages are the
most useful for cross-lingual parsing of Irish by using these treebanks to train delexicalised pars-
ing models which are then applied to sentences from the Irish Dependency Treebank. The best
results are achieved when using Indonesian, a language from the Austronesian language family.
1 Introduction
Considerable efforts have been made over the past decade to develop natural language processing re-
sources for the Irish language (U?? Dhonnchadha et al., 2003; U?? Dhonnchadha and van Genabith, 2006;
U?? Dhonnchadha, 2009; Lynn et al., 2012a; Lynn et al., 2012b; Lynn et al., 2013). One such resource
is the Irish Dependency Treebank (Lynn et al., 2012a) which contains just over 1000 gold standard de-
pendency parse trees. These trees are labelled with deep syntactic information, marking grammatical
roles such as subject, object, modifier, and coordinator. While a valuable resource, the treebank does not
compare in size to similar resources of other languages.
1
The small size of the treebank affects the accu-
racy of any statistical parsing models learned from this treebank. Therefore, we would like to investigate
whether training data from other languages can be successfully utilised to improve Irish parsing.
Cross-lingual transfer parsing involves training a parser on one language, and parsing data of another
language. McDonald et al. (2011) describe two types of cross-lingual parsing, direct transfer parsing in
which a delexicalised version of the source language treebank is used to train a parsing model which
is then used to parse the target language, and a more complicated projected transfer approach in which
the direct transfer approach is used to seed a parsing model which is then trained to obey source-target
constraints learned from a parallel corpus. These experiments revealed that languages that were typo-
logically similar were not necessarily the best source-target pairs, sometimes due to variations between
their language-specific annotation schemes. In more recent work, however, McDonald et al. (2013) re-
ported improved results on cross-lingual direct transfer parsing using a universal annotation scheme, to
which six chosen treebanks are mapped for uniformity purposes. Underlying the experiments with this
new annotation scheme is the universal part-of-speech (POS) tagset designed by Petrov et al. (2012).
While their results confirm that parsers trained on data from languages in the same language group (e.g.
Romance and Germanic) show the most accurate results, they also show that training data taken across
language-groups also produces promising results. We attempt to apply the direct transfer approach with
Irish as the target language.
The Irish language belongs to the Celtic branch of the Indo-European language family. The natural
first step in cross-lingual parsing for Irish would be to look to those languages of the Celtic language
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
For example, the Danish dependency treebank has 5,540 trees (Kromann, 2003); the Finnish dependency treebank has
15,126 trees (Haverinen et al., 2013)
41
group, i.e. Welsh, Scots Gaelic, Manx, Breton and Cornish, as a source of training data. However,
these languages are just as, if not further, under-resourced. Thus, we attempt to use the languages of the
universal dependency treebanks (McDonald et al., 2013).
The paper is organised as follows. In Section 2, we give an overview of the status of the Irish lan-
guage and the Irish Dependency Treebank. Section 3 describes the mapping of the Irish Dependency
Treebank?s POS tagset (U?? Dhonnchadha and van Genabith, 2006) to that of Petrov et al. (2012), and
the Irish Dependency Treebank annotation scheme (Lynn et al. (2012b)) to the Universal Dependency
Scheme. Following that, in Section 4 we carry out cross-lingual direct transfer parsing experiments with
ten harmonised treebanks to assess whether any of these languages are suitable for such parsing transfer
for Irish. Section 5 summarises our work.
2 Irish Language and Treebank
Irish, a minority EU language, is the national and official language of Ireland. Despite this status, Irish
is only spoken on a daily basis by a minority. As a Celtic language, Irish shares specific linguistic
features with other Celtic languages, such as a VSO (verb-subject-object) word order and interesting
morphological features such as inflected prepositions and initial mutations, for example.
Compared to other EU-official languages, Irish language technology is under-resourced, as highlighted
by a recent study (Judge et al., 2012). In the area of morpho-syntactic processing, recent years have seen
the development of a part-of-speech tagger (U?? Dhonnchadha and van Genabith, 2006), a morphological
analyser (U?? Dhonnchadha et al., 2003), a shallow chunker (U?? Dhonnchadha, 2009), a dependency tree-
bank (Lynn et al., 2012a; Lynn et al., 2012b) and statistical dependency parsing models for MaltParser
(Nivre et al., 2006) and Mate parser (Bohnet, 2010) trained on this treebank (Lynn et al., 2013).
The annotation scheme for the Irish Dependency Treebank (Lynn et al., 2012b) was inspired by Lexical
Functional Grammar (Bresnan, 2001) and has its roots in the dependency annotation scheme described
by C?etino?glu et al. (2010). It was extended and adapted to suit the linguistic characterisics of the Irish
language. The final label set consists of 47 dependency labels, defining grammatical and functional
relations between the words in a sentence. The label set is hierarchical in nature with labels such as
vparticle (verb particle) and vocparticle (vocative particle), for example, representing more
fine-grained versions of the particle label.
3 A universal dependency scheme for the Irish Dependency Treebank
In this section, we describe how a ?universal? version of the Irish Dependency Treebank was created by
mapping the original POS tags to universal POS tags and mapping the original dependency scheme to the
universal dependency scheme. The result of this effort is an alternative version of the Irish Dependency
Treebank which will be made available to the research community along with the original.
3.1 Mapping the Irish POS tagset to the Universal POS tagset
The Universal POS tagset (Petrov et al., 2012) has been designed to facilitate unsupervised and cross-
lingual part-of-speech tagging and parsing research, by simplifying POS tagsets and unifying them across
languages. The Irish Dependency Treebank was built upon a POS-tagged corpus developed by U?? Dhon-
nchadha and van Genabith (2006). The treebank?s tagset contains both coarse- and fine-grained POS tags
which we map to the Universal POS tags (e.g. Prop Noun? NOUN). Table 1 shows the mappings.
Most of the POS mappings made from the Irish POS tagset to the universal tagset are intuitive. How-
ever, some decisions require explanation.
Cop ? VERB There are two verbs ?to be? in Irish: the substantive verb b?? and the copula is. For
that reason, the Irish POS tagset differentiates the copula by using the POS tag Cop. In Irish syntax
literature, there is some discussion over its syntactic role, whether it is a verb or a linking particle. The
role normally played is that of a linking element between a subject and a predicate. However, Lynn et al.
(2012a)?s syntactic analysis of the copula is in line with that of Stenson (1981), regarding it as a verb. In
addition, because the copula is often labelled in the Irish annotation scheme as the syntactic head of the
matrix clause, we have chosen VERB as the most suitable mapping for this part of speech.
42
Part-of-speech (POS) mappings
Universal Irish Universal Irish
NOUN
Noun Noun, Pron Ref,
Subst Subst, Verbal Noun,
Prop Noun
ADP
Prep Deg, Prep Det, Prep Pron,
Prep Simp, Prep Poss,
Prep CmpdNoGen, Prep Cmpd,
Prep Art, Pron Prep
PRON
Pron Pers, Pron Idf, Pron Q,
Pron Dem
ADV
Adv Temp, Adv Loc, Adv Dir,
Adv Q, Adv Its, Adv Gn
VERB
Cop Cop, Verb PastInd, Verb PresInd,
Verb PresImp, Verb VI, Verb VT,
Verb VTI, Verb PastImp, Verb Cond,
Verb FutInd, Verb VD, Verb Imper
PRT
Part Vb, Part Sup, Part Inf, Part Pat,
Part Voc, Part Ad, Part Deg, Part Comp,
Part Rel, Part Num, Part Cp,
DET Art Art, Det Det NUM Num Num
ADJ Prop Adj, Verbal Adj, Adj Adj X
Item Item, Abr Abr, CM CM, CU CU,
CC CC, Unknown Unknown,
Guess Abr, Itj Itj, Foreign Foreign,
CONJ Conj Coord, Conj Subord . . . ... ... ? ? ! ! : : ? . Punct Punct
Table 1: Mapping of Irish Coarse and Fine-grained POS pairs (coarse fine) to Universal POS tagset.
Pron Prep?ADP Pron Prep is the Irish POS tag for pronominal prepositions, which are also referred
to as prepositional pronouns. Characteristic of Celtic languages, they are prepositions inflected with their
pronominal objects ? compare, for example, le mo chara ?with my friend? with leis ?with him?. While
the Irish POS labelling scheme labels them as pronouns in the first instance, our dependency labelling
scheme treats the relationship between them and their syntactic heads as obl (obliques) or padjunct
(prepositional adjuncts). Therefore, we map them to ADP (adpositions).
3.2 Mapping the Irish Dependency Scheme to the Universal Dependency Scheme
The departure point for the design of the Universal Dependency Annotation Scheme (McDonald et
al., 2013) was the Stanford typed dependency scheme (de Marneffe and Manning, 2008), which was
adapted based on a cross-lingual analysis of six languages: English, French, German, Korean, Spanish
and Swedish. Existing English and Swedish treebanks were automatically mapped to the new universal
scheme. The rest of the treebanks were developed manually to ensure consistency in annotation. The
study also reports some structural changes (e.g. Swedish treebank coordination structures).
2
There are 41 dependency relation labels to choose from in the universal annotation scheme
3
. McDon-
ald et al. (2013) use all labels in the annotation of the German and English treebanks. The remaining
languages use varying subsets of the label set. In our study we map the Irish dependency annotation
scheme to 30 of the universal labels. The mappings are given in Table 2.
As with the POS mapping discussed in Section 3.1, mapping the Irish dependency scheme to the
universal scheme was relatively straightforward, due in part, perhaps, to a similar level of granularity
suggested by the similar label set sizes (Irish 47; standard universal 41). That said, there were significant
considerations made in the mapping process, which involved some structural change in the treebank and
the introduction of more specific analyses in the labelling scheme. These are discussed below.
3.2.1 Structural Differences
The following structural changes were made manually before the dependency labels were mapped to the
universal scheme.
coordination The most significant structural change made to the treebank was an adjustment to the
analysis of coordination. The original Irish Dependency Treebank subscribes to the LFG coordination
analysis, where the coordinating conjunction (e.g. agus ?and?) is the head, with the coordinates as its
dependents, labelled coord (see Figure 1). The Universal Dependency Annotation scheme, on the
2
There are two versions of the annotation scheme: the standard version (where copulas and adpositions are syntactic heads),
and the content-head version which treats content words as syntactic heads. We are using the standard version for our study.
3
The vmod label is used only in the content-head version.
43
Dependency Label Mappings
Universal Irish Universal Irish
root top csubj csubj
acomp adjpred, advpred, ppred dep for
adpcomp N/A det det, det2, dem
adpmod padjunct, obl, obl2, obl ag dobj obj, vnobj, obj q
adpobj pobj mark subadjunct
advcl N/A nmod addr, nadjunct
advmod
adjunct, advadjunct, quant,
advadjunct q
nsubj subj, subj q
amod adjadjunct num N/A
appos app p punctuation
attr npred parataxis N/A
aux toinfinitive poss poss
cc N/A prt
particle, vparticle, nparticle, advparticle,
vocparticle, particlehead, cleftparticle,
qparticle, aug
ccomp comp rcmod relmod
compmod nadjunct rel relparticle
conj coord xcomp xcomp
Table 2: Mapping of Irish Dependency Annotation Scheme to Universal Dependency Annotation Scheme
other hand, uses right-adjunction, where the first coordinate is the head of the coordination, and the
rest of the phrase is adjoined to the right, labelling coordinating conjunctions as cc and the following
coordinates as conj (Figure 2).
coord det subj advpred top coord det subj advpred obl det pobj
Bh?? an l?a an-te agus bh?? gach duine sti?ugtha leis an tart
Be-PAST the day very-hot and be-PAST every person parched with the thirst
?The day was very hot and everyone was parched with the thirst?
Figure 1: LFG-style coordination of original Irish Dependency Treebank
top det subj advpred cc conj det subj advpred obl det pobj
Bh?? an l?a an-te agus bh?? gach duine sti?ugtha leis an tart
Be-PAST the day very-hot and be-PAST every person parched with the thirst
?The day was very hot and everyone was parched with the thirst?
Figure 2: Stanford-style coordination changes to original Irish Dependency Treebank
subordinate clauses In the original Irish Dependency Treebank, the link between a matrix clause and
its subordinate clause is similar to that of LFG: the subordinating conjunction (e.g. mar ?because?, nuair
?when?) is a subadjunct dependent of the matrix verb, and the head of the subordinate clause is a
comp dependent of the subordinating conjunction (Figure 3). In contrast, the universal scheme is in
line with the Stanford analysis of subordinate clauses, where the head of the clause is dependent on the
matrix verb, and the subordinating conjunction is a dependent of the clause head (Figure 4).
3.2.2 Differences between dependency types
We found that the original Irish scheme makes distinctions that the universal scheme does not ? this
finer-grained information takes the form of the following Irish-specific dependency types: advpred,
44
top subj xcomp obl det pobj adjadjunct subadjunct comp subj ppred pobj num
Caithfidh t?u brath ar na himreoir?? ?aiti?ula nuair at?a t?u i Roinn 1
Have-to-FUT you rely on the players local when REL-be-PRES you in Division 1
?You have to rely on the local players when you are in Division 1?
Figure 3: LFG-style subordinate clause analysis (with original Irish Dependency labels)
top subj xcomp obl det pobj adjadjunct subadjunct comp subj ppred pobj num
Caithfidh t?u brath ar na himreoir?? ?aiti?ula nuair at?a t?u i Roinn 1
Have-to-FUT you rely on the players local when REL-be-PRES you in Division 1
?You have to rely on the local players when you are in Division 1?
Figure 4: Stanford-style subordinate clause analysis (with original Irish Dependency labels)
ppred, subj q, obj q, advadjunct q, obl, obl2. In producing the universal version of the tree-
bank, these Irish-specific dependency types are mapped to less informative universal ones (see Table 2).
Conversely, we found that the universal scheme makes distinctions that the Irish scheme does not. Some
of these dependency types are not needed for Irish. For example, there is no indirect object iobj in Irish,
nor is there a passive construction that would require nsubjpass, csubjpass or auxpass. Also, in
the Irish Dependency Treebank, the copula is usually the root (top) or the head of a subordinate clause
(e.g. comp) which renders the universal type cop redundant. Others that are not used are adp, expl,
infmod, mwe, neg, partmod. However, we did identify some dependency relationships in the univer-
sal scheme that we introduce to the universal Irish Dependency Treebank (adpcomp, adposition,
advcl, num, parataxis). These are explained below.
comp? adpcomp, advcl, parataxis, ccomp The following new mappings were previously subsumed
by the Irish dependency label comp (complement clause). The mapping for comp has thus been split
between adpcomp, advcl, parataxis and ccomp.
? adpcomp is a clausal complement of an adposition. An example from the English data is ?some
understanding of what the company?s long-term horizon should begin to look like?, where ?begin?,
as the head of the clause, is a dependent of the preposition ?of?. An example of how we use this
label in Irish is: an l??ne l?antosach is m?o cl?u a th?ainig as Ciarra?? ?o bh?? aimsir Sheehy ann ?the most
renowned forward line to come out of Kerry since Sheehy?s time? (lit. ?from it was Sheehy?s time?).
The verb bh?? ?was?, head of the dependent clause, is an adcomp dependent of the preposition ?o.
? advcl is used to identify adverbial clause modifiers. In the English data, they are often introduced
by subordinating conjunctions such as ?when?, ?because?, ?although?, ?after?, ?however?, etc. An
example is ?However, because the guaranteed circulation base is being lowered, ad rates will be
higher?. Here, ?lowered? is a advcl dependent of ?will?. An example of usage is: T?a truailli?u m?or
san ?ait mar nach bhfuil c?oras s?earachais ann ?There is a lot of pollution in the area because there
is no sewerage system?, where bhfuil ?is? is an advcl dependent of T?a ?is?.
45
? parataxis labels clausal structures that are separated from the previous clause with punctuation
such as ? ... : () ; and so on. Examples in Irish Is l?eir go bhfuil ag ?eir?? le feachtas an IDA ?
meastar gur in
?
Eirinn a lonna??tear timpeall 30% de na hionaid ?It is clear that the IDA campaign is
succeeding ? it is believed that 30% of the centres are based in Ireland?. Here, meastar ?is believed?
is a parataxis dependent of Is ?is?.
? ccomp covers all other types of clausal complements. For example, in English, ?Mr. Amos says the
Show-Crier team will probably do two live interviews a day?. The head of the complement clause
here is ?do?, which is a comp dependent of the matrix verb ?says?. A similar Irish example is: D?uirt
siad nach bhfeiceann siad an cine?al seo chomh minic ?They said that they don?t see this type as
often?. Here, bhfeiceann ?see? is the head of the complement clause, which is a comp dependent of
the verb D?uirt ?Said?.
quant? num, advmod The Irish Dependency Scheme uses one dependency label (quant) to cover
all types of numerals and quantifiers. We now use the universal scheme to differentiate between quanti-
fiers such as m?or?an ?many? and numerals such as fiche ?twenty?.
nadjunct? nmod, compmod The Irish dependency label nadjunct accounts for all nominal mod-
ifiers. However, in order to map to the universal scheme, we discriminate two kinds: (i) nouns that mod-
ify nouns (usually genitive case in Irish) are mapped to compmod (e.g. plean marga??ochta ?marketing
plan?) and (ii) nouns that modify clauses are mapped to nmod (e.g. bliain ?o shin ?a year ago?).
4 Parsing Experiments
We now describe how we extend the direct transfer experiments described in McDonald et al. (2013)
to Irish. In Section 4.1, we describe the datasets used in our experiments and explain the experimental
design. In Section 4.2, we present the results, which we then discuss in Section 4.3.
4.1 Data and Experimental Setup
We present the datasets used in our experiments and explain how they are used. Irish is the target
language for all our parsing experiments.
Universal Irish Dependency Treebank This is the universal version of the Irish Dependency Treebank
which contains 1020 gold-standard trees, which have been mapped to the Universal POS tagset and
Universal Dependency Annotation Scheme, as described in Section 3. In order to establish a monolingual
baseline against which to compare our cross-lingual results, we perform a five-fold cross-validation by
dividing the full data set into five non-overlapping training/test sets. We also test our cross-lingual models
on an delexicalised version of this treebank.
Transfer source training data For our direct transfer cross-lingual parsing experiments, we use 10 of
the standard version harmonised training data sets
4
made available by McDonald et al. (2013): Brazilian
Portuguese (PT-BR), English (EN), French (FR), German (DE), Indonesian (ID), Italian (IT), Japanese
(JA), Korean (KO), Spanish (ES) and Swedish (SV). For the purposes of uniformity, we select the first
4447 trees from each treebank ? to match the number of trees in the smallest data set (Swedish). We
delexicalise all treebanks and use the universal POS tags as both the coarse- and fine-grained values.
5
We train a parser on all 10 source data sets outlined and use each induced parsing model to parse and test
on a delexicalised version of the Universal Irish Dependency Treebank.
Largest transfer source training data - Universal English Dependency Treebank English has the
largest source training data set (sections 2-21 of the Wall Street Journal data in the Penn Treebank (Mar-
cus et al., 1993) contains 39, 832 trees). As with the smaller transfer datasets, we delexicalise this dataset
and use the universal POS tag values only. We experiment with this larger training set in order to establish
whether more training data helps in a cross-lingual setting.
4
Version 2 data sets downloaded from https://code.google.com/p/uni-dep-tb/
5
Note that the downloaded treebanks had some fine-grained POS tags that were not used across all languages: e.g. VERB-
VPRT (Spanish), CD (English).
46
Parser and Evaluation Metrics We use a transition-based dependency parsing system, MaltParser
(Nivre et al., 2006) for all of our experiments. All our models are trained using the stacklazy algorithm,
which can handle the non-projective trees present in the Irish data. In each case we report Labelled
Attachment Score (LAS) and Unlabelled Attachment Score (UAS).
6
4.2 Results
All cross-lingual results are presented in Table 3. Note that when we train and test on Irish (our mono-
lingual baseline), we achieve an average accuracy of 78.54% (UAS) and 71.59% (LAS) over the five
cross-validation runs. The cross-lingual results are substantially lower than this baseline. The LAS
results range from 0.84 (JA) to 43.88 (ID) and the UAS from 16.74 (JA) to 61.69 (ID).
SingleT MultiT LargestT
Training EN FR DE ID IT JA KO PT-BR ES SV All EN
UAS 51.72 56.84 49.21 61.69 50.98 16.74 18.02 57.31 57.00 49.95 57.69 51.59
LAS 35.03 37.91 33.04 43.88 37.98 0.84 9.35 42.13 41.94 34.02 41.38 33.97
Experiment SingleT-30 MultiT-30 LargestT-30
Training EN FR DE ID IT JA KO PT-BR ES SV All EN
Avg sent len 23 24 16 21 21 9 11 24 26 14 19 23
UAS 55.97 60.98 53.42 64.86 54.47 16.88 19.27 60.47 60.53 54.40 61.40 55.54
LAS 38.42 41.44 36.24 46.45 40.56 1.19 10.08 45.04 45.23 37.76 44.63 37.08
Table 3: Multi-lingual transfer parsing results
A closer look at the single-source transfer parsing evaluation results (SingleT) shows that some lan-
guage sources are particularly strong for parsing accuracy of certain labels. For example, ROOT (for
Indonesian), adpobj (for French) and amod (for Spanish). In response to these varied results, we ex-
plore the possibility of combining the strengths of all the source languages (multi-source direct transfer
(MultiT) ? also implemented by McDonald et al. (2011)). A parser is trained on a concatenation of
all the delexicalised source data described in Section 4.1 and tested on the full delexicalised Universal
Irish Dependency Treebank. Combining all source data produces parsing results of 57.69% (UAS) and
41.38% (LAS), which is outperformed by the best individual source language model.
Parsing with the large English training set (LargestT) yielded results of 51.59 (UAS) and 33.97 (LAS)
compared to a UAS/LAS of 51.72/35.05 for the smaller English training set. We investigated more
closely why the larger training set did not improve performance by incrementally adding training sen-
tences to the smaller set ? none of these increments reveal any higher scores, suggesting that English is
not a suitable source training language for Irish.
It is well known that sentence length has a negative effect on parsing accuracy. As noted in earlier
experiments (Lynn et al., 2012b), the Irish Dependency Treebank contains some very long difficult-to-
parse sentences (some legal text exceeds 300 tokens in length). The average sentence length is 27 tokens.
By placing a 30-token limit on the Universal Irish Dependency Treebank we are left with 778 sentences,
with an average sentence length of 14. We use this new 30-token-limit version of the Irish Dependency
Treebank data to test our parsing models. The results are shown in the lower half of Table 3. Not
surprisingly, the results rise substantially for all models.
4.3 Discussion
McDonald et al. (2013)?s single-source transfer parsing results show that languages within the same
language groups make good source-target pairs. They also show reasonable accuracy of source-target
pairing across language groups. For instance, the baseline when parsing French is 81.44 (UAS) and 73.37
(LAS), while the transfer results obtained using an English treebank are 70.14 (UAS) and 58.20(LAS).
Our baseline parser for Irish yields results of 78.54 (UAS) and 71.59 (LAS), while Indonesian-Irish
transfer results are 61.69 (UAS) and 43.88 (LAS).
The lowest scoring source language is Japanese. This parsing model?s output shows less than 3%
accuracy when identifying the ROOT label. This suggests the effect that the divergent word orders have
6
All scores are micro-averaged.
47
on this type of cross-lingual parsing ? VSO (Irish) vs SOV (Japanese). Another factor that is likely to be
playing a role is the size of the Japanese sentences. The average sentence length in the Japanese training
data is only 9 words, which means that this dataset is comparatively smaller than the others. It is also
worth noting that the universal Japanese treebank uses only 15 of the 41 universal labels (the universal
Irish treebank uses 30 of these labels).
As our best performing model (Indonesian) is an Austronesian language, we investigate why this
language does better when compared to Indo-European languages. We compare the results obtained by
the Indonesian parser with those of the English parser (SingleT). Firstly, we note that the Indonesian
parser captures nominal modification much better than English, resulting in an increased precision-recall
score of 60/67 on compmod. This highlights that the similarities in noun-noun modification between
Irish and Indonesian helps cross-lingual parsing. In both languages the modifying noun directly follows
the head noun, e.g. ?the statue of the hero? translates in Irish as dealbh an laoich (lit. statue the hero);
in Indonesian as patung palawan (lit. statue hero). Secondly, our analysis shows that the English parser
does not capture long-distance dependencies as well as the Indonesian parser. For example, we have
observed an increased difference in precision-recall of 44%-44% on mark, 12%-17.88% on cc and
4%-23.17% on rcmod when training on Indonesian. Similar differences have also been observed when
we compare with the French and English (LargestT) parsers. The Irish language allows for the use
of multiple conjoined structures within a sentence and it appears that long-distance dependencies can
affect cross-lingual parsing. Indeed, excluding very long sentences from the test set reveals substantial
increases in precision-recall scores for labels such as advcl, cc, conj and ccomp ? all of which are
labels associated with long-distance dependencies.
With this study, we had hoped that we would be able to identify a way to bootstrap the development
of the Irish Dependency Treebank and parser through the use of delexicalised treebanks annotated with
the Universal Annotation Scheme. While the current treebank data might capture certain linguistic phe-
nomena well, we expected that some cross-linguistic regularities could be taken advantage of. Although
the best cross-lingual model failed to outperform the monolingual model, perhaps it might be possible to
combine the strengths of the Indonesian and Irish treebanks? We performed 5-fold cross-validation on
the combined Indonesian and Irish data sets. The results did not improve over the Irish model. We then
analysed the extent of their complementarity by counting the number of sentences where the Indonesian
model outperformed the Irish model. This happened in only 20 cases, suggesting that there is no benefit
in using the Indonesian data over the Irish data nor in combining them at the sentence-level.
5 Conclusion and Future Work
In this paper, we have reported an implementation of cross-lingual direct transfer parsing of the Irish
language. We have also presented and explained our mapping of the Irish Dependency Treebank to the
Universal POS tagset and Universal Annotation Scheme. Our parsing results show that an Austronesian
language surpasses Indo-European languages as source data for cross-lingual Irish parsing.
In extending this research, there are many interesting avenues which could be explored including
the use of Irish as a source language for another Celtic language and experimenting with the projected
transfer approach of McDonald et al. (2011).
Acknowledgements
This research is supported by the Science Foundation Ireland (Grant 12/CE/I2267) as part of the CNGL
(www.cngl.ie) at Dublin City University. We thank the three anonymous reviewers for their helpful
feedback. We also thank Elaine U?? Dhonnchadha (Trinity College Dublin) and Brian
?
O Raghallaigh
(Fiontar, Dublin City University) for their linguistic advice.
References
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of COL-
ING?10.
48
Joan Bresnan. 2001. Lexical Functional Syntax. Oxford: Blackwell.
?
Ozlem C?etino?glu, Jennifer Foster, Joakim Nivre, Deirdre Hogan, Aoife Cahill, and Josef van Genabith. 2010. LFG
without C-structures. In Proceedings of the 9th International Workshop on Treebanks and Linguistic Theories.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Workshop on Crossframework and Cross-domain Parser Evaluation (COLING2008).
Katri Haverinen, Jenna Nyblom, Timo Viljanen, Veronika Laippala, Samuel Kohonen, Anna Missil?a, Stina Ojala,
Tapio Salakoski, and Filip Ginter. 2013. Building the essential resources for Finnish: the Turku dependency
treebank. Language Resources and Evaluation, pages 1?39.
John Judge, Ailbhe N?? Chasaide, Rose N?? Dhubhda, Kevin P. Scannell, and Elaine U?? Dhonnchadha. 2012. The
Irish Language in the Digital Age. Springer Publishing Company, Incorporated.
Matthias Kromann. 2003. The Danish Dependency Treebank and the DTAG Treebank Tool. In Proceedings of
the Second Workshop on Treebanks and Linguistic Theories (TLT2003).
Teresa Lynn,
?
Ozlem C?etino?glu, Jennifer Foster, Elaine U?? Dhonnchadha, Mark Dras, and Josef van Genabith.
2012a. Irish treebanking and parsing: A preliminary evaluation. In Proceedings of the Eight International
Conference on Language Resources and Evaluation (LREC?12), pages 1939?1946.
Teresa Lynn, Jennifer Foster, Mark Dras, and Elaine U?? Dhonnchadha. 2012b. Active learning and the Irish
treebank. In Proceedings of the Australasian Language Technology Workshop (ALTA), pages 23?32.
Teresa Lynn, Jennifer Foster, and Mark Dras. 2013. Working with a small dataset ? semi-supervised depen-
dency parsing for Irish. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich
Languages, pages 1?11, Seattle, Washington, USA, October. Association for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of
english: The Penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
62?72, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,
Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu, and Castell?o Jungmee
Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of ACL ?13.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for depen-
dency parsing. In Proceedings of the Fifth International Conference on Language Resources and Evaluation
(LREC2006).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the
Eight International Conference on Language Resources and Evaluation (LREC?12).
Nancy Stenson. 1981. Studies in Irish Syntax. T?ubingen: Gunter Narr Verlag.
Elaine U?? Dhonnchadha and Josef van Genabith. 2006. A part-of-speech tagger for Irish using finite-state morphol-
ogy and constraint grammar disambiguation. In Proceedings of the 5th International Conference on Language
Resources and Evaluation (LREC 2006).
Elaine U?? Dhonnchadha, Caoilfhionn Nic Ph?aid??n, and Josef van Genabith. 2003. Design, implementation and
evaluation of an inflectional morphology finite state transducer for Irish. Machine Translation, 18:173?193.
Elaine U?? Dhonnchadha. 2009. Part-of-Speech Tagging and Partial Parsing for Irish using Finite-State Transduc-
ers and Constraint Grammar. Ph.D. thesis, Dublin City University.
49
