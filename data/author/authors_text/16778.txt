Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1718?1727, Dublin, Ireland, August 23-29 2014.
Towards multimodal modeling of physicians? diagnostic
confidence and self-awareness using medical narratives
Joseph Bullard
?
Cecilia Ovesdotter Alm
?
Qi Yu
?
Pengcheng Shi
?
Anne Haake
?
?
College of Computing and Information Sciences
?
College of Liberal Arts
Rochester Institute of Technology
jtb4478@cs.rit.edu
coagla|qi.yu|spcast|arhics@rit.edu
Abstract
Misdiagnosis is a problem in the medical field, often related to physicians? cognitive errors.
Overconfidence is considered a major cause of such errors. Intelligent diagnostic support sys-
tems could benefit from understanding how aware physicians are of their performance when they
estimate their confidence in a diagnosis (i.e. a physician?s diagnostic self-awareness). Shed-
ding light on the cognitive processes related to such awareness could also help improve medical
education. We use a multimodal dataset of medical narratives to computationally model diagnos-
tic confidence and self-awareness based on physicians? linguistic and eye movement behaviors.
Dermatologists viewed images of cutaneous conditions, providing a description, diagnosis, and
certainty level for each image case, while their speech and eye movements were recorded. We
define both a generalized and a personalized approach to binning confidence levels, used in clas-
sification experiments. We also introduce truly multimodal features, which focus on combining
linguistic and eye movement data into multimodal attributes. Results indicate that combinations
of multiple modalities can outperform their constituent modalities in isolation for these problems.
1 Introduction
Misdiagnosis in the medical field is estimated to be as high as 10%-15% (Berner and Graber, 2008;
Croskerry, 2009). Such errors can result in incorrect or delayed treatment, causing patients to experience
additional suffering. Graber et al. (2002) describe three types of diagnostic errors: no-fault errors, result-
ing from atypical disease presentation or limitations of medical knowledge; system errors, resulting from
problems with the health care system; and cognitive errors, resulting from biases or faulty interpretation
on the part of a physician. Cognitive errors in particular have potential for substantial reduction through
education and training aimed at developing clinicians? metacognitive skills. Understanding the cognitive
processes of physicians during diagnosis is also of critical importance for building human-centered di-
agnostic support systems, which could help detect and flag problematic diagnostic self-awareness cases.
Examples of cognitive errors include settling on a final diagnosis too early, without ever considering the
correct diagnosis (Berner and Graber, 2008), or confirmation bias, in which only evidence to confirm a
diagnostic hypothesis is considered (Croskerry, 2003). Overconfidence is generally thought to be a major
cause of such errors (Berner and Graber, 2008; Croskerry, 2008). For example, an overconfident physi-
cian may not question her original thoughts or explore alternative diagnoses until later in the treatment
process. In general, overconfidence may be a systemic problem, reinforced by patients? preferences for
confident doctors, and by a professional environment that favors decisive actions (Katz, 1984). Similarly,
underconfidence can erode patients? trust in their providers. In this study, we view the interplay between
confidence
1
and correctness as a two-dimensional problem (see Figure 1). Ideally, physicians would
have high confidence when correct and low confidence when incorrect, indicated by the upper-left and
lower-right quadrants in Figure 1.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
For consistency, this paper uses the term confidence, treated as interchangeable with certainty and similar synonymous
expressions which may have been used by clinicians in the medical narratives, such as sure, certain, confident, etc.
1718
Appropriate
Confidence
Overconfidence
Underconfidence
Appropriate
Confidence
Confident
Not confident
Correct Incorrect
Figure 1: Two-dimensional view of the confidence and correctness relationship as it relates to diagnostic
self-awareness. A similar conceptual model is presented by Pon-Barry and Shieber (2011). Ideally,
physicians should have high confidence when they are correct and low confidence when incorrect.
Contribution Diagnostic self-awareness is an important phenomenon with implications for clinical
training and practice, yet has received little focus from a computational perspective. We report on com-
putational modeling for predicting the confidence and correctness interplay in diagnosis using features
of physicians? speech, eye movements, and combinations thereof, as dermatologists performed medical
image inspection tasks while narrating their diagnostic thought process. In dermatology, visual expertise
and clinical knowledge are both important. A motivation behind our multimodal approach is that medi-
cal image inspection relies on both the physician?s visual perceptual expertise and conceptual knowledge
base, each of which can be regarded as expressed by eye movement behavior and linguistic behavior,
respectively. We aim to apply this decision modeling to intelligent diagnostic support and clinical tutor-
ing systems. Here we solve a foundational problem by successfully modeling the complex relationship
between physicians? confidence in and correctness of their diagnoses. We also make contributions in
multimodal and linguistic feature analysis: carefully assessing feature modalities that represent physi-
cians? behaviors, and introducing a novel multimodal feature type that focuses on fusing eye movement
and verbal data.
2 Previous Work
Although there are many causes of diagnostic errors (Graber et al., 2005), those resulting from cognitive
errors may be the most challenging to reduce (Croskerry, 2003; Graber et al., 2002), while their reduction
provides high impact. Examples of such errors include flawed perception, biased heuristics, and settling
on a final diagnosis too early (Graber et al., 2002), all of which can be caused by overconfidence (Berner
and Graber, 2008; Croskerry, 2008). Underconfidence may also be a problem if it prevents a physician
from pursuing a correct diagnosis (Friedman et al., 2005).
There is evidence for links between speech and confidence in terms of prosodic features, such as
pitch and loudness (Scherer et al., 1973; Pon-Barry and Shieber, 2011; Kimble and Seidel, 1991), as
well as other characteristics of spoken language, such as speech disfluencies (Womack et al., 2012)
and hedges (Smith and Clark, 1993). Prosodic features have been identified and successfully used in
intelligent tutoring systems (Liscombe et al., 2005), where a student?s confidence (or lack thereof) can
play a key role in effective system response. In medical diagnosis, prosodic and lexical features have
been useful indicators of physicians? confidence and diagnostic correctness, individually (Womack et al.,
2013; McCoy et al., 2012). Other potentially useful information may be evident in speech as well. In
a study by Womack et al. (2012) on a similar dataset, the authors found a relationship between speech
characteristics and physician experience: attending (experienced) physicians used more filled pauses and
spoke more than resident (in-training) physicians. Additionally, verbal features may expose differences
in diagnostic reasoning that may be useful predictors of confidence. Rogers (1996) analyzed a dataset of
spoken chest X-ray examinations by radiologists, remarking that reasoning styles influence physicians?
expectations, and confirmations or contradictions of those expectations can affect their self-reported
confidence levels.
1719
Most relevant literature focuses on linguistic features. Language, as the primary form of human ex-
pression, is certainly critical. However, analyzing meaning may require going beyond linguistic infer-
ence, depending on the context or application. Previous studies have successfully incorporated multiple
expressive modalities when examining linguistic and cognitive processes, such as facial expressions for
video sentiment analysis (P?erez-Rosas et al., 2013) and pointing gestures for referring actions (Gatt and
Paggio, 2013). In such studies, the additional modalities were carefully chosen based on the nature of
the performed tasks. Here, we deal with experts (dermatologists) inspecting images (skin conditions) for
diagnostic purposes, a task that heavily involves their use of visual perceptual expertise, in addition to
conceptual domain knowledge. For this reason, we incorporate features of their eye movements in our
study. There is evidence for ties between perceptual expertise and eye movements during image inspec-
tion tasks (Li et al., 2012b), and we explore if such ties may also relate to a physician?s confidence and
diagnostic self-awareness.
Integrating different expressive modalities is challenging. Previous work involving multimodality has
predominantly treated each in isolation. We further address this challenge by identifying and exploring
truly multimodal features that focus on combining verbal and eye movement data into complex multi-
modal attributes, as it seems reasonable that the two modalities together could be more informative if
linked, and that such complex features represent a natural interactive extension of multimodal semantics.
Evidence for ties between speech and eye movements specifically was found by Li et al. (2012a), in
which sequences of fixations and saccadic eye movements were identified to predominantly align with
particular conceptual units of thought (e.g. primary lesion type) expressed verbally in medical narratives.
3 Data Description and Analysis
This study takes advantage of a dataset previously reported on by Womack et al. (2013), which is briefly
described here for clarity, as Womack et al.?s work ignored the eye movement data. A group of 29 derma-
tologists (11 attending physicians, 18 residents) were each shown a series of 30 images of dermatological
conditions in random order and asked to narrate their diagnosis of each condition. They were asked to
provide a description of the case, a list of differential diagnoses to consider, a final diagnosis, and their
certainty of their final diagnosis, as a percentage. The physicians? verbal descriptions were recorded as
audio and later manually transcribed in detail, including pauses, disfluencies, and other speech phenom-
ena.
2
During this process, the physicians? eye movements were also tracked. Each image was displayed
on a 22? LCD monitor (1650x1050 pixels) with an attached 250Hz SensoMotoric Instruments RED
remote eye-tracker while IViewX software was recording the eye movements.
In this study, the time-aligned pair of verbal description and eye movements for one physician viewing
one image is henceforth called a narrative. Figure 2a shows an example of a verbal description for
one narrative and Figure 2b shows a visualization of the corresponding eye movements. The correct
diagnoses for all images were known for the experiment and each narrative was assigned a binary label
of correct or incorrect.
3
For the purposes of this multimodal study, 238 of the 870 narratives were
excluded due to technical issues that had occurred with the eye tracking or audio capture equipment,
or because the physicians had provided no confidence values for their diagnoses. The remaining 632
narratives were used for the analysis and experimentation reported on in this paper.
3.1 Case Studies towards Understanding Physicians? Confidence and Correctness
The physicians tended to evaluate their confidence towards the upper end of the spectrum, with a me-
dian of 70% confident over all narratives. But diagnostic confidence may be affected by many factors,
including professional experience, case difficulty, and personality. We examine both individual images
and physicians at the extremes of confidence to gain insight into the relationship between confidence and
correctness in the dataset. Table 1 summarizes information for the three image cases that received the
2
Some transcription imperfections may occur.
3
A limited number of narratives in the dataset were labeled half correct if one of two final diagnoses given was correct, and
partially correct if the final diagnosis was too broad. Here, we consider half to be correct, because in such cases the correct
diagnosis was still identified, but partial to be incorrect, because the correct diagnosis was technically not identified.
1720
... um two ... pa- ... pink to purple
macules ... on the ... volar wrist
differential diagnosis ... um fixed
drug eruption ... bites ... urticaria
... uh ... diagnosis fixed drug erup-
tion percent certainty fifty percent
next ...
(a) Sample verbal description. Ellipses
(?...?) show pauses.
(b) Sample eye movement visualization. Circles represent fixations, where the
center is the point of fixation and the radius is proportional to the time fixating at
that point. Lines represent saccades (movements) between fixation points.
Figure 2: Sample verbal description and eye movements for one narrative. The final diagnosis is correct
and the physician was 50% confident.
Confidence Conf. % Correct Rank
Highest
100 100 2
90 100 5
90 100 1
Lowest
50 24 25
50 35 29
45 0 20
Table 1: Images receiving highest and lowest me-
dian confidence values. Difficulty ranking pro-
vided by a dermatology expert with 1 reflecting
the easiest image and 30 the most difficult.
Confidence Conf. % Correct Exp.
Highest
90 53 R
85 50 A
85 41 R
Lowest
38 39 A
30 48 R
15 37 R
Table 2: Most and least confident physicians by
median confidence values given over all images.
The last column shows experience level: experi-
enced attending (A) or resident (R) physician.
highest median confidence values and the three that received the lowest. A domain expert (dermatolo-
gist and clinical educator) who was not a subject in the experiment gave each image a unique difficulty
ranking from 1 to 30, where the image ranked number 1 was considered the easiest to support a correct
diagnosis, and 30 the most difficult. As expected, the highest confidence images were among the easiest,
and vice versa. Accordingly, the higher confidence images were correctly diagnosed by every physician,
while those receiving the lowest confidence were correctly diagnosed much less often. The negative
correlation between image difficulty and median physician confidence was significant using Spearman?s
rank correlation (r
s
= ?0.544, p < 0.005). In other words, higher levels of case difficulty were associ-
ated with lower levels of physician confidence. In contrast, examination of the most and least confident
physicians yields less intuitive results. The physicians with the highest and lowest median confidence
values are shown in the top and bottom halves of Table 2, respectively. Notably, each of the two groups
contained both resident dermatologists-in-training and attending physicians with careers spanning mul-
tiple decades. Also, the most confident physicians were only correct roughly half of the time, and the
least confident physicians? correctness appears quite similar. While this may reflect the sample size, the
observation is interesting nonetheless. Clearly, this points to how complicated diagnostic self-awareness
is, and how potentially useful it would be to computationally infer a physician?s self-awareness for diag-
nostic cases based on their behaviors.
1721
3.2 Confidence Binning
Nearly all confidence values given were multiples of five, or simply numbers close to 100, such as 99%.
4
This makes discretization preferable to using real-numbered values for confidence. Additionally, the
analyses in Section 3.1 revealed patterns of over- or underconfidence in individual physicians. What this
indicates is that ?high? and ?low? confidence involve different numerical values in the minds of different
physicians. This subjectivity could be problematic in doctor-patient interactions and it adds complexity
for predictive modeling involving confidence. To explore the impact, we devise two alternative binary
binning schemes: generalized bins, based on the performance of all physicians in the dataset, and per-
sonalized bins, based on each individual physician?s performance in the training data only. In terms of
application, consider a diagnostic support system which could establish a history for each physician who
uses it. Such a system could implement a generalized binning scheme and predictive model for new
users, and later, after learning from repeated exposure to a given physician, switch to a model based on
that physician?s individual performance. In addition, binning choice may be influenced by context: in a
clinical tutoring system, it may be preferable to compare learners to experienced physicians as a target
population. For the generalized binning scheme, a confidence value greater than or equal to the median
over all physicians is considered high, while a value below is considered low. This results in a slight
imbalance towards high confidence (56% of narratives).
5
We construct the personalized binning scheme
similarly, but using a given physician?s own median confidence in the training data as the dividing line.
In this case, high confidence accounts for 58% of the narratives, similar to that of the generalized bins.
Calling a physician?s median confidence high lets us better distinguish the problem cases: cases of under-
confidence should be strictly less than their ?typical? confidence, while cases of overconfidence should
be at or above typical. The binning scheme used does not affect the correctness value for each narrative,
but it does change the distribution of high and low confidence, with the generalized scheme favoring
over- and underconfidence, and the personalized scheme favoring appropriate confidence. Arguably, the
latter is a better reflection of the expected: over- and underconfidence as the minority classes.
4 Approach and Methodology
There are many ways to approach the problem of predicting physicians? diagnostic self-awareness. Here
we formulate two classification problems, each tested under both binning schemes, yielding a total of
four classification models. We also outline the performance evaluation experiments for the models.
4.1 Classification Problems
We define two classification problems based on the chart in Figure 1 (above). First, we define Confidence
Only, which ignores correctness (the horizontal dimension of Figure 1) and predicts only confidence as
a binary high or low. Intuitively, low confidence might be considered a warning sign for a diagnosis,
alerting a physician to seek additional insight or information.
6
This first problem was used as a stepping
stone to explore and better understand confidence, before incorporating correctness. Next, we define
Confidence & Correctness, which relates confidence with the correctness of the diagnosis (considering
all four quadrants in Figure 1, individually) to better address the more problematic, but interesting, cases.
Distinguishing these four classes could be of use to intelligent tutoring or clinical support systems, which
could respond differently to over- or underconfident users. In general, the full separation of these classes
could ultimately allow for deeper analysis of physician self-awareness.
4.2 Model Evaluation
Before any development took place, the 632 narratives were randomly divided into three subsets: 442
(70%) for training (dev-train), 95 (15%) for testing during development and tuning (dev-test), and 95
4
There were only a few exceptions: one physician gave three values of 3%, another gave a 33% and a 66% (rounded down
from ?two-thirds?), and a third gave a 33%. The latter three cases could also seem intuitive depending on how many conditions
were listed in the differential diagnosis. For example, 66% might indicate that one disease seemed twice as likely as a another.
5
Other simple binning schemes dividing up the 0-100% range were explored, but this binary version allowed for a more
systematic approach to both generalized and personalized binning, without sacrificing performance.
6
Normally, a physician would likely administer tests after the differential diagnosis, before reaching a final diagnosis.
1722
(15%) for final summative evaluation after all development was completed (heldout-test). All three
subsets have similar class distributions. Each of the four classification models were evaluated in two
ways: (1) by training the model on the union of the dev-train and dev-test sets and testing on the heldout-
test set, and (2) by running 50 randomized iterations of 10-fold cross-validation on the entire collection
of 632 narratives. The first evaluation experiment addresses the problem of overfitting by excluding the
heldout-test set from all development, while the second addresses the problem of sampling bias in the
initial set divisions. The results are described in Section 5.2.
5 Models and Results
Here we describe the development and performance of each of the four computational models outlined
in Section 4. We report on logistic regression, which had the best performance in all metrics for all
experiments, after dimensionality reduction (see Section 5.1). The feature selection and modeling was
implemented in Python with the scikit-learn machine learning library (Pedregosa et al., 2011).
5.1 Feature Extraction and Selection
A total of 60 features were examined (see Table 3). The features represented three modalities, moti-
vated by the task the physicians performed and knowledge about dimensions of clinical expertise in this
domain: verbal, composed of lexical, prosodic, and structural features of the narratives; eye movement,
consisting of features of fixations and saccadic eye movements; and truly multimodal features, consisting
of overlapping or simultaneously occurring features from the other two modalities, to reflect integrated
multimodal semantics. Continuing with the theme of personalization, we also created a fourth category
of personal features, with demographics of the physician and statistics about their confidence and cor-
rectness in the training data, in order to model their ?past? performance. The latter simulates how a
system could learn from experience with a particular physician.
As discussed in Section 2, verbal features of confidence have been studied before, and many of the
verbal features used here are inspired by previous work. Some verbal features are based on word choice,
such as amplifiers (e.g. definitely, sure) and modals (e.g. could, might),
7
while other have to do with
silences (or pauses) or prosody. The eye movement and multimodal features are mostly concerned with
fixations, as it seems intuitive that fixation may be associated with thoughtfulness about a particular area
of the image, which may in turn reflect a physician?s confidence.
Initial feature selection was performed on the development data (dev-train and dev-test) using
scikit-learn?s random forest ensemble classifier. This allowed for human-friendly inspection of
useful features. Random forests (Breiman, 2001) are an ensemble method in which numerous decision
trees are constructed, each trained on a randomized subset of the development data, which allows for the
utility of features to be evaluated on many sub-distributions of the data. The importance of a feature can
then be approximated as the sum of the error reduction at each node that splits on that feature, weighted
by the population size at that node. This reflects the fact that features used near the root of the tree often
handle a larger number of individuals. The importance values for all features will sum to 1. We consider
any feature that appeared in the top 20 of the ranked features for any model to be important, and all such
types of features are marked in bold in Table 3. Interestingly, the useful features for all classification
models were almost the same, with a few transpositions in the ordering. The exception was past confi-
dence, which was useful under generalized, but disappeared under personalized, as expected, since the
personalized scheme effectively normalizes each physician?s confidence values.
Interpreting the results for the verbal features, silence duration (statistics about the durations of all
silences) and the duration of narrative were most useful. Intuitively, this may relate to thoughtfulness or
contemplation. Additionally, words per second, or speech rate, was also useful, again perhaps relating to
more careful or thorough inspection/diagnosis. As discussed earlier, ties between speech and confidence
have been well-studied, while eye movements are underreported. It seems intuitive that eye movement
7
Such word-choice features were mostly based on lexical lists, and some overlap may occur. The cutaneous
conditions feature contained multiword expressions. These could be improved by using resources such as UMLS
(http://www.nlm.nih.gov/research/umls/) or WordNet (http://wordnet.princeton.edu/).
1723
Verbal (29)
Duration of narrative
Number of silences
Silence duration (?, ?, ?)
Duration of initial silence
Number of filled pauses
Word type-token ratio
Words per second
Cutaneous conditions (n, %)
Pronouns 1st (n, %)
Pronouns 3rd (n, %)
Modals (n, %)
Amplifier words (n, %)
Speculative words (n, %)
Negations (n, %)
Pitch (m, M , ?)
Intensity (m, M , ?)
Eye movement (11)
Fixation duration (?, ?, ?) Number of fixations
Saccade duration (?, ?, ?) % image area fixated
Saccade amplitude (?, ?, ?)
Multimodal (14)
% of initial silence time fixating
% of total silent time fixating
% of total fixation time silent
Words per second during fixation
Pitch during fixations (?, range)
Intensity during fixations (?, range)
Pitch of filled pauses (m, M , ?)
Intensity of filled pauses (m, M , ?)
Personal (6)
Attending vs. Resident Past correctness
Years of experience
Past confidence (m, M , ?)
Table 3: Features examined for classification (60 total), grouped by modality. Symbols in parentheses
indicate statistics over all occurrences of a feature in a narrative: raw count (n), raw count divided by
the total number of words (%), sum (?), mean (?), standard deviation (?), min (m), max (M ), range
(range). Useful features are boldfaced. If a feature has multiple statistics, the useful ones are underlined.
features may be more related to correctness. For example, the most useful eye movement feature was
% image area fixated, computed using a grid overlaid onto the image. If more of the image was fixated
upon, then it may have contained more areas of interest, or more visual evidence may have been sought,
which may also be related to case difficulty. Similarly, features of saccade amplitude (the angle of a
saccadic eye movement) may reflect physicians feeling a need to explore additional visual evidence by
switching focus between distant areas in an image. It is not surprising that the useful individual features
from verbal and eye movement modalities were also useful when combined as multimodal features. In
particular, simultaneous silence and fixation were the most useful, which again might indicate contem-
plation and analytical cognitive processing. This suggests that expression of confidence and diagnostic
self-awareness is at least partially a multimodal phenomenon.
Although the random forest method could be used for dimensionality reduction, we instead use Princi-
ple Component Analysis (PCA) in evaluation below, as it gave better performance gains in development.
The purpose of the random forest method was to examine which verbal, eye movement, and multimodal
features were most informative for classification, as we are interested in understanding how these modal-
ities relate to confidence and correctness. The latent features resulting from PCA are linear combinations
of the features, and thus would not allow for such inspection. The number of PCA components was
optimized for classification accuracy in cross-validation for each of the four classification models. Each
problem had a different number of principal components, indicating that both the binning scheme and the
classification problem type affected which features were identified as more collectively discriminative
by PCA.
5.2 Results and Evaluation
Heldout narratives We addressed the problem of overfitting by withholding 15% (n = 95) of the
narratives as an unseen final evaluation set. All predictive models performed well above their respective
majority class baselines (see Table 4). The Confidence Only models were able to reach higher accuracy,
precision, and recall than the joint Confidence & Correctness models. The exception is the accuracy
relative to baseline for personalized Confidence Only, which may be due to its higher baseline. As men-
tioned in Section 3.2, the generalized binning scheme is biased towards over- and underconfidence, and
the personalized towards appropriate confidence. The per-class metrics (not shown here) reflect this fact,
with overconfidence having higher precision and recall under generalized binning than under personal-
ized. Additionally, under the personalized scheme underconfidence is particularly underrepresented and
thus more difficult to predict.
1724
Binning Problem N Majority Class % BL % Acc. P R
Generalized
Conf. Only 2 High Confidence 53 76 (+23) 0.76 0.76
Conf. & Corr. 4 Overconfidence 37 53 (+16) 0.42 0.42
Personalized
Conf. Only 2 High Confidence 65 77 (+12) 0.75 0.73
Conf. & Corr. 4 Appropriate High 37 53 (+16) 0.38 0.42
Table 4: Performance metrics for the heldout-test set under each binning scheme with logistic regression
and PCA. All four models performed well above the majority class baselines (% BL) of their respective
problems (each with N many class labels). Precision (P) and recall (R) are each macro-averaged.
Random cross-validation A potential drawback of the initial development strategy used here is that
the initial random splits may bias classification models. To address this problem, after the heldout testing,
50 randomized iterations of 10-fold cross-validation were performed on the total collection of narratives,
the results of which are in Table 5. The personalized binning scheme was designed to mimic a sys-
tem that could adapt to a physician?s performance history, and thus the statistics used for personalized
confidence binning were recomputed on the training data within each individual cross-validation fold.
It is therefore not possible to establish a baseline for the personalized confidence binning outside of
a given fold. Instead, we take the mean of the percent accuracy above baseline from each test fold
(
1
k
?
k
i=1
(accuracy
i
? baseline
i
)). All models performed well above their respective baselines, which
is in line with observations from heldout testing.
Binning Generalized Personalized
Problem C.O. C&C C.O. C&C
Acc. above
baseline
+14 +9 +13 +12
Precision 0.70 0.25 0.69 0.32
Recall 0.70 0.38 0.57 0.37
Table 5: Performance metrics for logistic re-
gression with 50 randomized iterations of cross-
validation using all narratives for Confidence Only
(C.O.) and Confidence & Correctness (C&C). We
average the accuracy above baseline from each in-
dividual fold. Precision and recall are each macro-
averaged for each problem.
Feature Generalized Personalized
modality C.O. C&C C.O. C&C
V +13 +9 +12 +11
E +7 +6 +11 +10
MM +7 +4 +6 +5
V+E +13 +9 +13 +11
V+MM +14 +8 +11 +11
E+MM +10 +6 +13 +11
V+E+MM +14 +9 +13 +12
Table 6: Modality study with cross-validation for
Verbal (V), Eye movement (E), and Multimodal
(MM) features, measured in accuracy above re-
spective baselines, averaged over all folds. Most
modality combinations equaled or slightly im-
proved on constituent modalities in isolation.
Modality study We also performed a study within the cross-validation testing to investigate the impact
of different feature modality combinations on classification (see Table 6). Importantly, the verbal modal-
ity alone was more powerful than the eye movement or multimodal features, but most combinations of
modalities resulted in slightly higher or equal accuracy compared to their isolated constituent modali-
ties. This suggests that, as we projected, considering multiple modalities of a physician?s behavior can
help reveal their confidence and self-awareness, but also that verbal features are the most informative,
likely since verbal expression is the primary means to tap into physicians? rich and tacit conceptual un-
derstanding of a diagnostic case. The multimodal features, which focused on combining verbal and eye
movement data, did not improve performance over baselines as much as the simple combination of the
individual verbal and eye movement features. One reason for this could be that a person?s speech and
eye movements are not perfectly temporally aligned (Vaidyanathan et al., 2012), and this asynchronous
relationship may affect the meaningfulness of our multimodal feature measurements. Additionally, these
eye movement features may be at a much finer spatial or temporal scale than the verbal features.
1725
6 Conclusions
This study examined a dataset of medical narratives consisting of verbal descriptions, eye movements,
and self-reported confidence values, and used it to model physicians? confidence in diagnosis, as well
as their diagnostic self-awareness. The Confidence Only problem involves the expression of confidence
based on clinicians? belief, but it is important to understand the relationship to clinicians? actual diag-
nostic performance. This distinction is key because, while predicting confidence alone is a stepping
stone, self-awareness is the ability to additionally align one?s confidence with unknown correctness,
which involves human intuitive and analytical reasoning (another topic of interest to the medical field,
see Hochberg et al. (2014)). Case studies of the most and least confident physicians revealed a com-
plex relationship between confidence and correctness, and highlighted the need for exploring clinical
self-awareness. We also defined a personalized binning scheme for physician confidence levels, taking
into account a physician?s past confidence when drawing the line between high and low confidence, and
compared this to a generalized binning scheme based on performance of all physicians. In tandem, these
approaches to confidence binning could be used by an intelligent diagnostic support system.
We incorporated previously unused eye movement information from this dataset, and introduced truly
multimodal features which directly combined physicians? verbal and eye movement behaviors. While
physicians? eye movement and multimodal features were not individually as powerful as verbal features,
combinations of the three groups mostly produced classification improvements that were slightly better
than, or at least as good as, their constituent feature groups in isolation. The best performance for the
majority of models was achieved by considering features from all three modalities. This suggests that
eye movements help convey confidence and diagnostic self-awareness. The multimodal features did not
help as much, which we believe is explained by the more flexible temporal relationship between speech
and eye movements in the human mind. We leave the multimodal alignment challenge to future work.
Some pitch features implemented without speaker-dependent analysis were useful for classification, but
future work may benefit from pitch feature representations that adapt to demographic variation. Another
area for future work beyond the scope of this study includes examining alternative ways of combining
confidence and correctness classes, such as merging the diagonals of Figure 1 into a binary classifica-
tion of appropriate vs. inappropriate (i.e. the union of over- and underconfidence). Such alternatives
may present additional challenges for classification, but could also provide benefits for simpler clinical
support applications that may not be concerned with differentiating all four classes.
Acknowledgements
This work was supported by a seed award, and its dissemination partially by a Kodak Endowed Chair
award, both from the Golisano College of Computing and Information Sciences at RIT. The original data
collection was supported by NIH grant 1 R21 LM01003901A1. The content is solely the responsibility
of the authors and does not necessarily represent the official views of the National Institutes of Health.
The authors also thank Rui Li, and appreciate the helpful comments from reviewers.
References
Eta S. Berner and Mark L. Graber. 2008. Overconfidence as a cause of diagnostic error in medicine. The American
Journal of Medicine, 121(5A):S2?S23.
Leo Breiman. 2001. Random forests. Machine Learning, 45:5?32.
Pat Croskerry. 2003. The importance of cognitive errors in diagnosis and strategies to minimize them. Academic
Medicine, 78(8):775?780, August.
Pat Croskerry. 2008. Overconfidence in clinical decision making. The American Journal of Medicine,
121(5A):S24?S29.
Pat Croskerry. 2009. A universal model of diagnostic reasoning. Academic Medicine, 84(8):1022?1028, August.
Charles P. Friedman, Guido G. Gatti, Timothy M. Franz, Gwendolyn C. Murphy, Frederic M. Wolf, Paul S. Heck-
erling, Paul L. Fine, Thomas M. Miller, and Arthur S. Elstein. 2005. Do physicians know when their diagnoses
are correct? Journal of General Internal medicine, 20:334?339, April.
1726
Albert Gatt and Patrizia Paggio. 2013. What and where: An empirical investigation of pointing gestures and de-
scriptions in multimodal referring actions. In Proceedings of the 14th European Workshop on Natural Language
Generation, pages 82?91, Sofia, Bulgaria, August 8-9.
Mark Graber, Ruthanna Gordon, and Nancy Franklin. 2002. Reducing diagnostic errors in medicine: What?s the
goal? Academic Medicine, 77(10):981?992, October.
Mark L. Graber, Nancy Franklin, and Ruthanna Gordon. 2005. Diagnostic error in internal medicine. Archives of
Internal Medicine, 165:1493?1499, July 11.
Limor Hochberg, Cecilia Ovesdotter Alm, Esa M. Rantanen, Caroline M. DeLong, and Anne Haake. 2014. Deci-
sion style in a clinical reasoning corpus. BioNLP 2014.
Jay Katz. 1984. Why doctors don?t disclose uncertainty. Hastings Center Report, 14:35?44.
Charles E. Kimble and Steven D. Seidel. 1991. Vocal signs of confidence. Journal of Nonverbal Behavior,
15:99?105.
Rui Li, Jeff Pelz, Pengcheng Shi, Cecilia Ovesdotter Alm, and Anne Haake. 2012a. Learning eye movement
patterns for characterization of perceptual expertise. In ETRA 2012 Proceedings of the Symposium on Eye
Tracking Research and Applications, pages 393?396, Santa Barbara, CA, March 28-30.
Rui Li, Jeff Pelz, Pengcheng Shi, and Anne Haake. 2012b. Learning image-derived eye movement patterns for
characterization of perceptual expertise. In Proceedings of CogSci 2012, pages 1900?1905.
Jackson Liscombe, Julia Hirschberg, and Jennifer J. Venditti. 2005. Detecting certainness in spoken tutorial
dialogues. In Proceedings of Interspeech 2005, pages 1837?1840, Lisbon, Portugal.
Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng Shi, and Anne Haake. 2012. Link-
ing uncertainty in physicians? narratives to diagnostic correctness. In Proceedings of the ACL-2012 Workshop
on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), pages 19?27, Jeju,
Republic of Korea, 13 July.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12:2825?2830.
Ver?onica P?erez-Rosas, Rada Mihalcea, and Louis-Phillippe Morency. 2013. Utterance-level multimodal sentiment
analysis. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages
973?982, Sofia, Bulgaria, August 4-9.
Heather Pon-Barry and Stuart M. Shieber. 2011. Recognizing uncertainty in speech. EURASIP Journal on
Advances in Signal Processing, 2011(251753).
Erika Rogers. 1996. A study of visual reasoning in medical diagnosis. In Proceedings of the Eighteenth Annual
Conference of the Cognitive Science Society, pages 213?218, La Jolla, California, 12-15 July.
Klaus R. Scherer, Harvey London, and Jared J. Wolf. 1973. The voice of confidence: Paralinguistic cues and
audience evaluation. Journal of Research in Personality, 7:31?44, June.
Vicki L. Smith and Herbert H. Clark. 1993. On the course of answering questions. Journal of Memory and
Language, 32(1):25?38.
Preethi Vaidyanathan, Jeff Pelz, Wilson McCoy, Cara Calvelli, Cecilia Ovesdotter Alm, Pengcheng Shi, and Anne
Haake. 2012. Visualinguistic approach to medical image understanding. In Proceedings of the AMIA 2012
Annual Symposium, Chicago, Illinois, November.
Kathryn Womack, Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng Shi, and Anne
Haake. 2012. Disfluencies as extra-propositional indicators of cognitive processing. In Proceedings of the
ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 1?9, Jeju, Republic of Korea, 13 July.
Kathryn Womack, Cecilia Ovesdotter Alm, Cara Calvelli, Jeff B. Pelz, Pengcheng Shi, and Anne Haake. 2013.
Markers of confidence and correctness in spoken medical narratives. In Proceedings of Interspeech 2013, pages
2549?2553, Lyon, France, August 25-29.
1727
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 1?9, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Disfluencies as Extra-Propositional Indicators of Cognitive Processing
Kathryn Womack
Dept. of ASL
& Interpreting Edu.
kaw8159@rit.edu
Wilson McCoy
Dept. of Interactive
Games & Media
wgm4143@rit.edu
Cecilia Ovesdotter Alm
Dept. of English
coagla@rit.edu
Cara Calvelli
College of Health
Sciences & Tech.
cfcscl@rit.edu
Jeff B. Pelz
Center for
Imaging Science
pelz@cis.rit.edu
Pengcheng Shi
Computing &
Information Sciences
spcast@rit.edu
Anne Haake
Computing &
Information Sciences
anne.haake@rit.edu
Rochester Institute of Technology
Abstract
We explore filled pause usage in spontaneous
medical narration. Expert physicians viewed
images of dermatological conditions and pro-
vided a description while working toward a
diagnosis. The narratives were analyzed for
differences in filled pauses used by attending
(experienced) and resident (in-training) physi-
cians and by male and female physicians. At-
tending physicians described more and used
more filled pauses than residents. No differ-
ence was found by speaker gender. Acoustic
speech features were examined for two types
of filled pauses: nasal (e.g. um) and non-nasal
(e.g. uh). Nasal filled pauses were more of-
ten followed by longer silent pauses. Scores
capturing diagnostic correctness and diagnos-
tic thoroughness for each narrative were com-
pared against filled pauses. The number of
filled and silent pauses trends upward as cor-
rectness scores increase, indicating a tentative
relationship between filled pause usage and
expertise. Also, we report on a computational
model for predicting types of filled pause.
1 Introduction
Although they are often not consciously realized,
disfluencies are common in everyday speech. In an
overview of several studies, Fox Tree (1995) esti-
mates that approximately 6% of speech is disflu-
ent. Disfluencies include filled pauses, silent pauses,
edited or repeated words, and sounds such as clear-
ing one?s throat or click noises. Disfluencies affect
the way that listeners comprehend speech in learn-
ing situations (Barr, 2003), formulate opinions of
the speaker as being more or less fluent (Lo?vgren
and van Doorn, 2005), and even parse grammatically
complex sentences (Bailey and Ferreira, 2003).
Since disfluencies are generally absent in writ-
ten text, they are irrelevant when analyzing text for
extra-propositional meaning, such as uncertainty or
modality (Vincze et al, 2008, for example). In con-
trast, when studying meaning in spoken language,
disfluencies provide information about a speaker?s
cognitive state. For example, they might indicate
cognitive load, uncertainty, confidence, thoughtful-
ness, problems in reasoning, or stylistic preferences
between individuals or groups of individuals. We
study filled pauses (e.g. um and uh) and leave other
disfluency types for future work.
The presence of filled pauses could indicate
context-dependent facets of cognitive reasoning pro-
cesses. We examine filled pauses present in the
speech of highly-trained dermatologists who were
shown images of dermatological conditions and
asked to provide a description and diagnosis. We
look at the difference between two different types
of filled pauses: those with nasal consonants, such
as um; and those without nasal consonants, such as
uh. We build a computational model to confirm find-
ings that nasal and non-nasal filled pauses differ by
prosodic and contextual features. In addition, we
first compare whether there is a difference between
filled pause use for variables such as level of physi-
cian expertise and gender. We also examine the rela-
tionship of correctness in the diagnostic process with
respect to filled pause use.
There is evidence that filled pauses indicate cog-
nitive processing difficulties and could change the
1
speaker?s intended meaning or the listener?s per-
ceived meaning of an utterance. However, such im-
plicit meanings are severely understudied in previ-
ous work, especially in specialized, high-stakes do-
mains such as medical diagnostics. Little is under-
stood about what factors impact the linguistic behav-
ior of using certain filled pauses rather than others,
and how the use of filled pauses differs based on
level of expertise, gender, or diagnostic correctness.
Looking into these differences is useful to form a
better understanding of the relationship between lan-
guage and specialized decision-making processes.
More specifically, it is necessary to improve the un-
derstanding of how speakers? use of filled pauses
differs based on the context of speech and how
they change the meaning and reception of speech in
extra-propositional ways.
2 Previous Work
Filled pauses in English include monosyllables with
and without nasal consonants, such as um and uh re-
spectively. Filled pauses are most common in un-
structured, spontaneous speech, but they are also
present in prompted, structured speech; and occur
in both monologues and dialogues.
Much research has been done into hedging, nega-
tion, and other propositional features that change
the meaning or modality of phrases (Morante and
Sporleder, in press). Less research has been done
into the usage of filled pauses and their relation-
ship to certainty and speculation. It has been shown
that disfluencies are used to indicate uncertainty in
speakers? forthcoming statements or to indicate that
the speaker is engaged in the discourse but working
to formulate their response (Brennan and Williams,
1995; Smith and Clark, 1993). These studies found
that speakers less confident of their answers take
longer to answer and use more disfluencies.
Recent studies have suggested that disfluencies
provide meaningful information about the speaker?s
cognitive or linguistic processes (Arnold et al,
2003; Bortfeld et al, 2001; Corley and Stewart,
2008; Oviatt, 1995, for example), and are uninten-
tional indications that the speaker is having difficulty
formulating upcoming speech.
More specifically, it has been shown that the two
major categories of filled pauses, i.e. nasal and non-
nasal, are specific indicators of the level of cognitive
load, with nasal filled pauses indicating higher load
and non-nasal filled pauses indicating lower load.
Barr (2001) performed an experiment in which a
speaker described one of several visible images to
a listener who then selected the image being de-
scribed. In this study as well as in Barr and Seyfid-
dinipur (2010), listeners focused on a topic that was
new to the discourse or exceptionally complex when
they heard the speaker say um. Although they did
not differentiate between nasal and non-nasal filled
pauses, Arnold et al (2003; 2007) found in similar
experiments that filled pauses often preceded unfa-
miliar or complex objects.
There is evidence that speakers use filled pauses
to indicate different processing difficulties. Clark
and Fox Tree (2002) describe four different filled
pauses that are annotated in the corpora they use.
These are uh, um, and their elongated versions u:h
and u:m. They argue that each of these corresponds
to a different following pause time with uh being
followed by the shortest pause time, then u:h, um,
and u:m followed by the longest. It is important to
note that their primary corpus is the London-Lund
Corpus of Spoken English, in which the pause times
were annotated based on the transcriber?s estimate of
pause time in units of ?one light foot? or ?one stress
unit? (Clark and Fox Tree, 2002, p. 80) rather than
measured in seconds.1
However, studies on filled pauses by Barr (2001)
and Smith and Clark (1993) measured the duration
of silent pauses in seconds and confirm that um was
followed by longer silent pauses than uh. The hy-
pothesis suggested by Barr, Clark and Fox Tree, and
Smith and Clark is that uh indicates a minor delay
and lower level of cognitive difficulty while um in-
dicates a major delay due to higher level of difficulty
in speech planning and production.
On the other hand, a study by O?Connell and
Kowal (2005) refuted the findings of Clark and Fox
Tree and showed that specific filled pauses could
not predict pause time in their corpus of TV inter-
views. O?Connell and Kowal?s corpus was six in-
terviews conducted by various TV personnel with
1The difference between listeners? perception of duration
and actual duration is an important one because perceptual and
actual duration do not always match (Megyesi and Gustafson-
Capkova, 2002; Spinos et al, 2002).
2
Hillary Clinton because these ?professional speak-
ers? (O?Connell and Kowal, 2005, p. 560) should
be more likely to use filled pauses according to con-
vention. However, speech in public TV interviews is
likely to be pre-planned and highly self-monitored
by the speakers, and it may not be appropriate to
consider this situation a model for spontaneous, less
formal, and less public speech. It has been shown
that rate and use of filled pauses can vary widely
within certain fields (Schachter et al, 1991), in situ-
ations that are more or less structured (Oviatt, 1995),
and depending on the formality of the situational
context (Bortfeld et al, 2001).
3 Data, Annotation, and Methods
Data were acquired from a study involving 16 der-
matologists, including 12 attending physicians and 4
residents. The participants were evenly split for gen-
der. These physicians were shown 50 images of dif-
ferent dermatological conditions and asked to pro-
vide a description and diagnosis of each. In a mod-
ification of the Master-Apprentice scenario (Beyer
and Holtzblatt, 1997), each observer explained his
or her thoughts and processes to a student who was
silent. These are monologues; however, the Master
has the feeling of interaction and of dialogue.
Audio of each description was recorded while
eye-movements were tracked. The relationship be-
tween eye-movements and extra-propositional fea-
tures will be the topic of a later study. The audio files
were manually single-annotated and time-aligned at
the word level in Praat, a software for acoustic and
phonetic analysis (Boersma, 2001). A section of
the spoken narrative with time-alignment is pictured
in Figure 1. Praat and Python scripts were used to
computationally extract measurements of pitch, in-
tensity, and duration for words, silent pauses, and
narratives. In total, there were 800 audio-recorded
narratives. At this time, 707 of these narratives have
been time-aligned and annotated and only these are
used in this study.
Four transcribers worked independently on time-
alignment, and they were given instructions by one
coordinator. Every spoken token was included in
the transcriptions, including filled pauses, extra-
linguistic sounds such as clicks, repairs, and silent
pauses. Annotators were instructed to mark only
Figure 1: Screenshot of the program Praat which was
used to time-align each narrative and extract acoustic
prosodic information about the physicians? speech.
silent pauses that were longer than 30 milliseconds,
because it has been shown that pauses under 20-30
ms are not consistently perceived by listeners in dis-
course (Kirsner et al, 2002; Lo?vgren and van Doorn,
2005).
After word-level time-alignment, each narrative
was independently annotated by three expert derma-
tologists who did not participate in the original data
elicitation procedure. Each narrative was examined
for medical lesion morphology (the description of
the condition), differential diagnosis (possible diag-
nostic conditions), and final diagnosis (the diagno-
sis that the observer found most likely). These inde-
pendent experts annotated the physicians? diagnostic
correctness for the three steps of the diagnostic pro-
cess. They annotated medical lesion morphology as
correct, incorrect, correct but incomplete, or none,
indicating that no medical morphology was given.
Final diagnosis was labeled as correct, incorrect, or
none, and differential diagnosis was rated as yes, no,
or no differential given. An analysis of the annotated
data set is discussed by McCoy et al (Forthcoming
2012).
4 Results and Discussion
4.1 Types of Filled Pauses
Nasal filled pauses included hm and um and non-
nasal filled pauses included ah, er, and uh. We an-
alyzed nasal and non-nasal filled pauses as groups
rather than each individual filled pause because the
number of filled pauses within each category was not
balanced. Higher token counts of uh and um were
identified, with fewer ah, er, and hm filled pauses. In
comparing use of nasal and non-nasal filled pauses,
3
FPs No. Dur. St. Dev. %
hm 78 0.48 s 0.20 2%
um 1439 0.51 s 0.19 36%
Total
(nasal)
1517 0.50 s 0.19 38%
ah 23 0.46 s 0.23 1%
er 9 0.26 s 0.09 <1%
uh 2401 0.36 s 0.16 61%
Total (non-
nasal)
2433 0.36 s 0.16 62%
Total (all) 3950 0.42 s 0.19 100%
Table 1: Total number of each type of filled pause (FPs)
with mean duration in seconds, standard deviation of the
mean duration, and percentage of all filled pauses.
we considered all 707 narratives. The number of to-
kens and average duration for each filled pause is
given in Table 1.
The average filled pause duration was slightly
longer for nasal than for non-nasal, likely due to the
segmental quality.
In total, 38% of the filled pauses in our data set are
nasal. However, observers vary widely in their indi-
vidual usage, from one observer who used 22 non-
nasal (10%) and 189 nasal (90%) filled pauses to an
observer at the other extreme who used 562 non-
nasal (97%) and only 19 nasal (3%) filled pauses.
Some people seem to have a tendency to use one
type of filled pause over the other.
Clark and Fox Tree (2002) found that nasal filled
pauses were more often followed by silent pauses
and that those silences were on average longer than
that of non-nasal filled pauses. Our data are consis-
tent with this as shown in Tables 2 and 3,2 and Fig-
ure 2. Of the total nasal filled pauses, 70% were fol-
lowed by a silent pause, whereas only 41% of non-
nasal filled pauses were followed by a silent pause.
The mean duration of silent pauses following
nasal filled pauses was 1.5 s while non-nasal was 1.1
s, which indicates a difference significant enough
that it could be recognized by a listener. These find-
ings show that nasal filled pauses are good indica-
tors of continuing delay, which supports Clark and
Fox Tree?s hypothesis that nasal and non-nasal filled
2The data were analyzed using two-sample t-tests assuming
unequal variances.
Nasal
(hm, um)
Non-nasal
(ah, er, uh)
p
Dur. of FPs 0.50 s 0.36 s < 0.01
Dur. of FPs +
SILs
2.46 s 1.37 s < 0.01
No. of FPs 1517 2433 n/a
Table 2: Mean duration in seconds of filled pauses (FPs),
and mean duration of the filled pause including the span
of any preceding and following silences. If there were no
silences, only the duration of the filled pause was used to
calculate the mean.
Nasal
(hm, um)
Non-nasal
(ah, er, uh)
p
Dur. of pre.
SILs
1.19 s 1.15 s 0.4
No. of pre.
SILs
1167 1197 n/a
Dur. of foll.
SILs
1.50 s 1.07 s < 0.01
No. of foll.
SILs
1059 1006 n/a
Table 3: Mean duration in seconds of silent pauses (SILs)
preceding filled pauses, silent pauses following filled
pauses, and the number of tokens for each. Durations
were only considered if there was a silence, so the num-
ber of silences was different for each calculation.
Figure 2: The percentage of nasal and non-nasal filled
pauses with a preceding silent pause, following silent
pause, and a silent pause both preceding and following.
pauses are used to indicate different levels of diffi-
culty in speech planning. Taken with the results of
experiments by Barr (2001) that nasal filled pauses
are more often used before a topic that is relatively
4
complex or new to discourse, it seems that nasal
filled pauses indicate a higher level of cognitive dif-
ficulty than non-nasal filled pauses.
In their previously-mentioned study, Clark and
Fox Tree also found that nasal filled pauses were
more often preceded by delays and that those delays
were longer. Similarly, in our data 77% of the nasal
filled pauses were preceded by silences, compared
with 49% of non-nasal.
No difference was found in the mean duration of
preceding silences, however. Although this conclu-
sion is tentative, it seems that the duration of the
preceding pause could be the maximum length of
silence a speaker feels is permissible before needing
to indicate their continuing participation in the dis-
course. This supports Jefferson?s (1989) findings of
a ?standard maximum silence? of around 1 second
in discourse. At that point, the speaker could need
to signal that they have more to say, using a nasal
filled pause if they anticipate a long delay or a non-
nasal filled pause if they anticipate a shorter delay.
The longer duration of surrounding silent pauses for
nasal filled pauses also supports the conclusion that
they indicate higher cognitive load and more pre-
planning. This critical finding highlights the im-
portance of considering filled pauses in computa-
tional modeling and hint at their potential usefulness
across phenomena of extra-propositional meaning.
4.2 Gender
Traditional stereotypes have held that women are
less confident speakers than men. When women and
men use the same number of hedge words or mod-
ifiers, women are judged more harshly as sounding
passive or uncertain (Bradley, 1981). Although dif-
ferent rates and ratios of filled pauses were identi-
fied, Acton (2011), Binnenpoorte et al (2005), and
Bortfeld et al (2001) all found that women used a
lower rate of filled pauses than men. Acton also
found that women consistently used a higher ratio
of nasal filled pauses.
Our data were analyzed at the level of diagnostic
narrative based on the means of: number of filled
pauses, filled pauses per second, the percentage of
filled pauses (i.e. the rate per 100 words), the num-
ber of nasal filled pauses, and the percentage of nasal
filled pauses. The difference between the means was
not statistically significant, confirmed by the com-
puted p-score.3 Hence, our data do not support a dif-
ference in men?s and women?s use of filled pauses.
There are several possible explanations for this.
For example, it has been shown that women tend
to be more conscious of their speaking style than
men because they are aware of the stereotyping men-
tioned previously (Gordon, 1994), and they may
make more effort to speak clearly. Acton (2011) and
Bortfeld et al (2001) noted different usage of filled
pauses by men and women in different situations.
Whereas our results point to gender neutrality and
refute the common gender bias as well as findings
of previous studies, we recognize that our results
could reflect that this study involved a largely ho-
mogeneous professional and educational group. The
studies mentioned thus far used corpora consisting
of casual conversations in various situations with in-
dividuals of various backgrounds. Further research
into gender differences in expert fields could clarify
this factor further.
4.3 Level of Expertise
Our data were analyzed based on the means per nar-
rative, similar to Section 4.2, but comparing levels
of expertise (attending versus resident physicians).
Attending physicians? narratives had a longer mean
duration and significantly more words. Attending
physicians also used more filled pauses, a higher rate
of filled pauses per 100 words, and a higher percent-
age of nasal filled pauses (see Table 4).4
One probable explanation for the difference is that
the experienced attendings noticed more about the
image, leading them to give more information about
their thought processes and go into more detail than
residents. It is possible also that the attendings?
experience could have provided them with a larger
conceptual space and options to explore. This ex-
plains the longer narrative time and the higher num-
ber of words used. Many of the dermatological
terms used are highly complex and may require ex-
planation on the part of the observer, and other stud-
3The mean of each category was determined for each ob-
server, and then analyzed using a two-sample t-test. In total, we
had 355 narratives from males and 352 from females.
4These results were calculated using the mean of each ob-
server and each narrative. A paired t-test was used to compare
means for residents on each image against means for attendings
on each image.
5
For Narra-
tives
Attendings?
Means
Residents?
Means
p
Total Dur. 46.1 s 33.8 s < 0.01
No. of Words 85.7 50.9 < 0.01
No. of FPs 6.3 1.9 < 0.01
% FPs 8% 4% < 0.01
% Nasal FPs 0.4% 0.2% < 0.01
Table 4: Analysis considered, at the narrative level, at-
tending and resident physicians? mean total duration,
number of words (including filled and silent pauses),
number of filled pauses (FPs), percentage of filled pauses
of total words (total words includes pauses; without
pauses, this rate would be higher), and percentage of
nasal filled pauses of total filled pauses.
ies have found that the filled pause rate increases as
the utterance length increases (Oviatt, 1995; Bort-
feld et al, 2001), so one would expect to see more
filled pauses used in longer descriptions.
One issue with our data is that the number of at-
tending physicians and the number of resident physi-
cians is not balanced. We had 592 narratives done by
12 attendings and 115 done by 4 residents. All val-
ues were calculated using means so the values are
not weighted based on the number of narratives ana-
lyzed. However, we have previously mentioned that
personal preference plays a role in the usage of filled
pauses, and we have a wider variety of attending ob-
servers than resident observers. It could be that our
resident observers happened to be the kinds of peo-
ple who do not use many filled pauses.
4.4 Diagnostic Correctness
Three scores were determined for each narrative.
The first score was the holistic expert score provided
by the expert annotators, based on ?relevancy, thor-
oughness, and accuracy? of each narrative from 1
to 3 with 3 being the best. The second score was
an overall correctness score which spanned from
0 to 3, with one-third of a point given per inde-
pendent annotator for each step (i.e. medical lesion
morphology, differential diagnosis, and final diag-
nosis) if correct and 13 ? 0.5 points given for cor-
rect but incomplete. The last score was the not-
given score which, similar to the correctness score,
spanned from 0 to 3 with one-third of a point given
per annotator for each step if the original observer
Figure 3: Average number of filled pauses per narrative
by observer (y-axis) against the holistic expert score, cor-
rectness score, and not-given score (x-axis).
did not provide that information.5
Correlation between these three scores and the
number or rate of words, filled pauses, and silent
pauses was not strong enough to make predictions,
indicating that more factors than just the scores
should be considered. However, certain trends were
evident. As the holistic expert and correctness
scores improved, the means of narratives? total du-
ration in seconds and total number of words also in-
creased. This finding, combined with the fact that
experienced physicians spoke more and had higher
average correctness and expert scores, indicates that
verbal behavior can reflect both heightened concep-
tual knowledge and level of expertise.
The number of filled pauses per narrative, num-
ber of silent pauses per narrative, and the total dura-
tion of filled and silent pauses (per narrative) also in-
creased as the holistic expert and correctness scores
improved and the not-given score decreased. The
graph of filled pauses in Figure 3 indicates that the
increase in the number of filled and silent pauses in-
volve more cognitive processing. That the not-given
score tends to inversely decrease could indicate very
little cognitive processing (e.g., if an observer was
so unsure that they did not even hazard a guess).
The number and percentage of nasal filled pauses,
as opposed to non-nasal filled pauses, increased at
5There was not a strong correlation between the holistic ex-
pert, correctness, and not-given scores, but each score measured
different criteria. The mean holistic expert score was 2.3 with
a standard deviation of 0.5; the mean correctness score was 1.6
with a standard deviation of 0.8; and the mean not-given score
was 0.26 with a standard deviation of 0.16.
6
a slightly higher rate as the holistic expert and cor-
rectness scores increased. This could indicate that
nasal filled pauses indicate a higher cognitive load
and therefore more consideration in the decision-
making process. However, as discussed in Section
4.1, this corpus has more non-nasal than nasal filled
pauses and some observers have a particular prefer-
ence, so this would need to be controlled and inves-
tigated further.
5 Computational Model of Filled Pauses
Based on Speech Features
A computational model was developed to classify
filled pauses as either nasal or non-nasal,6 based
on features discussed in our analysis and in previ-
ous work. This model performs above a majority
class baseline, supporting our findings that there are
differences between the two types of filled pauses,
given the features that we have examined, which can
be captured by a computational model.
The features considered for classification were
total duration and number of words in the narra-
tive; duration, intensity, mean pitch, minimum pitch,
and maximum pitch of the filled pause;7 the filled
pause?s time and word position in the narrative; time
and word position as a percentage of the total narra-
tive; and length of silent pauses8 on each side of the
filled pause. The CFS subset evaluation features se-
lection algorithm was first applied. The filled pause
duration, maximum pitch, left silence length, and
right silence length were maintained as features for
classification; other features were not used further.
The widely used J48 decision tree algorithm in
Weka9 was used to classify our data, which allowed
us to visualize our model. The experimental ap-
proach was guided by the relatively small size of
the dataset. We wanted to avoid over- or under-
interpretation of results based on just a small held-
out test set. The data were shuffled and partitioned
differently during tuning and testing to ensure dis-
6We also made a fine-grained model to classify specific
filled pauses ah, er, hm, uh, and um. It had 70% accuracy but
was generally unable to identify the least-often occurring ah, er,
and hm filled pauses, so it is not reported on here.
7Pitch features were extracted considering gender: 75-300
Hz for men and a 100-500 Hz for women.
8If there was no silence, the value was 0.
9See http://www.cs.waikato.ac.nz/ml/weka/.
Predicted
Nasal Non-nasal
Actual Nasal 900 617Non-nasal 462 1971
Table 5: Confusion matrix of classification results.
tinct identities of the data splits so that parameters
were not tuned on test folds. The algorithm?s pa-
rameters were tuned using 5-fold cross-validation;
the best-performing fold?s parameters were chosen.
The data were then shuffled anew and split into 10
folds with each fold being the test set for one experi-
mental run. Results are reported on the final 10-fold
cross-validation case.
The baseline for this model was 62% because the
majority class, non-nasal filled pauses, comprised
that percentage of the data set. Our model cor-
rectly classified 73% of the instances, performing
11% above the baseline. A confusion matrix of the
classifier output is shown in Table 5. The model per-
forms best for non-nasal filled pauses, likely because
they are more common.
The output of the decision tree indicated that du-
ration of the filled pause was the most important fea-
ture. As discussed in Section 4.1, this corresponds
with our previous statistical findings as well as those
of Clark and Fox Tree (2002) that there is a differ-
ence in duration of filled pauses. The next most
important features were the left and right silence
lengths, also supported by our analysis as well as
by Clark and Fox Tree (2002) and Barr (2001). The
last selected feature was the maximum pitch of the
filled pause, possibly due to phonemic qualities.
This computational model mirrors the findings of
Section 4.1 that the duration of filled pauses and of
surrounding silent pauses are a differentiating fac-
tor between nasal and non-nasal filled pauses and
that the contextual surroundings of each filled pause
type are different. The finding that the two distinct
types of filled pauses behave differently in this do-
main could also aid language processing systems for
clinicians in the medical field. Further research into
filled pause and other speech phenomena in each
step of the diagnostic process (i.e. medical lesion
morphology, differential diagnosis, and final diag-
nosis) could also be explored in future work.
7
6 Conclusion
The results of this study underscore the need for fur-
ther research into the production of disfluencies, es-
pecially in decision making situations and in special-
ized fields such as dermatology. Future work will
further explore their connection with highly relevant
extra-propositional meaning phenomena in diagnos-
tic verbal behaviors such as certainty, confidence,
correctness, and thoroughness.
This study has shown that the two main types of
filled pauses, nasal and non-nasal, differ in their us-
age. Nasal filled pauses are more likely to be pre-
ceded and followed by silent pauses, and these fol-
lowing silent pauses are more likely to be longer.
These findings are reinforced by the computational
model which identified the duration of the filled
pause, duration of surrounding silences, and pitch
as important for classification of filled pause type.
That longer and more frequent silent pauses sur-
round nasal filled pauses supports the hypothesis
that nasal filled pauses indicate a higher level of cog-
nitive load (Clark and Fox Tree, 2002) or a topic that
is new to the discourse or unusually complex (Barr,
2001; Barr and Seyfiddinipur, 2010).
The lack of differences in use of filled pauses by
speaker gender given the differences found by Ac-
ton (2011), Binnenpoorte et al (2005), and Bortfeld
et al (2001) shows that more research is needed to
understand gender variation in speech.
Another finding was that level of expertise in-
fluenced the use of filled pauses and overall narra-
tive length. On average, attending physicians spoke
longer, said more, used more filled pauses, and had
a higher percentage of nasal filled pauses. Attend-
ing physicians also had slightly higher holistic ex-
pert and correctness scores and were more likely to
provide medical lesion morphology, differential di-
agnosis, and final diagnosis. We believe that attend-
ing physicians likely noticed more about the images
due to their experience.
The differences by level of expertise (in our study,
between attending and resident physicians) need to
be verified and compared with more data and in non-
medical fields. The differences could also be re-
lated to teaching experience of the attending physi-
cians, so further research could compare experi-
enced physicians who are also teachers with those
who are not, and if their speaking style affects stu-
dents? comprehension. In general, differences in lin-
guistic behaviors in relation to levels of expertise
deserve more research, and might have long-term
implications for development of clinical decision-
support and training systems.
The information used by the physicians in our
study was limited; they were only shown images
of dermatological conditions without being able to
examine the patient, run diagnostic tests, or have
a patient history. This may have changed their
the behavior, along with factors such as the dif-
ficulty of diagnosis of each image and their role
in the Master-Apprentice scenario. Understanding
how these variables affect the diagnostic process
of physicians could help us understand how disflu-
encies are impacted by the contexts of diagnostic
decision-making.
The differences found between the use of filled
pauses based on level of expertise and on the correct-
ness of narratives seem to indicate that filled pauses
could provide partial information about the experts?
decision-making process as well as level of confi-
dence and certainty. This is especially important
in the medical domain in order to understand how
physicians? verbal behaviors are interpreted by other
physicians as well as by patients and students.
We recently collected a similar, larger data set and
we plan to further examine differences based on ex-
pertise in this new corpus. In the recent data collec-
tion, observers were also asked to rate their level of
certainty about the diagnosis. This provides the op-
portunity to examine the relationship between disflu-
encies and certainty. We have eye-tracking data for
both studies and future work will also look at eye-
movements in relation to the use of filled and silent
pauses, certainty, expertise level, and cognitive load.
Acknowledgements
Supported in part by NIH 1 R21 LM010039-01A1,
NSF IIS-0941452, RIT GCCIS Seed Funding, and
RIT Research Computing (http://rc.rit.edu). We
thank Lowell A. Goldsmith, M.D. and the anony-
mous reviewers for their comments, and Dr. Rube?n
Proan?o for input on statistical analysis.
8
References
Eric K. Acton. 2011. On gender differences in the dis-
tribution of um and uh. University of Pennsylvania
Working Papers in Linguistics, 17(2).
Jennifer E. Arnold, Maria Fagnano, and Michael K.
Tanenhaus. 2003. Disfluencies signal theee, um, new
information. Journal of Psycholinguistic Research,
32(1):25?36.
Jennifer E. Arnold, Carla L. Hudson Kam, and
Michael K. Tanenhaus. 2007. If you say thee uh you
are describing something hard: The on-line attribution
of disfluency during reference comprehension. Jour-
nal of Experimental Psychology: Learning, Memory,
and Cognition, 33(5):914?930.
Karl G.D. Bailey and Fernanda Ferreira. 2003. Disfluen-
cies affect the parsing of garden-path sentences. Jour-
nal of Memory and Language, 49:183?200.
Dale J. Barr and Mandana Seyfiddinipur. 2010. The role
of fillers in listener attributes for speaker disfluency.
Language and Cognitive Processes, 25(4):441?455.
Dale J. Barr. 2001. Trouble in mind: Paralinguistic
indices of effort and uncertainty in communication.
Oralite? and gestualite?: Communication Multimodale,
Interaction, pages 597?600.
Dale J. Barr. 2003. Paralinguistic correlates of con-
ceptual structure. Psychonomic Bulletin & Review,
10(2):462?467.
Hugh Beyer and Karen Holtzblatt. 1997. Contextual De-
sign: Defining Customer-Centered Systems. Morgan
Kaufmann.
Diana Binnenpoorte, Christophe Van Bael, Els den Os,
and Lou Boves. 2005. Gender in everyday speech and
language: A corpus-based study. Interspeech, pages
2213?2216.
Paul Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, pages 341?345.
Heather Bortfeld, Silvia D. Leon, Johnathan E. Bloom,
Michael F. Schober, and Susan E. Brennan. 2001.
Disfluency rates in conversation: Effects of age, re-
lationship, topic, role, and gender. Language and
Speech, 44(2):123?147.
Patricia Hayes Bradley. 1981. The folk-linguistics of
women?s speech: an empirical investigation. Commu-
nication Monographs, 48(1):78?91.
Susan E. Brennan and Maurice Williams. 1995. The
feeling of another?s knowing: Prosody and filled
pauses as cues to listeners about the metacognitive
states of speakers. Journal of Memory and Language,
34:383?398.
Herbert H. Clark and Jean E. Fox Tree. 2002. Using uh
and um in spontaneous speaking. Cognition, 84:73?
111.
Martin Corley and Oliver W. Stewart. 2008. Hesitation
disfluencies in spontaneous speech: The meaning of
um. Lang. and Linguistics Compass, 2(4):589?602.
Jean E. Fox Tree. 1995. The effects of false starts and
repetitions on the processing of subsequent words in
spontaneous speech. Journal of Memory and Lan-
guage, 34:709?738.
Elizabeth Gordon. 1994. Sex differences in language:
Another explanation? American Speech, 69(2):215?
221.
Gail Jefferson. 1989. Notes on a possible metric which
provides for a ?standard maximum? silence of approx-
imately one second in conversation. In Derek Roger
and Peter Bull, editors, Conversation, chapter 8, pages
166?196. Multilingual Matters, Clevedon, UK.
Kim Kirsner, John Dunn, Kathryn Hird, Tim Parkin, and
Craig Clark. 2002. Time for a pause. Proc. of the
9th Australian Int?l. Conf. on Speech Science & Tech.,
pages 52?57.
Tobias Lo?vgren and Jan van Doorn. 2005. Influence of
manipulation of short silent pause duration on speech
fluency. Proceedings of DiSS05, pages 123?126.
Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli,
Jeff Pelz, Pengcheng Shi, and Anne Haake.
Forthcoming-2012. Linking uncertainty in physi-
cians? narratives to diagnostic correctness. Proc. of
the ExProM 2012 Workshop.
Beata Megyesi and Sofia Gustafson-Capkova. 2002.
Production and perception of pauses and their linguis-
tic context in read and spontaneous speech in Swedish.
ICSLP 7.
Roser Morante and Caroline Sporleder. in press. Modal-
ity and negation: An introduction to the special issue.
Computational Linguistics.
Daniel C. O?Connell and Sabine Kowal. 2005. uh
and um revisited: Are they interjections for signal-
ing delay? Journal of Psycholinguistic Research,
34(6):555?576.
Sharon Oviatt. 1995. Predicting and managing spo-
ken disfluencies during human-computer interaction.
Computer Speech and Language, 9:19?35.
Stanley Schachter, Nicholas Christenfeld, Bernard Rav-
ina, and Frances Bilous. 1991. Speech disfluency and
the structure of knowledge. JPSP, 60(3):362?367.
Vicki L. Smith and Herbert H. Clark. 1993. On the
course of answering questions. Journal of Memory
and Language, 32:25?38.
Anna-Marie R. Spinos, Daniel C. O?Connell, and Sabine
Kowal. 2002. An empirical investigation of pause no-
tation. Pragmatics, 12(1):1?9.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The bioscope
corpus: Biomedical texts annotated for uncertainty,
negation, and their scopes. BMC Bioinformatics, 9.
9
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 19?27, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Linking Uncertainty in Physicians? Narratives to Diagnostic Correctness
Wilson McCoy
Department of Interactive
Games and Media
wgm4143@rit.edu
Cecilia Ovesdotter Alm
Department of English
coagla@rit.edu
Cara Calvelli
College of Health
Sciences and Technology
cfcscl@rit.edu
Jeff B. Pelz
Center for
Imaging Science
pelz@cis.rit.edu
Pengcheng Shi
Computing and
Information Sciences
pengcheng.shi@rit.edu
Rochester Institute of Technology
Anne Haake
Computing and
Information Sciences
anne.haake@rit.edu
Abstract
In the medical domain, misdiagnoses and di-
agnostic uncertainty put lives at risk and in-
cur substantial financial costs. Clearly, medi-
cal reasoning and decision-making need to be
better understood. We explore a possible link
between linguistic expression and diagnostic
correctness. We report on an unusual data set
of spoken diagnostic narratives used to com-
putationally model and predict diagnostic cor-
rectness based on automatically extracted and
linguistically motivated features that capture
physicians? uncertainty. A multimodal data
set was collected as dermatologists viewed im-
ages of skin conditions and explained their di-
agnostic process and observations aloud. We
discuss experimentation and analysis in initial
and secondary pilot studies. In both cases,
we experimented with computational model-
ing using features from the acoustic-prosodic
and lexical-structural linguistic modalities.
1 Introduction
Up to 20% of post-mortem diagnoses in the United
States are inconsistent with the diagnosis before
death (Graber, 2005). These misdiagnoses cost both
human lives and estimated millions of dollars every
year. To find where and why misdiagnoses occur, it
is necessary to improve our understanding of doc-
tors? diagnostic reasoning and how it is linked to di-
agnostic uncertainty and correctness. Our contribu-
tion begins to explore the computational modeling
of this phenomenon in diagnostic narratives. From a
cognitive science perspective, we are contributing to
the research on medical reasoning and how it is lin-
guistically expressed. In the long term, this area of
work could be a useful decision-making component
for flagging diagnoses that need further review.
The study used an unusual multimodal data set
collected in a modified Master-Apprentice interac-
tion scenario. It comprises both gaze and linguistic
data. The present study focuses on the linguistic data
which in turn can be conceptualized as consisting of
both acoustic-prosodic and lexical-structural modal-
ities. This data set can further be used to link vision
and language research to understand human cogni-
tion in expert decision-making scenarios.
We report on a study conducted in two phases.
First, an initial pilot study involved a preliminary an-
notation of a small subset of the collected diagnos-
tic narratives and also investigated the prediction of
diagnostic correctness using a set of linguistic fea-
tures from speech recordings and their verbal tran-
scriptions. This provided initial features relevant to
classification, helped us identify annotation issues,
and gave us insight on how to improve the annota-
tion scheme used for annotating ground truth data.
Next, a second pilot study was performed, build-
ing on what was learned in the initial pilot study.
The second pilot study involved a larger data set
with a revised and improved annotation scheme that
considered gradient correctness at different steps of
the diagnostic reasoning process: (1) medical lesion
morphology (e.g. recognizing the lesion type as a
scaly erythematous plaque), (2) differential diagno-
sis (i.e. providing a set of possible final diagnoses),
and (3) final diagnosis (e.g. identifying the disease
condition as psoriasis). We also experiment with
19
classification using an expanded feature set moti-
vated by the initial pilot study and by previously
published research. We report on results that con-
sider different algorithms, feature set modalities, di-
agnostic reasoning steps, and coarse vs. fine grained
classes as explained below in Section 4.3.
2 Previous Work
Much work has been done in the area of medi-
cal decision-making. Pelaccia et al (2011) have
viewed clinical reasoning through the lens of dual-
process theory. They posit that two systems are at
work in the mind of a clinician: the intuitive system
which quickly produces a response based on expe-
rience and a holistic view of the situation, versus
the analytic system which slowly and logically steps
through the problem with conscious use of knowl-
edge. Croskerry (2009) stated that ?[i]f the presen-
tation is not recognized, or if it is unduly ambiguous
or there is uncertainty, [analytic] processes engage
instead? (p. 1022); for instance, if a clinician is un-
familiar with a disease or unsure of their intuitive
answer. We assume that different reasoning systems
may cause changes in linguistic behaviors. For ex-
ample, when engaging the slower analytic system, it
seems reasonable that frequent pausing could appear
as an indication of, e.g., uncertainty or thoughtful-
ness.
Several studies have explored the task of detect-
ing uncertainty through language. Uncertainty de-
tection necessitates inference of extra-propositional
meaning and is arguably a subjective natural lan-
guage problem, i.e. part of a family of problems
that are increasingly receiving attention in compu-
tational linguistics. These problems involve more
dynamic classification targets and different perfor-
mance expectations (Alm, 2011). Pon-Barry and
Shieber (2009) have shown encouraging results in
finding uncertainty using acoustic-prosodic features
at the word, word?s local context, and whole utter-
ance levels. Henriksson and Velupillai (2010) used
?speculative words? (e.g., could, generally, should,
may, sort of, etc.) as well as ?certainty ampli-
fiers? (e.g., definitely, positively, must, etc.) to deter-
mine uncertainty in text. Velupillai (2010) also ap-
plied the same approach to medical texts and noted
that acoustic-prosodic features should be considered
alongside salient lexical-structural features as indi-
cators of uncertainty. In this work, we draw on the
insight of such previous work, but we also extend
the types of linguistic evidence considered for iden-
tifying possible links to diagnostic correctness.
As another type of linguistic evidence, disfluen-
cies make up potentially important linguistic evi-
dence. Zwarts and Johnson (2011) found that the
occurrence of disfluencies that had been removed
could be predicted to a satisfactory degree. Pakho-
mov (1999) observed that such disfluencies are just
as common in monologues as in dialogues even
though there is no need for the speakers to indicate
that they wish to continue speaking. This finding is
important for the work presented here because our
modified use of the Master-Apprentice scenario re-
sults in a particular dialogic interaction with the lis-
tener remaining silent. Perhaps most importantly,
Clark and Fox Tree (2002) postulated that filled
pauses (e.g., um, uh, er, etc.) play a meaningful
role in speech. For example, they may signal that
the speaker is yet to finish speaking or searching for
a word. There is some controversy about this claim,
however, as explained by Corley and Stewart (2008).
The scholarly controversy about the role of disfluen-
cies indicates that more research is needed to under-
stand the disfluency phenomenon, including how it
relates to extra-propositional meaning.
3 Data Set
The original elicitation experiment included 16
physicians with dermatological expertise. Of these,
12 were attending physicians and 4 were residents
(i.e. dermatologists in training). The observers were
shown a series of 50 images of dermatological con-
ditions. The summary of this collected data is shown
in Table 1, with reference to the pilot studies.
The physicians were instructed to narrate, in En-
glish, their thoughts and observations about each im-
age to a student, who remained silent, as they arrived
at a differential diagnosis or a possible final diagno-
sis. This data elicitation approach is a modified ver-
sion of the Master-Apprentice interaction scenario
(Beyer and Holtzblatt, 1997). This elicitation setup
is shown in Figure 1. It allows us to extract in-
formation about the Master?s (i.e. in this case, the
physician?s) cognitive process by coaxing them to
20
Data parameters Quantity
# of participating doctors 16
# of images for which
narratives were collected 50
# of time-aligned narratives
in the initial pilot study 160
# of time-aligned narratives
in the second pilot study 707
Table 1: This table summarizes the data. Of the collected
narratives, 707 are included in this work; audio is unavail-
able for some narratives.
vocalize their thoughts in rich detail. This teaching-
oriented scenario really is a monologue, yet induces
a feeling of dialogic interaction in the Master.
Figure 1: The Master-Apprentice interaction scenario al-
lows us to extract information about the Master?s (here:
doctor?s) cognitive processes.
The form of narratives collected can be analyzed
in many ways. Figure 2 shows two narratives, re-
cently elicited and similar to the ones in the study?s
data set, that are used here with permission as ex-
amples. In terms of diagnostic reasoning styles, re-
ferring to Pelaccia et al (2011), we can propose that
observer A may be using the intuitive system and
that observer B may be using the analytical system.
Observer A does not provide a differential diagnosis
and jumps straight to his/her final diagnosis, which
in this case is correct. We can postulate that observer
A looks at the general area of the lesion and uses
previous experience or heuristic knowledge to come
to the correct diagnosis. This presumed use of the
intuitive system could potentially relate to the depth
of previous experience with a disease, for example.
Observer B, on the other hand, might be using the
A. This patient has a pinkish papule with
surrounding hypopigmentation in a field of
other cherry hemagiomas and nevoid type
lesions. The only diagnosis that comes to
mind to me is Sutton?s nevus.
B. I think I?m looking at an abdomen, possibly.
I see a hypopigmented oval-shaped patch in
the center of the image. I see that there
are two brown macules as well. In the center
of the hypopigmented oval patch there
appears to be an area that may be a pink
macule. Differential diagnosis includes
halo nevus, melanoma, post-inflammatory
hypopigmentation. I favor a diagnosis of
maybe post-inflammatory hypopigmentation.
Figure 2: Two narratives collected in a recent elicitation
setup and used here with permission. Narratives A and B
are not part of the studied data set, but exemplify data set
narratives which could not be distributed. Observers A
and B are both looking at an image of a halo or Sutton?s
nevus as seen in Figure 3. Disfluencies are considered in
the experimental work but have been removed for read-
ability in these examples.
Figure 3: The image of a halo or Sutton?s nevus viewed
by the observers and the subject of example narratives.
analytical system. Observer B steps through the di-
agnosis in a methodical process and uses evidence
presented to rationalize the choice of final diagno-
sis. Observer B also provides a differential diagno-
sis unlike observer A. This suggests that observer
B is taking advantage of a process of elimination to
decide on a final diagnosis.
Another way to evaluate these narratives is in
terms of correctness and the related concept of diag-
21
nostic completeness. Whereas these newly elicited
narrative examples have not been annotated by doc-
tors, some observations can still be made. From the
point of view of final diagnosis, observer A is cor-
rect, unlike observer B. Assessment of diagnostic
correctness and completeness can also be made on
intermediate steps in the diagnostic process (e.g. dif-
ferential diagnoses or medical lesion morphological
description). Including such steps in the diagnos-
tic process is considered good practice. Observer A
does not supply a differential diagnosis and instead
skips to the final diagnosis. Observer B provides
the correct answer in the differential diagnosis but
gives the incorrect final diagnosis. Observer B fully
describes the medical lesion morphology presented.
Observer A, however, only describes the pink lesion
and does not discuss the other two brown lesions.
The speech of the diagnostic narratives was
recorded. At the same time, the observers? eye-
movements were tracked; the eye-tracking data
are considered in another report (Li et al, 2010).
We leave the integration of the linguistic and eye-
tracking data for future work.
After the collection of the raw audio data, the
utterances were manually transcribed and time-
aligned at the word level with the speech anal-
ysis tool Praat (Boersma, 2001).1 A sample of
the transcription process output is shown in Fig-
ure 4. Given our experimental context, off-the-shelf
automatic speech recognizers could not transcribe
the narratives to the desired quality and resources
were not available to create our own automatic tran-
1See http://www.fon.hum.uva.nl/praat/.
Figure 4: Transcripts were time-aligned in Praat which
was also used to extract acoustic-prosodic features.
scriber. Manual transcription also preserved disflu-
encies, which we believe convey meaningful infor-
mation. Disfluencies were transcribed to include
filled pauses (e.g. uh, um), false starts (e.g. pur-
reddish purple), repetitions, and click sounds.
This study is strengthened by its involvement of
medical experts. Trained dermatologists were re-
cruited in the original elicitation experiment as well
as the creation and application of both annotation
schemes. This is crucial in a knowledge-rich domain
such as medicine because the annotation scheme
must reflect the domain knowledge. Another study
reports on annotation details (McCoy et al, Forth-
coming 2012).
4 Classification Study
This section discusses the classification work, first
explaining the methodology for the initial pilot study
followed by interpretation of results. Next, the
methodology of the second pilot study is described.
4.1 Generic Model Overview
This work applies computational modeling de-
signed to predict diagnostic correctness in physi-
cians? narratives based on linguistic features from
the acoustic-prosodic and lexical-structural modali-
ties of language, shown in Table 2. Some tests dis-
cussed in 4.2 and 4.3 were performed with these
modalities separated. These features are inspired
by previous work conducted by Szarvas (2008),
Szarvas et al (2008), Litman et al (2009), Liscombe
et al (2005), and Su et al (2010).
We can formally express the created model in the
following way: Let ni be an instance in a set of nar-
ratives N , let j be a classification method, and let
li be a label in a set of class labels L. We want to
establish a function f(ni, j) : li where li is the label
assigned to the narrative based on linguistic features
from a set F , where F = f1, f2, ...fk, as described
in Table 2. The baseline for each classifier is de-
fined as the majority class ratio. Using scripts in
Praat (Boersma, 2001), Python, and NLTK (Bird et
al., 2009), we automatically extracted features for
each narrative. Each narrative was annotated with
multiple labels relating to its diagnostic correctness.
The labeling schemes used in the initial and second
pilot studies, respectively, are described in subsec-
22
tions 4.2 and 4.3.
4.2 Initial Pilot Study
The initial pilot classification study allowed the op-
portunity to refine the prediction target annotation
scheme, as well as to explore a preliminary set of lin-
guistic features. 160 narratives were assigned labels
Linguistic Feature at the narrative level
Modality
Acoustic- Total duration
prosodic Percent silence
Time silent
# of silences *
Time speaking
# of utterances *
Initial silence length
F0 mean (avg. pitch) ?
F0 min (min. pitch) ?
F0 max (max. pitch) ?
dB mean (avg. intensity) ?
dB max (max. intensity) ?
Lexical- # of words
structural words per minute
# of disfluencies ?
# of certainty amplifiers * ?
# of speculative words * ?
# of stop words * ?
# of content words * ?
# of negations * ?
# of nouns ?
# of verbs ?
# of adjectives ?
# of adverbs ?
Unigram of tokens
Bigram of tokens
Trigram of tokens
Table 2: Features used by their respective modalities.
Features marked with a * were only included in the sec-
ond pilot study. Features marked with ? were included
twice; once as their raw value and again as a z-score nor-
malized to its speaker?s data in the training set. Features
marked with ?were also included twice; once as their raw
count and again as their value divided by the total number
of words in that narrative. Disfluencies were considered
as words towards the total word count, silences were not.
No feature selection was applied.
of correct or incorrect for two steps of the diagnos-
tic process: diagnostic category and final diagno-
sis. These annotations were done by a dermatologist
who did not participate in the elicitation study (co-
author Cara Calvelli). For final diagnosis, 70% were
marked as correct, and for diagnostic category, 80%
were marked as correct. An outcome of the anno-
tation study was learning that the initial annotation
scheme needed to be refined. For example, diagnos-
tic category had a fuzzy interpretation, and correct-
ness and completeness of diagnoses are found along
a gradient in medicine. This led us to pursue an im-
proved annotation scheme with new class labels in
the second pilot study, as well as the adoption of a
gradient scale of correctness.
For the initial pilot study, basic features were ex-
tracted from the diagnostic narratives in two modal-
ities: acoustic-prosodic and lexical-structural (see
Table 2). To understand the fundamental aspects
of the problem, the initial pilot study experimented
with the linguistic modalities separately and to-
gether, using three foundational algorithms, as im-
plemented in NLTK (Naive Bayes, Maximum En-
tropy, Decision Tree), and a maximum vote classi-
fier based on majority consensus of the three basic
classifiers. The majority class baselines were 70%
for diagnosis and 80% for diagnostic category. The
small pilot data set was split into an 80% training set
and a 20% testing set. The following results were
obtained with the maximum vote classifier.
Utilizing only acoustic-prosodic features, the
maximum vote classifier performed 5% above the
baseline when testing final diagnosis and 6% below
it for diagnostic category. F0 min and initial silence
length appeared as important features. This initial si-
lence length could signal that the observers are able
to glean more information from the image, and us-
ing this information, they can make a more accurate
diagnosis.
Utilizing only lexical-structural features, the
model performed near the baseline (+1%) for final
diagnosis and 9% better than the baseline for diag-
nostic category. When combining acoustic-prosodic
and lexical-structural modalities, the majority vote
classifier performed above the baseline by 5% for fi-
nal diagnosis and 9% for diagnostic category. We
are cautious in our interpretation of these findings.
For example, the small size of the data set and the
23
particulars of the data split may have guided the re-
sults, and the concept of diagnostic category turned
out to be fuzzy and problematic. Nevertheless, the
study helped us refine our approach for the second
pilot study and redefine the annotation scheme.
4.3 Second Pilot Study
For the second pilot study, we hoped to gain further
insight into primarily two questions: (1) How accu-
rately do the tested models perform on three steps of
the diagnostic process, and what might influence the
performance? (2) In our study scenario, is a certain
linguistic modality more important for the classifi-
cation problem?
The annotation scheme was revised according to
findings from the initial pilot study. These revisions
were guided by dermatologist and co-author Cara
Calvelli. The initial pilot study scheme only anno-
tated for diagnostic category and final diagnosis. We
realized that diagnostic category was too slippery of
a concept, prone to misunderstanding, to be useful.
Instead, we replaced it with two new and more ex-
plicit parts of the diagnostic process: medical lesion
morphology and differential diagnosis.
For final diagnosis, the class label options of cor-
rect and incorrect could not characterize narratives
in which observers had not provided a final diag-
nosis. Therefore, a third class label of none was
added. New class labels were also created that cor-
responded to the diagnostic steps of medical lesion
morphology and differential diagnosis. Medical le-
sion morphology, which is often descriptively com-
plex, allowed the label options correct, incorrect,
and none, as well as correct but incomplete to deal
with correct but under-described medical morpholo-
gies. Differential diagnosis considered whether or
not the final diagnosis appeared in the differential
and thus involved the labels yes, no, and no differ-
ential given. Table 3 summarizes the refined anno-
tation scheme.
The examples in Figure 2 above can now be ana-
lyzed according to the new annotation scheme. Ob-
server A has a final diagnosis which should be la-
beled as correct but does not give a differential diag-
nosis, so the differential diagnosis label should be no
differential given. Observer A also misses parts of
the morphological description so the assigned med-
ical lesion morphology would likely be correct but
incomplete. Observer B provides what seems to be
a full morphological description as well as lists the
correct final diagnosis in the differential diagnosis,
yet is incorrect regarding final diagnosis. This narra-
tive?s labels for medical lesion morphology and dif-
ferential diagnosis would most likely be correct and
yes respectively. Further refinements may turn out
useful as the data set expands.
Diagnostic step Possible labels Count Ratio
Medical Correct 537 .83
Lesion Incorrect 36 .06
Morphology None Given 40 .06
Incomplete 32 .05
Differential Yes 167 .24
Diagnosis No 101 .14
No Differential 434 .62
Final Correct 428 .62
Diagnosis Incorrect 229 .33
None Given 35 .05
Table 3: Labels for various steps of the diagnostic process
as well as their count and ratios of the total narratives, af-
ter eliminating those with no annotator agreement. These
labels are explained in section 4.3.
Three dermatologists annotated the narratives, as-
signing a label of correctness for each step in the
diagnostic process for a given narrative. Table 3
shows the ratios of labels in the collected annota-
tions. Medical lesion morphology is largely correct
with only smaller ratios being assigned to other cat-
egories. Secondly, a large ratio of narratives were
assigned no differential given but of those that did
provide a differential diagnosis, the correct final di-
agnosis was more likely to be included than not. Re-
garding final diagnosis, a label of correct was most
often assigned and few narratives did not provide
any final diagnosis. These class imbalances, exist-
ing at each level, indicated that the smaller classes
with fewer instances would be quite challenging for
a computational classifier to learn.
Any narrative for which there was not agreement
for at least 2 of the 3 dermatologists in a diagnostic
step was discarded from the set of narratives consid-
ered in that diagnostic step.2
2Because narratives with disagreement were removed, the
total numbers of narratives in the experiment sets differ slightly
on the various step of the diagnostic process.
24
Comparing classification in terms of algorithms,
diagnostic steps, and individual classes
Weka (Witten and Frank, 2005)3 was used with
four classification algorithms, which have a widely
accepted use in computational linguistics.4
Standard performance measures were used to
evaluate the classifiers. Both acoustic-prosodic and
lexical-structural features were used in a leave-one-
out cross-validation scenario, given the small size of
the data set. The results are shown in Table 4. Ac-
curacy is considered in relation to the majority class
baseline in each case. With this in mind, the high
accuracies found when testing medical lesion mor-
phology are caused by a large class imbalance. Dif-
ferential diagnosis? best result is 5% more accurate
than its baseline while final diagnosis and medical
lesion morphology are closer to their baselines.
Final Dx Diff. Dx M. L. M.
Baseline .62 .62 .83
C4.5 .57 .62 .77
SVM .63 .67 .83
Naive Bayes .55 .61 .51
Log Regression .53 .64 .66
Table 4: Accuracy ratios of four algorithms (implemented
in Weka) as well as diagnostic steps? majority class base-
lines. Experiments used algorithms? default parameters
for final diagnosis (3 labels), differential diagnosis (3 la-
bels), and medical lesion morphology (4 labels) using
leave-one-out cross-validation.
In all scenarios, the SVM algorithm reached or
exceeded the majority class baseline. For this rea-
son, other experiments used SVM. The results for
the SVM algorithm when considering precision and
recall for each class label, at each diagnostic step,
are shown in Table 5. Precision is calculated as the
number of true positives for a given class divided by
the number of narratives classified as the given class.
Recall is calculated as the number of true positives
for a given class divided by the number of narra-
tives belonging to the given class. As Table 5 shows,
and as expected, labels representing large propor-
tions were better identified than labels representing
3See http://www.cs.waikato.ac.nz/ml/weka/.
4In this initial experimentation, not all features used were
independent, although this is not ideal for some algorithms.
Dx step Labels Precision Recall
Medical Correct .83 .99
Lesion Incorrect 0 0
Morphology None Given 0 0
Incomplete 0 0
Differential Yes .49 .44
Diagnosis No .26 .10
No Diff. .76 .89
Final Correct .67 .84
Diagnosis Incorrect .32 .47
None Given 0 0
Table 5: Precision and recall of class labels. These were
obtained using the Weka SVM algorithm with default pa-
rameters using leave-one-out cross-validation. These cor-
respond to the experiment for SVM in Table 4.
Final Diagnosis Diff. Diagnosis
Baseline .62 .62
Lex.-struct. .62 .67
Acous.-pros. .65 .62
All .63 .67
Table 6: Accuracy ratios for various modalities. Tests
were performed for final diagnosis and differential diag-
nosis tags with Weka?s SVM algorithm using a leave-
out-out cross-validation method. Lexical-structural and
acoustic-prosodic cases used only features in their respec-
tive set.
intermediate proportions, and classes with few in-
stances did poorly.
Experimentation with types of feature
To test if one linguistic modality was more impor-
tant for classification, experiments were run in each
of three different ways: with only lexical-structural
features, with only acoustic-prosodic features, and
with all features. We considered the final diagnosis
and differential diagnosis scenarios. It was decided
not to run this experiment in terms of medical lesion
morphology because of its extreme class imbalance
with a high baseline of 83%. Medical lesion mor-
phology also differs in being a descriptive step un-
like the other two which are more like conclusions.
Again, a leave-one-out cross-validation method was
used. The results are shown in Table 6.
These results show that, regarding final diagnosis,
considering only acoustic-prosodic features seemed
25
to yield somewhat higher accuracy than when fea-
tures were combined. This might reflect that, con-
ceptually, final diagnosis captures a global end step
in the decision-making process, and we extracted
voice features at a global level (across the narrative).
In the case of differential diagnosis, the lexical-
structural features performed best, matching the ac-
curacy of the combined feature set (5% over the ma-
jority class baseline). Future study could determine
which individual features in these sets were most im-
portant.
Experiments with alternative label groupings for
some diagnostic steps
Another set of experiments examined perfor-
mance for adjusted label combinations. To learn
more about the model, experiments were run in
which selected classes were combined or only cer-
tain classes were considered. The class proportions
thus changed due to the combinations and/or re-
moval of classes. This was done utilizing all fea-
tures, the Weka SVM algorithm, and a leave-one-
out methodology. Only logically relevant tests that
increased class balance are reported here.5
An experiment was run on the differential diagno-
sis step. The no differential given label was ignored
to allow the binary classification of narratives that
included differential diagnoses. The new majority
class baseline for this test was 62% and this classi-
fication performed 1% over its baseline. A similar
experiment was run on the final diagnosis diagnos-
tic step. Class labels of incorrect and none given
were combined to form binary set of class labels
with a 62% baseline. This classification performed
6% over the baseline, i.e., slightly improved perfor-
mance compared to the scenario with three class la-
bels.
5 Conclusion
In these pilot studies, initial insight has been gained
regarding the computational linguistic modeling of
extra-propositional meaning but we acknowledge
that these results need to be confirmed with new
data.
This paper extracted features, which could pos-
sibly relate to uncertainty, at the global level of a
5Other experiments were run but are not reported because
they have no use in future implementations.
narrative to classify correctness of three diagnostic
reasoning steps. These steps are in essence local
phenomena and a better understanding of how un-
certainty is locally expressed in the diagnostic pro-
cess is needed. Also, this work does not consider
parametrization of algorithms or the role of feature
selection. In future work, by considering only the
features that are most important, a better understand-
ing of linguistic expression in relation to diagnostic
correctness could be achieved, and likely result in
better performing models. One possible future adap-
tation would be the utilization of the Unified Medi-
cal Language System to improve the lexical features
used Woods et al (2006).
Other future work includes integrating eye move-
ment data into prediction models. The gaze modal-
ity informs us as to where the observers were look-
ing when they were verbalizing their diagnostic pro-
cess. We can thus map the narratives to how gaze
was positioned on an image. Behavioral indicators
of doctors? diagnostic reasoning likely extend be-
yond language. By integrating gaze and linguistic
information, much could be learned regarding per-
ceptual and conceptual knowledge.
Through this study, we have moved towards un-
derstanding reasoning in medical narratives, and we
have come one step closer to linking the spoken
words of doctors to their cognitive processes. In a
much more refined, future form, certainty or cor-
rectness detection could become useful to help un-
derstanding medical reasoning or help guide medi-
cal reasoning or detect misdiagnosis.
Acknowledgements
This research supported by NIH 1 R21 LM010039-
01A1, NSF IIS-0941452, RIT GCCIS Seed Fund-
ing, and RIT Research Computing (http://rc.rit.edu).
We would like to thank Lowell A. Goldsmith, M.D.
and the anonymous reviewers for their comments.
References
Cecilia Ovesdotter Alm. 2011. Subjective Natural Lan-
guage Problems: Motivations, Applications, Charac-
terizations, and Implications. Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics, pages 107?112.
26
Hugh Beyer and Karen Holtzblatt. 1997. Contextual De-
sign: Defining Customer-Centered Systems. Morgan
Kaufmann.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural Language Processing with Python. O?Reilly Me-
dia.
Paul Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, pages 341?345.
Herbert Clark and Jean Fox Tree. 2002. Using uh and um
in spontaneous speaking. Cognition, pages 73?111.
Martin Corley and Oliver Stewart. 2008. Hesitation dis-
fluencies in spontaneous speech: The meaning of um.
Language and Linguistics Compass, 5(2):589?602.
Pat Croskerry. 2009. A universal model of diagnostic
reasoning. Academic Medicine, pages 1022?1028.
Mark Graber. 2005. Diagnostic errors in medicine: A
case of neglect. The Joint Commission Journal on
Quality and Patient Safety, pages 106?113.
Aron Henriksson and Sumithra Velupillai. 2010. Levels
of certainty in knowledge-intensive corpora: An ini-
tial annotation study. Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing, pages 41?45.
Rui Li, Preethi Vaidyanathan, Sai Mulpuru, Jeff Pelz,
Pengcheng Shi, Cara Calvelli, and Anne Haake. 2010.
Human-centric approaches to image understanding
and retrieval. Image Processing Workshop, Western
New York, pages 62?65.
Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-
ditti. 2005. Detecting certainness in spoken tutorial
dialogues. Proceedings of Interspeech, pages 1837?
1840.
Diane Litman, Mihail Rotaru, and Greg Nicholas. 2009.
Classifying turn-level uncertainty using word-level
prosody. Proceedings of Interspeech, pages 2003?
2006.
Wilson McCoy, Cecilia Ovesdotter Alm, Cara Calvelli,
Rui Li, Jeff Pelz, Pengcheng Shi, and Anne Haake.
Forthcoming-2012. Annotation schemes to encode
domain knowledge in medical narratives. Proceedings
of the Sixth Linguistic Annotation Workshop.
Sergey Pakhomov. 1999. Modeling filled pauses in med-
ical dictations. Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, pages 619?624.
Thierry Pelaccia, Jacques Tardif, Emmanuel Triby, and
Bernard Charlin. 2011. An analysis of clinical rea-
soning through a recent and comprehensive approach:
the dual-process theory. Medical Education Online,
16:5890.
Heather Pon-Barry and Stuart Shieber. 2009. The im-
portance of sub-utterance prosody in predicting level
of certainty. Proceedings of NAACL HLT, pages 105?
108.
Qi Su, Chu-Ren Huang, and Helen Kai-yun Chen. 2010.
Evidentiality for text trustworthiness detection. Pro-
ceedings of the 2010 Workshop on NLP and Linguis-
tics: Finding the Common Ground ACL 2010, pages
10?17.
Gyorgy Szarvas, Veronika Vincze, Richard Farkas, and
Janos Csirik. 2008. The bioscope corpus: annotation
for negation, uncertainty and their scope in biomedical
texts. BioNLP 2008: Current Trends in Biomedical
Natural Language Processing, pages 38?45.
Gyorgy Szarvas. 2008. Hedge classification in biomed-
ical texts with a weakly supervised selection of key-
words. Proceedings of 46th Annual Meeting of the
Association of Computational Linguistics, pages 281?
289.
Sumithra Velupillai. 2010. Towards a better understand-
ing of uncertainties and speculations in Swedish clin-
ical text - analysis of an initial annotation trial. Pro-
ceedings of the Workshop on Negation and Speculation
in Natural Language Processing, pages 14?22.
Ian H.Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann.
James Woods, Charles Sneiderman, Karam Hameed,
Michael Ackerman, and Charlie Hatton. 2006. Using
umls metathesaurus concepts to describe medical im-
ages: dermatology vocabulary. Computers in Biology
and Medicine 36, pages 89?100.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disflu-
ency detection. Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics,
pages 703?711.
27
