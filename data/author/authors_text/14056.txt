SYNGRAPH: A Flexible Matching Method based on Synonymous
Expression Extraction from an Ordinary Dictionary and a Web Corpus
Tomohide Shibata?, Michitaka Odani?, Jun Harashima?,
Takashi Oonishi??, and Sadao Kurohashi?
?Kyoto University, Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan
??NEC Corporation, 1753, Shimonumabe, Nakahara-Ku, Kawasaki, Kanagawa 211-8666, Japan
{shibata,odani,harashima,kuro}@nlp.kuee.kyoto-u.ac.jp
t-onishi@bq.jp.nec.com
Abstract
This paper proposes a flexible matching
method that can assimilate the expressive
divergence. First, broad-coverage syn-
onymous expressions are automatically ex-
tracted from an ordinary dictionary, and
among them, those whose distributional
similarity in a Web corpus is high are used
for the flexible matching. Then, to overcome
the combinatorial explosion problem in the
combination of expressive divergence, an ID
is assigned to each synonymous group, and
SYNGRAPH data structure is introduced to
pack the expressive divergence. We con-
firmed the effectiveness of our method on
experiments of machine translation and in-
formation retrieval.
1 Introduction
In natural language, many expressions have almost
the same meaning, which brings great difficulty to
many NLP tasks, such as machine translation (MT),
information retrieval (IR), and question answering
(QA). For example, suppose an input sentence (1) is
given to a Japanese-English example-based machine
translation system.
(1) hotel ni
hotel
ichiban
best
chikai
near
eki wa
station
doko-desuka
where is
Even if a very similar translation example (TE)
?(2-a) ? (2-b)? exists in the TEs, a simple exact
matching method cannot utilize this example for the
translation.
(2) a. ryokan no
Japanese hotel
moyori no
nearest
eki wa
station
doko-desuka
where is
b. Where?s the nearest station to the hotel?
How to handle these synonymous expressions has
become one of the important research topics in NLP.
This paper presents a flexible matching method,
which can assimilate the expressive divergence, to
solve this problem. This method has the following
two features:
1. Synonymy relations and hypernym-hyponym
relations are automatically extracted from an
ordinary dictionary and a Web corpus.
2. Extracted synonymous expressions are effec-
tively handled by SYNGRAPH data structure,
which can pack the expressive divergence.
An ordinary dictionary is a knowledge source
to provide synonym and hypernym-hyponym rela-
tions (Nakamura and Nagao, 1988; Tsurumaru et al,
1986). A problem in using synonymous expressions
extracted from a dictionary is that some of them are
not appropriate since they are rarely used. For exam-
ple, a synonym pair ?suidou?1 = ?kaikyou(strait)? is
extracted.
Recently, some work has been done on corpus-
based paraphrase extraction (Lin and Pantel, 2001;
Barzilay and Lee, 2003). The basic idea of their
methods is that two words with similar meanings
are used in similar contexts. Although their methods
can obtain broad-coverage paraphrases, the obtained
paraphrases are not accurate enough to be utilized
1This word usually means ?water supply?.
787
for achieving precise matching since they contain
synonyms, near-synonyms, coordinate terms, hyper-
nyms, and inappropriate synonymous expressions.
Our approach makes the best use of an ordi-
nary dictionary and a Web corpus to extract broad-
coverage and precise synonym and hypernym-
hyponym expressions. First, synonymous expres-
sions are extracted from a dictionary. Then, the
distributional similarity of a pair of them is calcu-
lated using a Web corpus. Among extracted syn-
onymous expressions, those whose similarity is high
are used for the flexible matching. By utilizing only
synonymous expressions extracted from a dictionary
whose distributional similarity is high, we can ex-
clude synonymous expressions extracted from a dic-
tionary that are rarely used, and the pair of words
whose distributional similarity is high that is not ac-
tually a synonymous expression (is not listed in a
dictionary).
Another point of our method is to introduce SYN-
GRAPH data structure. So far, the effectiveness
of handling expressive divergence has been shown
for IR using a thesaurus-based query expansion
(Voorhees, 1994; Jacquemin et al, 1997). However,
their methods are based on a bag-of-words approach
and thus does not pay attention to sentence-level
synonymy with syntactic structure. MT requires
such precise handling of synonymy, and advanced
IR and QA also need it. To handle sentence-level
synonymy precisely, we have to consider the combi-
nation of expressive divergence, which may cause
combinatorial explosion. To overcome this prob-
lem, an ID is assigned to each synonymous group,
and then SYNGRAPH data structure is introduced
to pack expressive divergence.
2 Synonymy Database
This section describes a method for constructing a
synonymy database. First, synonym/hypernym re-
lations are automatically extracted from an ordinary
dictionary, and the distributional similarity of a pair
of synonymous expressions is calculated using a
Web corpus. Then, the extracted synonymous ex-
pressions whose similarity is high are used for the
flexible matching.
2.1 Synonym/hypernym Extraction from an
Ordinary Dictionary
Although there were some attempts to extract syn-
onymous expressions from a dictionary (Nakamura
and Nagao, 1988; Tsurumaru et al, 1986), they ex-
tracted only hypernym-hyponym relations from the
limited entries. In contrast, our method extracts not
only hypernym-hyponym relations, but also basic
synonym relations, predicate synonyms, adverbial
synonyms and synonym relations between a word
and a phrase.
The last word of the first definition sentence is
usually the hypernym of an entry word. Some defi-
nition sentences in a Japanese dictionary are shown
below (the left word of ?:? is an entry word, the right
sentence is a definition, and words in bold font is the
extracted words):
yushoku (dinner) : yugata (evening) no (of)
shokuji (meal).
jushin (barycenter) : omosa (weight) ga (is)
tsuriatte (balance) tyushin (center) tonaru
(become) ten (spot).
For example, the last word shokuji (meal) can be
extracted as the hypernym of yushoku (dinner). In
some cases, however, a word other than the last word
can be a hypernym or synonym. These cases can be
detected by sentence-final patterns as follows (the
underlined expressions represent the patterns):
Hypernyms
dosei (Saturn) : wakusei (planet) no (of) hitotsu
(one).
tobi (kite) : taka (hawk) no (of) issyu (kind).
Synonyms / Synonymous Phrases
ice : ice cream no (of) ryaku (abbreviation).
mottomo (most) : ichiban (best). (? one word defi-
nition)
moyori (nearest) : ichiban (best) chikai (near)
tokoro (place)2. (? less than three phrases)
2.2 Calculating the Distributional Similarity
using a Web Corpus
The similarity between a pair of synonymous ex-
pressions is calculated based on distributional sim-
ilarity (J.R.Firth, 1957; Harris, 1968) using the
Web corpus collected by (Kawahara and Kurohashi,
2006). The similarity between two predicates is de-
fined to be one between the patterns of case exam-
ples of each predicate (Kawahara and Kurohashi,
2001). The similarity between two nouns are defined
2If the last word of a sentence is a highly general term such
as koto (thing) and tokoro (place), it is removed from the syn-
onymous expression.
788
gakkou (school)
gakue n (a ca d e m y )
<school>
s h ogakkou (p r i m a r y  school)
s h ogaku (e le m e n t a r y  school)
<p r i m a r y  school>
koukou (hi g h school)
kout ougakkou (se n i or  hi g h)
<hi g h school>
t okor o (p la ce )
<p la ce >
h an t e n (b lob )
m ad ar a (m ot t le )
b uc h i (m a cu la )
<b lob >
t e n  (sp ot )
<sp ot >
j us h i n (b a r y ce n t e r )
<b a r y ce n t e r >
 m oy or i (n e a r e st )
 i c h i b an (b e st )  c h i kaku(n e a r )
<n e a r e st >
m ot t om o (m ost )
i c h i b an (b e st )
<m ost >
p oly se m i c w or dhy p e r n y m -hy p on y m  r e la t i on
 t e n  (sp ot )
<sp ot >
t e n  (sp ot )
p oc h i (d ot )
c h i s an a (sm a ll)   s h i r us h i(m a r k )
<sp ot >
 t e n  (sp ot )
b as h o (a r e a )
i c h i  (loca t i on )
<sp ot >
s h i r us h i  (m a r k )
<m a r k >
sy n on y m ou s g r ou p
Figure 1: An example of synonymy database.
as the ratio of the overlapped co-occurrence words
using the Simpson coefficient. The Simpson coeffi-
cient is computed as |T (w1)?T (w2)|min(|T (w1)|,|T (w2)|) , where T (w) is
the set of co-occurrence words of word w.
2.3 Integrating the Distributional Similarity
into the Synonymous Expressions
Synonymous expressions can be extracted from a
dictionary as described in Section 2.1. However,
some extracted synonyms/hypernyms are not appro-
priate since they are rarely used. Especially, in the
case of that a word has multiple senses, the syn-
onym/hypernym extracted from the second or later
definition might cause the inappropriate matching.
For example, since ?suidou? has two senses, the
two synonym pairs, ?suidou? = ?jyosuidou(water
supply)? and ?suidou? = ?kaikyou(strait)?, are ex-
tracted. The second sense is rarely used, and thus if
the synonymy pair extracted from the second defi-
nition is used as a synonym relation, an inappropri-
ate matching through this synonymmight be caused.
Therefore, only the pairs of synonyms/hypernyms
whose distributional similarity calculated in Section
2.2 is high are utilized for the flexible matching.
The similarity threshold is set to 0.4 for synonyms
and to 0.3 for hypernyms. For example, since the
similarity between ?suidou? and ?kaikyou? is 0.298,
this synonym is not utilized.
2.4 Synonymy Database Construction
With the extracted binomial relations, a synonymy
database can be constructed. Here, polysemic words
should be treated carefully3. When the relations
A=B and B=C are extracted, and B is not polysemic,
3If a word has two or more definition items in the dictionary,
the word can be regarded as polysemic.
they can be merged into A=B=C. However, if B is
polysemic, the synonym relations are not merged
through a polysemic word. In the same way, as for
hypernym-hyponym relations, A ? B and B ? C,
and A ? B and C ? B are not merged if B is pol-
ysemic. By merging binomial synonym relations
with the exception of polysemic words, synony-
mous groups are constructed first. They are given
IDs, hereafter called SYNID4. Then, hypernym-
hyponym relations are established between synony-
mous groups. We call this resulting data as syn-
onymy database. Figure 1 shows examples of syn-
onymous groups in the synonymy database. In this
paper, SYNID is denoted by using English gloss
word, surrounded by ? ? ? ?.
3 SYNGRAPH
3.1 SYNGRAPH Data Structure
SYNGRAPH data structure is an acyclic directed
graph, and the basis of SYNGRAPH is the depen-
dency structure of an original sentence (in this paper,
a robust parser (Kurohashi and Nagao, 1994) is al-
ways employed). In the dependency structure, each
node consists of one content word and zero or more
function words, which is called a basic node here-
after. If the content word of a basic node belongs to
a synonymous group, a new node with the SYNID is
attached to it, and it is called a SYN node hereafter.
For example, in Figure 2, the shaded nodes are basic
nodes and the other nodes are SYN nodes5.
Then, if the expression conjoining two or more
4Spelling variations such as use of Hiragana, Katakana
or Kanji are handled by the morphological analyzer JUMAN
(Kurohashi et al, 1994).
5The reason why we distinguish basic nodes from SYN
nodes is to give priority to exact matching over synonymous
matching.
789
hotel ni
<hotel> ni i c hi b a n( b es t)
<m os t> c hi k a i( n ea r )
<n ea r es t>
0.99
1 .0
0.99
0.99
1 .0
1 .0
m oy or i( n ea r es t)
0.99
1 .0
<n ea r es t>
N M S = 0 . 9 8
N M S = 0 . 9
ek i( s ta ti on ) w a N M S = 1 . 01 .0
ek i( s ta ti on ) w a1 .0
hotel no
<hotel> no0.99
1 .0
Figure 2: SYNGRAPH matching.
nodes corresponds to one synonymous group, a
SYN node is added there. In Figure 2, ?nearest? is
such a SYN node. Furthermore, if one SYN node
has a hyper synonymous group in the synonymy
database, the SYN node with the hyper SYNID is
also added.
In this SYNGRAPH data structure, each node has
a score, NS (Node Score), which reflects how much
the expression of the node is shifted from the orig-
inal expression. We explain how to calculate NSs
later.
3.2 SYNGRAPH Matching
Two SYNGRAPHs match if and only if
? all the nodes in one SYNGRAPH can be
matched to the nodes in the other one,
? the matched nodes in two SYNGRAPHs have
the same dependency structure, and
? the nodes can cover the original sentences.
An example of SYNGRAPH matching is illustrated
in Figure 2. When two SYNGRAPHs match each
other, their matching score is calculated as follows.
First, the matching score of the matching two nodes,
NMS (Node Match Score) is calculated with their
node scores, NS1 and NS2,
NMS = NS 1 ? NS 2 ? FI Penalty,
where we define FI Penalty (Function word Incon-
sistency Penalty) is 0.9 when their function words
are not the same, and 1.0 otherwise.
Then, the matching score of two SYNGRAPHs,
SMS (SYNGRAPH Match Score) is defined as the
average of NMSs weighted by the number of basic
nodes,
SMS =
?
(# of basic nodes ? NMS)?
# of basic nodes
.
In an example shown in Figure 2, the NMS of the
left-hand side hotel node and the right-hand side ho-
tel node is 0.9 (= 1.0 ? 1.0 ? 0.9). The NMS of the
left-hand side ?nearest? node and the right-hand side
?nearest? node is 0.98 (= 0.99 ? 0.99 ? 1.0). Then,
the SMS becomes 0.9?2+0.98?3+1.0?22+3+2 = 0.96.
3.3 SYNGRAPH Transformation of Synonymy
Database
The synonymy database is transformed into SYN-
GRAPHs, where SYNGRAPH matching is itera-
tively applied to interpret the mutual relationships
in the synonymy database, as follows:
Step 1: Each expression in each synonymous group
is parsed and transformed into a fundamental SYN-
GRAPH.
Step 2: SYNGRAPH matching is applied to check
whether a sub-tree of one expression is matched with
any other whole expressions. If there is a match, a
new node with the SYNID of the whole matched ex-
pression is assigned to the partially matched nodes
group. Furthermore, if the SYNID has a hyper syn-
onymous group, another new node with the hyper-
nym SYNID is also assigned. This checking process
starts from small parts to larger parts.
We define the NS of the newly assigned SYN
node as the SMS multiplied by a relation penalty.
Here, we define the synonymy relation penalty as
0.99 and the hypernym relation penalty as 0.7. For
instance, the NS of ?underwater? node is 0.99 and
that of ?inside? node is 0.7.
Step 3: Repeat Step 2, until no more new SYN node
can be assigned to any expressions. In the case of
Figure 3 example, the new SYN node, ?diving? is
given to ?suityu (underwater) ni (to) moguru (dive)?
of ?diving(sport)? at the second iteration.
4 Flexible Matching using SYNGRAPH
We use example-based machine translation (EBMT)
as an example to explain how our flexible matching
method works (Figure 4). EBMT generates a trans-
lation by combining partially matching TEs with an
input6. We use flexible matching to fully exploit the
TEs.
6How to select the best TEs and combine the selected TEs
for generating a translation is omitted in this paper.
790
ni
ni
ni
ni
sport
ni
ni
moguru(dive)
ni
<underwater>
<inside>
<diving>
0.99
0.7
0.99
1.0
1.0
diving1.0
<diving(sport)>
mizu(water)
suityu(underwater)
naka(inside)
1.0
1.0
1.0
<underwater>
<inside>
<inside>0.99
naka(inside)
<inside>
moguru(dive)
0.99
1.0
1.0
<inside>0.7
mizu(water)1.0
sensui(diving)1.0
<diving>Synonymy databaseTranslation example
naka(inside)
suityu(underwater) no
no
1.0
1.0
suru
sport
suru
<diving>
<diving(sport)>
 
 
sensui(diving)
0.99
1.0
0.93
1.0 <underwater> 0.99
Figure 3: SYNGRAPH transformation of synonymy database.input sentence translation examplestransform into a SYNGRAPH
Japanese English
Figure 4: Flexible matching using SYNGRAPH in
EBMT.
First, TEs are transformed into SYNGRAPHs by
SYNGRAPH matching with SYNGRAPHs of the
synonymy database. Since the synonymy database
has been transformed into SYNGRAPHs, we do not
need to care the combinations of synonymous ex-
pressions any more. In the example shown in Fig-
ure 3, ?sensui (diving) suru (do) sport? in the TE is
given ?diving(sport)? node just by looking at SYN-
GRAPHs in ?diving(sport)? synonymous group.
Then, an input sentence is transformed into a
SYNGRAPH by SYNGRAPH matching, and then
the SYNGRAPH matching is applied between all
the sub trees of the input SYNGRAPH and SYN-
GRAPHs of TEs to retrieve the partially matching
TEs.
5 Experiments and Discussion
5.1 Evaluation on Machine Translation Task
To see the effectiveness of the our proposed method,
we conducted our evaluations on a MT task us-
ing Japanese-English translation training corpus
(20,000 sentence pairs) and 506 test sentences of
IWSLT?057. As an evaluation measure, NIST and
BLEU were used based on 16 reference English sen-
tences for each test sentence.
7http://www.is.cs.cmu.edu/iwslt2005/.
Table 1: Size of synonymy database.
# of synonymous group 5,046
# of hypernym-hyponym relation 18,590
The synonymy database used in the experiments
was automatically extracted from the REIKAI-
SHOGAKU dictionary (a dictionary for children),
which consists of about 30,000 entries. Table
1 shows the size of the constructed synonymy
database.
As a base translation system, we used an EBMT
system developed by (Kurohashi et al, 2005). Ta-
ble 2 shows the experimental results. ?None? means
the baseline system without using the synonymy
database. ?Synonym? is the system using only
synonymous relations, and it performed best and
achieved 1.2% improvement for NIST and 0.8%
improvement for BLEU over the baseline. These
differences are statistically significant (p < 0.05).
Some TEs that can be retrieved by our flexible
matching are shown below:
? input: fujin (lady) you (for) toile (toilet) ?
TE: josei (woman) you (for) toile (toilet)
? input: kantan-ni ieba (in short)?TE: tsumari
(in other words)
On the other hand, if the system also uses
hypernym-hyponym relation (?Synonym Hyper-
nym?), the score goes down. It proves that hyper-
nym examples are not necessarily good for trans-
lation. For example, for a translation of depato
(department store), its hypernym ?mise(store)? was
used, and it lowered the score.
Major errors are caused by the deficiency of word
sense disambiguation. When a polysemic word oc-
curs in a sentence, multiple SYNIDs are attached
to the word, and thus, the incorrect matching might
be occurred. Incorporation of unsupervised word-
791
Table 2: Evaluation results on MT task.
Synonymy DB NIST BLEU
None 8.023 0.375
Synonym 8.121 0.378
Synonym Hypernym 8.010 0.374
Table 3: Evaluation results on IR task.
Method Synonymy DB R-prec
Best IREX system ? 0.493
BM25 ? 0.474
None 0.492
Our method Synonym 0.509
Synonym Hypernym 0.514
sense-disambiguation of words in dictionary defini-
tions and matching sentences is one of our future
research targets.
5.2 Evaluation on Information Retrieval Task
To demonstrate the effectiveness of our method
in other NLP tasks, we also evaluated it in IR.
More concretely, we extended word-based impor-
tance weighting of Okapi BM25 (Robertson et al,
1994) to SYN node-based weighting. We used the
data set of IR evaluation workshop IREX, which
contains 30 queries and their corresponding relevant
documents in 2-year volume of newspaper articles8.
Table 3 shows the experimental results, which are
evaluated with R-precision. The baseline system is
our implementation of OKAPI BM25. Differently
from the MT task, the system using both synonym
and hypernym-hyponym relations performed best,
and its improvement over the baseline was 7.8%
relative. This difference is statistically significant
(p < 0.05). This result shows the wide applicabil-
ity of our flexible matching method for NLP tasks.
Some examples that can be retrieved by our flexible
matching are shown below:
? query: gakkou-ni (school) computer-wo
(computer) dounyuu (introduce) ? docu-
ment: shou-gakkou-ni (elementary school)
pasokon-wo (personal computer) dounyuu
(introduce)
6 Conclusion
This paper proposed a flexible matching method by
extracting synonymous expressions from an ordi-
nary dictionary and a Web corpus, and introducing
SYNGRAPH data structure. We confirmed the ef-
fectiveness of our method on experiments of ma-
chine translation and information retrieval.
8http://nlp.cs.nyu.edu/irex/.
Our future research targets are to incorporate
word sense disambiguation to our framework, and
to extend SYNGRAPH matching to more structural
paraphrases.
References
Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence align-
ment. In HLT-NAACL 2003, pages 16?23.
Zellig Harris. 1968. Mathematical Structures of Language.
Wiley.
Christian Jacquemin, Judith L. Klavans, and Evelyne Tzouker-
mann. 1997. Expansion of multi-word terms for indexing
and retrieval using morphology and syntax. In 35th Annual
Meeting of the Association for Computational Linguistics,
pages 24?31.
J.R.Firth. 1957. A synopsis of linguistic theory, 1933-1957. In
Studies in Linguistic Analysis, pages 1?32. Blackwell.
Daisuke Kawahara and Sadao Kurohashi. 2001. Japanese case
frame construction by coupling the verb and its closest case
component. In Proc. of HLT 2001, pages 204?210.
Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame
compilation from the web using high-performance comput-
ing. In Proc. of LREC-06.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic anal-
ysis method of long japanese sentences based on the detec-
tion of conjunctive structures. Computational Linguistics,
20(4):507?534.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and
Makoto Nagao. 1994. Improvements of Japanese mor-
phological analyzer JUMAN. In Proc. of the International
Workshop on Sharable Natural Language, pages 22?28.
Sadao Kurohashi, Toshiaki Nakazawa, Kauffmann Alexis, and
Daisuke Kawahara. 2005. Example-based machine transla-
tion pursuing fully structural NLP. In Proc. of IWSLT?05,
pages 207?212.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question answering. Natural Language Engineer-
ing, 7(4):343?360.
Junichi Nakamura and Makoto Nagao. 1988. Extraction of se-
mantic information from an ordinary english dictionary and
its evaluation. In Proc. of the 12th COLING, pages 459?464.
S. E. Robertson, S. Walker, S. Jones, M.M. Hancock-Beaulieu,
and M. Gatford. 1994. Okapi at TREC-3. In the third Text
REtrieval Conference (TREC-3).
Hiroaki Tsurumaru, Toru Hitaka, and Sho Yoshida. 1986. An
attempt to automatic thesaurus construction from an ordinary
japanese language dictionary. In Proc. of the 11th COLING,
pages 445?447.
Ellen M. Voorhees. 1994. Query expansion using lexical-
semantic relations. In SIGIR, pages 61?69.
792
Proceedings of NAACL-HLT 2013, pages 655?660,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Translation Acquisition Using Synonym Sets
Daniel Andrade Masaaki Tsuchida Takashi Onishi Kai Ishikawa
Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan
{s-andrade@cj, m-tsuchida@cq,
t-onishi@bq, k-ishikawa@dq}.jp.nec.com
Abstract
We propose a new method for translation ac-
quisition which uses a set of synonyms to ac-
quire translations from comparable corpora.
The motivation is that, given a certain query
term, it is often possible for a user to specify
one or more synonyms. Using the resulting
set of query terms has the advantage that we
can overcome the problem that a single query
term?s context vector does not always reliably
represent a terms meaning due to the context
vector?s sparsity. Our proposed method uses
a weighted average of the synonyms? context
vectors, that is derived by inferring the mean
vector of the von Mises-Fisher distribution.
We evaluate our method, using the synsets
from the cross-lingually aligned Japanese and
English WordNet. The experiments show that
our proposed method significantly improves
translation accuracy when compared to a pre-
vious method for smoothing context vectors.
1 Introduction
Automatic translation acquisition is an important
task for various applications. For example, finding
term translations can be used to automatically up-
date existing bilingual dictionaries, which are an in-
dispensable resource for tasks such as cross-lingual
information retrieval and text mining.
Various previous research like (Rapp, 1999; Fung,
1998) has shown that it is possible to acquire word
translations from comparable corpora.
We suggest here an extension of this approach
which uses several query terms instead of a single
query term. A user who searches a translation for
a query term that is not listed in an existing bilin-
gual dictionary, might first try to find a synonym
of that term. For example, the user might look up
a synonym in a thesaurus1 or might use methods
for automatic synonym acquisition like described
in (Grefenstette, 1994). If the synonym is listed in
the bilingual dictionary, we can consider the syn-
onym?s translations as the translations of the query
term. Otherwise, if the synonym is not listed in the
dictionary either, we use the synonym together with
the original query term to find a translation.
We claim that using a set of synonymous query
terms to find a translation is better than using a single
query term. The reason is that a single query term?s
context vector is, in general, unreliable due to spar-
sity. For example, a low frequent query term tends to
have many zero entries in its context vector. To mit-
igate this problem it has been proposed to smooth
a query?s context vector by its nearest neighbors
(Pekar et al, 2006). However, nearest neighbors,
which context vectors are close the query?s context
vector, can have different meanings and therefore
might introduce noise.
The contributions of this paper are two-fold. First,
we confirm experimentally that smoothing a query?s
context vector with its synonyms leads in deed to
higher translation accuracy, compared to smoothing
with nearest neighbors. Second, we propose a sim-
ple method to combine a set of context vectors that
performs in this setting better than a method previ-
ously proposed by (Pekar et al, 2006).
Our approach to combine a set of context vec-
1Monolingual thesauri are, arguably, easier to construct than
bilingual dictionaries.
655
tors is derived by learning the mean vector of a von
Mises-Fisher distribution. The combined context
vector is a weighted-average of the original context-
vectors, where the weights are determined by the
word occurrence frequencies.
In the following section we briefly show the rela-
tion to other previous work. In Section 3, we explain
our method in detail, followed by an empirical eval-
uation in Section 4. We summarize our results in
Section 6.
2 Related Work
There are several previous works on extract-
ing translations from comparable corpora ranging
from (Rapp, 1999; Fung, 1998), and more re-
cently (Haghighi et al, 2008; Laroche and Langlais,
2010), among others. Essentially, all these meth-
ods calculate the similarity of a query term?s context
vector with each translation candidate?s context vec-
tor. The context vectors are extracted from the com-
parable corpora, and mapped to a common vector
space with the help of an existing bilingual dictio-
nary.
The work in (De?jean et al, 2002) uses cross-
lingually aligned classes in a multilingual thesaurus
to improve the translation accuracy. Their method
uses the probability that the query term and a trans-
lation candidate are assigned to the same class. In
contrast, our method does not need cross-lingually
aligned classes.
Ismail and Manandhar (2010) proposes a method
that tries to improve a query?s context vector by us-
ing in-domain terms. In-domain terms are the terms
that are highly associated to the query, as well as
highly associated to one of the query?s highly asso-
ciated terms. Their method makes it necessary that
the query term has enough highly associated context
terms.2 However, a low-frequent query term might
not have enough highly associated terms.
In general if a query term has a low-frequency in
the corpus, then its context vector is sparse. In that
case, the chance of finding a correct translation is
reduced (Pekar et al, 2006). Therefore, Pekar et al
(2006) suggest to use distance-based averaging to
smooth the context vector of a low-frequent query
2In their experiments, they require that a query word has at
least 100 associated terms.
term. Their smoothing strategy is dependent on the
occurrence frequency of a query term and its close
neighbors. Let us denote q the context vector of the
query word, and K be the set of its close neighbors.
The smoothed context vector q? is then derived by
using:
q? = ? ? q + (1 ? ?) ?
?
x?K
wx ? x , (1)
where wx is the weight of neighbor x, and all
weights sum to one. The context vectors q and x
are interpreted as probability vectors and therefore
L1-normalized. The weight wx is a function of the
distance between neighbor x and query q. The pa-
rameter ? determines the degree of smoothing, and
is a function of the frequency of the query term and
its neighbors:
? = log f(q)logmaxx?K?{q} f(x)
(2)
where f(x) is the frequency of term x. Their method
forms the baseline for our proposed method.
3 Proposed Method
Our goal is to combine the context vectors to one
context vector which is less sparse and more reli-
able than the original context vector of query word
q. We assume that for each occurrence of a word,
its corresponding context vector was generated by
a probabilistic model. Furthermore, we assume that
synonyms are generated by the same probability dis-
tribution. Finally we use the mean vector of that dis-
tribution to represent the combined context vector.
By using the assumption that each occurrence of a
word corresponds to one sample of the probability
distribution, our model places more weight on syn-
onyms that are highly-frequent than synonyms that
occur infrequently. This is motivated by the assump-
tion that context vectors of synonyms that occur with
high frequency in the corpus, are more reliable than
the ones of low-frequency synonyms.
When comparing context vectors, work
like Laroche and Langlais (2010) observed
that often the cosine similarity performs superior
to other distance-measures, like, for example, the
euclidean distance. This suggests that context
vectors tend to lie in the spherical vector space,
656
and therefore the von Mises-Fisher distribution is
a natural choice for our probabilistic model. The
von Mises-Fisher distribution was also successfully
used in the work of (Basu et al, 2004) to cluster
text data.
The von Mises-Fisher distribution with location
parameter ?, and concentration parameter ? is de-
fined as:
p(x|?, ?) = c(?) ? e??x??T ,
where c(?) is a normalization constant, and ||x|| =
||?|| = 1, and ? ? 0. || denotes here the L2-norm.
The cosine-similarity measures the angle between
two vectors, and the von Mises distribution defines
a probability distribution over the possible angles.
The parameter ? of the von Mises distribution is es-
timated as follows (Jammalamadaka and Sengupta,
2001): Given the words x1, ..., xn, we denote the
corresponding context vectors as x1, ...,xn, and as-
sume that each context vector is L2-normalized.
Then, the mean vector ? is calculated as:
? = 1Z
n
?
i=1
xi
n
where Z ensures that the resulting context vector is
L2-normalized, i.e. Z is ||
?n
i=1
xi
n ||. For our pur-
pose, ? is irrelevant and is assumed to be any fixed
positive constant.
Since we assume that each occurrence of a word x
in the corpus corresponds to one observation of the
corresponding word?s context vector x, we get the
following formula:
? = 1Z ? ?
n
?
i=1
f(xi)
?n
j=1 f(xj)
? xi
where Z ? is now ||
?n
i=1
f(xi)
?n
j=1 f(xj)
? xi||. We then
use the vector ? as the combined vector of the
words? context vectors xi.
Our proposed procedure to combine the context
vector of query word q and its synonyms can be sum-
marized as follows:
1. Denote the context vectors of q and its syn-
onyms as x1, ...,xn, and L2-normalize each
context vector.
2. Calculate the weighted average of the vectors
x1, ...,xn, whereas the weights correspond to
the frequencies of each word xi.
3. L2-normalize the weighted average.
4 Experiments
As source and target language corpora we use a cor-
pus extracted from a collection of complaints con-
cerning automobiles compiled by the Japanese Min-
istry of Land, Infrastructure, Transport and Tourism
(MLIT)3 and the USA National Highway Traffic
Safety Administration (NHTSA)4, respectively. The
Japanese corpus contains 24090 sentences that were
POS tagged using MeCab (Kudo et al, 2004). The
English corpus contains 47613 sentences, that were
POS tagged using Stepp Tagger (Tsuruoka et al,
2005), and use the Lemmatizer (Okazaki et al,
2008) to extract and stem content words (nouns,
verbs, adjectives, adverbs).
For creating the context vectors, we calculate the
association between two content words occurring
in the same sentence, using the log-odds-ratio (Ev-
ert, 2004). It was shown in (Laroche and Langlais,
2010) that the log-odds-ratio in combination with
the cosine-similarity performs superior to several
other methods like PMI5 and LLR6. For comparing
two context vectors we use the cosine similarity.
To transform the Japanese and English context
vectors into the same vector space, we use a bilin-
gual dictionary with around 1.6 million entries.7
To express all context vectors in the same vector
space, we map the context vectors in English to con-
text vectors in Japanese.8 First, for all the words
which are listed in the bilingual dictionary we calcu-
late word translation probabilities. These translation
probabilities are calculated using the EM-algorithm
described in (Koehn and Knight, 2000). We then
create a translation matrix T which contains in each
3http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html
4http://www-odi.nhtsa.dot.gov/downloads/index.cfm
5point-wise mutual information
6log-likelihood ratio
7The bilingual dictionary was developed in the course of our
Japanese language processing efforts described in (Sato et al,
2003).
8Alternatively, we could, for example, use canonical corre-
lation analysis to match the vectors to a common latent vector
space, like described in (Haghighi et al, 2008).
657
column the translation probabilities for a word in
English into any word in Japanese. Each context
vector in English is then mapped into Japanese us-
ing the linear transformation described by the trans-
lation matrix T . For word x with context vector x in
English, let x? be its context vector after transforma-
tion into Japanese, i.e. x? = T ? x.
The gold-standard was created by considering
all nouns in the Japanese and English WordNet
where synsets are aligned cross-lingually. This way
we were able to create a gold-standard with 215
Japanese nouns, and their respective English trans-
lations that occur in our comparable corpora.9 Note
that the cross-lingual alignment is needed only for
evaluation. For evaluation, we consider only the
translations that occur in the corresponding English
synset as correct.
Because all methods return a ranked list of trans-
lation candidates, the accuracy is measured using the
rank of the translation listed in the gold-standard.
The inverse rank is the sum of the inverse ranks of
each translation in the gold-standard.
In Table 1, the first row shows the results when us-
ing no smoothing. Next, we smooth the query?s con-
text vector by using Equation (1) and (2). The set of
neighbors K is defined as the k-terms in the source
language that are closest to the query word, with re-
spect to the cosine similarity (sim). The weight wx
for a neighbor x is set to wx = 100.13?sim(x,q) in
accordance to (Pekar et al, 2006). For k we tried
values between 1 and 100, and got the best inverse
rank when using k=19. The resulting method (Top-
k Smoothing) performs consistently better than the
method using no smoothing, see Table 1, second
row. Next, instead of smoothing the query word with
its nearest neighbors, we use as the set K the set of
synonyms of the query word (Syn Smoothing). Ta-
ble 1 shows a clear improvement over the method
that uses nearest neighbor-smoothing. This confirms
our claim that using synonyms for smoothing can
lead to better translation accuracy than using nearest
neighbors. In the last row of Table 1, we compare
our proposed method to combine context vectors of
synonyms (Syn Mises-Combination), with the pre-
9The resulting synsets in Japanese and English, contain in
average 2.2 and 2.8 words, respectively. The ambiguity of a
query term in our gold-standard is low, since, in average, a
query term belongs to only 1.2 different synsets.
vious method (Syn Smoothing). A pair-wise com-
parison of our proposed method with Syn Smooth-
ing shows a statistically significant improvement (p
< 0.01).10
Finally, we also show the result when simply
adding each synonym vector to the query?s context
vector to form a new combined context vector (Syn
Sum).11 Even though, this approach does not use the
frequency information of a word, it performs bet-
ter than Syn Smoothing. We suppose that this is
due to the fact that it actually indirectly uses fre-
quency information, since the log-odds-ratio tends
to be higher for words which occur with high fre-
quency in the corpus.
Method Top1 Top5 Top10 MIR
No Smoothing 0.14 0.30 0.36 0.23
Top-k Smoothing 0.16 0.33 0.43 0.26
Syn Smoothing 0.18 0.35 0.46 0.28
Syn Sum 0.23 0.46 0.57 0.35
Syn Mises-Combination 0.31 0.46 0.55 0.40
Table 1: Shows Top-n accuracy and mean inverse rank
(MIR) for baseline methods which use no synonyms
(No Smoothing, Top-k Smoothing), the proposed method
(Syn Mises-Combination) which uses synonyms, and al-
ternative methods that also use synonyms (Syn Smooth-
ing, Syn Sum).
5 Discussion
We first discuss an example where the query terms
are???? (cruise) and?? (cruise). Both words
can have the same meaning. The resulting trans-
lation candidates suggested by the baseline meth-
ods and the proposed method is shown in Table 2.
Using no smoothing, the baseline method outputs
the correct translation for ???? (cruise) and ?
? (cruise) at rank 10 and 15, respectively. When
combining both queries to form one context vector
our proposed method (Syn Mises-Combination) re-
trieves the correct translation at rank 2. Note that we
considered all nouns that occur three or more times
as possible translation candidates. As can be seen
in Table 2, this also includes spelling mistakes like
?sevice? and ?infromation?.
10We use the sign-test (Wilcox, 2009) to test the hypothesis
that the proposed method ranks higher than the baseline.
11No normalization is performed before adding the context
vectors.
658
Method Query Output Rank
No Smoothing ???? ..., affinity, delco, cruise, sevice, sentrum,... 10
No Smoothing ?? ..., denali, attendant, cruise, abs, tactic,... 15
Top-k Smoothing ???? pillar, multi, cruise, star, affinity,... 3
Top-k Smoothing ?? ..., burnout, dipstick, cruise, infromation, speed, ... 8
Syn Smoothing ???? smoothed with?? ..., affinity, delco, cruise, sevice, sentrum,... 10
Syn Smoothing ?? smoothed with???? ..., alldata, mode, cruise, expectancy, mph,... 8
Syn Sum ????,?? assumption, level, cruise, reimbursment, infromation,... 3
Syn Mises-Combination ????,?? pillar, cruise, assumption, level, speed,... 2
Table 2: Shows the results for ???? and ?? which both have the same meaning ?cruise?. The third column
shows part of the ranked translation candidates separated by comma. The last column shows the rank of the correct
translation ?cruise?. Syn Smoothing uses Equation (1) with q corresponding to the context vector of the query word,
andK contains only the context vector of the term that is used for smoothing.
Finally, we note that some terms in our test set
are ambiguous, and the ambiguity is not resolved by
using the synonyms of only one synset. For exam-
ple, the term ?? (steering, guidance) belongs to
the synset ?steering, guidance? which includes the
terms??? (steering, guidance) and??? (guid-
ance), ?? (guidance). Despite this conflation of
senses in one synset, our proposed method can im-
prove the finding of (one) correct translation. The
baseline system using only?? (steering, guidance)
outputs the correct translation ?steering? at rank 4,
whereas our method using all four terms outputs it
at rank 2.
6 Conclusions
We proposed a new method for translation acquisi-
tion which uses a set of synonyms to acquire transla-
tions. Our approach combines the query term?s con-
text vector with all the context vectors of its syn-
onyms. In order to combine the vectors we use a
weighted average of each context vector, where the
weights are determined by a term?s occurrence fre-
quency.
Our experiments, using the Japanese and English
WordNet (Bond et al, 2009; Fellbaum, 1998), show
that our proposed method can increase the transla-
tion accuracy, when compared to using only a single
query term, or smoothing with nearest neighbours.
Our results suggest that instead of directly search-
ing for a translation, it is worth first looking for syn-
onyms, for example by considering spelling varia-
tions or monolingual resources.
References
S. Basu, M. Bilenko, and R.J. Mooney. 2004. A prob-
abilistic framework for semi-supervised clustering. In
Proceedings of the ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 59?68.
F. Bond, H. Isahara, S. Fujita, K. Uchimoto, T. Kurib-
ayashi, and K. Kanzaki. 2009. Enhancing the
japanese wordnet. In Proceedings of the 7th Workshop
on Asian Language Resources, pages 1?8. Association
for Computational Linguistics.
H. De?jean, E?. Gaussier, and F. Sadat. 2002. An approach
based on multilingual thesauri and model combination
for bilingual lexicon extraction. In Proceedings of the
International Conference on Computational Linguis-
tics, pages 1?7. International Committee on Computa-
tional Linguistics.
S. Evert. 2004. The statistics of word cooccurrences:
word pairs and collocations. Doctoral dissertation, In-
stitut fu?r maschinelle Sprachverarbeitung, Universita?t
Stuttgart.
C. Fellbaum. 1998. Wordnet: an electronic lexical
database. Cambrige, MIT Press, Language, Speech,
and Communication.
P. Fung. 1998. A statistical view on bilingual lexicon ex-
traction: from parallel corpora to non-parallel corpora.
Lecture Notes in Computer Science, 1529:1?17.
G. Grefenstette. 1994. Explorations in automatic the-
saurus discovery. Springer.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, pages
771?779. Association for Computational Linguistics.
A. Ismail and S. Manandhar. 2010. Bilingual lexicon
extraction from comparable corpora using in-domain
terms. In Proceedings of the International Conference
on Computational Linguistics, pages 481 ? 489.
659
S.R. Jammalamadaka and A. Sengupta. 2001. Topics in
circular statistics, volume 5. World Scientific Pub Co
Inc.
P. Koehn and K. Knight. 2000. Estimating word trans-
lation probabilities from unrelated monolingual cor-
pora using the em algorithm. In Proceedings of the
National Conference on Artificial Intelligence, pages
711?715. Association for the Advancement of Artifi-
cial Intelligence.
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Ap-
plying conditional random fields to Japanese morpho-
logical analysis. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 230?237. Association for Computational Lin-
guistics.
A. Laroche and P. Langlais. 2010. Revisiting context-
based projection methods for term-translation spotting
in comparable corpora. In Proceedings of the In-
ternational Conference on Computational Linguistics,
pages 617 ? 625.
N. Okazaki, Y. Tsuruoka, S. Ananiadou, and J. Tsujii.
2008. A discriminative candidate generator for string
transformations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 447?456. Association for Computational Lin-
guistics.
V. Pekar, R. Mitkov, D. Blagoev, and A. Mulloni. 2006.
Finding translations for low-frequency words in com-
parable corpora. Machine Translation, 20(4):247?
266.
R. Rapp. 1999. Automatic identification of word transla-
tions from unrelated English and German corpora. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, pages 519?526. Asso-
ciation for Computational Linguistics.
K. Sato, T. Ikeda, T. Nakata, and S. Osada. 2003. In-
troduction of a Japanese language processing mid-
dleware used for CRM. In Annual Meeting of the
Japanese Association for Natural Language Process-
ing (in Japanese), pages 109?112.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. Lecture
Notes in Computer Science, 3746:382?392.
R.R. Wilcox. 2009. Basic Statistics: Understanding
Conventional Methods and Modern Insights. Oxford
University Press.
660
Proceedings of the ACL 2010 Conference Short Papers, pages 1?5,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Paraphrase Lattice for Statistical Machine Translation
Takashi Onishi and Masao Utiyama and Eiichiro Sumita
Language Translation Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, JAPAN
{takashi.onishi,mutiyama,eiichiro.sumita}@nict.go.jp
Abstract
Lattice decoding in statistical machine
translation (SMT) is useful in speech
translation and in the translation of Ger-
man because it can handle input ambigu-
ities such as speech recognition ambigui-
ties and German word segmentation ambi-
guities. We show that lattice decoding is
also useful for handling input variations.
Given an input sentence, we build a lattice
which represents paraphrases of the input
sentence. We call this a paraphrase lattice.
Then, we give the paraphrase lattice as an
input to the lattice decoder. The decoder
selects the best path for decoding. Us-
ing these paraphrase lattices as inputs, we
obtained significant gains in BLEU scores
for IWSLT and Europarl datasets.
1 Introduction
Lattice decoding in SMT is useful in speech trans-
lation and in the translation of German (Bertoldi
et al, 2007; Dyer, 2009). In speech translation,
by using lattices that represent not only 1-best re-
sult but also other possibilities of speech recogni-
tion, we can take into account the ambiguities of
speech recognition. Thus, the translation quality
for lattice inputs is better than the quality for 1-
best inputs.
In this paper, we show that lattice decoding is
also useful for handling input variations. ?Input
variations? refers to the differences of input texts
with the same meaning. For example, ?Is there
a beauty salon?? and ?Is there a beauty par-
lor?? have the same meaning with variations in
?beauty salon? and ?beauty parlor?. Since these
variations are frequently found in natural language
texts, a mismatch of the expressions in source sen-
tences and the expressions in training corpus leads
to a decrease in translation quality. Therefore,
we propose a novel method that can handle in-
put variations using paraphrases and lattice decod-
ing. In the proposed method, we regard a given
source sentence as one of many variations (1-best).
Given an input sentence, we build a paraphrase lat-
tice which represents paraphrases of the input sen-
tence. Then, we give the paraphrase lattice as an
input to the Moses decoder (Koehn et al, 2007).
Moses selects the best path for decoding. By using
paraphrases of source sentences, we can translate
expressions which are not found in a training cor-
pus on the condition that paraphrases of them are
found in the training corpus. Moreover, by using
lattice decoding, we can employ the source-side
language model as a decoding feature. Since this
feature is affected by the source-side context, the
decoder can choose a proper paraphrase and trans-
late correctly.
This paper is organized as follows: Related
works on lattice decoding and paraphrasing are
presented in Section 2. The proposed method is
described in Section 3. Experimental results for
IWSLT and Europarl dataset are presented in Sec-
tion 4. Finally, the paper is concluded with a sum-
mary and a few directions for future work in Sec-
tion 5.
2 Related Work
Lattice decoding has been used to handle ambigu-
ities of preprocessing. Bertoldi et al (2007) em-
ployed a confusion network, which is a kind of lat-
tice and represents speech recognition hypotheses
in speech translation. Dyer (2009) also employed
a segmentation lattice, which represents ambigui-
ties of compound word segmentation in German,
Hungarian and Turkish translation. However, to
the best of our knowledge, there is no work which
employed a lattice representing paraphrases of an
input sentence.
On the other hand, paraphrasing has been used
to enrich the SMT model. Callison-Burch et
1
Input sentence 
Paraphrase Lattice
Output sentence 
Paraphrase
List
SMT model
Parallel Corpus
(for paraphrase)
Parallel Corpus
(for training)
Paraphrasing
Lattice Decoding
Figure 1: Overview of the proposed method.
al. (2006) and Marton et al (2009) augmented
the translation phrase table with paraphrases to
translate unknown phrases. Bond et al (2008)
and Nakov (2008) augmented the training data by
paraphrasing. However, there is no work which
augments input sentences by paraphrasing and
represents them in lattices.
3 Paraphrase Lattice for SMT
Overview of the proposed method is shown in Fig-
ure 1. In advance, we automatically acquire a
paraphrase list from a parallel corpus. In order to
acquire paraphrases of unknown phrases, this par-
allel corpus is different from the parallel corpus
for training.
Given an input sentence, we build a lattice
which represents paraphrases of the input sentence
using the paraphrase list. We call this lattice a
paraphrase lattice. Then, we give the paraphrase
lattice to the lattice decoder.
3.1 Acquiring the paraphrase list
We acquire a paraphrase list using Bannard and
Callison-Burch (2005)?s method. Their idea is, if
two different phrases e1, e2 in one language are
aligned to the same phrase c in another language,
they are hypothesized to be paraphrases of each
other. Our paraphrase list is acquired in the same
way.
The procedure is as follows:
1. Build a phrase table.
Build a phrase table from parallel corpus us-
ing standard SMT techniques.
2. Filter the phrase table by the sigtest-filter.
The phrase table built in 1 has many inappro-
priate phrase pairs. Therefore, we filter the
phrase table and keep only appropriate phrase
pairs using the sigtest-filter (Johnson et al,
2007).
3. Calculate the paraphrase probability.
Calculate the paraphrase probability p(e2|e1)
if e2 is hypothesized to be a paraphrase of e1.
p(e2|e1) =
?
c
P (c|e1)P (e2|c)
where P (?|?) is phrase translation probability.
4. Acquire a paraphrase pair.
Acquire (e1, e2) as a paraphrase pair if
p(e2|e1) > p(e1|e1). The purpose of this
threshold is to keep highly-accurate para-
phrase pairs. In experiments, more than 80%
of paraphrase pairs were eliminated by this
threshold.
3.2 Building paraphrase lattice
An input sentence is paraphrased using the para-
phrase list and transformed into a paraphrase lat-
tice. The paraphrase lattice is a lattice which rep-
resents paraphrases of the input sentence. An ex-
ample of a paraphrase lattice is shown in Figure 2.
In this example, an input sentence is ?is there a
beauty salon ??. This paraphrase lattice contains
two paraphrase pairs ?beauty salon? = ?beauty
parlor? and ?beauty salon? = ?salon?, and rep-
resents following three sentences.
? is there a beauty salon ?
? is there a beauty parlor ?
? is there a salon ?
In the paraphrase lattice, each node consists of
a token, the distance to the next node and features
for lattice decoding. We use following four fea-
tures for lattice decoding.
? Paraphrase probability (p)
A paraphrase probability p(e2|e1) calculated
when acquiring the paraphrase.
hp = p(e2|e1)
? Language model score (l)
A ratio between the language model proba-
bility of the paraphrased sentence (para) and
that of the original sentence (orig).
hl = lm(para)lm(orig)
2
0 -- ("is"     , 1, 1, 1, 1)
1 -- ("there"  , 1, 1, 1, 1)
2 -- ("a"      , 1, 1, 1, 1)
3 -- ("beauty" , 1, 1, 1, 2) ("beauty" , 0.250, 1.172, 1, 1) ("salon" , 0.133, 0.537, 0.367, 3)
4 -- ("parlor" , 1, 1, 1, 2)
5 -- ("salon"  , 1, 1, 1, 1)
6 -- ("?"      , 1, 1, 1, 1)
Paraphrase probability (p)
Language model score (l)
Paraphrase length (d)
Distance to the next node Features for lattice decodingToken
Figure 2: An example of a paraphrase lattice, which contains three features of (p, l, d).
? Normalized language model score (L)
A language model score where the language
model probability is normalized by the sen-
tence length. The sentence length is calcu-
lated as the number of tokens.
hL = LM(para)LM(orig) ,
where LM(sent) = lm(sent)
1
length(sent)
? Paraphrase length (d)
The difference between the original sentence
length and the paraphrased sentence length.
hd = exp(length(para)? length(orig))
The values of these features are calculated only
if the node is the first node of the paraphrase, for
example the second ?beauty? and ?salon? in line
3 of Figure 2. In other nodes, for example ?par-
lor? in line 4 and original nodes, we use 1 as the
values of features.
The features related to the language model, such
as (l) and (L), are affected by the context of source
sentences even if the same paraphrase pair is ap-
plied. As these features can penalize paraphrases
which are not appropriate to the context, appropri-
ate paraphrases are chosen and appropriate trans-
lations are output in lattice decoding. The features
related to the sentence length, such as (L) and (d),
are added to penalize the language model score
in case the paraphrased sentence length is shorter
than the original sentence length and the language
model score is unreasonably low.
In experiments, we use four combinations of
these features, (p), (p, l), (p, L) and (p, l, d).
3.3 Lattice decoding
We use Moses (Koehn et al, 2007) as a decoder
for lattice decoding. Moses is an open source
SMT system which allows lattice decoding. In
lattice decoding, Moses selects the best path and
the best translation according to features added in
each node and other SMT features. These weights
are optimized using Minimum Error Rate Training
(MERT) (Och, 2003).
4 Experiments
In order to evaluate the proposed method, we
conducted English-to-Japanese and English-to-
Chinese translation experiments using IWSLT
2007 (Fordyce, 2007) dataset. This dataset con-
tains EJ and EC parallel corpus for the travel
domain and consists of 40k sentences for train-
ing and about 500 sentences sets (dev1, dev2
and dev3) for development and testing. We used
the dev1 set for parameter tuning, the dev2 set
for choosing the setting of the proposed method,
which is described below, and the dev3 set for test-
ing.
The English-English paraphrase list was ac-
quired from the EC corpus for EJ translation and
53K pairs were acquired. Similarly, 47K pairs
were acquired from the EJ corpus for EC trans-
lation.
4.1 Baseline
As baselines, we used Moses and Callison-Burch
et al (2006)?s method (hereafter CCB). In Moses,
we used default settings without paraphrases. In
CCB, we paraphrased the phrase table using the
automatically acquired paraphrase list. Then,
we augmented the phrase table with paraphrased
phrases which were not found in the original
phrase table. Moreover, we used an additional fea-
ture whose value was the paraphrase probability
(p) if the entry was generated by paraphrasing and
3
Moses (w/o Paraphrases) CCB Proposed Method
EJ 38.98 39.24 (+0.26) 40.34 (+1.36)
EC 25.11 26.14 (+1.03) 27.06 (+1.95)
Table 1: Experimental results for IWSLT (%BLEU).
1 if otherwise. Weights of the feature and other
features in SMT were optimized using MERT.
4.2 Proposed method
In the proposed method, we conducted experi-
ments with various settings for paraphrasing and
lattice decoding. Then, we chose the best setting
according to the result of the dev2 set.
4.2.1 Limitation of paraphrasing
As the paraphrase list was automatically ac-
quired, there were many erroneous paraphrase
pairs. Building paraphrase lattices with all erro-
neous paraphrase pairs and decoding these para-
phrase lattices caused high computational com-
plexity. Therefore, we limited the number of para-
phrasing per phrase and per sentence. The number
of paraphrasing per phrase was limited to three and
the number of paraphrasing per sentence was lim-
ited to twice the size of the sentence length.
As a criterion for limiting the number of para-
phrasing, we use three features (p), (l) and (L),
which are same as the features described in Sub-
section 3.2. When building paraphrase lattices, we
apply paraphrases in descending order of the value
of the criterion.
4.2.2 Finding optimal settings
As previously mentioned, we have three choices
for the criterion for building paraphrase lattices
and four combinations of features for lattice de-
coding. Thus, there are 3 ? 4 = 12 combinations
of these settings. We conducted parameter tuning
with the dev1 set for each setting and used as best
the setting which got the highest BLEU score for
the dev2 set.
4.3 Results
The experimental results are shown in Table 1. We
used the case-insensitive BLEU metric for eval-
uation. In EJ translation, the proposed method
obtained the highest score of 40.34%, which
achieved an absolute improvement of 1.36 BLEU
points over Moses and 1.10 BLEU points over
CCB. In EC translation, the proposed method also
obtained the highest score of 27.06% and achieved
an absolute improvement of 1.95 BLEU points
over Moses and 0.92 BLEU points over CCB. As
the relation of three systems is Moses < CCB <
Proposed Method, paraphrasing is useful for SMT
and using paraphrase lattices and lattice decod-
ing is especially more useful than augmenting the
phrase table. In ProposedMethod, the criterion for
building paraphrase lattices and the combination
of features for lattice decoding were (p) and (p, L)
in EJ translation and (L) and (p, l) in EC transla-
tion. Since features related to the source-side lan-
guage model were chosen in each direction, using
the source-side language model is useful for de-
coding paraphrase lattices.
We also tried a combination of Proposed
Method and CCB, which is a method of decoding
paraphrase lattices with an augmented phrase ta-
ble. However, the result showed no significant im-
provements. This is because the proposed method
includes the effect of augmenting the phrase table.
Moreover, we conducted German-English
translation using the Europarl corpus (Koehn,
2005). We used the WMT08 dataset1, which
consists of 1M sentences for training and 2K sen-
tences for development and testing. We acquired
5.3M pairs of German-German paraphrases from
a 1M German-Spanish parallel corpus. We con-
ducted experiments with various sizes of training
corpus, using 10K, 20K, 40K, 80K, 160K and 1M.
Figure 3 shows the proposed method consistently
get higher score than Moses and CCB.
5 Conclusion
This paper has proposed a novel method for trans-
forming a source sentence into a paraphrase lattice
and applying lattice decoding. Since our method
can employ source-side language models as a de-
coding feature, the decoder can choose proper
paraphrases and translate properly. The exper-
imental results showed significant gains for the
IWSLT and Europarl dataset. In IWSLT dataset,
we obtained 1.36 BLEU points over Moses in EJ
translation and 1.95 BLEU points over Moses in
1http://www.statmt.org/wmt08/
4
20
21
22
23
24
25
26
27
28
29
10 100 1000
Corpus size (K)
BLE
U s
cor
e (%
)
Moses
CCB
Proposed
Figure 3: Effect of training corpus size.
EC translation. In Europarl dataset, the proposed
method consistently get higher score than base-
lines.
In future work, we plan to apply this method
with paraphrases derived from a massive corpus
such as the Web corpus and apply this method to a
hierarchical phrase based SMT.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
597?604.
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In Proceedings of the International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), pages 1297?1300.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving Statistical Machine
Translation by Paraphrasing the Training Data. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 150?157.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of the
Human Language Technology conference - North
American chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pages 17?24.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for MT. In Proceed-
ings of the Human Language Technology confer-
ence - North American chapter of the Association
for Computational Linguistics (HLT-NAACL), pages
406?414.
Cameron S. Fordyce. 2007. Overview of the IWSLT
2007 Evaluation Campaign. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT), pages 1?12.
J Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving Translation Qual-
ity by Discarding Most of the Phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 177?180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the 10th Machine Translation Summit (MT Summit),
pages 79?86.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved Statistical Machine
Translation Using Monolingually-Derived Para-
phrases. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 381?390.
Preslav Nakov. 2008. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In
Proceedings of the European Conference on Artifi-
cial Intelligence (ECAI), pages 338?342.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL), pages 160?167.
5
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 434?438,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering Constraint Based on Document-Level Context
Takashi Onishi and Masao Utiyama and Eiichiro Sumita
Multilingual Translation Laboratory, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, JAPAN
{takashi.onishi,mutiyama,eiichiro.sumita}@nict.go.jp
Abstract
One problem with phrase-based statistical ma-
chine translation is the problem of long-
distance reordering when translating between
languages with different word orders, such as
Japanese-English. In this paper, we propose a
method of imposing reordering constraints us-
ing document-level context. As the document-
level context, we use noun phrases which sig-
nificantly occur in context documents contain-
ing source sentences. Given a source sen-
tence, zones which cover the noun phrases are
used as reordering constraints. Then, in de-
coding, reorderings which violate the zones
are restricted. Experiment results for patent
translation tasks show a significant improve-
ment of 1.20% BLEU points in Japanese-
English translation and 1.41% BLEU points in
English-Japanese translation.
1 Introduction
Phrase-based statistical machine translation is use-
ful for translating between languages with similar
word orders. However, it has problems with long-
distance reordering when translating between lan-
guages with different word orders, such as Japanese-
English. These problems are especially crucial when
translating long sentences, such as patent sentences,
because many combinations of word orders cause
high computational costs and low translation qual-
ity.
In order to address these problems, various meth-
ods which use syntactic information have been pro-
posed. These include methods where source sen-
tences are divided into syntactic chunks or clauses
and the translations are merged later (Koehn and
Knight, 2003; Sudoh et al, 2010), methods where
syntactic constraints or penalties for reordering are
added to a decoder (Yamamoto et al, 2008; Cherry,
2008; Marton and Resnik, 2008; Xiong et al, 2010),
and methods where source sentences are reordered
into a similar word order as the target language in
advance (Katz-Brown and Collins, 2008; Isozaki
et al, 2010). However, these methods did not
use document-level context to constrain reorderings.
Document-level context is often available in real-life
situations. We think it is a promising clue to improv-
ing translation quality.
In this paper, we propose a method where re-
ordering constraints are added to a decoder using
document-level context. As the document-level con-
text, we use noun phrases which significantly oc-
cur in context documents containing source sen-
tences. Given a source sentence, zones which cover
the noun phrases are used as reordering constraints.
Then, in decoding, reorderings which violate the
zones are restricted. By using document-level con-
text, contextually-appropriate reordering constraints
are preferentially considered. As a result, the trans-
lation quality and speed can be improved. Ex-
periment results for the NTCIR-8 patent transla-
tion tasks show a significant improvement of 1.20%
BLEU points in Japanese-English translation and
1.41%BLEU points in English-Japanese translation.
2 Patent Translation
Patent translation is difficult because of the amount
of new phrases and long sentences. Since a patent
document explains a newly-invented apparatus or
method, it contains many new phrases. Learning
phrase translations for these new phrases from the
434
Source ?????????????????????????????????
???????????????
Reference the pad electrode 11 is formed on the top surface of the semiconductor substrate 10 through an
interlayer insulation film 12 that is a first insulation film .
Baseline output an interlayer insulating film 12 is formed on the surface of a semiconductor substrate 10 , a
pad electrode 11 via a first insulating film .
Source + Zone ????????? <zone>??? <zone>??? </zone>????? <zone>?
?? </zone>?? </zone>???????????????????????
Proposed output pad electrode 11 is formed on the surface of the semiconductor substrate 10 through the inter-
layer insulating film 12 of the first insulating film .
Table 1: An example of patent translation.
training corpora is difficult because these phrases
occur only in that patent specification. Therefore,
when translating such phrases, a decoder has to com-
bine multiple smaller phrase translations. More-
over, sentences in patent documents tend to be long.
This results in a large number of combinations of
phrasal reorderings and a degradation of the transla-
tion quality and speed.
Table 1 shows how a failure in phrasal reorder-
ing can spoil the whole translation. In the baseline
output, the translation of ??????????
?? ?? ? ? ?? (an interlayer insulation film
12 that is a first insulation film) is divided into two
blocks, ?an interlayer insulating film 12? and ?a first
insulating film?. In this case, a reordering constraint
to translate ???????????????
??? as a single block can reduce incorrect reorder-
ings and improve the translation quality. However,
it is difficult to predict what should be translated as
a single block.
Therefore, how to specify ranges for reordering
constraints is a very important problem. We propose
a solution for this problem that uses the very nature
of patent documents themselves.
3 Proposed Method
In order to address the aforementioned problem, we
propose a method for specifying phrases in a source
sentence which are assumed to be translated as sin-
gle blocks using document-level context. We call
these phrases ?coherent phrases?. When translat-
ing a document, for example a patent specification,
we first extract coherent phrase candidates from the
document. Then, when translating each sentence in
the document, we set zones which cover the coher-
ent phrase candidates and restrict reorderings which
violate the zones.
3.1 Coherent phrases in patent documents
As mentioned in the previous section, specifying
coherent phrases is difficult when using only one
source sentence. However, we have observed that
document-level context can be a clue for specify-
ing coherent phrases. In a patent specification, for
example, noun phrases which indicate parts of the
invention are very important noun phrases. In pre-
vious example, ?? ? ? ?? ? ? ?? ?? ?
? ? ? ?? is a part of the invention. Since this
is not language dependent, in other words, this noun
phrase is always a part of the invention in any other
language, this noun phrase should be translated as a
single block in every language. In this way, impor-
tant phrases in patent documents are assumed to be
coherent phrases.
We therefore treat the problem of specifying co-
herent phrases as a problem of specifying important
phrases, and we use these phrases as constraints on
reorderings. The details of the proposed method are
described below.
3.2 Finding coherent phrases
We propose the following method for finding co-
herent phrases in patent sentences. First, we ex-
tract coherent phrase candidates from a patent docu-
ment. Next, the candidates are ranked by a criterion
which reflects the document-level context. Then,
we specify coherent phrases using the rankings. In
this method, using document-level context is criti-
cally important because we cannot rank the candi-
dates without it.
435
3.2.1 Extracting coherent phrase candidates
Coherent phrase candidates are extracted from a
context document, a document that contains a source
sentence. We extract all noun phrases as co-
herent phrase candidates since most noun phrases
can be translated as single blocks in other lan-
guages (Koehn and Knight, 2003). These noun
phrases include nested noun phrases.
3.2.2 Ranking with C-value
The candidates which have been extracted are nested
and have different lengths. A naive method can-
not rank these candidates properly. For example,
ranking by frequency cannot pick up an important
phrase which has a long length, yet, ranking by
length may give a long but unimportant phrase a
high rank. In order to select the appropriate coher-
ent phrases, measurements which give high rank to
phrases with high termhood are needed. As one such
measurement, we use C-value (Frantzi and Anani-
adou, 1996).
C-value is a measurement of automatic term
recognition and is suitable for extracting important
phrases from nested candidates. The C-value of a
phrase p is expressed in the following equation:
C-value(p)=
{
(l(p)?1)n(p) (c(p)=0)
(l(p)?1)
(
n(p)? t(p)c(p)
)
(c(p)>0)
where
l(p) is the length of a phrase p,
n(p) is the frequency of p in a document,
t(p) is the total frequency of phrases which contain
p as a subphrase,
c(p) is the number of those phrases.
Since phrases which have a large C-value fre-
quently occur in a context document, these phrases
are considered to be a significant unit, i.e., a part of
the invention, and to be coherent phrases.
3.2.3 Specifying coherent phrases
Given a source sentence, we find coherent phrase
candidates in the sentence in order to set zones for
reordering constraints. If a coherent phrase candi-
date is found in the source sentence, the phrase is re-
garded a coherent phrase and annotated with a zone
tag, which will be mentioned in the next section.
We check the coherent phrase candidates in the sen-
tence in descending C-value order, and stop when
the C-value goes below a certain threshold. Nested
zones are allowed, unless their zones conflict with
pre-existing zones. We then give the zone-tagged
sentence, an example is shown in Table 1, as a de-
coder input.
3.3 Decoding with reordering constraints
In decoding, reorderings which violate zones, such
as the baseline output in Table 1, are restricted and
we get a more appropriate translation, such as the
proposed output in Table 1.
We use the Moses decoder (Koehn et al, 2007;
Koehn and Haddow, 2009), which can specify re-
ordering constraints using <zone> and </zone> tags.
Moses restricts reorderings which violate zones and
translates zones as single blocks.
4 Experiments
In order to evaluate the performance of the proposed
method, we conducted Japanese-English (J-E) and
English-Japanese (E-J) translation experiments us-
ing the NTCIR-8 patent translation task dataset (Fu-
jii et al, 2010). This dataset contains a training set of
3 million sentence pairs, a development set of 2,000
sentence pairs, and a test set of 1,251 (J-E) and 1,119
(E-J) sentence pairs. Moreover, this dataset contains
the patent specifications from which sentence pairs
are extracted. We used these patent specifications as
context documents.
4.1 Baseline
We usedMoses as a baseline system, with all the set-
tings except distortion limit (dl) at the default. The
distortion limit is a maximum distance of reorder-
ing. It is known that an appropriate distortion-limit
can improve translation quality and decoding speed.
Therefore, we examined the effect of a distortion-
limit. In experiments, we compared dl = 6, 10, 20,
30, 40, and ?1 (unlimited). The feature weights
were optimized to maximize BLEU score by MERT
(Och, 2003) using the development set.
4.2 Compared methods
We compared two methods, the method of specify-
ing reordering constraints with a context document
436
w/o Context in ( this case ) , ( the leading end ) 15f of ( the segment operating body ) ( ( 15 swings ) in
( a direction opposite ) ) to ( the a arrow direction ) .
w/ Context in ( this case ) , ( ( the leading end ) 15f ) of ( ( ( the segment ) operating body ) 15 )
swings in a direction opposite to ( the a arrow direction ) .
Table 3: An example of the zone-tagged source sentence. <zone> and </zone> are replaced by ?(? and ?)?.
J?E E?J
System dl BLEU Time BLEU Time
Baseline
6 27.83 4.8 35.39 3.5
10 30.15 6.9 38.14 4.9
20 30.65 11.9 38.39 8.5
30 30.72 16.0 38.32 11.5
40 29.96 19.6 38.42 13.9
?1 30.35 28.7 37.80 18.4
w/o Context ?1 30.01 8.7 38.96 5.9
w/ Context ?1 31.55 12.0 39.21 8.0
Table 2: BLEU score (%) and average decoding time
(sec/sentence) in J-E/E-J translation.
(w/ Context) and the method of specifying reorder-
ing constraints without a context document (w/o
Context). In both methods, the feature weights used
in decoding are the same value as those for the base-
line (dl = ?1).
4.2.1 Proposed method (w/ Context)
In the proposed method, reordering constraints were
defined with a context document. For J-E transla-
tion, we used the CaboCha parser (Kudo and Mat-
sumoto, 2002) to analyze the context document. As
coherent phrase candidates, we extracted all sub-
trees whose heads are noun. For E-J translation, we
used the Charniak parser (Charniak, 2000) and ex-
tracted all noun phrases, labeled ?NP?, as coherent
phrase candidates. The parsers are used only when
extracting coherent phrase candidates. When speci-
fying zones for each source sentence, strings which
match the coherent phrase candidates are defined to
be zones. Therefore, the proposed method is robust
against parsing errors. We tried various thresholds
of the C-value and selected the value that yielded
the highest BLEU score for the development set.
4.2.2 w/o Context
In this method, reordering constraints were defined
without a context document. For J-E translation,
we converted the dependency trees of source sen-
tences processed by the CaboCha parser into brack-
eted trees and used these as reordering constraints.
For E-J translation, we used all of the noun phrases
detected by the Charniak parser as reordering con-
straints.
4.3 Results and Discussions
The experiment results are shown in Table 2. For
evaluation, we used the case-insensitive BLEU met-
ric (Papineni et al, 2002) with a single reference.
In both directions, our proposed method yielded
the highest BLEU scores. The absolute improve-
ment over the baseline (dl = ?1) was 1.20% in J-E
translation and 1.41% in E-J translation. Accord-
ing to the bootstrap resampling test (Koehn, 2004),
the improvement over the baseline was statistically
significant (p<0.01) in both directions. When com-
pared to the method without context, the absolute
improvement was 1.54% in J-E and 0.25% in E-J.
The improvement over the baseline was statistically
significant (p < 0.01) in J-E and almost significant
(p < 0.1) in E-J. These results show that the pro-
posed method using document-level context is effec-
tive in specifying reordering constraints.
Moreover, as shown in Table 3, although zone
setting without context is failed if source sen-
tences have parsing errors, the proposed method can
set zones appropriately using document-level con-
text. The Charniak parser tends to make errors on
noun phrases with ID numbers. This shows that
document-level context can possibly improve pars-
ing quality.
As for the distortion limit, while an appropriate
distortion-limit, 30 for J-E and 40 for E-J, improved
the translation quality, the gains from the proposed
method were significantly better than the gains from
the distortion limit. In general, imposing strong
constraints causes fast decoding but low translation
quality. However, the proposed method improves
the translation quality and speed by imposing appro-
priate constraints.
437
5 Conclusion
In this paper, we proposed a method for imposing
reordering constraints using document-level context.
In the proposed method, coherent phrase candidates
are extracted from a context document in advance.
Given a source sentence, zones which cover the co-
herent phrase candidates are defined. Then, in de-
coding, reorderings which violate the zones are re-
stricted. Since reordering constraints reduce incor-
rect reorderings, the translation quality and speed
can be improved. The experiment results for the
NTCIR-8 patent translation tasks show a significant
improvement of 1.20% BLEU points for J-E trans-
lation and 1.41% BLEU points for E-J translation.
We think that the proposed method is indepen-
dent of language pair and domains. In the future,
we want to apply our proposed method to other lan-
guage pairs and domains.
References
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132?139.
Colin Cherry. 2008. Cohesive Phrase-Based Decoding
for Statistical Machine Translation. In Proceedings of
ACL-08: HLT, pages 72?80.
Katerina T. Frantzi and Sophia Ananiadou. 1996. Ex-
tracting Nested Collocations. In Proceedings of COL-
ING 1996, pages 41?46.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-
hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and
Sayori Shimohata. 2010. Overview of the Patent
Translation Task at the NTCIR-8 Workshop. In Pro-
ceedings of NTCIR-8 Workshop Meeting, pages 371?
376.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A Simple Re-
ordering Rule for SOV Languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 244?251.
Jason Katz-Brown and Michael Collins. 2008. Syntac-
tic Reordering in Preprocessing for Japanese?English
Translation: MIT System Description for NTCIR-7
Patent Translation Task. In Proceedings of NTCIR-7
Workshop Meeting, pages 409?414.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
Submission to all Tracks of the WMT 2009 Shared
Task with Reordering and Speed Improvements to
Moses. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation, pages 160?164.
Philipp Koehn and Kevin Knight. 2003. Feature-Rich
Statistical Translation of Noun Phrases. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 177?180.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. In Pro-
ceedings of CoNLL-2002, pages 63?69.
Yuval Marton and Philip Resnik. 2008. Soft Syntac-
tic Constraints for Hierarchical Phrased-Based Trans-
lation. In Proceedings of ACL-08: HLT, pages 1003?
1011.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsutomu
Hirao, and Masaaki Nagata. 2010. Divide and Trans-
late: Improving Long Distance Reordering in Statisti-
cal Machine Translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, pages 418?427.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing Translation Boundaries for Phrase-Based Decod-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
136?144.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro
Sumita. 2008. Imposing Constraints from the Source
Tree on ITG Constraints for SMT. In Proceedings
of the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages 1?
9.
438
