Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 449?456,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Direct Word Sense Matching for Lexical Substitution
Ido Dagan1, Oren Glickman1, Alfio Gliozzo2, Efrat Marmorshtein1, Carlo Strapparava2
1Department of Computer Science, Bar Ilan University, Ramat Gan, 52900, Israel
2ITC-Irst, via Sommarive, I-38050, Trento, Italy
Abstract
This paper investigates conceptually and
empirically the novel sense matching task,
which requires to recognize whether the
senses of two synonymous words match in
context. We suggest direct approaches to
the problem, which avoid the intermediate
step of explicit word sense disambigua-
tion, and demonstrate their appealing ad-
vantages and stimulating potential for fu-
ture research.
1 Introduction
In many language processing settings it is needed
to recognize that a given word or term may be sub-
stituted by a synonymous one. In a typical in-
formation seeking scenario, an information need
is specified by some given source words. When
looking for texts that match the specified need the
source words might be substituted with synony-
mous target words. For example, given the source
word ?weapon? a system may substitute it with the
target synonym ?arm?.
This scenario, which is generally referred here
as lexical substitution, is a common technique
for increasing recall in Natural Language Process-
ing (NLP) applications. In Information Retrieval
(IR) and Question Answering (QA) it is typically
termed query/question expansion (Moldovan and
Mihalcea, 2000; Negri, 2004). Lexical Substi-
tution is also commonly applied to identify syn-
onyms in text summarization, for paraphrasing in
text generation, or is integrated into the features of
supervised tasks such as Text Categorization and
Information Extraction. Naturally, lexical substi-
tution is a very common first step in textual en-
tailment recognition, which models semantic in-
ference between a pair of texts in a generalized ap-
plication independent setting (Dagan et al, 2005).
To perform lexical substitution NLP applica-
tions typically utilize a knowledge source of syn-
onymous word pairs. The most commonly used
resource for lexical substitution is the manually
constructed WordNet (Fellbaum, 1998). Another
option is to use statistical word similarities, such
as in the database constructed by Dekang Lin (Lin,
1998). We generically refer to such resources as
substitution lexicons.
When using a substitution lexicon it is assumed
that there are some contexts in which the given
synonymous words share the same meaning. Yet,
due to polysemy, it is needed to verify that the
senses of the two words do indeed match in a given
context. For example, there are contexts in which
the source word ?weapon? may be substituted by
the target word ?arm?; however one should recog-
nize that ?arm? has a different sense than ?weapon?
in sentences such as ?repetitive movements could
cause injuries to hands, wrists and arms.?
A commonly proposed approach to address
sense matching in lexical substitution is applying
Word Sense Disambiguation (WSD) to identify
the senses of the source and target words. Then,
substitution is applied only if the words have the
same sense (or synset, in WordNet terminology).
In settings in which the source is given as a sin-
gle term without context, sense disambiguation
is performed only for the target word; substitu-
tion is then applied only if the target word?s sense
matches at least one of the possible senses of the
source word.
One might observe that such application of WSD
addresses the task at hand in a somewhat indi-
rect manner. In fact, lexical substitution only re-
quires knowing that the source and target senses
449
do match, but it does not require that the match-
ing senses will be explicitly identified. Selecting
explicitly the right sense in context, which is then
followed by verifying the desired matching, might
be solving a harder intermediate problem than re-
quired. Instead, we can define the sense match-
ing problem directly as a binary classification task
for a pair of synonymous source and target words.
This task requires to decide whether the senses of
the two words do or do not match in a given con-
text (but it does not require to identify explicitly
the identity of the matching senses).
A highly related task was proposed in (Mc-
Carthy, 2002). McCarthy?s proposal was to ask
systems to suggest possible ?semantically similar
replacements? of a target word in context, where
alternative replacements should be grouped to-
gether. While this task is somewhat more com-
plicated as an evaluation setting than our binary
recognition task, it was motivated by similar ob-
servations and applied goals. From another per-
spective, sense matching may be viewed as a lex-
ical sub-case of the general textual entailment
recognition setting, where we need to recognize
whether the meaning of the target word ?entails?
the meaning of the source word in a given context.
This paper provides a first investigation of the
sense matching problem. To allow comparison
with the classical WSD setting we derived an
evaluation dataset for the new problem from the
Senseval-3 English lexical sample dataset (Mihal-
cea and Edmonds, 2004). We then evaluated alter-
native supervised and unsupervised methods that
perform sense matching either indirectly or di-
rectly (i.e. with or without the intermediate sense
identification step). Our findings suggest that in
the supervised setting the results of the direct and
indirect approaches are comparable. However, ad-
dressing directly the binary classification task has
practical advantages and can yield high precision
values, as desired in precision-oriented applica-
tions such as IR and QA.
More importantly, direct sense matching sets
the ground for implicit unsupervised approaches
that may utilize practically unlimited volumes
of unlabeled training data. Furthermore, such
approaches circumvent the sisyphean need for
specifying explicitly a set of stipulated senses.
We present an initial implementation of such an
approach using a one-class classifier, which is
trained on unlabeled occurrences of the source
word and applied to occurrences of the target
word. Our current results outperform the unsuper-
vised baseline and put forth a whole new direction
for future research.
2 WSD and Lexical Expansion
Despite certain initial skepticism about the useful-
ness of WSD in practical tasks (Voorhees, 1993;
Sanderson, 1994), there is some evidence that
WSD can improve performance in typical NLP
tasks such as IR and QA. For example, (Shu?tze
and Pederson, 1995) gives clear indication of the
potential for WSD to improve the precision of an IR
system. They tested the use of WSD on a standard
IR test collection (TREC-1B), improving precision
by more than 4%.
The use of WSD has produced successful exper-
iments for query expansion techniques. In partic-
ular, some attempts exploited WordNet to enrich
queries with semantically-related terms. For in-
stance, (Voorhees, 1994) manually expanded 50
queries over the TREC-1 collection using syn-
onymy and other WordNet relations. She found
that the expansion was useful with short and in-
complete queries, leaving the task of proper auto-
matic expansion as an open problem.
(Gonzalo et al, 1998) demonstrates an incre-
ment in performance over an IR test collection us-
ing the sense data contained in SemCor over a
purely term based model. In practice, they ex-
perimented searching SemCor with disambiguated
and expanded queries. Their work shows that
a WSD system, even if not performing perfectly,
combined with synonymy enrichment increases
retrieval performance.
(Moldovan and Mihalcea, 2000) introduces the
idea of using WordNet to extend Web searches
based on semantic similarity. Their results showed
that WSD-based query expansion actually im-
proves retrieval performance in a Web scenario.
Recently (Negri, 2004) proposed a sense-based
relevance feedback scheme for query enrichment
in a QA scenario (TREC-2003 and ACQUAINT),
demonstrating improvement in retrieval perfor-
mance.
While all these works clearly show the potential
usefulness of WSD in practical tasks, nonetheless
they do not necessarily justify the efforts for refin-
ing fine-grained sense repositories and for build-
ing large sense-tagged corpora. We suggest that
the sense matching task, as presented in the intro-
450
duction, may relieve major drawbacks of applying
WSD in practical scenarios.
3 Problem Setting and Dataset
To investigate the direct sense matching problem
it is necessary to obtain an appropriate dataset of
examples for this binary classification task, along
with gold standard annotation. While there is
no such standard (application independent) dataset
available it is possible to derive it automatically
from existing WSD evaluation datasets, as de-
scribed below. This methodology also allows
comparing direct approaches for sense matching
with classical indirect approaches, which apply an
intermediate step of identifying the most likely
WordNet sense.
We derived our dataset from the Senseval-3 En-
glish lexical sample dataset (Mihalcea and Ed-
monds, 2004), taking all 25 nouns, adjectives and
adverbs in this sample. Verbs were excluded since
their sense annotation in Senseval-3 is not based
on WordNet senses. The Senseval dataset includes
a set of example occurrences in context for each
word, split to training and test sets, where each ex-
ample is manually annotated with the correspond-
ing WordNet synset.
For the sense matching setting we need exam-
ples of pairs of source-target synonymous words,
where at least one of these words should occur in
a given context. Following an applicative moti-
vation, we mimic an IR setting in which a sin-
gle source word query is expanded (substituted)
by a synonymous target word. Then, it is needed
to identify contexts in which the target word ap-
pears in a sense that matches the source word. Ac-
cordingly, we considered each of the 25 words in
the Senseval sample as a target word for the sense
matching task. Next, we had to pick for each target
word a corresponding synonym to play the role of
the source word. This was done by creating a list
of all WordNet synonyms of the target word, under
all its possible senses, and picking randomly one
of the synonyms as the source word. For example,
the word ?disc? is one of the words in the Sense-
val lexical sample. For this target word the syn-
onym ?record? was picked, which matches ?disc?
in its musical sense. Overall, 59% of all possible
synsets of our target words included an additional
synonym, which could play the role of the source
word (that is, 41% of the synsets consisted of the
target word only). Similarly, 62% of the test exam-
ples of the target words were annotated by a synset
that included an additional synonym.
While creating source-target synonym pairs it
was evident that many WordNet synonyms corre-
spond to very infrequent senses or word usages,
such as the WordNet synonyms germ and source.
Such source synonyms are useless for evaluat-
ing sense matching with the target word since the
senses of the two words would rarely match in per-
ceivable contexts. In fact, considering our motiva-
tion for lexical substitution, it is usually desired to
exclude such obscure synonym pairs from substi-
tution lexicons in practical applications, since they
would mostly introduce noise to the system. To
avoid this problem the list of WordNet synonyms
for each target word was filtered by a lexicogra-
pher, who excluded manually obscure synonyms
that seemed worthless in practice. The source syn-
onym for each target word was then picked ran-
domly from the filtered list. Table 1 shows the 25
source-target pairs created for our experiments. In
future work it may be possible to apply automatic
methods for filtering infrequent sense correspon-
dences in the dataset, by adopting algorithms such
as in (McCarthy et al, 2004).
Having source-target synonym pairs, a classifi-
cation instance for the sense matching task is cre-
ated from each example occurrence of the target
word in the Senseval dataset. A classification in-
stance is thus defined by a pair of source and target
words and a given occurrence of the target word in
context. The instance should be classified as pos-
itive if the sense of the target word in the given
context matches one of the possible senses of the
source word, and as negative otherwise. Table 2
illustrates positive and negative example instances
for the source-target synonym pair ?record-disc?,
where only occurrences of ?disc? in the musical
sense are considered positive.
The gold standard annotation for the binary
sense matching task can be derived automatically
from the Senseval annotations and the correspond-
ing WordNet synsets. An example occurrence of
the target word is considered positive if the an-
notated synset for that example includes also the
source word, and Negative otherwise. Notice that
different positive examples might correspond to
different senses of the target word. This happens
when the source and target share several senses,
and hence they appear together in several synsets.
Finally, since in Senseval an example may be an-
451
source-target source-target source-target source-target source-target
statement-argument subdivision-arm atm-atmosphere hearing-audience camber-bank
level-degree deviation-difference dissimilar-different trouble-difficulty record-disc
raging-hot ikon-image crucial-important sake-interest bare-simple
opinion-judgment arrangement-organization newspaper-paper company-party substantial-solid
execution-performance design-plan protection-shelter variety-sort root-source
Table 1: Source and target pairs
sentence annotation
This is anyway a stunning disc, thanks to the playing of the Moscow Virtuosi with Spivakov. positive
He said computer networks would not be affected and copies of information should be made on
floppy discs.
negative
Before the dead soldier was placed in the ditch his personal possessions were removed, leaving
one disc on the body for identification purposes
negative
Table 2: positive and negative examples for the source-target synonym pair ?record-disc?
notated with more than one sense, it was consid-
ered positive if any of the annotated synsets for the
target word includes the source word.
Using this procedure we derived gold standard
annotations for all the examples in the Senseval-
3 training section for our 25 target words. For the
test set we took up to 40 test examples for each tar-
get word (some words had fewer test examples),
yielding 913 test examples in total, out of which
239 were positive. This test set was used to eval-
uate the sense matching methods described in the
next section.
4 Investigated Methods
As explained in the introduction, the sense match-
ing task may be addressed by two general ap-
proaches. The traditional indirect approach would
first disambiguate the target word relative to a pre-
defined set of senses, using standard WSD meth-
ods, and would then verify that the selected sense
matches the source word. On the other hand, a
direct approach would address the binary sense
matching task directly, without selecting explicitly
a concrete sense for the target word. This section
describes the alternative methods we investigated
under supervised and unsupervised settings. The
supervised methods utilize manual sense annota-
tions for the given source and target words while
unsupervised methods do not require any anno-
tated sense examples. For the indirect approach
we assume the standard WordNet sense repository
and corresponding annotations of the target words
with WordNet synsets.
4.1 Feature set and classifier
As a vehicle for investigating different classifica-
tion approaches we implemented a ?vanilla? state
of the art architecture for WSD. Following com-
mon practice in feature extraction (e.g. (Yarowsky,
1994)), and using the mxpost1 part of speech tag-
ger and WordNet?s lemmatization, the following
feature set was used: bag of word lemmas for the
context words in the preceding, current and fol-
lowing sentence; unigrams of lemmas and parts
of speech in a window of +/- three words, where
each position provides a distinct feature; and bi-
grams of lemmas in the same window. The SVM-
Light (Joachims, 1999) classifier was used in the
supervised settings with its default parameters. To
obtain a multi-class classifier we used a standard
one-vs-all approach of training a binary SVM for
each possible sense and then selecting the highest
scoring sense for a test example.
To verify that our implementation provides a
reasonable replication of state of the art WSD we
applied it to the standard Senseval-3 Lexical Sam-
ple WSD task. The obtained accuracy2 was 66.7%,
which compares reasonably with the mid-range of
systems in the Senseval-3 benchmark (Mihalcea
and Edmonds, 2004). This figure is just a few
percent lower than the (quite complicated) best
Senseval-3 system, which achieved about 73% ac-
curacy, and it is much higher than the standard
Senseval baselines. We thus regard our classifier
as a fair vehicle for comparing the alternative ap-
proaches for sense matching on equal grounds.
1ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz
2The standard classification accuracy measure equals pre-
cision and recall as defined in the Senseval terminology when
the system classifies all examples, with no abstentions.
452
4.2 Supervised Methods
4.2.1 Indirect approach
The indirect approach for sense matching fol-
lows the traditional scheme of performing WSD
for lexical substitution. First, the WSD classifier
described above was trained for the target words
of our dataset, using the Senseval-3 sense anno-
tated training data for these words. Then, the clas-
sifier was applied to the test examples of the target
words, selecting the most likely sense for each ex-
ample. Finally, an example was classified as pos-
itive if the selected synset for the target word in-
cludes the source word, and as negative otherwise.
4.2.2 Direct approach
As explained above, the direct approach ad-
dresses the binary sense matching task directly,
without selecting explicitly a sense for the target
word. In the supervised setting it is easy to ob-
tain such a binary classifier using the annotation
scheme described in Section 3. Under this scheme
an example was annotated as positive (for the bi-
nary sense matching task) if the source word is
included in the Senseval gold standard synset of
the target word. We trained the classifier using the
set of Senseval-3 training examples for each tar-
get word, considering their derived binary anno-
tations. Finally, the trained classifier was applied
to the test examples of the target words, yielding
directly a binary positive-negative classification.
4.3 Unsupervised Methods
It is well known that obtaining annotated training
examples for WSD tasks is very expensive, and
is often considered infeasible in unrestricted do-
mains. Therefore, many researchers investigated
unsupervised methods, which do not require an-
notated examples. Unsupervised approaches have
usually been investigated within Senseval using
the ?All Words? dataset, which does not include
training examples. In this paper we preferred us-
ing the same test set which was used for the super-
vised setting (created from the Senseval-3 ?Lexi-
cal Sample? dataset, as described above), in order
to enable comparison between the two settings.
Naturally, in the unsupervised setting the sense la-
bels in the training set were not utilized.
4.3.1 Indirect approach
State-of-the-art unsupervised WSD systems are
quite complex and they are not easy to be repli-
cated. Thus, we implemented the unsupervised
version of the Lesk algorithm (Lesk, 1986) as a
reference system, since it is considered a standard
simple baseline for unsupervised approaches. The
Lesk algorithm is one of the first algorithms de-
veloped for semantic disambiguation of all-words
in unrestricted text. In its original unsupervised
version, the only resource required by the algo-
rithm is a machine readable dictionary with one
definition for each possible word sense. The algo-
rithm looks for words in the sense definitions that
overlap with context words in the given sentence,
and chooses the sense that yields maximal word
overlap. We implemented a version of this algo-
rithm using WordNet sense-definitions with con-
text length of ?10 words before and after the tar-
get word.
4.3.2 The direct approach: one-class learning
The unsupervised settings for the direct method
are more problematic because most of unsuper-
vised WSD algorithms (such as the Lesk algo-
rithm) rely on dictionary definitions. For this rea-
son, standard unsupervised techniques cannot be
applied in a direct approach for sense matching, in
which the only external information is a substitu-
tion lexicon.
In this subsection we present a direct unsuper-
vised method for sense matching. It is based on
the assumption that typical contexts in which both
the source and target words appear correspond to
their matching senses. Unlabeled occurrences of
the source word can then be used to provide evi-
dence for lexical substitution because they allow
us to recognize whether the sense of the target
word matches that of the source. Our strategy is
to represent in a learning model the typical con-
texts of the source word in unlabeled training data.
Then, we exploit such model to match the contexts
of the target word, providing a decision criterion
for sense matching. In other words, we expect that
under a matching sense the target word would oc-
cur in prototypical contexts of the source word.
To implement such approach we need a learning
technique that does not rely on the availability of
negative evidence, that is, a one-class learning al-
gorithm. In general, the classification performance
of one-class approaches is usually quite poor, if
compared to supervised approaches for the same
tasks. However, in many practical settings one-
class learning is the only available solution.
For our experiments we adopted the one-class
SVM learning algorithm (Scho?lkopf et al, 2001)
453
implemented in the LIBSVM package,3 and repre-
sented the unlabeled training examples by adopt-
ing the feature set described in Subsection 4.1.
Roughly speaking, a one-class SVM estimates the
smallest hypersphere enclosing most of the train-
ing data. New test instances are then classified
positively if they lie inside the sphere, while out-
liers are regarded as negatives. The ratio between
the width of the enclosed region and the number
of misclassified training examples can be varied
by setting the parameter ? ? (0, 1). Smaller val-
ues of ? will produce larger positive regions, with
the effect of increasing recall.
The appealing advantage of adopting one-class
learning for sense matching is that it allows us to
define a very elegant learning scenario, in which it
is possible to train ?off-line? a different classifier
for each (source) word in the lexicon. Such a clas-
sifier can then be used to match the sense of any
possible target word for the source which is given
in the substitution lexicon. This is in contrast to
the direct supervised method proposed in Subsec-
tion 4.2, where a different classifier for each pair
of source - target words has to be defined.
5 Evaluation
5.1 Evaluation measures and baselines
In the lexical substitution (and expansion) set-
ting, the standard WSD metrics (Mihalcea and Ed-
monds, 2004) are not suitable, because we are in-
terested in the binary decision of whether the tar-
get word matches the sense of a given source word.
In analogy to IR, we are more interested in positive
assignments, while the opposite case (i.e. when the
two words cannot be substituted) is less interest-
ing. Accordingly, we utilize the standard defini-
tions of precision, recall and F1 typically used in
IR benchmarks. In the rest of this section we will
report micro averages for these measures on the
test set described in Section 3.
Following the Senseval methodology, we evalu-
ated two different baselines for unsupervised and
supervised methods. The random baseline, used
for the unsupervised algorithms, was obtained by
choosing either the positive or the negative class
at random resulting in P = 0.262, R = 0.5,
F1 = 0.344. The Most Frequent baseline has
been used for the supervised algorithms and is ob-
tained by assigning the positive class when the
3Freely available from www.csie.ntu.edu.tw/
/?cjlin/libsvm.
percentage of positive examples in the training set
is above 50%, resulting in P = 0.65, R = 0.41,
F1 = 0.51.
5.2 Supervised Methods
Both the indirect and the direct supervised meth-
ods presented in Subsection 4.2 have been tested
and compared to the most frequent baseline.
Indirect. For the indirect methodology we
trained the supervised WSD system for each tar-
get word on the sense-tagged training sample. As
described in Subsection 4.2, we implemented a
simple SVM-based WSD system (see Section 4.2)
and applied it to the sense-matching task. Results
are reported in Table 3. The direct strategy sur-
passes the most frequent baseline F1 score, but the
achieved precision is still below it. We note that in
this multi-class setting it is less straightforward to
tradeoff recall for precision, as all senses compete
with each other.
Direct. In the direct supervised setting, sense
matching is performed by training a binary clas-
sifier, as described in Subsection 4.2.
The advantage of adopting a binary classifica-
tion strategy is that the precision/recall tradeoff
can be tuned in a meaningful way. In SVM learn-
ing, such tuning is achieved by varying the param-
eter J , that allows us to modify the cost function
of the SVM learning algorithm. If J = 1 (default),
the weight for the positive examples is equal to the
weight for the negatives. When J > 1, negative
examples are penalized (increasing recall), while,
whenever 0 < J < 1, positive examples are penal-
ized (increasing precision). Results obtained by
varying this parameter are reported in Figure 1.
Figure 1: Direct supervised results varying J
454
Supervised P R F1 Unsupervised P R F1
Most Frequent Baseline 0.65 0.41 0.51 Random Baseline 0.26 0.50 0.34
Multiclass SVM Indirect 0.59 0.63 0.61 Lesk Indirect 0.24 0.19 0.21
Binary SVM (J = 0.5) Direct 0.80 0.26 0.39 One-Class ? = 0.3 Direct 0.26 0.72 0.39
Binary SVM (J = 1) Direct 0.76 0.46 0.57 One-Class ? = 0.5 Direct 0.29 0.56 0.38
Binary SVM (J = 2) Direct 0.68 0.53 0.60 One-Class ? = 0.7 Direct 0.28 0.36 0.32
Binary SVM (J = 3) Direct 0.69 0.55 0.61 One-Class ? = 0.9 Direct 0.23 0.10 0.14
Table 3: Classification results on the sense matching task
Adopting the standard parameter settings (i.e.
J = 1, see Table 3), the F1 of the system
is slightly lower than for the indirect approach,
while it reaches the indirect figures when J in-
creases. More importantly, reducing J allows us
to boost precision towards 100%. This feature is
of great interest for lexical substitution, particu-
larly in precision oriented applications like IR and
QA, for filtering irrelevant candidate answers or
documents.
5.3 Unsupervised methods
Indirect. To evaluate the indirect unsupervised
settings we implemented the Lesk algorithm, de-
scribed in Subsection 4.3.1, and evaluated it on
the sense matching task. The obtained figures,
reported in Table 3, are clearly below the base-
line, suggesting that simple unsupervised indirect
strategies cannot be used for this task. In fact, the
error of the first step, due to low WSD accuracy
of the unsupervised technique, is propagated in
the second step, producing poor sense matching.
Unfortunately, state-of-the-art unsupervised sys-
tems are actually not much better than Lesk on all-
words task (Mihalcea and Edmonds, 2004), dis-
couraging the use of unsupervised indirect meth-
ods for the sense matching task.
Direct. Conceptually, the most appealing solu-
tion for the sense matching task is the one-class
approach proposed for the direct method (Section
4.3.2). To perform our experiments, we trained a
different one-class SVM for each source word, us-
ing a sample of its unlabeled occurrences in the
BNC corpus as training set. To avoid huge train-
ing sets and to speed up the learning process, we
fixed the maximum number of training examples
to 10000 occurrences per word, collecting on av-
erage about 6500 occurrences per word.
For each target word in the test sample, we ap-
plied the classifier of the corresponding source
word. Results for different values of ? are reported
in Figure 2 and summarized in Table 3.
Figure 2: One-class evaluation varying ?
While the results are somewhat above the base-
line, just small improvements in precision are re-
ported, and recall is higher than the baseline for
? < 0.6. Such small improvements may suggest
that we are following a relevant direction, even
though they may not be useful yet for an applied
sense-matching setting.
Further analysis of the classification results for
each word revealed that optimal F1 values are ob-
tained by adopting different values of ? for differ-
ent words. In the optimal (in retrospect) param-
eter settings for each word, performance for the
test set is noticeably boosted, achieving P = 0.40,
R = 0.85 and F1 = 0.54. Finding a principled un-
supervised way to automatically tune the ? param-
eter is thus a promising direction for future work.
Investigating further the results per word, we
found that the correlation coefficient between the
optimal ? values and the degree of polysemy of
the corresponding source words is 0.35. More in-
terestingly, we noticed a negative correlation (r
= -0.30) between the achieved F1 and the degree
of polysemy of the word, suggesting that polyse-
mous source words provide poor training models
for sense matching. This can be explained by ob-
serving that polysemous source words can be sub-
stituted with the target words only for a strict sub-
455
set of their senses. On the other hand, our one-
class algorithm was trained on all the examples
of the source word, which include irrelevant ex-
amples that yield noisy training sets. A possible
solution may be obtained using clustering-based
word sense discrimination methods (Pedersen and
Bruce, 1997; Schu?tze, 1998), in order to train dif-
ferent one-class models from different sense clus-
ters. Overall, the analysis suggests that future re-
search may obtain better binary classifiers based
just on unlabeled examples of the source word.
6 Conclusion
This paper investigated the sense matching task,
which captures directly the polysemy problem in
lexical substitution. We proposed a direct ap-
proach for the task, suggesting the advantages of
natural control of precision/recall tradeoff, avoid-
ing the need in an explicitly defined sense reposi-
tory, and, most appealing, the potential for novel
completely unsupervised learning schemes. We
speculate that there is a great potential for such
approaches, and suggest that sense matching may
become an appealing problem and possible track
in lexical semantic evaluations.
Acknowledgments
This work was partly developed under the collab-
oration ITC-irst/University of Haifa.
References
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
C. Fellbaum. 1998. WordNet. An Electronic Lexical
Database. MIT Press.
J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigarran.
1998. Indexing with wordnet synsets can improve
text retrieval. In ACL, Montreal, Canada.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in kernel methods: support vector
learning, chapter 11, pages 169 ? 184. MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
ACM-SIGDOC Conference, Toronto, Canada.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th
international conference on Computational linguis-
tics, pages 768?774, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Automatic identification of infre-
quent word senses. In Proceedings of COLING,
pages 1220?1226.
Diana McCarthy. 2002. Lexical substitution as a task
for wsd evaluation. In Proceedings of the ACL-
02 workshop on Word sense disambiguation, pages
109?115, Morristown, NJ, USA. Association for
Computational Linguistics.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3: Third International Workshop
on the Evaluation of Systems for the Semantic Anal-
ysis of Text, Barcelona, Spain, July.
D. Moldovan and R. Mihalcea. 2000. Using wordnet
and lexical operators to improve internet searches.
IEEE Internet Computing, 4(1):34?43, January.
M. Negri. 2004. Sense-based blind relevance feedback
for question answering. In SIGIR-2004 Workshop
on Information Retrieval For Question Answering
(IR4QA), Sheffield, UK, July.
T. Pedersen and R. Bruce. 1997. Distinguishing word
sense in untagged text. In EMNLP, Providence, Au-
gust.
M. Sanderson. 1994. Word sense disambiguation and
information retrieval. In SIGIR, Dublin, Ireland,
June.
B. Scho?lkopf, J. Platt, J. Shawe-Taylor, A. J. Smola,
and R. C. Williamson. 2001. Estimating the support
of a high-dimensional distribution. Neural Compu-
tation, 13:1443?1471.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1).
H. Shu?tze and J. Pederson. 1995. Information retrieval
based on word senses. In Proceedings of the 4th
Annual Symposium on Document Analysis and In-
formation Retrieval, Las Vegas.
E. Voorhees. 1993. Using WordNet to disambiguate
word sense for text retrieval. In SIGIR, Pittsburgh,
PA.
E. Voorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 17th ACM
SIGIR Conference, Dublin, Ireland, June.
D. Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration
in spanish and french. In ACL, pages 88?95, Las
Cruces, New Mexico.
456
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 43?48,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Probabilistic Setting and Lexical Cooccurrence Model  
for Textual Entailment  
Oren Glickman and Ido Dagan 
Department of Computer Science 
 Bar Ilan University 
{glikmao,Dagan}@cs.biu.ac.il 
 
Abstract 
This paper proposes a general probabilis-
tic setting that formalizes a probabilistic 
notion of textual entailment.  We further 
describe a particular preliminary model 
for lexical-level entailment, based on 
document cooccurrence probabilities, 
which follows the general setting. The 
model was evaluated on two application 
independent datasets, suggesting the rele-
vance of such probabilistic approaches for 
entailment modeling.  
1 Introduction 
Many Natural Language Processing (NLP) 
applications need to recognize when the meaning 
of one text can be expressed by, or inferred from, 
another text. Information Retrieval (IR), Question 
Answering (QA), Information Extraction (IE), text 
summarization and Machine Translation (MT) 
evaluation are examples of applications that need 
to assess this semantic relationship between text 
segments. The Textual Entailment Recognition 
task (Dagan et al, 2005) has recently been pro-
posed as an application independent framework for 
modeling such inferences.  
Within the textual entailment framework, a text 
t is said to entail a textual hypothesis h if the truth 
of h can be inferred from t. Textual entailment cap-
tures generically a broad range of inferences that 
are relevant for multiple applications. For example, 
a QA system has to identify texts that entail a hy-
pothesized answer. Given the question "Does John 
Speak French?", a text that includes the sentence 
"John is a fluent French speaker" entails the sug-
gested answer "John speaks French." In many 
cases, though, entailment inference is uncertain 
and has a probabilistic nature. For example, a text 
that includes the sentence "John was born in 
France." does not strictly entail the above answer. 
Yet, it is clear that it does increase substantially the 
likelihood that the hypothesized answer is true.  
The uncertain nature of textual entailment calls 
for its explicit modeling in probabilistic terms. We 
therefore propose a general generative probabilistic 
setting for textual entailment, which allows a clear 
formulation of concrete probabilistic models for 
this task. We suggest that the proposed setting may 
provide a unifying framework for modeling uncer-
tain semantic inferences from texts.   
An important sub task of textual entailment, 
which we term lexical entailment, is recognizing if 
the lexical concepts in a hypothesis h are entailed 
from a given text t, even if the relations which hold 
between these concepts may not be entailed from t. 
This is typically a necessary, but not sufficient, 
condition for textual entailment. For example, in 
order to infer from a text the hypothesis "Chrysler 
stock rose," it is a necessary that the concepts of 
Chrysler, stock and rise must be inferred from the 
text. However, for proper entailment it is further 
needed that the right relations hold between these 
concepts. In this paper we demonstrate the rele-
vance of the general probabilistic setting for mod-
eling lexical entailment, by devising a preliminary 
model that is based on document co-occurrence 
probabilities in a bag of words representation.  
Although our proposed lexical system is rela-
tively simple, as it doesn?t rely on syntactic or 
other deeper analysis, it nevertheless was among 
the top ranking systems in the first Recognising 
Textual Entailment (RTE) Challenge (Glickman et 
al., 2005a). The model was evaluated also on an 
additional dataset, where it compares favorably 
with a state-of-the-art heuristic score. These results 
suggest that the proposed probabilistic framework 
is a promising basis for devising improved models 
that incorporate richer information.  
43
2 Probabilistic Textual Entailment 
2.1 Motivation 
A common definition of entailment in formal se-
mantics (Chierchia. and McConnell-Ginet, 1990) 
specifies that a text t entails another text h (hy-
pothesis, in our terminology) if h is true in every 
circumstance (possible world) in which t is true. 
For example, in examples 1 and 3 from Table 1 
we?d assume humans to agree that the hypothesis 
is necessarily true in any circumstance for which 
the text is true. In such intuitive cases, textual en-
tailment may be perceived as being certain, or, tak-
ing a probabilistic perspective, as having a 
probability of 1. 
In many other cases, though, entailment infer-
ence is uncertain and has a probabilistic nature. In 
example 2, the text doesn?t contain enough infor-
mation to infer the hypothesis? truth. And in exam-
ple 4, the meaning of the word hometown is 
ambiguous and therefore one cannot infer for cer-
tain that the hypothesis is true. In both of these 
cases there are conceivable circumstances for 
which the text is true and the hypothesis false. Yet, 
it is clear that in both examples, the text does in-
crease substantially the likelihood of the correct-
ness of the hypothesis, which naturally extends the 
classical notion of certain entailment. Given the 
text, we expect the probability that the hypothesis 
is indeed true to be relatively high, and signifi-
cantly higher than its probability of being true 
without reading the text. Aiming to model applica-
tion needs, we suggest that the probability of the 
hypothesis being true given the text reflects an ap-
propriate confidence score for the correctness of a 
particular textual inference. In the next sub-
sections we propose a concrete probabilistic setting 
that formalizes the notion of truth probabilities in 
such cases.  
2.2 A Probabilistic Setting 
Let T denote a space of possible texts, and t?T a 
specific text. Let H denote the set of all possible 
hypotheses. A hypothesis h?H is a propositional 
statement which can be assigned a truth value. For 
now it is assumed that h is represented as a textual 
statement, but in principle it could also be ex-
pressed as a formula in some propositional lan-
guage.  
A semantic state of affairs is captured by a 
mapping from H to {0=false, 1=true}, denoted by 
w: H ? {0, 1} (called here possible world, follow-
ing common terminology). A possible world w 
represents a concrete set of truth value assignments 
for all possible propositions. Accordingly, W de-
notes the set of all possible worlds. 
2.2.1 A Generative Model 
We assume a probabilistic generative model for 
texts and possible worlds. In particular, we assume 
that texts are generated along with a concrete state 
of affairs, represented by a possible world. Thus, 
whenever the source generates a text t, it generates 
also corresponding hidden truth assignments that 
constitute a possible world w. 
The probability distribution of the source, over 
all possible texts and truth assignments T ? W, is 
assumed to reflect inferences that are based on the 
generated texts. That is, we assume that the distri-
bution of truth assignments is not bound to reflect 
the state of affairs in a particular "real" world, but 
only the inferences about propositions' truth which 
are related to the text. In particular, the probability 
for generating a true hypothesis h that is not related 
at all to the corresponding text is determined by 
some prior probability P(h). For example, h="Paris 
is the capital of France" might have a prior smaller 
than 1 and might well be false when the generated 
text is not related at all to Paris or France. In fact, 
we may as well assume that the notion of textual 
entailment is relevant only for hypotheses for 
which P(h) < 1, as otherwise (i.e. for tautologies) 
there is no need to consider texts that would sup-
port h's truth. On the other hand, we assume that 
the probability of h being true (generated within w) 
would be higher than the prior when the corre-
sponding t does contribute information that sup-
ports h's truth. 
example text hypothesis 
1 John is a French Speaker 
2 John was born in France John speaks French  
3 Harry's birthplace is Iowa 
4 Harry is returning to his Iowa hometown  Harry was born in Iowa 
Table 1: example sentence pairs  
 
44
We define two types of events over the prob-
ability space for T ? W: 
I) For a hypothesis h, we denote as Trh the random 
variable whose value is the truth value assigned to 
h in a given world. Correspondingly, Trh=1 is the 
event of h being assigned a truth value of 1 (true). 
II) For a text t, we use t itself to denote also the 
event that the generated text is t (as usual, it is 
clear from the context whether t denotes the text or 
the corresponding event).  
2.3 Probabilistic textual entailment 
definition 
We say that a text t probabilistically entails a hy-
pothesis h (denoted as t ? h) if t increases the like-
lihood of h being true, that is, if P(Trh = 1| t) > 
P(Trh  = 1) or equivalently if the pointwise mutual 
information, I(Trh=1,t), is greater then 0. Once 
knowing that t?h, P(Trh=1| t) serves as a probabil-
istic confidence value for h being true given t. 
Application settings would typically require 
that P(Trh = 1| t) obtains a high value; otherwise, 
the text would not be considered sufficiently rele-
vant to support h's truth (e.g. a supporting text in 
QA or IE should entail the extracted information 
with high confidence). Finally, we ignore here the 
case in which t contributes negative information 
about h, leaving this relevant case for further in-
vestigation. 
2.4 Model Properties 
It is interesting to notice the following properties 
and implications of our model: 
A) Textual entailment is defined as a relationship 
between texts and propositions whose representa-
tion is typically based on text as well, unlike logi-
cal entailment which is a relationship between 
propositions only. Accordingly, textual entail-
ment confidence is conditioned on the actual gen-
eration of a text, rather than its truth. For 
illustration, we would expect that the text ?His 
father was born in Italy? would logically entail 
the hypothesis ?He was born in Italy? with high 
probability ? since most people who?s father was 
born in Italy were also born there. However we 
expect that the text would actually not probabilis-
tically textually entail the hypothesis since most 
people for whom it is specifically reported that 
their father was born in Italy were not born in 
Italy.1 
B) We assign probabilities to propositions (hy-
potheses) in a similar manner to certain probabil-
istic reasoning approaches (e.g. Bacchus, 1990; 
Halpern, 1990). However, we also assume a gen-
erative model of text, similar to probabilistic lan-
guage and machine translation models, which 
supplies the needed conditional probability distri-
bution. Furthermore, since our conditioning is on 
texts rather than propositions we do not assume 
any specific logic representation language for text 
meaning, and only assume that textual hypotheses 
can be assigned truth values.     
C) Our framework does not distinguish between 
textual entailment inferences that are based on 
knowledge of language semantics (such as mur-
dering ? killing) and inferences based on domain 
or world knowledge (such as live in Paris ? live 
in France). Both are needed in applications and it 
is not clear at this stage where and how to put 
such a borderline. 
D) An important feature of the proposed frame-
work is that for a given text many hypotheses are 
likely to be true. Consequently, for a given text t 
and hypothesis h, ?hP(Trh=1|t) does not sum to 1.  
This differs from typical generative settings for 
IR and MT (Ponte and croft, 1998; Brown et al, 
1993), where all conditioned events are disjoint 
by construction.  In the proposed model, it is 
rather the case that P(Trh=1|t) + P(Trh=0|t) = 1, as 
we are interested in the probability that a single 
particular hypothesis is true (or false). 
E) An implemented model that corresponds to our 
probabilistic setting is expected to produce an 
estimate for P(Trh = 1| t). This estimate is ex-
pected to reflect all probabilistic aspects involved 
in the modeling, including inherent uncertainty of 
the entailment inference itself (as in example 2 of 
Table 1), possible uncertainty  regarding the cor-
rect disambiguation of the text (example 4), as 
well as probabilistic estimates that stem from the 
particular model structure.  
3 A Lexical Entailment Model 
We suggest that the proposed setting above pro-
vides the necessary grounding for probabilistic 
                                                          
1
 This seems to be the case, when analyzing the results of en-
tering the above text in a web search engine.     
45
modeling of textual entailment. Since modeling the 
full extent of the textual entailment problem is 
clearly a long term research goal, in this paper we 
rather focus on the above mentioned sub-task of 
lexical entailment - identifying when the lexical 
elements of a textual hypothesis h are inferred 
from a given text t.  
To model lexical entailment we first assume that 
the meanings of the individual content words in a 
hypothesis can be assigned truth values. One pos-
sible interpretation for such truth values is that 
lexical concepts are assigned existential meanings. 
For example, for a given text t, Trbook=1 if it can be 
inferred in t?s state of affairs that a book exists. 
Our model does not depend on any such particular 
interpretation, though, as we only assume that truth 
values can be assigned for lexical items but do not 
explicitly annotate or evaluate this sub-task.  
Given this setting, a hypothesis is assumed to be 
true if and only if all its lexical components are 
true as well. This captures our target perspective of 
lexical entailment, while not modeling here other 
entailment aspects. When estimating the entailment 
probability we assume that the truth probability of 
a term u in a hypothesis h is independent of the 
truth of the other terms in h, obtaining:  
P(Trh = 1| t) = ?u?hP(Tru=1|t) 
P(Trh = 1) = ?u?hP(Tru=1) (1) 
In order to estimate P(Tru=1|v1, ?, vn) for a 
given word u and text t={v1, ?, vn}, we further 
assume that the majority of the probability mass 
comes from a specific entailing word in t: 
)|1(max)|1( vutvu TTrtTr =?==? ?  (2) 
where Tv denotes the event that a generated text 
contains the word v. This corresponds to expecting 
that each word in h will be entailed from a specific 
word in t (rather than from the accumulative con-
text of t as a whole2). Alternatively, one can view 
(2) as inducing an alignment between terms in the 
h to the terms in the t, somewhat similar to align-
ment models in statistical MT (Brown et al, 1993).  
Thus we propose estimating the entailment 
probability based on lexical entailment probabili-
ties from (1) and (2) as follows: 
?? ? =?==? hu vutvh TTrtTr )|1(max)|1(  (3) 
                                                          
2
 Such a model is proposed in (Glickman et al, 2005b)  
3.1 Estimating Lexical Entailment 
Probabilities   
We perform unsupervised empirical estimation of 
the lexical entailment probabilities, P(Tru=1|Tv), 
based on word co-occurrence frequencies in a cor-
pus. Following our proposed probabilistic model 
(cf. Section  2.2.1), we assume that the domain 
corpus is a sample generated by a language source. 
Each document represents a generated text and a 
(hidden) possible world. Given that the possible 
world of the text is not observed we do not know 
the truth assignments of hypotheses for the ob-
served texts. We therefore further make the sim-
plest assumption that all hypotheses stated 
verbatim in a document are true and all others are 
false and hence P(Tru=1|Tv) = P(Tu |Tv). This simple 
co-occurrence probability, which we denote as 
lexical entailment probability ? lep(u,v), is easily 
estimated from the corpus based on maximum like-
lihood counts:  
v
vu
vu
n
n
TTrvulep ,)|1(),( ?=?=
 
(4) 
where nv is the number of documents containing 
word v and nu,v is the number of documents con-
taining both u and v.  
Given our definition of the textual entailment 
relationship (cf. Section  2.3) for a given word v we 
only consider for entailment words u for which 
P(Tru=1|Tv)> P(Tru=1) or based on our estimations, 
for which nu,v/nu > nv/N (N is total number of 
documents in the corpus).  
We denote as tep the textual entailment probability 
estimation as derived from (3) and (4) above: 
? ? ?= hu tv vulephttep ),(max),(  (5) 
3.2 Baseline model 
As a baseline model for comparison, we use a 
score developed within the context of text summa-
rization. (Monz and de Rijke, 2001) propose mod-
eling the directional entailment between two texts 
t1, t2 via the following score:  
?
?
?
??=
2
21
)(
)(
),( )(21
tw
ttw
widf
widf
ttentscore
 (6) 
where idf(w) = log(N/nw), N is total number of 
documents in corpus and nw is number of docu-
46
ments containing word w.  A practically equivalent 
measure was independently proposed in the con-
text of QA by (Saggion et al, 2004)3. This baseline 
measure captures word overlap, considering only 
words that appear in both texts and weighs them 
based on their inverse document frequency. 
4 The RTE challenge dataset 
The RTE dataset (Dagan et al, 2005) consists 
of sentence pairs annotated for entailment. Fo this 
dataset we used word cooccurrence frequencies 
obtained from a web search engine. The details of 
this experiment are described in Glickman et al, 
2005a. The resulting accuracy on the test set was 
59% and the resulting confidence weighted score 
was 0.57. Both are statistically significantly better 
than chance at the 0.01 level. The baseline model 
(6) from Section  3.2, which takes into account only 
terms appearing in both the text and hypothesis, 
achieved an accuracy of only 56%. Although our 
proposed lexical system is relatively simple, as it 
doesn?t rely on syntactic or other deeper analysis, 
it nevertheless was among the top ranking systems 
in the RTE Challenge. 
5 RCV1 dataset  
In addition to the RTE dataset we were interested 
in evaluating the model on a more representative 
set of texts and hypotheses that better corresponds 
to applicative settings. We focused on the informa-
tion seeking setting, common in applications such 
as QA and IR, in which a hypothesis is given and it 
is necessary to identify texts that entail it.  
An annotator was asked to choose 60 hypothe-
ses based on sentences from the first few docu-
ments in the Reuters Corpus Volume 1 (Rose et al, 
2002). The annotator was instructed to choose sen-
tential hypotheses such that their truth could easily 
be evaluated. We further required that the hypothe-
ses convey a reasonable information need in such a 
way that they might correspond to potential ques-
tions, semantic queries or IE relations. Table 2 
shows a few of the hypotheses.  
In order to create a set of candidate entailing 
texts for the given set of test hypotheses, we fol-
lowed the common practice of WordNet based ex-
                                                          
3
 (Saggion et al, 2004) actually proposed the above score with 
no normalizing denominator. However for a given hypothesis 
it results with the same ranking of candidate entailing texts. 
pansion (Nie and Brisebois, 1996; Yang and Chua, 
2002). Using WordNet, we expanded the hypothe-
ses? terms with morphological alternations and 
semantically related words4.  
For each hypothesis stop words were removed 
and all content words were expanded as described 
above. Boolean Search included a conjunction of 
the disjunction of the term?s expansions and was 
performed at the paragraph level over the full 
Reuters corpus, as common in IR for QA. Since we 
wanted to focus our research on semantic variabil-
ity we excluded from the result set paragraphs that 
contain all original words of the hypothesis or their 
morphological derivations. The resulting dataset 
consists of 50 hypotheses and over a million re-
trieved paragraphs (10 hypotheses had only exact 
matches). The number of paragraphs retrieved per 
hypothesis range from 1 to 400,000.5  
5.1 Evaluation 
The model?s entailment probability, tep, was com-
pared to the following two baseline models. The 
first, denoted as base, is the na?ve baseline in 
which all retrieved texts are presumed to entail the 
hypothesis with equal confidence. This baseline 
corresponds to systems which perform blind ex-
pansion with no weighting. The second baseline, 
entscore, is the entailment score (6) from  3.2.  
The top 20 best results for all methods were 
given to judges to be annotated for entailment. 
Judges were asked to annotate an example as true 
if given the text they can infer with high confi-
dence that the hypothesis is true (similar to the 
guidelines published for the RTE Challenge data-
set). Accordingly, they were instructed to annotate 
the example as false if either they believe the hy-
pothesis is false given the text or if the text is unre-
lated to the hypothesis. In total there were 1683 
text-hypothesis pairs, which were randomly di-
vided between two judges. In order to measure 
agreement, we had 200 of the pairs annotated by 
both judges, yielding a moderate agreement (a 
Kappa of 0.6). 
                                                          
4
 The following WordNet relations were used: Synonyms, see 
also, similar to, hypernyms/hyponyms, meronyms/holonyms, 
pertainyms, attribute, entailment, cause and domain 
5
 The dataset is available at:  
http://ir-srv.cs.biu.ac.il:64080/emsee05_dataset.zip 
47
5.2 Results 
 base entscore tep 
precision 0.464 0.568 0.647 
cws 0.396 0.509 0.575 
Table 2: Results 
Table 2 includes the results of macro averaging the 
precision at top-20 and the average confidence 
weighted score (cws) achieved for the 50 hypothe-
ses. Applying Wilcoxon Signed-Rank Test, our 
model performs significantly better (at the 0.01 
level) than entscore and base for both precision and 
cws. Analyzing the results showed that many of 
the mistakes were not due to wrong expansion but 
rather to a lack of a deeper analysis of the text and 
hypothesis (e.g. example 3 in Table 2). Indeed this 
is a common problem with lexical models. Incor-
porating additional linguistic levels into the prob-
abilistic entailment model, such as syntactic 
matching, co-reference resolution and word sense 
disambiguation, becomes a challenging target for 
future research. 
6 Conclusions 
This paper proposes a generative probabilistic set-
ting that formalizes the notion of probabilistic tex-
tual entailment, which is based on the conditional 
probability that a hypothesis is true given the text. 
This probabilistic setting provided the necessary 
grounding for a concrete probabilistic model of 
lexical entailment that is based on document co-
occurrence statistics in a bag of words representa-
tion.  Although the illustrated lexical system is 
relatively simple, as it doesn?t rely on syntactic or 
other deeper analysis, it nevertheless achieved en-
couraging results. The results suggest that such a 
probabilistic framework is a promising basis for 
improved implementations incorporating deeper 
types of knowledge and a common test-bed for 
more sophisticated models.   
Acknowledgments 
This work was supported in part by the IST Pro-
gramme of the European Community, under the 
PASCAL Network of Excellence, IST-2002-
506778. This publication only reflects the authors' 
views. We would also like to thank Ruthie Mandel 
and Tal Itzhak Ron for their annotation work. 
 
References 
Fahiem Bacchus. 1990. Representing and Reasoning 
with Probabilistic Knowledge, M.I.T. Press. 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics, 
19(2):263?311. 
Chierchia, Gennaro, and Sally McConnell-Ginet. 2001. 
Meaning and grammar: An introduction to seman-
tics, 2nd. edition. Cambridge, MA: MIT Press. 
Ido Dagan, Oren Glickman and Bernardo Magnini. 
2005. The PASCAL Recognising Textual Entailment 
Challenge. In Proceedings of the PASCAL Chal-
lenges Workshop for Recognizing Textual Entail-
ment. Southampton, U.K. 
Oren Glickman, Ido Dagan and Moshe Koppel. 2005a. 
Web Based Probabilistic Textual Entailment, 
PASCAL Challenges Workshop for Recognizing 
Textual Entailment. 
Oren Glickman, Ido Dagan and Moshe Koppel. 2005b. 
A Probabilistic Classification Approach for Lexical 
Textual Entailment, Twentieth National Conference 
on Artificial Intelligence (AAAI-05). 
Joseph Y. Halpern. 1990. An analysis of first-order lo-
gics of probability. Artificial Intelligence 46:311-350. 
Christof Monz, Maarten de Rijke. 2001. Light-Weight 
Entailment Checking for Computational Semantics. 
In Proc. of the third workshop on inference in com-
putational semantics (ICoS-3).  
Jian-Yun Nie and Martin Brisebois. 1996. An Inferential 
Approach to Information Retrieval and Its Implemen-
tation Using a Manual Thesaurus. Artificial Intelli-
gence Revue 10(5-6): 409-439. 
Jay M. Ponte, W. Bruce Croft, 1998. A Language Mod-
eling Approach to Information Retrieval. SIGIR con-
ference on Research and Development in Information 
Retrieval. 
Tony G. Rose, Mary Stevenson, and Miles Whitehead. 
2002. The Reuters Corpus volume 1 - from yester-
day?s news to tomorrow?s language resources. Third 
International Conference on Language Resources and 
Evaluation (LREC). 
Hui Yang and Tat-Seng Chua. 2002. The integration of 
lexical knowledge and external resources for ques-
tion answering. The eleventh Text REtrieval Confer-
ence (TREC-11). 
48
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 55?60,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Definition and Analysis of Intermediate Entailment Levels
Roy Bar-Haim, Idan Szpektor, Oren Glickman
Computer Science Department
Bar Ilan University
Ramat-Gan 52900, Israel
{barhair,szpekti,glikmao}@cs.biu.ac.il
Abstract
In this paper we define two intermediate
models of textual entailment, which corre-
spond to lexical and lexical-syntactic lev-
els of representation. We manually anno-
tated a sample from the RTE dataset ac-
cording to each model, compared the out-
come for the two models, and explored
how well they approximate the notion of
entailment. We show that the lexical-
syntactic model outperforms the lexical
model, mainly due to a much lower rate
of false-positives, but both models fail to
achieve high recall. Our analysis also
shows that paraphrases stand out as a
dominant contributor to the entailment
task. We suggest that our models and an-
notation methods can serve as an evalua-
tion scheme for entailment at these levels.
1 Introduction
Textual entailment has been proposed recently as
a generic framework for modeling semantic vari-
ability in many Natural Language Processing ap-
plications, such as Question Answering, Informa-
tion Extraction, Information Retrieval and Docu-
ment Summarization. The textual entailment rela-
tionship holds between two text fragments, termed
text and hypothesis, if the truth of the hypothesis can
be inferred from the text.
Identifying entailment is a complex task that in-
corporates many levels of linguistic knowledge and
inference. The complexity of modeling entail-
ment was demonstrated in the first PASCAL Chal-
lenge Workshop on Recognizing Textual Entailment
(RTE) (Dagan et al, 2005). Systems that partici-
pated in the challenge used various combinations of
NLP components in order to perform entailment in-
ferences. These components can largely be classi-
fied as operating at the lexical, syntactic and seman-
tic levels (see Table 1 in (Dagan et al, 2005)). How-
ever, only little research was done to analyze the
contribution of each inference level, and on the con-
tribution of individual inference mechanisms within
each level.
This paper suggests that decomposing the com-
plex task of entailment into subtasks, and analyz-
ing the contribution of individual NLP components
for these subtasks would make a step towards bet-
ter understanding of the problem, and for pursuing
better entailment engines. We set three goals in this
paper. First, we consider two modeling levels that
employ only part of the inference mechanisms, but
perform perfectly at each level. We explore how
well these models approximate the notion of entail-
ment, and analyze the differences between the out-
come of the different levels. Second, for each of the
presented levels, we evaluate the distribution (and
contribution) of each of the inference mechanisms
typically associated with that level. Finally, we sug-
gest that the definitions of entailment at different
levels of inference, as proposed in this paper, can
serve as guidelines for manual annotation of a ?gold
standard? for evaluating systems that operate at a
particular level. Altogether, we set forth a possible
methodology for annotation and analysis of entail-
55
ment datasets.
We introduce two levels of entailment: Lexical
and Lexical-Syntactic. We propose these levels as
intermediate stages towards a complete entailment
model. We define an entailment model for each
level and manually evaluate its performance over a
sample from the RTE test-set. We focus on these
two levels as they correspond to well-studied NLP
tasks, for which robust tools and resources exist,
e.g. parsers, part of speech taggers and lexicons. At
each level we included inference types that represent
common practice in the field. More advanced pro-
cessing levels which involve logical/semantic infer-
ence are less mature and were left beyond the scope
of this paper.
We found that the main difference between the
lexical and lexical-syntactic levels is that the lexical-
syntactic level corrects many false-positive infer-
ences done at the lexical level, while introducing
only a few false-positives of its own. As for iden-
tifying positive cases (recall), both systems exhibit
similar performance, and were found to be comple-
mentary. Neither of the levels was able to iden-
tify more than half of the positive cases, which
emphasizes the need for deeper levels of analysis.
Among the different inference components, para-
phrases stand out as a dominant contributor to the
entailment task, while synonyms and derivational
transformations were found to be the most frequent
at the lexical level.
Using our definitions of entailment models as
guidelines for manual annotation resulted in a high
level of agreement between two annotators, suggest-
ing that the proposed models are well-defined.
Our study follows on previous work (Vander-
wende et al, 2005), which analyzed the RTE Chal-
lenge test-set to find the percentage of cases in
which syntactic analysis alone (with optional use
of thesaurus for the lexical level) suffices to decide
whether or not entailment holds. Our study extends
this work by considering a broader range of infer-
ence levels and inference mechanisms and providing
a more detailed view. A fundamental difference be-
tween the two works is that while Vanderwende et al
did not make judgements on cases where additional
knowledge was required beyond syntax, our entail-
ment models were evaluated over all of the cases,
including those that require higher levels of infer-
ence. This allows us to view the entailment model at
each level as an idealized system approximating full
entailment, and to evaluate its overall success.
The rest of the paper is organized as follows: sec-
tion 2 provides definitions for the two entailment
levels; section 3 describes the annotation experiment
we performed, its results and analysis; section 4 con-
cludes and presents planned future work.
2 Definition of Entailment Levels
In this section we present definitions for two en-
tailment models that correspond to the Lexical and
Lexical-Syntactic levels. For each level we de-
scribe the available inference mechanisms. Table 1
presents several examples from the RTE test-set to-
gether with annotation of entailment at the different
levels.
2.1 The Lexical entailment level
At the lexical level we assume that the text T and
hypothesis H are represented by a bag of (possibly
multi-word) terms, ignoring function words. At this
level we define that entailment holds between T and
H if every term h in H can be matched by a corre-
sponding entailing term t in T . t is considered as en-
tailing h if either h and t share the same lemma and
part of speech, or t can be matched with h through a
sequence of lexical transformations of the types de-
scribed below.
Morphological derivations This inference mech-
anism considers two terms as equivalent if one can
be obtained from the other by some morphologi-
cal derivation. Examples include nominalizations
(e.g. ?acquisition ? acquire?), pertainyms (e.g.
?Afghanistan ? Afghan?), or nominal derivations
like ?terrorist ? terror?.
Ontological relations This inference mechanism
refers to ontological relations between terms. A
term is inferred from another term if a chain of valid
ontological relations between the two terms exists
(Andreevskaia et al, 2005). In our experiment we
regarded the following three ontological relations
as providing entailment inferences: (1) ?synonyms?
(e.g. ?free ? release? in example 1361, Table 1);
(2) ?hypernym? (e.g. ?produce ? make?) and (3)
?meronym-holonym? (e.g. ?executive ? company?).
56
No. Text Hypothesis Task Ent. Lex.
Ent.
Syn.
Ent.
322 Turnout for the historic vote for the first
time since the EU took in 10 new mem-
bers in May has hit a record low of
45.3%.
New members joined the
EU.
IR true false true
1361 A Filipino hostage in Iraq was released. A Filipino hostage was
freed in Iraq.
CD true true true
1584 Although a Roscommon man by birth,
born in Rooskey in 1932, Albert ?The
Slasher? Reynolds will forever be a
Longford man by association.
Albert Reynolds was born
in Co. Roscommon.
QA true true true
1911 The SPD got just 21.5% of the vote
in the European Parliament elections,
while the conservative opposition par-
ties polled 44.5%.
The SPD is defeated by
the opposition parties.
IE true false false
2127 Coyote shot after biting girl in Vanier
Park.
Girl shot in park. IR false true false
Table 1: Examples of text-hypothesis pairs, taken from the PASCAL RTE test-set. Each line includes the
example number at the RTE test-set, the text and hypothesis, the task within the test-set, whether entailment
holds between the text and hypothesis (Ent.), whether Lexical entailment holds (Lex. Ent.) and whether
Lexical-Syntactic entailment holds (Syn. Ent.).
Lexical World knowledge This inference mech-
anism refers to world knowledge reflected at the
lexical level, by which the meaning of one term
can be inferred from the other. It includes both
knowledge about named entities, such as ?Tal-
iban ? organization? and ?Roscommon ? Co.
Roscommon? (example 1584 in Table 1), and other
lexical relations between words, such as WordNet?s
relations ?cause? (e.g. ?kill ? die?) and ?entail? (e.g.
?snore ? sleep?).
2.2 The Lexical-syntactic entailment level
At the lexical-syntactic level we assume that the
text and the hypothesis are represented by the set of
syntactic dependency relations of their dependency
parse. At this level we ignore determiners and aux-
iliary verbs, but do include relations involving other
function words. We define that entailment holds be-
tween T and H if the relations within H can be
?covered? by the relations in T . In the trivial case,
lexical-syntactic entailment holds if all the relations
composing H appear verbatim in T (while addi-
tional relations within T are allowed). Otherwise,
such coverage can be obtained by a sequence of
transformations applied to the relations in T , which
should yield all the relations in H .
One type of such transformations are the lexical
transformations, which replace corresponding lexi-
cal items, as described in sub-section 2.1. When ap-
plying morphological derivations it is assumed that
the syntactic structure is appropriately adjusted. For
example, ?Mexico produces oil? can be mapped to
?oil production by Mexico? (the NOMLEX resource
(Macleod et al, 1998) provides a good example for
systematic specification of such transformations).
Additional types of transformations at this level
are specified below.
Syntactic transformations This inference mech-
anism refers to transformations between syntactic
structures that involve the same lexical elements and
preserve the meaning of the relationships between
them (as analyzed in (Vanderwende et al, 2005)).
Typical transformations include passive-active and
apposition (e.g. ?An Wang, a native of Shanghai ?
An Wang is a native of Shanghai?).
57
Entailment paraphrases This inference mecha-
nism refers to transformations that modify the syn-
tactic structure of a text fragment as well as some
of its lexical elements, while holding an entailment
relationship between the original text and the trans-
formed one. Such transformations are typically de-
noted as ?paraphrases? in the literature, where a
wealth of methods for their automatic acquisition
were proposed (Lin and Pantel, 2001; Shinyama et
al., 2002; Barzilay and Lee, 2003; Szpektor et al,
2004). Following the same spirit, we focus here on
transformations that are local in nature, which, ac-
cording to the literature, may be amenable for large
scale acquisition. Examples include: ?X is Y man
by birth ? X was born in Y? (example 1584 in Ta-
ble 1), ?X take in Y ? Y join X?1 and ?X is holy
book of Y ? Y follow X?2.
Co-reference Co-references provide equivalence
relations between different terms in the text and
thus induce transformations that replace one term
in a text with any of its co-referenced terms. For
example, the sentence ?Italy and Germany have
each played twice, and they haven?t beaten anybody
yet.?3 entails ?Neither Italy nor Germany have
won yet?, involving the co-reference transformation
?they ? Italy and Germany?.
Example 1584 in Table 1 demonstrates the
need to combine different inference mechanisms
to achieve lexical-syntactic entailment, requiring
world-knowledge, paraphrases and syntactic trans-
formations.
3 Empirical Analysis
In this section we present the experiment that we
conducted in order to analyze the two entailment
levels, which are presented in section 2, in terms of
relative performance and correlation with the notion
of textual entailment.
3.1 Data and annotation procedure
The RTE test-set4 contains 800 Text-Hypothesis
pairs (usually single sentences), which are typical
1Example no 322 in the PASCAL RTE test-set.
2Example no 1575 in the PASCAL RTE test-set.
3Example no 298 in the PASCAL RTE test-set.
4The complete RTE dataset can be obtained at
http://www.pascal-network.org/Challenges/RTE/Datasets/
to various NLP applications. Each pair is annotated
with a boolean value, indicating whether the hypoth-
esis is entailed by the text or not, and the test-set
is balanced in terms of positive and negative cases.
We shall henceforth refer to this annotation as the
gold standard. We constructed a sample of 240 pairs
from four different tasks in the test-set, which corre-
spond to the main applications that may benefit from
entailment: information extraction (IE), information
retrieval (IR), question answering (QA), and compa-
rable documents (CD). We randomly picked 60 pairs
from each task, and in total 118 of the cases were
positive and 122 were negative.
In our experiment, two of the authors annotated,
for each of the two levels, whether or not entailment
can be established in each of the 240 pairs. The an-
notators agreed on 89.6% of the cases at the lexical
level, and 88.8% of the cases at the lexical-syntactic
level, with Kappa statistics of 0.78 and 0.73, re-
spectively, corresponding to ?substantial agreement?
(Landis and Koch, 1977). This relatively high level
of agreement suggests that the notion of lexical and
lexical-syntactic entailment we propose are indeed
well-defined.
Finally, in order to establish statistics from the an-
notations, the annotators discussed all the examples
they disagreed on and produced a final joint deci-
sion.
3.2 Evaluating the different levels of entailment
L LS
True positive (118) 52 59
False positive (122) 36 10
Recall 44% 50%
Precision 59% 86%
F1 0.5 0.63
Accuracy 58% 71%
Table 2: Results per level of entailment.
Table 2 summarizes the results obtained from our
annotated dataset for both lexical (L) and lexical-
syntactic (LS) levels. Taking a ?system?-oriented
perspective, the annotations at each level can be
viewed as the classifications made by an idealized
system that includes a perfect implementation of the
inference mechanisms in that level. The first two
58
rows show for each level how the cases, which were
recognized as positive by this level (i.e. the entail-
ment holds), are distributed between ?true positive?
(i.e. positive according to the gold standard) and
?false positive? (negative according to the gold stan-
dard). The total number of positive and negative
pairs in the dataset is reported in parentheses. The
rest of the table details recall, precision, F1 and ac-
curacy.
The distribution of the examples in the RTE test-
set cannot be considered representative of a real-
world distribution (especially because of the con-
trolled balance between positive and negative exam-
ples). Thus, our statistics are not appropriate for
accurate prediction of application performance. In-
stead, we analyze how well these simplified models
of entailment succeed in approximating ?real? en-
tailment, and how they compare with each other.
The proportion between true and false positive
cases at the lexical level indicates that the correla-
tion between lexical match and entailment is quite
low, reflected in the low precision achieved by this
level (only 59%). This result can be partly attributed
to the idiosyncracies of the RTE test-set: as reported
in (Dagan et al, 2005), samples with high lexical
match were found to be biased towards the negative
side. Interestingly, our measured accuracy correlates
well with the performance of systems at the PAS-
CAL RTE Workshop, where the highest reported ac-
curacy of a lexical system is 0.586 (Dagan et al,
2005).
As one can expect, adding syntax considerably re-
duces the number of false positives - from 36 to only
10. Surprisingly, at the same time the number of true
positive cases grows from 52 to 59, and correspond-
ingly, precision rise to 86%. Interestingly, neither
the lexical nor the lexical-syntactic level are able to
cover more than half of the positive cases (e.g. ex-
ample 1911 in Table 1).
In order to better understand the differences be-
tween the two levels, we next analyze the overlap
between them, presented in Table 3. Looking at
Table 3(a), which contains only the positive cases,
we see that many examples were recognized only by
one of the levels. This interesting phenomenon can
be explained on the one hand by lexical matches that
could not be validated in the syntactic level, and on
the other hand by the use of paraphrases, which are
Lexical-Syntactic
H ? T H; T
Lexical H ? T 38 14H; T 21 45
(a) positive examples
Lexical-Syntactic
H ? T H; T
Lexical H ? T 7 29H; T 3 83
(b) negative examples
Table 3: Correlation between the entailment lev-
els. (a) includes only the positive examples from
the RTE dataset sample, and (b) includes only the
negative examples.
introduced only in the lexical-syntactic level. (e.g.
example 322 in Table 1).
This relatively symmetric situation changes as we
move to the negative cases, as shown in Table 3(b).
By adding syntactic constraints, the lexical-syntactic
level was able to fix 29 false positive errors, misclas-
sified at the lexical level (as demonstrated in exam-
ple 2127, Table 1), while introducing only 3 new
false-positive errors. This exemplifies the impor-
tance of syntactic matching for precision.
3.3 The contribution of various inference
mechanisms
Inference Mechanism f 4R %
Synonym 19 14.4% 16.1%
Morphological 16 10.1% 13.5%
Lexical World knowledge 12 8.4% 10.1%
Hypernym 7 4.2% 5.9%
Mernoym 1 0.8% 0.8%
Entailment Paraphrases 37 26.2% 31.3%
Syntactic transformations 22 16.9% 18.6%
Coreference 10 5.0% 8.4%
Table 4: The frequency (f ), contribution to recall
(4R) and percentage (%), within the gold standard
positive examples, of the various inference mecha-
nisms at each level, ordered by their significance.
59
In order to get a sense of the contribution of the
various components at each level, statistics on the in-
ference mechanisms that contributed to the coverage
of the hypothesis by the text (either full or partial)
were recorded by one annotator. Only the positive
cases in the gold standard were considered.
For each inference mechanism we measured its
frequency, its contribution to the recall of the related
level and the percentage of cases in which it is re-
quired for establishing entailment. The latter also
takes into account cases where only partial cover-
age could be achieved, and thus indicates the signif-
icance of each inference mechanism for any entail-
ment system, regardless of the models presented in
this paper. The results are summarized in Table 4.
From Table 4 it stands that paraphrases are the
most notable contributors to recall. This result in-
dicates the importance of paraphrases to the en-
tailment task and the need for large-scale para-
phrase collections. Syntactic transformations are
also shown to contribute considerably, indicating the
need for collections of syntactic transformations as
well. In that perspective, we propose our annota-
tion framework as means for evaluating collections
of paraphrases or syntactic transformations in terms
of recall.
Finally, we note that the co-reference moderate
contribution can be partly attributed to the idiosyn-
cracies of the RTE test-set: the annotators were
guided to replace anaphors with the appropriate ref-
erence, as reported in (Dagan et al, 2005).
4 Conclusions
In this paper we presented the definition of two en-
tailment models, Lexical and Lexical-Syntactic, and
analyzed their performance manually. Our experi-
ment shows that the lexical-syntactic level outper-
forms the lexical level in all measured aspects. Fur-
thermore, paraphrases and syntactic transformations
emerged as the main contributors to recall. These
results suggest that a lexical-syntactic framework
is a promising step towards a complete entailment
model.
Beyond these empirical findings we suggest that
the presented methodology can be used generically
to annotate and analyze entailment datasets.
In future work, it would be interesting to analyze
higher levels of entailment, such as logical inference
and deep semantic understanding of the text.
Acknowledgements
We would like to thank Ido Dagan for helpful discus-
sions and for his scientific supervision. This work
was supported in part by the IST Programme of the
European Community, under the PASCAL Network
of Excellence, IST-2002-506778. This publication
only reflects the authors? views.
References
Alina Andreevskaia, Zhuoyan Li and Sabine Bergler.
2005. Can Shallow Predicate Argument Structures
Determine Entailment?. In Proceedings of Pascal
Challenge Workshop on Recognizing Textual Entail-
ment, 2005.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003. pages 16-23, Edmonton, Canada.
Ido Dagan, Bernardo Magnini and Oren Glickman. 2005.
The PASCAL Recognising Textual Entailment Chal-
lenge. In Proceedings of Pascal Challenge Workshop
on Recognizing Textual Entailment, 2005.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159-174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for Question Answering. Natural Language
Engineering, 7(4):343-360.
C. Macleod, R. Grishman, A. Meyers, L. Barrett and R.
Reeves. 1998. Nomlex: A lexicon of nominalizations.
In Proceedings of the 8th International Congress of the
European Association for Lexicography, 1998. Lie`ge,
Belgium: EURALEX.
Yusuke Shinyama and Satoshi Sekine, Kiyoshi Sudo and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of Human
Language Technology Conference (HLT 2002). San
Diego, USA.
Idan Szpektor, Hristo Tanev, Ido Dagan and Bonnaven-
tura Coppola. 2004. Scaling Web-based Acquistion
of Entailment Relations. In Proceedings of EMNLP
2004.
Lucy Vanderwende, Deborah Coughlin and Bill Dolan.
2005. What Syntax Contribute in Entailment Task. In
Proceedings of Pascal Challenge Workshop on Recog-
nizing Textual Entailment, 2005.
60
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 172?179,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Lexical Reference: a Semantic Matching Subtask
Oren Glickman and Eyal Shnarch and Ido Dagan
Computer Science Department
Bar Ilan University
Ramat Gan, Israel
{glikmao, dagan}@cs.biu.ac.il
Abstract
Semantic lexical matching is a prominent
subtask within text understanding applica-
tions. Yet, it is rarely evaluated in a di-
rect manner. This paper proposes a def-
inition for lexical reference which cap-
tures the common goals of lexical match-
ing. Based on this definition we created
and analyzed a test dataset that was uti-
lized to directly evaluate, compare and im-
prove lexical matching models. We sug-
gest that such decomposition of the global
semantic matching task is critical in order
to fully understand and improve individual
components.
1 Introduction
A fundamental task for text understanding ap-
plications is to identify semantically equivalent
pieces of text. For example, Question Answer-
ing (QA) systems need to match corresponding
parts in the question and in the answer passage,
even though such parts may be expressed in dif-
ferent terms. Summarization systems need to rec-
ognize (redundant) semantically matching parts
in multiple sentences that are phrased differently.
Other applications, such as information extraction
and retrieval, face pretty much the same seman-
tic matching task. The degree of semantic match-
ing found is typically factored into systems? scor-
ing and ranking mechanisms. The recently pro-
posed framework of textual entailment (Dagan et
al., 2006) attempts to formulate the generic seman-
tic matching problem in an application indepen-
dent manner.
The most commonly implemented semantic
matching component addresses the lexical level.
At this level the goal is to identify whether the
meaning of a lexical item of one text is expressed
also within the other text. Typically, lexical match-
ing models measure the degree of literal lexical
overlap, augmented with lexical substitution cri-
teria based on resources such as Wordnet or the
output of statistical similarity methods (see Sec-
tion 2). Many systems apply semantic matching
only at the lexical level, which is used to approx-
imate the overall degree of semantic matching be-
tween texts. Other systems incorporate lexical
matching as a component within more complex
models that examine matching at higher syntactic
and semantic levels.
While lexical matching models are so promi-
nent within semantic systems they are rarely eval-
uated in a direct manner. Typically, improve-
ments to a lexical matching model are evaluated by
their marginal contribution to overall system per-
formance. Yet, such global and indirect evaluation
does not indicate the absolute performance of the
model relative to the sheer lexical matching task
for which it was designed. Furthermore, the indi-
rect application-dependent evaluation mode does
not facilitate improving lexical matching models
in an application dependent manner, and does not
allow proper comparison of such models which
were developed (and evaluated) by different re-
searchers within different systems.
This paper proposes a generic definition for the
lexical matching task, which we term lexical ref-
erence. This definition is application indepen-
dent and enables annotating test datasets that eval-
uate directly lexical matching models. Conse-
quently, we created a dataset annotated for lexical
reference, using a sample of sentence pairs (text-
hypothesis) from the 1st Recognising Textual En-
tailment dataset. Further analysis identified sev-
172
eral sub-types of lexical reference, pointing at the
many interesting cases where lexical reference is
derived from a complete context rather than from
a particular matching lexical item.
Next, we used the lexical reference dataset to
evaluate and compare several state-of-the-art ap-
proaches for lexical matching. Having a direct
evaluation task enabled us to capture the actual
performance level of these models, to reveal their
relative strengths and weaknesses, and even to
construct a simple combination of two models that
outperforms all the original ones. Overall, we sug-
gest that it is essential to decompose global se-
mantic matching and textual entailment tasks into
proper subtasks, like lexical reference. Such de-
composition is needed in order to fully understand
the behavior of individual system components and
to guide their future improvements.
2 Background
2.1 Term Matching
Thesaurus-based term expansion is a commonly
used technique for enhancing the recall of NLP
systems and coping with lexical variability. Ex-
pansion consists of altering a given text (usu-
ally a query) by adding terms of similar meaning.
WordNet is commonly used as a source of related
words for expansion. For example, many QA sys-
tems perform expansion in the retrieval phase us-
ing query related words based on WordNet?s lexi-
cal relations such as synonymy or hyponymy (e.g
(Harabagiu et al, 2000; Hovy et al, 2001)). Lex-
ical similarity measures (e.g. (Lin, 1998)) have
also been suggested to measure semantic similar-
ity. They are based on the distributional hypothe-
sis, suggesting that words that occur within similar
contexts are semantically similar.
2.2 Textual Entailment
The Recognising Textual Entailment (RTE-1) chal-
lenge (Dagan et al, 2006) is an attempt to promote
an abstract generic task that captures major seman-
tic inference needs across applications. The task
requires to recognize, given two text fragments,
whether the meaning of one text can be inferred
(entailed) from another text. Different techniques
and heuristics were applied on the RTE-1 dataset
to specifically model textual entailment. Interest-
ingly, a number of works (e.g. (Bos and Mark-
ert, 2005; Corley and Mihalcea, 2005; Jijkoun and
de Rijke, 2005; Glickman et al, 2006)) applied or
utilized lexical based word overlap measures. Var-
ious word-to-word similarity measures where ap-
plied, including distributional similarity (such as
(Lin, 1998)), web-based co-occurrence statistics
and WordNet based similarity measures (such as
(Leacock et al, 1998)).
2.3 Paraphrase Acquisition
A substantial body of work has been dedicated to
learning patterns of semantic equivalency between
different language expressions, typically consid-
ered as paraphrases. Recently, several works ad-
dressed the task of acquiring paraphrases (semi-)
automatically from corpora. Most attempts were
based on identifying corresponding sentences in
parallel or ?comparable? corpora, where each cor-
pus is known to include texts that largely corre-
spond to texts in another corpus (e.g. (Barzilay
and McKeown, 2001)). Distributional Similarity
was also used to identify paraphrase patterns from
a single corpus rather than from a comparable
set of corpora (Lin and Pantel, 2001). Similarly,
(Glickman and Dagan, 2004) developed statistical
methods that match verb paraphrases within a reg-
ular corpus.
3 The Lexical Reference Dataset
3.1 Motivation and Definition
One of the major observations of the 1st Recog-
nizing Textual Entailment (RTE-1) challenge re-
ferred to the rich structure of entailment modeling
systems and the need to evaluate and optimize in-
dividual components within them. When building
such a compound system it is valuable to test each
component directly during its development, rather
than indirectly evaluating the component?s perfor-
mance via the behavior of the entire system. If
given tools to evaluate each component indepen-
dently researchers can target and perfect the per-
formance of the subcomponents without the need
of building and evaluating the entire end-to-end
system.
A common subtask, addressed by practically all
participating systems in RTE-1, was to recognize
whether each lexical meaning in the hypothesis is
referenced by some meaning in the corresponding
text. We suggest that this common goal can be
captured through the following definition:
Definition 1 A word w is lexically referenced by
a text t if there is an explicit or implied reference
173
from a set of words in t to a possible meaning of
w.
Lexical reference may be viewed as a natural ex-
tension of textual entailment for sub-sentential hy-
potheses such as words. In this work we fo-
cus on words meanings, however this work can
be directly generalized to word compounds and
phrases. A concrete version of detailed annotation
guidelines for lexical reference is presented in the
next section.1 Lexical Reference is, in some sense,
a more general notion than paraphrases. If the text
includes a paraphrase for w then naturally it does
refer to w?s meaning. However, a text need not
include a paraphrase for the concrete meaning of
the referenced word w, but only an implied refer-
ence. Accordingly, the referring part might be a
large segment of the text, which captures informa-
tion different than w?s meaning, but still implies a
reference to w as part of the text?s meaning.
It is typically a necessary, but not sufficient,
condition for textual entailment that the lexical
concepts in a hypothesis h are referred in a given
text t. For example, in order to infer from a text
the hypothesis ?a dog bit a man,? it is a neces-
sary that the concepts of dog, bite and man must
be referenced by the text, either directly or in an
implied manner. However, for proper entailment
it is further needed that the right relations would
hold between these concepts2. Therefore lexical
entailment should typically be a component within
a more complex entailment modeling (or semantic
matching) system.
3.2 Dataset Creation and Annotation Process
We created a lexical reference dataset derived
from the RTE-1 development set by randomly
choosing 400 out of the 567 text-hypothesis exam-
ples. We then created sentence-word examples for
all content words in the hypotheses which do not
appear in the corresponding sentence and are not
a morphological derivation of a word in it (since a
simple morphologic module could easily identify
these cases). This resulted in a total of 708 lexi-
cal reference examples. Two annotators annotated
these examples as described in the next section.
1These terms should not be confused with the use of lex-
ical entailment in WordNet, which is used to describe an en-
tailment relationship between verb lexical types, nor with the
related notion of reference in classical linguistics, generally
describing the relation between nouns or pronouns and ob-
jects that are named by them (Frege, 1892)
2or quoting the known journalism saying ? ?Dog bites
man? isn?t news, but ?Man bites dog? is.
Taking the same approach as of the RTE-1 dataset
creation (Dagan et al, 2006), we limited our ex-
periments to the resulting 580 examples that the
two annotators agreed upon3.
3.2.1 Annotation guidelines
We asked two annotators to annotate the
sentence-word examples according to the follow-
ing guidelines. Given a sentence and a target word
the annotators were asked to decide whether the
target word is referred by the sentence (true) or
not (false). Annotators were guided to mark the
pair as true in the following cases:
Word: if there is a word in the sentence which,
in the context of the sentence, implies a meaning
of the target word (e.g. a synonym or hyponym),
or which implies a reference to the target word?s
meaning (e.g. blind?see, sight). See examples 1-
2 in Table 1 where the word that implies the refer-
ence is emphasized in the text. Note that in exam-
ple 2 murder is not a synonym of died nor does it
share the same meaning of died; however it is clear
from its presence in the sentence that it refers to a
death. Also note that in example 8 although home
is a possible synonym for house, in the context of
the text it does not appear in that meaning and the
example should be annotated as false.
Phrase: if there is a multi-word independent ex-
pression in the sentence that implies the target (im-
plication in the same sense that a Word does). See
examples 3-4 in Table 1.
Context: if there is a clear reference to the mean-
ing of the target word by the overall meaning of
some part(s) of the sentence (possibly all the sen-
tence), though it is not referenced by any single
word or phrase. The reference is derived from the
complete context of the relevant sentence part. See
examples 5-7 in Table 1.
If there is no reference from the sentence to
the target word the annotators were instructed to
choose false. In example 9 in Table 1 the target
word ?HIV-positive? should be considered as one
word that cannot be broken down from its unit and
although both the general term ?HIV status? and
the more specific term ?HIV negative? are referred
to, the target word cannot be understood or derived
from the text. In example 10 although the year
1945 may refer to a specific war, there is no ?war?
either specifically or generally understood by the
text.
3dataset avaiable at http://ir-srv.cs.biu.ac.
il:64080/emnlp06_dataset.zip
174
ID TEXT TARGET VALUE
1 Oracle had fought to keep the forms from being released. document word
2 The court found two men guilty of murdering Shapour Bakhtiar. died word
3 The new information prompted them to call off the search. cancelled phrase
4 Milan, home of the famed La Scala opera house,. . . located phrase
5 Successful plaintiffs recovered punitive damages in Texas discrimination cases 53 legal context
6 Recreational marijuana smokers are no more likely to develop oral cancer than nonusers. risk context
7 A bus ticket cost nowadays 5.2 NIS whereas last year it cost 4.9. increase context
8 Pakistani officials announced that two South African men in their custody had confessed to
planning attacks at popular tourist spots in their home country.
house false
9 For women who are HIV negative or who do not know their HIV status, breastfeeding should
be promoted for six months.
HIV-positive false
10 On Feb. 1, 1945, the Polish government made Warsaw its capital, and an office for urban
reconstruction was set up.
war false
Table 1: Lexical Reference Annotation Examples
3.2.2 Annotation results
Wemeasured the agreement on the lexical refer-
ence binary task (in which Word, Phrase and Con-
text are conflated to true). The resulting kappa
statistic of 0.63 is regarded as substantial agree-
ment (Landis and Koch, 1997). The resulting
dataset is not balanced in terms of true and false
examples and a straw-baseline for accuracy is
0.61, representing a system which predicts all ex-
amples as true.
3.3 Dataset Analysis
In a similar manner to (Bar-Haim et al, 2005; Van-
derwende et al, 2005) we investigated the rela-
tionship between lexical reference and textual en-
tailment. We checked the performance of a textual
entailment system which relies solely on an ideal
lexical reference component which makes no mis-
takes and asserts that a hypothesis is entailed from
a text if and only if all content words in the hypoth-
esis are referred in the text. Based on the lexical
reference dataset annotations, such an ?ideal? sys-
tem would obtain an accuracy of 74% on the cor-
responding subset of the textual entailment task.
The corresponding precision is 68% and a recall
of 82%. This is significantly higher than the re-
sults of the best performing systems that partici-
pated in the challenge on the RTE-1 test set. This
suggests that lexical reference is a valuable sub-
task for entailment. Interestingly, a similar entail-
ment system based on a lexical reference compo-
nent which doesn?t account for the contextual lex-
ical reference (i.e. all Context annotations are re-
garded as false) would achieve an accuracy of only
63% with 41% precision and a recall of 63%. This
suggests that lexical reference in general and con-
textual entailment in particular, play an important
(though not sufficient) role in entailment recogni-
tion.
Further, we wanted to investigate the validity
of the assumption that for entailment relationship
to hold all content words in the hypothesis must
be referred by the text. We examined the exam-
ples in our dataset which were derived from text-
hypothesis pairs that were annotated as true (en-
tailing) in the RTE dataset. Out of 257 such exam-
ples only 34 were annotated as false by both anno-
tators. Table 2 lists a few such examples in which
entailment at whole holds, however, there exists a
word in the hypothesis (highlighted in the table)
which is not lexically referenced by the text. In
many cases, the target word was part of a non com-
positional compound in the hypothesis, and there-
fore should not be expected to be referenced by
the text (see examples 1-2). This finding indicates
that the basic assumption is a reasonable approxi-
mation for entailment. We could not have revealed
this fact without the dataset for the subtask of lex-
ical reference.
4 Lexical Reference Models
The lexical reference dataset facilitates qualita-
tive and quantitative comparison of various lexical
models. This section describes four state-of-the-
art models that can be applied to the lexical refer-
ence task. The performance of these models was
tested and analyzed, as described in the next sec-
tion, using the lexical reference dataset. All mod-
els assign a [0, 1] score to a given pair of text t
and target word u which can be interpreted as the
confidence that u is lexically referenced in t.
175
ID TEXT HYPOTHESIS ENTAIL-
MENT
REFER-
ENCE
1 Iran is said to give up al Qaeda members. Iran hands over al Qaeda members. true false
2 It would help the economy by putting people
back to work and more money in the hands of
consumers.
More money in the hands of consumers
means more money can be spent to get the
economy going.
true false
3 The Securities and Exchange Commission?s
new rule to beef up the independence of mutual
fund boards represents an industry defeat.
The SEC?s new rule will give boards inde-
pendence.
true false
4 Texas Data Recovery is also successful at re-
trieving lost data from notebooks and laptops,
regardless of age, make or model.
In the event of a disaster you could use Texas
Data Recovery and you will have the capabil-
ity to restore lost data.
true false
Table 2: examples demonstrating cases when lexical entailment does not correlate with entailment. Tar-
get word is shown in bold.
4.1 WordNet
Following the common practice in NLP applica-
tions (see Section 2.1) we evaluated the perfor-
mance of a straight-forward utilization of Word-
Net?s lexical information. Our wordnet model first
lemmatizes the text and target word. It then as-
signs a score of 1 if the text contains a synonym,
hyponym or derived form of the target word and a
score of 0 otherwise.
4.2 Similarity
As a second measure we used the distributional
similarity measure of (Lin, 1998). For a text t and
a word u we assign the max similarity score as fol-
lows:
similarity(t, u) = max
v?t
sim(u, v) (1)
where sim(u, v) is the similarity score for u and
v4.
4.3 Alignment model
(Glickman et al, 2006) was among the top scor-
ing systems on the RTE-1 challenge and supplies a
probabilistically motivated lexical measure based
on word co-occurrence statistics. It is defined for
a text t and a word u as follows:
align(t, u) = max
v?t
P(u|v) (2)
where P(u|v) is simply the co-occurrence proba-
bility ? the probability that a sentence containing v
also contains u. The co-occurrence statistics were
collected from the Reuters Corpus Volume 1.
4the scores were obtained from the following online re-
source: http://www.cs.ualberta.ca/?lindek/
downloads.htm
4.4 Baysean model
(Glickman et al, 2005) provide a contextual mea-
sure which takes into account the whole context
of the text rather than from a single word in the
text as do the previous models. This model is
the only model which addresses contextual refer-
ence rather than just word-to-word matching. The
model is based on a Na??ve Bayes text classification
approach in which corpus sentences serve as doc-
uments and the class is the reference of the target
word u. Sentences containing the word u are used
as positive examples while all other sentences are
considered as negative examples. It is defined for
a text t and a word u as follows:
bayes(t, u) =
P(u)
?
v?t P(v|u)
n(v,t)
P(?u)
?
v?t P(v|?u)
n(v,t)+P(u)
?
v?t P(v|u)
n(v,t)
(3)
where n(w, t) is the number of times word w ap-
pears in t, P(u) is the probability that a sentence
contains the word u andP(v|?u) is the probability
that a sentence NOT containing u contains v. In
order to reduce data size and to account for zero
probabilities we applied smoothing and informa-
tion gain based feature selection on the data prior
to running the model. The co-occurrence prob-
abilities were collected from sentences from the
Reuters corpus in a similar manner to the align-
ment model.
4.5 Combined Model
The WordNet and Bayesian models are derived
from quite different motivations. One would ex-
pect the WordNet model to be better in identify-
ing the word-to-word explicit reference examples
while the Bayesian model is expected to model the
contextualy implied references. For this reason we
tried to combine forces by evaluating a na??ve linear
176
interpolation of the two models (by simply averag-
ing the score of the two models). This model have
not been previously suggested and to the best of
our knowledge this type of combination is novel.
5 Empirical Evaluation and Analysis
5.1 Results
In order to evaluate the scores produced by the
various models as a potential component in an en-
tailment system we compared the recall-precision
graphs. In addition we compared the average pre-
cision which is a single number measure equiv-
alent to the area under an uninterpolated recall-
precision curve and is commonly used to evaluate
a systems ranking ability (Voorhees and Harman,
1999). On our dataset an average precision greater
than 0.65 is better than chance at the 0.05 level
and an average precision greater than 0.66 is sig-
nificant at the 0.01 level.
Figure 1 compares the average precision and
recall-precision results for the various models. As
can be seen, the combined wordnet+bayes model
performs best. In terms of average precision,
the similarity and wordnet models are comparable
and are slightly better than bayes. The alignment
model, however, is not significantly better than
random guessing. The recall-precision figure indi-
cates that the baysian model succeeds to rank quite
well both within the the positively scored wordnet
examples and within the negatively scored word-
net examples and thus resulting in improved av-
erage precision of the combined model. A better
understanding of the systems? performance is evi-
dent from the following analysis.
5.2 Analysis
Table 3 lists a few examples from the lexical refer-
ence dataset alng with their gold-standard anno-
tation and the Bayesian model score. Manual in-
spection of the data shows that the Bayesian model
commonly assigns a low score to correct examples
which have an entailing trigger word or phrase in
the sentence but yet the context of the sentence as a
whole is not typical for the target hypothesized en-
tailed word. For example, in example 5 the entail-
ing phrase ?set in place? and in example 6 the en-
tailing word ?founder? do appear in the text how-
ever the contexts of the sentences are not typical
news domain contexts of issued or founded. An in-
teresting future work would be to change the gen-
erative story and model to account for such cases.
The WordNet model identified a matching word
in the text for 99 out of the 580 examples. This
corresponds to a somewhat low recall of 25% and
a quite high precision of 90%. Table 4 lists typical
mistakes of the wordnet model. Examples 1-3 are
false positive examples in which there is a word
in the text (emphasized in the table) which is a
synonym or hyponym of the target word for some
sense in WordNet, however in the context of the
text it is not of such a sense. Examples 4-6 show
false negative examples, in which the annotators
identified a trigger word in the text (emphasized
in the table) but yet it or no other word in the text
is a synonym or hyponym of the target word.
5.3 Subcategory analysis
word phrase context false
word 178 16 59 32
phrase 4 12 9 4
context 15 5 56 25
false 24 5 38 226
Table 5: inter-annotator confusion matrix for the
auxiliary annotation.
As seen above, the combined model outper-
forms the others since it identifies both word-
to-word lexical reference as well as context-to-
word lexical reference. These are quite different
cases. We asked the annotators to state the sub-
category when they annotated an example as true
(as described in the annotation guidelines in Sec-
tion 3.2.1). The Word subcategory corresponds
to a word-to-word match and Phrase and Context
subcategories correspond to more than one word
to word match. As can be expected, the agreement
on such a task resulted in a lower Kappa of 0.5
which corresponds to moderate agreement (Landis
and Koch, 1997). the confusion matrix between
the two annotators is presented in Table 5. This de-
composition enables the evaluation of the strength
and weakness of different lexical reference mod-
ules, free from the context of the bigger entailment
system.
We used the subcategories dataset to test the
performances of the different models. Table 6
lists for each subcategory the recall of correctly
identified examples for each model?s 25% recall
level. The table shows that the wordnet and simi-
larity models? strength is in identifying examples
where lexical reference is triggered by a dominant
word in the sentence. The bayes model, however,
177
Figure 1: comparison of average precision (left) and recall-precision (right) results for the various models
id text token annotation score
1 QNX Software Systems Ltd., a leading provider of real-time software and ser-
vices to the embedded computing market, is pleased to announce the appoint-
ment of Mr. Sachin Lawande to the position of vice president, engineering ser-
vices.
named PHRASE 0.98
2 NIH?s FY05 budget request of $28.8 billion includes $2 billion for the National
Institute of General Medical Sciences, a 3.4-percent increase, and $1.1 billion
for the National Center for Research Resources, and a 7.2-percent decrease from
FY04 levels.
reduced WORD 0.91
3 Pakistani officials announced that two South African men in their custody had
confessed to planning attacks at popular tourist spots in their home country.
security CONTEXT 0.80
4 With $549 million in cash as of June 30, Google can easily afford to make
amends.
shares FALSE 0.03
5 In the year 538, Cyrus set in place a policy which demanded the return of the
various gods to their proper places.
issued PHRASE 7e-4
6 The black Muslim activist said that he had relieved Muhammad of his duties
?until he demonstrates that he is willing to conform to the manner of representing
Allah and the honorable Elijah Muhammad (founder of the Nation of Islam)?.
founded WORD 3e-6
Table 3: A sample from the lexical reference dataset alng with the Bayesian model?s score
id text token annotation
1 Kerry hit Bush hard on his conduct on the war in Iraq shot FALSE
2 Pakistani officials announced that two South African men in their custody had confessed to
planning attacks at popular tourist spots in their home country
forces FALSE
3 It would help the economy by putting people back to work and more money in the hands of
consumers
get FALSE
4 Eating lots of foods that are a good source of fiber may keep your blood glucose from rising
too fast after you eat
sugar WORD
5 Hippos do come into conflict with people quite often human WORD
6 Weinstock painstakingly reviewed dozens of studies for evidence of any link between sun-
screen use and either an increase or decrease in melanoma
cancer WORD
Table 4: A few erroneous examples of WordNet model
is better at identifying phrase and context exam-
ples. The combined WordNet and Bayesian mod-
els? strength can be explained by the quite dif-
ferent behaviors of the two models - the Word-
Net model seems to be better in identifying the
word-to-word explicit reference examples while
the Bayesian model is better in modeling the con-
textual implied references.
6 Conclusions
This paper proposed an explicit task definition for
lexical reference. This task captures directly the
goal of common lexical matching models, which
typically operate within more complex systems
178
method word disagreement phrase/context
wordnet 38% 9% 17%
similarity 39% 7% 17%
bayes 22% 21% 37%
Table 6: Breakdown of recall of correctly identi-
fied example types at an overall system?s recall of
25%. Disagreement refers to examples for which
the annotators did not agree on the subcategory an-
notation (word vs. phrase/context).
that address more complex tasks. This defini-
tion enabled us to create an annotated dataset for
the lexical reference task, which provided insights
into interesting sub-classes that require different
types of modeling. The dataset enabled us to
make a direct evaluation and comparison of lexical
matching models, reveal insightful differences be-
tween them, and create a simple improved model
combination. In the long run, we believe that
the availability of such datasets will facilitate im-
proved models that consider the various sub-cases
of lexical reference, as well as applying supervised
learning to optimize model combination and per-
formance.
References
[Bar-Haim et al2005] Roy Bar-Haim, Idan Szpecktor,
and Oren Glickman. 2005. Definition and analysis
of intermediate entailment levels. In Proceedings
of the ACL Workshop on Empirical Modeling of Se-
mantic Equivalence and Entailment, pages 55?60,
Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
[Barzilay and McKeown2001] Regina Barzilay and
Kathleen McKeown. 2001. Extracting paraphrases
from a parallel corpus. In ACL, pages 50?57.
[Bos and Markert2005] Johan Bos and Katja Markert.
2005. Recognising textual entailment with logical
inference techniques. In EMNLP.
[Corley and Mihalcea2005] Courtney Corley and Rada
Mihalcea. 2005. Measuring the semantic similarity
of texts. In Proceedings of the ACL Workshop on
Empirical Modeling of Semantic Equivalence and
Entailment, pages 13?18.
[Dagan et al2006] Ido Dagan, Oren Glickman, and
Bernardo Magnini, editors. 2006. The PASCAL
Recognising Textual Entailment Challenge, volume
3944. Lecture Notes in Computer Science.
[Frege1892] Gottlob Frege. 1892. On sense and
reference. Reprinted in P. Geach and M. Black,
eds., Translations from the Philosophical Writings
of Gottlob Frege. 1960.
[Glickman and Dagan2004] Oren Glickman and Ido
Dagan, 2004. Recent Advances in Natural Lan-
guage Processing III, chapter Acquiring lexical
paraphrases from a single corpus, pages 81?90.
John Benjamins.
[Glickman et al2005] Oren Glickman, Ido Dagan, and
Moshe Koppel. 2005. A probabilistic classification
approach for lexical textual entailment. In AAAI,
pages 1050?1055.
[Glickman et al2006] Oren Glickman, Ido Dagan, and
Moshe Koppel. 2006. A lexical alignment model
for probabilistic textual entailment, volume 3944.
In Lecture Notes in Computer Science, pages 287 ?
298. Springer.
[Harabagiu et al2000] Sanda M. Harabagiu, Dan I.
Moldovan, Marius Pasca, Rada Mihalcea, Mihai
Surdeanu, Razvan C. Bunescu, Roxana Girju, Vasile
Rus, and Paul Morarescu. 2000. Falcon: Boosting
knowledge for answer engines. In TREC.
[Hovy et al2001] Eduard H. Hovy, Ulf Hermjakob, and
Chin-Yew Lin. 2001. The use of external knowl-
edge of factoid QA. In Text REtrieval Conference.
[Jijkoun and de Rijke2005] Valentin Jijkoun and
Maarten de Rijke. 2005. Recognizing textual
entailment using lexical similarity. Proceedings of
the PASCAL Challenges Workshop on Recognising
Textual Entailment (and forthcoming LNAI book
chapter).
[Landis and Koch1997] J. R. Landis and G. G. Koch.
1997. The measurements of observer agreement for
categorical data. Biometrics, 33:159?174.
[Leacock et al1998] Claudia Leacock, George A.
Miller, and Martin Chodorow. 1998. Using
corpus statistics and wordnet relations for sense
identification. Comput. Linguist., 24(1):147?165.
[Lin and Pantel2001] Dekang Lin and Patrik Pantel.
2001. Discovery of inference rules for question an-
swering. Natural Language Engineering, 4(7):343?
360.
[Lin1998] Dekang Lin. 1998. Automatic retrieval and
clustering of similar words. In Proceedings of the
17th international conference on Computational lin-
guistics, pages 768?774, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
[Vanderwende et al2005] Lucy Vanderwende, Deborah
Coughlin, and Bill Dolan. 2005. What syntax
can contribute in entailment task. Proceedings of
the PASCAL Challenges Workshop on Recognising
Textual Entailment.
[Voorhees and Harman1999] Ellen M. Voorhees and
Donna Harman. 1999. Overview of the seventh text
retrieval conference. In Proceedings of the Seventh
Text REtrieval Conference (TREC-7). NIST Special
Publication.
179
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 45?52, New York City, June 2006. c?2006 Association for Computational Linguistics
Investigating Lexical Substitution Scoring for Subtitle Generation
Oren Glickman and Ido Dagan
Computer Science Department
Bar Ilan University
Ramat Gan, Israel
{glikmao,dagan}@cs.biu.ac.il
Mikaela Keller and Samy Bengio
IDIAP Research Institute
Martigny,
Switzerland
{mkeller,bengio}@idiap.ch
Walter Daelemans
CNTS
Antwerp, Belgium
walter.daelemans@ua.ac.be
Abstract
This paper investigates an isolated setting
of the lexical substitution task of replac-
ing words with their synonyms. In par-
ticular, we examine this problem in the
setting of subtitle generation and evaluate
state of the art scoring methods that pre-
dict the validity of a given substitution.
The paper evaluates two context indepen-
dent models and two contextual models.
The major findings suggest that distribu-
tional similarity provides a useful comple-
mentary estimate for the likelihood that
two Wordnet synonyms are indeed substi-
tutable, while proper modeling of contex-
tual constraints is still a challenging task
for future research.
1 Introduction
Lexical substitution - the task of replacing a word
with another one that conveys the same meaning -
is a prominent task in many Natural Language Pro-
cessing (NLP) applications. For example, in query
expansion for information retrieval a query is aug-
mented with synonyms of the original query words,
aiming to retrieve documents that contain these syn-
onyms (Voorhees, 1994). Similarly, lexical substi-
tutions are applied in question answering to identify
answer passages that express the sought answer in
different terms than the original question. In natu-
ral language generation it is common to seek lex-
ical alternatives for the same meaning in order to
reduce lexical repetitions. In general, lexical sub-
stitution aims to preserve a desired meaning while
coping with the lexical variability of expressing that
meaning. Lexical substitution can thus be viewed
within the general framework of recognizing entail-
ment between text segments (Dagan et al, 2005), as
modeling entailment relations at the lexical level.
In this paper we examine the lexical substitu-
tion problem within a specific setting of text com-
pression for subtitle generation (Daelemans et al,
2004). Subtitle generation is the task of generat-
ing target language TV subtitles for video recordings
of a source language speech. The subtitles should
be of restricted length, which is often shorter than
the full translation of the original speech, yet they
should maintain as much as possible the meaning
of the original content. In a typical (automated)
subtitling process the original speech is first trans-
lated fully into the target language and then the tar-
get translation is compressed to optimize the length
requirements. One of the techniques employed in
the text compression phase is to replace a target lan-
guage word in the original translation with a shorter
synonym of it, thus reducing the character length of
the subtitle. This is a typical lexical substitution
task, which resembles similar operations in other
text compression and generation tasks (e.g. (Knight
and Marcu, 2002)).
This paper investigates the task of assigning like-
lihood scores for the correctness of such lexical sub-
stitutions, in which words in the original translation
are replaced with shorter synonyms. In our experi-
ments we use WordNet as a source of candidate syn-
onyms for substitution. The goal is to score the like-
lihood that the substitution is admissible, i.e. yield-
ing a valid sentence that preserves the original mean-
ing. The focus of this paper is thus to utilize the
subtitling setting in order to investigate lexical sub-
45
stitution models in isolation, unlike most previous
literature in which this sub-task has been embedded
in larger systems and was not evaluated directly.
We examine four statistical scoring models, of
two types. Context independent models score the
general likelihood that the original word is ?replace-
able? with the candidate synonym, in an arbitrary
context. That is, trying to filter relatively bizarre
synonyms, often of rare senses, which are abundant
in WordNet but are unlikely to yield valid substitu-
tions. Contextual models score the ?fitness? of the
replacing word within the context of the sentence, in
order to filter out synonyms of senses of the original
word that are not the right sense in the given context.
We set up an experiment using actual subti-
tling data and human judgements and evaluate the
different scoring methods. Our findings suggest
the dominance, in this setting, of generic context-
independent scoring. In particular, considering dis-
tributional similarity amongst WordNet synonyms
seems effective for identifying candidate substitu-
tions that are indeed likely to be applicable in actual
texts. Thus, while distributional similarity alone is
known to be too noisy as a sole basis for meaning-
preserving substitutions, its combination withWord-
Net alows reducing the noise caused by the many
WordNet synonyms that are unlikely to correspond
to valid substitutions.
2 Background and Setting
2.1 Subtitling
Automatic generation of subtitles is a summariza-
tion task at the level of individual sentences or occa-
sionally of a few contiguous sentences. Limitations
on reading speed of viewers and on the size of the
screen that can be filled with text without the image
becoming too cluttered, are the constraints that dy-
namically determine the amount of compression in
characters that should be achieved in transforming
the transcript into subtitles. Subtitling is not a trivial
task, and is expensive and time-consuming when ex-
perts have to carry it out manually. As for other NLP
tasks, both statistical (machine learning) and linguis-
tic knowledge-based techniques have been consid-
ered for this problem. Examples of the former are
(Knight and Marcu, 2002; Hori et al, 2002), and of
the latter are (Grefenstette, 1998; Jing and McKe-
own, 1999). A comparison of both approaches in
the context of a Dutch subtitling system is provided
in (Daelemans et al, 2004). The required sentence
simplification is achieved either by deleting mate-
rial, or by paraphrasing parts of the sentence into
shorter expressions with the same meaning. As a
special case of the latter, lexical substitution is often
used to achieve a compression target by substituting
a word by a shorter synonym. It is on this subtask
that we focus in this paper. Table 1 provides a few
examples. E.g. by substituting ?happen? by ?occur?
(example 3), one character is saved without affecting
the sentence meaning .
2.2 Experimental Setting
The data used in our experiments was collected in
the context of the MUSA (Multilingual Subtitling of
Multimedia Content) project (Piperidis et al, 2004)1
and was kindly provided for the current study. The
data was provided by the BBC in the form of Hori-
zon documentary transcripts with the corresponding
audio and video. The data for two documentaries
was used to create a dataset consisting of sentences
from the transcripts and the corresponding substitu-
tion examples in which selected words are substi-
tuted by a shorter Wordnet synonym. More con-
cretely, a substitution example thus consists of an
original sentence s = w1 . . . wi . . . wn, a specific
source word wi in the sentence and a target (shorter)
WordNet synonym w? to substitute the source. See
Table 1 for examples. The dataset consists of 918
substitution examples originating from 231 different
sentences.
An annotation environment was developed to al-
low efficient annotation of the substitution examples
with the classes true (admissible substitution, in the
given context) or false (inadmissible substitution).
About 40% of the examples were judged as true.
Part of the data was annotated by an additional an-
notator to compute annotator agreement. The Kappa
score turned out to be 0.65, corresponding to ?Sub-
stantial Agreement? (Landis and Koch, 1997). Since
some of the methods we are comparing need tuning
we held out a random subset of 31 original sentences
(with 121 corresponding examples) for development
and kept for testing the resulting 797 substitution ex-
1http://sinfos.ilsp.gr/musa/
46
id sentence source target judgment
1 The answer may be found in the behaviour of animals. answer reply false
2 . . . and the answer to that was - Yes answer reply true
3
We then wanted to know what would happen if
we delay the movement of the subject?s left hand
happen occur true
4 subject topic false
5 subject theme false
6 people weren?t laughing they were going stone sober. stone rock false
7 if we can identify a place where the seizures are coming from then we can go in
and remove just that small area.
identify place false
8 my approach has been the first to look at the actual structure of the laugh sound. approach attack false
9 He quickly ran into an unexpected problem. problem job false
10 today American children consume 5 times more Ritalin than the rest of the world
combined
consume devour false
Table 1: Substitution examples from the dataset alng with their annotations
amples from the remaining 200 sentences.
3 Compared Scoring Models
We compare methods for scoring lexical substitu-
tions. These methods assign a score which is ex-
pected to correspond to the likelihood that the syn-
onym substitution results in a valid subtitle which
preserves the main meaning of the original sentence.
We examine four statistical scoring models, of
two types. The context independent models score
the general likelihood that the source word can be
replaced with the target synonym regardless of the
context in which the word appears. Contextual mod-
els, on the other hand, score the fitness of the target
word within the given context.
3.1 Context Independent Models
Even though synonyms are substitutable in theory,
in practice there are many rare synonyms for which
the likelihood of substitution is very low and will be
substitutable only in obscure contexts. For exam-
ple, although there are contexts in which the word
job is a synonym of the word problem2, this is not
typically the case and overall job is not a good tar-
get substitution for the source problem (see example
9 in Table 1). For this reason synonym thesauruses
such as WordNet tend to be rather noisy for practi-
cal purposes, raising the need to score such synonym
substitutions and accordingly prioritize substitutions
that are more likely to be valid in an arbitrary con-
text.
2WordNet lists job as a possible member of the synset for a
state of difficulty that needs to be resolved, as might be used in
sentences like ?it is always a job to contact him?
As representative approaches for addressing this
problem, we chose two methods that rely on statisti-
cal information of two types: supervised sense dis-
tributions from SemCor and unsupervised distribu-
tional similarity.
3.1.1 WordNet based Sense Frequencies
(semcor)
The obvious reason that a target synonym cannot
substitute a source in some context is if the source
appears in a different sense than the one in which
it is synonymous with the target. This means that a
priori, synonyms of frequent senses of a source word
are more likely to provide correct substitutions than
synonyms of the word?s infrequent senses.
To estimate such likelihood, our first measure is
based on sense frequencies from SemCor (Miller et
al., 1993), a corpus annotated with Wordnet senses.
For a given source word u and target synonym v the
score is calculated as the percentage of occurrences
of u in SemCor for which the annotated synset con-
tains v (i.e. u?s occurrences in which its sense is
synonymous with v). This corresponds to the prior
probability estimate that an occurrence of u (in an
arbitrary context) is actually a synonym of v. There-
fore it is suitable as a prior score for lexical substi-
tution.3
3.1.2 Distributional Similarity (sim)
The SemCor based method relies on a supervised
approach and requires a sense annotated corpus. Our
3Note that WordNet semantic distance measures such as
those compared in (Budanitsky and Hirst, 2001) are not appli-
cable here since they measure similarity between synsets rather
than between synonymous words within a single synset.
47
second method uses an unsupervised distributional
similarity measure to score synonym substitutions.
Such measures are based on the general idea of
Harris? Distributional Hypothesis, suggesting that
words that occur within similar contexts are seman-
tically similar (Harris, 1968).
As a representative of this approach we use Lin?s
dependency-based distributional similarity database.
Lin?s database was created using the particular dis-
tributional similarity measure in (Lin, 1998), applied
to a large corpus of news data (64 million words) 4.
Two words obtain a high similarity score if they oc-
cur often in the same contexts, as captured by syn-
tactic dependency relations. For example, two verbs
will be considered similar if they have large common
sets of modifying subjects, objects, adverbs etc.
Distributional similarity does not capture directly
meaning equivalence and entailment but rather a
looser notion of meaning similarity (Geffet and Da-
gan, 2005). It is typical that non substitutable words
such as antonyms or co-hyponyms obtain high sim-
ilarity scores. However, in our setting we apply
the similarity score only for WordNet synonyms in
which it is known a priori that they are substitutable
is some contexts. Distributional similarity may thus
capture the statistical degree to which the two words
are substitutable in practice. In fact, it has been
shown that prominence in similarity score corre-
sponds to sense frequency, which was suggested as
the basis for an unsupervised method for identifying
the most frequent sense of a word (McCarthy et al,
2004).
3.2 Contextual Models
Contextual models score lexical substitutions based
on the context of the sentence. Such models
try to estimate the likelihood that the target word
could potentially occur in the given context of the
source word and thus may replace it. More con-
cretely, for a given substitution example consist-
ing of an original sentence s = w1 . . . wi . . . wn,
and a designated source word wi, the contextual
models we consider assign a score to the substi-
tution based solely on the target synonym v and
the context of the source word in the original sen-
4available at http://www.cs.ualberta.ca/
?lindek/downloads.htm
tence, {w1, . . . , wi?1, wi+1, . . . , wn}, which is rep-
resented in a bag-of-words format.
Apparently, this setting was not investigated much
in the context of lexical substitution in the NLP lit-
erature. We chose to evaluate two recently proposed
models that address exactly the task at hand: the first
model was proposed in the context of lexical model-
ing of textual entailment, using a generative Na??ve
Bayes approach; the second model was proposed
in the context of machine learning for information
retrieval, using a discriminative neural network ap-
proach. The two models were trained on the (un-
annotated) sentences of the BNC 100 million word
corpus (Burnard, 1995) in bag-of-words format. The
corpus was broken into sentences, tokenized, lem-
matized and stop words and tokens appearing only
once were removed. While training of these models
is done in an unsupervised manner, using unlabeled
data, some parameter tuning was performed using
the small development set described in Section 2.
3.2.1 Bayesian Model (bayes)
The first contextual model we examine is the one
proposed in (Glickman et al, 2005) to model tex-
tual entailment at the lexical level. For a given tar-
get word this unsupervised model takes a binary text
categorization approach. Each vocabulary word is
considered a class, and contexts are classified as to
whether the given target word is likely to occur in
them. Taking a probabilistic Na??ve-Bayes approach
the model estimates the conditional probability of
the target word given the context based on corpus co-
occurrence statistics. We adapted and implemented
this algorithm and trained the model on the sen-
tences of the BNC corpus.
For a bag-of-words context C =
{w1, . . . , wi?1, wi+1, . . . , wn} and target word
v the Na??ve Bayes probability estimation for the
conditional probability of a word v may occur in a
given a context C is as follows:
P(v|C) =
P(C|v) P(v)
P(C|v) P(v)+P(C|?v) P(?v)
?
P(v)
?
w?C P(w|v)
P(v)
?
w?C P(w|v)+P(?v)
?
w?C P(w|?v)
(1)
where P(w|v) is the probability that a word w ap-
pears in the context of a sentence containing v and
correspondingly P(w|?v) is the probability that w
48
appears in a sentence not containing v. The prob-
ability estimates were obtained from the processed
BNC corpus as follows:
P(w|v) =
|w appears in sentences containing v|
|words in sentences containing v|
P(w|?v) =
|w occurs in sentences not containing v|
|words in sentences not containing v|
To avoid 0 probabilities these estimates were
smoothed by adding a small constant to all counts
and normalizing accordingly. The constant value
was tuned using the development set to maximize
average precision (see Section 4.1). The estimated
probability, P(v|C), was used as the confidence
score for each substitution example.
3.2.2 Neural Network Model (nntr)
As a second contextual model we evaluated the
Neural Network for Text Representation (NNTR)
proposed in (Keller and Bengio, 2005). NNTR is
a discriminative approach which aims at modeling
how likely a given word v is in the context of a piece
of text C, while learning a more compact represen-
tation of reduced dimensionality for both v and C.
NNTR is composed of 3 Multilayer Perceptrons,
noted mlpA(), mlpB() and mlpC(), connected as
follow:
NNTR(v, C) = mlpC [mlpA(v),mlpB(C)].
mlpA(v) and mlpB(C) project respectively the
vector space representation of the word and text
into a more compact space of lower dimensionality.
mlpC() takes as input the new representations of v
and C and outputs a score for the contextual rele-
vance of v to C.
As training data, couples (v,C) from the BNC cor-
pus are provided to the learning scheme. The target
training value for the output of the system is 1 if v is
indeed in C and -1 otherwise. The hope is that the
neural network will be able to generalize to words
which are not in the piece of text but are likely to be
related to it.
In essence, this model is trained by minimizing
the weighted sum of the hinge loss function over
negative and positive couples, using stochastic Gra-
dient Descent (see (Keller and Bengio, 2005) for fur-
ther details). The small held out development set of
the substitution dataset was used to tune the hyper-
parameters of the model, maximizing average preci-
sion (see Section 4.1). For simplicity mlpA() and
mlpB() were reduced to Perceptrons. The output
size of mlpA() was set to 20, mlpB() to 100 and the
number of hidden units of mlpC() was set to 500.
There are a couple of important conceptual differ-
ences of the discriminative NNTR model compared
to the generative Bayesian model described above.
First, the relevancy of v to C in NNTR is inferred
in a more compact representation space of reduced
dimensionality, which may enable a higher degree
of generalization. Second, in NNTR we are able to
control the capacity of the model in terms of num-
ber of parameters, enabling better control to achieve
an optimal generalization level with respect to the
training data (avoiding over or under fitting).
4 Empirical Results
4.1 Evaluation Measures
We compare the lexical substitution scoring methods
using two evaluation measures, offering two differ-
ent perspectives of evaluation.
4.1.1 Accuracy
The first evaluation measure is motivated by simu-
lating a decision step of a subtitling system, in which
the best scoring lexical substitution is selected for
each given sentence. Such decision may correspond
to a situation in which each single substitution may
suffice to obtain the desired compression rate, or
might be part of a more complex decision mecha-
nism of the complete subtitling system. We thus
measure the resulting accuracy of subtitles created
by applying the best scoring substitution example
for every original sentence. This provides a macro
evaluation style since we obtain a single judgment
for each group of substitution examples that corre-
spond to one original sentence.
In our dataset 25.5% of the original sentences
have no correct substitution examples and for 15.5%
of the sentences all substitution examples were an-
notated as correct. Accordingly, the (macro aver-
aged) accuracy has a lower bound of 0.155 and up-
per bound of 0.745.
49
4.1.2 Average Precision
As a second evaluation measure we compare the
average precision of each method over all the exam-
ples from all original sentences pooled together (a
micro averaging approach). This measures the po-
tential of a scoring method to ensure high precision
for the high scoring examples and to filter out low-
scoring incorrect substitutions.
Average precision is a single figure measure com-
monly used to evaluate a system?s ranking ability
(Voorhees and Harman, 1999). It is equivalent to the
area under the uninterpolated recall-precision curve,
defined as follows:
average precision =
?N
i=1 P(i)T (i)?N
i=1
T (i)
P(i) =
?i
k=1
T (k)
i
(2)
where N is the number of examples in the test
set (797 in our case), T (i) is the gold annotation
(true=1, false=0) and i ranges over the examples
ranked by decreasing score. An average precision
of 1.0 means that the system assigned a higher score
to all true examples than to any false one (perfect
ranking). A lower bound of 0.26 on our test set cor-
responds to a system that ranks all false examples
above the true ones.
4.2 Results
Figure 1 shows the accuracy and average precision
results of the various models on our test set. The ran-
dom baseline and corresponding significance levels
were achieved by averaging multiple runs of a sys-
tem that assigned random scores. As can be seen in
the figures, the models? behavior seems to be con-
sistent in both evaluation measures.
Overall, the distributional similarity based
method (sim) performs much better than the
other methods. In particular, Lin?s similarity
also performs better than semcor, the other
context-independent model. Generally, the context
independent models perform better than the contex-
tual ones. Between the two contextual models, nntr
is superior to Bayes. In fact the Bayes model is not
significantly better than random scoring.
4.3 Analysis and Discussion
When analyzing the data we identified several rea-
sons why some of the WordNet substitutions were
judged as false. In some cases the source word as
appearing in the original sentence is not in a sense
for which it is a synonym of the target word. For ex-
ample, in many situations the word answer is in the
sense of a statement that is made in reply to a ques-
tion or request. In such cases, such as in example 2
from Table 1, answer can be successfully replaced
with reply yielding a substitution which conveys the
original meaning. However, in situations such as in
example 1 the word answer is in the sense of a gen-
eral solution and cannot be replaced with reply. This
is also the case in examples 4 and 5 in which subject
does not appear in the sense of topic or theme.
Having an inappropriate sense, however, is not the
only reason for incorrect substitutions. In example 8
approach appears in a sense which is synonymous
with attack and in example 9 problem appears in a
sense which is synonymous with a quite uncommon
use of the word job. Nevertheless, these substitu-
tions were judged as unacceptable since the desired
sense of the target word after the substitution is not
very clear from the context. In many other cases,
such as in example 7, though semantically correct,
the substitution was judged as incorrect due to stylis-
tic considerations.
Finally, there are cases, such as in example 6
in which the source word is part of a collocation
and cannot be replaced with semantically equivalent
words.
When analyzing the mistakes of the distributional
similarity method it seems as if many were not nec-
essarily due to the method itself but rather to imple-
mentation issues. The online source we used con-
tains only the top most similar words for any word.
In many cases substitutions were assigned a score of
zero since they were not listed among the top scoring
similar words in the database. Furthermore, the cor-
pus that was used for training the similarity scores
was news articles in American English spelling and
does not always supply good scores to words of
British spelling in our BBC dataset (e.g. analyse,
behavioural, etc.).
The similarity based method seems to perform
better than the SemCor based method since, as noted
above, even when the source word is in the appro-
priate sense it not necessarily substitutable with the
target. For this reason we hypothesize that apply-
ing Word Sense Disambiguation (WSD) methods to
50
Figure 1: Accuracy and Average Precision Results
classify the specificWordNet sense of the source and
target words may have only a limited impact on per-
formance.
Overall, context independent models seem to per-
form relatively well since many candidate synonyms
are a priori not substitutable. This demonstrates that
such models are able to filter out many quirky Word-
Net synonyms, such as problem and job.
Fitness to the sentence context seems to be a less
frequent factor and not that trivial to model. Local
context (adjacent words) seems to play more of a
role than the broader sentence context. However,
these two types of contexts were not distinguished in
the bag-of-words representations of the two contex-
tual methods that we examined. It will be interesting
to investigate in future research using different fea-
ture types for local and global context, as commonly
done for Word Sense Disambiguation (WSD). Yet,
it would still remain a challenging task to correctly
distinguish, for example, the contexts for which an-
swer is substitutable by reply (as in example 2) from
contexts in which it is not (as in example 1).
So far we have investigated separately the perfor-
mance of context independent and contextual mod-
els. In fact, the accuracy performance of the (con-
text independent) sim method is not that far from
the upper bound, and the analysis above indicated a
rather small potential for improvement by incorpo-
rating information from a contextual method. Yet,
there is still a substantial room for improvement in
the ranking quality of this model, as measured by av-
erage precision, and it is possible that a smart com-
bination with a high-quality contextual model would
yield better performance. In particular, we would
expect that a good contextual model will identify the
cases in which for potentially good synonyms pair,
the source word appears in a sense that is not substi-
tutable with the target, such as in examples 1, 4 and
5 in Table 1. Investigating better contextual models
and their optimal combination with context indepen-
dent models remains a topic for future research.
5 Conclusion
This paper investigated an isolated setting of the lex-
ical substitution task, which has typically been em-
bedded in larger systems and not evaluated directly.
The setting allowed us to analyze different types of
state of the art models and their behavior with re-
spect to characteristic sub-cases of the problem.
The major conclusion that seems to arise from
our experiments is the effectiveness of combining a
knowledge based thesaurus such as WordNet with
distributional statistical information such as (Lin,
1998), overcoming the known deficiencies of each
method alone. Furthermore, modeling the a pri-
ori substitution likelihood captures the majority of
cases in the evaluated setting, mostly because Word-
Net provides a rather noisy set of substitution candi-
dates. On the other hand, successfully incorporating
local and global contextual information, as similar
to WSD methods, remains a challenging task for fu-
ture research. Overall, scoring lexical substitutions
51
is an important component in many applications and
we expect that our findings are likely to be broadly
applicable.
References
[Budanitsky and Hirst2001] Alexander Budanitsky and
Graeme Hirst. 2001. Semantic distance in word-
net: An experimental, application-oriented evalua-
tion of five measures. In Workshop on WordNet and
Other Lexical Resources: Second Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 29?34.
[Burnard1995] Lou Burnard. 1995. Users Reference
Guide for the British National Corpus. Oxford Uni-
versity Computing Services, Oxford.
[Daelemans et al2004] Walter Daelemans, Anja Ho?thker,
and Erik Tjong Kim Sang. 2004. Automatic sen-
tence simplification for subtitling in dutch and english.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation, pages 1045?
1048.
[Dagan et al2005] Ido Dagan, Oren Glickman, and
Bernardo Magnini. 2005. The pascal recognising tex-
tual entailment challenge. Proceedings of the PAS-
CAL Challenges Workshop on Recognising Textual
Entailment.
[Geffet and Dagan2005] Maayan Geffet and Ido Dagan.
2005. The distributional inclusion hypotheses and lex-
ical entailment. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 107?114, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
[Glickman et al2005] Oren Glickman, Ido Dagan, and
Moshe Koppel. 2005. A probabilistic classifica-
tion approach for lexical textual entailment. In AAAI,
pages 1050?1055.
[Grefenstette1998] Gregory Grefenstette. 1998. Produc-
ing Intelligent Telegraphic Text Reduction to Provide
an Audio Scanning Service for the Blind. pages 111?
117, Stanford, CA, March.
[Harris1968] Zelig Harris. 1968. Mathematical Struc-
tures of Language. New York: Wiley.
[Hori et al2002] Chiori Hori, Sadaoki Furui, RobMalkin,
Hua Yu, and Alex Waibel. 2002. Automatic
speech summarization applied to english broadcast
news speech. volume 1, pages 9?12.
[Jing and McKeown1999] Hongyan Jing and Kathleen R.
McKeown. 1999. The decomposition of human-
written summary sentences. In SIGIR ?99: Proceed-
ings of the 22nd annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 129?136, New York, NY, USA. ACM
Press.
[Keller and Bengio2005] Mikaela Keller and Samy Ben-
gio. 2005. A neural network for text representation.
In Wodzisaw Duch, Janusz Kacprzyk, and Erkki Oja,
editors, Artificial Neural Networks: Biological Inspi-
rations ICANN 2005: 15th International Conference,
Warsaw, Poland, September 11-15, 2005. Proceedings,
Part II, volume 3697 / 2005 of Lecture Notes in Com-
puter Science, page p. 667. Springer-Verlag GmbH.
[Knight and Marcu2002] Kevin Knight and Daniel
Marcu. 2002. Summarization beyond sentence
extraction: a probabilistic approach to sentence
compression. Artif. Intell., 139(1):91?107.
[Landis and Koch1997] J. R. Landis and G. G. Koch.
1997. The measurements of observer agreement for
categorical data. Biometrics, 33:159?174.
[Lin1998] Dekang Lin. 1998. Automatic retrieval and
clustering of similar words. In Proceedings of the
17th international conference on Computational lin-
guistics, pages 768?774, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
[McCarthy et al2004] Diana McCarthy, Rob Koeling,
JulieWeeds, and John Carroll. 2004. Finding predom-
inant senses in untagged text. In ACL, pages 280?288,
Morristown, NJ, USA. Association for Computational
Linguistics.
[Miller et al1993] George A. Miller, Claudia Leacock,
Randee Tengi, and Ross T. Bunker. 1993. A semantic
concordance. In HLT ?93: Proceedings of the work-
shop on Human Language Technology, pages 303?
308, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
[Piperidis et al2004] Stelios Piperidis, Iason Demiros,
Prokopis Prokopidis, Peter Vanroose, Anja Ho?thker,
Walter Daelemans, Elsa Sklavounou, Manos Kon-
stantinou, and Yannis Karavidas. 2004. Multimodal
multilingual resources in the subtitling process. In
Proceedings of the 4th International Language Re-
sources and Evaluation Conference (LREC 2004), Lis-
bon.
[Voorhees and Harman1999] Ellen M. Voorhees and
Donna Harman. 1999. Overview of the seventh text
retrieval conference. In Proceedings of the Seventh
Text REtrieval Conference (TREC-7). NIST Special
Publication.
[Voorhees1994] Ellen M. Voorhees. 1994. Query expan-
sion using lexical-semantic relations. In SIGIR ?94:
Proceedings of the 17th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, pages 61?69, New York, NY, USA.
Springer-Verlag New York, Inc.
52
