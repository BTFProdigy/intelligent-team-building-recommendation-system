Proceedings of the Workshop on Statistical Machine Translation, pages 102?121,
New York City, June 2006. c?2006 Association for Computational Linguistics
Manual and Automatic Evaluation of Machine Translation
between European Languages
Philipp Koehn
School of Informatics
University of Edinburgh
pkoehn@inf.ed.ac.uk
Christof Monz
Department of Computer Science
Queen Mary, University of London
christof@dcs.qmul.ac.uk
Abstract
We evaluated machine translation perfor-
mance for six European language pairs
that participated in a shared task: translat-
ing French, German, Spanish texts to En-
glish and back. Evaluation was done auto-
matically using the BLEU score and man-
ually on fluency and adequacy.
For the 2006 NAACL/HLT Workshop on Ma-
chine Translation, we organized a shared task to
evaluate machine translation performance. 14 teams
from 11 institutions participated, ranging from com-
mercial companies, industrial research labs to indi-
vidual graduate students.
The motivation for such a competition is to estab-
lish baseline performance numbers for defined train-
ing scenarios and test sets. We assembled various
forms of data and resources: a baseline MT system,
language models, prepared training and test sets,
resulting in actual machine translation output from
several state-of-the-art systems and manual evalua-
tions. All this is available at the workshop website1.
The shared task is a follow-up to the one we orga-
nized in the previous year, at a similar venue (Koehn
and Monz, 2005). As then, we concentrated on the
translation of European languages and the use of the
Europarl corpus for training. Again, most systems
that participated could be categorized as statistical
phrase-based systems. While there is now a num-
ber of competitions ? DARPA/NIST (Li, 2005),
IWSLT (Eck and Hori, 2005), TC-Star ? this one
focuses on text translation between various Euro-
pean languages.
This year?s shared task changed in some aspects
from last year?s:
? We carried out a manual evaluation in addition
to the automatic scoring. Manual evaluation
1http://www.statmt.org/wmt06/
was done by the participants. This revealed
interesting clues about the properties of auto-
matic and manual scoring.
? We evaluated translation from English, in ad-
dition to into English. English was again
paired with German, French, and Spanish.
We dropped, however, one of the languages,
Finnish, partly to keep the number of tracks
manageable, partly because we assumed that it
would be hard to find enough Finnish speakers
for the manual evaluation.
? We included an out-of-domain test set. This al-
lows us to compare machine translation perfor-
mance in-domain and out-of-domain.
1 Evaluation Framework
The evaluation framework for the shared task is sim-
ilar to the one used in last year?s shared task. Train-
ing and testing is based on the Europarl corpus. Fig-
ure 1 provides some statistics about this corpus.
1.1 Baseline system
To lower the barrier of entrance to the competition,
we provided a complete baseline MT system, along
with data resources. To summarize, we provided:
? sentence-aligned, tokenized training corpus
? a development and development test set
? trained language models for each language
? the phrase-based MT decoder Pharaoh
? a training script to build models for Pharaoh
The performance of the baseline system is simi-
lar to the best submissions in last year?s shared task.
We are currently working on a complete open source
implementation of a training and decoding system,
which should become available over the summer.
102
Training corpus
Spanish ? English French ? English German ? English
Sentences 730,740 688,031 751,088
Foreign words 15,676,710 15,323,737 15,256,793
English words 15,222,105 13,808,104 16,052,269
Distinct foreign words 102,886 80,349 195,291
Distinct English words 64,123 61,627 65,889
Language model data
English Spanish French German
Sentence 1,003,349 1,070,305 1,066,974 1,078,141
Words 27,493,499 29,129,720 31,604,879 26,562,167
In-domain test set
English Spanish French German
Sentences 2,000
Words 59,307 61,824 66,783 55,533
Unseen words 141 206 164 387
Ratio of unseen words 0.23% 0.40% 0.24% 0.70%
Distinct words 6,031 7,719 7,230 8,812
Distinct unseen words 139 203 163 385
Out-of-domain test set
English Spanish French German
Sentences 1,064
Words 25,919 29,826 31,937 26,818
Unseen words 464 368 839 913
Ratio of unseen words 1.79% 1.23% 2.62% 3.40%
Distinct words 5,166 5,689 5,728 6,594
Distinct unseen words 340 267 375 637
Figure 1: Properties of the training and test sets used in the shared task. The training data is the Europarl cor-
pus, from which also the in-domain test set is taken. There is twice as much language modelling data, since
training data for the machine translation system is filtered against sentences of length larger than 40 words.
Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.
103
ID Participant
cmu Carnegie Mellon University, USA (Zollmann and Venugopal, 2006)
lcc Language Computer Corporation, USA (Olteanu et al, 2006b)
ms Microsoft, USA (Menezes et al, 2006)
nrc National Research Council, Canada (Johnson et al, 2006)
ntt Nippon Telegraph and Telephone, Japan (Watanabe et al, 2006)
rali RALI, University of Montreal, Canada (Patry et al, 2006)
systran Systran, France
uedin-birch University of Edinburgh, UK ? Alexandra Birch (Birch et al, 2006)
uedin-phi University of Edinburgh, UK ? Philipp Koehn (Birch et al, 2006)
upc-jg University of Catalonia, Spain ? Jesu?s Gime?nez (Gime?nez and Ma`rquez, 2006)
upc-jmc University of Catalonia, Spain ? Josep Maria Crego (Crego et al, 2006)
upc-mr University of Catalonia, Spain ? Marta Ruiz Costa-jussa` (Costa-jussa` et al, 2006)
upv University of Valencia, Spain (Sa?nchez and Bened??, 2006)
utd University of Texas at Dallas, USA (Olteanu et al, 2006a)
Figure 2: Participants in the shared task. Not all groups participated in all translation directions.
1.2 Test Data
The test data was again drawn from a segment of
the Europarl corpus from the fourth quarter of 2000,
which is excluded from the training data. Partici-
pants were also provided with two sets of 2,000 sen-
tences of parallel text to be used for system develop-
ment and tuning.
In addition to the Europarl test set, we also col-
lected 29 editorials from the Project Syndicate web-
site2, which are published in all the four languages
of the shared task. We aligned the texts at a sen-
tence level across all four languages, resulting in
1064 sentence per language. For statistics on this
test set, refer to Figure 1.
The out-of-domain test set differs from the Eu-
roparl data in various ways. The text type are edi-
torials instead of speech transcripts. The domain is
general politics, economics and science. However, it
is also mostly political content (even if not focused
on the internal workings of the European Union) and
opinion.
1.3 Participants
We received submissions from 14 groups from 11
institutions, as listed in Figure 2. Most of these
groups follow a phrase-based statistical approach to
machine translation. Microsoft?s approach uses de-
2http://www.project-syndicate.com/
pendency trees, others use hierarchical phrase mod-
els. Systran submitted their commercial rule-based
system that was not tuned to the Europarl corpus.
About half of the participants of last year?s shared
task participated again. The other half was replaced
by other participants, so we ended up with roughly
the same number. Compared to last year?s shared
task, the participants represent more long-term re-
search efforts. This may be the sign of a maturing
research environment.
While building a machine translation system is
a serious undertaking, in future we hope to attract
more newcomers to the field by keeping the barrier
of entry as low as possible.
For more on the participating systems, please re-
fer to the respective system description in the pro-
ceedings of the workshop.
2 Automatic Evaluation
For the automatic evaluation, we used BLEU, since it
is the most established metric in the field. The BLEU
metric, as all currently proposed automatic metrics,
is occasionally suspected to be biased towards sta-
tistical systems, especially the phrase-based systems
currently in use. It rewards matches of n-gram se-
quences, but measures only at most indirectly over-
all grammatical coherence.
The BLEU score has been shown to correlate
well with human judgement, when statistical ma-
104
chine translation systems are compared (Dodding-
ton, 2002; Przybocki, 2004; Li, 2005). However, a
recent study (Callison-Burch et al, 2006), pointed
out that this correlation may not always be strong.
They demonstrated this with the comparison of sta-
tistical systems against (a) manually post-edited MT
output, and (b) a rule-based commercial system.
The development of automatic scoring methods is
an open field of research. It was our hope that this
competition, which included the manual and auto-
matic evaluation of statistical systems and one rule-
based commercial system, will give further insight
into the relation between automatic and manual eval-
uation. At the very least, we are creating a data re-
source (the manual annotations) that may the basis
of future research in evaluation metrics.
2.1 Computing BLEU Scores
We computed BLEU scores for each submission with
a single reference translation. For each sentence,
we counted how many n-grams in the system output
also occurred in the reference translation. By taking
the ratio of matching n-grams to the total number of
n-grams in the system output, we obtain the preci-
sion pn for each n-gram order n. These values for
n-gram precision are combined into a BLEU score:
BLEU = BP ? exp(
4?
n=1
log pn) (1)
BP = min(1, e1?r/c) (2)
The formula for the BLEU metric also includes a
brevity penalty for too short output, which is based
on the total number of words in the system output c
and in the reference r.
BLEU is sensitive to tokenization. Because of
this, we retokenized and lowercased submitted out-
put with our own tokenizer, which was also used to
prepare the training and test data.
2.2 Statistical Significance
Confidence Interval: Since BLEU scores are not
computed on the sentence level, traditional methods
to compute statistical significance and confidence
intervals do not apply. Hence, we use the bootstrap
resampling method described by Koehn (2004).
Following this method, we repeatedly ? say,
1000 times ? sample sets of sentences from the out-
put of each system, measure their BLEU score, and
use these 1000 BLEU scores as basis for estimating
a confidence interval. When dropping the top and
bottom 2.5% the remaining BLEU scores define the
range of the confidence interval.
Pairwise comparison: We can use the same method
to assess the statistical significance of one system
outperforming another. If two systems? scores are
close, this may simply be a random effect in the test
data. To check for this, we do pairwise bootstrap re-
sampling: Again, we repeatedly sample sets of sen-
tences, this time from both systems, and compare
their BLEU scores on these sets. If one system is bet-
ter in 95% of the sample sets, we conclude that its
higher BLEU score is statistically significantly bet-
ter.
The bootstrap method has been critized by Riezler
and Maxwell (2005) and Collins et al (2005), as be-
ing too optimistic in deciding for statistical signifi-
cant difference between systems. We are therefore
applying a different method, which has been used at
the 2005 DARPA/NIST evaluation.
We divide up each test set into blocks of 20 sen-
tences (100 blocks for the in-domain test set, 53
blocks for the out-of-domain test set), check for each
block, if one system has a higher BLEU score than
the other, and then use the sign test.
The sign test checks, how likely a sample of better
and worse BLEU scores would have been generated
by two systems of equal performance.
Let say, if we find one system doing better on 20
of the blocks, and worse on 80 of the blocks, is it
significantly worse? We check, how likely only up
to k = 20 better scores out of n = 100 would have
been generated by two equal systems, using the bi-
nomial distribution:
p(0..k;n, p) =
k?
i=0
(
i
n
)
pipn?i
= 0.5n
k?
i=0
(
i
n
) (3)
If p(0..k;n, p) < 0.05, or p(0..k;n, p) > 0.95
then we have a statistically significant difference be-
tween the systems.
105
Figure 3: Annotation tool for manual judgement of adequacy and fluency of the system output. Translations
from 5 randomly selected systems for a randomly selected sentence is presented. No additional information
beyond the instructions on this page are given to the judges. The tool tracks and reports annotation speed.
3 Manual Evaluation
While automatic measures are an invaluable tool
for the day-to-day development of machine trans-
lation systems, they are only a imperfect substitute
for human assessment of translation quality, or as
the acronym BLEU puts it, a bilingual evaluation
understudy.
Many human evaluation metrics have been pro-
posed. Also, the argument has been made that ma-
chine translation performance should be evaluated
via task-based evaluation metrics, i.e. how much it
assists performing a useful task, such as supporting
human translators or aiding the analysis of texts.
The main disadvantage of manual evaluation is
that it is time-consuming and thus too expensive to
do frequently. In this shared task, we were also con-
fronted with this problem, and since we had no fund-
ing for paying human judgements, we asked partic-
ipants in the evaluation to share the burden. Par-
ticipants and other volunteers contributed about 180
hours of labor in the manual evaluation.
3.1 Collecting Human Judgements
We asked participants to each judge 200?300 sen-
tences in terms of fluency and adequacy, the most
commonly used manual evaluation metrics. We set-
tled on contrastive evaluations of 5 system outputs
for a single test sentence. See Figure 3 for a screen-
shot of the evaluation tool.
Presenting the output of several system allows
the human judge to make more informed judge-
ments, contrasting the quality of the different sys-
tems. The judgements tend to be done more in form
of a ranking of the different systems. We assumed
that such a contrastive assessment would be benefi-
cial for an evaluation that essentially pits different
systems against each other.
While we had up to 11 submissions for a trans-
lation direction, we did decide against presenting
all 11 system outputs to the human judge. Our ini-
tial experimentation with the evaluation tool showed
that this is often too overwhelming.
Making the ten judgements (2 types for 5 sys-
tems) takes on average 2 minutes. Typically, judges
106
initially spent about 3 minutes per sentence, but then
accelerate with experience. Judges where excluded
from assessing the quality of MT systems that were
submitted by their institution. Sentences and sys-
tems were randomly selected and randomly shuffled
for presentation.
We collected around 300?400 judgements per
judgement type (adequacy or fluency), per system,
per language pair. This is less than the 694 judge-
ments 2004 DARPA/NIST evaluation, or the 532
judgements in the 2005 DARPA/NIST evaluation.
This decreases the statistical significance of our re-
sults compared to those studies. The number of
judgements is additionally fragmented by our break-
up of sentences into in-domain and out-of-domain.
3.2 Normalizing the judgements
The human judges were presented with the follow-
ing definition of adequacy and fluency, but no addi-
tional instructions:
Adequacy Fluency
5 All Meaning Flawless English
4 Most Meaning Good English
3 Much Meaning Non-native English
2 Little Meaning Disfluent English
1 None Incomprehensible
Judges varied in the average score they handed
out. The average fluency judgement per judge
ranged from 2.33 to 3.67, the average adequacy
judgement ranged from 2.56 to 4.13. Since different
judges judged different systems (recall that judges
were excluded to judge system output from their
own institution), we normalized the scores.
The normalized judgement per judge is the raw
judgement plus (3 minus average raw judgement for
this judge). In words, the judgements are normal-
ized, so that the average normalized judgement per
judge is 3.
Another way to view the judgements is that they
are less quality judgements of machine translation
systems per se, but rankings of machine translation
systems. In fact, it is very difficult to maintain con-
sistent standards, on what (say) an adequacy judge-
ment of 3 means even for a specific language pair.
The way judgements are collected, human judges
tend to use the scores to rank systems against each
other. If one system is perfect, another has slight
flaws and the third more flaws, a judge is inclined
to hand out judgements of 5, 4, and 3. On the other
hand, when all systems produce muddled output, but
one is better, and one is worse, but not completely
wrong, a judge is inclined to hand out judgements of
4, 3, and 2. The judgement of 4 in the first case will
go to a vastly better system output than in the second
case.
We therefore also normalized judgements on a
per-sentence basis. The normalized judgement per
sentence is the raw judgement plus (0 minus average
raw judgement for this judge on this sentence).
Systems that generally do better than others will
receive a positive average normalized judgement per
sentence. Systems that generally do worse than oth-
ers will receive a negative one.
One may argue with these efforts on normaliza-
tion, and ultimately their value should be assessed
by assessing their impact on inter-annotator agree-
ment. Given the limited number of judgements we
received, we did not try to evaluate this.
3.3 Statistical Significance
Confidence Interval: To estimate confidence inter-
vals for the average mean scores for the systems, we
use standard significance testing.
Given a set of n sentences, we can compute the
sample mean x? and sample variance s2 of the indi-
vidual sentence judgements xi:
x? =
1
n
n?
i=1
xi (4)
s2 =
1
n? 1
n?
i=1
(xi ? x?)
2 (5)
The extend of the confidence interval [x??d, x?+d]
can be computed by
d = 1.96 ?
s
?
n
(6)
Pairwise Comparison: As for the automatic evalu-
ation metric, we want to be able to rank different sys-
tems against each other, for which we need assess-
ments of statistical significance on the differences
between a pair of systems.
Unfortunately, we have much less data to work
with than with the automatic scores. The way we
107
Basis Diff. Ratio
Sign test on BLEU 331 75%
Bootstrap on BLEU 348 78%
Sign test on Fluency 224 50%
Sign test on Adequacy 225 51%
Figure 4: Number and ratio of statistically signifi-
cant distinction between system performance. Au-
tomatic scores are computed on a larger tested than
manual scores (3064 sentences vs. 300?400 sen-
tences).
collected manual judgements, we do not necessar-
ily have the same sentence judged for both systems
(judges evaluate 5 systems out of the 8?10 partici-
pating systems).
Still, for about good number of sentences, we do
have this direct comparison, which allows us to ap-
ply the sign test, as described in Section 2.2.
4 Results and Analysis
The results of the manual and automatic evaluation
of the participating system translations is detailed in
the figures at the end of this paper. The scores and
confidence intervals are detailed first in the Figures
7?10 in table form (including ranks), and then in
graphical form in Figures 11?16. In the graphs, sys-
tem scores are indicated by a point, the confidence
intervals by shaded areas around the point.
In all figures, we present the per-sentence normal-
ized judgements. The normalization on a per-judge
basis gave very similar ranking, only slightly less
consistent with the ranking from the pairwise com-
parisons.
The confidence intervals are computed by boot-
strap resampling for BLEU, and by standard signif-
icance testing for the manual scores, as described
earlier in the paper.
Pairwise comparison is done using the sign test.
Often, two systems can not be distinguished with
a confidence of over 95%, so there are ranked the
same. This actually happens quite frequently (more
below), so that the rankings are broad estimates. For
instance: if 10 systems participate, and one system
does better than 3 others, worse then 2, and is not
significant different from the remaining 4, its rank is
in the interval 3?7.
Domain BLEU Fluency Adequacy
in-domain 26.63 3.17 3.58
out-of-domain 20.37 2.74 3.08
Figure 5: Evaluation scores for in-domain and out-
of-domain test sets, averaged over all systems
4.1 Close results
At first glance, we quickly recognize that many sys-
tems are scored very similar, both in terms of man-
ual judgement and BLEU. There may be occasion-
ally a system clearly at the top or at the bottom, but
most systems are so close that it is hard to distin-
guish them.
In Figure 4, we displayed the number of system
comparisons, for which we concluded statistical sig-
nificance. For the automatic scoring method BLEU,
we can distinguish three quarters of the systems.
While the Bootstrap method is slightly more sensi-
tive, it is very much in line with the sign test on text
blocks.
For the manual scoring, we can distinguish only
half of the systems, both in terms of fluency and ad-
equacy. More judgements would have enabled us
to make better distinctions, but it is not clear what
the upper limit is. We can check, what the conse-
quences of less manual annotation of results would
have been: With half the number of manual judge-
ments, we can distinguish about 40% of the systems,
10% less.
4.2 In-domain vs. out-of-domain
The test set included 2000 sentences from the
Europarl corpus, but also 1064 sentences out-of-
domain test data. Since the inclusion of out-of-
domain test data was a very late decision, the par-
ticipants were not informed of this. So, this was a
surprise element due to practical reasons, not mal-
ice.
All systems (except for Systran, which was not
tuned to Europarl) did considerably worse on out-
of-domain training data. This is demonstrated by
average scores over all systems, in terms of BLEU,
fluency and adequacy, as displayed in Figure 5.
The manual scores are averages over the raw un-
normalized scores.
108
Language Pair BLEU Fluency Adequacy
French-English 26.09 3.25 3.61
Spanish-English 28.18 3.19 3.71
German-English 21.17 2.87 3.10
English-French 28.33 2.86 3.16
English-Spanish 27.49 2.86 3.34
English-German 14.01 3.15 3.65
Figure 6: Average scores for different language
pairs. Manual scoring is done by different judges,
resulting in a not very meaningful comparison.
4.3 Language pairs
It is well know that language pairs such as English-
German pose more challenges to machine transla-
tion systems than language pairs such as French-
English. Different sentence structure and rich target
language morphology are two reasons for this.
Again, we can compute average scores for all sys-
tems for the different language pairs (Figure 6). The
differences in difficulty are better reflected in the
BLEU scores than in the raw un-normalized man-
ual judgements. The easiest language pair according
to BLEU (English-French: 28.33) received worse
manual scores than the hardest (English-German:
14.01). This is because different judges focused on
different language pairs. Hence, the different av-
erages of manual scores for the different language
pairs reflect the behaviour of the judges, not the
quality of the systems on different language pairs.
4.4 Manual judgement vs. BLEU
Given the closeness of most systems and the wide
over-lapping confidence intervals it is hard to make
strong statements about the correlation between hu-
man judgements and automatic scoring methods
such as BLEU.
We confirm the finding by Callison-Burch et al
(2006) that the rule-based system of Systran is not
adequately appreciated by BLEU. In-domain Sys-
tran scores on this metric are lower than all statistical
systems, even the ones that have much worse human
scores. Surprisingly, this effect is much less obvious
for out-of-domain test data. For instance, for out-of-
domain English-French, Systran has the best BLEU
and manual scores.
Our suspicion is that BLEU is very sensitive to
jargon, to selecting exactly the right words, and
not synonyms that human judges may appreciate
as equally good. This is can not be the only ex-
planation, since the discrepancy still holds, for in-
stance, for out-of-domain French-English, where
Systran receives among the best adequacy and flu-
ency scores, but a worse BLEU score than all but
one statistical system.
This data set of manual judgements should pro-
vide a fruitful resource for research on better auto-
matic scoring methods.
4.5 Best systems
So, who won the competition? The best answer
to this is: many research labs have very competi-
tive systems whose performance is hard to tell apart.
This is not completely surprising, since all systems
use very similar technology.
For some language pairs (such as German-
English) system performance is more divergent than
for others (such as English-French), at least as mea-
sured by BLEU.
The statistical systems seem to still lag be-
hind the commercial rule-based competition when
translating into morphological rich languages, as
demonstrated by the results for English-German and
English-French.
The predominate focus of building systems that
translate into English has ignored so far the difficult
issues of generating rich morphology which may not
be determined solely by local context.
4.6 Comments on Manual Evaluation
This is the first time that we organized a large-scale
manual evaluation. While we used the standard met-
rics of the community, the we way presented trans-
lations and prompted for assessment differed from
other evaluation campaigns. For instance, in the
recent IWSLT evaluation, first fluency annotations
were solicited (while withholding the source sen-
tence), and then adequacy annotations.
Almost all annotators reported difficulties in
maintaining a consistent standard for fluency and ad-
equacy judgements, but nevertheless most did not
explicitly move towards a ranking-based evaluation.
Almost all annotators expressed their preference to
move to a ranking-based evaluation in the future. A
few pointed out that adequacy should be broken up
109
into two criteria: (a) are all source words covered?
(b) does the translation have the same meaning, in-
cluding connotations?
Annotators suggested that long sentences are al-
most impossible to judge. Since all long sen-
tence translation are somewhat muddled, even a con-
trastive evaluation between systems was difficult. A
few annotators suggested to break up long sentences
into clauses and evaluate these separately.
Not every annotator was fluent in both the source
and the target language. While it is essential to be
fluent in the target language, it is not strictly nec-
essary to know the source language, if a reference
translation was given. However, ince we extracted
the test corpus automatically from web sources, the
reference translation was not always accurate ? due
to sentence alignment errors, or because translators
did not adhere to a strict sentence-by-sentence trans-
lation (say, using pronouns when referring to enti-
ties mentioned in the previous sentence). Lack of
correct reference translations was pointed out as a
short-coming of our evaluation. One annotator sug-
gested that this was the case for as much as 10% of
our test sentences. Annotators argued for the impor-
tance of having correct and even multiple references.
It was also proposed to allow annotators to skip
sentences that they are unable to judge.
5 Conclusions
We carried out an extensive manual and automatic
evaluation of machine translation performance on
European language pairs. While many systems had
similar performance, the results offer interesting in-
sights, especially about the relative performance of
statistical and rule-based systems.
Due to many similarly performing systems, we
are not able to draw strong conclusions on the ques-
tion of correlation of manual and automatic evalua-
tion metrics. The bias of automatic methods in favor
of statistical systems seems to be less pronounced on
out-of-domain test data.
The manual evaluation of scoring translation on
a graded scale from 1?5 seems to be very hard to
perform. Replacing this with an ranked evalua-
tion seems to be more suitable. Human judges also
pointed out difficulties with the evaluation of long
sentences.
Acknowledgements
The manual evaluation would not have been possible
without the contributions of the manual annotators:
Jesus Andres Ferrer, Abhishek Arun, Amittai Axel-
rod, Alexandra Birch, Chris Callison-Burch, Jorge
Civera, Marta Ruiz Costa-jussa`, Josep Maria Crego,
Elsa Cubel, Chris Irwin Davis, Loic Dugast, Chris
Dyer, Andreas Eisele, Cameron Fordyce, Jesu?s
Gime?nez, Fabrizio Gotti, Hieu Hoang, Eric Joanis
Howard Johnson, Philipp Koehn, Beata Kouchnir,
Roland Kuhn, Elliott Macklovitch, Arul Menezes,
Marian Olteanu, Chris Quirk, Reinhard Rapp, Fatiha
Sadat, Joan Andreu Sa`nchez, Germa?n Sanchis,
Michel Simard, Ashish Venugopal, and Taro Watan-
abe.
This work was supported in part under the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022.
References
Birch, A., Callison-Burch, C., Osborne, M., and
Koehn, P. (2006). Constraining the phrase-based,
joint probability statistical translation model. In
Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 154?157, New York
City. Association for Computational Linguistics.
Callison-Burch, C., Osborne, M., and Koehn, P.
(2006). Re-evaluating the role of BLEU in ma-
chine translation research. In Proceedings of
EACL.
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine trans-
lation. In Proceedings of ACL.
Costa-jussa`, M. R., Crego, J. M., de Gispert, A.,
Lambert, P., Khalilov, M., Marin?o, J. B., Fonol-
losa, J. A. R., and Banchs, R. (2006). Talp phrase-
based statistical translation system for european
language pairs. In Proceedings on the Workshop
on Statistical Machine Translation, pages 142?
145, New York City. Association for Computa-
tional Linguistics.
Crego, J. M., de Gispert, A., Lambert, P., Costa-
jussa`, M. R., Khalilov, M., Banchs, R., Marin?o,
J. B., and Fonollosa, J. A. R. (2006). N-gram-
based smt system enhanced with reordering pat-
terns. In Proceedings on the Workshop on Statis-
110
tical Machine Translation, pages 162?165, New
York City. Association for Computational Lin-
guistics.
Doddington, G. (2002). The NIST automated mea-
sure and its relation to IBM?s BLEU. In Proceed-
ings of LREC-2002 Workshopon Machine Trans-
lation Evaluation: Human Evaluators Meet Auto-
mated Metrics, Gran Canaria, Spain.
Eck, M. and Hori, C. (2005). Overview of the iwslt
2005 evaluation campaign. In Proc. of the Inter-
national Workshop on Spoken Language Transla-
tion.
Gime?nez, J. and Ma`rquez, L. (2006). The ldv-
combo system for smt. In Proceedings on
the Workshop on Statistical Machine Translation,
pages 166?169, New York City. Association for
Computational Linguistics.
Johnson, H., Sadat, F., Foster, G., Kuhn, R., Simard,
M., Joanis, E., and Larkin, S. (2006). Portage:
with smoothed phrase tables and segment choice
models. In Proceedings on the Workshop on
Statistical Machine Translation, pages 134?137,
New York City. Association for Computational
Linguistics.
Koehn, P. (2004). Statistical significance tests for
machine translation evaluation. In Lin, D. and
Wu, D., editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain. Association for
Computational Linguistics.
Koehn, P. and Monz, C. (2005). Shared task: Statis-
tical machine translation between European lan-
guages. In Proceedings of the ACL Workshop
on Building and Using Parallel Texts, pages 119?
124, Ann Arbor, Michigan. Association for Com-
putational Linguistics.
Li, A. (2005). Results of the 2005 NIST machine
translation evaluation. In Machine Translation
Workshop.
Menezes, A., Toutanova, K., and Quirk, C. (2006).
Microsoft research treelet translation system:
Naacl 2006 europarl evaluation. In Proceedings
on the Workshop on Statistical Machine Transla-
tion, pages 158?161, New York City. Association
for Computational Linguistics.
Olteanu, M., Davis, C., Volosen, I., and Moldovan,
D. (2006a). Phramer - an open source statisti-
cal phrase-based translator. In Proceedings on
the Workshop on Statistical Machine Translation,
pages 146?149, New York City. Association for
Computational Linguistics.
Olteanu, M., Suriyentrakorn, P., and Moldovan, D.
(2006b). Language models and reranking for ma-
chine translation. In Proceedings on the Workshop
on Statistical Machine Translation, pages 150?
153, New York City. Association for Computa-
tional Linguistics.
Patry, A., Gotti, F., and Langlais, P. (2006). Mood at
work: Ramses versus pharaoh. In Proceedings on
the Workshop on Statistical Machine Translation,
pages 126?129, New York City. Association for
Computational Linguistics.
Przybocki, M. (2004). NIST machine translation
2004 evaluation ? summary of results. In Machine
Translation Evaluation Workshop.
Riezler, S. and Maxwell, J. T. (2005). On some pit-
falls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization,
pages 57?64, Ann Arbor, Michigan. Association
for Computational Linguistics.
Sa?nchez, J. A. and Bened??, J. M. (2006). Stochas-
tic inversion transduction grammars for obtaining
word phrases for phrase-based statistical machine
translation. In Proceedings on the Workshop on
Statistical Machine Translation, pages 130?133,
New York City. Association for Computational
Linguistics.
Watanabe, T., Tsukada, H., and Isozaki, H. (2006).
Ntt system description for the wmt2006 shared
task. In Proceedings on the Workshop on Statis-
tical Machine Translation, pages 122?125, New
York City. Association for Computational Lin-
guistics.
Zollmann, A. and Venugopal, A. (2006). Syntax
augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical
Machine Translation, pages 138?141, New York
City. Association for Computational Linguistics.
111
French-English (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-jmc +0.19?0.08 (1-7) +0.09?0.08 (1-8) 30.42?0.86 (1-6)
lcc +0.14?0.07 (1-6) +0.13?0.06 (1-7) 30.81?0.85 (1-4)
utd +0.13?0.08 (1-7) +0.14?0.07 (1-6) 30.53?0.87 (2-7)
upc-mr +0.13?0.08 (1-8) +0.13?0.07 (1-6) 30.33?0.88 (1-7)
nrc +0.12?0.10 (1-7) +0.06?0.11 (2-6) 29.62?0.84 (8)
ntt +0.11?0.08 (1-8) +0.14?0.08 (2-8) 30.72?0.87 (1-7)
cmu +0.10?0.08 (3-7) +0.05?0.07 (4-8) 30.18?0.80 (2-7)
rali -0.02?0.08 (5-8) +0.00?0.08 (3-9) 30.39?0.91 (3-7)
systran -0.08?0.09 (9) -0.17?0.09 (8-9) 21.44?0.65 (10)
upv -0.76?0.09 (10) -0.52?0.09 (10) 24.10?0.89 (9)
Spanish-English (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-jmc +0.15?0.08 (1-7) +0.18?0.08 (1-6) 31.01?0.97 (1-5)
ntt +0.10?0.08 (1-7) +0.10?0.08 (1-8) 31.29?0.88 (1-5)
lcc +0.08?0.07 (1-8) +0.04?0.06 (2-8) 31.46?0.87 (1-4)
utd +0.08?0.06 (1-8) +0.08?0.07 (2-7) 31.10?0.89 (1-5)
nrc +0.06?0.10 (2-8) +0.08?0.07 (1-9) 30.04?0.79 (6)
upc-mr +0.06?0.07 (1-8) +0.08?0.07 (1-6) 29.43?0.83 (7)
uedin-birch +0.03?0.11 (1-8) -0.07?0.15 (2-10) 29.01?0.81 (8)
rali +0.00?0.07 (3-9) -0.02?0.07 (3-9) 30.80?0.87 (2-5)
upc-jg -0.10?0.07 (7-9) -0.11?0.07 (6-9) 28.03?0.83 (9)
upv -0.45?0.10 (10) -0.41?0.10 (9-10) 23.91?0.83 (10)
German-English (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
uedin-phi +0.30?0.09 (1-2) +0.33?0.08 (1) 27.30?0.86 (1)
lcc +0.15?0.07 (2-7) +0.12?0.07 (2-7) 25.97?0.81 (2)
nrc +0.12?0.07 (2-7) +0.14?0.07 (2-6) 24.54?0.80 (5-7)
utd +0.08?0.07 (3-7) +0.01?0.08 (2-8) 25.44?0.85 (3-4)
ntt +0.07?0.08 (2-9) +0.06?0.09 (2-8) 25.64?0.83 (3-4)
upc-mr +0.00?0.09 (3-9) -0.21?0.09 (6-9) 23.68?0.79 (8)
rali -0.01?0.06 (4-9) +0.00?0.07 (3-9) 24.60?0.80 (5-7)
upc-jmc -0.02?0.09 (2-9) -0.04?0.09 (3-9) 24.43?0.86 (5-7)
systran -0.05?0.10 (3-9) -0.05?0.09 (3-9) 15.86?0.59 (10)
upv -0.55?0.09 (10) -0.38?0.08 (10) 18.08?0.77 (9)
Figure 7: Evaluation of translation to English on in-domain test data
112
English-French (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
nrc +0.08?0.09 (1-5) +0.09?0.09 (1-5) 31.75?0.83 (1-6)
upc-mr +0.08?0.08 (1-4) +0.04?0.07 (1-5) 31.50?0.76 (1-6)
upc-jmc +0.03?0.09 (1-6) +0.02?0.08 (1-6) 31.75?0.78 (1-5)
systran -0.01?0.12 (2-7) +0.06?0.12 (1-6) 25.07?0.71 (7)
utd -0.03?0.07 (3-7) -0.05?0.07 (3-7) 31.42?0.85 (3-6)
rali -0.08?0.09 (1-7) -0.09?0.09 (2-7) 31.79?0.85 (1-6)
ntt -0.09?0.09 (4-7) -0.06?0.08 (4-7) 31.92?0.84 (1-5)
English-Spanish (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
ms +0.23?0.09 (1-5) +0.13?0.09 (1-7) 29.76?0.82 (7-8)
upc-mr +0.20?0.09 (1-4) +0.17?0.09 (1-5) 31.06?0.86 (1-4)
utd +0.18?0.08 (1-5) +0.15?0.08 (1-6) 30.73?0.90 (1-4)
nrc +0.12?0.09 (2-7) +0.17?0.08 (1-6) 29.97?0.86 (5-6)
ntt +0.10?0.09 (3-7) +0.14?0.08 (1-6) 30.93?0.85 (1-4)
upc-jmc +0.04?0.10 (2-7) +0.01?0.08 (2-7) 30.44?0.86 (1-4)
rali -0.05?0.08 (5-8) -0.03?0.08 (6-8) 29.38?0.85 (5-6)
uedin-birch -0.18?0.14 (6-9) -0.17?0.13 (6-10) 28.49?0.87 (7-8)
upc-jg -0.32?0.11 (9) -0.37?0.09 (8-10) 27.46?0.78 (9)
upv -0.83?0.15 (9-10) -0.59?0.15 (8-10) 23.17?0.73 (10)
English-German (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-mr +0.28?0.08 (1-3) +0.14?0.08 (1-5) 17.24?0.81 (3-5)
ntt +0.19?0.08 (1-5) +0.09?0.06 (2-6) 18.15?0.89 (1-3)
upc-jmc +0.17?0.08 (1-5) +0.13?0.08 (1-4) 17.73?0.81 (1-3)
nrc +0.17?0.08 (2-4) +0.11?0.08 (1-5) 17.52?0.78 (4-5)
rali +0.08?0.10 (3-6) +0.03?0.09 (2-6) 17.93?0.85 (1-4)
systran -0.08?0.11 (5-6) +0.00?0.10 (3-6) 9.84?0.52 (7)
upv -0.84?0.12 (7) -0.51?0.10 (7) 13.37?0.78 (6)
Figure 8: Evaluation of translation from English on in-domain test data
113
French-English (Out of Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-jmc +0.23?0.09 (1-5) +0.13?0.11 (1-8) 21.79?0.92 (1-4)
cmu +0.22?0.11 (1-8) +0.13?0.09 (1-9) 21.15?0.86 (4-7)
systran +0.19?0.15 (1-8) +0.15?0.14 (1-7) 19.42?0.82 (9)
lcc +0.13?0.12 (1-9) +0.11?0.11 (1-9) 21.77?0.88 (1-5)
upc-mr +0.12?0.12 (2-8) +0.11?0.10 (1-7) 21.95?0.94 (1-3)
utd +0.04?0.10 (1-9) +0.01?0.10 (1-8) 21.39?0.94 (3-7)
ntt -0.02?0.12 (3-9) +0.08?0.11 (1-9) 21.34?0.85 (3-7)
nrc -0.03?0.14 (3-8) +0.00?0.11 (3-9) 21.15?0.86 (3-7)
rali -0.09?0.12 (4-9) -0.10?0.11 (5-9) 20.17?0.85 (8)
upv -0.76?0.16 (10) -0.58?0.14 (10) 15.55?0.79 (10)
Spanish-English (Out of Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-jmc +0.28?0.10 (1-2) +0.17?0.10 (1-6) 27.92?0.94 (1-3)
uedin-birch +0.25?0.16 (1-7) +0.18?0.19 (1-6) 25.20?0.91 (5-8)
nrc +0.18?0.16 (2-8) +0.09?0.09 (1-8) 25.40?0.94 (5-7)
ntt +0.11?0.10 (2-7) +0.17?0.10 (2-6) 26.85?0.89 (3-4)
upc-mr +0.08?0.11 (2-8) +0.10?0.10 (1-7) 25.62?0.87 (5-8)
lcc +0.04?0.10 (4-9) +0.07?0.11 (3-7) 27.18?0.92 (1-4)
utd +0.03?0.11 (2-9) +0.03?0.10 (2-8) 27.41?0.96 (1-3)
upc-jg -0.09?0.11 (4-9) -0.09?0.09 (7-9) 23.42?0.87 (9)
rali -0.09?0.11 (4-9) -0.15?0.11 (6-9) 25.03?0.91 (6-8)
upv -0.63?0.14 (10) -0.47?0.11 (10) 19.17?0.78 (10)
German-English (Out of Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
systran +0.30?0.12 (1-4) +0.21?0.12 (1-4) 15.56?0.71 (7-9)
uedin-phi +0.22?0.09 (1-6) +0.21?0.10 (1-7) 18.87?0.84 (1)
lcc +0.18?0.10 (1-6) +0.20?0.10 (1-7) 17.96?0.79 (2-3)
utd +0.08?0.09 (2-7) +0.07?0.08 (2-6) 16.97?0.76 (4-6)
ntt +0.07?0.12 (1-9) +0.21?0.13 (1-7) 17.37?0.76 (3-5)
nrc +0.04?0.10 (3-8) +0.04?0.09 (2-8) 15.93?0.76 (7-8)
upc-mr +0.02?0.10 (4-8) -0.11?0.09 (6-8) 16.89?0.79 (4-6)
upc-jmc -0.01?0.10 (4-8) -0.04?0.11 (3-9) 17.57?0.80 (2-5)
rali -0.14?0.08 (8-9) -0.14?0.08 (8-9) 15.22?0.69 (8-9)
upv -0.64?0.11 (10) -0.54?0.09 (10) 11.78?0.71 (10)
Figure 9: Evaluation of translation to English on out-of-domain test data
114
English-French (Out of Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
systran +0.50?0.20 (1) +0.41?0.18 (1) 25.31?0.88 (1)
upc-jmc +0.09?0.11 (2-5) +0.09?0.11 (2-4) 23.30?0.75 (2-6)
upc-mr +0.09?0.11 (2-4) +0.04?0.09 (2-4) 23.21?0.75 (2-6)
utd -0.02?0.11 (2-6) -0.05?0.09 (2-6) 22.79?0.86 (7)
rali -0.12?0.12 (4-7) -0.17?0.12 (5-7) 23.34?0.89 (2-6)
nrc -0.13?0.13 (4-7) -0.16?0.10 (4-7) 23.66?0.91 (2-5)
ntt -0.23?0.12 (4-7) -0.06?0.10 (4-7) 22.99?0.96 (3-6)
English-Spanish (Out of Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-mr +0.35?0.11 (1-3) +0.19?0.10 (1-6) 26.62?0.92 (1-2)
ms +0.33?0.16 (1-7) +0.15?0.13 (1-8) 26.15?0.88 (6-7)
utd +0.21?0.13 (2-6) +0.13?0.11 (1-7) 25.26?0.78 (3-5)
nrc +0.18?0.12 (1-6) +0.07?0.11 (2-7) 25.58?0.85 (3-5)
upc-jmc +0.17?0.15 (2-7) +0.24?0.12 (1-6) 25.59?0.95 (3-5)
ntt +0.12?0.13 (2-7) +0.12?0.13 (1-7) 26.52?0.90 (1-2)
rali -0.17?0.16 (6-8) -0.05?0.13 (4-8) 24.03?0.83 (6-8)
uedin-birch -0.36?0.24 (6-10) -0.16?0.16 (5-9) 23.18?0.88 (7-8)
upc-jg -0.45?0.13 (8-9) -0.42?0.10 (9-10) 22.04?0.84 (9)
upv -1.09?0.21 (9) -0.64?0.19 (8-9) 16.83?0.72 (10)
English-German (Out of Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
systran +0.47?0.15 (1) +0.39?0.15 (1-2) 10.78?0.69 (1-6)
upc-mr +0.31?0.13 (2-3) +0.21?0.11 (1-3) 10.96?0.70 (1-5)
upc-jmc +0.22?0.14 (2-3) +0.01?0.10 (3-6) 10.64?0.66 (1-6)
rali +0.13?0.12 (4-6) -0.06?0.10 (4-6) 10.57?0.65 (1-6)
nrc +0.00?0.11 (4-6) +0.05?0.09 (2-6) 10.64?0.65 (2-6)
ntt -0.03?0.12 (4-6) +0.08?0.11 (3-5) 10.51?0.64 (1-6)
upv -0.94?0.13 (7) -0.57?0.10 (7) 6.55?0.53 (7)
Figure 10: Evaluation of translation from English on out-of-domain test data
115
French-English
In domain Out of Domain
21 22 23 24 25 26 27 28 29 30 31
BLEU-0.8
-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
Adequacy
?
rali
?utd
?
?systran
?lccnrc?
?upv
??ntt
?
cmu?
upc-jmc
?upc-mr?
?
15 16 17 18 19 20 21 22
BLEU-0.8
-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
Adequacy
rali?
utd?
systran?
lcc?
?
nrc?
?upv
?ntt
cmu? ?upc-jmc
?upc-mr
21 22 23 24 25 26 27 28 29 30 31
BLEU-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
Fluency
?
rali
?utd
?
?systran
??lcc?
nrc?
?upv
ntt
?cmuupc-jmc?
upc-mr?
?
15 16 17 18 19 20 21 22
BLEU-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
Fluency
rali?
?utd
systran?
??lccnrc?
?upv
ntt?
cmu? ?
upc-jmc
??upc-mr
Figure 11: Correlation between manual and automatic scores for French-English
116
Spanish-English
In Domain Out of Domain
23 24 25 26 27 28 29 30 31 32
BLEU-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
Adequacy
?
rali
?upc-jg
??utd?lcc
?upv
?
nrc
ntt?
uedin-birch?
upc-jmc
?
upc-mr?
19 20 21 22 23 24 25 26 27 28
BLEU-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
Adequacy
?raliupc-jg?
?utdlcc?
?upv
?nrc
?ntt
uedin-birch? ?upc-jmc
upc-mr?
23 24 25 26 27 28 29 30 31 32
BLEU-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
Fluency
?rali
upc-jg?
?
utd? ?lcc
?upv
nrc
? ?ntt
?
uedin-birch
?upc-jmc
upc-mr?
19 20 21 22 23 24 25 26 27 28
BLEU-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
Fluency
?rali
?upc-jg
?utd?lcc
?upv
nrc?
ntt
?uedin-birch? ?upc-jmc
?upc-mr
Figure 12: Correlation between manual and automatic scores for Spanish-English
117
German-English
In Domain Out of Domain
15 16 17 18 19 20 21 22 23 24 25 26 27
BLEU-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
0.4
Adequacy
?rali
utd?
?systran
lcc
?nrc?
?upv
?ntt
upc-mr? ?
upc-jmc
uedin-phi?
12 13 14 15 16 17 18 19 20
BLEU-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
0.4
Adequacy
?rali
utd?
?
?systran
lcc?
nrc?
?upv
?ntt?
upc-mr? ?upc-jmc
?uedin-phi
15 16 17 18 19 20 21 22 23 24 25 26 27
BLEU-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
0.4
Fluency
rali?
? ?utd
?systran
?lccnrc?
?upv
?ntt
?upc-mr
upc-jmc?
uedin-phi?
12 13 14 15 16 17 18 19 20
BLEU-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
0.4
Fluency
?rali
?utd
systran? ??lcc
nrc?
?upv
ntt?
?upc-mr
?upc-jmc
uedin-phi?
?
Figure 13: Correlation between manual and automatic scores for German-English
118
English-French
In Domain Out of Domain
25 26 27 28 29 30 31 32
BLEU-0.3
-0.2
-0.1
0.0
0.1
0.2
Adequacy
?nrc
?nttrali?
?upc-jmcupc-mr?
utd??systran
20 21 22 23 24 25 26
BLEU-0.3
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
Adequacy
?nrc
?
ntt
rali?
?upc-jmcupc-mr
utd?
systran?
25 26 27 28 29 30 31 32
BLEU-0.3
-0.2
-0.1
0.0
0.1
0.2
Fluency
?nrc
?ntt?
rali
?upc-jmcupc-mr?
utd?
?systran
20 21 22 23 24 25 26
BLEU-0.3
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
Fluency
?nrc
?ntt
rali?
?upc-jmc
upc-mr?
utd?
systran?
Figure 14: Correlation between manual and automatic scores for English-French
119
English-Spanish
In Domain Out of Domain
23 24 25 26 27 28 29 30 31 32
BLEU-0.9
-0.8
-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
Adequacy
rali?
upc-jg?
?utdnrc?
?upv
?ntt
ms?
?uedin-birch
?upc-jmc
?upc-mr
16 17 18 19 20 21 22 23 24 25 26 27
BLEU-1.1
-1.0
-0.9
-0.8
-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
0.4
Adequacy
?rali
upc-jg?
utd?
??nrc?
?upv
?ntt
ms?
?uedin-birch
upc-jmc
?upc-mr
23 24 25 26 27 28 29 30 31 32
BLEU-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
Fluency
?
rali
upc-jg?
??utd
nrc?
?upv
ntt?
?ms?
uedin-birch?
?upc-jmc
?upc-mr
16 17 18 19 20 21 22 23 24 25 26 27
BLEU-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
Fluency
?rali
upc-jg?
utd?
?nrc
?upv
?nttms?
?uedin-birch
upc-jmc? ?upc-mr
Figure 15: Correlation between manual and automatic scores for English-Spanish
120
English-German
In Domain Out of Domain
9 10 11 12 13 14 15 16 17 18 19
BLEU-0.9
-0.8
-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
Adequacy
nrc?
?upv
?rali
?ntt
?upc-mr
upc-jmc?
?
?systran
6 7 8 9 10 11
BLEU-1.0
-0.9
-0.8
-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
0.4
0.5
Adequacy
?nrc
?upv
rali?
ntt?
?upc-mr
upc-jmc?
?systran
9 10 11 12 13 14 15 16 17 18 19
BLEU-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
Fluency
nrc?
?upv
rali?
?ntt
upc-mr??upc-jmc
?systran
6 7 8 9 10 11
BLEU-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
-0.0
0.1
0.2
0.3
0.4
Fluency
?nrc
?upv
rali?
ntt
?upc-mr
??upc-jmc
?systran
Figure 16: Correlation between manual and automatic scores for English-German
121
Proceedings of the Second Workshop on Statistical Machine Translation, pages 136?158,
Prague, June 2007. c?2007 Association for Computational Linguistics
(Meta-) Evaluation of Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb clsp jhu edu
Cameron Fordyce
CELCT
fordyce celct it
Philipp Koehn
University of Edinburgh
pkoehn inf ed ac uk
Christof Monz
Queen Mary, University of London
christof dcs qmul ac uk
Josh Schroeder
University of Edinburgh
j schroeder ed ac uk
Abstract
This paper evaluates the translation quality
of machine translation systems for 8 lan-
guage pairs: translating French, German,
Spanish, and Czech to English and back.
We carried out an extensive human evalua-
tion which allowed us not only to rank the
different MT systems, but also to perform
higher-level analysis of the evaluation pro-
cess. We measured timing and intra- and
inter-annotator agreement for three types of
subjective evaluation. We measured the cor-
relation of automatic evaluation metrics with
human judgments. This meta-evaluation re-
veals surprising facts about the most com-
monly used methodologies.
1 Introduction
This paper presents the results for the shared trans-
lation task of the 2007 ACL Workshop on Statistical
Machine Translation. The goals of this paper are
twofold: First, we evaluate the shared task entries
in order to determine which systems produce trans-
lations with the highest quality. Second, we analyze
the evaluation measures themselves in order to try to
determine ?best practices? when evaluating machine
translation research.
Previous ACL Workshops on Machine Transla-
tion were more limited in scope (Koehn and Monz,
2005; Koehn and Monz, 2006). The 2005 workshop
evaluated translation quality only in terms of Bleu
score. The 2006 workshop additionally included a
limited manual evaluation in the style of NIST ma-
chine translation evaluation workshop. Here we ap-
ply eleven different automatic evaluation metrics,
and conduct three different types of manual evalu-
ation.
Beyond examining the quality of translations pro-
duced by various systems, we were interested in ex-
amining the following questions about evaluation
methodologies: How consistent are people when
they judge translation quality? To what extent do
they agree with other annotators? Can we im-
prove human evaluation? Which automatic evalu-
ation metrics correlate most strongly with human
judgments of translation quality?
This paper is organized as follows:
? Section 2 gives an overview of the shared task.
It describes the training and test data, reviews
the baseline system, and lists the groups that
participated in the task.
? Section 3 describes the manual evaluation. We
performed three types of evaluation: scoring
with five point scales, relative ranking of trans-
lations of sentences, and ranking of translations
of phrases.
? Section 4 lists the eleven different automatic
evaluation metrics which were also used to
score the shared task submissions.
? Section 5 presents the results of the shared task,
giving scores for each of the systems in each of
the different conditions.
? Section 6 provides an evaluation of the dif-
ferent types of evaluation, giving intra- and
136
inter-annotator agreement figures for the man-
ual evaluation, and correlation numbers for the
automatic metrics.
2 Shared task overview
This year?s shared task changed in some aspects
from last year?s:
? We gave preference to the manual evaluation of
system output in the ranking of systems. Man-
ual evaluation was done by the volunteers from
participating groups and others. Additionally,
there were three modalities of manual evalua-
tion.
? Automatic metrics were also used to rank the
systems. In total eleven metrics were applied,
and their correlation with the manual scores
was measured.
? As in 2006, translation was from English, and
into English. English was again paired with
German, French, and Spanish. We additionally
included Czech (which was fitting given the lo-
cation of the WS).
Similar to the IWSLT International Workshop on
Spoken Language Translation (Eck and Hori, 2005;
Paul, 2006), and the NIST Machine Translation
Evaluation Workshop (Lee, 2006) we provide the
shared task participants with a common set of train-
ing and test data for all language pairs. The major
part of data comes from current and upcoming full
releases of the Europarl data set (Koehn, 2005).
2.1 Description of the Data
The data used in this year?s shared task was similar
to the data used in last year?s shared task. This year?s
data included training and development sets for the
News Commentary data, which was the surprise out-
of-domain test set last year.
The majority of the training data for the Spanish,
French, and German tasks was drawn from a new
version of the Europarl multilingual corpus. Addi-
tional training data was taken from the News Com-
mentary corpus. Czech language resources were
drawn from the News Commentary data. Additional
resources for Czech came from the CzEng Paral-
lel Corpus (Bojar and Z?abokrtsky?, 2006). Overall,
there are over 30 million words of training data per
language from the Europarl corpus and 1 million
words from the News Commentary corpus. Figure 1
provides some statistics about the corpora used this
year.
2.2 Baseline system
To lower the barrier of entrance to the competition,
we provided a complete baseline MT system, along
with data resources. To summarize, we provided:
? sentence-aligned training corpora
? development and dev-test sets
? language models trained for each language
? an open source decoder for phrase-based SMT
called Moses (Koehn et al, 2006), which re-
places the Pharaoh decoder (Koehn, 2004)
? a training script to build models for Moses
The performance of this baseline system is similar
to the best submissions in last year?s shared task.
2.3 Test Data
The test data was again drawn from a segment of
the Europarl corpus from the fourth quarter of 2000,
which is excluded from the training data. Partici-
pants were also provided with three sets of parallel
text to be used for system development and tuning.
In addition to the Europarl test set, we also col-
lected editorials from the Project Syndicate web-
site1, which are published in all the five languages
of the shared task. We aligned the texts at a sentence
level across all five languages, resulting in 2,007
sentences per language. For statistics on this test set,
refer to Figure 1.
The News Commentary test set differs from the
Europarl data in various ways. The text type are ed-
itorials instead of speech transcripts. The domain is
general politics, economics and science. However, it
is also mostly political content (even if not focused
on the internal workings of the European Union) and
opinion.
2.4 Participants
We received submissions from 15 groups from 14
institutions, as listed in Table 1. This is a slight
1http://www.project-syndicate.com/
137
Europarl Training corpus
Spanish? English French? English German? English
Sentences 1,259,914 1,288,901 1,264,825
Foreign words 33,159,337 33,176,243 29,582,157
English words 31,813,692 32,615,285 31,929,435
Distinct foreign words 345,944 344,287 510,544
Distinct English words 266,976 268,718 250,295
News Commentary Training corpus
Spanish? English French? English German? English Czech? English
Sentences 51,613 43,194 59,975 57797
Foreign words 1,263,067 1,028,672 1,297,673 1,083,122
English words 1,076,273 906,593 1,238,274 1,188,006
Distinct foreign words 84,303 68,214 115,589 142,146
Distinct English words 70,755 63,568 76,419 74,042
Language model data
English Spanish French German
Sentence 1,407,285 1,431,614 1,435,027 1,478,428
Words 34,539,822 36,426,542 35,595,199 32,356,475
Distinct words 280,546 385,796 361,205 558,377
Europarl test set
English Spanish French German
Sentences 2,000
Words 53,531 55,380 53,981 49,259
Distinct words 8,558 10,451 10,186 11,106
News Commentary test set
English Spanish French German Czech
Sentences 2,007
Words 43,767 50,771 49,820 45,075 39,002
Distinct words 10,002 10,948 11,244 12,322 15,245
Figure 1: Properties of the training and test sets used in the shared task. The training data is drawn from the
Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple
languages.
138
ID Participant
cmu-uka Carnegie Mellon University, USA (Paulik et al, 2007)
cmu-syntax Carnegie Mellon University, USA (Zollmann et al, 2007)
cu Charles University, Czech Republic (Bojar, 2007)
limsi LIMSI-CNRS, France (Schwenk, 2007)
liu University of Linko?ping, Sweden(Holmqvist et al, 2007)
nrc National Research Council, Canada (Ueffing et al, 2007)
pct a commercial MT provider from the Czech Republic
saar Saarland University & DFKI, Germany (Chen et al, 2007)
systran SYSTRAN, France & U. Edinburgh, UK (Dugast et al, 2007)
systran-nrc National Research Council, Canada (Simard et al, 2007)
ucb University of California at Berkeley, USA (Nakov and Hearst, 2007)
uedin University of Edinburgh, UK (Koehn and Schroeder, 2007)
umd University of Maryland, USA (Dyer, 2007)
upc University of Catalonia, Spain (Costa-Jussa` and Fonollosa, 2007)
upv University of Valencia, Spain (Civera and Juan, 2007)
Table 1: Participants in the shared task. Not all groups participated in all translation directions.
increase over last year?s shared task where submis-
sions were received from 14 groups from 11 insti-
tutions. Of the 11 groups that participated in last
year?s shared task, 6 groups returned this year.
This year, most of these groups follow a phrase-
based statistical approach to machine translation.
However, several groups submitted results from sys-
tems that followed a hybrid approach.
While building a machine translation system is a
serious undertaking we hope to attract more new-
comers to the field by keeping the barrier of entry
as low as possible. The creation of parallel corpora
such as the Europarl, the CzEng, and the News Com-
mentary corpora should help in this direction by pro-
viding freely available language resources for build-
ing systems. The creation of an open source baseline
system should also go a long way towards achieving
this goal.
For more on the participating systems, please re-
fer to the respective system description in the pro-
ceedings of the workshop.
3 Human evaluation
We evaluated the shared task submissions using both
manual evaluation and automatic metrics. While
automatic measures are an invaluable tool for the
day-to-day development of machine translation sys-
tems, they are an imperfect substitute for human
assessment of translation quality. Manual evalua-
tion is time consuming and expensive to perform,
so comprehensive comparisons of multiple systems
are rare. For our manual evaluation we distributed
the workload across a number of people, including
participants in the shared task, interested volunteers,
and a small number of paid annotators. More than
100 people participated in the manual evaluation,
with 75 of those people putting in at least an hour?s
worth of effort. A total of 330 hours of labor was in-
vested, nearly doubling last year?s all-volunteer ef-
fort which yielded 180 hours of effort.
Beyond simply ranking the shared task submis-
sions, we had a number of scientific goals for the
manual evaluation. Firstly, we wanted to collect
data which could be used to assess how well au-
tomatic metrics correlate with human judgments.
Secondly, we wanted to examine different types of
manual evaluation and assess which was the best.
A number of criteria could be adopted for choos-
ing among different types of manual evaluation: the
ease with which people are able to perform the task,
their agreement with other annotators, their reliabil-
ity when asked to repeat judgments, or the number
of judgments which can be collected in a fixed time
period.
There are a range of possibilities for how human
139
evaluation of machine translation can be done. For
instance, it can be evaluated with reading compre-
hension tests (Jones et al, 2005), or by assigning
subjective scores to the translations of individual
sentences (LDC, 2005). We examined three differ-
ent ways of manually evaluating machine translation
quality:
? Assigning scores based on five point adequacy
and fluency scales
? Ranking translated sentences relative to each
other
? Ranking the translations of syntactic con-
stituents drawn from the source sentence
3.1 Fluency and adequacy
The most widely used methodology when manually
evaluatingMT is to assign values from two five point
scales representing fluency and adequacy. These
scales were developed for the annual NIST Machine
Translation Evaluation Workshop by the Linguistics
Data Consortium (LDC, 2005).
The five point scale for adequacy indicates how
much of the meaning expressed in the reference
translation is also expressed in a hypothesis trans-
lation:
5 = All
4 = Most
3 = Much
2 = Little
1 = None
The second five point scale indicates how fluent
the translation is. When translating into English the
values correspond to:
5 = Flawless English
4 = Good English
3 = Non-native English
2 = Disfluent English
1 = Incomprehensible
Separate scales for fluency and adequacy were
developed under the assumption that a translation
might be disfluent but contain all the information
from the source. However, in principle it seems that
people have a hard time separating these two as-
pects of translation. The high correlation between
people?s fluency and adequacy scores (given in Ta-
bles 17 and 18) indicate that the distinction might be
false.
?
people
's
Iraq
to
services
basic
other
and
,
care
health
,
food
provide
cannot
it
if
occupation
its
sustain
US
the
Can
?k
?
n
n
e
n
a
n
b
i
e
t
e
n
D
i
e
n
s
t
l
e
i
s
t
u
n
g
e
n
g
r
u
n
d
l
e
g
e
n
d
e
a
n
d
e
r
e
u
n
d
G
e
s
u
n
d
h
e
i
t
s
f
?
r
s
o
r
g
e
,N
a
h
r
u
n
g
n
i
c
h
t
V
o
l
k
i
r
a
k
i
s
c
h
e
n
d
e
m
s
i
e
w
e
n
n
,U
S
A
d
i
e
K
?
n
n
e
n
a
u
f
r
e
c
h
t
e
r
h
a
l
t
e
n
B
e
s
e
t
z
u
n
g
 
 
i
h
r
e
R
e
f
e
r
e
n
c
e
 
t
r
a
n
s
l
a
t
i
o
n
NP
NP
NP
VP
NP
VP
S
S
CNP
NP
Constituents selected 
for evaluation
Target phrases
highlighted via
word alignments
Parsed source
sentence
Figure 2: In constituent-based evaluation, the source
sentence was parsed, and automatically aligned with
the reference translation and systems? translations
Another problem with the scores is that there are
no clear guidelines on how to assign values to trans-
lations. No instructions are given to evaluators in
terms of how to quantify meaning, or how many
grammatical errors (or what sort) separates the dif-
ferent levels of fluency. Because of this many judges
either develop their own rules of thumb, or use the
scales as relative rather than absolute. These are
borne out in our analysis of inter-annotator agree-
ment in Section 6.
3.2 Ranking translations of sentences
Because fluency and adequacy were seemingly diffi-
cult things for judges to agree on, and because many
people from last year?s workshop seemed to be using
them as a way of ranking translations, we decided to
try a separate evaluation where people were simply
140
asked to rank translations. The instructions for this
task were:
Rank each whole sentence translation
from Best to Worst relative to the other
choices (ties are allowed).
These instructions were just as minimal as for flu-
ency and adequacy, but the task was considerably
simplified. Rather than having to assign each trans-
lation a value along an arbitrary scale, people simply
had to compare different translations of a single sen-
tence and rank them.
3.3 Ranking translations of syntactic
constituents
In addition to having judges rank the translations
of whole sentences, we also conducted a pilot
study of a new type of evaluation methodology,
which we call constituent-based evaluation. In our
constituent-based evaluation we parsed the source
language sentence, selected constituents from the
tree, and had people judge the translations of those
syntactic phrases. In order to draw judges? attention
to these regions, we highlighted the selected source
phrases and the corresponding phrases in the transla-
tions. The corresponding phrases in the translations
were located via automatic word alignments.
Figure 2 illustrates the constituent based evalu-
ation when applied to a German source sentence.
The German source sentence is parsed, and vari-
ous phrases are selected for evaluation. Word align-
ments are created between the source sentence and
the reference translation (shown), and the source
sentence and each of the system translations (not
shown). We parsed the test sentences for each of
the languages aside from Czech. We used Cowan
and Collins (2005)?s parser for Spanish, Arun and
Keller (2005)?s for French, Dubey (2005)?s for Ger-
man, and Bikel (2002)?s for English.
The word alignments were created with Giza++
(Och and Ney, 2003) applied to a parallel corpus
containing 200,000 sentence pairs of the training
data, plus sets of 4,007 sentence pairs created by
pairing the test sentences with the reference transla-
tions, and the test sentences paired with each of the
system translations. The phrases in the translations
were located using techniques from phrase-based
statistical machine translation which extract phrase
pairs fromword alignments (Koehn et al, 2003; Och
and Ney, 2004). Because the word-alignments were
created automatically, and because the phrase ex-
traction is heuristic, the phrases that were selected
may not exactly correspond to the translations of the
selected source phrase. We noted this in the instruc-
tions to judges:
Rank each constituent translation from
Best to Worst relative to the other choices
(ties are allowed). Grade only the high-
lighted part of each translation.
Please note that segments are selected au-
tomatically, and they should be taken as
an approximate guide. They might in-
clude extra words that are not in the actual
alignment, or miss words on either end.
The criteria that we used to select which con-
stituents were to be evaluated were:
? The constituent could not be the whole source
sentence
? The constituent had to be longer three words,
and be no longer than 15 words
? The constituent had to have a corresponding
phrase with a consistent word alignment in
each of the translations
The final criterion helped reduce the number of
alignment errors.
3.4 Collecting judgments
We collected judgments using a web-based tool.
Shared task participants were each asked to judge
200 sets of sentences. The sets consisted of 5 sys-
tem outputs, as shown in Figure 3. The judges
were presented with batches of each type of eval-
uation. We presented them with five screens of ade-
quacy/fluency scores, five screens of sentence rank-
ings, and ten screens of constituent rankings. The
order of the types of evaluation were randomized.
In order to measure intra-annotator agreement
10% of the items were repeated and evaluated twice
by each judge. In order to measure inter-annotator
agreement 40% of the items were randomly drawn
from a common pool that was shared across all
141
http://www.statmt.org/wmt07/shared-task/judge/do_task.php
WMT07 Manual Evaluation
Rank Segments
You have judged 25 sentences for WMT07 German-English News Corpus, 190 sentences total taking 64.9 seconds per sentence.
Source: K?nnen die USA ihre Besetzung aufrechterhalten, wenn sie dem irakischen Volk nicht Nahrung, Gesundheitsf?rsorge und andere 
grundlegende Dienstleistungen anbieten k?nnen?
Reference: Can the US sustain its occupation if it cannot provide food, health care, and other basic services to Iraq's people?
Translation Rank
The United States can maintain its employment when it the Iraqi people not food, health care and other 
basic services on offer?.
1
Worst
2 3 4 5
Best
The US can maintain its occupation, if they cannot offer the Iraqi people food, health care and other basic 
services?
1
Worst
2 3 4 5
Best
Can the US their occupation sustained if it to the Iraqi people not food, health care and other basic 
services can offer?
1
Worst
2 3 4 5
Best
Can the United States maintain their occupation, if the Iraqi people do not food, health care and other 
basic services can offer?
1
Worst
2 3 4 5
Best
The United States is maintained, if the Iraqi people, not food, health care and other basic services can 
offer?
1
Worst
2 3 4 5
Best
Annotator: ccb Task: WMT07 German-English News Corpus
Instructions: 
Rank each constituent translation from Best to Worst relative to the other choices (ties are allowed). Grade 
only the highlighted part of each translation.
Please note that segments are selected automatically, and they should be taken as an approximate guide. 
They might include extra words on either end that are not in the actual alignment, or miss words.
 
Figure 3: For each of the types of evaluation, judges were shown screens containing up to five different
system translations, along with the source sentence and reference translation.
annotators so that we would have items that were
judged by multiple annotators.
Judges were allowed to select whichever data set
they wanted, and to evaluate translations into what-
ever languages they were proficient in. Shared task
participants were excluded from judging their own
systems.
Table 2 gives a summary of the number of judg-
ments that we collected for translations of individ-
ual sentences. Since we had 14 translation tasks and
four different types of scores, there were 55 differ-
ent conditions.2 In total we collected over 81,000
judgments. Despite the large number of conditions
we managed to collect more than 1,000 judgments
for most of them. This provides a rich source of data
for analyzing the quality of translations produced by
different systems, the different types of human eval-
uation, and the correlation of automatic metrics with
human judgments.3
2We did not perform a constituent-based evaluation for
Czech to English because we did not have a syntactic parser
for Czech. We considered adapting our method to use Bojar
(2004)?s dependency parser for Czech, but did not have the time.
3The judgment data along with all system translations are
available at http://www.statmt.org/wmt07/
4 Automatic evaluation
The past two ACL workshops on machine trans-
lation used Bleu as the sole automatic measure of
translation quality. Bleu was used exclusively since
it is the most widely used metric in the field and
has been shown to correlate with human judgments
of translation quality in many instances (Dodding-
ton, 2002; Coughlin, 2003; Przybocki, 2004). How-
ever, recent work suggests that Bleu?s correlation
with human judgments may not be as strong as pre-
viously thought (Callison-Burch et al, 2006). The
results of last year?s workshop further suggested that
Bleu systematically underestimated the quality of
rule-based machine translation systems (Koehn and
Monz, 2006).
We used the manual evaluation data as a means of
testing the correlation of a range of automatic met-
rics in addition to Bleu. In total we used eleven
different automatic evaluation measures to rank the
shared task submissions. They are:
? Meteor (Banerjee and Lavie, 2005)?Meteor
measures precision and recall of unigrams
when comparing a hypothesis translation
142
Language Pair Test Set Adequacy Fluency Rank Constituent
English-German Europarl 1,416 1,418 1,419 2,626
News Commentary 1,412 1,413 1,412 2,755
German-English Europarl 1,525 1,521 1,514 2,999
News Commentary 1,626 1,620 1,601 3,084
English-Spanish Europarl 1,000 1,003 1,064 1,001
News Commentary 1,272 1,272 1,238 1,595
Spanish-English Europarl 1,174 1,175 1,224 1,898
News Commentary 947 949 922 1,339
English-French Europarl 773 772 769 1,456
News Commentary 729 735 728 1,313
French-English Europarl 834 833 830 1,641
News Commentary 1,041 1,045 1,035 2,036
English-Czech News Commentary 2,303 2,304 2,331 3,968
Czech-English News Commentary 1,711 1,711 1,733 0
Totals 17,763 17,771 17,820 27,711
Table 2: The number of items that were judged for each task during the manual evaluation
against a reference. It flexibly matches words
using stemming and WordNet synonyms. Its
flexible matching was extended to French,
Spanish, German and Czech for this workshop
(Lavie and Agarwal, 2007).
? Bleu (Papineni et al, 2002)?Bleu is currently
the de facto standard in machine translation
evaluation. It calculates n-gram precision and
a brevity penalty, and can make use of multi-
ple reference translations as a way of capturing
some of the allowable variation in translation.
We use a single reference translation in our ex-
periments.
? GTM (Melamed et al, 2003)?GTM general-
izes precision, recall, and F-measure to mea-
sure overlap between strings, rather than over-
lap between bags of items. An ?exponent? pa-
rameter which controls the relative importance
of word order. A value of 1.0 reduces GTM to
ordinary unigram overlap, with higher values
emphasizing order.4
? Translation Error Rate (Snover et al, 2006)?
4The GTM scores presented here are an F-measure with a
weight of 0.1, which counts recall at 10x the level of precision.
The exponent is set at 1.2, which puts a mild preference towards
items with words in the correct order. These parameters could
be optimized empirically for better results.
TER calculates the number of edits required to
change a hypothesis translation into a reference
translation. The possible edits in TER include
insertion, deletion, and substitution of single
words, and an edit which moves sequences of
contiguous words.
? ParaEval precision and ParaEval recall (Zhou
et al, 2006)?ParaEval matches hypothesis and
reference translations using paraphrases that
are extracted from parallel corpora in an unsu-
pervised fashion (Bannard and Callison-Burch,
2005). It calculates precision and recall using a
unigram counting strategy.
? Dependency overlap (Amigo? et al, 2006)?
This metric uses dependency trees for the hy-
pothesis and reference translations, by comput-
ing the average overlap between words in the
two trees which are dominated by grammatical
relationships of the same type.
? Semantic role overlap (Gime?nez and Ma`rquez,
2007)?This metric calculates the lexical over-
lap between semantic roles (i.e., semantic argu-
ments or adjuncts) of the same type in the the
hypothesis and reference translations. It uni-
formly averages lexical overlap over all seman-
tic role types.
143
? Word Error Rate over verbs (Popovic and Ney,
2007)?WER? creates a new reference and a
new hypothesis for each POS class by extract-
ing all words belonging to this class, and then
to calculate the standardWER.We show results
for this metric over verbs.
? Maximum correlation training on adequacy and
on fluency (Liu and Gildea, 2007)?a lin-
ear combination of different evaluation metrics
(Bleu, Meteor, Rouge, WER, and stochastic it-
erative alignment) with weights set to maxi-
mize Pearson?s correlation with adequacy and
fluency judgments. Weights were trained on
WMT-06 data.
The scores produced by these are given in the ta-
bles at the end of the paper, and described in Sec-
tion 5. We measured the correlation of the automatic
evaluation metrics with the different types of human
judgments on 12 data conditions, and report these in
Section 6.
5 Shared task results
The results of the human evaluation are given in Ta-
bles 9, 10, 11 and 12. Each of those tables present
four scores:
? FLUENCY and ADEQUACY are normalized ver-
sions of the five point scores described in Sec-
tion 3.1. The tables report an average of the
normalized scores.5
? RANK is the average number of times that a
system was judged to be better than any other
system in the sentence ranking evaluation de-
scribed in Section 3.2.
? CONSTITUENT is the average number of times
that a system was judged to be better than any
other system in the constituent-based evalua-
tion described in Section 3.3.
There was reasonably strong agreement between
these four measures at which of the entries was the
best in each data condition. There was complete
5Since different annotators can vary widely in how they as-
sign fluency and adequacy scores, we normalized these scores
on a per-judge basis using the method suggested by Blatz et al
(2003) in Chapter 5, page 97.
SYSTRAN (systran) 32%
University of Edinburgh (uedin) 20%
University of Catalonia (upc) 15%
LIMSI-CNRS (limsi) 13%
University of Maryland (umd) 5%
National Research Council of Canada?s
joint entry with SYSTRAN (systran-nrc)
5%
Commercial Czech-English system (pct) 5%
University of Valencia (upv) 2%
Charles University (cu) 2%
Table 3: The proportion of time that participants?
entries were top-ranked in the human evaluation
University of Edinburgh (uedin) 41%
University of Catalonia (upc) 12%
LIMSI-CNRS (limsi) 12%
University of Maryland (umd) 9%
Charles University (cu) 4%
Carnegie Mellon University (cmu-syntax) 4%
Carnegie Mellon University (cmu-uka) 4%
University of California at Berkeley (ucb) 3%
National Research Council?s joint entry
with SYSTRAN (systran-nrc)
2%
SYSTRAN (systran) 2%
Saarland University (saar) 0.8%
Table 4: The proportion of time that participants?
entries were top-ranked by the automatic evaluation
metrics
agreement between them in 5 of the 14 conditions,
and agreement between at least three of them in 10
of the 14 cases.
Table 3 gives a summary of how often differ-
ent participants? entries were ranked #1 by any of
the four human evaluation measures. SYSTRAN?s
entries were ranked the best most often, followed
by University of Edinburgh, University of Catalonia
and LIMSI-CNRS.
The following systems were the best perform-
ing for the different language pairs: SYSTRAN
was ranked the highest in German-English, Uni-
versity of Catalonia was ranked the highest in
Spanish-English, LIMSI-CNRS was ranked high-
est in French-English, and the University of Mary-
land and a commercial system were the highest for
144
Evaluation type P (A) P (E) K
Fluency (absolute) .400 .2 .250
Adequacy (absolute) .380 .2 .226
Fluency (relative) .520 .333 .281
Adequacy (relative) .538 .333 .307
Sentence ranking .582 .333 .373
Constituent ranking .693 .333 .540
Constituent ranking .712 .333 .566
(w/identical constituents)
Table 5: Kappa coefficient values representing the
inter-annotator agreement for the different types of
manual evaluation
Czech-English.
While we consider the human evaluation to be
primary, it is also interesting to see how the en-
tries were ranked by the various automatic evalua-
tion metrics. The complete set of results for the auto-
matic evaluation are presented in Tables 13, 14, 15,
and 16. An aggregate summary is provided in Table
4. The automatic evaluation metrics strongly favor
the University of Edinburgh, which garners 41% of
the top-ranked entries (which is partially due to the
fact it was entered in every language pair). Signif-
icantly, the automatic metrics disprefer SYSTRAN,
which was strongly favored in the human evaluation.
6 Meta-evaluation
In addition to evaluating the translation quality of
the shared task entries, we also performed a ?meta-
evaluation? of our evaluation methodologies.
6.1 Inter- and Intra-annotator agreement
We measured pairwise agreement among annotators
using the kappa coefficient (K) which is widely used
in computational linguistics for measuring agree-
ment in category judgments (Carletta, 1996). It is
defined as
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. We define chance
agreement for fluency and adequacy as 15 , since they
are based on five point scales, and for ranking as 13
Evaluation type P (A) P (E) K
Fluency (absolute) .630 .2 .537
Adequacy (absolute) .574 .2 .468
Fluency (relative) .690 .333 .535
Adequacy (relative) .696 .333 .544
Sentence ranking .749 .333 .623
Constituent ranking .825 .333 .738
Constituent ranking .842 .333 .762
(w/identical constituents)
Table 6: Kappa coefficient values for intra-annotator
agreement for the different types of manual evalua-
tion
since there are three possible out comes when rank-
ing the output of a pair of systems: A > B, A = B,
A < B.
For inter-annotator agreement we calculated
P (A) for fluency and adequacy by examining all
items that were annotated by two or more annota-
tors, and calculating the proportion of time they as-
signed identical scores to the same items. For the
ranking tasks we calculated P (A) by examining all
pairs of systems which had been judged by two or
more judges, and calculated the proportion of time
that they agreed that A > B, A = B, or A < B.
For intra-annotator agreement we did similarly, but
gathered items that were annotated on multiple oc-
casions by a single annotator.
Table 5 gives K values for inter-annotator agree-
ment, and Table 6 gives K values for intra-annoator
agreement. These give an indication of how often
different judges agree, and how often single judges
are consistent for repeated judgments, respectively.
The interpretation of Kappa varies, but according to
Landis and Koch (1977) 0??.2 is slight, .21??.4
is fair, .41??.6 is moderate, .61??.8 is substantial
and the rest almost perfect.
The K values for fluency and adequacy should
give us pause about using these metrics in the fu-
ture. When we analyzed them as they are intended to
be?scores classifying the translations of sentences
into different types?the inter-annotator agreement
was barely considered fair, and the intra-annotator
agreement was only moderate. Even when we re-
assessed fluency and adequacy as relative ranks the
agreements increased only minimally.
145
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 0.08
 0  10  20  30  40  50  60
n
u
m
 
s
e
n
t
e
n
c
e
s
 
t
a
k
i
n
g
 
t
h
i
s
 
l
o
n
g
 
(
%
)
time to judge one sentence (seconds)
constituent ranksentence rankfluency+adequacy scoring
Figure 4: Distributions of the amount of time it took
to judge single sentences for the three types of man-
ual evaluation
The agreement on the other two types of man-
ual evaluation that we introduced were considerably
better. The both the sentence and constituent ranking
had moderate inter-annotator agreement and sub-
stantial intra-annotator agreement. Because the con-
stituent ranking examined the translations of short
phrases, often times all systems produced the same
translations. Since these trivially increased agree-
ment (since they would always be equally ranked)
we also evaluated the inter- and intra-annotator
agreement when those items were excluded. The
agreement remained very high for constituent-based
evaluation.
6.2 Timing
We used the web interface to collect timing infor-
mation. The server recorded the time when a set of
sentences was given to a judge and the time when
the judge returned the sentences. We divided the
time that it took to do a set by the number of sen-
tences in the set. The average amount of time that it
took to assign fluency and adequacy to a single sen-
tence was 26 seconds.6 The average amount of time
it took to rank a sentence in a set was 20 seconds.
The average amount of time it took to rank a high-
lighted constituent was 11 seconds. Figure 4 shows
the distribution of times for these tasks.
6Sets which took longer than 5 minutes were excluded from
these calculations, because there was a strong chance that anno-
tators were interrupted while completing the task.
These timing figures are promising because they
indicate that the tasks which the annotators were the
most reliable on (constituent ranking and sentence
ranking) were also much quicker to complete than
the ones that they were unreliable on (assigning flu-
ency and adequacy scores). This suggests that flu-
ency and adequacy should be replaced with ranking
tasks in future evaluation exercises.
6.3 Correlation between automatic metrics and
human judgments
To measure the correlation of the automatic metrics
with the human judgments of translation quality we
used Spearman?s rank correlation coefficient ?. We
opted for Spearman rather than Pearson because it
makes fewer assumptions about the data. Impor-
tantly, it can be applied to ordinal data (such as the
fluency and adequacy scales). Spearman?s rank cor-
relation coefficient is equivalent to Pearson correla-
tion on ranks.
After the raw scores that were assigned to systems
by an automatic metric and by one of our manual
evaluation techniques have been converted to ranks,
we can calculate ? using the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher value for ? is
making predictions that are more similar to the hu-
man judgments than an automatic evaluation metric
with a lower ?.
Table 17 reports ? for the metrics which were
used to evaluate translations into English.7. Table
7 summarizes the results by averaging the correla-
tion numbers by equally weighting each of the data
conditions. The table ranks the automatic evalua-
tion metrics based on how well they correlated with
human judgments. While these are based on a rela-
tively few number of items, and while we have not
performed any tests to determine whether the dif-
ferences in ? are statistically significant, the results
7The Czech-English conditions were excluded since there
were so few systems
146
are nevertheless interesting, since three metrics have
higher correlation than Bleu:
? Semantic role overlap (Gime?nez and Ma`rquez,
2007), which makes its debut in the proceed-
ings of this workshop
? ParaEval measuring recall (Zhou et al, 2006),
which has a model of allowable variation in
translation that uses automatically generated
paraphrases (Callison-Burch, 2007)
? Meteor (Banerjee and Lavie, 2005) which also
allows variation by introducing synonyms and
by flexibly matches words using stemming.
Tables 18 and 8 report ? for the six metrics which
were used to evaluate translations into the other lan-
guages. Here we find that Bleu and TER are the
closest to human judgments, but that overall the cor-
relations are much lower than for translations into
English.
7 Conclusions
Similar to last year?s workshop we carried out an ex-
tensive manual and automatic evaluation of machine
translation performance for translating from four
European languages into English, and vice versa.
This year we substantially increased the number of
automatic evaluation metrics and were also able to
nearly double the efforts of producing the human
judgments.
There were substantial differences in the results
results of the human and automatic evaluations. We
take the human judgments to be authoritative, and
used them to evaluate the automatic metrics. We
measured correlation using Spearman?s coefficient
and found that three less frequently used metrics
were stronger predictors of human judgments than
Bleu. They were: semantic role overlap (newly in-
troduced in this workshop) ParaEval-recall and Me-
teor.
Although we do not claim that our observations
are indisputably conclusive, they again indicate that
the choice of automatic metric can have a signifi-
cant impact on comparing systems. Understanding
the exact causes of those differences still remains an
important issue for future research.
metric A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
O
V
E
R
A
L
L
Semantic
role overlap
.774 .839 .803 .741 .789
ParaEval-
Recall
.712 .742 .768 .798 .755
Meteor .701 .719 .745 .669 .709
Bleu .690 .722 .672 .602 .671
1-TER .607 .538 .520 .514 .644
Max adequ-
correlation
.651 .657 .659 .534 .626
Max fluency
correlation
.644 .653 .656 .512 .616
GTM .655 .674 .616 .495 .610
Dependency
overlap
.639 .644 .601 .512 .599
ParaEval-
Precision
.639 .654 .610 .491 .598
1-WER of
verbs
.378 .422 .431 .297 .382
Table 7: Average corrections for the different auto-
matic metrics when they are used to evaluate trans-
lations into English
metric A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
O
V
E
R
A
L
L
Bleu .657 .445 .352 .409 .466
1-TER .589 .419 .361 .380 .437
Max fluency
correlation
.534 .419 .368 .400 .430
Max adequ-
correlation
.498 .414 .385 .409 .426
Meteor .490 .356 .279 .304 .357
1-WER of
verbs
.371 .304 .359 .359 .348
Table 8: Average corrections for the different auto-
matic metrics when they are used to evaluate trans-
lations into the other languages
147
This year?s evaluation also measured the agree-
ment between human assessors by computing the
Kappa coefficient. One striking observation is
that inter-annotator agreement for fluency and ad-
equacy can be called ?fair? at best. On the other
hand, comparing systems by ranking them manually
(constituents or entire sentences), resulted in much
higher inter-annotator agreement.
Acknowledgments
This work was supported in part by the EuroMa-
trix project funded by the European Commission
(6th Framework Programme), and in part by the
GALE program of the US Defense Advanced Re-
search Projects Agency, Contract No. HR0011-06-
C-0022.
We are grateful to Jesu?s Gime?nez, Dan Melamed,
Maja Popvic, Ding Liu, Liang Zhou, and Abhaya
Agarwal for scoring the entries with their automatic
evaluation metrics. Thanks to Brooke Cowan for
parsing the Spanish test sentences, to Josh Albrecht
for his script for normalizing fluency and adequacy
on a per judge basis, and to Dan Melamed, Rebecca
Hwa, Alon Lavie, Colin Bannard andMirella Lapata
for their advice about statistical tests.
References
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of COLING-ACL06.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of ACL.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, Michigan.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL-2005.
Dan Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of HLT.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. CLSP Summer Workshop Final
Report WS2003, Johns Hopkins University.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2006. CzEng:
Czech-English Parallel Corpus, Release version 0.5.
Prague Bulletin of Mathematical Linguistics, 86.
Ondr?ej Bojar. 2004. Problems of inducing large
coverage constraint-based dependency grammar for
Czech. In Constraint Solving and Language Process-
ing, CSLP 2004, volume LNAI 3438. Springer.
Ondr?ej Bojar. 2007. English-to-Czech factored machine
translation. In Proceedings of the ACL-2007 Work-
shop on Statistical Machine Translation (WMT-07),
Prague.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In Proceedings of EACL.
Chris Callison-Burch. 2007. Paraphrasing and Transla-
tion. Ph.D. thesis, University of Edinburgh, Scotland.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison. 2007.
Multi-engine machine translation with an open-source
decoder for statistical machine translation. In Pro-
ceedings of the ACL-2007 Workshop on Statistical Ma-
chine Translation (WMT-07), Prague.
Jorge Civera and Alfons Juan. 2007. Domain adaptation
in statistical machine translation with mixture mod-
elling. In Proceedings of the ACL-2007 Workshop on
Statistical Machine Translation (WMT-07), Prague.
Marta R. Costa-Jussa` and Jose? A.R. Fonollosa. 2007.
Analysis of statistical and morphological classes to
generate weighted reordering hypotheses on a statisti-
cal machine translation system. In Proceedings of the
ACL-2007 Workshop on Statistical Machine Transla-
tion (WMT-07), Prague.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality. In
Proceedings of MT Summit IX.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of Spanish. In
Proceedings of EMNLP 2005.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Human Language Technology: Notebook
Proceedings, pages 128?132, San Diego.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In Proceedings of ACL.
148
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of the ACL-2007
Workshop on Statistical Machine Translation (WMT-
07), Prague.
Christopher J. Dyer. 2007. The ?noisier channel?: trans-
lation from morphologically complex languages. In
Proceedings of the ACL-2007 Workshop on Statistical
Machine Translation (WMT-07), Prague.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proceedings of
International Workshop on Spoken Language Transla-
tion.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguistic fea-
tures for automatic evaluation of heterogenous mt sys-
tems. In Proceedings of ACL Workshop on Statistical
Machine Translation.
Maria Holmqvist, Sara Stymne, and Lars Ahrenberg.
2007. Getting to know Moses: Initial experiments
on German-English factored translation. In Proceed-
ings of the ACL-2007 Workshop on Statistical Machine
Translation (WMT-07), Prague.
Douglas Jones, Wade Shen, Neil Granoien, Martha Her-
zog, and Clifford Weinstein. 2005. Measuring trans-
lation quality by testing english speakers with a new
defense language proficiency test for arabic. In Pro-
ceedings of the 2005 International Conference on In-
telligence Analysis.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between European lan-
guages. In Proceedings of ACL 2005 Workshop on
Parallel Text Translation.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the ACL-2007 Workshop on
Statistical Machine Translation (WMT-07), Prague.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris
Callison-Burch, Alexandra Constantin, Brooke
Cowan, Chris Dyer, Marcello Federico, Evan Herbst,
Hieu Hoang, Christine Moran, Wade Shen, and
Richard Zens. 2006. Factored translation models.
CLSP Summer Workshop Final Report WS-2006,
Johns Hopkins University.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159?174.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels
of correlation with human judgments. In Proceedings
of the Workshop on Statistical Machine Translation,
Prague, June. Association for Computational Linguis-
tics.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Audrey Lee. 2006. NIST 2006 machine translation eval-
uation official results. Official release of automatic
evaluation scores for all submissions, November.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In Proceedings of NAACL.
Dan Melamed, Ryan Green, and Jospeh P. Turian. 2003.
Precision and recall of machine translation. In Pro-
ceedings of HLT/NAACL.
Preslav Nakov and Marti Hearst. 2007. UCB system de-
scription for the WMT 2007 shared task. In Proceed-
ings of the ACL-2007 Workshop on Statistical Machine
Translation (WMT-07), Prague.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Michael Paul. 2006. Overview of the IWSLT 2006
evaluation campaign. In Proceedings of International
Workshop on Spoken Language Translation.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja
Hildebrand, and Stephan Vogel. 2007. The ISL
phrase-based MT system for the 2007 ACL Workshop
on Statistical Machine Translation. In Proceedings of
the ACL-2007 Workshop on Statistical Machine Trans-
lation (WMT-07), Prague.
149
Maja Popovic and Hermann Ney. 2007. Word error rates:
Decomposition over POS classes and applications for
error analysis. In Proceedings of ACL Workshop on
Statistical Machine Translation.
Mark Przybocki. 2004. NIST 2004 machine translation
evaluation results. Confidential e-mail to workshop
participants, May.
Holger Schwenk. 2007. Building a statistical machine
translation system for French using the Europarl cor-
pus. In Proceedings of the ACL-2007 Workshop on
Statistical Machine Translation (WMT-07), Prague.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and
Roland Kuhn. 2007. Rule-based translation with sta-
tistical phrase-based post-editing. In Proceedings of
the ACL-2007 Workshop on Statistical Machine Trans-
lation (WMT-07), Prague.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Statistical Machine
Translation in the Americas.
Nicola Ueffing, Michel Simard, Samuel Larkin, and
Howard Johnson. 2007. NRC?s PORTAGE system for
WMT 2007. In Proceedings of the ACL-2007 Work-
shop on Statistical Machine Translation (WMT-07),
Prague.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proceedings of EMNLP.
Andreas Zollmann, Ashish Venugopal, Matthias Paulik,
and Stephan Vogel. 2007. The syntax augmented MT
(SAMT) system for the shared task in the 2007 ACL
Workshop on Statistical Machine Translation. In Pro-
ceedings of the ACL-2007 Workshop on Statistical Ma-
chine Translation (WMT-07), Prague.
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
German-English Europarl
cmu-uka 0.511 0.496 0.395 0.206
liu 0.541 0.55 0.415 0.234
nrc 0.474 0.459 0.354 0.214
saar 0.334 0.404 0.119 0.104
systran 0.562 0.594 0.530 0.302
uedin 0.53 0.554 0.43 0.187
upc 0.534 0.533 0.384 0.214
German-English News Corpus
nrc 0.459 0.429 0.325 0.245
saar 0.278 0.341 0.108 0.125
systran 0.552 0.56 0.563 0.344
uedin 0.508 0.536 0.485 0.332
upc 0.536 0.512 0.476 0.330
English-German Europarl
cmu-uka 0.557 0.508 0.416 0.333
nrc 0.534 0.511 0.328 0.321
saar 0.369 0.383 0.172 0.196
systran 0.543 0.525 0.511 0.295
uedin 0.569 0.576 0.389 0.350
upc 0.565 0.522 0.438 0.3
English-German News Corpus
nrc 0.453 0.4 0.437 0.340
saar 0.186 0.273 0.108 0.121
systran 0.542 0.556 0.582 0.351
ucb 0.415 0.403 0.332 0.289
uedin 0.472 0.445 0.455 0.303
upc 0.505 0.475 0.377 0.349
Table 9: Human evaluation for German-English sub-
missions
150
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
Spanish-English Europarl
cmu-syntax 0.552 0.568 0.478 0.152
cmu-uka 0.557 0.564 0.392 0.139
nrc 0.477 0.489 0.382 0.143
saar 0.328 0.336 0.126 0.075
systran 0.525 0.566 0.453 0.156
uedin 0.593 0.610 0.419 0.14
upc 0.587 0.604 0.5 0.188
upv 0.562 0.573 0.326 0.154
Spanish-English News Corpus
cmu-uka 0.522 0.495 0.41 0.213
nrc 0.479 0.464 0.334 0.243
saar 0.446 0.46 0.246 0.198
systran 0.525 0.503 0.453 0.22
uedin 0.546 0.534 0.48 0.268
upc 0.566 0.543 0.537 0.312
upv 0.435 0.459 0.295 0.151
English-Spanish Europarl
cmu-uka 0.563 0.581 0.391 0.23
nrc 0.546 0.548 0.323 0.22
systran 0.495 0.482 0.329 0.224
uedin 0.586 0.638 0.468 0.225
upc 0.584 0.578 0.444 0.239
upv 0.573 0.587 0.406 0.246
English-Spanish News Corpus
cmu-uka 0.51 0.492 0.45 0.277
nrc 0.408 0.392 0.367 0.224
systran 0.501 0.507 0.481 0.352
ucb 0.449 0.414 0.390 0.307
uedin 0.429 0.419 0.389 0.266
upc 0.51 0.488 0.404 0.311
upv 0.405 0.418 0.250 0.217
Table 10: Human evaluation for Spanish-English
submissions
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
French-English Europarl
limsi 0.634 0.618 0.458 0.290
nrc 0.553 0.551 0.404 0.253
saar 0.384 0.447 0.176 0.157
systran 0.494 0.484 0.286 0.202
systran-nrc 0.604 0.6 0.503 0.267
uedin 0.616 0.635 0.514 0.283
upc 0.616 0.619 0.448 0.267
French-English News Corpus
limsi 0.575 0.596 0.494 0.312
nrc 0.472 0.442 0.306 0.241
saar 0.280 0.372 0.183 0.159
systran 0.553 0.534 0.469 0.288
systran-nrc 0.513 0.49 0.464 0.290
uedin 0.556 0.586 0.493 0.306
upc 0.576 0.587 0.493 0.291
English-French Europarl
limsi 0.635 0.627 0.505 0.259
nrc 0.517 0.518 0.359 0.206
saar 0.398 0.448 0.155 0.139
systran 0.574 0.526 0.353 0.179
systran-nrc 0.575 0.58 0.512 0.225
uedin 0.620 0.608 0.485 0.273
upc 0.599 0.566 0.45 0.256
English-French News Corpus
limsi 0.537 0.495 0.44 0.363
nrc 0.481 0.484 0.372 0.324
saar 0.243 0.276 0.086 0.121
systran 0.536 0.546 0.634 0.440
systran-nrc 0.557 0.572 0.485 0.287
ucb 0.401 0.391 0.316 0.245
uedin 0.466 0.447 0.485 0.375
upc 0.509 0.469 0.437 0.326
Table 11: Human evaluation for French-English
submissions
151
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
Czech-English News Corpus
cu 0.468 0.478 0.362 ?
pct 0.418 0.388 0.220 ?
uedin 0.458 0.471 0.353 ?
umd 0.550 0.592 0.627 ?
English-Czech News Corpus
cu 0.523 0.510 0.405 0.440
pct 0.542 0.541 0.499 0.381
uedin 0.449 0.433 0.249 0.258
Table 12: Human evaluation for Czech-English sub-
missions
152
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
A
L
L
PA
R
A
E
V
A
L
-P
R
E
C
IS
IO
N
D
E
P
E
N
D
E
N
C
Y
-O
V
E
R
L
A
P
S
E
M
A
N
T
IC
-R
O
L
E
-O
V
E
R
L
A
P
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
E
N
C
Y
M
A
X
-C
O
R
R
-A
D
E
Q
U
A
C
Y
German-English Europarl
cmu-uka 0.559 0.247 0.326 0.455 0.528 0.531 0.259 0.182 0.848 1.91 1.910
liu 0.559 0.263 0.329 0.460 0.537 0.535 0.276 0.197 0.846 1.91 1.910
nrc 0.551 0.253 0.324 0.454 0.528 0.532 0.263 0.185 0.848 1.88 1.88
saar 0.477 0.198 0.313 0.447 0.44 0.527 0.228 0.157 0.846 1.76 1.710
systran 0.560 0.268 0.342 0.463 0.543 0.541 0.261 0.21 0.849 1.91 1.91
systran-2 0.501 0.154 0.238 0.376 0.462 0.448 0.237 0.154 ? 1.71 1.73
uedin 0.56 0.277 0.319 0.480 0.536 0.562 0.298 0.217 0.855 1.96 1.940
upc 0.541 0.250 0.343 0.470 0.506 0.551 0.27 0.193 0.846 1.89 1.88
German-English News Corpus
nrc 0.563 0.221 0.333 0.454 0.514 0.514 0.246 0.157 0.868 1.920 1.91
saar 0.454 0.159 0.288 0.413 0.405 0.467 0.193 0.120 0.86 1.700 1.64
systran 0.570 0.200 0.275 0.418 0.531 0.472 0.274 0.18 0.858 1.910 1.93
systran-2 0.556 0.169 0.238 0.397 0.511 0.446 0.258 0.163 ? 1.86 1.88
uedin 0.577 0.242 0.339 0.459 0.534 0.524 0.287 0.181 0.871 1.98 1.970
upc 0.575 0.233 0.339 0.455 0.527 0.516 0.265 0.171 0.865 1.96 1.96
English-German Europarl
cmu-uka 0.268 0.189 0.251 ? ? ? ? ? 0.884 1.66 1.63
nrc 0.272 0.185 0.221 ? ? ? ? ? 0.882 1.660 1.630
saar 0.239 0.174 0.237 ? ? ? ? ? 0.881 1.61 1.56
systran 0.198 0.123 0.178 ? ? ? ? ? 0.866 1.46 1.42
uedin 0.277 0.201 0.273 ? ? ? ? ? 0.889 1.690 1.66
upc 0.266 0.177 0.195 ? ? ? ? ? 0.88 1.640 1.62
English-German News Corpus
nrc 0.257 0.157 0.25 ? ? ? ? ? 0.891 1.590 1.560
saar 0.162 0.098 0.212 ? ? ? ? ? 0.881 1.400 1.310
systran 0.223 0.143 0.266 ? ? ? ? ? 0.887 1.55 1.500
ucb 0.256 0.156 0.249 ? ? ? ? ? 0.889 1.59 1.56
ucb-2 0.252 0.152 0.229 ? ? ? ? ? ? 1.57 1.55
uedin 0.266 0.166 0.266 ? ? ? ? ? 0.891 1.600 1.58
upc 0.256 0.167 0.266 ? ? ? ? ? 0.89 1.590 1.56
Table 13: Automatic evaluation scores for German-English submissions
153
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
PA
R
A
E
V
A
L
-P
R
E
C
D
E
P
E
N
D
E
N
C
Y
S
E
M
A
N
T
IC
-R
O
L
E
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
Spanish-English Europarl
cmu-syntax 0.602 0.323 0.414 0.499 0.59 0.588 0.338 0.254 0.866 2.10 2.090
cmu-syntax-2 0.603 0.321 0.408 0.494 0.593 0.584 0.336 0.249 ? 2.09 2.09
cmu-uka 0.597 0.32 0.42 0.501 0.581 0.595 0.336 0.247 0.867 2.09 2.080
nrc 0.596 0.313 0.402 0.484 0.581 0.581 0.321 0.227 0.867 2.04 2.04
saar 0.542 0.245 0.32 0.432 0.531 0.511 0.272 0.198 0.854 1.870 1.870
systran 0.593 0.290 0.364 0.469 0.586 0.550 0.321 0.238 0.858 2.02 2.03
systran-2 0.535 0.202 0.288 0.406 0.524 0.49 0.263 0.187 ? 1.81 1.84
uedin 0.6 0.324 0.414 0.499 0.584 0.589 0.339 0.252 0.868 2.09 2.080
upc 0.600 0.322 0.407 0.492 0.593 0.583 0.334 0.253 0.865 2.08 2.08
upv 0.594 0.315 0.400 0.493 0.582 0.581 0.329 0.249 0.865 2.060 2.06
Spanish-English News Corpus
cmu-uka 0.64 0.299 0.428 0.497 0.617 0.575 0.339 0.246 0.89 2.17 2.17
cmu-uka-2 0.64 0.297 0.427 0.496 0.616 0.574 0.339 0.246 ? 2.17 2.17
nrc 0.641 0.299 0.434 0.499 0.615 0.584 0.329 0.238 0.892 2.160 2.160
saar 0.607 0.244 0.338 0.447 0.587 0.512 0.303 0.208 0.879 2.04 2.05
systran 0.628 0.259 0.35 0.453 0.611 0.523 0.325 0.221 0.877 2.08 2.10
systran-2 0.61 0.233 0.321 0.438 0.602 0.506 0.311 0.209 ? 2.020 2.050
uedin 0.661 0.327 0.457 0.512 0.634 0.595 0.363 0.264 0.893 2.25 2.24
upc 0.654 0.346 0.480 0.528 0.629 0.616 0.363 0.265 0.895 2.240 2.23
upv 0.638 0.283 0.403 0.485 0.614 0.562 0.334 0.234 0.887 2.15 2.140
English-Spanish Europarl
cmu-uka 0.333 0.311 0.389 ? ? ? ? ? 0.889 1.98 2.00
nrc 0.322 0.299 0.376 ? ? ? ? ? 0.886 1.92 1.940
systran 0.269 0.212 0.301 ? ? ? ? ? 0.878 1.730 1.760
uedin 0.33 0.316 0.399 ? ? ? ? ? 0.891 1.980 1.990
upc 0.327 0.312 0.393 ? ? ? ? ? 0.89 1.960 1.98
upv 0.323 0.304 0.379 ? ? ? ? ? 0.887 1.95 1.97
English-Spanish News Corpus
cmu-uka 0.368 0.327 0.469 ? ? ? ? ? 0.903 2.070 2.090
cmu-uka-2 0.355 0.306 0.461 ? ? ? ? ? ? 2.04 2.060
nrc 0.362 0.311 0.448 ? ? ? ? ? 0.904 2.04 2.060
systran 0.335 0.281 0.439 ? ? ? ? ? 0.906 1.970 2.010
ucb 0.374 0.331 0.464 ? ? ? ? ? ? 2.09 2.11
ucb-2 0.375 0.325 0.456 ? ? ? ? ? ? 2.09 2.110
ucb-3 0.372 0.324 0.457 ? ? ? ? ? ? 2.08 2.10
uedin 0.361 0.322 0.479 ? ? ? ? ? 0.907 2.08 2.09
upc 0.361 0.328 0.467 ? ? ? ? ? 0.902 2.06 2.08
upv 0.337 0.285 0.432 ? ? ? ? ? 0.900 1.98 2.000
Table 14: Automatic evaluation scores for Spanish-English submissions
154
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
PA
R
A
E
V
A
L
-P
R
E
C
D
E
P
E
N
D
E
N
C
Y
S
E
M
A
N
T
IC
-R
O
L
E
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
French-English Europarl
limsi 0.604 0.332 0.418 0.504 0.589 0.591 0.344 0.259 0.865 2.100 2.10
limsi-2 0.602 0.33 0.417 0.504 0.587 0.592 0.302 0.257 ? 2.05 2.05
nrc 0.594 0.312 0.403 0.488 0.578 0.58 0.324 0.244 0.861 2.05 2.050
saar 0.534 0.249 0.354 0.459 0.512 0.546 0.279 0.202 0.856 1.880 1.88
systran 0.549 0.211 0.308 0.417 0.525 0.501 0.277 0.201 0.849 1.850 1.890
systran-nrc 0.594 0.313 0.404 0.492 0.578 0.580 0.330 0.248 0.862 2.06 2.060
uedin 0.595 0.318 0.424 0.505 0.574 0.599 0.338 0.254 0.865 2.08 2.08
upc 0.6 0.319 0.409 0.495 0.588 0.583 0.337 0.255 0.861 2.08 2.080
French-English News Corpus
limsi 0.595 0.279 0.405 0.478 0.563 0.555 0.289 0.235 0.875 2.030 2.020
nrc 0.587 0.257 0.389 0.470 0.557 0.546 0.301 0.22 0.876 2.020 2.020
saar 0.503 0.206 0.301 0.418 0.475 0.476 0.245 0.169 0.864 1.80 1.78
systran 0.568 0.202 0.28 0.415 0.554 0.472 0.292 0.198 0.866 1.930 1.96
systran-nrc 0.591 0.269 0.398 0.475 0.558 0.547 0.323 0.226 0.875 2.050 2.06
uedin 0.602 0.27 0.392 0.471 0.569 0.545 0.326 0.233 0.875 2.07 2.07
upc 0.596 0.275 0.400 0.476 0.567 0.552 0.322 0.233 0.876 2.06 2.06
English-French Europarl
limsi 0.226 0.306 0.366 ? ? ? ? ? 0.891 1.940 1.96
nrc 0.218 0.294 0.354 ? ? ? ? ? 0.888 1.930 1.96
saar 0.190 0.262 0.333 ? ? ? ? ? 0.892 1.86 1.87
systran 0.179 0.233 0.313 ? ? ? ? ? 0.885 1.79 1.83
systran-nrc 0.220 0.301 0.365 ? ? ? ? ? 0.892 1.940 1.960
uedin 0.207 0.262 0.301 ? ? ? ? ? 0.886 1.930 1.950
upc 0.22 0.299 0.379 ? ? ? ? ? 0.892 1.940 1.960
English-French News Corpus
limsi 0.206 0.255 0.354 ? ? ? ? ? 0.897 1.84 1.87
nrc 0.208 0.257 0.369 ? ? ? ? ? 0.9 1.87 1.900
saar 0.151 0.188 0.308 ? ? ? ? ? 0.896 1.65 1.65
systran 0.199 0.243 0.378 ? ? ? ? ? 0.901 1.860 1.90
systran-nrc 0.23 0.290 0.408 ? ? ? ? ? 0.903 1.940 1.98
ucb 0.201 0.237 0.366 ? ? ? ? ? 0.897 1.830 1.860
uedin 0.197 0.234 0.340 ? ? ? ? ? 0.899 1.87 1.890
upc 0.212 0.263 0.391 ? ? ? ? ? 0.900 1.87 1.90
Table 15: Automatic evaluation scores for French-English submissions
155
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
PA
R
A
E
V
A
L
-P
R
E
C
D
E
P
E
N
D
E
N
C
Y
S
E
M
A
N
T
IC
-R
O
L
E
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
Czech-English News Corpus
cu 0.545 0.215 0.334 0.441 0.502 0.504 0.245 0.165 0.867 1.87 1.88
cu-2 0.558 0.223 0.344 0.447 0.510 0.514 0.254 0.17 ? 1.90 1.910
uedin 0.54 0.217 0.340 0.445 0.497 0.51 0.243 0.160 0.865 1.860 1.870
umd 0.581 0.241 0.355 0.460 0.531 0.526 0.273 0.184 0.868 1.96 1.97
English-Czech News Corpus
cu 0.429 0.134 0.231 ? ? ? ? ? ? 1.580 1.53
cu-2 0.430 0.132 0.219 ? ? ? ? ? ? 1.58 1.520
uedin 0.42 0.119 0.211 ? ? ? ? ? ? 1.550 1.49
Table 16: Automatic evaluation scores for Czech-English submissions
156
ADEQUACY
FLUENCY
RANK
CONSTITUENT
METEOR
BLEU
1-TER
GTM
PARAEVAL-REC
PARAEVAL-PREC
DEPENDENCY
SEMANTIC-ROLE
1-WER-OF-VS
MAX-CORR-FLU
MAX-CORR-ADEQ
G
erm
an-E
nglish
N
ew
s
C
orpus
adequacy
1
0.900
0.900
0.900
0.600
0.300
-0.025
0.300
0.700
0.300
0.700
0.700
-0.300
0.300
0.600
fl
uency
?
1
1.000
1.000
0.700
0.400
-0.025
0.400
0.900
0.400
0.900
0.900
-0.100
0.400
0.700
rank
?
?
1
1.000
0.700
0.400
-0.025
0.400
0.900
0.400
0.900
0.900
-0.100
0.400
0.700
constituent
?
?
?
1
0.700
0.400
-0.025
0.400
0.900
0.400
0.900
0.900
-0.100
0.400
0.700
G
erm
an-E
nglish
E
uroparl
adequacy
1
0.893
0.821
0.750
0.599
0.643
0.787
0.68
0.750
0.643
0.464
0.750
0.206
0.608
0.447
fl
uency
?
1
0.964
0.537
0.778
0.858
0.500
0.821
0.821
0.787
0.571
0.93
0.562
0.821
0.661
rank
?
?
1
0.500
0.902
0.821
0.393
0.714
0.858
0.643
0.464
0.858
0.652
0.893
0.769
constituent
?
?
?
1
0.456
0.464
0.714
0.18
0.750
0.250
0.214
0.43
0.117
0.214
0.126
Spanish-E
nglish
N
ew
s
C
orpus
adequacy
1
1.000
0.964
0.893
0.643
0.68
0.68
0.68
0.68
0.68
0.634
0.714
0.571
0.68
0.68
fl
uency
?
1
0.964
0.893
0.643
0.68
0.68
0.68
0.68
0.68
0.634
0.714
0.571
0.68
0.68
rank
?
?
1
0.858
0.714
0.750
0.750
0.750
0.750
0.750
0.741
0.787
0.608
0.750
0.750
constituent
?
?
?
1
0.787
0.821
0.821
0.821
0.714
0.821
0.599
0.750
0.750
0.714
0.714
Spanish-E
nglish
E
uroparl
adequacy
1
0.93
0.452
0.333
0.596
0.810
0.62
0.690
0.542
0.714
0.762
0.739
0.489
0.638
0.638
fl
uency
?
1
0.571
0.524
0.596
0.787
0.43
0.500
0.732
0.524
0.690
0.810
0.346
0.566
0.566
rank
?
?
1
0.643
0.739
0.596
0.43
0.262
0.923
0.406
0.500
0.739
0.168
0.542
0.542
constituent
?
?
?
1
0.262
0.143
-0.143
-0.143
0.816
-0.094
0.000
0.477
-0.226
0.042
0.042
F
rench-E
nglish
N
ew
s
C
orpus
adequacy
1
0.964
0.964
0.858
0.787
0.750
0.68
0.68
0.787
0.571
0.321
0.787
0.456
0.68
0.554
fl
uency
?
1
1.000
0.93
0.750
0.787
0.714
0.714
0.750
0.608
0.214
0.858
0.367
0.608
0.482
rank
?
?
1
0.93
0.750
0.787
0.714
0.714
0.750
0.608
0.214
0.858
0.367
0.608
0.482
constituent
?
?
?
1
0.858
0.858
0.787
0.787
0.858
0.643
0.393
0.964
0.349
0.750
0.661
F
rench-E
nglish
E
uroparl
adequacy
1
0.884
0.778
0.991
0.982
0.956
0.902
0.902
0.812
0.902
0.956
0.956
0.849
0.964
0.991
fl
uency
?
1
0.858
0.893
0.849
0.821
0.93
0.93
0.571
0.93
0.858
0.821
0.787
0.849
0.858
rank
?
?
1
0.821
0.670
0.68
0.858
0.858
0.43
0.858
0.787
0.68
0.893
0.741
0.714
constituent
?
?
?
1
0.956
0.93
0.93
0.93
0.750
0.93
0.964
0.93
0.893
0.956
0.964
Table
17:
C
orrelation
of
the
autom
atic
evaluation
m
etrics
w
ith
the
hum
an
judgm
ents
w
hen
translating
into
E
nglish
157
A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
M
E
T
E
O
R
B
L
E
U
1-
T
E
R
1-
W
E
R
-O
F
-V
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
English-German News Corpus
adequacy 1 0.943 0.83 0.943 0.187 0.43 0.814 0.243 0.33 0.187
fluency ? 1 0.714 0.83 0.100 0.371 0.758 0.100 0.243 0.100
rank ? ? 1 0.771 0.414 0.258 0.671 0.414 0.414 0.414
constituent ? ? ? 1 0.13 0.371 0.671 0.243 0.243 0.13
English-German Europarl
adequacy 1 0.714 0.487 0.714 0.487 0.600 0.314 0.371 0.487 0.487
fluency ? 1 0.543 0.43 0.258 0.200 -0.085 0.03 0.258 0.258
rank ? ? 1 0.03 -0.37 -0.256 -0.543 -0.485 -0.37 -0.37
constituent ? ? ? 1 0.887 0.943 0.658 0.83 0.887 0.887
English-Spanish News Corpus
adequacy 1 0.714 0.771 0.83 0.314 0.658 0.487 0.03 0.314 0.600
fluency ? 1 0.943 0.887 -0.200 0.03 0.143 0.200 -0.085 0.258
rank ? ? 1 0.943 -0.029 0.087 0.258 0.371 -0.029 0.371
constituent ? ? ? 1 -0.143 0.143 0.200 0.314 -0.085 0.258
English-Spanish Europarl
adequacy 1 0.83 0.943 0.543 0.658 0.943 0.943 0.943 0.83 0.658
fluency ? 1 0.771 0.543 0.714 0.771 0.771 0.771 0.83 0.714
rank ? ? 1 0.600 0.600 0.887 0.887 0.887 0.771 0.600
constituent ? ? ? 1 0.43 0.43 0.43 0.43 0.371 0.43
English-French News Corpus
adequacy 1 0.952 0.762 0.452 0.690 0.787 0.690 0.709 0.596 0.686
fluency ? 1 0.810 0.477 0.62 0.739 0.714 0.792 0.62 0.780
rank ? ? 1 0.762 0.239 0.381 0.500 0.757 0.596 0.601
constituent ? ? ? 1 -0.048 0.096 0.143 0.411 0.333 0.304
English-French Europarl
adequacy 1 0.964 0.750 0.93 0.608 0.528 0.287 -0.07 0.652 0.376
fluency ? 1 0.858 0.893 0.643 0.562 0.214 -0.07 0.652 0.376
rank ? ? 1 0.750 0.821 0.76 0.393 0.214 0.830 0.697
constituent ? ? ? 1 0.571 0.473 0.18 -0.07 0.652 0.447
Table 18: Correlation of the automatic evaluation metrics with the human judgments when translating out
of English
158
Proceedings of the Third Workshop on Statistical Machine Translation, pages 70?106,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Further Meta-Evaluation of Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb cs jhu edu
Cameron Fordyce
camfordyce gmail com
Philipp Koehn
University of Edinburgh
pkoehn inf ed ac uk
Christof Monz
Queen Mary, University of London
christof dcs qmul ac uk
Josh Schroeder
University of Edinburgh
j schroeder ed ac uk
Abstract
This paper analyzes the translation qual-
ity of machine translation systems for 10
language pairs translating between Czech,
English, French, German, Hungarian, and
Spanish. We report the translation quality
of over 30 diverse translation systems based
on a large-scale manual evaluation involv-
ing hundreds of hours of effort. We use the
human judgments of the systems to analyze
automatic evaluation metrics for translation
quality, and we report the strength of the cor-
relation with human judgments at both the
system-level and at the sentence-level. We
validate our manual evaluation methodol-
ogy by measuring intra- and inter-annotator
agreement, and collecting timing informa-
tion.
1 Introduction
This paper presents the results the shared tasks of the
2008 ACL Workshop on Statistical Machine Trans-
lation, which builds on two past workshops (Koehn
andMonz, 2006; Callison-Burch et al, 2007). There
were two shared tasks this year: a translation task
which evaluated translation between 10 pairs of Eu-
ropean languages, and an evaluation task which ex-
amines automatic evaluation metrics.
There were a number of differences between this
year?s workshop and last year?s workshop:
? Test set selection ? Instead of creating our test
set by reserving a portion of the training data,
we instead hired translators to translate a set of
newspaper articles from a number of different
sources. This out-of-domain test set contrasts
with the in-domain Europarl test set.
? New language pairs ? We evaluated the qual-
ity of Hungarian-English machine translation.
Hungarian is a challenging language because it
is agglutinative, has many cases and verb con-
jugations, and has freer word order. German-
Spanish was our first language pair that did not
include English, but was not manually evalu-
ated since it attracted minimal participation.
? System combination ? Saarland University
entered a system combination over a number
of rule-based MT systems, and provided their
output, which were also treated as fully fledged
entries in the manual evaluation. Three addi-
tional groups were invited to apply their system
combination algorithms to all systems.
? Refined manual evaluation ? Because last
year?s study indicated that fluency and ade-
quacy judgments were slow and unreliable, we
dropped them from manual evaluation. We re-
placed them with yes/no judgments about the
acceptability of translations of shorter phrases.
? Sentence-level correlation ? In addition to
measuring the correlation of automatic evalu-
ation metrics with human judgments at the sys-
tem level, we also measured how consistent
they were with the human rankings of individ-
ual sentences.
The remainder of this paper is organized as fol-
lows: Section 2 gives an overview of the shared
70
translation task, describing the test sets, the mate-
rials that were provided to participants, and a list of
the groups who participated. Section 3 describes the
manual evaluation of the translations, including in-
formation about the different types of judgments that
were solicited and how much data was collected.
Section 4 presents the results of the manual eval-
uation. Section 5 gives an overview of the shared
evaluation task, describes which automatic metrics
were submitted, and tells how they were evaluated.
Section 6 presents the results of the evaluation task.
Section 7 validates the manual evaluation methodol-
ogy.
2 Overview of the shared translation task
The shared translation task consisted of 10 language
pairs: English to German, German to English, En-
glish to Spanish, Spanish to English, English to
French, French to English, English to Czech, Czech
to English, Hungarian to English, and German to
Spanish. Each language pair had two test sets drawn
from the proceedings of the European parliament, or
from newspaper articles.1
2.1 Test data
The test data for this year?s task differed from previ-
ous years? data. Instead of only reserving a portion
of the training data as the test set, we hired people
to translate news articles that were drawn from a va-
riety of sources during November and December of
2007. We refer to this as the News test set. A total
of 90 articles were selected, 15 each from a variety
of Czech-, English-, French-, German-, Hungarian-
and Spanish-language news sites:2
Hungarian: Napi (3 documents), Index (2),
Origo (5), Ne?pszabadsa?g (2), HVG (2),
Uniospez (1)
Czech: Aktua?lne? (1), iHNed (4), Lidovky (7),
Novinky (3)
French: Liberation (4), Le Figaro (4), Dernieres
Nouvelles (2), Les Echos (3), Canoe (2)
1For Czech news editorials replaced the European parlia-
ment transcripts as the second test set, and for Hungarian the
newspaper articles was the only test set.
2For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
Original source language avg. BLEU
Hungarian 8.8
German 11.0
Czech 15.2
Spanish 17.3
English 17.7
French 18.6
Table 1: Difficulty of the test set parts based on the
original language. For each part, we average BLEU
scores from the Edinburgh systems for 12 language
pairs of the shared task.
Spanish: Cinco Dias (7), ABC.es (3), El Mundo (5)
English: BBC (3), Scotsman (3), Economist (3),
Times (3), New York Times (3)
German: Financial Times Deutschland (3), Su?d-
deutsche Zeitung (3), Welt (3), Frankfurter All-
gemeine Zeitung (3), Spiegel (3)
The translations were created by the members of
EuroMatrix consortium who hired a mix of profes-
sional and non-professional translators. All trans-
lators were fluent or native speakers of both lan-
guages, and all translations were proofread by a na-
tive speaker of the target language. All of the trans-
lations were done directly, and not via an intermedi-
ate language. So for instance, each of the 15 Hun-
garian articles were translated into Czech, English,
French, German and Spanish. The total cost of cre-
ating the 6 test sets consisting of 2,051 sentences
in each language was approximately 17,200 euros
(around 26,500 dollars at current exchange rates, at
slightly more than 10c/word).
Having a test set that is balanced in six differ-
ent source languages and translated across six lan-
guages raises some interesting questions. For in-
stance, is it easier, when the machine translation sys-
tem translates in the same direction as the human
translator? We found no conclusive evidence that
shows this. What is striking, however, that the parts
differ dramatically in difficulty, based on the orig-
inal source language. For instance the Edinburgh
French-English system has a BLEU score of 26.8 on
the part that was originally Spanish, but a score of on
9.7 on the part that was originally Hungarian. For
average scores for each original language, see Ta-
ble 1.
71
In order to remain consistent with previous eval-
uations, we also created a Europarl test set. The
Europarl test data was again drawn from the tran-
scripts of EU parliamentary proceedings from the
fourth quarter of 2000, which is excluded from the
Europarl training data. Our rationale behind invest-
ing a considerable sum to create the News test set
was that we believe that it more accurately repre-
sents the quality of systems? translations than when
we simply hold out a portion of the training data
as the test set, as with the Europarl set. For in-
stance, statistical systems are heavily optimized to
their training data, and do not perform as well on
out-of-domain data (Koehn and Schroeder, 2007).
Having both the News test set and the Europarl test
set alows us to contrast the performance of systems
on in-domain and out-of-domain data, and provides
a fairer comparison between systems trained on the
Europarl corpus and systems that were developed
without it.
2.2 Provided materials
To lower the barrier of entry for newcomers to the
field, we provided a complete baseline MT system,
along with data resources. We provided:
? sentence-aligned training corpora
? language model data
? development and dev-test sets
? Moses open source toolkit for phrase-based sta-
tistical translation (Koehn et al, 2007)
The performance of this baseline system is similar
to the best submissions in last year?s shared task.
The training materials are described in Figure 1.
2.3 Submitted systems
We received submissions from 23 groups from 18
institutions, as listed in Table 2. We also eval-
uated seven additional commercial rule-based MT
systems, bringing the total to 30 systems. This is
a significant increase over last year?s shared task,
where there were submissions from 15 groups from
14 institutions. Of the 15 groups that participated in
last year?s shared task, 11 groups returned this year.
One of the goals of the workshop was to attract sub-
missions from newcomers to the field, and we are
please to have attracted many smaller groups, some
as small as a single graduate student and her adviser.
The 30 submitted systems represent a broad
range of approaches to statistical machine transla-
tion. These include statistical phrase-based and rule-
based (RBMT) systems (which together made up the
bulk of the entries), and also hybrid machine trans-
lation, and statistical tree-based systems. For most
language pairs, we assembled a solid representation
of the state of the art in machine translation.
In addition to individual systems being entered,
this year we also solicited a number of entries which
combined the results of other systems. We invited
researchers at BBN, Carnegie Mellon University,
and the University of Edinburgh to apply their sys-
tem combination algorithms to all of the systems
submitted to shared translation task. We designated
the translations of the Europarl set as the develop-
ment data for combination techniques which weight
each system.3 CMU combined the French-English
systems, BBN combined the French-English and
German-English systems, and Edinburgh submitted
combinations for the French-English and German-
English systems as well as a multi-source system
combination which combined all systems which
translated from any language pair into English for
the News test set. The University of Saarland also
produced a system combination over six commercial
RBMT systems (Eisele et al, 2008). Saarland gra-
ciously provided the output of these systems, which
we manually evaluated alongside all other entries.
For more on the participating systems, please re-
fer to the respective system descriptions in the pro-
ceedings of the workshop.
3 Human evaluation
As with last year?s workshop, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention
that automatic measures are an imperfect substitute
for human assessment of translation quality. There-
fore, rather than select an official automatic eval-
uation metric like the NIST Machine Translation
Workshop does (Przybocki and Peterson, 2008), we
define the manual evaluation to be primary, and use
3Since the performance of systems varied significantly be-
tween the Europarl and News test sets, such weighting might
not be optimal. However this was a level playing field, since
none of the individual systems had development data for the
News set either.
72
Europarl Training Corpus
Spanish? English French? English German? English German? Spanish
Sentences 1,258,778 1,288,074 1,266,520 1,237,537
Words 36,424,186 35,060,653 38,784,144 36,046,219 33,404,503 35,259,758 32,652,649 35,780,165
Distinct words 149,159 96,746 119,437 97,571 301,006 96,802 298,040 148,206
News Commentary Training Corpus
Spanish? English French? English German? English German? Spanish
Sentences 64,308 55,030 72,291 63,312
Words 1,759,972 1,544,633 1,528,159 1,329,940 1,784,456 1,718,561 1,597,152 1,751,215
Distinct words 52,832 38,787 42,385 36,032 84,700 40,553 78,658 52,397
Hunglish Training Corpus CzEng Training Corpus
Hungarian? English
Sentences 1,517,584
Words 26,082,667 31,458,540
Distinct words 717,198 192,901
Czech? English
Sentences 1,096,940
Words 15,336,783 17,909,979
Distinct words 339,683 129,176
Europarl Language Model Data
English Spanish French German
Sentence 1,412,546 1,426,427 1,438,435 1,467,291
Words 34,501,453 36,147,902 35,680,827 32,069,151
Distinct words 100,826 155,579 124,149 314,990
Europarl test set
English Spanish French German
Sentences 2,000
Words 60,185 61,790 64,378 56,624
Distinct words 6,050 7,814 7,361 8,844
News Commentary test set
English Czech
Sentences 2,028
Words 45,520 39,384
Distinct words 7,163 12,570
News Test Set
English Spanish French German Czech Hungarian
Sentences 2,051
Words 43,482 47,155 46,183 41,175 36,359 35,513
Distinct words 7,807 8,973 8,898 10,569 12,732 13,144
Figure 1: Properties of the training and test sets used in the shared task. The training data is drawn from the
Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple
languages. For Czech and Hungarian we use other available parallel corpora. Note that the number of
words is computed based on the provided tokenizer and that the number of distinct words is the based on
lowercased tokens.
73
ID Participant
BBN-COMBO BBN system combination (Rosti et al, 2008)
CMU-COMBO Carnegie Mellon University system combination (Jayaraman and Lavie, 2005)
CMU-GIMPEL Carnegie Mellon University Gimpel (Gimpel and Smith, 2008)
CMU-SMT Carnegie Mellon University SMT (Bach et al, 2008)
CMU-STATXFER Carnegie Mellon University Stat-XFER (Hanneman et al, 2008)
CU-TECTOMT Charles University TectoMT (Zabokrtsky et al, 2008)
CU-BOJAR Charles University Bojar (Bojar and Hajic?, 2008)
CUED Cambridge University (Blackwood et al, 2008)
DCU Dublin City University (Tinsley et al, 2008)
LIMSI LIMSI (De?chelotte et al, 2008)
LIU Linko?ping University (Stymne et al, 2008)
LIUM-SYSTRAN LIUM / Systran (Schwenk et al, 2008)
MLOGIC Morphologic (Nova?k et al, 2008)
PCT a commercial MT provider from the Czech Republic
RBMT1?6 Babelfish, Lingenio, Lucy, OpenLogos, ProMT, SDL (ordering anonymized)
SAAR University of Saarbruecken (Eisele et al, 2008)
SYSTRAN Systran (Dugast et al, 2008)
UCB University of California at Berkeley (Nakov, 2008)
UCL University College London (Wang and Shawe-Taylor, 2008)
UEDIN University of Edinburgh (Koehn et al, 2008)
UEDIN-COMBO University of Edinburgh system combination (Josh Schroeder)
UMD University of Maryland (Dyer, 2007)
UPC Universitat Politecnica de Catalunya, Barcelona (Khalilov et al, 2008)
UW University of Washington (Axelrod et al, 2008)
XEROX Xerox Research Centre Europe (Nikoulina and Dymetman, 2008)
Table 2: Participants in the shared translation task. Not all groups participated in all language pairs.
74
the human judgments to validate automatic metrics.
Manual evaluation is time consuming, and it re-
quires a monumental effort to conduct it on the
scale of our workshop. We distributed the work-
load across a number of people, including shared
task participants, interested volunteers, and a small
number of paid annotators. More than 100 people
participated in the manual evaluation, with 75 peo-
ple putting in more than an hour?s worth of effort,
and 25 putting in more than four hours. A collective
total of 266 hours of labor was invested.
We wanted to ensure that we were using our anno-
tators? time effectively, so we carefully designed the
manual evaluation process. In our analysis of last
year?s manual evaluation we found that the NIST-
style fluency and adequacy scores (LDC, 2005) were
overly time consuming and inconsistent.4 We there-
fore abandoned this method of evaluating the trans-
lations.
We asked people to evaluate the systems? output
in three different ways:
? Ranking translated sentences relative to each
other
? Ranking the translations of syntactic con-
stituents drawn from the source sentence
? Assigning absolute yes or no judgments to the
translations of the syntactic constituents.
The manual evaluation software asked for re-
peated judgments from the same individual, and had
multiple people judge the same item, and logged the
time it took to complete each judgment. This al-
lowed us to measure intra- and inter-annotator agree-
ment, and to analyze the average amount of time it
takes to collect the different kinds of judgments. Our
analysis is presented in Section 7.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rela-
tively intuitive and straightforward task. We there-
fore kept the instructions simple. The instructions
for this task were:
4It took 26 seconds on average to assign fluency and ade-
quacy scores to a single sentence, and the inter-annotator agree-
ment had a Kappa of between .225?.25, meaning that annotators
assigned the same scores to identical sentences less than 40% of
the time.
Rank each whole sentence translation
from Best to Worst relative to the other
choices (ties are allowed).
Ranking several translations at a time is a variant
of force choice judgments where a pair of systems
is presented and an annotator is asked ?Is A better
than B, worse than B, or equal to B.? In our exper-
iments, annotators were shown five translations at a
time, except for the Hungarian and Czech language
pairs where there were fewer than five system sub-
missions. In most cases there were more than 5 sys-
tems submissions. We did not attempt to get a com-
plete ordering over the systems, and instead relied
on random selection and a reasonably large sample
size to make the comparisons fair.
?
people
's
Iraq
to
services
basic
other
and
,
care
health
,
food
provide
cannot
it
if
occupation
its
sustain
US
the
Can
?k
?
n
n
e
n
a
n
b
i
e
t
e
n
D
i
e
n
s
t
l
e
i
s
t
u
n
g
e
n
g
r
u
n
d
l
e
g
e
n
d
e
a
n
d
e
r
e
u
n
d
G
e
s
u
n
d
h
e
i
t
s
f
?
r
s
o
r
g
e
,N
a
h
r
u
n
g
n
i
c
h
t
V
o
l
k
i
r
a
k
i
s
c
h
e
n
d
e
m
s
i
e
w
e
n
n
,U
S
A
d
i
e
K
?
n
n
e
n
a
u
f
r
e
c
h
t
e
r
h
a
l
t
e
n
B
e
s
e
t
z
u
n
g
 
 
i
h
r
e
R
e
f
e
r
e
n
c
e
 
t
r
a
n
s
l
a
t
i
o
n
NP
NP
NP
VP
NP
VP
S
S
CNP
NP
Constituents selected 
for evaluation
Target phrases
highlighted via
word alignments
Parsed source
sentence
Figure 2: In constituent-based evaluation, the source
sentence was parsed, and automatically aligned with
the reference translation and systems? translations
75
Language Pair Test Set Constituent Rank Yes/No Judgments Sentence Ranking
English-German Europarl 2,032 2,034 1,004
News 2,170 2,221 1,115
German-English Europarl 1,705 1,674 819
News 1,938 1,881 1,944
English-Spanish Europarl 1,200 1,247 615
News 1,396 1,398 700
Spanish-English Europarl 1,855 1,921 948
News 2,063 1,939 1,896
English-French Europarl 1,248 1,265 674
News 1,741 1,734 843
French-English Europarl 1,829 1,841 909
News 2,467 2,500 2,671
English-Czech News 2,069 2,070 1,045
Commentary 1,840 1,815 932
Czech-English News 0 0 1,400
Commentary 0 0 1,731
Hungarian-English News 0 0 937
All-English News 0 0 4,868
Totals 25,553 25,540 25,051
Table 3: The number of items that were judged for each task during the manual evaluation. The All-English
judgments were reused in the News task for individual language pairs.
3.2 Ranking translations of syntactic
constituents
We continued the constituent-based evaluation that
we piloted last year, wherein we solicited judgments
about the translations of short phrases within sen-
tences rather than whole sentences. We parsed the
source language sentence, selected syntactic con-
stituents from the tree, and had people judge the
translations of those syntactic phrases. In order to
draw judges? attention to these regions, we high-
lighted the selected source phrases and the corre-
sponding phrases in the translations. The corre-
sponding phrases in the translations were located via
automatic word alignments.
Figure 2 illustrates how the source and reference
phrases are highlighted via automatic word align-
ments. The same is done for sentence and each
of the system translations. The English, French,
German and Spanish test sets were automatically
parsed using high quality parsers for those languages
(Bikel, 2002; Arun and Keller, 2005; Dubey, 2005;
Bick, 2006).
The word alignments were created with Giza++
(Och and Ney, 2003) applied to a parallel corpus
containing the complete Europarl training data, plus
sets of 4,051 sentence pairs created by pairing the
test sentences with the reference translations, and
the test sentences paired with each of the system
translations. The phrases in the translations were
located using standard phrase extraction techniques
(Koehn et al, 2003). Because the word-alignments
were created automatically, and because the phrase
extraction is heuristic, the phrases that were selected
may not exactly correspond to the translations of the
selected source phrase. We noted this in the instruc-
tions to judges:
Rank each constituent translation from
Best to Worst relative to the other choices
(ties are allowed). Grade only the high-
lighted part of each translation.
Please note that segments are selected au-
tomatically, and they should be taken as
an approximate guide. They might in-
clude extra words that are not in the actual
alignment, or miss words on either end.
76
The criteria that we used to select which con-
stituents to evaluate were:
? The constituent could not be the whole source
sentence
? The constituent had to be longer three words,
and be no longer than 15 words
? The constituent had to have a corresponding
phrase with a consistent word alignment in
each of the translations
The final criterion helped reduce the number of
alignment errors, but may have biased the sample
to phrases that are more easily aligned.
3.3 Yes/No judgments for the translations of
syntactic constituents
This year we introduced a variant on the constituent-
based evaluation, where instead of asking judges
to rank the translations of phrases relative to each
other, we asked them to indicate which phrasal trans-
lations were acceptable and which were not.
Decide if the highlighted part of each
translation is acceptable, given the refer-
ence. This should not be a relative judg-
ment against the other system translations.
The instructions also contained the same caveat
about the automatic alignments as above. For each
phrase the judges could click on ?Yes?, ?No?, or
?Not Sure.? The number of times people clicked on
?Not Sure? varied by language pair and task. It was
selected as few as 5% of the time for the English-
Spanish News task to as many as 12.5% for the
Czech-English News task.
3.4 Collecting judgments
We collected judgments using a web-based tool that
presented judges with batches of each type of eval-
uation. We presented them with five screens of sen-
tence rankings, ten screens of constituent rankings,
and ten screen of yes/no judgments. The order of the
types of evaluation were randomized.
In order to measure intra-annotator agreement
10% of the items were repeated and evaluated twice
by each judge. In order to measure inter-annotator
agreement 40% of the items were randomly drawn
from a common pool that was shared across all
annotators so that we would have items that were
judged by multiple annotators.
Judges were allowed to select whichever data set
they wanted, and to evaluate translations into what-
ever languages they were proficient in. Shared task
participants were excluded from judging their own
systems.
In addition to evaluation each language pair indi-
vidually, we also combined all system translations
into English for the News test set, taking advantage
of the fact that our test sets were parallel across all
languages. This allowed us to gather interesting data
about the difficulty of translating from different lan-
guages into English.
Table 3 gives a summary of the number of judg-
ments that we collected for translations of individ-
ual sentences. We evaluated 14 translation tasks
with three different types of judgments for most of
them, for a total of 46 different conditions. In to-
tal we collected over 75,000 judgments. Despite the
large number of conditions we managed to collect
between 1,000?2,000 judgments for the constituent-
based evaluation, and several hundred to several
thousand judgments for the sentence ranking tasks.
4 Translation task results
Tables 4, 5, and 6 summarize the results of the hu-
man evaluation of the quality of the machine trans-
lation systems. Table 4 gives the results for the man-
ual evaluation which ranked the translations of sen-
tences. It shows the average number of times that
systems were judged to be better than or equal to
any other system. Table 5 similarly summarizes
the results for the manual evaluation which ranked
the translations of syntactic constituents. Table 6
shows how many times on average a system?s trans-
lated constituents were judged to be acceptable in
the Yes/No evaluation. The bolded items indicate
the system that performed the best for each task un-
der that particular evaluate metric.
Table 7 summaries the results for the All-English
task that we introduced this year. Appendix C gives
an extremely detailed pairwise comparison between
each of the systems, along with an indication of
whether the differences are statistically significant.
The highest ranking entry for the All-English task
77
was the University of Edinburgh?s system combina-
tion entry. It uses a technique similar to Rosti et
al. (2007) to perform system combination. Like the
other system combination entrants, it was tuned on
the Europarl test set and tested on the News test set,
using systems that submitted entries to both tasks.
The University of Edinburgh?s system combi-
nation went beyond other approaches by combin-
ing output from multiple languages pairs (French-
English, German-English and Spanish-English),
resulting in 37 component systems. Rather
than weighting individual systems, it incorporated
weighted features that indicated which language the
system was originally translating from. This entry
was part of ongoing research in multi-lingual, multi-
source translation. Since there was no official multi-
lingual system combination track, this entry should
be viewed only as a contrastive data point.
We analyzed the All-English judgments to see
which source languages were preferred more often,
thinking that this might be a good indication of how
challenging it is for current MT systems to trans-
late from each of the languages into English. For
this analysis we collapsed all of the entries derived
from one source language into an equivalence class,
and judged them against the others. Therefore, all
French systems were judged against all German sys-
tems, and so on. We found that French systems were
judged to be better than or equal to other systems
69% of the time, Spanish systems 64% of the time,
German systems 47% of the time, Czech systems
39% of the time, and Hungarian systems 29% of the
time.
We performed a similar analysis by collapsing the
RBMT systems into one equivalence class, and the
other systems into another. We evaluated how well
these two classes did on the sentence ranking task
for each language pair and test set, and found that
RBMT was a surprisingly good approach in many
of the conditions. RBMT generally did better on the
News test set and for translations into German, sug-
gesting that SMT?s forte is in test sets where it has
appropriate tuning data and for language pairs with
less reordering than between German and English.
BBN-COMBO
CMU-COMBO
CMU-GIMPEL
CMU-SMT
CMU-STATXFER
CU-BOJAR
CU-TECTOMT
CUED
CUED-CONTR
DCU
LIMSI
LIU
LIUM-SYSTRAN
LIUM-SYS-CONTR
MORPHOLOGIC
PC-TRANSLATOR
RBMT2
RBMT3
RBMT4
RBMT5
RBMT6
SAAR
SAAR-CONTR
SYSTRAN
UCB
UCL
UEDIN
UEDIN-COMBO
UMD
UPC
UW
XEROX
C
zech-E
nglish
C
om
m
entary
.745
.445
.603
.717
C
zech-E
nglish
N
ew
s
.675
.583
.646
E
nglish-C
zech
C
om
m
entary
.714
.488
.663
.486
E
nglish-C
zech
N
ew
s
.634
.494
.715
.502
E
nglish-F
rench
E
uroparl
.791
.775
.416
.608
.263
.436
.744
.786
.444
.766
E
nglish-F
rench
N
ew
s
.667
.655
.602
.780
.734
.657
.511
.573
.545
.317
E
nglish-G
erm
an
E
uroparl
.612
.584
.581
.615
.583
.681
.471
.432
.527
.386
.667
E
nglish-G
erm
an
N
ew
s
.361
.426
.787
.664
.752
.667
.555
.463
.444
E
nglish-S
panish
E
uroparl
.667
.737
.554
.560
.413
.436
.717
.500
.714
.593
.735
E
nglish-S
panish
N
ew
s
.494
.537
.683
.674
.724
.715
.548
.586
.481
.601
F
rench-E
nglish
E
uroparl
.415
.697
.642
.776
.792
.400
.504
.484
.323
.577
.753
.465
.524
.707
F
rench-E
nglish
N
ew
s
.659
.592
.379
.549
.643
.632
.660
.693
.581
.575
.654
.565
.540
.639
.614
.608
G
erm
an-E
nglish
E
uroparl
.364
.485
.614
.627
.596
.610
.543
.537
.677
.416
.679
G
erm
an-E
nglish
N
ew
s
.514
.354
.518
.559
.742
.725
.731
.668
.590
.607
.649
.548
.441
H
ungarian-E
nglish
N
ew
s
.853
.321
S
panish-E
nglish
E
uroparl
.714
.676
.677
.780
.427
.488
.350
.470
.671
.425
.660
.687
S
panish-E
nglish
N
ew
s
.567
.563
.674
.583
.667
.768
.577
.613
.669
.543
.561
.602
Table
4:
S
um
m
ary
results
for
the
sentence
ranking
judgm
ents.
T
he
num
bers
reportthe
percentof
tim
e
thateach
system
w
as
judged
to
be
greater
than
or
equalto
any
other
system
.
B
old
indicates
the
highestscore
for
thattask.
78
C
zech-E
nglish
C
om
m
entary
C
zech-E
nglish
N
ew
s
E
nglish-C
zech
C
om
m
entary
.732
.538
.609
.614
E
nglish-C
zech
N
ew
s
.663
.615
.674
.610
E
nglish-F
rench
E
uroparl
.876
.881
.561
.675
.546
.561
.807
.656
.870
E
nglish-F
rench
N
ew
s
.649
.760
.716
.768
.763
.671
.725
.746
.661
.437
E
nglish-G
erm
an
E
uroparl
.774
.750
.812
.577
.585
.582
.508
.518
.770
.690
.822
E
nglish-G
erm
an
N
ew
s
.649
.570
.720
.682
.748
.602
.563
.610
.556
E
nglish-S
panish
E
uroparl
.825
.855
.561
.592
.458
.573
.849
.592
.818
.775
.790
E
nglish-S
panish
N
ew
s
.721
.694
.694
.754
.570
.644
.696
.653
.625
.595
F
rench-E
nglish
E
uroparl
.626
.907
.854
.906
.917
.523
.648
.697
.517
.783
.865
.713
.741
.894
F
rench-E
nglish
N
ew
s
.506
.745
.787
.801
.765
.780
.652
.655
.726
.615
.640
.735
.773
G
erm
an-E
nglish
E
uroparl
.554
.752
.795
.580
.640
.643
.579
.587
.843
.601
.832
G
erm
an-E
nglish
N
ew
s
.502
.715
.674
.772
.755
.740
.674
.640
.757
.775
.744
H
ungarian-E
nglish
N
ew
s
S
panish-E
nglish
E
uroparl
.847
.846
.868
.854
.455
.561
.469
.567
.893
.646
.865
.870
S
panish-E
nglish
N
ew
s
.715
.760
.818
.739
.644
.608
.699
.700
.760
.706
.758
.763
Table
5:
S
um
m
ary
results
for
the
constituent
ranking
judgm
ents.
T
he
num
bers
report
the
percent
of
tim
e
that
each
system
w
as
judged
to
be
greater
than
or
equalto
any
other
system
.
B
old
indicates
the
highestscore
for
thattask.
BBN-COMBO
CMU-COMBO
CMU-GIMPEL
CMU-SMT
CMU-STATXFER
CU-BOJAR
CU-TECTOMT
CUED
CUED-CONTR
DCU
LIMSI
LIU
LIUM-SYSTRAN
LIUM-SYS-CONTR
MORPHOLOGIC
PC-TRANSLATOR
RBMT2
RBMT3
RBMT4
RBMT5
RBMT6
SAAR
SAAR-CONTR
SYSTRAN
UCB
UCL
UEDIN
UEDIN-COMBO
UMD
UPC
UW
XEROX
C
zech-E
nglish
C
om
m
entary
C
zech-E
nglish
N
ew
s
E
nglish-C
zech
C
om
m
entary
.594
.427
.506
.409
E
nglish-C
zech
N
ew
s
.540
.422
.518
.441
E
nglish-F
rench
E
uroparl
.745
.843
.490
.504
.442
.351
.701
.596
.750
E
nglish-F
rench
N
ew
s
.730
.748
.589
.593
.640
.576
.591
.586
.633
.302
E
nglish-G
erm
an
E
uroparl
.822
.794
.788
.692
.571
.665
.447
.466
.774
.611
.849
E
nglish-G
erm
an
N
ew
s
.559
.494
.689
.689
.750
.553
.598
.544
.518
E
nglish-S
panish
E
uroparl
.804
.872
.582
.598
.635
.600
.806
.714
.888
.903
.785
E
nglish-S
panish
N
ew
s
.459
.532
.638
.759
.599
.623
.639
.568
.493
.366
F
rench-E
nglish
E
uroparl
.612
.833
.876
.886
.891
.535
.620
.712
.540
.717
.860
.811
.734
.910
F
rench-E
nglish
N
ew
s
.554
.736
.788
.805
.789
.696
.628
.640
.762
.663
.638
.701
.718
G
erm
an-E
nglish
E
uroparl
.534
.803
.831
.759
.744
.667
.633
.630
.823
.492
.856
G
erm
an-E
nglish
N
ew
s
.470
.725
.638
.717
.731
.738
.589
.684
.669
.716
.632
H
ungarian-E
nglish
N
ew
s
S
panish-E
nglish
E
uroparl
.882
.857
.853
.902
.648
.562
.590
.550
.869
.730
.879
.857
S
panish-E
nglish
N
ew
s
.635
.638
.694
.675
.610
.651
.594
.635
.697
.635
.622
.707
Table
6:
S
um
m
ary
results
for
the
Yes/N
o
judgm
ents
for
constituenttranslations
judgm
ents.
T
he
num
bers
reportthe
percentof
each
system
?s
transla-
tions
thatw
ere
judged
to
be
acceptable.
B
old
indicates
the
highestscore
for
thattask.
79
UEDIN-COMBOxx .717 SAARfr .584
LIUM-SYSTRAN-Cfr .708 SAAR-Cde .574
RBMT5fr .706 RBMT4de .573
UEDIN-COMBOfr .704 CUEDes .572
LIUM-SYSTRANfr .702 RBMT3de .552
RBMT4es .699 CMU-SMTes .548
LIMSIfr .699 UCBes .547
BBN-COMBOfr .695 LIMSIes .537
SAARes .678 RBMT6de .509
CUED-CONTRASTes .674 RBMT5de .493
CMU-COMBOfr .661 LIMSIde .469
UEDINes .654 LIUde .447
CUEDfr .652 SAARde .445
CUED-CONTRASTfr .638 CMU-STATXFRfr .444
RBMT4fr .637 UMDcz .429
UPCes .633 BBN-COMBOde .407
RBMT3es .628 UEDINde .402
RBMT2de .627 MORPHOLOGIChu .387
SAAR-CONTRASTfr .624 DCUcz .380
UEDINfr .616 UEDIN-COMBOde .327
RBMT6fr .615 UEDINcz .293
RBMT6es .615 CMU-STATXFERde .280
RBMT3fr .612 UEDINhu .188
Table 7: The average number of times that each
system was judged to be better than or equal to all
other systems in the sentence ranking task for the
All-English condition. The subscript indicates the
source language of the system.
5 Shared evaluation task overview
The manual evaluation data provides a rich source
of information beyond simply analyzing the qual-
ity of translations produced by different systems. In
particular, it is especially useful for validating the
automatic metrics which are frequently used by the
machine translation research community. We con-
tinued the shared task which we debuted last year,
by examining how well various automatic metrics
correlate with human judgments.
In addition to examining how well the automatic
evaluation metrics predict human judgments at the
system-level, this year we have also started to mea-
sure their ability to predict sentence-level judg-
ments.
The automatic metrics that were evaluated in this
year?s shared task were the following:
? Bleu (Papineni et al, 2002)?Bleu remains the
de facto standard in machine translation eval-
uation. It calculates n-gram precision and a
brevity penalty, and can make use of multi-
ple reference translations as a way of capturing
some of the allowable variation in translation.
We use a single reference translation in our ex-
periments.
? Meteor (Agarwal and Lavie, 2008)?Meteor
measures precision and recall for unigrams and
applies a fragmentation penalty. It uses flex-
ible word matching based on stemming and
WordNet-synonymy. A number of variants are
investigated here: meteor-baseline and meteor-
ranking are optimized for correlation with ad-
equacy and ranking judgments respectively.
mbleu and mter are Bleu and TER computed
using the flexible matching used in Meteor.
? Gimenez and Marquez (2008) measure over-
lapping grammatical dependency relationships
(DP), semantic roles (SR), and discourse repre-
sentations (DR). The authors further investigate
combining these with other metrics including
TER, Bleu, GTM, Rouge, and Meteor (ULC
and ULCh).
? Popovic and Ney (2007) automatically eval-
uate translation quality by examining se-
quences of parts of speech, rather than
words. They calculate Bleu (posbleu) and
F-measure (pos4gramFmeasure) by matching
part of speech 4grams in a hypothesis transla-
tion against the reference translation.
In addition to the above metrics, which scored
the translations on both the system-level5 and the
sentence-level, there were a number of metrics
which focused on the sentence-level:
? Albrecht and Hwa (2008) use support vector re-
gression to score translations using past WMT
manual assessment data as training examples.
The metric uses features derived from target-
side language models and machine-generated
translations (svm-pseudo-ref) as well as refer-
ence human translations (svm-human-ref).
? Duh (2008) similarly used support vector ma-
chines to predict an ordering over a set of
5We provide the scores assigned to each system by these
metrics in Appendix A.
80
system translations (svm-rank). Features in-
cluded in Duh (2008)?s training were sentence-
level BLEU scores and intra-set ranks com-
puted from the entire set of translations.
? USaar?s evaluation metric (alignment-prob)
uses Giza++ to align outputs of multiple sys-
tems with the corresponding reference transla-
tions, with a bias towards identical one-to-one
alignments through a suitably augmented cor-
pus. The Model4 log probabilities in both di-
rections are added and normalized to a scale
between 0 and 1.
5.1 Measuring system-level correlation
To measure the correlation of the automatic metrics
with the human judgments of translation quality at
the system-level we used Spearman?s rank correla-
tion coefficient ?. We converted the raw scores as-
signed each system into ranks. We assigned a rank-
ing to the systems for each of the three types of man-
ual evaluation based on:
? The percent of time that the sentences it pro-
duced were judged to be better than or equal to
the translations of any other system.
? The percent of time that its constituent transla-
tions were judged to be better than or equal to
the translations of any other system.
? The percent of time that its constituent transla-
tions were judged to be acceptable.
We calculated ? three times for each automatic met-
ric, comparing it to each type of human evaluation.
Since there were no ties ? can be calculated using
the simplified equation:
? = 1 ?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher value for ? is
making predictions that are more similar to the hu-
man judgments than an automatic evaluation metric
with a lower ?.
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
O
V
E
R
A
L
L
meteor-ranking .81 .72 .77 .76
ULCh .68 .79 .82 .76
meteor-baseline .77 .75 .74 .75
posbleu .77 .8 .66 .74
pos4gramFmeasure .75 .62 .82 .73
ULC .66 .67 .84 .72
DR .79 .55 .76 .70
SR .79 .53 .76 .69
DP .57 .79 .65 .67
mbleu .61 .77 .56 .65
mter .47 .72 .68 .62
bleu .61 .59 .44 .54
svm-rank .21 .24 .35 .27
Table 8: Average system-level correlations for the
automatic evaluation metrics on translations into En-
glish
5.2 Measuring consistency at the sentence-level
Measuring sentence-level correlation under our hu-
man evaluation framework was made complicated
by the fact that we abandoned the fluency and ad-
equacy judgments which are intended to be abso-
lute scales. Some previous work has focused on
developing automatic metrics which predict human
ranking at the sentence-level (Kulesza and Shieber,
2004; Albrecht and Hwa, 2007a; Albrecht and Hwa,
2007b). Such work generally used the 5-point flu-
ency and adequacy scales to combine the transla-
tions of all sentences into a single ranked list. This
list could be compared against the scores assigned
by automatic metrics and used to calculate corre-
lation coefficients. We did not gather any absolute
scores and thus cannot compare translations across
different sentences. Given the seemingly unreliable
fluency and adequacy assignments that people make
even for translations of the same sentences, it may
be dubious to assume that their scoring will be reli-
able across sentences.
The data points that we have available consist of a
set of 6,400 human judgments each ranking the out-
put of 5 systems. It?s straightforward to construct a
ranking of each of those 5 systems using the scores
81
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
O
V
E
R
A
L
L
posbleu .57 .78 .80 .72
bleu .54 .79 .6 .64
meteor-ranking .55 .74 .55 .61
meteor-baseline .42 .78 .57 .59
pos4gramFmeasure .37 .49 .79 .55
mter .54 .50 .55 .53
svm-rank .55 .56 .46 .52
mbleu .63 .47 .43 .51
Table 9: Average system-level correlations for the
automatic evaluation metrics on translations into
French, German and Spanish
assigned to their translations of that sentence by the
automatic evaluation metrics. When the automatic
scores have been retrieved, we have 6,400 pairs of
ranked lists containing 5 items. How best to treat
these is an open discussion, and certainly warrants
further thought. It does not seem like a good idea
to calculate ? for each pair of ranked list, because
5 items is an insufficient number to get a reliable
correlation coefficient and its unclear if averaging
over all 6,400 lists would make sense. Furthermore,
many of the human judgments of 5 contained ties,
further complicating matters.
Therefore rather than calculating a correlation co-
efficient at the sentence-level we instead ascertained
how consistent the automatic metrics were with the
human judgments. The way that we calculated con-
sistency was the following: for every pairwise com-
parison of two systems on a single sentence by a per-
son, we counted the automatic metric as being con-
sistent if the relative scores were the same (i.e. the
metric assigned a higher score to the higher ranked
system). We divided this by the total number of pair-
wise comparisons to get a percentage. Because the
systems generally assign real numbers as scores, we
excluded pairs that the human annotators ranked as
ties.
6 Evaluation task results
Tables 8 and 9 report the system-level ? for each au-
tomatic evaluation metric, averaged over all trans-
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
DP .514 .527 .536
DR .500 .511 .530
SR .498 .489 .511
ULC .559 .554 .561
ULCh .562 .542 .542
alignment-prob .517 .538 .535
mbleu .505 .516 .544
meteor-baseline .512 .520 .542
meteor-ranking .512 .517 .539
mter .436 .471 .480
pos4gramFmeasure .495 .517 .52
posbleu .435 .43 .454
svm-human-ref .542 .541 .552
svm-pseudo-ref .538 .538 .543
svm-rank .493 .499 .497
Table 10: The percent of time that each automatic
metric was consistent with human judgments for
translations into English
lations directions into English and out of English6
For the into English direction the Meteor score with
its parameters tuned on adequacy judgments had
the strongest correlation with ranking the transla-
tions of whole sentences. It was tied with the com-
bined method of Gimenez and Marquez (2008) for
the highest correlation over all three types of human
judgments. Bleu was the second to lowest ranked
overall, though this may have been due in part to the
fact that we were using test sets which had only a
single reference translation, since the cost of creat-
ing multiple references was prohibitively expensive
(see Section 2.1).
In the reverse direction, for translations out of En-
glish into the other languages, Bleu does consider-
ably better, placing second overall after the part-of-
speech variant on it proposed by Popovic and Ney
(2007). Yet another variant of Bleu which utilizes
Meteor?s flexible matching has the strongest corre-
lation for sentence-level ranking. Appendix B gives
a break down of the correlations for each of the lan-
6Tables 8 and 9 exclude the Spanish-English News Task,
since it had a negative correlation with most of the automatic
metrics. See Tables 19 and 20.
82
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
mbleu 0.520 0.521 0.52
meteor-baseline 0.514 0.494 0.520
meteor-ranking 0.522 0.501 0.534
mter 0.454 0.441 0.457
pos4gramFmeasure 0.515 0.525 0.512
posbleu 0.436 0.446 0.416
svm-rank 0.514 0.531 0.51
Table 11: The percent of time that each automatic
metric was consistent with human judgments for
translations into other languages
guage pairs and test sets.
Tables 10 and 11 report the consistency of the au-
tomatic evaluation metrics with human judgments
on a sentence-by-sentence basis, rather than on the
system level. For the translations into English the
ULC metric (which itself combines many other met-
rics) had the strongest correlation with human judg-
ments, correctly predicting the human ranking of a
each pair of system translations of a sentence more
than half the time. This is dramatically higher than
the chance baseline, which is not .5, since it must
correctly rank a list of systems rather than a pair. For
the reverse direction meteor-ranking performs very
strongly. The svn-rank which had the lowest over-
all correlation at the system level does the best at
consistently predicting the translations of syntactic
constituents into other languages.
7 Validation and analysis of the manual
evaluation
In addition to scoring the shared task entries, we also
continued on our campaign for improving the pro-
cess of manual evaluation.
7.1 Inter- and Intra-annotator agreement
We measured pairwise agreement among annotators
using the kappa coefficient (K) which is widely used
in computational linguistics for measuring agree-
ment in category judgments (Carletta, 1996). It is
defined as
K =
P (A) ? P (E)
1 ? P (E)
Evaluation type P (A) P (E) K
Sentence ranking .578 .333 .367
Constituent ranking .671 .333 .506
Constituent (w/identicals) .678 .333 .517
Yes/No judgments .821 .5 .642
Yes/No (w/identicals) .825 .5 .649
Table 12: Kappa coefficient values representing the
inter-annotator agreement for the different types of
manual evaluation
Evaluation type P (A) P (E) K
Sentence ranking .691 .333 .537
Constituent ranking .825 .333 .737
Constituent (w/identicals) .832 .333 .748
Yes/No judgments .928 .5 .855
Yes/No (w/identicals) .930 .5 .861
Table 13: Kappa coefficient values for intra-
annotator agreement for the different types of man-
ual evaluation
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. We define chance
agreement for ranking tasks as 13 since there are
three possible outcomes when ranking the output of
a pair of systems: A > B, A = B, A < B, and for
the Yes/No judgments as 12 since we ignored those
items marked ?Not Sure?.
For inter-annotator agreement we calculated
P (A) for the yes/no judgments by examining all
items that were annotated by two or more annota-
tors, and calculating the proportion of time they as-
signed identical scores to the same items. For the
ranking tasks we calculated P (A) by examining all
pairs of systems which had been judged by two or
more judges, and calculated the proportion of time
that they agreed that A > B, A = B, or A < B.
For intra-annotator agreement we did similarly, but
gathered items that were annotated on multiple oc-
casions by a single annotator.
Table 12 givesK values for inter-annotator agree-
ment, and Table 13 gives K values for intra-
annotator agreement. These give an indication of
how often different judges agree, and how often sin-
gle judges are consistent for repeated judgments, re-
83
spectively. The interpretation of Kappa varies, but
according to Landis and Koch (1977), 0?.2 is slight,
.2? .4 is fair, .4? .6 is moderate, .6? .8 is substan-
tial and the rest almost perfect. The inter-annotator
agreement for the sentence ranking task was fair, for
the constituent ranking it was moderate and for the
yes/no judgments it was substantial.7 For the intra-
annotator agreement K indicated that people had
moderate consistency with their previous judgments
on the sentence ranking task, substantial consistency
with their previous constituent ranking judgments,
and nearly perfect consistency with their previous
yes/no judgments.
These K values indicate that people are able to
more reliably make simple yes/no judgments about
the translations of short phrases than they are to
rank phrases or whole sentences. While this is an
interesting observation, we do not recommend do-
ing away with the sentence ranking judgments. The
higher agreement on the constituent-based evalua-
tion may be influenced based on the selection cri-
teria for which phrases were selected for evalua-
tion (see Section 3.2). Additionally, the judgments
of the short phrases are not a great substitute for
sentence-level rankings, at least in the way we col-
lected them. The average correlation coefficient be-
tween the constituent-based judgments with the sen-
tence ranking judgments is only ? = 0.51. Tables
19 and 20 give a detailed break down of the cor-
relation of the different types of human judgments
with each other on each translation task. It may
be possible to select phrases in such a way that the
constituent-based evaluations are a better substitute
for the sentence-based ranking, for instance by se-
lecting more of constituents from each sentence, or
attempting to cover most of the words in each sen-
tence in a phrase-by-phrase manner. This warrants
further investigation. It might also be worthwhile to
refine the instructions given to annotators about how
to rank the translations of sentences to try to improve
their agreement, which is currently lower than we
would like it to be (although it is substantially bet-
ter than the previous fluency and adequacy scores,
7Note that for the constituent-based evaluations we verified
that the high K was not trivially due to identical phrasal trans-
lations. We excluded screens where all five phrasal translations
presented to the annotator were identical, and report both num-
bers.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  10  20  30  40  50  60
p
e
r
c
e
n
t
 
o
f
 
s
e
n
t
e
n
c
e
s
 
t
a
k
i
n
g
 
t
h
i
s
 
l
o
n
g
time to judge one sentence (seconds)
yes/no judgmentsconstituent ranksentence rank
Figure 3: Distributions of the amount of time it took
to judge single sentences for the three types of man-
ual evaluation
which had a K < .25 in last year?s evaluation).
7.2 Timing
We used the web interface to collect timing infor-
mation. The server recorded the time when a set of
sentences was given to a judge and the time when
the judge returned the sentences. It took annotators
an average of 18 seconds per sentence to rank a list
of sentences.8 It took an average of 10 seconds per
sentence for them to rank constituents, and an av-
erage of 8.5 seconds per sentence for them to make
yes/no judgments. Figure 3 shows the distribution
of times for these tasks.
These timing figures indicate that the tasks which
the annotators were the most reliable on (yes/no
judgments and constituent ranking) were also much
quicker to complete than the ones they were less re-
liable on (ranking sentences). Given that they are
faster at judging short phrases, they can do propor-
tionally more of them. For instance, we could collect
211 yes/no judgments in the same amount of time
that it would take us to collect 100 sentence ranking
judgments. However, this is partially offset by the
fact that many of the translations of shorter phrases
are identical, which means that we have to collect
more judgments in order to distinguish between two
systems.
8Sets which took longer than 5 minutes were excluded from
these calculations, because there was a strong chance that anno-
tators were interrupted while completing the task.
84
7.3 The potential for re-usability of human
judgments
One strong advantage of the yes/no judgments over
the ranking judgments is their potential for reuse.
We have invested hundreds of hours worth of effort
evaluating the output of the translation systems sub-
mitted to this year?s workshop and last year?s work-
shop. While the judgments that we collected pro-
vide a wealth of information for developing auto-
matic evaluation metrics, we cannot not re-use them
to evaluate our translation systems after we update
their parameters or change their behavior in anyway.
The reason for this is that altered systems will pro-
duce different translations than the ones that we have
judged, so our relative rankings of sentences will no
longer be applicable. However, the translations of
short phrases are more likely to be repeated than the
translations of whole sentences.
Therefore if we collect a large number of yes/no
judgments for short phrases, we could build up a
database that contains information about what frag-
mentary translations are acceptable for each sen-
tence in our test corpus. When we change our sys-
tem and want to evaluate it, we do not need to man-
ually evaluate those segments that match against the
database, and could instead have people evaluate
only those phrasal translations which are new. Ac-
cumulating these judgments over time would give
a very reliable idea of what alternative translations
were allowable. This would be useful because it
could alleviate the problems associated with Bleu
failing to recognize allowable variation in translation
when multiple reference translations are not avail-
able (Callison-Burch et al, 2006). A large database
of human judgments might also be useful as an
objective function for minimum error rate training
(Och, 2003) or in other system development tasks.
8 Conclusions
Similar to previous editions of this workshop we car-
ried out an extensive manual and automatic evalua-
tion of machine translation performance for trans-
lating from European languages into English, and
vice versa. One important aspect in which this year?s
shared task differed from previous years was the in-
troduction of an additional newswire test set that
was different in nature to the training data. We
also added new language pairs to our evaluation:
Hungarian-English and German-Spanish.
As in previous years we were pleased to notice an
increase in the number of participants. This year we
received submissions from 23 groups from 18 insti-
tutions. In addition, we evaluated seven commercial
rule-based MT systems.
The goal of this shared-task is two-fold: First we
want to compare state-of-the-art machine translation
systems, and secondly we aim to measure to what
extent different evaluation metrics can be used to as-
sess MT quality.
With respect to MT quality we noticed that the in-
troduction of test sets from a different domain did
have an impact on the ranking of systems. We ob-
served that rule-based systems generally did better
on the News test set. Overall, it cannot be con-
cluded that one approach clearly outperforms other
approaches, as systems performed differently on the
various translation tasks. One general observation is
that for the tasks where statistical combination ap-
proaches participated, they tended to score relatively
high, in particular with respect to Bleu.
With respect to measuring the correlation between
automated evaluation metrics and human judgments
we found that using Meteor and ULCh (which uti-
lizes a variety of metrics, including Meteor) resulted
in the highest Spearman correlation scores on aver-
age, when translating into English. When translat-
ing from English into French, German, and Spanish,
Bleu and posbleu resulted in the highest correlations
with human judgments.
Finally, we investigated inter- and intra-annotator
agreement of human judgments using Kappa coef-
ficients. We noticed that ranking whole sentences
results in relatively low Kappa coefficients, mean-
ing that there is only fair agreement between the as-
sessors. Constituent ranking and acceptability judg-
ments on the other hand showmoderate and substan-
tial inter-annotator agreement, respectively. Intra-
annotator agreement was substantial to almost per-
fect, except for the sentence ranking assessment
where agreement was only moderate. Although it
is difficult to draw exact conclusions from this, one
might wonder whether the sentence ranking task is
simply too complex, involving too many aspects ac-
cording to which translations can be ranked.
The huge wealth of the data generated by this
85
workshop, including the human judgments, system
translations and automatic scores, is available at
http://www.statmt.org/wmt08/ for other
researchers to analyze.
Acknowledgments
This work was supported in parts by the EuroMatrix
project funded by the European Commission (6th
Framework Programme), the GALE program of the
US Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022, and the US Na-
tional Science Foundation under grant IIS-0713448.
We are grateful to Abhaya Agarwal, John Hen-
derson, Rebecca Hwa, Alon Lavie, Mark Przybocki,
Stuart Shieber, and David Smith for discussing dif-
ferent possibilities for calculating the sentence-level
correlation of automatic evaluation metrics with hu-
man judgments in absence of absolute scores. Any
errors in design remain the responsibility of the au-
thors.
Thank you to Eckhard Bick for parsing the Span-
ish test set. See http://beta.visl.sdu.dk for
more information about the constraint-based parser.
Thanks to Greg Hanneman and Antti-Veikko Rosti
for applying their system combination algorithms to
our data.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, June. Association for Computational
Linguistics.
Joshua Albrecht and Rebecca Hwa. 2007a. A
re-examination of machine learning approaches for
sentence-level mt evaluation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2007), Prague, Czech Repub-
lic.
Joshua Albrecht and Rebecca Hwa. 2007b. Regres-
sion for sentence-level mt evaluation with pseudo ref-
erences. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
2007), Prague, Czech Republic.
Joshua Albrecht and Rebecca Hwa. 2008. The role of
pseudo references in MT evaluation. In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, pages 187?190, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of ACL.
Amittai Axelrod, Mei Yang, Kevin Duh, and Katrin
Kirchhoff. 2008. The University of Washington ma-
chine translation system for ACL WMT 2008. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 123?126, Columbus, Ohio, June.
Association for Computational Linguistics.
Nguyen Bach, Qin Gao, and Stephan Vogel. 2008. Im-
proving word alignment with language model based
confidence scores. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 151?
154, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Eckhard Bick. 2006. A constraint grammar-based parser
for Spanish. In Proceedings of the 4th Workshop on
Information and Human Language Technology (TIL-
2006), Ribeiro Preto, Brazil.
Dan Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceed-
ings of Second International Conference on Human
Language Technology Research (HLT-02), San Diego,
California.
Graeme Blackwood, Adria` de Gispert, Jamie Brunning,
and William Byrne. 2008. European language transla-
tion with weighted finite state transducers: The CUED
MT system for the 2008 ACL workshop on SMT. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 131?134, Columbus, Ohio,
June. Association for Computational Linguistics.
Ondr?ej Bojar and Jan Hajic?. 2008. Phrase-based and
deep syntactic English-to-Czech statistical machine
translation. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 143?146,
Columbus, Ohio, June. Association for Computational
Linguistics.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006), Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
86
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
He?le`ne Bonneau-Maynard, Olivier Galibert, Jean-Luc
Gauvain, Philippe Langlais, and Franc?ois Yvon. 2008.
Limsi?s statistical translation systems for WMT?08. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 107?110, Columbus, Ohio,
June. Association for Computational Linguistics.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In Proceedings of ACL.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2008.
Can we relearn an RBMT system? In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, pages 175?178, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Kevin Duh. 2008. Ranking vs. regression in ma-
chine translation evaluation. In Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 191?194, Columbus, Ohio, June. Association
for Computational Linguistics.
Christopher J. Dyer. 2007. The ?noisier channel?: trans-
lation from morphologically complex languages. In
Proceedings of the ACL-2007 Workshop on Statistcal
Machine Translation (WMT-07), Prague, Czech Re-
public.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using moses to integrate multiple
rule-based machine translation engines into a hybrid
system. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 179?182, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Jesus Gimenez and Lluis Marquez. 2008. A smorgas-
bord of features for automatic MT evaluation. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 195?198, Columbus, Ohio, June.
Association for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 9?17, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Greg Hanneman, Edmund Huber, Abhaya Agarwal,
Vamshi Ambati, Alok Parlikar, Erik Peterson, and
Alon Lavie. 2008. Statistical transfer systems for
French-English and German-English machine transla-
tion. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 163?166, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proceedings of the 10th Annual Confer-
ence of the European Association for Machine Trans-
lation, pages 143?152, Budapest, Hungary, May.
Maxim Khalilov, Adolfo Herna?ndez H., Marta R. Costa-
jussa`, Josep M. Crego, Carlos A. Henr??quez Q., Pa-
trik Lambert, Jose? A. R. Fonollosa, Jose? B. Marin?o,
and Rafael E. Banchs. 2008. The TALP-UPC Ngram-
based statistical machine translation system for ACL-
WMT 2008. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 127?130,
Columbus, Ohio, June. Association for Computational
Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation, New
York, New York.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the ACL-2007 Workshop on Statist-
cal Machine Translation (WMT-07), Prague, Czech
Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
of the North American chapter of the Association for
Computational Linguistics (HLT/NAACL-2003), Ed-
monton, Alberta.
Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris
Callison-Burch, Alexandra Constantin, Brooke
Cowan, Chris Dyer, Marcello Federico, Evan Herbst,
Hieu Hoang, Christine Moran, Wade Shen, and
Richard Zens. 2007. Open source toolkit for statisti-
cal machine translation: Factored translation models
and confusion network decoding. CLSP Summer
Workshop Final Report WS-2006, Johns Hopkins
University.
Philipp Koehn, Abhishek Arun, and Hieu Hoang. 2008.
Towards better machine translation quality for the
German-English language pairs. In Proceedings of
the Third Workshop on Statistical Machine Transla-
tion, pages 139?142, Columbus, Ohio, June. Associ-
ation for Computational Linguistics.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
87
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation, Baltimore, MD, October 4?6.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159?174.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: Experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 147?150,
Columbus, Ohio, June. Association for Computational
Linguistics.
Vassilina Nikoulina and Marc Dymetman. 2008. Using
syntactic coupling features for discriminating phrase-
based translations (wmt-08 shared translation task). In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 159?162, Columbus, Ohio,
June. Association for Computational Linguistics.
Attila Nova?k, La?szlo? Tihanyi, and Ga?bor Pro?sze?ky. 2008.
The MetaMorpho translation system. In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, pages 111?114, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: Amethod for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL-2002), Philadelphia, Pennsylvania.
Maja Popovic and Hermann Ney. 2007. Word error rates:
Decomposition over POS classes and applications for
error analysis. In Proceedings of ACL Workshop on
Machine Translation, Prague, Czech Republic.
Mark Przybocki and Kay Peterson, editors. 2008. Pro-
ceedings of the 2008 NIST Open Machine Translation
Evaluation Workshop. Arlington, Virginia, March.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system combi-
nation for machine translation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2007), Prague, Czech Repub-
lic.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186, Columbus, Ohio,
June. Association for Computational Linguistics.
Holger Schwenk, Jean-Baptiste Fouet, and Jean Senel-
lart. 2008. First steps towards a general purpose
French/English statistical machine translation system.
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 119?122, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of morphological analysis in transla-
tion between German and English. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 135?138, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
John Tinsley, Yanjun Ma, Sylwia Ozdowska, and Andy
Way. 2008. MaTrEx: The DCU MT system for WMT
2008. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 171?174, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel
regression framework for machine translation: UCL
system description for WMT 2008 shared translation
task. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 155?158, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zdenek Zabokrtsky, Jan Ptacek, and Petr Pajas. 2008.
TectoMT: Highly modular MT system with tectogram-
matics used as transfer layer. In Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 167?170, Columbus, Ohio, June. Association
for Computational Linguistics.
88
A Automatic scores for each system
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
S
V
M
-R
A
N
K
English-Czech News Commentary Task
CU-BOJAR 0.15 0.21 0.43 0.35 0.28 4.57
CU-BOJAR-CONTRAST-1 0.04 0.11 0.32 0.25 0.18 0.90
CU-BOJAR-CONTRAST-2 0.14 0.2 0.42 0.34 0.27 2.86
CU-TECTOMT 0.09 0.15 0.37 0.29 0.23 2.13
PC-TRANSLATOR 0.08 0.14 0.35 0.28 0.19 2.09
UEDIN 0.12 0.18 0.4 0.32 0.25 2.28
English-Czech News Task
CU-BOJAR 0.11 0.18 0.37 0.3 0.18 4.72
CU-BOJAR-CONTRAST-1 0.02 0.10 0.26 0.2 0.12 0.80
CU-BOJAR-CONTRAST-2 0.09 0.16 0.35 0.28 0.15 2.65
CU-TECTOMT 0.06 0.13 0.32 0.25 0.16 2.14
PC-TRANSLATOR 0.08 0.14 0.33 0.26 0.14 2.40
UEDIN 0.08 0.15 0.34 0.27 0.15 2.13
Table 14: Automatic evaluation metric for translations into Czech
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
M
E
T
E
O
R
-R
M
T
E
R
P
O
S
F
4G
-A
M
P
O
S
F
4G
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
English-French News Task
LIMSI 0.2 0.26 0.16 0.34 0.33 0.48 0.44 0.43 9.74
LIUM-SYSTRAN 0.20 0.26 0.16 0.35 0.34 0.49 0.44 0.44 7.38
LIUM-SYSTRAN-CONTRAST 0.20 0.26 0.16 0.35 0.34 0.48 0.44 0.44 7.02
RBMT1 0.13 0.19 0.12 0.28 0.24 0.42 0.37 0.35 5.46
RBMT3 0.17 0.23 0.14 0.31 0.31 0.45 0.4 0.40 5.60
RBMT4 0.19 0.24 0.15 0.33 0.32 0.48 0.43 0.43 6.80
RBMT5 0.17 0.23 0.14 0.32 0.31 0.47 0.42 0.42 6.15
RBMT6 0.16 0.22 0.13 0.32 0.3 0.46 0.40 0.41 5.60
SAAR 0.15 0.22 0.15 0.33 0.28 0.46 0.41 0.42 6.12
SAAR-CONTRAST 0.17 0.23 0.15 0.33 0.30 0.47 0.42 0.41 5.50
UEDIN 0.16 0.23 0.14 0.32 0.32 0.44 0.39 0.38 4.79
XEROX 0.13 0.2 0.12 0.29 0.29 0.41 0.34 0.34 3.91
XEROX-CONTRAST 0.13 0.2 0.12 0.29 0.29 0.41 0.35 0.35 3.86
English-French Europarl Task
LIMSI 0.32 0.36 0.24 0.42 0.44 0.56 0.53 0.53 8.84
LIUM-SYSTRAN 0.32 0.36 0.24 0.42 0.45 0.56 0.53 0.53 7.46
LIUM-SYSTRAN-CONTRAST 0.31 0.36 0.23 0.42 0.44 0.56 0.52 0.53 6.69
RBMT1 0.15 0.20 0.13 0.29 0.26 0.44 0.4 0.37 3.89
RBMT3 0.18 0.24 0.15 0.34 0.33 0.47 0.42 0.43 4.13
RBMT4 0.2 0.25 0.17 0.35 0.35 0.5 0.45 0.45 4.70
RBMT5 0.12 0.16 0.09 0.22 0.06 0.37 0.32 0.32 3.01
RBMT6 0.17 0.23 0.14 0.33 0.32 0.47 0.42 0.42 3.93
SAAR 0.26 0.29 0.21 0.41 0.34 0.53 0.49 0.48 7.75
SAAR-CONTRAST 0.28 0.32 0.23 0.41 0.39 0.55 0.51 0.52 6.45
UCL 0.24 0.28 0.19 0.37 0.41 0.49 0.44 0.42 4.16
UEDIN 0.30 0.35 0.23 0.42 0.43 0.54 0.51 0.51 6.56
Table 15: Automatic evaluation metric for translations into French
89
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
English-German News Task
LIMSI 0.11 0.18 0.19 0.45 0.22 0.36 0.29 0.28 7.83
LIU 0.10 0.17 0.18 0.44 0.24 0.36 0.28 0.27 4.03
RBMT1 0.12 0.18 0.18 0.44 0.22 0.39 0.33 0.32 5.42
RBMT2 0.13 0.19 0.20 0.46 0.24 0.4 0.33 0.33 5.76
RBMT3 0.12 0.18 0.19 0.44 0.24 0.39 0.32 0.32 4.70
RBMT4 0.14 0.19 0.2 0.46 0.25 0.41 0.35 0.34 5.58
RBMT5 0.11 0.17 0.17 0.43 0.21 0.38 0.31 0.31 4.49
RBMT6 0.10 0.16 0.17 0.43 0.2 0.37 0.3 0.29 4.81
SAAR 0.13 0.19 0.19 0.44 0.27 0.38 0.31 0.3 4.04
SAAR-CONTRAST 0.12 0.18 0.18 0.43 0.26 0.37 0.3 0.28 3.71
UEDIN 0.12 0.17 0.18 0.45 0.23 0.37 0.30 0.29 4.37
English-German Europarl Task
CMU-GIMPEL 0.20 0.24 0.27 0.54 0.32 0.43 0.37 0.37 9.54
LIMSI 0.20 0.24 0.27 0.53 0.32 0.43 0.37 0.37 6.97
LIU 0.2 0.24 0.27 0.53 0.32 0.43 0.38 0.37 6.95
RBMT1 0.11 0.16 0.16 0.42 0.19 0.38 0.32 0.32 5.01
RBMT2 0.12 0.17 0.19 0.46 0.21 0.39 0.32 0.31 5.93
RBMT3 0.11 0.16 0.17 0.43 0.21 0.38 0.31 0.30 4.75
RBMT4 0.12 0.17 0.18 0.45 0.22 0.41 0.34 0.33 5.42
RBMT5 0.1 0.14 0.16 0.42 0.19 0.39 0.32 0.31 4.42
RBMT6 0.09 0.14 0.15 0.42 0.18 0.38 0.30 0.29 4.40
SAAR 0.20 0.25 0.26 0.53 0.32 0.43 0.38 0.37 6.67
SAAR-CONTRAST 0.2 0.24 0.26 0.52 0.31 0.43 0.37 0.37 6.35
UCL 0.16 0.20 0.23 0.49 0.31 0.4 0.33 0.31 5.12
UEDIN 0.21 0.25 0.27 0.54 0.32 0.44 0.38 0.38 7.02
English-Spanish News Task
CMU-SMT 0.19 0.24 0.25 0.34 0.32 0.32 0.25 0.26 8.34
LIMSI 0.19 0.25 0.26 0.34 0.34 0.33 0.26 0.26 5.92
RBMT1 0.16 0.22 0.23 0.32 0.30 0.31 0.23 0.23 5.36
RBMT3 0.19 0.24 0.25 0.33 0.34 0.33 0.26 0.26 5.42
RBMT4 0.21 0.26 0.26 0.34 0.35 0.34 0.28 0.28 6.36
RBMT5 0.18 0.24 0.25 0.33 0.32 0.33 0.26 0.26 5.84
RBMT6 0.19 0.24 0.24 0.33 0.33 0.32 0.25 0.26 5.42
SAAR 0.20 0.27 0.26 0.34 0.37 0.34 0.28 0.28 5.04
SAAR-CONTRAST 0.2 0.26 0.25 0.34 0.37 0.34 0.27 0.27 4.86
UCB 0.20 0.26 0.26 0.34 0.34 0.33 0.26 0.27 5.70
UEDIN 0.18 0.25 0.25 0.33 0.35 0.33 0.26 0.26 4.30
UPC 0.18 0.23 0.24 0.32 0.35 0.32 0.25 0.24 3.97
English-Spanish Europarl Task
CMU-SMT 0.32 0.36 0.33 0.42 0.45 0.40 0.35 0.36 0.10
LIMSI 0.31 0.36 0.33 0.42 0.45 0.4 0.35 0.35 7.80
RBMT1 0.16 0.22 0.24 0.32 0.31 0.32 0.25 0.25 4.47
RBMT3 0.20 0.25 0.25 0.34 0.35 0.33 0.27 0.27 4.66
RBMT4 0.21 0.25 0.26 0.34 0.36 0.34 0.28 0.28 4.85
RBMT5 0.18 0.24 0.25 0.34 0.33 0.34 0.27 0.27 5.03
RBMT6 0.18 0.23 0.25 0.33 0.33 0.33 0.26 0.26 4.57
SAAR 0.31 0.35 0.33 0.41 0.44 0.40 0.35 0.35 7.59
SAAR-CONTRAST 0.30 0.34 0.33 0.41 0.44 0.4 0.34 0.35 7.42
UCL 0.25 0.29 0.29 0.37 0.43 0.36 0.29 0.29 4.67
UEDIN 0.32 0.36 0.33 0.42 0.45 0.40 0.35 0.35 7.25
UPC 0.30 0.34 0.32 0.40 0.46 0.4 0.35 0.34 6.18
UW 0.32 0.36 0.33 0.42 0.45 0.40 0.35 0.35 7.36
UW-CONTRAST 0.32 0.35 0.33 0.42 0.45 0.40 0.35 0.36 7.21
Table 16: Automatic evaluation metric for translations into German and Spanish
90
D
P
D
R
S
R
U
L
C
U
L
C
H
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
Spanish-English Europarl Task
CMU-SMT 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 9.72
CUED 0.33 0.43 0.25 0.29 0.33 0.32 0.38 0.59 0.48 0.50 0.51 0.47 0.47 7.41
CUED-CONTRAST 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 7.00
DCU 0.34 0.43 0.25 0.29 0.33 0.32 0.38 0.59 0.48 0.50 0.51 0.47 0.48 6.78
LIMSI 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 6.73
RBMT3 0.26 0.37 0.19 0.22 0.27 0.19 0.26 0.51 0.41 0.36 0.45 0.4 0.39 5.46
RBMT4 0.26 0.37 0.19 0.22 0.27 0.18 0.26 0.52 0.42 0.36 0.45 0.39 0.38 5.57
RBMT5 0.25 0.36 0.18 0.22 0.27 0.18 0.25 0.51 0.41 0.36 0.44 0.39 0.38 4.74
RBMT6 0.24 0.34 0.18 0.21 0.26 0.17 0.25 0.51 0.41 0.36 0.44 0.38 0.37 4.71
SAAR 0.34 0.44 0.26 0.29 0.33 0.32 0.39 0.59 0.48 0.51 0.52 0.49 0.48 6.30
SAAR-CONTRAST 0.33 0.43 0.25 0.28 0.33 0.30 0.37 0.59 0.48 0.47 0.51 0.47 0.46 7.33
UCL 0.29 0.4 0.21 0.25 0.29 0.25 0.32 0.55 0.43 0.47 0.47 0.42 0.4 4.02
UEDIN 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.50 0.52 0.48 0.48 6.61
UPC 0.33 0.43 0.25 0.28 0.33 0.32 0.38 0.59 0.48 0.5 0.52 0.48 0.48 6.82
French-English News Task
BBN-COMBO 0.27 0.37 0.2 0.23 0.28 0.21 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-COMBO 0.26 0.36 0.18 0.22 0.27 0.19 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-COMBO-CONTRAST n/a n/a n/a n/a n/a 0.19 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-STATXFER 0.21 0.32 0.14 0.19 0.23 0.14 0.22 0.48 0.39 0.28 0.38 0.32 0.30 9.91
CMU-STATXFER-CONTRAST 0.21 0.30 0.14 0.18 0.23 0.14 0.21 0.47 0.38 0.26 0.38 0.31 0.29 6.47
CUED 0.25 0.35 0.17 0.21 0.26 0.18 0.27 0.51 0.41 0.37 0.41 0.35 0.34 6.34
CUED-CONTRAST 0.26 0.37 0.18 0.22 0.27 0.19 0.28 0.52 0.42 0.38 0.42 0.37 0.36 6.29
LIMSI 0.26 0.37 0.18 0.22 0.27 0.20 0.28 0.51 0.40 0.40 0.43 0.38 0.37 5.75
LIUM-SYSTRAN 0.27 0.38 0.19 0.23 0.27 0.21 0.29 0.51 0.41 0.41 0.44 0.39 0.38 6.32
LIUM-SYSTRAN-CONTRAST 0.27 0.38 0.19 0.23 0.28 0.21 0.29 0.51 0.41 0.41 0.44 0.39 0.38 5.93
RBMT3 0.24 0.36 0.17 0.21 0.26 0.16 0.24 0.49 0.40 0.29 0.42 0.36 0.34 7.61
RBMT4 0.25 0.37 0.17 0.21 0.26 0.17 0.25 0.49 0.4 0.33 0.42 0.36 0.35 6.17
RBMT5 0.25 0.37 0.18 0.22 0.27 0.18 0.25 0.51 0.41 0.33 0.43 0.37 0.36 6.97
RBMT6 0.24 0.36 0.17 0.21 0.26 0.16 0.24 0.49 0.39 0.30 0.41 0.35 0.34 6.51
SAAR 0.24 0.14 0.17 0.19 0.22 0.15 0.24 0.47 0.37 0.39 0.39 0.32 0.31 3.22
SAAR-CONTRAST 0.26 0.36 0.18 0.22 0.27 0.17 0.27 0.51 0.41 0.36 0.41 0.35 0.35 6.01
UEDIN 0.25 0.36 0.17 0.21 0.26 0.18 0.26 0.51 0.41 0.35 0.42 0.36 0.35 5.97
UEDIN-COMBO 0.26 0.36 0.18 0.23 0.27 n/a n/a n/a n/a n/a n/a n/a n/a n/a
French-English Europarl Task
CMU-STATXFER 0.24 0.34 0.18 0.22 0.26 0.2 0.26 0.52 0.42 0.37 0.42 0.36 0.35 9.85
CMU-STATXFER-CONTRAST 0.25 0.34 0.19 0.22 0.26 0.2 0.26 0.53 0.42 0.38 0.42 0.36 0.35 7.10
CUED 0.34 0.44 0.26 0.29 0.33 0.32 0.38 0.59 0.48 0.50 0.51 0.47 0.47 0.11
CUED-CONTRAST 0.34 0.44 0.26 0.29 0.34 0.32 0.39 0.59 0.48 0.51 0.51 0.47 0.47 9.34
DCU 0.33 0.43 0.25 0.28 0.33 0.31 0.37 0.58 0.47 0.49 0.50 0.46 0.46 9.16
LIMSI 0.34 0.44 0.26 0.29 0.34 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 9.59
LIUM-SYSTRAN 0.35 0.45 0.27 0.3 0.34 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.49 9.75
LIUM-SYSTRAN-CONTRAST 0.34 0.44 0.26 0.29 0.34 0.33 0.39 0.59 0.48 0.50 0.52 0.48 0.48 9.23
RBMT3 0.25 0.36 0.10 0.20 0.24 0.17 0.25 0.51 0.41 0.35 0.43 0.37 0.36 7.36
RBMT4 0.27 0.36 0.19 0.22 0.27 0.18 0.26 0.51 0.41 0.37 0.43 0.38 0.37 5.92
RBMT5 0.27 0.38 0.21 0.23 0.28 0.20 0.28 0.53 0.43 0.4 0.45 0.4 0.39 7.20
RBMT6 0.24 0.35 0.18 0.21 0.26 0.16 0.24 0.5 0.40 0.35 0.42 0.36 0.35 5.96
SAAR 0.32 0.41 0.23 0.27 0.31 0.27 0.33 0.54 0.43 0.49 0.49 0.44 0.41 4.76
SAAR-CONTRAST 0.33 0.43 0.25 0.28 0.33 0.3 0.36 0.58 0.48 0.47 0.51 0.47 0.46 0.10
SYSTRAN 0.3 0.4 0.23 0.26 0.30 0.26 0.34 0.55 0.45 0.46 0.48 0.43 0.43 7.01
UCL 0.3 0.40 0.22 0.26 0.3 0.26 0.32 0.55 0.44 0.47 0.47 0.42 0.41 6.35
UEDIN 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.50 0.52 0.48 0.48 9.41
Table 17: Automatic evaluation metric for translations into English
91
D
P
D
R
S
R
U
L
C
U
L
C
H
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
Czech-English News Commentary Task
DCU 0.25 0.34 0.18 0.22 0.27 0.21 0.29 0.54 0.44 0.42 0.42 0.36 0.36 2.45
SYSTRAN 0.19 0.28 0.12 0.17 0.21 0.15 0.23 0.45 0.36 0.34 0.36 0.29 0.29 0.76
UEDIN 0.24 0.31 0.16 0.21 0.25 0.22 0.30 0.54 0.44 0.43 0.41 0.35 0.35 1.37
UMD 0.26 0.34 0.19 0.23 0.28 0.24 0.33 0.56 0.45 0.49 0.44 0.39 0.38 1.41
Czech-English News Task
DCU 0.19 0.30 0.13 0.17 0.22 0.12 0.22 0.45 0.35 0.32 0.36 0.28 0.28 1.78
UEDIN 0.19 0.28 0.12 0.17 0.21 0.12 0.21 0.44 0.34 0.32 0.35 0.27 0.27 0.65
UMD 0.2 0.29 0.12 0.18 0.22 0.13 0.22 0.44 0.34 0.36 0.36 0.29 0.27 0.52
German-English News Task
BBN-COMBO 0.23 0.34 0.14 0.21 0.25 0.18 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-STATXFER 0.16 0.27 0.09 0.15 0.19 0.11 0.18 0.43 0.34 0.25 0.33 0.25 0.24 7.84
LIMSI 0.22 0.33 0.13 0.19 0.23 0.17 0.25 0.47 0.37 0.36 0.4 0.33 0.32 5.58
LIU 0.21 0.32 0.06 0.18 0.22 0.15 0.24 0.48 0.38 0.33 0.38 0.31 0.31 5.51
RBMT1 0.22 0.33 0.14 0.19 0.23 0.14 0.22 0.44 0.35 0.28 0.37 0.31 0.30 6.13
RBMT2 0.24 0.37 0.17 0.21 0.26 0.15 0.24 0.5 0.40 0.31 0.4 0.33 0.32 7.14
RBMT3 0.24 0.37 0.16 0.21 0.26 0.16 0.24 0.49 0.4 0.32 0.41 0.34 0.34 6.97
RBMT4 0.25 0.38 0.17 0.21 0.27 0.16 0.25 0.50 0.40 0.34 0.41 0.35 0.34 7.03
RBMT5 0.23 0.36 0.15 0.20 0.25 0.15 0.23 0.48 0.39 0.32 0.4 0.33 0.32 5.94
RBMT6 0.22 0.34 0.14 0.19 0.24 0.14 0.22 0.47 0.38 0.31 0.39 0.32 0.31 5.65
SAAR 0.22 0.33 0.14 0.2 0.24 0.15 0.24 0.47 0.37 0.36 0.39 0.32 0.31 4.67
SAAR-CONTRAST 0.24 0.35 0.16 0.21 0.25 0.17 0.26 0.5 0.4 0.36 0.4 0.33 0.33 5.80
SAAR-CONTRAST-2 0.21 0.33 0.14 0.19 0.23 0.15 0.24 0.47 0.37 0.36 0.39 0.32 0.31 4.80
UEDIN 0.23 0.34 0.09 0.19 0.23 0.16 0.25 0.48 0.39 0.35 0.4 0.33 0.33 5.72
German-English Europarl Task
CMU-STATXFER 0.2 0.31 0.12 0.19 0.22 0.17 0.23 0.49 0.39 0.34 0.39 0.32 0.31 7.11
LIMSI 0.28 0.38 0.18 0.24 0.28 0.27 0.33 0.55 0.44 0.43 0.47 0.42 0.42 8.04
LIU 0.28 0.39 0.09 0.23 0.26 0.27 0.33 0.55 0.44 0.44 0.47 0.43 0.43 7.46
RBMT1 0.21 0.3 0.14 0.18 0.22 0.12 0.19 0.42 0.33 0.27 0.36 0.30 0.28 4.61
RBMT2 0.24 0.35 0.16 0.20 0.25 0.14 0.23 0.49 0.39 0.32 0.39 0.33 0.32 5.42
RBMT3 0.24 0.35 0.16 0.20 0.25 0.15 0.23 0.48 0.39 0.32 0.40 0.34 0.33 5.43
RBMT4 0.24 0.36 0.15 0.20 0.25 0.14 0.23 0.49 0.39 0.34 0.41 0.34 0.34 5.11
RBMT5 0.23 0.34 0.15 0.2 0.24 0.14 0.22 0.48 0.38 0.33 0.4 0.33 0.32 4.55
RBMT6 0.22 0.33 0.13 0.18 0.23 0.13 0.21 0.47 0.37 0.31 0.38 0.31 0.31 4.08
SAAR 0.29 0.39 0.19 0.25 0.28 0.27 0.33 0.55 0.44 0.43 0.47 0.42 0.42 7.32
SAAR-CONTRAST 0.28 0.37 0.18 0.24 0.28 0.26 0.32 0.54 0.43 0.43 0.47 0.42 0.42 6.77
UCL 0.24 0.36 0.16 0.22 0.25 0.2 0.25 0.49 0.39 0.41 0.42 0.35 0.32 4.26
UEDIN 0.30 0.41 0.20 0.26 0.3 0.28 0.34 0.56 0.45 0.45 0.48 0.44 0.44 7.96
Spanish-English News Task
CMU-SMT 0.24 0.35 0.17 0.21 0.25 0.18 0.26 0.48 0.38 0.39 0.41 0.35 0.34 8.00
CUED 0.25 0.36 0.17 0.21 0.26 0.19 0.28 0.50 0.40 0.38 0.42 0.36 0.36 6.03
CUED-CONTRAST 0.26 0.37 0.18 0.22 0.27 0.21 0.3 0.52 0.42 0.39 0.44 0.38 0.38 6.27
LIMSI 0.26 0.37 0.18 0.22 0.27 0.20 0.28 0.50 0.4 0.41 0.43 0.38 0.37 4.93
RBMT3 0.25 0.38 0.17 0.22 0.27 0.18 0.26 0.50 0.41 0.32 0.43 0.38 0.36 7.54
RBMT4 0.26 0.38 0.18 0.22 0.27 0.18 0.26 0.51 0.42 0.32 0.44 0.39 0.37 7.81
RBMT5 0.26 0.38 0.08 0.20 0.25 0.2 0.27 0.51 0.42 0.33 0.44 0.38 0.37 6.89
RBMT6 0.25 0.36 0.17 0.21 0.26 0.18 0.25 0.51 0.41 0.33 0.43 0.37 0.36 6.83
SAAR 0.26 0.37 0.19 0.22 0.27 0.19 0.29 0.51 0.41 0.39 0.43 0.37 0.37 5.23
SAAR-CONTRAST 0.26 0.37 0.18 0.22 0.27 0.19 0.28 0.51 0.41 0.37 0.42 0.37 0.36 5.95
UCB 0.25 0.35 0.17 0.21 0.26 0.19 0.27 0.5 0.39 0.39 0.42 0.36 0.35 4.40
UEDIN 0.24 0.35 0.17 0.21 0.26 0.18 0.27 0.50 0.40 0.36 0.41 0.35 0.34 5.07
UEDIN-COMBO 0.27 0.36 0.19 0.23 0.27 n/a n/a n/a n/a n/a n/a n/a n/a n/a
UPC 0.25 0.36 0.17 0.21 0.26 0.19 0.26 0.49 0.39 0.4 0.43 0.37 0.36 4.38
Table 18: Automatic evaluation metric for translations into English
92
B Break down of correlation for each task
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
D
P
D
R
S
R
U
L
C
U
L
C
H
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
All-English News Task
RANK 1 n/a n/a 0.83 0.73 0.83 0.83 0.87 0.71 0.7 0.82 0.79 0.41 0.79 0.8 0.80 0.25
French-English News Task
RANK 1 0.69 0.63 0.92 0.83 0.89 0.90 0.90 0.81 0.80 0.88 0.80 0.57 0.87 0.9 0.9 ?
0.21
CONST ? 1 0.81 0.83 0.52 0.81 0.86 0.81 0.93 0.9 0.76 0.64 0.73 0.69 0.72 0.85 ?
0.52
YES/NO ? ? 1 0.71 0.57 0.76 0.77 0.74 0.79 0.75 0.67 0.59 0.62 0.66 0.67 0.79 ?
0.26
French-English Europarl Task
RANK 1 0.95 0.9 0.94 0.95 0.93 0.95 0.93 0.92 0.90 0.88 0.87 0.92 0.94 0.94 0.91 0.50
CONST ? 1 0.91 0.97 0.97 0.98 0.98 0.97 0.97 0.96 0.97 0.95 0.96 0.97 0.97 0.96 0.56
YES/NO ? ? 1 0.94 0.94 0.94 0.96 0.96 0.96 0.97 0.92 0.93 0.92 0.95 0.95 0.97 0.47
German-English News Task
RANK 1 0.56 0.56 0.85 0.93 0.92 0.85 0.95 0.12 0.09 0.83 0.89 ?
0.11
0.63 0.60 0.58 0.36
CONST ? 1 0.48 0.54 0.48 0.59 0.66 0.57 0.64 0.65 0.61 0.55 0.51 0.57 0.63 0.56 ?
0.02
YES/NO ? ? 1 0.68 0.61 0.69 0.73 0.67 0.60 0.41 0.54 0.56 0.33 0.79 0.83 0.70 0.08
German-English Europarl Task
RANK 1 0.63 0.81 0.76 0.59 0.46 0.57 0.60 0.30 0.39 0.40 0.66 0.25 0.53 0.53 0.64 0.35
CONST ? 1 0.78 0.87 0.92 0.51 0.83 0.86 0.69 0.69 0.76 0.80 0.69 0.88 0.88 0.88 0.61
YES/NO ? ? 1 0.88 0.77 0.48 0.77 0.78 0.66 0.67 0.64 0.86 0.58 0.74 0.74 0.85 0.78
Spanish-English News Task
RANK 1 ?
0.07
0.44 0.75 0.76 0.68 0.71 0.81 0.19 0.01 0.66 0.63 ?
0.12
0.73 0.76 0.66 0.36
CONST ? 1 0.66 ?
0.03
?
0.44
0.29 0.29 0.14 0.45 0.66 ?
0.11
?
0.33
0.77 ?
0.37
?
0.34
0.16 ?
0.58
YES/NO ? ? 1 0.29 0.05 0.73 0.64 0.55 0.48 0.47 0.09 ?
0.11
0.71 0.06 0.1 0.39 ?
0.43
Spanish-English Europarl Task
RANK 1 0.69 0.76 0.78 0.73 0.73 0.8 0.77 0.78 0.79 0.83 0.84 0.77 0.73 0.73 0.80 0.87
CONST ? 1 0.68 0.76 0.77 0.75 0.69 0.73 0.64 0.67 0.64 0.68 0.73 0.78 0.78 0.73 0.56
YES/NO ? ? 1 0.94 0.93 0.95 0.96 0.95 0.98 0.97 0.91 0.91 0.95 0.94 0.94 0.98 0.69
Table 19: Correlation of automatic evaluation metrics with the three types of human judgments for transla-
tion into English
93
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
English-French News Task
RANK 1 0.55 0.48 0.73 0.62 0.3 0.47 0.56 0.69 0.69 0.66 0.72
CONST ? 1 0.35 0.49 0.47 0.39 0.49 0.24 0.59 0.59 0.58 0.45
YES/NO ? ? 1 0.81 0.92 0.71 0.73 0.78 0.73 0.73 0.76 0.76
English-French Europarl Task
RANK 1 0.98 0.88 0.95 0.95 0.95 0.95 0.90 0.97 0.97 0.93 0.93
CONST ? 1 0.94 0.98 0.98 0.98 0.98 0.93 1 1 0.97 0.91
YES/NO ? ? 1 0.97 0.97 0.97 0.97 0.92 0.95 0.95 0.92 0.83
English-German News Task
RANK 1 0.57 0.71 0.58 0.42 0.43 0.13 0.25 0.90 0.90 0.90 0.32
CONST ? 1 0.78 0.75 0.83 0.82 0.55 0.60 0.72 0.72 0.72 0.58
YES/NO ? ? 1 0.62 0.54 0.51 0.36 0.23 0.75 0.75 0.75 0.76
English-German Europarl Task
RANK 1 0.28 0.57 0.36 0.36 0.42 0.39 0.26 0.38 0.38 0.50 0.56
CONST ? 1 0.87 0.88 0.88 0.91 0.90 0.93 0.88 0.88 0.80 0.85
YES/NO ? ? 1 0.89 0.89 0.96 0.96 0.84 0.86 0.86 0.87 0.98
English-Spanish News Task
RANK 1 ?
0.30
0.49 ?
0.04
?
0.47
?
0.25
?
0.29
?
0.33
?
0.19
?
0.19
?
0.07
0.02
CONST ? 1 0.43 0.79 0.61 0.64 0.56 0.2 0.59 0.59 0.55 0.56
YES/NO ? ? 1 0.55 0.41 0.43 0.31 0.13 0.65 0.65 0.72 0.16
English-Spanish Europarl Task
RANK 1 0.90 0.63 0.8 0.83 0.84 0.83 0.73 0.79 0.79 0.76 0.80
CONST ? 1 0.73 0.84 0.86 0.81 0.8 0.74 0.84 0.83 0.84 0.86
YES/NO ? ? 1 0.68 0.75 0.66 0.67 0.90 0.67 0.66 0.73 0.68
Table 20: Correlation of automatic evaluation metrics with the three types of human judgments for transla-
tion into other languages
94
C Pairwise system comparisons by human judges
The following tables show pairwise comparisons between systems for each language pair, test set, and
manual evaluation type. The numbers in each of the tables? cells indicate the percent of that the system in
that column was judged to be better than the system in that row. Bolding indicates the winner of the two
systems. The difference between 100 and the sum of the complimentary cells is the percent of time that the
two systems were judged to be equal.
Because there were so many systems and data conditions the significance of each pairwise comparison
needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differ-
ences (rather than differences that are attributable to chance). In the following tables ? indicates statistical
significance at p <= 0.05 and ? indicates statistical significance at p <= 0.01, according to the Sign Test.
B
B
N
-C
M
B
C
M
U
-C
M
B
C
M
U
-X
F
R
C
U
E
D
C
U
E
D
-C
L
IM
S
I
L
IU
M
-S
Y
S
L
IU
M
-S
Y
S
-C
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
U
E
D
IN
-C
M
B
BBN-CMB 0.32 0.18? 0.21 0.42 0.37 0.29 0.24 0.33 0.48 0.48 0.32 0.29 0.44 0.48 0.21
CMU-CMB 0.50 0.26 0.29 0.42 0.4 0.44 0.48 0.49 0.38 0.45 0.55 0.32 0.34 0.34 0.46
CMU-XFR 0.67? 0.44 0.60? 0.75? 0.58 0.73? 0.62 0.59 0.54 0.77? 0.48 0.54 0.65? 0.71? 0.58
CUED 0.46 0.41 0.20? 0.47 0.56 0.47 0.51? 0.41 0.54 0.57 0.37 0.43 0.61 0.39 0.15
CUED-C 0.27 0.22 0.08? 0.20 0.31 0.54 0.52? 0.32 0.52 0.50 0.31 0.40 0.38 0.30 0.52
LIMSI 0.34 0.4 0.29 0.31 0.41 0.23? 0.52 0.38 0.50 0.39 0.49 0.42 0.32 0.26 0.30
LIUM-SYS 0.37 0.32 0.13? 0.39 0.27 0.60? 0.24 0.44 0.46 0.46 0.33 0.24? 0.25 0.30 0.19
LI-SYS-C 0.40 0.26 0.24 0.20? 0.13? 0.30 0.24 0.44 0.42 0.43 0.35 0.21? 0.30 0.30 0.31
RBMT3 0.46 0.43 0.26 0.38 0.46 0.48 0.39 0.39 0.41 0.44 0.26 0.36 0.50 0.68? 0.44
RBMT4 0.36 0.33 0.31 0.36 0.39 0.35 0.50 0.45 0.45 0.49 0.40 0.35 0.57 0.51 0.53
RBMT5 0.37 0.33 0.12? 0.32 0.33 0.33 0.39 0.46 0.25 0.22 0.21 0.37 0.44 0.49 0.57
RBMT6 0.50 0.33 0.37 0.34 0.50 0.39 0.44 0.50 0.48 0.37 0.55 0.42 0.48 0.41 0.41
SAAR 0.50 0.46 0.37 0.38 0.44 0.52 0.6? 0.54? 0.44 0.53 0.44 0.29 0.34 0.52 0.50
SAAR-C 0.31 0.47 0.23? 0.30 0.24 0.51 0.50 0.47 0.25 0.31 0.33 0.35 0.26 0.47 0.38
UED 0.35 0.37 0.13? 0.39 0.55 0.50 0.50 0.43 0.24? 0.37 0.36 0.41 0.31 0.47 0.36
UED-CMB 0.57 0.36 0.16 0.46 0.38 0.30 0.63 0.39 0.39 0.37 0.35 0.53 0.27 0.48 0.36
> OTHERS 0.43 0.37 0.22 0.34 0.41 0.44 0.45 0.45 0.4 0.42 0.47 0.37 0.34 0.43 0.44 0.42
? OTHERS 0.66 0.59 0.38 0.55 0.64 0.63 0.66 0.69 0.58 0.58 0.65 0.57 0.54 0.64 0.61 0.61
Table 21: Sentence-level ranking for the French-English News Task.
C
M
U
-X
F
R
C
U
E
D
D
C
U
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
S
Y
S
T
R
A
N
U
C
L
U
E
D
IN
CMU-XFR 0.53 0.50 0.74? 0.79? 0.55 0.46 0.50 0.36 0.73 0.92? 0.36 0.44 0.77?
CUED 0.29 0.42 0.29 0.48 0.16? 0.53 0.16? 0.18? 0.18? 0.55 0.06? 0.21 0.38
DCU 0.46 0.29 0.38 0.47 0.37 0.27 0.24 0.29 0.35 0.55 0.18 0.25 0.50
LIMSI 0.11? 0.21 0.44 0.11 0.12? 0.17 0.29 0.05? 0.30 0.32 0.19 0.29 0.33
LIUM-SYS 0.14? 0.16 0.24 0.32 0.06? 0.13 0.22 0.12? 0.14? 0.33 0.20? 0.26 0.32
RBMT3 0.36 0.79? 0.58 0.88? 0.72? 0.40 0.57 0.21 0.67 0.72? 0.50 0.54 0.67
RBMT4 0.50 0.40 0.64 0.67 0.56 0.40 0.42 0.21? 0.52 0.67 0.33 0.47 0.75
RBMT5 0.38 0.79? 0.60 0.57 0.56 0.24 0.42 0.26 0.48 0.72? 0.50 0.46 0.60
RBMT6 0.54 0.79? 0.67 0.77? 0.82? 0.47 0.79? 0.53 0.71? 0.83? 0.56 0.47 0.77?
SAAR 0.27 0.59? 0.57 0.47 0.71? 0.22 0.29 0.48 0.18? 0.50 0.35 0.23 0.50
SAAR-C 0.04? 0.15 0.31 0.39 0.48 0.14? 0.24 0.21? 0.08? 0.21 0.17? 0.20 0.57
SYSTRAN 0.50 0.81? 0.65 0.52 0.64? 0.38 0.62 0.33 0.32 0.41 0.71? 0.56 0.55
UCL 0.31 0.64 0.56 0.57 0.47 0.46 0.40 0.39 0.27 0.55 0.60 0.44 0.47
UED 0.24? 0.43 0.35 0.33 0.42 0.28 0.25 0.33 0.15? 0.29 0.26 0.25 0.27
> OTHERS 0.32 0.50 0.5 0.54 0.55 0.28 0.4 0.35 0.21 0.41 0.59 0.32 0.35 0.55
? OTHERS 0.42 0.7 0.64 0.78 0.79 0.40 0.50 0.48 0.32 0.58 0.75 0.47 0.52 0.71
Table 22: Sentence-level ranking for the French-English Europarl Task.
95
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
X
E
R
O
X
LIMSI 0.29 0.25 0.60? 0.52 0.48 0.13 0.30 0.13? 0.17?
LIUM-SYSTRAN 0.36 0.41 0.51 0.41 0.53 0.22 0.26 0.27 0.04?
RBMT3 0.56 0.34 0.48 0.52 0.40 0.31 0.53 0.37 0.11?
RBMT4 0.13? 0.36 0.31 0.29 0.19? 0.26 0.15? 0.17? 0.09?
RBMT5 0.33 0.35 0.29 0.42 0.26 0.17? 0.32 0.17? 0.12?
RBMT6 0.42 0.38 0.37 0.43? 0.44 0.32 0.32 0.28 0.11?
SAAR 0.56 0.52 0.51 0.56 0.69? 0.41 0.33 0.46 0.3
SAAR-CONTRAST 0.55 0.44 0.33 0.63? 0.56 0.46 0.21 0.41 0.22?
UEDIN 0.48? 0.48 0.41 0.60? 0.65? 0.53 0.41 0.43 0.09?
XEROX 0.63? 0.74? 0.78? 0.74? 0.71? 0.75? 0.44 0.64? 0.63?
> OTHERS 0.44 0.43 0.41 0.54 0.53 0.43 0.28 0.37 0.32 0.13
? OTHERS 0.67 0.66 0.60 0.78 0.73 0.66 0.51 0.57 0.55 0.32
Table 23: Sentence-level ranking for the English-French News Task.
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
C
L
U
E
D
IN
LIMSI 0.23 0.21? 0.32 0.10? 0.15? 0.35 0.27 0.15? 0.17
LIUM-SYSTRAN 0.28 0.39 0.11? 0.21? 0.22 0.40 0.19? 0.15
RBMT3 0.75? 0.59 0.38 0.39 0.49 0.70? 0.81? 0.47 0.81?
RBMT4 0.64 0.36 0.28 0.24? 0.18 0.61 0.48 0.42 0.50
RBMT5 0.85? 0.89? 0.49 0.62? 0.67? 0.78? 0.91? 0.63? 0.93?
RBMT6 0.85? 0.62? 0.26 0.42 0.24? 0.83? 0.82? 0.47 0.68?
SAAR 0.41 0.52 0.17? 0.30 0.11? 0.06? 0.41 0.11? 0.41
SAAR-CONTRAST 0.47 0.40 0.11? 0.26 0.03? 0.06? 0.32 0.27 0.26
UCL 0.80? 0.70? 0.42 0.47 0.22? 0.44 0.71? 0.61 0.78?
UEDIN 0.46 0.41 0.11? 0.33 0.04? 0.15? 0.32 0.36 0.03?
> OTHERS 0.62 0.54 0.26 0.4 0.17 0.27 0.56 0.6 0.32 0.54
? OTHERS 0.79 0.78 0.42 0.61 0.26 0.44 0.74 0.79 0.44 0.77
Table 24: Sentence-level ranking for the English-French Europarl Task.
B
B
N
-C
M
B
C
M
U
-X
F
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
U
E
D
IN
-C
M
B
BBN-COMBO 0.1? 0.22 0.37 0.62? 0.69? 0.74? 0.66? 0.41 0.63? 0.60? 0.35 0.40
CMU-STATXFER 0.71? 0.44 0.54 0.76? 0.79? 0.73? 0.74? 0.80? 0.62? 0.65? 0.54? 0.37
LIMSI 0.44 0.24 0.41 0.67? 0.65? 0.69? 0.54 0.50 0.50 0.63 0.38 0.22
LIU 0.37 0.27 0.34 0.55? 0.56 0.61? 0.50 0.45 0.48 0.56 0.32 0.34
RBMT2 0.21? 0.14? 0.31? 0.20? 0.27 0.43 0.29 0.34 0.30 0.13? 0.25? 0.24?
RBMT3 0.18? 0.13? 0.19? 0.27 0.56 0.37 0.33 0.32 0.29 0.29 0.19? 0.17?
RBMT4 0.22? 0.12? 0.17? 0.18? 0.46 0.51 0.3 0.31 0.18? 0.26? 0.28 0.17?
RBMT5 0.22? 0.12? 0.32 0.36 0.58 0.51 0.40 0.29 0.23? 0.37 0.3 0.28
RBMT6 0.55 0.08? 0.40 0.4 0.51 0.51 0.47 0.51 0.49 0.52 0.22? 0.43
SAAR 0.23? 0.21? 0.40 0.39 0.52 0.50 0.61? 0.53? 0.38 0.50? 0.26? 0.13?
SAAR-CONTRAST 0.23? 0.19? 0.3 0.37 0.71? 0.37 0.60? 0.37 0.33 0.17? 0.48 0.13?
UEDIN 0.23 0.13? 0.38 0.3 0.68? 0.65? 0.55 0.59 0.64? 0.67? 0.38 0.42
UEDIN-COMBO 0.35 0.41 0.59 0.50 0.72? 0.66? 0.83? 0.56 0.52 0.50? 0.67? 0.38
> OTHERS 0.32 0.17 0.34 0.35 0.61 0.56 0.57 0.49 0.45 0.41 0.46 0.33 0.28
? OTHERS 0.51 0.35 0.52 0.56 0.74 0.73 0.73 0.67 0.59 0.61 0.65 0.55 0.44
Table 25: Sentence-level ranking for the German-English News Task.
96
C
M
U
-X
F
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-STATXFER 0.57? 0.77? 0.53 0.71? 0.69? 0.50 0.58 0.82? 0.46 0.75?
LIMSI 0.17? 0.35 0.71? 0.63 0.76? 0.50 0.59 0.52 0.23 0.67?
LIU 0.14? 0.35 0.50 0.29 0.67 0.3 0.42 0.35 0.27 0.57
RBMT2 0.27 0.24? 0.46 0.39 0.33 0.36 0.42 0.50 0.33 0.46
RBMT3 0.23? 0.3 0.57 0.45 0.40 0.31 0.38 0.56 0.32 0.55
RBMT4 0.22? 0.19? 0.29 0.50 0.48 0.39 0.48 0.41 0.32 0.61
RBMT5 0.40 0.40 0.56 0.54 0.57 0.52 0.3 0.48 0.29? 0.54
RBMT6 0.27 0.32 0.48 0.46 0.53 0.44 0.51 0.55 0.36 0.61
SAAR 0.12? 0.19 0.30 0.44 0.41 0.48 0.32 0.42 0.20? 0.40
UCL 0.35 0.54 0.46 0.63 0.61 0.68 0.68? 0.61 0.63? 0.65?
UEDIN 0.22? 0.17? 0.32 0.42 0.42 0.36 0.41 0.27 0.40 0.23?
> OTHERS 0.24 0.32 0.46 0.51 0.51 0.53 0.43 0.43 0.53 0.30 0.58
? OTHERS 0.36 0.49 0.61 0.63 0.6 0.61 0.54 0.54 0.68 0.42 0.68
Table 26: Sentence-level ranking for the German-English Europarl Task.
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
E
D
IN
LIMSI 0.44 0.8? 0.67? 0.81? 0.76? 0.63? 0.53 0.47?
LIU 0.29 0.80? 0.68? 0.81? 0.62? 0.63? 0.25 0.31
RBMT2 0.13? 0.07? 0.35 0.33 0.32? 0.20? 0.17? 0.09?
RBMT3 0.18? 0.27? 0.50 0.52 0.45 0.29? 0.26 0.21?
RBMT4 0.09? 0.12? 0.47 0.30 0.42 0.22? 0.15? 0.17?
RBMT5 0.12? 0.26? 0.59? 0.42 0.40 0.33 0.28 0.24?
RBMT6 0.25? 0.22? 0.6? 0.61? 0.63? 0.50 0.36 0.33
SAAR 0.28 0.63 0.66? 0.56 0.7? 0.62 0.46 0.45
UEDIN 0.24? 0.42 0.75? 0.66? 0.73? 0.68? 0.51 0.36
> OTHERS 0.19 0.28 0.64 0.54 0.61 0.54 0.40 0.3 0.27
? OTHERS 0.36 0.43 0.79 0.66 0.75 0.67 0.56 0.46 0.44
Table 27: Sentence-level ranking for the English-German News Task.
C
M
U
-G
IM
P
E
L
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-GIMPEL 0.29 0.28 0.41 0.49 0.56 0.44 0.24? 0.09? 0.24? 0.52
LIMSI 0.45 0.31 0.48 0.45 0.54 0.40 0.35 0.40 0.29? 0.47
LIU 0.34 0.47 0.56 0.44 0.65? 0.37 0.30 0.31 0.19? 0.50
RBMT2 0.51 0.48 0.41 0.41 0.48 0.22? 0.24? 0.62 0.26? 0.43
RBMT3 0.40 0.50 0.47 0.47 0.60 0.33 0.3? 0.11 0.26? 0.50
RBMT4 0.39 0.37 0.27? 0.41 0.35 0.22? 0.14? 0.25 0.33 0.46
RBMT5 0.49 0.47 0.54 0.64? 0.60 0.64? 0.32 0.47 0.45 0.64?
RBMT6 0.71? 0.50 0.58 0.57? 0.65? 0.74? 0.46 0.41 0.36 0.60
SAAR 0.73? 0.40 0.39 0.39 0.78 0.58 0.47 0.35 0.31 0.50
UCL 0.61? 0.6? 0.67? 0.59? 0.68? 0.64 0.53 0.51 0.62 0.70?
UEDIN 0.25 0.27 0.30 0.52 0.41 0.49 0.26? 0.31 0.25 0.23?
> OTHERS 0.47 0.43 0.43 0.51 0.51 0.59 0.36 0.3 0.37 0.3 0.54
? OTHERS 0.61 0.58 0.58 0.62 0.58 0.68 0.47 0.43 0.53 0.39 0.67
Table 28: Sentence-level ranking for the English-German Europarl Task.
97
C
M
U
-S
M
T
C
U
E
D
C
U
E
D
-C
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.41 0.62? 0.33 0.54? 0.57? 0.42 0.46 0.46 0.29 0.34 0.37
CUED 0.29 0.24 0.27 0.54? 0.76? 0.61? 0.50 0.39 0.46 0.26 0.42
CUED-CONTRAST 0.19? 0.24 0.23 0.47 0.48 0.28 0.41 0.37 0.26 0.26 0.33
LIMSI 0.33 0.30 0.51 0.41 0.56? 0.47 0.41 0.46 0.33 0.37 0.43
RBMT3 0.19? 0.23? 0.37 0.43 0.39 0.28 0.3 0.33 0.39 0.30 0.49
RBMT4 0.19? 0.14? 0.27 0.21? 0.27 0.21? 0.30 0.27 0.17? 0.29? 0.23?
RBMT5 0.37 0.19? 0.56 0.35 0.47 0.57? 0.56 0.43 0.24? 0.35 0.52
RBMT6 0.41 0.30 0.29 0.39 0.43 0.50 0.25 0.46 0.34 0.44 0.46
SAAR 0.29 0.25 0.43 0.32 0.50 0.42 0.33 0.31 0.2? 0.26 0.3
UCB 0.29 0.36 0.52 0.49 0.46 0.61? 0.6? 0.41 0.56? 0.39 0.28
UEDIN 0.39 0.37 0.52 0.30 0.50 0.61? 0.58 0.39 0.46 0.24 0.44
UPC 0.26 0.36 0.47 0.35 0.40 0.59? 0.32 0.42 0.46 0.33 0.41
> OTHERS 0.29 0.28 0.43 0.34 0.45 0.55 0.39 0.40 0.42 0.29 0.34 0.39
? OTHERS 0.57 0.56 0.67 0.58 0.67 0.77 0.58 0.61 0.67 0.54 0.56 0.60
Table 29: Sentence-level ranking for the Spanish-English News Task.
C
M
U
-S
M
T
C
U
E
D
D
C
U
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
CMU-SMT 0.36 0.38 0.37 0.10? 0.20? 0.14? 0.32 0.39 0.22 0.25 0.38
CUED 0.40 0.38 0.53 0.33 0.30 0.30 0.20? 0.32 0.08? 0.36 0.29
DCU 0.34 0.38 0.46 0.32 0.19? 0.26? 0.21? 0.32 0.33 0.25 0.46
LIMSI 0.31 0.30 0.21 0.05? 0.09? 0.15? 0.18? 0.24 0.10? 0.19 0.48
RBMT3 0.83? 0.62 0.58 0.73? 0.56 0.25 0.37 0.60? 0.31 0.66? 0.78?
RBMT4 0.73? 0.54 0.76? 0.74? 0.28 0.38 0.24 0.53 0.29 0.56 0.65?
RBMT5 0.79? 0.55 0.67? 0.75? 0.58 0.57 0.59? 0.70? 0.44 0.71? 0.67
RBMT6 0.52 0.77? 0.66? 0.68? 0.42 0.49 0.18? 0.55 0.41 0.54 0.71
SAAR 0.43 0.42 0.41 0.47 0.20? 0.32 0.17? 0.30 0.22? 0.35 0.32
UCL 0.56 0.71? 0.56 0.70? 0.42 0.57 0.33 0.44 0.59? 0.81? 0.67
UEDIN 0.28 0.46 0.39 0.31 0.29? 0.42 0.25? 0.39 0.35 0.15? 0.40
UPC 0.44 0.39 0.43 0.36 0.07? 0.23? 0.24 0.29 0.27 0.20 0.40
> OTHERS 0.50 0.5 0.49 0.53 0.28 0.36 0.24 0.32 0.44 0.26 0.45 0.51
? OTHERS 0.71 0.68 0.68 0.78 0.43 0.49 0.35 0.47 0.67 0.43 0.66 0.69
Table 30: Sentence-level ranking for the Spanish-English Europarl Task.
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.39 0.57 0.52? 0.62? 0.56? 0.50 0.41 0.42 0.56?
LIMSI 0.42 0.56 0.53 0.63? 0.58 0.32 0.39 0.35 0.35
RBMT3 0.23 0.3 0.34 0.46 0.50 0.39 0.17 0.21? 0.06?
RBMT4 0.25? 0.30 0.47 0.31 0.35 0.38 0.36 0.32 0.19
RBMT5 0.21? 0.20? 0.28 0.42 0.42 0.29? 0.24 0.17? 0.23
RBMT6 0.23? 0.23 0.31 0.41 0.42 0.23? 0.19 0.24? 0.24
SAAR 0.36 0.52 0.39 0.43 0.67? 0.54? 0.36 0.29 0.42
UCB 0.37 0.39 0.52 0.39 0.49 0.52 0.46 0.27 0.25
UEDIN 0.35 0.48 0.62? 0.48 0.64? 0.61? 0.50 0.47 0.53?
UPC 0.11? 0.41 0.63? 0.48 0.50 0.57 0.42 0.63 0.06?
> OTHERS 0.28 0.36 0.47 0.45 0.52 0.51 0.38 0.34 0.27 0.33
? OTHERS 0.49 0.54 0.68 0.67 0.72 0.72 0.55 0.59 0.48 0.60
Table 31: Sentence-level ranking for the English-Spanish News Task.
98
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
U
W
CMU-SMT 0.28 0.47 0.33 0.17? 0.26 0.50 0.25 0.48? 0.44 0.28
LIMSI 0.38 0.19? 0.33 0.16? 0.23 0.33 0.14? 0.14 0.35 0.32
RBMT3 0.42 0.62? 0.42 0.36 0.29 0.54 0.28 0.39 0.50 0.75?
RBMT4 0.46 0.47 0.42 0.19 0.31 0.61 0.50 0.40 0.50 0.57
RBMT5 0.70? 0.64? 0.59 0.48 0.35 0.65? 0.52 0.64 0.61 0.63?
RBMT6 0.63 0.58 0.47 0.56 0.50 0.78? 0.32 0.58 0.33 0.71?
SAAR 0.33 0.40 0.33 0.30 0.23? 0.19? 0.20 0.27 0.24 0.33
UCL 0.46 0.64? 0.41 0.46 0.36 0.41 0.60 0.65? 0.42 0.57?
UEDIN 0.09? 0.29 0.48 0.45 0.28 0.27 0.41 0.19? 0.25 0.17
UPC 0.22 0.40 0.50 0.43 0.28 0.40 0.52 0.26 0.56 0.58
UW 0.44 0.32 0.06? 0.29 0.17? 0.21? 0.33 0.14? 0.33 0.33
> OTHERS 0.43 0.46 0.4 0.4 0.26 0.28 0.53 0.28 0.46 0.4 0.49
? OTHERS 0.67 0.74 0.55 0.56 0.41 0.44 0.72 0.50 0.71 0.59 0.74
Table 32: Sentence-level ranking for the English-Spanish Europarl Task.
DCU UEDIN UMD
DCU 0.26? 0.4
UEDIN 0.37? 0.46?
UMD 0.4 0.31?
> OTHERS 0.38 0.28 0.43
? OTHERS 0.68 0.58 0.65
Table 33: Sentence-level ranking for the Czech-English News Task.
DCU SYSTRAN UEDIN UMD
DCU 0.21? 0.19? 0.37
SYSTRAN 0.59? 0.47? 0.61?
UEDIN 0.42? 0.27? 0.50?
UMD 0.38 0.18? 0.29?
> OTHERS 0.46 0.22 0.31 0.49
? OTHERS 0.75 0.45 0.60 0.72
Table 34: Sentence-level ranking for the Czech-English Commentary Task.
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.32? 0.51? 0.27?
CU-TECTOMT 0.52? 0.58? 0.42
PC-TRANSLATOR 0.35? 0.25? 0.26?
UEDIN 0.5? 0.40 0.59?
> OTHERS 0.45 0.32 0.56 0.32
? OTHERS 0.63 0.49 0.72 0.50
Table 35: Sentence-level ranking for the English-Czech News Task.
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.28? 0.38 0.19?
CU-TECTOMT 0.58? 0.53? 0.43
PC-TRANSLATOR 0.45 0.3? 0.26?
UEDIN 0.60? 0.37 0.56?
> OTHERS 0.54 0.32 0.49 0.29
? OTHERS 0.71 0.49 0.66 0.49
Table 36: Sentence-level ranking for the English-Czech Commentary Task.
99
MLOGIC UEDIN
MORPHOLOGIC 0.15?
UEDIN 0.68?
> OTHERS 0.68 0.15
? OTHERS 0.85 0.32
Table 37: Sentence-level ranking for the Hungarian-English News Task.
C
M
U
-X
F
R
C
U
E
D
C
U
E
D
-C
L
IM
S
I
L
IU
M
-S
Y
S
L
IU
M
-S
Y
S
-C
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
CMU-XFR 0.37 0.49? 0.62? 0.57? 0.61? 0.49 0.49 0.48? 0.41 0.56? 0.39 0.46?
CUED 0.28 0.21 0.30 0.30 0.13 0.28 0.18 0.27 0.28 0.31 0.34 0.18
CUED-C 0.2? 0.11 0.30? 0.19 0.33 0.18? 0.21 0.24 0.2? 0.2? 0.17? 0.24
LIMSI 0.13? 0.20 0.13? 0.27 0.22 0.23 0.24 0.2 0.20? 0.16? 0.23 0.22
LIUM-SYS 0.18? 0.17 0.27 0.17 0.20 0.18? 0.41 0.29 0.24 0.26 0.22 0.26
LI-SYS-C 0.18? 0.28 0.24 0.25 0.07 0.33 0.2? 0.27 0.18? 0.23 0.25 0.19
RBMT3 0.28 0.34 0.52? 0.28 0.40? 0.37 0.27 0.46? 0.27 0.30 0.39 0.34
RBMT4 0.29 0.40 0.34 0.31 0.39 0.43? 0.33 0.34 0.34 0.27 0.41 0.31
RBMT5 0.22? 0.24 0.34 0.3 0.27 0.43 0.14? 0.24 0.13? 0.32 0.32 0.32
RBMT6 0.3 0.41 0.50? 0.39? 0.33 0.58? 0.3 0.33 0.37? 0.33 0.52? 0.37
SAAR 0.27? 0.33 0.43? 0.37? 0.4 0.42 0.41 0.36 0.32 0.41 0.23 0.41
SAAR-C 0.28 0.32 0.38? 0.27 0.27 0.45 0.23 0.21 0.20 0.23? 0.18 0.19
UED 0.19? 0.15 0.20 0.25 0.29 0.19 0.28 0.27 0.19 0.24 0.21 0.26
> OTHERS 0.24 0.27 0.33 0.32 0.32 0.37 0.29 0.28 0.30 0.27 0.29 0.31 0.29
? OTHERS 0.51 0.75 0.79 0.80 0.77 0.78 0.65 0.66 0.73 0.62 0.64 0.74 0.77
Table 38: Constituent ranking for the French-English News Task
C
M
U
-X
F
R
C
U
E
D
D
C
U
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
S
Y
S
T
R
A
N
U
C
L
U
E
D
IN
CMU-XFR 0.42? 0.4? 0.37? 0.54? 0.16? 0.21 0.41 0.23 0.49? 0.42? 0.34 0.45 0.50?
CUED 0.03? 0.13 0.08 0.14 0.13? 0.13? 0.08? 0.05? 0.08 0.04 0.15 0.11 0.07
DCU 0.09? 0.08 0.10 0.12 0.06? 0.20 0.31 0.16? 0.14 0.22 0.13 0.10 0.16
LIMSI 0.1? 0.05 0.19 0.05 0.04? 0.08? 0.19 0.11? 0.18 0.09 0.05? 0.05?
LIUM-SYS 0.03? 0.14 0.19 0.07 0 0.08? 0.03? 0.05? 0.03? 0.09 0.15 0.14 0.08
RBMT3 0.44? 0.61? 0.50? 0.58? 0.56? 0.41? 0.38 0.32 0.37 0.53? 0.44 0.50? 0.58?
RBMT4 0.39 0.44? 0.43 0.45? 0.35? 0.12? 0.31 0.23 0.42 0.39 0.33 0.32 0.35
RBMT5 0.19 0.47? 0.29 0.35 0.37? 0.18 0.17 0.23 0.35 0.33 0.19 0.46 0.40
RBMT6 0.36 0.65? 0.54? 0.48? 0.55? 0.26 0.40 0.50 0.50? 0.52? 0.47? 0.60? 0.44
SAAR 0.07? 0.25 0.24 0.18 0.37? 0.23 0.36 0.23 0.12? 0.12 0.23 0.13 0.37?
SAAR-C 0.09? 0.18 0.12 0.16 0.16 0.09? 0.18 0.2 0.06? 0.12 0.09 0.14 0.15
SYSTRAN 0.34 0.40 0.21 0.38? 0.23 0.25 0.36 0.22 0.15? 0.23 0.28 0.31 0.30?
UCL 0.25 0.34 0.28 0.31? 0.19 0.11? 0.24 0.23 0.11? 0.24 0.31 0.34 0.37?
UED 0.10? 0.10 0.16 0.05 0.08 0.03? 0.15 0.14 0.18 0.07? 0.13 0.07? 0.11?
> OTHERS 0.2 0.32 0.27 0.28 0.28 0.12 0.22 0.25 0.15 0.26 0.27 0.22 0.25 0.28
? OTHERS 0.63 0.91 0.85 0.91 0.92 0.52 0.65 0.7 0.52 0.78 0.87 0.71 0.74 0.89
Table 39: Constituent ranking for the French-English Europarl Task
100
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
X
E
R
O
X
LIMSI 0.27 0.43 0.43 0.29 0.53? 0.32 0.37 0.30 0.14?
LIUM-SYSTRAN 0.09 0.33 0.36 0.18 0.35 0.16? 0.25 0.22 0.13?
RBMT3 0.36 0.33 0.22 0.31 0.28 0.4 0.26 0.26? 0.20?
RBMT4 0.25 0.26 0.30 0.23 0.16? 0.28 0.26 0.24 0.13?
RBMT5 0.31 0.33 0.22 0.28 0.17 0.27 0.25 0.23 0.13?
RBMT6 0.26? 0.30 0.31 0.38? 0.32 0.33 0.36 0.39 0.25?
SAAR 0.32 0.41? 0.35 0.38 0.32 0.28 0.14 0.23 0.11?
SAAR-CONTRAST 0.25 0.26 0.36 0.30 0.33 0.36 0.05 0.22 0.13?
UEDIN 0.29 0.34 0.45? 0.4 0.33 0.40 0.31 0.35 0.13?
XEROX 0.66? 0.55? 0.61? 0.65? 0.58? 0.51? 0.53? 0.57? 0.45?
> OTHERS 0.31 0.34 0.38 0.38 0.33 0.33 0.3 0.31 0.29 0.15
? OTHERS 0.65 0.76 0.72 0.77 0.76 0.67 0.73 0.75 0.66 0.44
Table 40: Constituent ranking for the English-French News Task
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
LIMSI 0.14 0.09? 0.10? 0.24 0.11? 0.13 0.08? 0.12
LIUM-SYSTRAN 0.19? 0.19? 0.15 0.12? 0.06 0.06? 0.09
RBMT3 0.65? 0.59? 0.33 0.43 0.32 0.50? 0.39 0.46?
RBMT4 0.53? 0.47? 0.19 0.27 0.18? 0.33 0.38 0.39
RBMT5 0.48 0.38 0.32 0.48 0.47 0.55? 0.44 0.51?
RBMT6 0.54? 0.49? 0.32 0.41? 0.26 0.52? 0.45 0.58?
SAAR 0.21 0.17 0.23? 0.25 0.21? 0.17? 0.19 0.13
UCL 0.37? 0.33? 0.38 0.35 0.36 0.32 0.34 0.31?
UEDIN 0.12 0.11 0.17? 0.23 0.13? 0.13? 0.07 0.07?
> OTHERS 0.38 0.36 0.25 0.30 0.26 0.24 0.33 0.27 0.34
? OTHERS 0.88 0.88 0.56 0.68 0.55 0.56 0.81 0.66 0.87
Table 41: Constituent ranking for the English-French Europarl Task
C
M
U
-X
F
E
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
CMU-STATXFER 0.47? 0.44 0.52? 0.53? 0.57? 0.49? 0.41 0.49 0.58? 0.49?
LIMSI 0.17? 0.18 0.35 0.34 0.40 0.33 0.43 0.19 0.28 0.19
LIU 0.25 0.3 0.37 0.35 0.44 0.28 0.40 0.21 0.33 0.32?
RBMT2 0.19? 0.26 0.30 0.19 0.32 0.16? 0.20 0.26 0.23 0.21
RBMT3 0.22? 0.36 0.26 0.23 0.24 0.23 0.14? 0.15 0.28 0.29
RBMT4 0.20? 0.35 0.23 0.21 0.24 0.22 0.19? 0.36 0.32 0.31
RBMT5 0.26? 0.28 0.38 0.34? 0.31 0.35 0.26 0.3 0.43? 0.35
RBMT6 0.38 0.37 0.39 0.34 0.44? 0.4? 0.30 0.28 0.26 0.38
SAAR 0.29 0.22 0.37 0.29 0.10 0.28 0.19 0.22 0.26 0.18
SAAR-CONTRAST 0.18? 0.33 0.29 0.19 0.22 0.24 0.15? 0.26 0.18 0.23
UEDIN 0.11? 0.3 0.13? 0.23 0.35 0.3 0.2 0.37 0.30 0.31
> OTHERS 0.22 0.33 0.3 0.31 0.32 0.35 0.25 0.29 0.28 0.33 0.30
? OTHERS 0.50 0.72 0.67 0.77 0.76 0.74 0.67 0.64 0.76 0.78 0.74
Table 42: Constituent ranking for the German-English News Task
101
C
M
U
-X
F
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-STATXFER 0.51? 0.51? 0.38 0.38 0.41 0.37 0.44 0.48? 0.39 0.6?
LIMSI 0.18? 0.22 0.3 0.30 0.23 0.22? 0.32 0.27 0.18? 0.29
LIU 0.14? 0.22 0.26? 0.32 0.22? 0.16? 0.31 0.20 0.08? 0.12
RBMT2 0.38 0.51 0.52? 0.40 0.32 0.25 0.31 0.51 0.40 0.7?
RBMT3 0.32 0.42 0.45 0.28 0.46 0.16 0.20? 0.56? 0.38 0.43
RBMT4 0.32 0.45 0.52? 0.31 0.24 0.13? 0.30 0.49? 0.44 0.48?
RBMT5 0.44 0.57? 0.53? 0.34 0.31 0.43? 0.19 0.54? 0.39 0.54?
RBMT6 0.33 0.51 0.48 0.33 0.47? 0.33 0.33 0.47? 0.42 0.51?
SAAR 0.12? 0.1 0.15 0.26 0.09? 0.19? 0.17? 0.23? 0.11? 0.14
UCL 0.30 0.43? 0.49? 0.40 0.40 0.30 0.41 0.39 0.38? 0.51?
UEDIN 0.11? 0.16 0.12 0.18? 0.25 0.2? 0.18? 0.23? 0.14 0.12?
> OTHERS 0.27 0.40 0.41 0.31 0.32 0.32 0.25 0.3 0.41 0.30 0.44
? OTHERS 0.55 0.75 0.8 0.58 0.64 0.64 0.58 0.59 0.84 0.60 0.83
Table 43: Constituent ranking for the German-English Europarl Task
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
E
D
IN
LIMSI 0.29 0.46 0.45 0.37 0.36 0.29? 0.33 0.22
LIU 0.32 0.53? 0.45? 0.51? 0.5? 0.38 0.31 0.36
RBMT2 0.33 0.32? 0.29 0.29 0.20? 0.25? 0.28 0.28?
RBMT3 0.34 0.3? 0.4 0.33 0.3? 0.34 0.20? 0.27?
RBMT4 0.26 0.25? 0.31 0.3 0.23? 0.23? 0.20? 0.21?
RBMT5 0.46 0.33? 0.55? 0.46? 0.40? 0.32 0.32 0.29?
RBMT6 0.52? 0.40 0.47? 0.44 0.53? 0.40 0.27 0.37
SAAR 0.38 0.3 0.39 0.42? 0.44? 0.40 0.44 0.34
UEDIN 0.30 0.24 0.53? 0.52? 0.51? 0.56? 0.45 0.36
> OTHERS 0.36 0.31 0.46 0.41 0.42 0.37 0.33 0.28 0.29
? OTHERS 0.65 0.57 0.72 0.68 0.75 0.60 0.56 0.61 0.56
Table 44: Constituent ranking for the English-German News Task
C
M
U
-G
IM
P
E
L
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-GIMPEL 0.12 0.27 0.21? 0.30 0.21? 0.27? 0.21? 0.22 0.22 0.23
LIMSI 0.22 0.22 0.34 0.29? 0.29? 0.23? 0.29? 0.2 0.21 0.19
LIU 0.18 0.2 0.20? 0.25? 0.17? 0.16? 0.12? 0.28 0.21 0.18
RBMT2 0.54? 0.41 0.62? 0.28 0.33 0.35 0.28 0.61? 0.43 0.47?
RBMT3 0.47 0.47? 0.47? 0.4 0.33 0.32 0.28 0.56? 0.47 0.48?
RBMT4 0.52? 0.57? 0.52? 0.42 0.32 0.27? 0.28 0.47 0.45 0.39
RBMT5 0.49? 0.57? 0.65? 0.42 0.38 0.48? 0.31 0.76? 0.51 0.52?
RBMT6 0.51? 0.54? 0.60? 0.41 0.39 0.40 0.41 0.51? 0.53? 0.51?
SAAR 0.24 0.29 0.17 0.26? 0.22? 0.25 0.20? 0.21? 0.31 0.12
UCL 0.28 0.32 0.29 0.33 0.38 0.32 0.32 0.29? 0.19 0.30
UEDIN 0.1 0.13 0.22 0.2? 0.18? 0.22 0.21? 0.18? 0.15 0.17
> OTHERS 0.37 0.37 0.42 0.32 0.30 0.31 0.28 0.25 0.39 0.35 0.35
? OTHERS 0.77 0.75 0.81 0.58 0.59 0.58 0.51 0.52 0.77 0.69 0.82
Table 45: Constituent ranking for the English-German Europarl Task
102
C
M
U
-S
M
T
C
U
E
D
C
U
E
D
-C
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.19 0.17 0.26 0.38 0.27 0.45 0.32 0.35 0.27 0.26 0.2
CUED 0.21 0.21 0.24 0.24 0.2 0.34 0.25 0.27 0.18 0.26 0.21
CUED-CONTRAST 0.17 0.08 0.12 0.24 0.23? 0.27 0.25 0.21 0.12 0.11 0.26
LIMSI 0.17 0.25 0.26 0.34 0.18? 0.33 0.33 0.31 0.17 0.26 0.23
RBMT3 0.29 0.31 0.35 0.37 0.21 0.4 0.31 0.32 0.43 0.42 0.52?
RBMT4 0.38 0.34 0.54? 0.47? 0.35 0.24 0.32 0.46? 0.37 0.40 0.53
RBMT5 0.24 0.31 0.40 0.33 0.25 0.18 0.31 0.33 0.32 0.28 0.38
RBMT6 0.33 0.29 0.28 0.33 0.26 0.27 0.16 0.26 0.3 0.39 0.41
SAAR 0.26 0.27 0.33 0.26 0.21 0.12? 0.25 0.24 0.20 0.28 0.20
UCB 0.25 0.30 0.23 0.27 0.31 0.27 0.40 0.34 0.28 0.32 0.26
UEDIN 0.19 0.20 0.19 0.24 0.27 0.33 0.31 0.27 0.21 0.21 0.25
UPC 0.1 0.21 0.17 0.2 0.22? 0.28 0.4 0.24 0.29 0.30 0.2
> OTHERS 0.24 0.25 0.28 0.28 0.28 0.23 0.33 0.29 0.3 0.26 0.3 0.32
? OTHERS 0.72 0.76 0.82 0.74 0.64 0.61 0.7 0.70 0.76 0.71 0.76 0.76
Table 46: Constituent ranking for the Spanish-English News Task
C
M
U
-S
M
T
C
U
E
D
D
C
U
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
CMU-SMT 0.2 0.20 0.1 0.1? 0.18? 0.04? 0.18? 0.16 0.17 0.19 0.19
CUED 0.18 0.13 0.19 0.14? 0.12? 0.1? 0.2? 0.13 0.12? 0.22 0.12
DCU 0.15 0.13 0.11 0.09? 0.10? 0.13? 0.09? 0.19 0.15? 0.14 0.15
LIMSI 0.03 0.15 0.16 0.19? 0.18? 0.15? 0.19? 0.19 0.08? 0.07 0.22
RBMT3 0.7? 0.73? 0.59? 0.49? 0.19 0.36 0.22 0.62? 0.55? 0.68? 0.73?
RBMT4 0.55? 0.62? 0.51? 0.55? 0.23 0.22 0.17 0.56? 0.43 0.56? 0.44?
RBMT5 0.60? 0.61? 0.53? 0.61? 0.32 0.38 0.28 0.63? 0.53 0.7? 0.59?
RBMT6 0.52? 0.48? 0.51? 0.49? 0.23 0.26 0.19 0.49? 0.53? 0.52? 0.50?
SAAR 0.14 0.10 0.12 0.15 0.10? 0.12? 0.05? 0.07? 0.14? 0.05 0.18
UCL 0.38 0.37? 0.46? 0.45? 0.28? 0.32 0.29 0.24? 0.38? 0.38? 0.36
UEDIN 0.06 0.14 0.14 0.18 0.15? 0.16? 0.05? 0.16? 0.15 0.10? 0.21
UPC 0.19 0.12 0.20 0.12 0.07? 0.17? 0.09? 0.14? 0.04 0.17 0.14
> OTHERS 0.32 0.33 0.32 0.32 0.17 0.2 0.15 0.17 0.33 0.28 0.34 0.35
? OTHERS 0.85 0.85 0.87 0.85 0.46 0.56 0.47 0.57 0.89 0.65 0.87 0.87
Table 47: Constituent ranking for the Spanish-English Europarl Task
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.20 0.36 0.37 0.24? 0.36 0.32 0.21 0.17 0.27
LIMSI 0.23 0.4 0.46? 0.33 0.39 0.31 0.23 0.17 0.18
RBMT3 0.33 0.35 0.22 0.19? 0.3 0.31 0.49 0.34 0.22
RBMT4 0.30 0.25? 0.25 0.17? 0.17? 0.24 0.19? 0.34 0.30
RBMT5 0.53? 0.42 0.50? 0.41? 0.35 0.50? 0.44 0.37 0.29
RBMT6 0.36 0.35 0.34 0.39? 0.32 0.35 0.36 0.37 0.38
SAAR 0.33 0.36 0.38 0.28 0.24? 0.38 0.29 0.22? 0.24
UCB 0.32 0.29 0.35 0.54? 0.33 0.45 0.31 0.19 0.29
UEDIN 0.29 0.33 0.36 0.42 0.42 0.39 0.45? 0.30 0.44
UPC 0.36 0.42 0.50 0.49 0.42 0.44 0.51 0.21 0.26
> OTHERS 0.34 0.33 0.38 0.39 0.29 0.35 0.36 0.31 0.27 0.29
? OTHERS 0.72 0.69 0.69 0.75 0.57 0.64 0.7 0.65 0.63 0.6
Table 48: Constituent ranking for the English-Spanish News Task
103
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
U
W
CMU-SMT 0.13 0.10? 0.21? 0.2? 0.2? 0.26 0.22 0.13 0.16 0.14
LIMSI 0.17 0.24 0.16? 0.20? 0.13? 0.21 0.06? 0.09 0.14 0.08
RBMT3 0.64? 0.45 0.24 0.30 0.21 0.57? 0.56 0.58? 0.32 0.58?
RBMT4 0.54? 0.52? 0.42 0.26 0.24 0.50? 0.35 0.43 0.47 0.44
RBMT5 0.61? 0.68? 0.46 0.44 0.37 0.64? 0.50 0.63? 0.62? 0.54
RBMT6 0.57? 0.48? 0.39 0.33 0.25 0.52? 0.33 0.54? 0.46 0.46
SAAR 0.19 0.14 0.07? 0.19? 0.09? 0.14? 0.13? 0.17 0.26 0.18
UCL 0.43 0.46? 0.29 0.37 0.38 0.42 0.49? 0.37? 0.48 0.40
UEDIN 0.15 0.11 0.24? 0.20 0.13? 0.17? 0.30 0.14? 0.20 0.20
UPC 0.26 0.05 0.35 0.25 0.16? 0.23 0.34 0.21 0.23 0.10
UW 0.14 0.14 0.17? 0.22 0.23 0.2 0.32 0.20 0.20 0.35
> OTHERS 0.37 0.32 0.28 0.26 0.22 0.23 0.42 0.27 0.35 0.35 0.33
? OTHERS 0.83 0.86 0.56 0.59 0.46 0.57 0.85 0.59 0.82 0.78 0.79
Table 49: Constituent ranking for the English-Spanish Europarl Task
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.33 0.41 0.28?
CU-TECTOMT 0.37 0.42? 0.36
PC-TRANSLATOR 0.34 0.31? 0.32?
UEDIN 0.37? 0.37 0.43?
> OTHERS 0.36 0.34 0.42 0.32
? OTHERS 0.66 0.62 0.67 0.61
Table 50: Constituent ranking for the English-Czech News Task
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.25? 0.33? 0.22?
CU-TECTOMT 0.50? 0.44? 0.45
PC-TRANSLATOR 0.47? 0.3? 0.40
UEDIN 0.39? 0.37 0.39
> OTHERS 0.45 0.31 0.39 0.36
? OTHERS 0.73 0.54 0.61 0.61
Table 51: Constituent ranking for the English-Czech Commentary Task
104
French?English English?French
Europarl YES NO
CMU-XFR 0.61 0.39
CUED 0.83 0.17
DCU 0.88 0.12
LIMSI 0.89 0.11
LIUM-SYS 0.89 0.11
RBMT3 0.54 0.47
RBMT4 0.62 0.38
RBMT5 0.71 0.29
RBMT6 0.54 0.46
SAAR 0.72 0.28
SAAR-C 0.86 0.14
SYSTRAN 0.81 0.19
UCL 0.73 0.27
UEDIN 0.91 0.09
News YES NO
CMU-XFR 0.55 0.45
CUED 0.74 0.26
CUED-C 0.79 0.21
LIMSI 0.81 0.2
LIUM-SYS 0.79 0.21
LI-SYS-C 0.7 0.30
RBMT3 0.63 0.37
RBMT4 0.64 0.36
RBMT5 0.76 0.24
RBMT6 0.66 0.34
SAAR 0.64 0.36
SAAR-C 0.70 0.3
UEDIN 0.72 0.28
Europarl YES NO
LIMSI 0.75 0.26
LIUM-SYS 0.84 0.16
RBMT3 0.49 0.51
RBMT4 0.50 0.5
RBMT5 0.44 0.56
RBMT6 0.35 0.65
SAAR 0.70 0.3
UCL 0.6 0.40
UEDIN 0.75 0.25
News YES NO
LIMSI 0.73 0.27
LIUM-SYS 0.75 0.25
RBMT3 0.59 0.41
RBMT4 0.59 0.41
RBMT5 0.64 0.36
RBMT6 0.58 0.42
SAAR 0.59 0.41
SAAR-C 0.59 0.41
UEDIN 0.63 0.37
XEROX 0.30 0.7
German?English English?German
Europarl YES NO
CMU-XFER 0.53 0.47
LIMSI 0.80 0.2
LIU 0.83 0.17
RBMT2 0.76 0.24
RBMT3 0.74 0.26
RBMT4 0.67 0.33
RBMT5 0.63 0.37
RBMT6 0.63 0.37
SAAR 0.82 0.18
UCL 0.49 0.51
UEDIN 0.86 0.14
News YES NO
CMU-XFER 0.47 0.53
LIMSI 0.73 0.28
LIU 0.64 0.36
RBMT2 0.72 0.28
RBMT3 0.73 0.27
RBMT4 0.74 0.26
RBMT5 0.59 0.41
RBMT6 0.68 0.32
SAAR 0.67 0.33
SAAR-C 0.72 0.28
UEDIN 0.63 0.37
Europarl YES NO
CMU-GIMPEL 0.82? 0.18
LIMSI 0.79? 0.21
LIU 0.79? 0.21
RBMT2 0.69? 0.31
RBMT3 0.57 0.43
RBMT4 0.67? 0.34
RBMT5 0.45 0.55
RBMT6 0.47 0.53
SAAR 0.77? 0.23
UCL 0.61? 0.39
UEDIN 0.85? 0.15
News YES NO
LIMSI 0.56 0.44
LIU 0.49 0.51
RBMT2 0.69 0.31
RBMT3 0.69 0.31
RBMT4 0.75 0.25
RBMT5 0.55 0.45
RBMT6 0.6 0.40
SAAR 0.54 0.46
UEDIN 0.52 0.48
Spanish?English English?Spanish
Europarl YES NO
CMU-SMT 0.88 0.12
CUED 0.86 0.14
DCU 0.85 0.15
LIMSI 0.90 0.1
RBMT3 0.65 0.35
RBMT4 0.56 0.44
RBMT5 0.59 0.41
RBMT6 0.55 0.45
SAAR 0.87 0.13
UCL 0.73 0.27
UEDIN 0.88 0.12
UPC 0.86 0.14
News YES NO
CMU-SMT 0.64 0.37
CUED 0.64 0.36
CUED-C 0.69 0.31
LIMSI 0.68 0.33
RBMT3 0.61 0.39
RBMT4 0.65 0.35
RBMT5 0.59 0.41
RBMT6 0.64 0.37
SAAR 0.7 0.30
UCB 0.64 0.37
UEDIN 0.62 0.38
UPC 0.71 0.29
Europarl YES NO
CMU-SMT 0.80 0.2
LIMSI 0.87 0.13
RBMT3 0.58 0.42
RBMT4 0.6 0.40
RBMT5 0.64 0.37
RBMT6 0.60 0.40
SAAR 0.81 0.19
UCL 0.71 0.29
UEDIN 0.89 0.11
UPC 0.90 0.1
UW 0.79 0.22
News YES NO
CMU-SMT 0.46 0.54
LIMSI 0.53 0.47
RBMT3 0.64 0.36
RBMT4 0.76 0.24
RBMT5 0.6 0.40
RBMT6 0.62 0.38
SAAR 0.64 0.36
UCB 0.57 0.43
UEDIN 0.49 0.51
UPC 0.37 0.63
English?Czech
Commentary YES NO
CU-BOJAR 0.59 0.41
CU-TECTO 0.43 0.57
PC-TRANS 0.51 0.49
UEDIN 0.41 0.59
News YES NO
CU-BOJAR 0.54 0.46
CU-TECTO 0.42 0.58
PC-TRANS 0.52 0.48
UEDIN 0.44 0.56
Table 52: Yes/No Acceptability of Constituents
105
BBN-CMB-DE
BBN-CMB-FR
CMU-CMB-FR
CMU-SMT-ES
CMU-XFR-DE
CMU-XFR-FR
CUED-C-ES
CUED-C-FR
CUED-ES
CUED-FR
DCU-CZ
LIMSI-DE
LIMSI-ES
LIMSI-FR
LIU-DE
LIUM-S-C-FR
LIUM-SYS-FR
MLOGIC-HU
RBMT2-DE
RBMT3-DE
RBMT3-ES
RBMT3-FR
RBMT4-DE
RBMT4-ES
RBMT4-FR
RBMT5-DE
RBMT5-ES
RBMT5-FR
RBMT6-DE
RBMT6-ES
RBMT6-FR
SAAR-C-DE
SAAR-C-FR
SAAR-DE
SAAR-ES
SAAR-FR
UCB-ES
UED-CMB-DE
UED-CMB-FR
UED-CMB-XX
UED-CZ
UED-DE
UED-ES
UED-FR
UED-HU
UMD-CZ
UPC-ES
B
B
N
-C
M
B
-D
E
.50
.40
1
.20
.50
1
?
.64
1
.73
.31
.69
?
.71
.38
.70
.60
.60
.80
.77
?
.60
.63
.89
?
1
.57
.62
.83
.60
.17
.57
.55
.41
.70
.58
.71
.82
.75
.40
.33
1
.25
.36
.85
?
.50
.40
.60
B
B
N
-C
M
B
-F
R
.38
.14
.38
.09
?
.13
?
.33
.63
.20
.25
.13
.13
.60
.31
.46
.43
.27
.13
.67
.25
.46
.33
.38
.22
.43
.07
?
.33
.42
.50
.36
.25
.46
.40
.06
?
.30
.33
.50
.80
.14
?
.20
.67
.33
.25
.13
.42
C
M
U
-C
M
B
-F
R
.60
.71
.54
.09
?
.60
.29
.13
.57
.33
.23
.33
.33
.46
.44
.58
.40
.20
.54
.27
.50
.67
.11
.14
.44
.11
.25
.60
.09
?
.40
.29
.29
.25
.56
.20
.56
.25
.14
.38
.11
?
.11
.22
.36
.44
C
M
U
-S
M
T-E
S
.50
.31
.50
.17
.75
.46
.64
.43
.25
.54
.60
.83
?
.40
.50
.17
.14
.46
.50
.64
.73
.80
.67
.64
.33
.33
.67
.46
.50
.57
.50
.39
.36
.64
.70
.17
.50
.33
.14
.25
.33
.13
.38
.43
C
M
U
-X
F
R
-D
E
.60
.82
?
.91
?
.50
.78
.56
.89
?
.42
.73
?
.55
.27
.33
.88
?
.57
.73
?
.92
?
.75
.80
?
.82
.75
?
.67
.75
.86
.78
.91
?
.89
?
.79
?
.81
?
.80
.80
?
.67
.90
?
.64
.73
?
.80
.64
.33
1
.83
.11
.20
1
?
.90
?
.33
.50
.85
?
C
M
U
-X
F
R
-F
R
.50
.75
?
.67
.11
.70
.80
?
.88
?
.71
.50
.75
.50
.60
.71
.67
.50
.67
.60
.40
.43
.60
.67
.29
.25
.64
.75
.38
.75
.38
.50
.67
.18
.57
.44
.73
.33
.50
.75
.80
.69
.64
.50
.33
1
C
U
E
D
-C
-E
S
0
.56
.59
.22
.20
.18
.21
.19
0
.29
.15
.47
.14
.39
.50
.25
.39
.36
.43
.46
.33
.31
.56
.50
.07
?
.73
?
.31
.42
.42
.43
.50
.42
.40
.27
.18
.50
.38
.29
.10
?
.22
.33
.43
.33
C
U
E
D
-C
-F
R
.29
.13
.38
.39
.11
?
.10
?
.73
.50
.25
.36
.57
.40
.36
.11
?
.70
?
.60
.40
.58
.36
.56
.20
.39
.50
.60
.10
?
.50
.50
.30
.55
.46
.33
0
.17
.20
.39
.13
.88
0
.29
.39
.36
0
.40
.50
C
U
E
D
-E
S
.80
.29
.18
.25
0
.29
.38
.64
.25
.20
.14
.78
.25
.36
.88
.25
.36
.39
.69
.71
.58
.83
?
.67
.30
.50
.60
.47
.67
.43
.40
.20
.38
.50
.50
.57
.25
.50
.50
.08
?
0
.57
.20
.50
.67
C
U
E
D
-F
R
.18
.25
.22
.43
.09
?
.29
.69
.38
.27
.11
.10
?
.47
.33
.64
.15
.50
.38
.57
.50
.42
.43
.33
.50
.22
.46
.46
.33
.58
.43
.50
.56
.18
.44
.25
.38
.20
.25
0
.33
.13
.44
0
.10
?
.50
D
C
U
-C
Z
.39
.75
.69
.67
.36
.50
.90
?
.46
.58
1
.44
.22
.91
?
.56
.60
.85
?
1
.77
?
.78
.86
.75
.62
.57
.83
?
.30
.55
.80
.67
.77
?
.80
?
.79
?
.50
.33
.80
?
.89
?
.73
.17
.50
.60
.50
.54
.78
.80
.13
.38
.39
L
IM
S
I-D
E
.17
.63
.67
.39
.27
1
.71
.43
.80
.78
.38
.33
.57
.50
.77
?
1
?
.29
.50
.78
.50
.67
.71
.88
.71
?
.33
.57
.89
?
.30
.60
.80
.43
.78
?
.27
.36
.17
.44
1
1
.13
.50
.67
.50
.40
.30
.43
L
IM
S
I-E
S
.08
?
.40
.67
.30
.17
.25
.54
.60
.57
.80
?
.56
.50
.50
.43
.55
.50
.33
.43
.10
?
.67
.50
.63
.39
.69
.29
.75
.50
.29
.60
.82
.63
.20
.22
.55
.33
.29
.25
1
.75
.17
.25
.57
.50
.64
.50
L
IM
S
I-F
R
.14
.38
.18
.08
?
0
.40
.27
.36
.11
.35
.09
?
.29
.25
.23
.63
.30
.25
.38
.56
.36
.44
.22
.25
.50
.10
?
.31
.20
.20
.56
.40
.17
.20
.38
.50
.36
.33
1
.33
.50
.25
.08
?
.27
.24
.25
.29
.50
L
IU
-D
E
.50
.55
.33
.60
.40
.86
.89
?
.38
.44
.22
.25
.57
.54
.73
?
.80
1
.22
.55
.86
.83
.67
.67
.67
.25
.89
?
.71
.50
.60
.58
.60
.60
.60
.75
.71
.67
.33
1
1
.22
.22
.43
.67
.40
.69
L
IU
M
-S-C
-F
R
.20
.29
.17
.50
0
.14
.46
0
.43
.21
.20
.08
?
.18
.13
.09
?
.25
.11
?
.18
.39
.50
.27
.27
.46
.50
.13
?
.31
.55
.33
.46
.50
.42
.25
.33
.59
?
.33
.33
.33
.50
.25
.22
.18
0
.44
.60
L
IU
M
-S-F
R
.20
.36
.20
.67
.08
?
.11
.50
.20
.13
.62
.15
?
0
.38
.50
.20
.25
.14
.42
.36
.17
.43
.13
.60
.30
.25
.33
.52
.25
.18
.43
.39
.29
.20
.44
.16
?
.44
.50
.60
.08
?
.17
?
.23
.17
0
.16
?
.46
M
L
O
G
IC
-H
U
.40
.75
.60
.71
.25
.50
.63
.60
.75
.50
.43
.67
.50
.89
?
.71
.88
.67
.44
.86
?
1
?
.50
.75
.67
.83
.63
.63
.54
.63
.67
.50
.86
.33
.63
.33
.75
1
.40
1
1
.44
.25
.80
R
B
M
T2-D
E
.33
.39
.46
.07
?
.33
.46
.33
.64
.38
.08
?
.50
.36
.50
.33
.55
.58
.13
.17
.67
.38
.38
.70
.55
.22
.46
.46
.46
.43
.17
.10
.42
.43
.67
.29
.33
.40
.40
1
.10
?
.31
.54
.36
.14
.07
?
.56
R
B
M
T3-D
E
.08
?
.75
.64
.50
.18
.20
.64
.64
.31
.43
.22
.11
.80
?
.44
.36
.62
.64
.33
.67
.55
.46
.35
.90
?
.40
.14
.80
?
.40
.38
.38
.60
.25
.53
.44
.31
.56
.63
.17
.80
.60
.22
.55
.60
.17
.20
.67
R
B
M
T3-E
S
.40
.55
.50
.18
.13
?
.50
.36
.22
.23
.50
.14
.42
.33
.50
.14
.50
.83
.44
.33
.27
.39
.64
.50
.50
.36
.33
.31
.27
.46
.33
.09
.43
.23
.50
.46
.29
.75
.75
.25
.25
.36
.50
0
.20
.78
R
B
M
T3-F
R
.25
.58
.22
.27
.33
.29
.27
.40
.29
.33
.13
.33
.43
.50
.17
.55
.50
0
.56
.31
.62
.75
.63
.33
.13
.44
.38
.27
.60
.09
?
.57
.63
.17
.50
.38
.40
1
.11
.25
.73
.50
.25
.53
R
B
M
T4-D
E
.11
?
.50
.78
.20
.40
.67
.62
.25
.14
.15
.29
.13
.78
.25
.73
.75
.50
.55
.18
.22
.43
.50
.57
.29
.25
.50
.63
.42
.33
.25
.67
.58
.50
.60
.67
.14
.38
.50
.75
0
.22
.63
R
B
M
T4-E
S
.44
.57
.22
.14
.17
.38
.38
.08
?
.56
.14
.13
.31
.50
.33
.46
.30
0
.10
.10
?
.50
.38
.56
.29
.43
.25
.33
.38
.25
.25
.46
.33
.14
.09
.25
.33
.11
1
1
0
.09
?
.39
.50
0
.38
.38
R
B
M
T4-F
R
.43
.29
.22
.27
.11
.57
.22
.27
.33
.33
.08
?
.14
?
.23
.33
.22
.25
.60
.50
.46
.30
.50
.47
.57
.41
.67
.46
.58
.54
.43
.38
.36
.63
.14
.86
.36
.25
.60
.39
.50
.50
.33
.20
.38
R
B
M
T5-D
E
.23
.71
?
.67
.50
.09
?
.50
.38
.80
?
.40
.56
.60
.44
.57
.80
?
.63
.75
?
.75
.25
.67
.43
.83
.75
.29
.57
.17
.50
1
?
.13
.67
.53
.40
.71
.20
.67
.47
.38
.86
.25
.27
.25
.70
.67
.20
.38
.50
R
B
M
T5-E
S
.17
.67
.75
.27
.11
?
.27
.73
?
.50
.13
.46
.27
.43
.62
.11
?
.62
.50
.22
.36
.10
?
.43
.44
.43
.63
.46
.36
.50
.29
.44
.17
.57
.27
.29
.25
.60
.11
.33
.60
.67
.44
0
.30
.43
.17
.25
.58
R
B
M
T5-F
R
.30
.42
.10
.11
.14
?
.17
.09
?
.42
.30
.39
.20
.11
?
.33
.50
.21
.27
.29
.15
.40
.67
.38
.71
.33
.17
0
.40
.50
.40
.20
.25
.56
.07
?
.50
.31
.14
.50
.22
.40
.57
.54
.29
R
B
M
T6-D
E
.67
.25
.91
?
.36
.06
?
.50
.54
.70
.47
.53
.33
.50
.57
.70
.40
.67
.58
.25
.39
.38
.69
.73
.50
.50
.46
.50
.43
.50
.13
.50
.46
.62
.50
.46
.50
.46
.40
.33
.80
.29
.20
.57
.50
.29
.86
R
B
M
T6-E
S
.29
.55
.60
.50
.25
.25
.46
.22
.33
.08
?
.40
.20
.44
.55
.64
.38
.29
.63
.40
.30
.30
.25
.57
.33
.22
.60
.63
.67
.64
.42
.38
.67
.71
.46
0
.50
.22
.25
.50
.33
0
.13
.67
R
B
M
T6-F
R
.36
.63
.57
.43
.20
?
.63
.50
.83
.43
.36
.10
?
.18
.40
.17
.43
.50
.39
.67
.30
.39
.73
?
.38
.63
.38
.27
.83
.50
.38
.17
.29
.67
.22
.40
.33
.38
.33
.50
.13
.25
.63
.33
.14
.25
S
A
A
R
-C
-D
E
.41
.55
.67
.30
.25
.50
.50
.54
.60
.40
.14
?
.57
.25
.83
.30
.50
.62
.25
.70
.50
.47
.36
.50
.46
.55
.33
.14
.75
.36
.27
.59
.21
.13
.50
.36
.42
.33
1
.20
.33
.54
.88
.69
0
.44
S
A
A
R
-C
-F
R
.20
.40
.50
.54
0
.33
.50
.33
.60
.44
.38
0
.60
.60
.40
.42
.43
.17
.50
.33
.64
.13
.67
.67
.25
.46
.22
.31
.42
.27
.79
.18
.56
.18
.17
.60
.25
.09
.86
.50
.18
.44
.44
S
A
A
R
-D
E
.33
.77
?
.63
.43
.14
.64
.50
.91
?
.44
.82
.58
.73
.67
.50
.40
.67
.70
.33
.43
.44
.36
.67
.67
.86
.86
.60
.50
.86
?
.29
.56
.44
.53
.73
.33
.83
?
.31
.67
.86
?
.43
.33
.83
?
.56
.17
.78
.55
S
A
A
R
-E
S
.29
.60
.44
.18
.07
?
.29
.40
.67
.17
.44
.10
?
.46
.27
.50
.13
.12
?
.50
.14
.33
.46
.62
.36
.25
.55
.14
.17
.25
.40
.39
.17
.50
.42
.44
.47
.50
.11
.33
.11
?
.33
.39
.18
.08
.33
S
A
A
R
-F
R
.18
.44
.60
.30
.20
.56
.73
.60
.50
.58
0
.50
.33
.64
.29
.42
.63
?
.67
.57
.33
.25
.38
.42
.58
.43
.41
.30
.54
.50
.29
.33
.36
.36
.17
?
.43
.33
.40
.60
.33
.27
.20
.64
?
.31
.25
.58
U
C
B
-E
S
.33
.44
.33
.18
.18
.55
.62
.14
.63
.18
.44
.57
.44
.33
.67
.44
.25
.56
.25
.46
.60
.50
.56
.83
.63
.56
.57
.36
.46
.50
.50
.75
.50
.56
.58
.88
.71
.57
.14
.42
.67
.44
.31
U
E
D
-C
M
B
-D
E
.20
.67
.75
1
.67
.33
1
.88
.75
1
.83
.71
.75
.67
.67
1
.44
.60
.67
.57
1
1
.78
1
.67
.44
1
.60
1
?
1
.67
1
.83
1
.40
.13
.67
.75
?
.25
.50
.40
1
.50
.75
.50
U
E
D
-C
M
B
-F
R
.67
1
.57
.50
.50
.13
.50
.40
.33
.50
.25
.40
.20
.25
.50
.40
.50
.14
.20
.33
.33
1
.67
.40
.33
.33
.29
.38
.33
.33
.50
U
E
D
-C
M
B
-X
X
.50
.67
.25
.13
1
.50
.50
.40
.25
.20
.40
.13
.33
.40
.75
.33
1
.50
.50
.60
.50
0
.67
.56
.14
0
.38
0
.33
.25
U
E
D
-C
Z
.75
.79
?
.89
?
.86
.56
.83
.71
.77
?
.92
?
1
?
.20
.88
.67
.63
.67
1
.69
?
.60
.80
?
.80
.75
.67
.86
.75
?
.62
.46
.56
.67
.71
.78
.88
.67
.64
.43
.78
?
.73
.86
.50
1
1
?
.55
1
?
.80
?
0
.46
.90
?
U
E
D
-D
E
.27
.80
.78
1
.20
.80
?
.71
1
?
.67
.31
.25
.75
.92
?
.56
.67
.83
?
.62
.44
.67
.67
.63
.82
?
.25
.75
1
?
.40
.40
.75
.75
.23
1
.67
.67
.60
.57
.50
1
.36
.64
.67
.11
.25
.67
U
E
D
-E
S
.15
?
.17
.56
.42
0
.31
.56
.46
.50
.88
.11
.33
.29
.46
.29
.67
.69
.31
.36
.46
.27
.50
.54
.33
.30
.60
.43
.29
.50
.38
.13
.14
.17
?
.23
0
.17
.40
.83
.80
0
.27
.46
0
.13
U
E
D
-F
R
.50
.56
.75
.67
0
.18
.56
.50
.14
.44
.20
.17
.42
.53
.22
.64
.58
.44
.57
.30
.50
.38
.17
.42
.50
.22
.57
.31
.38
.44
.42
.23
.40
.33
.46
.39
.33
.75
1
.10
?
.11
.46
.31
.38
U
E
D
-H
U
1
.75
.67
.88
.67
.50
.75
1
?
.80
1
?
.38
.60
.83
.75
1
1
?
.86
?
.50
.86
.75
1
?
.80
1
?
1
?
.80
.80
.83
1
1
1
?
1
.80
.82
.67
1
1
.83
.50
.33
.67
.86
?
.67
1
?
1
.83
.40
U
M
D
-C
Z
.40
.50
.64
.50
.50
.67
.29
.50
.50
.80
?
.25
.40
.27
.71
.60
.44
.79
?
.79
?
.70
.80
.63
.67
.50
.63
.44
.63
.60
.43
.75
.64
.91
?
.56
.11
.54
.67
.44
.25
.67
1
.27
.50
.88
.69
.17
.62
U
P
C
-E
S
.40
.42
.44
.21
.15
?
.44
.38
.25
.38
.54
.43
.30
.50
.23
.40
.55
.33
.33
.22
.47
.13
.50
.50
.38
.25
.43
.14
.33
.63
.56
.44
.36
.53
.25
.39
.50
.50
.50
.10
?
.33
.63
.50
.40
.31
>
O
T
H
E
R
S
.29
.54
.51
.41
.15
.35
.51
.52
.43
.51
.25
.34
.39
.55
.33
.57
.57
.3
.5
.44
.52
.49
.48
.56
.50
.35
.46
.57
.39
.50
.49
.46
.49
.32
.51
.45
.4
.22
.56
.58
.19
.28
.53
.48
.11
.3
.52
?
O
T
H
E
R
S
.41
.7
.66
.55
.28
.44
.67
.64
.57
.65
.38
.47
.54
.7
.45
.71
.70
.39
.63
.55
.63
.61
.57
.7
.64
.49
.60
.71
.51
.62
.62
.57
.62
.45
.68
.58
.55
.33
.70
.72
.29
.40
.65
.62
.19
.43
.63
Table
53:
S
entence-levelranking
for
the
A
ll-E
nglish
N
ew
s
Task.
106
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 65?72, Vancouver, October 2005. c?2005 Association for Computational Linguistics
NeurAlign: Combining Word Alignments Using Neural Networks
Necip Fazil Ayan, Bonnie J. Dorr and Christof Monz
Department of Computer Science
University of Maryland
College Park, MD 20742
{nfa,bonnie,christof}@umiacs.umd.edu
Abstract
This paper presents a novel approach to
combining different word alignments. We
view word alignment as a pattern classifi-
cation problem, where alignment combi-
nation is treated as a classifier ensemble,
and alignment links are adorned with lin-
guistic features. A neural network model
is used to learn word alignments from the
individual alignment systems. We show
that our alignment combination approach
yields a significant 20-34% relative er-
ror reduction over the best-known align-
ment combination technique on English-
Spanish and English-Chinese data.
1 Introduction
Parallel texts are a valuable resource in natural lan-
guage processing and essential for projecting knowl-
edge from one language onto another. Word-level
alignment is a critical component of a wide range of
NLP applications, such as construction of bilingual
lexicons (Melamed, 2000), word sense disambigua-
tion (Diab and Resnik, 2002), projection of language
resources (Yarowsky et al, 2001), and statistical ma-
chine translation. Although word-level aligners tend
to perform well when there is sufficient training data,
the quality decreases as the size of training data de-
creases. Even with large amounts of training data,
statistical aligners have been shown to be suscepti-
ble to mis-aligning phrasal constructions (Dorr et al,
2002) due to many-to-many correspondences, mor-
phological language distinctions, paraphrased and
free translations, and a high percentage of function
words (about 50% of the tokens in most texts).
This paper presents a novel approach to align-
ment combination, NeurAlign, that treats each align-
ment system as a black box and merges their outputs.
We view word alignment as a pattern classification
problem and treat alignment combination as a classi-
fier ensemble (Hansen and Salamon, 1990; Wolpert,
1992). The ensemble-based approach was devel-
oped to select the best features of different learning
algorithms, including those that may not produce a
globally optimal solution (Minsky, 1991).
We use neural networks to implement the
classifier-ensemble approach, as these have previ-
ously been shown to be effective for combining clas-
sifiers (Hansen and Salamon, 1990). Neural nets
with 2 or more layers and non-linear activation func-
tions are capable of learning any function of the
feature space with arbitrarily small error. Neural
nets have been shown to be effective with (1) high-
dimensional input vectors, (2) relatively sparse data,
and (3) noisy data with high within-class variability,
all of which apply to the word alignment problem.
The rest of the paper is organized as follows: In
Section 2, we describe previous work on improv-
ing word alignments and use of classifier ensembles
in NLP. Section 3 gives a brief overview of neu-
ral networks. In Section 4, we present a new ap-
proach, NeurAlign, that learns how to combine indi-
vidual word alignment systems. Section 5 describes
our experimental design and the results on English-
Spanish and English-Chinese. We demonstrate that
NeurAlign yields significant improvements over the
best-known alignment combination technique.
65
j
i
Hidden layer
Output layer
Input layer
wij
ai
Figure 1: Multilayer Perceptron Overview
2 Related Work
Previous algorithms for improving word alignments
have attempted to incorporate additional knowledge
into their modeling. For example, Liu (2005) uses
a log-linear combination of linguistic features. Ad-
ditional linguistic knowledge can be in the form of
part-of-speech tags. (Toutanova et al, 2002) or de-
pendency relations (Cherry and Lin, 2003). Other
approaches to improving alignment have combined
alignment models, e.g., using a log-linear combina-
tion (Och and Ney, 2003) or mutually independent
association clues (Tiedemann, 2003).
A simpler approach was developed by Ayan et
al. (2004), where word alignment outputs are com-
bined using a linear combination of feature weights
assigned to the individual aligners. Our method is
more general in that it uses a neural network model
that is capable of learning nonlinear functions.
Classifier ensembles are used in several NLP ap-
plications. Some NLP applications for classifier en-
sembles are POS tagging (Brill and Wu, 1998; Ab-
ney et al, 1999), PP attachment (Abney et al, 1999),
word sense disambiguation (Florian and Yarowsky,
2002), and parsing (Henderson and Brill, 2000).
The work reported in this paper is the first appli-
cation of classifier ensembles to the word-alignment
problem. We use a different methodology to com-
bine classifiers that is based on stacked general-
ization (Wolpert, 1992), i.e., learning an additional
model on the outputs of individual classifiers.
3 Neural Networks
A multi-layer perceptron (MLP) is a feed-forward
neural network that consists of several units (neu-
rons) that are connected to each other by weighted
links. As illustrated in Figure 1, an MLP consists
of one input layer, one or more hidden layers, and
one output layer. The external input is presented to
the input layer, propagated forward through the hid-
den layers and creates the output vector in the output
layer. Each unit i in the network computes its output
with respect to its net input neti =
?
j wijaj , where
j represents all units in the previous layer that are
connected to the unit i. The output of unit i is com-
puted by passing the net input through a non-linear
activation function f , i.e. ai = f(neti).
The most commonly used non-linear activation
functions are the log sigmoid function f(x) =
1
1+e?x or hyperbolic tangent sigmoid function
f(x) = 1?e
?2x
1+e?2x . The latter has been shown to be
more suitable for binary classification problems.
The critical question is the computation of
weights associated with the links connecting the
neurons. In this paper, we use the resilient back-
propagation (RPROP) algorithm (Riedmiller and
Braun, 1993), which is based on the gradient descent
method, but converges faster and generalizes better.
4 NeurAlign Approach
We propose a new approach, NeurAlign, that learns
how to combine individual word alignment sys-
tems. We treat each alignment system as a classi-
fier and transform the combination problem into a
classifier ensemble problem. Before describing the
NeurAlign approach, we first introduce some termi-
nology used in the description below.
Let E = e1, . . . , et and F = f1, . . . , fs be two
sentences in two different languages. An alignment
link (i, j) corresponds to a translational equivalence
between words ei and fj . Let Ak be an align-
ment between sentences E and F , where each el-
ement a ? Ak is an alignment link (i, j). Let
A = {A1, . . . , Al} be a set of alignments between
E and F . We refer to the true alignment as T , where
each a ? T is of the form (i, j). A neighborhood
of an alignment link (i, j)?denoted by N(i, j)?
consists of 8 possible alignment links in a 3?3 win-
dow with (i, j) in the center of the window. Each
element of N(i, j) is called a neighboring link of
(i, j).
Our goal is to combine the information in
A1, . . . , Al such that the resulting alignment is
closer to T . A straightforward solution is to take the
intersection or union of the individual alignments, or
66
perform a majority voting for each possible align-
ment link (i, j). Here, we use an additional model
to learn how to combine outputs of A1, . . . , Al.
We decompose the task of combining word align-
ments into two steps: (1) Extract features; and (2)
Learn a classifier from the transformed data. We de-
scribe each of these two steps in turn.
4.1 Extracting Features
Given sentences E and F , we create a (potential)
alignment instance (i, j) for all possible word com-
binations. A crucial component of building a classi-
fier is the selection of features to represent the data.
The simplest approach is to treat each alignment-
system output as a separate feature upon which we
build a classifier. However, when only a few align-
ment systems are combined, this feature space is not
sufficient to distinguish between instances. One of
the strategies in the classification literature is to sup-
ply the input data to the set of features as well.
While combining word alignments, we use two
types of features to describe each instance (i, j):
(1) linguistic features and (2) alignment features.
Linguistic features include POS tags of both words
(ei and fj) and a dependency relation for one of
the words (ei). We generate POS tags using the
MXPOST tagger (Ratnaparkhi, 1996) for English
and Chinese, and Connexor for Spanish. Depen-
dency relations are produced using a version of the
Collins parser (Collins, 1997) that has been adapted
for building dependencies.
Alignment features consist of features that are ex-
tracted from the outputs of individual alignment sys-
tems. For each alignmentAk ? A, the following are
some of the alignment features that can be used to
describe an instance (i, j):
1. Whether (i, j) is an element of Ak or not
2. Translation probability p(fj |ei) computed
over Ak1
3. Fertility of (i.e., number of words in F that are
aligned to) ei in Ak
4. Fertility of (i.e., number of words in E that are
aligned to) fj in Ak
5. For each neighbor (x, y) ? N(i, j), whether
(x, y) ? Ak or not (8 features in total)
6. For each neighbor (x, y) ? N(i, j), transla-
tion probability p(fy|ex) computed overAk (8
features in total)
It is also possible to use variants, or combinations,
of these features to reduce feature space.
Figure 2 shows an example of how we transform
the outputs of 2 alignment systems, A1 and A2, for
an alignment link (i, j) into data with some of the
features above. We use -1 and 1 to represent the
absence and existence of a link, respectively. The
neighboring links are presented in row-by-row order.
X
XX
X
X
X
A1
A2
ei-1
ei
ei+1
fj-1 fj fj+1
1 (for A1), 0 (for A2)fertility(fj)
2 (for A1), 1 (for A2)fertility(ei)
2 (for A1), 3 (for A2)total neighbors
1, -1, -1, 1, 1, -1, -1, 1neighbors (A1? A2)
1, -1, -1, -1, 1, -1, -1, 1neighbors (A2)
-1, -1, -1, 1, -1, -1, -1, 1neighbors (A1)
1 (for A1), -1 (for A2)outputs of aligners
Modifierrel(ei)
Noun, Preppos(ei) , pos(fj)
Features for the alignment link ( i , j )
ei-1
ei
ei+1
fj-1 fj fj+1
Figure 2: An Example of Transforming Alignments
into Classification Data
For each sentence pair E = e1, . . . , et and F =
f1, . . . , fs, we generate s ? t instances to represent
the sentence pair in the classification data.
Supervised learning requires the correct output,
which here is the true alignment T . If an alignment
link (i, j) is an element of T , then we set the correct
output to 1, and to ?1, otherwise.
4.2 Learning A Classifier
Once we transform the alignments into a set of in-
stances with several features, the remaining task is to
learn a classifier from this data. In the case of word
alignment combination, there are important issues to
consider for choosing an appropriate classifier. First,
there is a very limited amount of manually annotated
data. This may give rise to poor generalizations be-
cause it is very likely that unseen data include lots
of cases that are not observed in the training data.
Second, the distribution of the data according to
the classes is skewed. In a preliminary study on an
English-Spanish data set, we found out that only 4%
of the all word pairs are aligned to each other by hu-
mans, among a possible 158K word pairs. More-
over, only 60% of those aligned word pairs were
1The translation probabilities can be borrowed from the ex-
isting systems, if available. Otherwise, they can be generated
from the outputs of individual alignment systems using likeli-
hood estimates.
67
A1 AlAi
FeatureExtraction ClassificationData Neural NetLearning
Output
Truth
EnrichedCorpus
Figure 3: NeurAlign1?Alignment Combination
Using All Data At Once
also aligned by the individual alignment systems
that were tested.
Finally, given the distribution of the data, it is dif-
ficult to find the right features to distinguish between
instances. Thus, it is prudent to use as many features
as possible and let the learning algorithm filter out
the redundant features.
Below, we describe how neural nets are used at
different levels to build a good classifier.
4.2.1 NeurAlign1: Learning All At Once
Figure 3 illustrates how we combine align-
ments using all the training data at the same time
(NeurAlign1). First, the outputs of individual align-
ments systems and the original corpus (enriched
with additional linguistic features) are passed to the
feature extraction module. This module transforms
the alignment problem into a classification problem
by generating a training instance for every pair of
words between the sentences in the original corpus.
Each instance is represented by a set of features (de-
scribed in Section 4.1). The new training data is
passed to a neural net learner, which outputs whether
an alignment link exists for each training instance.
4.2.2 NeurAlign2: Multiple Neural Networks
The use of multiple neural networks (NeurAlign2)
enables the decomposition of a complex problem
into smaller problems. Local experts are learned
for each smaller problem and these are then merged.
Following Tumer and Ghosh (1996), we apply spa-
tial partitioning of training instances using proxim-
ity of patterns in the input space to reduce the com-
plexity of the tasks assigned to individual classifiers.
We conducted a preliminary analysis on 100 ran-
domly selected English-Spanish sentence pairs from
a mixed corpus (UN + Bible + FBIS) to observe the
SPANISH
Adj Adv Comp Det Noun Prep Verb
E Adj 18 - - 82 40 96 66
N Adv - 8 - - 50 67 75
G Comp - - 12 - 46 37 96
L Det - - - 10 60 100 -
I Noun 42 77 100 94 23 98 84
S Prep - - - 93 70 22 100
H Verb 42 - - 100 66 78 43
Table 1: Error Rates according to POS Tags for
GIZA++ (E-to-S) (in percentages)
ClassificationData
DataPartitioning
Output
Truth
Parta
Parti
Partz
NNa
NNz
NNi NNCombination
Figure 4: NeurAlign2?Alignment Combination
with Partitioning
distribution of errors according to POS tags in both
languages. We examined the cases in which the in-
dividual alignment and the manual annotation were
different?a total of 3,348 instances, where 1,320 of
those are misclassified by GIZA++ (E-to-S).2 We
use a standard measure of error, i.e., the percentage
of misclassified instances out of the total number of
instances. Table 1 shows error rates (by percentage)
according to POS tags for GIZA++ (E-to-S).3
Table 1 shows that the error rate is relatively low
in cases where both words have the same POS tag.
Except for verbs, the lowest error rate is obtained
when both words have the same POS tag (the er-
ror rates on the diagonal). On the other hand, the
error rates are high in several other cases, as much
as 100%, e.g., when the Spanish word is a deter-
miner or a preposition.4 This suggests that dividing
the training data according to POS tag, and training
neural networks on each subset separately might be
better than training on the entire data at once.
Figure 4 illustrates the combination approach
with neural nets after partitioning the data into dis-
2For this analysis, we ignored the cases where both systems
produced an output of -1 (i.e., the words are not aligned).
3Only POS pairs that occurred at least 10 times are shown.
4The same analysis was done for the other direction and re-
sulted in similar distribution of error rates.
68
joint subsets (NeurAlign2). Similar to NeurAlign1,
the outputs of individual alignment systems, as well
as the original corpus, are passed to the feature ex-
traction module. Then the training data is split into
disjoint subsets using a subset of the available fea-
tures for partitioning. We learn different neural nets
for each partition, and then merge the outputs of the
individual nets. The advantage of this is that it re-
sults in different generalizations for each partition
and that it uses different subsets of the feature space
for each net.
5 Experiments and Results
This section describes our experimental design, in-
cluding evaluation metrics, data, and settings.
5.1 Evaluation Metrics
Let A be the set of alignment links for a set of sen-
tences. We take S to be the set of sure alignment
links and P be the set of probable alignment links
(in the gold standard) for the same set of sentences.
Precision (Pr), recall (Rc) and alignment error rate
(AER) are defined as follows:
Pr =
|A ? P |
|A|
Rc =
|A ? S|
|S|
AER = 1?
|A ? S|+ |A ? P |
|A|+ |S|
A manually aligned corpus is used as our gold stan-
dard. For English-Spanish data, the manual annota-
tion is done by a bilingual English-Spanish speaker.
Every link in the English-Spanish gold standard is
considered a sure alignment link (i.e., P = S).
For English-Chinese, we used 2002 NIST MT
evaluation test set. Each sentence pair was aligned
by two native Chinese speakers, who are fluent in
English. Each alignment link appearing in both an-
notations was considered a sure link, and links ap-
pearing in only one set were judged as probable. The
annotators were not aware of the specifics of our ap-
proach.
5.2 Evaluation Data and Settings
We evaluated NeurAlign1 and NeurAlign2, using 5-
fold cross validation on two data sets:
1. A set of 199 English-Spanish sentence pairs
(nearly 5K words on each side) from a mixed
corpus (UN + Bible + FBIS).
2. A set of 491 English-Chinese sentence pairs
(nearly 13K words on each side) from 2002
NIST MT evaluation test set.
We computed precision, recall and error rate on the
entire set of sentence pairs for each data set.5
To evaluate NeurAlign, we used GIZA++ in both
directions (E-to-F and F -to-E, where F is either
Chinese (C) or Spanish (S)) as input and a refined
alignment approach (Och and Ney, 2000) that uses
a heuristic combination method called grow-diag-
final (Koehn et al, 2003) for comparison. (We
henceforth refer to the refined-alignment approach
as ?RA.?)
For the English-Spanish experiments, GIZA++
was trained on 48K sentence pairs from a mixed
corpus (UN + Bible + FBIS), with nearly 1.2M of
words on each side, using 10 iterations of Model 1,
5 iterations of HMM, and 5 iterations of Model 4.
For the English-Chinese experiments, we used 107K
sentence pairs from FBIS corpus (nearly 4.1M En-
glish and 3.3M Chinese words) to train GIZA++, us-
ing 5 iterations of Model 1, 5 iterations of HMM, 3
iterations of Model 3, and 3 iterations of Model 4.
5.3 Neural Network Settings
In our experiments, we used a multi-layer percep-
tron (MLP) consisting of 1 input layer, 1 hidden
layer, and 1 output layer. The hidden layer consists
of 10 units, and the output layer consists of 1 unit.
All units in the hidden layer are fully connected to
the units in the input layer, and the output unit is
fully connected to all the units in the hidden layer.
We used hyperbolic tangent sigmoid function as the
activation function for both layers.
One of the potential pitfalls is overfitting as the
number of iterations increases. To address this, we
used the early stopping with validation set method.
In our experiments, we held out (randomly selected)
1/4 of the training set as the validation set.
Neural nets are sensitive to the initial weights. To
overcome this, we performed 5 runs of learning for
each training set. The final output for each training
is obtained by a majority voting over 5 runs.
5The number of alignment links varies over each fold.
Therefore, we chose to evaluate all data at once instead of eval-
uating on each fold and then averaging.
69
5.4 Results
This section describes the experiments on English-
Spanish and English-Chinese data for testing the
effects of feature selection, training on the en-
tire data (NeurAlign1) or on the partitioned data
(NeurAlign2), using two input alignments: GIZA++
(E-to-F ) and GIZA++ (F -to-E). We used the fol-
lowing additional features, as well as the outputs of
individual aligners, for an instance (i, j) (set of fea-
tures 2?7 below are generated separately for each
input alignment Ak):
1. posEi, posFj , relEi: POS tags and depen-
dency relation for ei and fj .
2. neigh(i, j): 8 features indicating whether a
neighboring link exists in Ak.
3. f ertEi, f ertFj : 2 features indicating the fer-
tility of ei and fj in Ak.
4. NC(i, j): Total number of existing links in
N(i, j) in Ak.
5. TP (i, j): Translation probability p(fj |ei) in
Ak.
6. NghTP(i, j): 8 features indicating the trans-
lation probability p(fy|ex) for each (x, y) ?
N(i, j) in Ak.
7. AvTP (i, j): Average translation probability
of the neighbors of (i, j) in Ak.
We performed statistical significance tests using
two-tailed paired t-tests. Unless otherwise indi-
cated, the differences between NeurAlign and other
alignment systems, as well as the differences among
NeurAlign variations themselves, were statistically
significant within the 95% confidence interval.
5.4.1 Results for English-Spanish
Table 2 summarizes the precision, recall and
alignment error rate values for each of our two
alignment system inputs plus the three alternative
alignment-combination approaches. Note that the
best performing aligner among these is the RA
method, with an AER of 21.2%. (We include this
in subsequent tables for ease of comparison.)
Feature Selection for Training All Data At Once:
NeurAlign1 Table 3 presents the results of train-
ing neural nets using the entire data (NeurAlign1)
with different subsets of the feature space. When we
used POS tags and the dependency relation as fea-
tures, NeurAlign1 performs worse than RA. Using
Alignments Pr Rc AER
E-to-S 87.0 67.0 24.3
S-to-E 88.0 67.5 23.6
Intersection 98.2 59.6 25.9
Union 80.6 74.9 22.3
RA 83.8 74.4 21.2
Table 2: Results for GIZA++ Alignments and Their
Simple Combinations
the neighboring links as the feature set gave slightly
(not significantly) better results than RA. Using POS
tags, dependency relations, and neighboring links
also resulted in better performance than RA but the
difference was not statistically significant.
When we used fertilities along with the POS tags
and dependency relations, the AER was 20.0%?a
significant relative error reduction of 5.7% over RA.
Adding the neighboring links to the previous feature
set resulted in an AER of 17.6%?a significant rela-
tive error reduction of 17% over RA.
Interestingly, when we removed POS tags and de-
pendency relations from this feature set, there was
no significant change in the AER, which indicates
that the improvement is mainly due to the neighbor-
ing links. This supports our initial claim about the
clustering of alignment links, i.e., when there is an
alignment link, usually there is another link in its
neighborhood. Finally, we tested the effects of using
translation probabilities as part of the feature set, and
found out that using translation probabilities did no
better than the case where they were not used. We
believe this happens because the translation proba-
bility p(fj |ei) has a unique value for each pair of ei
and fj ; therefore it is not useful to distinguish be-
tween alignment links with the same words.
Feature Selection for Training on Partitioned
Data: NeurAlign2 In order to train on partitioned
data (NeurAlign2), we needed to establish appropri-
ate features for partitioning the training data. Ta-
ble 4 presents the evaluation results for NeurAlign1
(i.e., no partitioning) and NeurAlign2 with different
features for partitioning (English POS tag, Spanish
POS tag, and POS tags on both sides). For training
on each partition, the feature space included POS
tags (e.g., Spanish POS tag in the case where parti-
tioning is based on English POS tag only), depen-
dency relations, neighborhood features, and fertili-
ties. We observed that partitioning based on POS
tags on one side reduced the AER to 17.4% and
70
Features Pr Rc AER
posEi, posFj , relEi 90.6 67.7 22.5
neigh(i, j) 91.3 69.5 21.1
posEi, posFj , relEi, 91.7 70.2 20.5
neigh(i, j)
posEi, posFj , relEi, 91.4 71.1 20.0
f ertEi, f ertFj
posEi, posFj , relEi, 89.5 76.3 17.6
neigh(i, j), NC(i, j)
f ertEi, f ertFj
neigh(i, j), NC(i, j) 89.7 75.7 17.9
f ertEi, f ertFj
posEi, posFj , relEi, 90.0 75.7 17.9
f ertEi, f ertFj ,
neigh(i, j), NC(i, j),
TP (i, j), AvTP (i, j)
RA 83.8 74.4 21.2
Table 3: Combination with Neural Networks:
NeurAlign1 (All-Data-At-Once)
17.1%, respectively. Using POS tags on both sides
reduced the error rate to 16.9%?a significant rel-
ative error reduction of 5.6% over no partitioning.
All four methods yielded statistically significant er-
ror reductions over RA?we will examine the fourth
method in more detail below.
Alignment Pr Rc AER
NeurAlign1 89.7 75.7 17.9
NeurAlign2[posEi] 91.1 75.4 17.4
NeurAlign2[posFj ] 91.2 76.0 17.1
NeurAlign2[posEi, posFj ] 91.6 76.0 16.9
RA 83.8 74.4 21.2
Table 4: Effects of Feature Selection for Partitioning
Once we determined that partitioning by POS tags
on both sides brought about the biggest gain, we ran
NeurAlign2 using this partitioning, but with differ-
ent feature sets. Table 5 shows the results of this
experiment. Using dependency relations, word fer-
tilities and translation probabilities (both for the link
in question and the neighboring links) yielded a sig-
nificantly lower AER (18.6%)?a relative error re-
duction of 12.3% over RA. When the feature set
consisted of dependency relations, word fertilities,
and neighborhood links, the AER was reduced to
16.9%?a 20.3% relative error reduction over RA.
We also tested the effects of adding translation prob-
abilities to this feature set, but as in the case of
NeurAlign1, this did not improve the alignments.
In the best case, NeurAlign2 achieved substan-
tial and significant reductions in AER over the in-
put alignment systems: a 28.4% relative error re-
duction over S-to-E and a 30.5% relative error re-
Features Pr Rc AER
relEi, f ertEi, f ertFj , 91.9 73.0 18.6
TP (i, j), AvTP (i, j),
NghTP (i, j)
neigh(i, j) 90.3 74.0 18.7
relEi, f ertEi, f ertFj , 91.6 76.0 16.9
neigh(i, j), NC(i, j)
relEi, f ertEi, f ertFj , 91.4 76.1 16.9
neigh(i, j), NC(i, j),
TP (i, j), AvTP (i, j)
RA 83.8 74.4 21.2
Table 5: Combination with Neural Networks:
NeurAlign2 (Partitioned According to POS tags)
duction over E-to-S. Compared to RA, NeurAlign2
also achieved significantly better results over RA:
relative improvements of 9.3% in precision, 2.2% in
recall, and 20.3% in AER.
5.4.2 Results for English-Chinese
The results of the input alignments to NeurAlign,
i.e., GIZA++ alignments in two different directions,
NeurAlign1 (i.e., no partitioning) and variations of
NeurAlign2 with different features for partitioning
(English POS tag, Chinese POS tag, and POS tags
on both sides) are shown in Table 6. For compar-
sion, we also include the results for RA in the table.
For brevity, we include only the features resulting
in the best configurations from the English-Spanish
experiments, i.e., POS tags, dependency relations,
word fertilities, and neighborhood links (the features
in the third row of Table 5). The ground truth used
during the training phase consisted of all the align-
ment links with equal weight.
Alignments Pr Rc AER
E-to-C 70.4 68.3 30.7
C-to-E 66.0 69.8 32.2
NeurAlign1 85.0 71.4 22.2
NeurAlign2[posEi] 85.7 74.6 20.0
NeurAlign2[posFj ] 85.7 73.2 20.8
NeurAlign2[posEi, posFj ] 86.3 74.7 19.7
RA 61.9 82.6 29.7
Table 6: Results on English-Chinese Data
Without any partitioning, NeurAlign achieves an
alignment error rate of 22.2%?a significant relative
error reduction of 25.3% over RA. Partitioning the
data according to POS tags results in significantly
better results over no partitioning. When the data is
partitioned according to both POS tags, NeurAlign
reduces AER to 19.7%?a significant relative error
reduction of 33.7% over RA. Compared to the input
71
alignments, the best version of NeurAlign achieves
a relative error reduction of 35.8% and 38.8%, re-
spectively.
6 Conclusions
We presented NeurAlign, a novel approach to com-
bining the outputs of different word alignment sys-
tems. Our approach treats individual alignment sys-
tems as black boxes, and transforms the individual
alignments into a set of data with features that are
borrowed from their outputs and additional linguis-
tic features (such as POS tags and dependency re-
lations). We use neural nets to learn the true align-
ments from these transformed data.
We show that using POS tags to partition the
transformed data, and learning a different classifier
for each partition is more effective than using the en-
tire data at once. Our results indicate that NeurAlign
yields a significant 28-39% relative error reduction
over the best of the input alignment systems and
a significant 20-34% relative error reduction over
the best known alignment combination technique on
English-Spanish and English-Chinese data.
We should note that NeurAlign is not a stand-
alone word alignment system but a supervised learn-
ing approach to improve already existing alignment
systems. A drawback of our approach is that it re-
quires annotated data. However, our experiments
have shown that significant improvements can be
obtained using a small set of annotated data. We
will do additional experiments to observe the effects
of varying the size of the annotated data while learn-
ing neural nets. We are also planning to investigate
whether NeurAlign helps when the individual align-
ers are trained using more data.
We will extend our combination approach to com-
bine word alignment systems based on different
models, and investigate the effectiveness of our tech-
nique on other language pairs. We also intend to
evaluate the effectiveness of our improved alignment
approach in the context of machine translation and
cross-language projection of resources.
Acknowledgments This work has been supported in
part by ONR MURI Contract FCPO.810548265, Coopera-
tive Agreement DAAD190320020, and NSF ITR Grant IIS-
0326553.
References
Steven Abney, Robert E. Schapire, and Yoram Singer. 1999.
Boosting applied to tagging and PP attachment. In Proceed-
ings of EMNLP?1999, pages 38?45.
Necip F. Ayan, Bonnie J. Dorr, and Nizar Habash. 2004. Multi-
Align: Combining linguistic and statistical techniques to
improve alignments for adaptable MT. In Proceedings of
AMTA?2004, pages 17?26.
Eric Brill and Jun Wu. 1998. Classifier combination for im-
proved lexical disambiguation. In Proc. of ACL?1998.
Colin Cherry and Dekang Lin. 2003. A probability model to
improve word alignment. In Proceedings of ACL?2003.
Micheal Collins. 1997. Three generative lexicalized models for
statistical parsing. In Proceedings of ACL?1997.
Mona Diab and Philip Resnik. 2002. An unsupervised method
for word sense tagging using parallel corpora. In Proceed-
ings of ACL?2002.
Bonnie J. Dorr, Lisa Pearl, Rebecca Hwa, and Nizar Habash.
2002. DUSTer: A method for unraveling cross-language di-
vergences for statistical word?level alignment. In Proceed-
ings of AMTA?2002.
Radu Florian and David Yarowsky. 2002. Modeling consensus:
Classifier combination for word sense disambiguation. In
Proceedings of EMNLP?2002, pages 25?32.
L. Hansen and P. Salamon. 1990. Neural network ensembles.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 12:993?1001.
John C. Henderson and Eric Brill. 2000. Bagging and boosting
a treebank parser. In Proceedings of NAACL?2000.
Philip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL/HLT?2003.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear models
for word alignment. In Proceedings of ACL?2005.
I. Dan Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221?249.
Marvin Minsky. 1999. Logical Versus Analogical or Symbolic
Versus Connectionist or Neat Versus Scruffy. AI Magazine,
12:34?51.
Franz J. Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings of ACL?2000.
Franz J. Och and Hermann Ney. 2003. A systematic compari-
son of various statistical alignment models. Computational
Linguistics, 29(1):9?51, March.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of EMNLP?1996.
Martin Riedmiller and Heinrich Braun. 1993. A direct adaptive
method for faster backpropagation learning: The RPROP al-
gorithm. In Proceedings of the IEEE Intl. Conf. on Neural
Networks, pages 586?591.
Jorg Tiedemann. 2003. Combining clues for word alignment.
In Proceedings of EACL?2003, pages 339?346.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Man-
ning. 2002. Extensions to HMM-based statistical word
alignment models. In Proceedings of EMNLP?2002.
Kagan Tumer and Joydeep Ghosh. 1996. Error correlation and
error reduction in ensemble classifiers. Connection Science,
Special Issue on Combining Artificial Neural Networks: En-
semble Approaches, 8(3?4):385?404, December.
David H. Wolpert. 1992. Stacked generalization. Neural Net-
works, 5(2):241?259.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001.
Inducing multilingual text analysis tools via robust projec-
tion across aligned corpora. In Proceedings of HLT?2001.
72
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 185?192, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Alignment Link Projection Using Transformation-Based Learning
Necip Fazil Ayan, Bonnie J. Dorr and Christof Monz
Department of Computer Science
University of Maryland
College Park, MD 20742
{nfa,bonnie,christof}@umiacs.umd.edu
Abstract
We present a new word-alignment ap-
proach that learns errors made by ex-
isting word alignment systems and cor-
rects them. By adapting transformation-
based learning to the problem of word
alignment, we project new alignment links
from already existing links, using features
such as POS tags. We show that our align-
ment link projection approach yields a sig-
nificantly lower alignment error rate than
that of the best performing alignment sys-
tem (22.6% relative reduction on English-
Spanish data and 23.2% relative reduction
on English-Chinese data).
1 Introduction
Word-level alignment is a critical component of a
wide range of NLP applications, such as construc-
tion of bilingual lexicons (Melamed, 2000), word
sense disambiguation (Diab and Resnik, 2002), pro-
jection of language resources (Yarowsky et al,
2001), and statistical machine translation. Although
word-level aligners tend to perform well when there
is enough training data, the quality of word align-
ment decreases as the size of training data de-
creases. Moreover, word-alignment systems are of-
ten tripped up by many-to-many correspondences,
morphological language distinctions, paraphrased
and free translations, and a high percentage of func-
tion words (about 50% of the tokens in most texts).
At the heart of the matter is a set of assumptions
that word-alignment algorithms must make in order
to reduce the hypothesis space, since word align-
ment is an exponential problem. Because of these
assumptions, learning algorithms tend to make sim-
ilar errors throughout the entire data.
This paper presents a new approach?Alignment
Link Projection (ALP)?that learns common align-
ment errors made by an alignment system and at-
tempts to correct them. Our approach assumes the
initial alignment system adequately captures certain
kinds of word correspondences but fails to handle
others. ALP starts with an initial alignment and then
fills out (i.e., projects) new word-level alignment re-
lations (i.e., links) from existing alignment relations.
ALP then deletes certain alignment links associated
with common errors, thus improving precision and
recall.
In our approach, we adapt transformation-based
learning (TBL) (Brill, 1995; Brill, 1996) to the prob-
lem of word alignment. ALP attempts to find an
ordered list of transformation rules (within a pre-
specified search space) to improve a baseline anno-
tation. The rules decompose the search space into
a set of consecutive words (windows) within which
alignment links are added, to or deleted from, the
initial alignment. This window-based approach ex-
ploits the clustering tendency of alignment links,
i.e., when there is a link between two words, there
is frequently another link in close proximity.
TBL is an appropriate choice for this problem for
the following reasons:
1. It can be optimized directly with respect to an
evaluation metric.
2. It learns rules that improve the initial predic-
tion iteratively, so that it is capable of correct-
ing previous errors in subsequent iterations.
3. It provides a readable description (or classifi-
cation) of errors made by the initial system,
thereby enabling alignment refinements.
185
The rest of the paper is organized as follows: In
the next section we describe previous work on im-
proving word alignments. Section 3 presents a brief
overview of TBL. Section 4 describes the adapta-
tion of TBL to the word alignment problem. Sec-
tion 5 compares ALP to various alignments and
presents results on English-Spanish and English-
Chinese. We show that ALP yields a significant re-
ductions in alignment error rate over that of the best
performing alignment system.
2 Related Work
One of the major problems with the IBM models
(Brown et al, 1993) and the HMM models (Vogel et
al., 1996) is that they are restricted to the alignment
of each source-language word to at most one target-
language word. The standard method to overcome
this problem to use the model in both directions
(interchanging the source and target languages) and
applying heuristic-based combination techniques to
produce a refined alignment (Och and Ney, 2000;
Koehn et al, 2003)?henceforth referred to as ?RA.?
Several researchers have proposed algorithms for
improving word alignment systems by injecting ad-
ditional knowledge or combining different align-
ment models. These approaches include an en-
hanced HMM alignment model that uses part-of-
speech tags (Toutanova et al, 2002), a log-linear
combination of IBM translation models and HMM
models (Och and Ney, 2003), techniques that rely
on dependency relations (Cherry and Lin, 2003),
and a log-linear combination of IBM Model 3 align-
ment probabilities, POS tags, and bilingual dictio-
nary coverage (Liu et al, 2005). A common theme
for these methods is the use of additional features
for enriching the alignment process. These methods
perform better than the IBM models and their vari-
ants but still tend to make similar errors because of
the bias in their alignment modeling.
We adopt an approach that post-processes a given
alignment using linguistically-oriented rules. The
idea is similar to that of Ayan et al (2004), where
manually-crafted rules are used to correct align-
ment links related to language divergences. Our
approach differs, however, in that the rules are ex-
tracted automatically?not manually?by examin-
ing an initial alignment and categorizing the errors
according to features of the words.
Initial Annotation
Corpus
Templates
Rule Instantiation
Best Rule Selection
Rule Application
Rules
CorpusAnnotated
Ground Truth
Figure 1: TBL Architecture
3 Transformation-based Learning
As shown in Figure 1, the input to TBL is an unanno-
tated corpus that is first passed to an initial annotator
and then iteratively updated through comparison to a
manually-annotated reference set (or ground truth).
On each iteration, the output of the previous iteration
is compared against the ground truth, and an ordered
list of transformation rules is learned that make the
previous annotated data better resemble the ground
truth.
A set of rule templates determines the space of
allowable transformation rules. A rule template has
two components: a triggering environment (condi-
tion of the rule) and a rewrite rule (action taken). On
each iteration, these templates are instantiated with
features of the constituents of the templates when
the condition of the rule is satisfied.
This process eventually identifies all possible in-
stantiated forms of the templates. Among all these
possible rules, the transformation whose application
results in the best score?according to some objec-
tive function?is identified. This transformation is
added to the ordered list of transformation rules.
The learning stops when there is no transformation
that improves the current state of the data or a pre-
specified threshold is reached.
When presented with new data, the transforma-
tion rules are applied in the order that they were
added to the list of transformations. The output of
the system is the annotated data after all transforma-
tions are applied to the initial annotation.
4 Alignment Link Projection (ALP)
ALP is a TBL implementation that projects align-
ment links from an initial input alignment. We in-
duce several variations of ALP by setting four pa-
rameters in different ways:
186
ei
fj fj+1
NULL ei
fj fj+1
Figure 2: Graphical Representation of a Template
1. Initial alignment
2. Set of templates
3. Simple or generalized instantiation
4. Best rule selection
We describe each of these below using the following
definitions and notation:
? E = e1, . . . , ei, . . . , et is a sentence in lan-
guage L1 and F = f1, . . . , fj , . . . , fs is a sen-
tence in language L2.
? An alignment link (i, j) corresponds to a trans-
lational equivalence between ei and fj .
? A neighborhood of an alignment link (i, j)?
denoted by N(i, j)?consists of 8 possible
alignment links in a 3 ? 3 window with (i, j)
in the center of the window. Each element of
N(i, j) is called a neighboring link of (i, j).
? nullEA(i) is true if and only if ei is not
aligned to any word in F in a given alignment
A. Similarly, nullFA(j) is true if and only if
fj is not aligned to any word in E in a given
alignment A.
4.1 Initial Alignment
Any existing word-alignment system may be used
for the initial annotation step of the TBL algo-
rithm. For our experiments, we chose GIZA++ (Och
and Ney, 2000) and the RA approach (Koehn et
al., 2003)? the best known alignment combination
technique? as our initial aligners.1
4.2 TBL Templates
Our templates consider consecutive words (of size
1, 2 or 3) in both languages. The condition por-
tion of a TBL rule template tests for the existence
of an alignment link between two words. The ac-
tion portion involves the addition or deletion of an
alignment link. For example, the rule template in
Figure 2 is applicable only when a word (ei) in one
language is aligned to the second word (fj+1) of a
phrase (fj , fj+1) in the other language, and the first
1We treat these initial aligners as black boxes.
word (fj) of the phrase is unaligned in the initial
alignment. The action taken by this rule template is
to add a link between ei and fj .2
ALP employs 3 different sets of templates to
project new alignment links or delete existing links
in a given alignment:
1. Expansion of the initial alignment according
to another alignment
2. Deletion of spurious alignment links
3. Correction of multi-word (one-to-many or
many-to-one) correspondences
Each of these is described below.
4.2.1 Expansion Templates
Expansion templates are used to extend an initial
alignment given another alignment as the validation
set. This approach is similar to the one used in the
RA method in that it adds links based on knowl-
edge about neighboring links, but it differs in that it
also uses features of the words themselves to decide
which neighboring links to add.
Our expansion templates are presented in Table 1.
The first 8 templates add a new link to the initial
alignmentA if there is a neighboring link in the vali-
dation alignment V . The final two templates enforce
the presence of at least two neighboring links in the
validation set V before adding a new link.
Condition Action
(i, j) ? A, (i? 1, j ? 1) ? V add (i? 1, j ? 1)
(i, j) ? A, (i? 1, j) ? V add (i? 1, j)
(i, j) ? A, (i? 1, j + 1) ? V add (i? 1, j + 1)
(i, j) ? A, (i, j ? 1) ? V add (i, j ? 1)
(i, j) ? A, (i, j + 1) ? V add (i, j + 1)
(i, j) ? A, (i+ 1, j ? 1) ? V add (i+ 1, j ? 1)
(i, j) ? A, (i+ 1, j) ? V add (i+ 1, j)
(i, j) ? A, (i+ 1, j + 1) ? V add (i+ 1, j + 1)
(i? 1, j ? 1) ? A, (i+ 1, j + 1) ? A, add (i, j)
(i, j) ? V
(i+ 1, j ? 1) ? A, (i? 1, j + 1) ? A, add (i, j)
(i, j) ? V
Table 1: Templates for Expanding the Alignment A
According to a Validation Alignment V
4.2.2 Deletion Templates
Existing alignment algorithms (e.g., GIZA++) are
biased toward aligning some words, especially in-
frequent ones, in one language to many words in the
other language in order to minimize the number of
unaligned words, even if many incorrect alignment
2A thick line indicates an added link.
187
links are induced.3 Deletion templates are useful for
eliminating the resulting spurious links.
The basic idea is to remove alignment links
that do not have a neighboring link if the word
in question has already been aligned to another
word. Table 2 lists two simple templates to
clean up spurious links. We define the predicate
neighbor existsA(i, j) to denote whether there is
an alignment link in the neighborhood of the link
(i, j) in a given alignment A. For example, the first
template deletes spurious links for a particular word
ei in E.
Condition Action
(i, j) ? A, (i, k) ? A,
neighbor existsA(i, j), del (i, k)
not(neighbor existsA(i, k))
(i, j) ? A, (k, j) ? A,
neighbor existsA(i, j), del (e, j)
not(neighbor existsA(k, j))
Table 2: Templates for Deleting Spurious Links in a
Given Alignment A
4.2.3 Multi-Word Correction Templates
Current alignment algorithms produce one-to-one
word correspondences quite successfully. However,
accurate alignment of phrasal constructions (many-
to-many correspondences) is still problematic. On
the one hand, the ability to provide fully correct
phrasal alignments is impaired by the occurrence of
high-frequency function words and/or words that are
not exact translations of the words in the other lan-
guage. On the other hand, we have observed that
most alignment systems are capable of providing
partially correct phrasal alignments.4
Our templates for handling multi-word correspon-
dences are grounded in the outcome of this finding.
That is, we make the (frequently correct) assumption
that at least one alignment link in a many-to-many
correspondence is correctly identified in the initial
3This is a well-known characteristic of statistical alignment
systems?motivated by the need to ensure a target-word trans-
lation ei for each source word fj while modeling p(F |E) ?for
downstream MT.
4Specifically, we conducted a preliminary study using 40
manually-aligned English-Spanish sentences from a mixed cor-
pus (UN + Bible + FBIS) as our gold standard. We found that,
in most cases where the human annotator aligned one word to
two words, an existing alignment system identified at least one
of the two alignment links correctly.
Condition Action
nullFA(j), (i, j + 1) ? A add (i, j)
nullFA(j + 1), (i, j) ? A add (i, j + 1)
(i, j) ? A, (i, j + 1) ? A del (i, j)
(i, j) ? A, (i, j + 1) ? A del (i, j + 1)
nullFA(j), nullFA(j + 1) add (i, j),
add (i, j + 1)
nullEA(i), (i+ 1, j) ? A add (i, j)
nullEA(i+ 1), (i, j) ? A add (i+ 1, j)
(i, j) ? A, (i+ 1, j) ? A del (i, j)
(i, j) ? A, (i+ 1, j) ? A del (i+ 1, j)
nullEA(i), nullEA(i+ 1) add (i, j)
add (i+ 1, j)
(i+ 1, j + 1) ? A add (i, j)
nullEA(i), nullFA(j),
(i, j) ? A, nullEA(i+ 1), add (i+ 1, j + 1)
nullFA(j + 1)
(i, j) ? A, (i+ 1, j) ? A, add (i, j + 1)
(i+ 1, j + 1) ? A
(i, j) ? A, (i, j + 1) ? A, add (i+ 1, j)
(i+ 1, j + 1) ? A
(i? 1, j) ? A, (i+ 1, j) ? A add (i, j)
nullEA(i)
(i, j ? 1) ? A, (i, j + 1) ? A add (i, j)
nullFA(j)
Table 3: Templates for Handling Multi-Word Corre-
spondences in a Given Alignment A
Condition Action
(i, j) ? A del (i, j)
nullEA(i), nullFA(j) add (i, j)
Table 4: Templates for Correcting One-to-One Cor-
respondences in a Given Alignment A
alignment. Table 3 lists the templates for correct-
ing alignment links in multi-word correspondences.
The first five templates handle (ei ? fjfj+1) cor-
respondences, the next five handle (eiei+1 ? fj)
correspondences, the next four handle (eiei+1 ?
fjfj+1) correspondences, and the final two handle
(ei?1eiei+1 ? fj) and (ei ? fj?1fjfj+1) corre-
spondences.
The alignment rules given above may introduce
errors that require additional cleanup. Thus, we in-
troduce two simple templates (shown in Table 4) to
accommodate the deletion or addition of links be-
tween a single pair of words.
4.3 Instantiation of Templates
ALP starts with a set of templates and an initial
alignment and attempts to instantiate the templates
during the learning process. The templates can be
instantiated using two methods: Simple (a word is
instantiated with a specific feature) or Generalized (a
word is instantiated using a special keyword any-
188
thing).
ALP requires only a small amount of manually
aligned data for this process?a major strength of
the system. However, if we were to instantiate the
templates with the actual words of the manual align-
ment, the frequency counts (from such a small data
set) would not be high enough to derive reasonable
generalizations. Thus, ALP adds new links based on
linguistic features of words, rather than the words
themselves. Using these features is what sets ALP
apart from systems like the RA approach. Specifi-
cally, three features are used to instantiate the tem-
plates:
? POS tags on both sides: We assign POS
tags using the MXPOST tagger (Ratnaparkhi,
1996) for English and Chinese, and Connexor
for Spanish.
? Dependency relations: ALP utilizes depen-
dencies for a better generalization?if a depen-
dency parser is available in either language.
In our experiments, we used a dependency
parser only in English (a version of the Collins
parser (Collins, 1997) that has been adapted
for building dependencies) but not in the other
language.
? A set of closed-class words: We use 16 dif-
ferent classes, 9 of which are different seman-
tic verb classes while the other 7 are function
words, prepositions, and complementizers.5
If both POS tags and dependency relations are
available, they can be used together to instantiate
the templates. That is, a word can be instantiated
in a TBL template with: (1) a POS tag (e.g., Noun,
Adj); (2) a relation (e.g., Subj, Obj); (3) a parameter
class (e.g., Change of State); or (4) different subsets
of (1)?(3). We also employ a more generalized form
of instantiation, where words in the templates may
match the keyword anything.
4.4 Best Rule Selection
The rules are selected using two different metrics:
The accuracy of the rule or the overall impact of the
application of the rule on the entire data.
Two different mechanisms may be used for select-
ing the best rule after generating all possible instan-
tiations of templates:
5These are based on the parameter classes of (Dorr et al,
2002).
1. Rule Accuracy: The goal is to minimize the
errors introduced by the application of a trans-
formation rule. To measure accuracy of a rule
r, we use good(r)?2?bad(r), where good(r)
is the number of alignment links that are cor-
rected by the rule, and bad(r) is the number of
incorrect alignment links produced.
2. Overall impact on the training data: The ac-
curacy mechanism (above) is useful for bias-
ing the system toward higher precision. How-
ever, if the overall system is evaluated using a
metric other than precision (e.g., recall), the
accuracy mechanism may not guarantee that
the best rule is chosen at each step. Thus, we
choose the best rule according to the evalua-
tion metric to be used for the overall system.
5 Experiments and Results
This section describes our evaluation of ALP vari-
ants using different combinations of settings of the
four parameters described above. The two language
pairs examined are English-Spanish and English-
Chinese.
5.1 Evaluation Metrics
Let A be the set of alignment links for a set of sen-
tences. We take S to be the set of sure alignment
links and P be the set of probable alignment links
(in the gold standard) for the same set of sentences.
Precision (Pr), recall (Rc) and alignment error rate
(AER) are defined as follows:
Pr =
|A ? P |
|A|
Rc =
|A ? S|
|S|
AER = 1?
|A ? S|+ |A ? P |
|A|+ |S|
A manually aligned corpus is used as our gold stan-
dard. For English-Spanish data, the manual an-
notation was done by a bilingual English-Spanish
speaker. Every link in the English-Spanish gold
standard is considered a sure alignment link.
For English-Chinese, we used 2002 NIST MT
evaluation test set, and each sentence pair was
aligned by two native Chinese speakers who are flu-
ent in English. Each alignment link appearing in
both annotations was considered a sure link, and
189
links appearing in only one set were judged as prob-
able. The annotators were not aware of the specifics
of our approach.
5.2 Evaluation Data
We evaluated ALP using 5-fold cross validation on
two different data sets:
1. A set of 199 English-Spanish sentence pairs
(nearly 5K words on each side) from a mixed
corpus (UN + Bible + FBIS).
2. A set of 491 English-Chinese sentence pairs
(nearly 13K words on each side) from 2002
NIST MT evaluation test set.
We divided the pairs of sentences randomly into 5
groups. Then, for each fold, we used 4 groups as the
ground truth (for training), and used the other group
as our gold standard (for evaluation). This process
was repeated 5 times so that each sentence pair was
tested exactly once. We computed precision, recall
and error rate on the entire set for each data set.6
For an initial alignment, we used GIZA++ in both
directions (E-to-F and F -to-E, where F is either
Chinese (C) or Spanish (S)), and also two different
combined alignments: intersection of E-to-F and
F -to-E; and RA using a heuristic combination ap-
proach called grow-diag-final (Koehn et al, 2003).
For the English-Spanish experiments, GIZA++
was trained on 48K sentence pairs from a mixed
corpus (UN + Bible + FBIS), with nearly 1.2M of
words on each side, using 10 iterations of Model 1,
5 iterations of HMM and 5 iterations of Model 4.
For the English-Chinese experiments, we used 107K
sentence pairs from FBIS corpus (nearly 4.1M En-
glish and 3.3M Chinese words) to train GIZA++, us-
ing 5 iterations of Model 1, 5 iterations of HMM, 3
iterations of Model 3, and 3 iterations of Model 4.
5.3 Results for English-Spanish
For our initial alignments we used: (1) Intersec-
tion of GIZA++ English-to-Spanish and Spanish-
to-English; (2) GIZA++ English-to-Spanish; (3)
GIZA++ Spanish-to-English; and (4) RA. Of these,
RA is the best, with an error rate of 21.2%. For ease
of comparison, the RA score appears in all result ta-
bles below.
6The number of alignment links varies over each fold.
Therefore, we chose to evaluate all data at once instead of eval-
uating on each fold and then averaging.
Tables 5?7 compare ALP to each of these four
alignments using different settings of 4 parameters:
ALP[IA, T, I, BRS], where IA is the initial align-
ment, T is the set of templates, I is the instantia-
tion method, and BRS is the metric for the best rule
selection at each iteration. TE is the set of expan-
sion templates from Table 1, TD is the set of dele-
tion templates from Table 2, and TMW is the set of
multi-word templates from Table 3 (supplemented
with templates from Table 4).
As mentioned in Section 4.3, we use two instanti-
ation methods: (1) simple instantiation (sim), where
the words are instantiated using a specific POS tag,
relation, parameter class or combination of those;
and (2) generalized instantiation (gen), where the
words can be instantiated using the keyword any-
thing. Two different metrics are used to select the
best rule: The accuracy of the rule (acc) and the
AER on the entire training data after applying the
rule (aer).7
We performed statistical significance tests using
two-tailed paired t-tests. Unless otherwise indicated,
the differences between ALP and initial alignments
(for all ALP variations and all initial alignments)
were found to be statistically significant within the
95% confidence interval. Moreover, the differences
among ALP variations themselves were statistically
significant within 95% confidence interval.
Using Intersection as Initial Alignment We ran
ALP using the intersection of GIZA++ (E-to-S)
and GIZA++(S-to-E) alignments as the initial align-
ment in two different ways: (1) With TE using the
union of the unidirectional GIZA++ alignments as
the validation set, and (2) with TD and TMW applied
one after another. Table 5 presents the precision, re-
call and AER results.
Alignments Pr Rc AER
Intersection (Int) 98.2 59.6 25.9
ALP[Int, TE , gen, aer] 90.9 69.9 21.0
ALP[Int, (TD, TMW ), gen, aer] 88.8 72.3 20.3
RA 83.8 74.4 21.2
Table 5: ALP Results Using GIZA++ Intersection as
Initial Alignment for English-Spanish
Using the expansion templates (TE) against a val-
7We use only sure alignment links as the ground truth to
learn rules inside ALP. Therefore, AER here refers to the AER
of sure alignment links.
190
Alignments Pr Rc AER
E-to-S 87.0 67.0 24.3
ALP[E-to-S,(TD, TMW ), gen, aer] 85.6 76.4 19.3
S-to-E 88.0 67.5 23.6
ALP[S-to-E,(TD, TMW ), gen, aer] 87.1 76.7 18.4
RA 83.8 74.4 21.2
Table 6: ALP Results Using GIZA++ (Each Direc-
tion) as Initial Alignment for English-Spanish
idation set produced results comparable to the RA
method. The major difference is that ALP resulted
in a much higher precision but in a lower recall be-
cause ALP is more selective in adding a new link
during the expansion stage. This difference is due to
the additional constraints provided by word features.
The version of ALP that applies deletion (TD) and
multi-word (TMW ) templates sequentially achieves
lower recall but higher precision than RA. In the best
case, ALP achieves a statistically significant rela-
tive reduction of 21.6% in AER over the Intersection
alignment. When compared to RA, ALP achieves a
lower AER but the difference is not significant.
Using Unidirectional GIZA++ Alignments as Ini-
tial Alignment In a second set of experiments, we
applied ALP to the unidirectional GIZA++ align-
ments, using deletion (TD) and multi-word (TMW )
templates, generalized instantiation, and AER for
the best rule selection. Table 6 presents the preci-
sion, recall and AER results.
For both directions, ALP achieves a lower preci-
sion but much higher recall than that of the initial
unidirectional alignment. Overall, there was a rela-
tive reduction of 20.6?22.0% in AER. When com-
pared to RA, the version of ALP that uses unidirec-
tional GIZA++ alignments brings about significant
reductions in AER: 9.0% relative reduction in one
direction and 13.2% relative reduction in the other
direction.
Using RA as Initial Alignment In a third experi-
ment, we compared RA with variations of ALP us-
ing RA as the initial alignment. We used the tem-
plates in two different ways: (1) with a combination
of TD and TMW (i.e., TD ?TMW ), and (2) with two
consecutive runs of ALP, first with TD and then with
TMW using the output of the first run as the initial
annotation in the second run (i.e., TD, TMW ). Ta-
ble 7 presents precision, recall and AER results, us-
ing different methods for template instantiation and
Alignments Pr Rc AER
ALP[RA, (TD, TMW ), sim, acc] 87.8 77.7 17.6
ALP[RA, (TD, TMW ), sim, aer] 87.9 79.0 16.8
ALP[RA, (TD ? TMW ), gen, aer] 86.2 80.0 17.0
ALP[RA, (TD, TMW ), gen, aer] 86.9 80.5 16.4
RA 83.8 74.4 21.2
Table 7: ALP Results Using RA as Initial Alignment
for English-Spanish
best rule selection.
The results indicate that using AER is better than
using accuracy for choosing the best rule. Using
generalized instantiation instead of simple instantia-
tion results in a better AER. Running ALP with dele-
tion (TD) templates followed by multi-word (TMW )
templates results in a lower AER than running ALP
only once with combined templates.
The highest performing variant of ALP, shown
in the fourth line of the table, uses RA as the ini-
tial alignment, template sets TD, TMW , general-
ized instantiation, and AER for best rule selection.
This variant is significantly better than RA, with a
22.6% relative reduction in AER. When compared
to the unidirectional alignments (E-to-S and S-to-
E) given in Table 6, this variant of ALP yields nearly
the same precision (around 87.0%) but a 19.2% rel-
ative improvement in recall. The overall relative re-
duction in AER is 30.5% in the S-to-E direction and
32.5% in the E-to-S direction.
5.4 Results for English-Chinese
Our experiments for English-Chinese were designed
with a similar structure to that of English-Spanish,
i.e., the same four initial alignments. Once again,
RA performs the best out of these initial alignments,
with an error rate of 29.7%. The results of the ini-
tial alignments, and variations of ALP based on dif-
ferent initial alignments are shown in Table 8. For
brevity, we include only the ALP parameter settings
resulting in the best configurations from the English-
Spanish experiments. For learning rules from the
templates, we used only the sure alignment links as
the ground truth while learning rules inside ALP.
On the English-Chinese data, ALP yields signif-
icantly lower error rates with respect to the initial
alignments. When ALP is run with the intersection
of two GIZA++ alignments, the relative reduction
is 5.4% in AER. When ALP is run with E-to-C as
initial alignment, the relative reduction in AER is
13.4%. For the other direction, ALP produces a rel-
191
Alignments Pr Rc AER
Intersection (Int) 94.8 53.6 31.2
ALP[Int, (TD, TMW ), gen, aer] 91.7 56.8 29.5
E-to-C 70.4 68.3 30.7
ALP[E-to-C,(TD, TMW ), gen, aer] 79.1 68.1 26.6
C-to-E 66.0 69.8 32.2
ALP[C-to-E,(TD, TMW ), gen, aer] 83.3 66.0 26.2
RA 61.9 82.6 29.7
ALP[RA,(TD, TMW ), gen, aer] 82.1 72.7 22.8
Table 8: ALP Results Using Different Initial Align-
ments for English-Chinese
ative reduction of 18.6% in AER. Finally, when RA
is given to ALP as an initial alignment, ALP results
in a relative reduction of 23.2% in AER. When com-
pared to RA, all variations of ALP, except the one
starting with the intersection, yield statistically sig-
nificantly lower AER. Another important finding is
that ALP yields significantly higher precision than
the initial alignments but usually lower recall.
6 Conclusion
We have presented ALP, a new approach that re-
fines alignments by identifying the types of errors
made by existing alignment systems and correcting
them. Our approach adapts TBL to the problem of
word-level alignment by examining word features
as well as neighboring links. We use POS tags,
closed-class words in both languages, and depen-
dency relations in one language to classify the er-
rors made by the initial alignment system. We show
that ALP yields at least a 22.6% relative reduction
on English-Spanish data and 23.2% relative reduc-
tion on English-Chinese data in alignment error rate
over that of the best performing system.
We should note that ALP is not a stand-alone
word alignment system but a supervised learning ap-
proach to improve already existing alignment sys-
tems. ALP takes advantage of clustering of align-
ment links to project new links given a reasonable
initial alignment. We have shown that ALP is quite
successful in projecting alignment links for two dif-
ferent languages?Spanish and Chinese.
Statistical alignment systems are more successful
with increasing amount of training data. Whether
ALP improves the statistical alignment systems
when they are trained on more data is an interesting
research problem, which we plan to tackle in future.
Finally, we will evaluate the improved alignments
in the context of an end-to-end application, such as
machine translation.
Acknowledgments This work has been supported, in
part, by ONR MURI Contract FCPO.810548265, Coopera-
tive Agreement DAAD190320020, and NSF ITR Grant IIS-
0326553.
References
Necip F. Ayan, Bonnie J. Dorr, and Nizar Habash. 2004. Multi-
Align: Combining linguistic and statistical techniques to
improve alignments for adaptable MT. In Proceedings of
AMTA?2004, pages 17?26.
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?565.
Eric Brill. 1996. Learning to parse with transformations. In
Recent Advances in Parsing Technology. Kluwer Academic
Publishers.
Peter F. Brown, Stephan A. Della-Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A probability model
to improve word alignment. In Proceedings of ACL?2003,
pages 88?95.
Micheal Collins. 1997. Three generative lexicalized models for
statistical parsing. In Proceedings of ACL?1997.
Mona Diab and Philip Resnik. 2002. An unsupervised method
for word sense tagging using parallel corpora. In Proceed-
ings of ACL?2002.
Bonnie J. Dorr, Lisa Pearl, Rebecca Hwa, and Nizar Habash.
2002. DUSTer: A method for unraveling cross-language di-
vergences for statistical word?level alignment. In Proceed-
ings of AMTA?2002.
Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL/HLT?2003.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear models
for word alignment. In Proceedings of ACL?2005.
I. Dan Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221?249.
Franz J. Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings of ACL?2000, pages 440?
447.
Franz J. Och and Hermann Ney. 2003. A systematic compari-
son of various statistical alignment models. Computational
Linguistics, 29(1):9?51, March.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of EMNLP?1996.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Man-
ning. 2002. Extensions to HMM-based statistical word
alignment models. In Proceedings of EMNLP?2002, pages
87?94.
Stefan Vogel, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
Proceedings of COLING?1996, pages 836?841.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001.
Inducing multilingual text analysis tools via robust projec-
tion across aligned corpora. In Proceedings of HLT?2001,
pages 109?116.
192
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 779?786, Vancouver, October 2005. c?2005 Association for Computational Linguistics
The Hiero Machine Translation System:
Extensions, Evaluation, and Analysis
David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, Michael Subotin
Institute for Advanced Computer Studies (UMIACS)
University of Maryland, College Park, MD 20742, USA
{dchiang,alopez,nmadnani,christof,resnik,msubotin}@umiacs.umd.edu
Abstract
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has been largely absent from the best
performing machine translation systems in recent
community-wide evaluations. In this paper, we dis-
cuss a new hierarchical phrase-based statistical ma-
chine translation system (Chiang, 2005), present-
ing recent extensions to the original proposal, new
evaluation results in a community-wide evaluation,
and a novel technique for fine-grained comparative
analysis of MT systems.
1 Introduction
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has, for the last several years, been
absent from the best performing machine transla-
tion systems in community-wide evaluations. Statis-
tical phrase-based models (e.g. (Och and Ney, 2004;
Koehn et al, 2003; Marcu andWong, 2002)) charac-
terize a source sentence f as a flat partition of non-
overlapping subsequences, or ?phrases?, f?1 ? ? ? f?J ,
and the process of translation involves selecting tar-
get phrases e?i corresponding to the f? j and modify-
ing their sequential order. The need for some way
to model aspects of syntactic behavior, such as the
tendency of constituents to move together as a unit,
is widely recognized?the role of syntactic units is
well attested in recent systematic studies of trans-
lation (Fox, 2002; Hwa et al, 2002; Koehn and
Knight, 2003), and their absence in phrase-based
models is quite evident when looking at MT system
output. Nonetheless, attempts to incorporate richer
linguistic features have generally met with little suc-
cess (Och et al, 2004a).
Chiang (2005) introduces Hiero, a hierarchical
phrase-based model for statistical machine transla-
tion. Hiero extends the standard, non-hierarchical
notion of ?phrases? to include nonterminal sym-
bols, which permits it to capture both word-level and
phrase-level reorderings within the same framework.
The model has the formal structure of a synchronous
CFG, but it does not make any commitment to a
linguistically relevant analysis, and it does not re-
quire syntactically annotated training data. Chiang
(2005) reported significant performance improve-
ments in Chinese-English translation as compared
with Pharaoh, a state-of-the-art phrase-based system
(Koehn, 2004).
In Section 2, we review the essential elements
of Hiero. In Section 3 we describe extensions to
this system, including new features involving named
entities and numbers and support for a fourfold
scale-up in training set size. Section 4 presents new
evaluation results for Chinese-English as well as
Arabic-English translation, obtained in the context
of the 2005 NISTMT Eval exercise. In Section 5, we
introduce a novel technique for fine-grained com-
parative analysis of MT systems, which we em-
ploy in analyzing differences between Hiero?s and
Pharaoh?s translations.
2 Hiero
Hiero is a stochastic synchronous CFG, whose pro-
ductions are extracted automatically from unanno-
tated parallel texts, and whose rule probabilities
form a log-linear model learned by minimum-error-
rate training; together with a modified CKY beam-
search decoder (similar to that of Wu (1996)). We
describe these components in brief below.
779
S ? ?S 1 X 2 ,S 1 X 2 ?
S ? ?X 1 ,X 1 ?
X ? ?yu X 1 you X 2 , have X 2 with X 1 ?
X ? ?X 1 de X 2 , the X 2 that X 1 ?
X ? ?X 1 zhiyi, one of X 1 ?
X ? ?Aozhou,Australia?
X ? ?shi, is?
X ? ?shaoshu guojia, few countries?
X ? ?bangjiao, diplomatic relations?
X ? ?Bei Han,North Korea?
Figure 1: Example synchronous CFG
2.1 Grammar
A synchronous CFG or syntax-directed transduction
grammar (Lewis and Stearns, 1968) consists of pairs
of CFG rules with aligned nonterminal symbols. We
denote this alignment by coindexation with boxed
numbers (Figure 1). A derivation starts with a pair
of aligned start symbols, and proceeds by rewrit-
ing pairs of aligned nonterminal symbols using the
paired rules (Figure 2).
Training begins with phrase pairs, obtained as by
Och, Koehn, and others: GIZA++ (Och and Ney,
2000) is used to obtain one-to-many word align-
ments in both directions, which are combined into a
single set of refined alignments using the ?final-and?
method of Koehn et al (2003); then those pairs of
substrings that are exclusively aligned to each other
are extracted as phrase pairs.
Then, synchronous CFG rules are constructed
out of the initial phrase pairs by subtraction: ev-
ery phrase pair ? f? , e?? becomes a rule X ? ? f? , e??,
and a phrase pair ? f? , e?? can be subtracted from a
rule X ? ??1 f??2, ?1e??2? to form a new rule X ?
??1X i ?2, ?1X i ?2?, where i is an index not already
used. Various filters are also applied to reduce the
number of extracted rules. Since one of these filters
restricts the number of nonterminal symbols to two,
our extracted grammar is equivalent to an inversion
transduction grammar (Wu, 1997).
2.2 Model
The model is a log-linear model (Och and Ney,
2002) over synchronous CFG derivations. The
weight of a derivation is PLM(e)?LM , the weighted
language model probability, multiplied by the prod-
uct of the weights of the rules used in the derivation.
The weight of each rule is, in turn:
(1) w(X ? ??, ??) =
?
i
?i(X ? ??, ??)?i
where the ?i are features defined on rules. The ba-
sic model uses the following features, analogous to
Pharaoh?s default feature set:
? P(? | ?) and P(? | ?)
? the lexical weights Pw(? | ?) and Pw(? | ?)
(Koehn et al, 2003);1
? a phrase penalty exp(1);
? a word penalty exp(l), where l is the number of
terminals in ?.
The exceptions to the above are the two ?glue?
rules, which are the rules with left-hand side S in
Figure 1. The second has weight one, and the first
has weight w(S ? ?S 1 X 2 ,S 1 X 2 ?) = exp(??g),
the idea being that parameter ?g controls the model?s
preference for hierarchical phrases over serial com-
bination of phrases.
Phrase translation probabilities are estimated by
relative-frequency estimation. Since the extraction
process does not generate a unique derivation for
each training sentence pair, a distribution over pos-
sible derivations is hypothesized, which gives uni-
form weight to all initial phrases extracted from a
sentence pair and uniform weight to all rules formed
out of an initial phrase. This distribution is then used
to estimate the phrase translation probabilities.
The lexical-weighting features are estimated us-
ing a method similar to that of Koehn et al (2003).
The language model is a trigram model with mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998), trained using the SRI-LM toolkit (Stolcke,
2002).
1This feature uses word alignment information, which is dis-
carded in the final grammar. If a rule occurs in training with
more than one possible word alignment, Koehn et al take the
maximum lexical weight; Hiero uses a weighted average.
780
?S 1 ,S 1 ? ? ?S 2 X 3 ,S 2 X 3 ?
? ?S 4 X 5 X 3 ,S 4 X 5 X 3 ?
? ?X 6 X 5 X 3 ,X 6 X 5 X 3 ?
? ?Aozhou X 5 X 3 ,Australia X 5 X 3 ?
? ?Aozhou shi X 3 ,Australia is X 3 ?
? ?Aozhou shi X 7 zhiyi,Australia is one of X 7 ?
? ?Aozhou shi X 8 de X 9 zhiyi,Australia is one of the X 9 that X 8 ?
? ?Aozhou shi yu X 1 you X 2 de X 9 zhiyi,Australia is one of the X 9 that have X 2 with X 1 ?
Figure 2: Example partial derivation of a synchronous CFG.
The feature weights are learned by maximizing
the BLEU score (Papineni et al, 2002) on held-out
data, using minimum-error-rate training (Och, 2003)
as implemented by Koehn. The implementation was
slightly modified to ensure that the BLEU scoring
matches NIST?s definition and that hypotheses in
the n-best lists are merged when they have the same
translation and the same feature vector.
3 Extensions
In this section we describe our extensions to the base
Hiero system that improve its performance signif-
icantly. First, we describe the addition of two new
features to the Chinese model, in a manner similar
to that of Och et al (2004b); then we describe how
we scaled the system up to a much larger training
set.
3.1 New features
The LDC Chinese-English named entity lists (900k
entries) are a potentially valuable resource, but
previous experiments have suggested that simply
adding them to the training data does not help
(Vogel et al, 2003). Instead, we placed them in
a supplementary phrase-translation table, giving
greater weight to phrases that occurred less fre-
quently in the primary training data. For each en-
try ? f , {e1, . . . , en}?, we counted the number of times
c( f ) that f appeared in the primary training data,
and assigned the entry the weight 1c( f )+1 , which
was then distributed evenly among the supplemen-
tary phrase pairs {? f , ei?}. We then created a new
model feature for named entities. When one of these
supplementary phrase pairs was used in transla-
tion, its feature value for the named-entity feature
was the weight defined above, and its value in the
other phrase-translation and lexical-weighting fea-
tures was zero. Since these scores belonged to a sep-
arate feature from the primary translation probabili-
ties, they could be reweighted independently during
minimum-error-rate training.
Similarly, to process Chinese numbers and dates,
we wrote a rule-based Chinese number/date transla-
tor, and created a new model feature for it. Again,
the weight given to this module was optimized
during minimum-error-rate training. In some cases
we wrote the rules to provide multiple uniformly-
weighted English translations for a Chinese phrase
(for example,k? (bari) could become ?the 8th? or
?on the 8th?), allowing the language model to decide
between the options.
3.2 Scaling up training
Chiang (2005) reports on experiments in Chinese-
English translation using a model trained on
7.2M+9.2M words of parallel data.2 For the NIST
MT Eval 2005 large training condition, consider-
ably more data than this is allowable. We chose
to use only newswire data, plus data from Sino-
rama, a Taiwanese news magazine.3 This amounts
to almost 30M+30M words. Scaling to this set re-
quired reducing the initial limit on phrase lengths,
previously fixed at 10, to avoid explosive growth of
2Here and below, the notation ?X + Y words? denotes X
words of foreign text and Y words of English text.
3From Sinorama, only data from 1991 and later were used,
as articles prior to that were translated quite loosely.
781
the extracted grammar. However, since longer initial
phrases can be beneficial for translation accuracy,
we adopted a variable length limit: 10 for the FBIS
corpus and other mainland newswire sources, and 7
for the HK News corpus and Sinorama. (During de-
coding, limits of up to 15 were sometimes used; in
principle these limits should all be the same, but in
practice it is preferable to tune them separately.)
For Arabic-English translation, we used the ba-
sic Hiero model, without special features for named
entities or numbers/dates. We again used only the
newswire portions of the allowable training data; we
also excluded the Ummah data, as the translations
were found to be quite loose. Since this amounted
to only about 1.5M+1.5M words, we used a higher
initial phrase limit of 15 during both training and de-
coding.
4 Evaluation
Figure 1 shows the performance of several systems
on NIST MT Eval 2003 Chinese test data: Pharaoh
(2004 version), trained only on the FBIS data; Hi-
ero, with various combinations of the new features
and the larger training data.4 This table also shows
Hiero?s performance on the NIST 2005 MT evalua-
tion task.5 The metric here is case-sensitive BLEU.6
Figure 2 shows the performance of two systems
on Arabic in the NIST 2005 MT Evaluation task:
DC, a phrase-based decoder for a model trained by
Pharaoh, and Hiero.
5 Analysis
Over the last few years, several automatic metrics
for machine translation evaluation have been intro-
duced, largely to reduce the human cost of itera-
tive system evaluation during the development cy-
cle (Lin and Och, 2004; Melamed et al, 2003; Pap-
ineni et al, 2002). All are predicated on the concept
4The third line, corresponding to the model without new fea-
tures trained on the larger data, may be slightly depressed be-
cause the feature weights from the fourth line were used instead
of doing minimum-error-rate training specially for this model.
5Full results are available at http://www.nist.gov/
speech/tests/summaries/2005/mt05.htm. For this test, a
phrase length limit of 15 was used during decoding.
6For this task, the translation output was uppercased using
the SRI-LM toolkit: essentially, it was decoded again using
an HMM whose states and transitions are a trigram language
model of cased English, and whose emission probabilities are
reversed, i.e., probability of cased word given lowercased word.
System Features Train MT03 MT05
Pharaoh standard FBIS 0.268
Hiero standard FBIS 0.288
Hiero standard full 0.329
Hiero +nums, names full 0.339 0.300
Table 1: Chinese results. (BLEU-4; MT03 case-
insensitive, MT05 case-sensitive)
System Train MT05
DC full 0.399
Hiero full 0.450
Table 2: Arabic results. (BLEU-4; MT03 case-
insensitive, MT05 scores case-sensitive.
of n-gram matching between the sentence hypothe-
sized by the translation system and one or more ref-
erence translations?that is, human translations for
the test sentence. Although the motivations and for-
mulae underlying these metrics are all different, ul-
timately they all produce a single number represent-
ing the ?goodness? of the MT system output over a
set of reference documents. This facility is valuable
in determining whether a given system modification
has a positive impact on overall translation perfor-
mance. However, the metrics are all holistic. They
provide no insight into the specific competencies or
weaknesses of one system relative to another.
Ideally, we would like to use automatic methods
to provide immediate diagnostic information about
the translation output?what the system does well,
and what it does poorly. At the most general level,
we want to know how our system performs on the
two most basic problems in translation?word trans-
lation and reordering. Unigram precision and recall
statistics tell us something about the performance of
an MT system?s internal translation dictionaries, but
nothing about reordering. It is thought that higher or-
der n-grams correlate with the reordering accuracy
of MT systems, but this is again a holistic metric.
What we would really like to know is howwell the
system is able to capture systematic reordering pat-
terns in the input, which ones it is successful with,
and which ones it has difficulty with. Word n-grams
are little help here: they are too many, too sparse, and
it is difficult to discern general patterns from them.
782
5.1 A New Analysis Method
In developing a new analysis method, we are moti-
vated in part by recent studies suggesting that word
reorderings follow general patterns with respect to
syntax, although there remains a high degree of flex-
ibility (Fox, 2002; Hwa et al, 2002). This suggests
that in a comparative analysis of two MT systems, it
may be useful to look for syntactic patterns that one
system captures well in the target language and the
other does not, using a syntax based metric.
We propose to summarize reordering patterns us-
ing part-of-speech sequences. Unfortunately, recent
work has shown that applying statistical parsers to
ungrammatical MT output is unreliable at best, with
the parser often assigning unreasonable probabili-
ties and incongruent structure (Yamada and Knight,
2002; Och et al, 2004a). Anticipating that this
would be equally problematic for part-of-speech
tagging, we make the conservative choice to apply
annotation only to the reference corpus. Word n-
gram correspondences with a reference translation
are used to infer the part-of-speech tags for words in
the system output.
First, we tagged the reference corpus with parts
of speech. We used MXPOST (Ratnaparkhi, 1996),
and in order to discover more general patterns, we
map the tag set down after tagging, e.g. NN, NNP,
NNPS and NNS all map to NN. Second, we com-
puted the frequency freq(ti . . . t j) of every possible
tag sequence ti . . . t j in the reference corpus. Third,
we computed the correspondence between each hy-
pothesis sentence and each of its corresponding ref-
erence sentences using an approximation to max-
imum matching (Melamed et al, 2003). This al-
gorithm provides a list of runs or contiguous se-
quences of words ei . . . e j in the reference that are
also present in the hypothesis. (Note that runs are
order-sensitive.) Fourth, for each recalled n-gram
ei . . . e j, we looked up the associated tag sequence
ti . . . t j and incremented a counter recalled(ti . . . t j).
Finally, we computed the recall of tag patterns,
R(ti . . . t j) = recalled(ti . . . t j)/freq(ti . . . t j), for all
patterns in the corpus.
By examining examples of these tag sequences in
the reference corpus and their hypothesized trans-
lations, we expect to gain some insight into the
comparative strengths and weaknesses of the MT
systems? reordering models. (An interactive plat-
form for this analysis is demonstrated by Lopez and
Resnik (2005).)
5.2 Chinese
We performed tag sequence analysis on the Hiero
and Pharaoh systems trained on the FBIS data only.
Table 3 shows those n-grams for which Hiero and
Pharaoh?s recall differed significantly (p < 0.01).
The numbers shown are the ratio of Hiero?s recall
to Pharaoh?s. Note that the n-grams on which Hi-
ero had better recall are dominated by fragments of
prepositional phrases (in the Penn Treebank tagset,
prepositions are tagged IN or TO).
Our hypothesis is that Hiero produces English PPs
better because many of them are translated from
Chinese phrases which have an NP modifying an NP
to its right, often connected with the particle? (de).
These are often translated into English as PPs, which
modify the NP to the left. A correct translation, then,
would have to reorder the two NPs. Notice in the ta-
ble that Hiero recalls proportionally more n-grams
as n increases, corroborating the intuition that Hiero
should be better at longer-distance reorderings.
Investigating this hypothesis qualitatively, we in-
spected the first five occurrences of the n-grams of
the first type on the list (JJ NN IN DT NN). Of
these, we omit one example because both systems
recalled the n-gram correctly, and one because they
differed only in lexical choice (Hiero matched the
5-gram with one reference sentence, Pharaoh with
zero). The other three examples are shown below (H
= Hiero, P = Pharaoh):
(2) T?
UN
?h
security
?Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 119?124,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Shared Task: Statistical Machine Translation between European Languages
Philipp Koehn
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, UK
pkoehn@inf.ed.ac.uk
Christof Monz
UMIACS
University of Maryland
College Park, MD 20742, USA
christof@umiacs.umd.edu
Abstract
The ACL-2005 Workshop on Paral-
lel Texts hosted a shared task on
building statistical machine translation
systems for four European language
pairs: French?English, German?English,
Spanish?English, and Finnish?English.
Eleven groups participated in the event.
This paper describes the goals, the task
definition and resources, as well as results
and some analysis.
Statistical machine translation is currently the
dominant paradigm in machine translation research.
Annual competitions are held for Chinese?English
and Arabic?English by NIST (sponsored by the US
military funding agency DARPA), which creates a
forum to present and compare novel ideas and leads
to steady progress in the field.
One of the advantages of statistical machine trans-
lation is that the currently applied methods are fairly
language-independent. Building a new machine
translation system for a new language pair is not
much more than a matter of running a training pro-
cess on a training corpus of parallel text (a text in
one language paired with a translation in another).
It is therefore possible to hold a competition
where research groups have only a few weeks to
build machine translation systems for language pairs
that they have not previously worked on. We effec-
tively demonstrated this with our shared task. For in-
stance, seven teams built Finnish?English machine
translation systems, a language pair that was cer-
tainly not of their immediate concern before.
In contrast to the bigger NIST competition, we
wanted to keep the barrier of entry as low as possi-
ble. We provided not only training data from the Eu-
roparl corpus (Koehn, 2005), but also additional re-
sources: sentence and word alignments, the decoder
Pharaoh1 (Koehn, 2004b), and a language model,
so that participation was feasible even as a graduate
level class project.
Using about 15 million words of translated text,
participants were asked to build a phrase-based sta-
tistical machine translation system. The focus of
the task was to build a probabilistic phrase transla-
tion table, since most of the other resources were
provided ? for more on phrase-based statistical
machine translation, refer to Koehn et al (2003).
The participants? systems were compared by how
well they translated 2000 previously unseen test sen-
tences from the same domain.
The shared task operated within an extremely
short timeframe. The workshop and hence the
shared task was accepted on February 22, 2005 and
announced on March 3. The official test data was
made available on April 3, results were due one
week later. Despite this tight schedule, eleven re-
search groups participated and built a total of 32 ma-
chine translation systems for the four language pairs.
1 Goals
When setting up this competition, we were moti-
vated by a number of goals. We set out to:
Create a platform to demonstrate the effective-
ness of novel ideas: The research community is
easily balkanized, where different groups work on
1http://www.isi.edu/licensed-sw/pharaoh/
119
different data sets and under different conditions,
so that it becomes often hard to assess, how effec-
tive a novel method is. By creating an environment
with common test and training sets, language model,
preprocessing, and even decoder, the effect of other
model choices can be more easily demonstrated.
Work on new language pairs, new problems:
Different language pairs pose different challenges.
We picked Finnish?English and German?English
for the special problems of rich morphology, word
order, which are a challenge to current phrase-based
SMT methods.
Enable more researchers to get engaged in
SMT research: One of our main goals with provid-
ing as many resources as possible was to keep the
barrier of entry low. Participants could use the word
alignment and other resources and focus on phrase
extraction. We hoped to attract researchers that are
relatively new to the field. We were satisfied to learn
that many entries are by graduate students working
on their own.
Promote and create free resources: Academic
research thrives on freely available resources. The
field of statistical machine translation has been
blessed with a long tradition of freely available soft-
ware tools ? such as GIZA++ (Och and Ney, 2003)
? and parallel corpora ? such as the Canadian
Hansards2. Following this lead, we made word
alignments and a language model available for this
competition in addition to our previously published
resources (Europarl and Pharaoh). The competition
created resources as well. Most teams agreed to
share system output and their model files. You can
download them from the competition web site3.
Promote work on European language pairs:
Finally, we wanted to promote work on European
languages. The increasing economic and political
ties within the European Union create a huge need
for translation services. We would like to see re-
searchers rise to the challenge of creating high qual-
ity machine translation systems to fill these needs.
We are very grateful for the strong participation,
especially by researchers who are relatively new to
the field.
2http://www.isi.edu/natural-language/download/hansard/
3http://www.statmt.org/wpt05/mt-shared-task/
2 Rules of Engagement
We set up a machine translation competition for
four language pairs. We chose Spanish?English and
French?English, because many researchers would
be familiar with these languages. We chose
German?English for its special problems with word
order (such as nested constructions and split verb
groups) and morphology. Finally, we picked
Finnish?English for the rich agglutinative morphol-
ogy of Finnish.
Statistical machine translation systems are typi-
cally trained on sentence-aligned parallel corpora.
We selected Europarl4, a freely available parallel
corpus in eleven languages. In addition, we also
made a word alignment available, which was de-
rived using a variant of the current default method
for word alignment ? Och and Ney (2003)?s refined
method.
Figure 1 details some properties of the parallel
corpora. The training corpus is most of the Europarl
corpus, only the text of sessions from last quarter of
the year 2000 was reserved for testing. The corpus
has the size of roughly 15 million English words in
700,000 sentences ? these numbers differ for each of
the four parallel corpora due to the different number
of discarded sentences during sentence alignment
and after enforcing a 40 word length limit for sen-
tences.
The number of foreign words differs even more
dramatically. The effect of Finnish morphology
manifests itself in a low number of words (just over
11 million), but a high number of distinct words
(more than 5 times as many as in the English half).
The test corpus consists of 2000 sentences aligned
across all five languages. Note that the output of
each system is compared against the same English
references for all source languages. The number of
total words, distinct words, and words not seen in the
training data reflects again the morphology effect.
For researchers willing to create their own word
alignment, we suggested the use of GIZA++5, an
implementation of the IBM word-based machine
translation models, which also assisted the creation
of the provided word alignments.
We trained a language model on the English part
4http://www.statmt.org/europarl/
5http://www.fjoch.com/GIZA++.html
120
Spanish?English French?English Finnish?English German?English
Training corpus
Sentences 730,740 688,031 716,960 751,088
Source words 15,676,710 15,323,737 11,318,287 15,256,793
English words 15,222,105 13,808,104 15,492,903 16,052,269
Distinct source words 102,886 80,349 358,345 195,291
Distinct English words 64,123 61,627 64,662 65,889
Test corpus
Sentences 2,000
Source words 60,276 65,029 41,431 54,247
English words 57,945
Distinct source words 7,782 7,285 11,996 8,666
Distinct English words 6,054
Unseen source words 209 143 737 377
Figure 1: Properties of the Europarl training and test corpora used in the shared task
of the Europarl corpus using the SRI language mod-
eling toolkit (Stolke, 2002). Finally, we suggested
the use of Pharaoh (Koehn, 2004b), a phrase-based
machine translation decoder.
How well does this setup match the state of the
art? The MIT system using the Pharaoh decoder
(Koehn, 2004a) proved to be very competitive in
last year?s NIST evaluation. However, the field is
moving fast, and a number of steps help to improve
upon the provided baseline setup, e.g., larger lan-
guage models trained on general text (up to a bil-
lion words have been used), better reodering mod-
els (e.g., suggested by Tillman (2004) and Och
et al (2004)), better language-specific preprocessing
(Koehn and Knight, 2003) and restructuring (Collins
et al, 2005), additional feature functions such as
word class language models, and minimum error
rate training (Och, 2003) to optimize parameters.
Some of these steps (e.g., improved reorder-
ing models) go beyond the current capabilities of
Pharaoh. However, we are hopeful that freely avail-
able software continues to match or at least follow
closely the state of the art.
We announced the shared task on March 3, and
provided all the resources mentioned above (also a
development test corpus to track the quality of sys-
tems being developed). The test schedule called for
the translation of 2000 sentence for each of the four
language pairs in the week between April 3?10. We
allowed late submissions up to April 17.
3 Results
Eleven teams from eight institutions in Europe and
North America participated, see Figure 2 for a com-
plete list. The figure also indicates, if a team used
the Pharaoh decoder (eight teams), the provided lan-
guage model (seven teams) and the provided word
alignment (four did, three of those with additional
preprocessing or additional data).
Translation performance was measured using the
BLEU score (Papineni et al, 2002), which measures
n-gram overlap with a reference translation. In our
case, we only used a single reference translation,
since the test set was taken from a held-out portion
of the Europarl corpus. On the other hand we used a
relatively large number of test sentences to guaran-
tee that the BLEU results are stable despite the fact
that we used only one reference translation for each
sentence.
Shared tasks like this one, of course, bring out the
competitive spirit of participants and can draw criti-
cisms about being a horse race. From an outside per-
spective, however, it is far more interesting to learn
which methods and ideas proved to be successful,
than who won the competition.
Taking stock of the results ? see Figure 3 ? one
observes a very packed field at the top. While the
participants from the University of Washington pro-
duced the best translations for every single language
pair, the distance to many other participant scores
121
ID Team Pharaoh LM Word Al.
cmu-b Carnegie Mellon University, USA - Bing Zhao yes yes no
cmu-j Carnegie Mellon University, USA - Ying (Joy) Zhang yes yes no
glasgow University of Glasgow, UK yes yes yes+
nrc National Research Council, Canada no no no
rali University of Montreal / RALI, Canada yes yes no
saar Saarland University, Germany yes yes yes
uji University Jaume I, Spain yes yes yes+
upc-j Polytechnic University of Catalonia, Spain - Jesus Gimenez yes yes no
upc-m Polytechnic University of Catalonia, Spain - Marta Ruiz no no no
upc-r Polytechnic University of Catalonia, Spain - Rafael Banchs no no no
uw University of Washington, USA yes no yes+
Figure 2: The eleven participating teams: the table also lists, if the Pharaoh decoder, the provided language
model, and the provided word alignment was used (yes+ indicates additional preprocessing)
is within a BLEU percentage point or two. As one
might have expected, the scores are best for Spanish
and French, and worst for Finnish. Figure 4 shows
some typical output of the submitted systems.
The proceedings to the workshop include detailed
system descriptions of all participants. Novel phrase
extraction approaches were proposed, along with
better preprocessing, language modeling, rescoring,
and other ideas. We are certain that better perfor-
mance can be achieved by combining some of the
methods used by different participants.
And hence, we would like to pose the challenge to
the research community to build and test better sys-
tems using the provided resources. We will gladly
list additional results on the competition web site.
4 Survey
Following the end of the competition, we sent out a
questionnaire to the participants. One of the ques-
tions what they would like to see different in a po-
tential future competition.
We listed four potential changes: 70% of the re-
spondends checked translation from English, 50%
checked out of domain test data, 40% checked more
language pairs, 0% checked fewer language pairs.
Additional suggestions were: alternatives to the
BLEU scoring method (maybe human judgment by
participants themselves), transitive translation using
pivot languages, translation of resource-poor lan-
guages, and more time to prepare for the task.
5 Outlook
Given the short timeframe, one should view the sys-
tem performances (albeit very competitive with the
state of the art) as a baseline effort on the task of
open domain text translation between European lan-
guages.
We hope that future researchers will use the pro-
vided environment as a test bed for their machine
translation systems. We will continue to publish any
scores reported to us.
Since we placed much of the systems? output on-
line, the interested reader may be inspired to more
closely explore the quality and shortcomings. Even
some of the model files have been made available,
so it is even possible to download and install some
of the systems.
References
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine trans-
lation. In Proceedings of the 43rd Meeting
of the Association for Computational Linguistics
(ACL?05), Main Volume, pages 531?540, Ann Ar-
bor, Michigan.
Koehn, P. (2004a). The foundation for statistical ma-
chine translation at MIT. In Proceedings of Ma-
chine Translation Evaluation Workshop 2004.
Koehn, P. (2004b). Pharaoh: a beam search decoder
for statistical machine translation. In 6th Confer-
ence of the Association for Machine Translation
122
Spanish-English
System BLEU 1/2/3/4-gram precision (bp)
uw 30.95 64.1/36.6/24.0/16.3 (1.000)
upc-r 30.07 63.1/35.8/23.2/15.6 (1.000)
upc-m 29.84 63.9/35.5/23.0/15.5 (0.995)
nrc 29.08 62.7/34.9/22.2/14.7 (1.000)
rali 28.49 62.4/34.5/21.9/14.4 (0.992)
upc-j 28.13 61.5/33.8/21.4/14.1 (1.000)
saar 26.69 61.0/33.1/20.7/13.5 (0.973)
cmu-j 26.14 61.2/32.4/19.8/12.6 (0.986)
uji 21.65 59.7/27.8/15.2/8.7 (1.000)
French-English
System BLEU 1/2/3/4-gram precision (bp)
uw 30.27 64.8/36.8/23.8/16.0 (0.981)
upc-r 30.20 63.9/36.2/23.3/15.6 (0.998)
nrc 29.53 63.7/35.8/22.7/14.9 (0.997)
rali 28.89 62.6/34.7/22.0/14.6 (1.000)
cmu-b 27.65 63.1/34.0/20.9/13.3 (0.995)
cmu-j 26.71 61.9/33.0/20.3/13.1 (0.984)
saar 26.29 60.8/32.5/20.1/12.9 (0.982)
glasgow 23.01 57.3/28.0/16.7/10.5 (1.000)
uji 21.25 59.8/27.7/14.8/8.3 (1.000)
Finnish-English
System BLEU 1/2/3/4-gram precision (bp)
uw 22.01 59.0/28.6/16.1/9.4 (0.979)
nrc 20.95 57.8/27.2/14.8/8.4 (0.996)
upc-r 20.31 56.6/26.0/14.3/8.3 (0.993)
rali 18.87 55.2/24.7/13.1/7.1 (0.998)
saar 16.76 58.4/26.3/14.2/8.0 (0.819)
uji 13.79 60.0/23.2/10.8/5.3 (0.821)
cmu-j 12.66 53.9/21.7/10.7/5.7 (0.775)
German-English
System BLEU 1/2/3/4-gram precision (bp)
uw 24.77 62.2/31.8/18.8/11.7 (0.965)
upc-r 24.26 59.7/30.1/17.6/11.0 (1.000)
nrc 23.21 60.3/29.8/17.1/10.3 (0.979)
rali 22.91 58.9/29.0/16.8/10.3 (0.982)
saar 20.48 58.0/27.5/15.5/9.2 (0.938)
cmu-j 18.93 59.2/26.8/14.3/8.1 (0.914)
uji 18.89 59.3/25.5/13.0/7.2 (0.976)
Figure 3: The scores for the participating systems
(BLEU and its components n-gram-precision and
brevity penalty)
in the Americas, AMTA, Lecture Notes in Com-
puter Science. Springer.
Koehn, P. (2005). Europarl: A parallel corpus for
statistical machine translation. In MT Summit X
(submitted).
Koehn, P. and Knight, K. (2003). Empirical methods
for compound splitting. In Proceedings of Meet-
ing of the European Chapter of the Association of
Computational Linguistics (EACL).
Koehn, P., Och, F. J., and Marcu, D. (2003). Statisti-
cal phrase based translation. In Proceedings of the
Joint Conference on Human Language Technolo-
gies and the Annual Meeting of the North Ameri-
can Chapter of the Association of Computational
Linguistics (HLT-NAACL).
Och, F. J. (2003). Minimum error rate training for
statistical machine translation. In Proceedings
of the 41st Annual Meeting of the Association of
Computational Linguistics (ACL).
Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A.,
Yamada, K., Fraser, A., Kumar, S., Shen, L.,
Smith, D., Eng, K., Jain, V., Jin, Z., and Radev,
D. (2004). A smorgasbord of features for statis-
tical machine translation. In Proceedings of the
Joint Conference on Human Language Technolo-
gies and the Annual Meeting of the North Ameri-
can Chapter of the Association of Computational
Linguistics (HLT-NAACL).
Och, F. J. and Ney, H. (2003). A systematic compar-
ison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?52.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of the
40th Annual Meeting of the Association of Com-
putational Linguistics (ACL).
Stolke, A. (2002). SRILM - an extensible language
modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Process-
ing.
Tillman, C. (2004). A unigram orientation model
for statistical machine translation. In Proceed-
ings of the Joint Conference on Human Language
Technologies and the Annual Meeting of the North
American Chapter of the Association of Computa-
tional Linguistics (HLT-NAACL).
123
Reference
We know all too well that the present Treaties are inadequate and that the Union will need a better
and different structure in future , a more constitutional structure which clearly distinguishes the
powers of the Member States and those of the Union .
Input Spanish
Sabemos muy bien que los Tratados actuales no bastan y que , en el futuro , sera? necesario desarrol-
lar una estructura mejor y diferente para la Unio?n Europea , una estructura ma?s constitucional que
tambie?n deje bien claras cua?les son las competencias de los Estados miembros y cua?les pertenecen
a la Unio?n .
Best system (Spanish?English)
we all know very well that the current treaties are not enough and that , in the future , it will
be necessary to develop a structure better and different for the european union , a structure more
constitutional also make it clear what the competences of the member states and what belongs to
the union .
Worst System (Spanish?English)
we know very well that the current treaties not enough and that , in the future , will be necessary
develop a better structure and different to the european union , a structure more constitutional that
also be well clear the powers of the member states and what belong to the union .
Input French
Nous savons tre`s bien que les Traite?s actuels ne suffisent pas et qu ? il sera ne?cessaire a` l ? avenir
de de?velopper une structure plus efficace et diffe?rente pour l ? Union , une structure plus consti-
tutionnelle qui indique clairement quelles sont les compe?tences des e?tats membres et quelles sont
les compe?tences de l ? Union .
Best system (French?English)
we know very well that the current treaties are not enough and that it will be needed in the future
to develop a structure more effective and different for the union , a structure more constitutional
which clearly indicates what are the competence of member states and what are the powers of the
union .
Input Finnish
Tieda?mme oikein hyvin , etta? nykyiset perustamissopimukset eiva?t ole riitta?via? ja etta? tulevaisu-
udessa on tarpeen kehitta?a? unionille parempi ja toisenlainen rakenne , siis perustuslaillisempi
rakenne , jossa mys ilmaistaan selkea?mmin , mita? ja?senvaltioiden ja unionin toimivaltaan kuuluu
Best system (Finnish?English)
we know very well that the existing founding treaties do not need to be developed for the union
and a different structure , therefore perustuslaillisempi structure , which also expresses clearly
what the member states and the union ?s competence is not sufficient and that better in the future .
Input German
Uns ist sehr wohl bewusst , dass die geltenden Vertra?ge unzula?nglich sind und ku?nftig eine andere
, effizientere Struktur fu?r die Union entwickelt werden muss , na?mlich eine sta?rker konstitutionell
ausgepra?gte Struktur mit einer klaren Abgrenzung zwischen den Befugnissen der Mitgliedstaaten
und den Kompetenzen der Union .
Best system (German?English)
the union must be developed , with a major institutional structure with a clear demarcation
between the powers of the member states and the competences of the union is well aware that the
existing treaties are inadequate and in the future , a different , more efficient structure for us .
Figure 4: The first sentence of the test corpus and system translations
124
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 1?8, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Methodology for Extrinsic Evaluation of Text Summarization:
Does ROUGE Correlate?
Bonnie J. Dorr and Christof Monz and Stacy President and Richard Schwartz? and David Zajic
Department of Computer Science and UMIACS
University of Maryland
College Park, MD 20742
{bonnie,christof,stacypre,dmzajic}@umiacs.umd.edu
?BBN Technologies
9861 Broken Land Parkway
Columbia, Maryland 21046
schwartz@bbn.com
Abstract
This paper demonstrates the usefulness of sum-
maries in an extrinsic task of relevance judgment
based on a new method for measuring agree-
ment, Relevance-Prediction, which compares sub-
jects? judgments on summaries with their own judg-
ments on full text documents. We demonstrate that,
because this measure is more reliable than previ-
ous gold-standard measures, we are able to make
stronger statistical statements about the benefits of
summarization. We found positive correlations be-
tween ROUGE scores and two different summary
types, where only weak or negative correlations
were found using other agreement measures. How-
ever, we show that ROUGE may be sensitive to the
choice of summarization style. We discuss the im-
portance of these results and the implications for fu-
ture summarization evaluations.
1 Introduction
People often prefer to read a summary of a text document,
e.g., news headlines, scientific abstracts, movie previews
and reviews, and meeting minutes. Correspondingly, the
explosion of online textual material has prompted ad-
vanced research in document summarization. Although
researchers have demonstrated that users can read sum-
maries faster than full text (Mani et al, 2002) with some
loss of accuracy, researchers have found it difficult to
draw strong conclusions about the usefulness of summa-
rization due to the low level of interannotator agreement
in the gold standards that they have used. Definitive con-
clusions about the usefulness of summaries would pro-
vide justification for continued research and development
of new summarization methods.
To investigate the question of whether text summariza-
tion is useful in an extrinsic task, we examined human
performance in a relevance assessment task using a hu-
man text surrogate (i.e. text intended to stand in the place
of a document). We use single-document English sum-
maries as these are sufficient for investigating task-based
usefulness, although more elaborate surrogates are possi-
ble, e.g., those that span more than one document (Radev
and McKeown, 1998; Mani and Bloedorn, 1998).
The next section motivates the need for develop-
ing a new framework for measuring task-based useful-
ness. Section 3 presents a novel extrinsic measure called
Relevance-Prediction. Section 4 demonstrates that this is
a more reliable measure than that of previous gold stan-
dard methods, e.g., the LDC-Agreement method used for
SUMMAC-style evaluations, and that this reliability al-
lows us to make stronger statistical statements about the
benefits of summarization. We expect these findings to
be important for future summarization evaluations.
Section 5 presents the results of correlation between
task usefulness and the Recall Oriented Understudy for
Gisting Evaluation (ROUGE) metric (Lin and Hovy,
2003).1 While we show that ROUGE correlates with task
usefulness (using our Relevance-Prediction measure), we
detect a slight difference between informative, extractive
headlines (containing words from the full document) and
less informative, non-extractive ?eye-catchers? (contain-
ing words that might not appear in the full document, and
intended to entice a reader to read the entire document).
Section 6 further highlights the importance of this
point and discusses the implications for automatic eval-
uation of non-extractive summaries. To evaluate non-
extractive summaries reliably, an automatic measure may
require knowledge of sophisticated meaning units.2 It is
our hope that the conclusions drawn herein will prompt
investigation into more sophisticated automatic metrics
as researchers shift their focus to non-extractive sum-
maries.
1ROUGE has been previously used as the primary automatic
evaluation metric by NIST in the 2003 and 2004 DUC Evalua-
tions.
2The content units proposed in recent methods (Nenkova
and Passonneau, 2004) are a first step in this direction.
1
2 Background
In the past, assessments of usefulness involved a wide
range of both intrinsic and extrinsic (task-based) mea-
sures (Sparck-Jones and Gallier, 1996). Intrinsic evalu-
ations focus on coherence and informativeness (Jing et
al., 1998) and often involve quality comparisons between
automatic summaries and reference summaries that are
pre-determined to be of high quality. Human intrinsic
measures determine quality by assessing document accu-
racy, fluency, and clarity. Automatic intrinsic measures
such as ROUGE use n-gram scoring to produce rankings
of summarization methods.
Extrinsic evaluations concentrate on the use of sum-
maries in a specific task, e.g., executing instructions, in-
formation retrieval, question answering, and relevance
assessments (Mani, 2001). In relevance assessments, a
user reads a topic or event description and judges rele-
vance of a document to the topic/event based solely on its
summary.3 These have been used in many large-scale ex-
trinsic evaluations, e.g., SUMMAC (Mani et al, 2002)
and the Document Understanding Conference (DUC)
(Harman and Over, 2004). The task chosen for such eval-
uations must support a very high degree of interannota-
tor agreement, i.e., consistent relevance decisions across
subjects with respect to a predefined gold standard.
Unfortunately, a consistent gold standard has not yet
been reported. For example, in two previous studies
(Mani, 2001; Tombros and Sanderson, 1998), users?
judgments were compared to ?gold standard judgments?
produced by members of the University of Pennsylva-
nia?s Linguistic Data Consortium. Although these judg-
ments were supposed to represent the correct relevance
judgments for each of the documents associated with an
event, both studies reported that annotators? judgments
varied greatly and that this was a significant issue for the
evaluations. In the SUMMAC experiments, the Kappa
score (Carletta, 1996; Eugenio and Glass, 2004) for in-
terannotator agreement was reported to be 0.38 (Mani et
al., 2002). In fact, large variations have been found in the
initial summary scoring of an individual participant and a
subsequent scoring that occurs a few weeks later (Mani,
2001; van Halteren and Teufel, 2003).
This paper attempts to overcome the problem of in-
terannotator inconsistency by measuring summary effec-
tiveness in an extrinsic task using a much more consistent
form of user judgment instead of a gold standard. Us-
ing Relevance-Prediction increases the confidence in our
results and strengthens the statistical statements we can
make about the benefits of summarization.
The next section describes an alternative approach to
measuring task-based usefulness, where the usage of ex-
ternal judgments as a gold standard is replaced by the
3A topic is an event or activity, along with all directly re-
lated events and activities. An event is something that happens
at some specific time and place, and the unavoidable conse-
quences.
user?s own decisions on the full text. Following the lead
of earlier evaluations (Oka and Ueda, 2000; Mani et al,
2002; Sakai and Sparck-Jones, 2001), we focus on rele-
vance assessment as our extrinsic task.
3 Evaluation of Usefulness of Summaries
We define a new extrinsic measure of task-based useful-
ness called Relevance-Prediction, where we compare a
summary-based decision to the subject?s own full-text de-
cision rather than to a different subject?s decision. Our
findings differ from that of the SUMMAC results (Mani
et al, 2002) in that using Relevance-Prediction as an al-
ternative to comparision to a gold standard is a more re-
alistic agreement measure for assessing usefulness in a
relevance assessment task. For example, users perform-
ing browsing tasks must examine document surrogates,
but open the full-text only if they expect the document to
be interesting to them. They are not trying to decide if
the document will be interesting to someone else.
To determine the usefulness of summarization, we fo-
cus on two questions:
? Can users make judgments on summaries that are
consistent with their full-text judgments?
? Can users make judgments on summaries more
quickly than on full document text?
First we describe the Relevance-Prediction measure for
determining whether users can make accurate judgments
with a summary. Following this, we describe our exper-
iments and results using this measure, including the tim-
ing results of summaries compared to full documents.
3.1 Relevance-Prediction Measure
To answer the first question above, we define a mea-
sure called Relevance-Prediction, where subjects build
their own ?gold standard? based on the full-text docu-
ments. Agreement is measured by comparing subjects?
surrogate-based judgments against their own judgments
on the corresponding texts. The subject?s judgment is as-
signed a value of 1 if his/her surrogate judgment is the
same as the corresponding full-text judgment, and 0 oth-
erwise. These values were summed over all judgments
for a surrogate type and were divided by the total num-
ber of judgments for that surrogate type to determine the
effectiveness of the associated summary method.
Formally, given a summary/document pair (s, d), if
subjects make the same judgment on s that they did on
d, we say j(s, d) = 1. If subjects change their judg-
ment between s and d, we say j(s, d) = 0. Given a set
of summary/document pairs DSi associated with event i,
the Relevance-Prediction score is computed as follows:
Relevance-Prediction(i) =
?
s,d?DSi
j(s, d)
|DSi|
This approach provides a more reliable comparison
mechanism than gold standard judgments provided by
2
other individuals. Specifically, Relevance-Prediction is
more helpful in illuminating the usefulness of summaries
for a real-world scenario, e.g., a browsing environment,
where credit is given when an individual subject would
choose (or reject) a document under both conditions. To
our knowledge, this subject-driven approach to testing
usefulness has never before been used.
3.2 Experiment Design
Ten human subjects were recruited to evaluate full-text
documents and two summary types.4 The original text
documents were taken from the Topic Detection and
Tracking 3 (TDT-3) corpus (Allan et al, 1999) which
contains news stories and headlines, topic and event de-
scriptions, and a mapping between news stories and their
related topic and/or events. Although the TDT-3 collec-
tion contains transcribed speech documents, our investi-
gation was restricted to documents that were originally
text, i.e., newspaper or newswire, not broadcast news.
For our experiment we selected three distinct events
and related document sets5 from TDT-3. For each event,
the subjects were given a description of the event (writ-
ten by LDC) and then asked to judge relevance of a set
of 20 documents associated with that event (using three
different presentation types to be discussed below).
The events used from the TDT data set were events
from world news occurring in 1998. It is possible that
the subjects had some prior knowledge about the events,
yet we believe that this would not affect their ability to
complete the task. Subjects? background knowledge of an
event can also make this task more similar to real-world
browsing tasks, in which subjects are often familiar with
the event or topic they are searching for.
The 20 documents were retrieved by a search engine.
We used a constrained subset where exactly half (10)
were judged relevant by the LDC annotators. Because all
20 documents were somewhat similar to the event, this
approach ensured that our task would be more difficult
than it would be if we had chosen documents from com-
pletely unrelated events (where the choice of relevance
would be obvious even from a poorly written summary).
Each document was pre-annotated with the headline
associated with the original newswire source. These
headlines were used as the first summary type. We re-
fer to them as HEAD (Headline Surrogate). The average
length of the HEAD surrogates was 53 characters. In ad-
dition, we commissioned human-generated summaries6
of each document as the second summary type; we refer
4We required all human subjects to be native-English speak-
ers to ensure that the accuracy of judgments was not degraded
by language barriers.
5The three event and related document sets contained
enough data points to achieve statistically significant results.
6The human summarizers were instructed to create a sum-
mary no greater than 75 characters for each specified full text
document. The summaries were not compared for writing style
or quality.
to this as HUM (Human Surrogate). The average length
of the HUM surrogates was 72 characters. Although nei-
ther of these summaries was produced automatically, our
experiment allowed us to focus on the question of sum-
mary usefulness and to learn about the differences in pre-
sentation style as a first step toward experimentation with
the output of automatic summarization systems.
Two main factors were measured: (1) differences
in judgments for the three presentation types (HEAD,
HUM, and the full-text document) and (2) judgment time.
Each subject made a total of 60 judgments for each pre-
sentation type since there were 3 distinct events and 20
documents per event. To facilitate the analysis of the data,
the subjects? judgments were constrained to two possibil-
ities, relevant or not relevant.7
Although the HEAD and HUM surrogates were both
produced by humans, they differed in style. The HEAD
surrogates were shorter than the HUM surrogates by
26%. Many of these were ?eye-catchers? designed to en-
tice the reader to examine the entire document (i.e., pur-
chase the newspaper); that is, the HEAD surrogates were
not intended to stand in the place of the full document.
By contrast, the writers of the HUM surrogates were in-
structed to write text that conveyed what happened in the
full document. We observed that the HUM surrogates
used more words and phrases extracted from the full doc-
uments than the HEAD surrogates.
Experiments were conducted using a web browser (In-
ternet Explorer) on a PC in the presence of the experi-
menter. Subjects were given written and verbal instruc-
tions for completing their task and were asked to make
relevance judgments on a practice event set. The judg-
ments from the practice event set were not included in
our experimental results or used in our analyses. The
written instructions were given to aid subjects in deter-
mining requirements for relevance. For example, in an
Election event documents describing new people in of-
fice, new public officials, change in governments or par-
liaments were suggested as evidence for relevance.
Each of the ten subjects made judgments on 20 doc-
uments for each of three different events. After reading
each document or summary, the subjects clicked on a ra-
dio button corresponding to their judgment and clicked
a submit button to move to the next document descrip-
tion. Subjects were not allowed to move to the next sum-
mary/document until a valid selection was made. No
backing up was allowed. Judgment time was computed
as the number of seconds it took the subject to read the
full text document or surrogate, comprehend it, compare
it to the event description, and make a judgment (timed
up until the subject clicked the submit button).
7If we allowed subjects to make additional judgments such
as somewhat relevant, this could possibly encourage subjects to
always choose this when they were the least bit unsure. Previ-
ous experiments indicate that this additional selection method
may increase the level of variability in judgments (Zajic et al,
2004).
3
3.3 Order of Document/Surrogate Presentation
One concern with our evaluation methodology was the
issue of possible memory effects or priming: if the same
subjects saw a summary and a full document about the
same event, their answers might be tainted. Thus, prior to
the full experiment, we conducted pre-experiments (us-
ing 4 participants) with an extreme form of influence: we
presented the summary and full text in immediate suc-
cession. In these experiments, we compared two docu-
ment presentation approaches, termed ?Drill Down? and
?Complete Set.? In the ?Drill Down? document presen-
tation approach all three presentation types were shown
for each document, in sequence: first a single HEAD sur-
rogate, followed by the corresponding HUM surrogate,
followed by the full text document. This process was re-
peated 10 times.
In the ?Complete Set? document-presentation ap-
proach we presented the complete set of documents us-
ing one surrogate type, followed by the complete set us-
ing another surrogate type, and so on. That is, the 10
HEAD surrogates were displayed all at once, followed
by the corresponding 10 HUM surrogates, followed by
the corresponding 10 full-text documents.
The results indicated that there was almost no effect
between the two document-presentation approaches. The
performance varied only slightly and neither approach
consistently allowed subjects to perform better than the
other. Therefore, we determined that the subjects were
not associating a given summary with its corresponding
full-text documents. This may be due, in part, to the fact
that all 20 documents were related to the event?and ac-
cording to the LDC relevance judgments half of these
were actually about the same event.
Given that the variations were insignificant in these
pre-experiments, we selected only the Complete-Set ap-
proach (no Drill-Down) for the full experiment. How-
ever, we still needed to vary the ordering for the two sur-
rogate presentation types associated with each full-text
document. Thus, each 20-document set was divided in
half for each subject. In the first half, the subject saw the
first 10 documents as: (1) HEAD surrogates, then HUM
surrogates and then the full-text document; or (2) HUM
surrogates, then HEAD surrogates, and then the full-text
document. In the second half, the subject saw the alter-
native ordering, e.g., if a subject saw HEAD surrogates
before HUM surrogates in the first half, he/she saw the
HUM surrogates before HEAD surrogates for the sec-
ond half. Either way, the full-text document was always
shown last so as not to introduce judgment effects asso-
ciated with reading the entire document before either sur-
rogate type.
In addition to varying the ordering for the surrogate
type, the ordering of the surrogates and full documents
within the events were also varied. The subjects were
grouped in pairs, and each pair viewed the surrogates and
documents in a different order than the other pairs.
3.4 Experimental Hypotheses
We hypothesized that the summaries would allow sub-
jects to achieve a Relevance-Prediction rate of 70?90%.
Since these summaries were significantly shorter than the
original document text, we expected that the rate would
not be 100% compared to the judgments made on the full
document text. However, we expected higher than a 50%
ratio, i.e., higher than that of random judgments on all of
the surrogates. We also expected high performance be-
cause the meaning of the original document text is best
preserved when written by a human (Mani, 2001).
A second hypothesis is that the HEAD surrogates
would yield a significantly lower agreement rate than that
of the HUM surrogates. Our commissioned HUM surro-
gates were written to stand in place of the full document,
whereas the HEAD surrogates were written to catch a
reader?s interest. This suggests that the HEAD surrogates
might not provide as informative a description of the orig-
inal documents as the HUM surrogates.
We also tested a third hypothesis: that our Relevance-
Prediction measure would be more reliable than that of
the LDC-Agreement method used for SUMMAC-style
evaluations (thus providing a more stable framework for
evaluating summarization techniques). LDC-Agreement
compares a subject?s judgment on a surrogate or full text
against the ?correct? judgments as assigned by the TDT
corpus annotators (Linguistic Data Consortium 2001).
Finally, we tested the hypothesis that using a text sum-
mary for judging relevance would take considerably less
time than using the corresponding full-text document.
4 Experimental Results
Table 1 shows the subjects? judgments using both
Relevance-Prediction and LDC-Agreement for each of
three events. Using our Relevance-Prediction measure,
the HUM surrogates yield averages between 79% and
86%, with an overall average of 81%, thus confirming
our first hypothesis.
However, we failed to confirm our second hypothe-
sis. The HEAD Relevance-Prediction rates were between
71% and 82%, with an overall average of 76%, which
was lower than the rates for HUM, but the difference
was not statistically significant. It appeared that subjects
were able to make consistent relevance decisions from the
non-extractive HEAD surrogates, even though these were
shorter and less informative than the HUM surrogates.
A closer look reveals that the HEAD summaries some-
times contained enough information to judge relevance,
yielding almost the same number of true positives (and
true negatives) as the HUM summaries. For example, a
document about the formation of a coalition government
to avoid violence in Cambodia has the HEAD surrogate
Cambodians hope new government can avoid past mis-
takes. By contrast, the HUM surrogate for this same event
was Rival parties to form a coalition government to avoid
violence in Cambodia. Although the HEAD surrogate
4
Surrogate EVENT 1 EVENT 2 EVENT 3 Overall Avg Avg Time
LDC RP LDC RP LDC RP LDC RP (seconds)
HEAD 67% 76% 66% 71% 70% 82% 67% 76% 4.60
HUM 69% 80% 73% 86% 62% 79% 68% 81% 4.57
DOC ? ? ? ? ? ? ? ? 13.38
Table 1: Relevance-Prediction (RP) and LDC-Agreement (LDC) Rates for HEAD and HUM Surrogates for each Event
uses words that do not appear in the original document
(hope and mistakes), the subject may infer the relevance
of this surrogate by relating hope to the notion of forming
a coalition government and mistakes to violence.
On the other hand, we found that the lower degree of
informativeness of HEAD surrogates gave rise to over
50% more false negatives than the HUM summaries. This
statistically significant difference will be discussed fur-
ther in Section 6.
As for our third hypothesis, Table 1 illustrates a
substantial difference between the two agreement mea-
sures. For each of the three events, the Relevance-
Prediction rate is at least five percent higher than that
of the LDC-Agreement approach, with an average of
8.8% increase for the HEAD summary and a 13.3% aver-
age increase for the HUM summary. The average rates
across events show a statistically significant difference
between LDC-Agreement and Relevance-Prediction for
both HUM summaries with p<0.01 and HEAD sum-
maries with p<0.05. This significance was determined
through use of a single factor ANOVA statistical analysis.
The higher Relevance-Prediction rate supports our state-
ment that this approach provides a more stable framework
for evaluating different summarization techniques.
Finally, the average timing results shown in Table 1
confirm our fourth hypothesis. The subjects took 4-5 sec-
onds (on average) to make judgments on both the HEAD
and HUM summaries, as compared to about 13.4 seconds
to make judgments on full text documents. This shows
that it takes subjects almost 3 times longer to make judg-
ments on full text documents as it took to make judgments
on the summaries (HEAD and HUM). This finding is not
surprising since text summaries are an order of magnitude
shorter than full-text documents.
5 Correlation with Intrinsic Evaluation
Metric: ROUGE
We now turn to the task of correlating our extrinsic task
performance with scores produced by an intrinsic evalu-
ation measure. We used the Recall Oriented Understudy
for Gisting Evaluation (ROUGE) metric version 1.2.1. In
previous studies (Dorr et al, 2004) ROUGE was shown
to have a very low correlation with the LDC-Agreement
measurement results of the extrinsic task. This was at-
tributed to low interannotator agreement in the gold stan-
dard. Our goal was to test whether our new Relevance-
Prediction technique would allow us to induce higher cor-
relations with ROUGE.
5.1 Extrinsic Agreement Data
To reduce the effect of outliers on the correlation between
ROUGE and the human judgments, we averaged over all
judgments for each subject (20 judgments ? 3 events) to
produce 60 data points. These data points were then par-
titioned into either 1, 2, or 4 partitions of equal size. (Par-
titions of size four have 15 data points, partitions of size
two have 30 data points, and partitions of size one have
60 data points per subject?or a total of 600 datapoints
across all 10 subjects). To ensure that the correlation did
not depend on a specific partition, we repeated this same
process using 10,000 different (randomly generated) par-
titions for each of the three partition sizes.
Partitioned data points of size four provided a high de-
gree of noise reduction without compromising the size
of the data set (15 points). Larger partition sizes would
result in too few data points and compromise the statis-
tical significance of our correlation results. In order to
show the variation within a single partition, we used the
partitioning of size 4 with the smallest mean square er-
ror on the human headline compared to the other parti-
tionings as a representative partition. For this represen-
tative partitioning, the individual data points P1?P15 of
that partition are shown for each of the two agreement
measures in Tables 2 and 3. This shows that, across parti-
tions, the maximum and minimum Relevance-Prediction
rates for HEAD (93% and 60%) are higher than the cor-
responding LDC-Agreement rates (85% and 50%). The
same trend is seen with the HUM surrogates: Relevance-
Prediction maximum of 98%, minimum of 68%; and
LDC-Agreement maximum 88%, minimum of 55%.
5.2 Intrinsic ROUGE Score
To correlate the partitioned agreement scores above with
our intrinsic measure, we first ran ROUGE on all 120 sur-
rogates in our experiment (i.e., the HUM and HEAD sur-
rogates for each of the 60 event/document pairs) and then
averaged the ROUGE scores for all surrogates belong-
ing to the same partitions (for each of the three partition
sizes). These partitioned ROUGE values were then used
for detecting correlations with the corresponding parti-
tioned agreement scores described above.
Table 4 shows the ROUGE scores, based on 3 ref-
erence summaries per document, for partitions P1?P15
used in the previous tables.8 For brevity, we include
8We commissioned a total of 180 human-generated refer-
ence summaries (3 for each of 60 documents) (in addition to
the human generated summaries used in the experiment).
5
Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15
HEAD 80% 80% 85% 70% 73% 60% 80% 75% 60% 75% 88% 68% 80% 93% 83%
HUM 83% 88% 85% 68% 75% 75% 93% 75% 98% 90% 75% 70% 80% 90% 78%
Table 2: Relevance-Prediction Rates for HEAD and HUM Surrogates (Representative Partition of Size 4)
Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15
HEAD 70% 73% 85% 70% 63% 60% 60% 85% 50% 73% 70% 78% 65% 63% 73%
HUM 68% 75% 58% 68% 75% 70% 68% 80% 88% 58% 63% 55% 55% 60% 78%
Table 3: LDC-Agreement Rates for HEAD and HUM Surrogates (Representative Partition of Size 4)
Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 Avg
HEAD .10 .23 .13 .27 .20 .24 .26 .22 .13 .08 .30 .16 .26 .27 .30 .211
HUM .16 .22 .17 .23 .19 .36 .39 .29 .28 .25 .37 .22 .22 .39 .27 .269
Table 4: Average Rouge-1 Scores for HEAD and HUM Surrogates (Representative Partition of Size 4)
only ROUGE 1-gram measurement (R1).9 The ROUGE
scores for HEAD surrogates were slightly lower than
those for HUM surrogates. This is consistent with
our statements earlier about the difference between non-
extractive ?eye-catchers? and informative headlines. Be-
cause ROUGE measures whether a particular summary
has the same words (or n-grams) as a reference summary,
a more constrained choice of words (as found in the ex-
tractive HUM surrogates) makes it more likely that the
summary would match the reference.
A summary in which the word choice is less
constrained?as in the non-extractive HEAD
surrogates?is less likely to share n-grams with the
reference. Thus, we may see non-extractive summaries
that have almost identical meanings, but very different
words. This raises the concern that ROUGE may be
sensitive to the style of summarization that is used.
Section 6 discusses this point further.
5.3 Intrinsic and Extrinsic Correlation
To test whether ROUGE correlates more highly with
Relevance-Prediction than with LDC-Agreement, we cal-
culated the correlation for the results of both techniques
using Pearson?s r (Siegel and Castellan, 1988):
?n
i=1(ri ? r?)(si ? s?)??n
i=1(ri ? r?)
2
??n
i=1(si ? s?)
2
where ri is the ROUGE score of surrogate i, r? is the av-
erage ROUGE score of all data points, si is the agree-
ment score of summary i (using Relevance-Prediction or
LDC-Agreement), and s? is the average agreement score.
Pearson?s statistics is commonly used in summarization
and machine translation evaluation, see e.g. (Lin, 2004;
Lin and Och, 2004).
As one might expect, there is some variability in the
correlation between ROUGE and human judgments for
9We also computed ROUGE 2-gram, ROUGE L and
ROUGE W, but the trend for these did not differ from ROUGE-
1.
Figure 1: Distribution of the Correlation Variation for
Relevance-Prediction on HEAD and HUM
the different partitions. However, the boxplots for both
HEAD and HUM indicate that the first and third quartile
were relatively close to the median (see Figure 1).
Table 5 shows the Pearson Correlations with ROUGE-
1 using Relevance-Prediction and LDC-Agreement. For
Relevance-Prediction, we observed a positive correlation
for both surrogate types, with a slightly higher corre-
lation for HEAD than HUM. For LDC-Agreement, we
observed no correlation (or a minimally negative one)
with ROUGE-1 scores, for both the HEAD and HUM
surrogates. The highest correlation was observed for
Relevance-Prediction on HEAD.
We conclude that ROUGE correlates more highly with
the Relevance-Prediction measurement than the LDC-
Agreement measurement, although we should add that
none of the correlations in Table 5 were statistically sig-
nificant at p < 0.05. The low LDC-Agreement scores are
consistent with previous studies where poor correlations
6
Surrogate P = 1 P = 2 P = 4
HEAD (RP) 0.1270 0.1943 0.3140
HUM (RP) 0.0632 0.1096 0.1391
HEAD (LDC) -0.0968 -0.0660 -0.0099
HUM (LDC) -0.0395 -0.0236 -0.0187
Table 5: Pearson Correlations with ROUGE-1 for
Relevance-Prediction (RP) and LDC-Agreement (LDC),
where Partition size (P) = 1, 2, and 4
were attributed to low interannotator agreement rates.
6 Discussion
Our results suggest that ROUGE may be sensitive to the
style of summarization that is used. As we observed
above, many of the HEAD surrogates were not actually
summaries of the full text, but were eye-catchers. Of-
ten, these surrogates did not allow the subject to judge
relevance correctly, resulting in lower agreement. In ad-
dition, these same surrogates often did not use a high per-
centage of words that were actually from the story, result-
ing in low ROUGE scores. (We noticed that most words
in the HUM surrogates appeared in the corresponding
stories.) There were three consequences of this difference
between HEAD and HUM: (1) The rate of agreement was
lower for HEAD than for HUM; (2) The average ROUGE
score was lower for HEAD than for HUM; and (3) The
correlation of ROUGE scores with agreement was higher
for HEAD than for HUM.
A further analysis supports the (somewhat counterin-
tuitive) third point above. Although the ROUGE scores
of true positives (and true negatives) were significantly
lower for HEAD surrogates (0.2127 and 0.2162) than
for HUM surrogates (0.2696 and 0.2715), the number of
false negatives was substantially higher for HEAD sur-
rogates than for HUM surrogates. These cases corre-
sponded to much lower ROUGE scores for HEAD sur-
rogates (0.1996) than for HUM (0.2586) surrogates.
A summary of this analysis is given in Table 6, where
true positives and negatives are indicated by Rel/Rel
and NonRel/NonRel, respectively, and false positives and
negatives are indicated by Rel/NonRel and NonRel/Rel,
respectively.10 The numbers in parentheses after each
ROUGE score refer to the standard deviation for that
10We also included (average) elapsed times for summary
judgments in each of the four categories. One might expect a
?relevant? judgment to be much quicker than a ?non-relevant?
judgment (since the latter might require reading the full sum-
mary). However, it turned out non-relevant judgments did not
always take longer. In fact, the NonRel/NonRel cases took con-
siderably less time than the Rel/Rel and Rel/NonRel cases. On
the other hand, the NonRel/Rel cases took considerably more
time?almost as much time as reading the full text documents?
an indication that the subjects may have re-read the summary a
number of times, perhaps vacillating back and forth. Still, the
overall time savings was significant, given that the vast major-
ity of the non-relevant judgments were in the NonRel/NonRel
category.
score. This was computed as follows:
Std .-Dev . =
?
?N
i=1(xi ? x?)
2
N
where N is the number of surrogates in a particular judg-
ment category (e.g., N = 245 for the HEAD-based Non-
Rel/Rel judgments), xi is the ROUGE score for the ith
surrogate, and r? is the average of all ROUGE scores in
that category.
Although there were very few false positives (less than
6% for both HEAD and HUM), the number of false nega-
tives (NonRel/Rel) was particularly high for HEAD (50%
higher than for HUM). This difference was statistically
significant at p<0.01 using the t-test. The large number
of false negatives with HEAD may be attributed to the
eye-catching nature of these surrogates. A subject may
be misled into thinking that this surrogate is not related
to an event because the surrogate does not contain words
from the event description and is too broad for the subject
to extract definitive information (e.g., the surrogate There
he goes again!). Because the false negatives were associ-
ated with the lowest average ROUGE score (0.1996), we
speculate that, if a correlation exists between Relevance-
Prediction and ROUGE, the false negatives may be a ma-
jor contributing factor.
Based on this experiment, we conjecture that ROUGE
may not be a good method for measuring the useful-
ness of summaries when the summaries are not extrac-
tive. That is, if someone intentionally writes summaries
that contain different words than the story, the summaries
will also likely contain different words than a reference
summary, resulting in low ROUGE scores. However,
the summaries, if well-written, could still result in high
agreement with the judgments made on the full text.
7 Conclusion
We have shown that two types of human summaries,
HEAD and HUM, can be useful for relevance assessment
in that they help a user achieve 70-85% agreement in rel-
evance judgments. We observed a 65% reduction in judg-
ment time between full texts and summaries. These find-
ings are important in that they establish the usefulness
of summarization and they support research and devel-
opment of additional summarization methods, including
automatic methods.
We introduced a new method for measuring agree-
ment, Relevance-Prediction, which takes a subject?s
full-text judgment as the standard against which the
same subject?s summary judgment is measured. Be-
cause Relevance-Prediction was more reliable than LDC-
Agreement judgments, we encourage others to use this
measure in future summarization evaluations.
Using this new method, we were able to find positive
correlations between relevance assessments and ROUGE
scores for HUM and HEAD surrogates, where only
7
Judgment HEAD HUM
(Surr/Doc) Raw R1-Avg Avg Time Raw R1-Avg Avg Time
Rel/Rel 211 (35%) 0.2127 (?0.120) 4.6 251 (42%) 0.2696 (?0.130) 4.2
Rel/NonRel 27 (5%) 0.2115 (?0.110) 7.1 35 (6%) 0.2725 (?0.131) 4.6
NonRel/Rel 117 (19%) 0.1996 (?0.127) 8.5 77 (13%) 0.2586 (?0.120) 13.8
NonRel/NonRel 245 (41%) 0.2162 (?0.126) 2.5 237 (39%) 0.2715 (?0.131) 1.9
TOTAL 600 (100%) 0.2115 (?0.124) 4.6 600 (100%) 0.2691 (?0.129) 4.6
Table 6: Subjects? Judgments and Corresponding Average ROUGE 1 Scores
negative correlations were found using LDC-Agreement
scores. We found that both the Relevance-Prediction and
the ROUGE-1 scores were higher for human-generated
summaries than for the original headlines. It appears
that most of the difference is induced by surrogates that
are eye-catchers (rather than true summaries), where both
agreement and ROUGE scores are low.
Our future work will include further experimentation
with automatic summarization methods to determine the
level of Relevance-Prediction. We aim to determine how
well automatic summarizers help users complete tasks,
and to investigate which automatic summarizers perform
better than others. We also plan to test for correlations
between ROUGE and human task performance with auto-
matic summaries, to further investigate whether ROUGE
is a good predictor of human task performance.
Acknowledgements
This work was supported in part by DARPA TIDES Cooperative
Agreement N66001-00-2-8910.
References
James Allan, Hubert Jin, Martin Rajman, Charles Wayne,
Daniel Gildea, Victor Lavrenko, Rose Hoberman, and David
Caputo. 1999. Topic-based Novelty Detection. Technical
Report 1999 Summer Workshop at CLSP Final Report, Johns
Hopkins, Maryland.
Jean Carletta. 1996. Assessing Agreement on Classification
Tasks: The Kappa Statistic. Computational Lingusitics,
22(2):249?254, June.
Bonnie J. Dorr, Christof Monz, Douglas Oard, Stacy President,
and David Zajic. 2004. Extrinsic Evaluation of Automatic
Metrics for Summarization. Technical report, University of
Maryland, College Park, MD. LAMP-TR-115, CAR-TR-
999, CS-TR-4610, UMIACS-TR-2004-48.
Barbara Di Eugenio and Michael Glass. 2004. Squibs and Dis-
cussions - The Kappa Statistic: A Second Look. Computa-
tional Linguistics, pages 95?101.
Donna Harman and Paul Over. 2004. Proceedings of the DUC
2004. Boston, MA.
Hongyan Jing, Regina Barzilay, Kathleen McKeown, and
Michael Elhadad. 1998. Summarization evaluation meth-
ods: Experiments and analysis. In Proceedings of the AAAI
Symposium on Intelligent Summarization, Stanford Univer-
sity, CA, March 23-25.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic Evaluation
of Summaries Using N-gram Co-Occurrence Statistics. In
Proceedings of HLT-NAACL 2003 Workshop, pages 71?78,
Edmonton Canada, May-June.
Chin-Yew Lin and Franz Joseph Och. 2004. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics for
Machine Translation. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics (COLING
2004), Geneva, Switzerland, August 23?27.
Chin-Yew Lin. 2004. ROUGE: a Package for Automatic Eval-
uation of Summaries. In Proceedings of the Workshop on
Text Summarization Branches Out (WAS 2004), Barcelona,
Spain, July 25?26.
I. Mani and E. Bloedorn. 1998. Summarizing Similarities and
Differences Among Related Documents. Information Re-
trieval, 1(1):35?67.
Inderjeet Mani, Gary Klein, David House, and Lynette
Hirschman. 2002. SUMMAC: a text summarization eval-
uation. Natural Language Engineering, 8(1):43?68.
Inderjeet Mani. 2001. Summarization Evaluation: An
Overview. In Proceedings of the NAACL 2001 Workshop on
Automatic Summarization.
Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluating
Content Selection in Summarization: The Pyramid Method.
In Proceedings of the NAACL 2004, Boston, MA.
Mamiko Oka and Yoshihiro Ueda. 2000. Evaluation of Phrase-
Representation Summarization Based on an Information Re-
trieval Task. In Proceedings of the ANLP/NAACL 2000
Workshop on Automatic Summarization, pages 59?68, New
Brunswick, NJ.
Dragomir Radev and Kathleen McKeown. 1998. Generat-
ing Natural Language Summaries from Multiple On-Line
Sources. Computational Linguistics, pages 469?500.
Tetsuya Sakai and Karen Sparck-Jones. 2001. Generic Sum-
maries for Indexing in Information Retrieval - Detailed Test
Results. Technical Report TR513, Computer Laboratory,
University of Cambridge.
Sidney Siegel and N. John Castellan, Jr. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill, New
York, second edition.
Karen Sparck-Jones and J.R. Gallier. 1996. Evaluating Natu-
ral Language Processing Systems: An Analysis and Review.
Springer, Berlin.
Anastasios Tombros and Mark Sanderson. 1998. Advantages
of query biased summaries in information retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information Re-
trieval, pages 2?10.
Hans van Halteren and Simone Teufel. 2003. Examining the
Consensus Between Human Summaries: Initial Experiments
with Factoid Analysis. In Proceedings of the HLT-NAACL
03 Text Summarization Workshop.
David Zajic, Bonnie J. Dorr, Richard Schwartz, and Stacy Presi-
dent. 2004. Headline Evaluation Experiment Results. Tech-
nical report, University of Maryland, College Park, MD.
UMIACS-TR-2004-18.
8
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 415?423,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Automatic Single-Document Key Fact Extraction from Newswire Articles
Itamar Kastner
Department of Computer Science
Queen Mary, University of London, UK
itk1@dcs.qmul.ac.uk
Christof Monz
ISLA, University of Amsterdam
Amsterdam, The Netherlands
christof@science.uva.nl
Abstract
This paper addresses the problem of ex-
tracting the most important facts from a
news article. Our approach uses syntac-
tic, semantic, and general statistical fea-
tures to identify the most important sen-
tences in a document. The importance
of the individual features is estimated us-
ing generalized iterative scaling methods
trained on an annotated newswire corpus.
The performance of our approach is evalu-
ated against 300 unseen news articles and
shows that use of these features results in
statistically significant improvements over
a provenly robust baseline, as measured
using metrics such as precision, recall and
ROUGE.
1 Introduction
The increasing amount of information that is avail-
able to both professional users (such as journal-
ists, financial analysts and intelligence analysts)
and lay users has called for methods condensing
information, in order to make the most important
content stand out. Several methods have been pro-
posed over the last two decades, among which
keyword extraction and summarization are the
most prominent ones. Keyword extraction aims
to identify the most relevant words or phrases in
a document, e.g., (Witten et al, 1999), while sum-
marization aims to provide a short (commonly 100
words), coherent full-text summary of the docu-
ment, e.g., (McKeown et al, 1999). Key fact ex-
traction falls in between key word extraction and
summarization. Here, the challenge is to identify
the most relevant facts in a document, but not nec-
essarily in a coherent full-text form as is done in
summarization.
Evidence of the usefulness of key fact extraction
is CNN?s web site which since 2006 has most of its
news articles preceded by a list of story highlights,
see Figure 1. The advantage of the news highlights
as opposed to full-text summaries is that they are
much ?easier on the eye? and are better suited for
quick skimming.
So far, only CNN.com offers this service and we
are interested in finding out to what extent it can
be automated and thus applied to any newswire
source. Although these highlights could be eas-
ily generated by the respective journalists, many
news organization shy away from introducing an
additional manual stage into the workflow, where
pushback times of minutes are considered unac-
ceptable in an extremely competitive news busi-
ness which competes in terms of seconds rather
than minutes. Automating highlight generation
can help eliminate those delays.
Journalistic training emphasizes that news arti-
cles should contain the most important informa-
tion in the beginning, while less important infor-
mation, such as background or additional details,
appears further down in the article. This is also
the main reason why most summarization systems
applied to news articles do not outperform a sim-
ple baseline that just uses the first 100 words of an
article (Svore et al, 2007; Nenkova, 2005).
On the other hand, most of CNN?s story high-
lights are not taken from the beginning of the ar-
ticles. In fact, more than 50% of the highlights
stem from sentences that are not among the first
100 words of the articles. This makes identify-
ing story highlights a much more challenging task
than single-document summarization in the news
domain.
In order to automate story highlight identifica-
tion we automatically extract syntactic, semantic,
415
Figure 1: CNN.com screen shot of a story excerpt
with highlights.
and purely statistical features from the document.
The weights of the features are estimated using
machine learning techniques, trained on an anno-
tated corpus. In this paper, we focus on identify-
ing the relevant sentences in the news article from
which the highlights were generated. The system
we have implemented is named AURUM: AUto-
matic Retrieval of Unique information with Ma-
chine learning. A full system would also contain
a sentence compression step (Knight and Marcu,
2000), but since both steps are largely indepen-
dent of each other, existing sentence compression
or simplification techniques can be applied to the
sentences identified by our approach.
The remainder of this paper is organized as fol-
lows: The next section describes the relevant work
done to date in keyfact extraction and automatic
summarization. Section 3 lays out our features
and explains how they were learned and estimated.
Section 4 presents the experimental setup and our
results, and Section 5 concludes with a short dis-
cussion.
2 Related Work
As mentioned above, the problem of identifying
story highlight lies somewhere between keyword
extraction and single-document summarization.
The KEA keyphrase extraction system (Witten
et al, 1999) mainly relies on purely statistical
features such as term frequencies, using the tf.idf
measure from Information Retrieval,1 as well as
on a term?s position in the text. In addition to tf.idf
scores, Hulth (2004) uses part-of-speech tags and
NP chunks and complements this with machine
learning; the latter has been used to good results
in similar cases (Turney, 2000; Neto et al, 2002).
The B&C system (Barker and Cornacchia, 2000),
also used linguistic methods to a very limited ex-
tent, identifying NP heads.
INFORMATIONFINDER (Krulwich and Burkey,
1996) requires user feedback to train the system,
whereby a user notes whether a given document
is of interest to them and specifies their own key-
words which are then learned by the system.
Over the last few years, numerous single-
as well as multi-document summarization ap-
proaches have been developed. In this paper we
will focus mainly on single-document summariza-
tion as it is more relevant to the issue we aim to
address and traditionally proves harder to accom-
plish. A good example of a powerful approach is
a method named Maximum Marginal Relevance
which extracts a sentence for the summary only
if it is different than previously selected ones,
thereby striving to reduce redundancy (Carbonell
and Goldstein, 1998).
More recently, the work of Svore et al (2007)
is closely related to our approach as it has also ex-
ploited the CNN Story Highlights, although their
focus was on summarization and using ROUGE
as an evaluation and training measure. Their ap-
proach also heavily relies on additional data re-
sources, mainly indexed Wikipedia articles and
Microsoft Live query logs, which are not readily
available.
Linguistic features are today used mostly in
summarization systems, and include the standard
features sentence length, n-gram frequency, sen-
tence position, proper noun identification, similar-
ity to title, tf.idf, and so-called ?bonus?/?stigma?
words (Neto et al, 2002; Leite et al, 2007; Pol-
lock and Zamora, 1975; Goldstein et al, 1999).
On the other hand, for most of these systems, sim-
ple statistical features and tf.idf still turn out to be
the most important features.
Attempts to integrate discourse models have
also been made (Thione et al, 2004), hand in hand
with some of Marcu?s (1995) earlier work.
1tf(t, d) = frequency of term t in document d.
idf(t,N) = inverse frequency of documents d containing
term t in corpus N , log( |N||dt| )
416
Regarding syntax, it seems to be used mainly
in sentence compression or trimming. The algo-
rithm used by Dorr et al (2003) removes subor-
dinate clauses, to name one example. While our
approach does not use syntactical features as such,
it is worth noting these possible enhancements.
3 Approach
In this section we describe which features were
used and how the data was annotated to facilitate
feature extraction and estimation.
3.1 Training Data
In order to determine the features used for pre-
dicting which sentences are the sources for story
highlights, we gathered statistics from 1,200 CNN
newswire articles. An additional 300 articles were
set aside to serve as a test set later on. The arti-
cles were taken from a wide range of topics: poli-
tics, business, sport, health, world affairs, weather,
entertainment and technology. Only articles with
story highlights were considered.
For each article we extracted a number of n-
gram statistics, where n ? {1, 2, 3}.
n-gram score. We observed the frequency and
probability of unigrams, bigrams and trigrams ap-
pearing in both the article body and the highlights
of a given story. An important phrase (of length
n ? 3) in the article would likely be used again
in the highlights. These phrases were ranked and
scored according to the probability of their appear-
ing in a given text and its highlights.
Trigger phrases. These are phrases which cause
adjacent words to appear in the highlights. Over
the entire set, such phrases become significant. We
specified a limit of 2 words to the left and 4 words
to the right of a phrase. For example, the word ac-
cording caused other words in the same sentence
to appear in the highlights nearly 25% of the time.
Consider the highlight/sentence pair in Table 1:
highlight: 61 percent of those polled now say it was not
worth invading Iraq, poll says
Text: Now, 61 percent of those surveyed say it was
not worth invading Iraq, according to the poll.
Table 1: Example highlight with source sentence.
The word according receives a score of 3 since
{invading, Iraq, poll} are all in the highlight. It
should be noted that the trigram {invading Iraq
according} would receive an identical score, since
{not, worth, poll} are in the highlights as well.
Spawned phrases. Conversely, spawned
phrases occur frequently in the highlights and in
close proximity to trigger phrases. Continuing
the example in Table 1, {invading, Iraq, poll, not,
worth} are all considered to be spawned phrases.
Of course, simply using the identities of words
neglects the issue of lexical paraphrasing, e.g.,
involving synonyms, which we address to some
extent by using WordNet and other features de-
scribed in this Section. Table 2 gives an example
involving paraphrasing.
highlight: Sources say men were planning to shoot sol-
diers at Army base
Text: The federal government has charged five al-
leged Islamic radicals with plotting to kill U.S.
soldiers at Fort Dix in New Jersey.
Table 2: An example of paraphrasing between a
highlight and its source sentence.
Other approaches have tried to select linguistic
features which could be useful (Chuang and Yang,
2000), but these gather them under one heading
rather than treating them as separate features. The
identification of common verbs has been used both
as a positive (Turney, 2000) and as a negative
feature (Goldstein et al, 1999) in some systems,
whereas we score such terms according to a scale.
Turney also uses a ?final adjective? measure. Use
of a thesaurus has also shown to improve results in
automatic summarization, even in multi-document
environments (McKeown et al, 1999) and other
languages such as Portuguese (Leite et al, 2007).
3.2 Feature Selection
By manually inspecting the training data, the lin-
guistic features were selected. AURUM has two
types of features: sentence features, such as the
position of the sentence or the existence of a nega-
tion word, receive the same value for the entire
sentence. On the other hand, word features are
evaluated for each of the words in the sentence,
normalized over the number of words in the sen-
tence.
Our features resemble those suggested by previ-
ous works in keyphrase extraction and automatic
summarization, but map more closely to the jour-
nalistic characteristics of the corpus, as explained
in the following.
417
Figure 2: Positions of sentences from which high-
lights (HLs) were generated.
3.2.1 Sentence Features
These are the features which apply once for each
sentence.
Position of the sentence in the text. Intuitively,
facts of greater importance will be placed at the
beginning of the text, and this is supported by the
data, as can be seen in Figure 2. Only half of the
highlights stem from sentences in the first fifth of
the article. Nevertheless, selecting sentences from
only the first few lines is not a sure-fire approach.
Table 3 presents an article in which none of the
first four sentences were in the highlights. While
the baseline found no sentences, AURUM?s perfor-
mance was better.
The sentence positions score is defined as pi =
1 ? (log i/logN), where i is the position of the
sentence in the article and N the total number of
sentences in the article.
Numbers or dates. This is especially evident
in news reports mentioning figures of casualties,
opinion poll results, or financial news.
Source attribution. Phrasings such as accord-
ing to a source or officials say.
Negations. Negations are often used for intro-
ducing new or contradictory information: ?Kelly
is due in a Chicago courtroom Friday for yet an-
other status hearing, but there?s still no trial date
in sight.2? We selected a number of typical nega-
tion phrases to this end.
Causal adverbs. Manually compiled list of
phrases, including in order to, hoping for and be-
cause.
2This sentence was included in the highlights
Temporal adverbs. Manually compiled list of
phrases, such as after less than, for two weeks and
Thursday.
Mention of the news agency?s name. Journal-
istic scoops and other exclusive nuggets of infor-
mation often recall the agency?s name, especially
when there is an element of self-advertisement
involved, as in ?. . . The debates are being held
by CNN, WMUR and the New Hampshire Union
Leader.? It is interesting to note that an opposite
approach has previously been taken (Goldstein et
al., 1999), albeit involving a different corpus.
Story Highlights:
?Memorial Day marked by parades, cookouts, cer-
emonies
? AAA: 38 million Americans expected to travel at
least 50 miles during weekend
? President Bush gives speech at Arlington National
Cemetery
? Gulf Coast once again packed with people cele-
brating holiday weekend
First sentences of article:
1. Veterans and active soldiers unfurled a 90-by-
100-foot U. S. flag as the nation?s top commander
in the Middle East spoke to a Memorial Day crowd
gathered in Central Park on Monday.
2. Navy Adm. William Fallon, commander of U. S.
Central Command, said America should remember
those whom the holiday honors.
3. ?Their sacrifice has enabled us to enjoy the things
that we, I think in many cases, take for granted,?
Fallon said.
4. Across the nation, flags snapped in the wind over
decorated gravestones as relatives and friends paid
tribute to their fallen soldiers.
Sentences the Highlights were derived from:
5. Millions more kicked off summer with trips to
beaches or their backyard grills.
6. AAA estimated 38 million Americans would
travel 50 miles or more during the weekend ? up
1.7 percent from last year ? even with gas aver-
aging $3.20 a gallon for self-service regular.
7. In the nation?s capital, thousands of motorcy-
cles driven by military veterans and their loved ones
roared through Washington to the Vietnam Veterans
Memorial.
9. President Bush spoke at nearby Arlington Na-
tional Cemetery, honoring U. S. troops who have
fought and died for freedom and expressing his re-
solve to succeed in the war in Iraq.
21. Elsewhere, Alabama?s Gulf Coast was once
again packed with holiday-goers after the damage
from hurricanes Ivan and Katrina in 2004 and 2005
kept the tourists away.
Table 3: Sentence selection outside the first four
sentences (correctly identified sentence by AURUM
in boldface).
418
3.2.2 Word Features
These features are tested on each word in the sen-
tence.
?Bonus? words. A list of phrases similar to sen-
sational, badly, ironically, historic, identified from
the training data. This is akin to ?bonus?/?stigma?
words (Neto et al, 2002; Leite et al, 2007; Pol-
lock and Zamora, 1975; Goldstein et al, 1999).
Verb classes. After exploring the training data
we manually compiled two classes of verbs,
each containing 15-20 inflected and uninflected
lexemes, talkVerbs and actionVerbs.
talkVerbs include verbs such as {report, men-
tion, accuse} and actionVerbs refer to verbs
such as {provoke, spend, use}. Both lists also con-
tain the WordNet synonyms of each word in the
list (Fellbaum, 1998).
Proper nouns. Proper nouns and other parts of
speech were identified running Charniak?s parser
(Charniak, 2000) on the news articles.
3.2.3 Sentence Scoring
The overall score of a sentence is computed as the
weighted linear combination of the sentence and
word scores. The score ?(s) of sentence s is de-
fined as follows:
?(s) = wposppos(s) +
n?
k=1
wkfk +
|s|?
j=1
m?
k=1
wkgjk
Each of the sentences s in the article was tested
against the position feature ppos(s) and against
each of the sentence features fk, see Section 3.2.1,
where pos(s) returns the position of sentence s.
Each word j of sentence s is tested against all ap-
plicable word features gjk, see Section 3.2.2. A
weight (wpos and wk) is associated with each fea-
ture. How to estimate the weights is discussed
next.
3.3 Parameter Estimation
There are various optimization methods that allow
one to estimate the weights of features, includ-
ing generalized iterative scaling and quasi-Newton
methods (Malouf, 2002). We opted for general-
ized iterative scaling as it is commonly used for
other NLP tasks and off-the-shelf implementations
exist. Here we used YASMET.3
3A maximum entropy toolkit by Franz Josef Och, http:
//www.fjoch.com/YASMET.html
We used a development set of 240 news arti-
cles to train YASMET. As YASMET is a supervised
optimizer, we had to generate annotated data on
which it was to be trained. For each document in
the development set, we labeled each sentence as
to whether a story highlight was generated from it.
For instance, in the article presented in Figure 3,
sentences 5, 6, 7, 9 and 21 were marked as high-
light sources, whereas all other sentences in the
document were not.4
When annotating, all sentences that were di-
rectly relevant to the highlights were marked, with
preference given to those appearing earlier in the
story or containing more precise information. At
this point it is worth noting that while the over-
lap between different editors is unknown, the high-
lights were originally written by a number of dif-
ferent people, ensuring enough variation in the
data and helping to avoid over-fitting to a specific
editor.
4 Experiments and Results
The CNN corpus was divided into a training set
and a development and test set. As we had
only 300 manually annotated news articles and we
wanted to maximize the number of documents us-
able for parameter estimation, we applied cross-
folding, which is commonly used for situations
with limited data. The dev/test set was randomly
partitioned into five folds. Four of the five folds
were used as development data (i.e. for parame-
ter estimation with YASMET), while the remaining
fold was used for testing. The procedure was re-
peated five times, each time with four folds used
for development and a separate one for testing.
Cross-folding is safe to use as long as there are
no dependencies between the folds, which is safe
to assume here.
Some statistics on our training and develop-
ment/test data can be found in Table 4.
Corpus subset Dev/Test Train
Documents 300 1220
Avg. sentences per article 33.26 31.02
Avg. sentence length 20.62 20.50
Avg. number of highlights 3.71 3.67
Avg. number of highlight sources 4.32 -
Avg. highlight length in words 10.26 10.28
Table 4: Characteristics of the evaluation corpus.
4The annotated data set is available at: http://www.
science.uva.nl/?christof/data/hl/.
419
Most summarization evaluation campaigns,
such as NIST?s Document Understanding Confer-
ences (DUC), impose a maximum length on sum-
maries (e.g., 75 characters for the headline gen-
eration task or 100 words for the summarization
task). When identifying sentences from which
story highlights are generated, the situation is
slightly different, as the number of story highlights
is not fixed. On the other hand, most stories have
between three and four highlights, and on aver-
age between four and five sentences per story from
which the highlights were generated. This varia-
tion led to us to carry out two sets of experiments:
In the first experiment (fixed), the number of
highlight sources is fixed and our system always
returns exactly four highlight sources. In the sec-
ond experiment (thresh), our system can return
between three and six highlight sources, depend-
ing on whether a sentence score passes a given
threshold. The threshold ? was used to allocate
sentences si of article a to the highlight list HL
by first finding the highest-scoring sentence for
that article ?(sh). The threshold score was thus
? ? ?(sh) and sentences were judged accordingly.
The algorithm used is given in Figure 3.
initialize HL, sh
sort si in s by ?(si)
set sh = s0
for each sentence si in article a:
if |HL| < 3
include si
else if (? ? ?(sh) ? ?(si))&& (|HL| ? 5)
include si
else
discard si
return HL
Figure 3: Procedure for selecting highlight
sources.
All scores were compared to a baseline, which
simply returns the first n sentences of a news
article. n = 4 in the fixed experiment.
For the thresh experiment, the baseline al-
ways selected the same number of sentences as
AURUM-thresh, but from the beginning of the
article. Although this is a very simple baseline, it
is worth reiterating that it is also a very compet-
itive baseline, which most single-document sum-
marization systems fail to beat due to the nature of
news articles.
Since we are mainly interested in determining
to what extent our system is able to correctly iden-
tify the highlight sources, we chose precision and
recall as evaluation metrics. Precision is the per-
centage of all returned highlight sources which are
correct:
Precision =
|R ? T |
|R|
where R is the set of returned highlight sources
and T is the set of manually identified true sources
in the test set. Recall is defined as the percentage
of all true highlight sources that have been cor-
rectly identified by the system:
Recall =
|R ? T |
|T |
Precision and recall can be combined by using the
F-measure, which is the harmonic mean of the
two:
F-measure =
2(precision ? recall)
precision+ recall
Table 5 shows the results for both experiments
(fixed and thresh) as an average over the
folds. To determine whether the observed differ-
ences between two approaches are statistically sig-
nificant and not just caused by chance, we applied
statistical significance testing. As we did not want
to make the assumption that the score differences
are normally distributed, we used the bootstrap
method, a powerful non-parametric inference test
(Efron, 1979). Improvements at a confidence level
of more than 95% are marked with ???.
We can see that our approach consistently
outperforms the baseline, and most of the
improvements?in particular the F-measure
scores?are statistically significant at the 0.95
level. As to be expected, AURUM-fixed
achieves higher precision gains, while
AURUM-thresh achieves higher recall gains. In
addition, for 83.3 percent of the documents, our
system?s F-measure score is higher than or equal
to that of the baseline.
Figure 4 shows how far down in the documents
our system was able to correctly identify highlight
sources. Although the distribution is still heavily
skewed towards extracting sentences from the be-
ginning of the document, it is so to a lesser extent
than just using positional information as a prior;
see Figure 2.
In a third set of experiments we measured the
n-gram overlap between the sentences we have
identified as highlight sources and the actual story
highlights in the ground truth. To this end we use
420
System Recall Precision F-Measure Extracted
Baseline-fixed 40.69 44.14 42.35 240
AURUM-fixed 41.88 (+2.96%?) 45.40 (+2.85%) 43.57 (+2.88%?) 240
Baseline-thresh 42.91 41.82 42.36 269
AURUM-thresh 44.49 (+3.73%?) 43.30 (+3.53%) 43.88 (+3.59%?) 269
Table 5: Evaluation scores for the four extraction systems.
System ROUGE-1 ROUGE-2
Baseline-fixed 47.73 15.98
AURUM-fixed 49.20 (+3.09%?) 16.53 (+3.63%?)
Baseline-thresh 55.11 19.31
AURUM-thresh 56.73 (+2.96%?) 19.66 (+1.87%)
Table 6: ROUGE scores for AURUM-fixed, returning 4 sentences, and AURUM-thresh, returning
between 3 and 6 sentences.
Figure 4: Position of correctly extracted sources
by AURUM-thresh.
ROUGE (Lin, 2004), a recall-oriented evaluation
package for automatic summarization. ROUGE
operates essentially by comparing n-gram co-
occurrences between a candidate summary and a
number of reference summaries, and comparing
that number in turn to the total number of n-grams
in the reference summaries:
ROUGE-n =
?
S?References
?
ngramn?S
Match(ngramn)
?
S?References
?
ngramn?S
Count(ngramn)
Where n is the length of the n-gram, with lengths
of 1 and 2 words most commonly used in current
evaluations. ROUGE has become the standard tool
for evaluating automatic summaries, though it is
not the optimal system for this experiment. This is
due to the fact that it is geared towards a different
task?as ours is not automatic summarization per
se?and that ROUGE works best judging between
a number of candidate and model summaries. The
ROUGE scores are shown in Table 6.
Similar to the precision and recall scores, our
approach consistently outperforms the baseline,
with all but one difference being statistically sig-
nificant. Furthermore, in 76.2 percent of the doc-
uments, our system?s ROUGE-1 score is higher
than or equal to that of the baseline, and like-
wise for 85.2 percent of ROUGE-2 scores. Our
ROUGE scores and their improvements over the
baseline are comparable to the results of Svore
et al (2007), who optimized their approach to-
wards ROUGE and gained significant improve-
ments from using third-party data resources, both
of which our approach does not require.5
Table 7 shows the unique sentences extracted by
every system, which are the number of sentences
one system extracted correctly while the other did
not; this is thus an intuitive measure of how much
two systems differ. Essentially, a system could
simply pick the first two sentences of each arti-
cle and might thus achieve higher precision scores,
since it is less likely to return ?wrong? sentences.
However, if the scores are similar but there is a
difference in the number of unique sentences ex-
tracted, this means a system has gone beyond the
first 4 sentences and extracted others from deeper
down inside the text.
To get a better understanding of the impor-
tance of the individual features we examined the
weights as determined by YASMET. Table 8 con-
tains example output from the development sets,
with feature selection determined implicitly by
the weights the MaxEnt model assigns, where
non-discriminative features receive a low weight.
5Since the test data of (Svore et al, 2007) is not publicly
available we were unable to carry out a more detailed com-
parison.
421
Clearly, sentence position is of highest impor-
tance, while trigram ?trigger? phrases were quite
important as well. Simple bigrams continued to
be a good indicator of data value, as is often
the case. Proper nouns proved to be a valuable
pointer to new information, but mention of the
news agency?s name had less of an impact than
originally thought. Other particularly significant
features included temporal adjectives, superlatives
and all n-gram measures.
System Unique highlight sources Baseline
AURUM-fixed 11.8 7.2
AURUM-thresh 14.2 7.6
Table 7: Unique recall scores for the systems.
Feature Weight Feature Weight
Sentence pos. 10.23 Superlative 4.15
Proper noun 5.18 Temporal adj. 1.75
Trigger 3-gram 3.70 1-gram score 2.74
Spawn 2-gram 3.73 3-gram score 3.75
CNN mention 1.30 Trigger 2-gram 3.74
Table 8: Typical weights learned from the data.
5 Conclusions
A system for extracting essential facts from a news
article has been outlined here. Finding the data
nuggets deeper down is a cross between keyphrase
extraction and automatic summarization, a task
which requires more elaborate features and param-
eters.
Our approach emphasizes a wide variety of fea-
tures, including many linguistic features. These
features range from the standard (n-gram fre-
quency), through the essential (sentence position),
to the semantic (spawned phrases, verb classes and
types of adverbs).
Our experimental results show that a combina-
tion of statistical and linguistic features can lead
to competitive performance. Our approach not
only outperformed a notoriously difficult baseline
but also achieved similar performance to the ap-
proach of (Svore et al, 2007), without requiring
their third-party data resources.
On top of the statistically significant improve-
ments of our approach over the baseline, we see
value in the fact that it does not settle for sentences
from the beginning of the articles.
Most single-document automatic summariza-
tion systems use other features, ranging from
discourse structure to lexical chains. Consider-
ing Marcu?s conclusion (2003) that different ap-
proaches should be combined in order to create
a good summarization system (aided by machine
learning), there seems to be room yet to use ba-
sic linguistic cues. Seeing as how our linguis-
tic features?which are predominantly semantic?
aid in this task, it is quite possible that further in-
tegration will aid in both automatic summarization
and keyphrase extraction tasks.
References
Ken Barker and Nadia Cornacchia. 2000. Using noun
phrase heads to extract document keyphrases. In
Proceedings of the 13th Conference of the CSCSI,
AI 2000, volume 1882 of Lecture Notes in Artificial
Intelligence, pages 40?52.
Jaime G. Carbonell and Jade Goldstein. 1998. The
use of MMR, diversity-based reranking for reorder-
ing documents and producing summaries. In Pro-
ceedings of SIGIR 1998, pages 335?336.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 132?139.
Wesley T. Chuang and Jihoon Yang. 2000. Extracting
sentence segments for text summarization: A ma-
chine learning approach. In Proceedings of the 23rd
ACM SIGIR, pages 152?159.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge Trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 03 Summarization Workshop, pages 1?8.
Brad Efron. 1979. Bootstrap methods: Another look
at the jackknife. Annals of Statistics, 7(1):1?26.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text docu-
ments: Sentence selection and evaluation metrics.
In Proceedings of the 22nd annual international
ACM SIGIR on Research and Development in IR,
pages 121?128.
Anette Hulth. 2004. Combining Machine Learn-
ing and Natural Language Processing for Automatic
Keyword Extraction. Ph.D. thesis, Department of
Computer and Systems Sciences, Stockholm Uni-
versity.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization?step one: Sentence compres-
sion. In Proceedings of AAAI 2000, pages 703?710.
422
Bruce Krulwich and Chad Burkey. 1996. Learning
user information interests through the extraction of
semantically significant phrases. In M. Hearst and
H. Hirsh, editors, AAAI 1996 Spring Symposium on
Machine Learning in Information Access.
Daniel S. Leite, Lucia H.M. Rino, Thiago A.S. Pardo,
and Maria das Grac?as V. Nunes. 2007. Extrac-
tive automatic summarization: Does more linguis-
tic knowledge make a difference? In TextGraphs-2:
Graph-Based Algorithms for Natural Language Pro-
cessing, pages 17?24, Rochester, New York, USA.
Association for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004), Barcelona, Spain.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Conference on Natural Lan-
guage Learning (CoNLL-2002), pages 49?55.
Daniel Marcu. 1995. Discourse trees are good in-
dicators of importance in text. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Auto-
matic Text Summarization, pages 123?136, Cam-
bridge, MA. MIT Press.
Daniel Marcu. 2003. Automatic abstracting. In Ency-
clopedia of Library and Information Science, pages
245?256.
Kathleen McKeown, Judith Klavans, Vasileios Hatzi-
vassiloglou, Regina Barzilay, and Eleazar Eskin.
1999. Towards multidocument summarization by
reformulation: Progress and prospects. In Proceed-
ing of the 16th national conference of the American
Association for Artificial Intelligence (AAAI-1999),
pages 453?460.
Ani Nenkova. 2005. Automatic text summarization of
newswire: Lessons learned from the document un-
derstanding conference. In 20th National Confer-
ence on Artificial Intelligence (AAAI 2005).
J. Larocca Neto, A.A. Freitas, and C.A.A Kaestner.
2002. Automatic text summarization using a ma-
chine learning approach. In XVI Brazilian Symp. on
Artificial Intelligence, volume 2057 of Lecture Notes
in Artificial Intelligence, pages 205?215.
J. J. Pollock and Antonio Zamora. 1975. Automatic
abstracting research at chemical abstracts service.
Journal of Chemical Information and Computer Sci-
ences, 15(4).
Krysta M. Svore, Lucy Vanderwende, and Christo-
pher J.C. Burges. 2007. Enhancing single-
document summarization by combining RankNet
and third-party sources. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 448?
457.
Gian Lorenzo Thione, Martin van den Berg, Livia
Polanyi, and Chris Culy. 2004. Hybrid text sum-
marization: Combining external relevance measures
with structural analysis. In Proceedings of the ACL-
04, pages 51?55.
Peter D. Turney. 2000. Learning algorithms
for keyphrase extraction. Information Retrieval,
2(4):303?336.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. Kea:
Practical automatic keyphrase extraction. In Pro-
ceedings of the ACM Conference on Digital Li-
braries (DL-99).
423
Document Fusion for Comprehensive Event Description
Christof Monz
Institute for Logic, Language and Computation
University of Amsterdam
1018 TV Amsterdam, The Netherlands
christof@science.uva.nl
www.science.uva.nl/?christof
Abstract
This paper describes a fully imple-
mented system for fusing related news
stories into a single comprehensive de-
scription of an event. The basic compo-
nents and the underlying algorithm are
explained. The system uses a compu-
tationally feasible and robust notion of
entailment for comparing information
stemming from different documents.
We discuss the issue of evaluating doc-
ument fusion and provide some prelim-
inary results.
1 Introduction
Conventional text retrieval systems respond to a
user?s query by providing a (ranked) list of doc-
uments which potentially satisfy the information
need. After having identified a number of docu-
ments which are actually relevant, the user reads
some of those documents to get the information
requested. To be sure to get a comprehensive ac-
count of a particular topic, the list of documents
one has to read may be rather long, including a se-
vere amount of redundancy; i.e., documents par-
tially conveying the same information.
Although this problems basically holds for any
text retrieval situation, where comprehensiveness
is relevant, it becomes particularly evident in the
retrieval of news texts.
News agencies, such as AP, BBC, CNN, or
Reuters, often describe the same event differ-
ently. For instance, they provide different back-
ground information, helping the reader to situate
the story, they interview different people to com-
ment on an event, and they provide additional,
conflicting or more accurate information, depend-
ing on their sources.
To get a description of an event which is as
comprehensive as possible and also as short as
possible, a user has to compile his or her own
description by taking parts of the original news
stories, ignoring duplicate information. Typical
users include journalists and intelligence analysts,
for whom compiling and fusing information is an
integral part of their work (Carbonell et al, 2000).
Obviously, if done manually, this process can be
rather laborious as it involves numerous compar-
isons, depending on the number and length of the
documents.
The aim of this paper is to describe an approach
automatizing this process by fusing information
stemming from different documents to generate
a single comprehensive document, containing the
information of all original documents without re-
peating information which is conveyed by two or
more documents.
The work described in this paper is closely re-
lated to the area of multi-document summariza-
tion (Barzilay et al, 1999; Mani and Bloedorn,
1999; McKeown and Radev, 1995; Radev, 2000),
where related documents are analyzed to use fre-
quently occurring segments for identifying rele-
vant information that has to be included in the
summary. Our work differs from the work on
multi-document summarization as we focus on
document fusion disregarding summarization. On
the contrary, we are not aiming for the shortest
description containing the most relevant informa-
tion, but for the shortest description containing
all information. For instance, even historic back-
ground information is included, as long as it al-
lows the reader to get a more comprehensive de-
scription of an event.
Although the techniques that are used for
multi-document fusion and multi-document sum-
marization are similar, the task of fusion is com-
plementary to the summarization task. They dif-
fer in the way that, roughly speaking, multi-
document summarization is the intersection of
information within a topic, whereas multi-
document fusion is the union of information.
They are similar to the extent that in both cases
nearly equivalent information stemming from dif-
ferent documents within the topic has to be iden-
tified as such.
The remainder of this paper is structured as fol-
lows: Section 2 introduces the main components
and challenges of implementing a document fu-
sion system. Issues of evaluating document fu-
sion and some preliminary evaluation of our sys-
tem are presented in Section 3. In Section 4,
some conclusions and prospects on future work
are given.
2 Fusing Documents
Before developing a document fusion system,
some basic issues have to be considered.
1. On which level of granularity are the docu-
ments fused (i.e., word or phrase level, sen-
tence level, or paragraph level?
2. How to decide whether news fragments from
different sources convey the same informa-
tion?
3. How to ensure readability of the fused docu-
ment? I.e., where should information stem-
ming from different documents be placed in
the fused document, retaining a natural flow
of information.
Each of the these issues is addressed in the fol-
lowing subsections.
2.1 Segmentation
In the current implementation, we decided to
fuse documents on paragraph level for two rea-
sons: First, paragraphs are less context-dependent
than sentences and are therefore easier to com-
pare. Second, compiling paragraphs yields a bet-
ter readability of the fused document. It should
be noted that paragraphs are rather short in news
stories, rarely being longer than three sentences.
When putting together (fusing) pieces of text
from different sources in a way that was not an-
ticipated by the writers of the news stories, it can
introduce information gaps. For instance, if a
paragraph containing a pronoun is taken out of its
original context and placed in a new context (the
fused document), this can lead to dangling pro-
nouns, which cannot be correctly resolved any-
more. In general, this problem does not only hold
for pronouns but for all kind of anaphoric ex-
pressions such as pronouns, definite noun phrases
(e.g., the negotiations) and anaphoric adverbials
(e.g., later). To cope with this problem simple
segmentation is applied as a pre-processing step
where paragraphs that contain pronouns or simple
definite noun phrases are attached to the preced-
ing paragraph. A more sophisticated approach to
text segmentation is described in (Hearst, 1997).
Obviously, it would be better to use an au-
tomatic anaphora resolution component to cope
with this problem, see, e.g., (Kennedy and Bogu-
raev, 1996; Kameyama, 1997), where anaphoric
expressions are replaced by their antecedents, but
at the moment, the integration of such a compo-
nent remains future work.
2.2 Informativity
(Radev, 2000) describes 24 cross-document rela-
tions that can hold between their segments, one of
which is the subsumption (or entailment) relation.
In the context of document fusion, we focus on
the entailment relation and how it can be formally
defined; unfortunately, (Radev, 2000) provides no
formal definition for any of the relations.
Computing the informativity of a segment
compared to another segment is an essential task
during document fusion. Here, we say that the i-
th segment of document d (si,d) is more informa-
tive than the j-th segment of document d? (sj,d?) if
si,d entails sj,d? . In theory, this should be proven
logically, but in practice this is far beyond the cur-
rent state of the art in natural language processing.
Additionally, a binary logical decision might also
be too strict for simulating the human understand-
ing of entailment.
A simple but nevertheless quite effective solu-
tion is based on one of the simpler similarity mea-
sures in information retrieval (IR), where texts are
simply represented as bags of (weighted) words.
The definition of the entailment score (es) is given
in (1). es(si,d, sj,d?) compares the sum of the
weights of terms that appear in both segments to
the total sum weights of sj,d? .
es(si,d, sj,d?) =
?
tk?si,d?sj,d?
idf k
?
tk?sj,d?
idf k
(1)
The weight of a term ti is its inverse document
frequency (idf i), as defined in (2), where N is the
number of all segments in the set of related doc-
uments (the topic) and ni is the number of seg-
ments in which the term ti occurs.
idf i = log
(
N
ni
)
(2)
Terms which occur in many segments (i.e., for
which ni is rather large), such as the, some, etc.,
receive a lower idf -score than terms that occur
only in a few segments. The underlying intuition
of the idf -score is that terms with a higher idf -
score are better suited for discriminating the con-
tent of a particular segment from the other seg-
ments in the topic, or to put it differently, they are
more content-bearing. Note, that the logarithm in
(2) is only used for dampening the differences.
The entailment score es(si,d, sj,d?) measures
how many of the words of the segment si,d oc-
cur in sj,d? , and how important those words are.
This is obviously a very shallow approach to en-
tailment computation, but nevertheless it proved
to be effective, see (Monz and de Rijke, 2001).
2.3 Implementation
In this subsection, we present the general algo-
rithm underlying the implementation, given a set
of documents belonging to topic T . The imple-
mentation has to tackle two basic tasks. First,
identify segments that are entailed by other seg-
ments and use the more informative one. Second,
place the remaining segments at positions with
similar content. The fusion algorithm depicted in
Figure 1 consists of five steps.
1. is basically a pre-processing step as ex-
plained above. 2. computes pairwise the cross-
document entailment scores for all segments in T .
Although the pairwise computation of es and sim
is exponential in the number of documents in T ,
it still remains computationally tractable in prac-
tice. For instance, for a topic containing 4 docu-
ments (the average case) it takes 10 CPU seconds
to compute all entailment and similarity relations.
For a topic containing 8 documents (an artificially
constructed extreme case) it takes 66 CPU sec-
onds; both on a 600 MHz Pentium III PC.
In 3., one of the documents is taken as base for
the fusion process. Starting with a ?real? docu-
ment improves the readability of the final fused
documents as it imposes some structure on the
fusion process. There are several ways to select
the base document. For instance, take the docu-
ment with the most unique terms, or the document
with the highest document weight (sum of all idf -
scores). In the current implementation we simply
took the longest document within the topic, which
ensures a good base coverage of an event.
4. and 5. are the actual fusion steps. Step 4. re-
places a segment si,dF in the fused document by
a segment sj,d? from another document if sj,d? is
the segment maximally entailing si,dF and if it is
significantly (above the threshold ?es ) more infor-
mative than si,dF . Choosing an optimal value for
?es is essential for the effectiveness of the fusion
system. Section 3 discusses some of our experi-
ments to determine ?es .
Step 5. is kind of complementary to step 4.,
where related but more informative segments are
identified. Step 5. identifies segments that add
new information to dF , where a segment sj,d? is
new if it has low similarity to all segments in dF ,
i.e., if the the similarity score is below the thresh-
old ?sim . If a segment sj,d? is new, it is placed
right after the segment in dF to which it is most
similar.
Similarity is implemented as the traditional co-
sine similarity in information retrieval, as defined
in (3). This similarity measure is also known
as the tfc.tfc measure, see (Salton and Buckley,
1988).
sim(si,d, sj,d?) =
?
tk?si,d?sj,d?
wk,si,d ? wk,sj,d?
? ?
tk?si,d
w2si,d ?
?
tk?sj,d?
w2sj,d?
(3)
Where wk,si,d is the weight associated with the
term tk in segment si,d. In the nominator of (3),
1. segmentize all documents in T
2. for all si,d, sj,d? s.t. d, d? ? T and d 6= d?: compute es(si,d, sj,d?)
3. select a document d ? T as fusion base document: dF
4. for all si,dF : find sj,d? s.t. dF 6= d? and sj,d? = arg maxsk,d?
: es(sk,d? , si,dF ) > es(si,dF , sk,d?)
if es(sj,d? , si,dF ) > ?es then replace si,dF by sj,d? in the fused document
5. for all sj,d? s.t. sj,d? 6? dF :
if for all si,dF : sim(si,dF , sj,d?) < ?sim ,
then find the most similar si,dF : si,dF = arg maxsk,dF
: sim(sj,d? , sk,dF )
and place sj,d? between si,dF and si+1,dF
Figure 1: Sketch of the document fusion algorithm.
the weights of the terms that occur in si,d and sj,d?
are summed up. The denominator is used for nor-
malization. Otherwise, longer documents tend to
result in a higher similarity score. In the current
implementation wk,si,d = idf k for all si,d. The
reader is referred to (Salton and Buckley, 1988;
Zobel and Moffat, 1998) for a broad spectrum of
similarity measures for information retrieval.
3 Evaluation Issues
The document fusion system is evaluated in two
steps. First, the effectiveness of entailment detec-
tion is evaluated, which is the key component of
our system. Then we present some preliminary
evaluation of the whole system focusing on the
quality of the fused documents.
3.1 Evaluating Entailment
Recently, we have started to build a small test col-
lection for evaluating entailment relations. The
reader is referred to (Monz and de Rijke, 2001)
for more details on the results presented in this
subsection.
For each of the 21 topics in our test corpus two
documents in the topic were randomly selected,
and given to a human assessor to determine all
subsumption relations between segments in dif-
ferent documents (within the same topic). Judg-
ments were made on a scale 0?2, according to the
extent to which one segment was found to entail
another.
Out of the 12083 possible subsumption rela-
tions between the text segments, 501 (4.15%) re-
ceived a score of 1, and 89 (0.73%) received a
score of 2.
Let a subsumption pair be an ordered pair of
segments (si,d, sj,d?) that may or may not stand
in the subsumption relation, and let a correct sub-
sumption pair be a subsumption pair (si,d, sj,d?)
for which si,d does indeed entail sj,d? . Further, a
computed subsumption pair is a subsumption pair
for which our subsumption method has produced
a score above the subsumption threshold.
Then, precision is the fraction of computed
subsumption pairs that is correct:
Precision = number of correct subsumption pairs computed
total number of subsumption pairs computed .
And recall is the proportion of the total number of
correct subsumption pairs that were computed:
Recall = number of correct subsumption pairs computed
total number of correct subsumption pairs .
Observe that precision and recall depend on the
subsumption threshold that we use.
We computed average recall and precision at 11
different subsumption thresholds, ranging from 0
to 1, with .1 increments; the average was com-
puted over all topics. The results are summarized
in Figures 2 (a) and (b).
Since precision and recall suggest two differ-
ent optimal subsumption thresholds, we use the F-
Score, or harmonic mean, which has a high value
only when both recall and precision are high.
F =
2
1
Recall +
1
Precision
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 Subsumption Threshold
 
Prec
ision
Human Judgm. > 0Human Judgm. > 1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 Subsumption Threshold
 
Reca
ll
Human Judgm. > 0Human Judgm. > 1
(a) (b)
Figure 2: (a) Average precision with human judgments > 0 and > 1. (b) Average recall with human
judgments > 0 and > 1.
The average F -scores are given in Figure 3.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 Subsumption Threshold
 
F?Sc
ore
Human Judgm. > 0Human Judgm. > 1
Figure 3: Average F -scores with human judg-
ments > 0 and > 1.
The optimal subsumption threshold for human
judgments > 0 is around 0.18, while it is approx-
imately 0.4 for human judgments > 1. This con-
firms the intuition that a higher threshold is more
effective when human judgments are stricter.
3.2 Evaluating Fusion
In the introduction, it was pointed out that docu-
ment fusion by hand can be rather laborious, and
the same holds for the evaluation of automatic
document fusion. Similar to automatic summa-
rization, there are no standard document collec-
tions or clear evaluation criteria aiding to autom-
atize the process of evaluation. One approach
could be to focus on news stories which mention
their sources. For instance CNN?s new stories of-
ten say that ?AP and Reuters contributed to this
story?. On the other hand one has to be cautious
to take those news stories as gold standard as the
respective contributions of the journalist and his
or her sources are not made explicit.
In the area of multi-document summarization,
there is a distinction between intrinsic and extrin-
sic evaluation, see (Mani et al, 1998). Intrin-
sic evaluation judges the quality directly based on
analysis of the summary. Usually, a human judge
assesses the quality of a summary based on some
standardized evaluation criteria.
In extrinsic evaluation, the usefulness of a sum-
mary is judged based on how it affects the com-
pletion of some other task. A typical task used
for extrinsic evaluation is ad-hoc retrieval, where
the relevance of a retrieved document is assessed
by a human judge based on the document?s sum-
mary. Then, those judgments are compared to
judgments based on original documents, see, e.g.,
(Brandow et al, 1995; Mani and Bloedorn, 1999).
At this stage we have just carried out some pre-
liminary evaluation. The test collection consists
of 69 news stories categorized into 21 topics. Cat-
egorization was done by hand, but it is also pos-
sible to have information filtering, see (Robertson
and Hull, 2001), or topic detection and tracking
(TDT) tools carrying out this task (Allan et al,
1998). All documents belonging to the same topic
were released on the same day and describe the
same event. Table 1 provides further details on
the collection.
avg. per topic
no. of docs. 3.3 docs.
length of a doc. 612 words
length of all docs. together 2115 words
length of longest doc. 783 words
length of shortest doc. 444 words
Table 1: Test collection (21 topics, 69 docu-
ments).
In addition to the aforementioned news agen-
cies, the collection includes texts from the L.A.
Times, Washington Post and Washington Times.
In general, a segment should be included in the
fused document if it did not occur before to avoid
redundancy (False Alarm), and if it adds informa-
tion, so no information is left out (Miss). As in IR
or TDT, Miss and False Alarm tend to be inversely
related; i.e., a decrease of Miss often results in an
increase of False Alarm and vice versa.
Table 2 illustrates the different possibilities
how the system responds as to whether a seg-
ment should be included in the fused document
and how a human reader judges.
system reader: reader:
judgement include exclude
include a b
exclude c d
Table 2: Contingency table.
Then, Miss and False Alarm can be defined as
in (4) and (5), respectively.
Miss =
c
a+ c
if a+ c > 0 (4)
False Alarm =
b
b+ d
if b+ d > 0 (5)
The fusion impact factor (fif) describes to what
extent the different sources actually contributed to
the fused document. For instance if the fused doc-
ument solely contains segments from one source,
fif equals 0, and if all sources equally contributed
it equals 1. This can be formalized as follows:
fif = 1?
?
d?T
| 1/nT ? nseg,d/nseg |
2 ? (1? 1/nT )
(6)
Where S is a set of related documents, and nT
is its size. nseg is the number of segments in the
fused document and nseg,d is the number of seg-
ments stemming from document d.
For our test collection, the average fusion im-
pact factor was 0.56. Of course the fif -score de-
pends on the choice of ?es and ?sim , in a way that
a lower value of ?es or a higher value of ?sim in-
creases the fif -score. In this case, ?es = 0.2 and
?sim = 0.05.
Table 3 shows the length of the fused docu-
ments in average compared to the longest, short-
est, and all documents in a topic, for ?es = 0.2
and ?sim = 0.05.
avg. compression
ratio per topic
all docs. together 0.55
longest doc. 1.36
shortest doc. 2.55
Table 3: Compression ratios.
Measuring Miss intrinsically is extremely labo-
rious; especially comparing the effectiveness of
different values for the thresholds ?es and ?sim is
infeasible in practice. Therefore, we decided to
measure Miss extrinsically. We used ad-hoc re-
trieval as the extrinsic evaluation task. The eval-
uation criterion is stated as follows: Using the
fused document of each topic as a query, what is
the average (non-interpolated) precision?
As baseline, we concatenated all documents
of each topic. This would constitute an event
description that does not miss any information
within the topic. This document is then used to
query a collection of 242,996 documents, con-
taining the 69 documents from our test collection.
Since the baseline is simply the concatenation of
all documents within the topic, one can expect
that all documents from that topic receive a high
rank in the set of retrieved documents. This aver-
age precision forms the optimal performance for
that topic. For instance, if a topic contains three
documents, and the ad-hoc retrieval ranks those
documents as 1, 3, and 6, there are three recall
levels: 33.3?%, 66.6?%, and 100%. The precision
at these levels is 1/1, 2/3, and 3/6 respectively,
which averages to 0.72?.
The next step is to compare the actually fused
documents to the baseline. It is to be expected that
the performance is worse, because the fused docu-
ments do not contain segments which are entailed
by other segments in the topic. For instance, if
the fused document for the aforementioned topic
is used as a query and the original documents of
the topic are ranked as 2, 4, and 9, the average
precision is (1/2 + 2/4 + 3/9)/3 = 0.4?.
Compared to 0.72? for the baseline, fusion leads
to a decrease of effectiveness of approximately
38.5%. Figure 4, gives the averaged precision for
the different values for ?es .
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.6
0.7
0.8
0.9
1
 Entailment Threshold 
 
Avge
rage
 Prec
ision
SystemBaseline
Figure 4: Average precision for ad-hoc retrieval.
It is not obvious how to interpret the numerical
value of the ad-hoc retrieval precision in terms of
Miss, but the degree of deviation from the base-
line gives a rough estimate of the completeness
of a fused document. At least, this allows for an
ordinally scaled ranking of the different methods
(in our case different values for ?es ), that are used
for generating the fused documents. Figure 4 il-
lustrates that in the context of the ad-hoc retrieval
evaluation an optimal entailment threshold (?es )
lies around 0.2. Table 4 shows the decrease in re-
trieval effectiveness in percent, compared to the
baseline. The average precision at 0.2 is 0.8614,
which is just ? 11.5% below the baseline.
For all ad-hoc retrieval experiments, the Lnu.ltu
weighting scheme, see (Singhal et al, 1996), has
been used, which is one of the best-performing
weighting schemes in ad-hoc retrieval. In addi-
tion to the 69 documents from our collection, the
retrieval collection contains articles from Associ-
Decrease in Decrease in
?es precision ?es precision
0.0 20.9% 0.6 23.8%
0.1 14.6% 0.7 23.8%
0.2 11.5% 0.8 23.8%
0.3 13.7% 0.9 23.8%
0.4 22.0% 1.0 24.4%
0.5 22.2%
Table 4: Differences to baseline retrieval.
ated Press 1988?1990 (from the TREC distribu-
tion), which also belong to the newswire or news-
paper domain. Any meta information such as the
name of the journalist or news agency is removed
to avoid matches based on that information.
In the context of multi-document summariza-
tion, (Stein et al, 2000) use topic clustering for
extrinsic evaluation. Although we did not carry
out any evaluation based on topic clustering, it
seems that it could also be applied to multi-
document fusion, given the close relationship be-
tween fusion and summarization on the one hand
and retrieval and clustering on the other hand.
4 Conclusions
The document fusion system described is just pro-
totype and there is much more space for improve-
ment. Although detecting redundancies by using
a shallow notion of entailment works reasonably
well, it is still far from perfect.
In the current implementation, text analysis is
very shallow. Pattern matching is used to avoid
dangling anaphora and lemmatization is used to
make the entailment and similarity scores unsus-
ceptible to morphological variations such as num-
ber and tense. A question for future research is to
what extent shallow parsing techniques can im-
prove the entailment scores. In particular, does
considering the relational structure of a sentence
improve computing entailment relations? This
has shown to be successful in inference-based ap-
proaches to question-answering, see (Harabagiu
et al, 2000), and document fusion might also ben-
efit from representations that are a bit deeper than
the one discussed in this paper.
Another open issue at this point is the need for
standards for evaluating the quality of document
fusion. We think that this can be done by using
standard IR measures like Miss and False Alarm.
Although Miss can be approximated extrinsically,
it is unclear whether this also possible for False
Alarm. Obviously, intrinsic evaluation is more re-
liable, but it remains an extremely laborious pro-
cess, where inter-judge disagreement is still an is-
sue, see (Radev et al, 2000).
Acknowledgments
The author would like to thank Maarten de Rijke
for providing the entailment judgments. This
work was supported by the Physical Sci-
ences Council with financial support from the
Netherlands Organization for Scientific Research
(NWO), project 612-13-001.
References
J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. 1998. Topic detection and tracking pilot
study final report. In Proceedings of the Broadcast
News Transcription and Understranding Workshop
(Sponsored by DARPA).
R. Barzilay, K. McKeown, and M. Elhadad. 1999. In-
formation fusion in the context of multi-document
summarization. In Proceedings of the 37th Annual
Meeting of the Association of Computational Lin-
guistics (ACL?99).
R. Brandow, K. Mitze, and L. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection. Information Processing & Management,
31(5):675?685.
J. Carbonell, D. Harman, E. Hovy, S. Maiorano,
J. Prange, and Sparck-Jones. K. 2000. Vision
statement to guide research in question answering
(Q&A) and text summarization. NIST Draft Publi-
cation.
S. Harabagiu, M. Pasca, and S. Maiorano. 2000. Ex-
periments with open-domain textual question an-
swering. In Proceedings of COLING-2000.
M. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
M. Kameyama. 1997. Recognizing referential
links: An information extraction perspective. In
R. Mitkov and B. Boguraev, editors, Proceedings
of ACL/EACL-97 Workshop on Operational Factors
in Practical, Robust Anaphora Resolution for Unre-
stricted Texts, pages 46?53.
C. Kennedy and B. Boguraev. 1996. Anaphora for
everyone: Pronominal anaphora resolution without
a parser. In Proceedings of the 16th International
Conference on Computational Linguistics (COL-
ING?96). Association for Computational Linguis-
tics.
I. Mani and E. Bloedorn. 1999. Summarizing sim-
ilarities and differences among related documents.
Information Retrieval, 1(1?2):35?67.
I. Mani, D. House, G. Klein, L. Hirschman, L. Obrst,
T. Firmin, M. Chrzanowski, and B. Sundheim.
1998. The Tipster SUMMAC text summariza-
tion evaluation, final report. Technical Report
98W0000138, Mitre.
K. McKeown and D. Radev. 1995. Generating sum-
maries of multiple news articles. In Proceedings of
the 18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 74?82.
C. Monz and M. de Rijke. 2001. Light-weight sub-
sumption checking for computational semantics. In
P. Blackburn and M. Kohlhase, editors, Proceedings
of the 3rd Workshop on Inference in Computational
Semantics (ICoS-3).
D. Radev, H. Jing, and M. Budzikowska. 2000.
Centroid-based summarization of multiple docu-
ments: Clustering, sentence extraction, and evalu-
ation. In Proceedings of the ANLP/NAACL-2000
Workshop on Summarization.
D. Radev. 2000. A common theory of information
fusion from multiple text sources, step one: Cross-
document structure. In In Proceedings of the 1st
ACL SIGDIAL Workshop on Discourse and Dia-
logue.
S. Robertson and D. Hull. 2001. The TREC-9 fil-
tering track final report. In Proceedings of The 9th
Text Retrieval Conference (TREC-9). NIST Special
Publication.
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information
Processing & Management, 24(5):513?523.
A. Singhal, G. Salton, M. Mitra, and C. Buckley.
1996. Document length normalization. Informa-
tion Processing & Management, 32(5):619?633.
G. Stein, G. Wise, T. Strzalkowski, and A. Bagga.
2000. Evaluating summaries for multiple docu-
ments in an interactive environment. In Proceed-
ings of the Second International Conference on
Language Resources and Evaluation (LREC?00),
pages 1651?1657.
J. Zobel and A. Moffat. 1998. Exploring the similarity
space. ACM SIGIR Forum, 32(1):18?34.
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 1?28,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Findings of the 2009 Workshop on Statistical Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb cs jhu edu
Philipp Koehn
University of Edinburgh
pkoehn inf ed ac uk
Christof Monz
University of Amsterdam
christof science uva nl
Josh Schroeder
University of Edinburgh
j schroeder ed ac uk
Abstract
This paper presents the results of the
WMT09 shared tasks, which included a
translation task, a system combination
task, and an evaluation task. We con-
ducted a large-scale manual evaluation of
87 machine translation systems and 22
system combination entries. We used the
ranking of these systems to measure how
strongly automatic metrics correlate with
human judgments of translation quality,
for more than 20 metrics. We present a
new evaluation technique whereby system
output is edited and judged for correctness.
1 Introduction
This paper presents the results of the shared tasks
of the 2009 EACL Workshop on Statistical Ma-
chine Translation, which builds on three previ-
ous workshops (Koehn and Monz, 2006; Callison-
Burch et al, 2007; Callison-Burch et al, 2008).
There were three shared tasks this year: a transla-
tion task between English and five other European
languages, a task to combine the output of multiple
machine translation systems, and a task to predict
human judgments of translation quality using au-
tomatic evaluation metrics. The performance on
each of these shared task was determined after a
comprehensive human evaluation.
There were a number of differences between
this year?s workshop and last year?s workshop:
? Larger training sets ? In addition to annual
increases in the Europarl corpus, we released
a French-English parallel corpus verging on 1
billion words. We also provided large mono-
lingual training sets for better language mod-
eling of the news translation task.
? Reduced number of conditions ? Previ-
ous workshops had many conditions: 10
language pairs, both in-domain and out-of-
domain translation, and three types of man-
ual evaluation. This year we eliminated
the in-domain Europarl test set and defined
sentence-level ranking as the primary type of
manual evaluation.
? Editing to evaluate translation quality ?
Beyond ranking the output of translation sys-
tems, we evaluated translation quality by hav-
ing people edit the output of systems. Later,
we asked annotators to judge whether those
edited translations were correct when shown
the source and reference translation.
The primary objectives of this workshop are to
evaluate the state of the art in machine transla-
tion, to disseminate common test sets and pub-
lic training data with published performance num-
bers, and to refine evaluation methodologies for
machine translation. All of the data, translations,
and human judgments produced for our workshop
are publicly available.1 We hope they form a
valuable resource for research into statistical ma-
chine translation, system combination, and auto-
matic evaluation of translation quality.
2 Overview of the shared translation and
system combination tasks
The workshop examined translation between En-
glish and five other languages: German, Spanish,
French, Czech, and Hungarian. We created a test
set for each language pair by translating news-
paper articles. We additionally provided training
data and a baseline system.
1http://statmt.org/WMT09/results.html
1
2.1 Test data
The test data for this year?s task was created by
hiring people to translate news articles that were
drawn from a variety of sources during the pe-
riod from the end of September to mid-October
of 2008. A total of 136 articles were selected, in
roughly equal amounts from a variety of Czech,
English, French, German, Hungarian, Italian and
Spanish news sites:2
Hungarian: hvg.hu (10), Napi (2), MNO (4),
Ne?pszabadsa?g (4)
Czech: iHNed.cz (3), iDNES.cz (4), Li-
dovky.cz (3), aktua?lne?.cz (2), Novinky (1)
French: dernieresnouvelles (1), Le Figaro (2),
Les Echos (4), Liberation (4), Le Devoir (9)
Spanish: ABC.es (11), El Mundo (12)
English: BBC (11), New York Times (6), Times
of London (4),
German: Su?ddeutsche Zeitung (3), Frankfurter
Allgemeine Zeitung (3), Spiegel (8), Welt (3)
Italian: ADN Kronos (5), Affari Italiani (2),
ASCA (1), Corriere della Sera (4), Il Sole 24
ORE (1), Il Quotidiano (1), La Republica (8)
Note that Italian translation was not one of this
year?s official translation tasks.
The translations were created by the members
of EuroMatrix consortium who hired a mix of
professional and non-professional translators. All
translators were fluent or native speakers of both
languages. Although we made efforts to proof-
read all translations, many sentences still contain
minor errors and disfluencies. All of the transla-
tions were done directly, and not via an interme-
diate language. For instance, each of the 20 Hun-
garian articles were translated directly into Czech,
English, French, German, Italian and Spanish.
The total cost of creating the test sets consisting
of roughly 80,000 words across 3027 sentences in
seven languages was approximately 31,700 euros
(around 39,800 dollars at current exchange rates,
or slightly more than $0.08/word).
Previous evaluations additionally used test sets
drawn from the Europarl corpus. Our rationale be-
hind discontinuing the use of Europarl as a test set
was that it overly biases towards statistical systems
that were trained on this particular domain, and
2For more details see the XML test files. The docid
tag gives the source and the date for each document in the
test set, and the origlang tag indicates the original source
language.
that European Parliament proceedings were less of
general interest than news stories. We focus on a
single task since the use of multiple test sets in the
past spread our resources too thin, especially in the
manual evaluation.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
train language models, and development sets to
tune parameters. Some statistics about the train-
ing materials are given in Figure 1.
109 word parallel corpus
To create the large French-English parallel cor-
pus, we conducted a targeted web crawl of bilin-
gual web sites. These sites came from a variety of
sources including the Canadian government, the
European Union, the United Nations, and other
international organizations. The crawl yielded on
the order of 40 million files, consisting of more
than 1TB of data. Pairs of translated documents
were identified using a set of simple heuristics to
transform French URLs into English URLs (for in-
stance, by replacing fr with en). Documents that
matched were assumed to be translations of each
other.
All HTML and PDF documents were converted
into plain text, which yielded 2 million French
files paired with their English equivalents. Text
files were split so that they contained one sen-
tence per line and had markers between para-
graphs. They were sentence-aligned in batches of
10,000 document pairs, using a sentence aligner
that incorporates IBM Model 1 probabilities in ad-
dition to sentence lengths (Moore, 2002). The
document-aligned corpus contained 220 million
segments with 2.9 billion words on the French side
and 215 million segments with 2.5 billion words
on the English side. After sentence alignment,
there were 177 million sentence pairs with 2.5 bil-
lion French words and 2.2 billion English words.
The sentence-aligned corpus was cleaned to re-
move sentence pairs which consisted only of num-
bers or paragraph markers, or where the French
and English sentences were identical. The later
step helped eliminate documents that were not
actually translated, which was necessary because
we did not perform language identification. After
cleaning, the parallel corpus contained 105 million
sentence pairs with 2 billion French words and 1.8
billion English words.
2
Europarl Training Corpus
Spanish? English French? English German? English
Sentences 1,411,589 1,428,799 1,418,115
Words 40,067,498 41,042,070 44,692,992 40,067,498 39,516,645 37,431,872
Distinct words 154,971 108,116 129,166 107,733 320,180 104,269
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 74,512 64,223 82,740 79,930
Words 2,052,186 1,799,312 1,831,149 1,560,274 2,051,369 1,977,200 1,733,865 1,891,559
Distinct words 56,578 41,592 46,056 38,821 92,313 43,383 105,280 41,801
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
Hunglish Training Corpus CzEng Training Corpus
Hungarian? English
Sentences 1,517,584
Words 26,114,985 31,467,693
Distinct words 717,198 192,901
Czech? English
Sentences 1,096,940
Words 15,336,783 17,909,979
Distinct words 339,683 129,176
Europarl Language Model Data
English Spanish French German
Sentence 1,658,841 1,607,419 1,676,435 1,713,715
Words 44,983,136 45,382,287 50,577,097 41,457,414
Distinct words 117,577 162,604 138,621 348,197
News Language Model Data
English Spanish French German Czech Hungarian
Sentence 21,232,163 1,626,538 6,722,485 10,193,376 5,116,211 4,209,121
Words 504,094,159 48,392,418 167,204,556 185,639,915 81,743,223 86,538,513
Distinct words 1,141,895 358,664 660,123 1,668,387 929,318 1,313,578
News Test Set
English Spanish French German Czech Hungarian Italian
Sentences 2525
Words 65,595 68,092 72,554 62,699 55,389 54,464 64,906
Distinct words 8,907 10,631 10,609 12,277 15,387 16,167 11,046
News System Combination Development Set
English Spanish French German Czech Hungarian Italian
Sentences 502
Words 11,843 12,499 12,988 11,235 9,997 9,628 11,833
Distinct words 2,940 3,176 3,202 3,471 4,121 4,133 3,318
Figure 1: Statistics for the training and test sets used in the translation task. The number of words is
based on the provided tokenizer and the number of distinct words is the based on lowercased tokens.
3
In addition to cleaning the sentence-aligned par-
allel corpus we also de-duplicated the corpus, re-
moving all sentence pairs that occured more than
once in the parallel corpus. Many of the docu-
ments gathered in our web crawl were duplicates
or near duplicates, and a lot of the text is repeated,
as with web site navigation. We further elimi-
nated sentence pairs that varied from previous sen-
tences by only numbers, which helped eliminate
template web pages such as expense reports. We
used a Bloom Filter (Talbot and Osborne, 2007) to
do de-duplication, so it may have discarded more
sentence pairs than strictly necessary. After de-
duplication, the parallel corpus contained 28 mil-
lion sentence pairs with 0.8 billion French words
and 0.7 billion English words.
Monolingual news corpora
We have crawled the news sources that were the
basis of our test sets (and a few more additional
sources) since August 2007. This allowed us to
assemble large corpora in the target domain to be
mainly used as training data for language mod-
eling. We collected texts from the beginning of
our data collection period to one month before the
test set period, segmented these into sentences and
randomized the order of the sentences to obviate
copyright concerns.
2.3 Baseline system
To lower the barrier of entry for newcomers to the
field, we provided Moses, an open source toolkit
for phrase-based statistical translation (Koehn et
al., 2007). The performance of this baseline sys-
tem is similar to the best submissions in last year?s
shared task. Twelve participating groups used the
Moses toolkit for the development of their system.
2.4 Submitted systems
We received submissions from 22 groups from
20 institutions, as listed in Table 1, a similar
turnout to last year?s shared task. Of the 20
groups that participated with regular system sub-
missions in last year?s shared task, 12 groups re-
turned this year. A major hurdle for many was
a DARPA/GALE evaluation that occurred at the
same time as this shared task.
We also evaluated 7 commercial rule-based MT
systems, and Google?s online statistical machine
translation system. We note that Google did not
submit an entry itself. Its entry was created by
the WMT09 organizers using Google?s online sys-
tem.3 In personal correspondence, Franz Och
clarified that the online system is different from
Google?s research system in that it runs at faster
speeds at the expense of somewhat lower transla-
tion quality. On the other hand, the training data
used by Google is unconstrained, which means
that it may have an advantage compared to the re-
search systems evaluated in this workshop, since
they were trained using only the provided materi-
als.
2.5 System combination
In total, we received 87 primary system submis-
sions along with 42 secondary submissions. These
were made available to participants in the sys-
tem combination shared task. Based on feedback
that we received on last year?s system combina-
tion task, we provided two additional resources to
participants:
? Development set: We reserved 25 articles
to use as a dev set for system combina-
tion (details of the set are given in Table
1). These were translated by all participating
sites, and distributed to system combination
participants along with reference translations.
? n-best translations: We requested n-best
lists from sites whose systems could produce
them. We received 25 100-best lists accom-
panying the primary system submissions, and
5 accompanying the secondary system sub-
missions.
In addition to soliciting system combination en-
tries for each of the language pairs, we treated sys-
tem combination as a way of doing multi-source
translation, following Schroeder et al (2009). For
the multi-source system combination task, we pro-
vided all 46 primary system submissions from any
language into English, along with an additional 32
secondary systems.
Table 2 lists the six participants in the system
combination task.
3 Human evaluation
As with past workshops, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention
3http://translate.google.com
4
ID Participant
CMU-STATXFER Carnegie Mellon University?s statistical transfer system (Hanneman et al, 2009)
COLUMBIA Columbia University (Carpuat, 2009)
CU-BOJAR Charles University Bojar (Bojar et al, 2009)
CU-TECTOMT Charles University Tectogramatical MT (Bojar et al, 2009)
DCU Dublin City University (Du et al, 2009)
EUROTRANXP commercial MT provider from the Czech Republic
GENEVA University of Geneva (Wehrli et al, 2009)
GOOGLE Google?s production system
JHU Johns Hopkins University (Li et al, 2009)
JHU-TROMBLE Johns Hopkins University Tromble (Eisner and Tromble, 2006)
LIMSI LIMSI (Allauzen et al, 2009)
LIU Linko?ping University (Holmqvist et al, 2009)
LIUM-SYSTRAN University of Le Mans / Systran (Schwenk et al, 2009)
MORPHO Morphologic (Nova?k, 2009)
NICT National Institute of Information and Comm. Tech., Japan (Paul et al, 2009)
NUS National University of Singapore (Nakov and Ng, 2009)
PCTRANS commercial MT provider from the Czech Republic
RBMT1-5 commercial systems from Learnout&Houspie, Lingenio, Lucy, PROMT, SDL
RWTH RWTH Aachen (Popovic et al, 2009)
STUTTGART University of Stuttgart (Fraser, 2009)
SYSTRAN Systran (Dugast et al, 2009)
TALP-UPC Universitat Politecnica de Catalunya, Barcelona (R. Fonollosa et al, 2009)
UEDIN University of Edinburgh (Koehn and Haddow, 2009)
UKA University of Karlsruhe (Niehues et al, 2009)
UMD University of Maryland (Dyer et al, 2009)
USAAR University of Saarland (Federmann et al, 2009)
Table 1: Participants in the shared translation task. Not all groups participated in all language pairs.
ID Participant
BBN-COMBO BBN system combination (Rosti et al, 2009)
CMU-COMBO Carnegie Mellon University system combination (Heafield et al, 2009)
CMU-COMBO-HYPOSEL CMU system comb. with hyp. selection (Hildebrand and Vogel, 2009)
DCU-COMBO Dublin City University system combination
RWTH-COMBO RWTH Aachen system combination (Leusch et al, 2009)
USAAR-COMBO University of Saarland system combination (Chen et al, 2009)
Table 2: Participants in the system combination task.
5
Language Pair Sentence Ranking Edited Translations Yes/No Judgments
German-English 3,736 1,271 4,361
English-German 3,700 823 3,854
Spanish-English 2,412 844 2,599
English-Spanish 1,878 278 837
French-English 3,920 1,145 4,491
English-French 1,968 332 1,331
Czech-English 1,590 565 1,071
English-Czech 7,121 2,166 9,460
Hungarian-English 1,426 554 1,309
All-English 4,807 0 0
Multisource-English 2,919 647 2184
Totals 35,786 8,655 31,524
Table 3: The number of items that were judged for each task during the manual evaluation.
that automatic measures are an imperfect substi-
tute for human assessment of translation quality.
Therefore, we define the manual evaluation to be
primary, and use the human judgments to validate
automatic metrics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct it on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partic-
ipants, interested volunteers, and a small number
of paid annotators. More than 160 people partic-
ipated in the manual evaluation, with 100 people
putting in more than an hour?s worth of effort, and
30 putting in more than four hours. A collective
total of 479 hours of labor was invested.
We asked people to evaluate the systems? output
in two different ways:
? Ranking translated sentences relative to each
other. This was our official determinant of
translation quality.
? Editing the output of systems without dis-
playing the source or a reference translation,
and then later judging whether edited transla-
tions were correct.
The total number of judgments collected for the
different modes of annotation is given in Table 3.
In all cases, the output of the various translation
outputs were judged on equal footing; the output
of system combinations was judged alongside that
of the individual system, and the constrained and
unconstrained systems were judged together.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the in-
structions simple:
Rank translations from Best to Worst rel-
ative to the other choices (ties are al-
lowed).
In our the manual evaluation, annotators were
shown at most five translations at a time. For most
language pairs there were more than 5 systems
submissions. We did not attempt to get a com-
plete ordering over the systems, and instead relied
on random selection and a reasonably large sample
size to make the comparisons fair.
Relative ranking is our official evaluation met-
ric. Individual systems and system combinations
are ranked based on how frequently they were
judged to be better than or equal to any other sys-
tem. The results of this are reported in Section 4.
Appendix A provides detailed tables that contain
pairwise comparisons between systems.
3.2 Editing machine translation output
We experimented with a new type of evaluation
this year where we asked judges to edit the output
of MT systems. We did not show judges the refer-
ence translation, which makes our edit-based eval-
uation different than the Human-targeted Trans-
lation Error Rate (HTER) measure used in the
DARPA GALE program (NIST, 2008). Rather
than asking people to make the minimum number
of changes to the MT output in order capture the
same meaning as the reference, we asked them to
6
edit the translation to be as fluent as possible with-
out seeing the reference. Our hope was that this
would reflect people?s understanding of the out-
put.
The instructions that we gave our judges were
the following:
Correct the translation displayed, mak-
ing it as fluent as possible. If no correc-
tions are needed, select ?No corrections
needed.? If you cannot understand the
sentence well enough to correct it, select
?Unable to correct.?
Each translated sentence was shown in isolation
without any additional context. A screenshot is
shown in Figure 2.
Since we wanted to prevent judges from see-
ing the reference before editing the translations,
we split the test set between the sentences used
in the ranking task and the editing task (because
they were being conducted concurrently). More-
over, annotators edited only a single system?s out-
put for one source sentence to ensure that their un-
derstanding of it would not be influenced by an-
other system?s output.
3.3 Judging the acceptability of edited output
Halfway through the manual evaluation period, we
stopped collecting edited translations, and instead
asked annotators to do the following:
Indicate whether the edited transla-
tions represent fully fluent and meaning-
equivalent alternatives to the reference
sentence. The reference is shown with
context, the actual sentence is bold.
In addition to edited translations, unedited items
that were either marked as acceptable or as incom-
prehensible were also shown. Judges gave a sim-
ple yes/no indication to each item. A screenshot is
shown in Figure 3.
3.4 Inter- and Intra-annotator agreement
In order to measure intra-annotator agreement
10% of the items were repeated and evaluated
twice by each judge. In order to measure inter-
annotator agreement 40% of the items were ran-
domly drawn from a common pool that was shared
across all annotators so that we would have items
that were judged by multiple annotators.
INTER-ANNOTATOR AGREEMENT
Evaluation type P (A) P (E) K
Sentence ranking .549 .333 .323
Yes/no to edited output .774 .5 .549
INTRA-ANNOTATOR AGREEMENT
Evaluation type P (A) P (E) K
Sentence ranking .707 .333 .561
Yes/no to edited output .866 .5 .732
Table 4: Inter- and intra-annotator agreement for
the two types of manual evaluation
We measured pairwise agreement among anno-
tators using the kappa coefficient (K) which is de-
fined as
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance.
For inter-annotator agreement we calculated
P (A) for the yes/no judgments by examining all
items that were annotated by two or more anno-
tators, and calculating the proportion of time they
assigned identical scores to the same items. For
the ranking tasks we calculated P (A) by examin-
ing all pairs of systems which had been judged by
two or more judges, and calculated the proportion
of time that they agreed that A > B, A = B, or
A < B. Intra-annotator agreement was computed
similarly, but we gathered items that were anno-
tated on multiple occasions by a single annotator.
Table 4 gives K values for inter-annotator and
intra-annotator agreement. These give an indi-
cation of how often different judges agree, and
how often single judges are consistent for repeated
judgments, respectively. The interpretation of
Kappa varies, but according to Landis and Koch
(1977), 0 ? .2 is slight, .2 ? .4 is fair, .4 ? .6 is
moderate, .6? .8 is substantial and the rest almost
perfect.
Based on these interpretations the agreement for
yes/no judgments is moderate for inter-annotator
agreement and substantial for intra-annotator
agreement, but the inter-annotator agreement for
sentence level ranking is only fair.
We analyzed two possible strategies for improv-
ing inter-annotator agreement on the ranking task:
First, we tried discarding initial judgments to give
7
Edit MT Output
You have judged 19 sentences for WMT09 Multisource-English News Editing, 468 sentences total taking 74.4 seconds per sentence.
Original: They are often linked to other alterations sleep as nightmares, night terrors, the nocturnal enuresis (pee in bed) or the sleepwalking, but it is not 
always the case.
Edit:
Reset Edit
    Edited.
    No corrections needed.
    Unable to correct.
Annotator: ccb Task: WMT09 Multisource-English News Editing
Instructions: 
Correct the translation displayed, making it as fluent as possble. If no corrections are needed, select "No corrections needed." If you cannot understand
the sentence well enough to correct it, select "Unable to correct."
They are often linked to other sleep disorders, such as nightmares, night terrors, the nocturnal enuresis (bedwetting) or sleepwalking, but this is 
not always the case.
http://www.statmt.org/wmt09/judge/do_task.php
WMT09 Manual Evaluation
Figure 2: This screenshot shows an annotator editing the output of a machine translation system.
http://www.statmt.org/wmt09/judge/do_task.php
WMT09 Manual Evaluation
Judge Edited MT Output
You have judged 84 sentences for WMT09 French-English News Edit Acceptance, 459 sentences total taking 64.9 seconds per sentence.
Source: Au m?me moment, les gouvernements belges, hollandais et luxembourgeois ont en parti nationalis? le conglom?rat europ?en financier, Fortis. 
Les analystes de Barclays Capital ont d?clar? que les n?gociations fr?n?tiques de ce week end, conclues avec l'accord de sauvetage" semblent ne pas avoir 
r?ussi ? faire revivre le march?". 
Alors que la situation ?conomique se d?t?riorasse, la demande en mati?res premi?res, p?trole inclus, devrait se ralentir. 
"la prospective d'?quit? globale, de taux d'int?r?t et d'?change des march?s, est devenue incertaine" ont ?crit les analystes de Deutsche Bank dans une 
lettre ? leurs investisseurs." 
"nous pensons que les mati?res premi?res ne pourront ?chapper ? cette contagion. 
Reference: Meanwhile, the Belgian, Dutch and Luxembourg governments partially nationalized the European financial conglomerate Fortis. 
Analysts at Barclays Capital said the frantic weekend negotiations that led to the bailout agreement "appear to have failed to revive market sentiment." 
As the economic situation deteriorates, the demand for commodities, including oil, is expected to slow down. 
"The outlook for global equity, interest rate and exchange rate markets has become increasingly uncertain," analysts at Deutsche Bank wrote in a note to 
investors. 
"We believe commodities will be unable to escape the contagion.
Translation Verdict
While the economic situation is deteriorating, demand for commodities, including oil, should decrease.
Yes No
While the economic situation is deteriorating, the demand for raw materials, including oil, should slow down.
Yes No
Alors que the economic situation deteriorated, the request in rawmaterial enclosed, oil, would have to slow down.
Yes  No
While the financial situation damaged itself, the first matters affected, oil included, should slow down themselves.
Yes  No
While the economic situation is depressed, demand for raw materials, including oil, will be slow.
Yes No
Annotator: ccb Task: WMT09 French-English News Edit Acceptance
Instructions: 
Indicate whether the edited translations represent fully fluent and meaning-equivalent alternatives to the reference sentence. 
The reference is shown with context, the actual sentence is bold.
Figure 3: This screenshot shows an annotator judging the acceptability of edited translations.
8
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0.40
0.41
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.51
0.52
0.53
0.54
0.55
0.56
0.57
0.58
0.59
0.60
0.61
Inter-annotator agreement
Intra-annotator agreement
Proportion of judgments retained
Figure 4: The effect of discarding every annota-
tors? initial judgments, up to the first 50 items
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0.40
0.41
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.51
0.52
0.53
0.54
0.55
0.56
0.57
0.58
0.59
0.60
0.61
Inter-annotator agreement
Intra-annotator agreement
Proportion of judgments retained
Figure 5: The effect of removing annotators with
the lowest agreement, disregarding up to 40 anno-
tators
9
annotators a chance to learn to how to perform
the task. Second, we tried disregarding annota-
tors who have very low agreement with others, by
throwing away judgments for the annotators with
the lowest judgments.
Figures 4 and 5 show how the K values im-
prove for intra- and inter-annotator agreement un-
der these two strategies, and what percentage of
the judgments are retained as more annotators are
removed, or as the initial learning period is made
longer. It seems that the strategy of removing the
worst annotators is the best in terms of improv-
ing inter-annotator K, while retaining most of the
judgments. If we remove the 33 judges with the
worst agreement, we increase the inter-annotator
K from fair to moderate, and still retain 60% of
the data.
For the results presented in the rest of the paper,
we retain all judgments.
4 Translation task results
We used the results of the manual evaluation to
analyze the translation quality of the different sys-
tems that were submitted to the workshop. In our
analysis, we aimed to address the following ques-
tions:
? Which systems produced the best translation
quality for each language pair?
? Did the system combinations produce better
translations than individual systems?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 6 shows best individual systems. We de-
fine the best systems as those which had no other
system that was statistically significantly better
than them under the Sign Test at p ? 0.1.4 Multi-
ple systems are listed for many language pairs be-
cause it was not possible to draw a statistically sig-
nificant difference between the systems. Commer-
cial translation software (including Google, Sys-
tran, Morphologic, PCTrans, Eurotran XP, and
anonymized RBMT providers) did well in each of
the language pairs. Research systems that utilized
4In one case this definition meant that the system that was
ranked the highest overall was not considered to be one of
the best systems. For German-English translation RBMT5
was ranked highest overall, but was statistically significantly
worse than RBMT2.
only the provided data did as well as commercial
vendors in half of the language pairs.
The table also lists the best systems among
those which used only the provided materials.
To determine this decision we excluded uncon-
strained systems which employed significant ex-
ternal resources. Specifically, we ruled out all of
the commercial systems, since Google has access
to significantly greater data sources for its statisti-
cal system, and since the commercial RBMT sys-
tems utilize knowledge sources not available to
other workshop participants. The remaining sys-
tems were research systems that employ statisti-
cal models. We were able to draw distinctions
between half of these for each of the language
pairs. There are some borderline cases, for in-
stance LIMSI only used additional monolingual
training resources, and LIUM/Systran used addi-
tional translation dictionaries as well as additional
monolingual resources.
Table 5 summarizes the performance of the
system combination entries by listing the best
ranked combinations, and by indicating whether
they have a statistically significant difference with
the best individual systems. In general, system
combinations performed as well as the best indi-
vidual systems, but not statistically significantly
better than them. Moreover, it was hard to draw
a distinction between the different system combi-
nation strategies themselves. There are a number
of possibilities as to why we failed to find signifi-
cant differences:
? The number of judgments that we collected
were not sufficient to find a difference. Al-
though we collected several thousand judg-
ments for each language pair, most pairs of
systems were judged together fewer than 100
times.
? It is possible that the best performing indi-
vidual systems were sufficiently better than
the other systems and that it is difficult to im-
prove on them by combining them.
? Individual systems could have been weighted
incorrectly during the development stage,
which could happen if the automatic evalu-
ation metrics scores on the dev set did not
strongly correlate with human judgments.
? The lack of distinction between different
combinations could be due to the fact that
10
Language Pair Best system combinations Entries Significantly different than
best individual systems?
German-English RWTH-COMBO, BBN-COMBO,
CMU-COMBO, USAAR-COMBO
5 BBN-COMBO>GOOGLE, SYSTRAN,
USAAR-COMBO<RMBT2,
no difference for others
English-German USAAR-COMBO 1 worse than 3 best systems
Spanish-English CMU-COMBO, USAAR-COMBO,
BBN-COMBO
3 each better than one of the RBMT
systems, but there was no difference
with GOOGLE, TALP-UPC
English-Spanish USAAR-COMBO 1 no difference
French-English CMU-COMBO-HYPOSEL,
DCU-COMBO, CMU-COMBO
5 no difference
English-French USAAR-COMBO, DCU-COMBO 2 USAAR-COMBO>UKA,
DCU-COMBO>SYSTRAN, LIMSI,
no difference with others
Czech-English CMU-COMBO 2 no difference
Hungarian-English CMU-COMBO-HYPOSEL,
CMU-COMBO
3 both worse than MORPHO
Multisource-English RWTH-COMBO 3 n/a
Table 5: A comparison between the best system combinations and the best individual systems. It was
generally difficult to draw a statistically significant differences between the two groups, and between the
combinations themselves.
there is significant overlap in the strategies
that they employ.
Improved system combination warrants further in-
vestigation. We would suggest collecting addi-
tional judgments, and doing oracle experiments
where the contributions of individual systems are
weighted according to human judgments of their
quality.
Understandability
Our hope is that judging the acceptability of edited
output as discussed in Section 3 gives some indi-
cation of how often a system?s output was under-
standable. Figure 6 gives the percentage of times
that each system?s edited output was judged to
be acceptable (the percentage also factors in in-
stances when judges were unable to improve the
output because it was incomprehensible).
The edited output of the best perform-
ing systems under this evaluation model were
deemed acceptable around 50% of the time
for French-English, English-French, English-
Spanish, German-English, and English-German.
For Spanish-English the edited output of the best
system was acceptable around 40% of the time, for
English-Czech it was 30% and for Czech-English
and Hungarian-English it was around 20%.
This style of manual evaluation is experimental
and should not be taken to be authoritative. Some
caveats about this measure:
? Editing translations without context is diffi-
cult, so the acceptability rate is probably an
underestimate of how understandable a sys-
tem actually is.
? There are several sources of variance that are
difficult to control for: some people are better
at editing, and some sentences are more dif-
ficult to edit. Therefore, variance in the un-
derstandability of systems is difficult to pin
down.
? The acceptability measure does not strongly
correlate with the more established method of
ranking translations relative to each other for
all the language pairs.5
Please also note that the number of corrected
translations per system are very low for some
language pairs, as low as 23 corrected sentences
per system for the language pair English?French.
5The Spearman rank correlation coefficients for how the
two types of manual evaluation rank systems are .67 for de-
en, .67 for fr-en, .06 for es-en, .50 for cz-en, .36 for hu-en,
.65 for en-de, .02 for en-fr, -.6 for en-es, and .94 for en-cz.
11
French?English
625?836 judgments per system
System C? ?others
GOOGLE ? no .76
DCU ? yes .66
LIMSI ? no .65
JHU ? yes .62
UEDIN ? yes .61
UKA yes .61
LIUM-SYSTRAN no .60
RBMT5 no .59
CMU-STATXFER ? yes .58
RBMT1 no .56
USAAR no .55
RBMT3 no .54
RWTH ? yes .52
COLUMBIA yes .50
RBMT4 no .47
GENEVA no .34
English?French
422?517 judgments per system
System C? ?others
LIUM-SYSTRAN ? no .73
GOOGLE ? no .68
UKA ?? yes .66
SYSTRAN ? no .65
RBMT3 ? no .65
DCU ?? yes .65
LIMSI ? no .64
UEDIN ? yes .60
RBMT4 no .59
RWTH yes .58
RBMT5 no .57
RBMT1 no .54
USAAR no .48
GENEVA no .38
Hungarian?English
865?988 judgments per system
System C? ?others
MORPHO ? no .75
UMD ? yes .66
UEDIN yes .45
German?English
651?867 judgments per system
System C? ?others
RBMT5 no .66
USAAR ? no .65
GOOGLE ? no .65
RBMT2 ? no .64
RBMT3 no .64
RBMT4 no .62
STUTTGART ?? yes .61
SYSTRAN ? no .60
UEDIN ? yes .59
UKA ? yes .58
UMD ? yes .56
RBMT1 no .54
LIU ? yes .50
RWTH yes .50
GENEVA no .33
JHU-TROMBLE yes .13
English?German
977?1226 judgments per system
System C? ?others
RBMT2 ? no .66
RBMT3 ? no .64
RBMT5 ? no .64
USAAR no .58
RBMT4 no .58
RBMT1 no .57
GOOGLE no .54
UKA ? yes .54
UEDIN ? yes .51
LIU ? yes .49
RWTH ? yes .48
STUTTGART yes .43
Czech?English
1257?1263 judgments per system
System C? ?others
GOOGLE ? no .75
UEDIN ? yes .57
CU-BOJAR ? yes .51
Spanish?English
613?801 judgments per system
System C? ?others
GOOGLE ? no .70
TALP-UPC ?? yes .59
UEDIN ? yes .56
RBMT1 ? no .55
RBMT3 ? no .55
RBMT5 ? no .55
RBMT4 ? no .53
RWTH ? yes .51
USAAR no .51
NICT yes .37
English?Spanish
632?746 judgments per system
System C? ?others
RBMT3 ? no .66
UEDIN ?? yes .66
GOOGLE ? no .65
RBMT5 ? no .64
RBMT4 no .61
NUS ? yes .59
TALP-UPC yes .58
RWTH yes .51
RBMT1 no .25
USAAR no .48
English?Czech
4626?4784 judgments per system
System C? ?others
PCTRANS ? no .67
EUROTRANXP ? no .67
GOOGLE no .66
CU-BOJAR ? yes .61
UEDIN yes .53
CU-TECTOMT yes .48
Systems are listed in the order of how often their translations were ranked higher than or equal to any
other system. Ties are broken by direct comparison.
C? indicates constrained condition, meaning only using the supplied training data and possibly standard
monolingual linguistic tools (but no additional corpora).
? indicates a win in the category, meaning that no other system is statistically significantly better at
p-level?0.1 in pairwise comparison.
? indicates a constrained win, no other constrained system is statistically better.
For all pairwise comparisons between systems, please check the appendix.
Table 6: Official results for the WMT09 translation task, based on the human evaluation (ranking trans-
lations relative to each other)
12
Given these low numbers, the numbers presented
in Figure 6 should not be read as comparisons be-
tween systems, but rather viewed as indicating the
state of machine translation for different language
pairs.
5 Shared evaluation task overview
In addition to allowing us to analyze the transla-
tion quality of different systems, the data gath-
ered during the manual evaluation is useful for
validating the automatic evaluation metrics. Last
year, NIST began running a similar ?Metrics
for MAchine TRanslation? challenge (Metrics-
MATR), and presented their findings at a work-
shop at AMTA (Przybocki et al, 2008).
In this year?s shared task we evaluated a number
of different automatic metrics:
? Bleu (Papineni et al, 2002)?Bleu remains
the de facto standard in machine translation
evaluation. It calculates n-gram precision and
a brevity penalty, and can make use of multi-
ple reference translations as a way of captur-
ing some of the allowable variation in trans-
lation. We use a single reference translation
in our experiments.
? Meteor (Agarwal and Lavie, 2008)?Meteor
measures precision and recall for unigrams
and applies a fragmentation penalty. It uses
flexible word matching based on stemming
and WordNet-synonymy. meteor-ranking is
optimized for correlation with ranking judg-
ments.
? Translation Error Rate (Snover et al,
2006)?TER calculates the number of ed-
its required to change a hypothesis transla-
tion into a reference translation. The possi-
ble edits in TER include insertion, deletion,
and substitution of single words, and an edit
which moves sequences of contiguous words.
Two variants of TER are also included: TERp
(Snover et al, 2009), a new version which in-
troduces a number of different features, and
(Bleu ? TER)/2, a combination of Bleu and
Translation Edit Rate.
? MaxSim (Chan and Ng, 2008)?MaxSim
calculates a similarity score by comparing
items in the translation against the reference.
Unlike most metrics which do strict match-
ing, MaxSim computes a similarity score
for non-identical items. To find a maxi-
mum weight matching that matches each sys-
tem item to at most one reference item, the
items are then modeled as nodes in a bipar-
tite graph.
? wcd6p4er (Leusch and Ney, 2008)?a mea-
sure based on cder with word-based substitu-
tion costs. Leusch and Ney (2008) also sub-
mitted two contrastive metrics: bleusp4114,
a modified version of BLEU-S (Lin and
Och, 2004), with tuned n-gram weights, and
bleusp, with constant weights. wcd6p4er
is an error measure and bleusp is a quality
score.
? RTE (Pado et al, 2009)?The RTE metric
follows a semantic approach which applies
recent work in rich textual entailment to the
problem of MT evaluation. Its predictions are
based on a regression model over a feature
set adapted from an entailment systems. The
features primarily model alignment quality
and (mis-)matches of syntactic and semantic
structures.
? ULC (Gime?nez and Ma`rquez, 2008)?ULC
is an arithmetic mean over other automatic
metrics. The set of metrics used include
Rouge, Meteor, measures of overlap between
constituent parses, dependency parses, se-
mantic roles, and discourse representations.
The ULC metric had the strongest correlation
with human judgments in WMT08 (Callison-
Burch et al, 2008).
? wpF and wpBleu (Popovic and Ney, 2009) -
These metrics are based on words and part of
speech sequences. wpF is an n-gram based F-
measure which takes into account both word
n-grams and part of speech n-grams. wp-
BLEU is a combnination of the normal Blue
score and a part of speech-based Bleu score.
? SemPOS (Kos and Bojar, 2009) ? the Sem-
POS metric computes overlapping words, as
defined in (Gime?nez and Ma`rquez, 2007),
with respect to their semantic part of speech.
Moreover, it does not use the surface repre-
sentation of words but their underlying forms
obtained from the TectoMT framework.
13
re
f
b
b
n
-
c
g
o
o
g
l
e
c
m
u
-
c
o
m
b
o
c
u
-
b
o
j
a
r
u
e
d
i
n
0.140.160.180.230.250.98
Czech-English
r
e
f
g
o
o
g
l
e
c
m
u
-
s
t
a
t
x
b
b
n
-
c
o
m
b
o
c
o
l
u
m
b
i
a
c
m
u
-
c
o
m
b
o
c
m
u
-
c
o
m
b
o
-
h
y
p
d
c
u
-
c
o
m
b
o
u
k
a
r
w
t
h
r
b
m
t
4
l
i
m
s
i
r
b
m
t
1
l
i
u
m
-
s
y
s
t
r
n
u
s
a
a
r
-
c
o
m
b
o
d
c
u
u
e
d
i
n
j
h
u
r
b
m
t
3
r
b
m
t
5
u
s
a
a
r
g
e
n
e
v
a
0.210.280.280.280.290.300.310.330.330.340.340.340.350.380.390.400.410.410.470.500.520.85
French-English
r
e
f
r
w
t
h
-
c
c
m
u
-
c
o
m
b
o
b
b
n
-
c
o
m
b
o
r
b
m
t
5
u
s
a
a
r
-
c
g
o
o
g
l
e
c
m
u
-
c
m
b
-
h
u
s
a
a
r
r
b
m
t
3
s
t
u
t
t
g
a
r
t
r
b
m
t
4
r
b
m
t
1
u
k
a
r
w
t
h
u
m
d
u
e
d
i
n
r
b
m
t
2
s
y
s
t
r
a
n
l
i
u
g
e
n
e
v
a
j
h
u
-
t
r
o
m
b
l
e
0.030.060.200.210.250.260.260.270.280.300.300.310.310.320.330.340.350.360.370.410.470.83
German-English
r
e
f
u
s
a
a
r
-
c
g
o
o
g
l
e
r
b
m
t
5
r
w
t
h
t
a
l
p
-
u
p
c
u
e
d
i
n
u
s
a
a
r
r
b
m
t
3
n
u
s
r
b
m
t
4
r
b
m
t
1
0.080.100.190.210.270.270.280.320.330.380.520.69
English-Spanish
r
e
f
u
s
a
a
r
-
c
g
o
o
g
l
e
r
b
m
t
5
r
b
m
t
3
b
b
n
-
c
m
b
u
e
d
i
n
t
a
l
p
-
u
p
c
r
b
m
t
1
r
w
t
h
c
m
u
-
c
r
b
m
t
4
n
i
c
t
u
s
a
a
r
0.230.260.270.280.280.280.280.310.310.360.370.380.410.88
Spanish-English
r
e
f
g
o
o
g
l
e
p
c
t
r
a
n
s
e
u
r
o
t
r
a
n
x
p
u
e
d
i
n
c
u
-
b
o
j
a
r
c
u
-
t
e
c
t
o
m
t
0.190.210.230.260.320.320.91
English-Czech
r
e
f
g
o
o
g
l
e
u
k
a
l
i
m
s
i
u
s
a
a
r
-
c
r
b
m
t
3
r
b
m
t
5
u
e
d
i
n
u
s
a
a
r
r
b
m
t
4
l
i
u
m
-
s
y
s
r
w
t
h
d
c
u
-
c
m
b
d
c
u
s
y
s
t
r
a
n
r
b
m
t
1
g
e
n
e
v
a
0.080.100.220.270.300.310.320.340.370.400.400.430.440.450.480.490.79
English-French
r
e
f
m
o
r
p
h
o
c
m
u
-
c
m
b
-
h
c
m
u
-
c
o
m
b
o
u
m
d
u
e
d
i
n
b
b
n
-
c
m
b
0.110.120.150.190.210.220.93
Hungarian-English
r
e
f
r
b
m
t
5
r
b
m
t
3
g
o
o
g
l
e
r
b
m
t
1
r
b
m
t
2
u
s
a
a
r
u
s
a
a
r
-
c
m
b
r
b
m
t
4
r
w
t
h
u
k
a
u
e
d
i
n
s
t
u
t
t
g
a
r
t
l
i
u
0.120.180.190.260.280.310.310.320.330.350.370.420.470.85
English-German
r
e
f
r
w
t
h
b
b
n
c
m
u
0.250.270.320.90
Multsource-English
Figure 6: The percent of time that each system?s edited output was judged to be an acceptable translation.
These numbers also include judgments of the system?s output when it was marked either incomprehen-
sible or acceptable and left unedited. Note that the reference translation was edited alongside the system
outputs. Error bars show one positive and one negative standard deviation for the systems in that lan-
guage pair.
14
5.1 Measuring system-level correlation
We measured the correlation of the automatic met-
rics with the human judgments of translation qual-
ity at the system-level using Spearman?s rank cor-
relation coefficient ?. We converted the raw scores
assigned to each system into ranks. We assigned
a human ranking to the systems based on the per-
cent of time that their translations were judged to
be better than or equal to the translations of any
other system in the manual evaluation.
When there are no ties ? can be calculated using
the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all sys-
tems are ranked in the same order) and ?1 (where
the systems are ranked in the reverse order). Thus
an automatic evaluation metric with a higher abso-
lute value for ? is making predictions that are more
similar to the human judgments than an automatic
evaluation metric with a lower absolute ?.
5.2 Measuring sentence-level consistency
Because the sentence-level judgments collected
in the manual evaluation are relative judgments
rather than absolute judgments, it is not possi-
ble for us to measure correlation at the sentence-
level in the same way that previous work has done
(Kulesza and Shieber, 2004; Albrecht and Hwa,
2007a; Albrecht and Hwa, 2007b).
Rather than calculating a correlation coefficient
at the sentence-level we instead ascertained how
consistent the automatic metrics were with the hu-
man judgments. The way that we calculated con-
sistency was the following: for every pairwise
comparison of two systems on a single sentence by
a person, we counted the automatic metric as being
consistent if the relative scores were the same (i.e.
the metric assigned a higher score to the higher
ranked system). We divided this by the total num-
ber of pairwise comparisons to get a percentage.
Because the systems generally assign real num-
bers as scores, we excluded pairs that the human
annotators ranked as ties.
de
-e
n
(2
1
sy
st
em
s)
fr
-e
n
(2
1
sy
st
em
s)
es
-e
n
(1
3
sy
st
em
s)
cz
-e
n
(5
sy
st
em
s)
hu
-e
n
(6
sy
st
em
s)
A
ve
ra
ge
ulc .78 .92 .86 1 .6 .83
maxsim .76 .91 .98 .7 .66 .8
rte (absolute) .64 .91 .96 .6 .83 .79
meteor-rank .64 .93 .96 .7 .54 .75
rte (pairwise) .76 .59 .78 .8 .83 .75
terp -.72 -.89 -.94 -.7 -.37 -.72
meteor-0.6 .56 .93 .87 .7 .54 .72
meteor-0.7 .55 .93 .86 .7 .26 .66
bleu-ter/2 .38 .88 .78 .9 -.03 .58
nist .41 .87 .75 .9 -.14 .56
wpF .42 .87 .82 1 -.31 .56
ter -.43 -.83 -.84 -.6 -.01 -.54
nist (cased) .42 .83 .75 1 -.31 .54
bleu .41 .88 .79 .6 -.14 .51
bleusp .39 .88 .78 .6 -.09 .51
bleusp4114 .39 .89 .78 .6 -.26 .48
bleu (cased) .4 .86 .8 .6 -.31 .47
wpbleu .43 .86 .8 .7 -.49 .46
wcd6p4er -.41 -.89 -.76 -.6 .43 -.45
Table 7: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation into English.
en
-d
e
(1
3
sy
st
em
s)
en
-f
r
(1
6
sy
st
em
s)
en
-e
s
(1
1
sy
st
em
s)
en
-c
z
(5
sy
st
em
s)
A
ve
ra
ge
terp .03 -.89 -.58 -.4 -.46
ter -.03 -.78 -.5 -.1 -.35
bleusp4114 -.3 .88 .51 .1 .3
bleusp -.3 .87 .51 .1 .29
bleu -.43 .87 .36 .3 .27
bleu (cased) -.45 .87 .35 .3 .27
bleu-ter/2 -.37 .87 .44 .1 .26
wcd6p4er .54 -.89 -.45 -.1 -.22
nist (cased) -.47 .84 .35 .1 .2
nist -.52 .87 .23 .1 .17
wpF -.06 .9 .58 n/a n/a
wpbleu .07 .92 .63 n/a n/a
Table 8: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation out of English.
15
SemPOS .4 BLEUtecto .3
Meteor .4 BLEU .3
GTM(e=0.5)tecto .4 NISTlemma .1
GTM(e=0.5)lemma .4 NIST .1
GTM(e=0.5) .4 BLEUlemma .1
WERtecto .3 WERlemma -.1
TERtecto .3 WER -.1
PERtecto .3 TERlemma -.1
F-measuretecto .3 TER -.1
F-measurelemma .3 PERlemma -.1
F-measure .3 PER -.1
NISTtecto -.3
Table 9: The system-level correlation for auto-
matic metrics ranking five English-Czech systems
6 Evaluation task results
6.1 System-level correlation
Table 7 shows the correlation of automatic met-
rics when they rank systems that are translating
into English. Note that TERp, TER and wcd6p4er
are error metrics, so a negative correlation is bet-
ter for them. The strength of correlation varied for
the different language pairs. The automatic met-
rics were able to rank the French-English systems
reasonably well with correlation coefficients in the
range of .8 and .9. In comparison, metrics per-
formed worse for Hungarian-English, where half
of the systems had negative correlation. The ULC
metric once again had strongest correlation with
human judgments of translation quality. This was
followed closely by MaxSim and RTE, with Me-
teor and TERp doing respectably well in 4th and
5th place. Notably, Bleu and its variants were the
worst performing metrics in this translation direc-
tion.
Table 8 shows correlation for metrics which op-
erated on languages other than English. Most of
the best performing metrics that operate on En-
glish do not work for foreign languages, because
they perform some linguistic analysis or rely on
a resource like WordNet. For translation into for-
eign languages TERp was the best system overall.
The wpBleu and wpF metrics also did extremely
well, performing the best in the language pairs that
they were applied to. wpBleu and wpF were not
applied to Czech because the authors of the met-
ric did not have a Czech tagger. English-German
proved to be the most problematic language pair
to automatically evaluate, with all of the metrics
having a negative correlation except wpBleu and
TER.
Table 9 gives detailed results for how well vari-
ations on a number of automatic metrics do for
the task of ranking five English-Czech systems.6
These systems were submitted by Kos and Bojar
(2009), and they investigate the effects of using
Prague Dependency Treebank annotations during
automatic evaluation. They linearizing the Czech
trees and evaluated either the lemmatized forms of
the Czech (lemma) read off the trees or the Tec-
togrammatical form which retained only lemma-
tized content words (tecto). The table also demon-
strates SemPOS, Meteor, and GTM perform better
on Czech than many other metrics.
6.2 Sentence-level consistency
Tables 10 and 11 show the percent of times that the
metrics? scores were consistent with human rank-
ings of every pair of translated sentences.7 Since
we eliminated sentence pairs that were judged to
be equal, the random baseline for this task is 50%.
Many metrics failed to reach the baseline (includ-
ing most metrics in the out-of-English direction).
This indicates that sentence-level evaluation of
machine translation quality is very difficult. RTE
and ULC again do the best overall for the into-
English direction. They are followed closely by
wpF and wcd6p4er, which considerably improve
their performance over their system-level correla-
tions.
We tried a variant on measuring sentence-level
consistency. Instead of using the scores assigned
to each individual sentence, we used the system-
level score and applied it to every sentence that
was produced by that system. These can be
thought of as a metric?s prior expectation about
how a system should preform, based on their per-
formance on the whole data set. Tables 12 and 13
show that using the system-level scores in place
of the sentence-level scores results in considerably
higher consistency with human judgments. This
suggests an interesting line of research for improv-
ing sentence-level predictions by using the perfor-
mance on a larger data set as a prior.
7 Summary
As in previous editions of this workshop we car-
ried out an extensive manual and automatic eval-
uation of machine translation performance for
translating from European languages into English,
6PCTRANS was excluded from the English-Czech systems
because its SGML file was malformed.
7Not all metrics entered into the sentence-level task.
16
fr
-e
n
(6
26
8
pa
ir
s)
de
-e
n
(6
38
2
pa
ir
s)
es
-e
n
(4
10
6
pa
ir
s)
cz
-e
n
(2
25
1
pa
ir
s)
hu
-e
n
(2
19
3
pa
ir
s)
xx
-e
n
(1
95
2
pa
ir
s)
O
ve
ra
ll
(2
31
52
pa
ir
s)
ulc .55 .56 .51 .50 .51 .51 .54
rte (absolute) .54 .56 .51 .50 .55 .51 .53
wpF .54 .55 .50 .47 .48 .51 .52
wcd6p4er .54 .54 .49 .48 .48 .50 .52
maxsim .53 .55 .49 .47 .50 .49 .52
bleusp .54 .55 .49 .47 .46 .50 .51
bleusp4114 .53 .55 .48 .47 .46 .50 .51
rte (pairwise) .49 .48 .52 .53 .55 .52 .51
terp .52 .53 .48 .46 .45 .48 .50
meteor-0.6 .50 .53 .46 .48 .47 .47 .49
meteor-rank .50 .52 .46 .48 .47 .47 .49
meteor-0.7 .49 .52 .46 .48 .47 .47 .49
ter .48 .47 .43 .41 .40 .42 .45
wpbleu .46 .45 .46 .39 .35 .45 .44
Table 10: Sentence-level consistency of the auto-
matic metrics with human judgments for transla-
tions into English. Italicized numbers fall below
the random-choice baseline.
en
-f
r
(2
96
7
pa
ir
s)
en
-d
e
(6
56
3
pa
ir
s)
en
-e
s
(3
24
9
pa
ir
s)
en
-c
z
(1
12
42
pa
ir
s)
O
ve
ra
ll
(2
40
21
pa
ir
s)
wcd6p4er .57 .47 .52 .49 .50
bleusp4114 .57 .46 .54 .49 .50
bleusp .57 .46 .53 .48 .49
ter .50 .41 .45 .37 .41
terp .51 .39 .48 .27 .36
wpF .57 .46 .54 n/a .51
wpbleu .53 .37 .46 n/a .43
Table 11: Sentence-level consistency of the auto-
matic metrics with human judgments for transla-
tions out of English. Italicized numbers fall below
the random-choice baseline.
fr
-e
n
(6
26
8
pa
ir
s)
de
-e
n
(6
38
2
pa
ir
s)
es
-e
n
(4
10
6
pa
ir
s)
cz
-e
n
(2
25
1
pa
ir
s)
hu
-e
n
(2
19
3
pa
ir
s)
O
ve
ra
ll
(2
12
00
pa
ir
s)
Oracle .61 .63 .59 .61 .67 .62
rte (absolute) .60 .61 .59 .57 .65 .61
ulc .61 .62 .58 .61 .59 .60
maxsim .61 .62 .59 .57 .61 .60
meteor-rank .61 .61 .59 .57 .61 .60
meteor-0.6 .61 .61 .58 .57 .60 .60
rte (pairwise) .56 .61 .57 .59 .64 .59
terp .60 .61 .59 .57 .56 .59
meteor-0.7 .61 .61 .58 .57 .55 .59
ter .60 .59 .57 .55 .51 .58
wpF .60 .59 .57 .61 .46 .58
bleusp .61 .59 .56 .55 .48 .57
bleusp4114 .61 .59 .56 .55 .46 .57
wcd6p4er .61 .59 .57 .55 .44 .57
wpbleu .60 .59 .57 .57 .43 .57
Table 12: Consistency of the automatic met-
rics when their system-level ranks are treated as
sentence-level scores. Oracle shows the consis-
tency of using the system-level human ranks that
are given in Table 6.
en
-f
r
(2
96
7
pa
ir
s)
en
-d
e
(6
56
3
pa
ir
s)
en
-e
s
(3
24
9
pa
ir
s)
en
-c
z
(1
12
42
pa
ir
s)
O
ve
ra
ll
(2
40
21
pa
ir
s)
Oracle .62 .59 .63 .60 .60
terp .62 .50 .59 .53 .54
ter .61 .51 .58 .50 .53
bleusp .62 .48 .59 .50 .52
bleusp4114 .63 .48 .59 .50 .52
wcd6p4er .62 .46 .58 .50 .52
wpbleu .63 .51 .60 n/a .56
wpF .63 .50 .59 n/a .55
Table 13: Consistency of the automatic met-
rics when their system-level ranks are treated as
sentence-level scores. Oracle shows the consis-
tency of using the system-level human ranks that
are given in Table 6.
17
and vice versa.
The number of participants remained stable
compared to last year?s WMT workshop, with
22 groups from 20 institutions participating in
WMT09. This year?s evaluation also included 7
commercial rule-based MT systems and Google?s
online statistical machine translation system.
Compared to previous years, we have simpli-
fied the evaluation conditions by removing the in-
domain vs. out-of-domain distinction focusing on
news translations only. The main reason for this
was eliminating the advantage statistical systems
have with respect to test data that are from the
same domain as the training data.
Analogously to previous years, the main focus
of comparing the quality of different approaches
is on manual evaluation. Here, also, we reduced
the number of dimensions with respect to which
the different systems are compared, with sentence-
level ranking as the primary type of manual eval-
uation. In addition to the direct quality judgments
we also evaluated translation quality by having
people edit the output of systems and have as-
sessors judge the correctness of the edited output.
The degree to which users were able to edit the
translations (without having access to the source
sentence or reference translation) served as a mea-
sure of the overall comprehensibility of the trans-
lation.
Although the inter-annotator agreement in the
sentence-ranking evaluation is only fair (as mea-
sured by the Kappa score), agreement can be im-
proved by removing the first (up to 50) judgments
of each assessor, focusing on the judgments that
were made once the assessors are more familiar
with the task. Inter-annotator agreement with re-
spect to correctness judgments of the edited trans-
lations were higher (moderate), which is proba-
bly due to the simplified evaluation criterion (bi-
nary judgments versus rankings). Inter-annotator
agreement for both conditions can be increased
further by removing the judges with the worst
agreement. Intra-annotator agreement on the other
hand was considerably higher ranging between
moderate and substantial.
In addition to the manual evaluation criteria we
applied a large number of automated metrics to
see how they correlate with the human judgments.
There is considerably variation between the differ-
ent metrics and the language pairs under consid-
eration. As in WMT08, the ULC metric had the
highest overall correlation with human judgments
when translating into English, with MaxSim and
RTE following closely behind. TERp and wpBleu
were best when translating into other languages.
Automatically predicting human judgments at
the sentence-level proved to be quite challeng-
ing with many of the systems performing around
chance. We performed an analysis that showed
that if metrics? system-level scores are used in
place of their scores for individual sentences, that
they do quite a lot better. This suggests that prior
probabilities ought to be integrated into sentence-
level scoring.
All data sets generated by this workshop, in-
cluding the human judgments, system translations
and automatic scores, are publicly available for
other researchers to analyze.8
Acknowledgments
This work was supported in parts by the EuroMa-
trix project funded by the European Commission
(6th Framework Programme), the GALE program
of the US Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022, and
the US National Science Foundation under grant
IIS-0713448.
We are grateful to Holger Schwenk and Preslav
Nakov for pointing out the potential bias in our
method for ranking systems when self-judgments
are excluded. We analyzed the results and found
that this did not hold. We would like to thank
Maja Popovic for sharing thoughts about how to
improve the manual evaluation. Thanks to Cam
Fordyce for helping out with the manual evalua-
tion again this year.
An extremely big thanks to Sebastian Pado for
helping us work through the logic of segment-level
scoring of automatic evaluation metric.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor, M-
BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine trans-
lation output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Joshua Albrecht and Rebecca Hwa. 2007a. A re-
examination of machine learning approaches for
8http://www.statmt.org/wmt09/results.
html
18
sentence-level MT evaluation. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2007), Prague, Czech Re-
public.
Joshua Albrecht and Rebecca Hwa. 2007b. Regres-
sion for sentence-level MT evaluation with pseudo
references. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2007), Prague, Czech Republic.
Alexandre Allauzen, Josep Crego, Aure?lien Max, and
Fran cois Yvon. 2009. LIMSI?s statistical transla-
tion systems for WMT?09. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Mar-
tin Popel, Jan Pta?c?ek, Jan Rous?, and Zdene?k
Z?abokrtsky?. 2009. English-Czech MT in 2008. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation (WMT07), Prague, Czech Repub-
lic.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08), Colmbus, Ohio.
Marine Carpuat. 2009. Toward using morphology
in French-English phrase-based SMT. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, Athens, Greece, March. Association for
Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2008. An automatic
metric for machine translation evaluation based on
maximum similary. In In the Metrics-MATR Work-
shop of AMTA-2008, Honolulu, Hawaii.
Yu Chen, Michael Jellinghaus, Andreas Eisele,
Yi Zhang, Sabine Hunsicker, Silke Theison, Chris-
tian Federmann, and Hans Uszkoreit. 2009. Com-
bining multi-engine translations with moses. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Jinhua Du, Yifan He, Sergio Penkale, and Andy Way.
2009. MATREX: The DCU MT system for WMT
2009. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Lo??c Dugast, Jean Senellart, and Philipp Koehn.
2009. Statistical post editing and dictionary ex-
traction: Systran/Edinburgh submissions for ACL-
WMT2009. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Chris Dyer, Hendra Setiawan, Yuval Marton, and
Philip Resnik. 2009. The University of Mary-
land statistical machine translation system for the
fourth workshop on machine translation. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Jason Eisner and Roy W. Tromble. 2006. Local
search with very large-scale neighborhoods for op-
timal permutations in machine translation. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American chapter of the Associ-
ation for Computational Linguistics (HLT/NAACL-
2006), New York, New York.
Christian Federmann, Silke Theison, Andreas Eisele,
Hans Uszkoreit, Yu Chen, Michael Jellinghaus, and
Sabine Hunsicker. 2009. Translation combina-
tion using factored word substitution. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, Athens, Greece, March. Association for
Computational Linguistics.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of ACL Workshop on
Machine Translation.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. A smor-
gasbord of features for automatic MT evaluation.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198.
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark,
Alok Parlikar, and Alon Lavie. 2009. An
improved statistical transfer system for French-
English machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Kenneth Heafield, Greg Hanneman, and Alon Lavie.
2009. Machine translation system combination
with flexible word ordering. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Almut Silja Hildebrand and Stephan Vogel. 2009.
CMU system combination for WMT?09. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
19
Maria Holmqvist, Sara Stymne, Jody Foo, and Lars
Ahrenberg. 2009. Improving alignment for SMT by
reordering and augmenting the training corpus. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2009. Edin-
burgh?s submission to all tracks of the WMT2009
shared task with reordering and speed improvements
to Moses. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL
2006 Workshop on Statistical Machine Translation,
New York, New York.
Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris
Callison-Burch, Alexandra Constantin, Brooke
Cowan, Chris Dyer, Marcello Federico, Evan
Herbst, Hieu Hoang, Christine Moran, Wade Shen,
and Richard Zens. 2007. Open source toolkit for
statistical machine translation: Factored translation
models and confusion network decoding. CLSP
Summer Workshop Final Report WS-2006, Johns
Hopkins University.
Kamil Kos and Ondr?ej Bojar. 2009. Evaluation of Ma-
chine Translation Metrics for Czech as the Target
Language. Prague Bulletin of Mathematical Lin-
guistics, 92. in print.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-
ing approach to improving sentence-level MT evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation, Baltimore, MD, October 4?6.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Gregor Leusch and Hermann Ney. 2008. BLEUSP,
PINVWER, CDER: Three improved MT evaluation
measures. In In the Metrics-MATR Workshop of
AMTA-2008, Honolulu, Hawaii.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2009. The RWTH system combination system for
WMT 2009. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2004), Barcelona, Spain.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
the 5th Biennial Conference of the Association for
Machine Translation in the Americas (AMTA-2002),
Tiburon, California.
Preslav Nakov and Hwee Tou Ng. 2009. NUS
at WMT09: Domain adaptation experiments for
English-Spanish machine translation of news com-
mentary text. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, Athens,
Greece, March. Association for Computational Lin-
guistics.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
translation system for the EACL-WMT 2009. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
NIST. 2008. Evaluation plan for gale go/no-go phase
3 / phase 3.5 translation evaluations. June 18, 2008.
Attila Nova?k. 2009. Morphologic?s submission for
the WMT 2009 shared task. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Sebastian Pado, Michel Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Machine transla-
tion evaluation with textual entailment features. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2002),
Philadelphia, Pennsylvania.
Michael Paul, Andrew Finch, and Eiichiro Sumita.
2009. NICT@WMT09: Model adaptation and
transliteration for Spanish-English SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Maja Popovic and Hermann Ney. 2009. Syntax-
oriented evaluation measures for machine transla-
tion output. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
20
Maja Popovic, David Vilar, Daniel Stein, Evgeny Ma-
tusov, and Hermann Ney. 2009. The RWTH ma-
chine translation system for WMT 2009. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Mark Przybocki, Kay Peterson, and Se-
bastien Bronsart. 2008. Official results
of the NIST 2008 ?Metrics for MAchine
TRanslation? challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/.
Jose? A. R. Fonollosa, Maxim Khalilov, Marta R. Costa-
jussa?, Jose? B. Marin?o, Carlos A. Henra?quez Q.,
Adolfo Herna?ndez H., and Rafael E. Banchs. 2009.
The TALP-UPC phrase-based translation system
for EACL-WMT 2009. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hy-
pothesis alignment with flexible matching for build-
ing confusion networks: BBN system description
for WMT09 system combination task. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, Athens, Greece, March. Association for
Computational Linguistics.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word lattices for multi-source translation.
In 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Holger Schwenk, Sadaf Abdul Rauf, Loic Barrault, and
Jean Senellart. 2009. SMT and SPE machine trans-
lation systems for WMT?09. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA-2006), Cambridge, Massachusetts.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy,
or HTER? exploring different human judgments
with a tunable MT metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale lms on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), Prague, Czech Repub-
lic.
Eric Wehrli, Luka Nerima, and Yves Scherrer.
2009. Deep linguistic multilingual translation
and bilingual dictionaries. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
21
A Pairwise system comparisons by human judges
Tables 14?24 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row. Bolding indicates the winner of the two systems. The difference between
100 and the sum of the complimentary cells is the percent of time that the two systems were judged to
be equal.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
B Automatic scores
Tables 26 and 25 give the automatic scores for each of the systems.
G
E
N
E
V
A
G
O
O
G
L
E
JH
U
-T
R
O
M
B
L
E
L
IU
R
B
M
T
1
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
S
T
U
T
T
G
A
R
T
S
Y
S
T
R
A
N
U
E
D
IN
U
K
A
U
M
D
U
S
A
A
R
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
R
W
T
H
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
GENEVA .76? .08? .63? .54 .69? .73? .83? .78? .49? .77? .75? .74? .57? .74? .69? .75? .84? .60 .84? .71?
GOOGLE .15? .03? .23? .50 .43 .24? .39 .42 .39 .43 .33 .27? .29? .38 .48 .57? .44 .32 .35 .36
JHU-TROMBLE .75? .90? .77? .81? .84? .91? .94? .88? .79? .83? .83? .93? .89? .92? .90? .94? .90? .95? .91? .83?
LIU .29? .65? .12? .49 .63 .63? .57 .63? .41 .49 .46 .50 .49 .50 .41 .66? .53 .59? .62? .53
RBMT1 .32 .43 .11? .46 .42 .46 .50 .61? .34 .46 .58 .51 .42 .42 .56 .47 .53 .49 .58 .54
RBMT2 .25? .46 .09? .37 .45 .33 .45 .23? .3 .28 .47 .42 .31? .34 .39 .49 .61 .4 .32 .29?
RBMT3 .17? .59? .02? .26? .35 .46 .27 .45 .27 .36 .46 .42 .43 .26? .49 .4 .48 .58 .29 .31
RBMT4 .12? .47 .07? .37 .4 .45 .52 .60? .39 .39 .45 .39 .31? .29? .44 .54 .45 .37 .43 .30
RBMT5 .13? .34 .07? .30? .24? .57? .41 .29? .31 .50 .34 .3 .28? .43 .30 .49 .57 .3 .49 .21
RWTH .21? .55 .10? .41 .49 .55 .46 .46 .60 .44 .57 .48 .51? .41 .56 .64? .54 .56? .74? .59?
STUTTGART .17? .43 .13? .39 .43 .55 .39 .36 .33 .34 .38 .42 .52 .42 .49 .49 .28 .35 .56 .46
SYSTRAN .11? .63 .06? .42 .37 .47 .50 .32 .58 .34 .55 .36 .44 .35 .43 .61? .46 .41 .33 .44
UEDIN .10? .50? .03? .35 .49 .46 .39 .52 .55 .29 .39 .52 .35 .33 .42 .58? .43 .56 .59? .55
UKA .29? .58? .04? .32 .47 .63? .55 .54? .64? .24? .28 .39 .50 .29 .50 .48 .36 .57? .45 .45
UMD .16? .53 .08? .38 .49 .43 .63? .68? .49 .38 .39 .41 .50 .49 .46 .54 .44 .38 .46 .50
USAAR .19? .44 ? .41 .34 .49 .4 .44 .33 .36 .33 .45 .39 .32 .41 .46 .41 .31 .42 .11
BBN-COMBO .14? .31? .06? .26? .44 .44 .48 .36 .38 .23? .35 .26? .29? .34 .36 .37 .32 .23? .38 .32
CMU-COMBO .10? .36 .07? .37 .37 .36 .48 .40 .30 .28 .53 .41 .4 .43 .28 .34 .50 .33 .53 .44
CMU-COMBO-H .3 .46 ? .10? .39 .43 .40 .48 .57 .27? .41 .47 .28 .26? .38 .49 .65? .46 .41 .47
RWTH-COMBO .06? .38 ? .19? .36 .54 .43 .43 .30 .10? .33 .56 .22? .27 .23 .42 .32 .31 .41 .29
USAAR-COMBO .20? .55 .17? .3 .39 .57? .45 .59 .32 .27? .33 .47 .32 .33 .27 .16 .55 .44 .4 .50
> OTHERS .22 .51 .06 .38 .44 .52 .49 .49 .50 .33 .44 .48 .44 .42 .41 .47 .56 .48 .46 .51 .43
>= OTHERS .33 .65 .13 .50 .54 .64 .64 .62 .66 .50 .61 .60 .59 .58 .56 .65 .68 .63 .62 .70 .62
Table 14: Sentence-level ranking for the WMT09 German-English News Task
22
G
O
O
G
L
E
L
IU
R
B
M
T
1
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
S
T
U
T
T
G
A
R
T
U
E
D
IN
U
K
A
U
S
A
A
R
U
S
A
A
R
-C
O
M
B
O
GOOGLE .34? .56 .51 .55? .44 .56? .37 .41 .42 .45 .45 .43
LIU .58? .62? .55? .55? .61? .59? .37 .38 .47 .43 .58? .44
RBMT1 .39 .33? .56? .44 .50? .57? .41 .32? .37? .35? .45 .42
RBMT2 .35 .34? .34? .43 .37? .40 .25? .25? .31? .36? .37? .32?
RBMT3 .31? .35? .41 .35 .37? .41 .24? .25? .33? .43 .49 .36?
RBMT4 .48 .33? .33? .56? .55? .47 .37 .35? .34? .45 .44 .38
RBMT5 .36? .35? .33? .50 .53 .33 .36? .32? .35? .31? .25? .32?
RWTH .51 .46 .50 .60? .65? .51 .60? .38 .47 .48 .52 .54
STUTTGART .50 .47 .62? .65? .64? .57? .62? .46 .52? .54? .66? .53
UEDIN .50 .37 .53? .64? .62? .60? .55? .45 .28? .41 .53 .35
UKA .47 .42 .57? .58? .46 .44 .62? .35 .32? .36 .46 .41
USAAR .46 .36? .46 .55? .42 .42 .48? .42 .28? .39 .44 .41
USAAR-COMBO .37 .45 .54 .55? .55? .53 .61? .39 .40 .39 .46 .52
> OTHERS .44 .38 .48 .55 .53 .47 .54 .37 .33 .39 .42 .48 .41
>= OTHERS .54 .49 .57 .66 .64 .58 .64 .48 .43 .51 .54 .58 .52
Table 15: Sentence-level ranking for the WMT09 English-German News Task
G
O
O
G
L
E
N
IC
T
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
T
A
L
P
-U
P
C
U
E
D
IN
U
S
A
A
R
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
GOOGLE .21? .40 .40 .41 .38 .23? .35 .31? .25? .36 .14 .21
NICT .74? .52 .53 .63? .64? .55? .61? .65? .59? .62? .78? .66?
RBMT1 .56 .40 .34 .44 .46 .35 .48 .42 .42 .57? .52 .54
RBMT3 .40 .39 .40 .34 .36 .42 .4 .55 .50 .57? .48 .62?
RBMT4 .55 .32? .41 .46 .47 .39 .49 .49 .48 .54 .57? .54
RBMT5 .54 .30? .35 .44 .38 .45 .50 .49 .23 .51 .51 .66?
RWTH .64? .29? .50 .53 .53 .49 .42 .46 .43 .44 .51 .58?
TALP-UPC .48 .24? .44 .47 .41 .36 .39 .36 .32? .47 .45 .50
UEDIN .61? .16? .48 .42 .41 .46 .44 .43 .44 .49 .51 .41
USAAR .69? .28? .47 .44 .38 .35 .43 .60? .48 .64? .58? .56?
BBN-COMBO .35 .20? .32? .36? .39 .37 .36 .39 .32 .31? .50 .40
CMU-COMBO .19 .15? .33 .39 .32? .37 .36 .31 .37 .21? .35 .31
USAAR-COMBO .23 .20? .42 .31? .39 .25? .27? .35 .35 .32? .36 .29
> OTHERS .50 .26 .42 .42 .42 .42 .39 .44 .43 .37 .49 .49 .50
>= OTHERS .70 .37 .55 .55 .53 .55 .51 .59 .56 .51 .64 .70 .69
Table 16: Sentence-level ranking for the WMT09 Spanish-English News Task
G
O
O
G
L
E
N
U
S
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
T
A
L
P
-U
P
C
U
E
D
IN
U
S
A
A
R
U
S
A
A
R
-C
O
M
B
O
GOOGLE .39 .21? .49 .36 .48 .34? .39 .33 .36? .21
NUS .50 .11? .62? .51 .51 .35 .25 .47 .36 .43
RBMT1 .76? .80? .79? .79? .83? .64? .76? .80? .67? .64?
RBMT3 .42 .31? .16? .30? .43 .34 .29? .56 .24? .32
RBMT4 .47 .32 .11? .52? .49 .38 .36 .51 .39 .38
RBMT5 .42 .40 .11? .49 .35 .31? .39 .47 .18? .47
RWTH .59? .52 .26? .54 .51 .61? .46 .56? .39 .55?
TALP-UPC .49 .41 .17? .63? .52 .51 .29 .45? .39 .41
UEDIN .50 .32 .17? .36 .37 .46 .30? .29? .32? .36
USAAR .58? .56 .23? .67? .53 .47? .51 .49 .61? .58?
USAAR-COMBO .31 .45 .21? .54 .49 .50 .30? .43 .43 .33?
> OTHERS .50 .45 .17 .56 .47 .53 .38 .42 .52 .37 .43
>= OTHERS .65 .59 .25 .66 .61 .64 .51 .58 .66 .48 .61
Table 17: Sentence-level ranking for the WMT09 English-Spanish News Task
23
C
M
U
-S
T
A
T
X
F
E
R
C
O
L
U
M
B
IA
D
C
U
G
E
N
E
V
A
G
O
O
G
L
E
JH
U
L
IM
S
I
L
IU
M
-S
Y
S
T
R
A
N
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
U
E
D
IN
U
K
A
U
S
A
A
R
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
D
C
U
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
CMU-STATXFER .37 .44 .17? .63? .47 .46 .58? .34 .32 .25? .42 .48 .46 .28 .38 .58? .47 .39 .41 .35
COLUMBIA .56 .56? .37 .71? .48 .56? .35 .45 .28? .38 .42 .41 .33 .58 .50 .64? .52 .64? .71? .58?
DCU .27 .29? .15? .67? .45 .33 .34 .29 .31 .29 .27? .24 .37 .21? .39 .61? .4 .36 .37 .1
GENEVA .76? .54 .73? .71? .65? .73? .62? .66? .76? .46 .79? .57 .74? .72? .67? .69? .52 .71? .67? .64?
GOOGLE .23? .17? .12? .13? .21? .35 .09? .20? .27? .31? .44 .16? .21? .33 .27? .28 .30 .34 .37 .16?
JHU .40 .26 .38 .22? .60? .31 .44 .27 .37 .29? .41 .33 .37 .48 .48 .53 .47 .31 .47 .29
LIMSI .4 .16? .38 .19? .56 .49 .29 .37 .27 .20? .38 .23? .33 .29 .38 .61? .47 .31 .36 .26?
LIUM-SYSTRAN .23? .30 .42 .33? .61? .27 .45 .48 .31 .41 .44 .32 .35 .41 .39 .54? .61? .24 .67? .36
RBMT1 .53 .23 .42 .19? .57? .46 .51 .45 .47 .33 .46 .33 .41 .30 .61 .77? .51 .41 .50 .41
RBMT3 .57 .63? .55 .15? .69? .44 .57 .52 .41 .22? .38 .51 .43 .43 .31 .57? .46 .47 .38 .55
RBMT4 .58? .35 .51 .36 .67? .60? .63? .35 .41 .59? .40 .55 .50 .71? .52? .63? .65? .65? .66? .38
RBMT5 .42 .49 .54? .09? .38 .49 .49 .37 .27 .29 .34 .38 .39 .51 .18 .42 .58 .48 .50 .60?
RWTH .38 .39 .45 .32 .63? .46 .51? .34 .56 .39 .32 .52 .48 .46 .46 .66? .62? .61? .66? .54?
UEDIN .41 .21 .31 .19? .68? .46 .42 .35 .41 .38 .31 .46 .33 .34 .41 .41 .35 .44 .63? .37
UKA .40 .31 .54? .19? .51 .37 .44 .33 .52 .51 .17? .27 .32 .49 .34 .39 .53 .36 .44 .29
USAAR .44 .43 .52 .26? .62? .48 .46 .30 .30 .58 .17? .24 .44 .47 .41 .65? .52 .70? .55 .41
BBN-COMBO .21? .21? .12? .23? .26 .32 .28? .23? .12? .26? .22? .49 .09? .34 .23 .19? .44 .49? .28 .21?
CMU-COMBO .41 .36 .4 .28 .30 .35 .47 .21? .29 .42 .23? .31 .17? .49 .25 .42 .31 .37 .29 .25
CMU-COMBO-H .24 .21? .38 .23? .37 .39 .31 .24 .31 .41 .28? .31 .14? .33 .34 .24? .18? .3 .29 .27
DCU-COMBO .41 .13? .42 .20? .37 .29 .50 .19? .44 .49 .23? .46 .20? .21? .37 .39 .31 .26 .46 .19?
USAAR-COMBO .41 .25? .18 .28? .66? .53 .52? .48 .41 .38 .53 .17? .21? .42 .42 .47 .58? .58 .47 .63?
> OTHERS .40 .31 .41 .23 .56 .43 .46 .36 .37 .41 .30 .40 .33 .41 .40 .40 .50 .47 .46 .49 .36
>= OTHERS .58 .5 .66 .34 .76 .62 .65 .60 .56 .54 .47 .59 .52 .61 .61 .55 .73 .66 .71 .67 .57
Table 18: Sentence-level ranking for the WMT09 French-English News Task
D
C
U
G
E
N
E
V
A
G
O
O
G
L
E
L
IM
S
I
L
IU
M
-S
Y
S
T
R
A
N
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
S
Y
S
T
R
A
N
U
E
D
IN
U
K
A
U
S
A
A
R
D
C
U
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
DCU .12? .39 .47 .44 .33 .44 .27 .45 .24? .49 .24 .46 .26? .39 .33
GENEVA .62? .73? .69? .80? .50? .71? .50? .52? .56? .66? .46? .56? .57 .74? .84?
GOOGLE .46 .15? .28 .42 .26 .44 .26? .34 .29? .44 .24 .32 .29 .36 .32
LIMSI .25 .16? .45 .48 .23? .43 .30 .45 .27 .42 .34 .4 .36 .53? .38
LIUM-SYSTRAN .24 ? .45 .32 .17? .29 .17? .21? .38 .29 .17? .35 .17? .41 .41
RBMT1 .39 .25? .51 .51? .53? .46 .40 .29 .52 .36 .60? .63? .41 .44 .60?
RBMT3 .36 .11? .37 .37 .52 .24 .25? .27? .31 .44 .43 .32 .27? .53 .44
RBMT4 .36 .19? .58? .37 .57? .23 .61? .42 .32 .50 .22 .39 .44 .53 .56?
RBMT5 .41 .17? .53 .39 .61? .38 .58? .30 .41 .52? .41 .48 .13 .54 .60
RWTH .59? .21? .63? .50 .47 .29 .44 .37 .31 .37 .35 .51 .16? .50? .57?
SYSTRAN .35 .20? .33 .39 .38 .40 .22 .29 .26? .44 .47 .33 .32 .60? .45
UEDIN .38 .11? .41 .28 .77? .33? .51 .44 .49 .32 .37 .30 .31 .56 .56?
UKA .36 .09? .46 .4 .45 .23? .50 .39 .29 .29 .47 .26 .19? .41 .56?
USAAR .66? .27 .52 .49 .70? .31 .61? .29 .32 .64? .62 .51 .61? .76? .65?
DCU-COMBO .32 .11? .30 .18? .45 .22 .29 .33 .29 .13? .27? .26 .41 .12? .21
USAAR-COMBO .40 ? .39 .17 .26 .17? .28 .20? .28 .20? .39 .04? .06? .08? .39
> OTHERS .41 .15 .47 .39 .52 .29 .45 .32 .35 .35 .45 .34 .42 .28 .51 .49
>= OTHERS .65 .38 .68 .64 .73 .54 .65 .59 .57 .58 .65 .60 .66 .48 .74 .77
Table 19: Sentence-level ranking for the WMT09 English-French News Task
C
U
-B
O
JA
R
G
O
O
G
L
E
U
E
D
IN
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
CU-BOJAR .54? .44 .45? .52?
GOOGLE .28? .32? .18? .23
UEDIN .38 .51? .38 .45?
BBN-COMBO .31? .39? .32 .38?
CMU-COMBO .28? .29 .27? .24?
> OTHERS .31 .43 .34 .31 .40
>= OTHERS .51 .75 .57 .65 .73
Table 20: Sentence-level ranking for the WMT09 Czech-English News Task
24
C
U
-B
O
JA
R
C
U
-T
E
C
T
O
M
T
E
U
R
O
T
R
A
N
X
P
G
O
O
G
L
E
P
C
T
R
A
N
S
U
E
D
IN
CU-BOJAR .31? .45? .43? .48? .30?
CU-TECTOMT .51? .54? .56? .58? .42?
EUROTRANXP .35? .26? .39 .38 .29?
GOOGLE .31? .30? .42 .43? .26?
PCTRANS .33? .27? .36 .38? .30?
UEDIN .42? .37? .52? .50? .53?
> OTHERS .38 .30 .46 .45 .48 .31
>= OTHERS .61 .48 .67 .66 .67 .53
Table 21: Sentence-level ranking for the WMT09 English-Czech News Task
M
O
R
P
H
O
U
E
D
IN
U
M
D
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
MORPHO .21? .28? .24? .27? .28?
UEDIN .70? .59? .45? .55? .50?
UMD .61? .26? .21? .29 .38
BBN-COMBO .67? .23? .48? .41? .52?
CMU-COMBO .59? .25? .35 .29? .42
CMU-COMBO-HYPOSEL .55? .15? .34 .27? .34
> OTHERS .62 .22 .41 .29 .37 .42
>= OTHERS .75 .45 .66 .54 .62 .68
Table 22: Sentence-level ranking for the WMT09 Hungarian-English News Task
G
O
O
G
L
E
C
Z
G
O
O
G
L
E
E
S
G
O
O
G
L
E
F
R
R
B
M
T
2 D
E
R
B
M
T
3 D
E
R
B
M
T
3 E
S
R
B
M
T
3 F
R
R
B
M
T
5 E
S
R
B
M
T
5 F
R
B
B
N
-C
O
M
B
O
C
Z
B
B
N
-C
O
M
B
O
D
E
B
B
N
-C
O
M
B
O
E
S
B
B
N
-C
O
M
B
O
F
R
B
B
N
-C
O
M
B
O
H
U
B
B
N
-C
O
M
B
O
X
X
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
D
E
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
H
U
C
M
U
-C
O
M
B
O
C
Z
C
M
U
-C
O
M
B
O
H
U
C
M
U
-C
O
M
B
O
X
X
D
C
U
-C
O
M
B
O
F
R
R
W
T
H
-C
O
M
B
O
D
E
R
W
T
H
-C
O
M
B
O
X
X
U
S
A
A
R
-C
O
M
B
O
E
S
GOOGLECZ .61? .54? .47 .52 .51 .47 .61? .42 .38 .52 .55 .54 .11? .51 .48 .34 .49 .32 .53 .52 .50 .59 .53
GOOGLEES .33
? .42 .37 .38 .41 .35 .49 .45 .11? .39 .25 .36 .18? .26? .36 .22? .32 .18? .38 .4 .4 .38 .22
GOOGLEFR .27
? .42 .26? .36 .43 .47 .33 .35 .29? .23? .50 .23 .14? .29? .21? .11? .17? .22? .39 .48 .32 .36 .27
RBMT2DE .33 .49 .61? .41 .43 .25? .52 .38 .33 .41 .4 .55 .20? .66? .62? .18? .55 .35 .35 .58 .54 .61? .57?
RBMT3DE .37 .60 .54 .41 .42 .38 .45 .61 .48 .39 .40 .63? .32 .43 .25? .35 .35 .25? .56 .69? .46 .49 .46
RBMT3ES .34 .52 .46 .51 .54 .43 .36 .38 .30? .54 .41 .47 .25? .50 .42 .26? .43 .27? .52 .57 .47 .46 .26?
RBMT3FR .40 .58 .37 .63? .53 .57 .54 .50 .36 .64? .44 .55 .13? .60 .64? .4 .53 .31 .46 .48 .44 .52 .42
RBMT5ES .29
? .41 .55 .31 .48 .36 .33 .39 .16? .44 .50 .68? .23? .35 .48 .38 .37 .41 .60? .51 .51 .65? .32
RBMT5FR .47 .52 .45 .50 .33 .51 .34 .42 .29 .59 .44 .49 ? .49 .61? .28? .19? .35 .58? .60? .27 .59 .57
BBN-COMBOCZ .41 .74? .65? .55 .44 .67? .56 .80? .46 .46 .58 .70? .22? .73? .63? .32 .38 .48 .65? .72? .66? .70? .58
BBN-COMBODE .39 .54 .58? .41 .49 .44 .31? .44 .28 .49 .49 .52 .16? .52 .36 .22? .38 .33? .41 .68? .34 .52 .56
BBN-COMBOES .38 .40 .41 .43 .47 .55 .46 .25 .51 .31 .43 .44 .20? .50 .42 .30? .32 .29? .36 .62 .47 .44 .38
BBN-COMBOFR .38 .52 .35 .36 .27? .53 .40 .26? .33 .24? .44 .36 .12? .47 .47 .32 .44 .27? .41 .42 .33 .60? .35
BBN-COMBOHU .84? .75? .78? .60? .57 .70? .71? .62? .84? .65? .72? .63? .85? .78? .69? .60? .71? .50 .85? .78? .87? .86? .75?
BBN-COMBOXX .4 .54? .63? .34? .50 .47 .32 .45 .39 .20? .39 .45 .41 .14? .24? .21? .3 .21? .46 .40 .47 .41 .41
CMU-CMB-HYPDE .48 .43 .68? .29? .64? .46 .31? .30 .30? .23? .41 .39 .32 .19? .74? .21? .32 .31 .50 .74? .38 .56? .53
CMU-CMB-HYPHU .63 .75? .78? .70? .55 .63? .46 .58 .59? .50 .61? .70? .59 .13? .68? .69? .65? .39 .75? .71? .82? .80? .68?
CMU-COMBOCZ .32 .59 .81? .36 .50 .46 .41 .50 .60? .28 .54 .52 .47 .20? .55 .56 .26? .13? .55 .69? .57 .66? .55
CMU-COMBOHU .62 .76? .69? .58 .68? .67? .59 .54 .54 .48 .67? .64? .70? .32 .74? .60 .50 .77? .66? .72? .61 .82? .82?
CMU-COMBOXX .4 .50 .33 .51 .37 .43 .44 .29? .24? .32? .56 .43 .39 .13? .39 .39 .16? .30 .32? .39 .4 .46 .4
DCU-COMBOFR .44 .57 .29 .32 .25? .29 .26 .35 .27? .19? .23? .38 .42 .15? .34 .20? .12? .19? .17? .50 .55 .49 .30?
RWTH-COMBODE .41 .43 .52 .37 .39 .53 .50 .35 .53 .25? .40 .47 .54 .10? .47 .41 .07? .38 .30 .53 .38 .56 .49
RWTH-COMBOXX .31 .38 .44 .26? .41 .39 .31 .26? .32 .18? .29 .44 .19? .10? .36 .25? .11? .28? .15? .39 .42 .28 .44
USAAR-COMBOES .37 .37 .54 .21? .4 .58? .39 .47 .31 .32 .34 .28 .55 .11? .38 .38 .20? .38 .18? .44 .67? .43 .44
> OTHERS .41 .54 .54 .43 .45 .49 .41 .44 .44 .32 .46 .46 .50 .16 .51 .45 .26 .40 .29 .52 .57 .48 .55 .47
>= OTHERS .52 .67 .70 .55 .55 .57 .52 .58 .58 .43 .57 .59 .62 .27 .62 .58 .37 .52 .36 .63 .68 .59 .69 .62
Table 23: Sentence-level ranking for the WMT09 All-English News Task
25
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
BBN-COMBO .37 .40?
CMU-COMBO .41 .44?
RWTH-COMBO .32? .34?
> OTHERS .36 .35 .42
>= OTHERS .62 .58 .67
Table 24: Sentence-level ranking for the WMT09 Multisource-English News Task
26
R
A
N
K
B
L
E
U
B
L
E
U
-C
A
S
E
D
B
L
E
U
-T
E
R
B
L
E
U
S
P
B
L
E
U
S
P
41
14
M
A
X
S
IM
M
E
T
E
O
R
-0
.6
M
E
T
E
O
R
-0
.7
M
E
T
E
O
R
-R
A
N
K
IN
G
N
IS
T
N
IS
T
-C
A
S
E
D
R
T
E
-A
B
S
O
L
U
T
E
R
T
E
-P
A
IR
W
IS
E
T
E
R
T
E
R
P
U
L
C
W
C
D
6P
4E
R
W
P
F
W
P
B
L
E
U
German-English News Task
BBN-COMBO 0.68 0.24 0.22 ?0.17 0.29 0.31 0.51 0.55 0.6 0.41 7.08 6.78 0.13 0.1 0.54 0.63 0.31 0.45 0.36 0.31
CMU-COMBO 0.63 0.22 0.21 ?0.19 0.28 0.29 0.49 0.54 0.58 0.4 6.95 6.71 0.12 0.09 0.56 0.66 0.29 0.47 0.35 0.29
CMU-COMBO-HYPOSEL 0.62 0.23 0.21 ?0.19 0.28 0.3 0.49 0.54 0.57 0.4 6.79 6.5 0.11 0.09 0.57 0.66 0.29 0.47 0.35 0.3
GENEVA 0.33 0.1 0.09 ?0.33 0.17 0.18 0.38 0.43 0.44 0.30 4.88 4.65 0.03 0.04 0.71 0.86 0.22 0.58 0.25 0.17
GOOGLE 0.65 0.21 0.20 ?0.2 0.27 0.28 0.48 0.54 0.57 0.39 6.85 6.65 0.11 0.11 0.56 0.65 0.29 0.48 0.35 0.28
JHU-TROMBLE 0.13 0.07 0.06 ?0.38 0.09 0.1 0.34 0.43 0.41 0.29 4.90 4.25 0.02 0.02 0.81 1 0.19 0.61 0.22 0.12
LIU 0.50 0.19 0.18 ?0.22 0.25 0.27 0.46 0.51 0.54 0.38 6.35 6.02 0.06 0.05 0.61 0.72 0.27 0.49 0.33 0.26
RBMT1 0.54 0.14 0.13 ?0.29 0.20 0.21 0.43 0.50 0.53 0.37 5.30 5.07 0.04 0.04 0.67 0.76 0.26 0.55 0.29 0.22
RBMT2 0.64 0.17 0.16 ?0.26 0.23 0.24 0.48 0.52 0.55 0.38 6.06 5.75 0.1 0.12 0.63 0.70 0.29 0.51 0.31 0.24
RBMT3 0.64 0.17 0.16 ?0.25 0.23 0.25 0.48 0.52 0.55 0.38 5.98 5.71 0.09 0.09 0.61 0.68 0.29 0.51 0.32 0.25
RBMT4 0.62 0.16 0.14 ?0.27 0.21 0.23 0.45 0.5 0.52 0.36 5.65 5.36 0.06 0.07 0.65 0.72 0.27 0.52 0.30 0.23
RBMT5 0.66 0.16 0.15 ?0.26 0.22 0.24 0.47 0.51 0.54 0.37 5.76 5.52 0.07 0.06 0.63 0.70 0.28 0.52 0.31 0.24
RWTH 0.50 0.19 0.18 ?0.21 0.25 0.26 0.45 0.50 0.53 0.36 6.44 6.24 0.06 0.03 0.60 0.74 0.27 0.49 0.33 0.26
RWTH-COMBO 0.7 0.23 0.22 ?0.18 0.29 0.30 0.50 0.55 0.59 0.41 7.06 6.81 0.11 0.07 0.54 0.63 0.30 0.46 0.36 0.31
STUTTGART 0.61 0.2 0.18 ?0.22 0.26 0.27 0.48 0.52 0.56 0.38 6.39 6.11 0.1 0.06 0.60 0.69 0.29 0.49 0.33 0.27
SYSTRAN 0.6 0.19 0.17 ?0.22 0.24 0.26 0.47 0.52 0.55 0.38 6.40 6.08 0.08 0.07 0.60 0.71 0.28 0.5 0.33 0.26
UEDIN 0.59 0.20 0.19 ?0.22 0.26 0.27 0.47 0.52 0.55 0.38 6.47 6.24 0.07 0.04 0.61 0.70 0.27 0.49 0.34 0.27
UKA 0.58 0.21 0.2 ?0.20 0.27 0.28 0.47 0.52 0.56 0.38 6.66 6.43 0.08 0.04 0.58 0.69 0.28 0.48 0.34 0.28
UMD 0.56 0.21 0.19 ?0.19 0.26 0.28 0.47 0.52 0.56 0.38 6.74 6.42 0.08 0.04 0.56 0.69 0.28 0.48 0.34 0.27
USAAR 0.65 0.17 0.15 ?0.26 0.23 0.24 0.47 0.51 0.54 0.38 5.89 5.64 0.06 0.05 0.64 0.71 0.28 0.52 0.31 0.24
USAAR-COMBO 0.62 0.17 0.16 ?0.25 0.23 0.24 0.47 0.51 0.55 0.38 5.99 6.85 0.07 0.06 0.64 0.70 0.28 0.51 0.32 0.25
Spanish-English News Task
BBN-COMBO 0.64 0.29 0.27 ?0.13 0.34 0.35 0.53 0.57 0.62 0.43 7.64 7.35 0.16 0.13 0.51 0.61 0.33 0.42 0.4 0.35
CMU-COMBO 0.7 0.28 0.27 ?0.13 0.33 0.35 0.53 0.58 0.62 0.43 7.65 7.46 0.21 0.2 0.51 0.60 0.34 0.42 0.40 0.36
GOOGLE 0.70 0.29 0.28 ?0.13 0.34 0.35 0.53 0.58 0.62 0.43 7.68 7.50 0.23 0.22 0.5 0.59 0.34 0.42 0.41 0.36
NICT 0.37 0.22 0.22 ?0.19 0.27 0.29 0.48 0.54 0.57 0.39 6.91 6.74 0.1 0.1 0.60 0.71 0.3 0.46 0.36 0.3
RBMT1 0.55 0.19 0.18 ?0.24 0.25 0.26 0.49 0.54 0.57 0.40 6.07 5.93 0.11 0.12 0.62 0.69 0.3 0.49 0.34 0.28
RBMT3 0.55 0.20 0.2 ?0.22 0.26 0.27 0.50 0.54 0.58 0.41 6.24 6.08 0.13 0.14 0.60 0.65 0.31 0.48 0.36 0.29
RBMT4 0.53 0.2 0.19 ?0.22 0.25 0.27 0.48 0.53 0.57 0.4 6.20 6.03 0.10 0.11 0.60 0.67 0.3 0.48 0.35 0.28
RBMT5 0.55 0.20 0.2 ?0.22 0.26 0.27 0.5 0.54 0.58 0.40 6.26 6.10 0.12 0.11 0.6 0.65 0.31 0.48 0.36 0.29
RWTH 0.51 0.24 0.23 ?0.16 0.3 0.31 0.49 0.54 0.58 0.4 7.12 6.95 0.11 0.08 0.56 0.68 0.31 0.45 0.37 0.32
TALP-UPC 0.59 0.26 0.25 ?0.15 0.31 0.33 0.51 0.56 0.6 0.41 7.28 7.02 0.13 0.11 0.54 0.64 0.32 0.44 0.38 0.33
UEDIN 0.56 0.26 0.25 ?0.15 0.32 0.33 0.51 0.56 0.60 0.42 7.25 7.04 0.16 0.1 0.55 0.64 0.32 0.43 0.39 0.34
USAAR 0.51 0.2 0.19 ?0.22 0.25 0.27 0.48 0.54 0.57 0.4 6.31 6.14 0.11 0.09 0.62 0.67 0.3 0.48 0.34 0.28
USAAR-COMBO 0.69 0.29 0.27 ?0.13 0.34 0.35 0.53 0.58 0.62 0.43 7.58 7.25 0.20 0.13 0.51 0.6 0.34 0.42 0.4 0.35
French-English News Task
BBN-COMBO 0.73 0.31 0.3 ?0.11 0.36 0.38 0.54 0.59 0.64 0.45 7.88 7.58 0.14 0.12 0.2 0.20 0.36 0.40 0.41 0.37
CMU-COMBO 0.66 0.3 0.29 ?0.12 0.35 0.36 0.53 0.58 0.63 0.44 7.72 7.57 0.15 0.12 0.24 0.26 0.35 0.41 0.41 0.37
CMU-COMBO-HYPOSEL 0.71 0.28 0.26 ?0.14 0.33 0.35 0.53 0.57 0.61 0.43 7.40 7.15 0.1 0.08 0.31 0.33 0.34 0.42 0.4 0.35
CMU-STATXFER 0.58 0.24 0.23 ?0.18 0.29 0.31 0.49 0.54 0.58 0.40 6.89 6.75 0.08 0.07 0.38 0.42 0.31 0.46 0.37 0.32
COLUMBIA 0.50 0.23 0.22 ?0.18 0.29 0.30 0.49 0.54 0.58 0.40 6.85 6.68 0.07 0.07 0.36 0.39 0.31 0.46 0.36 0.31
DCU 0.66 0.27 0.25 ?0.15 0.32 0.34 0.52 0.56 0.61 0.42 7.29 6.94 0.09 0.07 0.32 0.34 0.33 0.43 0.38 0.34
DCU-COMBO 0.67 0.31 0.31 ?0.11 0.36 0.37 0.54 0.59 0.64 0.44 7.84 7.69 0.14 0.12 0.21 0.22 0.35 0.41 0.42 0.38
GENEVA 0.34 0.14 0.14 ?0.29 0.21 0.22 0.43 0.49 0.52 0.36 5.32 5.15 0.05 0.05 0.54 0.52 0.26 0.53 0.29 0.22
GOOGLE 0.76 0.31 0.30 ?0.10 0.36 0.37 0.54 0.58 0.63 0.44 8 7.84 0.17 0.13 0.17 0.2 0.36 0.41 0.42 0.38
JHU 0.62 0.27 0.23 ?0.15 0.32 0.33 0.51 0.56 0.6 0.41 7.23 6.68 0.08 0.05 0.33 0.36 0.32 0.43 0.37 0.32
LIMSI 0.65 0.26 0.25 ?0.16 0.30 0.32 0.51 0.56 0.60 0.42 7.02 6.87 0.09 0.07 0.35 0.36 0.33 0.44 0.38 0.33
LIUM-SYSTRAN 0.60 0.27 0.26 ?0.15 0.32 0.33 0.51 0.56 0.60 0.42 7.26 7.10 0.10 0.06 0.33 0.36 0.33 0.43 0.39 0.35
RBMT1 0.56 0.18 0.18 ?0.25 0.24 0.25 0.48 0.53 0.57 0.4 5.89 5.73 0.07 0.06 0.51 0.45 0.3 0.50 0.34 0.26
RBMT3 0.54 0.2 0.19 ?0.22 0.25 0.27 0.48 0.53 0.56 0.39 6.12 5.96 0.07 0.06 0.45 0.45 0.30 0.49 0.35 0.28
RBMT4 0.47 0.19 0.18 ?0.24 0.24 0.26 0.48 0.52 0.56 0.39 5.97 5.83 0.07 0.06 0.46 0.45 0.3 0.49 0.34 0.27
RBMT5 0.59 0.19 0.19 ?0.24 0.25 0.26 0.49 0.54 0.57 0.40 6.03 5.9 0.09 0.07 0.46 0.43 0.31 0.49 0.35 0.28
RWTH 0.52 0.25 0.24 ?0.16 0.30 0.32 0.5 0.55 0.59 0.40 7.09 6.94 0.07 0.03 0.35 0.39 0.32 0.44 0.38 0.32
UEDIN 0.61 0.25 0.24 ?0.16 0.31 0.32 0.50 0.55 0.59 0.41 7.04 6.85 0.08 0.04 0.35 0.38 0.32 0.44 0.38 0.33
UKA 0.61 0.26 0.25 ?0.15 0.31 0.33 0.51 0.55 0.6 0.41 7.17 7.00 0.08 0.04 0.34 0.37 0.32 0.44 0.38 0.34
USAAR 0.55 0.19 0.18 ?0.24 0.24 0.26 0.48 0.54 0.57 0.4 6.08 5.92 0.07 0.06 0.46 0.44 0.3 0.49 0.34 0.26
USAAR-COMBO 0.57 0.26 0.25 ?0.16 0.31 0.33 0.51 0.55 0.59 0.41 7.13 6.85 0.08 0.02 0.33 0.35 0.32 0.44 0.38 0.33
Czech-English News Task
BBN-COMBO 0.65 0.22 0.20 ?0.19 0.27 0.29 0.47 0.52 0.56 0.39 6.74 6.45 0.24 0.3 0.52 0.60 0.29 0.47 0.34 0.29
CMU-COMBO 0.73 0.22 0.20 ?0.2 0.27 0.29 0.47 0.53 0.57 0.39 6.72 6.46 0.34 0.34 0.53 0.60 0.29 0.47 0.35 0.29
CU-BOJAR 0.51 0.16 0.15 ?0.26 0.22 0.24 0.43 0.5 0.52 0.36 5.84 5.54 0.26 0.28 0.61 0.69 0.26 0.52 0.31 0.24
GOOGLE 0.75 0.21 0.20 ?0.19 0.26 0.28 0.46 0.52 0.55 0.38 6.82 6.61 0.32 0.33 0.53 0.62 0.29 0.47 0.35 0.28
UEDIN 0.57 0.2 0.19 ?0.23 0.25 0.27 0.45 0.50 0.54 0.37 6.2 6 0.22 0.25 0.56 0.63 0.27 0.49 0.33 0.27
Hungarian-English News Task
BBN-COMBO 0.54 0.14 0.13 ?0.29 0.19 0.21 0.38 0.45 0.46 0.32 5.46 5.2 0.16 0.18 0.71 0.83 0.23 0.55 0.27 0.2
CMU-COMBO 0.62 0.14 0.13 ?0.29 0.19 0.21 0.39 0.46 0.47 0.32 5.52 5.24 0.28 0.22 0.71 0.82 0.23 0.55 0.28 0.2
CMU-COMBO-HYPOSEL 0.68 0.14 0.12 ?0.29 0.19 0.21 0.39 0.45 0.46 0.32 5.51 5.16 0.25 0.25 0.71 0.82 0.23 0.55 0.27 0.2
MORPHO 0.75 0.1 0.09 ?0.36 0.15 0.17 0.39 0.45 0.46 0.32 4.75 4.55 0.34 0.49 0.79 0.83 0.23 0.6 0.26 0.17
UEDIN 0.45 0.12 0.11 ?0.32 0.18 0.19 0.37 0.42 0.43 0.30 4.95 4.74 0.12 0.12 0.75 0.87 0.21 0.58 0.27 0.19
UMD 0.66 0.13 0.12 ?0.28 0.18 0.2 0.36 0.44 0.45 0.30 5.41 5.12 0.21 0.13 0.68 0.85 0.22 0.55 0.27 0.18
Table 25: Automatic evaluation metric scores for translations into English
27
R
A
N
K
B
L
E
U
B
L
E
U
-C
A
S
E
D
B
L
E
U
-T
E
R
B
L
E
U
S
P
B
L
E
U
S
P
41
14
N
IS
T
N
IS
T
-C
A
S
E
D
T
E
R
T
E
R
P
W
C
D
6P
4E
R
W
P
F
W
P
B
L
E
U
English-German News Task
GOOGLE 0.54 0.15 0.14 ?0.29 0.20 0.22 5.36 5.25 0.62 0.74 0.54 0.3 0.23
LIU 0.49 0.14 0.13 ?0.29 0.2 0.21 5.35 5.18 0.65 0.78 0.54 0.3 0.23
RBMT1 0.57 0.11 0.11 ?0.32 0.17 0.19 4.69 4.59 0.67 0.81 0.57 0.28 0.21
RBMT2 0.66 0.13 0.13 ?0.30 0.19 0.21 5.08 4.99 0.62 0.75 0.55 0.30 0.23
RBMT3 0.64 0.12 0.12 ?0.29 0.2 0.21 4.8 4.71 0.62 0.76 0.54 0.31 0.25
RBMT4 0.58 0.11 0.10 ?0.33 0.17 0.18 4.66 4.57 0.7 0.84 0.57 0.27 0.2
RBMT5 0.64 0.13 0.12 ?0.3 0.19 0.20 5.03 4.94 0.64 0.79 0.55 0.3 0.23
RWTH 0.48 0.14 0.13 ?0.28 0.2 0.21 5.51 5.41 0.62 0.78 0.53 0.3 0.23
STUTTGART 0.43 0.12 0.12 ?0.31 0.18 0.20 5.06 4.82 0.67 0.79 0.55 0.29 0.21
UEDIN 0.51 0.15 0.15 ?0.27 0.21 0.23 5.53 5.42 0.63 0.77 0.53 0.31 0.24
UKA 0.54 0.15 0.15 ?0.27 0.21 0.22 5.6 5.48 0.62 0.75 0.52 0.31 0.24
USAAR 0.58 0.12 0.11 ?0.33 0.18 0.19 4.83 4.71 0.69 0.8 0.57 0.28 0.21
USAAR-COMBO 0.52 0.16 0.15 ?0.27 0.21 0.23 5.6 5.39 0.62 0.75 0.52 0.31 0.24
English-Spanish News Task
GOOGLE 0.65 0.28 0.27 ?0.15 0.33 0.34 7.27 7.07 0.36 0.42 0.42 0.37 0.31
NUS 0.59 0.25 0.23 ?0.17 0.30 0.31 6.96 6.67 0.48 0.59 0.44 0.34 0.28
RBMT1 0.25 0.15 0.14 ?0.27 0.20 0.22 5.32 5.17 0.55 0.66 0.51 0.24 0.16
RBMT3 0.66 0.18 0.17 ?0.18 0.28 0.3 5.79 5.63 0.49 0.59 0.45 0.33 0.27
RBMT4 0.61 0.21 0.2 ?0.20 0.26 0.28 6.47 6.28 0.52 0.64 0.47 0.31 0.25
RBMT5 0.64 0.22 0.21 ?0.2 0.27 0.29 6.53 6.34 0.52 0.64 0.46 0.32 0.26
RWTH 0.51 0.22 0.21 ?0.18 0.27 0.29 6.83 6.63 0.50 0.65 0.46 0.32 0.26
TALP-UPC 0.58 0.25 0.23 ?0.17 0.3 0.31 6.96 6.69 0.47 0.58 0.44 0.34 0.28
UEDIN 0.66 0.25 0.24 ?0.17 0.30 0.31 6.94 6.73 0.48 0.59 0.44 0.34 0.29
USAAR 0.48 0.20 0.19 ?0.21 0.26 0.27 6.36 6.16 0.54 0.66 0.47 0.30 0.24
USAAR-COMBO 0.61 0.28 0.26 ?0.14 0.33 0.34 7.36 6.97 0.39 0.48 0.42 0.36 0.31
English-French News Task
DCU 0.65 0.24 0.22 ?0.19 0.29 0.30 6.69 6.39 0.63 0.72 0.47 0.38 0.34
DCU-COMBO 0.74 0.28 0.27 ?0.15 0.33 0.34 7.29 7.12 0.58 0.67 0.44 0.42 0.38
GENEVA 0.38 0.15 0.14 ?0.27 0.20 0.22 5.59 5.39 0.68 0.82 0.53 0.32 0.25
GOOGLE 0.68 0.25 0.24 ?0.17 0.30 0.31 6.90 6.71 0.62 0.7 0.46 0.40 0.36
LIMSI 0.64 0.25 0.24 ?0.17 0.3 0.31 6.94 6.77 0.60 0.71 0.46 0.4 0.35
LIUM-SYSTRAN 0.73 0.26 0.24 ?0.17 0.31 0.32 7.02 6.83 0.61 0.71 0.45 0.40 0.36
RBMT1 0.54 0.18 0.17 ?0.23 0.24 0.26 6.12 5.96 0.65 0.76 0.5 0.35 0.29
RBMT3 0.65 0.22 0.20 ?0.20 0.27 0.28 6.48 6.29 0.63 0.72 0.48 0.38 0.33
RBMT4 0.59 0.18 0.17 ?0.24 0.24 0.25 6.02 5.86 0.66 0.77 0.50 0.35 0.3
RBMT5 0.57 0.20 0.19 ?0.21 0.26 0.27 6.31 6.15 0.63 0.74 0.49 0.36 0.31
RWTH 0.58 0.22 0.21 ?0.19 0.27 0.28 6.67 6.51 0.62 0.75 0.48 0.38 0.32
SYSTRAN 0.65 0.23 0.22 ?0.19 0.28 0.29 6.7 6.47 0.63 0.74 0.47 0.39 0.34
UEDIN 0.60 0.24 0.23 ?0.18 0.29 0.30 6.75 6.57 0.62 0.71 0.47 0.39 0.35
UKA 0.66 0.24 0.23 ?0.18 0.29 0.30 6.82 6.65 0.61 0.71 0.46 0.39 0.35
USAAR 0.48 0.19 0.18 ?0.23 0.24 0.26 6.16 5.98 0.66 0.76 0.5 0.34 0.29
USAAR-COMBO 0.77 0.27 0.25 ?0.15 0.32 0.33 7.24 6.93 0.59 0.69 0.44 0.41 0.37
English-Czech News Task
CU-BOJAR 0.61 0.14 0.13 ?0.28 0.21 0.23 5.18 4.96 0.63 0.82 0.01 n/a n/a
CU-TECTOMT 0.48 0.07 0.07 ?0.35 0.14 0.16 4.17 4.03 0.71 0.96 0.01 n/a n/a
EUROTRANXP 0.67 0.1 0.09 ?0.33 0.16 0.18 4.38 4.26 0.7 0.93 0.01 n/a n/a
GOOGLE 0.66 0.14 0.13 ?0.30 0.20 0.22 4.96 4.84 0.66 0.82 0.01 n/a n/a
PCTRANS 0.67 0.09 0.09 ?0.34 0.17 0.18 4.34 4.19 0.71 0.90 0.01 n/a n/a
UEDIN 0.53 0.14 0.13 ?0.29 0.21 0.22 5.04 4.9 0.64 0.84 0.01 n/a n/a
English-Hungarian News Task
MORPHO 0.79 0.08 0.08 ?0.37 0.15 0.16 4.04 3.92 0.83 1 0.6 n/a n/a
UEDIN 0.32 0.1 0.09 ?0.33 0.17 0.18 4.48 4.32 0.78 1 0.56 n/a n/a
Table 26: Automatic evaluation metric scores for translations out of English
28
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1918?1927, Dublin, Ireland, August 23-29 2014.
Class-Based Language Modeling for
Translating into Morphologically Rich Languages
Arianna Bisazza and Christof Monz
Informatics Institute, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{a.bisazza,c.monz}@uva.nl
Abstract
Class-based language modeling (LM) is a long-studied and effective approach to overcome data
sparsity in the context of n-gram model training. In statistical machine translation (SMT), differ-
ent forms of class-based LMs have been shown to improve baseline translation quality when used
in combination with standard word-level LMs but no published work has systematically com-
pared different kinds of classes, model forms and LM combination methods in a unified SMT
setting. This paper aims to fill these gaps by focusing on the challenging problem of translating
into Russian, a language with rich inflectional morphology and complex agreement phenomena.
We conduct our evaluation in a large-data scenario and report statistically significant BLEU im-
provements of up to 0.6 points when using a refined variant of the class-based model originally
proposed by Brown et al. (1992).
1 Introduction
Class-based n-gram modeling is an effective approach to overcome data sparsity in language model (LM)
training. By grouping words with similar distributional behavior into equivalence classes, class-based
LMs have less parameters to train and can make predictions based on longer histories. This makes them
particularly attractive in situations where n-gram coverage is low due to shortage of training data or to
specific properties of the language at hand.
While translation into English has drawn most of the research effort in statistical machine translation
(SMT) so far, there is now a growing interest in translating into languages that are more challenging
for standard n-gram modeling techniques. Notably, morphologically rich languages are characterized by
high type/token ratios (T/T) that reflect in high out-of-vocabulary word rates and frequent backing-off to
low order n-gram estimates, even when large amounts of training data are used. These problems have
been long studied in the field of speech recognition but much less in SMT, although the target LM is a
core component of all state-of-the-art SMT frameworks.
Partly inspired by successful research in the field of speech recognition, various forms of class-based
LMs have been shown to improve the quality of SMT when used in combination with standard word-
level LMs. These approaches, however, have mostly focused on English (Uszkoreit and Brants, 2008;
Dyer et al., 2011; Monz, 2011; Hassan et al., 2007; Birch et al., 2007) with only recent exceptions
(Green and DeNero, 2012; Ammar et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). Moreover,
there is no published work that systematically evaluates different kinds of classes, model forms and LM
combination methods in a unified SMT setting. On the contrary, most of the existing literature on LM
combination uses mixtures of multiple word-level LMs for domain adaptation purposes.
This paper aims to fill these gaps by applying various class-based LM techniques to the challenging
problem of translating into a morphologically rich language. In particular we focus on English-Russian,
a language pair for which a fair amount of both parallel data and monolingual data has been provided by
the Workshop of Machine Translation (Bojar et al., 2013). Russian is characterized by a rich inflectional
morphology, with a particularly complex nominal declension (six core cases, three genders and two
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1918
number categories). This results in complex agreement phenomena and an extremely rich vocabulary.
Indeed, by examining our training data (see Section 4), we find the Russian T/T ratio to be almost two
times higher than the English one.
Given this task, we make a number of contributions leading to a better understanding of ways to utilize
class-based language models for translating into morphologically rich languages. We conduct a compar-
ative evaluation of different target LMs along the following axes: (1) Classes: data-driven versus shallow
morphology-based; (2) Model forms: simple class sequence (stream-based) versus original class-based
(Brown et al., 1992); and (3) Combination frameworks: model-level log-linear combination versus word-
level linear interpolation. When comparing the different model forms we pay particular attention to the
role word emission probabilities play in class-based models, which turns out to be a significant factor
for translating into morphologically rich languages. In this context we also evaluate for the first time a
specific form of class-based LM called fullibm (Goodman, 2001) within statistical MT.
2 Class-based language models
As introduced by (Brown et al., 1992), the idea of class-based n-gram language modeling is to group
words with similar distributional behavior into equivalence classes. The word transition probability is
then decomposed into a class transition probability and a word emission probability:
P
class
(w
i
|w
i?1
i?n+1
) = p
0
(C(w
i
)|C(w
i?1
i?n+1
)) ? p
1
(w
i
|C(w
i
)) (1)
This results in models that are more compact and more robust to data sparsity. Often, in the context of
SMT, the word emission probability is dropped and only the class sequence is modeled. In this work, we
refer to this model form as stream-based n-gram LM:
1
P
stream
(w
i
|w
i?1
i?n+1
) = p
0
(C(w
i
)|C(w
i?1
i?n+1
)) (2)
Stream-based LMs are used, for instance, in factored SMT (Koehn et al., 2007), and in general many
of the ?class-based LMs? mentioned in the SMT literature are actually of the latter form (2) (Dyer et
al., 2011; Green and DeNero, 2012; Ammar et al., 2013; Chahuneau et al., 2013; Wuebker et al., 2013;
Durrani et al., 2014). One exception is the work of Uszkoreit and Brants (2008), who incorporate word
emission probabilities in their class-based LM used as an additional feature function in the log-linear
combination (cf. Section 3.1). Interestingly, we are not aware of work that compares actual class-based
LMs and stream-based LMs with respect to SMT quality.
While class-based LMs are known to be effective at counteracting data sparsity issues due to rich
vocabularies, it is worth noting that they adhere to the fundamental constraints of n-gram modeling.
Thus, grammatical agreement may be improved by a class-based LM approach only within a limited
context window. Previous work that attempted to overcome this limitation includes (i) syntactic LMs for
n-best reranking (Hasan et al., 2006; Carter and Monz, 2011) or integrated into decoding with significant
engineering challenges (Galley and Manning, 2009; Schwartz et al., 2011) and (ii) unification-based
constraints applied to a syntax-based SMT framework (Williams and Koehn, 2011).
We will now describe different kinds of word-to-class mapping functions used by class-based LMs.
These can be completely data-driven or based on different sorts of linguistic or orthographic features.
2.1 Data-driven classes
The most popular form of class-based LMs was introduced by (Brown et al., 1992). In this approach, the
corpus vocabulary is partitioned into a preset number of clusters by directly maximizing the likelihood
of a training corpus. No linguistic or orthographic features are taken into account while training the
classes.
2
Later work has focused on decreasing the large computational cost of the exchange algorithm
proposed by Brown et al. (1992), either with a distributed algorithm (Uszkoreit and Brants, 2008) or by
using a whole-context distributional vector space model (Sch?utze and Walsh, 2011). In this paper we use
the standard SRILM implementation of Brown clustering.
1
Not to be confused with the incrementally trainable stream-based LMs of Levenberg and Osborne (2009).
2
Och (1999) extends a similar approach to bilingual clustering with the aim of generalizing the applicability of translation
rules in an alignment template SMT framework.
1919
2.2 Linguistic classes
Linguistic knowledge is another way to establish word equivalence classes. Common examples include
lemma, part of speech and morphology-based classes, each of which can capture different aspects of
the word sequence, such as the relative order of syntactic constituents or grammatical agreement. Has-
san et al. (2007) and Birch et al. (2007) went as far as scoring n-grams of Combinatorial Categorial
Grammar supertags. When using linguistic classes, one has to deal with the fact that the same word can
belong to different classes when used in different contexts. Solutions to this problem include tagging
the target word sequence as it is generated (Koehn et al., 2007; Birch et al., 2007; Green and DeNero,
2012), choosing the most probable class sequence for each phrase pair (Monz, 2011) or?even more
lightweight?choosing the most probable class for each word (Bisazza and Federico, 2012).
Alternatively, simpler deterministic class mappings can be derived by using shallow linguistic knowl-
edge, such as suffixes or orthographic features. The former can be obtained with a rule-based stemmer
(as in this work), or, even more simply, by selecting the ? most common word suffixes in a training
corpus and then mapping each word to its longest matching suffix (M?uller et al., 2012). Orthographic
features may include capitalization information or the presence of digits, punctuation or other special
characters (M?uller et al., 2012).
2.3 Hybrid surface/class models
M?uller et al. (2012) obtain the best perplexity reduction when excluding frequent words from the class
mapping. That is, each word with more than ? occurrences in the training corpus is assigned to a singleton
class with word emission probability equal to 1. The frequency threshold ? is determined with a grid
search on a monolingual held-out set. Optimal values for perplexities are shown to vary considerably
among languages. In this work we follow this setup closely.
It is worth noting that Bisazza and Federico (2012) have applied a similar idea to the problem of
style adaptation: they train a hybrid POS/word n-gram LM on an in-domain corpus and use it as an
additional SMT feature function with the goal of counterbalancing the bias towards the style of the large
out-of-domain data. The idea of modeling sequences of mixed granularity (word/subword) was earlier
introduced to speech recognition by Yazgan and Sarac?lar (2004).
The most extensive comparison of distributional, morphological and hybrid classes that we are aware
of is the work by M?uller et al. (2012), but that does not include any SMT evaluation. Looking at perplex-
ity results over a large number of European language pairs (not including Russian), M?uller et al. (2012)
conclude that a hybrid suffix/word class-based LM simply built on frequency-based suffixes performs as
well as a model trained on much more expensive distributional classes. Motivated by this finding, we
evaluate these two kinds of classes in the context of SMT into a morphologically rich language.
2.4 Fullibm language model
As outlined above, the class-based LMs generally used in SMT are in fact stream-based models in the
sense that they only estimate the probability of the class sequence (see Equation 2). However, the clas-
sic form of class-based LM (Brown et al., 1992) also includes a class-to-word emission probability
p
1
(w
i
|C(w
i
)) whose utility has not been properly assessed in the context of SMT.
Besides, we observe that a variety of class-based LM variants have been studied in speech recognition
but not in SMT. In particular, Goodman (2001) presents a generalization of the standard class-based form
where the word emission is also conditioned on the class history rather than on the current class alone.
The resulting model is called fullibm:
P
fullibm
(w
i
|w
i?1
i?n+1
) = p
0
(C(w
i
)|C(w
i?1
i?n+1
)) ? p
1
(w
i
|C(w
i
i?n+1
)) (3)
We expect this model to yield more refined, context-sensitive word emission distributions which may
result in better target LM probabilities for our SMT system.
1920
3 SMT combining framework
Class-based LMs are rarely used in isolation, but are rather combined with standard word-level models.
There exist at least two ways to combine multiple LMs into a log-linear SMT decoder: (i) as separate
feature functions in the global log-linear combination or (ii) as components of a linear mixture counting
as a single feature function in the global combination.
3.1 Log-linear combination
The standard log-linear approach to SMT allows for the combination of m arbitrary model components
(or feature functions), each weighted by a corresponding weight ?
m
:
p(x|h) =
?
m
p
m
(x|h)
?
m
(4)
In typical SMT settings, p
m
(x|h) are phrase- or word-level translation probabilities, reordering prob-
abilities, and so on. Treating the new LM as an additional feature function has the advantage that its
weight can be directly optimized for SMT quality together with all other feature weights, using standard
parameter tuning techniques (Och, 2003; Hopkins and May, 2011).
3.2 Linear interpolation
The other widely used combining framework is linear interpolation or mixture model:
p(x|h) =
?
q
?
q
p
q
(x|h) (5)
More specifically, word LMs are usually interpolated as a word-level weighted average of the n-gram
probabilities:
p
mixLM
(e) =
n
?
i=1
(
?
q
?
q
p
q
(e
i
|h
i
)
)
(6)
The drawback of this approach is that the linear interpolation weights, or lambdas, cannot be set with
standard SMT tuning techniques. Instead, interpolation weights are typically determined by maximizing
the likelihood of a held-out monolingual data set, but this does not always outperform simple uniform
weighting in terms of translation quality.
3
Despite the lambda optimization issue, linear interpolation with uniform or maximum-likelihood
weights has been shown to work better for SMT than log-linear combination when combining regu-
lar word n-gram LMs (Foster and Kuhn, 2007). However, to the best of our knowledge, the linear
interpolation of word- and class-based LMs has never been tested in SMT.
In their intrinsic evaluation, M?uller et al. (2012) show that linear mixing with hybrid class/surface
models of various kinds consistently decrease the perplexity of a Kneser-Ney smoothed word-level LM,
with relative improvements ranging between 3% (English) and 11% (Finnish). All their models are
interpolated with class-specific lambda weights, according to the following formula:
P
mix
(w
i
|w
i?1
i?n+1
) = ?
C(w
i?1
)
? P
class
(w
i
|w
i?1
i?n+1
) + (1? ?
C(w
i?1
)
) ? P
word
(w
i
|w
i?1
i?n+1
) (7)
where P
word
corresponds to the standard n-gram model using the lexical forms. Equation 7 can be seen
as a generalization of the simple interpolation ?P
class
+ (1 ? ?)P
word
used by Brown et al. (1992).
The class-specific lambdas are estimated by a deleted interpolation algorithm (Bahl et al., 1991). In our
experiments, we test both generic and class-specific lambda interpolation for SMT.
3
Foster and Kuhn (2007) also tried more sophisticated techniques to set interpolation weights but did not obtain significant
improvements.
1921
Corpus Lang. #Sent. #Tok. T/T
paral.train
EN
1.9M
48.9M .0107
RU 45.9M .0204
Wiki dict.
EN/RU 508K ? ?
mono.train
RU 21.0M 390M .0068
newstest12
EN
3K 64K ?
newstest13
3K 56K ?
Table 1: Training and test data statistics: number of sentences, number of tokens and type/token ratio
(T/T). All numbers refer to tokenized, lowercased data.
4 Evaluation
We perform a series of experiments to compare the effectiveness for SMT of various class mapping
functions, different model forms, and different LM combining frameworks.
The task, organized by the Workshop of Machine Translation (WMT, Bojar et al. (2013)), consists
of translating a set of news stories from English to Russian. As shown in Table 1, the available data
includes a fairly large parallel training corpus (1.9M sentences) from various sources, a set of Wikipedia
parallel headlines shared by CMU,
4
and a larger monolingual corpus for model training (21M sentences).
By measuring the type/token ratios of the two sides of a parallel corpus, we can estimate the difference
in morphological complexity between two languages: as shown in Table 1, the Russian T/T is almost
two times higher than the English one (.0204 vs .0107) in the WMT13 parallel training data. As is
usually the case, much more data is available for LM training. Nevertheless we report a rather high
out-of-vocabulary word rate on the devsets? reference translations (2.28%).
4.1 Baseline
Our baseline is an in-house phrase-based (Koehn et al., 2003) statistical machine translation system very
similar to Moses (Koehn et al., 2007). All system runs use hierarchical lexicalized reordering (Gal-
ley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuous
reordering, all with respect to left-to-right and right-to-left decoding. Other features include linear distor-
tion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-
level 5-gram target language model trained on all available monolingual data with modified Kneser-Ney
smoothing (Chen and Goodman, 1999). The distortion limit is set to 6 and for each source phrase the top
30 translation candidates are considered.
The feature weights for all approaches were tuned by using pairwise ranking optimization (Hopkins
and May, 2011) on newstest12. During tuning, 14 PRO parameter estimation runs are performed in paral-
lel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO
runs are then averaged and passed on to the next decoding iteration. Performing weight estimation inde-
pendently for a number of samples corrects for some of the instability that can be caused by individual
samples.
4.2 Language models
The additional LMs are trained with Witten-Bell smoothing (Witten and Bell, 1991), which is a common
choice for class-based LM training as Kneser-Ney smoothing cannot be used for computing discount
factors when the count-of-counts are zero. The main series of experiments employ 5-gram models, but
we also evaluate the usefulness of increasing the order to 7-gram (see Table 3).
5
Data-driven clusters are learned with the standard Brown clustering algorithm, which greedily maxi-
mizes the log likelihood of a class bigram model on the training data. Following Ammar et al. (2013),
we set the number of data-driven clusters to 600. In preliminary experiments we also tested a 256-cluster
setting, but 600 yielded better BLEU scores. For time reasons, we train the clusters on a subset of the
4
http://www.statmt.org/wmt13/wiki-titles.ru-en.tar.gz
5
For this second series of experiments we use the feature weights tuned for the corresponding 5-gram LMs.
1922
LM type smoothing vocab. PP
words Kneser-Ney 2.7M 270
Brown clusters Witten-Bell 600 588
suffixes Witten-Bell 968 2455
suffix/word hybrid (?=5000) Witten-Bell 8530 460
Linear interp.
PP
generic ? class-spec.??s
words + clusters 225 224
words + suffixes 266 265
words + hybrid 243 247
Table 2: Intrinsic evaluation of various types of LMs and their linear interpolations. Perplexity (PP) is
computed on a separate held-out set of 5K Russian sentences. All models are 5-grams.
monolingual data including all the parallel data (news commentary) and the large commoncrawl corpus
for a total of 1M sentences (22M tokens). We then map all monolingual data to the learned clusters and
use that to train all our cluster-based LMs.
For the suffix-based class LMs we closely follow the setup of M?uller et al. (2012) with the only
difference that we use the Russian Snowball stemmer
6
to segment the vocabulary instead of frequency-
based suffixes. The suffix threshold ? (see Section 2.3) is determined by minimizing perplexity on a
separate held-out set (5K sentences): ?=5000 is the optimal setting among {2000, 5000, 10000, 20000}.
7
The same held-out set is used to estimate both the generic and the class-specific lambdas for the linear
interpolation experiments.
Table 2 presents an overview of the LMs used in our experiments. We can see on the left side that
all class-based LMs have notably higher perplexities compared to the word-level, with the fully suffix-
based LM performing worst by far. Nevertheless, all class-based models yield a decrease in perplexity
when they are interpolated with the word-level model (right side). The best improvement is achieved
by the data-driven classes (225 versus 270, that is -17%), but the result of the hybrid LM is also quite
successful (-10%) and much in line with the improvements reported by M?uller et al. (2012) on other
Slavic languages. Because the fully suffix-based LM yields only a modest reduction, we do not to include
it in the SMT evaluation. The right side of Table 2 also shows that using class-specific interpolation
weights is not significantly better, and sometimes is even worse than using only one generic ?, at least
from the point of view of perplexity. Since weight estimation for linear interpolation is still an open
problem for SMT, we decide nevertheless to compare these two interpolation methods in our translation
experiments (see Table 4).
4.3 SMT results
Table 3 shows the results for English to Russian translation using log-linear combination with Brown
clusters and the hybrid suffix/word classes. Translation quality is measured by case-insensitive BLEU
(Papineni et al., 2002) on newstest13 using one reference translation. The relative improvements of the
different class-based LM runs are with respect to the baseline which uses a word-based LM only and
achieves comparable results to the state-of-the-art. We use approximate randomization (Noreen, 1989)
to test for statistically significant differences between runs (Riezler and Maxwell, 2005).
We can see from Table 2(a) that using a stream-based LM as an additional feature, which is log-linearly
interpolated with the other decoder features during parameter estimation, leads to small but statistically
significant improvements. The results also indicate that using a higher n-gram class model (7-gram)
does not yield additional improvements over a 5-gram class model, which is in contrast with the results
reported by Wuebker et al. (2013) on a French-German task.
Since the stream-based models ignore word emission probabilities, one would expect further improve-
ments from the theoretically more correct class-based model which include word emission probabilities
(see Equation 1). Somewhat surprisingly, this is not the case. On the contrary, both 5- and 7-gram
class-based models perform slightly worse than the stream-based models. We suspect that this is due to
the limited context used to estimate the emission probabilities in the original Brown class-based mod-
els. To verify this we compared this to the fullibm model (Equation 3) which conditions word emission
6
http://snowball.tartarus.org/algorithms/russian/stemmer.html
7
Our training corpus is considerably larger than those used by M?uller et al. (2012), therefore we search among higher values.
1923
(a) Brown clusters (600)
surface stem
Additional LM BLEU ? BLEU ?
? none [baseline] 18.8 ? 24.7 ?
? 5g stream-based 19.1 +0.3
?
24.8 +0.1
7g stream-based 19.1 +0.3
?
24.9 +0.2
? 5g class-based 18.9 +0.1 24.6 ?0.1
7g class-based 18.8 ?0.0 24.7 ?0.0
5g fullibm 19.4 +0.6
?
25.0 +0.3
?
7g fullibm 19.3 +0.5
?
25.0 +0.3
?
(b) Suffixes/words, ? = 5000
surface stem
Additional LM BLEU ? BLEU ?
? none [baseline] 18.8 ? 24.7 ?
? 5g stream-based 18.9 +0.1 24.6 ?0.1
7g stream-based 18.9 +0.1 24.6 ?0.1
? 5g class-based 19.0 +0.2
?
24.8 +0.1
7g class-based 19.1 +0.3
?
24.7 ?0.0
5g fullibm 19.1 +0.3
?
24.8 +0.1
7g fullibm 19.2 +0.4
?
24.9 +0.2
?
Table 3: SMT translation quality on newstest13 when using different kinds of class-based language mod-
els as additional features in the log-linear combination. The settings used for weight tuning are marked
with ?. Statistically significant differences wrt the baseline are marked with
?
at the p ? .01 level and
?
at the p ? .05 level.
probabilities on the entire n-gram class history of length n ? 1. The fullibm class-based models yield
the biggest statistically significant improvements over the baseline and also compare favorably to the
stream-based and original class-based models. Similarly to stream- and class-based models we do not
observe a difference in performance between 5- and 7-gram models for fullibm.
Table 2(b) shows the results obtained by the shallow morphology-based classes inspired by M?uller
et al. (2012). This form of classes is easy to implement in many languages and computationally much
cheaper than the Brown clusters. Although less than the data-driven class models, the hybrid suffix/word
models also appear to improve translation quality. We can see that fullibm again yields the highest
improvements, but we can also observe more consistent trends where longer n-grams help and class-
based models are preferable to stream-based models without emission probabilities.
When translating into a morphologically rich language, such as Russian, the role of the target lan-
guage model is two-fold. On the one hand, it helps choose the correct meaning from the available phrase
translation candidates, on the other hand, it helps choose the correct surface realization of the trans-
lation candidate that agrees grammatically with the previous target context. For morphologically rich
languages the second aspect plays a considerably larger role than for morphologically poor languages.
To disentangle these two roles of the language model we also evaluated the different language models
with respect to stem-based information only, stripping off any inflectional information using the Snow-
ball stemmer. These results are also reported in Table 3 and in general exhibit the same trend as the
surface-based BLEU scores. Again, fullibm performs best, and the original class-based LMs do not lead
to any improvements over the baseline. As a general observation, we find that the surface-level gains
are most of the time larger than the stem-level ones, which suggests that the additional LMs are mainly
improving the choice of word inflections.
All systems compared in Table 3 use a class language model as an additional feature, which is log-
linearly interpolated with the other decoder features. Alternatively, the word- and the class-based lan-
(a) Brown clusters (600)
surface stem
Additional LM BLEU ? BLEU ?
? none [baseline] 18.8 ? 24.7 ?
? 5g class, log-linear comb. 18.9 +0.1 24.6 ?0.1
? 5g class, linear (global ?) 18.5 ?0.3 24.4 ?0.3
5g class, linear (class ??s) 18.6 ?0.2 24.5 ?0.2
(b) Suffixes/words, ? = 5000
surface stem
Additional LM BLEU ? BLEU ?
? none [baseline] 18.8 ? 24.7 ?
? 5g class, log-linear comb. 19.0 +0.2
?
24.8 +0.1
? 5g class, linear (global ?) 18.9 +0.1 24.8 +0.1
5g class, linear (class ??s) 18.6 ?0.1 24.6 ?0.1
Table 4: SMT translation quality on newstest13 when using different LM combining frameworks: ad-
ditional feature in the log-linear combination or linear interpolation with perplexity-tuned weights (one
global lambda or class-specific lambdas).
1924
guage models may be linearly interpolated with weights determined by maximizing the likelihood of a
held-out monolingual data set (see Section 3.2). While linear interpolation often outperforms log-linear
interpolation for combining language models for domain adaptation (Foster and Kuhn, 2007), this does
not seem to be the case for language models for morphologically rich target languages. The results
presented in Table 4 consistently show that linear interpolation under-performs log-linear combination
under all conditions. Even using class-specific interpolation weights as suggested by M?uller et al. (2012)
did not lead to any further improvements.
5 Conclusion
We have presented the first systematic comparison of different forms of class-based LMs and different
class LM combination methods in the context of SMT into a morphologically rich language.
First of all, our results have shown that careful modeling of class-to-word emission probabilities?
often omitted from the models used in SMT?is actually important for improving translation quality.
In particular, we have achieved best results when using a refined variant of the original class-based
LM, called fullibm, which had never been tested for SMT but only for speech recognition (Goodman,
2001). Secondly, we have found that a rather simple LM based on shallow morphology-based classes
can get close, in terms of BLEU, to the performance of more computationally expensive data-driven
classes. Although the reported improvements are modest, they are statistically significant and obtained
in a competitive large-data scenario against a state-of-the-art baseline.
On the downside, and somewhat in contrast with previous findings in domain adaptation, we have
observed that linear interpolation of word- and class-based LMs with perplexity-tuned weights performs
worse than the log-linear combination of models with model-level weights globally tuned for translation
quality. This result was confirmed also when using class-specific lambdas as suggested by M?uller et al.
(2012).
Indeed, modeling morphologically rich languages remains a challenging problem for SMT but, with
our evaluation, we have contributed to assess how far existing language modeling techniques may go
in this direction. Natural extensions of this work include combining multiple LMs based on different,
and possibly complementary, kinds of classes such as data-driven and suffix-based, or using supervised
morphological analyzers instead of a simple stemmer. In a broader perspective, we believe that future re-
search should question the fundamental constraints of n-gram modeling and develop innovative modeling
techniques that conform to the specific requirements of translating into morphologically rich languages.
Acknowledgments
This research was funded in part by the Netherlands Organisation for Scientific Research (NWO) under
project numbers 639.022.213 and 612.001.218. We kindly thank Thomas M?uller for providing code and
support for the weight optimization of linearly interpolated models.
References
Waleed Ammar, Victor Chahuneau, Michael Denkowski, Greg Hanneman, Wang Ling, Austin Matthews, Kenton
Murray, Nicola Segall, Alon Lavie, and Chris Dyer. 2013. The CMU machine translation systems at WMT
2013: Syntax, synthetic translation options, and pseudo-references. In Proceedings of the Eighth Workshop on
Statistical Machine Translation, pages 70?77, Sofia, Bulgaria, August. Association for Computational Linguis-
tics.
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, Robert L. Mercer, and David Nahamoo. 1991. A fast algorithm
for deleted interpolation. In Eurospeech. ISCA.
Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007. CCG supertags in factored statistical machine trans-
lation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 9?16, Prague, Czech
Republic, June. Association for Computational Linguistics.
Arianna Bisazza and Marcello Federico. 2012. Cutting the long tail: Hybrid language models for translation style
adaptation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computa-
tional Linguistics, pages 439?448, Avignon, France, April. Association for Computational Linguistics.
1925
Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44, Sofia,
Bulgaria, August. Association for Computational Linguistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?479.
Simon Carter and Christof Monz. 2011. Syntactic discriminative language model rerankers for statistical machine
translation. Machine Translation, 25(4):317?339.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into morphologically rich
languages with synthetic phrases. In Proceedings of the 2013 Conference on Empirical Methods in Natural
Language Processing, pages 1677?1687, Seattle, Washington, USA, October. Association for Computational
Linguistics.
Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling.
Computer Speech and Language, 4(13):359?393.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012. On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages
200?209, Montr?eal, Canada, June. Association for Computational Linguistics.
Nadir Durrani, Philipp Koehn, Helmut Schmid, and Alexander Fraser. 2014. Investigating the usefulness of gen-
eralized word representations in SMT. In Proceedings of the 25th International Conference on Computational
Linguistics (COLING), Dublin, Ireland, August.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and Noah A. Smith. 2011. The CMU-ARK German-English
Translation System. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 337?343,
Edinburgh, Scotland, July. Association for Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-model adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages 128?135, Prague, Czech Republic, June. Association for
Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model.
In EMNLP ?08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
848?856, Morristown, NJ, USA. Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2009. Quadratic-time dependency parsing for machine translation.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP, pages 773?781, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Joshua T. Goodman. 2001. A bit of progress in language modeling. Computer Speech & Language, 15(4):403?
434.
Spence Green and John DeNero. 2012. A class-based agreement model for generating accurately inflected trans-
lations. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL ?12,
pages 146?155, Stroudsburg, PA, USA. Association for Computational Linguistics.
Sa?sa Hasan, Oliver Bender, and Hermann Ney. 2006. Reranking translation hypotheses using structural prop-
erties. In Proceedings of the EACL?06 Workshop on Learning Structured Information in Natural Language
Applications, pages 41?48, Trento, Italy, April.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Supertagged phrase-based statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288?295,
Prague, Czech Republic, June. Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP?11).
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of
HLT-NAACL 2003, pages 127?133, Edmonton, Canada.
1926
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th
Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech Republic. Association for Computational Linguistics.
Abby Levenberg and Miles Osborne. 2009. Stream-based randomised language models for smt. In Proceedings
of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP
?09, pages 756?764, Stroudsburg, PA, USA. Association for Computational Linguistics.
Christof Monz. 2011. Statistical Machine Translation with Local Language Models. In Proceedings of the 2011
Conference on Empirical Methods in Natural Language Processing, pages 869?879, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Thomas M?uller, Hinrich Sch?utze, and Helmut Schmid. 2012. A comparative investigation of morphological
language modeling for the languages of the European Union. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 386?395, Montr?eal, Canada, June. Association for Computational Linguistics.
Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. An Introduction. Wiley-Interscience.
Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In Proceedings of the 9th
Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 71?76.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Erhard Hinrichs and Dan
Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th Annual Meeting of the Association of Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA.
Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 57?64, Ann Arbor, Michigan, June. Association for Computational
Linguistics.
Hinrich Sch?utze and Michael Walsh. 2011. Half-context language models. Comput. Linguist., 37(4):843?865,
December.
Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental syntactic language
models for phrase-based translation. In Proceedings of the 49th Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies, pages 620?631, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language
modeling in machine translation. In Proceedings of ACL-08: HLT, pages 755?762, Columbus, Ohio, June.
Association for Computational Linguistics.
Philip Williams and Philipp Koehn. 2011. Agreement constraints for statistical machine translation into german.
In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 217?226, Edinburgh, Scotland,
July. Association for Computational Linguistics.
Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel
events in adaptive text compression. IEEE Trans. Inform. Theory, IT-37(4):1085?1094.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Hermann Ney. 2013. Improving statistical machine translation
with word class models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1377?1381, Seattle, Washington, USA, October. Association for Computational Linguistics.
Ali Yazgan and Murat Sarac?lar. 2004. Hybrid language models for out of vocabulary word detection in large
vocabulary conversational speech recognition. In Proceedings of ICASSP, volume 1, pages I ? 745?8 vol.1,
may.
1927
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 869?879,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Statistical Machine Translation with Local Language Models
Christof Monz
Informatics Institute, University of Amsterdam
P.O. Box 94323, 1090 GH Amsterdam, The Netherlands
c.monz@uva.nl
Abstract
Part-of-speech language modeling is com-
monly used as a component in statistical ma-
chine translation systems, but there is mixed
evidence that its usage leads to significant im-
provements. We argue that its limited effec-
tiveness is due to the lack of lexicalization.
We introduce a new approach that builds a
separate local language model for each word
and part-of-speech pair. The resulting mod-
els lead to more context-sensitive probabil-
ity distributions and we also exploit the fact
that different local models are used to esti-
mate the language model probability of each
word during decoding. Our approach is evalu-
ated for Arabic- and Chinese-to-English trans-
lation. We show that it leads to statistically
significant improvements for multiple test sets
and also across different genres, when com-
pared against a competitive baseline and a sys-
tem using a part-of-speech model.
1 Introduction
Language models are an important component of
current statistical machine translation systems. They
affect the selection of phrase translation candidates
and reordering choices by estimating the probability
that an application of a phrase translation is a flu-
ent continuation of the current translation hypoth-
esis. The size and domain of the language model
can have a significant impact on translation quality.
Brants et al (2007) have shown that each doubling
of the training data from the news domain (used to
build the language model), leads to improvements of
approximately 0.5 BLEU points. On the other hand,
each doubling using general web data leads to im-
provements of approximately 0.15 BLEU points.
While large n-gram language models do lead
to improved translation quality, they still lack any
generalization beyond the surface forms (Schwenk,
2007). Consider example (1), which is a short sen-
tence fragment from the MT09 Arabic-English test
set, with the corresponding machine translation out-
put (1.b), from a phrase-based statistical machine
translation system, and reference translation (1.c).
(1) a. ?Yj. ?? ?Q
J? ?J
 	?Am? HAmProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1676?1688,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Word Translation Prediction for Morphologically Rich Languages
with Bilingual Neural Networks
Ke Tran Arianna Bisazza Christof Monz
Informatics Institute, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{m.k.tran,a.bisazza,c.monz}@uva.nl
Abstract
Translating into morphologically rich lan-
guages is a particularly difficult problem
in machine translation due to the high de-
gree of inflectional ambiguity in the tar-
get language, often only poorly captured
by existing word translation models. We
present a general approach that exploits
source-side contexts of foreign words to
improve translation prediction accuracy.
Our approach is based on a probabilistic
neural network which does not require lin-
guistic annotation nor manual feature en-
gineering. We report significant improve-
ments in word translation prediction accu-
racy for three morphologically rich target
languages. In addition, preliminary results
for integrating our approach into a large-
scale English-Russian statistical machine
translation system show small but statisti-
cally significant improvements in transla-
tion quality.
1 Introduction
The ability to make context-sensitive translation
decisions is one of the major strengths of phrase-
based SMT (PSMT). However, the way PSMT ex-
ploits source-language context has several limita-
tions as pointed out, for instance, by Quirk and
Menezes (2006) and Durrani et al. (2013). First,
the amount of context used to translate a given
input word depends on the phrase segmentation,
with hypotheses resulting from different segmen-
tations competing with one another. Another issue
is that, given a phrase segmentation, each source
phrase is translated independently from the oth-
ers, which can be problematic especially for short
phrases. As a result, the predictive translation of
a source phrase does not access useful linguistic
clues in the source sentence that are outside of the
scope of the phrase.
Lexical weighting tackles the problem of un-
reliable phrase probabilities, typically associated
with long phrases, but does not alleviate the prob-
lem of context segmentation. An important share
of the translation selection task is then left to the
language model (LM), which is certainly very ef-
fective but can only leverage target language con-
text. Moreover, decisions that are taken at early
decoding stages?such as the common practice
of retaining only top n translation options for
each source span?depend only on the translation
models and on the target context available in the
phrase.
Source context based translation models (Gim-
pel and Smith, 2008; Mauser et al., 2009; Jeong
et al., 2010; Haque et al., 2011) naturally ad-
dress these limitations. These models can ex-
ploit a boundless context of the input text, but
they assume that target words can be predicted in-
dependently from each other, which makes them
easy to integrate into state-of-the-art PSMT sys-
tems. Even though the independence assump-
tion is made on the target side, these models have
shown the benefits of utilizing source context, es-
pecially in translating into morphologically rich
languages. One drawback of previous research
on this topic, though, is that it relied on rich
sets of manually designed features, which in turn
required the availability of linguistic annotation
tools like POS taggers and syntactic parsers.
In this paper, we specifically focus on im-
proving the prediction accuracy for word transla-
tions. Achieving high levels of word translation
accuracy is particularly challenging for language
1676
pairs where the source language is morphologi-
cally poor, such as English, and the target lan-
guage is morphologically rich, such as Russian,
i.e., language pairs with a high degree of surface
realization ambiguity (Minkov et al., 2007). To
address this problem we propose a general ap-
proach based on bilingual neural networks (BNN)
exploiting source-side contextual information.
This paper makes a number of contributions:
Unlike previous approaches our models do not re-
quire any form of linguistic annotation (Minkov
et al., 2007; Kholy and Habash, 2012; Chahuneau
et al., 2013), nor do they require any feature en-
gineering (Gimpel and Smith, 2008). Moreover,
besides directly predicting fully inflected forms
as Jeong et al. (2010), our approach can also
model stem and suffix prediction explicitly. Pre-
diction accuracy is evaluated with respect to three
morphologically rich target languages (Bulgarian,
Czech, and Russian) showing that our approach
consistently yields substantial improvements over
a competitive baseline. We also show that these
improvements in prediction accuracy can be ben-
eficial in an end-to-end machine translation sce-
nario by integrating into a large-scale English-
Russian PSMT system. Finally, a detailed analysis
shows that our approach induces a positive bias on
phrase translation probabilities leading to a better
ranking of the translation options employed by the
decoder.
2 Lexical coverage of SMT models
The first question we ask is whether translation
can be improved by a more accurate selection of
the translation options already existing in the SMT
models, as opposed to generating new options.
To answer this question we measure the lexical
coverage of a baseline PSMT system trained on
English-Russian.
1
We choose this language pair
because of the morphological richness on the tar-
get side: Russian is characterized by a highly in-
flectional morphology with a particularly complex
nominal declension (six core cases, three genders
and two number categories). As suggested by
Green and DeNero (2012), we compute the re-
call of reference tokens in the set of target to-
kens that the decoder could produce in a trans-
lation of the source, that is the target tokens of
all phrase pairs that matched the input sentence
1
Training data and SMT setup are described in Section 6.
and that were actually used for decoding.
2
We
call this the decoder?s lexical search space. Then,
we compare the reference/space recall against the
reference/MT-output recall: that is, the percent-
age of reference tokens that also appeared in the
1-best translation output by the SMT system. Re-
sults for the WMT12 benchmark are presented in
Table 1. From the first two rows, we see that only a
rather small part of the correct target tokens avail-
able to the decoder are actually produced in the
1-best MT output (50% against 86%). Although
our word-level analysis does not directly estimate
phrase-level coverage, these numbers suggest that
a large potential for translation improvement lies
in better lexical selection during decoding.
Token recall:
reference/MT-search-space 86.0%
reference/MT-output 50.0%
stem-only reference/MT-output 12.3%
of which reachable 11.2%
Table 1: Lexical coverage analysis of the baseline
SMT system (English-Russian wmt12).
To quantify the importance of morphology, we
count how many reference tokens matched the
MT output only at the stem level
3
and for how
many of those the correct surface form existed
in the search space (reachable matches). These
two numbers represent the upper bound of the im-
provement achievable by a model only predicting
suffixes given the target stems. As shown in Ta-
ble 1, such a model could potentially increase the
reference/MT-output recall by 12.3% with genera-
tion of new inflected forms, and by 11.2% without.
Thus, also when it comes to morphology, gener-
ation seems to be of secondary importance com-
pared to better selection in our experimental setup.
3 Predicting word translations in context
It is standard practice in PSMT to use word-
to-word translation probabilities as an additional
phrase score. More specifically, state-of-the-art
PSMT systems employ the maximum-likelihood
estimate of the context-independent probability
of a target word given its aligned source word
P (t
j
|s
i
) for each word alignment link a
ij
.
2
This corresponds to the top 30 phrases sorted by
weighted phrase, lexical and LM probabilities, for each
source span. Koehn (2004) and our own experience suggest
that using more phrases has little or no impact on MT quality.
3
Word segmentation for this analysis is performed by the
Russian Snowball stemmer, see also Section 5.3.
1677
[?????????????????] [??????? ?????]
constitutionality of the] [indiana law] [.]
[.]
[the
Figure 1: Fragment of English sentence and its in-
correct Russian translation produced by the base-
line SMT system. Square brackets indicate phrase
boundaries.
The main goal of our work is to improve the
estimation of such probabilities by exploiting the
context of s
i
, which in turn we expect will re-
sult in better phrase translation selection. Figure
1 illustrates this idea: the translation of ?law? in
this example has a wrong case?nominative in-
stead of genitive. Due to the rare word ?Indi-
ana/????????, the target LM must backoff to the
bigram history and does not penalize this choice
sufficiently. However, a model that has access to
the word ?of? in the near source context could bias
the translation of ?law? to the correct case.
We then model P (t
j
|c
s
i
) with source context
c
s
i
defined as a fixed-length word sequence cen-
tered around s
i
:
c
s
i
= s
i?k
, ..., s
i
, ..., s
i+k
Our definition of context is similar to the n ? 1
word history used in n-gram LMs. Similarly to
previous work in source context-sensitive trans-
lation modeling (Jeong et al., 2010; Chahuneau
et al., 2013), target words are predicted indepen-
dently from each other, which allows for an ef-
ficient decoding integration. We are particularly
interested in translating into morphologically rich
languages where source context can provide useful
information for the prediction of target translation,
for example, the gender of the subject in a source
sentence constrains the morphology of the transla-
tion of the source verb. Therefore, we integrate the
notions of stem and suffix directly into the model.
We assume the availability of a word segmenta-
tion function g that takes a target word t as in-
put and returns its stem and suffix: g(t) = (?, ?).
Then, the conditional probability p(t
j
|c
s
i
) can be
decomposed into stem probability and suffix prob-
ability:
p(t
j
|c
s
i
) = p(?
j
|c
s
i
)p(?
j
|c
s
i
, ?
j
) (1)
These two probabilities can be estimated sepa-
rately, which yields the two subtasks:
1. predict target stem ? given source context c
s
;
2. predict target suffix ? given source context c
s
and target stem ?.
Based on the results of our analysis, we focus
on the selection of existing translation candidates.
We then restrict our prediction on a set of pos-
sible target candidates depending on the task in-
stead of considering all target words in the vocab-
ulary. More specifically, for each source word s
i
,
our candidate generation function returns the set of
target words T
s
= {t
1
, . . . , t
m
} that were aligned
to s
i
in the parallel training corpus, which in turn
corresponds to the set of target words that the SMT
system can produce for a given source. In practice,
we use a pruned version of T
s
to speed up training
and reduce noise (see details in Section 5).
As for the morphological models, given T
s
and
g, we can obtain L
s
= {?
1
, . . . , ?
k
}, the set of
possible target stem translations of s, and M
?
=
{?
1
, . . . , ?
l
}, the set of possible suffixes for a tar-
get stem ?. The use of L
s
, and M
?
is similar to
stemming and inflection operations in (Toutanova
et al., 2008) while the set T
s
is similar to the GEN
function in (Jeong et al., 2010).
4
Our approach differs crucially from previous
work (Minkov et al., 2007; Chahuneau et al.,
2013) in that it does not require linguistic fea-
tures such as part-of-speech and syntactic tree on
the source side. The proposed models automati-
cally learn features that are relevant for each of the
modeled tasks, directly from word-aligned data.
To make the approach completely language inde-
pendent, the word segmentation function g can be
trained with an unsupervised segmentation tool.
The effects of using different word segmentation
techniques are discussed in Section 5.
4 Bilingual neural networks for
translation prediction
Probabilistic neural network (NN), or continuous
space, language models have received increas-
ing attention over the last few years and have
been applied to several natural language process-
ing tasks (Bengio et al., 2003; Collobert and We-
ston, 2008; Socher et al., 2011; Socher et al.,
2012). Within statistical machine translation, they
4
Note that our suffix generation function M
?
is restricted
to the forms observed in the target monolingual data, but not
to those aligned to a source word s, which opens the possi-
bility of generating inflected forms that are missing from the
translation models. We leave this possibility to future work.
1678
have been used for monolingual target language
modeling (Schwenk et al., 2006; Le et al., 2011;
Duh et al., 2013; Vaswani et al., 2013), n-gram
translation modeling (Son et al., 2012), phrase
translation modeling (Schwenk, 2012; Zou et al.,
2013; Gao et al., 2014) and minimal translation
modeling (Hu et al., 2014). The recurrent neural
network LMs of Auli et al. (2013) are primarily
trained to predict target word sequences. However,
they also experiment with an additional input layer
representing source side context.
Our models differ from most previous work in
neural language modeling in that we predict a tar-
get translation given a source context while pre-
vious models predict the next word given a tar-
get word history. Unlike previous work in phrase
translation modeling with NNs, our models have
the advantage of accessing source context that can
fall outside the phrase boundaries.
We now describe our models in a general set-
ting, predicting target translations given a source
context, where target translations can be either
words, stems or suffixes.
5
4.1 Neural network architecture
Following a common approach in deep learning
for NLP (Bengio et al., 2003; Collobert and We-
ston, 2008), we represent each source word s
i
by
a column vector r
s
i
? R
d
. Given a source con-
text c
s
i
= s
i?k
, ..., s
i
, ..., s
i+k
of k words on the
left and k words on the right of s
i
, the context rep-
resentation r
c
s
i
? R
(2k+1)?d
is obtained by con-
catenating the vector representations of all words
in c
s
i
:
r
c
s
i
= r
s
i?k
 ... r
s
i+k
Our main BNN architecture for word or stem
prediction (Figure 2a) is a feed-forward neural
network (FFNN) with one hidden layer, a matrix
W
1
? R
n?(2k+1)d
connecting the input layer to
the hidden layer, a matrix W
2
? R
|V
t
|?n
connect-
ing the hidden layer to the output layer, and a bias
vector b
2
? R
|V
t
|
where |V
t
| is the size of target
translations vocabulary. The target translation dis-
tribution P
BNN
(t|c
s
i
) for a given source context
c
s
i
is computed by a forward pass:
softmax
(
W
2
?(W
1
r
c
s
i
) + b
2
)
(2)
where ? is a nonlinearity (tanh, sigmoid or rec-
tified linear units). The parameters of the neural
5
The source code of our models is available at https:
//bitbucket.org/ketran
network are ? = {r
s
i
,W
1
,W
2
,b
2
}.
The suffix prediction BNN is obtained by
adding the target stem representation r
?
to the in-
put layer (see Figure 2b).
4.2 Model variants
We encounter two major issues with FFNNs: (i)
They do not provide a natural mechanism to com-
pute word surface conditional probability p(t|c
s
)
given individual stem probability p(?|c
s
) and suf-
fix probability p(?|c
s
, ?), and (ii) FFNNs do not
provide the flexibility to capture long dependen-
cies among words if they lie outside the source
context window. Hence, we consider two BNN
variants: a log-bilinear model (LBL) and a con-
volutional neural network model (ConvNet). LBL
could potentially address (i) by factorizing target
representations into target stem and suffix repre-
sentations whereas ConvNets offer the advantage
of modeling variable input length (ii) (Kalchbren-
ner et al., 2014).
Log-bilinear model. The FFNN models stem
and suffix probabilities separately. A log-bilinear
model instead could directly model word predic-
tion through a factored representation of target
words, similarly to Botha and Blunsom (2014).
Thus, no probability mass would be wasted over
stem-suffix combinations that are not in the candi-
date generation function. The LBL model speci-
fies the conditional distribution for the word trans-
lation t
j
? T
s
i
given a source context c
s
i
:
P?(tj |cs
i
) =
exp(s?(tj , cs
i
))
?
t
?
j
?T
s
i
exp(s?(t
?
j
, c
s
i
))
(3)
We use an additional set of word representations
q
t
j
? R
n
for target translations t
j
. The LBL
model computes a predictive representation q? of a
source context c
s
i
by taking a linear combination
of the source word representations r
s
i+m
with the
position-dependent weight matrices C
m
? R
n?d
:
q? =
k
?
m=?k
C
m
r
s
i+m
(4)
The score function s?(tj , cs
i
) measures the simi-
larity between the predictive representation q? and
the target representation q
t
j
:
s?(tj , cs
i
) = q?
T
q
t
j
+ b
T
h
q
t
j
+ b
t
j
(5)
1679
P?(t|csi)
rsi k rsi rsi+k
W1
W2
 (x)
(a) BNN for word prediction.
P?(?| , csi)
rsi k rsi rsi+k
W1
W2
 (x)
r 
(b) BNN for suffix prediction.
Figure 2: Feed-forward BNN architectures for predicting target translations: (a) word model (similar to
stem model), and (b) suffix model with an additional vector representation r
?
for target stems ?.
Here b
t
j
is the bias term associated with target
word t
j
. b
h
? R
n
are the representation bi-
ases. s?(tj , cs
i
) can be seen as the negative en-
ergy function of the target translation t
j
and its
context c
s
i
. The parameters of the model thus
are ? = {r
s
i
,C
m
,q
t
j
,b
h
, b
t
j
}. Our log-bilinear
model is a modification of the log-bilinear model
proposed for n-gram language modeling in (Mnih
and Hinton, 2007).
Convolutional neural network model. This
model (Figure 3) computes the predictive repre-
sentation q? by applying a sequence of 2k convo-
lutional layers {L
1
, . . . ,L
2k
}. The source context
c
s
i
is represented as a matrix m
c
s
i
? R
d?(2k+1)
:
m
c
s
i
=
[
r
s
i?k
; . . . ; r
s
i+k
]
(6)
q?
rs1 rs2 rs3 rs4 rs5 rs6rs0
Figure 3: Convolutional neural network model.
Edges with the same color indicate the same ker-
nel weight matrix.
Each convolutional layer L
i
consists of a one-
dimensional filter m
i
? R
d?2
. Each row of m
i
is convolved with the corresponding row in the
previous layer resulting in a weight matrix whose
number of columns decreases by one. Thus after
2k convolutional layers, the network transforms
the source context matrix m
c
s
i
to a feature vec-
tor q? ? R
d
. A fully connected layer with weight
matrix W followed by a softmax layer are placed
after the last convolutional layer L
2k
to perform
classification. The parameters of the convolutional
neural network model are ? = {r
s
i
,m
j
,W}.
Here, we focus on a fixed length input, how-
ever convolutional neural networks may be used to
model variable length input (Kalchbrenner et al.,
2014; Kalchbrenner and Blunsom, 2013).
4.3 Training
In training, for each example (t, c
s
), we maximize
the conditional probability P?(t|cs) of a correct
target label t. The contribution of the training ex-
ample (t, c
s
) to the gradient of the log conditional
probability is given by:
?
??
logP?(t|cs) =
?
??
s?(t|cs)
?
?
t
?
?T
s
P?(t
?
|c
s
)
?
??
s?(t
?
, c
s
)
Note that in the gradient, we do not sum over all
target translations T but a set of possible candi-
dates T
s
of a source word s. In practice |T
s
| ? 200
with our pruning settings (see Section 5.1), thus
training time for one example does not depend on
the vocabulary size. Our training criterion can be
seen as a form of contrastive estimation (Smith
and Eisner, 2005), however we explicitly move the
probability mass from competing candidates to the
correct translation candidate, thus obtaining more
reliable estimates of the conditional probabilities.
The BNN parameters are initialized randomly
according to a zero-mean Gaussian. We regularize
the models with L
2
. As an alternative to the L
2
regularizer, we also experiment with dropout (Hin-
ton et al., 2012), where the neurons are randomly
zeroed out with dropout rate p. This technique is
known to be useful in computer vision tasks but
has been rarely used in NLP tasks. In FFNN, we
use dropout after the hidden layer, while in Con-
vNet, dropout applies after the last convolutional
layer. The dropout rate p is set to 0.3 in our exper-
1680
iments. We use rectified nonlinearities
6
in FFNN
and after each convolutional layer in ConvNet. We
train our BNN models with the standard stochastic
gradient descent.
5 Evaluating word translation prediction
In this section, we assess the ability of our BNN
models to predict the correct translation of a word
in context. In addition to English-Russian, we also
consider translation prediction for Czech and Bul-
garian. As members of the Slavic language fam-
ily, Czech and Bulgarian are also characterized by
highly inflectional morphology. Czech, like Rus-
sian, displays a very rich nominal inflection with
as many as 14 declension paradigms. Bulgarian,
unlike Russian, is not affected by case distinctions
but is characterized by a definiteness suffix.
5.1 Experimental setup
The following parallel corpora are used to train the
BNN models:
? English-Russian: WMT13 data (News Com-
mentary and Yandex corpora);
? English-Czech: CzEng 1.0 corpus (Bojar et
al., 2012) (Web Pages and News sections);
? English-Bulgarian: a mix of crawled news
data, TED talks and Europarl proceedings.
Detailed corpus statistics are given in Table 2. For
each language pair, accuracies are measured on a
held-out set of 10K parallel sentences.
To prepare the candidate generation function,
each dataset is first word-aligned with GIZA++,
then a bilingual lexicon with maximum-likelihood
probabilities (P
mle
) is built from the symmetrized
alignment. After some frequency and signifi-
cance pruning,
7
the top 200 translations sorted by
P
mle
(t|s) ? P
mle
(s|t) are kept as candidate word
translations for each source word in the vocabu-
lary. Word alignments are also used to train the
BNN models: each alignment link constitutes a
training sample, with no special treatment of un-
aligned words and 1-to-many alignments.
The context window size k is set to 3 (cor-
responding to 7-gram) and the dimensionality of
6
We find that using rectified linear units gives better re-
sults than sigmoid and tanh.
7
Each lexicon is pruned with minimum word frequency 5,
minimum source-target word pair frequency 2, minimum log
odds ratio 10.
source word representations to 100 in all experi-
ments. The number of hidden units in our feed-
forward neural networks and the target translation
embedding size in LBL models are set to 200. All
models are trained for 10 iterations with learning
rate set to 0.001.
En-Ru En-Cs En-Bg
Sentences 1M 1M 0.8M
Src. tokens 26.5M 19.2M 19.3M
Trg. tokens 24.7M 16.7M 18.9M
Src. T/T .0109 .0105 .0051
Trg. T/T .0247 .0163 .0104
Table 2: BNN training corpora statistics: number
of sentences, tokens, and type/token ratio (T/T).
5.2 Word, stem and suffix prediction
accuracy
We measure accuracy at top-n, i. e. the number
of times the correct translation was in the top n
candidates sorted by a model. For each subtask?
word, stem and suffix prediction?the BNN
model is compared to the context-independent
maximum-likelihood baseline P
mle
(t|s) on which
the PSMT lexical weighting score is based. Note
that this is a more realistic baseline than the uni-
form models sometimes reported in the litera-
ture. The oracle corresponds to the percentage of
aligned source-target word pairs in the held-out set
that are covered by the candidate generation func-
tion. Out of the missing links, about 4% is due
to lexicon pruning. Results for all three language
pairs are presented in Table 3. In this series of
experiments, the morphological BNNs utilize un-
supervised segmentation models trained on each
target language following Lee et al. (2011).
8
As shown in Table 3, the BNN models outper-
form the baseline by a large margin in all tasks and
languages. In particular, word prediction accuracy
at top-1 increases by +6.4%, +24.6% and +9.0%
absolute in English-Russian, English-Czech and
English-Bulgarian respectively, without the use of
any features based on linguistic annotation. While
the baseline and oracle differences among lan-
guages can be explained by different levels of
overlap between training and held-out set, we can-
not easily explain why the Czech BNN perfor-
mance is so much higher. When comparing the
8
We use the C++ implementation available at http://
groups.csail.mit.edu/rbg/code/morphsyn
1681
Model En-Ru En-Cs En-Bg
Word prediction (%):
Baseline 33.0 / 50.1 42.0 / 59.9 47.9 / 66.0
Word BNN
39.4 / 56.6 66.6 / 81.4 56.9 / 74.0
+6.4 / +6.5 +24.6/+21.5 +9.0 / +8.0
Oracle 79.5 / 0.00 90.2 / 0.00 86.9 / 0.00
Stem prediction (%):
Baseline 40.7 / 58.2 46.1 / 64.3 51.9 / 70.1
Stem BNN
45.1 / 62.5 66.1 / 81.6 56.7 / 74.4
+4.4 / +4.3 +20.0/+17.3 +4.8 / +4.3
Suffix prediction (%):
Baseline 71.2 / 85.6 78.8 / 93.2 81.5 / 92.4
Suffix BNN
77.0 / 89.7 91.9 / 97.4 87.7 / 94.9
+5.8 / +4.1 +13.1 /+4.2 +6.2 / +2.5
Table 3: BNN prediction accuracy (top-1/top-3)
compared to a context-independent maximum-
likelihood baseline.
three prediction subtasks, we find that word pre-
diction is the hardest task as expected. Stem pre-
diction accuracies are considerably higher than
word prediction accuracies in Russian, but almost
equal in the other two languages. Finally, base-
line accuracies for suffix prediction are by far
the highest, ranging between 71.2% and 81.5%,
which is primarily explained by a smaller num-
ber of candidates to choose from. Also on this
task, the BNN model achieves considerable gains
of +5.8%, +13.1% and +6.2% at top-1, without the
need of manual feature engineering.
From these figures, it is hard to predict whether
word BNNs or morphological BNNs will have a
better effect on SMT performance. On one hand,
the word-level BNN achieves the highest gain over
the MLE baseline. On the other, the stem- and
suffix-level BNNs provide two separate scoring
functions, whose weights can be directly tuned for
translation quality. A preliminary answer to this
question is given by the SMT experiments pre-
sented in Section 6.
5.3 Effect of word segmentation
This section analyzes the effect of using different
segmentation techniques. We consider two super-
vised tagging methods that produce lemma and in-
flection tag for each token in a context-sensitive
manner: TreeTagger (Sharoff et al., 2008) for Rus-
sian and the Morce tagger (Spoustov? et al., 2007)
for Czech.
9
Finally, we employ the Russian Snow-
ball rule-based stemmer as a light-weight context-
9
Annotation included in the CzEng 1.0 corpus release.
Figure 4: Effect of different word segmentation
techniques (U: unsupervised, S: supervised, R:
rule-based stemmer) on stem and suffix prediction
accuracy. The dark part of each bar stands for top-
1, the light one for top-3 accuracy.
insensitive segmentation technique.
10
As shown in Figure 4, accuracies for both stem
and suffix prediction vary noticeably with the seg-
mentation used. However, higher stem accuracies
corresponds to lower suffix accuracies and vice
versa, which can be mainly due to a general pref-
erence of a tool to segment more or less than an-
other. In summary, the unsupervised segmentation
methods and the light-weight stemmer appear to
perform comparably to the supervised methods.
5.4 Effect of training data size
We examine the predictive power of our models
with respect to the size of training data. Table 4
shows the accuracies of stem and suffix models
trained on 200K and 1M English-Russian sentence
pairs with unsupervised word segmentation. Sur-
prisingly, we observe only a minor loss when we
decrease the training data size, which suggests that
our models are robust even on a small data set.
# Train sent. Stem Acc. Suffix Acc.
1M 45.1 / 62.5 77.0 / 89.7
200K 44.6 / 61.8 75.7 / 88.6
Table 4: Accuracy at top-1/top-3 (%) of stem and
suffix BNNs with different training data sizes.
5.5 Fine-grained evaluation
We evaluate the suffix BNN model at the part-of-
speech (POS) level. Table 5 provides suffix pre-
diction accuracy per POS for En-Ru. For this
analysis, Russian data is segmented by TreeTag-
10
http://snowball.tartarus.org/
algorithms/russian/stemmer.html
1682
ger. Additionally, we report the average number
of suffixes per stem given the part-of-speech.
Our results are consistent with the findings of
Chahuneau et al. (2013):
11
the prediction of ad-
jectives is more difficult than that of other POS
while Russian verb prediction is relatively easier
in spite of the higher number of suffixes per stem.
These differences reflect the importance of source
versus target context features in the prediction of
the target inflection: For instance, adjectives agree
in gender with the nouns they modify, but this may
be only inferred from the target context.
POS A V N M P
Acc. (%) 49.6 61.9 62.8 84.5 64.4
|M
?
| 18.2 18.4 9.2 7.1 13.3
Table 5: Suffix prediction accuracy at top-1 (%),
breakdown by category (A: adjectives, V: verbs,
N: nouns, M: numerals and P: pronouns). |M
?
|
denotes the average number of suffixes per stem.
5.6 Neural Network variants
Table 6 shows the stem and suffix accuracies of
BNN variants on English-Czech. Although none
of the variants outperform our main FFNN archi-
tecture, we observe similar performances by the
LBL on stem prediction, and by the ConvNet on
suffix prediction. This suggests that future work
could exploit their additional flexibilities (see Sec-
tion 4.2) to improve the BNN predictive power.
As for the low suffix accuracy by the LBL, it
can be explained by the absence of nonlinearity
transformation. Nonlinearity is important for the
suffix model where the prediction of target suf-
fix ?
j
often does not depend linearly on s
i
and
?
j
. The predictive representation of target stem
in the LBL stem model, however, mainly depends
on the source representation r
s
i
through a position
dependent weight matrix C
0
. Thus, we observe a
smaller accuracy drop in the stem model than in
the suffix model. Conversely, the ConvNet per-
forms poorly on stem prediction because it cap-
tures the meaning of the whole source context in-
stead of emphasizing the importance of the source
word s
i
as the main predictor of the target transla-
tion t
j
.
11
Chahuneau et al. (2013) report an average accuracy of
63.1% for the prediction of A, V, N, M suffixes. When we
train our model on the same dataset (news-commentary) we
obtain a comparable result (64.7% vs 63.1%).
Unexpectedly, no improvement is obtained by
the use of dropout regularizer (see Section 4.3).
Model Stem Acc Suffix Acc
FFNN 66.1 / 81.6 91.9 / 97.4
FFNN+do 64.6 / 81.1 91.5 / 97.5
LBL 63.6 / 79.6 86.4 / 96.4
ConvNet+do 58.6 / 75.6 90.3 / 96.9
Table 6: Accuracies at top-1/top-3 (%) of stem and
suffix models. +do indicates dropout instead of L
2
regularizer. FFNN is our main architecture.
6 SMT experiments
While the main objective of this paper is to im-
prove prediction accuracy of word translations,
see Section 5, we are also interested in know-
ing to which extent these improvements carry over
within an end-to-end machine translation task. To
this end, we integrate our translation prediction
models described in Section 4 into our existing
English-Russian SMT system.
For each phrase pair matching the input, the
phrase BNN score P
BNN-p
is computed as follows:
P
BNN-p
(s?,
?
t, a) =
|s?|
?
i=1
?
?
?
?
?
1
|{a
i
}|
?
j?{a
i
}
P
BNN
(t
j
|c
s
i
) if |{a
i
}| > 0
P
mle
(NULL|s
i
) otherwise
where a is the word-level alignment of the phrase
pair (s?,
?
t) and {a
i
} is the set of target positions
aligned to s
i
. If a source-target link cannot be
scored by the BNN model, we give it a P
BNN
probability of 1 and increment a separate count
feature ?. Note that the same phrase pair can get
different BNN scores if used in different source
side contexts.
Our baseline is an in-house phrase-based
(Koehn et al., 2003) statistical machine transla-
tion system very similar to Moses (Koehn et al.,
2007). All system runs use hierarchical lexicalized
reordering (Galley and Manning, 2008; Cherry
et al., 2012), distinguishing between monotone,
swap, and discontinuous reordering, all with re-
spect to left-to-right and right-to-left decoding.
Other features include linear distortion, bidirec-
tional lexical weighting (Koehn et al., 2003), word
and phrase penalties, and finally a word-level 5-
gram target LM trained on all available mono-
lingual data with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1999). The distortion
1683
Corpus Lang. #Sent. #Tok.
paral.train
EN
1.9M
48.9M
RU 45.9M
Wiki dict.
EN/RU 508K ?
mono.train
RU 21.0M 390M
WMT2012
EN
3K 64K
WMT2013
3K 56K
Table 7: SMT training and test data statistics. All
numbers refer to tokenized, lowercased data.
limit is set to 6 and for each source phrase the top
30 translation candidates are considered. When
translating into a morphologically rich language,
data sparsity issues in the target language become
particularly apparent. To compensate for this we
also experiment with a 5-gram suffix-based LM in
addition to the surface-based LM (M?ller et al.,
2012; Bisazza and Monz, 2014).
The BNN models are integrated as additional
log-probability feature functions (logP
BNN-p
):
one feature for the word prediction model or two
features for the stem and suffix models respec-
tively, plus the penalty feature ?.
Table 7 shows the data used to train our English-
Russian SMT system. The feature weights for all
approaches were tuned by using pairwise rank-
ing optimization (Hopkins and May, 2011) on the
wmt12 benchmark (Callison-Burch et al., 2012).
During tuning, 14 PRO parameter estimation runs
are performed in parallel on different samples of
the n-best list after each decoder iteration. The
weights of the individual PRO runs are then av-
eraged and passed on to the next decoding itera-
tion. Performing weight estimation independently
for a number of samples corrects for some of the
instability that can be caused by individual sam-
ples. The wmt13 set (Bojar et al., 2013) was used
for testing. We use approximate randomization
(Noreen, 1989) to test for statistically significant
differences between runs (Riezler and Maxwell,
2005).
Translation quality is measured with case-
insensitive BLEU[%] using one reference trans-
lation. As shown in Table 8, statistically signif-
icant improvements over the respective baseline
(Baseline and Base+suffLM) are marked
N
at the
p < .01 level. Integrating our bilingual neural net-
work approach into our SMT system yields small
but statistically significant improvements of 0.4
BLEU over a competitive baseline. We can also
SMT system wmt12 (dev) wmt13 (test)
Baseline 24.7 18.9
+ stem/suff. BNN 25.1 19.3
N
Base+suffLM 24.5 19.2
+ word BNN 24.5 19.3
+ stem/suff. BNN 24.7 19.6
N
Table 8: Effect of our BNN models on English-
Russian translation quality (BLEU[%]).
see that it is beneficial to add a suffix-based lan-
guage model to the baseline system. The biggest
improvement is obtained by combining the suffix-
based language model and our BNN approach,
yielding 0.7 BLEU over a competitive, state-of-
the-art baseline, of which 0.4 BLEU are due to our
BNNs. Finally, one can see that the BNNs mod-
eling stems and suffixes separately perform bet-
ter than a BNN directly predicting fully inflected
forms.
To better understand the BNN effect on the
SMT system, we analyze the set of phrase pairs
that are employed by the decoder to translate each
sentence. This set is ranked by the weighted com-
bination of phrase translation and lexical weight-
ing scores, target language model score and, if
available, phrase BNN scores. As shown in Ta-
ble 9, the morphological BNN models have a pos-
itive effect on the decoder?s lexical search space
increasing the recall of reference tokens among
the top 1 and 3 phrase translation candidates. The
mean reciprocal rank (MRR) also improves from
0.655 to 0.662. Looking at the 1-best SMT output,
we observe a slight increase of reference/output
recall (50.0% to 50.7%), which is less than the in-
crease we observe for the top 1 translation candi-
dates (57.6% to 59.0%). One possible explanation
is that the new, more accurate translation distribu-
tions are overruled by other SMT model scores,
Token recall (wmt12): Baseline +BNN
reference/MT-search-space [top-1] 57.6% 59.0%
reference/MT-search-space [top-3] 70.7% 70.9%
reference/MT-search-space [top-30] 86.0% 85.0%
reference/MT-search-space [MRR] 0.655 0.662
reference/MT-output 50.0% 50.7%
stem-only reference/MT-output 12.3% 11.5%
of which reachable 11.2% 10.3%
Table 9: Target word coverage analysis of the
English-Russian SMT system before and after
adding the morphological BNN models.
1684
like the target LM, that are based on traditional
maximum-likelihood estimates. While the suffix-
based LMs proved beneficial in our experiments,
we speculate that higher gains could be obtained
by coupling our approach with a morphology-
aware neural LM like the one recently presented
by Botha and Blunsom (2014).
7 Related work
While most relevant literature has been discussed
in earlier sections, the following approaches are
particularly related to ours: Minkov et al. (2007)
and Toutanova et al. (2008) address target inflec-
tion prediction with a log-linear model based on
rich morphological and syntactic features. Their
model exploits target context and is applied to
inflect the output of a stem-based SMT system,
whereas our models predict target words (or pairs
of stem-suffix) independently and are integrated
into decoding. Chahuneau et al. (2013) address
the same problem with another feature-rich dis-
criminative model that can be integrated in decod-
ing, like ours, but they also use it to inflect on-
the-fly stemmed phrases. It is not clear what part
of their SMT improvements is due to the gener-
ation of new phrases or to better scoring. Jeong
et al. (2010) predict surface word forms in con-
text, similarly to our word BNN, and integrate the
scores into the SMT system. Unlike us, they rely
on linguistic feature-rich log-linear models to do
that. Gimpel and Smith (2008) propose a similar
approach to directly predict phrases in context, in-
stead of words.
All those approaches employed features that
capture the global structure of source sentences,
like dependency relations. By contrast, our mod-
els access only local context in the source sen-
tence but they achieve accuracy gains comparably
to models that also use global sentence structure.
8 Conclusions
We have proposed a general approach to predict
word translations in context using bilingual neu-
ral network architectures. Unlike previous NN ap-
proaches, we model word, stem and suffix dis-
tributions in the target language given context in
the source language. Instead of relying on man-
ually engineered features, our models automati-
cally learn abstract word representations and fea-
tures that are relevant for the modeled task directly
from word-aligned parallel data. Our preliminary
results with LBL and ConvNet architectures sug-
gest that potential improvement may be achieved
by factorizing target representations or by dynam-
ically modeling source context size. Evaluated
on three morphologically rich languages, our ap-
proach achieves considerable gains in word, stem
and suffix accuracy over a context-independent
maximum-likelihood baseline. Finally, we have
shown that the proposed BNN models can be
tightly integrated into a phrase-based SMT sys-
tem, resulting in small but statistically significant
BLEU improvement over a competitive, large-
scale English-Russian baseline.
Our analysis shows that the number of correct
target words occurring in highly scored phrase
translation candidates increases after integrating
the morphological BNNs. However, only few of
these end up in the 1-best translation output. Fu-
ture work will investigate the benefits of coupling
our BNN models with target language models that
also exploit abstract word representations, such as
Botha and Blunsom (2014) and Auli et al. (2013).
Acknowledgments
This research was funded in part by the
Netherlands Organization for Scientific Research
(NWO) under project numbers 639.022.213 and
612.001.218. We would like to thank Ekaterina
Garmash for helping with the error analysis of the
English-Russian translations.
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054, Seattle, Washington, USA, October.
Yoshua Bengio, R?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137?1155.
Arianna Bisazza and Christof Monz. 2014. Class-
based language modeling for translating into mor-
phologically rich languages. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics, pages 1918?1927,
Dublin, Ireland.
Ond
?
rej Bojar, Zden
?
ek ?abokrtsk?, Ond
?
rej Du?ek, Pe-
tra Galu?
?
c?kov?, Martin Majli?, David Mare
?
cek, Ji
?
r?
Mar??k, Michal Nov?k, Martin Popel, and Ale? Tam-
chyna. 2012. The joy of parallelism with czeng
1685
1.0. In Proceedings of LREC2012, Istanbul, Turkey,
May. ELRA, European Language Resources Asso-
ciation.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Jan A. Botha and Phil Blunsom. 2014. Composi-
tional Morphology for Word Representations and
Language Modelling. In Proceedings of the 31st In-
ternational Conference on Machine Learning, Bei-
jing, China, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?al, Canada, June. Association for
Computational Linguistics.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1677?1687, Seat-
tle, USA, October.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
4(13):359?393.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 200?209, Montr?al, Canada, June. Asso-
ciation for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of the 25th Annual International Confer-
ence on Machine Learning, volume 12, pages 2493?
2537.
Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-
jime Tsukada. 2013. Adaptation data selection us-
ing neural language models: Experiments in ma-
chine translation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 678?683, Sofia, Bulgaria, August.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model with minimal translation units, but de-
code with phrases. In Proceedings of Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1?11, Atlanta, Georgia, USA, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ?08: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 848?856, Morristown, NJ, USA.
Association for Computational Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase repre-
sentations for translation modeling. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, pages 699?709. Associ-
ation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 9?17, Columbus, Ohio,
USA.
Spence Green and John DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?12, pages 146?155, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rejwanul Haque, Sudip Kumar Naskar, Antal Bosch,
and Andy Way. 2011. Integrating source-
language context into phrase-based statistical ma-
chine translation. Machine Translation, 25(3):239?
285, September.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. arXiv
preprint arXiv:1207.0580.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20?29,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A discriminative lexicon
model for complex morphology. In The Ninth Con-
ference of the Association for Machine Translation
in the Americas.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700?1709, Seattle,
USA, October.
1686
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 655?665. Association for
Computational Linguistics.
Ahmed El Kholy and Nizar Habash. 2012. Translate,
predict or generate: Modeling rich morphology in
statistical machine translation. In Proceedings of
the 16th Conference of the European Association for
Machine Translation (EAMT).
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127?133, Ed-
monton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic.
Association for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: A beam search
decoder for phrase-based statistical machine trans-
lation models. In Robert E. Frederking and
Kathryn B. Taylor, editors, Proceedings of the 6th
Conference of the Association for Machine Transla-
tions in the Americas (AMTA 2004), pages 115?124.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gau-
vain, and Fran?ois Yvon. 2011. Structured output
layer neural network language model. In Proceed-
ings of Proceedings of ICASSP.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2011. Modeling syntactic context improves mor-
phological segmentation. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning, pages 1?9, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Arne Mauser, Sa?a Hasan, and Hermann Ney. 2009.
Extending statistical machine translation with dis-
criminative and trigger-based lexicon models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, pages 210?218, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 128?135.
Andriy Mnih and Geoffrey E. Hinton. 2007. Three
new graphical models for statistical language mod-
elling. In Proceedings of the 24th International
Conference on Machine Learning, pages 641?648,
New York, NY, USA.
Thomas M?ller, Hinrich Sch?tze, and Helmut Schmid.
2012. A comparative investigation of morphological
language modeling for the languages of the Euro-
pean Union. In Proceedings of the 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 386?395, Montr?al, Canada,
June. Association for Computational Linguistics.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley-
Interscience.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? challenging the conventional wisdom in
statistical machine translation. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 9?16, New York
City, USA, June. Association for Computational
Linguistics.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57?64, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Holger Schwenk, Daniel Dechelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proceedings
of the COLING/ACL 2006 Conference, pages 723?
730, Sydney, Australia, July. Association for Com-
putational Linguistics.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In Proceedings of COLING.
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Design-
ing and evaluating a russian tagset. In Pro-
ceedings of the Sixth International Conference on
Language Resources and Evaluation (LREC?08),
Marrakech, Morocco. European Language Re-
sources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
1687
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Le Hai Son, Alexandre Allauzen, and Fran?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 39?48, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Drahom?ra Spoustov?, Jan Haji?c, Jan Votrubec, Pavel
Krbec, and Pavel Kv?eto?n. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, pages 67?74, Prague, Czech Republic, June.
Association for Computational Linguistics.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of the Associa-
tion for Computational Linguistics.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1387?1392, Seattle, October.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393?
1398, Seattle, USA, October.
1688
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1689?1700,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Dependency-Based Bilingual Language Models for
Reordering in Statistical Machine Translation
Ekaterina Garmash and Christof Monz
Informatics Institute, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{e.garmash,c.monz}@uva.nl
Abstract
This paper presents a novel approach to
improve reordering in phrase-based ma-
chine translation by using richer, syntac-
tic representations of units of bilingual
language models (BiLMs). Our method
to include syntactic information is simple
in implementation and requires minimal
changes in the decoding algorithm. The
approach is evaluated in a series of Arabic-
English and Chinese-English translation
experiments. The best models demon-
strate significant improvements in BLEU
and TER over the phrase-based baseline,
as well as over the lexicalized BiLM by
Niehues et al. (2011). Further improve-
ments of up to 0.45 BLEU for Arabic-
English and up to 0.59 BLEU for Chinese-
English are obtained by combining our de-
pendency BiLM with a lexicalized BiLM.
An improvement of 0.98 BLEU is ob-
tained for Chinese-English in the setting of
an increased distortion limit.
1 Introduction
In statistical machine translation (SMT) reorder-
ing (also called distortion) refers to the order in
which source words are translated to generate the
translation in the target language. Word orders
can differ significantly across languages. For in-
stance, Arabic declarative sentences can be verb-
initial, while the corresponding English translation
should realize the verb after the subject, hence re-
quiring a reordering. Determining the correct re-
ordering during decoding is a major challenge for
SMT. This problem has received a lot of attention
in the literature (see, e.g., Tillmann (2004), Zens
and Ney (2003), Al-Onaizan and Papineni (2006)),
as choosing the correct reordering improves read-
ability of the translation and can have a substan-
tial impact on translation quality (Birch, 2011). In
this paper, we only consider those approaches that
include a reordering feature function into the log-
linear interpolation used during decoding.
The simplest reordering model is linear distor-
tion (Koehn et al., 2003) which scores the distance
between phrases translated at steps t and t + 1 of
the derivation. This model ignores any contex-
tual information, as the distance between trans-
lated phrases is its only parameter. Lexical dis-
tortion modeling (Tillmann, 2004) conditions re-
ordering probabilities on the phrase pairs trans-
lated at the current and previous steps. Unlike
linear distortion, it characterizes reordering not in
terms of distance but type: monotone, swap, or
discontinuous.
In this paper, we base our approach to reorder-
ing on bilingual language models (Marino et al.,
2006; Niehues et al., 2011). Instead of directly
characterizing reordering, they model sequences
of elementary translation events as a Markov pro-
cess.
1
Originally, Marino et al. (2006) used this
kind of model as the translation model, while more
recently it has been used as an additional model
in PBSMT systems (Niehues et al., 2011). We
adopt and generalize the approach of Niehues et al.
(2011) to investigate several variations of bilingual
language models. Our method consists of labeling
elementary translation events (tokens of bilingual
LMs) with their different contextual properties.
What kind of contextual information should be
incorporated in a reordering model? Lexical in-
formation has been used by Tillmann (2004) but
is known to suffer from data sparsity (Galley and
Manning, 2008). Also previous contributions to
bilingual language modeling (Marino et al., 2006;
Niehues et al., 2011) have mostly used lexical
information, although Crego and Yvon (2010a)
and Crego and Yvon (2010b) label bilingual to-
1
Note that the standard PBSMT translation model as-
sumes that events of translating separate phrases in a sentence
are independent.
1689
kens with a rich set of POS tags. But in gen-
eral, reordering is considered to be a syntactic phe-
nomenon and thus the relevant features are syn-
tactic (Fox, 2002; Cherry, 2008). Syntactic in-
formation is incorporated in tree-based approaches
in SMT, allowing one to provide a more detailed
definition of translation events and to redefine de-
coding as parsing of a source string (Liu et al.,
2006; Huang et al., 2006; Marton and Resnik,
2008), of a target string (Shen et al., 2008), or
both (Chiang, 2007; Chiang, 2010). Reordering
is a result of a given derivation, and CYK-based
decoding used in tree-based approaches is more
syntax-aware than the simple PBSMT decoding
algorithm. Although tree-based approaches poten-
tially offer a more accurate model of translation,
they are also a lot more complex and requiring
more intricate optimization and estimation tech-
niques (Huang and Mi, 2010).
Our idea is to keep the simplicity of PBSMT but
move towards the expressiveness typical of tree-
based models. We incrementally build up the syn-
tactic representation of a translation during decod-
ing by adding precomputed fragments from the
source parse tree. The idea to combine the mer-
its of the two SMT paradigms has been proposed
before, where Huang and Mi (2010) introduce in-
cremental decoding for a tree-based model. On a
very general level, our approach is similar to theirs
in that it keeps track of a sequence of source syn-
tactic subtrees that are being translated at consec-
utive decoding steps. An important difference is
that they keep track of whether the visited subtrees
have been fully translated, while in our approach,
once a syntactic structural unit has been added to
the history, it is not updated anymore.
In this paper, we focus on source syntactic in-
formation. During decoding we have full access
to the source sentence, which allows us to obtain
a better syntactic analysis (than for a partial sen-
tence) and to precompute the units that the model
operates with. We investigate the following re-
search questions: How well can we capture re-
ordering regularities of a language pair by incor-
porating source syntactic parameters into the units
of a bilingual language model? What kind of
source syntactic parameters are necessary and suf-
ficient?
Our contributions can be summarized as fol-
lows: We argue that the contextual information
used in the original bilingual models (Niehues et
al., 2011) is insufficient and introduce a simple
model that exploits source-side syntax to improve
reordering (Sections 2 and 3). We perform a thor-
ough comparison between different variants of our
general model and compare them to the original
approach. We carry out translation experiments
on multiple test sets, two language pairs (Arabic-
English and Chinese-English), and with respect to
two metrics (BLEU and TER). Finally, we present
a preliminary analysis of the reorderings resulting
from the proposed models (Section 4).
2 Motivation
In this section, we elaborate on our research ques-
tions and provide background for our approach.
We also discuss existing bilingual n-gram mod-
els and argue that they are often not expressive
enough to differentiate between alternative re-
orderings. We should first note that the most com-
monly used n-gram model to distinguish between
reorderings is a target language model, which does
not take translation correspondence into account
and just models target-side fluency. Al-Onaizan
and Papineni (2006) show that target language
models by themselves are not sufficient to cor-
rectly characterize reordering. In what follows we
only discuss bilingual models.
The word-aligned sentence pair in Figure 1.a
2
demonstrates a common Arabic-English reorder-
ing. As stated in the introduction, bilingual lan-
guage models capture reordering regularities as a
sequence of elementary translation events
3
. In the
given example, one could decompose the sequen-
tial process of translation as follows: First trans-
late the first word Alwzyr as the minister, then ArjE
as attributed, then ArtfAE as the increase and so
on. The sequence of elementary translation events
is modeled as an n-gram model (Equation 1, where
t
i
is a translation event). There are numerous ways
in which t
i
can be defined. Below we first discuss
how they have been defined within previous ap-
proaches, and then introduce our definition.
p(t
1
, . . . , t
m
) =
m
?
i=1
p(t
i
|t
i?n+1
. . . t
i?1
) (1)
2.1 Lexicalized bilingual LMs
By including both source and target information
into the representation of translation events we ob-
2
We used Buckwalter transliteration for Arabic words.
3
By an elementary translation event we mean a translation
of some substructure of a sentence.
1690
the minister attributed the increase of oil prices
w ArjE Alwzyr
ArtfAE AsEAr Albtrwl
(a) The original word alignment.
the
Alwzyr
minister
Alwzyr
attributed
ArjE
the
ArtfAE
increase 
ArtfAE
of 
empty
oil
Albtrwl
prices
AsEAr
(b) BiLM tokens extracted from sentence (a).
empty
w
of 
oil
Albtrwl
prices
AsEAr
the minister
Alwzyr ArjE
the
the increase 
ArtfAE
(c) MTU tokens extracted from sentence (a).
Figure 1: Arabic-English parallel sentence, automatically word-aligned. The bilingual token sequences
are produced according to two alternative definitions (BiLM and MTU).
tain a bilingual LM. The richer representation al-
lows for a finer distinction between reorderings.
For example, Arabic has a morphological marker
of definiteness on both nouns and adjectives. If
we first translate a definite adjective and then an
indefinite noun, it will probably not be a likely se-
quence according to the translation model. This
kind of intuition underlies the model of Niehues et
al. (2011), a bilingual LM (BiLM), which defines
elementary translation events t
1
, ..., t
n
as follows:
t
i
= ?e
i
, {f |f ? A(e
i
)}?, (2)
where e
i
is the i-th target word and A : E ?
P(F ) is an alignment function, E and F refer-
ring to target and source sentences, and P(?) is the
powerset function. In other words, the i-th trans-
lation event consists of the i-th target word and all
source words aligned to it. Niehues et al. (2011)
refer to the defined translation events t
i
as bilin-
gual tokens and we adopt this terminology.
There are alternative definitions of bilingual
language models. Our choice of the above defi-
nition is supported by the fact that it produces an
unambiguous segmentation of a parallel sentence
into tokens. Ambiguous segmentation is unde-
sirable because it increases the token vocabulary,
and thus the model sparsity. Another disadvan-
tage comes from the fact that we want to compare
permutations of the same set of elements. For ex-
ample, the two different segmentations of ba into
[ba] and [b][a] still represent the same permuta-
tion of the sequence ab. In Figure 1 one can pro-
duce a segmentation of (AsEAr Albtrwl, oil prices)
into (Albtrwl, oil) and (AsEAr, prices) or leave
it as is. If we allow for both segmentations, the
learnt probability parameters may be different for
the sum of (Albtrwl, oil) and (AsEAr, prices) and
for the unsegmented phrase.
Durrani et al. (2011) introduce an alternative
method for unambiguous bilingual segmentation
where tokens are defined as minimal phrases,
called minimal translation units (MTUs). Figure 1
compares the BiLM and MTU tokenization for a
specific example. Since Niehues et al. (2011) have
shown their model to work successfully as an addi-
tional feature in combination with commonly used
standard phrase-based features, we use their ap-
proach as the main point of reference and base our
approach on their segmentation method. In the
rest of the text we refer to Niehues et al. (2011)
as the original BiLM.
4
At the same time, we do
not see any specific obstacles for combining our
work with MTUs.
2.2 Suitability of lexicalized BiLM to model
reordering
As mentioned in the introduction, lexical informa-
tion is not very well-suited to capture reordering
regularities. Consider Figure 2.a. The extracted
sequence of bilingual tokens is produced by align-
ing source words with respect to target words (so
that they are in the same order), as demonstrated
by the shaded part of the picture. If we substituted
the Arabic translation of Egyptian for the Arabic
translation of Israeli, the reordering should remain
the same. What matters for reordering is the syn-
tactic role or context of a word. By using unneces-
sarily fine-grained categories we risk running into
sparsity issues.
Niehues et al. (2011) also described an alterna-
tive variant of the original BiLM, where words are
substituted by their POS tags (Figure 2.a, shaded
part). Also, however, POS information by itself
may be insufficiently expressive to separate cor-
4
Although, strictly speaking, it is not the original ap-
proach (see the references in Section 1).
1691
Egyptian
exports
to
trAjEt
SAdrAt mSr l Aldwl AlErbyp
VBD NNS NNP IN DTNN DTJJ
JJ NNS TO
Arabic
countries declined ?
?
JJ NNS VBD
trAjEt
SAdrAtmSr
l AldwlAlErbyp
NNSNNP IN DTJJ
?
DTNN VBD
(a)
trAjEt
SAdrAt mSr l Aldwl AlErbyp
VBD NNS NNP IN DTNN DTJJ
Arabic
JJ
AlErbyp
DTJJ
countries
NNS
Aldwl
DTNN
declined
VBD
trAjEt
VBD
Egyptian
exports
to
JJ NNS TO
SAdrAtmSr
l
NNSNNP IN
(b)
Figure 2: Arabic-English parallel sentence, automatically parsed and word-aligned, with corresponding
sequences of bilingual tokens (in the shaded part). Comparison between translations produced via correct
(a) and incorrect (b) reorderings.
JJ NNS TO
JJ NNS VBD
NNS!NNP
VBD!NNS NNS!IN
DTNN!DTJJ
IN!DTNN
ROOT!VBD
(a)
JJ NNS TO
JJ NNS VBD
NNS!NNP
VBD!NNS NNS!INDTNN!DTJJ
IN!DTNN
ROOT!VBD
(b)
Figure 3: Sequences of bilingual tokens with
source words substituted with their and their par-
ents? POS tags: correct (a) and incorrect (b) re-
orderings.
rect and incorrect reorderings, see Figure 2.b. Al-
though the corresponding sequence of POS-tag-
substituted bilingual tokens is different from the
correct sequence (Figure 2.b, shaded part), it still
is a likely sequence. Indeed, the log-probabilities
of the two sequences with respect to a 4-gram
BiLM model
5
result in a higher probability of
?10.25 for the incorrect reordering than for the
correct one (?10.39).
Since fully lexicalized bilingual tokens suffer
from data sparsity and POS-based bilingual tokens
are insufficiently expressive, the question is which
level of syntactic information strikes the right bal-
ance between expressiveness and generality.
5
Section 4 contains details about data and software setup.
2.3 BiLM with dependency information
Dependency grammar is commonly used in NLP
to formalize role-based relations between words.
The intuitive notion of syntactic modification is
captured by the primitive binary relation of depen-
dence. Dependency relations do not change with
the linear order of words (Figure 2) and therefore
can provide a characterization of a word?s syntac-
tic class that invariant under reordering.
If we incorporate dependency relations into the
representation of bilingual tokens, the incorrect re-
ordering in Figure 2.b will produce a highly un-
likely sequence. For example, we can substitute
each source word with its POS tag and its par-
ent?s POS tag (Figure 3). Again, we computed
4-gram log-probabilities for the corresponding se-
quences: the correct reordering results in a sub-
stantially higher probability of?10.58 than the in-
correct one (?13.48). We may consider situations
where more fine-grained distinctions are required.
In the next section, we explore different represen-
tations based on source dependency trees.
3 Dependency-based BiLM
In this section, we introduce our model which
combines the BiLM from Niehues et al. (2011)
with source dependency information. We fur-
ther give details on how the proposed models are
trained and integrated into a phrase-based decoder.
1692
3.1 The general framework
In the previous section we outlined our framework
as composed of two steps: First, a parallel sen-
tence is tokenized according to the BiLM model
(Niehues et al., 2011). Next, words in the bilingual
tokens are substituted with their contextual prop-
erties. It is thus convenient to use the following
generalized definition for a token sequence t
1
...t
n
in our framework:
t
i
= ?ContE (e
i
), {ContF (f)|f ? A(e
i
)}?, (3)
where e
i
is the i-th target word, A : E ? P(F )
is an alignment function, F and E are source and
target sentences, and ContE and ContF are tar-
get and source contextual functions, respectively.
A contextual function returns a word?s contextual
property, based on its sentential context (source or
target). See Figure 4 for an example of a sequence
of BiLM tokens with a ContF defined as return-
ing the POS tag of the source word combined with
the POS tags of its parent, grandparent and sib-
lings, and ContE defined as an identity function
(see Section 3.2 for a detailed explanation of the
functions and notation).
In this work we focus on source contextual
functions (ContF ). We also exploit some very
simple target contextual functions, but do not go
into an in-depth exploration.
3.2 Dependency-based contextual functions
In NLP approaches exploiting dependency struc-
ture, two kinds of relations are of special impor-
tance: the parent-child relation and the sibling re-
lation. Shen et al. (2008) work with two well-
formed dependency structures, both of which are
defined in such a way that there is one common
parent and a set of siblings. Li et al. (2012) charac-
terize rules in hierarchical SMT by labeling them
with the POS tags of the parents of the words in-
side the rule. Lerner and Petrov (2013) model re-
ordering as a sequence of classification steps based
on a dependency parse of a sentence. Their model
first decides how a word is reordered with respect
to its parent and then how it is reordered with re-
spect to its siblings.
Based on these previous approaches, we pro-
pose to characterize contextual syntactic roles of
a word in terms of POS tags of the words them-
selves and their relatives in a dependency tree. It
is straightforward to incorporate parent informa-
tion since each node has a unique parent. As for
siblings information, we incorporate POS tags of
the closest sibling to the left and the closest to the
right. We do not include all of the siblings to avoid
overfitting. In addition to these basic syntactic re-
lations, we consider the grandparent relation.
The following list is a summary of the source
contextual functions that we use. We describe
a function with respect to the kind of contextual
property of a word it returns: (i) the word itself
(Lex); (ii) POS label of the word (Pos); (iii) POS
label of the word?s parent; (iv) POS of the word?s
closest sibling to the left, concatenated with the
POS tag of the closest sibling to the right; (v)
the POS label of the word?s grandparent. We use
target-side contextual functions returning: (i) an
empty string, (ii) POS of the word, (iii) the word
itself.
Notation. We do not use the above functions
separately to define individual BiLM models, but
use combinations of these functions. We use the
following notation for function combinations: ???
horizontally connects source (on the left) and tar-
get (on the right) contextual functions for a given
model. For example, Lex?Lex refers to the original
(lexicalized) BiLM. We use arrows (?) to des-
ignate parental information (the arrow goes from
parent to child). Pos?Pos refers to a combination
of a function returning the POS of a word and the
POS of its parent (as in Figure 3). Pos?Pos?Pos
is a combination of the previous with the func-
tion returning the grandparent?s POS. Finally, we
use +sibl to indicate the use of the sibling func-
tion described above: For example, Pos?Pos+sibl
is a source function that returns the word?s POS,
its parent?s POS and the POS labels of the closest
siblings to left and right.
6
Pos+sibl?Pos is a source
function returning the word?s own POS, the POS
of a word?s parent, and the POS tags of the par-
ent?s siblings (left- and right-adjacent).
Figure 4 represents the sentence from Figure 2
during decoding in a system with an integrated
Pos?Pos?Pos+sibl?Lex feature. It shows the se-
quence of produced bilingual tokens and corre-
sponding labels in the introduced notation.
3.3 Training
Training of dependency-based BiLMs consists of
a sequence of extraction steps: After having pro-
duced word-alignments for a bitext (Section 4),
6
In case there is no sibling on one of the sides,  (empty
word) is returned.
1693
Egyptian
exports
trAjEt SAdrAt mSr l Aldwl AlErbyp
VBD NNS NNP IN DTNN
DTJJ
JJ NNS TO
to
Egyptian
VBD NNS NNP
IN
to
VBD NNS
NNP
IN
exports
VBD NNS
?
Figure 4: Sequence of bilingual tokens pro-
duced by a Pos?Pos?Pos+sibl?Lex after
translating three words of the source sentence:
VBD?NNS?+NNS+IN?Egyptian, ROOT?VBD?
+NNS+?exports, VBD?NNS?NNP+IN+?to (if there
is no sibling on either of the sides,  is returned).
sentences are segmented according to Equation 3.
We produce a dependency parse of a source sen-
tence and a POS-tag labeling of a target sen-
tence. For Chinese, we use the Stanford depen-
dency parser (Chang et al., 2009). For Arabic a
dependency parser is not available for public use,
so we produce a constituency parse with the Stan-
ford parser (Green and Manning, 2010) and ex-
tract dependencies based on the rules in Collins
(1999). For English POS-tagging, we use the
Stanford POS-tagger (Toutanova et al., 2003). Af-
ter having produced a labeled sequence of tokens,
we learn a 5-gram model using SRILM (Stolcke
et al., 2011). Kneyser-Ney smoothing is used
for all model variations except for Pos?Pos where
Witten-Bell smoothing is used due to zero count-
of-counts.
3.4 Decoder integration
Dependency-based BiLMs are integrated into our
phrase-based SMT decoder as follows: Before
translating a sentence, we produce its dependency
parse. Phrase-internal word-alignments, needed
to segment the translation hypothesis into tokens,
are stored in the phrase table, based on the most
frequent internal alignment observed during train-
ing. Likewise, we store the most likely target-side
POS-labeling for each phrase pair.
The decoding algorithm is augmented with one
additional feature function and one additional, cor-
responding feature weight. At each step of the
derivation, as a new phrase pair is added to the
Training set N. of lines N. of tokens
Source side of Ar-En set 4,376,320 148M
Target side of Ar-En set 4,376,320 146M
Source side of Ch-En set 2,104,652 20M
Target side of Ch-En set 2,104,652 28M
Table 1: Training data for Arabic-English and
Chinese-English experiments.
partial translation hypothesis, this function seg-
ments the new phrase into bilingual tokens (given
the internal alignment information) and substitutes
the words in the phrase pair with syntactic labels
(given the source parse and the target POS labeling
associated with the phrase). The new syntactified
bilingual tokens are added to the stack of preced-
ing n?1 tokens, and the feature function computes
the weighted updated model probability. During
decoding, the probabilities of the BiLMs are com-
puted in a stream-based fashion, with bilingual
tokens as string tokens, and not in a class-based
fashion, with syntactic source-side representations
emitting the corresponding target words (Bisazza
and Monz, 2014).
4 Experiments
4.1 Setup
We conduct translation experiments with a base-
line PBSMT system with additionally one of the
dependency-based BiLM feature functions speci-
fied in Section 3. We compare the translation per-
formance to a baseline PBSMT system and to a
baseline augmented with the original BiLMs from
(Niehues et al., 2011).
Word-alignment is produced with GIZA++
(Och and Ney, 2003). We use an in-house imple-
mentation of a PBSMT system similar to Moses
(Koehn et al., 2007). Our baseline contains
all standard PBSMT features including language
model, lexical weighting, and lexicalized reorder-
ing. The distortion limit is set to 5. A 5-gram LM
is trained on the English Gigaword corpus (1.6B
tokens) using SRILM with modified Kneyser-Ney
smoothing and interpolation. The BiLMs were
trained as described in Section 3.3. Informa-
tion about the parallel data used for training the
Arabic-English
7
and Chinese-English systems
8
is
7
The following Arabic-English parallel corpora were
used: LDC2006E25, LDC2004T18, several gale corpora,
LDC2004T17, LDC2005E46, LDC2007T08, LDC2004E13.
8
The following Chinese-English parallel corpora
were used: LDC2002E18, LDC2002L27, LDC2003E07,
LDC2003E14, LDC2005T06, LDC2005T10, LDC2005T34,
1694
Configuration MT08 MT09 MT08+MT09
BLEU TER BLEU TER BLEU TER
a PBSMT baseline 45.12 47.94 48.16 44.30 46.57 46.21
b Lex?Lex 45.27 47.79 48.85
N
43.96
M
46.98
N
45.96
M
Pos?Pos 44.80 47.84 48.22 44.14
M,?
46.44 46.07
c Pos?Pos?Pos 45.66
N,M
47.17
N,N
49.00
N,?
43.45
N,N
47.25
N,M
45.40
N,N
d Pos?Pos?sibl?Pos 45.46
M,?
47.45
N,M
48.69
N,?
43.64
N,M
47.00
N,?
45.64
N,?
e Pos?Pos?Pos?Pos 45.68
N,M
47.42
N,M
49.09
N,?
43.59
N,N
47.30
N,M
45.60
N,N
f Lex?Lex + Pos?Pos?Pos?Pos 45.63
N,M
47.48
N,M
49.30
N,N
43.60
N,M
47.38
N,N
45.63
N,N
Table 2: BLEU and TER scores for Arabic-English experiments. Statistically significant improvements
over the baseline (a) are marked
N
at the p < .01 level and
M
at the p < .05 level. Additionally,
?,N
and
?,M
indicate significant improvements with respect to BiLM Lex?Lex (b). Since TER is an error rate, lower
scores are better.
Configuration MT08 MT09 MT08+MT09
BLEU TER BLEU TER BLEU TER
Pos?Pos?  45.66
N,M
47.44
N,M
48.78
N,?
43.94
N,?
47.15
N,?
45.77
N,M
Pos?Pos?Pos 45.66
N,M
47.17
N,N
49.00
N,?
43.45
N,N
47.25
N,M
45.40
N,N
Pos?Pos?Lex 45.48
M,?
47.34
N,N
48.90
N,?
43.87
N,M
47.12
N,?
45.69
N,N
Table 3: Different combinations of a target contextual function with the Pos?Pos source contextual
function for Arabic-English. See Table 2 for the notation regarding statistical significance.
shown in Table 1.
The feature weights were tuned by using pair-
wise ranking optimization (Hopkins and May,
2011) on the MT04 benchmark (for both language
pairs). During tuning, 14 PRO parameter estima-
tion runs are performed in parallel on different
samples of the n-best list after each decoder itera-
tion. The weights of the individual PRO runs are
then averaged and passed on to the next decoding
iteration. Performing weight estimation indepen-
dently for a number of samples corrects for some
of the instability that can be caused by individual
samples. For testing, we used MT08 and MT09 for
Arabic, and MT06 and MT08 for Chinese. We use
approximate randomization (Noreen, 1989; Rie-
zler and Maxwell, 2005) to test for statistically sig-
nificant differences.
In the next two subsections we discuss the gen-
eral results for Arabic and Chinese, where we use
case-insensitive BLEU (Papineni et al., 2002) and
TER (Snover et al., 2006) as evaluation metrics.
This is followed by a preliminary analysis of ob-
served reorderings where we compare 4-gram pre-
cision results and conduct experiments with an in-
creased distortion limit.
4.2 Arabic-English translation experiments
We are interested in how a translation system
with an integrated dependency-based BiLM fea-
and several gale corpora.
ture performs as compared to the standard PB-
SMT baseline and, more importantly, to the orig-
inal BiLM model. We consider two variants of
BiLM discussed by Niehues et al. (2011): the stan-
dard one, Lex?Lex, and the simplest syntactic one,
Pos?Pos. Results for the experiments can be found
in Table 2. In the discussion below we mostly fo-
cus on the experimental results for the large, com-
bined test set MT08+MT09.
Table 2.a?b compares the performance of the
baseline and original BiLM systems. Lex?Lex
yields strongly significant improvements over the
baseline for BLEU and weakly significant im-
provements for TER. Therefore, for the rest of the
experiments we are interested in obtaining further
improvements over Lex?Lex.
Pos?Pos?Pos (Table 2.c) demonstrates the effect
of adding minimal dependency information to a
BiLM.
9
It results in strongly significant improve-
ments over the baseline and weak improvements
over Lex?Lex in terms of BLEU. We additionally
ran experiments with the different target functions
(Table 3). ?Pos shows the highest results, and ? the
lowest ones: this implies that a rather expressive
source syntactic representation alone still benefits
from target-side syntactic information. Below, our
dependency-based systems only use ?Pos.
Next, we tested the effect of adding more source
9
Additional significance testing, which is not shown in
Table 2, shows a strongly significant improvement over the
original syntactic BiLM Pos?Pos.
1695
Configuration MT06 MT08 MT06+MT08
BLEU TER BLEU TER BLEU TER
a PBSMT baseline 31.89 57.79 25.53 60.71 28.99 59.14
b Lex?Lex 32.84
N
57.40
N
25.91
M
60.23
N
29.69
N
58.72
N
Pos?Pos 32.31
N
57.89 25.66 60.79 29.28 59.24
c Pos?Pos?Pos 32.86
N,?
57.05
N,M
26.09
N,?
59.87
N,M
29.78
N,?
58.36
N,N
d Pos?Pos?sibl?Pos 32.27
M,?
56.63
N,M
25.75 59.47
N,N
29.30
M,?
57.95
N,N
e Pos?Pos?Pos?Pos 33.09
N,?
57.54 26.35
N,M
59.70
N,N
30.05
N,N
58.54
N,?
f Lex?Lex + Pos?Pos?Pos?Pos 33.43
N,N
57.00
N,N
26.50
N,N
59.79
N,N
30.28
N,N
58.30
N,N
Table 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines. See Table 2
for the notation regarding statistical significance.
Configuration MT06 MT08 MT06+MT08
BLEU TER BLEU TER BLEU TER
Pos?Pos?  32.43
N,?
57.42
N,?
25.84 60.51 29.43
N,?
58.86
N,?
Pos?Pos?Pos 32.86
N,?
57.05
N,M
26.09
N,?
59.87
N,M
29.78
N,?
58.36
N,N
Pos?Pos?Lex 32.69
N,?
57.03
N,M
25.72 60.17
N,?
29.52
N,?
58.49
N,M
Table 5: Different combinations of a target contextual function with the Pos?Pos source contextual func-
tion for Chinese-English. See Table 2 for the notation regarding statistical significance.
dependency information. Pos?Pos+sibl?Pos (Ta-
ble 2.d) only improves over the PBSMT baseline
(but also shows weak improvements over Lex?Lex
for TER). It significantly degrades the perfor-
mance with respect to the Pos?Pos?Pos system (Ta-
ble 2.c). Pos?Pos?Pos?Pos (Table 2.e) shows the
best results overall for BLEU, although it must be
pointed out that the difference with Pos?Pos?Pos is
very small. With respect to TER, Pos?Pos?Pos out-
performs the grandparent variant.
So far, we can conclude that source par-
ent information helps improve translation perfor-
mance. Increased specificity of a parent (par-
ent specified by a grandparent) tends to further
improve performance. Up to now, we have
only used syntactic information and obtained con-
siderable improvements over Pos?Pos, surpass-
ing the improvement provided by Lex?Lex. Can
we gain further improvements by also adding
lexical information? To this end, we con-
duct experiments combining the best performing
dependency-based BiLM (Pos?Pos?Pos?Pos) and
the lexicalized BiLM (Lex?Lex). We hypothesize
that the two models improve different aspects of
translation: Lex?Lex is biased towards improving
lexical choice and Pos?Pos?Pos?Pos towards im-
proving reordering. Combining these two models,
we may improve both aspects. The metric results
for the combined set indeed support this hypothe-
sis (Table 2.f).
4.3 Chinese-English translation experiments
The results of the Chinese-English experiments
are shown in Table 4. In the discussion below
we mostly focus on the experimental results for
the large, combined test set MT06+MT08. We
observe the same general pattern for the Pos?Pos
source function (Table 4.c) as for Arabic-English:
the system with the ?Pos target function has the
highest scores (Table 5). All of the Pos?Pos? con-
figurations show statistically significant improve-
ments over the PBSMT baseline. For TER, two
of the three Pos?Pos? variants significantly out-
perform Lex?Lex. The system with sibling in-
formation (Table 4.d) obtains quite low BLEU
results, just as in the Arabic experiments. On
the other hand, its TER results are the highest
overall. The system with the Pos?Pos?Pos?Pos
function (Table 4.e) achieves the best results
among dependency-based BiLMs for BLEU. Fi-
nally, combining Pos?Pos?Pos?Pos and Lex?Lex re-
sults in the largest and significant improvements
over all competing systems for BLEU.
4.4 Preliminary analysis of reordering in
translation experiments
In general, the experimental results show that us-
ing source dependency information yields consis-
tent improvements for translating from Arabic and
Chinese into English. On the other hand, we have
pointed out some discrepancies between the two
metrics employed, suggesting that different sys-
tem configurations may improve different aspects
1696
Configuration Ar-En Ch-En
MT08 MT09 MT08+MT09 MT06 MT08 MT06+MT08
a PBSMT baseline 26.14 29.81 27.88 14.48 10.96 12.89
b Lex?Lex 26.33 30.55 28.32 15.43 11.45 13.65
Pos?Pos 25.95 30.06 27.89 14.76 11.01 13.07
c Pos?Pos?Pos 26.91 31.08 28.87 15.29 11.52 13.60
e Pos?Pos?sibl?Pos 26.71 30.73 28.60 15.27 11.67 13.66
d Pos?Pos?Pos?Pos 26.78 31.09 28.80 15.42 11.70 13.77
f Lex?Lex + Pos?Pos?Pos?Pos 26.80 31.27 28.90 15.87 11.85 14.07
Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.
Configuration MT08 MT09 MT08+MT09
BLEU TER 4gram BLEU TER 4gram BLEU TER 4gram
Lex?Lex 45.19 47.06 26.41 48.39 44.11 30.23 46.72 45.97 28.21
Pos?Pos?Pos?Pos 45.49 47.31
M
26.66 48.90
N
43.57
N
30.92 47.12
N
45.52
N
28.66
Table 7: BLEU, TER and 4-gram precision scores for Arabic-English Lex?Lex and Pos?Pos?Pos?Pos
with a distortion limit of 10.
Configuration MT06 MT08 MT06+MT08
BLEU TER 4gram BLEU TER 4gram BLEU TER 4gram
Lex?Lex 33.26 56.81 16.06 25.67 60.19 11.42 29.79 58.38 13.96
Pos?Pos?Pos?Pos 33.92
N
56.29
N
16.26 27.00
N
59.58
N
12.26 30.77
N
57.82
N
14.46
Table 8: BLEU, TER and 4-gram precision scores for Chinese-English Lex?Lex and
Pos?Pos?Pos?Pos with a distortion limit of 10.
of translation. To this end, we conducted some ad-
ditional evaluations to understand how reordering
is affected by the proposed features.
We use 4-gram precision as a metric of how
much of the reference set word order is preserved.
Table 6 shows the corresponding results for both
languages. Just as in the previous two sections,
configurations with parental information produce
the best results. For Arabic, all of the depen-
dency configurations outperform Lex?Lex. But the
system with two feature functions, one of which
is Lex?Lex, still obtains the best results, which
may suggest that the lexicalized BiLM also helps
to differentiate between word orders. For Chi-
nese, Pos?Pos?Pos?Pos and the system combining
the latter and Lex?Lex also obtain the best results.
However, other dependency-based configurations
do not outperform Lex?Lex.
All the experiments so far were run with a dis-
tortion limit of 5. But both of the languages, es-
pecially Chinese, often require reorderings over a
longer distance. We performed additional experi-
ments with a distortion limit of 10 for the Lex?Lex
and Pos?Pos?Pos?Pos systems (Tables 7 and 8). It
is more difficult to translate with a higher distor-
tion limit (Green et al., 2010) as the set of permu-
tations grows larger thereby making it more diffi-
cult to differentiate between correct and incorrect
continuations of the current hypothesis. It has also
been noted that higher distortion limits are more
likely to result in improvements for Chinese rather
than Arabic to English translation (Chiang, 2007;
Green et al., 2010).
We compared performance of fixed BiLM mod-
els at distortion lengths of 5 and 10. Arabic-
English results did not reveal statistically signif-
icant differences between the two distortion lim-
its for Pos?Pos?Pos?Pos. On the other hand, for
Lex?Lex BLEU decreases when using a distor-
tion limit of 10 compared to a limit of 5. This
implies that the dependency BiLM is more ro-
bust in the more challenging reordering setting
than the lexicalized BiLM. Chinese-English re-
sults for Pos?Pos?Pos?Pos do show significant im-
provements over the distortion limit of 5 (up to
0.49 BLEU higher than the best result in Table 4).
This indicates that the dependency-based BiLM is
better capable to take advantage of the increased
distortion limit and discriminate between correct
and incorrect reordering choices.
Comparing the results for Pos?Pos?Pos?Pos and
Lex?Lex at a distortion limit of 10, we obtain
strongly significant improvements for all metrics.
For Chinese, a larger distortion limit helps for both
configurations, but more so for our dependency
BiLM, yielding an improvement of 0.98 BLEU
1697
over the original, lexicalized BiLM (Table 8).
5 Conclusions
In this paper, we have introduced a simple, yet ef-
fective way to include syntactic information into
phrase-based SMT. Our method consists of en-
riching the representation of units of a bilingual
language model (BiLM). We argued that the very
limited contextual information used in the original
bilingual models (Niehues et al., 2011) can capture
reorderings only to a limited degree and proposed
a method to incorporate information from a source
dependency tree in bilingual units. In a series
of translation experiments we performed a thor-
ough comparison between various syntactically-
enriched BiLMs and competing models. The re-
sults demonstrated that adding syntactic informa-
tion from a source dependency tree to the repre-
sentations of bilingual tokens in an n-gram model
can yield statistically significant improvements
over the competing systems.
A number of additional evaluations provided an
indication for better modeling of reordering phe-
nomena. The proposed dependency-based BiLMs
resulted in an increase in 4-gram precision and
provided further significant improvements over
all considered metrics in experiments with an in-
creased distortion limit.
In this paper, we have focused on rather elemen-
tary dependency relations, which we are planning
to expand on in future work. Our current approach
is still strictly tied to the number of target tokens.
In particular, we are interested in exploring ways
to better capture the notion of syntactic cohesion
in translation (Fox, 2002; Cherry, 2008) within our
framework.
Acknowledgments
We thank Arianna Bisazza and the reviewers for
their useful comments. This research was funded
in part by the Netherlands Organization for Sci-
entific Research (NWO) under project numbers
639.022.213 and 612.001.218.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 529?536, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Alexandra Birch. 2011. Reordering Metrics for Statis-
tical Machine Translation. Ph.D. thesis, University
of Edinburgh.
Arianna Bisazza and Christof Monz. 2014. Class-
based language modeling for translating into mor-
phologically rich languages. In Proceedings of
the 25th International Conference on Computa-
tional Linguistics (COLING 2014), pages 1918?
1927, Dublin, Ireland, August.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51?59. Association for Computational Linguistics.
Colin Cherry. 2008. Cohesive phrase-based decoding
for statistical machine translation. In Proceedings
of Association for Computational Linguistics, pages
72?80.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1443?1452. Association for Com-
putational Linguistics.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Josep M. Crego and Franc?ois Yvon. 2010a. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, 24(2):159?
175.
Josep M. Crego and Franc?ois Yvon. 2010b. Improv-
ing reordering with linguistically informed bilin-
gual n-grams. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 197?205. Association for Computational Lin-
guistics.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 1045?1054. Association for Com-
putational Linguistics.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing, pages 304?3111. Association for
Computational Linguistics.
1698
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 848?856. Association for Computational Lin-
guistics.
Spence Green and Christopher D. Manning. 2010.
Better arabic parsing: Baselines, evaluations, and
analysis. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages
394?402. Association for Computational Linguis-
tics.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost
for statistical machine translation. In Proceedings
of the 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 867?875. Association for Com-
putational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1352?1362. Association for Computational
Linguistics.
Liang Huang and Haitao Mi. 2010. Efficient incre-
mental decoding for tree-to-string translation. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
273?283. Association for Computational Linguis-
tics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, pages
223?226.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
48?54. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics on Interactive
Poster and Demonstration Sessions, pages 177?180.
Association for Computational Linguistics.
Uri Lerner and Slav Petrov. 2013. Source-side classi-
fier preordering for machine translation. In Proceed-
ings of the Empirical Methods in Natural Language
Processing.
Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van
Genabith. 2012. Head-driven hierarchical phrase-
based translation. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics, pages 33?37. Association for Computa-
tional Linguistics.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 609?616. Association for
Computational Linguistics.
Jos?e B Marino, Rafael E Banchs, Josep M. Crego,
Adria de Gispert, Patrik Lambert, Jos?e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of the Association for Com-
putational Linguistics, pages 1003?1011.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider context by using bilin-
gual language models in machine translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 198?206. Association
for Computational Linguistics.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley-
Interscience.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting of the Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization.
Libin Shen, Jinxi Xu, and Ralph M. Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of the Association for Computational
Linguistics, pages 577?585.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA, pages 223?231.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and out-
look. In Proceedings of IEEE Automatic Speech
Recognition and Understanding Workshop, page 5.
1699
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of of the North American Chapter of the
Association for Computational Linguistics, pages
101?104. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 173?180. Association for Computational Lin-
guistics.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 144?151. Association for Compu-
tational Linguistics.
1700
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 2?11,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Power-Law Distributions for Paraphrases Extracted from Bilingual
Corpora
Spyros Martzoukos Christof Monz
Informatics Institute, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{s.martzoukos, c.monz}@uva.nl
Abstract
We describe a novel method that extracts
paraphrases from a bitext, for both the
source and target languages. In order
to reduce the search space, we decom-
pose the phrase-table into sub-phrase-tables
and construct separate clusters for source
and target phrases. We convert the clus-
ters into graphs, add smoothing/syntactic-
information-carrier vertices, and compute
the similarity between phrases with a ran-
dom walk-based measure, the commute
time. The resulting phrase-paraphrase
probabilities are built upon the conversion
of the commute times into artificial co-
occurrence counts with a novel technique.
The co-occurrence count distribution be-
longs to the power-law family.
1 Introduction
Paraphrase extraction has emerged as an impor-
tant problem in NLP. Currently, there exists an
abundance of methods for extracting paraphrases
from monolingual, comparable and bilingual cor-
pora (Madnani and Dorr, 2010; Androutsopou-
los and Malakasiotis, 2010); we focus on the lat-
ter and specifically on the phrase-table that is ex-
tracted from a bitext during the training stage of
Statistical Machine Translation (SMT). Bannard
and Callison-Burch (2005) introduced the pivot-
ing approach, which relies on a 2-step transition
from a phrase, via its translations, to a paraphrase
candidate. By incorporating the syntactic struc-
ture of phrases (Callison-Burch, 2005), the qual-
ity of the paraphrases extracted with pivoting can
be improved. Kok and Brockett (2010) (hence-
forth KB) used a random walk framework to de-
termine the similarity between phrases, which
was shown to outperform pivoting with syntac-
tic information, when multiple phrase-tables are
used. In SMT, extracted paraphrases with asso-
ciated pivot-based (Callison-Burch et al 2006;
Onishi et al 2010) and cluster-based (Kuhn et
al., 2010) probabilities have been found to im-
prove the quality of translation. Pivoting has also
been employed in the extraction of syntactic para-
phrases, which are a mixture of phrases and non-
terminals (Zhao et al 2008; Ganitkevitch et al
2011).
We develop a method for extracting para-
phrases from a bitext for both the source and tar-
get languages. Emphasis is placed on the qual-
ity of the phrase-paraphrase probabilities as well
as on providing a stepping stone for extracting
syntactic paraphrases with equally reliable prob-
abilities. In line with previous work, our method
depends on the connectivity of the phrase-table,
but the resulting construction treats each side sep-
arately, which can potentially be benefited from
additional monolingual data.
The initial problem in harvesting paraphrases
from a phrase-table is the identification of the
search space. Previous work has relied on breadth
first search from the query phrase with a depth
of 2 (pivoting) and 6 (KB). The former can be
too restrictive and the latter can lead to excessive
noise contamination when taking shallow syntac-
tic information features into account. Instead, we
choose to cluster the phrase-table into separate
source and target clusters and in order to make this
task computationally feasible, we decompose the
phrase-table into sub-phrase-tables. We propose
a novel heuristic algorithm for the decomposition
of the phrase-table (Section 2.1), and use a well-
established co-clustering algorithm for clustering
2
each sub-phrase-table (Section 2.2).
The underlying connectivity of the source
and target clusters gives rise to a natural graph
representation for each cluster (Section 3.1).
The vertices of the graphs consist of phrases
and features with a dual smoothing/syntactic-
information-carrier role. The latter allow (a) re-
distribution of the mass for phrases with no appro-
priate paraphrases and (b) the extraction of syn-
tactic paraphrases. The proximity among vertices
of a graph is measured by means of a randomwalk
distance measure, the commute time (Aldous and
Fill, 2001). This measure is known to perform
well in identifying similar words on the graph of
WordNet (Rao et al 2008) and a related measure,
the hitting time is known to perform well in har-
vesting paraphrases on a graph constructed from
multiple phrase-tables (KB).
Generally in NLP, power-law distributions are
typically encountered in the collection of counts
during the training stage. The distances of Sec-
tion 3.1 are converted into artificial co-occurrence
counts with a novel technique (Section 3.2). Al-
though they need not be integers, the main chal-
lenge is the type of the underlying distributions;
it should ideally emulate the resulting count dis-
tributions from the phrase extraction stage of a
monolingual parallel corpus (Dolan et al 2004).
These counts give rise to the desired probability
distributions by means of relative frequencies.
2 Sub-phrase-tables & Clustering
2.1 Extracting Connected Components
For the decomposition of the phrase-table into
sub-phrase-tables it is convenient to view the
phrase-table as an undirected, unweighted graph
P with the vertex set being the source and target
phrases and the edge set being the phrase-table en-
tries. For the rest of this section, we do not distin-
guish between source and target phrases, i.e. both
types are treated equally as vertices of P . When
referring to the size of a graph, we mean the num-
ber of vertices it contains.
A trivial initial decomposition of P is achieved
by identifying all its connected components (com-
ponents for brevity), i.e. the mutually disjoint
connected subgraphs, {P0, P1, ..., Pn}. It turns
out (see Section 4.1) that the largest component,
say P0, is of significant size. We call P0 giant
and it needs to be further decomposed. This is
done by identifying all vertices such that, upon
removal, the component becomes disconnected.
Such vertices are called articulation points or cut-
vertices. Cut-vertices of high connectivity degree
are removed from the giant component (see Sec-
tion 4.1). For the remaining vertices of the giant
component, new components are identified and
we proceed iteratively, while keeping track of the
cut-vertices that are removed at each iteration, un-
til the size of the largest component is less than a
certain threshold ? (see Section 4.1).
Note that at each iteration, when removing cut-
vertices from a giant component, the resulting col-
lection of components may include graphs con-
sisting of a single vertex. We refer to such ver-
tices as residues. They are excluded from the re-
sulting collection and are considered for separate
treatment, as explained later in this section.
The cut-vertices need to be inserted appropri-
ately back to the components: Starting from the
last iteration step, the respective cut-vertices are
added to all the components of P0 which they
used to ?glue? together; this process is performed
iteratively, until there are no more cut-vertices to
add. By ?addition? of a cut-vertex to a component,
we mean the re-establishment of edges between
the former and other vertices of the latter. The
result is a collection of components whose total
number of unique vertices is less than the number
of vertices of the initial giant component P0.
These remaining vertices are the residues. We
then construct the graph R which consists of
the residues together with all their translations
(even those that are included in components of
the above collection) and then identify its compo-
nents {R0, ..., Rm}. It turns out, that the largest
component, say R0, is giant and we repeat the de-
composition process that was performed on P0.
This results in a new collection of components
as well as new residues: The components need
to be pruned (see Section 4.1) and the residues
give rise to a new graph R? which is constructed
in the same way asR. We proceed iteratively until
the number of residues stops changing. For each
remaining residue u, we identify its translations,
and for each translation v we identify the largest
component of which v is a member and add u to
that component.
The final result is a collection C = D ? F ,
where D is the collection of components emerg-
ing from the entire iterative decomposition of P0
3
and R, and F = {P1, ..., Pn}. Figure 1 shows
the decomposition of a connected graph G0; for
simplicity we assume that only one cut-vertex is
removed at each iteration and ties are resolved ar-
bitrarily. In Figure 2 the residue graph is con-
structed and its two components are identified.
The iterative insertion of the cut vertices is also
depicted. The resulting two components together
with those from R form the collection D for G0.
The addition of cut-vertices into multiple com-
ponents, as well as the construction method of the
residue-based graph R, can yield the occurrences
of a vertex in multiple components in D. We ex-
ploit this property in two ways:
(a) In order to mitigate the risk of excessive de-
composition (which implies greater risk of good
paraphrases being in different components), as
well as to reduce the size of D, a conserva-
tive merging algorithm of components is em-
ployed. Suppose that the elements of D are
ranked according to size in ascending order as
D = {D1, ..., Dk, Dk+1, ..., D|D|}, where |Di| ?
?, for i = 1, ..., k, and some threshold ? (see Sec-
tion 4.1). Each component Di with i ? {1, ..., k}
is examined as follows: For each vertex of Di the
number of its occurrences inD is inspected; this is
done in order to identify an appropriate vertex b to
act as a bridge between Di and other components
of which b is a member. Note that translations of
a vertex b with smaller number of occurrences in
D are less likely to capture their full spectrum of
paraphrases. We thus choose a vertex b from Di
with the smallest number of occurrences in D ,
resolving ties arbitrarily, and proceed with merg-
ing Di with the largest component, say Dj with
j ? {1, ..., |D| ? 1}, of which b is also a member.
The resulting merged component Dj? contains all
vertices and edges of Di and Dj and new edges,
which are formed according to the rule: if u is a
vertex of Di and v is a vertex of Dj and (u, v) is
a phrase-table entry, then (u, v) is an edge in Dj? .
As long as no connected component has identi-
fied Di as the component with which it should be
merged, then Di is deleted from the collection D.
(b) We define an idf -inspired measure for each
phrase pair (x, x?) of the same type (source or tar-
get) as
idf(x, x?) =
1
log |D|
log
(
2c(x, x?)|D|
c(x) + c(x?)
)
, (1)
where c(x, x?) is the number of components in
which the phrases x and x? co-occur, and equiv-
alently for c(?). The purpose of this measure is
for pruning paraphrase candidates and its use is
explained in Section 3.1. Note that idf(x, x?) ?
[0, 1].
The merging process and the idf measure are
irrelevant for phrases belonging to the compo-
nents of F , since the vertex set of each compo-
nent of F is mutually disjoint with the vertex set
of any other component in C.
  
G0 s1s2s3s4 t1t 2t3 c0={s2 } G11r={t 2 }
s1s4 t1 G12s3s4G12 G21s3 t 4c1={t3} r? r?{s4 }
t 4 s3 t3t 4t3t 4
Figure 1: The decomposition of G0 with vertices
si and tj : The cut-vertex of the ith iteration is de-
noted by ci, and r collects the residues after each
iteration. The task is completed in Figure 2.
  
G s0s1 t 0t2 s0s1 t 0t2s2 t 1 =c3 s3 t3 =c4=c4s2 t 1t2 s2 t2t 1s3s0 t3 s0s2 t2t 1
Figure 2: Top: Residue graph with its components
(no further decomposition is required). Bottom:
Adding cut-vertices back to their components.
2.2 Clustering Connected Components
The aim of this subsection is to generate sep-
arate clusters for the source and target phrases
of each sub-phrase-table (component) C ? C.
For this purpose the Information-Theoretic Co-
Clustering (ITC) algorithm (Dhillon et al 2003)
is employed, which is a general principled cluster-
ing algorithm that generates hard clusters (i.e. ev-
4
ery element belongs to exactly one cluster) of two
interdependent quantities and is known to per-
form well on high-dimensional and sparse data.
In our case, the interdependent quantities are the
source and target phrases and the sparse data is
the phrase-table.
ITC is a search algorithm similar to K-means,
in the sense that a cost function, is minimized at
each iteration step and the number of clusters for
both quantities are meta-parameters. The number
of clusters is set to the most conservative initial-
ization for both source and target phrases, namely
to as many clusters as there are phrases. At each
iteration, new clusters are constructed based on
the identification of the argmin of the cost func-
tion for each phrase, which gradually reduces the
number of clusters.
We observe that conservative choices for the
meta-parameters often result in good paraphrases
being in different clusters. To overcome this prob-
lem, the hard clusters are converted into soft (i.e.
an element may belong to several clusters): One
step before the stopping criterion is met, we mod-
ify the algorithm so that instead of assigning a
phrase to the cluster with the smallest cost we se-
lect the bottom-X clusters ranked by cost. Addi-
tionally, only a certain number of phrases is cho-
sen for soft clustering. Both selections are done
conservatively with criteria based on the proper-
ties of the cost functions.
The formation of clusters leads to a natural re-
finement of the idf measure defined in eqn. (1):
The quantity c(x, x?) is redefined as the number
of components in which the phrases x and x? co-
occur in at least one cluster.
3 Monolingual Graphs & Counts
We proceed with converting the clusters into di-
rected, weighted graphs and then extract para-
phrases for both the source and target side. For
brevity we explain the process restricted to the
source clusters of a sub-phrase-table, but the same
method applies for the target side and for all sub-
phrase-tables in the collection C.
3.1 Monolingual graphs
Each source cluster is converted into a graph G as
follows: The vertex set consists of the phrases of
the cluster and an edge between s and s? exists, if
(a) s and s? have at least one translation from the
same target cluster, and (b) idf(s, s?) is greater
than some threshold ? (see Section 4.1). If two
phrases that satisfy condition (b) and have trans-
lations in more than one common target cluster,
a distinct such edge is established. All edges are
bi-directional with distinct weights for both direc-
tions.
Figure 3 depicts an example of such a construc-
tion; a link between a phrase si and a target cluster
implies the existence of at least one translation for
si in that cluster. We are not interested in the tar-
get phrases and they are thus not shown. For sim-
plicity we assume that condition (b) is always sat-
isfied and the extracted graph contains the maxi-
mum possible edges. Observe that phrases s3 and
s4 have two edges connecting them, (due to tar-
get clusters Tc and Td) and that the target cluster
Ta is irrelevant to the construction of the graph,
since s1 is the only phrase with translations in it.
This conversion of a source cluster into a graph G
  
s1 s2 s4 s5s3 s8s7s6Ta Tb Tc Td Te Tf
s2
s1 s3 s4s5 s6
s7 s8
Figure 3: Top: A source cluster containing
phrases s1,..., s8 and the associated target clusters
Ta,..., Tf . Bottom: The extracted graph from the
source cluster. All edges are bi-directional.
results in the formation of subgraphs in G, where
each subgraph is generated by a target cluster. In
general, if condition (b) is not always satisfied,
then G need not be connected and each connected
component is treated as a distinct graph.
Analogous to KB, we introduce feature vertices
to G: For each phrase vertex s, its part-of-speech
(POS) tag sequence and stem sequence are iden-
tified and inserted into G as new vertices with
bi-directional weighted edges connected to s. If
phrase vertices s and s? have the same POS tag se-
quence, then they are connected to the same POS
tag feature vertex. Similarly for stem feature ver-
tices. See Figure 4 for an example. Note that we
do not allow edges between POS tag and stem fea-
5
  
s124534
876
Tab Tabcd8e
f?53??
??cd8e
????87?
f?53?
???87?
Figure 4: Adding feature vertices to the extracted
graph (has) ?? (owns) ?? (i have) ?? (i had).
Phrase, POS tag feature and stem feature ver-
tices are drawn in circles, dotted rectangles and
solid rectangles respectively. All edges are bi-
directional.
ture vertices. The purpose of the feature vertices,
unlike KB, is primarily for smoothing and secon-
darily for identifying paraphrases with the same
syntactic information and this will become clear
in the description of the computation of weights.
The set of all phrase vertices that are adja-
cent to s is written as ?(s), and referred to
as the neighborhood of s. Let n(s, t) denote
the co-occurrence count of a phrase-table entry
(s, t) (Koehn, 2009). We define the strength of
s in the subgraph generated by cluster T as
n(s;T ) =
?
t?T
n(s, t), (2)
which is simply a partial occurrence count for s.
We proceed with computing weights for all edges
of G:
Phrase??phrase weights: Inspired by the
notion of preferential attachment (Yule, 1925),
which is known to produce power-law weight dis-
tributions for evolving weighted networks (Barrat
et al 2004), we set the weight of a directed
edge from s to s? to be proportional to the
strengths of s? in all subgraphs in which both
s and s? are members. Thus, in the random
walk framework, s is more likely to visit
a stronger (more reliable) neighbor. If Ts,s? =
{T |s and s? coexist in subgraph generated by T},
then the weight w(s ? s?) of the directed edge
from s to s? is given by
w(s ? s?) =
?
T?Ts,s?
n(s?;T ), (3)
if s? ? ?(s) and 0 otherwise.
Phrase??feature weights: As mentioned
above, feature vertices have the dual role of car-
rying syntactic information and smoothing. From
eqn. (3) it can be deduced that, if for a phrase
s, the amount of its outgoing weights is close to
the amount of its incoming weights, then this is
an indication that at least a significant part of its
neighborhood is reliable; the larger the strengths,
the more certain the indication. Otherwise, either
s or a significant part of its neighborhood is
unreliable. The amount of weight from s to its
feature vertices should depend on this observation
and we thus let
net(s) =
?
?
?
?
?
?
?
s???(s)
(w(s ? s?)? w(s? ? s))
?
?
?
?
?
?
+ ,
(4)
where  prevents net(s) from becoming 0 (see
Section 4.1). The net weight of a phrase vertex
s is distributed over its feature vertices as
w(s ? fX) =< w(s ? s
?) > +net(s), (5)
where the first summand is the average weight
from s to its neighboring phrase vertices and
X = POS,STEM. If s has multiple POS tag
sequences, we distribute the weight of eqn. (5)
relatively to the co-occurrences of s with the re-
spective POS tag feature vertices. The quantity
< w(s ? s?) > accounts for the basic smoothing
and is augmented by a value net(s) that measures
the reliability of s?s neighborhood; the more unre-
liable the neighborhood, the larger the net weight
and thus larger the overall weights to the feature
vertices.
The choice for the opposite direction is trivial:
w(fX ? s) =
1
|{s? : (fX , s?) is an edge }|
, (6)
where X = POS,STEM. Note the effect of
eqns. (4)?(6) in the case where the neighborhood
of s has unreliable strengths: In a random walk
the feature vertices of s will be preferred and the
resulting similarities between s and other phrase
vertices will be small, as desired. Nonetheless,
if the syntactic information is the same with any
other phrase vertex inG, then the paraphrases will
be captured.
The transition probability from any vertex u to
any other vertex v in G, i.e., the probability of
6
hopping from u to v in one step, is given by
p(u ? v) =
w(u ? v)
?
v? w(u ? v
?)
, (7)
where we sum over all vertices adjacent to u inG.
We can thus compute the similarity between any
two vertices u and v in G by their commute time,
i.e., the expected number of steps in a round trip,
in a random walk from u to v and then back to u,
which is denoted by ?(u, v) (see Section 4.1 for
the method of computation of ?). Since ?(u, v) is
a distance measure, the smaller its value, the more
similar u and v are.
3.2 Counts
We convert the distance ?(u, v) of a vertex pair
u, v in a graph G into a co-occurrence count
nG(u, v) with a novel technique: In order to as-
sess the quality of the pair u, v with respect to G
we compare ?(u, v) with ?(u, x) and ?(v, x) for
all other vertices x in G. We thus consider the av-
erage distance of u with the other vertices of G
other than v, and similarly for v. This quantity is
denoted by ?(u; v) and ?(v;u) respectively, and
by definition it is given by
?(i; j) =
?
x?G
x 6=j
?(i, x)pG(x|i) (8)
where pG(x|i) ? p(x|G, i) is a yet unknown
probability distribution with respect to G. The
quantity (?(u; v)+?(v;u))/2 can then be viewed
as the average distance of the pair u, v to the rest
of the graph G. The co-occurrence count of u and
v in G is thus defined by
nG(u, v) =
?(u; v) + ?(v;u)
2?(u, v)
. (9)
In order to calculate the probabilities pG(?|?) we
employ the following heuristic: Starting with a
uniform distribution p(0)G (?|?) at timestep t = 0,
we iterate
?(t)(i; j) =
?
x?G
x 6=j
?(i, x)p(t)G (x|i) (10)
n(t)G (u, v) =
?(t)(u; v) + ?(t)(v;u)
2?(u, v)
(11)
p(t+1)G (v|u) =
n(t)G (u, v)
?
x?G n
(t)
G (u, v)
(12)
for all pairs of vertices u, v in G until conver-
gence. Experimentally, we find that convergence
is always achieved. After the execution of this it-
erative process we divide each count by the small-
est count in order to achieve a lower bound of 1.
A pair u, v may appear in multiple graphs in the
same sub-phrase-tableC. The total co-occurrence
count of u and v in C and the associated condi-
tional probabilities are thus given by
nC(u, v) =
?
G?C
nG(u, v) (13)
pC(v|u) =
nC(u, v)
?
x?C nC(u, x)
. (14)
A pair u, v may appear in multiple sub-phrase-
tables and for the calculation of the final count
n(u, v) we need to average over the associated
counts from all sub-phrase-tables. Moreover, we
have to take into account the type of the vertices:
For the simplest case where both u and v repre-
sent phrase vertices, their expected count is, by
definition, given by
n(s, s?) =
?
C
nC(s, s
?)p(C|s, s?). (15)
On the other hand, if at least one of u or v is
a feature vertex, then we have to consider the
phrase vertex that generates this feature: Suppose
that u is the phrase vertex s=?acquire? and v the
POS tag vertex f=?NN? and they co-occur in two
sub-phrase-tables C and C ? with positive counts
nC(s, f) and nC?(s, f) respectively; the feature
vertex f is generated by the phrase vertices ?own-
ership? in C and by ?possession? in C ?. In that
case, an interpolation of the counts nC(s, f) and
nC?(s, f) as in eqn. (15) would be incorrect and
a direct sum nC(s, f) + nC?(s, f) would provide
the true count. As a result we have
n(s, f) =
?
s?
?
C
nC(s, f(s
?))p(C|s, f(s?)),
(16)
where the first summation is over all phrase ver-
tices s? such that f(s?) = f . With a similar argu-
ment we can write
n(f, f ?) =
?
s,s?
?
C
nC(f(s), f(s
?))?
? p(C|f(s), f(s?)). (17)
7
For the interpolants, from standard probability we
find
p(C|u, v) =
pC(v|u)p(C|u)
?
C? pC?(v|u)p(C
?|u)
, (18)
where the probabilities p(C|u) can be computed
by considering the likelihood function
`(u) =
N?
i=1
p(xi|u) =
N?
i=1
?
C
pC(xi|u)p(C|u)
and by maximizing the average log-likelihood
1
N log `(u), where N is the total number of ver-
tices with which u co-occurs with positive counts
in all sub-phrase-tables.
Finally, the desired probability distributions are
given by the relative frequencies
p(v|u) =
n(u, v)
?
x n(u, x)
, (19)
for all pairs of vertices u, v.
4 Experiments
4.1 Setup
The data for building the phrase-table P
is drawn from DE-EN bitexts crawled from
www.project-syndicate.org, which is
a standard resource provider for the WMT
campaigns (News Commentary bitexts, see,
e.g. (Callison-Burch et al 2007) ). The filtered
bitext consists of 125K sentences; word align-
ment was performed running GIZA++ in both di-
rections and generating the symmetric alignments
using the ?grow-diag-final-and? heuristics. The
resulting P has 7.7M entries, 30% of which are
?1-1?, i.e. entries (s, t) that satisfy p(s|t) =
p(t|s) = 1. These entries are irrelevant for para-
phrase harvesting for both the baseline and our
method, and are thus excluded from the process.
The initial giant component P0 contains 1.7M
vertices (Figure 5), of which 30% become
residues and are used to construct R. At each it-
eration of the decomposition of a giant compo-
nent, we remove the top 0.5% ? size cut-vertices
ranked by degree of connectivity, where size is
the number of vertices of the giant component and
set ? = 2500 as the stopping criterion. The latter
choice is appropriate for the subsequent step of
co-clustering the components, for both time com-
plexity and performance of the ITC algorithm.
100 102 104 106100
101102
103104
105106
107
rank
size 100 102 104 106100
105P0
Figure 5: Log-log plot of ranked components ac-
cording to their size (number of source and target
phrases) for: (a) Components extracted from P .
?1-1? components are not shown. (b) Components
extracted from the decomposition of P0.
In the components emerging from the decompo-
sition of R0, we observe an excessive number
of cut-vertices. Note that vertices that consist
these components can be of two types: i) for-
mer residues, i.e., residues that emerged from the
decomposition of P0, and ii) other vertices of
P0. Cut-vertices can be of either type. For each
component, we remove cut-vertices that are not
translations of the former residues of that com-
ponent. Following this pruning strategy, the de-
generacy of excessive cut-vertices does not reap-
pear in the subsequent iterations of decompos-
ing components generated by new residues, but
the emergence of two giant components was ob-
served: One consisting mostly of source type ver-
tices and one of target type vertices. Without go-
ing into further details, the algorithm can extend
to multiple giant components straightforwardly.
For the merging process of the collection D we
set ? = 5000, to avoid the emergence of a giant
component. The sizes of the resulting sub-phrase-
tables are shown in Figure 6. For the ITC algo-
rithm we use the smoothing technique discussed
in (Dhillon and Guan, 2003) with ? = 106.
For the monolingual graphs, we set ? = 0.65
and discard graphs with more than 20 phrase ver-
tices, as they contain mostly noise. Thus, the sizes
of the graphs allow us to use analytical methods
to compute the commute times: For a graph G,
we form the transition matrix P , whose entries
P (u, v) are given by eqn. (7), and the fundamen-
8
100 102 104 106100
101102
103104
105106
rank
size
 
 
before mergingafter merging
Figure 6: Log-log plot of ranked sub-phrase-
tables according to their size (number of source
and target phrases).
tal matrix (Grinstead and Snell, 2006; Boley et al
2011) Z = (I?P +1piT )?1, where I is the iden-
tity matrix, 1 denotes the vector of all ones and pi
is the vector of stationary probabilities (Aldous
and Fill, 2001) which is such that piTP = piT
and piT1 = 1 and can be computed as in (Hunter,
2000). The commute time between any vertices u
and v in G is then given by (Grinstead and Snell,
2006)
?(u, v) = (Z(v, v)? Z(u, v))/pi(v) +
+ (Z(u, u)? Z(v, u))/pi(u). (20)
For the parameter of eqn. (4), an appropriate
choice is  = |?(s)| + 1; for reliable neighbor-
hoods, this quantity is insignificant. POS tags and
lemmata are generated with TreeTagger1.
Figure 7 depicts the most basic type of graph
that can be extracted from a cluster; it includes
two source phrase vertices a, b, of different syn-
tactic information. Suppose that both a and
b are highly reliable with strengths n(a;T ) =
n(b;T ) = 40, for some target cluster T . The re-
sulting conditional probabilities adequately repre-
sent the proximity of the involved vertices. On
the other hand, the range of the co-occurrence
counts is not compatible with that of the strengths.
This is because i) there are no phrase vertices with
small strength in the graph, and ii) eqn. (9) is es-
sentially a comparison between a pair of vertices
and the rest of the graph. To overcome this prob-
lem inflation vertices ia and ib of strength 1 with
accompanying feature vertices are introduced to
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
the graph. Figure 8 depicts the new graph, where
the lengths of the edges represent the magnitude
of commute times. Observe that the quality of
the probabilities is preserved but the counts are
inflated, as required.
In general, if a source phrase vertex s has at
least one translation t such that n(s, t) ? 3, then a
triplet (is, f(is), g(is)) is added to the graph as in
Figure 8. The inflation vertex is establishes edges
with all other phrase and inflation vertices in the
graph and weights are computed as in Section 3.1.
The pipeline remains the same up to eqn. (13),
where all counts that include inflation vertices are
ignored.
  
G st =G{c=G{
t =s {
c=s {r =s}G{ 0 123r =t =G{}G{ 0 124r=c =G{}G{ 0 124r =t =s {}G{ 0 1?r=c=s {}G{ 0 1?
? =G? s { 0 213? =G? t =G{{ 0 21?? =G? c=G{{ 0 21?? =G? t =s {{ 0 ??? =G? c =s {{ 0 ??
Figure 7: Top: A graph with source phrase ver-
tices a and b, both of strength 40, with accom-
panying distinct POS sequence vertices f(?) and
stem sequence vertices g(?). Bottom: The result-
ing co-occurrence counts and conditional proba-
bilities for a.
  
G=s{t } 0 122G=c=t }{t } 0 123G=r =t }{t } 0 123G=c=s }{t } 0 14?G=r =s }{t } 0 14?
tsc=t }c=s }
r =t }
r =s }
? t? s c=? t }
r =? t }
c=? s }r =? s }? =t ? s } 0 441?? =t ? c=t }} 0 4?1?? =t ? r =t }} 0 4?1?? =t ? c=s }} 0 31?? =t ? r =s }} 0 31?
Figure 8: The inflated version of Figure 7.
9
4.2 Results
Our method generates conditional probabilities
for any pair chosen from {phrase, POS sequence,
stem sequence}, but for this evaluation we restrict
ourselves to phrase pairs. For a phrase s, the qual-
ity of a paraphrase s? is assessed by
P (s?|s) ? p(s?|s) + p(f1(s
?)|s) + p(f2(s
?)|s),
(21)
where f1(s?) and f2(s?) denote the POS tag se-
quence and stem sequence of s? respectively. All
three summands of eqn. (21) are computed from
eqn. (19). The baseline is given by pivoting (Ban-
nard and Callison-Burch, 2005),
P (s?|s) =
?
t
p(t|s)p(s?|t), (22)
where p(t|s) and p(s?|t) are the phrase-based rel-
ative frequencies of the translation model.
We select 150 phrases (an equal number for
unigrams, bigrams and trigrams), for which we
expect to see paraphrases, and keep the top-10
paraphrases for each phrase, ranked by the above
measures. We follow (Kok and Brockett, 2010;
Metzler et al 2011) in the evaluation of the ex-
tracted paraphrases: Each phrase-paraphrase pair
is manually annotated with the following options:
0) Different meaning; 1) (i) Same meaning, but
potential replacement of the phrase with the para-
phrase in a sentence ruins the grammatical struc-
ture of the sentence. (ii) Tokens of the paraphrase
are morphological inflections of the phrase?s to-
kens. 2) Samemeaning. Although useful for SMT
purposes, ?super/substrings of? are annotated with
0 to achieve an objective evaluation.
Both methods are evaluated in terms of the
Mean Expected Precision (MEP) at k; the Ex-
pected Precision for each selected phrase s at
rank k is computed by Es[p@k] = 1k
?k
i=1 pi,
where pi is the proportion of positive annotations
for item i. The desired metric is thus given by
MEP@k = 1150
?
s Es[p@k]. The contribution
to pi can be restricted to perfect paraphrases only,
which leads to a strict strategy for harvesting para-
phrases. Table 1 summarizes the results of our
evaluation and
we deduce that our method can lead to improve-
ments over the baseline.
An important accomplishment of our method
is that the distribution of counts n(u, v), (as given
Method
Lenient MEP Strict MEP
@1 @5 @10 @1 @5 @10
Baseline .58 .47 .41 .43 .33 .28
Graphs .72 .61 .52 .53 .40 .33
Table 1: Mean Expected Precision (MEP) at k un-
der lenient and strict evaluation criteria.
by eqns. (15)?(17)) for all vertices u and v, be-
longs to the power-law family (Figure 9). This is
evidence that the monolingual graphs can simu-
late the phrase extraction process of a monolin-
gual parallel corpus. Intuitively, we may think of
the German side of the DE?EN parallel corpus as
the ?English? approximation to a ?EN??EN par-
allel corpus, and the monolingual graphs as the
word alignment process.
100 102 104 106 108100
101
102
103
104
105
rank
co?o
ccur
renc
e co
unt
Figure 9: Log-log plot of ranked pairs of English
vertices according to their counts
5 Conclusions & Future Work
We have described a new method that harvests
paraphrases from a bitext, generates artificial
co-occurrence counts for any pair chosen from
{phrase, POS sequence, stem sequence}, and po-
tentially identifies patterns for the syntactic infor-
mation of the phrases. The quality of the para-
phrases? ranked lists outperforms that of a stan-
dard baseline. The quality of the resulting condi-
tional probabilities is promising and will be eval-
uated implicitly via an application to SMT.
This research was funded by the European
Commission through the CoSyne project FP7-
ICT- 4-248531.
10
References
David Aldous and James A. Fill. 2001. Reversible
Markov Chains and Random Walks on Graphs.
http://www.stat.berkeley.edu/?aldous/RWG/
book.html
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual En-
tailment Methods. Journal of Artificial Intelligence
Research, 38:135?187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proc.
ACL, pp. 597?604.
Alain Barrat, Marc Barthlemy, and Alessandro Vespig-
nani. 2004. Modeling the Evolution of Weighted
Networks. Phys. Rev. Lett., 92.
Daniel Boley, Gyan Ranjan, and Zhi-Li Zhang. 2011.
Commute Times for a Directed Graph using an
Asymmetric Laplacian. Linear Algebra and its Ap-
plications, Issue 2, pp. 224?242.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora.
Proc. EMNLP, pp. 196?205.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007
(Meta-) Evaluation of Machine Translation. Proc.
Workshop on Statistical Machine Translation, pp.
136?158.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006 Improved statistical machine trans-
lation using paraphrases. Proc. HLT/NAACL, pp.
17?24.
Inderjit S. Dhillon and Yuqiang Guan. 2003. Informa-
tion Theoretic Clustering of Sparse Co-Occurrence
Data. Proc. IEEE Int?l Conf. Data Mining, pp. 517?
520.
Inderjit S. Dhillon, Subramanyam Mallela, and Dhar-
mendra S. Modha. 2003. Information-Theoretic
Coclustering. Proc. ACM SIGKDD Int?l Conf.
Knowledge Discovery and Data Mining, pp. 89?98.
William Dolan, Chris Quirk, and Chris Brockett.
2004. Unsupervised construction of large para-
phrase corpora: Exploiting massively parallel news
sources. Proc. COLING, pp. 350-356.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme 2011. Learn-
ing Sentential Paraphrases from Bilingual Paral-
lel Corpora for Text-to-Text Generation. Proc.
EMNLP, pp. 1168?1179.
Charles Grinstead and Laurie Snell. 2006. Introduc-
tion to Probability. Second ed., American Mathe-
matical Society.
Jeffrey J. Hunter. 2000. A Survey of Generalized In-
verses and their Use in Stochastic Modelling. Res.
Lett. Inf. Math. Sci., Vol. 1, pp. 25?36.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press, Cambridge, UK.
Stanley Kok and Chris Brockett. 2010. Hitting the
Right Paraphrases in Good Time. Proc. NAACL,
pp.145?153.
Roland Kuhn, Boxing Chen, George Foster, and Evan
Stratford. 2010. Phrase Clustering for Smoothing
TM Probabilities: or, how to Extract Paraphrases
from Phrase Tables. Proc. COLING, pp.608?616.
Nitin Madnani and Bonnie Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of
Data-Driven Methods. Computational Linguistics,
36(3):341?388.
Donald Metzler, Eduard Hovy, and Chunliang
Zhang. 2011. An Empirical Evaluation of Data-
Driven Paraphrase Generation Techniques. Proc.
ACL:Short Papers, pp. 546?551.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase Lattice for Statistical Machine
Translation. Proc. ACL:Short Papers, pp. 1?5.
Delip Rao, David Yarowsky, and Chris Callison-
Burch. 2008. Affinity Measures based on the Graph
Laplacian. Proc. Textgraphs Workshop on Graph-
based Algorithms for NLP at COLING, pp. 41?48.
George U. Yule. 1925. A Mathematical Theory of
Evolution, based on the Conclusions of Dr. J. C.
Willis, F.R.S. Philos. Trans. R. Soc. London, B 213,
pp. 21?87.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. Proc. ACL, pp.
780?788.
11
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 109?119,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Adaptation of Statistical Machine Translation Model for Cross-Lingual
Information Retrieval in a Service Context
Vassilina Nikoulina
Xerox Research Center Europe
vassilina.nikoulina@xrce.xerox.com
Bogomil Kovachev
Informatics Institute
University of Amsterdam
B.K.Kovachev@uva.nl
Nikolaos Lagos
Xerox Research Center Europe
nikolaos.lagos@xrce.xerox.com
Christof Monz
Informatics Institute
University of Amsterdam
C.Monz@uva.nl
Abstract
This work proposes to adapt an existing
general SMT model for the task of translat-
ing queries that are subsequently going to
be used to retrieve information from a tar-
get language collection. In the scenario that
we focus on access to the document collec-
tion itself is not available and changes to
the IR model are not possible. We propose
two ways to achieve the adaptation effect
and both of them are aimed at tuning pa-
rameter weights on a set of parallel queries.
The first approach is via a standard tuning
procedure optimizing for BLEU score and
the second one is via a reranking approach
optimizing for MAP score. We also extend
the second approach by using syntax-based
features. Our experiments show improve-
ments of 1-2.5 in terms of MAP score over
the retrieval with the non-adapted transla-
tion. We show that these improvements are
due both to the integration of the adapta-
tion and syntax-features for the query trans-
lation task.
1 Introduction
Cross Lingual Information Retrieval (CLIR) is an
important feature for any digital content provider
in today?s multilingual environment. However,
many of the content providers are not willing to
change existing well-established document index-
ing and search tools, nor to provide access to
their document collection by a third-party exter-
nal service. The work presented in this paper as-
sumes such a context of use, where a query trans-
lation service allows translating queries posed to
the search engine of a content provider into sev-
eral target languages, without requiring changes
to the undelying IR system used and without ac-
cessing, at translation time, the content provider?s
document set. Keeping in mind these constraints,
we present two approaches on query translation
optimisation.
One of the important observations done dur-
ing the CLEF 2009 campaign (Ferro and Peters,
2009) related to CLIR was that the usage of Sta-
tistical Machine Translation (SMT) systems (eg.
Google Translate) for query translation led to
important improvements in the cross-lingual re-
trieval performance (the best CLIR performance
increased from ?55% of the monolingual baseline
in 2008 to more than 90% in 2009 for French
and German target languages). However, general-
purpose SMT systems are not necessarily adapted
for query translation. That is because SMT sys-
tems trained on a corpus of standard parallel
phrases take into account the phrase structure im-
plicitly. The structure of queries is very differ-
ent from the standard phrase structure: queries are
very short and the word order might be different
than the typical full phrase one. This problem can
be seen as a problem of genre adaptation for SMT,
where the genre is ?query?.
To our knowledge, no suitable corpora of par-
allel queries is available to train an adapted SMT
system. Small corpora of parallel queries1 how-
ever can be obtained (eg. CLEF tracks) or man-
ually created. We suggest to use such corpora
in order to adapt the SMT model parameters for
query translation. In our approach the parameters
of the SMT models are optimized on the basis of
the parallel queries set. This is achieved either di-
rectly in the SMT system using the MERT (Mini-
mum Error Rate Training) algorithm and optimiz-
1Insufficient for a full SMT system training (?500 entries)
109
ing according to the BLEU2(Papineni et al 2001)
score, or via reranking the Nbest translation can-
didates generated by a baseline system based on
new parameters (and possibly new features) that
aim to optimize a retrieval metric.
It is important to note that both of the pro-
posed approaches allow keeping the MT system
independent of the document collection and in-
dexing, and thus suitable for a query translation
service. These two approaches can also be com-
bined by using the model produced with the first
approach as a baseline that produces the Nbest list
of translations that is then given to the reranking
approach.
The remainder of this paper is organized as fol-
lows. We first present related work addressing the
problem of query translation. We then describe
two approaches towards adapting an SMT system
to the query-genre: tuning the SMT system on a
parallel set of queries (Section 3.1) and adapting
machine translation via the reranking framework
(Section 3.2). We then present our experimental
settings and results (Section 4) and conclude in
section 5.
2 Related work
We may distinguish two main groups of ap-
proaches to CLIR: document translation and
query translation. We concentrate on the second
group which is more relevant to our settings. The
standard query translation methods use different
translation resources such as bilingual dictionar-
ies, parallel corpora and/or machine translation.
The aspect of disambiguation is important for the
first two techniques.
Different methods were proposed to deal with
disambiguation issues, often relying on the docu-
ment collection or embedding the translation step
directly into the retrieval model (Hiemstra and
Jong, 1999; Berger et al 1999; Kraaij et al
2003). Other methods rely on external resources
like query logs (Gao et al 2010), Wikipedia (Ja-
didinejad and Mahmoudi, 2009) or the web (Nie
and Chen, 2002; Hu et al 2008). (Gao et al
2006) proposes syntax-based translation models
to deal with the disambiguation issues (NP-based,
dependency-based). The candidate translations
proposed by these models are then reranked with
the model learned to minimize the translation er-
2Standard MT evaluation metric
ror on the training data.
To our knowledge, existing work that use MT-
based techniques for query translation use an out-
of-the-box MT system, without adapting it for
query translation in particular (Jones et al 1999;
Wu et al 2008) (although some query expan-
sion techniques might be applied to the produced
translation afterwards (Wu and He, 2010)).
There is a number of works done for do-
main adaptation in Statistical Machine Transla-
tion. However, we want to distinguish between
genre and domain adaptation in this work. Gen-
erally, genre can be seen as a sub-problem of do-
main. Thus, we consider genre to be the general
style of the text e.g. conversation, news, blog,
query (responsible mostly for the text structure)
while the domain reflects more what the text is
about ? eg. social science, healthcare, history, so
domain adaptation involves lexical disambigua-
tion and extra lexical coverage problems. To our
knowledge, there is not much work addressing ex-
plicitly the problem of genre adaptation for SMT.
Some work done on domain adaptation could be
applied to genre adaptation, such as incorporating
available in-domain corpora in the SMT model:
either monolingual (Bertoldi and Federico, 2009;
Wu et al 2008; Zhao et al 2004; Koehn and
Schroeder, 2007), or small parallel data used for
tuning the SMT parameters (Zheng et al 2010;
Pecina et al 2011).
3 Our approach
This work is based on the hypothesis that the
general-purpose SMT system needs to be adapted
for query translation. Although in (Ferro and
Peters, 2009) it has been mentioned that using
Google translate (general-purpose MT) for query
translation allowed to CLEF participants to obtain
the best CLIR performance, there is still 10% gap
between monolingual and cross-lingual IR. We
believe that, as in (Clinchant and Renders, 2007),
more adapted query translation, possibly further
combined with query expansion techniques, can
lead to improved retrieval.
The problem of the SMT adaptation for query-
genre translation has different quality aspects.
On the one hand, we want our model to pro-
duce a ?good? translation (well-formed and trans-
mitting the information contained in the source
query) of an input query. On the other hand, we
want to obtain good retrieval performance using
110
the proposed translation. These two aspects are
not necessarily correlated: a bag-of-word transla-
tion can lead to good retrieval performance, even
though it won?t be syntactically well-formed; at
the same time a well-formed translation can lead
to worse retrieval if the wrong lexical choice is
done. Moreover, often the retrieval demands some
linguistic preprocessing (eg. lemmatisation, PoS
tagging) which in interaction with badly-formed
translations might bring some noise.
A couple of works studied the correlation be-
tween the standard MT evaluation metrics and
the retrieval precision. Thus, (Fujii et al 2009)
showed a good correlation of the BLEU scores
with the MAP scores for Cross-Lingual Patent
Retrieval. However, the topics in patent search
(long and well structured) are very different from
standard queries. (Kettunen, 2009) also found a
pretty high correlation ( 0.8 ? 0.9) between stan-
dard MT evaluation metrics (METEOR(Banerjee
and Lavie, 2005), BLEU, NIST(Doddington,
2002)) and retrieval precision for long queries.
However, the same work shows that the correla-
tion decreases ( 0.6? 0.7) for short queries.
In this paper we propose two approaches to
SMT adaptation for queries. The first one op-
timizes BLEU, while the second one optimizes
Mean Average Precision (MAP), a standard met-
ric in information retrieval. We?ll address the is-
sue of the correlation between BLEU and MAP in
Section 4.
Both of the proposed approaches rely on the
phrase-based SMT (PBMT) model (Koehn et al
2003) implemented in the Open Source SMT
toolkit MOSES (Koehn et al 2007).
3.1 Tuning for genre adaptation
First, we propose to adapt the PBMT model by
tuning the model?s weights on a parallel set of
queries. This approach addresses the first as-
pect of the problem, which is producing a ?good?
translation. The PBMT model combines differ-
ent types of features via a log-linear model. The
standard features include (Koehn, 2010, Chapter
5): language model, word penalty, distortion, dif-
ferent translation models, etc. The weights of
these features are learned during the tuning step
with the MERT (Och, 2003) algorithm. Roughly
the MERT algorithm tunes feature weights one by
one and optimizes them according to the BLEU
score obtained.
Our hypothesis is that the impact of different
features should be different depending on whether
we translate a full sentence, or a query-genre en-
try. Thus, one would expect that in the case
of query-genre the language model or the distor-
tion features should get less importance than in
the case of the full-sentence translation. MERT
tuning on a genre-adapted parallel corpus should
leverage this information from the data, adapting
the SMT model to the query-genre. We would
also like to note that the tuning approach (pro-
posed for domain adaptation by (Zheng et al
2010)) seems to be more appropriate for genre
adaptation than for domain adaptation where the
problem of lexical ambiguity is encoded in the
translation model and re-weighting the main fea-
tures might not be sufficient.
We use the MERT implementation provided
with the Moses toolkit with default settings. Our
assumption is that this procedure although not ex-
plicitly aimed at improving retrieval performance
will nevertheless lead to ?better? query transla-
tions when compared to the baseline. The results
of this apporach allow us also to observe whether
and to what extent changes in BLEU scores are
correlated to changes in MAP scores.
3.2 Reranking framework for query
translation
The second approach addresses the retrieval qual-
ity problem. An SMT system is usually trained to
optimize the quality of the translation (eg. BLEU
score for SMT), which is not necessarily corre-
lated with the retrieval quality (especially for the
short queries). Thus, for example, the word or-
der which is crucial for translation quality (and is
taken into account by most MT evaluation met-
rics) is often ignored by IR models. Our second
approach follows (Nie, 2010, pp.106) argument
that ?the translation problem is an integral part
of the whole CLIR problem, and unified CLIR
models integrating translation should be defined?.
We propose integrating the IR metric (MAP) into
the translation model optimisation step via the
reranking framework.
Previous attempts to apply the reranking ap-
proach to SMT did not show significant improve-
ments in terms of MT evaluation metrics (Och
et al 2003; Nikoulina and Dymetman, 2008).
One of the reasons being the poor diversity of the
Nbest list of the translations. However, we be-
111
lieve that this approach has more potential in the
context of query translation.
First of all the average query length is ?5 words,
which means that the Nbest list of the translations
is more diverse than in the case of general phrase
translation (average length 25-30 words).
Moreover, the retrieval precision is more natu-
rally integrated into the reranking framework than
standard MT evaluation metrics such as BLEU.
The main reason is that the notion of Average Re-
trieval Precision is well defined for a single query
translation, while BLEU is defined on the corpus
level and correlates poorly with human quality
judgements for the individual translations (Specia
et al 2009; Callison-Burch et al 2009).
Finally, the reranking framework allows a lot
of flexibility. Thus, it allows enriching the base-
line translation model with new complex features
which might be difficult to introduce into the
translation model directly.
Other works applied the reranking framework
to different NLP tasks such as Named Entities
Extraction (Collins, 2001), parsing (Collins and
Roark, 2004), and language modelling (Roark et
al., 2004). Most of these works used the reranking
framework to combine generative and discrimina-
tive methods when both approaches aim at solv-
ing the same problem: the generative model pro-
duces a set of hypotheses, and the best hypoth-
esis is chosen afterwards via the discriminative
reranking model, which allows to enrich the base-
line model with the new complex and heteroge-
neous features. We suggest using the reranking
framework to combine two different tasks: Ma-
chine Translation and Cross-lingual Information
Retrieval. In this context the reranking framework
doesn?t only allow enriching the baseline transla-
tion model but also performing training using a
more appropriate evaluation metric.
3.2.1 Reranking training
Generally, the reranking framework can be re-
sumed in the following steps :
1. The baseline (generic-purpose) MT system
generates a list of candidate translations
GEN(q) for each query q;
2. A vector of features F (t) is assigned to each
translation t ? GEN(q);
3. The best translation t? is chosen as the one
maximizing the translation score, which is
defined as a weighted linear combination of
features: t?(?) = argmaxt?GEN(q) ??F (t)
As shown above the best translation is selected ac-
cording to features? weights ?. In order to learn
the weights ? maximizing the retrieval perfor-
mance, an appropriate annotated training set has
to be created. We use the CLEF tracks to create
the training set. The retrieval scores annotations
are based on the document relevance annotations
performed by human annotators during the CLEF
campaign.
The annotated training set is created out of
queries {q1, ..., qK} with an Nbest list of trans-
lations GEN(qi) of each query qi, i ? {1..K} as
follows:
? A list of N (we take N = 1000) translations
(GEN(qi)) is produced by the baseline MT
model for each query qi, i = 1..K.
? Each translation t ? GEN(qi) is used
to perform a retrieval from a target docu-
ment collection, and an Average Precision
score (AP (t)) is computed for each t ?
GEN(qi) by comparing its retrieval to the
relevance annotations done during the CLEF
campaign.
The weights ? are learned with the objective of
maximizing MAP for all the queries of the train-
ing set, and, therefore, are optimized for retrieval
quality.
The weights optimization is done with
the Margin Infused Relaxed Algorithm
(MIRA)(Crammer and Singer, 2003), which
was applied to SMT by (Watanabe et al 2007;
Chiang et al 2008). MIRA is an online learning
algorithm where each weights update is done to
keep the new weights as close as possible to the
old weights (first term), and score oracle trans-
lation (the translation giving the best retrieval
score : t?i = argmaxtAP (t)) higher than each
non-oracle translation (tij) by a margin at least as
wide as the loss lij (second term):
? = min??
1
2??
?
? ??2 +
C
?K
i=1 maxj=1..N
(
lij ? ?
?
? (F (t?i )? F (tij)
)
The loss lij is defined as the difference in the re-
trieval average precision between the oracle and
non-oracle translations: lij = AP (t?i )?AP (tij).
C is the regularization parameter which is chosen
via 5-fold cross-validation.
112
3.2.2 Features
One of the advantages of the reranking frame-
work is that new complex features can be easily
integrated. We suggest to enrich the reranking
model with different syntax-based features, such
as:
? features relying on dependency structures:
called therein coupling features (proposed by
(Nikoulina and Dymetman, 2008));
? features relying on Part of Speech Tagging:
called therein PoS mapping features.
By integrating the syntax-based features we
have a double goal: showing the potential of
the reranking framework with more complex fea-
tures, and examining whether the integration of
syntactic information could be useful for query
translation.
Coupling features. The goal of the coupling
features is to measure the similarity between
source and target dependency structures. The ini-
tial hypothesis is that a better translation should
have a dependency structure closer to the one of
the source query.
In this work we experiment with two dif-
ferent coupling variants proposed in (Nikoulina
and Dymetman, 2008), namely, Lexicalised and
Label-specific coupling features.
The generic coupling features are based on
the notion of ?rectangles? that are of the follow-
ing type : ((s1, ds12, s2), (t1, dt12, t2)), where
ds12 is an edge between source words s1 and s2,
dt12 is an edge between target words t1 and t2,
s1 is aligned with t1 and s2 is aligned with t2.
Lexicalised features take into account the qual-
ity of lexical alignment, by weighting each rect-
angle (s1, s2, t1, t2) by a probability of align-
ing s1 to t1 and s2 to t2 (eg. p(s1|t1)p(s2|t2) or
p(t1|s1)p(t2|s2)).
The Label-Specific features take into account
the nature of the aligned dependencies. Thus, the
rectangles of the form ((s1, subj, s2), (t1, subj,
t2)) will get more weight than a rectangle ((s1,
subj, s2), (t1, nmod, t2)). The importance of
each ?rectangle? is learned on the parallel anno-
tated corpus by introducing a collection of Label-
Specific coupling features, each for a specific pair
of source label and target label.
PoS mapping features. The goal of the PoS
mapping features is to control the correspondence
of Part Of Speech Tags between an input query
and its translation. As the coupling features, the
PoS mapping features rely on the word align-
ments between the source sentence and its trans-
lation3. A vector of sparse features is introduced
where each component corresponds to a pair of
PoS tags aligned in the training data. We intro-
duce a generic PoS map variant, which counts a
number of occurrences of a specific pair of PoS
tags, and lexical PoS map variant, which weights
down these pairs by a lexical alignment score
(p(s|t) or p(t|s)).
4 Experiments
4.1 Experimental basis
4.1.1 Data
To simulate parallel query data we used trans-
lation equivalent CLEF topics. The data set used
for the first approach consists of the CLEF topic
data from the following years and tasks: AdHoc-
main track from 2000 to 2008; CLEF AdHoc-
TEL track 2008; Domain Specific tracks from
2000 to 2008; CLEF robust tracks 2007 and 2008;
GeoCLEf tracks 2005-2007. To avoid the issue of
overlapping topics we removed duplicates. The
created parallel queries set contained 500 ? 700
parallel entries (depending on the language pair,
Table 1) and was used for Moses parameters tun-
ing.
In order to create the training set for the rerank-
ing approach, we need to have access to the rele-
vance judgements. We didn?t have access to all
relevance judgements of the previously desribed
tracks. Thus we used only a subset of the previ-
ously extracted parallel set, which includes CLEF
2000-2008 topics from the AdHoc-main, AdHoc-
TEL and GeoCLEF tracks.
The number of queries obtained altogether is
shown in (Table 1).
4.1.2 Baseline
We tested our approaches on the CLEF AdHoc-
TEL 2009 task (50 topics). This task dealt
with monolingual and cross-lingual search in a
library catalog. The monolingual retrieval is
3This alignment can be either produced by a toolkit like
GIZA++(Och and Ney, 2003) or obtained directly by a sys-
tem that produced the Nbest list of the translations (Moses).
113
Language pair Number of queries
Total queries
En - Fr, Fr - En 470
En - De, De - En 714
Annotated queries
En - Fr, Fr - En 400
En - De, De - En 350
Table 1: Top: total number of parallel queries gathered
from all the CLEF tasks (size of the tuning set). Bot-
tom: number of queries extracted from the tasks for
which the human relevance judgements were availble
(size of the reranking training set).
performed with the lemur4 toolkit (Ogilvie and
Callan, 2001). The preprocessing includes lem-
matisation (with the Xerox Incremental Parser-
XIP (A??t-Mokhtar et al 2002)) and filtering out
the function words (based on XIP PoS tagging).
Table 2 shows the performance of the monolin-
gual retrieval model for each collection. The
monolingual retrieval results are comparable to
the CLEF AdHoc-TEL 2009 participants (Ferro
and Peters, 2009). Let us note here that it is not
the case for our CLIR results since we didn?t ex-
ploit the fact that each of the collections could ac-
tually contain the entries in a language other than
the official language of the collection.
The cross-lingual retrieval is performed as fol-
lows :
? the input query (eg. in English) is first trans-
lated into the language of the collection (eg.
German);
? this translation is used to search the target
collection (eg. Austrian National Library for
German ) .
The baseline translation is produced with
Moses trained on Europarl. Table 2 reports the
baseline performance both in terms of MT evalu-
ation metrics (BLEU) and Information Retrieval
evaluation metric MAP (Mean Average Preci-
sion).
The 1best MAP score corresponds to the case
when the single translation is proposed for the
retrieval by the query translation model. 5best
MAP score corresponds to the case when the 5
top translations proposed by the translation ser-
vice are concatenated and used for the retrieval.
4http://www.lemurproject.org/
The 5best retrieval can be seen as a sort of query
expansion, without accessing the document col-
lection or any external resources.
Given that the query length is shorter than for a
standard sentence, the 4-gramm BLEU (used for
standart MT evaluation) might not be able to cap-
ture the difference between the translations (eg.
English-German 4-gramm BLEU is equal to 0 for
our task). For that reason we report both 3- and
4-gramm BLEU scores.
Note, that the French-English baseline retrieval
quality is much better than the German-English.
This is probably due to the fact that our German-
English translation system doesn?t use any de-
coumpounding, which results into many non-
translated words.
4.2 Results
We performed the query-genre adaptation ex-
periments for English-French, French-English,
German-English and English-German language
pairs.
Ideally, we would have liked to combine the
two approaches we proposed: use the query-
genre-tuned model to produce the Nbest list
which is then reranked to optimize the MAP
score. However, it was not possible in our exper-
imental settings due to the small amount of train-
ing data available. We thus simply compare these
two approaches to a baseline approach and com-
ment on their respective performance.
4.2.1 Query-genre tuning approach
For the CLEF-tuning experiments we used the
same translation model and language model as for
the baseline (Europarl-based). The weights were
then tuned on the CLEF topics described in sec-
tion 4.1.1. We then tested the system obtained on
50 parallel queries from the CLEF AdHoc-TEL
2009 task.
Table 3 describes the results of the evalua-
tion. We observe consistent 1-best MAP improve-
ments, but unstable BLEU (3-gramm) (improve-
ments for English-German, and degradation for
other language pairs), although one would have
expected BLEU to be improved in this experi-
mental setting given that BLEU was the objective
function for MERT. These results, on one side,
confirm the remark of (Kettunen, 2009) that there
is a correlation (although low) between BLEU
and MAP scores. The unstable BLEU scores
114
MAP
MAP MAP BLEU BLEU
1-best 5-best 4-gramm 3-gramm
Monolingual IR Bilingual IR
English 0.3159
French-English 0.1828 0.2186 0.1199 0.1568
German-English 0.0941 0.0942 0.2351 0.2923
French 0.2386 English-French 0.1504 0.1543 0.2863 0.3423
German 0.2162 English-German 0.1009 0.1157 0.0000 0.1218
Table 2: Baseline MAP scores for monolingual and bilingual CLEF AdHoc TEL 2009 task.
MAP MAP BLEU BLEU
1-best 5-best 4-gramm 3-gramm
Fr-En 0.1954 0.2229 0.1062 0.1489
De-En 0.1018 0.1078 0.2240 0.2486
En-Fr 0.1611 0.1516 0.2072 0.2908
En-De 0.1062 0.1132 0.0000 0.1924
Table 3: BLEU and MAP performance on CLEF AdHoc TEL 2009 task for the genre-tuned model.
might also be explained by the small size of the
test set (compared to a standard test set of 1000
full-sentences).
Secondly, we looked at the weights of the fea-
tures both in the baseline model (Europarl-tuned)
and in the adapted model (CLEF-tuned), shown in
Table 4. We are unsure how suitable the sizes of
the CLEF tuning sets are, especially for the pairs
involving English and French. Nevertheless we
do observe and comment on some patterns.
For the pairs involving English and German
the distortion weight is much higher when tuning
with CLEF data compared to tuning with Europarl
data. The picture is reversed when looking at the
two pairs involving English and French. This is
to be expected if we interpret a high distortion
weight as follows: ?it is not encouraged to place
source words that are near to each other far away
from each other in the translation?. Indeed, the lo-
cal reorderings are much more frequent between
English and French (e.g. white house = maison
blanche), while the long-distance reorderings are
more typcal between English and German.
The word penalty is consistenly higher over all
pairs when tuning with CLEF data compared to
tuning with Europarl data. We could see an ex-
planation for this pattern in the smaller size of
the CLEF sentences if we interpret higher word
penalty as a preference for shorter translations.
This can be explained both with the smaller aver-
age size of the queries and with the specific query
structure: mostly content words and fewer func-
tion words when compared to the full sentence.
The language model weight is consistently
though not drastically smaller when tuning with
CLEF data. We suppose that this is due to the
fact that a Europarl-base language model is not
the best choice for translating query data.
4.2.2 Reranking approach
The reranking experiments include different
features combinations. First, we experiment with
the Moses features only in order to make this ap-
proach comparable with the first one. Secondly,
we compare different syntax-based features com-
binations, as described in section 3.2.2. Thus, we
compare the following reranking models (defined
by the feature set): moses, lex (lexical coupling
+ moses features), lab (label-specific coupling +
moses features), posmaplex (lexical PoS mapping
+ moses features ), lab-lex (label-specific cou-
pling + lexical coupling + moses features), lab-
lex-posmap (label-specific coupling + lexical cou-
pling features + generic PoS mapping). To reduce
the size of feature-functions vectors we take only
the 20 most frequent features in the training data
for Label-specific coupling and PoS mapping fea-
tures. The computation of the syntax features is
based on the rule-based XIP parser, where some
heuristics specific to query processing have been
integrated into English and French (but not Ger-
man) grammars (Brun et al 2012).
The results of these experiments are illustrated
115
Lng pair Tune set DW LM ?(f |e) lex(f |e) ?(e|f) lex(e|f) PP WP
Fr-En
Europarl 0.0801 0.1397 0.0431 0.0625 0.1463 0.0638 -0.0670 -0.3975
CLEF 0.0015 0.0795 -0.0046 0.0348 0.1977 0.0208 -0.2904 0.3707
De-En
Europarl 0.0588 0.1341 0.0380 0.0181 0.1382 0.0398 -0.0904 -0.4822
CLEF 0.3568 0.1151 0.1168 0.0549 0.0932 0.0805 0.0391 -0.1434
En-Fr
Europarl 0.0789 0.1373 0.0002 0.0766 0.1798 0.0293 -0.0978 -0.4002
CLEF 0.0322 0.1251 0.0350 0.1023 0.0534 0.0365 -0.3182 -0.2972
En-De
Europarl 0.0584 0.1396 0.0092 0.0821 0.1823 0.0437 -0.1613 -0.3233
CLEF 0.3451 0.1001 0.0248 0.0872 0.2629 0.0153 -0.0431 0.1214
Table 4: Feature weights for the query-genre tuned model. Abbreviations: DW - distortion weight, LM - language
model weight, PP - phrase penalty, WP - word penalty, ?-phrase translation probability, lex-lexical weighting.
Query Example MAP bleu1
Src1 Weibliche Ma?rtyrer
Ref Female Martyrs
T1 female martyrs 0.07 1
T2 Women martyr 0.4 0
Src 2 Genmanipulation am
Menschen
Ref Human Gene Manipula-
tion
T1 On the genetic manipula-
tion of people
0.044 0.167
T2 genetic manipulation of
the human being
0.069 0.286
Src 3 Arbeitsrecht in der Eu-
ropa?ischen Union
Ref European Union Labour
Laws
T1 Labour law in the Euro-
pean Union
0.015 0.5
T2 labour legislation in the
European Union
0.036 0.5
Table 5: Some examples of queries translations (T1:
baseline, T2: after reranking with lab-lex), MAP and
1-gramm BLEU scores for German-English.
in Figure 1. To keep the figure more readable,
we report only on 3-gramm BLEU scores. When
computing the 5best MAP score, the order in the
Nbest list is defined by a corresponding reranking
model. Each reranking model is illustrated by a
single horizontal red bar. We compare the rerank-
ing results to the baseline model (vertical line) and
also to the results of the first approach (yellow bar
labelled MERT:moses) on the same figure.
First, we remark that the adapted models
(query-genre tuning and reranking) outperform
the baseline in terms of MAP (1best and 5 best)
for French-English and German-English transla-
tions for most of the models. The only exception
is posmaplex model (based on PoS tagging) for
German which can be explained by the fact that
the German grammar used for query processing
was not adapted for queries as opposed to English
and French grammars. However, we do not ob-
serve the same tendency for BLEU score, where
only a few of the adapted models outperform the
baseline, which confirms the hypothesis of the
low correlation between BLEU and MAP scores
in these settings. Table 5 gives some examples of
the queries translations before (T1) and after (T2)
reranking. These examples also illustrate differ-
ent types of disagreement between MAP and 1-
gramm BLEU5 score.
The results for English-German and English-
French look more confusing. This can be partly
due to the more rich morphology of the target lan-
guages which may create more noise in the syn-
tax structure. Reranking however improves over
the 1-best MAP baseline for English-German, and
5-best MAP is also improved excluding the mod-
els involving PoS tagging for German (posmap,
posmaplex, lab-lex-posmap). The results for
English-French are more difficult to interpret. To
find out the reason of such a behavior, we looked
at the translations. We observed the following to-
kenization problem for French: the apostrophe is
systematically separated, e.g. ?d ? aujourd ? hui?.
This leads to both noisy pre-retrieval preprocess-
ing (eg. d is tagged as a NOUN) and noisy syntax-
based feature values, which might explain the un-
stable results.
Finally, we can see that the syntax-based fea-
tures can be beneficial for the final retrieval qual-
ity: the models with syntax features can outper-
form the model basd on the moses features only.
The syntax-based features leading to the most sta-
5The higher order BLEU scores are equal to 0 for most
of the individual translations.
116
Figure 1: Reranking results. The vertical line corresponds to the baseline scores. The lowest bar (MERT:moses,
in yellow): the results of the tuning approach, other bars(in red): the results of the reranking approach.
ble results seem to be lab-lex (combination of lex-
ical and label-specific coupling): it leads to the
best gains over 1-best and 5-best MAP for all lan-
guage pairs excluding English-French. This is a
surprising result given the fact that the underlying
IR model doesn?t take syntax into account in any
way. In our opinion, this is probably due to the
interaction between the pre-retrieval preprocess-
ing (lemmatisation, PoS tagging) done with the
linguistic tools which might produce noisy results
when applied to the SMT outputs. The rerank-
ing with syntax-based features allows to choose
a better-formed query for which the PoS tagging
and lemmatisation tools produce less noise which
leads to a better retrieval.
5 Conclusion
In this work we proposed two methods for query-
genre adaptation of an SMT model: the first
method addressing the translation quality aspect
and the second one the retrieval precision aspect.
We have shown that CLIR performance in terms
of MAP is improved between 1-2.5 points. We
believe that the combination of these two meth-
ods would be the most beneficial setting, although
we were not able to prove this experimentally
(due to the lack of training data). None of these
methods require access to the document collec-
tion at test time, and can be used in the context
of a query translation service. The combination
of our adapted SMT model with other state-of-the
art CLIR techniques (eg. query expansion with
PRF) will be explored in future work.
Acknowledgements
This research was supported by the European
Union?s ICT Policy Support Programme as part of
the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430 (Project GALATEAS).
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond shallowness: in-
117
cremental deep parsing. Natural Language Engi-
neering, 8:121?144, June.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
an automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Adam Berger, John Lafferty, and John La Erty. 1999.
The weaver system for document retrieval. In In
Proceedings of the Eighth Text REtrieval Confer-
ence (TREC-8, pages 163?174.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 182?189. Association for Computa-
tional Linguistics.
Caroline Brun, Vassilina Nikoulina, and Nikolaos La-
gos. 2012. Linguistically-adapted structural query
annotation for digital libraries in the social sciences.
In Proceedings of the 6th EACL Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities, Avignon, France, April.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 224?233. Association
for Computational Linguistics.
Ste?phane Clinchant and Jean-Michel Renders. 2007.
Query translation through dictionary adaptation. In
CLEF?07, pages 182?187.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics.
Michael Collins. 2001. Ranking algorithms for
named-entity extraction: boosting and the voted
perceptron. In ACL?02: Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 489?496, Philadelphia, Pennsyl-
vania. Association for Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
George Doddington. 2002. Automatic evaluation
of Machine Translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, pages 138?145, San Diego,
California. Morgan Kaufmann Publishers Inc.
Nicola Ferro and Carol Peters. 2009. CLEF 2009
ad hoc track overview: TEL and persian tasks.
In Working Notes for the CLEF 2009 Workshop,
Corfu, Greece.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2009. Evaluating effects of ma-
chine translation accuracy on cross-lingual patent
retrieval. In Proceedings of the 32nd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, SIGIR ?09, pages
674?675.
Jianfeng Gao, Jian-Yun Nie, and Ming Zhou. 2006.
Statistical query translation models for cross-
language information retrieval. 5:323?359, Decem-
ber.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Kam-
Fai Wong, and Hsiao-Wuen Hon. 2010. Exploit-
ing query logs for cross-lingual query suggestions.
ACM Trans. Inf. Syst., 28(2).
Djoerd Hiemstra and Franciska de Jong. 1999. Dis-
ambiguation strategies for cross-language informa-
tion retrieval. In Proceedings of the Third European
Conference on Research and Advanced Technology
for Digital Libraries, pages 274?293.
Rong Hu, Weizhu Chen, Peng Bai, Yansheng Lu,
Zheng Chen, and Qiang Yang. 2008. Web query
translation via web log mining. In Proceedings of
the 31st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?08, pages 749?750. ACM.
Amir Hossein Jadidinejad and Fariborz Mahmoudi.
2009. Cross-language information retrieval us-
ing meta-language index construction and structural
queries. In Proceedings of the 10th cross-language
evaluation forum conference on Multilingual in-
formation access evaluation: text retrieval experi-
ments, CLEF?09, pages 70?77, Berlin, Heidelberg.
Springer-Verlag.
Gareth Jones, Sakai Tetsuya, Nigel Collier, Akira Ku-
mano, and Kazuo Sumita. 1999. Exploring the
use of machine translation resources for english-
japanese cross-language information retrieval. In In
Proceedings of MT Summit VII Workshop on Ma-
chine Translation for Cross Language Information
Retrieval, pages 181?188.
Kimmo Kettunen. 2009. Choosing the best mt pro-
grams for clir purposes ? can mt metrics be help-
ful? In Proceedings of the 31th European Confer-
ence on IR Research on Advances in Information
Retrieval, ECIR ?09, pages 706?712, Berlin, Hei-
delberg. Springer-Verlag.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
118
?07, pages 224?227. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In ACL ?07: Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, pages 177?180. As-
sociation for Computational Linguistics.
Philip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Wessel Kraaij, Jian-Yun Nie, and Michel Simard.
2003. Embedding web-based statistical trans-
lation models in cross-language information re-
trieval. Computational Linguistiques, 29:381?419,
September.
Jian-yun Nie and Jiang Chen. 2002. Exploiting the
web as parallel corpora for cross-language informa-
tion retrieval. Web Intelligence, pages 218?239.
Jian-Yun Nie. 2010. Cross-Language Information Re-
trieval. Morgan & Claypool Publishers.
Vassilina Nikoulina and Marc Dymetman. 2008. Ex-
periments in discriminating phrase-based transla-
tions on the basis of syntactic coupling features. In
Proceedings of the ACL-08: HLT Second Workshop
on Syntax and Structure in Statistical Translation
(SSST-2), pages 55?60. Association for Computa-
tional Linguistics, June.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003.
Syntax for Statistical Machine Translation: Final
report of John Hopkins 2003 Summer Workshop.
Technical report, John Hopkins University.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Paul Ogilvie and James P. Callan. 2001. Experiments
using the lemur toolkit. In TREC.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation.
Pavel Pecina, Antonio Toral, Andy Way, Vassilis Pa-
pavassiliou, Prokopis Prokopidis, and Maria Gi-
agkou. 2011. Towards using web-crawled data for
domain adaptation in statistical machine translation.
In Proceedings of the 15th Annual Conference of
the European Associtation for Machine Translation,
pages 297?304, Leuven, Belgium. European Asso-
ciation for Machine Translation.
Brian Roark, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proceedings of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?04), July.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13th Annual Confer-
ence of the EAMT, page 28?35, Barcelona, Spain.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 764?773, Prague, Czech Republic.
Association for Computational Linguistics.
Dan Wu and Daqing He. 2010. A study of query
translation using google machine translation sys-
tem. Computational Intelligence and Software En-
gineering (CiSE).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics (Col-
ing2008), pages 993?100.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the 20th international conference on
Computational Linguistics, COLING ?04. Associ-
ation for Computational Linguistics.
Zhongguang Zheng, Zhongjun He, Yao Meng, and
Hao Yu. 2010. Domain adaptation for statisti-
cal machine translation in development corpus se-
lection. In Universal Communication Symposium
(IUCS), 2010 4th International, pages 2?7. IEEE.
119
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 356?366,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
User Edits Classification Using Document Revision Histories
Amit Bronner
Informatics Institute
University of Amsterdam
a.bronner@uva.nl
Christof Monz
Informatics Institute
University of Amsterdam
c.monz@uva.nl
Abstract
Document revision histories are a useful
and abundant source of data for natural
language processing, but selecting relevant
data for the task at hand is not trivial.
In this paper we introduce a scalable ap-
proach for automatically distinguishing be-
tween factual and fluency edits in document
revision histories. The approach is based
on supervised machine learning using lan-
guage model probabilities, string similar-
ity measured over different representations
of user edits, comparison of part-of-speech
tags and named entities, and a set of adap-
tive features extracted from large amounts
of unlabeled user edits. Applied to con-
tiguous edit segments, our method achieves
statistically significant improvements over
a simple yet effective edit-distance base-
line. It reaches high classification accuracy
(88%) and is shown to generalize to addi-
tional sets of unseen data.
1 Introduction
Many online collaborative editing projects such as
Wikipedia1 keep track of complete revision histo-
ries. These contain valuable information about the
evolution of documents in terms of content as well
as language, style and form. Such data is publicly
available in large volumes and constantly grow-
ing. According to Wikipedia statistics, in August
2011 the English Wikipedia contained 3.8 million
articles with an average of 78.3 revisions per ar-
ticle. The average number of revision edits per
month is about 4 million in English and almost 11
million in total for all languages.2
1http://www.wikipedia.org
2Average for the 5 years period between August 2006
and August 2011. The count includes edits by registered
Exploiting document revision histories has
proven useful for a variety of natural language
processing (NLP) tasks, including sentence com-
pression (Nelken and Yamangil, 2008; Yamangil
and Nelken, 2008) and simplification (Yatskar et
al., 2010; Woodsend and Lapata, 2011), informa-
tion retrieval (Aji et al 2010; Nunes et al 2011),
textual entailment recognition (Zanzotto and Pen-
nacchiotti, 2010), and paraphrase extraction (Max
and Wisniewski, 2010; Dutrey et al 2011).
The ability to distinguish between factual
changes or edits, which alter the meaning, and flu-
ency edits, which improve the style or readability,
is a crucial requirement for approaches exploit-
ing revision histories. The need for an automated
classification method has been identified (Nelken
and Yamangil, 2008; Max and Wisniewski, 2010),
but to the best of our knowledge has not been di-
rectly addressed. Previous approaches have either
applied simple heuristics (Yatskar et al 2010;
Woodsend and Lapata, 2011) or manual annota-
tions (Dutrey et al 2011) to restrict the data to
the type of edits relevant to the NLP task at hand.
The work described in this paper shows that it is
possible to automatically distinguish between fac-
tual and fluency edits. This is very desirable as
it does not rely on heuristics, which often gener-
alize poorly, and does not require manual anno-
tation beyond a small collection of training data,
thereby allowing for much larger data sets of re-
vision histories to be used for NLP research.
In this paper, we make the following novel con-
tributions:
We address the problem of automated classi-
fication of user edits as factual or fluency edits
users, anonymous users, software bots and reverts. Source:
http://stats.wikimedia.org.
356
by defining the scope of user edits, extracting a
large collection of such user edits from the En-
glish Wikipedia, constructing a manually labeled
dataset, and setting up a classification baseline.
A set of features is designed and integrated into
a supervised machine learning framework. It is
composed of language model probabilities and
string similarity measured over different represen-
tations, including part-of-speech tags and named
entities. Despite their relative simplicity, the fea-
tures achieve high classification accuracy when
applied to contiguous edit segments.
We go beyond labeled data and exploit large
amounts of unlabeled data. First, we demonstrate
that the trained classifier generalizes to thousands
of examples identified by user comments as spe-
cific types of fluency edits. Furthermore, we in-
troduce a new method for extracting features from
an evolving set of unlabeled user edits. This
method is successfully evaluated as an alternative
or supplement to the initial supervised approach.
2 Related Work
The need for user edits classification is implicit in
studies of Wikipedia edit histories. For example,
Viegas et al(2004) use revision size as a simpli-
fied measure for the change of content, and Kittur
et al(2007) use metadata features to predict user
edit conflicts.
Classification becomes an explicit requirement
when exploiting edit histories for NLP research.
Yamangil and Nelken (2008) use edits as train-
ing data for sentence compression. They make
the simplifying assumption that all selected edits
retain the core meaning. Zanzotto and Pennac-
chiotti (2010) use edits as training data for textual
entailment recognition. In addition to manually
labeled edits, they use Wikipedia user comments
and a co-training approach to leverage unlabeled
edits. Woodsend and Lapata (2011) and Yatskar
et al(2010) use Wikipedia comments to identify
relevant edits for learning sentence simplification.
The work by Max and Wisniewski (2010) is
closely related to the approach proposed in this
paper. They extract a corpus of rewritings, dis-
tinguish between weak semantic differences and
strong semantic differences, and present a typol-
ogy of multiple subclasses. Spelling corrections
are heuristically identified but the task of auto-
matic classification is deferred. Follow-up work
by Dutrey et al(2011) focuses on automatic para-
phrase identification using a rule based approach
and manually annotated examples.
Wikipedia vandalism detection is a user ed-
its classification problem addressed by a yearly
competition (since 2010) in conjunction with the
CLEF conference (Potthast et al 2010; Potthast
and Holfeld, 2011). State-of-the-art solutions in-
volve supervised machine learning using various
content and metadata features. Content features
use spelling, grammar, character- and word-level
attributes. Many of them are relevant for our ap-
proach. Metadata features allow detection by pat-
terns of usage, time and place, which are gener-
ally useful for the detection of online malicious
activities (West et al 2010; West and Lee, 2011).
We deliberately refrain from using such features.
A wide range of methods and approaches has
been applied to the similar tasks of textual en-
tailment and paraphrase recognition, see Androut-
sopoulos and Malakasiotis (2010) for a compre-
hensive review. These are all related because
paraphrases and bidirectional entailments repre-
sent types of fluency edits.
A different line of research uses classifiers to
predict sentence-level fluency (Zwarts and Dras,
2008; Chae and Nenkova, 2009). These could be
useful for fluency edits detection. Alternatively,
user edits could be a potential source of human-
produced training data for fluency models.
3 Definition of User Edits Scope
Within our approach we distinguish between edit
segments, which represent the comparison (diff)
between two document revisions, and user edits,
which are the input for classification.
An edit segment is a contiguous sequence of
deleted, inserted or equal words. The difference
between two document revisions (vi, vj) is repre-
sented by a sequence of edit segments E. Each
edit segment (?, wm1 ) ? E is a pair, where ? ?
{deleted , inserted , equal} and wm1 is a m-word
substring of vi, vj or both (respectively).
A user edit is a minimal set of sentences over-
lapping with deleted or inserted segments. Given
the two sets of revision sentences (Svi , Svj ), let
?(?, wm1 ) = {s ? Svi ? Svj | w
m
1 ? s 6= ?} (1)
be the subset of sentences overlapping with a
given edit segment, and let
?(s) = {(?, wm1 ) ? E | w
m
1 ? s 6= ?} (2)
357
be the subset of edit segments overlapping with a
given sentence.
A user edit is a pair (pre ? Svi , post ? Svj )
where
?s ? pre ? post ?? ? {deleted , inserted} ?wm1
(?, wm1 ) ? ?(s)? ?(?, w
m
1 ) ? pre ? post (3)
?s ? pre ? post ?? ? {deleted , inserted} ?wm1
(?, wm1 ) ? ?(s) (4)
Table 1 illustrates different types of edit seg-
ments and user edits. The term replaced segment
refers to adjacent deleted and inserted segments.
Example (1) contains a replaced segment because
the deleted segment (?1700s?) is adjacent to the
inserted segment (?18th century?). Example (2)
contains an inserted segment (?and largest profes-
sional?), a replaced segment (?(est.? ? ?estab-
lished in?) and a deleted segment (?)?). User edits
of both examples consist of a single pre sentence
and a single post sentence because deleted and in-
serted segments do not cross any sentence bound-
ary. Example (3) contains a replaced segment (?.
He? ? ?who?). In this case the deleted segment
(?. He?) overlaps with two sentences and there-
fore the user edit consists of two pre sentences.
4 Features for Edits Classification
We design a set of features for supervised classi-
fication of user edits. The design is guided by two
main considerations: simplicity and interoperabil-
ity. Simplicity is important because there are po-
tentially hundreds of millions of user edits to be
classified. This amount continues to grow at rapid
pace and a scalable solution is required. Interop-
erability is important because millions of user ed-
its are available in multiple languages. Wikipedia
is a flagship project, but there are other collabora-
tive editing projects. The solution should prefer-
ably be language- and project-independent. Con-
sequently, we refrain from deeper syntactic pars-
ing, Wikipedia-specific features, and language re-
sources that are limited to English.
Our basic intuition is that longer edits are likely
to be factual and shorter edits are likely to be
fluency edits. The baseline method is therefore
character-level edit distance (Levenshtein, 1966)
between pre- and post-edited text.
Six feature categories are added to the baseline.
Most features take the form of threefold counts re-
ferring to deleted, inserted and equal elements of
(1) Revisions 368209202 & 378822230
pre (?By the mid 1700s, Medzhybizh was the seat of
power in Podilia Province.?)
post (?By the mid 18th century, Medzhybizh was
the seat of power in Podilia Province.?)
diff (equal , ?By the mid?) , (deleted, ?1700s?) ,
(inserted , ?18th century?) , (equal , ?, Medzhy-
bizh was the seat of power in Podilia Province.?)
(2) Revisions 148109085 & 149440273
pre (?Original Society of Teachers of the Alexander
Technique (est. 1958).?)
post (?Original and largest professional Society of
Teachers of the Alexander Technique estab-
lished in 1958.?)
diff (equal , ?Original?) , (inserted , ?and largest
professional?) , (equal , ?Society of Teachers of
the Alexander Technique?) , (deleted , ?(est.?) ,
(inserted , ? established in?) , (equal , ?1958?) ,
(deleted , ?)?) , (equal , ?.?)
(3) Revisions 61406809 & 61746002
pre (?Fredrik Modin is a Swedish ice hockey left
winger.? , ?He is known for having one of the
hardest slap shots in the NHL.?)
post (?Fredrik Modin is a Swedish ice hockey left
winger who is known for having one of the hard-
est slap shots in the NHL.?)
diff (equal , ?Fredrik Modin is a Swedish ice hockey
left winger?) , (deleted , ?. He?) , (inserted ,
?who?) , (equal , ?is known for having one of
the hardest slap shots in the NHL.?)
Table 1: Examples of user edits and the corre-
sponding edit segments (revision numbers corre-
spond to the English Wikipedia).
each user edit. For instance, example (1) in Table
1 has one deleted token, two inserted tokens and
14 equal tokens. Many features use string similar-
ity calculated over alternative representations.
Character-level features include counts of
deleted, inserted and equal characters of different
types, such as word & non-word characters or dig-
its & non-digits. Character types may help iden-
tify edits types. For example, the change of dig-
its may suggest a factual edit while the change of
non-word characters may suggest a fluency edit.
Word-level features count deleted, inserted
and equal words using three parallel represen-
tations: original case, lower case, and lemmas.
Word-level edit distance is calculated for each
representation. Table 2 illustrates how edit dis-
tance may vary across different representations.
358
Rep. User Edit Dist
Words pre Branch lines were built in Kenya 4
post A branch line was built in Kenya
Lowcase pre branch lines were built in kenya 3
post a branch line was built in kenya
Lemmas pre branch line be build in Kenya 1
post a branch line be build in Kenya
PoS tags pre NN NNS VBD VBN IN NNP 2
post DT NN NN VBD VBN IN NNP
NE tags pre LOCATION 0
post LOCATION
Table 2: Word- and tag-level edit distance mea-
sured over different representations (example
from Wikipedia revisions 2678278 & 2682972).
Fluency edits may shift words, which sometimes
may be slightly modified. Fluency edits may also
add or remove words that already appear in con-
text. Optimal calculation of edit distance with
shifts is computationally expensive (Shapira and
Storer, 2002). Translation error rate (TER) pro-
vides an approximation but it is designed for the
needs of machine translation evaluation (Snover
et al 2006). To have a more sensitive estima-
tion of the degree of edit, we compute the minimal
character-level edit distance between every pair of
words that belong to different edit segments. For
each pair of edit segments (?, wm1 ), (?
?, w?k1) over-
lapping with a user edit, if ? 6= ?? we compute:
?w ? wm1 : min
w??w?k1
EditDist(w,w?) (5)
Binned counts of the number of words with a min-
imal edit distance of 0, 1, 2, 3 or more charac-
ters are accumulated per edit segment type (equal,
deleted or inserted).
Part-of-speech (PoS) features include counts
of deleted, inserted and equal PoS tags (per tag)
and edit distance at the tag level between PoS tags
before and after the edit. Similarly, named-entity
(NE) features include counts of deleted, inserted
and equal NE tags (per tag, excluding OTHER)
and edit distance at the tag level between NE tags
before and after the edit. Table 2 illustrates the
edit distance at different levels of representation.
We assume that a deleted NE tag, e.g. PERSON
or LOCATION, could indicate a factual edit. It
could however be a fluency edit where the NE is
replaced by a co-referent like ?she? or ?it?. Even
if we encounter an inserted PRP PoS tag, the fea-
tures do not capture the explicit relation between
the deleted NE tag and the inserted PoS tag. This
is an inherent weakness of these features when
compared to parsing-based alternatives.
An additional set of counts, NE values, de-
scribes the number of deleted, inserted and equal
normalized values of numeric entities such as
numbers and dates. For instance, if the word
?100? is replaced by ?200? and the respective nu-
meric values 100.0 and 200.0 are normalized, the
counts of deleted and inserted NE values will be
incremented and suggest a factual edit. If on the
other hand ?100? is replaced by ?hundred? and the
latter is normalized as having the numeric value
100.0, then the count of equal NE values will be
incremented, rather suggesting a fluency edit.
Acronym features count deleted, inserted and
equal acronyms. Potential acronyms are extracted
from word sequences that start with a capital letter
and from words that contain multiple capital let-
ters. If, for example, ?UN? is replaced by ?United
Nations?, ?MicroSoft? by ?MS? or ?Jean Pierre?
by ?J.P?, the count of equal acronyms will be in-
cremented, suggesting a fluency edit.
The last category, language model (LM) fea-
tures, takes a different approach. These features
look at n-gram based sentence probabilities be-
fore and after the edit, with and without normal-
ization with respect to sentence lengths. The ratio
of the two probabilities, P?ratio(pre, post) is com-
puted as follows:
P? (wm1 ) ?
m?
i=1
P (wi|w
i?1
i?n+1) (6)
P?norm(w
m
1 ) = P? (w
m
1 )
1
m (7)
P?ratio(pre, post) =
P?norm(post)
P?norm(pre)
(8)
log P?ratio(pre, post) = log
P?norm(post)
P?norm(pre)
(9)
= log P?norm(post)? log P?norm(pre)
=
1
|post |
log P? (post)?
1
|pre|
log P? (pre)
Where P? is the sentence probability estimated as
a product of n-gram conditional probabilities and
P?norm is the sentence probability normalized by
the sentence length. We hypothesize that the rel-
ative change of normalized sentence probabilities
is related to the edit type. As an additional feature,
the number of out of vocabulary (OOV) words be-
fore and after the edit is computed. The intuition
359
Dataset Labeled Subset
Number of User Edits:
923,820 (100%) 2,008 (100%)
Edit Segments Distribution:
Replaced 535,402 (57.96%) 1,259 (62.70%)
Inserted 235,968 (25.54%) 471 (23.46%)
Deleted 152,450 (16.5%) 278 (13.84%)
Character-level Edit Distance Distribution:
1 202,882 (21.96%) 466 (23.21%)
2 81,388 (8.81%) 198 (9.86%)
3-10 296,841 (32.13%) 645 (32.12%)
11-100 342,709 (37.10%) 699 (34.81%)
Word-level Edit Distance Distribution:
1 493,095 (53.38%) 1,008 (54.18%)
2 182,770 (19.78%) 402 (20.02%)
3 77,603 (8.40%) 161 (8.02%)
4-10 170,352 (18.44%) 357 (17.78%)
Labels Distribution:
Fluency - 1,008 (50.2%)
Factual - 1,000 (49.8%)
Table 3: Dataset of nearly 1 million user edits
with single deleted, inserted or replaced segments,
of which 2K are labeled. The labels are almost
equally distributed. The distribution over edit seg-
ment types and edit distance intervals is detailed.
is that unknown words are more likely to be in-
dicative of factual edits.
5 Experiments
5.1 Experimental Setup
First, we extract a large amount of user edits from
revision histories of the English Wikipedia.3 The
extraction process scans pairs of subsequent re-
visions of article pages and ignores any revision
that was reverted due to vandalism. It parses the
Wikitext and filters out markup, hyperlinks, tables
and templates. The process analyzes the clean text
of the two revisions4 and computes the difference
between them.5 The process identifies the overlap
between edit segments and sentence boundaries
and extracts user edits. Features are calculated
and user edits are stored and indexed. LM features
are calculated against a large English 4-gram lan-
3Dump of all pages with complete edit history as of Jan-
uary 15, 2011 (342GB bz2), http://dumps.wikimedia.org.
4Tokenization, sentence split, PoS & NE tags by Stanford
CoreNLP, http://nlp.stanford.edu/software/corenlp.shtml.
5Myers? O(ND) difference algorithm (Myers, 1986),
http://code.google.com/p/google-diff-match-patch.
guage model built by SRILM (Stolcke, 2002) with
modified interpolated Kneser-Ney smoothing us-
ing the AFP and Xinhua portions of the English
Gigaword corpus (LDC2003T05).
We extract a total of 4.3 million user edits of
which 2.52 million (almost 60%) are insertions
and deletions of complete sentences. Although
these may include fluency edits such as sentence
reordering or rewriting from scratch, we assume
that the large majority is factual. Of the remaining
1.78 million edits, the majority (64.5%) contains
single deleted, inserted or replaced segments. We
decide to focus on this subset because sentences
with multiple non-contiguous edit segments are
more likely to contain mixed cases of unrelated
factual and fluency edits, as illustrated by exam-
ple (2) in Table 1. Learning to classify contigu-
ous edit segments seems to be a reasonable way
of breaking down the problem into smaller parts.
We filter out user edits with edit distance longer
than 100 characters or 10 words that we assume to
be factual. The resulting dataset contains 923,820
user edits: 58% replaced segments, 25.5% in-
serted segments and 16.5% deleted segments.
Manual labeling of user edits is carried out by
a group of annotators with near native or native
level of English. All annotators receive the same
written guidelines. In short, fluency labels are
assigned to edits of letter case, spelling, gram-
mar, synonyms, paraphrases, co-referents, lan-
guage and style. Factual labels are assigned to
edits of dates, numbers and figures, named enti-
ties, semantic change or disambiguation, addition
or removal of content. A random set of 2,676 in-
stances is labeled: 2,008 instances with a majority
agreement of at least two annotators are selected
as training set, 270 instances are held out as de-
velopment set, 164 trivial fluency corrections of a
single letter?s case and 234 instances with no clear
agreement among annotators are excluded. The
last group (8.7%) emphasizes that the task is, to
a limited extent, subjective. It suggests that auto-
mated classification of certain user edits would be
difficult. Nevertheless, inter-rater agreement be-
tween annotators is high to very high. Kappa val-
ues between 0.74 to 0.84 are measured between
six pairs of annotators, each pair annotated a com-
mon subset of at least 100 instances. Table 3 de-
scribes the resulting dataset, which we also make
available to the research community.6
6Available for download at http://staff.
360
Character-level Edit Distance
? ? 4 > 4?
Fluency (725) Factual (821)
Factual (179) Fluency (283)
Figure 1: A decision tree that uses character-level
edit distance as a sole feature. The tree correctly
classifies 76% of the labeled user edits.
Feature set SVM RF Logit
Baseline 76.26% 76.26% 76.34%
+ Char-level 83.71%? 84.45%? 84.01%?
+ Word-level 78.38%?? 81.38%?? 78.13%??
+ PoS 76.58%? 76.97% 78.35%??
+ NE 82.71%? 83.12%? 82.38%?
+ Acronyms 76.55% 76.61% 76.96%
+ LM 76.20% 77.42% 76.52%
All Features 87.14%?? 87.14%? 85.64%??
Table 4: Classification accuracy using the base-
line, each feature set added to the baseline, and
all features combined. Statistical significance at
p < 0.05 is indicated by ? w.r.t the baseline (us-
ing the same classifier), and by ? w.r.t to another
classifier marked by ? (using the same features).
Highest accuracy per classifier is marked in bold.
5.2 Feature Analysis
We experiment with three classifiers: Support
Vector Machines (SVM), Random Forests (RF)
and Logistic Regression (Logit).7 SVMs (Cortes
and Vapnik, 1995) and Logistic Regression (or
Maximum Entropy classifiers) are two widely
used machine learning techniques. SVMs have
been applied to many text classification problems
(Joachims, 1998). Maximum Entropy classifiers
have been applied to the similar tasks of para-
phrase recognition (Malakasiotis, 2009) and tex-
tual entailment (Hickl et al 2006). Random
Forests (Breiman, 2001) as well as other decision
tree algorithms are successfully used for classi-
fying Wikipedia edits for the purpose of vandal-
ism detection (Potthast et al 2010; Potthast and
Holfeld, 2011).
Experiments begin with the edit-distance base-
science.uva.nl/?abronner/uec/data.
7Using Weka classifiers: SMO (SVM), RandomForest &
Logistic (Hall et al 2009). Classifier?s parameters are tuned
using the held-out development set.
Feature set SVM RF Logit
flu. / fac. flu. / fac. flu. / fac.
Baseline 0.85 / 0.67 0.74 / 0.79 0.85 / 0.67
+ Char-level 0.85 / 0.82 0.83 / 0.86 0.86 / 0.82
+ Word-level 0.88 / 0.69 0.81 / 0.82 0.86 / 0.70
+ PoS 0.85 / 0.68 0.78 / 0.76 0.84 / 0.72
+ NE 0.86 / 0.79 0.79 / 0.87 0.87 / 0.78
+ Acronyms 0.87 / 0.66 0.83 / 0.70 0.86 / 0.68
+ LM 0.85 / 0.67 0.79 / 0.76 0.84 / 0.69
All Features 0.88 / 0.86 0.86 / 0.88 0.87 / 0.84
Table 5: Fraction of correctly classified edits per
type: fluency edits (left) and factual edits (right),
using the baseline, each feature set added to the
baseline, and all features combined.
line. Then each one of the feature groups is sep-
arately added to the baseline. Finally, all features
are evaluated together. Table 4 reports the per-
centage of correctly classified edits (classifiers?
accuracy), and Table 5 reports the fraction of cor-
rectly classified edits per type. All results are for
10-fold cross validation. Statistical significance
against the baseline and between classifiers is cal-
culated at p < 0.05 using paired t-test.
The first interesting result is the highly predic-
tive power of the single-feature baseline. It con-
firms the intuition that longer edits are mainly fac-
tual. Figure 1 shows that the edit distance of 72%
of the user edits labeled as fluency is between 1 to
4, while the edit distance of 82% of those labeled
as factual is greater than 4. The cut-off value is
found by a single-node decision tree that uses edit
distance as a sole feature. The tree correctly clas-
sifies 76% of the instances. This result implies
that the actual challenge is to correctly classify
short factual edits and long fluency edits.
Character-level features and named-entity fea-
tures lead to significant improvements over the
baseline for all classifiers. Their strength lies in
their ability to identify short factual edits such
as changes of numeric values or proper names.
Word-level features also significantly improve the
baseline but their contribution is smaller. PoS
and acronym features lead to small statistically-
insignificant improvements over the baseline.
The poor contribution of LM features is sur-
prising. It might be due to the limited context
of n-grams, but it might be that LM probabili-
ties are not a good predictor for the task. Re-
moving LM features from the set of all features
361
Fluency Edits Misclassified as Factual
Equivalent or redundant in context 14
Paraphrases 13
Equivalent numeric patterns 7
Replacing first name with last name 4
Acronyms 4
Non specific adjectives or adverbs 3
Other 5
Factual Edits Misclassified as Fluency
Short correction of content 35
Opposites 3
Similar names 3
Noise (unfiltered vandalism) 3
Other 6
Table 6: Error types based on manual examina-
tion of 50 fluency edit misclassifications and 50
factual edit misclassifications.
leads to a small decrease in classification accu-
racy, namely 86.68% instead of 87.14% for SVM.
This decrease is not statistically significant.
The highest accuracy is achieved by both SVM
and RF and there are few significant differences
among the three classifiers. The fraction of cor-
rectly classified edits per type (Table 5) reveals
that for SVM and Logit, most fluency edits are
correctly classified by the baseline and most im-
provements over the baseline are attributed to bet-
ter classification of factual edits. This is not the
case for RF, where the fraction of correctly classi-
fied factual edits is higher and the fraction of cor-
rectly classified fluency edits is lower. This in-
sight motivates further experimentation. Repeat-
ing the experiment with a meta-classifier that uses
a majority voting scheme, achieves an improved
accuracy of 87.58%. This improvement is not sta-
tistically significant.
5.3 Error Analysis
To have better understanding of errors made by
the classifier, 50 fluency edit misclassifications
and 50 factual edit misclassifications are ran-
domly selected and manually examined. The er-
rors are grouped into categories as summarized in
Table 6. These explain certain limitations of the
classifier and suggest possible improvements.
Fluency edit misclassifications: 14 instances
(28%) are phrases (often co-referents) that are ei-
ther equivalent or redundant in the given context.
Correctly Classified Fluency Edits
?Adventure education makes intentional use of intention-
ally uses challenging experiences for learning.?
?He served as president from October 1 , 1985 and retired
through his retirement on June 30 , 2002.?
?In 1973, he helped organize assisted in organizing his
first ever visit to the West.?
Correctly Classified Factual Edits
?Over the course of the next two years five months, the
unit completed a series of daring raids.?
?Scottish born David Tennant has reportedly said he
would like his Doctor to wear a kilt.?
?This family joined the strip in late 1990 around March
1991.?
Table 7: Examples of correctly classified user ed-
its. Deleted segments are struck out, inserted are
bold (revision numbers are omitted for brevity).
For example: ?in 1986? ? ?that year?, ?when
she returned?? ?when Ruffa returned? and ?the
core member of the group are?? ?the core mem-
bers are?. 13 (26%) are paraphrases misclassified
as factual edits. Examples are: ?made cartoons?
? ?produced animated cartoons? and ?with the
implication that they are similar to? ? ?imply-
ing a connection to?. 7 modify numeric patterns
that do not change the meaning such as the year
?37? ? ?1937?. 4 replace a first name of a per-
son with the last name. 4 contain acronyms, e.g.
?Display PostScript? ? ?Display PostScript (or
DPS)?. Acronym features are correctly identified
but the classifier fails to recognize a fluency edit.
3 modify adjectives or adverbs that do not change
the meaning such as ?entirely? and ?various?.
Factual edit misclassifications: the big major-
ity, 35 instances (70%), could be characterized as
short corrections, often replacing a similar word,
that make the content more accurate or more
precise. Examples (context is omitted): ?city?
? ?village?, ?emigrated? ? ?immigrated? and
?electrical?? ?electromagnetic?. 3 are opposites
or antonyms such as ?previous? ? ?next? and
?lived? ? ?died?. 3 are modifications of similar
person or entity names, e.g. ?Kelly? ? ?Kate?.
3 are instances of unfiltered vandalism, i.e. noisy
examples. Other misclassifications include verb
tense modifications such as ?is? ? ?was? and
?consists? ? ?consisted?. These are difficult to
362
Comment Test Set Classified as
Size Fluency Edits
?grammar? 1,122 88.9%
?spelling? 2,893 97.6%
?typo? 3,382 91.6%
?copyedit? 3,437 68.4%
Random set 5,000 49.4%
Table 8: Classifying unlabeled data selected by
user comments that suggest a fluency edit. The
SVM classifier is trained using the labeled data.
User comments are not used as features.
classify because the modification of verb tense in
a given context is sometimes factual and some-
times a fluency edit.
These findings agree with the feature analy-
sis. Fluency edit misclassifications are typically
longer phrases that carry the same meaning while
factual edit misclassifications are typically sin-
gle words or short phrases that carry different
meaning. The main conclusion is that the clas-
sifier should take into account explicit content
and context. Putting aside the consideration of
simplicity and interoperability, features based on
co-reference resolution and paraphrase recogni-
tion are likely to improve fluency edits classi-
fication, and features from language resources
that describe synonymy and antonymy relations
are likely to improve factual edits classification.
While this conclusion may come at no surprise, it
is important to highlight the high classification ac-
curacy that is achieved without such capabilities
and resources. Table 7 presents several examples
of correct classification produced by our classifier.
6 Exploiting Unlabeled Data
We extracted a large set of user edits but our ap-
proach has been limited to a restricted number of
labeled examples. This section attempts to find
whether the classifier generalizes beyond labeled
data and whether unlabeled data could be used to
improve classification accuracy.
6.1 Generalizing Beyond Labeled Data
The aim of the next experiment is to test how well
the supervised classifier generalizes beyond the
labeled test set. The problem is the availability
of test data. There is no shared task for user ed-
its classification and no common test set to eval-
Replaced by Frequency Edit class
?second? 144 Factual
?First? 38 Fluency
?last? 31 Factual
?1st? 22 Fluency
?third? 22 Factaul
Table 9: User edits replacing the word ?first? with
another single word: most frequent 5 out of 524.
Replaced by Frequency Replaced by Frequency
?Adams? 7 ?Squidward? 6
?Joseph? 7 ?Alexander? 5
?Einstein? 6 ?Davids? 5
?Galland? 6 ?Haim? 5
?Lowe? 6 ?Hickes? 5
Table 10: Fluency edits replacing the word ?He?
with proper noun: most frequent 10 out of 1,381.
uate against. We resort to Wikipedia user com-
ments. It is a problematic option because it is un-
reliable. Users may add a comment when submit-
ting an edit, but it is not mandatory. The com-
ment is a free text with no predefined structure.
It could be meaningful or nonsense. The com-
ment is per revision. It may refer to one, some
or all edits submitted for a given revision. Nev-
ertheless, we identify several keywords that rep-
resent certain types of fluency edits: ?grammar?,
?spelling?, ?typo?, and ?copyedit?. The first three
clearly indicate grammar and spelling corrections.
The last indicates a correction of format and style,
but also of accuracy of the text. Therefore it only
represents a bias towards fluency edits.
We extract unlabeled edits whose comment is
equal to one of the keywords and construct a test
set per keyword. An additional test set consists of
randomly selected unlabeled edits with any com-
ment. The five test sets are classified by the SVM
classifier trained using the labeled data and the set
of all features. To remove any doubt, user com-
ments are not part of any feature of the classifier.
The results in Table 8 show that most unlabeled
edits whose comments are ?grammar?, ?spelling?
or ?typo? are indeed classified as fluency ed-
its. The classification of edits whose comment is
?copyedit? is biased towards fluency edits, but as
expected the result is less distinct. The classifica-
tion of the random set is balanced, as expected.
363
Feature set SVM RF Logit
Baseline 76.26% 76.26% 76.34%
All Features 87.14%?? 87.14%? 85.64%??
Unlabeled only 78.11%? 83.49%?? 78.78%??
Base + unlabeled 80.86%?? 85.45%?? 81.83%??
All + unlabeled 87.23% 88.35%??? 85.92%?
Table 11: Classification accuracy using features
from unlabeled data. The first two rows are identi-
cal to Table 4. Statistical significance at p < 0.05
is indicated by: ? w.r.t the baseline; ? w.r.t all fea-
tures excluding features from unlabeled data; and
? w.r.t to another classifier marked by ? (using the
same features). The best result is marked in bold.
6.2 Features from Unlabeled Data
The purpose of the last experiment is to exploit
unlabeled data in order to extract additional fea-
tures for the classifier. The underlying assumption
is that reoccurring patterns may indicate whether
a user edit is factual or a fluency edit.
We could assume that fluency edits would re-
occur across many revisions, while factual edits
would only appear in revisions of specific docu-
ments. However, this assumption does not nec-
essarily hold. Table 9 gives a simple example of
single word replacements for which the most re-
occurring edit is actually factual and other factual
and fluency edits reoccur in similar frequencies.
Finding user edits reoccurrence is not trivial.
We could rely on exact matches of surface forms,
but this may lead to data sparseness issues. Flu-
ency edits that exchange co-referents and proper
nouns, as illustrated by the example in Table 10,
may reoccur frequently but this fact could not
be revealed by exact matching of specific proper
nouns. On the other hand, using a bag of word
approach may find too many unrelated edits.
We introduce a two-step method that measures
the reoccurrence of edits in unlabeled data us-
ing exact and approximate matching over multi-
ple representations. The method provides a set of
frequencies that is fed into the classifier and al-
lows for learning subtle patterns of reoccurrence.
Staying consistent with our initial design consid-
erations, the method is simple and interoperable.
Given a user edit (pre, post), the method does
not compare pre with post in any way. It only
compares pre with pre-edited sentences of other
unlabeled edits and post with post-edited sen-
tences of other unlabeled edits. The first step is to
select candidates using a bag of words approach.
The second step is a comparison of the user edit
with each one of the candidates while increment-
ing counts of similarity measures. These account
for exact matches between different representa-
tions (original and low case, lemmas, PoS and NE
tags) as well as for approximate matches using
character- and word-level edit distance between
those representations. An additional feature is the
number of distinct documents in the candidate set.
We compute the set of features for the labeled
dataset based on the unlabeled data. The number
of candidates is set to 1,000 per user edit. We
re-train the classifiers using five configurations:
Baseline and All Features are identical to the first
experiment. Unlabeled only uses the new feature
set without any other feature. Base + Unlabeled
adds the new feature set to the baseline. All + Un-
labeled uses all available features. All results are
for 10-fold cross validation with statistical signif-
icance at p < 0.05 by paired t-test, see Table 11.
We find that features extracted from unlabeled
data outperform the baseline and lead to statisti-
cally significant improvements when added to it.
The combination of all features allows Random
Forests to achieve the highest statistically signifi-
cant accuracy level of 88.35%.
7 Conclusions
This work addresses the task of user edits clas-
sification as factual or fluency edits. It adopts
a supervised machine learning approach and
uses character- and word- level features, part-
of-speech tags, named entities, language model
probabilities, and a set of features extracted from
large amounts of unlabeled data. Our experiments
with contiguous user edits extracted from revision
histories of the English Wikipedia achieve high
classification accuracy and demonstrate general-
ization to data beyond labeled edits.
Our approach shows that machine learning
techniques can successfully distinguish between
user edit types, making them a favorable alterna-
tive to heuristic solutions. The simple and adap-
tive nature of our method allows for application to
large and evolving sets of user edits.
Acknowledgments. This research was funded
in part by the European Commission through the
CoSyne project FP7-ICT-4-248531.
364
References
A. Aji, Y. Wang, E. Agichtein, and E. Gabrilovich.
2010. Using the past to score the present: Extend-
ing term weighting models through revision history
analysis. In Proceedings of the 19th ACM inter-
national conference on Information and knowledge
management, pages 629?638.
I. Androutsopoulos and P. Malakasiotis. 2010. A sur-
vey of paraphrasing and textual entailment meth-
ods. Journal of Artificial Intelligence Research,
38(1):135?187.
L. Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5?32.
J. Chae and A. Nenkova. 2009. Predicting the fluency
of text with shallow structural features: case stud-
ies of machine translation and human-written text.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 139?147.
C. Cortes and V. Vapnik. 1995. Support-vector net-
works. Machine learning, 20(3):273?297.
C. Dutrey, D. Bernhard, H. Bouamor, and A. Max.
2011. Local modifications and paraphrases in
Wikipedia?s revision history. Procesamiento del
Lenguaje Natural, Revista no 46:51?58.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data
mining software: an update. ACM SIGKDD Explo-
rations Newsletter, 11(1):10?18.
A. Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink,
and Y. Shi. 2006. Recognizing textual entailment
with LCCs GROUNDHOG system. In Proceedings
of the Second PASCAL Challenges Workshop.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. Machine Learning: ECML-98, pages 137?
142.
A. Kittur, B. Suh, B.A. Pendleton, and E.H. Chi. 2007.
He says, she says: Conflict and coordination in
Wikipedia. In Proceedings of the SIGCHI confer-
ence on Human factors in computing systems, pages
453?462.
V.I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
P. Malakasiotis. 2009. Paraphrase recognition using
machine learning to combine similarity measures.
In Proceedings of the ACL-IJCNLP 2009 Student
Research Workshop, pages 27?35.
A. Max and G. Wisniewski. 2010. Min-
ing naturally-occurring corrections and paraphrases
from Wikipedia?s revision history. In Proceedings
of LREC, pages 3143?3148.
E.W. Myers. 1986. An O(ND) difference algorithm
and its variations. Algorithmica, 1(1):251?266.
R. Nelken and E. Yamangil. 2008. Mining
Wikipedia?s article revision history for training
computational linguistics algorithms. In Proceed-
ings of the AAAI Workshop on Wikipedia and Arti-
ficial Intelligence: An Evolving Synergy, pages 31?
36.
S. Nunes, C. Ribeiro, and G. David. 2011. Term
weighting based on document revision history.
Journal of the American Society for Information
Science and Technology, 62(12):2471?2478.
M. Potthast and T. Holfeld. 2011. Overview of the 2nd
international competition on Wikipedia vandalism
detection. Notebook for PAN at CLEF 2011.
M. Potthast, B. Stein, and T. Holfeld. 2010. Overview
of the 1st international competition on Wikipedia
vandalism detection. Notebook Papers of CLEF,
pages 22?23.
D. Shapira and J. Storer. 2002. Edit distance with
move operations. In Combinatorial Pattern Match-
ing, pages 85?98.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
Association for Machine Translation in the Ameri-
cas, pages 223?231.
A. Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Proceedings of the interna-
tional conference on spoken language processing,
volume 2, pages 901?904.
F.B. Viegas, M. Wattenberg, and K. Dave. 2004.
Studying cooperation and conflict between authors
with history flow visualizations. In Proceedings of
the SIGCHI conference on Human factors in com-
puting systems, pages 575?582.
A.G. West and I. Lee. 2011. Multilingual vandalism
detection using language-independent & ex post
facto evidence. Notebook for PAN at CLEF 2011.
A.G. West, S. Kannan, and I. Lee. 2010. Detecting
Wikipedia vandalism via spatio-temporal analysis
of revision metadata. In Proceedings of the Third
European Workshop on System Security, pages 22?
28.
K. Woodsend and M. Lapata. 2011. Learning to
simplify sentences with quasi-synchronous gram-
mar and integer programming. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 409?420.
E. Yamangil and R. Nelken. 2008. Mining Wikipedia
revision histories for improving sentence compres-
sion. In Proceedings of ACL-08: HLT, Short Pa-
pers, pages 137?140.
M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, and
L. Lee. 2010. For the sake of simplicity: Unsu-
pervised extraction of lexical simplifications from
Wikipedia. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 365?368.
365
F.M. Zanzotto and M. Pennacchiotti. 2010. Expand-
ing textual entailment corpora from Wikipedia us-
ing co-training. In Proceedings of the 2nd Work-
shop on Collaboratively Constructed Semantic Re-
sources, COLING 2010.
S. Zwarts and M. Dras. 2008. Choosing the right
translation: A syntactically informed classification
approach. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume
1, pages 1153?1160.
366
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 30?38,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Maximizing Component Quality in Bilingual Word-Aligned Segmentations
Spyros Martzoukos Christophe Costa Flor
?
encio Christof Monz
Intelligent Systems Lab Amsterdam, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{S.Martzoukos, C.CostaFlorencio, C.Monz}@uva.nl
Abstract
Given a pair of source and target language
sentences which are translations of each other
with known word alignments between them,
we extract bilingual phrase-level segmenta-
tions of such a pair. This is done by identi-
fying two appropriate measures that assess the
quality of phrase segments, one on the mono-
lingual level for both language sides, and one
on the bilingual level. The monolingual mea-
sure is based on the notion of partition refine-
ments and the bilingual measure is based on
structural properties of the graph that repre-
sents phrase segments and word alignments.
These two measures are incorporated in a ba-
sic adaptation of the Cross-Entropy method
for the purpose of extracting an N -best list
of bilingual phrase-level segmentations. A
straight-forward application of such lists in
Statistical Machine Translation (SMT) yields
a conservative phrase pair extraction method
that reduces phrase-table sizes by 90% with
insignificant loss in translation quality.
1 Introduction
Given a pair of source and target language sen-
tences which are translations of each other with
known word alignments between them, the problem
of extracting high quality bilingual phrase segmen-
tations is defined as follows: Maximize the quality
of phrase segments, i.e., groupings of consecutive
words, in both language sides, subject to constraints
imposed by the underlying word alignments. The
purpose of this work is to provide a solution to this
maximization problem and investigate the effect of
the resulting high quality bilingual phrase segments
on SMT. For brevity, ?phrase-level sentence segmen-
tation? and ?phrase segment? will henceforth be sim-
ply referred to as ?segmentation? and ?segment? re-
spectively.
The exact definition of segments? quality depends
on the application. Our notion of a segmentation of
maximum quality is defined as the set of consecutive
words of the sentence that captures maximum col-
locational and/or grammatical characteristics. This
implies that a sequence of tokens is identified as a
segment if its fully compositional expressive power
is higher than the expressive power of any combina-
tion of partial compositions. Since this definition is
fairly general it is thus suitable for most NLP tasks.
In particular, it is tailored to the type of segments
that are suitable for the purposes of SMT and is in
line with previous work (Blackwood et al., 2008;
Paul et al., 2010).
With this definition in mind, we introduce a
monolingual segment quality measure that is based
on assessing the cost of converting one segmentation
into another by means of an elementary operation.
This operation, namely the ?splitting? of a segment
into two segments, together with all possible seg-
mentations of a sentence are known to form a par-
tially ordered set (Guo, 1997). Such a construction
is known as partition refinement and gives rise to the
desired monolingual surface quality measure.
The presence of word alignments between the
sentence pair provides additional structure which
should not be ignored. In the language of graph the-
ory, a segment can also be viewed as a chain, i.e., a
graph in which vertices are the segment?s words and
30
an edge between two words exists if and only if these
words are consecutive. Then, a bilingual segmenta-
tion is represented by the graph that is formed by all
its source and target language chains together with
edges induced by word alignments. Motivated by
the phrase pair extraction methods of SMT (Och et
al., 1999; Koehn et al., 2003), we focus on the con-
nected components, or simply components of such a
representation. We explain that the extent to which
we can delete word alignments from a component
without violating its component status, gives rise to
a bilingual, purely structural quality measure.
The surface and structural measures are incorpo-
rated in one algorithm that extracts an N -best list
of bilingual word-aligned segmentations. This algo-
rithm, which is an adaptation of the Cross-Entropy
method (Rubinstein, 1997), performs joint maxi-
mization of surface (in both languages) and struc-
tural quality measures. Components of graph repre-
sentations of the resulting N -best lists give rise to
high quality translation units. These units, which
form a small subset of all possible (continuous) con-
sistent phrase pairs, are used to construct SMT mod-
els. Results on Czech?English and German?English
datasets show a 90% reduction in phrase-table sizes
with insignificant loss in translation quality which
are in line with other pruning techniques in SMT
(Johnson et al., 2007; Zens et al., 2012).
2 Monolingual Surface Quality Measure
Given a sentence s
1
s
2
...s
k
that consists of words
s
i
, 1 ? i ? k, we introduce an empirical count-
based measure that assesses the quality of its seg-
mentations. By fixing a segmentation ?, we are in-
terested in assessing the cost of perturbing ? and
generating another segmentation ?
?
. A perturbation
of ? is achieved by splitting a segment of ? into
two new segments, while keeping all other segments
fixed. For example, for a sentence with five words, if
? : (s
1
s
2
)(s
3
s
4
s
5
), where brackets are used to dis-
tinguish the segments s
1
s
2
and s
3
s
4
s
5
, then ? can
be perturbed in three different ways:
? ?
?
: (s
1
)(s
2
)(s
3
s
4
s
5
), by splitting the first seg-
ment of ?.
? ?
??
: (s
1
s
2
)(s
3
)(s
4
s
5
), by splitting at the first
position of the second segment of ?.
? ?
???
: (s
1
s
2
)(s
3
s
4
)(s
5
), by splitting at the sec-
ond position of the second segment of ?,
so that ?
?
, ?
??
and ?
???
are the perturbations of ?.
Such perturbations are known as partition refine-
ments in the literature (Stanley, 1997). The set of all
segmentations of a sentence, equipped with the split-
ting operation forms a partially ordered set (Guo,
1997), and its visual representation is known as the
Hasse diagram. Figure 1 shows such a partially or-
dered set for a sentence with four words.
  
?s1 s2 s3 s4??s1??s2 s3 s4? ? s1 s2?? s3 s4? ?s1 s2 s3??s4?
?s1??s2?? s3 s4? ?s1??s2 s3?? s4? ?s1 s2??s3?? s4??s1??s2?? s3??s4?
Figure 1: Hasse diagram of segmentation refine-
ments for a sentence with four words.
The cost of perturbing a segmentation into an-
other, i.e., the weight of a directed edge in the Hasse
diagram, is calculated from n-gram counts that are
extracted from a monolingual training corpus. Let
n(s) be the empirical count of phrase s in the corpus.
Given a segmentation ? of a sentence, let seg(?) de-
note the set of ??s segments. In the above example
we have for instance seg(?
??
) = {s
1
s
2
, s
3
, s
4
s
5
}.
The probability of s in ? is given by relative fre-
quencies
p
?
(s) =
n(s)
?
s
?
?seg(?)
n(s
?
)
. (1)
The cost of perturbing ? into ?
?
by splitting a seg-
ment ss? of ? into segments s and s? is defined by
cost
???
?
(s, s?) = log
p
?
(ss?)
p
?
?
(s)p
?
?
(s?)
, (2)
and we say that s and s? are co-responsible for the
perturbation ? ? ?
?
. Intuitively, this cost function
yields the amount of energy (log of probability) that
is lost when performing a perturbation. On a more
31
technical level, it is closely related to metric spaces
on partially ordered sets (Monjardet, 1981; Orum
and Joslyn, 2009), but we do not go into further de-
tails here.
The cost function admits a measure for the seg-
ments that are co-responsible for perturbing ? into
?
?
and we define the gain of s from the perturbation
? ? ?
?
as
gain
???
?
(s) = ?cost
???
?
(s, s?). (3)
A segment smay be co-responsible for different per-
turbations, and we have to consider all such pertur-
bations. Let
R(s) = {? ? ?
?
: s /? seg(?), s ? seg(?
?
)} (4)
denote the set of perturbations for which s is co-
responsible. Then, the average gain of s in the sen-
tence is given by
gain(s) =
1
|R(s)|
?
{???
?
}?R(s)
gain
???
?
(s). (5)
Intuitively, gain(s) measures how difficult it is to
break phrase s into sub-phrases. Finally, the surface
quality measure of a segmentation ? of a sentence is
given by
g(?) =
?
s?seg(?)
gain(s). (6)
Note that g is a real number. The relation g(?) >
g(?
?
) implies that ? is a better segmentation than ?
?
.
We conclude this section with two remarks: (i)
The exact computation of gain(s) for each possi-
ble segment s is computationally expensive since
all perturbations need to be considered. In prac-
tice we can simply generate a random sample of no
more than 1500 segmentations and compute gain(?)
based on that sample only. (ii) Each sentence of
the monolingual training corpus (from which the n-
gram counts are extracted) should have the begin-
ning and end-of-sentence tokens. The count for each
of them is equal to the number of sentences in the
corpus, and they are treated as regular words. With-
out going into further details they provide the pur-
pose of normalization.
3 Bilingual Structural Quality Measure
Given a word-aligned sentence pair, we introduce a
purely structural measure that assesses the quality of
its bilingual segmentations. By ?purely structural?
we mean that the focus is entirely on combinatorial
aspects of the bilingual segmentations and the word
alignments. For that reason we turn to a graph theo-
retic framework.
A segment can also be viewed as a chain, i.e., a
graph in which vertices are the segment?s words and
an edge between two words exists if and only if these
words are consecutive. Then, a source segmentation
? and a target segmentation ? are graphs that con-
sist of source chains and target chains respectively.
The graph formed by ?, ? and the translation edges
induced by word alignments is thus a graph repre-
sentation of a bilingual word-aligned segmentation.
We focus on a particular type of subgraphs of this
representation, namely its connected components, or
simply components. A component is a graph such
that (a) there exists a path between any two of its
vertices, and (b) there does not exist a path between
a vertex of the component and a vertex outside the
component. Condition (a) means, both technically
and intuitively, that a component is connected and
Condition (b) requires connectivity to be maximal.
Components play a key role in SMT. The most
widely used strategy for extracting high quality
phrase-level translations without linguistic informa-
tion, namely the consistency method (Och et al.,
1999; Koehn et al., 2003) is entirely based on com-
ponents of word aligned unsegmented sentence pairs
(Martzoukos et al., 2013). In particular, each ex-
tracted translation is either a component or the union
of components. Since an unsegmented sentence
pair is just one possible configuration of all possi-
ble bilingual segmentations, we consequently have
no direct reason to investigate further than compo-
nents.
In order to get an intuition of the measure that will
be introduced in this section, we begin with an ex-
ample. Figure 2, shows two different configurations
of the pair (?, ?) for the same sentence pair with
known and fixed word alignments. Both configu-
rations have the same number of edges that connect
source vertices (3) and the same number of edges
that connect target vertices (2). However, one would
32
  
1 2 3 4 ? ? ?1 2 3 4 ? ?1 2 3 4 ? ? ?1 2 3 4 ? ?
Figure 2: Graph representations of two bilingual
segmentations with fixed word alignments. Source
and target vertices are shown with circles and
squares respectively.
expect the top configuration to represent a better
bilingual segmentation. This is because it has more
components (4 opposed to 2 for the bottom config-
uration) and because it consists of ?tighter? clusters,
i.e., ?tighter? components.
A general measure that would capture this obser-
vation requires a balance between the number of
edges of source and target chains, the number of
components and the number of translation edges, all
coupled with how these edges and vertices are con-
nected. This might seem as a daunting task that can
be tackled with a combination of heuristics, but there
is actually a graph-theoretic measure that can fully
describe the sought structure. We proceed with in-
troducing this measure.
Let C denote the set of components of the graph
representation of a bilingual word-aligned segmen-
tation. We are interested in measuring the extent to
which we can delete translation edges from c ? C,
while retaining its component status. Let a
c
denote
the subset of translation edges that are restricted to
the component c. We define the positive integer
gain(c) = number of ways of
deleting translation edges from a
c
,
while keeping c connected, (7)
where the option of deleting nothing is counted. In-
tuitively, by keeping the edges of the chains fixed
the quantity gain(c) measures how difficult it is to
perturb a component from its connected state to a
disconnected state.
Figure 3 shows two components c and c
?
that sat-
isfy gain(c) = gain(c
?
) = 3. Both components
are equally difficult to be perturbed into a discon-
nected state, but only superficially. The actual struc-
tural quality of c is revealed when it is ?compared? to
component c? that consists of the same source and tar-
get vertices, the same translation edges but its source
vertices form exactly one chain and similarly for its
target vertices; c? is essentially the ?upper bound? of
c. In general, the maximum value of gain(c), with
  
cc '?c
Figure 3: Superficially similar components c and c
?
.
Comparing c with c? yields c?s true structural quality.
respect to a fixed set of source and target vertices
and translation edges, is attained when it consists
of exactly one source chain and exactly one target
chain. It is not difficult to see that the desired max-
imum value is always 2
|a
c
|
? 1. In the example of
Figure 3, the structural quality of c and c
?
is thus
3/(2
5
? 1) = 9.7% and 3/(2
2
? 1) = 100% respec-
tively. Hence, the measure that evaluates the struc-
tural quality of a bilingual word-aligned segmenta-
tion (?, ?) is given by
f(?, ?) =
(
?
c?C
gain(c)
2
|a
c
|
? 1
)
1
|C|
, (8)
which takes values in (0, 1]. The relation f(?, ?) >
f(?
?
, ?
?
) implies that (?, ?) is a better bilingual seg-
mentation than (?
?
, ?
?
).
We conclude this section with two remarks: (i) A
component with no translation edges, i.e., a source
or target segment whose words are all unaligned, has
a contribution of 1/0 in (8). In practice we exclude
such components from C. (ii) In graph theory the
quantity gain(c) is known as the number of con-
nected spanning subgraphs (CSSGs) of graph c and
is the key quantity of network reliability (Valiant,
1979; Coulbourn, 1987). Finding the number of
CSSGs of a general graph is a known #P-hard prob-
lem (Welsh, 1997). In our setting, graphs have spe-
cific formation (source and target chains connected
via translation edges) and we are interested in the
deletion of translation edges only; it is possible to
33
compute gain(?) in polynomial time, but we do not
go into further details here.
4 Extracting Bilingual Segmentations with
the Cross-Entropy Method
Equipped with the measures of Sections 2 and 3 we
turn to extracting anN -best list of bilingual segmen-
tations for a given sentence pair. The search space is
exponential in the total number of words of the sen-
tence pair. We propose a new approach for this task,
by noting a direct connection with the combinato-
rial problems that can be solved efficiently and ef-
fectively with the Cross-Entropy (CE) method (Ru-
binstein, 1997).
The CE method is an iterative self-tuning sam-
pling method that has applications in various com-
binatorial and continuous global optimization prob-
lems as well as in rare event detection. A detailed
account on the CE method is beyond the scope of
this work, and we thus simply describe its applica-
tion to our problem.
In particular, we first establish the connection be-
tween the most basic form of the CE method and the
problem of finding the best monolingual segmen-
tation of a sentence, with respect to some scoring
function (not necessarily the one that was introduced
in Section 2). This connection yields a simple, ef-
ficient and effective algorithm for the monolingual
maximization problem. Then, the transition to the
bilingual level is done by incorporating the measure
of Section 3 in the algorithm, thus performing joint
maximization of surface and structural quality. Fi-
nally, the generation of theN -best list will be trivial.
A segmentation of a given sentence has a bit-
string representation in the following way: If two
consecutive words in the sentence belong to the
same segment in the segmentation, then this pair of
words is encoded by ?1?, otherwise by ?0?. Such a
representation is bijective and, thus, for the rest of
this section, we do not distinguish between a seg-
mentation and its bit-string representation. In this
setting, the CE method takes its most basic form
(De Boer et al., 2005). In a nutshell, it is a re-
peated application of (a) sampling bit-strings from
a parametrized probability mass function, (b) scor-
ing them and keeping only a small high-performing
subsample, and (c) updating the parameters of the
probability mass function based on that subsample
only.
We assume no prior knowledge on the quality
of bit-strings, so that they are all equally likely. In
other words, each position of a randomly chosen
bit-string can be either a ?0? or a ?1? with probability
1/2. The aim is to tune these position probabilities
towards the best bit-string, with respect to some
scoring function g. In particular, let the sentence
have n words and let ` = n ? 1 be the length of
bit-strings. A bit-string labeled by an integer i is
denoted by b
i
and its jth bit by b
ij
. The algorithm is
as follows:
0. Initialize the bit-string position probabilities
p
0
= (p
0
1
, ..., p
0
`
) = (1/2, ..., 1/2) and set M = 20`
(sample size), ? = d1%Me (keep top 1% of
samples), ? = 0.7 (smoothing parameter) and t = 1
(iteration).
1. Generate a sample b
1
, ..., b
M
of bit-strings, each
of length `, such that b
ij
?Bernoulli(p
t?1
j
), for all
i = 1, ...,M and j = 1, ..., `.
1.1 Compute scores g(b
1
), ..., g(b
M
).
1.2 Order them descendingly as g(b
pi(1)
) > ... >
g(b
pi(M)
).
2. Focus on the best performing ones: Compute
?
t
= g(b
pi(?)
); samples performing less than this
threshold will be ignored.
3. Use the best performing sub-sample of b
1
, ..., b
M
to update position probabilities:
p
t
j
=
?
M
i=1
I
i
(?
t
)b
ij
?
M
i=1
I
i
(?
t
)
, j = 1, ..., `, (9)
where the choice function I
i
is given by
I
i
(?
t
) =
{
1, if g(b
i
) > ?
t
0, otherwise.
4. Smooth the updated position probabilities as
p
t
j
:= ?p
t
j
+ (1 ? ?)p
t?1
j
, j = 1, ..., `. (10)
E. If for some t > 5we have ?
t
= ?
t?1
= ... = ?
t?5
then stop. Else, t := t + 1 and go to Step 1.
34
The values for the parameters M , ? and ? re-
ported here are in line with the ones suggested in the
literature (Rubinstein and Kroese, 2004) for combi-
natorial problems such as this one. After the execu-
tion of the algorithm, the updated vector of position
probabilities converges to sequence of ?0?s and ?1?s,
which corresponds to the best segmentation under g.
The extension to bilingual level is done by incor-
porating the structural quality measure of Section 3.
The setting is similar, i.e., samples are again bit-
strings, but of length ` = n + m ? 2, where n and
m are the number of words in the source and tar-
get sentence respectively. The first n ? 1 bits corre-
spond to the source sentence and the rest to the target
sentence. The surface quality score of such a bit-
string is given by the harmonic mean of its source
and target surface quality scores.
1
The bit-string
scoring function throughout Steps 1 ? 3 is given by
the harmonic mean of surface and structural quality
scores. Finally, N -best lists are trivially generated,
simply by collecting the top-N performing accumu-
lated samples of a maximization process.
5 Experiments
Given a sentence pair with known and fixed word
alignments, the result of the method described in
Section 4 is an N -best list of bilingual segmenta-
tions of such a pair. The objective function provides
a balance between compositional expressive power
of segments in both languages and synchronization
via word alignments. Thus, each (continuous) com-
ponent of such a bilingual segmentation leads to the
extraction of a high quality phrase pair.
As was mentioned in Section 3, each extracted
phrase pair of standard phrase-based SMT is con-
structed from a component or from the union of
components of an unsegmented word-aligned sen-
tence pair. For each sentence pair, all possible
(continuous) components and (continuous) unions
of components give rise to the extracted (contin-
uous) phrase pairs. In this section we investigate
the impact to SMT models and translation quality,
when extracting phrase pairs (from the N -best lists)
1
As it was mentioned in Section 2 the surface quality score
in (6) is a real number. At each iteration of the algorithm the
surface score of a segmentation can be converted into a number
in [0, 1] via Min-Max normalization. This holds for both source
and target sides of a bit-string (independently).
Cz?En De?En
Europarl (v7) 642,505 1,889,791
News Commentary (v8) 139,679 177,079
Total 782,184 2,066,870
Table 1: Number of filtered parallel sentences for
Czech?English and German?English.
that correspond to components only. A reduction
in phrase-table size is guaranteed because we are
essentially extracting only a subset of all possible
continuous phrase pairs. The challenge is to verify
whether this subset can provide a sufficient transla-
tion model.
Both the baseline and our system are standard
phrase-basedMT systems. Bidirectional word align-
ments are generated with GIZA++ (Och and Ney,
2003) and ?grow-diag-final-and?. These are used
to construct a phrase-table with bidirectional phrase
probabilities, lexical weights and a reordering model
with monotone, swap and discontinuous orienta-
tions, conditioned on both the previous and the next
phrase. 4-gram interpolated language models with
Kneser-Ney smoothing are built with SRILM (Stol-
cke, 2002). A distortion limit of 6 and a phrase-
penalty are also used. All model parameters are
tuned with MERT (Och, 2003). Decoding during
tuning and testing is done with Moses (Koehn et. al,
2007). Since our system only affects which phrases
are extracted, lexical weights and reordering orien-
tations are the same for both systems.
Datasets are from the WMT?13 translation task
(Bojar et al., 2013): Translation and reordering
models are trained on Czech?English and German?
English corpora (Table 1). Language models and
segment measures gain , as defined in (5), are trained
on 35.3M Czech, 50.0M German and 94.5M En-
glish sentences from the provided monolingual data.
Tuning is done on newstest2010 and performance
is evaluated on newstest2008, newstest2009, new-
stest2011 and newstest2012 with BLEU (Papineni
et al., 2001).
In our experiments the size of anN -best list varies
according to the total number of words in the sen-
tence pair, say w. For the purposes of phrase ex-
traction in SMT we would ideally require all local
maxima to be part of an N -best list. This would
35
Method
Czech?English English?Czech Czech?English
?08 ?09 ?11 ?12 ?08 ?09 ?11 ?12 PT size (retain%)
Baseline 19.6 20.6 22.6 20.6 14.8 15.6 16.6 14.9 44.6M (100%)
N -best 19.7 20.4 22.4 20.3 14.4 15.2 16.3 14.3 4.4M (9.8%)
N -best & unseg. 19.6 20.5 22.6 20.7 14.6 15.4 16.8 14.7 4.6M (10.4%)
Table 2: BLEU scores and phrase-table (PT) sizes for Czech?English. Phrase-table of ?Baseline? is con-
structed from all consistent phrase pairs. Phrase-table of ?N -best? is constructed from consistent phrase
pairs that are components of the top-N bilingual word-aligned segmentations of each sentence pair. Simi-
larly for ?N -best & unseg.?, but consistent phrase pairs that are components of each (unsegmented) sentence
pair are also included.
Method
German?English English?German German?English
?08 ?09 ?11 ?12 ?08 ?09 ?11 ?12 PT size (retain%)
Baseline 21.4 20.8 21.3 22.1 15.1 15.1 16.0 16.5 102.3M (100%)
N -best 21.3 20.6 21.3 21.8 15.0 15.0 15.6 16.0 9.4M (9.2%)
N -best & unseg. 21.5 20.8 21.5 22.0 15.4 15.2 15.7 16.2 9.9M (9.7%)
Table 3: Similar to Table 2, but for German?English.
guarantee the extraction of all high quality phrase
pairs, with (empirically) desired variations, while
keeping N small. Since the CE method performs
global optimization, the resulting members of an N -
best list are in the vicinity of the global maximum.
Consequently, we cannot guarantee the inclusion of
local maxima. We set N = d30%we so that at
least some variation from the global maximum is in-
cluded, but is not large enough to contaminate the
lists with noisy bilingual segmentations. The result-
ing lists have 22 bilingual segmentations on aver-
age for both language pairs. Figure 4 shows typical
German?English best performing bilingual segmen-
tations.
BLEU scores are reported in Tables 2 and 3 for
Czech?English and German?English respectively.
Methods ?Baseline? and ?N -best? are the ones de-
scribed above. Phrase-table sizes are reduced as
expected and performance when translating to En-
glish is comparable. The significant drops in new-
stest2012 when translating from the morphologi-
cally poorer language (English) prompts us to in-
clude more ?basic? phrase pairs in the phrase-tables.
This leads to augmenting each N -best list by its un-
segmented sentence pair. Consequently, method ?N -
best & unseg.? extracts the same phrase pairs as ?N -
best?, together with those from components of the
unsegmented sentence pairs. As a result, transla-
tion quality is comparable to ?Baseline? across all
language directions and small phrase-table sizes are
retained.
6 Discussion and Future Work
This work can also be viewed as an attempt to un-
derstand bilinguality as a generalization of mono-
linguality. There is conceptual common ground on
what gain(x) for phrase x (Section 2) or component
x (Section 3) computes. In both cases it measures
how ?stable? a unit is. The stability of a phrase x is
determined by how difficult it is to split x into multi-
ple phrases. The partially ordered set framework of
partition refinements is the natural setting for such
computations. In order to determine the stability
of a component we turn to empirical evidence from
SMT: ?good? phrase pairs are extracted from com-
ponents or unions of components of the graph that
represents word-aligned sentence pairs. The stabil-
ity of a component x is therefore determined by how
difficult it is to break x into multiple components. It
is thus interesting to investigate whether there exists
a general approach that unifies partition refinements
and network reliability for the purpose of identifying
highly stable multilingual units.
36
  
12 34??? ? ? ? ? 34 ??4 ?? ?? ?? ?2??? ?3? ? ?
3?? ? ? ? ?34??4 ?? ? ?
?? ? ? 2?4??
?? ?? ??4?41?
14 ?? ? 3?? ?23?? ? ? ? ? ? ? ? ? ? ? ? ? ?3? 3 ?? ? ? ? ?3? ? ?
?4??? ? ?34?14? 3???4? ?? ? ? ? ? ? ?
?3? ?? ? ? ?3? ?? ? ? ? ? ?4 12 432?4 ?? ?221??1?4
2? ?? ? ?4 ?3? ?? ? ? ? ? ? ? ?4 ??3?? ? ? ? ? ? 4?1??221?
14 ???34???4??4 ??1??? ?4 ?? ? ? ? ? ?1? ?4? ?? ? ? ? ? ? ?1?? ? ?4 ?41?4
4??? ?13?1?4? ?? ? ? ? ?4 ??1?? ? ? ?34? 34? ?? ? ? ? ? ? ? ?34 ?41?4 ???? ? ? ? ? 14
??? ? ?? ?1?421?1???1??? ?32?1?3?
?? ? ? ? ? ?4??21?1?3??32?1?3??
?? ?
?? ?4?
3???? ? ? ?4??? ? ? ? ? ? ? ? ?? ?? ? ? ? ? ? ? ? ? ?4?4???? ?1??? ?4??1?
?? ? ? ??? ? ? ??? ?3143?????? ?2?4?3???34???? ? ? ?2?4??? ??4
??44?4
?34
?3???? ? ?4?1??? ? ? ? ? ? ?4?1??1??3??421??1?
??3???? ? ?? ?3???? ??1??? ? ?? ? ?2??? ? ?? ? ?
?
?? ?
3?
3???11?????? ? ?2???3?1????1?1??21??
??3412???34?2??1????2???3?1??? ? ??3??3213??4???4??
Figure 4: Typical fragments from best performing
German?English segmentations.
The focus has been on bilingual segmentations,
but as was mentioned in Section 2, it is possible
to apply the CE method for generating monolingual
segmentations. By using (6) as the objective func-
tion, we observed that the resulting segmentations
yield promising applications in n-gram topic model-
ing, named entity recognition and Chinese segmen-
tation. However, in the spirit of Ries et al. (1996),
attempts to minimize perplexity instead of maximiz-
ing (6), resulted in larger segments and the segment
quality definition of Section 1 was not met.
The sizes of the resulting phrase-tables together
with the type of phrase pairs that are extracted lead
to applications involving discontinuous phrase pairs.
In (Galley and Manning, 2010) there was evidence
that discontinuous phrase pairs that are extracted
from discontinuous components of word-aligned
sentence pairs can improve translation quality.
1
As
the number of such components is much bigger than
the continuous ones, (Gimpel and Smith, 2011) pro-
pose a Bayesian nonparametric model for finding the
most probable discontinuous phrase pairs. This can
also be done from the N -best lists that are generated
in Section 4, and it would be interesting to see the
effect of such phrase pairs in our existing models.
In a longer version of this work we intend to
study the effect in translation quality when varying
some of the parameters (size of N -best lists, sample
sizes for training gain in Section 2 and for the CE
method), as well as when extracting source-driven
bilingual segmentations as in (Sanchis-Trilles et al.,
2011).
7 Conclusions
In this work, we have presented a solution to the
problem of extracting bilingual segmentations in the
presence of word alignments. Two measures that as-
sess the quality of bilingual segmentations based on
the expressive power of segments in both languages
and their synchronization via word alignments have
been introduced. We have established the link be-
tween the CE method and finding the best monolin-
gual and bilingual segmentations. These measures
formed the objective function of the CE method
whose maximization resulted in an N -best list of
bilingual segmentations for a given sentence pair.
By extracting only phrase pairs that correspond to
components from bilingual segmentations of those
lists, we found that phrase table sizes can be reduced
with insignificant loss in translation quality.
Acknowledgements
This research was funded in part by the Euro-
pean Commission through the CoSyne project FP7-
ICT-4-248531 and the Netherlands Organisation
for Scientific Research (NWO) under project nr.
639.022.213.
1
By ?discontinuous component? we mean a component
whose source or target words (vertices) form a discontinuous
substring in the source or target sentence respectively.
37
References
Graeme Blackwood, Adria de Gispert, and William
Byrne. 2008. Phrasal Segmentation Models for Sta-
tistical Machine Translation. In COLING.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp Koehn,
Christof Monz, Matt Post, Radu Soricut, and Lucia
Specia. 2013. Findings of the 2013 Workshop on Sta-
tistical Machine Translation. In WMT.
Charlie J. Coulbourn. 1987. The Combinatorics of Net-
work Reliability. Oxford University Press.
Pieter-Tjerk De Boer, Dirk P. Kroese, Shie Mannor, and
Reuven Y. Rubinstein. 2005. A Tutorial on the Cross-
Entropy Method. Annals of Operations Research,
vol. 134, pages 19?67.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate Non-Hierarchical Phrase-Based Translation. In
NAACL.
Kevin Gimpel and Noah A. Smith. 2011. Generative
Models of Monolingual and Bilingual Gappy Patterns.
In WMT.
Jin Guo. 1997. Critical Tokenization and its Properties.
Computational Linguistics, vol. 23(4), pages 569?596.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by discard-
ing most of the phrase-table. In EMNLP-CoNLL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ond?rej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL,
demonstration session.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In HLT-NAACL.
Spyros Martzoukos, Christophe Costa Flor?encio, and
Christof Monz. 2013. Investigating Connectivity and
Consistency Criteria for Phrase Pair Extraction in Sta-
tistical Machine Translation. In Meeting on Mathe-
matics of Language.
Bernard Monjardet. 1981. Metrics on partially ordered
sets ? a survey. Discrete Mathematics, vol. 35, pages
173?184.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In ACL.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, vol. 29 (1), pages 19?51.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In EMNLP-VLC.
Chris Orum and Cliff A. Joslyn. 2009. Valuations and
Metrics on Partially Ordered Sets. Computing Re-
search Repository - CORR, vol. abs/0903.2.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Michael Paul, Andrew Finch, and Eiichiro Sumita. 2010.
Integration of Multiple Bilingually-Learned Segmen-
tation Schemes into Statistical Machine Translation.
In WMT and MetricsMATR.
Klaus Ries, Finn Dag Bu, and Alex Waibel. 1996. Class
phrase models for language modeling. In ICSLP.
Reuven Y. Rubinstein. 1997. Optimization of Computer
Simulation Models with Rare Events. European Jour-
nal of Operations Research, vol. 99, pages 89?112.
Reuven Y. Rubinstein and Dirk P. Kroese. 2004. The
Cross-Entropy Method: A Unified Approach to Com-
binatorial Optimization, Monte-Carlo Simulation and
Machine Learning. Springer-Verlag, New York.
Germ?an Sanchis-Trilles, Daniel Ortiz-Mart??nez, Jes?us
Gonz?alez-Rubio, Jorge Gonz?alez, and Francisco
Casacuberta. 2011. Bilingual segmentation for
phrasetable pruning in Statistical Machine Translation.
In EAMT.
Richard P. Stanley. 1997. Enumerative Combinatorics,
Volume 1. Cambridge University Press.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In ICSLP.
Leslie G. Valiant. 1979. The complexity of enumeration
and reliability problems. SIAM Journal on Comput-
ing, vol. 8, pages 410?421.
Dominic J. A. Welsh. 1997. Approximate counting.
Surveys in Combinatorics, London Math. Soc. Lecture
Notes Ser., 241, pages 287?324.
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
Systematic Comparison of Phrase Table Pruning Tech-
niques. In EMNLP.
38
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 17?53,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Findings of the 2010 Joint Workshop on
Statistical Machine Translation and Metrics for Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb@cs.jhu.edu
Philipp Koehn
University of Edinburgh
pkoehn@inf.ed.ac.uk
Christof Monz
University of Amsterdam
c.monz@uva.nl
Kay Peterson and Mark Przybocki
National Institute of Standards and Technology
kay.peterson,mark.przybocki@nist.gov
Omar F. Zaidan
Johns Hopkins University
ozaidan@cs.jhu.edu
Abstract
This paper presents the results of the
WMT10 and MetricsMATR10 shared
tasks,1 which included a translation task,
a system combination task, and an eval-
uation task. We conducted a large-scale
manual evaluation of 104 machine trans-
lation systems and 41 system combina-
tion entries. We used the ranking of these
systems to measure how strongly auto-
matic metrics correlate with human judg-
ments of translation quality for 26 metrics.
This year we also investigated increasing
the number of human judgments by hiring
non-expert annotators through Amazon?s
Mechanical Turk.
1 Introduction
This paper presents the results of the shared
tasks of the joint Workshop on statistical Ma-
chine Translation (WMT) and Metrics for MA-
chine TRanslation (MetricsMATR), which was
held at ACL 2010. This builds on four previ-
ous WMT workshops (Koehn and Monz, 2006;
Callison-Burch et al, 2007; Callison-Burch et al,
2008; Callison-Burch et al, 2009), and one pre-
vious MetricsMATR meeting (Przybocki et al,
2008). There were three shared tasks this year:
a translation task between English and four other
European languages, a task to combine the out-
put of multiple machine translation systems, and
a task to predict human judgments of translation
quality using automatic evaluation metrics. The
1The MetricsMATR analysis was not complete in time for
the publication deadline. An updated version of paper will be
made available on http://statmt.org/wmt10/ prior
to July 15, 2010.
performance on each of these shared task was de-
termined after a comprehensive human evaluation.
There were a number of differences between
this year?s workshop and last year?s workshop:
? Non-expert judgments ? In addition to hav-
ing shared task participants judge translation
quality, we also collected judgments from
non-expert annotators hired through Ama-
zon?s Mechanical Turk. By collecting a large
number of judgments we hope to reduce the
burden on shared task participants, and to in-
crease the statistical significance of our find-
ings. We discuss the feasibility of using non-
experts evaluators, by analyzing the cost, vol-
ume and quality of non-expert annotations.
? Clearer results for system combination ?
This year we excluded Google translations
from the systems used in system combina-
tion. In last year?s evaluation, the large mar-
gin between Google and many of the other
systems meant that it was hard to improve on
when combining systems. This year, the sys-
tem combinations perform better than their
component systems more often than last year.
? Fewer rule-based systems ? This year there
were fewer rule-based systems submitted. In
past years, University of Saarland compiled a
large set of outputs from rule-based machine
translation (RBMT) systems. The RBMT
systems were not submitted this year. This
is unfortunate, because they tended to outper-
form the statistical systems for German, and
they were often difficult to rank properly us-
ing automatic evaluation metrics.
The primary objectives of this workshop are to
evaluate the state of the art in machine transla-
17
tion, to disseminate common test sets and pub-
lic training data with published performance num-
bers, and to refine evaluation methodologies for
machine translation. As with past years, all of the
data, translations, and human judgments produced
for our workshop are publicly available.2 We hope
they form a valuable resource for research into sta-
tistical machine translation, system combination,
and automatic evaluation of translation quality.
2 Overview of the shared translation and
system combination tasks
The workshop examined translation between En-
glish and four other languages: German, Span-
ish, French, and Czech. We created a test set for
each language pair by translating newspaper arti-
cles. We additionally provided training data and
two baseline systems.
2.1 Test data
The test data for this year?s task was created
by hiring people to translate news articles that
were drawn from a variety of sources from mid-
December 2009. A total of 119 articles were se-
lected, in roughly equal amounts from a variety
of Czech, English, French, German and Spanish
news sites:3
Czech: iDNES.cz (5), iHNed.cz (1), Lidov-
ky (16)
French: Les Echos (25)
Spanish: El Mundo (20), ABC.es (4), Cinco
Dias (11)
English: BBC (5), Economist (2), Washington
Post (12), Times of London (3)
German: Frankfurter Rundschau (11), Spie-
gel (4)
The translations were created by the profes-
sional translation agency CEET4. All of the trans-
lations were done directly, and not via an interme-
diate language.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
2http://statmt.org/wmt10/results.html
3For more details see the XML test files. The docid
tag gives the source and the date for each document in the
test set, and the origlang tag indicates the original source
language.
4http://www.ceet.eu/
train language models, and development sets to
tune parameters. Some statistics about the train-
ing materials are given in Figure 1.
2.3 Baseline systems
To lower the barrier of entry for newcomers to
the field, we provided two open source toolkits
for phrase-based and parsing-based statistical ma-
chine translation (Koehn et al, 2007; Li et al,
2009).
2.4 Submitted systems
We received submissions from 33 groups from 29
institutions, as listed in Table 1, a 50% increase
over last year?s shared task.
We also evaluated 2 commercial off the shelf
MT systems, and two online statistical machine
translation systems. We note that these companies
did not submit entries themselves. The entries for
the online systems were done by translating the
test data via their web interfaces. The data used
to train the online systems is unconstrained. It is
possible that part of the reference translations that
were taken from online news sites could have been
included in the online systems? language models.
2.5 System combination
In total, we received 153 primary system submis-
sions along with 28 secondary submissions. These
were made available to participants in the sys-
tem combination shared task. Based on feedback
that we received on last year?s system combina-
tion task, we provided two additional resources to
participants:
? Development set: We reserved 25 articles
to use as a dev set for system combination.
These were translated by all participating
sites, and distributed to system combination
participants along with reference translations.
? n-best translations: We requested n-best
lists from sites whose systems could produce
them. We received 20 n-best lists accompa-
nying the system submissions.
Table 2 lists the 9 participants in the system
combination task.
3 Human evaluation
As with past workshops, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention
18
Europarl Training Corpus
Spanish? English French? English German? English
Sentences 1,650,152 1,683,156 1,540,549
Words 47,694,560 46,078,122 50,964,362 47,145,288 40,756,801 43,037,967
Distinct words 173,033 95,305 123,639 95,846 316,365 92,464
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 98,598 84,624 100,269 94,742
Words 2,724,141 2,432,064 2,405,082 2,101,921 2,505,583 2,443,183 2,050,545 2,290,066
Distinct words 69,410 46,918 53,763 43,906 101,529 47,034 125,678 45,306
United Nations Training Corpus
Spanish? English French? English
Sentences 6,222,450 7,230,217
Words 213,877,170 190,978,737 243,465,100 216,052,412
Distinct words 441,517 361,734 402,491 412,815
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 7,227,409
Words 72,993,427 84,856,749
Distinct words 1,088,642 522,770
Europarl Language Model Data
English Spanish French German
Sentence 1,843,035 1,822,021 1,855,589 1,772,039
Words 50,132,615 51,223,902 54,273,514 43,781,217
Distinct words 99,206 178,934 127,689 328,628
News Language Model Data
English Spanish French German Czech
Sentence 48,653,884 3,857,414 15,670,745 17,474,133 13,042,040
Words 1,148,480,525 106,716,219 382,563,246 321,165,206 205,614,201
Distinct words 1,451,719 548,169 998,595 1,855,993 1,715,376
News Test Set
English Spanish French German Czech
Sentences 2489
Words 62,988 65,654 68,107 62,390 53,171
Distinct words 9,457 11,409 10,775 12,718 15,825
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and
the number of distinct words is based on the provided tokenizer.
19
ID Participant
AALTO Aalto University, Finland (Virpioja et al, 2010)
CAMBRIDGE Cambridge University (Pino et al, 2010)
CMU Carnegie Mellon University?s Cunei system (Phillips, 2010)
CMU-STATXFER Carnegie Mellon University?s statistical transfer system (Hanneman et al, 2010)
COLUMBIA Columbia University
CU-BOJAR Charles University Bojar (Bojar and Kos, 2010)
CU-TECTO Charles University Tectogramatical MT (Z?abokrtsky? et al, 2010)
CU-ZEMAN Charles University Zeman (Zeman, 2010)
DCU Dublin City University (Penkale et al, 2010)
DFKI Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (Federmann et al, 2010)
EU European Parliament, Luxembourg (Jellinghaus et al, 2010)
EUROTRANS commercial MT provider from the Czech Republic
FBK Fondazione Bruno Kessler (Hardmeier et al, 2010)
GENEVA University of Geneva
HUICONG Shanghai Jiao Tong University (Cong et al, 2010)
JHU Johns Hopkins University (Schwartz, 2010)
KIT Karlsruhe Institute for Technology (Niehues et al, 2010)
KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)
LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al, 2010)
LIMSI LIMSI (Allauzen et al, 2010)
LIU Linko?ping University (Stymne et al, 2010)
LIUM University of Le Mans (Lambert et al, 2010)
NRC National Research Council Canada (Larkin et al, 2010)
ONLINEA an online machine translation system
ONLINEB an online machine translation system
PC-TRANS commercial MT provider from the Czech Republic
POTSDAM Potsdam University
RALI RALI - Universite? de Montre?al (Huet et al, 2010)
RWTH RWTH Aachen (Heger et al, 2010)
SFU Simon Fraser University (Sankaran et al, 2010)
UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010)
UEDIN University of Edinburgh (Koehn et al, 2010)
UMD University of Maryland (Eidelman et al, 2010)
UPC Universitat Polite`cnica de Catalunya (Henr??quez Q. et al, 2010)
UPPSALA Uppsala University (Tiedemann, 2010)
UPV Universidad Polite?cnica de Valencia (Sanchis-Trilles et al, 2010)
UU-MS Uppsala University - Saers (Saers et al, 2010)
Table 1: Participants in the shared translation task. Not all groups participated in all language pairs.
20
ID Participant
BBN-COMBO BBN system combination (Rosti et al, 2010)
CMU-COMBO-HEAFIELD CMU system combination (Heafield and Lavie, 2010)
CMU-COMBO-HYPOSEL CMU system combo with hyp. selection (Hildebrand and Vogel, 2010)
DCU-COMBO Dublin City University system combination (Du et al, 2010)
JHU-COMBO Johns Hopkins University system combination (Narsale, 2010)
KOC-COMBO Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)
LIUM-COMBO University of Le Mans system combination (Barrault, 2010)
RWTH-COMBO RWTH Aachen system combination (Leusch and Ney, 2010)
UPV-COMBO Universidad Polite?cnica de Valencia (Gonza?lez-Rubio et al, 2010)
Table 2: Participants in the system combination task.
Language Pair Sentence Ranking Edited Translations Yes/No Judgments
German-English 5,212 830 824
English-German 6,847 755 751
Spanish-English 5,653 845 845
English-Spanish 2,587 920 690
French-English 4,147 925 921
English-French 3,981 1,325 1,223
Czech-English 2,688 490 488
English-Czech 6,769 1,165 1,163
Totals 37,884 7,255 6,905
Table 3: The number of items that were collected for each task during the manual evaluation. An item
is defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/no
judgment in the judgment task.
21
that automatic measures are an imperfect substi-
tute for human assessment of translation quality.
Therefore, we define the manual evaluation to be
primary, and use the human judgments to validate
automatic metrics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct it on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partic-
ipants, interested volunteers, and a small number
of paid annotators. More than 120 people partic-
ipated in the manual evaluation5, with 89 people
putting in more than an hour?s worth of effort, and
29 putting in more than four hours. A collective
total of 337 hours of labor was invested.6
We asked people to evaluate the systems? output
in two different ways:
? Ranking translated sentences relative to each
other. This was our official determinant of
translation quality.
? Editing the output of systems without dis-
playing the source or a reference translation,
and then later judging whether edited transla-
tions were correct.
The total number of judgments collected for the
different modes of annotation is given in Table 3.
In all cases, the output of the various translation
systems were judged on equal footing; the output
of system combinations was judged alongside that
of the individual system, and the constrained and
unconstrained systems were judged together.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the in-
structions simple:
Rank translations from Best to Worst rel-
ative to the other choices (ties are al-
lowed).
5We excluded data from three errant annotators, identified
as follows. We considered annotators completing at least 3
screens, whose P (A) with others (see 3.2) is less than 0.33.
Out of seven such annotators, four were affiliated with shared
task teams. The other three had no apparent affiliation, and
so we discarded their data, less than 5% of the total data.
6Whenever an annotator appears to have spent more than
ten minutes on a single screen, we assume they left their sta-
tion and left the window open, rather than actually needing
more than ten minutes. In those cases, we assume the time
spent to be ten minutes.
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions. For each of the lan-
guage pairs, there were more than 5 submissions.
We did not attempt to get a complete ordering over
the systems, and instead relied on random selec-
tion and a reasonably large sample size to make
the comparisons fair.
Relative ranking is our official evaluation met-
ric. Individual systems and system combinations
are ranked based on how frequently they were
judged to be better than or equal to any other sys-
tem. The results of this are reported in Section 4.
Appendix A provides detailed tables that contain
pairwise comparisons between systems.
3.2 Inter- and Intra-annotator agreement in
the ranking task
We were interested in determining the inter- and
intra-annotator agreement for the ranking task,
since a reasonable degree of agreement must ex-
ist to support our process as a valid evaluation
setup. To ensure we had enough data to measure
agreement, we purposely designed the sampling of
source segments shown to annotators so that items
were likely to be repeated, both within an annota-
tor?s assigned tasks and across annotators. We did
so by assigning an annotator a batch of 20 screens
(each with three ranking sets; see 3.1) that were to
be completed in full before generating new screens
for that annotator.
Within each batch, the source segments for nine
of the 20 screens (45%) were chosen from a small
pool of 60 source segments, instead of being sam-
pled from the larger pool of 1,000 source segments
designated for the ranking task.7 The larger pool
was used to choose source segments for nine other
screens (also 45%). As for the remaining two
screens (10%), they were chosen randomly from
the set of eighteen screens already chosen. Fur-
thermore, in the two ?local repeat? screens, the
system choices were also preserved.
Heavily sampling from a small pool of source
segments ensured we had enough data to measure
inter-annotator agreement, while purposely mak-
ing 10% of each annotator?s screens repeats of pre-
viously seen sets in the same batch ensured we
7Each language pair had its own 60-sentence pool, dis-
joint from other language pairs? pools, but ach of the 60-
sentence pools was a subset of the 1,000-sentence pool.
22
INTER-ANNOTATOR AGREEMENT
P (A) K
With references 0.658 0.487
Without references 0.626 0.439
WMT ?09 0.549 0.323
INTRA-ANNOTATOR AGREEMENT
P (A) K
With references 0.755 0.633
Without references 0.734 0.601
WMT ?09 0.707 0.561
Table 4: Inter- and intra-annotator agreement for
the sentence ranking task. In this task, P (E) is
0.333.
had enough data to measure intra-annotator agree-
ment.
We measured pairwise agreement among anno-
tators using the kappa coefficient (K), which is de-
fined as
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance.
For inter-annotator agreement for the ranking
tasks we calculated P (A) by examining all pairs
of systems which had been judged by two or more
judges, and calculated the proportion of time that
they agreed thatA > B, A = B, orA < B. Intra-
annotator agreement was computed similarly, but
we gathered items that were annotated on multiple
occasions by a single annotator.
Table 4 gives K values for inter-annotator and
intra-annotator agreement. These give an indi-
cation of how often different judges agree, and
how often single judges are consistent for repeated
judgments, respectively. The exact interpretation
of the kappa coefficient is difficult, but according
to Landis and Koch (1977), 0? .2 is slight, .2? .4
is fair, .4 ? .6 is moderate, .6 ? .8 is substantial
and the rest is almost perfect.
Based on these interpretations the agreement
for sentence-level ranking is moderate for inter-
annotator agreement and substantial for intra-
annotator agreement. These levels of agreement
are higher than in previous years, partially due to
the fact that that year we randomly included the
references along the system outputs. In general,
judges tend to rank the reference as the best trans-
lation, so people have stronger levels of agreement
when it is included. That said, even when compar-
isons involving reference are excluded, we still see
an improvement in agreement levels over last year.
3.3 Editing machine translation output
In addition to simply ranking the output of sys-
tems, we also had people edit the output of MT
systems. We did not show them the reference
translation, which makes our edit-based evalu-
ation different from the Human-targeted Trans-
lation Edit Rate (HTER) measure used in the
DARPA GALE program (NIST, 2008). Rather
than asking people to make the minimum number
of changes to the MT output in order capture the
same meaning as the reference, we asked them to
edit the translation to be as fluent as possible with-
out seeing the reference. Our hope was that this
would reflect people?s understanding of the out-
put.
The instructions given to our judges were as fol-
lows:
Correct the translation displayed, mak-
ing it as fluent as possible. If no correc-
tions are needed, select ?No corrections
needed.? If you cannot understand the
sentence well enough to correct it, select
?Unable to correct.?
A screenshot is shown in Figure 2. This year,
judges were shown the translations of 5 consec-
utive source sentences, all produced by the same
machine translation system. In last year?s WMT
evaluation they were shown only one sentence at a
time, which made the task more difficult because
the surrounding context could not be used as an
aid to understanding.
Since we wanted to prevent judges from see-
ing the reference before editing the translations,
we split the test set between the sentences used
in the ranking task and the editing task (because
they were being conducted concurrently). More-
over, annotators edited only a single system?s out-
put for one source sentence to ensure that their un-
derstanding of it would not be influenced by an-
other system?s output.
3.4 Judging the acceptability of edited output
Halfway through the manual evaluation period, we
stopped collecting edited translations, and instead
asked annotators to do the following:
23
Edit Machine Translation Outputs
Instructions:
You are shown several machine translation outputs.
Your task is to edit each translation to make it as fluent as possible.
It is possible that the translation is already fluent. In that case, select No corrections needed.
If you cannot understand the sentence well enough to correct it, select Unable to correct.
The sentences are all from the same article. You can use the earlier and later sentences
to help understand a confusing sentence.
Your edited translations           The machine translations
   
The shortage of snow in mountain worries the hoteliers
Edited     No corrections needed     Unable to
correct         Reset
 
The shortage of snow in mountain
worries the hoteliers
   
The deserted tracks are not putting down problem only at the exploitants 
of skilift.
Edited     No corrections needed     Unable to
correct         Reset
 
The deserted tracks are not
putting down problem only at the
exploitants of skilift.
   
The lack of snow deters the people to reserving their stays at the ski in 
the hotels and pension.
Edited     No corrections needed     Unable to
correct         Reset
 
The lack of snow deters the people
to reserving their stays at the ski
in the hotels and pension.
   
Thereby, is always possible to track free bedrooms for all the dates in 
winter, including Christmas and Nouvel An.
Edited     No corrections needed     Unable to
correct         Reset
 
Thereby, is always possible to
track free bedrooms for all the
dates in winter, including
Christmas and Nouvel An.
   
We have many of visit on our site
Figure 2: This screenshot shows what an annotator sees when beginning to edit the output of a machine
translation system.
24
Indicate whether the edited transla-
tions represent fully fluent and meaning-
equivalent alternatives to the reference
sentence. The reference is shown with
context, the actual sentence is bold.
In addition to edited translations, unedited items
that were either marked as acceptable or as incom-
prehensible were also shown. Judges gave a sim-
ple yes/no indication to each item.
4 Translation task results
We used the results of the manual evaluation to
analyze the translation quality of the different sys-
tems that were submitted to the workshop. In our
analysis, we aimed to address the following ques-
tions:
? Which systems produced the best translation
quality for each language pair?
? Did the system combinations produce better
translations than individual systems?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 5 shows the best individual systems. We
define the best systems as those which had no
other system that was statistically significantly
better than them under the Sign Test at p ? 0.1.
Multiple systems are listed as the winners for
many language pairs because it was not possible to
draw a statistically significant difference between
the systems. There is no individual system clearly
outperforming all other systems across the differ-
ent language pairs. With the exception of French-
English and English-French one can observe that
top-performing constrained systems did as well as
the unconstrained system ONLINEB.
Table 6 shows the best combination systems.
For all language directions, except Spanish-
English, one can see that the system combina-
tion runs outperform the individual systems and
that in most cases the differences are statistically
significant. While this is to be expected, system
combination is not guaranteed to improve perfor-
mance as some of the lower ranked combination
runs show, which are outperformed by individual
systems. Also note that except for Czech-English
translation the online systems ONLINEA and ON-
LINEB where not included for the system combi-
nation runs
Understandability
Our hope is that judging the acceptability of edited
output as discussed in Section 3 gives some indi-
cation of how often a system?s output was under-
standable. Figure 3 gives the percentage of times
that each system?s edited output was judged to
be acceptable (the percentage also factors in in-
stances when judges were unable to improve the
output because it was incomprehensible).
This style of manual evaluation is experimental
and should not be taken to be authoritative. Some
caveats about this measure:
? There are several sources of variance that are
difficult to control for: some people are better
at editing, and some sentences are more dif-
ficult to edit. Therefore, variance in the un-
derstandability of systems is difficult to pin
down.
? The acceptability measure does not strongly
correlate with the more established method of
ranking translations relative to each other for
all the language pairs.
5 Shared evaluation task overview
In addition to allowing the analysis of subjective
translation quality measures for different systems,
the judgments gathered during the manual evalu-
ation may be used to evaluate how well the au-
tomatic evaluation metrics serve as a surrogate to
the manual evaluation processes. NIST began run-
ning a ?Metrics for MAchine TRanslation? chal-
lenge (MetricsMATR), and presented their find-
ings at a workshop at AMTA (Przybocki et al,
2008). This year we conducted a joint Metrics-
MATR and WMT workshop, with NIST running
the shared evaluation task and analyzing the re-
sults.
In this year?s shared evaluation task 14 different
research groups submitted a total of 26 different
automatic metrics for evaluation:
Aalto University of Science and Technology
(Dobrinkat et al, 2010)
? MT-NCD ? A machine translation metric
based on normalized compression distance
(NCD), a general information-theoretic mea-
sure of string similarity. MT-NCD mea-
sures the surface level similarity between two
strings with a general compression algorithm.
More similar strings can be represented with
25
French-English
551?755 judgments per system
System C? ?others
LIUM ?? Y 0.71
ONLINEB ? N 0.71
NRC ?? Y 0.66
CAMBRIDGE ?? Y +GW 0.66
LIMSI ? Y +GW 0.65
UEDIN Y 0.65
RALI ?? Y +GW 0.65
JHU Y 0.59
RWTH ?? Y +GW 0.55
LIG Y 0.53
ONLINEA N 0.52
CMU-STATXFER Y 0.51
HUICONG Y 0.51
DFKI N 0.42
GENEVA Y 0.27
CU-ZEMAN Y 0.21
English-French
664?879 judgments per system
System C? ?others
UEDIN ?? Y 0.70
ONLINEB ? N 0.68
RALI ?? Y +GW 0.66
LIMSI ?? Y +GW 0.66
RWTH ?? Y +GW 0.63
CAMBRIDGE ? Y +GW 0.63
LIUM Y 0.63
NRC Y 0.62
ONLINEA N 0.55
JHU Y 0.53
DFKI N 0.40
GENEVA Y 0.35
EU N 0.32
CU-ZEMAN Y 0.26
KOC Y 0.26
Czech-English
788?868 judgments per system
System C? ?others
ONLINEB ? N 0.7
UEDIN ? Y 0.61
CMU Y 0.55
CU-BOJAR N 0.55
AALTO Y 0.43
ONLINEA N 0.37
CU-ZEMAN Y 0.22
German-English
723?879 judgments per system
System C? ?others
ONLINEB ? N 0.73
KIT ?? Y +GW 0.72
UMD ?? Y 0.68
UEDIN ? Y 0.66
FBK ? Y +GW 0.66
ONLINEA ? N 0.63
RWTH Y +GW 0.62
LIU Y 0.59
UU-MS Y 0.55
JHU Y 0.53
LIMSI Y +GW 0.52
UPPSALA Y 0.51
DFKI N 0.50
HUICONG Y 0.47
CMU Y 0.46
AALTO Y 0.42
CU-ZEMAN Y 0.36
KOC Y 0.23
English-German
1284?1542 judgments per system
System C? ?others
ONLINEB ? N 0.70
DFKI ? N 0.62
UEDIN ?? Y 0.62
KIT ? Y 0.60
ONLINEA N 0.59
FBK ? Y 0.56
LIU Y 0.55
RWTH Y 0.51
LIMSI Y 0.51
UPPSALA Y 0.47
JHU Y 0.46
SFU Y 0.34
KOC Y 0.30
CU-ZEMAN Y 0.28
English-Czech
1375?1627 judgments per system
System C? ?others
ONLINEB ? N 0.70
CU-BOJAR ? N 0.66
PC-TRANS ? N 0.62
UEDIN ?? Y 0.62
CU-TECTO Y 0.60
EUROTRANS N 0.54
CU-ZEMAN Y 0.50
SFU Y 0.45
ONLINEA N 0.44
POTSDAM Y 0.44
DCU N 0.38
KOC Y 0.33
Spanish-English
1448?1577 judgments per system
System C? ?others
ONLINEB ? N 0.70
UEDIN ?? Y 0.69
CAMBRIDGE Y +GW 0.61
JHU Y 0.61
ONLINEA N 0.54
UPC ? Y 0.51
HUICONG Y 0.50
DFKI N 0.45
COLUMBIA Y 0.45
CU-ZEMAN Y 0.27
English-Spanish
540?722 judgments per system
System C? ?others
ONLINEB ? N 0.71
ONLINEA ? N 0.69
UEDIN ? Y 0.61
DCU N 0.61
DFKI ? N 0.55
JHU ? Y 0.55
UPV ? Y 0.55
CAMBRIDGE ? Y +GW 0.54
UHC-UPV ? Y 0.54
SFU Y 0.40
CU-ZEMAN Y 0.23
KOC Y 0.19
Systems are listed in the order of how often their translations were ranked higher than or equal to any other system. Ties are
broken by direct comparison.
C? indicates constrained condition, meaning only using the supplied training data, standard monolingual linguistic tools, and
optionally the LDC?s GigaWord, which was allowed this year (entries that used the GigaWord are marked +GW).
? indicates a win in the category, meaning that no other system is statistically significantly better at p-level?0.1 in pairwise
comparison.
? indicates a constrained win, no other constrained system is statistically better.
For all pairwise comparisons between systems, please check the appendix.
Table 5: Official results for the WMT10 translation task, based on the human evaluation (ranking trans-
lations relative to each other)
26
French-English
589?716 judgments per combo
System ?others
RWTH-COMBO ? 0.77
CMU-HYP-COMBO ? 0.77
DCU-COMBO ? 0.72
LIUM ? 0.71
CMU-HEA-COMBO ? 0.70
UPV-COMBO ? 0.68
NRC 0.66
CAMBRIDGE 0.66
UEDIN ? 0.65
LIMSI ? 0.65
JHU-COMBO 0.65
RALI 0.65
LIUM-COMBO 0.64
BBN-COMBO 0.64
RWTH 0.55
English-French
740?829 judgments per combo
System ?others
RWTH-COMBO ? 0.75
CMU-HEA-COMBO ? 0.74
UEDIN 0.70
KOC-COMBO ? 0.68
UPV-COMBO 0.66
RALI ? 0.66
LIMSI 0.66
RWTH 0.63
CAMBRIDGE 0.63
Czech-English
766?843 judgments per combo
System ?others
CMU-HEA-COMBO ? 0.71
ONLINEB ? 0.7
BBN-COMBO ? 0.70
RWTH-COMBO ? 0.65
UPV-COMBO ? 0.63
JHU-COMBO 0.62
UEDIN 0.61
German-English
743?835 judgments per combo
System ?others
BBN-COMBO ? 0.77
RWTH-COMBO ? 0.75
CMU-HEA-COMBO 0.73
KIT ? 0.72
UMD ? 0.68
JHU-COMBO 0.67
UEDIN ? 0.66
FBK 0.66
CMU-HYP-COMBO 0.65
UPV-COMBO 0.64
RWTH 0.62
KOC-COMBO 0.59
English-German
1340?1469 judgments per combo
System ?others
RWTH-COMBO ? 0.65
DFKI ? 0.62
UEDIN ? 0.62
KIT ? 0.60
CMU-HEA-COMBO ? 0.59
KOC-COMBO 0.59
FBK ? 0.56
UPV-COMBO 0.55
English-Czech
1405?1496 judgments per combo
System ?others
DCU-COMBO ? 0.75
ONLINEB ? 0.70
RWTH-COMBO 0.70
CMU-HEA-COMBO 0.69
UPV-COMBO 0.68
CU-BOJAR 0.66
KOC-COMBO 0.66
PC-TRANS 0.62
UEDIN 0.62
Spanish-English
1385?1535 judgments per combo
System ?others
UEDIN ? 0.69
CMU-HEA-COMBO ? 0.66
UPV-COMBO ? 0.66
BBN-COMBO 0.62
JHU-COMBO 0.55
UPC 0.51
English-Spanish
516?673 judgments per combo
System ?others
CMU-HEA-COMBO ? 0.68
KOC-COMBO 0.62
UEDIN ? 0.61
UPV-COMBO 0.60
RWTH-COMBO 0.59
DFKI ? 0.55
JHU 0.55
UPV 0.55
CAMBRIDGE ? 0.54
UPV-NNLM ? 0.54
System combinations are listed in the order of how often their translations were ranked higher than or equal to any other system.
Ties are broken by direct comparison. We show the best individual systems alongside the system combinations, since the goal
of combination is to produce better quality translation than the component systems.
? indicates a win for the system combination meaning that no other system or system combination is statistically signifi-
cantly better at p-level?0.1 in pairwise comparison.
? indicates an individual system that none of the system combinations beat by a statistically significant margin at p-
level?0.1.
For all pairwise comparisons between systems, please check the appendix.
Note: ONLINEA and ONLINEB were not included among the systems being combined in the system combination shared tasks,
except in the Czech-English and English-Czech conditions, where ONLINEB was included.
Table 6: Official results for the WMT10 system combination task, based on the human evaluation (rank-
ing translations relative to each other)
27
System % Yes Yes count No count N/A count Total count *** en-cz ***
ref 0.97 63 2 0 65 en-cz
dcu-c 0.58 29 21 0 50 en-cz
onlineB 0.55 22 18 0 40 en-cz
rwth-c 0.49 56 59 0 115 en-cz
koc-c 0.45 29 36 0 65 en-cz
pc-trans 0.43 26 34 0 60 en-cz
upv-c 0.42 23 32 0 55 en-cz
cu-bojar 0.4 20 30 0 50 en-cz
eurotrans 0.4 18 27 0 45 en-cz
uedin 0.34 24 46 0 70 en-cz
cu-tecto 0.34 29 55 1 85 en-cz
cmu-hea-c 0.29 13 32 0 45 en-cz
sfu 0.24 14 44 0 58 en-cz
potsdam 0.24 13 42 0 55 en-cz
cu-zeman 0.21 15 55 0 70 en-cz
koc 0.21 21 79 0 100 en-cz
onlineA 0.2 13 52 0 65 en-cz
dcu 0.19 13 57 0 70 en-cz
0.1260077028
*** en-de ***
ref 0.94 47 3 0 50 en-de
onlineA 0.8 20 5 0 25 en-de
koc-c 0.68 17 8 0 25 en-de
uppsala 0.65 26 14 0 40 en-de
uedin 0.62 50 30 0 80 en-de
kit 0.62 37 23 0 60 en-de
upv-c 0.57 30 23 0 53 en-de
onlineB 0.52 21 19 0 40 en-de
dfki 0.52 13 12 0 25 en-de
koc 0.51 18 17 0 35 en-de
limsi 0.51 18 16 1 35 en-de
liu 0.51 28 27 0 55 en-de
rwth 0.5 15 15 0 30 en-de
rwth-c 0.49 22 23 0 45 en-de
jhu 0.48 12 13 0 25 en-de
cmu-hea-c 0.47 14 16 0 30 en-de
fbk 0.4 4 6 0 10 en-de
sfu 0.31 11 24 0 35 en-de
cu-zeman 0.19 10 40 3 53 en-de
0.1364453014
System % Yes Yes count No count N/A count Total count *** en-es ***
ref 0.83 48 10 0 58 en-es
onlineB 0.58 25 18 0 43 en-es
upv 0.5 20 20 0 40 en-es
rwth-c 0.46 13 15 0 28 en-es
dcu 0.42 16 22 0 38 en-es
koc 0.4 17 24 1 42 en-es
upv-nnlm 0.39 15 23 0 38 en-es
onlineA 0.38 11 18 0 29 en-es
jhu 0.38 17 27 1 45 en-es
koc-c 0.38 20 33 0 53 en-es
uedin 0.36 12 21 0 33 en-es
upb-c 0.32 13 27 0 40 en-es
cmu-hea-c 0.32 16 34 0 50 en-es
camb 0.3 12 27 1 40 en-es
dfki 0.29 7 17 0 24 en-es
cu-zeman 0.29 16 39 0 55 en-es
sfu 0.26 9 25 0 34 en-es
0.0845946216
System % Yes Yes count No count N/A count Total count *** en-fr ***
ref 0.91 64 4 2 70 en-fr
rwth-c 0.54 27 23 0 50 en-fr
onlineB 0.52 47 42 1 90 en-fr
upv-c 0.51 34 33 0 67 en-fr
koc-c 0.48 32 34 0 66 en-fr
uedin 0.48 30 32 1 63 en-fr
rali 0.47 21 24 0 45 en-fr
rwth 0.45 25 30 0 55 en-fr
lium 0.43 20 27 0 47 en-fr
camb 0.42 26 36 0 62 en-fr
onlineA 0.41 15 22 0 37 en-fr
limsi 0.37 26 44 0 70 en-fr
jhu 0.37 27 46 0 73 en-fr
nrc 0.36 13 23 0 36 en-fr
cmu-hea-c 0.32 22 47 0 69 en-fr
geneva 0.31 32 70 0 102 en-fr
eu 0.3 13 30 0 43 en-fr
dfki 0.28 16 42 0 58 en-fr
koc 0.21 12 44 1 57 en-fr
cu-zeman 0.17 11 52 0 63 en-fr
0.1045877454
System % Yes Yes count No count N/A count Total count *** cz-en ***
ref 1.00 33 0 0 33 cz-en
cu-bojar 0.6 3 2 0 5 cz-en
upv-c 0.43 15 20 0 35 cz-en
cmu-hea-c 0.35 14 26 0 40 cz-en
rwth-c 0.32 16 34 0 50 cz-en
onlineB 0.3 12 28 0 40 cz-en
bbn-c 0.28 17 43 0 60 cz-en
uedin 0.28 11 28 1 40 cz-en
aalto 0.27 8 22 0 30 cz-en
jhu-c 0.26 13 37 0 50 cz-en
onlineA 0.2 6 24 0 30 cz-en
cmu 0.17 5 25 0 30 cz-en
cu-zeman 0.09 4 40 1 45 cz-en
0.1292958787
System % Yes Yes count No count N/A count Total count *** de-en ***
ref 0.98 44 1 0 45 de-en
umd 0.8 8 2 0 10 de-en
bbn-c 0.67 10 5 0 15 de-en
onlineB 0.65 13 7 0 20 de-en
cmu-hea-c 0.52 12 11 0 23 de-en
jhu-c 0.51 18 17 0 35 de-en
upv-c 0.51 18 16 1 35 de-en
fbk 0.5 20 20 0 40 de-en
uppsala 0.5 20 19 1 40 de-en
limsi 0.46 30 34 1 65 de-en
kit 0.45 18 22 0 40 de-en
liu 0.44 19 24 0 43 de-en
uedin 0.44 11 14 0 25 de-en
dfki 0.4 12 18 0 30 de-en
onlineA 0.4 6 9 0 15 de-en
rwth 0.4 14 21 0 35 de-en
cmu-hyp-c 0.37 11 19 0 30 de-en
huicong 0.36 9 16 0 25 de-en
koc-c 0.36 9 14 2 25 de-en
rwth-c 0.36 10 18 0 28 de-en
koc 0.31 11 23 1 35 de-en
cu-zeman 0.3 12 28 0 40 de-en
uu-ms 0.26 13 37 0 50 de-en
jhu 0.26 9 26 0 35 de-en
cmu 0.24 6 19 0 25 de-en
aalto 0.07 1 14 0 15 de-en
0.1512635669
System % Yes Yes count No count N/A count Total count *** es-en ***
ref 0.98 39 0 1 40 es-en
onlineB 0.71 39 15 1 55 es-en
onlineA 0.64 32 18 0 50 es-en
upv-c 0.6 36 24 0 60 es-en
huicong 0.54 27 23 0 50 es-en
jhu 0.54 35 30 0 65 es-en
cmu-hea-c 0.52 26 23 1 50 es-en
bbn-c 0.51 36 33 1 70 es-en
uedin 0.51 33 30 2 65 es-en
jhu-c 0.47 28 31 1 60 es-en
dfki 0.46 16 18 1 35 es-en
upc 0.43 28 36 1 65 es-en
cu-zeman 0.4 18 26 1 45 es-en
camb 0.36 25 45 0 70 es-en
columbia 0.29 19 46 0 65 es-en
0.1104436607
System % Yes Yes count No count N/A count Total count *** fr-en ***
ref 0.91 32 3 0 35 fr-en
cmu-hyp-c 0.7 21 9 0 30 fr-en
uedin 0.58 23 17 0 40 fr-en
bbn-c 0.56 14 10 1 25 fr-en
rwth-c 0.53 16 14 0 30 fr-en
onlineB 0.51 28 27 0 55 fr-en
camb 0.5 20 19 1 40 fr-en
rali 0.48 31 34 0 65 fr-en
lium 0.46 23 27 0 50 fr-en
dcu-c 0.45 15 16 2 33 fr-en
lig 0.45 9 11 0 20 fr-en
cmu-statxfer 0.44 11 14 0 25 fr-en
nrc 0.43 15 20 0 35 fr-en
dfki 0.4 8 12 0 20 fr-en
jhu 0.4 10 14 1 25 fr-en
jhu-c 0.4 22 30 3 55 fr-en
upv-c 0.4 14 20 1 35 fr-en
lium-c 0.4 27 41 0 68 fr-en
cmu-hea-c 0.35 14 26 0 40 fr-en
limsi 0.35 14 26 0 40 fr-en
onlineA 0.33 20 40 0 60 fr-en
huicong 0.32 13 25 2 40 fr-en
cu-zeman 0.24 6 19 0 25 fr-en
geneva 0.24 6 19 0 25 fr-en
rwth 0.2 1 4 0 5 fr-en
0.1143475506
0
0.25
0.5
0.75
1
r
e
f
d
c
u
-
c
o
n
l
i
n
e
B
r
w
t
h
-
c
k
o
c
-
c
p
c
-
t
r
a
n
s
u
p
v
-
c
c
u
-
b
o
j
a
r
e
u
r
o
t
r
a
n
s
u
e
d
i
n
c
u
-
t
e
c
t
o
c
m
u
-
h
e
a
-
c
s
f
u
p
o
t
s
d
a
m
c
u
-
z
e
m
a
n
k
o
c
o
n
l
i
n
e
A
d
c
u
.19.2.21.21.24.24.29.34.34.4.4.42.43.45.49.55.58.97
English-Czech
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
A
k
o
c
-
c
u
p
p
s
a
l
a
u
e
d
i
n
k
i
t
u
p
v
-
c
o
n
l
i
n
e
B
d
f
k
i
k
o
c
l
i
m
s
i
l
i
u
r
w
t
h
r
w
t
h
-
c
j
h
u
c
m
u
-
h
e
a
-
c
f
b
k
s
f
u
c
u
-
z
e
m
a
n
.19.31.4.47.48.49.5.51.51.51.52.52.57.62.62.65.68.8.94
English-German
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
B
u
p
v
r
w
t
h
-
c
d
c
u
k
o
c
u
p
v
-
n
n
l
m
o
n
l
i
n
e
A
j
h
u
k
o
c
-
c
u
e
d
i
n
u
p
b
-
c
c
m
u
-
h
e
a
-
c
c
a
m
b
d
f
k
i
c
u
-
z
s
f
u
.26.29.29.3.32.32.36.38.38.38.39.4.42.46.5.58.83
English-Spanish
0
0.25
0.5
0.75
1
r
e
f
r
w
t
h
-
c
o
n
l
i
n
e
B
u
p
v
-
c
k
o
c
-
c
u
e
d
i
n
r
a
l
i
r
w
t
h
l
i
u
m
c
a
m
b
o
n
l
i
n
e
A
l
i
m
s
i
j
h
u
n
r
c
c
m
u
-
h
e
a
-
c
g
e
n
e
v
a
e
u
d
f
k
i
k
o
c
c
u
-
z
e
m
a
n
.17.21.28.3.31.32.36.37.37.41.42.43.45.47.48.48.51.52.54.91
English-French
0
0.25
0.5
0.75
1
r
e
f
u
m
d
b
b
n
-
c
o
n
l
i
n
e
B
c
m
u
-
h
e
a
-
c
j
h
u
-
c
u
p
v
-
c
f
b
k
u
p
p
s
a
l
a
l
i
m
s
i
k
i
t
l
i
u
u
e
d
i
n
d
f
k
i
o
n
l
i
n
e
A
r
w
t
h
c
m
u
-
h
y
p
-
c
h
u
i
c
o
n
g
k
o
c
-
c
r
w
t
h
-
c
k
o
c
c
u
-
z
e
m
a
n
u
u
-
m
s
j
h
u
c
m
u
a
a
l
t
o
.07.24.26.26.3.31.36.36.36.37.4.4.4.44.44.45.46.5.5.51.51.52.65.67.8.98
German-English
0
0.25
0.5
0.75
1
r
e
f
c
u
-
b
o
j
a
r
u
p
v
-
c
c
m
u
-
h
e
a
-
c
r
w
t
h
-
c
o
n
l
i
n
e
B
b
b
n
-
c
u
e
d
i
n
a
a
l
t
o
j
h
u
-
c
o
n
l
i
n
e
A
c
m
u
c
u
-
z
e
m
a
n
.09.17.2.26.27.28.28.3.32.35.43.61.0
Czech-English
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
B
o
n
l
i
n
e
A
u
p
v
-
c
h
u
i
c
o
n
g
j
h
u
c
m
u
-
h
e
a
-
c
b
b
n
-
c
4
5
j
h
u
-
c
d
f
k
i
u
p
c
c
u
-
z
e
m
a
n
c
a
m
b
c
o
l
u
m
b
i
a
.29.36.4.43.46.47.51.51.52.54.54.6.64.71.98
Spanish-English
0
0.25
0.5
0.75
1
r
e
f
c
m
u
-
h
y
p
-
c
u
e
d
i
n
b
b
n
-
c
r
w
t
h
-
c
o
n
l
i
n
e
B
c
a
m
b
r
a
l
i
l
i
u
m
d
c
u
-
c
l
i
g
c
m
u
-
s
t
a
t
x
f
e
r
n
r
c
d
f
k
i
j
h
u
j
h
u
-
c
u
p
v
-
c
l
i
u
m
-
c
c
m
u
-
h
e
a
-
c
l
i
m
s
i
o
n
l
i
n
e
A
h
u
i
c
o
n
g
c
u
-
z
e
m
a
n
g
e
n
e
v
a
r
w
t
h
.2.24.24.32.33.35.35.4.4.4.4.4.43.44.45.45.46.48.5.51.53.56.58.7.91
French-English
Figure 3: The percent of time that each system?s edited output was judged to be an acceptable translation.
These numbers also include judgments of the system?s output when it was marked either incomprehen-
sible or acceptable and left unedited. Note that the reference translation was edited alongside the system
outputs. Error bars show one positive and one negative standard deviation for the systems in that lan-
guage pair.
28
a shorter description when concatenated be-
fore compression than when concatenated af-
ter compression. MT-NCD does not require
any language specific resources.
? MT-mNCD ? Enhances MT-NCD with flex-
ible word matching provided by stemming
and synonyms. It works analogously to
M-BLEU and M-TER and uses METEOR?s
aligner module to find relaxed word-to-word
alignments. MT-mNCD exploits English
WordNet data and increases correlation to hu-
man judgments for English over MT-NCD.
Due to a processing issue inherent to the metric,
the scores reported were generated excluding the
first segment of each document. Also, a separate
issue was found for the MT-mNCD metric, and ac-
cording to the developer the scores reported here
would like change with a correction of the issue.
BabbleQuest International8
? Badger 2.0 full ? Uses the Smith-Waterman
alignment algorithm with Gotoh improve-
ments to measure segment similarity. The
full version uses a multilingual knowledge
base to assign a substitution cost which sup-
ports normalization of word infection and
similarity.
? Badger 2.0 lite ? The lite version uses default
gap, gap extension and substitution costs.
City University of Hong Kong (Wong and Kit,
2010)
? ATEC 2.1 ? This version of ATEC extends
the measurement of word choice and word or-
der by various means. The former is assessed
by matching word forms at linguistic levels,
including surface form, stem, sense and se-
mantic similarity, and further by weighting
the informativeness of both matched and un-
matched words. The latter is quantified in
term of the discordance of word position and
word sequence between an MT output and its
reference.
Due to a version discrepancy of the metric, final
scores for ATECD-2.1 differ from those reported
here, but only minimally.
8http://www.babblequest.com/badger2
Carnegie Mellon University (Denkowski and
Lavie, 2010)
? METEOR-NEXT-adq ? Evaluates a machine
translation hypothesis against one or more
reference translations by calculating a simi-
larity score based on an alignment between
the hypothesis and reference strings. Align-
ments are based on exact, stem, synonym,
and paraphrase matches between words and
phrases in the strings. Metric parameters are
tuned to maximize correlation with human
judgments of translation quality (adequacy
judgments).
? METEOR-NEXT-hter ? METEOR-NEXT
tuned to HTER.
? METEOR-NEXT-rank ? METEOR-NEXT
tuned to human judgments of rank.
Columbia University9
? SEPIA ? A syntactically-aware machine
translation evaluation metric designed with
the goal of assigning bigger weight to gram-
matical structural bigrams with long surface
spans that cannot be captured with surface n-
gram metrics. SEPIA uses a dependency rep-
resentation produced for both hypothesis and
reference(s). SEPIA is configurable to allow
using different combinations of structural n-
grams, surface n-grams, POS tags, depen-
dency relations and lemmatization. SEPIA is
a precision-based metric and as such employs
clipping and length penalty to minimize met-
ric gaming.
Charles University Prague (Bojar and Kos,
2010)
? SemPOS ? Computes overlapping of autose-
mantic (content-bearing) word lemmas in the
candidate and reference translations given a
fine-grained semantic part of speech (sem-
pos) and outputs average overlapping score
over all sempos types. The overlapping is de-
fined as the number of matched lemmas di-
vided by the total number of lemmas in the
candidate and reference translations having
the same sempos type.
9http://www1.ccls.columbia.edu/?SEPIA/
29
? SemPOS-BLEU ? A linear combination of
SemPOS and BLEU with equal weights.
BLEU is computed on surface forms of au-
tosemantic words that are used by SemPOS,
i.e. auxiliary verbs or prepositions are not
taken into account.
Dublin City University (He et al, 2010)
? DCU-LFG ? A combination of syntactic and
lexical information. It measures the similar-
ity of the hypothesis and reference in terms
of matches of Lexical Functional Grammar
(LFG) dependency triples. The matching
module can also access the WordNet syn-
onym dictionary and Snover?s paraphrase
database10.
University of Edinburgh (Birch and Osborne,
2010)
? LRKB4 ? A novel metric which directly mea-
sures reordering success using Kendall?s tau
permutation distance metrics. The reordering
component is combined with a lexical metric,
capturing the two most important elements
of translation quality. This simple combined
metric only has one parameter, which makes
its scores easy to interpret. It is also fast
to run and language-independent. It uses
Kendall?s tau permutation.
? LRHB4 ? LRKB4, replacing Kendall?s tau
permutation distance metric with the Ham-
ming distance permutation distance metric.
Due to installation issues, the reported submitted
scores for these two metrics have not been verified
to produce identical scores at NIST.
Harbin Institute of Technology, China
? I-letter-BLEU ? Normal BLEU based on let-
ters. Moreover, the maximum length of N-
gram is decided by the average length for
each sentence, respectively.
? I-letter-recall ? A geometric mean of N-gram
recall based on letters. Moreover, the maxi-
mum length of N-gram is decided by the av-
erage length for each sentence, respectively.
10Available at http://www.umiacs.umd.edu/
?snover/terp/.
? SVM-RANK ? Uses support vector ma-
chines rank models to predict an ordering
over a set of system translations with lin-
ear kernel. Features include Meteor-exact,
BLEU-cum-1, BLEU-cum-2, BLEU-cum-5,
BLEU-ind-1, BLEU-ind-2, ROUGE-L re-
call, letter-based TER, letter-based BLEU-
cum-5, letter-based ROUGE-L recall, and
letter-based ROUGE-S recall.
National University of Singapore (Liu et al,
2010)
? TESLA-M ? Based on matching of bags of
unigrams, bigrams, and trigrams, with con-
sideration of WordNet synonyms. The match
is done in the framework of real-valued lin-
ear programming to enable the discounting of
function words.
? TESLA ? Built on TESLA-M, this metric
also considers bilingual phrase tables to dis-
cover phrase-level synonyms. The feature
weights are tuned on the development data
using SVMrank.
Stanford University
? Stanford ? A discriminatively trained
string-edit distance metric with various
similarity-matching, synonym-matching, and
dependency-parse-tree-matching features.
The model resembles a Conditional Random
Field, but performs regression instead of
classification. It is trained on Arabic, Chi-
nese, and Urdu data from the MT-Eval 2008
dataset.
Due to installation issues, the reported scores for
this metric have not been verified to produce iden-
tical scores at NIST.
University of Maryland11
? TER-plus (TERp) ? An extension of the
Translation Edit Rate (TER) metric that mea-
sures the number of edits between a hypoth-
esized translation and a reference translation.
TERp extends TER by using stemming, syn-
onymy, and paraphrases as well as tunable
edit costs to better measure the distance be-
tween the two translations. This version
of TERp improves upon prior versions by
adding brevity and length penalties.
11http://www.umiacs.umd.edu/?snover/
terp
30
Scores were not submitted along with this metric,
and due to installation issues were not produced at
NIST in time to be included in this report.
University Polite`cnica de Catalunya/University
de Barcelona (Comelles et al, 2010)
? DR ? An arithmetic mean over a set of
three metrics based on discourse representa-
tions, respectively computing lexical overlap,
morphosyntactic overlap, and semantic tree
matching.
? DRdoc ? Is analogous to DR but, instead of
operating at the segment level, it analyzes
similarities over whole document discourse
representations.
? ULCh ? An arithmetic mean over a
heuristically-defined set of metrics operat-
ing at different linguistic levels (ROUGE,
METEOR, and measures of overlap between
constituent parses, dependency parses, se-
mantic roles, and discourse representations).
University of Southern California, ISI
? BEwT-E ? Basic Elements with Transfor-
mations for Evaluation, is a recall-oriented
metric that compares basic elements, small
portions of contents, between the two trans-
lations. The basic elements (BEs) consist
of content words and various combinations
of syntactically-related words. A variety of
transformations are performed to allow flexi-
ble matching so that words and syntactic con-
structions conveying similar content in dif-
ferent manners may be matched. The trans-
formations cover synonymy, preposition vs.
noun compounding, differences in tenses,
etc. BEwT-E was originally created for sum-
marization evaluation and is English-specific.
? Bkars ? Measures overlap between character
trigrams in the system and reference trans-
lations. It is heavily weighted toward recall
and contains a fragmentation penalty. Bkars
produces a score both with and without stem-
ming (using the Snowball package of stem-
mers) and averages the results together. It is
not English-specific.
Scores were not submitted for BEwT-E; the run-
time required for this metric to process the WMT-
10 data set prohibited the production of scores in
time for publication.
6 Evaluation task results
The results reported here are preliminary; a final
release of results will be published on the WMT10
website before July 15, 2010. Metric developers
submitted metrics for installation at NIST; they
were also asked to submit metric scores on the
WMT10 test set alng with their metrics. Not
all developers submitted scores, and not all met-
rics were verified to produce the same scores as
submitted at NIST in time for publication. Any
such caveats are reported with the description of
the metrics above.
The results reported here are limited to a com-
parison of metric scores on the full WMT10
test set with human assessments on the human-
assessed subset. An analysis comparing the hu-
man assessments with the automatic metrics run
only on the human-assessed subset will follow at
a later date.
The WMT10 system output used to generate
the reported metric scores was found to have im-
properly escaped characters for a small number of
segments. While we plan to regenerate the met-
ric scores with this issue resolved, we do not ex-
pect this to significantly alter the results, given the
small number of segments affected.
6.1 System Level Metric Scores
The tables in Appendix B list the metric scores
for the language pairs processed by each metric.
These first four tables present scores for transla-
tions out of English into Czech, French, German
and Spanish. In addition to the metric scores of
the submitted metrics identified above, we also
present (1) the ranking of the system as deter-
mined by the human assessments; and (2) the
metrics scores for two popular baseline metrics,
BLEU as calculated by NIST?s mteval software12
and the NIST score. For each method of system
measurement the absolute highest score is identi-
fied by being outlined in a box.
Similarly, the remaining tables in Appendix B
list the metric scores for the submitted metrics and
the two baseline metrics, and the ranking based
on the human assessments for translations into En-
glish from Czech, French, German and Spanish.
As some metrics employ language-specific re-
sources, not all metrics produced scores for all lan-
guage pairs.
12ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a-20091001.tar.gz
31
cz-
en
fr-
en
de-
en
es-
en
avg
SemPOS .78 .77 .60 .95 .77
IQmt-DRdoc .61 .79 .65 .98 .76
SemPOS-BLEU .75 .70 .61 .96 .75
i-letter-BLEU .71 .70 .60 .98 .75
NIST .85 .72 .55 .86 .74
TESLA .70 .70 .60 .97 .74
MT-NCD .71 .72 .58 .95 .74
Bkars .71 .67 .58 .98 .74
ATEC-2.1 .71 .67 .59 .96 .73
meteor-next-rank .69 .68 .60 .96 .73
IQmt-ULCh .70 .64 .60 .99 .73
IQmt-DR .68 .67 .60 .97 .73
meteor-next-hter .71 .66 .59 .95 .73
meteor-next-adq .69 .67 .60 .96 .73
badger-2.0-lite .70 .70 .56 .94 .73
DCU-LFG .69 .69 .58 .96 .73
badger-2.0-full .69 .70 .57 .94 .73
SEPIA .71 .70 .57 .92 .73
SVM-rank .66 .65 .61 .98 .73
i-letter-recall .65 .64 .61 .98 .72
TESLA-M .67 .67 .57 .95 .72
BLEU-4-v13a .69 .68 .52 .90 .70
LRKB4 .63 .62 .53 .89 .67
LRHB4 .62 .65 .50 .87 .66
MT-mNCD .69 .64 .52 .70 .64
Stanford .58 .19 .60 .46 .46
Table 7: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation into English.
It is noticeable that system combinations are of-
ten among those achieving the highest scores.
6.2 System-Level Correlations
To assess the performance of the automatic met-
rics, we correlated the metrics? scores with the hu-
man rankings at the system level. We assigned a
consolidated human-assessment rank to each sys-
tem based on the number of times that the given
system?s translations were ranked higher than or
equal to the translations of any other system in
the manual evaluation of the given language pair.
We then compared the ranking of systems by the
human assessments to that provided by the au-
tomatic metric system level scores on the com-
plete WMT10 test set for each language pair, us-
ing Spearman?s ? rank correlation coefficient. The
correlations are shown in Table 7 for translations
to English, and Table 8 out of English, with base-
line metrics listed at the bottom. The highest cor-
relation for each language pair and the highest
overall average are bolded.
Overall, correlations are higher for translations
to English than compared to translations from En-
glish. For all language pairs, there are a number
of new metrics that yield noticeably higher corre-
en-
cz
en-
fr
en-
de
en-
es
avg
SVM-rank .29 .54 .68 .67 .55
TESLA-M .27 .49 .74 .66 .54
LRKB4 .39 .58 .47 .71 .54
i-letter-recall .28 .51 .61 .66 .52
LRHB4 .39 .59 .41 .63 .51
i-letter-BLEU .26 .49 .56 .65 .49
ATEC-2.1 .38 .52 .44 .62 .49
badger-2.0-full .37 .58 .41 .59 .49
Bkars .22 .54 .52 .66 .48
BLEU-4-v13a .35 .58 .39 .57 .47
badger-2.0-lite .32 .57 .41 .59 .47
TESLA .09 .62 .66 .50 .47
meteor-next-rank .34 .59 .39 .51 .46
Stanford .34 .48 .70 .32 .46
MT-NCD .17 .54 .51 .61 .46
NIST .30 .52 .41 .50 .43
MT-mNCD .26 .49 .17 .43 .34
SemPOS .31 n/a n/a n/a .31
SemPOS-BLEU .29 n/a n/a n/a .29
Table 8: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation out of English.
lations with human assessments than either of the
two included baseline metrics. In particular, Bleu
performed in the bottom half of the into-English
and out-of-English directions.
6.3 Segment-Level Metric Analysis
The method employed to collect human judgments
of rank preferences at the segment level produces
a sparse matrix of decision points. It is unclear
whether attempts to normalize the segment level
rankings to 0.0?1.0 values, representing the rela-
tive rank of a system per segment given the num-
ber of comparisons it is involved with, is proper.
An intuitive display of how well metrics mirror the
human judgments may be shown via a confusion
matrix. We compare the human ranks to the ranks
as determined by a metric. Below, we show an ex-
ample of the confusion matrix for the SVM-rank
metric which had the highest summed diagonal
(occurrences when a particular rank by the met-
ric?s score exactly matches the human judgments)
for all segments translated into English. The num-
bers provided are percentages of the total count.
The summed diagonal constitutes 39.01% of all
counts in this example matrix. The largest cell is
the 1/1 ranking cell (top left). We included the
reference translation as a system in this analysis,
which is likely to lead to a lot of agreement on the
highest rank between humans and automatic met-
rics.
32
Metric Human Rank
Rank 1 2 3 4 5
1 12.79 4.48 2.75 1.82 0.92
2 2.77 7.94 5.55 3.79 2.2
3 1.57 4.29 6.74 5.4 4.46
4 0.97 2.42 3.76 4.99 6.5
5 0.59 1.54 1.84 3.38 6.55
No allowances for ties were made in this analy-
sis. That is, if a human ranked two system transla-
tions the same, this analysis expects the metrics to
provide the same score in order to get them both
correct. Future analysis could relax this constraint.
As not all human rankings start with the highest
possible rank of ?1? (due to ties and withholding
judgment on a particular system output being al-
lowed), we set the highest automatic metric rank
to the highest human rank and shifted the lower
metric ranks down accordingly.
Table 9 shows the summed diagonal percent-
ages of the total count of all datapoints for all met-
rics that WMT10 scores were available for, both
combined for all languages to English (X-English)
and separately for each language into English.
The results are ordered by the highest percent-
age for the summed diagonal on all languages
to English combined. There are quite noticeable
changes in ranking of the metrics for the separate
language pairs; further analysis into the reasons
for this will be necessary.
We plan to also analyze metric performance for
translation into English.
7 Feasibility of Using Non-Expert
Annotators in Future WMTs
In this section we analyze the data that we col-
lected data by posting the ranking task on Ama-
zon?s Mechanical Turk (MTurk). Although we did
not use this data when creating the official results,
our hope was that it may be useful in future work-
shops in two ways. First, if we find that it is pos-
sible to obtain a sufficient amount of data of good
quality, then we might be able to reduce the time
commitment expected from the system develop-
ers in future evaluations. Second, the additional
collected labels might enable us to detect signifi-
cant differences between systems that would oth-
erwise be insignificantly different using only the
data from the volunteers (which we will now refer
to as the ?expert? data).
7.1 Data collection
To that end, we prepared 600 ranking sets for each
of the eight language pairs, with each set con-
taining five MT outputs to be ranked, using the
same interface used by the volunteers. We posted
the data to MTurk and requested, for each one,
five redundant assignments, from different work-
ers. Had all the 5? 8? 600 = 24,000 assignments
been completed, we would have obtained 24,000
? 5 = 120,000 additional rank labels, compared
to the 37,884 labels we collected from the volun-
teers (Table 3). In actuality, we collected closer to
55,000 rank labels, as we discuss shortly.
To minimize the amount of data that is of poor
quality, we placed two requirements that must be
satisfied by any worker before completing any of
our tasks. First, we required that a worker have an
existing approval rating of at least 85%. Second,
we required a worker to reside in a country where
the target language of the task can be assumed to
be the spoken language. Finally, anticipating a
large pool of workers located in the United States,
we felt it possible for us to add a third restriction
for the *-to-English language pairs, which is that a
worker must have had at least five tasks previously
approved on MTurk.13 We organized the ranking
sets in groups of 3 per screen, with a monetary re-
ward of $0.05 per screen.
When we created our tasks, we had no expecta-
tion that all the assignments would be completed
over the tasks? lifetime of 30 days. This was in-
deed the case (Table 10), especially for language
pairs with a non-English target language, due to
workers being in short supply outside the US.
Overall, we see that the amount of data collected
from non-US workers is relatively small (left half
of Table 10), whereas the pool of US-based work-
ers is much larger, leading to much higher com-
pletion rates for language pairs with English as the
target language (right half of Table 10). This is in
spite of the additional restriction we placed on US
workers.
13We suspect that newly registered workers on MTurk al-
ready start with an ?approval rating? of 100%, and so requir-
ing a high approval rating alone might not guard against new
workers. It is not entirely clear if our suspicion is true, but our
past experiences with MTurk usually involved a noticeably
faster completion rate than what we experienced this time
around, indicating our suspicion might very well be correct.
33
Metric *-English Czech-English French-English German-English Spanish-English
SVM-rank 39.01 41.21 36.07 38.81 40.3
i-letter-recall 38.85 41.71 36.19 38.8 39.5
MT-NCD 38.77 42.55 35.31 38.7 39.48
i-letter-BLEU 38.69 40.54 36.05 38.82 39.64
meteor-next-rank 38.5 40.1 34.41 39.25 40.05
meteor-next-adq 38.27 39.58 34.41 39.5 39.35
meteor-next-hter 38.21 38.61 34.1 39.13 40.18
Bkars 37.98 40.1 35.08 38.6 38.52
Stanford 37.97 39.87 36.19 38.27 38.09
ATEC-2.1 37.95 40.06 34.96 38.6 38.53
TESLA 37.57 38.68 34.38 38.67 38.36
NIST 37.47 39.54 35.54 37.13 38.2
SemPOS 37.21 38.8 37.39 35.73 37.69
SemPOS-BLEU 37.16 38.05 36.57 37.11 37.21
badger-2.0-full 37.12 37.5 36 36.21 38.62
badger-2.0-lite 37.08 37.2 35.88 36.23 38.69
SEPIA 37.06 38.98 34.6 36.46 38.52
BLEU-4-v13a 36.71 37.83 34.84 36.44 37.81
LRHB4 36.14 38.35 34.65 34.24 37.93
TESLA-M 36.13 37.01 34 35.79 37.6
LRKB4 36.12 38.72 33.47 35.25 37.63
IQmt-ULCh 35.86 37.64 33.95 35.81 36.45
IQmt-DR 35.77 36.27 34.43 34.43 37.74
DCU-LFG 34.72 36.38 32.29 33.87 36.49
MT-mNCD 34.51 34.93 31.78 35.73 35.13
IQmt-DRdoc 31.9 33.85 28.99 32.9 32.18
Table 9: The segment-level performance for metrics for the into-English direction.
en-de en-es en-fr en-cz de-en es-en fr-en cz-en
Location DE ES/MX FR CZ US US US US
Completed 1 time 37% 38% 29% 19% 3.5% 1.5% 14% 2.0%
Completed 2 times 18% 14% 12% 1.5% 6.0% 5.5% 19% 4.5%
Completed 3 times 2.5% 4.5% 0.5% 0.0% 8.5% 11% 20% 10%
Completed 4 times 1.5% 0.5% 0.5% 0.0% 22% 19% 23% 17%
Completed 5 times 0.0% 0.5% 0.0% 0.0% 60% 63% 22% 67%
Completed ? once 59% 57% 42% 21% 100% 99% 96% 100%
Label count 2,583 2,488 1,578 627 12,570 12,870 9,197 13,169
(% of expert data) (38%) (96%) (40%) (9%) (241%) (228%) (222%) (490%)
Table 10: Statistics for data collected on MTurk for the ranking task. In total, 55,082 rank labels were
collected across the eight language pairs (145% of expert data). Each language pair had 600 sets, and
we requested each set completed by 5 different workers. Since each set provides 5 labels, we could have
potentially obtained 600 ? 5 ? 5 = 15,000 labels for each language pair. The Label count row indicates
to what extent that potential was met (over the 30-day lifetime of our tasks), and the ?Completed...? rows
give a breakdown of redundancy. For instance, the right-most column indicates that, in the cz-en group,
2.0% of the 600 sets were completed by only one worker, while 67% of the sets were completed by 5
workers, with 100% of the sets completed at least once. The total cost of this data collection effort was
roughly $200.
34
INTER-ANNOTATOR AGREEMENT
P (A) K K?
With references 0.466 0.198 0.487
Without references 0.441 0.161 0.439
INTRA-ANNOTATOR AGREEMENT
P (A) K K?
With references 0.539 0.309 0.633
Without references 0.538 0.307 0.601
Table 11: Inter- and intra-annotator agreement for
the MTurk workers on the sentence ranking task.
(As before, P (E) is 0.333.) For comparison, we
repeat here the kappa coefficients of the experts
(K?), taken from Table 4.
7.2 Quality of MTurk data
It is encouraging to see that we can collect a large
amount of rank labels from MTurk. That said, we
still need to guard against data from bad work-
ers, who are either not being faithful and click-
ing randomly, or who might simply not be compe-
tent enough. Case in point, if we examine inter-
and intra-annotator agreement on the MTurk data
(Table 11), we see that the agreement rates are
markedly lower than their expert counterparts.
Another indication of the presence of bad work-
ers is a low reference preference rate (RPR),
which we define as the proportion of time a ref-
erence translation wins (or ties in) a comparison
when it appears in one. Intuitively, the RPR
should be quite high, since it is quite rare that an
MT output ought to be judged better than the refer-
ence. This rate is 96.5% over the expert data, but
only 83.7% over the MTurk data. Compare this
to a randomly-clicking RPR of 66.67% (because
the two acceptable answers are that the reference
is either better than a system?s output or tied with
it).
Also telling would be the rate at which MTurk
workers agree with experts. To ensure that we ob-
tain enough overlapping data to calculate such a
rate, we purposely select one-sixth14 of our rank-
ing sets so that the five-system group is exactly one
that has been judged by an expert. This way, at
least one-sixth of the comparisons obtained from
an MTurk worker?s labels are comparisons for
14This means that on average Turkers ranked a set of sys-
tem outputs that had been ranked by experts on every other
screen, since each screen?s worth of work had three sets.
which we already have an expert judgment. When
we calculate the rate of agreement on this data,
we find that MTurk workers agree with the ex-
pert workers 53.2% of the time, or K = 0.297, and
when references are excluded, the agreement rate
is 50.0%, or K = 0.249. Ideally, we would want
those values to be in the 0.4?0.5 range, since that
is where the inter-annotator kappa coefficient lies
for the expert annotators.
7.3 Filtering MTurk data by agreement with
experts
We can use the agreement rate with experts to
identify MTurk workers who are not performing
the task as required. For each worker w of the
669 workers for whom we have such data, we
compute the worker?s agreement rate with the ex-
perts, and from it a kappa coefficient Kexp(w) for
that worker. (Given that P (E) is 0.333, Kexp(w)
ranges between?0.5 and +1.0.) We sort the work-
ers based on Kexp(w) in ascending order, and ex-
amine properties of the MTurk data as we remove
the lowest-ranked workers one by one (Figure 4).
We first note that the amount of data we ob-
tained from MTurk is so large, that we could af-
ford to eliminate close to 30% of the labels, and
we would still have twice as much data than us-
ing the expert data alone. We also note that two
workers in particular (the 103rd and 130th to be
removed) are likely responsible for the majority
of the bad data, since removing their data leads to
noticeable jumps in the reference preference rate
and the inter-annotator agreement rate (right two
curves of Figure 4). Indeed, examining the data for
those two workers, we find that their RPR values
are 55.7% and 51.9%, which is a clear indication
of random clicking.15
Looking again at those two curves shows de-
grading values as we continue to remove workers
in large droves, indicating a form of ?overfitting?
to agreement with experts (which, naturally, con-
tinues to increase until reaching 1.0; bottom left
curve). It is therefore important, if one were to fil-
ter out the MTurk data by removing workers this
way, to choose a cutoff carefully so that no crite-
rion is degraded dramatically.
In Appendix A, after reporting head-to-head
comparisons using only the expert data, we also
report head-to-head comparisons using the expert
15In retrospect, we should have performed this type of
analysis as the data was being collected, since such workers
could have been identified early on and blocked.
35
-20
40
60
80
100
120
140
160
0 100 200 300 400 500 600 700
# Workers Removed
M
T
u
r
k
 
D
a
t
a
 
R
e
m
a
i
n
i
n
g
(
%
 
o
f
 
E
x
p
e
r
t
 
D
a
t
a
)
-
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0 100 200 300 400 500 600 700
# Workers Removed
A
g
r
e
e
m
e
n
t
 
w
i
t
h
 
E
x
p
e
r
t
 
D
a
t
a
 
(
k
a
p
p
a
)
82%
84%
86%
88%
90%
92%
94%
96%
98%
100%
0 100 200 300 400 500 600 700
# Workers Removed
R
e
f
e
r
e
n
c
e
 
P
r
e
f
e
r
e
n
c
e
 
R
a
t
e
0.10
0.15
0.20
0.25
0.30
0 100 200 300 400 500 600 700
# Workers Removed
I
n
t
e
r
-
A
n
n
o
t
a
t
o
r
 
A
g
r
e
e
m
e
n
t
 
(
k
a
p
p
a
)
Figure 4: The effect of removing an increasing number of MTurk workers. The order in which workers
are removed is by Kexp(w), the kappa agreement coefficient with expert data (excluding references).
data combined with the MTurk data, in order to
be able to detect more significant differences be-
tween the systems. We choose the 300-worker
point as a reasonable cutoff point before combin-
ing the MTurk data with the expert data, based
on the characteristics of the MTurk data at that
point: a high reference preference rate, high inter-
annotator agreement, and, critically, a kappa co-
efficient vs. expert data of 0.449, which is close
to the expert inter-annotator kappa coefficient of
0.439.
7.4 Feasibility of using only MTurk data
In the previous subsection, we outlined an ap-
proach by which MTurk data can be filtered out
using expert data. Since we were to combine the
filtered MTurk data with the expert data to ob-
tain more significant differences, it was reason-
able to use agreement with experts to quantify the
MTurk workers? competency. However, we also
would like to know whether it is feasible to use the
MTurk data alone. Our aim here is not to boost the
differences we see by examining expert data, but
to eliminate our reliance on obtaining expert data
in the first place.
We briefly examined some simple ways of fil-
tering/combining the MTurk data, and measured
the Spearman rank correlations obtained from the
MTurk data (alone), as compared to the rankings
obtained using the expert data (alone), and report
them in Table 12. (These correlations do not in-
clude the references.)
We first see that even when using the MTurk
data untouched, we already obtain relatively high
correlation with expert ranking (?Unfiltered?).
This is especially true for the *-to-English lan-
guage pairs, where we collected much more data
than English-to-*. In fact, the relationship be-
tween the amount of data and the correlation val-
ues is very strong, and it is reasonable to expect
the correlation numbers for English-to-* to catch
up had more data been collected.
We also measure rank correlations when apply-
ing some simple methods of cleaning/weighting
MTurk data. The first method (?Voting?) is per-
forming a simple vote whenever redundant com-
parisons (i.e. from different workers) are avail-
able. The second method (?Kexp-filtered?) first re-
moves labels from the 300 worst workers accord-
ing to agreement with experts. The third method
36
(?RPR-filtered?) first removes labels from the 62
worst workers according to their RPR. The num-
bers 300 and 62 were chosen since those are the
points at which the MTurk data reaches the level
of expert data in the inter-annotator agreement and
RPR of the experts.
The fourth and fifth methods (?Weighted by
Kexp? and ?Weighted by K(RPR)?) do not re-
move any data, instead assigning weights to work-
ers based on their agreement with experts and their
RPR, respectively. Namely, for each worker, the
weight assigned by the fourth method is Kexp for
that worker, and the weight assigned by the fifth
method is K(RPR) for that worker.
Examining the correlation coefficients obtained
from those methods (Table 12), we see mixed re-
sults, and there is no clear winner among those
methods. It is also difficult to draw any conclusion
as to which method performs best when. However,
it is encouraging to see that the two RPR-based
methods perform well. This is noteworthy, since
there is no need to use expert data to weight work-
ers, which means that it is possible to evaluate a
worker using inherent, ?built-in? properties of that
worker?s own data, without resorting to making
comparisons with other workers or with experts.
8 Summary
As in previous editions of this workshop we car-
ried out an extensive manual and automatic eval-
uation of machine translation performance for
translating from European languages into English,
and vice versa.
The number of participants grew substantially
compared to previous editions of the WMT work-
shop, with 33 groups from 29 institutions partic-
ipating in WMT10. Most groups participated in
the translation task only, while the system combi-
nation task attracted a somewhat smaller number
of participants
Unfortunately, fewer rule-based systems partic-
ipated in this year?s edition of WMT, compared
to previous editions. We hope to attract more
rule-based systems in future editions as they in-
crease the variation of translation output and for
some language pairs, such as German-English,
tend to outperform statistical machine translation
systems.
This was the first time that the WMT workshop
was held as a joint workshop with NIST?s Metric-
sMATR evaluation initiative. This joint effort was
very productive as it allowed us to focus more on
the two evaluation dimensions: manual evaluation
of MT performance and the correlation between
manual metrics and automated metrics.
This year was also the first time we have in-
troduced quality assessments by non-experts. In
previous years all assessments were carried out
through peer evaluation exclusively consisting of
developers of machine translation systems, and
thereby people who are used to machine transla-
tion output. This year we have facilitated Ama-
zon?s Mechanical Turk to investigate two as-
pects of manual evaluation: How stable are man-
ual assessments across different assessor profiles
(experts vs. non-experts) and how reliable are
quality judgments of non-expert users? While
the intra- and inter-annotator agreements between
non-expert assessors are considerably lower than
for their expert counterparts, the overall rankings
of translation systems exhibit a high degree of cor-
relation between experts and non-experts. This
correlation can be further increased by applying
various filtering strategies reducing the impact of
unreliable non-expert annotators.
As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.16
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-
0022, and the US National Science Foundation un-
der grant IIS-0713448.
References
Alexandre Allauzen, Josep M. Crego, lknur Durgar El-
Kahlout, and Francois Yvon. 2010. Limsi?s statisti-
cal translation systems for wmt?10. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 29?34, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Lo??c Barrault. 2010. Many: Open source mt system
combination at wmt?10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
16http://www.statmt.org/wmt09/results.
html
37
Label Unfiltered Voting Kexp-filtered RPR-filtered Weighted by Weighted by
count Kexp K(RPR)
en-de 2,583 0.862 0.779 0.818 0.862 0.868 0.862
en-es 2,488 0.759 0.785 0.797 0.797 0.768 0.806
en-fr 1,578 0.826 0.840 0.791 0.814 0.802 0.814
en-cz 627 0.833 0.818 0.354 0.833 0.851 0.828
de-en 12,570 0.914 0.925 0.920 0.931 0.933 0.926
es-en 12,870 0.934 0.969 0.965 0.987 0.978 0.987
fr-en 9,197 0.880 0.865 0.920 0.919 0.907 0.917
cz-en 13,169 0.951 0.909 0.965 0.944 0.930 0.944
Table 12: Spearman rank coefficients for the MTurk data across the various language pairs, using differ-
ent methods to clean the data or weight workers. (These correlations were computed after excluding the
references.) Kexp is the kappa coefficient of the worker?s agreement rate with experts, with P (A) = 0.33.
K(RPR) is the kappa coefficient of the worker?s RPR (see 7.2), with P (A) = 0.66. In Kexp-filtering,
42% of labels remain, after removing 300 workers. In K(RPR)-filtering, 69% of labels remain, after
removing 62 workers.
and MetricsMATR, pages 252?256, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Ergun Bicici and S. Serdar Kozat. 2010. Adaptive
model weighting and transductive regression for pre-
dicting best system combinations. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 257?262, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Ergun Bicici and Deniz Yuret. 2010. L1 regularized
regression for reranking and system combination in
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 263?270, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
302?307, Uppsala, Sweden, July. Association for
Computational Linguistics.
Ondrej Bojar and Kamil Kos. 2010. 2010 failures
in english-czech phrase-based mt. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 35?41, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with amazons mechan-
ical turk. In Proceedings NAACL-2010 Workshop on
Creating Speech and Language Data With Amazons
Mechanical Turk, Los Angeles.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation (WMT07), Prague, Czech Repub-
lic.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, , Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the findings
of the 2009 workshop on statistical machine trans-
lation. In Proceedings of the Fourth Workshop on
Statistical Machine Translation (WMT09), Athens,
Greece.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: Evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2009), Singapore.
Elisabet Comelles, Jesus Gimenez, Lluis Marquez,
Irene Castellon, and Victoria Arranz. 2010.
Document-level automatic mt evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 308?313, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Hui Cong, Zhao Hai, Lu Bao-Liang, and Song Yan.
2010. An empirical study on development set se-
lection strategy for machine translation learning.
In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 42?46, Uppsala, Sweden, July. Association
for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2010. Meteor-
next and the meteor paraphrase tables: Improved
38
evaluation support for five target languages. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 314?
317, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Marcus Dobrinkat, Tero Tapiovaara, Jaakko Va?yrynen,
and Kimmo Kettunen. 2010. Normalized compres-
sion distance based measures for metricsmatr 2010.
In Proceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR, pages
318?323, Uppsala, Sweden, July. Association for
Computational Linguistics.
Jinhua Du, Pavel Pecina, and Andy Way. 2010. An
augmented three-pass system combination frame-
work: Dcu combination system for wmt 2010. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
271?276, Uppsala, Sweden, July. Association for
Computational Linguistics.
Vladimir Eidelman, Chris Dyer, and Philip Resnik.
2010. The university of maryland statistical ma-
chine translation system for the fifth workshop on
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 47?51, Uppsala, Sweden,
July. Association for Computational Linguistics.
Christian Federmann, Andreas Eisele, Yu Chen, Sabine
Hunsicker, Jia Xu, and Hans Uszkoreit. 2010.
Further experiments with shallow hybrid mt sys-
tems. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 52?56, Uppsala, Sweden, July. Association
for Computational Linguistics.
Jesu?s Gonza?lez-Rubio, Germa?n Sanchis-Trilles, Joan-
Andreu Sa?nchez, Jesu?s Andre?s-Ferrer, Guillem
Gasco?, Pascual Mart??nez-Go?mez, Martha-Alicia
Rocha, and Francisco Casacuberta. 2010. The upv-
prhlt combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 277?
281, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Greg Hanneman, Jonathan Clark, and Alon Lavie.
2010. Improved features and grammar selection for
syntax-based mt. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 57?62, Uppsala, Sweden, July.
Association for Computational Linguistics.
Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. Fbk at wmt 2010: Word lattices for
morphological reduction and chunk-based reorder-
ing. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 63?67, Uppsala, Sweden, July. Association
for Computational Linguistics.
Yifan He, Jinhua Du, Andy Way, and Josef van Gen-
abith. 2010. The dcu dependency-based metric in
wmt-metricsmatr 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 324?328, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Kenneth Heafield and Alon Lavie. 2010. Cmu multi-
engine machine translation for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 68?
73, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Carmen Heger, Joern Wuebker, Matthias Huck, Gregor
Leusch, Saab Mansour, Daniel Stein, and Hermann
Ney. 2010. The rwth aachen machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 74?78, Uppsala, Sweden,
July. Association for Computational Linguistics.
Carlos A. Henr??quez Q., Marta Ruiz Costa-jussa`, Vi-
das Daudaravicius, Rafael E. Banchs, and Jose? B.
Marin?o. 2010. Using collocation segmentation to
augment the phrase table. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 79?83, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Almut Silja Hildebrand and Stephan Vogel. 2010.
Cmu system combination via hypothesis selection
for wmt?10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 282?285, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry,
and Philippe Langlais. 2010. The rali machine
translation system for wmt 2010. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 84?90, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Michael Jellinghaus, Alexandros Poulis, and David
Kolovratn??k. 2010. Exodus - exploring smt for eu
institutions. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 91?95, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL
2006 Workshop on Statistical Machine Translation,
New York, New York.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions, Prague, Czech Republic.
39
Philipp Koehn, Barry Haddow, Philip Williams, and
Hieu Hoang. 2010. More linguistic annotation for
statistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 96?101, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger
Schwenk. 2010. Lium smt machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 102?107, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Samuel Larkin, Boxing Chen, George Foster, Ulrich
Germann, Eric Joanis, Howard Johnson, and Roland
Kuhn. 2010. Lessons from nrcs portage system at
wmt 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 108?113, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Gregor Leusch and Hermann Ney. 2010. The rwth
system combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 290?
295, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 114?118, Uppsala, Sweden, July.
Association for Computational Linguistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. Tesla: Translation evaluation of sentences
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 329?
334, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Sushant Narsale. 2010. Jhu system combination
scheme for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 286?289, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Jan Niehues, Teresa Herrmann, Mohammed Mediani,
and Alex Waibel. 2010. The karlsruhe institute
for technology translation system for the acl-wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 119?123, Uppsala, Sweden, July. Association
for Computational Linguistics.
NIST. 2008. Evaluation plan for gale go/no-go phase
3 / phase 3.5 translation evaluations. June 18, 2008.
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat,
Pratyush Banerjee, Ankit K. Srivastava, Jinhua Du,
Pavel Pecina, Sudip Kumar Naskar, Mikel L. For-
cada, and Andy Way. 2010. Matrex: The dcu mt
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 124?129, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Aaron Phillips. 2010. The cunei machine translation
platform for wmt ?10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 130?135, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Juan Pino, Gonzalo Iglesias, Adria` de Gispert, Graeme
Blackwood, Jamie Brunning, and William Byrne.
2010. The cued hifst system for the wmt10 trans-
lation shared task. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 136?141, Uppsala, Sweden,
July. Association for Computational Linguistics.
Marion Potet, Laurent Besacier, and Herve? Blanchon.
2010. The lig machine translation system for wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 142?147, Uppsala, Sweden, July. Association
for Computational Linguistics.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 ?Met-
rics for MAchine TRanslation? challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation, Honolulu, Hawaii.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. Bbn system descrip-
tion for wmt10 system combination task. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 296?
301, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010.
Linear inversion transduction grammar alignments
as a second translation path. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 148?152, Uppsala,
40
Sweden, July. Association for Computational Lin-
guistics.
Germa?n Sanchis-Trilles, Jesu?s Andre?s-Ferrer, Guillem
Gasco?, Jesu?s Gonza?lez-Rubio, Pascual Mart??nez-
Go?mez, Martha-Alicia Rocha, Joan-Andreu
Sa?nchez, and Francisco Casacuberta. 2010.
Upv-prhlt english?spanish system for wmt10. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
153?157, Uppsala, Sweden, July. Association for
Computational Linguistics.
Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.
2010. Incremental decoding for phrase-based sta-
tistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 197?204, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Lane Schwartz. 2010. Reproducible results in parsing-
based machine translation: The jhu shared task sub-
mission. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 158?163, Uppsala, Sweden, July.
Association for Computational Linguistics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and oovs: Two problems for translation
between german and english. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 164?169, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Jo?rg Tiedemann. 2010. To cache or not to cache?
experiments with adaptive models in statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 170?175, Uppsala, Sweden,
July. Association for Computational Linguistics.
Sami Virpioja, Jaakko Va?yrynen, Andre Man-
sikkaniemi, and Mikko Kurimo. 2010. Apply-
ing morphological decompositions to statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 176?181, Uppsala, Sweden,
July. Association for Computational Linguistics.
Zdene?k Z?abokrtsky?, Martin Popel, and David Marec?ek.
2010. Maximum entropy translation model in
dependency-based mt framework. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 182?187, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Billy Wong and Chunyu Kit. 2010. The parameter-
optimized atec metric for mt evaluation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 335?
339, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Francisco Zamora-Martinez and Germa?n Sanchis-
Trilles. 2010. Uch-upv english?spanish system for
wmt10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 188?192, Uppsala, Sweden, July.
Association for Computational Linguistics.
Daniel Zeman. 2010. Hierarchical phrase-based mt
at the charles university for the wmt 2010 shared
task. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 193?196, Uppsala, Sweden, July. Association
for Computational Linguistics.
41
A Pairwise system comparisons by human judges
Tables 13?20 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row. Bolding indicates the winner of the two systems. The difference between
100 and the sum of the complimentary cells is the percent of time that the two systems were judged to
be equal.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
B Automatic scores
The tables on pages 33?32 give the automatic scores for each of the systems.
C Pairwise system comparisons for combined expert and non-expert data
Tables 21?20 show pairwise comparisons between systems for the into English direction when non-
expert judgments have been added.
The number of pairwise comparisons at the ? level of significance increases from 48 to 50, and the
number at the ? level of significants increases from 79 to 80 (basically same number). However, the
? level of significance went up considerably, from 280 to 369. That?s a 31% increase. 75 of ? are
comparisons involving the reference, then the non-reference ? count went up from 205 to 294, a 43%
increase.
R
E
F
C
A
M
B
R
ID
G
E
C
M
U
-S
T
A
T
X
F
E
R
C
U
-Z
E
M
A
N
D
F
K
I
G
E
N
E
V
A
H
U
IC
O
N
G
JH
U
L
IG
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
C
M
U
-H
Y
P
O
S
E
L
-C
O
M
B
O
D
C
U
-C
O
M
B
O
JH
U
-C
O
M
B
O
L
IU
M
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .00? .00? .00? .00? .04? .03? .00? .00? .00? .00? .04? .00? .04? .00? .00? .00? .00? .05? .06? .03? .09? .04? .04?
CAMBRIDGE .79? ? .36 .16? .12? .23? .27 .43 .26? .38 .24 .3 .28 .51 .34 .23 .37 .24 .32 .46 .24 .29 .45 .59? .44
CMU-STATXFER .84? .58 ? .16? .48 .14? .19 .39 .33 .54 .54? .50? .36 .50 .70? .55? .50 .46 .58? .67? .50 .56? .48 .58? .52?
CU-ZEMAN 1.00? .77? .72? ? .76? .37 .73? .74? .79? .77? .77? .81? .75? .94? .86? .77? .89? .67 .77? .79? .81? .81? .77? .96? .86?
DFKI 1.00? .72? .45 .12? ? .32 .48 .50 .52 .53 .56 .65 .53 .62 .55 .43 .61? .50 .68? .73? .70? .60 .59? .72? .71?
GENEVA 1.00? .69? .76? .48 .56 ? .47 .71? .79? .72? .79? .71? .68? .76? .83? .57 .86? .72? .71? .69? .76? .65? .88? .96? .70
HUICONG .86? .54 .29 .12? .26 .37 ? .48 .31 .43 .63? .62? .53 .55 .53? .44 .50 .55 .52 .68? .52? .51 .52? .57 .53
JHU .83? .39 .42 .13? .33 .19? .3 ? .3 .36 .56? .56? .47 .52 .46 .29 .36 .42 .42 .59? .50 .31 .43 .29 .37
LIG .97? .63? .36 .15? .37 .18? .40 .60 ? .62? .57? .39 .35 .54? .46 .33 .34 .38 .54? .48? .42 .44 .50 .61? .56
LIMSI .96? .41 .23 .19? .31 .17? .32 .50 .28? ? .35 .42 .21 .62? .25 .21 .33 .22 .42 .35 .43 .32 .26 .35 .41
LIUM .83? .33 .21? .13? .41 .05? .13? .15? .09? .3 ? .39 .19 .36 .43 .26 .23? .28 .29 .45 .28 .26 .28 .33 .28
NRC .96? .3 .10? .10? .32 .24? .15? .22? .22 .33 .43 ? .26 .58 .26 .24 .3 .50 .36 .45 .47? .23 .38 .36? .35
ONLINEA .96? .55 .57 .14? .42 .16? .42 .4 .39 .53 .52 .47 ? .52? .46 .36 .64 .57 .59 .50 .59 .42 .46 .43 .48
ONLINEB .87? .37 .33 .03? .29 .12? .31 .26 .16? .12? .39 .35 .20? ? .33 .38 .17? .36 .29 .21 .33 .3 .3 .32 .21?
RALI .89? .45 .15? .06? .35 .04? .12? .42 .35 .46 .32 .42 .39 .52 ? .32 .31 .26 .43 .41 .27 .43 .40 .63? .26
RWTH .91? .46 .21? .05? .51 .36 .44 .46 .53 .39 .48 .48 .39 .48 .48 ? .39 .38 .39 .52 .46 .53? .52 .50? .25
UEDIN .96? .40 .33 .03? .28? .03? .28 .29 .49 .38 .61? .3 .32 .50? .34 .24 ? .42 .33 .43 .48 .18? .13 .27 .38
BBN-C .90? .48 .46 .29 .39 .22? .27 .27 .46 .43 .28 .35 .33 .39 .29 .34 .26 ? .28 .44? .33 .26 .62? .36 .28
CMU-HEA-C .89? .50 .23? .14? .30? .21? .26 .25 .17? .33 .43 .16 .36 .43 .26 .29 .24 .24 ? .48 .27 .13 .25 .30 .15
CMU-HYP-C .81? .17 .19? .11? .19? .19? .14? .14? .19? .40 .23 .18 .29 .46 .35 .29 .21 .15? .17 ? .26 .18 .07? .32 .21
DCU-C .88? .27 .25 .11? .22? .24? .20? .28 .21 .35 .50 .10? .31 .44 .27 .29 .22 .21 .2 .30 ? .12? .26 .26 .08
JHU-C .86? .48 .16? .16? .33 .21? .35 .41 .32 .44 .39 .35 .39 .37 .26 .19? .50? .23 .32 .43 .40? ? .36 .27 .39
LIUM-C .87? .41 .36 .13? .31? .08? .21? .48 .31 .47 .44 .24 .39 .52 .28 .28 .33 .27? .25 .67? .26 .44 ? .54? .48
RWTH-C .88? .18? .13? .04? .22? .04? .14 .24 .25? .3 .33 .05? .43 .50 .30? .13? .23 .14 .18 .21 .19 .23 .11? ? .24
UPV-C .92? .25 .12? .10? .16? .3 .25 .34 .29 .31 .34 .29 .39 .65? .39 .36 .3 .45 .27 .36 .23 .16 .24 .28 ?
> others .90 .44 .31 .13 .33 .18 .29 .37 .34 .42 .44 .38 .37 .51 .41 .31 .38 .35 .38 .48 .39 .36 .40 .46 .37
>= others .98 .66 .51 .21 .42 .27 .51 .59 .53 .65 .71 .66 .52 .71 .65 .55 .65 .64 .70 .77 .72 .65 .64 .77 .68
Table 13: Sentence-level ranking for the WMT10 French-English News Task
42
R
E
F
C
A
M
B
R
ID
G
E
C
U
-Z
E
M
A
N
D
F
K
I
E
U
G
E
N
E
V
A
JH
U
K
O
C
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .08? .02? .00? .04? .08? .13? .06? .09? .09? .07? .16? .11? .12? .12? .12? .05? .07? .08? .09?
CAMBRIDGE .82? ? .16? .24? .15? .07? .35 .10? .42 .36 .43 .27 .67? .46 .39 .44 .40 .46 .48? .40
CU-ZEMAN .98? .82? ? .47 .54? .62? .71? .41 .79? .82? .70? .67? .85? .90? .75? .72? .92? .82? .88? .82?
DFKI .95? .66? .31 ? .46 .25? .78? .36 .59 .62? .75? .65? .45 .56? .75? .69? .71? .63? .57 .65?
EU .96? .78? .30? .41 ? .55 .68? .16? .76? .72? .82? .67? .63? .86? .78? .78? .76? .76? .75? .71?
GENEVA .86? .81? .23? .55? .34 ? .65? .25? .65? .70? .69? .66? .77? .71? .70? .89? .75? .63? .84? .75?
JHU .77? .42 .15? .22? .22? .22? ? .06? .58? .47 .52? .49 .70? .61? .53 .64? .53? .65? .68? .50
KOC .85? .67? .4 .58 .55? .69? .82? ? .76? .85? .81? .72? .86? .82? .86? .85? .77? .77? .74? .79?
LIMSI .84? .23 .08? .29 .09? .30? .21? .08? ? .33 .37 .17? .51 .40 .29 .45 .49 .40 .61? .28
LIUM .85? .39 .07? .32? .11? .21? .44 .07? .46 ? .44 .4 .32 .44 .37 .64? .35 .40 .35 .42
NRC .91? .43 .15? .20? .11? .25? .21? .09? .31 .45 ? .32 .48 .44 .49 .61? .52? .30 .58? .40
ONLINEA .80? .51 .21? .33? .23? .15? .41 .14? .60? .42 .54 ? .52? .56? .36 .67? .61? .45 .50 .44
ONLINEB .87? .23? .08? .43 .23? .11? .12? .08? .27 .36 .43 .25? ? .38 .31 .33 .52 .33? .46 .29
RALI .83? .38 .05? .27? .11? .15? .22? .10? .36 .44 .49 .31? .50 ? .38 .44 .42 .37 .38 .34
RWTH .76? .33 .11? .12? .15? .17? .34 .05? .34 .44 .29 .42 .49 .40 ? .56 .48 .44 .53? .50
UEDIN .84? .29 .20? .17? .12? .09? .19? .07? .33 .23? .24? .24? .56 .31 .3 ? .36? .27 .51 .18?
CMU-HEAFIELD-COMBO .90? .23 .04? .23? .18? .12? .22? .11? .32 .41 .20? .23? .28 .31 .31 .11? ? .29 .24 .3
KOC-COMBO .91? .26 .08? .31? .17? .28? .20? .07? .23 .26 .19 .36 .57? .37 .32 .32 .42 ? .38 .34
RWTH-COMBO .85? .21? .02? .36 .16? .07? .12? .07? .16? .3 .30? .4 .34 .32 .06? .26 .35 .16 ? .21?
UPV-COMBO .87? .38 .08? .30? .19? .19? .37 .11? .39 .24 .33 .37 .44 .27 .34 .46? .35 .28 .50? ?
> others .87 .43 .15 .30 .22 .25 .38 .13 .44 .45 .46 .41 .53 .49 .44 .52 .53 .45 .53 .45
>= others .92 .63 .26 .40 .32 .35 .53 .26 .66 .63 .62 .55 .68 .66 .63 .70 .74 .68 .75 .66
Table 14: Sentence-level ranking for the WMT10 English-French News Task
43
R
E
F
A
A
L
T
O
C
M
U
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
H
U
IC
O
N
G
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
U
E
D
IN
U
M
D
U
P
P
S
A
L
A
U
U
-M
S
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
C
M
U
-H
Y
P
O
S
E
L
-C
O
M
B
O
JH
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .03? .00? .06? .03? .00? .00? .05? .00? .00? .03? .06? .09? .06? .00? .09? .03? .03? .14? .03? .06? .03? .03? .06? .00?
AALTO 1.00? ? .50 .31 .60 .69? .39 .41 .71? .31 .45 .60? .59? .65? .66? .64? .81? .45 .41 .69? .72? .75? .55 .55? .76? .57?
CMU .93? .31 ? .29 .49 .57? .38 .50 .74? .13? .44 .59? .57? .59? .60? .67? .59? .41 .50 .68? .67? .46 .64? .55? .67? .54?
CU-ZEMAN 1.00? .44 .56 ? .58 .64? .17 .44 .75? .38 .50 .54? .76? .79? .73? .72? .72? .50? .73? .78? .80? .68? .72? .62? .68? .73?
DFKI .92? .25 .32 .27 ? .53 .36 .46 .65? .07? .50 .47 .47 .69? .56 .35 .55 .58 .47 .67? .61? .52 .47 .38 .67? .51
FBK .97? .20? .16? .14? .38 ? .11? .31 .45 .10? .22? .36 .50 .57? .37 .43 .40 .12? .17? .48? .43 .35 .38 .22 .38 .39
HUICONG .93? .35 .28 .46 .43 .75? ? .52 .69? .16? .39 .42 .64? .79? .31 .51? .78? .27 .41 .49 .74? .68? .60? .37 .68? .56?
JHU .86? .34 .29 .16 .43 .31 .26 ? .61? .15? .35 .36 .45 .69? .52? .56? .64? .27 .36 .70? .53 .47 .66? .52 .68? .44
KIT .89? .21? .10? .14? .29? .33 .19? .14? ? .03? .27 .21? .36 .46 .17? .29 .24 .25? .25? .48 .23? .31 .38 .2 .36 .12?
KOC .96? .58 .77? .48 .70? .77? .58? .71? .97? ? .77? .90? .72? .82? .76? .84? .81? .84? .66? .83? .87? .79? .77? .75? .93? .71?
LIMSI 1.00? .23 .28 .35 .35 .53? .33 .45 .41 .19? ? .49 .48 .63? .49 .63? .52 .36 .29 .73? .53? .45 .59? .29 .56? .59?
LIU .88? .12? .15? .16? .39 .21 .46 .36 .61? .00? .27 ? .44 .63? .49 .45 .53 .27? .33 .67? .55? .46 .44 .32 .37 .55
ONLINEA .92? .15? .23? .24? .42 .34 .21? .35 .50 .10? .32 .36 ? .41 .4 .44 .37 .32 .34 .36 .4 .47 .3 .26 .48 .41
ONLINEB .68? .18? .29? .17? .26? .24? .18? .23? .33 .18? .23? .27? .34 ? .3 .15? .29 .24? .15? .44 .28 .33? .20? .21? .38 .3
RWTH .88? .17? .20? .20? .37 .49 .41 .23? .61? .16? .4 .3 .43 .56 ? .39 .50 .26 .49 .37 .29 .34 .41 .26 .44 .2
UEDIN .89? .14? .22? .13? .62 .34 .18? .22? .39 .03? .17? .3 .44 .67? .42 ? .39 .15? .14? .52? .40 .36 .43 .26 .41 .38
UMD .91? .07? .14? .08? .36 .34 .11? .25? .48 .16? .24 .34 .52 .56 .41 .45 ? .16? .21? .41 .28 .29 .43 .29 .25 .23
UPPSALA .97? .32 .34 .17? .36 .54? .23 .37 .70? .00? .41 .62? .56 .68? .57 .64? .59? ? .2 .63? .69? .51? .60? .33 .69? .63?
UU-MS .82? .22 .43 .14? .45 .51? .19 .21 .68? .14? .39 .52 .60 .64? .44 .53? .61? .28 ? .36 .58? .52? .53? .30 .64? .44
BBN-C .86? .25? .10? .07? .27? .17? .23 .18? .35 .07? .15? .12? .32 .41 .3 .19? .22 .15? .27 ? .39 .06? .23? .11? .21 .18?
CMU-HEA-C .87? .14? .15? .08? .29? .33 .04? .26 .53? .00? .20? .24? .44 .31 .46 .23 .53 .15? .13? .27 ? .40 .2 .14? .22 .28
CMU-HYP-C .94? .25? .24 .14? .44 .3 .15? .26 .47 .08? .45 .31 .42 .67? .24 .36 .46 .14? .21? .50? .32 ? .43 .28 .51? .42
JHU-C .97? .34 .11? .20? .29 .34 .29? .03? .38 .12? .07? .29 .55 .67? .34 .32 .23 .24? .24? .48? .40 .32 ? .27 .37 .31
KOC-C .88? .00? .23? .21? .53 .44 .29 .22 .43 .08? .36 .50 .53 .63? .39 .37 .39 .28 .19 .64? .61? .38 .55 ? .48? .46
RWTH-C .82? .09? .06? .29? .25? .25 .18? .18? .24 .03? .19? .26 .36 .54 .25 .26 .33 .06? .14? .29 .22 .23? .3 .17? ? .13?
UPV-C .97? .17? .21? .17? .36 .36 .23? .19 .67? .20? .18? .29 .41 .40 .40 .38 .48 .17? .31 .50? .43 .27 .27 .27 .65? ?
> others .91 .23 .25 .20 .39 .42 .24 .30 .53 .11 .31 .38 .47 .59 .42 .43 .48 .27 .30 .53 .49 .42 .44 .31 .51 .41
>= others .96 .42 .46 .36 .50 .66 .47 .53 .72 .23 .52 .59 .63 .73 .62 .66 .68 .51 .55 .77 .73 .65 .67 .59 .75 .64
Table 15: Sentence-level ranking for the WMT10 German-English News Task
R
E
F
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
S
F
U
U
E
D
IN
U
P
P
S
A
L
A
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .03? .06? .01? .02? .05? .00? .00? .01? .04? .03? .01? .01? .01? .02? .01? .01? .05? .06?
CU-ZEMAN .97? ? .85? .67? .62? .78? .58? .70? .64? .80? .85? .64? .52 .80? .61? .79? .69? .76? .73?
DFKI .89? .14? ? .36? .24? .38 .30? .27? .36? .36? .55 .35? .21? .41 .39 .46 .38? .47 .37?
FBK .97? .30? .59? ? .35? .42 .12? .36 .48 .48 .64? .39 .29? .46 .30? .44 .46 .48 .38
JHU .98? .27? .72? .57? ? .59? .30? .51 .53 .56? .65? .43 .39 .66? .45 .56 .61? .52 .47
KIT .92? .18? .55 .42 .29? ? .23? .32 .32? .43 .53? .41 .27? .43 .23? .41 .41 .42 .37
KOC 1.00? .37? .64? .82? .62? .70? ? .74? .74? .74? .82? .63? .48 .62? .65? .73? .67? .81? .71?
LIMSI .95? .27? .68? .39 .45 .49 .17? ? .49 .74? .70? .51 .28? .58? .32 .51 .53? .52? .31
LIU .95? .32? .59? .4 .36 .58? .21? .37 ? .39 .74? .33? .23? .55? .36? .49 .42 .46 .38
ONLINEA .95? .16? .55? .4 .36? .45 .21? .23? .50 ? .56? .38 .23? .41 .23? .48 .4 .50 .33?
ONLINEB .92? .12? .42 .26? .27? .33? .14? .23? .21? .32? ? .24? .14? .39 .19? .29? .27? .36 .32?
RWTH .98? .33? .61? .51 .47 .46 .30? .33 .52? .55 .71? ? .33? .57? .45 .40 .51? .47 .46
SFU .98? .42 .77? .66? .51 .69? .48 .68? .69? .72? .77? .56? ? .82? .53 .65? .69? .73? .62?
UEDIN .94? .17? .51 .4 .31? .49 .34? .25? .30? .52 .52 .36? .10? ? .33? .31 .42 .38 .22?
UPPSALA .97? .36? .55 .51? .47 .70? .25? .46 .57? .67? .71? .41 .38 .54? ? .53? .42 .58? .40
CMU-HEAFIELD-COMBO .96? .17? .49 .36 .36 .37 .21? .35 .49 .42 .64? .38 .28? .48 .28? ? .35 .46 .35
KOC-COMBO .99? .27? .56? .32 .27? .32 .23? .32? .41 .55 .64? .30? .21? .37 .36 .41 ? .34 .36
RWTH-COMBO .92? .17? .50 .34 .35 .41 .09? .25? .38 .4 .54 .38 .20? .42 .19? .28 .35 ? .16?
UPV-COMBO .93? .23? .58? .38 .36 .51 .23? .50 .49 .57? .60? .42 .28? .51? .3 .38 .46 .48? ?
> others .95 .24 .57 .44 .37 .48 .24 .39 .45 .51 .63 .40 .27 .51 .34 .45 .44 .49 .39
>= others .98 .28 .62 .56 .46 .60 .30 .51 .55 .59 .70 .51 .34 .62 .47 .59 .59 .65 .55
Table 16: Sentence-level ranking for the WMT10 English-German News Task
44
R
E
F
C
A
M
B
R
ID
G
E
C
O
L
U
M
B
IA
C
U
-Z
E
M
A
N
D
F
K
I
H
U
IC
O
N
G
JH
U
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
U
P
C
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .01? .01? .01? .00? .00? .00? .00? .00? .01? .02? .05? .01? .04?
CAMBRIDGE .95? ? .23? .14? .34? .31? .41 .34 .62? .45? .35 .40? .42 .22? .44
COLUMBIA .97? .58? ? .25? .52 .45 .59? .53? .65? .60? .47 .56? .55? .45 .58?
CU-ZEMAN .96? .71? .59? ? .60? .68? .79? .66? .75? .80? .66? .79? .78? .69? .75?
DFKI .97? .51? .37 .23? ? .43 .59? .52? .66? .62? .48 .53? .55? .55? .64?
HUICONG .95? .50? .34 .21? .41 ? .45 .50 .66? .61? .39 .50? .59? .40 .52?
JHU .98? .39 .22? .12? .30? .33 ? .37 .56? .51? .34 .39 .34? .22? .34
ONLINEA .96? .46 .37? .23? .32? .38 .44 ? .59? .53? .4 .50 .36 .30? .54?
ONLINEB .88? .25? .21? .16? .23? .21? .27? .23? ? .35 .24? .28? .34? .22? .36
UEDIN .96? .31? .28? .10? .25? .19? .25? .31? .48 ? .23? .27? .31 .23? .2
UPC .94? .47 .4 .20? .41 .33 .43 .46 .66? .56? ? .50? .52? .48? .49?
BBN-COMBO .95? .26? .31? .09? .32? .34? .33 .37 .54? .44? .33? ? .35 .24? .34
CMU-HEAFIELD-COMBO .91? .39 .21? .08? .34? .22? .16? .42 .57? .45 .31? .31 ? .14? .27
JHU-COMBO .95? .40? .32 .15? .36? .31 .44? .50? .66? .50? .32? .47? .43? ? .43?
UPV-COMBO .92? .35 .28? .16? .27? .23? .38 .28? .47 .30 .28? .26 .35 .25? ?
> others .95 .41 .30 .15 .33 .32 .39 .39 .56 .48 .34 .41 .43 .32 .43
>= others .99 .61 .45 .27 .45 .50 .61 .54 .70 .69 .51 .62 .66 .55 .66
Table 17: Sentence-level ranking for the WMT10 Spanish-English News Task
R
E
F
C
A
M
B
R
ID
G
E
C
U
-Z
E
M
A
N
D
C
U
D
F
K
I
JH
U
K
O
C
O
N
L
IN
E
A
O
N
L
IN
E
B
S
F
U
U
E
D
IN
U
P
V
U
C
H
-U
P
V
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .02? .07? .15? .07? .02? .11? .14? .07? .07? .03? .06? .09? .06? .03? .07?
CAMBRIDGE .91? ? .28? .45 .38 .45 .11? .52 .61? .21? .52 .47 .35 .54 .51 .39 .49
CU-ZEMAN .95? .70? ? .79? .75? .85? .49 .83? .82? .74? .87? .67? .85? .81? .80? .70? .74?
DCU .93? .32 .21? ? .45 .32 .09? .70? .59 .24? .48 .38 .29 .32 .36 .24 .14?
DFKI .80? .41 .15? .45 ? .38 .12? .64? .57 .4 .57 .31 .41 .59 .50 .48 .47
JHU .90? .37 .10? .52 .56 ? .17? .67? .67? .26? .34 .3 .49 .54 .53? .47 .35
KOC .98? .87? .47 .88? .73? .76? ? .76? .87? .67? .83? .86? .90? .87? .90? .86? .86?
ONLINEA .82? .42 .08? .30? .18? .24? .20? ? .49 .36 .25? .17? .25? .45 .30? .29 .18?
ONLINEB .76? .26? .10? .32 .37 .22? .10? .34 ? .21? .28 .24? .32 .33 .22? .19? .27?
SFU .91? .54? .19? .67? .51 .63? .27? .64 .72? ? .74? .57? .68? .77? .71? .64? .46
UEDIN .91? .3 .08? .4 .38 .34 .14? .71? .49 .09? ? .34 .4 .58 .33 .3 .31
UPV .94? .34 .07? .41 .53 .54 .07? .73? .61? .27? .45 ? .37 .51 .44 .38 .48?
UCH-UPV .90? .55 .07? .58 .51 .41 .08? .69? .52 .24? .51 .46 ? .47 .41 .49 .49
CMU-HEAFIELD-COMBO .83? .29 .13? .37 .38 .35 .07? .48 .54 .08? .29 .26 .28 ? .17? .21? .21
KOC-COMBO .88? .27 .15? .40 .42 .24? .03? .62? .60? .15? .41 .27 .34 .53? ? .3 .40
RWTH-COMBO .92? .36 .21? .52 .33 .31 .10? .55 .65? .14? .37 .22 .41 .52? .48 ? .31
UPV-COMBO .91? .32 .13? .69? .4 .32 .09? .76? .52? .36 .38 .19? .31 .45 .35 .28 ?
> others .89 .39 .15 .48 .44 .41 .14 .61 .58 .29 .46 .36 .42 .51 .44 .39 .40
>= others .93 .54 .23 .61 .55 .55 .19 .69 .71 .40 .61 .55 .54 .68 .62 .59 .60
Table 18: Sentence-level ranking for the WMT10 English-Spanish News Task
45
R
E
F
A
A
L
T
O
C
M
U
C
U
-B
O
JA
R
C
U
-Z
E
M
A
N
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .04? .02? .03? .00? .02? .00? .03? .03? .04? .01? .04? .02?
AALTO .88? ? .49 .51 .22? .38 .64? .55? .57? .71? .64? .65? .59?
CMU .97? .35 ? .4 .14? .18? .59? .49? .45? .57? .50? .34 .43
CU-BOJAR .90? .33 .43 ? .12? .20? .64? .45 .45 .54? .42 .42 .41
CU-ZEMAN .99? .60? .77? .75? ? .56? .81? .78? .88? .79? .84? .84? .76?
ONLINEA .92? .46 .68? .59? .28? ? .65? .54? .72? .75? .58? .57? .66?
ONLINEB .97? .27? .28? .21? .10? .17? ? .25? .32 .22 .21? .32 .28
UEDIN .95? .28? .26? .38 .07? .22? .49? ? .60? .52? .33 .31 .32
BBN-COMBO .92? .31? .20? .39 .08? .15? .41 .16? ? .27 .25 .3 .26
CMU-HEAFIELD-COMBO .90? .13? .23? .25? .07? .15? .31 .23? .34 ? .18? .35 .28
JHU-COMBO .93? .20? .19? .33 .08? .25? .48? .39 .38 .52? ? .37 .42
RWTH-COMBO .92? .18? .37 .38 .13? .25? .34 .28 .43 .40 .26 ? .25
UPV-COMBO .96? .25? .36 .41 .11? .27? .45 .35 .37 .44 .31 .34 ?
> others .93 .28 .36 .38 .11 .23 .49 .38 .47 .48 .38 .40 .40
>= others .98 .43 .55 .55 .22 .37 .70 .61 .70 .71 .62 .65 .63
Table 19: Sentence-level ranking for the WMT10 Czech-English News Task
R
E
F
C
U
-B
O
JA
R
C
U
-T
E
C
T
O
C
U
-Z
E
M
A
N
D
C
U
E
U
R
O
T
R
A
N
S
K
O
C
O
N
L
IN
E
A
O
N
L
IN
E
B
P
C
-T
R
A
N
S
P
O
T
S
D
A
M
S
F
U
U
E
D
IN
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
D
C
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .04? .04? .03? .01? .05? .03? .08? .04? .04? .03? .02? .02? .04? .08? .04? .07? .04?
CU-BOJAR .87? ? .46 .27? .12? .28? .16? .17? .44 .4 .11? .27? .41 .28 .52? .28 .42 .43
CU-TECTO .88? .36 ? .30? .23? .38 .17? .28? .56? .44 .29? .27? .36 .45 .51? .4 .58? .35
CU-ZEMAN .91? .58? .51? ? .38 .49 .19? .39 .62? .63? .36 .41 .48 .51? .58? .48? .54? .55?
DCU .98? .73? .52? .43 ? .59? .22? .47 .74? .63? .47? .53? .56? .77? .77? .62? .76? .71?
EUROTRANS .88? .61? .47 .33 .30? ? .10? .33 .51 .54? .25? .27? .49 .57? .59? .49 .57? .60?
KOC .93? .69? .67? .54? .49? .77? ? .54? .71? .70? .51? .55? .64? .72? .78? .65? .76? .78?
ONLINEA .91? .62? .57? .51 .39 .44 .24? ? .66? .62? .39 .43 .55? .60? .61? .59? .73? .61?
ONLINEB .91? .31 .29? .27? .13? .33 .14? .19? ? .44 .22? .09? .39 .19 .34 .24? .22? .39
PC-TRANS .88? .45 .43 .24? .26? .29? .21? .24? .49 ? .22? .27? .37 .43 .55? .33? .49 .41
POTSDAM .88? .60? .51? .40 .27? .59? .25? .47 .63? .64? ? .45 .52? .56? .69? .61? .70? .68?
SFU .95? .52? .56? .4 .30? .61? .27? .39 .65? .64? .29 ? .55? .54? .76? .53? .70? .60?
UEDIN .94? .39 .44 .33 .23? .32 .20? .26? .32 .49 .25? .26? ? .43 .57? .18 .46? .42
CMU-HEAFIELD-COMBO .91? .42 .39 .23? .10? .27? .14? .19? .23 .35 .24? .19? .28 ? .48? .28 .34 .29
DCU-COMBO .84? .23? .27? .23? .03? .31? .10? .21? .42 .31? .15? .10? .16? .20? ? .18? .27? .22?
KOC-COMBO .91? .37 .49 .25? .10? .39 .17? .32? .42? .55? .17? .27? .26 .33 .41? ? .32 .22
RWTH-COMBO .88? .29 .34? .28? .05? .26? .10? .17? .48? .43 .16? .15? .24? .33 .46? .36 ? .29
UPV-COMBO .92? .37 .52 .22? .09? .25? .10? .19? .28 .47 .15? .25? .33 .24 .49? .34 .39 ?
> others .91 .45 .44 .32 .20 .39 .16 .29 .49 .49 .25 .28 .40 .43 .54 .39 .50 .45
>= others .96 .66 .60 .50 .38 .54 .33 .44 .70 .62 .44 .45 .62 .69 .75 .66 .70 .68
Table 20: Sentence-level ranking for the WMT10 English-Czech News Task
46
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-. +,/- +,/. +,-0 +,/+ +,-1 +,-0 +,-. +,-1 +,/+ +,-/ +,2- +,3/ +,24 +,./ +,/3 +,--''567*8'* +,4+ +,/1 !"#$ !"#% !"#% !"#% !"#% !"#! !"## !"#& !"#' !"() !"'& !"*& !"%% !"#$ !"%)78&69:67*8'* !"&' +,/1 +,// +,/- +,/- +,/. +,/. +,/+ +,/- !"#& +,/+ !"() !"'& +,.; !"%% !"#$ +,-178& +,// +,// +,/. +,/2 +,/2 +,/3 +,/2 +,-; +,/2 +,/- +,-1 +,24 !"'& +,.. +,-3 +,/- +,-;7&6'*<"= +,// +,/; +,/. +,/+ +,/3 +,-0 +,/+ +,-/ +,/3 +,/. +,-4 +,2/ +,3/ +,.+ +,.4 +,/. +,-;7&6>?8"5 +,22 +,-0 +,/2 +,-/ +,-; +,-/ +,-/ +,.0 +,-/ +,-; +,-2 +,23 +,3. +,2+ +,.+ +,/+ +,-3<@&67*8'* +,;2 +,/4 !"#$ !"#% !"#% +,/. !"#% +,-1 +,/. +,// +,-0 +,21 !"'& +,./ +,-2 +,/. +,-4*5#A5?B +,.4 +,/- +,/2 +,-0 +,/+ +,-1 +,/+ +,-2 +,-4 +,-0 +,-- +,2- +,3- +,24 +,.- +,-0 +,-2*5#A5?C +,4+ !"#) !"#$ !"#% +,/. +,/. +,/. +,-0 +,/- +,/; +,/+ !"() !"'& +,.; +,-. +,/. +,-;=D)@67*8'* +,;/ +,/1 +,// !"#% !"#% +,/. +,/. +,-0 +,/. +,/; +,-0 !"() !"'& +,./ +,-2 +,// +,-1&?EA5 +,;3 +,/4 +,/- +,/2 +,/. +,/3 +,/2 +,-1 +,/. +,// +,-0 +,21 +,3; +,.- +,-2 +,// +,-1&FG67*8'* +,;. +,/1 +,// !"#% !"#% +,/. +,/. +,-0 +,/- +,/; +,/+ !"() !"'& +,./ +,-. +,// +,-1!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-0 +,/2 -,04 !"%' +,+2 +,+4 +,24 +,-2 +,2/ +,/2 +,/2 +,31 ;,2+''567*8'* +,/- !"#& /,-; !"%+ !"!* +,33 !"** !"%+ !"*! !"#+ !"#+ !"(# &"'!78&69:67*8'* +,/- +,/4 /,-. !"%+ !"!* +,+0 !"** +,-1 !"*! +,/4 +,/4 !"(# ;,0178& +,/+ +,/. /,3+ +,-/ +,+2 +,+0 +,.3 +,-; +,21 +,/- +,/- +,22 ;,;;7&6'*<"= +,/2 +,// /,2. +,-/ +,+2 +,+/ +,.+ +,-/ +,24 +,// +,// +,30 ;,2-7&6>?8"5 +,-4 +,-0 -,44 +,.; +,+2 +,+. +,2. +,.4 +,22 +,-0 +,-0 +,3- /,-/<@&67*8'* +,/. +,// /,2/ +,-; +,+2 +,+. +,.2 +,-; +,20 +,/; +,/; +,2. 4,++*5#A5?B +,-0 +,/3 -,14 +,-2 +,+2 +,+4 +,24 +,-. +,2/ +,/2 +,/2 +,31 ;,3.*5#A5?C !"## !"#& /,-3 !"%+ !"!* !"'$ +,.2 +,-4 !"*! !"#+ !"#+ +,2- ;,3.=D)@67*8'* +,/- +,/; /,-+ !"%+ !"!* +,+4 +,.2 +,-4 +,20 +,/4 +,/4 +,2- 4,+;&?EA5 +,/. !"#& /,-+ !"%+ !"!* +,3- +,.2 +,-4 +,20 +,/4 +,/4 +,2. ;,;-&FG67*8'* +,/- !"#& #"%& !"%+ !"!* +,+4 !"** +,-4 +,20 !"#+ !"#+ +,2- ;,00
="5H
IJK)"5L*=E MNJFO(#?P?=(=?7"## KQR(J"5H MNKSB CNDM6N CH"=%
RNMNTJ("EU RNMNTJ(="5HRNMNTJ(@)?= KNVOB K?8VTK,-./012345670888((66((((((R?)=A7%(%&'8A)?E()*(WOKM(R?)=A7%RBMJ(2+3+X(F#&%()D*('"%?#A5?(8?)=A7%(!CSNY("5E(WOKM$,((K7*=?%(L*=(Z"##Z(!?5[=?(\RM3+()?%)%?)$("5E(Z%&'Z(!%&'%?)(*L()@?(@&8"5#]("%%?%%?E(E")"$("=?(%@*D5, SJ^C- SJ9C-
WOKMCSNY(G3."
RM6W_I RM68W_I C"E`?=(L&## C"E`?=(#A)? BMN_(2,3
I=E*7 YS_@O(#?P?=(CSNY
K?8VTK(CSNY I_Y6S:a
MNKSB(R
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./01 ./0. %&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 %&'% %&'* ./2.+"-'56789 ./00 ./0. %&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* ./1. ./1; %&'* ./2.+-&*<=*+,-', ./>. ./0. %&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 ./1; %&'* ./2.+-&*?@A*+,-', %&(( ./0. %&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* %&-/ %&-) ./2> ./2.+-&*%B"BCD95 ./24 ./2: ./22 ./22 ./21 ./1; ./20 ./2: ./24 ./E; %&/( %&.( %&-. ./20 ./1:+&*F9-") ./E4 ./2E ./22 ./24 ./2. ./10 ./2E ./21 ./1: ./E1 %&/' %&,. %&.( ./23 ./117+&*+,-', ./>E %&+/ %&'( %&') %&'* %&'- %&') ./0E %&'- %&.. %&/* %&-. %&'% %&'* %&'/7G6 ./1E ./20 ./23 ./2E ./24 ./10 ./23 ./21 ./1; ./E2 ./4> ./3E ./1. ./22 ./1089)9H" ./E> ./22 ./2E ./1; ./1; ./11 ./2. ./24 ./10 ./E3 ./41 ./E> ./3> ./23 ./11?&6+,)8 ./24 ./20 ./22 ./22 ./21 ./2. ./20 ./2: ./24 ./E: ./4> ./3E ./11 ./22 ./1>I?&*+,-', ./02 ./0. %&'( ./2: %&'* ./23 ./2: ./04 ./23 ./3E %&/* ./1. ./1: %&'* ./2.I?& ./2; ./0. %&'( ./2> ./20 ./2E ./2: ./0. ./23 ./34 %&/* ./3: ./1> ./2> ./1;#68 ./23 ./2; ./20 ./22 ./22 ./24 ./20 ./2; ./2E ./3. ./4> ./30 ./12 ./20 ./1:#6-%6 ./02 ./2; ./20 ./2> ./20 ./2E ./2> ./0. ./23 ./34 ./4> ./3: ./1> ./2> ./1:#6&-*+,-', ./01 ./0. %&'( ./2: ./2> ./23 ./2: ./04 ./23 %&.. %&/* ./14 ./1; ./2> ./1;#6&- ./>4 ./0. %&'( ./2: ./2> ./23 %&') ./04 %&'- ./3E %&/* ./14 ./1; %&'* ./2.)5+ ./00 ./0. %&'( ./2> ./20 ./2E ./2: ./04 ./23 ./3E %&/* ./3; ./1: ./2> ./1;,)#6)9J ./2E ./2; ./20 ./22 ./22 ./2. ./20 ./2: ./24 ./3. ./4> ./30 ./11 ./21 ./10,)#6)9K ./>4 ./0. %&'( ./2: %&'* ./23 ./2: ./04 %&'- ./3E %&/* ./1E ./1; ./2> ./1;5"#6 ./02 ./0. ./20 ./2> ./20 ./2E ./2: ./04 %&'- ./3E %&/* ./1. ./1: ./2> ./1;5LB?*+,-', %&(( ./0. %&'( ./2; %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* ./2.5LB? ./22 ./2; ./20 ./2> ./20 ./2E ./2: ./0. ./23 ./34 ./4> ./3: ./10 %&'( ./1;&976) ./02 ./0. ./20 ./2> ./2> ./23 ./2: ./04 %&'- ./3E %&/* ./1. ./1; %&'* ./1;&AH*+,-', ./0: ./0. %&'( ./2: %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* %&'/!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', %&'* ./0. 2/:. ./24 %&%. ./.E %&.( ./2. %&.- %&+/ ./3. (&*/+"-'56789 ./2> ./0. 2/:. ./24 %&%. ./.2 %&.( ./2. ./33 %&+/ ./3. >/02+-&*<=*+,-', %&'* ./0. 2/:4 %&', %&%. ./.3 %&.( %&'/ ./31 %&+/ ./3. >/0;+-&*?@A*+,-', %&'* ./0. 2/:1 %&', %&%. ./.3 %&.( %&'/ %&.- %&+/ ./3. >/0;+-&*%B"BCD95 ./22 ./2> 2/20 ./1: %&%. ./.3 ./31 ./1; ./34 ./2: ./E2 >/40+&*F9-") ./24 ./21 2/3. ./1. ./.E ./.E ./E> ./1. ./E> ./22 ./4; 0/4E7+&*+,-', %&'* %&+/ 2/:> ./2E %&%. ./.3 %&.( %&'/ %&.- %&+/ %&./ >/>:7G6 ./24 ./20 2/10 ./10 ./.E ./.1 ./34 ./12 ./E; ./22 ./4; 0/.489)9H" ./1; ./22 2/E; ./12 ./.E ./.2 ./E; ./13 ./E> ./21 ./4> 2/0>?&6+,)8 ./22 ./2: 2/0. ./1: %&%. ./.3 ./33 ./1> ./34 ./2> ./E1 0/;;I?&*+,-', ./2> ./0. 2/>> ./24 %&%. ./.4 ./30 ./2. ./33 ./0. ./E; >/04I?& ./2> ./2; 2/>1 ./2. %&%. ./.: ./32 ./2. ./3E ./0. ./E: >/11#68 ./22 ./2: 2/0E ./1: %&%. ./.2 ./31 ./1; ./34 ./2: ./E0 >/43#6-%6 ./20 ./2; 2/>E ./24 %&%. ./.2 ./30 ./2. ./33 ./2; ./E> >/34#6&-*+,-', ./2> ./0. 2/>> ./24 %&%. ./.3 %&.( %&'/ ./33 ./0. ./E; >/>.#6&- %&'* %&+/ '&** %&', %&%. ./.0 %&.( %&'/ %&.- %&+/ ./E; >/2;)5+ ./2> ./0. 2/>; ./2. %&%. ./.1 ./30 ./2. ./33 ./0. ./E; >/23,)#6)9J ./22 ./2: 2/21 ./2. %&%. ./.: ./33 ./1: ./34 ./2: ./E2 >/E;,)#6)9K %&'* ./0. 2/:. %&', %&%. %&// %&.( %&'/ %&.- %&+/ ./E; >/>35"#6 ./2> ./0. 2/:. %&', %&%. ./.0 ./30 ./2. ./33 ./0. ./E: >/115LB?*+,-', %&'* %&+/ 2/:> %&', %&%. ./.4 %&.( %&'/ %&.- %&+/ ./3. >/>35LB? ./20 ./2; 2/>3 ./2. %&%. ./.1 ./30 ./2. ./33 ./0. ./E: >/3;&976) ./2> ./0. 2/:E ./24 %&%. ./.1 ./30 ./2. ./33 %&+/ ./E: >/11&AH*+,-', %&'* %&+/ '&** %&', %&%. ./.3 %&.( ./2. %&.- %&+/ ./3. >/01
0123456738#9:5;(((**(((((M9B56+%(%&'-6N97(B,(OPQR(M9B56+%MJRS(E.4.T(A#&%(BL,('"%9#6)9(-9B56+%(!KUVW(")7(OPQR$/((Q+,59%(D,5(X"##X(!9)Y59(ZMR4.(B9%B%9B$(")7(X%&'X(!%&'%9B(,D(B?9(?&-")#@("%%9%%97(7"B"$("59(%?,L)/5")[ MR*O\] MR*-O\] K"7895(D&## K"7895(#6B9 ]\W*U=^ US_K1 US<K1Q9-`aQ Q9-`aQ(KUVW
P(#9N95(KUVW P(#9N95(59+"## QbM(S")[ RVQUJ(M RVQUJ QB")D,57
MVRVaS("7c MVRVaS(?B95 MVRVaS(5")[ QV`PJJRV\(E/4
KUVW(43" OPQRRVSA ]S ]57,+ WU\? KVLR*V K["5%
47
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-. +,/- +,/0 +,/+ +,-1 +,-0 +,/+ +,/. +,-2 +,./ +,3/ +,./ +,0/ +,-4 +,02''567*8'* !"## !"$% +,// !"$$ !"$& +,/+ +,/2 +,/1 +,/. !"'( +,39 +,02 +,-2 !"$& !"&)78&6:;67*8'* +,90 +,/4 +,// !"$$ !"$& +,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !"$& !"&)78&6<=>67*8'* +,2/ +,/4 +,// +,/- +,/0 +,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-378& +,-2 +,/- +,/3 +,/+ +,/+ +,-- +,/3 +,/0 +,-9 +,.2 +,32 +,0+ +,04 +,/3 +,047&6?@8"5 +,02 +,/3 +,/0 +,-9 +,-9 +,-. +,-4 +,/+ +,-/ +,.0 +,3- +,.+ +,0. +,/+ +,02ABC +,/+ +,/- +,/. +,-1 +,-4 +,-- +,/3 +,/. +,-9 +,.- +,32 +,.1 +,0/ +,/+ +,02DE +,22 +,/4 +,/- +,/3 +,/3 +,-1 +,// +,/9 +,/+ +,.9 +,32 +,03 +,-. +,/. +,01<&C7*5F +,-9 +,/0 +,/3 +,-1 +,-4 +,-0 +,/+ +,/. +,-9 +,./ +,3/ +,./ +,02 +,/+ +,02G<&67*8'* +,29 +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3G<& +,/0 +,// +,/. +,/. +,/3 +,-/ +,/3 +,/- +,-9 +,.9 +,32 +,.1 +,09 +,-1 +,09EC) +,9. +,/4 +,/- +,/0 +,/. +,-1 +,// +,/9 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-+E*767*8'* +,/1 +,/2 +,/- +,/0 +,/. +,-4 +,/0 +,/2 +,-1 +,.4 +,32 +,03 +,-3 +,/. +,-+E*7 +,.0 +,-1 +,-9 +,-/ +,-/ +,09 +,-0 +,-/ +,-3 +,.3 +,30 +,.3 +,.4 +,-1 +,0/#C8%C +,/. +,/9 +,/- +,/. +,/3 +,-4 +,/- +,/2 +,/+ +,.4 +,32 +,0. +,-3 +,/. +,01#C& +,/1 +,/9 +,/- +,/3 +,/+ +,-9 +,/0 +,// +,-1 +,.9 +,32 +,0. +,-3 +,/. +,01*5#C5@H +,20 +,/9 +,/- +,/. +,/3 +,-4 +,/0 +,// +,-1 +,.9 +,32 +,03 +,-+ +,-1 +,02*5#C5@I +,90 !"$% !"$* !"$$ !"$& !"$) +,/9 !"*! !"$' !"'( !"(+ !"'# !"&# !"$& +,-3JK)<67*8'* +,9/ +,/4 +,// +,/- !"$& +,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !"$& +,-3JK)< +,2. +,/4 +,// +,/0 +,/. +,-1 +,// +,/9 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-3&@AC5 +,22 +,/4 +,/- +,/0 +,/. +,/+ +,// +,/9 +,/3 +,.1 +,32 +,00 +,-0 +,/0 +,01&8A +,24 +,/4 +,// +,/0 +,/. +,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,00 +,-- +,/0 +,-+&>>%"#" +,/3 +,// +,/0 +,/+ +,/+ +,-/ +,/3 +,/- +,-4 +,.2 +,3/ +,.4 +,04 +,/3 +,04&>L67*8'* +,2- +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3&&68% +,// +,// +,/. +,/+ +,-1 +,-/ +,/3 +,/0 +,-9 +,.2 +,3/ +,.1 +,09 +,/3 +,04!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,/+ +,/. /,+3 +,-3 +,+. +,+3 +,.1 +,-. +,./ +,/0 +,31 2,04''567*8'* +,// +,/9 /,-2 +,-4 !"!' +,+- +,0/ +,-9 +,0+ +,/4 !")* #")+78&6:;67*8'* +,// +,/9 /,-9 +,-4 !"!' +,+0 +,0- +,-2 +,0+ +,/4 !")* 9,3178&6<=>67*8'* +,/- +,/9 /,-2 +,-9 !"!' +,+- +,00 +,-2 +,.1 +,/4 +,./ 9,+178& +,/+ +,/0 /,+4 +,-. +,+. +,+0 +,0+ +,-- +,.2 +,/. +,.3 2,-97&6?@8"5 +,-4 +,/3 -,14 +,09 +,+. +,+. +,.2 +,01 +,.0 +,/3 +,32 /,93ABC +,-1 +,/0 /,3. +,-/ +,+. +,+2 +,03 +,-/ +,.4 +,/0 +,39 /,1/DE +,/- +,/9 /,-0 +,-9 !"!' +,+. +,00 +,-2 +,.4 +,/4 +,.. 2,29<&C7*5F +,/+ +,/0 /,33 +,-. +,+. +,+. +,0+ +,-0 +,.2 +,/. +,31 2,30G<&67*8'* +,/- +,/9 /,-0 +,-9 !"!' +,+. +,0- +,-2 +,.1 +,/4 +,./ 9,30G<& +,/3 +,/0 /,3. +,-. +,+. +,+/ +,03 +,-- +,.9 +,/- +,.3 2,90EC) +,/- +,/4 /,/3 +,-4 !"!' +,+/ +,0- +,-9 +,0+ +,/4 +,.- 2,13E*767*8'* +,/. +,// /,03 +,-/ +,+. +,+. +,00 +,-/ +,.4 +,/2 +,.- 2,44E*7 +,-0 +,-4 -,9. +,02 +,+. +,+3 +,.9 +,-+ +,.- +,-9 +,32 /,01#C8%C +,/0 +,/2 /,0/ +,-9 +,+. +,+- +,0. +,-2 +,.4 +,/9 +,.0 2,90#C& +,/0 +,/2 /,0- +,-2 +,+. +,+0 +,0. +,-2 +,.4 +,/2 +,.. 2,2+*5#C5@H +,/0 +,// /,.- +,-2 +,+. +,3+ +,00 +,-2 +,.1 +,/2 +,.3 2,44*5#C5@I !"$* !"*! $"#( !"$) !"!' !"(# !"'* !"&% !"') !"*( !")* 9,34JK)<67*8'* +,// +,/4 /,/+ +,-4 !"!' +,+. +,0/ +,-9 +,0+ +,/1 !")* 9,.+JK)< +,/- +,/9 /,-- +,-9 !"!' +,+0 +,00 +,-2 +,.1 +,/4 +,.- 2,12&@AC5 +,/- +,/4 /,/0 +,-1 !"!' +,+2 +,0- +,-4 +,0+ +,/4 +,.0 2,4+&8A +,// +,/4 /,/+ +,-4 !"!' +,+9 +,00 +,-9 +,.1 +,/4 +,.0 2,99&>>%"#" +,/3 +,/- /,.3 +,-0 +,+. +,+. +,03 +,-- +,.9 +,/- +,.3 2,/3&>L67*8'* +,/- +,/9 /,-/ +,-4 !"!' +,+0 +,0- +,-/ +,.1 +,/4 +,./ 9,3/&&68% +,/+ +,/- /,34 +,-- +,+. +,+. +,03 +,-/ +,.9 +,/- +,.+ 2,0.
,-./0123145678(((((66(((((M@)JC7%(%&'8CN@A()*(OPQR(M@)JC7%MHRS(.+3+T(>#&%()K*('"%@#C5@(8@)JC7%(!IUVW("5A(OPQR$,((Q7*J@%(X*J(Y"##Y(!@5ZJ@([MR3+()@%)%@)$("5A(Y%&'Y(!%&'%@)(*X()<@(<&8"5#=("%%@%%@A(A")"$("J@(%<*K5,J"5E MR6O\] MR68O\] I"AF@J(X&## I"AF@J(#C)@ ]\W6U;^ US_I- US:I-Q@8`aQ Q@8`aQ(IUVW
P(#@N@J(IUVW P(#@N@J(J@7"## QbM(S"5E RVQUH(M RVQUH Q)"5X*JA
MVRVaS("Ac MVRVaS(<)@J MVRVaS(J"5E QV`PHHRV\(.,3
IUVW(30" OPQRRVS> ]S ]JA*7 WU\< IVKR6V IE"J%
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./01 ./02 ./34 ./35 ./35 ./36 ./0. ./07 ./33 ./76 ./24 ./67 ./37 ./35 !"#$+"-'89:;< ./02 ./0. ./34 ./34 ./3= ./37 ./35 ./01 ./33 ./76 ./24 ./62 ./31 ./35 !"#$+-&*>?*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0. ./07 ./33 ./76 ./24 ./62 ./31 ./35 !"#$+,#&-'9" ./63 ./34 ./30 ./33 ./36 ./3. ./30 ./35 ./31 ./7. ./2= ./73 ./6= ./3= ./64+&*@<-") ./1= ./33 ./34 ./31 ./31 ./64 ./36 ./30 ./3. ./1= ./23 ./13 ./62 ./33 ./63:A9 ./63 ./34 ./33 ./36 ./37 ./65 ./30 ./34 ./32 ./14 ./24 ./73 ./60 ./30 ./6=B&9+,); ./3. ./3= ./30 ./30 ./33 ./32 ./34 ./02 ./37 ./72 ./2= ./73 ./6= ./3= ./64CB&*+,-', ./33 ./0. ./34 ./34 ./34 ./37 ./35 ./01 ./36 ./77 ./24 ./6. ./32 ./34 ./3.CB& ./02 ./0. ./3= ./3= ./3= ./37 ./35 ./01 ./36 ./77 ./24 ./75 ./3. ./34 ./65,)#9)"D ./36 ./0. ./3= ./3= ./3= ./37 ./34 ./02 ./36 ./71 ./2= ./75 ./64 ./33 ./60,)#9)<E !"%! !"&' !"#( !"&) !"&$ !"#& !"&) !"&# !"#% !"'& !")! !"*% !"## ./35 ./3.&<:9) ./05 ./02 ./3= ./34 ./34 ./36 ./0. ./07 ./33 ./77 ./24 ./61 ./31 ./35 !"#$&F+ ./32 ./35 ./30 ./33 ./33 ./32 ./34 ./0. ./37 ./72 ./2= ./7= ./64 ./34 ./65&FG*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0. ./07 ./33 ./76 ./24 ./61 ./37 !"&! !"#$!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./35 ./02 3/51 ./31 !"!' ./.6 ./74 ./3. ./76 ./01 ./71 =/56+"-'89:;< ./34 ./0. 3/43 ./32 !"!' ./.= ./7= ./3. ./77 ./02 ./72 =/==+-&*>?*+,-', ./35 ./02 3/5. ./31 !"!' ./.6 ./7= ./3. ./76 ./01 ./72 =/4=+,#&-'9" ./33 ./3= 3/37 ./6= ./.1 ./.6 ./76 ./60 ./7. ./3= ./1= =/25+&*@<-") ./36 ./30 3/60 ./61 ./.1 ./.7 ./14 ./62 ./1= ./30 ./11 0/03:A9 ./36 ./34 3/07 ./3. ./.1 ./.4 ./76 ./6= ./72 ./34 ./17 0/0.B&9+,); ./3= ./35 3/=7 ./65 !"!' ./.3 ./76 ./64 ./72 ./35 ./10 =/7=CB&*+,-', ./34 ./0. 3/47 ./32 !"!' ./.2 ./7= ./3. ./77 ./02 ./7. =/42CB& ./34 ./0. 3/47 ./32 !"!' ./.4 ./70 ./65 ./77 ./02 ./15 =/=1,)#9)"D ./3= ./35 3/=. ./31 !"!' ./22 ./70 ./3. ./71 ./02 ./14 =/==,)#9)<E !"&$ !"&' &"!' !"#& !"!' !")# !"*! !"#' !"'& !"&* !"'' +"'(&<:9) ./35 ./01 3/5= ./36 !"!' ./21 ./74 ./32 ./76 ./01 ./7. =/04&F+ ./30 ./0. 3/=3 ./3. !"!' ./.7 ./73 ./64 ./71 ./0. ./10 =/13&FG*+,-', ./35 ./01 3/5= ./37 !"!' ./.6 ./74 ./3. ./76 ./01 ./72 =/41
,-./01234/560127((((**(((((H<I89+%(%&'-9J<:(I,(KLMN(H<I89+%HDNO(1.2.P(F#&%(IQ,('"%<#9)<(-<I89+%(!ERST("):(KLMN$/((M+,8<%(U,8(V"##V(!<)W8<(XHN2.(I<%I%<I$("):(V%&'V(!%&'%<I(,U(IB<(B&-")#Y("%%<%%<:(:"I"$("8<(%B,Q)/8")Z HN*K[\ HN*-K[\ E":;<8(U&## E":;<8(#9I< \[T*R?] RO^E6 RO>E6M<-_`M M<-_`M(ERST
L(#<J<8(ERST L(#<J<8(8<+"## MaH(O")Z NSMRD(H NSMRD MI")U,8:
HSNS`O(":b HSNS`O(BI<8 HSNS`O(8")Z MS_LDDNS[(1/2
ERST(27" KLMNNSOF \O \8:,+ TR[B ESQN*S EZ"8%
48
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. /012 /034 /035 /03/ /056 /076 !"#$ /076 /052 /038 /055)&+'.9": /011 /036 /035 /052 /051 /071 /057 /071 /056 /03/ /057)&+;<);. /01/ /036 /037 /051 /057 /075 /05/ /077 /05= /054 /058)&+><*"? /03/ /033 /03= /051 /057 /078 /072 /075 /05= /051 /072@)&+).*'. !"%$ !"$& !"$$ !"$' !"#( !")& !"#$ !")( !"$' !"$* !"#+@)& /074 /035 /038 /051 /057 /07= /072 /077 /058 /052 /057<&:.;:"?% /035 /035 /038 /057 /058 /0=2 /076 /078 /074 /054 /05=A.)+).*'. /011 /036 /035 /052 /051 /076 /057 /076 /056 /038 /053A.) /077 /03= /052 /055 /058 /0=2 /071 /077 /072 /052 /05=.?#B?<C /055 /033 /038 /053 /058 /078 /074 /077 /058 /054 /05=.?#B?<D /06/ /034 /035 /052 /051 /076 /055 /076 /054 /038 /053E)+;:"?% /01= /033 /038 /055 /058 /078 /076 /0=2 /071 /052 /05=E.;%@"* /055 /033 /038 /053 /05= /078 /074 /075 /05= /052 /05=:F;G+).*'. /06/ /036 /037 /03/ /056 /074 /055 /076 /052 /038 /053%H& /053 /033 /03= /055 /058 /078 /074 /077 /058 /052 /057&<@B? /01= /036 /037 /054 /053 /071 /05= /076 /056 /03/ /055&EI+).*'. /014 /034 /033 /03/ /056 /074 !"#$ /076 /052 /038 /053!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. !"#+ /056 504/ !"$$ +/0/= /0/3 /037 /086 301=)&+'.9": /053 /051 506/ /037 ,!"!' /0/1 /038 /081 3056)&+;<);. /057 /053 501= /037 ,!"!' /0/2 /03/ /087 30=3)&+><*"? /058 /05= 5071 /03/ ,!"!' /0/3 /056 /08= 5021@)&+).*'. /056 !"#( #"&! !"$$ ,!"!' !"'# !"$# !"'& $"&+@)& /05/ /058 50=3 /056 ,!"!' /0/3 /051 /087 5023<&:.;:"?% /05/ /05= 505= /038 ,!"!' /0/5 /051 /08/ 5055A.)+).*'. /053 /051 506/ /037 ,!"!' /0/= /038 /086 3034A.) /074 /05/ 50=5 /057 ,!"!' /0/7 /055 /08= 5037.?#B?<C /058 /057 5054 /038 ,!"!' /0/1 /056 /08= 5013.?#B?<D !"#+ !"#( 5046 /035 ,!"!' /08/ /037 /086 3031E)+;:"?% /058 /057 503= /03/ ,!"!' /0/3 /056 /08/ 5031E.;%@"* /05/ /05= 5074 /052 ,!"!' /0/7 /051 /08= 5065:F;G+).*'. !"#+ /056 5048 !"$$ ,!"!' /0/5 /038 /084 304=%H& /058 /057 5055 /03/ ,!"!' /0/5 /056 /088 5014&<@B? /053 /051 5067 /035 ,!"!' /088 /03= /081 3057&EI+).*'. !"#+ !"#( 5045 /035 ,!"!' /0/1 /037 /084 3044
:"?A
JKL;"?H.:@ MNKEO(#<P<:(:<)"## LQR(K"?A MNLSC DNFM+N DA":%
RNMNTK("@U RNMNTK(:"?ARNMNTK(G;<: LNVOC L<*VTL-./0123,45673888((++((((((R<;:B)%(%&'*B;<@(;.(WOLM(R<;:B)%RCMK(=/8/X(E#&%(;F.('"%<#B?<(*<;:B)%(!DSNY("?@(WOLM$0((L).:<%(H.:(Z"##Z(!<?[:<(\RM8/(;<%;%<;$("?@(Z%&'Z(!%&'%<;(.H(;G<(G&*"?#]("%%<%%<@(@";"$(":<(%G.F?0 SK^D5 SK,D5
WOLMDSNY(I87"
RM+W_J RM+*W_J D"@`<:(H&## D"@`<:(#B;< CMN_(=08
J:@.) YS_GO(#<P<:(DSNY
L<*VTL(DSNY J_Y+S-a
MNLSC(R
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0123 0145 0146 !"#$ 0144 0164 0167 0148 0140)*&9:;9)<*'< 0156 0148 !"## !"#$ !"#% !"&$ !"#' !"%! !"#()&9=/*"> 01?2 014@ 0143 0165 0162 0138 0163 0146 0163-A, 0160 0146 014@ 0140 0167 0137 0166 0144 0162/& 013? 0143 014@ 0168 0165 0138 0166 0142 0162./>/B" 0134 0144 014@ 0167 0168 0135 016? 0146 0166CD& 0143 0145 0143 0146 0143 0166 0167 0148 0168E<)9)<*'< 0128 !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(E<) 01?2 0143 014@ 0167 0168 0135 0163 0144 0162#,*%, 0122 0148 0146 0144 0146 0164 0167 0147 0140#,&* 0123 0148 0146 0142 0144 0162 !"#' !"%! 014@>+) 012? 0145 0146 0146 0143 0164 0167 0148 0167<>#,>/F 0144 0145 0143 0146 0143 0166 0168 0148 0167<>#,>/G 0128 0148 0146 !"#$ 0144 0162 !"#' !"%! 014@+"#, 0122 0148 0146 0142 0144 0162 0140 !"%! 014@+HID9)<*'< !"$# !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(+HID 0123 0148 0146 0144 0146 0164 0140 0147 014@&/-,> 0150 0148 0146 0142 0144 0164 0140 !"%! 014@&JB9)<*'< 0122 !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0144 0145 4143 014? 9010? 0102 0148 01?8 5165)*&9:;9)<*'< !"#$ 0147 4156 0146 *!"!' 0103 !"%! !"+! 514?)&9=/*"> 0140 014? 4103 0160 90103 0103 014? 01@2 4163-A, 014@ 0146 41?7 0162 9010? 0102 0146 01?0 21@2/& 0140 0146 41?@ 0162 9010? 0103 0143 01@7 4176./>/B" 0140 014? 41@0 0168 90103 0104 014? 01@8 2106CD& 0144 0145 414@ 0140 9010? 0104 0145 01?6 2124E<)9)<*'< !"#$ 0147 4152 0146 *!"!' 0100 !"%! 01?7 5143E<) 0167 014@ 4102 0164 9010? 010? 014? 01?0 21@6#,*%, 0142 0148 4125 0146 9010? 0102 0148 01?5 51?0#,&* !"#$ 0147 4152 0146 *!"!' 0105 0147 01?7 5138>+) 0142 0148 412@ 014? *!"!' 0102 0148 01?5 51??<>#,>/F 0144 0145 4148 014? 9010? 0105 0148 01?4 2177<>#,>/G !"#$ !"%! #",' 0146 9010? !"'& !"%! 01?8 513?+"#, !"#$ 0147 4153 0146 *!"!' 0102 0147 01?8 513?+HID9)<*'< !"#$ 0147 4157 !"## *!"!' 0106 !"%! !"+! 512?+HID 0142 0148 4127 0143 *!"!' 0102 0147 01?8 51?8&/-,> 0142 0148 4150 0146 *!"!' 0107 0147 01?8 51?6&JB9)<*'< !"#$ 0147 4158 !"## *!"!' 0104 !"%! !"+! $"%#
-./0123*456.738(((99(((((K/I+,)%(%&'*,L/-(I<(MNOP(K/I+,)%KFPQ(?0@0R(J#&%(IH<('"%/#,>/(*/I+,)%(!GSTU(">-(MNOP$1((O)<+/%(V<+(W"##W(!/>X+/(YKP@0(I/%I%/I$(">-(W%&'W(!%&'%/I(<V(ID/(D&*">#Z("%%/%%/-(-"I"$("+/(%D<H>1+">E KP9M[\ KP9*M[\ G"-./+(V&## G"-./+(#,I/ \[U9S;] SQ^G6 SQ:G6O/*_`O O/*_`O(GSTU
N(#/L/+(GSTU N(#/L/+(+/)"## OaK(Q">E PTOSF(K PTOSF OI">V<+-
KTPT`Q("-b KTPT`Q(DI/+ KTPT`Q(+">E OT_NFFPT[(?1@
GSTU(@3" MNOPPTQJ \Q \+-<) US[D GTHP9T GE"+%
49
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. /012 /011 /034 /034 /035 /064 /074 !"#$ !"%&)&+89*": /07; /035 !"&' /037 /03< /06/ /077 /036 /06<=>? /057 /016 /035 /033 /033 /063 /073 /031 /066@A /015 /011 /034 /031 /031 /065 /075 /031 /063BC& /035 /016 /034 /035 /031 /063 /071 /036 /06<A?D /05/ /011 /034 /034 /035 /065 /075 /031 /066A.)+).*'. /012 /011 /034 /034 /035 /064 /075 /035 !"%&A.) /06/ /01/ /031 /037 /037 /06< /077 /033 /067#?*%? /01< /013 /035 /031 /033 /061 /071 /031 /066#?& /011 /013 /034 /035 /031 /061 /071 /033 /066.:#?:9E /012 /013 /035 /035 /031 /061 /071 /033 /067.:#?:9F !"$! !"&( /03; !"#) !"#$ !"%* !"') !"#$ !"%&GHDC+).*'. /051 /011 /034 /034 /035 /064 /074 /035 !"%&GHDC /01< /013 /034 /035 /031 /061 /075 /031 /063%I& /063 /01< /031 /033 /036 /072 /07< /062 /07;&9=?: /057 /011 /034 /035 /031 /065 /075 /035 /063&JJ%"#" /034 /013 /034 /031 /033 /061 /071 /031 /063&JK+).*'. /011 /011 /034 /034 !"#$ /064 /074 /031 /063!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+C9"L9#=+).*'. /034 /03; 30;5 /031 /0/6 /0/1 /016 /0<; 102;)&+89*": /03< /037 3067 /066 /0/6 /0/6 /034 /0<7 10/6=>? /031 /034 3042 /035 !"!# !"+' /01/ /0<3 107;@A /035 /034 3045 /031 /0/6 /0/6 /017 /0<5 10;1BC& /033 /031 3017 /037 /0/6 /0/4 /01/ /0<1 1042A?D /034 /03; 30;6 /031 /0/6 /0/5 /016 /0<4 102;A.)+).*'. /035 /034 3044 /031 /0/6 /0// /016 /0<4 102;A.) /03< /037 303/ /062 /0/6 /0/7 /035 /0<3 10<2#?*%? /031 /035 3055 /036 /0/6 /0/1 /01< /0<5 104/#?& /035 /03; 3045 /031 /0/6 /0/3 /017 /0<5 10;1.:#?:9E /035 /03; 30;7 /035 !"!# /0/2 /017 /0<1 1051.:#?:9F !"#* !"&! &"!& !"#) !"!# /07/ !"&& !"+* ("+)GHDC+).*'. /035 /03; 3044 /031 /0/6 /0/6 /016 /0<4 50/5GHDC /031 /034 304< /033 /0/6 /0/1 /017 /0<5 10;5%I& /06; /062 6023 /06; /0/6 /0/6 /031 /0<< 30;7&9=?: /034 /03; 30;1 /035 /0/6 /0/; /016 /0<4 1023&JJ%"#" /031 /035 3051 /036 /0/6 /0/6 /01< /0<5 1043&JK+).*'. /035 /034 3041 /033 /0/6 /0/6 /016 /0<4 50</
,-./012345678-(((((++(((((M9DG?)%(%&'*?N9=(D.(OPQR(M9DG?)%MERS(7/</T(J#&%(DH.('"%9#?:9(*9DG?)%(!FUVW(":=(OPQR$0((Q).G9%(I.G(X"##X(!9:YG9(ZMR</(D9%D%9D$(":=(X%&'X(!%&'%9D(.I(DC9(C&*":#[("%%9%%9=(="D"$("G9(%C.H:0G":A MR+O\] MR+*O\] F"=^9G(I&## F"=^9G(#?D9 ]\W+U-_ US`F3 US,F3Q9*abQ Q9*abQ(FUVW
P(#9N9G(FUVW P(#9N9G(G9)"## QcM(S":A RVQUE(M RVQUE QD":I.G=
MVRVbS("=d MVRVbS(CD9G MVRVbS(G":A QVaPEERV\(70<
FUVW(<6" OPQRRVSJ ]S ]G=.) WU\C FVHR+V FA"G%
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0123 0124 0125 0126 0122 0132 0172 0126 0120)*&89:8);*'; 015< 0150 !"#$ 0124 012< 013< !"%& 012< 0127)&8=/*"> 017? 0122 !"#$ 012@ 0120 013@ 0177 0127 013?-)& 015@ 0150 !"#$ 012< 0126 0136 0172 012< 012@-A, 0122 0126 0123 0123 0127 0137 017? 0123 0136BC& 0122 0124 0125 0126 0122 0132 0172 0125 013<D;)8);*'; 0157 0150 !"#$ 012< 0126 0136 !"%& 012< 012@D;) 01@4 0123 012? 012@ 0134 01?< 017@ 012? 0135;>#,>/E 0154 0150 !"#$ 012< 0126 013< 0172 012< 012@;>#,>/F !"$' !"&' !"#$ !"&! !"#( !")( !"%& !"&! !"#*+GHC8);*'; 0124 0124 0125 0124 012< 013< !"%& 012< 012@%I& 0130 012< 0122 0122 0123 0137 0173 012? 0132&/-,> 015@ 0150 0125 012< 0126 0136 0172 012< 012@&J'8);*'; 0150 0150 !"#$ 0124 012< 013< !"%& 012< 012@&JK8>>#* 0123 0124 0125 0125 0122 0132 0173 0126 0134&JK 0122 0124 0125 0126 0125 0135 0172 0126 0120!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0126 0124 2162 0132 !"!' 0100 0150 0174 6120)*&89:8);*'; 0124 015@ 2140 013< !"!' 0100 0157 !"*% 6145)&8=/*"> 0123 0125 213@ 0130 0100 0100 0125 017@ 5123-)& 0124 015@ 2140 0136 !"!' 0100 015@ 01?0 6167-A, 0123 0126 2122 0137 0100 0100 0126 0173 51<?BC& 0126 0124 2167 0132 !"!' 0100 0150 017< 612<D;)8);*'; 012< 0150 21<3 0135 !"!' 0100 015@ 01?0 6164D;) 012@ 012? 21@4 01?5 0100 0100 012? 017@ 51?6;>#,>/E 0124 0157 2146 013< !"!' 0100 0157 01?0 6152;>#,>/F !"&! !"&* &"'% !")( !"!' 0100 0153 !"*% 61<6+GHC8);*'; 0124 015@ 21<4 0136 !"!' 0100 0150 01?@ $"($%I& 0123 0122 21?2 0137 0100 0100 0125 0173 61@<&/-,> 012< 015@ 21<5 0136 !"!' !"!' 015@ 01?0 6155&J'8);*'; 012< 0150 21<< 0136 !"!' 0100 015@ 01?@ 614?&JK8>>#* 0126 0124 216@ 0133 !"!' 0100 0124 017< 61?4&JK 012< 0150 21<0 0132 !"!' 0100 0150 0174 613<
+,-./012345,/016((((88(((((L/H+,)%(%&'*,M/-(H;(NOPQ(L/H+,)%LEQR(70@0S(J#&%(HG;('"%/#,>/(*/H+,)%(!FTUV(">-(NOPQ$1((P);+/%(I;+(W"##W(!/>X+/(YLQ@0(H/%H%/H$(">-(W%&'W(!%&'%/H(;I(HC/(C&*">#Z("%%/%%/-(-"H"$("+/(%C;G>1+">D LQ8N[\ LQ8*N[\ F"-./+(I&## F"-./+(#,H/ \[V8T:] TR^F3 TR9F3P/*_`P P/*_`P(FTUV
O(#/M/+(FTUV O(#/M/+(+/)"## PaL(R">D QUPTE(L QUPTE PH">I;+-
LUQU`R("-b LUQU`R(CH/+ LUQU`R(+">D PU_OEEQU[(71@
FTUV(@?" NOPQQURJ \R \+-;) VT[C FUGQ8U FD"+%
50
R
E
F
A
A
L
T
O
C
M
U
C
U
-B
O
JA
R
C
U
-Z
E
M
A
N
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
B
B
N
-C
C
M
U
-H
E
A
-C
JH
U
-C
R
W
T
H
-C
U
P
V
-C
REF ? .03? .02? .03? .01? .03? .02? .05? .02? .06? .03? .05? .03?
AALTO .93? ? .54? .54? .23? .36 .58? .56? .65? .69? .64? .67? .62?
CMU .94? .30? ? .47 .14? .22? .52? .41 .50? .57? .45? .44 .38
CU-BOJAR .94? .26? .38 ? .10? .22? .61? .47? .46 .55? .42 .49? .44
CU-ZEMAN .98? .58? .73? .77? ? .55? .79? .71? .84? .80? .77? .79? .75?
ONLINEA .94? .41 .61? .57? .23? ? .68? .63? .71? .71? .63? .54? .61?
ONLINEB .93? .30? .31? .26? .10? .17? ? .32? .35 .31 .22? .29? .38
UEDIN .91? .27? .35 .34? .11? .18? .47? ? .54? .50? .35 .29 .35
BBN-C .95? .21? .22? .36 .06? .17? .38 .26? ? .32 .24? .31? .26?
CMU-HEA-C .90? .17? .19? .23? .09? .18? .32 .27? .34 ? .31? .31? .30?
JHU-C .93? .19? .30? .35 .09? .24? .50? .34 .47? .45? ? .41? .36
RWTH-C .91? .16? .35 .29? .12? .27? .41? .37 .42? .42? .23? ? .24?
UPV-C .94? .24? .40 .36 .09? .28? .39 .32 .46? .47? .33 .36? ?
> others .93 .26 .37 .38 .11 .24 .47 .40 .49 .49 .38 .41 .40
>= others .97 .42 .56 .55 .25 .39 .67 .62 .70 .70 .61 .65 .62
Table 21: Sentence-level ranking for the WMT10 Czech-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
51
R
E
F
A
A
L
T
O
C
M
U
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
H
U
IC
O
N
G
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
U
E
D
IN
U
M
D
U
P
P
S
A
L
A
U
U
-M
S
B
B
N
-C
C
M
U
-H
E
A
-C
C
M
U
-H
Y
P
O
-C
JH
U
-C
K
O
C
-C
R
W
T
H
-C
U
P
V
-C
REF ? .00? .02? .00? .07? .04? .03? .00? .06? .04? .00? .02? .07? .07? .07? .02? .09? .03? .03? .10? .04? .04? .03? .02? .07? .06?
AALTO 1.00? ? .43 .39 .48 .60? .38 .41 .74? .18? .42 .57? .50? .63? .55? .68? .79? .42 .33 .71? .61? .66? .54 .51? .66? .56?
CMU .95? .34 ? .19? .45 .52? .38 .50 .63? .17? .51? .55? .56? .66? .55? .60? .56? .30 .40 .62? .64? .49? .58? .46 .64? .46?
CU-ZEMAN 1.00? .44 .64? ? .43 .72? .31 .45? .69? .36 .55 .62? .75? .75? .78? .75? .75? .48? .56? .79? .82? .72? .68? .63? .67? .84?
DFKI .92? .29 .33 .35 ? .37 .40 .34 .59 .08? .42 .50 .49 .64? .35 .44 .44 .50 .41 .70? .61? .57 .46 .47 .62? .44
FBK .93? .26? .23? .17? .49 ? .12? .30 .52? .08? .20? .45? .41 .62? .44 .44 .48? .18? .25? .53? .47 .38 .38 .22? .41 .51?
HUICONG .92? .34 .39 .37 .38 .71? ? .53? .67? .18? .51? .47 .60? .65? .49? .55? .78? .35 .41 .56? .77? .74? .58? .41 .65? .57?
JHU .92? .35 .30 .17? .52 .45 .25? ? .58? .16? .43 .38 .57? .60? .54? .60? .70? .29 .25 .65? .75? .56? .62? .49? .66? .48?
KIT .90? .14? .16? .14? .35 .28? .19? .16? ? .03? .29? .20? .35 .53? .21? .24? .30 .20? .22? .44 .29 .38 .35 .24 .40 .24?
KOC .95? .66? .71? .51 .75? .80? .58? .68? .93? ? .75? .87? .72? .74? .74? .81? .81? .78? .66? .89? .85? .80? .80? .72? .91? .73?
LIMSI .99? .26 .24? .32 .45 .61? .25? .38 .50? .10? ? .50? .55? .69? .52? .57? .57? .29? .22? .60? .52? .42 .47? .37 .60? .56?
LIU .87? .17? .20? .14? .34 .22? .31 .38 .66? .04? .27? ? .51? .53? .52? .53? .51 .20? .33 .64? .59? .48? .48 .51 .37 .53?
ONLINEA .90? .25? .29? .18? .34 .43 .23? .28? .49 .08? .32? .30? ? .44 .38 .40 .42 .32? .35? .39 .47 .51 .27? .35 .43 .40
ONLINEB .76? .22? .24? .14? .27? .27? .25? .25? .32? .22? .21? .28? .32 ? .27? .21? .30? .23? .15? .41 .31 .40 .23? .16? .42 .29
RWTH .89? .22? .23? .13? .49 .35 .29? .21? .62? .15? .32? .29? .46 .57? ? .39 .49 .25 .38 .41 .27 .34 .36 .27 .48? .22?
UEDIN .91? .15? .20? .12? .49 .35 .24? .22? .49? .04? .22? .30? .46 .62? .43 ? .39 .11? .15? .45 .33 .40 .45 .33 .34 .33
UMD .91? .12? .23? .06? .35 .29? .11? .16? .47 .14? .23? .35 .40 .55? .36 .47 ? .16? .17? .44 .29? .27 .37 .26 .27 .24?
UPPSALA .94? .30 .41 .23? .35 .53? .26 .37 .66? .03? .54? .71? .57? .65? .45 .72? .67? ? .25 .59? .69? .49? .63? .33 .60? .64?
UU-MS .83? .28 .42 .24? .41 .49? .28 .42 .68? .10? .55? .48 .55? .63? .49 .56? .60? .32 ? .52? .58? .61? .64? .46? .64? .50?
BBN-C .90? .15? .16? .10? .22? .17? .22? .18? .41 .06? .16? .21? .35 .45 .30 .26 .34 .13? .20? ? .42? .14? .27 .11? .25 .21?
CMU-HEA-C .83? .20? .18? .07? .29? .32 .06? .10? .49 .05? .26? .21? .41 .33 .37 .43 .58? .10? .14? .18? ? .33 .32 .11? .34 .24?
CMU-HYPO-C .96? .24? .20? .07? .37 .33 .12? .21? .40 .10? .41 .26? .40 .54 .25 .37 .44 .13? .17? .49? .31 ? .34 .23? .51? .45
JHU-C .97? .33 .22? .18? .31 .30 .27? .18? .33 .12? .19? .33 .59? .60? .39 .32 .30 .19? .20? .44 .29 .34 ? .21? .36 .23
KOC-C .93? .11? .31 .17? .41 .50? .25 .27? .44 .11? .42 .36 .47 .68? .43 .41 .40 .33 .18? .59? .57? .46? .47? ? .52? .43
RWTH-C .87? .20? .10? .21? .25? .27 .15? .23? .24 .02? .20? .30 .34 .47 .27? .34 .36 .14? .20? .33 .26 .21? .24 .20? ? .17?
UPV-C .93? .14? .20? .10? .42 .29? .25? .25? .57? .20? .22? .33? .39 .45 .47? .40 .50? .24? .28? .44? .42? .27 .34 .28 .56? ?
> others .92 .25 .28 .18 .39 .41 .25 .30 .52 .12 .34 .39 .47 .57 .42 .46 .51 .27 .28 .52 .49 .45 .44 .34 .50 .42
>= others .96 .46 .49 .35 .53 .62 .45 .51 .71 .24 .54 .58 .63 .72 .63 .66 .70 .50 .51 .75 .73 .68 .67 .59 .74 .64
Table 22: Sentence-level ranking for the WMT10 German-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
R
E
F
C
A
M
B
R
ID
G
E
C
O
L
U
M
B
IA
C
U
-Z
E
M
A
N
D
F
K
I
H
U
IC
O
N
G
JH
U
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
U
P
C
B
B
N
-C
C
M
U
-H
E
A
-C
JH
U
-C
U
P
V
-C
REF ? .05? .01? .02? .03? .03? .01? .02? .04? .03? .04? .03? .07? .05? .04?
CAMBRIDGE .90? ? .24? .11? .35? .26? .43 .35 .50? .45? .33? .40 .46 .28? .41
COLUMBIA .97? .61? ? .25? .47 .44 .61? .53? .62? .59? .48? .59? .57? .45? .57?
CU-ZEMAN .92? .73? .59? ? .62? .66? .71? .65? .75? .79? .58? .75? .78? .71? .72?
DFKI .95? .50? .41 .21? ? .46 .56? .52? .65? .62? .47 .52? .56? .52? .60?
HUICONG .93? .57? .34 .21? .36 ? .47? .43 .67? .58? .40 .51? .62? .46? .52?
JHU .94? .39 .22? .16? .30? .32? ? .41 .52? .47? .37 .41 .33? .28 .35
ONLINEA .92? .45 .35? .24? .34? .41 .41 ? .60? .58? .38 .55? .46 .36 .57?
ONLINEB .87? .34? .24? .15? .21? .19? .33? .25? ? .34? .26? .34? .37? .24? .40
UEDIN .94? .33? .26? .12? .24? .22? .25? .25? .50? ? .25? .28? .32? .25? .26
UPC .89? .45? .36? .23? .39 .37 .42 .48 .62? .57? ? .54? .51? .50? .53?
BBN-C .91? .33 .25? .11? .32? .30? .34 .31? .51? .41? .30? ? .36 .26? .31
CMU-HEA-C .89? .37 .20? .10? .29? .23? .23? .35 .50? .44? .31? .34 ? .23? .31
JHU-C .89? .39? .31? .17? .37? .33? .38 .42 .63? .47? .31? .42? .42? ? .37?
UPV-C .91? .35 .30? .16? .29? .26? .32 .28? .44 .35 .27? .27 .30 .24? ?
> others .92 .42 .29 .16 .33 .32 .39 .37 .54 .48 .34 .42 .44 .35 .43
>= others .97 .62 .45 .29 .46 .50 .61 .52 .68 .68 .51 .64 .65 .58 .66
Table 23: Sentence-level ranking for the WMT10 Spanish-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
52
R
E
F
C
A
M
B
R
ID
G
E
C
M
U
-S
T
A
T
X
F
E
R
C
U
-Z
E
M
A
N
D
F
K
I
G
E
N
E
V
A
H
U
IC
O
N
G
JH
U
L
IG
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
B
B
N
-C
C
M
U
-H
E
A
-C
C
M
U
-H
Y
P
O
-C
D
C
U
-C
JH
U
-C
L
IU
M
-C
R
W
T
H
-C
U
P
V
-C
REF ? .02? .00? .00? .00? .00? .05? .02? .00? .00? .00? .02? .06? .02? .04? .02? .04? .03? .02? .05? .05? .04? .05? .06? .02?
CAMBRIDGE .82? ? .42 .16? .12? .35 .31 .45 .21? .47 .29 .38 .28? .54 .43 .33 .38 .28 .39 .45? .24 .25 .34 .54? .37
CMU-STATXFER .91? .50 ? .17? .41 .17? .28 .44 .36 .48? .56? .57? .47 .56? .70? .49 .50 .47 .61? .68? .55? .50 .42 .52? .51?
CU-ZEMAN 1.00? .74? .71? ? .74? .46 .67? .73? .73? .74? .75? .76? .75? .89? .78? .66? .83? .74? .87? .73? .80? .83? .77? .95? .82?
DFKI 1.00? .77? .48 .17? ? .27? .49 .52 .48 .64? .69? .67? .47 .62? .53 .47 .64? .60? .73? .72? .79? .58? .66? .73? .74?
GENEVA .98? .58 .70? .44 .59? ? .55? .67? .70? .70? .77? .73? .63? .81? .81? .69? .77? .73? .62? .66? .75? .60? .73? .88? .67?
HUICONG .89? .53 .34 .13? .34 .30? ? .41 .36 .43 .70? .56? .57 .59? .56? .43 .55? .45 .51? .64? .48 .49 .49 .53? .57?
JHU .88? .36 .38 .11? .34 .25? .35 ? .33? .46 .49? .48 .40 .50 .40 .34 .36 .39 .33 .59? .54? .41 .42 .40 .41
LIG .98? .65? .34 .18? .44 .26? .39 .56? ? .60? .55? .51? .45 .54? .53 .39 .38 .52? .54? .53? .51? .53? .55 .51 .58?
LIMSI .98? .40 .24? .23? .23? .15? .29 .38 .25? ? .28 .38 .27? .64? .35 .30 .41 .27 .33 .49 .45 .37 .28 .45 .39
LIUM .90? .40 .19? .12? .30? .11? .11? .26? .15? .36 ? .36 .25? .37 .39 .26 .29 .24 .34 .49? .34 .33 .34 .31 .38
NRC .93? .31 .06? .15? .29? .23? .20? .32 .16? .38 .36 ? .23? .53 .36 .24? .31 .44 .37 .47? .45? .29 .39 .38 .42
ONLINEA .92? .60? .47 .15? .44 .22? .32 .46 .34 .57? .52? .60? ? .52? .34 .44 .57? .56 .51 .51 .64? .46 .51 .41 .60
ONLINEB .85? .35 .32? .09? .33? .10? .29? .31 .25? .17? .40 .34 .24? ? .38 .32? .28 .39 .30 .42 .37 .41 .35 .32 .22?
RALI .90? .31 .19? .10? .38 .10? .17? .47 .35 .38 .33 .38 .48 .48 ? .29? .31 .29 .38 .40 .38 .34 .31 .57? .21?
RWTH .93? .43 .33 .12? .47 .26? .39 .40 .47 .35 .45 .49? .44 .53? .54? ? .44? .42 .48 .51? .54? .48? .49 .50? .26
UEDIN .92? .42 .32 .10? .22? .10? .28? .30 .42 .30 .55 .36 .23? .43 .33 .20? ? .41 .24 .52? .46 .25 .22 .27 .37
BBN-C .92? .49 .33 .24? .28? .18? .40 .39 .28? .45 .27 .27 .36 .39 .35 .35 .31 ? .26 .45? .43 .26 .58? .36 .28
CMU-HEA-C .90? .41 .21? .06? .23? .29? .28? .27 .22? .39 .40 .22 .39 .43 .29 .30 .40 .28 ? .43 .28 .15? .25 .26 .16
CMU-HYPO-C .84? .18? .20? .14? .20? .22? .21? .19? .16? .31 .22? .21? .36 .38 .34 .27? .22? .16? .24 ? .36 .23 .10? .33 .24
DCU-C .92? .27 .24? .12? .17? .23? .30 .29? .24? .32 .43 .22? .28? .41 .23 .27? .28 .22 .23 .25 ? .23 .23 .24 .17
JHU-C .88? .47 .26 .10? .33? .24? .36 .34 .24? .41 .39 .40 .42 .39 .34 .25? .42 .28 .37? .38 .39 ? .37 .32 .38?
LIUM-C .90? .48 .42 .13? .25? .20? .33 .50 .30 .44 .37 .34 .37 .52 .43 .34 .33 .22? .34 .56? .33 .43 ? .49? .44
RWTH-C .89? .22? .19? .03? .23? .12? .19? .23 .27 .30 .36 .19 .47 .54 .26? .16? .27 .19 .26 .28 .16 .22 .16? ? .22
UPV-C .89? .27 .15? .10? .16? .29? .30? .31 .25? .36 .42 .24 .32 .64? .46? .34 .27 .44 .33 .44 .23 .17? .31 .24 ?
> others .91 .43 .32 .14 .31 .21 .31 .39 .31 .42 .44 .40 .38 .52 .43 .33 .40 .37 .40 .49 .43 .38 .4 .44 .39
>= others .97 .64 .51 .24 .40 .31 .50 .59 .50 .63 .68 .65 .51 .68 .65 .55 .66 .63 .69 .75 .71 .64 .62 .74 .67
Table 24: Sentence-level ranking for the WMT10 French-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
53
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 22?64,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Findings of the 2011 Workshop on Statistical Machine Translation
Chris Callison-Burch
Center for Language and Speech Processing
Johns Hopkins University
Philipp Koehn
School of Informatics
University of Edinburgh
Christof Monz
Informatics Institute
University of Amsterdam
Omar F. Zaidan
Center for Language and Speech Processing
Johns Hopkins University
Abstract
This paper presents the results of the WMT11
shared tasks, which included a translation
task, a system combination task, and a task for
machine translation evaluation metrics. We
conducted a large-scale manual evaluation of
148 machine translation systems and 41 sys-
tem combination entries. We used the rank-
ing of these systems to measure how strongly
automatic metrics correlate with human judg-
ments of translation quality for 21 evaluation
metrics. This year featured a Haitian Creole
to English task translating SMS messages sent
to an emergency response service in the af-
termath of the Haitian earthquake. We also
conducted a pilot ?tunable metrics? task to test
whether optimizing a fixed system to differ-
ent metrics would result in perceptibly differ-
ent translation quality.
1 Introduction
This paper presents the results of the shared tasks
of the Workshop on statistical Machine Translation
(WMT), which was held at EMNLP 2011. This
workshop builds on five previous WMT workshops
(Koehn and Monz, 2006; Callison-Burch et al,
2007; Callison-Burch et al, 2008; Callison-Burch
et al, 2009; Callison-Burch et al, 2010). The work-
shops feature three shared tasks: a translation task
between English and other languages, a task to com-
bine the output of multiple machine translation sys-
tems, and a task to predict human judgments of
translation quality using automatic evaluation met-
rics. The performance for each of these shared tasks
is determined through a comprehensive human eval-
uation. There were a two additions to this year?s
workshop that were not part of previous workshops:
? Haitian Creole featured task ? In addition to
translation between European language pairs,
we featured a new translation task: translating
Haitian Creole SMS messages that were sent
to an emergency response hotline in the im-
mediate aftermath of the 2010 Haitian earth-
quake. The goal of this task is to encourage re-
searchers to focus on challenges that may arise
in future humanitarian crises. We invited Will
Lewis, Rob Munro and Stephan Vogel to pub-
lish a paper about their experience developing
translation technology in response to the crisis
(Lewis et al, 2011). They provided the data
used in the Haitian Creole featured translation
task. We hope that the introduction of this new
dataset will provide a testbed for dealing with
low resource languages and the informal lan-
guage usage found in SMS messages.
? Tunable metric shared task ? We conducted
a pilot of a new shared task to use evaluation
metrics to tune the parameters of a machine
translation system. Although previous work-
shops have shown evaluation metrics other than
BLEU are more strongly correlated with human
judgments when ranking outputs from multiple
systems, BLEU remains widely used by system
developers to optimize their system parameters.
We challenged metric developers to tune the
parameters of a fixed system, to see if their met-
rics would lead to perceptibly better translation
quality for the system?s resulting output.
22
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dissem-
inate common test sets and public training data with
published performance numbers, and to refine eval-
uation methodologies for machine translation. As
with previous workshops, all of the data, transla-
tions, and collected human judgments are publicly
available.1 We hope these datasets form a valuable
resource for research into statistical machine transla-
tion, system combination, and automatic evaluation
of translation quality.
2 Overview of the Shared Translation and
System Combination Tasks
The recurring task of the workshop examines trans-
lation between English and four other languages:
German, Spanish, French, and Czech. We created a
test set for each language pair by translating newspa-
per articles. We additionally provided training data
and two baseline systems.
2.1 Test data
The test data for this year?s task was created by
hiring people to translate news articles that were
drawn from a variety of sources from early Decem-
ber 2010. A total of 110 articles were selected, in
roughly equal amounts from a variety of Czech, En-
glish, French, German, and Spanish news sites:2
Czech: aktualne.cz (4), Novinky.cz (7), iH-
Ned.cz (4), iDNES.cz (4)
French: Canoe (5), Le Devoir (5), Le Monde (5),
Les Echos (5), Liberation (5)
Spanish: ABC.es (6), Cinco Dias (6), El Period-
ico (6), Milenio (6), Noroeste (7)
English: Economist (4), Los Angeles Times (6),
New York Times (4), Washington Post (4)
German: FAZ (3), Frankfurter Rundschau (2), Fi-
nancial Times Deutschland (3), Der Spie-
gel (5), Su?ddeutsche Zeitung (3)
The translations were created by the professional
translation agency CEET.3 All of the translations
1http://statmt.org/wmt11/results.html
2For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
3http://www.ceet.eu/
were done directly, and not via an intermediate lan-
guage.
Although the translations were done profession-
ally, in some cases errors still cropped up. For in-
stance, in parts of the English-French translations,
some of the English source remains in the French
reference as if the translator forgot to delete it.
2.2 Training data
As in past years we provided parallel corpora to train
translation models, monolingual corpora to train lan-
guage models, and development sets to tune system
parameters. Some statistics about the training mate-
rials are given in Figure 1.
2.3 Baseline systems
To lower the barrier of entry for newcomers to
the field, we provided two open source toolkits for
phrase-based and parsing-based statistical machine
translation (Koehn et al, 2007; Li et al, 2010).
2.4 Submitted systems
We received submissions from 56 groups across 37
institutions, as listed in Tables 1, 2 and 3. We also
included two commercial off-the-shelf MT systems,
two online statistical MT systems, and five online
rule-based MT systems. (Not all systems supported
all language pairs.) We note that these nine compa-
nies did not submit entries themselves, and are there-
fore anonymized in this paper. Rather, their entries
were created by translating the test data via their web
interfaces.4 The data used to construct these systems
is not subject to the same constraints as the shared
task participants. It is possible that part of the refer-
ence translations that were taken from online news
sites could have been included in the online systems?
models, for instance. We therefore categorize all
commercial systems as unconstrained when evalu-
ating the results.
2.5 System combination
In total, we had 148 primary system entries (includ-
ing the 46 entries crawled from online sources), and
60 contrastive entries. These were made available to
4We would like to thank Ondr?ej Bojar for harvesting the
commercial entries (2), Christian Federmann for the statistical
MT entries (14), and Herve? Saint-Amand for the rule-based MT
entries (30)!
23
Europarl Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,786,594 1,825,077 1,739,154 462,351
Words 51,551,370 49,411,045 54,568,499 50,551,047 45,607,269 47,978,832 10,573,983 12,296,772
Distinct words 171,174 113,655 137,034 114,487 362,563 111,934 152,788 56,095
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 132,571 115,562 136,227 122,754
Words 3,739,293 3,285,305 3,290,280 2,866,929 3,401,766 3,309,619 2,658,688 2,951,357
Distinct words 73,906 53,699 59,911 50,323 120,397 53,921 130,685 50,457
United Nations Training Corpus
Spanish? English French? English
Sentences 10,662,993 12,317,600
Words 348,587,865 304,724,768 393,499,429 344,026,111
Distinct words 578,599 564,489 621,721 729,233
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 7,227,409
Words 72,993,427 84,856,749
Distinct words 1,088,642 522,770
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,032,006 1,942,761 2,002,266 1,985,560 479,636
Words 54,720,731 55,105,358 57,860,307 48,648,697 10,770,230
Distinct words 119,315 176,896 141,742 376,128 154,129
News Language Model Data
English Spanish French German Czech
Sentence 30,888,595 3,416,184 11,767,048 17,474,133 12,333,268
Words 777,425,517 107,088,554 302,161,808 289,171,939 216,692,489
Distinct words 2,020,549 595,681 1,250,259 3,091,700 2,068,056
News Test Set
English Spanish French German Czech
Sentences 3003
Words 75,762 79,710 85,999 73,729 65,427
Distinct words 10,088 11,989 11,584 14,345 16,922
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of
distinct words (case-insensitive) is based on the provided tokenizer.
24
ID Participant
ALACANT University of Alicante (Sa?nchez-Cartagena et al, 2011)
CEU-UPV CEU University Cardenal Herrera
& Polytechnic University of Valencia (Zamora-Martinez and Castro-Bleda, 2011)
CMU-DENKOWSKI Carnegie Mellon University - Denkowski (Denkowski and Lavie, 2011b)
CMU-DYER Carnegie Mellon University - Dyer (Dyer et al, 2011)
CMU-HANNEMAN Carnegie Mellon University - Hanneman (Hanneman and Lavie, 2011)
COPENHAGEN Copenhagen Business School
CST Centre for Language Technology @ Copenhagen University (Rish?j and S?gaard, 2011)
CU-BOJAR Charles University - Bojar (Marec?ek et al, 2011)
CU-MARECEK Charles University - Marec?ek (Marec?ek et al, 2011)
CU-POPEL Charles University - Popel (Popel et al, 2011)
CU-TAMCHYNA Charles University - Tamchyna (Bojar and Tamchyna, 2011)
CU-ZEMAN Charles University - Zeman (Zeman, 2011)
DFKI-FEDERMANN Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz - Federmann
(Federmann and Hunsicker, 2011)
DFKI-XU Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz - Xu (Xu et al, 2011b)
HYDERABAD IIIT-Hyderabad
ILLC-UVA Institute for Logic, Language and Computation @ University of Amsterdam
(Khalilov and Sima?an, 2011)
JHU Johns Hopkins University (Weese et al, 2011)
KIT Karlsruhe Institute of Technology (Herrmann et al, 2011)
KOC Koc University (Bicici and Yuret, 2011)
LATL-GENEVA Language Technology Laboratory @ University of Geneva (Wehrli et al, 2009)
LIA-LIG Laboratoire Informatique d?Avignon @ The University of Avignon
& Laboratoire d?Informatique de Grenoble @ University of Grenoble (Potet et al, 2011)
LIMSI LIMSI (Allauzen et al, 2011)
LINGUATEC Linguatec Language Technologies (Aleksic and Thurmair, 2011)
LIU Linko?ping University (Holmqvist et al, 2011)
LIUM University of Le Mans (Schwenk et al, 2011)
PROMT ProMT
RWTH-FREITAG RWTH Aachen - Freitag (Huck et al, 2011)
RWTH-HUCK RWTH Aachen - Huck (Huck et al, 2011)
RWTH-WUEBKER RWTH Aachen - Wu?bker (Huck et al, 2011)
SYSTRAN SYSTRAN
UEDIN University of Edinburgh (Koehn et al, 2007)
UFAL-UM Charles University and University of Malta (Corb??-Bellot et al, 2005)
UOW University of Wolverhampton (Aziz et al, 2011)
UPM Technical University of Madrid (Lo?pez-Luden?a and San-Segundo, 2011)
UPPSALA Uppsala University (Koehn et al, 2007)
UPPSALA-FBK Uppsala University
& Fondazione Bruno Kessler (Hardmeier et al, 2011)
ONLINE-[A,B] two online statistical machine translation systems
RBMT-[1?5] five online rule-based machine translation systems
COMMERCIAL-[1,2] two commercial machine translation systems
Table 1: Participants in the shared translation task (European language pairs; individual system track). Not all teams
participated in all language pairs. The translations from commercial and online systems were crawled by us, not
submitted by the respective companies, and are therefore anonymized.
25
ID Participant
BBN-COMBO Raytheon BBN Technologies (Rosti et al, 2011)
CMU-HEAFIELD-COMBO Carnegie Mellon University (Heafield and Lavie, 2011)
JHU-COMBO Johns Hopkins University (Xu et al, 2011a)
KOC-COMBO Koc University (Bicici and Yuret, 2011)
LIUM-COMBO University of Le Mans (Barrault, 2011)
QUAERO-COMBO Quaero Project? (Freitag et al, 2011)
RWTH-LEUSCH-COMBO RWTH Aachen (Leusch et al, 2011)
UOW-COMBO University of Wolverhampton (Specia et al, 2010)
UPV-PRHLT-COMBO Polytechnic University of Valencia (Gonza?lez-Rubio and Casacuberta, 2011)
UZH-COMBO University of Zurich (Sennrich, 2011)
Table 2: Participants in the shared system combination task. Not all teams participated in all language pairs.
? The Quaero Project entry combined outputs they received directly from LIMSI, KIT, SYSTRAN, and RWTH.
participants in the system combination shared task.
Continuing our practice from last year?s workshop,
we separated the test set into a tuning set and a final
held-out test set for system combinations. The tun-
ing portion was distributed to system combination
participants along with reference translations, to aid
them set any system parameters.
In the European language pairs, the tuning set
consisted of 1,003 segments taken from 37 docu-
ments, whereas the test set consisted of 2,000 seg-
ments taken from 73 documents. In the Haitian Cre-
ole task, the split was 674 segments for tuning and
600 for testing.
Table 2 lists the 10 participants in the system com-
bination task.
3 Featured Translation Task
The featured translation task of WMT11 was to
translate Haitian Creole SMS messages into En-
glish. These text messages were sent by people in
Haiti in the aftermath of the January 2010 earth-
quake. In the wake of the earthquake, much of the
country?s conventional emergency response services
failed. Since cell phone towers remained stand-
ing after the earthquake, text messages were a vi-
able mode of communication. Munro (2010) de-
scribes how a text-message-based emergency report-
ing system was set up by a consortium of volunteer
organizations named ?Mission 4636? after a free
SMS short code telephone number that they estab-
lished. The SMS messages were routed to a system
for reporting trapped people and other emergencies.
Search and rescue teams within Haiti, including the
US Military, recognized the quantity and reliabil-
ity of actionable information in these messages and
used them to provide aid.
The majority of the SMS messages were writ-
ten in Haitian Creole, which was not spoken by
most of first responders deployed from overseas.
A distributed, online translation effort was estab-
lished, drawing volunteers from Haitian Creole- and
French-speaking communities around the world.
The volunteers not only translated messages, but
also categorized them and pinpointed them on a
map.5 Collaborating online, they employed their lo-
cal knowledge of locations, regional slang, abbre-
viations and spelling variants to process more than
40,000 messages in the first six weeks alone. First
responders indicated that this volunteer effort helped
to save hundreds of lives and helped direct the first
food and aid to tens of thousands. Secretary of State
Clinton described one success of the Mission 4636
program:?The technology community has set up in-
teractive maps to help us identify needs and target
resources. And on Monday, a seven-year-old girl
and two women were pulled from the rubble of a
collapsed supermarket by an American search-and-
rescue team after they sent a text message calling
for help.? Ushahidi@Tufts described another:?The
World Food Program delivered food to an informal
camp of 2500 people, having yet to receive food or
water, in Diquini to a location that 4636 had identi-
5A detailed map of Haiti was created by a crowdsourcing
effort in the aftermath of the earthquake (Lacey-Hall, 2011).
26
ID Participant
BM-I2R Barcelona Media
& Institute for Infocomm Research (Costa-jussa` and Banchs, 2011)
CMU-DENKOWSKI Carnegie Mellon University - Denkowski (Denkowski and Lavie, 2011b)
CMU-HEWAVITHARANA Carnegie Mellon University - Hewavitharana (Hewavitharana et al, 2011)
HYDERABAD IIIT-Hyderabad
JHU Johns Hopkins University (Weese et al, 2011)
KOC Koc University (Bicici and Yuret, 2011)
LIU Linko?ping University (Stymne, 2011)
UMD-EIDELMAN University of Maryland - Eidelman (Eidelman et al, 2011)
UMD-HU University of Maryland - Hu (Hu et al, 2011)
UPPSALA Uppsala University (Hardmeier et al, 2011)
Table 3: Participants in the featured translation task (Haitian Creole SMS into English; individual system track). Not
all teams participated in both the ?Clean? and ?Raw? tracks.
fied for them.?
In parallel with Rob Munro?s crowdsourcing
translation efforts, the Microsoft Translator team de-
veloped a Haitian Creole statistical machine transla-
tion engine from scratch in a compressed timeframe
(Lewis, 2010). Despite the impressive number
of translations completed by volunteers, machine
translation was viewed as a potentially useful tool
for higher volume applications or to provide trans-
lations of English medical documents into Haitian
Creole. The Microsoft Translator team quickly as-
sembled parallel data from a number of sources,
including Mission 4636 and from the archives of
Carnegie Mellon?s DIPLOMAT project (Frederking
et al, 1997). Through a series of rapid prototyp-
ing efforts, the team improved their system to deal
with non-standard orthography, reduced pronouns,
and SMS shorthand. They deployed a functional
translation system to relief workers in the field in
less than 5 days ? impressive even when measured
against previous rapid MT development efforts like
DARPA?s surprise language exercise (Oard, 2003;
Oard and Och, 2003).
We were inspired by the efforts of Rob Munro and
Will Lewis on translating Haitian Creole in the af-
termath of the disaster, so we worked with them to
create a featured task at WMT11. We thank them for
generously sharing the data they assembled in their
own efforts. We invited Rob Munro, Will Lewis,
and Stephan Vogel to speak at the workshop on the
topic of developing translation technology for future
crises, and they recorded their thoughts in an invited
publication (Lewis et al, 2011).
3.1 Haitian Creole Data
For the WMT11 featured translation task, we
anonymized the SMS Haitian Creole messages
along with the translations that the Mission 4636
volunteers created. Examples of these messages are
given in Table 4. The goal of anonymizing the SMS
data was so that it may be shared with researchers
who are developing translation and mapping tech-
nologies to support future emergency relief efforts
and social development. We ask that any researcher
working with these messages to be aware that they
are actual communications sent by people in need in
a time of crisis. Researchers who use this data are
asked to be cognizant of the following:
? Some messages may be distressing in content.
? The people who sent the messages (and who
are discussed in them) were victims of a natural
disaster and a humanitarian crisis. Please treat
the messages with the appropriate respect for
these individuals.
? The primary motivation for using this data
should be to understand how we can better re-
spond to future crises.
Participants who received the Haitian Creole data
for WMT11 were given anonymization guidelines
27
mwen se [FIRSTNAME] mwen gen twaset ki mouri mwen
mande nou ed pou nou edem map tan repons
I am [FIRSTNAME], I have three sisters who have died. I
ask help for us, I await your response.
Ki kote yap bay manje Where are they giving out food?
Eske lekol kolej marie anne kraze?mesi Was the College Marie Anne school destroyed? Thank you.
Nou pa ka anpeche moustik yo mo`de nou paske yo anpil. We can?t prevent the mosquitoes from biting because there
are so many.
tanpri ke`m ap kase mwen pa ka pran nouvel manmanm. Please heart is breaking because I have no news of my
mother.
4636:Opital Medesen san Fwontie` delmas 19 la fe`men.
Opital sen lwi gonzag nan delma 33 pran an chaj gratwit-
man tout moun ki malad ou blese
4636: The Doctors without Borders Hospital in Delmas 19
is closed. The Saint Louis Gonzaga hospital in Delmas 33
is taking in sick and wounded people for free
Mwen re?se?voua mesaj nou yo 5 sou 5 men mwen ta vle di
yon bagay kile` e koman nap kapab fe`m jwin e`d sa yo pou
moune b la kay mwen ki sinistwe? adre`s la se?
I received your message 5/5 but I would like to ask one
thing when and how will you be able to get the aid to me for
the people around my house who are victims of the earth-
quake? The address is
Sil vous plait map chehe [LASTNAME][FIRSTNAME].di
yo relem nan [PHONENUMBER].mwen se [LAST-
NAME] [FIRSTNAME]
I?m looking for [LASTNAME][FIRSTNAME]. Tell him
to call me at [PHONENUMBER] I am [LASTNAME]
[FIRSTNAME]
Bonswa mwen rele [FIRSTNAME] [LASTNAME] kay
mwen krase mwen pagin anyin poum mange ak fanmi-m
tampri di yon mo pou mwen fem jwen yon tante tou ak
mange. .mrete n
Hello my name is [FIRSTNAME] [LASTNAME]my house
fell down, I?ve had nothing to eat and I?m hungry. Please
help me find food. I live
Mwen viktim kay mwen kraze e`skem ka ale sendomeng
mwen gen paspo`
I?m a victim. My home has been destroyed. Am I allowed
to go to the Dominican Republic? I have a Passport.
KISAM DWE FE LEGEN REPLIK,ESKE MOUN SAINT
MARC AP JWENN REPLIK.
What should I do when there is an aftershock? Will the
people of Saint Marc have aftershocks?
MWEN SE YON JEN ETIDYAN AN ASYANS ENFO-
MATIK KI PASE ANPIL MIZE NAN TRANBLEMAN
DE TE 12 JANVYE A TOUT FANMIM FIN MOURI
MWEN SANTIM SEL MWEN TE VLE ALE VIV
I?m a young student in computer science, who has suffered
a lot during and after the earthquake of January 12th. All
my family has died and I feel alone. I wanted to go live.
Mw rele [FIRSTNAME], mw fe` mason epi mw abite
laple`n. Yo dim minustah ap bay djob mason ki kote pou
mw ta pase si mw ta vle jwenn nan djob sa yo.
My name is [FIRSTNAME], I?m a construction worker and
I live in La Plaine. I heard that the MINUSTAH was giving
jobs to construction workers. What do I have to go to find
one of these jobs?
Souple mande lapolis pou fe on ti pase nan magloire am-
broise prolonge zone muler ak cadet jeremie ginyin jen ga-
son ki ap pase nan zone sa yo e ki agresi
please ask the police to go to magloire ambroise going to-
wards the ?muler? area and cadet jeremie because there are
very aggressive young men in these areas
KIBO MOUN KA JWENN MANJE POU YO MANJE
ANDEYO KAPITAL PASKE DEPI 12 JANVYE YO
VOYE MANJE POU PEP LA MEN NOU PA JANM
JWENN ANYEN. NAP MOURI AK GRANGOU
Where can people get food to eat outside of the capital be-
cause since January 12th, they?ve sent food for the people
but we never received anything. We are dying of hunger
Mwen se [FIRSTNAME][LASTNAME] mwen nan aken
mwen se yon je`n ki ansent mwen te genyen yon paran ki tap
ede li mouri po`toprens, mwen pral akouye nan ko`mansman
feviye
I am [FIRSTNAME][LASTNAME] I am in Aquin I am a
pregnant young person I had a parent who was helping me,
she died in Port-au-Prince, I?m going to give birth at the
start of February
Table 4: Examples of some of the Haitian Creole SMS messages that were sent to the 4636 short code along with
their translations into English. Translations were done by volunteers who wanted to help with the relief effort. Prior
to being distributed, the messages were anonymized to remove names, phone numbers, email addresses, etc. The
anonymization guidelines specified that addresses be retained to facilitate work on mapping technologies.
28
Training set Parallel Words
sentences per lang
In-domain SMS data 17,192 35k
Medical domain 1,619 10k
Newswire domain 13,517 30k
Glossary 35,728 85k
Wikipedia parallel sentence 8,476 90k
Wikipedia named entities 10,499 25k
The bible 30,715 850k
Haitisurf dictionary 3,763 4k
Krengle dictionary 1,687 3k
Krengle sentences 658 3k
Table 5: Training data for the Haitian Creole-English fea-
tured translation task. The in-domain SMS data consists
primarily of raw (noisy) SMS data. The in-domain data
was provided by Mission 4636. The other data is out-of-
domain. It comes courtesy of Carnegie Mellon Univer-
sity, Microsoft Research, Haitisurf.com, and Krengle.net.
alongside the SMS data. The WMT organizers re-
quested that if they discovered messages with incor-
rect or incomplete anonymization, that they notify
us and correct the anonymization using the version
control repository.
To define the shared translation task, we divided
the SMS messages into an in-domain training set,
along with designated dev, devtest, and test sets. We
coordinated with Microsoft and CMU to make avail-
able additional out-of-domain parallel corpora. De-
tails of the data are given in Table 5. In addition
to this data, participants in the featured task were
allowed to use any of the data provided in the stan-
dard translation task, as well as linguistic tools such
as taggers, parsers, or morphological analyzers.
3.2 Clean and Raw Test Data
We provided two sets of testing and development
data. Participants used their systems to translate two
test sets consisting of 1,274 unseen Haitian Creole
SMS messages. One of the test sets contains the
?raw? SMS messages as they were sent, and the
other contains messages that were cleaned up by hu-
man post-editors. The English side is the same in
both cases, and the only difference is the Haitian
Creole input sentences.
The post-editors were Haitian Creole language
informants hired by Microsoft Research. They pro-
vided a number of corrections to the SMS messages,
including expanding SMS shorthands, correcting
spelling/grammar/capitalization, restoring diacritics
that were left out of the original message, and
cleaning up accented characters that were lost when
the message was transmitted in the wrong encoding.
Original Haitian Creole messages:
Sil vou ple? e?de mwen avek moun ki vik-
tim yo nan tranbleman de te? a,ki kite? poto-
prins ki vini nan provins- mwen ede ak ti
kob mwen te ginyin kounie? a
4636: Manje vin pi che nan PaP apre tran-
bleman te-a. mamit diri ap van?n 250gd
kounye, sete 200gd avan. Mayi-a 125gd,
avan sete 100gd
Edited Haitian Creole messages:
Silvouple ede mwen ave`k moun ki viktim
yo nan tranblemannte` a, ki kite Po`toprens
ki vini nan pwovens, mwen ede ak ti ko`b
mwen te genyen kounye a
4636: Manje vin pi che` nan PaP apre tran-
blemannte` a. Mamit diri ap vann 250gd
kounye a, sete 200gd avan. Mayi-a 125gd,
avan sete 100gd.
For the test and development sets the informants
also edited the English translations. For instance,
there were cases where the original crowdsourced
translation summarized the content of the message
instead of translating it, instances where parts of
the source were omitted, and where explanatory
notes were added. The editors improved the trans-
lations so that they were more suitable for machine
translation, making them more literal, correcting
disfluencies on the English side, and retranslating
them when they were summaries.
Crowdsourced English translation:
We are in the area of Petit Goave, we
would like .... we need tents and medi-
cation for flu/colds...
Post-edited translation:
We are in the area of Petit Goave, we
would like to receive assistance, however,
29
it should not be the way I see the Minus-
tah guys are handling the people. We need
lots of tents and medication for flu/colds,
and fever
The edited English is provided as the reference for
both the ?clean? and the ?raw? sets, since we intend
that distinction to refer to the form that the source
language comes in, rather than the target language.
Tables 47 and 48 in the Appendix show a signifi-
cant difference in the translation quality between the
clean and the raw test sets. In most cases, systems?
output for the raw condition was 4 BLEU points
lower than for the clean condition. We believe that
the difference in performance on the raw vs. cleaned
test sets highlight the importance of handling noisy
input data.
All of the in-domain training data is in the raw for-
mat. The original SMS messages are unaltered, and
the translations are just as the volunteered provided
them. In some cases, the original SMS messages are
written in French or English instead of Haitian Cre-
ole, or contain a mixture of languages. It may be
possible to further improve the quality of machine
translation systems trained from this data by improv-
ing the quality of the data itself.
3.3 Goals and Challenges
The goals of the Haitian Creole to English transla-
tion task were:
? To focus researchers on the problems presented
by low resource languages
? To provide a real-world data set consisting of
SMS messages, which contain abbreviations,
non-standard spelling, omitted diacritics, and
other noisy character encodings
? To develop techniques for building translation
systems that will be useful in future crises
There are many challenges in translating noisy
data in a low resource language, and there are a vari-
ety of strategies that might be considered to attempt
to tackle them. For instance:
? Automated cleaning of the raw (noisy) SMS
data in the training set.
? Leveraging a larger French-English model to
translate out of vocabulary Haitian words, by
creating a mapping from Haitian words onto
French.
? Incorporation of morphological and/or syntac-
tic models to better cope with the low resource
language pair.
It is our hope that by introducing this data as a
shared challenge at WMT11 that we will establish a
useful community resource so that researchers may
explore these challenges and publish about them in
the future.
4 Human Evaluation
As with past workshops, we placed greater empha-
sis on the human evaluation than on the automatic
evaluation metric scores. It is our contention that
automatic measures are an imperfect substitute for
human assessment of translation quality. Therefore,
we define the manual evaluation to be primary, and
use the human judgments to validate automatic met-
rics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partici-
pants, interested volunteers, and a small number of
paid annotators (recruited by the participating sites).
More than 130 people participated in the manual
evaluation, with 91 people putting in more than an
hour?s worth of effort, and 29 putting in more than
four hours. There was a collective total of 361 hours
of labor.
We asked annotators to evaluate system outputs
by ranking translated sentences relative to each
other. This was our official determinant of trans-
lation quality. The total number of judgments col-
lected for the different ranking tasks is given in Ta-
ble 6.
We performed the manual evaluation of the indi-
vidual systems separately from the manual evalua-
tion of the system combination entries, rather than
comparing them directly against each other. Last
year?s results made it clear that there is a large (ex-
pected) gap in performance between the two groups.
This year, we opted to reduce the number of pairwise
30
comparisons with the hope that we would be more
likely to find statistically significant differences be-
tween the systems in the same groups. To that same
end, we also eliminated the editing/acceptability
task that was featured in last year?s evaluation, in-
stead we had annotators focus solely on the system
ranking task.
4.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the instruc-
tions simple:
You are shown a source sentence followed
by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions, and asked to rank them.
With the exception of a few tasks in the system
combination track, there were many more than 5
systems participating in any given task?up to 23
for the English-German individual systems track.
Rather than attempting to get a complete ordering
over the systems, we instead relied on random se-
lection and a reasonably large sample size to make
the comparisons fair.
We use the collected rank labels to assign each
system a score that reflects how highly that system
was usually ranked by the annotators. The score for
some systemA reflects how frequently it was judged
to be better than or equal to other systems. Specif-
ically, each block in which A appears includes four
implicit pairwise comparisons (against the other pre-
sented systems). A is rewarded once for each of the
four comparisons in which A wins or ties. A?s score
is the number of such winning (or tying) pairwise
comparisons, divided by the total number of pair-
wise comparisons involving A.
The system scores are reported in Section 5. Ap-
pendix A provides detailed tables that contain pair-
wise head-to-head comparisons between pairs of
systems.
4.2 Inter- and Intra-annotator agreement in
the ranking task
We were interested in determining the inter- and
intra-annotator agreement for the ranking task, since
a reasonable degree of agreement must exist to sup-
port our process as a valid evaluation setup. To en-
sure we had enough data to measure agreement, we
purposely designed the sampling of source segments
and translations shown to annotators in a way that
ensured some items would be repeated, both within
the screens completed by an individual annotator,
and across screens completed by different annota-
tors.
We did so by ensuring that 10% of the generated
screens are exact repetitions of previously gener-
ated screen within the same batch of screens. Fur-
thermore, even within the other 90%, we ensured
that a source segment appearing in one screen ap-
pears again in two more screens (though with differ-
ent system outputs). Those two details, intentional
repetition of source sentences and intentional repeti-
tion of system outputs, ensured we had enough data
to compute meaningful inter- and intra-annotator
agreement rates.
We measured pairwise agreement among anno-
tators using Cohen?s kappa coefficient (?) (Cohen,
1960), which is defined as
? =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the anno-
tators agree, and P (E) is the proportion of time that
they would agree by chance. Note that ? is basically
a normalized version of P (A), one which takes into
account how meaningful it is for annotators to agree
with each other, by incorporating P (E). Note also
that ? has a value of at most 1 (and could possibly
be negative), with higher rates of agreement result-
ing in higher ?.
The above definition of ? is actually used by sev-
eral definitions of agreement measures, which differ
in how P (A) and P (E) are computed.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate at
31
Inividual System Track System Combination Track
Language Pair # Systems Label Labels # Systems Label Labels
Count per System Count per System
Czech-English 8 2,490 276.7 4 1,305 261.0
English-Czech 10 8,985 816.8 2 2,700 900.0
German-English 20 4,620 220.0 8 1,950 216.7
English-German 22 6,540 284.4 4 2,205 441.0
Spanish-English 15 2,850 178.1 6 2,115 302.1
English-Spanish 15 5,595 349.7 4 3,000 600.0
French-English 18 3,540 186.3 6 1,500 214.3
English-French 17 4,590 255.0 2 900 300.0
Haitian (Clean)-English 9 3,360 336.0 3 1,200 300.0
Haitian (Raw)-English 6 1,875 267.9 2 900 300.0
Urdu-English 8 3,165 351.7 N/A N/A N/A
(tunable metrics task)
Overall 148 47,610 299.4 41 17,775 348.5
Table 6: A summary of the WMT11 ranking task, showing the number of systems and number of labels collected in
each of the individual and system combination tracks. The system count does not include the reference translation,
which was included in the evaluation, and so a value under ?Labels per System? can be obtained only after adding 1
to the system count, before dividing the label count (e.g. in German-English, 4, 620/21 = 220.0).
which annotators agree, in the context of pairwise
comparisons. P (A) is computed similarly for intra-
annotator agreement (i.e. self-consistency), but over
pairwise comparisons that were annotated more than
once by a single annotator.
As for P (E), it should capture the probability that
two annotators would agree randomly. Therefore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is com-
puted empirically, by observing how often annota-
tors actually rank two systems as being tied. We
note here that this empirical computation is a depar-
ture from previous years? analyses, where we had
assumed that the three categories are equally likely
(yielding P (E) = 19 +
1
9 +
1
9 =
1
3 ). We believe that
this is a more principled approach, which faithfully
reflects the motivation of accounting for P (E) in the
first place.6
6Even if we wanted to assume a ?random clicker? model,
setting P (E) = 13 is still not entirely correct. Given that
Table 7 gives ? values for inter-annotator and
intra-annotator agreement across the various evalu-
ation tasks. These give an indication of how often
different judges agree, and how often single judges
are consistent for repeated judgments, respectively.
There are some general and expected trends that
can be seen in this table. First of all, intra-annotator
agreement is higher than inter-annotator agreement.
Second, reference translations are noticeably better
than other system outputs, which means that anno-
tators have an artificially high level of agreement on
pairwise comparisons that include a reference trans-
lation. For this reason, we also report the agreement
levels when such comparisons are excluded.
The exact interpretation of the kappa coefficient is
difficult, but according to Landis and Koch (1977),
0?0.2 is slight, 0.2?0.4 is fair, 0.4?0.6 is moder-
ate, 0.6? 0.8 is substantial, and 0.8? 1.0 is almost
perfect. Based on these interpretations, the agree-
ment for sentence-level ranking is moderate to sub-
stantial for most tasks.
annotators rank five outputs at once, P (A = B) = 15 , not
1
3 , since there are only five (out of 25) label pairs that satisfy
A = B. Working this back into P (E)?s definition, we have
P (A > B) = P (A < B) = 25 , and therefore P (E) = 0.36
rather than 0.333.
32
INTER-ANNOTATOR AGREEMENT (I.E. ACROSS ANNOTATORS)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
European languages, individual systems 0.601 0.362 0.375 0.561 0.355 0.320
European languages, system combinations 0.671 0.335 0.505 0.598 0.342 0.389
Haitian-English, individual systems 0.691 0.362 0.516 0.639 0.350 0.446
Haitian-English, system combinations 0.761 0.358 0.628 0.674 0.335 0.509
Tunable metrics task (Urdu-English) 0.692 0.337 0.535 0.641 0.363 0.437
WMT10 (European languages, all systems) 0.658 0.374 0.454 0.626 0.367 0.409
INTRA-ANNOTATOR AGREEMENT (I.E. SELF-CONSISTENCY)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
European languages, individual systems 0.722 0.362 0.564 0.685 0.355 0.512
European languages, system combinations 0.787 0.335 0.680 0.717 0.342 0.571
Haitian-English, individual systems 0.763 0.362 0.628 0.700 0.350 0.539
Haitian-English, system combinations 0.882 0.358 0.816 0.784 0.335 0.675
Tunable metrics task (Urdu-English) 0.857 0.337 0.784 0.856 0.363 0.774
WMT10 (European languages, all systems) 0.755 0.374 0.609 0.734 0.367 0.580
Table 7: Inter- and intra-annotator agreement rates, for the various manual evaluation tracks of WMT11. See Tables 49
and 50 below for a detailed breakdown by language pair.
However, one result that is of concern is that
agreement rates are noticeably lower for European
language pairs, in particular for the individual sys-
tems track. When excluding reference comparisons,
the inter- and intra-annotator agreement levels are
0.320 and 0.512, respectively. Not only are those
numbers lower than for the other tasks, but they
are also lower than last year?s numbers, which were
0.409 and 0.580.
We investigated this result a bit deeper. Tables 49
and 50 in the Appendix break down the results fur-
ther, by reporting agreement levels for each lan-
guage pair. One observation is that the agreement
level for some language pairs deviates in a non-
trivial amount from the overall agreement rate.
Let us focus on inter-annotator agreement rates
in the individual track (excluding reference compar-
isons), in the top right portion of Table 49. The over-
all ? is 0.320, but it ranges from 0.264 for German-
English, to 0.477 for Spanish-English.
What distinguishes those two language pairs from
each other? If we examine the results in Table 8,
we see that Spanish-English had two very weak sys-
tems, which were likely easy for annotators to agree
on comparisons involving them. (This is the con-
verse of annotators agreeing more often on com-
parisons involving the reference.) English-French is
similar in that regard, and it too has a relatively high
agreement rate.
On the other hand, the participants in German-
English formed a large pool of more closely-
matched systems, where the gap separating the bot-
tom system is not as pronounced. So it seems that
the low agreement rates are indicative of a more
competitive evaluation and more closely-matched
systems.
5 Results of the Translation Tasks
We used the results of the manual evaluation to an-
alyze the translation quality of the different systems
that were submitted to the workshop. In our analy-
sis, we aimed to address the following questions:
? Which systems produced the best translation
quality for each language pair?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
33
Czech-English
1023?1166 comparisons/system
System C? ?others
UEDIN ?? Y 0.69
ONLINE-B ? N 0.68
CU-BOJAR N 0.60
JHU N 0.57
UPPSALA Y 0.57
SYSTRAN N 0.51
CST Y 0.47
CU-ZEMAN Y 0.44
Spanish-English
583?833 comparisons/system
System C? ?others
ONLINE-B ? N 0.72
ONLINE-A ? N 0.72
KOC ? Y 0.67
SYSTRAN ? N 0.66
ALACANT ? N 0.66
RBMT-1 N 0.63
RBMT-3 N 0.61
RBMT-2 N 0.60
RBMT-4 N 0.60
RBMT-5 N 0.51
UEDIN Y 0.51
UPM Y 0.50
UFAL-UM Y 0.47
HYDERABAD Y 0.17
CU-ZEMAN Y 0.16
French-English
608?883 comparisons/system
System C? ?others
ONLINE-A ? N 0.66
LIMSI ?? Y+G 0.66
ONLINE-B ? N 0.66
LIA-LIG Y 0.64
KIT ?? Y+G 0.64
LIUM Y+G 0.63
CMU-DENKOWSKI ? Y 0.62
JHU Y+G 0.61
RWTH-HUCK Y+G 0.58
RBMT-1 ? N 0.58
CMU-HANNEMAN Y+G 0.58
RBMT-3 N 0.55
SYSTRAN N 0.54
RBMT-4 N 0.53
RBMT-2 N 0.52
UEDIN Y 0.50
RBMT-5 N 0.45
CU-ZEMAN Y 0.37
English-Czech
3126?3397 comparisons/system
System C? ?others
ONLINE-B ? N 0.65
CU-BOJAR N 0.64
CU-MARECEK ? N 0.63
CU-TAMCHYNA N 0.62
UEDIN ? Y 0.59
CU-POPEL ? Y 0.58
COMMERCIAL2 N 0.51
COMMERCIAL1 N 0.51
JHU N 0.49
CU-ZEMAN Y 0.43
English-Spanish
1300?1480 comparisons/system
System C? ?others
ONLINE-B ? N 0.74
ONLINE-A ? N 0.72
RBMT-3 ? N 0.71
PROMT ? N 0.70
CEU-UPV ? Y 0.65
UEDIN ? Y 0.64
UPPSALA ? Y 0.61
RBMT-4 N 0.61
RBMT-1 N 0.60
UOW Y 0.59
RBMT-2 N 0.57
KOC Y 0.56
RBMT-5 N 0.54
CU-ZEMAN Y 0.49
UPM Y 0.34
English-French
868?1121 comparisons/system
System C? ?others
LIMSI ?? Y+G 0.73
ONLINE-B ? N 0.70
KIT ?? Y+G 0.69
RWTH-HUCK Y+G 0.65
LIUM Y+G 0.64
RBMT-1 N 0.61
ONLINE-A N 0.60
UEDIN Y 0.58
RBMT-3 N 0.58
RBMT-5 N 0.55
UPPSALA Y 0.55
JHU Y 0.55
UPPSALA-FBK Y 0.54
RBMT-4 N 0.49
RBMT-2 N 0.46
LATL-GENEVA N 0.39
CU-ZEMAN Y 0.20
German-English
741?998 comparisons/system
System C? ?others
ONLINE-B ? N 0.72
CMU-DYER ?? Y+G 0.66
ONLINE-A ? N 0.66
RBMT-3 N 0.64
LINGUATEC N 0.63
RBMT-4 N 0.61
RBMT-1 N 0.60
DFKI-XU N 0.60
RWTH-WUEBKER ? Y+G 0.59
KIT Y+G 0.57
LIU Y 0.57
LIMSI Y+G 0.56
RBMT-5 N 0.56
UEDIN Y 0.55
RBMT-2 N 0.54
CU-ZEMAN Y 0.47
UPPSALA Y 0.47
KOC Y 0.45
JHU Y+G 0.43
CST Y 0.37
English-German
1051?1230 comparisons/system
System C? ?others
RBMT-3 ? N 0.73
ONLINE-B ? N 0.73
RBMT-1 ? N 0.70
DFKI-FEDERMANN ? N 0.68
DFKI-XU N 0.67
RBMT-4 ? N 0.66
RBMT-2 ? N 0.66
ONLINE-A ? N 0.65
LIMSI ? Y+G 0.65
KIT ? Y 0.64
UEDIN Y 0.60
LIU Y 0.59
RBMT-5 N 0.58
RWTH-FREITAG Y 0.56
COPENHAGEN ? Y 0.56
JHU Y 0.54
KOC Y 0.53
UOW Y 0.53
CU-TAMCHYNA Y 0.50
UPPSALA Y 0.49
ILLC-UVA Y 0.48
CU-ZEMAN Y 0.38
C? indicates whether system is constrained: trained only using supplied training data, standard monolingual linguis-
tic tools, and, optionally, LDC?s English Gigaword. Eentries that used the Gigaword are marked with +G.
? indicates a win: no other system is statistically significantly better at p-level?0.10 in pairwise comparison.
? indicates a constrained win: no other constrained system is statistically better.
Table 8: Official results for the WMT11 translation task. Systems are ordered by their ?others score, reflecting how
often their translations won or tied pairwise comparisons. For detailed head-to-head comparisons, see Appendix A.
34
Czech-English
1036?1042 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.64
BBN-COMBO ? 0.62
JHU-COMBO 0.58
UPV-PRHLT-COMBO 0.47
English-Czech
1788?1792 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.48
UPV-PRHLT-COMBO 0.41
German-English
811?927 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.70
RWTH-LEUSCH-COMBO 0.65
BBN-COMBO 0.61
UZH-COMBO ? 0.60
JHU-COMBO 0.56
UPV-PRHLT-COMBO 0.52
QUAERO-COMBO 0.46
KOC-COMBO 0.45
English-German
1746?1752 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.61
UZH-COMBO ? 0.58
UPV-PRHLT-COMBO 0.56
KOC-COMBO 0.46
Spanish-English
1132?1249 comparisons/combo
System ?others
RWTH-LEUSCH-COMBO ? 0.71
CMU-HEAFIELD-COMBO ? 0.67
BBN-COMBO ? 0.64
UPV-PRHLT-COMBO 0.64
JHU-COMBO 0.62
KOC-COMBO 0.56
English-Spanish
2360?2378 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.69
UOW-COMBO 0.63
UPV-PRHLT-COMBO 0.59
KOC-COMBO 0.58
French-English
820?916 comparisons/combo
System ?others
BBN-COMBO ? 0.67
RWTH-LEUSCH-COMBO ? 0.63
CMU-HEAFIELD-COMBO 0.62
JHU-COMBO ? 0.59
LIUM-COMBO 0.53
UPV-PRHLT-COMBO 0.53
English-French
586?587 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.51
UPV-PRHLT-COMBO 0.43
? indicates a win: no other system combination is statistically significantly better at p-level?0.10 in pairwise
comparison.
Table 9: Official results for the WMT11 system combination task. Systems are ordered by their ?others score,
reflecting how often their translations won or tied pairwise comparisons. For detailed head-to-head comparisons, see
Appendix A.
35
Haitian Creole (Clean)-English
(individual systems)
1256?1435 comparisons/system
System ?others
BM-I2R ? 0.71
CMU-DENKOWSKI 0.66
CMU-HEWAVITHARANA 0.64
UMD-EIDELMAN 0.63
UPPSALA 0.57
LIU 0.55
UMD-HU 0.52
HYDERABAD 0.43
KOC 0.31
Haitian Creole (Raw)-English
(individual systems)
1065?1136 comparisons/system
System ?others
BM-I2R ? 0.65
CMU-HEWAVITHARANA 0.60
CMU-DENKOWSKI 0.59
LIU 0.55
UMD-EIDELMAN 0.52
JHU 0.41
Haitian Creole (Clean)-English
(system combinations)
896?898 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.52
UPV-PRHLT-COMBO 0.48
KOC-COMBO 0.38
Haitian Creole (Raw)-English
(system combinations)
600?600 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO 0.47
UPV-PRHLT-COMBO 0.43
? indicates a win: no other system is statistically significantly better at p-level?0.10 in pairwise comparison.
Table 10: Official results for the WMT11 featured translation task (Haitian Creole SMS into English). Systems are
ordered by their ?others score, reflecting how often their translations won or tied pairwise comparisons. For detailed
head-to-head comparisons, see Appendix A.
36
Tables 8?10 show the system ranking for each
of the translation tasks. For each language pair,
we define a system as ?winning? if no other system
was found statistically significantly better (using the
Sign Test, at p ? 0.10). In some cases, multiple sys-
tems are listed as winners, either due to a large num-
ber of participants or a low number of judgments per
system pair, both of which are factors that make it
difficult to achieve statistical significance.
We start by examining the results for the individ-
ual system track for the European languages (Ta-
ble 8). In Spanish?English and German?English,
unconstrained systems are observed to perform bet-
ter than constrained systems. In other language
pairs, particularly French?English, constrained
systems are found to be able to be on the same level
or outperform unconstrained systems. It also seems
that making use of the Gigaword corpora is likely
to yield better systems, even when translating out of
English, as in English-French and English-German.
For English-German the rule-based MT systems per-
formed well.
Of the participating teams, there is no individ-
ual system clearly outperforming all other systems
across the different language pairs. However, one
of the crawled systems, ONLINE-B, performs con-
sistently well, being one of the winners in all eight
language pairs.
As for the system combination track (Table 9),
the CMU-HEAFIELD-COMBO entry performed quite
well, being a winner in seven out of eight language
pairs. This performance is carried over to the Haitian
Creole task, where it again comes out on top (Ta-
ble 10). In the individual track of the Haitian Creole
task, BM-I2R is the sole winner in both the ?clean?
and ?raw? tracks.
6 Evaluation Task
In addition to allowing us to analyze the translation
quality of different systems, the data gathered during
the manual evaluation is useful for validating auto-
matic evaluation metrics. Our evaluation shared task
is similar to the MetricsMATR workshop (Metrics
for MAchine TRanslation) that NIST runs (Przy-
bocki et al, 2008; Callison-Burch et al, 2010). Ta-
ble 11 lists the participants in this task, along with
their metrics.
A total of 21 metrics and their variants were sub-
mitted to the evaluation task by 9 research groups.
We asked metrics developers to score the outputs of
the machine translation systems and system com-
binations at the system-level and at the segment-
level. The system-level metrics scores are given in
the Appendix in Tables 39?48. The main goal of the
evaluation shared task is not to score the systems,
but instead to validate the use of automatic metrics
by measuring how strongly they correlate with hu-
man judgments. We used the human judgments col-
lected during the manual evaluation for the transla-
tion task and the system combination task to calcu-
late how well metrics correlate at system-level and
at the segment-level.
This year the strongest metric was a new metric
developed by Columbia and ETS called MTeRater-
Plus. MTeRater-Plus is a machine-learning-based
metric that use features from ETS?s e-rater, an auto-
mated essay scoring engine designed to assess writ-
ing proficiency (Attali and Burstein, 2006). The fea-
tures include sentence-level and document-level in-
formation. Some examples of the e-rater features
include:
? Preposition features that calculate the proba-
bility of prepositions appearing in the given
context of a sentence (Tetreault and Chodorow,
2008)
? Collocation features that indicate whether the
collocations in the document are typical of na-
tive use (Futagi et al, 2008).
? A sentence fragment feature that counts the
number of ill-formed sentences in a document.
? A feature that counts the number of words with
inflection errors
? A feature that counts the the number of article
errors in the sentence citeHan2006.
MTeRater uses only the e-rater features, and mea-
sures fluency without any need for reference transla-
tions. MTeRater-Plus is a meta-metric that incorpo-
rates adequacy by combining MTeRater with other
MT evaluation metrics and heuristics that take the
reference translations into account.
Please refer to the proceedings for papers provid-
ing detailed descriptions of all of the metrics.
37
Metric IDs Participant
AMBER, AMBER-NL, AMBER-IT National Research Council Canada (Chen and Kuhn, 2011)
F15, F15G3 Koc? University (Bicici and Yuret, 2011)
METEOR-1.3-ADQ, METEOR-1.3-RANK Carnegie Mellon University (Denkowski and Lavie, 2011a)
MTERATER, MTERATER-PLUS Columbia / ETS (Parton et al, 2011)
MP4IBM1, MPF, WMPF DFKI (Popovic?, 2011; Popovic? et al, 2011)
PARSECONF DFKI (Avramidis et al, 2011)
ROSE, ROSE-POS The University of Sheffield (Song and Cohn, 2011)
TESLA-B, TESLA-F, TESLA-M National University of Singapore (Dahlmeier et al, 2011)
TINE University of Wolverhampton (Rios et al, 2011)
BLEU provided baseline (Papineni et al, 2002)
TER provided baseline (Snover et al, 2006)
Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics
as baselines.
E
N
-C
Z
-
10
S
Y
S
T
E
M
S
E
N
-D
E
-
22
S
Y
S
T
E
M
S
E
N
-E
S
-
15
S
Y
S
T
E
M
S
E
N
-F
R
-
17
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
A
V
E
R
A
G
E
W
/O
C
Z
System-level correlation for translation out of English
TESLA-M .90 .95 .96 .94
TESLA-B .81 .90 .91 .87
MPF .72 .63 .87 .89 .78 .80
WMPF .72 .61 .87 .89 .77 .79
MP4IBM1 -.76 -.91 -.71 -.61 .75 .74
ROSE .65 .41 .90 .86 .71 .73
BLEU .65 .44 .87 .86 .70 .72
AMBER-TI .56 .54 .88 .84 .70 .75
AMBER .56 .53 .87 .84 .70 .74
AMBER-NL .56 .45 .88 .83 .68 .72
F15G3 .50 .30 .89 .84 .63 .68
METEORrank .65 .30 .74 .85 .63 .63
F15 .52 .19 .86 .85 .60 .63
TER -.50 -.12 -.81 -.84 .57 .59
TESLA-F .86 .80 -.83 .28
Table 12: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average abso-
lute value. We did not calculate correlations with the hu-
man judgments for the system combinations for the out of
English direction, because none of them had more than 4
items.
6.1 System-Level Metric Analysis
We measured the correlation of the automatic met-
rics with the human judgments of translation quality
at the system-level using Spearman?s rank correla-
tion coefficient ?. We converted the raw scores as-
signed to each system into ranks. We assigned a hu-
man ranking to the systems based on the percent of
time that their translations were judged to be better
than or equal to the translations of any other system
in the manual evaluation. The reference was not in-
cluded as an extra translation.
When there are no ties, ? can be calculated using
the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher absolute value
for ? is making predictions that are more similar to
the human judgments than an automatic evaluation
metric with a lower absolute ?.
The system-level correlations are shown in Ta-
ble 13 for translations into English, and Table 12
out of English, sorted by average correlation across
the language pairs. The highest correlation for
each language pair and the highest overall average
are bolded. This year, nearly all of the metrics
38
C
Z
-E
N
-
8
S
Y
S
T
E
M
S
D
E
-E
N
-
20
S
Y
S
T
E
M
S
D
E
-E
N
-
8
C
O
M
B
O
S
E
S
-E
N
-
15
S
Y
S
T
E
M
S
E
S
-E
N
-
6
C
O
M
B
O
S
F
R
-E
N
-
18
S
Y
S
T
E
M
S
F
R
-E
N
-
6
C
O
M
B
O
S
A
V
E
R
A
G
E
(E
U
R
O
P
E
A
N
L
A
N
G
S
)
H
T
-E
N
(C
L
E
A
N
)
-
9
S
Y
S
T
E
M
S
H
T
-E
N
(R
A
W
)
-
6
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
(A
L
L
L
A
N
G
S
)
System-level correlation for metrics scoring translations into English
MTERATER-PLUS -.95 -.90 -.93 -.91 -.94 -.93 -.77 .90 -.82 -.54 .85
TINE-SRL-MATCH .95 .69 .95 .95 1.00 .87 .66 .87
TESLA-F .95 .70 .98 .96 .94 .90 .60 .86 .93 .83 .87
TESLA-B .98 .88 .98 .91 .94 .91 .31 .84 .93 .83 .85
MTERATER -.91 -.88 -.91 -.88 -.89 -.79 -.60 .83 .13 .77 .55
METEOR-1.3-ADQ .93 .68 .91 .91 .83 .93 .66 .83 .95 .77 .84
TESLA-M .95 .94 .95 .82 .94 .87 .31 .83 .95 .83 .84
METEOR-1.3-RANK .91 .71 .91 .88 .77 .93 .66 .82 .95 .83 .84
AMBER-NL .88 .58 .91 .88 .94 .94 .60 .82
AMBER-TI .88 .63 .93 .85 .83 .94 .60 .81
AMBER .88 .59 .91 .86 .83 .95 .60 .80
MPF .95 .69 .91 .83 .60 .87 .54 .77 .95 .77 .79
WMPF .95 .66 .86 .83 .60 .87 .54 .76 .93 .77 .78
F15 .93 .45 .88 .96 .49 .87 .60 .74
F15G3 .93 .48 .83 .94 .49 .88 .60 .74
ROSE .88 .59 .83 .92 .60 .86 .26 .70 .93 .77 .74
BLEU .88 .48 .83 .90 .49 .85 .43 .69 .90 .83 .73
TER -.83 -.33 -.64 -.89 -.37 -.77 -.89 .67 -.93 -.83 .72
MP4IBM1 -.91 -.56 -.50 -.12 -.43 -.08 .14 .35
DFKI-PARSECONF .31 .52
Table 13: System-level Spearman?s rho correlation of the automatic evaluation metrics with the human judgments
for translation into English, ordered by average absolute value for the European languages. We did not calculate
correlations with the human judgments for the system combinations for Czech to English and for Haitian Creole to
English, because they had too few items (? 4) for reliable statistics.
39
F
R
-E
N
(6
33
7
PA
IR
S
)
D
E
-E
N
(8
95
0
PA
IR
S
)
E
S
-E
N
(5
97
4
PA
IR
S
)
C
Z
-E
N
(3
69
5
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations into English
MTERATER-PLUS .30 .36 .45 .36 .37
TESLA-F .28 .24 .39 .32 .31
TESLA-B .28 .26 .36 .29 .30
METEOR-1.3-RANK .23 .25 .38 .28 .29
METEOR-1.3-ADQ .24 .25 .37 .27 .28
MPF .25 .23 .34 .28 .28
AMBER-TI .24 .26 .33 .27 .28
AMBER .24 .25 .33 .27 .27
WMPF .24 .23 .34 .26 .27
AMBER-NL .24 .24 .30 .27 .26
MTERATER .19 .26 .33 .24 .26
TESLA-M .21 .23 .29 .23 .24
TINE-SRL-MATCH .20 .19 .30 .24 .23
F15G3 .17 .15 .29 .21 .21
F15 .16 .14 .27 .22 .20
MP4IBM1 .15 .16 .18 .12 .15
DFKI-PARSECONF n/a .24 n/a n/a
Table 14: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average correla-
tion.
had stronger correlation with human judgments than
BLEU. The metrics that had the strongest correlation
this year included two metrics, MTeRater and TINE,
as well as metrics that have demonstrated strong cor-
relation in previous years like TESLA and Meteor.
6.2 Segment-Level Metric Analysis
We measured the metrics? segment-level scores with
the human rankings using Kendall?s tau rank corre-
lation coefficient. The reference was not included as
an extra translation.
We calculated Kendall?s tau as:
? =
num concordant pairs - num discordant pairs
total pairs
where a concordant pair is a pair of two translations
of the same segment in which the ranks calculated
from the same human ranking task and from the cor-
responding metric scores agree; in a discordant pair,
they disagree. In order to account for accuracy- vs.
E
N
-F
R
(6
93
4
PA
IR
S
)
E
N
-D
E
(1
07
32
PA
IR
S
)
E
N
-E
S
(8
83
7
PA
IR
S
)
E
N
-C
Z
(1
16
51
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations out of English
AMBER-TI .32 .22 .31 .21 .27
AMBER .31 .21 .31 .22 .26
MPF .31 .22 .30 .20 .26
WMPF .31 .22 .29 .19 .25
AMBER-NL .30 .19 .29 .20 .25
METEOR-1.3-RANK .31 .14 .26 .19 .23
F15G3 .26 .08 .22 .13 .17
F15 .26 .07 .22 .12 .17
MP4IBM1 .21 .13 .13 .06 .13
TESLA-B .29 .20 .28 n/a
TESLA-M .25 .18 .27 n/a
TESLA-F .30 .19 .26 n/a
Table 15: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average corre-
lation.
error-based metrics correctly, counts of concordant
vs. discordant pairs were calculated specific to these
two metric types. The possible values of ? range
between 1 (where all pairs are concordant) and ?1
(where all pairs are discordant). Thus an automatic
evaluation metric with a higher value for ? is mak-
ing predictions that are more similar to the human
judgments than an automatic evaluation metric with
a lower ? .
We did not include cases where the human rank-
ing was tied for two systems. As the metrics produce
absolute scores, compared to five relative ranks in
the human assessment, it would be potentially un-
fair to the metric to count a slightly different met-
ric score as discordant with a tie in the relative hu-
man rankings. A tie in automatic metric rank for
two translations was counted as discordant with two
corresponding non-tied human judgments.
The correlations are shown in Table 14 for trans-
lations into English, and Table 15 out of English,
sorted by average correlation across the four lan-
guage pairs. The highest correlation for each lan-
guage pair and the highest overall average are
40
ID Participant Metric Name
CMU-METEOR Carnegie Mellon University METEOR (Denkowski and Lavie, 2011a)
CU-SEMPOS-BLEU Charles University SemPOS/BLEU (Macha?c?ek and Bojar, 2011)
NUS-TESLA-F National University of Singapore TESLA-F (Dahlmeier et al, 2011)
RWTH-CDER RWTH Aachen CDER (Leusch and Ney, 2009)
SHEFFIELD-ROSE The University of Sheffield ROSE (single reference) (Song and Cohn, 2011)
STANFORD-DCP Stanford DCP (based on Liu and Gildea (2005))
BLEU provided baseline BLEU
BLEU-SINGLE provided baseline BLEU (single reference)
Table 16: Participants in the tunable-metric shared task. For comparison purposes, we included two BLEU-optimized
systems in the evaluation as baselines.
bolded. There is a clear winner for the metrics that
score translations into English: the MTeRater-Plus
metric (Parton et al, 2011) has the highest segment
level correlation across the board. For metrics that
score translation into other languages, there is not
such a clear-cut winner. The AMBER metric variants
do well, as do MPF and WMPF.
7 Tunable Metrics Task
This year we introduced a new shared task that fo-
cuses on using evaluation metrics to tune the param-
eters of a statistical machine translation system. The
intent of this task was to get researchers who de-
velop automatic evaluation metrics for MT to work
on the problem of using their metric to optimize
the parameters of MT systems. Previous workshops
have demonstrated that a number of metrics perform
better than BLEU in terms of having stronger cor-
relation with human judgments about the rankings
of multiple machine translation systems. However,
most MT system developers still optimize the pa-
rameters of their systems to BLEU. Here we aim
to investigate the question of whether better metrics
will result in better quality output when a system is
optimized to them.
Because this was the first year that we ran the
tunable metrics task, participation was limited to a
few groups on an invitation-only basis. Table 16
lists the participants in this task. Metrics developers
were invited to integrate their evaluation metric into
a MERT optimization routine, which was then used
to tune the parameters of a fixed statistical machine
translation system. We evaluated whether the sys-
tem tuned on their metrics produced higher-quality
output than the baseline system that was tuned to
BLEU, as is typically done. In order to evaluate
whether the quality was better, we conducted a man-
ual evaluation, in the same fashion that we evalu-
ate the different MT systems submitted to the shared
translation task.
We provide the participants with a fixed MT sys-
tem for Urdu-English, along with a small parallel
set to be used for tuning. Specifically, we provide
developers with the following components:
? Decoder - the Joshua decoder was used in this
pilot.
? Decoder configuration file - a Joshua configu-
ration file that ensures all systems use the same
search parameters.
? Translation model - an Urdu-to-English trans-
lation model, with syntax-based SCFG rules
(Baker et al, 2010).
? Language model - a large 5-gram language
model trained on the English Gigaword corpus
? Development set - a development set, with 4
English reference sets, to be used to optimize
the system parameters.
? Test set - a test set consisting of 883 Urdu sen-
tences, to be translated by the tuned system (no
references provided).
? Optimization routine - we provide an imple-
mentation of minimum error rate training that
allows new metrics to be easily integrated as
the objective function.
41
Tunable Metrics Task
1324?1484 comparisons/system
System ?others >others
BLEU ? 0.79 0.28
BLEU-SINGLE ? 0.77 0.27
CMU-METEOR ? 0.76 0.27
RWTH-CDER 0.76 0.26
CU-SEMPOS-BLEU ? 0.74 0.29
STANFORD-DCP ? 0.73 0.27
NUS-TESLA-F 0.68 0.28
SHEFFIELD-ROSE 0.05 0.00
? indicates a win: no other system combination is sta-
tistically significantly better at p-level?0.10 in pair-
wise comparison.
Table 17: Official results for the WMT11 tunable-metric
task. Systems are ordered by their ?others score, re-
flecting how often their translations won or tied pairwise
comparisons. The > column reflects how often a system
strictly won a pairwise comparison.
We provided the metrics developers with Omar
Zaidan?s Z-MERT software (Zaidan, 2009), which
implements Och (2003)?s minimum error rate train-
ing procedure. Z-MERT is designed to be modular
with respect to the objective function, and allows
BLEU to be easily replaced with other automatic
evaluation metrics. Metric developers incorporated
their metrics into Z-MERT by subclassing the Eval-
uationMetric.java abstract class. They ran Z-MERT
on the dev set with the provided decoder/models,
and created a weight vector for the system param-
eters.
Each team produced a distinct final weight vec-
tor, which was used to produce English translations
of sentences in the test set. The different transla-
tions produced by tuning the system to different met-
rics were then evaluated using the manual evaluation
pipeline.7
7.1 Results of the Tunable Metrics Task
The results of the evaluation are in Table 18. The
scores show that the entries were quite close to each
other, with the notable exception of the SHEFFIELD-
ROSE-tuned system, which produced overly-long
7We also recased and detokenized each system?s output, to
ensure the outputs are more readable and easier to evaluate.
R
E
F
B
L
E
U
B
L
E
U
-S
IN
G
L
E
C
M
U
-M
E
T
E
O
R
C
U
-S
E
M
P
O
S
-B
L
E
U
N
U
S
-T
E
S
L
A
-F
R
W
T
H
-C
D
E
R
S
H
E
F
F
IE
L
D
-R
O
S
E
S
T
A
N
F
O
R
D
-D
C
P
REF ? .15? .11? .13? .09? .09? .10? .00? .11?
BLEU .78? ? .15 .11 .20 .19? .13? .01? .14
BLEU-SINGLE .82? .20 ? .11 .16 .21 .11 .00? .20
CMU-METEOR .84? .09 .15 ? .21 .20 .19 .00? .19
CU-SEMPOS-BLEU .82? .23 .21 .21 ? .12? .18 .00? .21
NUS-TESLA-F .80? .32? .31 .28 .28? ? .31 .00? .28
RWTH-CDER .79? .22? .16 .16 .22 .23 ? .00? .15
SHEFFIELD-ROSE .98? .93? .93? .96? .95? .95? .93? ? .94?
STANFORD-DCP .82? .17 .18 .26 .27 .28 .15 .00? ?
> others .83 .28 .27 .27 .29 .28 .26 .00 .27
>= others .90 .79 .77 .76 .74 .68 .76 .05 .73
Table 18: Head to head comparisons for the tunable met-
rics task. The numbers indicate how often the system in
the column was judged to be better than the system in
the row. The difference between 100 and the sum of the
corresponding cells is the percent of time that the two
systems were judged to be equal.
and erroneous output (possibly due to an implemen-
tation issue). This is also evident from the fact that
38% of pairwise comparisons indicated a tie be-
tween the two systems, with the tie rate increasing
to a full 47% when excluding comparisons involving
the reference. This is a very high tie rate ? the cor-
responding figure in, say, European language pairs
(individual systems) is only 21%.
What makes the different entries appear even
more closely-matched is that the ranking changes
significantly when ordering systems by their
>others score rather than the ?others score (i.e.
when rewarding only wins, and not rewarding ties).
NUS-TESLA-F goes from being a bottom entry to be-
ing a top entry, with CU-SEMPOS-BLEU also bene-
fiting, changing from the middle to the top rank.
Either way, we see that a BLEU -tuned system
is performing just as well as systems tuned to the
other metrics. This might be an indication that some
work remains to be done before a move away from
BLEU-tuning is fully justified. On the other hand,
the close results might be an artifact of the language
pair choice. Urdu-English translation is still a rel-
atively difficult problem, and MT outputs are still
of a relatively low quality. It might be the case that
human annotators are simply not very good at distin-
42
guishing one bad translation from another bad trans-
lation, especially at such a fine-grained level.
It is worth noting that the designers of the TESLA
family replicated the setup of this tunable metric task
for three European language pairs, and found that
human judges did perceive a difference in quality
between a TESLA-tuned system and a BLEU -tuned
system (Liu et al, 2011).
7.2 Anticipated Changes Next Year
This year?s effort was a pilot of the task, so we in-
tentionally limited the task to some degree, to make
it easier to iron out the details. Possible changes for
next year include:
? More language pairs / translations into lan-
guages other than English. This year we fo-
cus on Urdu-English because the language pair
requires a lot of reordering, and our syntactic
model has more parameters to optimize than
the standard Hiero and phrase-based models.
? Provide some human judgments about the
model?s output, so that people can experiment
with regression models.
? Include a single reference track along with the
multiple reference track. Some metrics may be
better at dealing with the (more common) case
of there being only a single reference transla-
tion available for every source sentence.
? Allow for experimentation with the MIRA op-
timization routine instead of MERT. MIRA can
scale to a greater number of features, but re-
quires that metrics be decomposable.
8 Summary
As in previous editions of this workshop we car-
ried out an extensive manual and automatic evalua-
tion of machine translation performance for translat-
ing from European languages into English, and vice
versa.
The number of participants grew slightly com-
pared to previous editions of the WMT workshop,
with 36 groups from 27 institutions participating in
the translation task of WMT11, 10 groups from 10
institutions participating in the system combination
task, and 10 groups from 8 institutions participating
in the featured translation task (Haitian Creole SMS
into English).
This year was also the first time that we included a
language pair (Haitian-English) with non-European
source language and with very limited resources for
the source language side. Also the genre of the
Haitian-English task differed from previous WMT
tasks as the Haitian-English translations are SMS
messages.
WMT11 also introduced a new shared task focus-
ing on evaluation metrics to tune the parameters of
a statistical machine translation system in which 6
groups have participated.
As in previous years, all data sets generated by
this workshop, including the human judgments, sys-
tem translations and automatic scores, are publicly
available for other researchers to analyze.8
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-0022,
the US National Science Foundation under grant
IIS-0713448, and the CoSyne project FP7-ICT-4-
248531 funded by the European Commission. The
views and findings are the authors? alone. A big
thank you to Ondr?ej Bojar, Simon Carter, Chris-
tian Federmann, Will Lewis, Rob Munro and Herve?
Saint-Amand, and to the shared task participants.
References
Vera Aleksic and Gregor Thurmair. 2011. Personal
Translator at WMT2011. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Alexandre Allauzen, He?le`ne Bonneau-Maynard, Hai-Son
Le, Aure?lien Max, Guillaume Wisniewski, Franc?ois
Yvon, Gilles Adda, Josep Maria Crego, Adrien
Lardilleux, Thomas Lavergne, and Artem Sokolov.
2011. LIMSI @ WMT11. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3):159?174.
Eleftherios Avramidis, Maja Popovic?, David Vilar, and
Aljoscha Burchardt. 2011. Evaluate with confidence
8http://statmt.org/wmt11/results.html
43
estimation: Machine ranking of translation outputs us-
ing grammatical features. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Wilker Aziz, Miguel Rios, and Lucia Specia. 2011. Shal-
low semantic trees for SMT. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Kathryn Baker, Michael Bloodgood, Chris Callison-
Burch, Bonnie Dorr, Scott Miller, Christine Pi-
atko, Nathaniel W. Filardo, and Lori Levin. 2010.
Semantically-informed syntactic machine translation:
A tree-grafting approach. In Proceedings of AMTA.
Lo??c Barrault. 2011. MANY improvements for
WMT?11. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Ergun Bicici and Deniz Yuret. 2011. RegMT system for
machine translation, system combination, and evalua-
tion. In Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Improving
translation model by monolingual data. In Proceed-
ings of the Sixth Workshop on Statistical Machine
Translation.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion (WMT07), Prague, Czech Republic.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation (WMT09), Athens, Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT10), Uppsala, Swe-
den.
Boxing Chen and Roland Kuhn. 2011. Amber: A mod-
ified bleu, enhanced ranking metric. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measur-
ment, 20(1):37?46.
Antonio M. Corb??-Bellot, Mikel L. Forcada, Sergio Ortiz-
Rojas, Juan Antonio Pe?rez-Ortiz, Gema Ram??rez-
Sa?nchez, Felipe Sa?nchez-Mart??nez, In?aki Alegria,
Aingeru Mayor, and Kepa Sarasola. 2005. An open-
source shallow-transfer machine translation engine for
the romance languages of Spain. In Proceedings of the
European Association for Machine Translation, pages
79?86.
Marta R. Costa-jussa` and Rafael E. Banchs. 2011. The
BM-I2R Haitian-Cre?ole-to-English translation system
description for the WMT 2011 evaluation campaign.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011.
TESLA at WMT 2011: Translation evaluation and tun-
able metric. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Michael Denkowski and Alon Lavie. 2011a. Meteor 1.3:
Automatic metric for reliable optimization and evalu-
ation of machine translation systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Michael Denkowski and Alon Lavie. 2011b. METEOR-
Tuned Phrase-Based SMT: CMU French-English and
Haitian-English Systems for WMT 2011. Technical
Report CMU-LTI-11-011, Language Technologies In-
stitute, Carnegie Mellon University.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Vladimir Eidelman, Kristy Hollingshead, and Philip
Resnik. 2011. Noisy SMS machine translation in low-
density languages. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Christian Federmann and Sabine Hunsicker. 2011.
Stochastic parse tree selection for an existing RBMT
system. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
Robert Frederking, Alexander Rudnicky, and Christopher
Hogan. 1997. Interactive speech translation in the
DIPLOMAT project. In Proceedings of the ACL-1997
Workshop on Spoken Language Translation.
Markus Freitag, Gregor Leusch, Joern Wuebker, Stephan
Peitz, Hermann Ney, Teresa Herrmann, Jan Niehues,
Alex Waibel, Alexandre Allauzen, Gilles Adda,
Josep Maria Crego, Bianka Buschbeck, Tonio Wand-
macher, and Jean Senellart. 2011. Joint WMT sub-
mission of the QUAERO project. In Proceedings of
the Sixth Workshop on Statistical Machine Translation.
Yoko Futagi, Paul Deane, Martin Chodorow, and Joel
Tetreault. 2008. A computational approach to de-
tecting collocation errors in the writing of non-native
speakers of English. Computer Assisted Language
Learning Journal.
Jesu?s Gonza?lez-Rubio and Francisco Casacuberta. 2011.
The UPV-PRHLT combination system for WMT 2011.
44
In Proceedings of the Sixth Workshop on Statistical
Machine Translation.
Greg Hanneman and Alon Lavie. 2011. CMU syntax-
based machine translation at WMT 2011. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Christian Hardmeier, Jo?rg Tiedemann, Markus Saers,
Marcello Federico, and Mathur Prashant. 2011. The
Uppsala-FBK systems at WMT 2011. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Kenneth Heafield and Alon Lavie. 2011. CMU system
combination in WMT 2011. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Teresa Herrmann, Mohammed Mediani, Jan Niehues,
and Alex Waibel. 2011. The Karlsruhe Institute of
Technology translation systems for the WMT 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation.
Sanjika Hewavitharana, Nguyen Bach, Qin Gao, Vamshi
Ambati, and Stephan Vogel. 2011. CMU Haitian
Creole-English translation system for WMT 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation.
Maria Holmqvist, Sara Stymne, and Lars Ahrenberg.
2011. Experiments with word alignment, normaliza-
tion and clause reordering for SMT between English
and German. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-
delman, Olivia Buzek, and Benjamin B. Bederson.
2011. The value of monolingual crowdsourcing in
a real-world translation scenario: Simulation using
Haitian Creole emergency SMS messages. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Matthias Huck, Joern Wuebker, Christoph Schmidt,
Markus Freitag, Stephan Peitz, Daniel Stein, Arnaud
Dagnelies, Saab Mansour, Gregor Leusch, and Her-
mann Ney. 2011. The RWTH Aachen machine trans-
lation system for WMT 2011. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Maxim Khalilov and Khalil Sima?an. 2011. ILLC-UvA
translation system for EMNLP-WMT 2011. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation, New
York, New York.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the ACL-2007 Demo and Poster Sessions,
Prague, Czech Republic.
Oliver Lacey-Hall. 2011. The guardian?s poverty matters
blog: How remote teams can help the rapid response
to disasters, March.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Gregor Leusch and Hermann Ney. 2009. Edit distances
with block movements and error rate confidence esti-
mates. Machine Translation, 23:129?140.
Gregor Leusch, Markus Freitag, and Hermann Ney.
2011. The RWTH system combination system for
WMT 2011. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
William Lewis, Robert Munro, and Stephan Vogel. 2011.
Crisis MT: Developing a cookbook for MT in crisis
situations. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
William D. Lewis. 2010. Haitian Creole: How to
build and ship an MT engine from scratch in 4 days,
17hours, & 30 minutes. In Proceedings of EAMT
2010.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with syn-
tax, semirings, discriminative training and other good-
ies. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, Upp-
sala, Sweden, July.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceedings
of the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 25?32.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of EMNLP.
Vero?nica Lo?pez-Luden?a and Rube?n San-Segundo. 2011.
UPM system for the translation task. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Matous? Macha?c?ek and Ondr?ej Bojar. 2011. Approxi-
mating a deep-syntactic metric for MT evaluation and
tuning. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
45
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, and
Ondr?ej Bojar. 2011. Two-step translation with gram-
matical post-processing. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Robert Munro. 2010. Crowdsourced translation for
emergency response in Haiti: the global collabora-
tion of local knowledge. In Proceedings of the AMTA
Workshop on Collaborative Crowdsourcing for Trans-
lation.
Douglas W. Oard and Franz Josef Och. 2003. Rapid-
response machine translation for unexpected lan-
guages. In Proceedings of MT Summit IX.
Douglas W. Oard. 2003. The surprise language exer-
cises. ACM Transactions on Asian Language Infor-
mation Processing, 2(2):79?84.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL-2002), Philadelphia, Pennsylvania.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation.
Martin Popel, David Marec?ek, Nathan Green, and
Zdene?k Z?abokrtsky?. 2011. Influence of parser choice
on dependency-based MT. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Maja Popovic?, David Vilar, Eleftherios Avramidis, and
Aljoscha Burchardt. 2011. Evaluation without ref-
erences: IBM1 scores as evaluation metrics. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Maja Popovic?. 2011. Morphemes and POS tags for n-
gram based evaluation metrics. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Marion Potet, Raphae?l Rubino, Benjamin Lecouteux,
Ste?phane Huet, Laurent Besacier, Herve? Blanchon,
and Fabrice Lefe`vre. 2011. The LIGA (LIG/LIA)
machine translation system for WMT 2011. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 ?Met-
rics for MAchine TRanslation? challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation, Honolulu, Hawaii.
Miguel Rios, Wilker Aziz, and Lucia Specia. 2011.
TINE: A metric to assess MT adequacy. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Christian Rish?j and Anders S?gaard. 2011. Factored
translation with unsupervised word clusters. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected BLEU training for
graphs: BBN system description for WMT11 system
combination task. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
V??ctor M. Sa?nchez-Cartagena, Felipe Sa?nchez-Mart??nez,
and Juan Antonio Pe?rez-Ortiz. 2011. The Univer-
sitat d?Alacant hybrid machine translation system for
WMT 2011. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Sadaf Abdul-Rauf, Haithem Afli,
and Kashif Shah. 2011. LIUM?s SMT machine trans-
lation systems for WMT 2011. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Rico Sennrich. 2011. The UZH system combination sys-
tem for WMT 2011. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Americas
(AMTA-2006), Cambridge, Massachusetts.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence level MT eval-
uation. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Ma-
chine translation evaluation versus quality estimation.
Machine Translation, 24(1):39?50.
Sara Stymne. 2011. Spell checking techniques for re-
placement of unknown words and data cleaning for
Haitian Creole SMS translation. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of preposition error detection. In Proceedings
of COLING, Manchester, UK.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the Thrax
grammar extractor. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Eric Wehrli, Luka Nerima, and Yves Scherrer. 2009.
Deep linguistic multilingual translation and bilingual
dictionaries. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 90?94.
46
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011a.
Description of the JHU system combination scheme
for WMT 2011. In Proceedings of the Sixth Workshop
on Statistical Machine Translation.
Jia Xu, Hans Uszkoreit, Casey Kennington, David Vilar,
and Xiaojun Zhang. 2011b. DFKI hybrid machine
translation system for WMT 2011 - on the integration
of SMT and RBMT. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Francisco Zamora-Martinez and Maria Jose Castro-
Bleda. 2011. CEU-UPV English-Spanish system for
WMT11. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Daniel Zeman. 2011. Hierarchical phrase-based MT at
the Charles University for the WMT 2011 shared task.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation.
47
A Pairwise System Comparisons by Human Judges
Tables 19?38 show pairwise comparisons between systems for each language pair. The numbers in each of
the tables? cells indicate the percentage of times that the system in that column was judged to be better than
the system in that row. Bolding indicates the winner of the two systems. The difference between 100 and
the sum of the complementary cells is the percent of time that the two systems were judged to be equal.
Because there were so many systems and data conditions the significance of each pairwise comparison
needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differences
(rather than differences that are attributable to chance). In the following tables ? indicates statistical signif-
icance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical significance at
p ? 0.01, according to the Sign Test.
B Automatic Scores
Tables 39?48 give the automatic scores for each of the systems.
C Meta-evaluation
Tables 49 and 50 give a detailed breakdown of intra- and inter-annotator agreement rates for all of manual
evaluation tracks of WMT11, broken down by language pair.
48
R
E
F
C
S
T
C
U
-B
O
JA
R
C
U
-Z
E
M
A
N
JH
U
O
N
L
IN
E
-B
S
Y
S
T
R
A
N
U
E
D
IN
U
P
P
S
A
L
A
REF ? .02? .04? .01? .04? .04? .04? .05? .04?
CST .88? ? .49? .36 .49? .59? .41 .58? .44?
CU-BOJAR .91? .27? ? .27? .30 .48? .28? .41? .41
CU-ZEMAN .94? .31 .49? ? .47? .67? .47? .64? .49?
JHU .89? .29? .39 .28? ? .47? .36 .41? .36
ONLINE-B .84? .20? .27? .19? .28? ? .24? .30 .27?
SYSTRAN .91? .31 .49? .30? .39 .59? ? .56? .37
UEDIN .89? .16? .25? .16? .27? .36 .23? ? .25?
UPPSALA .84? .28? .40 .24? .37 .49? .38 .45? ?
> others .89 .23 .36 .23 .33 .46 .31 .43 .33
>= others .96 .47 .60 .44 .57 .68 .51 .69 .57
Table 19: Ranking scores for entries in the Czech-English task (individual system track).
R
E
F
C
O
M
M
E
R
C
IA
L
-1
C
O
M
M
E
R
C
IA
L
-2
C
U
-B
O
JA
R
C
U
-M
A
R
E
C
E
K
C
U
-P
O
P
E
L
C
U
-T
A
M
C
H
Y
N
A
C
U
-Z
E
M
A
N
JH
U
O
N
L
IN
E
-B
U
E
D
IN
REF ? .05? .04? .04? .04? .05? .05? .04? .03? .04? .04?
COMMERCIAL-1 .91? ? .36 .53? .50? .47? .44? .33? .33? .55? .45?
COMMERCIAL-2 .87? .42 ? .52? .47? .47? .50? .30? .40 .50? .43
CU-BOJAR .89? .31? .31? ? .29 .41 .21? .19? .27? .42? .31?
CU-MARECEK .88? .31? .37? .27 ? .35? .28 .21? .30? .39 .28?
CU-POPEL .85? .33? .29? .43 .45? ? .41 .27? .31? .50? .39
CU-TAMCHYNA .87? .34? .35? .30? .32 .40 ? .22? .25? .45? .32
CU-ZEMAN .91? .47? .52? .56? .56? .55? .55? ? .44? .64? .54?
JHU .91? .43? .41 .50? .47? .51? .51? .31? ? .52? .48?
ONLINE-B .86? .27? .32? .33? .39 .33? .29? .18? .23? ? .31?
UEDIN .85? .34? .40 .40? .37? .42 .36 .24? .25? .44? ?
> others .88 .33 .34 .39 .39 .40 .36 .23 .28 .44 .35
>= others .96 .51 .51 .64 .63 .58 .62 .43 .49 .65 .59
Table 20: Ranking scores for entries in the English-Czech task (individual system track).
49
R
E
F
C
M
U
-D
Y
E
R
C
S
T
C
U
-Z
E
M
A
N
D
F
K
I-
X
U
JH
U
K
IT
K
O
C
L
IM
S
I
L
IN
G
U
A
T
E
C
L
IU
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-W
U
E
B
K
E
R
U
E
D
IN
U
P
P
S
A
L
A
REF ? .05? .02? .03? .04? .00? .08? .04? .00? .07? .05? .07? .14? .02? .08? .00? .06? .08? .02? .10? .08?
CMU-DYER .95? ? .18? .17? .33 .26? .22? .12? .29? .43 .23? .43 .54 .32 .20? .40 .43 .48 .31 .19? .18?
CST .96? .74? ? .42 .62? .35 .68? .44? .47? .78? .62? .77? .73? .81? .70? .74? .67? .53? .65? .47 .51
CU-ZEMAN .97? .67? .22 ? .56? .26? .41 .22? .48 .66? .46 .60? .62? .73? .57? .60? .62? .53? .40 .44 .48
DFKI-XU .94? .44 .06? .24? ? .10? .26 .17? .49? .47 .21? .42 .45 .52 .42 .45 .51 .39 .40 .48 .29
JHU 1.00?.61? .33 .55? .64? ? .59? .45 .51? .59 .52? .68? .63? .62? .64? .65? .58? .46 .61? .44 .38
KIT .87? .65? .12? .21 .44 .23? ? .34 .40 .54 .30 .43 .57? .44 .43 .47 .50 .53 .40 .28 .17?
KOC .96? .64? .09? .49? .66? .36 .43 ? .43 .69? .57? .69? .63? .62? .41 .63? .59 .52? .51 .59? .40
LIMSI .96? .54? .24? .30 .22? .25? .38 .27 ? .63? .52 .43 .55? .43 .43 .59? .47 .40 .41 .32 .44
LINGUATEC .91? .45 .13? .24? .38 .32 .34 .18? .27? ? .26? .45 .62? .46 .20? .49 .53 .36 .41 .32? .29?
LIU .89? .49? .14? .29 .54? .25? .48 .24? .31 .64? ? .47 .61? .52 .46 .48 .50 .23? .48 .37 .36
ONLINE-A .88? .47 .12? .25? .42 .18? .41 .19? .39 .39 .30 ? .32 .26? .28 .46 .36 .35 .42 .19? .27?
ONLINE-B .78? .38 .16? .23? .33 .28? .26? .16? .26? .29? .22? .38 ? .23? .23? .29? .29? .22? .27 .22? .18?
RBMT-1 .96? .42 .09? .18? .35 .21? .51 .23? .43 .41 .38 .56? .62? ? .31 .46 .39 .13 .48 .50 .30?
RBMT-2 .86? .54? .15? .28? .48 .29? .43 .41 .39 .55? .44 .51 .64? .43 ? .55? .47 .54? .44 .41 .29?
RBMT-3 .92? .42 .11? .27? .32 .23? .47 .18? .19? .34 .38 .49 .55? .38 .26? ? .36 .29? .34 .33 .28?
RBMT-4 .88? .36 .19? .24? .38 .29? .43 .38 .45 .32 .37 .44 .56? .33 .34 .45 ? .35 .29? .51 .24?
RBMT-5 .92? .45 .27? .27? .45 .32 .37 .27? .47 .47 .61? .55 .67? .26 .24? .53? .46 ? .45 .47 .39
RWTH-WUEBKER .93? .50 .23? .26 .33 .20? .24 .36 .41 .44 .39 .47 .55 .44 .38 .53 .56? .45 ? .21 .39
UEDIN .88? .59? .24 .28 .28 .33 .50 .24? .45 .65? .40 .67? .62? .34 .39 .52 .41 .36 .43 ? .48
UPPSALA .92? .64? .27 .29 .39 .44 .58? .32 .41 .66? .53 .68? .69? .59? .59? .58? .61? .54 .36 .31 ?
> others .92 .50 .17 .28 .40 .26 .40 .26 .38 .51 .40 .51 .57 .43 .38 .49 .47 .39 .41 .36 .32
>= others .95 .66 .37 .47 .60 .43 .57 .45 .56 .63 .57 .66 .72 .60 .54 .64 .61 .56 .59 .55 .47
Table 21: Ranking scores for entries in the German-English task (individual system track).
50
R
E
F
C
O
P
E
N
H
A
G
E
N
C
U
-T
A
M
C
H
Y
N
A
C
U
-Z
E
M
A
N
D
F
K
I-
F
E
D
E
R
M
A
N
N
D
F
K
I-
X
U
IL
L
C
-U
V
A
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-F
R
E
IT
A
G
U
E
D
IN
U
O
W
U
P
P
S
A
L
A
REF ? .08? .06? .00? .13? .02? .05? .05? .02? .02? .16? .06? .11? .07? .14? .14? .19? .11? .11? .16? .07? .07? .08?
COPENHAGEN .85? ? .31 .09? .60? .39 .25 .32 .41 .27 .36 .34 .49? .61? .56? .61? .64? .64? .60 .26 .49 .30 .16
CU-TAMCHYNA .92? .37 ? .13? .61? .48? .30 .38 .58? .33 .39 .41? .55? .57? .72? .69? .81? .49 .59? .47 .39 .40 .43
CU-ZEMAN 1.00?.60? .41? ? .76? .78? .51? .47? .64? .53? .66? .49? .77? .68? .69? .64? .70? .64? .72? .55? .47 .44 .50
DFKI-FEDERMANN .72? .19? .17? .16? ? .39 .25? .38 .38 .24? .32 .29 .35 .40 .43 .33 .39 .19 .33? .22? .31 .11? .30
DFKI-XU .84? .31 .21? .08? .37 ? .25? .32 .34 .12? .37 .30 .35 .47 .54? .30 .51? .43 .37 .20? .22? .25? .14?
ILLC-UVA .90? .39 .37 .25? .63? .50? ? .41? .58? .35 .56? .38 .55? .63? .61? .63? .71? .75? .62? .33 .56? .38 .41
JHU .91? .45 .27 .27? .41 .40 .20? ? .37 .27 .43 .50? .58? .59? .43 .55? .72? .50 .50 .50? .47 .46 .22?
KIT .87? .24 .23? .17? .41 .43 .26? .37 ? .16? .51 .27? .37 .45? .47 .39 .58? .53 .47 .23? .24 .21? .17?
KOC .95? .35 .35 .13? .61? .65? .38 .42 .57? ? .47? .33 .47? .62? .61? .53? .64? .63? .45 .20 .38 .37 .18?
LIMSI .77? .31 .26 .11? .48 .35 .18? .30 .33 .23? ? .36 .39 .50? .52 .47 .48 .39 .42 .18? .22? .28 .14?
LIU .84? .32 .20? .25? .51 .38 .26 .21? .51? .35 .39 ? .51 .49? .63? .52? .56 .48? .56 .29 .38 .25 .25
ONLINE-A .75? .21? .24? .09? .48 .41 .22? .30? .37 .25? .37 .37 ? .46 .37 .41 .47 .33 .44 .27? .28 .22? .16?
ONLINE-B .91? .17? .15? .13? .44 .22 .17? .16? .20? .15? .24? .25? .27 ? .43 .35 .48 .33 .17? .17? .26 .12? .20?
RBMT-1 .80? .23? .11? .20? .37 .28? .18? .29 .38 .25? .36 .30? .41 .38 ? .34 .45 .36 .02? .17? .17? .28? .24?
RBMT-2 .80? .20? .10? .16? .43 .38 .20? .27? .45 .22? .36 .30? .38 .51 .43 ? .48 .40 .42 .31? .28? .16? .25?
RBMT-3 .65? .18? .14? .15? .37 .29? .17? .22? .25? .20? .27 .33 .33 .29 .30 .31 ? .34 .16? .24? .35 .20? .11?
RBMT-4 .80? .21? .28 .22? .19 .26 .09? .32 .29 .27? .39 .27? .43 .44 .38 .38 .45 ? .42 .29? .36 .27? .31?
RBMT-5 .88? .35 .31? .15? .54? .51 .26? .34 .36 .36 .44 .35 .44 .59? .37? .33 .62? .38 ? .29 .45 .38 .30
RWTH-FREITAG .80? .31 .27 .17? .62? .55? .19 .25? .56? .30 .49? .41 .53? .59? .56? .53? .62? .57? .45 ? .36 .38 .24
UEDIN .82? .27 .27 .27 .46 .47? .17? .28 .36 .33 .48? .27 .47 .43 .75? .55? .52 .50 .43 .21 ? .35 .27
UOW .86? .39 .21 .23 .74? .53? .36 .38 .64? .20 .38 .41 .74? .61? .56? .64? .57? .65? .38 .26 .41 ? .31
UPPSALA .79? .32 .35 .29 .54 .57? .34 .51? .51? .45? .53? .43 .73? .70? .55? .64? .77? .57? .55 .43 .33 .41 ?
> others .84 .29 .24 .17 .48 .42 .24 .31 .42 .27 .40 .34 .46 .51 .51 .47 .56 .46 .41 .29 .34 .29 .25
>= others .91 .56 .50 .38 .68 .67 .48 .54 .64 .53 .65 .59 .65 .730 .70 .66 .732 .66 .58 .56 .60 .53 .49
Table 22: Ranking scores for entries in the English-German task (individual system track).
R
E
F
A
L
A
C
A
N
T
C
U
-Z
E
M
A
N
H
Y
D
E
R
A
B
A
D
K
O
C
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
S
Y
S
T
R
A
N
U
E
D
IN
U
FA
L
-U
M
U
P
M
REF ? .03? .02? .00? .02? .03? .12? .15? .04? .07? .05? .02? .03? .03? .03? .07?
ALACANT .86? ? .07? .08? .30 .52 .31 .27? .29? .54 .49 .32? .51 .27? .26? .26?
CU-ZEMAN .98? .89? ? .48 .84? .85? .94? .90? .83? .87? .85? .78? .97? .79? .79? .91?
HYDERABAD .98? .86? .27 ? .88? .95? .92? .85? .96? .74? .82? .80? .88? .91? .80? .86?
KOC .93? .48 .06? .06? ? .28 .39 .40 .34 .44 .38 .26? .59? .22? .20? .18?
ONLINE-A .90? .28 .02? .02? .48 ? .32 .34 .34 .26? .34 .19? .35 .20? .11? .20?
ONLINE-B .79? .33 .04? .00? .47 .30 ? .24? .31? .31? .27? .25? .33 .27? .21? .07?
RBMT-1 .81? .52? .05? .11? .50 .57 .62? ? .50 .36 .34 .17 .40 .39 .34 .30?
RBMT-2 .96? .61? .09? .04? .52 .47 .59? .37 ? .39 .46 .27 .58? .29? .24? .45
RBMT-3 .88? .31 .09? .13? .44 .56? .60? .53 .37 ? .47 .14? .52 .40 .23? .31
RBMT-4 .90? .38 .08? .16? .50 .53 .60? .41 .43 .38 ? .43 .52 .33? .18? .22?
RBMT-5 .94? .61? .06? .10? .54? .70? .63? .37 .45 .59? .41 ? .66? .42 .50 .43
SYSTRAN .92? .33 .02? .10? .25? .53 .53 .42 .30? .36 .38 .27? ? .21? .41 .24?
UEDIN .95? .63? .13? .02? .63? .67? .59? .47 .61? .53 .59? .42 .53? ? .32? .45
UFAL-UM .94? .63? .10? .11? .56? .70? .74? .51 .61? .59? .74? .36 .47 .61? ? .44
UPM .85? .54? .02? .03? .62? .61? .81? .59? .45 .55 .68? .40 .60? .42 .38 ?
> others .91 .51 .07 .10 .52 .56 .59 .48 .48 .47 .48 .35 .54 .39 .34 .36
>= others .96 .66 .16 .17 .67 .723 .723 .63 .60 .61 .60 .51 .66 .51 .47 .50
Table 23: Ranking scores for entries in the Spanish-English task (individual system track).
51
R
E
F
C
E
U
-U
P
V
C
U
-Z
E
M
A
N
K
O
C
O
N
L
IN
E
-A
O
N
L
IN
E
-B
P
R
O
M
T
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
U
E
D
IN
U
O
W
U
P
M
U
P
P
S
A
L
A
REF ? .06? .03? .09? .09? .09? .05? .03? .06? .04? .08? .02? .08? .02? .03? .04?
CEU-UPV .84? ? .21? .20? .43 .36 .42 .37 .34? .50? .31 .34 .32 .21? .13? .22
CU-ZEMAN .87? .56? ? .38? .56? .56? .58? .46? .40 .70? .46? .49? .51? .45? .19? .49?
KOC .84? .41? .22? ? .56? .51? .48? .54? .39 .55? .42 .35 .51? .44 .11? .34
ONLINE-A .72? .31 .24? .15? ? .36 .37 .28? .23? .35 .25? .20? .29? .25? .08? .09?
ONLINE-B .72? .30 .17? .18? .26 ? .29 .23? .20? .37 .20? .19? .19? .22? .02? .23?
PROMT .76? .29 .21? .25? .42 .43 ? .24? .24 .19 .27? .26? .32 .25? .18? .21?
RBMT-1 .85? .37 .29? .23? .51? .54? .48? ? .35 .45? .40? .05? .47 .39 .25? .39
RBMT-2 .86? .50? .35 .38 .51? .48? .35 .39 ? .41? .34 .36 .45 .36 .23? .41
RBMT-3 .86? .26? .18? .22? .40 .35 .19 .20? .22? ? .25? .23? .24? .33 .10? .22?
RBMT-4 .80? .45 .29? .34 .53? .51? .43? .21? .38 .43? ? .24? .34 .30 .20? .45?
RBMT-5 .96? .43 .29? .42 .57? .61? .46? .22? .38 .49? .47? ? .50 .46 .27? .47
UEDIN .74? .28 .20? .21? .46? .48? .43 .37 .31 .49? .45 .35 ? .20? .14? .23
UOW .90? .44? .18? .32 .46? .52? .56? .39 .39 .44 .45 .36 .38? ? .10? .32
UPM .93? .65? .53? .67? .74? .71? .69? .59? .51? .74? .60? .51? .64? .68? ? .62?
UPPSALA .84? .36 .21? .32 .49? .42? .45? .39 .35 .45? .29? .41 .35 .30 .15? ?
> others .83 .38 .24 .30 .47 .46 .41 .33 .32 .43 .35 .29 .38 .33 .14 .31
>= others .94 .65 .49 .56 .72 .74 .70 .60 .57 .71 .61 .54 .64 .59 .34 .61
Table 24: Ranking scores for entries in the English-Spanish task (individual system track).
R
E
F
C
M
U
-D
E
N
K
O
W
S
K
I
C
M
U
-H
A
N
N
E
M
A
N
C
U
-Z
E
M
A
N
JH
U
K
IT
L
IA
-L
IG
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-H
U
C
K
S
Y
S
T
R
A
N
U
E
D
IN
REF ? .10? .18? .06? .03? .14? .15? .14? .14? .12? .05? .12? .09? .05? .06? .05? .05? .07? .02?
CMU-DENKOWSKI .79? ? .35 .12? .34 .32 .41 .35 .21? .47? .46 .49 .32 .33 .36 .35 .25 .45 .29
CMU-HANNEMAN .79? .35 ? .17? .29 .44? .43 .52? .45 .45 .49 .51 .39 .44 .38 .35 .35 .43 .37
CU-ZEMAN .94? .61? .67? ? .54? .66? .66? .58? .60? .59? .88? .62? .59? .63? .60? .56 .68? .64? .40
JHU .82? .34 .29 .22? ? .26 .54? .40 .36 .43 .40 .49 .42 .40 .34 .35 .36 .47 .20?
KIT .79? .39 .20? .16? .40 ? .26? .46 .34 .38 .52 .38 .35 .39 .28 .38 .15? .32 .30
LIA-LIG .75? .24 .31 .28? .24? .59? ? .49 .27 .40 .46 .35 .26 .31? .29 .32 .32 .33? .35
LIMSI .86? .30 .25? .21? .31 .26 .26 ? .38 .40 .42 .35 .18? .43 .34 .16? .34 .34 .33
LIUM .78? .45? .33 .16? .38 .34 .44 .40 ? .38 .30 .44 .26? .33? .38 .28 .29 .33 .28
ONLINE-A .80? .23? .21 .22? .37 .35 .36 .33 .46 ? .43 .35 .16? .33 .24? .20? .26 .34 .27?
ONLINE-B .86? .37 .31 .04? .46 .22 .36 .33 .43 .26 ? .40 .20? .16? .44 .20? .41 .38 .22?
RBMT-1 .87? .44 .35 .23? .46 .44 .54 .48 .44 .53 .54 ? .39 .37 .33 .11? .39 .17? .35
RBMT-2 .84? .47 .37 .26? .40 .50 .45 .52? .54? .58? .67? .45 ? .51 .35 .22? .51 .57 .41
RBMT-3 .89? .44 .42 .19? .40 .43 .54? .46 .61? .50 .71? .37 .32 ? .42 .35 .42 .47 .40
RBMT-4 .85? .53 .36 .26? .51 .47 .55 .52 .46 .59? .40 .43 .50 .42 ? .34 .46 .44 .41
RBMT-5 .93? .58 .55 .33 .54 .54 .59 .70? .56 .66? .65? .36? .54? .46 .37 ? .50 .54? .54
RWTH-HUCK .92? .43 .38 .14? .36 .59? .41 .44 .29 .53 .48 .46 .30 .46 .32 .38 ? .37 .17?
SYSTRAN .93? .39 .38 .24? .44 .48 .60? .50 .40 .55 .57 .45? .36 .29 .44 .21? .49 ? .36
UEDIN .93? .48 .41 .40 .51? .48 .54 .49 .46 .60? .57? .52 .37 .47 .39 .39 .51? .52 ?
> others .85 .39 .36 .21 .39 .41 .46 .46 .41 .46 .50 .41 .33 .39 .35 .28 .37 .39 .32
>= others .91 .62 .58 .37 .61 .64 .64 .661 .63 .661 .66 .58 .52 .55 .53 .45 .58 .54 .50
Table 25: Ranking scores for entries in the French-English task (individual system track).
52
R
E
F
C
U
-Z
E
M
A
N
JH
U
K
IT
L
A
T
L
-G
E
N
E
V
A
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-H
U
C
K
U
E
D
IN
U
P
P
S
A
L
A
U
P
P
S
A
L
A
-F
B
K
REF ? .07? .06? .25? .07? .13? .20? .15? .20? .10? .09? .18? .11? .12? .14? .18? .16? .16?
CU-ZEMAN .92? ? .83? .86? .63? .85? .90? .86? .81? .89? .70? .75? .75? .61? .78? .79? .81? .81?
JHU .91? .07? ? .55? .30? .60? .50? .55? .59? .45 .41 .34? .30? .50 .40 .42 .42 .44
KIT .63? .04? .29? ? .18? .47 .37 .30? .37 .38 .30? .37 .24? .34 .28 .34 .24? .13?
LATL-GENEVA .86? .29? .54? .73? ? .77? .67? .71? .79? .55? .39 .66? .52 .58? .58? .51 .52 .58?
LIMSI .75? .04? .21? .29 .13? ? .23? .28? .37 .27? .27? .24? .24? .21? .27? .28? .25? .31
LIUM .76? .04? .26? .44 .24? .46? ? .33 .52 .48 .25? .36 .25? .28? .43 .40 .35 .32
ONLINE-A .78? .10? .31? .51? .22? .51? .46 ? .44 .39 .36 .41 .30? .41 .41 .32? .46 .33
ONLINE-B .70? .06? .27? .41 .13? .39 .32 .30 ? .47 .22? .26? .13? .28? .32 .26? .33 .27?
RBMT-1 .83? .07? .38 .46 .23? .56? .39 .41 .42 ? .17? .34 .36 .13 .52 .33? .40 .40
RBMT-2 .88? .25? .47 .59? .37 .65? .63? .51 .57? .54? ? .58? .39 .54? .63? .61? .47 .42
RBMT-3 .80? .19? .54? .42 .20? .60? .47 .44 .52? .42 .18? ? .21? .43 .51 .55 .41 .39
RBMT-4 .82? .22? .54? .63? .33 .63? .64? .54? .59? .41 .44 .46? ? .47 .68? .53 .42 .39
RBMT-5 .86? .18? .46 .53 .20? .62? .56? .46 .61? .22 .33? .40 .34 ? .43 .52 .40 .53?
RWTH-HUCK .76? .08? .33 .38 .21? .60? .40 .38 .43 .36 .18? .37 .21? .38 ? .39 .22? .29
UEDIN .78? .15? .37 .46 .34 .49? .38 .53? .58? .56? .33? .35 .36 .37 .47 ? .38 .31
UPPSALA .77? .07? .36 .53? .36 .49? .46 .46 .56 .46 .38 .42 .39 .55 .57? .39 ? .47
UPPSALA-FBK .80? .10? .40 .71? .27? .50 .47 .51 .53? .42 .48 .41 .52 .29? .50 .47 .40 ?
> others .80 .12 .39 .51 .25 .55 .48 .45 .52 .43 .32 .41 .33 .39 .46 .43 .39 .38
>= others .86 .20 .55 .69 .39 .73 .64 .60 .70 .61 .46 .58 .49 .55 .65 .58 .55 .54
Table 26: Ranking scores for entries in the English-French task (individual system track).
R
E
F
B
M
-I
2R
C
M
U
-D
E
N
K
O
W
S
K
I
C
M
U
-H
E
W
A
V
IT
H
A
R
A
N
A
H
Y
D
E
R
A
B
A
D
K
O
C
L
IU
U
M
D
-E
ID
E
L
M
A
N
U
M
D
-H
U
U
P
P
S
A
L
A
REF ? .03? .01? .03? .02? .01? .00? .01? .01? .02?
BM-I2R .91? ? .28? .27? .13? .08? .19? .30? .30? .24?
CMU-DENKOWSKI .93? .44? ? .25 .22? .15? .28? .33 .29? .31?
CMU-HEWAVITHARANA .91? .40? .31 ? .21? .16? .29? .35 .39 .30
HYDERABAD .96? .71? .59? .58? ? .27? .56? .57? .42 .52?
KOC .94? .78? .75? .64? .55? ? .65? .69? .62? .64?
LIU .92? .56? .42? .44? .27? .24? ? .43 .41 .39
UMD-EIDELMAN .94? .44? .35 .35 .17? .17? .34 ? .37 .31?
UMD-HU .90? .50? .57? .45 .35 .21? .46 .45 ? .42
UPPSALA .93? .48? .47? .39 .31? .20? .40 .43? .37 ?
> others .93 .49 .42 .39 .25 .17 .35 .40 .36 .35
>= others .98 .71 .66 .64 .43 .31 .55 .63 .52 .57
Table 27: Ranking scores for entries in the Haitian Creole (Clean)-English task (individual system track).
53
R
E
F
B
M
-I
2R
C
M
U
-D
E
N
K
O
W
S
K
I
C
M
U
-H
E
W
A
V
IT
H
A
R
A
N
A
JH
U
L
IU
U
M
D
-E
ID
E
L
M
A
N
REF ? .05? .03? .04? .02? .02? .03?
BM-I2R .83? ? .29? .25? .22? .30? .30?
CMU-DENKOWSKI .89? .44? ? .37? .23? .37 .30?
CMU-HEWAVITHARANA .86? .43? .26? ? .27? .37 .32
JHU .96? .62? .53? .49? ? .52? .47?
LIU .92? .48? .38 .34 .31? ? .36
UMD-EIDELMAN .92? .48? .44? .42 .29? .41 ?
> others .90 .43 .34 .33 .23 .34 .30
>= others .97 .65 .59 .60 .41 .55 .52
Table 28: Ranking scores for entries in the Haitian Creole (Raw)-English task (individual system track).
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .01? .02? .01? .01?
BBN-COMBO .91? ? .25 .18? .16?
CMU-HEAFIELD-COMBO .90? .24 ? .17? .12?
JHU-COMBO .92? .27? .29? ? .20?
UPV-PRHLT-COMBO .94? .41? .42? .36? ?
> others .92 .23 .24 .18 .12
>= others .99 .62 .64 .58 .47
Table 29: Ranking scores for entries in the Czech-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .04? .04?
CMU-HEAFIELD-COMBO .86? ? .17?
UPV-PRHLT-COMBO .88? .30? ?
> others .87 .17 .11
>= others .96 .48 .41
Table 30: Ranking scores for entries in the English-Czech task (system combination track).
54
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
Q
U
A
E
R
O
-C
O
M
B
O
R
W
T
H
-L
E
U
S
C
H
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
U
Z
H
-C
O
M
B
O
REF ? .11? .09? .04? .09? .10? .14? .05? .09?
BBN-COMBO .79? ? .45? .32 .21? .28? .39 .31? .36
CMU-HEAFIELD-COMBO .84? .23? ? .21? .17? .19? .25? .19? .31
JHU-COMBO .85? .42 .55? ? .25? .28? .40? .28? .47?
KOC-COMBO .83? .56? .62? .45? ? .41 .54? .40? .51?
QUAERO-COMBO .86? .52? .64? .45? .36 ? .54? .49? .48
RWTH-LEUSCH-COMBO .83? .28 .41? .22? .20? .22? ? .22? .38
UPV-PRHLT-COMBO .85? .47? .57? .42? .25? .26? .48? ? .49?
UZH-COMBO .86? .34 .38 .31? .29? .32 .41 .30? ?
> others .84 .36 .46 .30 .22 .26 .39 .27 .39
>= others .91 .61 .70 .56 .45 .46 .65 .52 .60
Table 31: Ranking scores for entries in the German-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
U
Z
H
-C
O
M
B
O
REF ? .11? .09? .10? .11?
CMU-HEAFIELD-COMBO .81? ? .19? .23? .32
KOC-COMBO .84? .48? ? .38? .47?
UPV-PRHLT-COMBO .81? .36? .23? ? .37?
UZH-COMBO .80? .34 .24? .31? ?
> others .81 .320 .19 .25 .318
>= others .90 .61 .46 .56 .58
Table 32: Ranking scores for entries in the English-German task (system combination track).
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-L
E
U
S
C
H
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .05? .09? .05? .07? .06? .08?
BBN-COMBO .81? ? .34 .27 .21? .27 .26
CMU-HEAFIELD-COMBO .84? .31 ? .18? .15? .29 .20
JHU-COMBO .83? .25 .32? ? .27 .35? .25
KOC-COMBO .84? .39? .39? .32 ? .39? .31?
RWTH-LEUSCH-COMBO .81? .24 .23 .16? .17? ? .14?
UPV-PRHLT-COMBO .77? .30 .26 .27 .22? .35? ?
> others .82 .25 .27 .21 .18 .28 .21
>= others .93 .64 .67 .62 .56 .71 .64
Table 33: Ranking scores for entries in the Spanish-English task (system combination track).
55
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
U
O
W
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .10? .07? .09? .08?
CMU-HEAFIELD-COMBO .70? ? .15? .21? .17?
KOC-COMBO .76? .35? ? .36? .19
UOW-COMBO .72? .29? .22? ? .25?
UPV-PRHLT-COMBO .76? .35? .16 .35? ?
> others .73 .27 .15 .25 .17
>= others .91 .69 .58 .63 .59
Table 34: Ranking scores for entries in the English-Spanish task (system combination track).
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
L
IU
M
-C
O
M
B
O
R
W
T
H
-L
E
U
S
C
H
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .04? .04? .06? .06? .06? .02?
BBN-COMBO .82? ? .35 .25 .18? .21? .21?
CMU-HEAFIELD-COMBO .90? .29 ? .30 .20? .29 .25?
JHU-COMBO .83? .35 .40 ? .31? .36 .21?
LIUM-COMBO .83? .42? .40? .44? ? .38? .35
RWTH-LEUSCH-COMBO .83? .34? .29 .30 .22? ? .21?
UPV-PRHLT-COMBO .91? .49? .40? .34? .30 .40? ?
> others .85 .32 .31 .28 .21 .28 .21
>= others .95 .67 .62 .59 .53 .63 .53
Table 35: Ranking scores for entries in the French-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .11? .11?
CMU-HEAFIELD-COMBO .74? ? .23?
UPV-PRHLT-COMBO .77? .38? ?
> others .76 .24 .17
>= others .89 .51 .43
Table 36: Ranking scores for entries in the English-French task (system combination track).
56
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .01? .01? .01?
CMU-HEAFIELD-COMBO .94? ? .29? .21?
KOC-COMBO .96? .48? ? .41?
UPV-PRHLT-COMBO .94? .34? .29? ?
> others .95 .28 .20 .21
>= others .99 .52 .38 .48
Table 37: Ranking scores for entries in the Haitian Creole (Clean)-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .02? .02?
CMU-HEAFIELD-COMBO .83? ? .24
UPV-PRHLT-COMBO .86? .29 ?
> others .84 .16 .13
>= others .98 .47 .43
Table 38: Ranking scores for entries in the Haitian Creole (Raw)-English task (system combination track).
57
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Czech-English News Task
BBN-COMBO 0.24 0.24 0.25 0.29 0.31 0.19 ?9627 ?10667 1.97 0.53 0.49 0.61 0.34 ?65 44 0.48 0.03 0.51 43
CMU-HEAFIELD-COMBO 0.24 0.24 0.24 0.28 0.3 0.18 ?9604 ?10933 1.97 0.54 0.5 0.60 0.33 ?65 43 0.48 0.03 0.52 42
CST 0.19 0.19 0.2 0.16 0.21 0.10 ?27410 ?27880 1.94 0.64 0.40 0.5 0.28 ?65 34 0.38 0.02 0.42 33
CU-BOJAR 0.21 0.21 0.22 0.19 0.24 0.13 ?23441 ?22289 1.95 0.64 0.44 0.55 0.30 ?65 37 0.42 0.02 0.46 36
CU-ZEMAN 0.20 0.2 0.21 0.14 0.21 0.11 ?33520 ?30938 1.93 0.66 0.38 0.52 0.29 ?66 31 0.37 0.02 0.40 30
JHU 0.22 0.21 0.22 0.2 0.25 0.13 ?21278 ?20480 1.95 0.60 0.43 0.55 0.30 ?65 37 0.42 0.02 0.46 36
JHU-COMBO 0.24 0.23 0.24 0.29 0.31 0.19 ?12563 ?12688 1.97 0.53 0.5 0.60 0.33 ?65 44 0.48 0.03 0.52 43
ONLINE-B 0.24 0.23 0.24 0.29 0.31 0.19 ?10673 ?11506 1.97 0.52 0.50 0.60 0.33 ?65 44 0.49 0.03 0.52 43
SYSTRAN 0.20 0.2 0.21 0.18 0.22 0.11 ?23996 ?24570 1.94 0.63 0.42 0.52 0.29 ?65 36 0.4 0.02 0.45 34
UEDIN 0.22 0.22 0.23 0.22 0.26 0.14 ?14958 ?15342 1.96 0.59 0.45 0.57 0.31 ?65 40 0.44 0.03 0.48 39
UPPSALA 0.21 0.20 0.21 0.20 0.23 0.12 ?22233 ?22509 1.95 0.62 0.43 0.53 0.29 ?65 37 0.41 0.02 0.46 36
UPV-PRHLT-COMBO 0.24 0.23 0.24 0.29 0.31 0.19 ?13904 ?15260 1.97 0.54 0.49 0.60 0.33 ?65 44 0.48 0.03 0.52 43
Table 39: Automatic evaluation metric scores for systems in the WMT11 Czech-English News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
D
F
K
I-
PA
R
S
E
C
O
N
F
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
German-English News Task
BBN-COMBO 0.23 0.22 0.23 0.25 0.28 0.16 ?17103 ?17837 1.97 0.56 0.46 0.06 0.59 0.32 ?43 42 0.46 0.03 0.49 41
CMU-DYER 0.21 0.21 0.22 0.22 0.25 0.13 ?26089 ?29214 1.95 0.59 0.44 0.04 0.56 0.31 ?45 39 0.43 0.03 0.47 38
CMU-HEAFIELD-COMBO 0.23 0.22 0.23 0.24 0.27 0.15 ?12868 ?16156 1.96 0.57 0.47 0.07 0.58 0.32 ?44 41 0.46 0.03 0.51 40
CST 0.19 0.18 0.19 0.17 0.22 0.11 ?61131 ?60157 1.94 0.63 0.39 0.03 0.5 0.27 ?46 34 0.37 0.02 0.41 33
CU-ZEMAN 0.2 0.19 0.20 0.14 0.22 0.11 ?64860 ?61329 1.93 0.65 0.37 0.06 0.51 0.28 ?47 31 0.37 0.02 0.4 30
DFKI-XU 0.21 0.20 0.21 0.21 0.25 0.14 ?40171 ?39455 1.95 0.58 0.44 0.03 0.54 0.3 ?45 38 0.42 0.02 0.46 37
JHU 0.19 0.19 0.2 0.17 0.22 0.11 ?62997 ?58673 1.94 0.64 0.39 0.03 0.51 0.28 ?45 34 0.38 0.02 0.41 33
JHU-COMBO 0.22 0.22 0.23 0.24 0.27 0.15 ?30492 ?27016 1.96 0.57 0.46 0.04 0.57 0.31 ?44 41 0.45 0.03 0.48 39
KIT 0.21 0.21 0.22 0.22 0.25 0.13 ?31064 ?31930 1.95 0.6 0.44 0.05 0.55 0.31 ?44 39 0.43 0.02 0.47 37
KOC 0.2 0.2 0.20 0.18 0.23 0.12 ?52337 ?50231 1.94 0.63 0.41 0.05 0.52 0.29 ?45 35 0.39 0.02 0.43 34
KOC-COMBO 0.21 0.21 0.21 0.22 0.26 0.14 ?40002 ?38374 1.96 0.59 0.44 0.03 0.54 0.3 ?44 38 0.42 0.02 0.46 37
LIMSI 0.21 0.20 0.21 0.20 0.24 0.13 ?39419 ?38297 1.95 0.61 0.43 0.04 0.54 0.3 ?44 38 0.42 0.02 0.46 36
LINGUATEC 0.19 0.19 0.2 0.16 0.22 0.11 ?26064 ?31116 1.94 0.68 0.42 0.15 0.53 0.29 ?46 35 0.42 0.02 0.47 34
LIU 0.21 0.20 0.21 0.2 0.24 0.13 ?40281 ?40496 1.95 0.62 0.43 0.04 0.53 0.29 ?44 37 0.41 0.02 0.45 36
ONLINE-A 0.22 0.21 0.22 0.21 0.26 0.14 ?25411 ?25675 1.95 0.6 0.45 0.06 0.57 0.31 ?44 39 0.45 0.03 0.48 38
ONLINE-B 0.22 0.22 0.23 0.23 0.27 0.15 ?15149 ?19578 1.96 0.58 0.46 0.06 0.57 0.32 ?44 41 0.46 0.03 0.5 39
QUAERO-COMBO 0.21 0.21 0.22 0.22 0.26 0.14 ?34486 ?33449 1.96 0.58 0.45 0.03 0.55 0.30 ?44 39 0.43 0.03 0.47 38
RBMT-1 0.20 0.2 0.21 0.16 0.21 0.11 ?32960 ?34972 1.94 0.67 0.42 0.08 0.52 0.29 ?45 36 0.42 0.02 0.46 34
RBMT-2 0.19 0.19 0.2 0.15 0.2 0.1 ?40842 ?43413 1.94 0.69 0.4 0.11 0.50 0.28 ?45 34 0.4 0.02 0.44 33
RBMT-3 0.20 0.2 0.21 0.17 0.22 0.11 ?32476 ?33417 1.94 0.65 0.42 0.09 0.53 0.29 ?44 36 0.42 0.02 0.47 35
RBMT-4 0.20 0.2 0.21 0.17 0.22 0.11 ?34287 ?34604 1.94 0.66 0.42 0.08 0.52 0.29 ?45 36 0.42 0.02 0.47 35
RBMT-5 0.19 0.19 0.20 0.15 0.20 0.10 ?49097 ?46635 1.94 0.68 0.40 0.07 0.50 0.28 ?46 34 0.4 0.02 0.44 33
RWTH-LEUSCH-COMBO 0.22 0.22 0.23 0.24 0.28 0.16 ?22878 ?22089 1.96 0.56 0.46 0.03 0.58 0.32 ?44 41 0.45 0.03 0.49 40
RWTH-WUEBKER 0.21 0.20 0.21 0.21 0.24 0.13 ?35973 ?37140 1.95 0.60 0.44 0.04 0.54 0.3 ?45 38 0.42 0.02 0.45 37
UEDIN 0.21 0.20 0.21 0.19 0.23 0.12 ?32791 ?34633 1.95 0.63 0.43 0.07 0.54 0.3 ?45 37 0.42 0.02 0.46 36
UPPSALA 0.20 0.2 0.21 0.2 0.23 0.12 ?40448 ?41548 1.95 0.63 0.42 0.06 0.53 0.29 ?45 37 0.41 0.02 0.44 36
UPV-PRHLT-COMBO 0.22 0.21 0.22 0.23 0.27 0.15 ?33413 ?31778 1.96 0.58 0.45 0.03 0.57 0.31 ?44 40 0.44 0.03 0.48 39
UZH-COMBO 0.22 0.21 0.22 0.23 0.27 0.15 ?16326 ?20831 1.96 0.58 0.45 0.07 0.57 0.31 ?44 40 0.45 0.03 0.48 39
Table 40: Automatic evaluation metric scores for systems in the WMT11 German-English News Task
(newssyscombtest2011)
58
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
French-English News Task
BBN-COMBO 0.25 0.25 0.26 0.31 0.32 0.21 ?19552 ?22107 1.98 0.48 0.51 0.64 0.36 ?43 47 0.49 0.03 0.54 46
CMU-DENKOWSKI 0.24 0.24 0.24 0.26 0.29 0.17 ?34357 ?37807 1.97 0.53 0.48 0.61 0.34 ?45 43 0.46 0.03 0.50 42
CMU-HANNEMAN 0.24 0.23 0.24 0.27 0.29 0.17 ?33662 ?37698 1.97 0.52 0.49 0.60 0.33 ?45 44 0.46 0.03 0.51 42
CMU-HEAFIELD-COMBO 0.25 0.25 0.25 0.30 0.31 0.2 ?18365 ?22937 1.98 0.5 0.51 0.63 0.35 ?44 46 0.49 0.03 0.54 45
CU-ZEMAN 0.22 0.22 0.23 0.17 0.24 0.13 ?67586 ?64688 1.94 0.6 0.41 0.56 0.31 ?47 34 0.39 0.02 0.42 33
JHU 0.24 0.24 0.24 0.25 0.29 0.17 ?41567 ?39578 1.96 0.53 0.47 0.61 0.34 ?45 42 0.46 0.03 0.5 41
JHU-COMBO 0.25 0.25 0.25 0.31 0.32 0.20 ?32785 ?31712 1.98 0.49 0.50 0.63 0.35 ?43 47 0.48 0.03 0.53 45
KIT 0.25 0.24 0.25 0.29 0.31 0.19 ?22678 ?28283 1.98 0.51 0.50 0.63 0.35 ?44 46 0.49 0.03 0.53 44
LIA-LIG 0.25 0.24 0.25 0.29 0.3 0.18 ?34063 ?34716 1.97 0.52 0.49 0.62 0.34 ?44 45 0.48 0.03 0.52 44
LIMSI 0.25 0.24 0.25 0.28 0.29 0.18 ?26269 ?29363 1.97 0.52 0.5 0.62 0.34 ?44 45 0.48 0.03 0.52 44
LIUM 0.25 0.24 0.25 0.29 0.30 0.19 ?29288 ?36137 1.98 0.52 0.49 0.62 0.34 ?44 45 0.48 0.03 0.53 44
LIUM-COMBO 0.25 0.24 0.25 0.31 0.31 0.2 ?30678 ?35365 1.98 0.50 0.5 0.62 0.34 ?44 46 0.48 0.03 0.53 45
ONLINE-A 0.25 0.24 0.25 0.27 0.3 0.18 ?38761 ?34096 1.97 0.52 0.49 0.62 0.34 ?44 44 0.48 0.03 0.52 43
ONLINE-B 0.25 0.24 0.25 0.29 0.31 0.19 ?19157 ?25284 1.98 0.50 0.51 0.62 0.35 ?45 46 0.49 0.03 0.54 44
RBMT-1 0.24 0.23 0.24 0.23 0.26 0.15 ?49115 ?39153 1.96 0.59 0.46 0.60 0.33 ?43 42 0.46 0.03 0.51 41
RBMT-2 0.23 0.22 0.23 0.21 0.24 0.13 ?59549 ?50466 1.95 0.63 0.44 0.57 0.32 ?43 40 0.43 0.02 0.48 39
RBMT-3 0.23 0.23 0.23 0.22 0.25 0.14 ?52047 ?45073 1.96 0.59 0.46 0.58 0.32 ?44 41 0.45 0.02 0.50 40
RBMT-4 0.23 0.22 0.24 0.22 0.25 0.14 ?54507 ?42933 1.96 0.63 0.45 0.59 0.33 ?43 40 0.44 0.02 0.49 39
RBMT-5 0.23 0.22 0.23 0.21 0.24 0.13 ?55545 ?48332 1.95 0.62 0.45 0.57 0.32 ?44 40 0.44 0.02 0.49 38
RWTH-HUCK 0.24 0.24 0.25 0.28 0.3 0.18 ?44018 ?42549 1.97 0.52 0.49 0.61 0.34 ?44 44 0.47 0.03 0.51 43
RWTH-LEUSCH-COMBO 0.26 0.25 0.26 0.31 0.32 0.20 ?21914 ?21746 1.98 0.49 0.51 0.64 0.35 ?43 47 0.50 0.03 0.54 46
SYSTRAN 0.24 0.23 0.24 0.25 0.27 0.16 ?34321 ?40119 1.96 0.54 0.48 0.59 0.33 ?44 43 0.46 0.03 0.51 41
UEDIN 0.23 0.23 0.24 0.25 0.27 0.16 ?47202 ?47955 1.96 0.56 0.47 0.59 0.33 ?45 42 0.45 0.03 0.49 40
UPV-PRHLT-COMBO 0.25 0.25 0.26 0.31 0.32 0.20 ?26947 ?28689 1.98 0.5 0.51 0.63 0.35 ?43 47 0.49 0.03 0.54 46
Table 41: Automatic evaluation metric scores for systems in the WMT11 French-English News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Spanish-English News Task
ALACANT 0.24 0.23 0.24 0.27 0.28 0.17 ?30135 ?29622 1.97 0.53 0.46 0.61 0.34 ?45 43 0.46 0.03 0.50 42
BBN-COMBO 0.25 0.25 0.25 0.32 0.33 0.21 ?15284 ?16192 1.98 0.48 0.5 0.64 0.35 ?44 47 0.49 0.03 0.53 46
CMU-HEAFIELD-COMBO 0.25 0.25 0.25 0.32 0.31 0.20 ?13456 ?16113 1.98 0.5 0.5 0.64 0.35 ?44 47 0.5 0.03 0.54 46
CU-ZEMAN 0.20 0.20 0.21 0.16 0.22 0.12 ?49428 ?48440 1.93 0.61 0.36 0.51 0.28 ?49 32 0.35 0.02 0.38 31
HYDERABAD 0.20 0.20 0.21 0.17 0.21 0.11 ?47754 ?47059 1.94 0.61 0.39 0.50 0.28 ?47 34 0.36 0.02 0.41 33
JHU-COMBO 0.25 0.25 0.25 0.32 0.32 0.20 ?23939 ?22685 1.98 0.49 0.49 0.63 0.35 ?44 47 0.48 0.03 0.52 46
KOC 0.24 0.24 0.24 0.26 0.29 0.17 ?22724 ?25857 1.96 0.53 0.46 0.61 0.34 ?45 42 0.46 0.03 0.49 41
KOC-COMBO 0.25 0.24 0.25 0.28 0.30 0.19 ?22678 ?22267 1.97 0.52 0.48 0.62 0.34 ?44 44 0.48 0.03 0.52 43
ONLINE-A 0.25 0.24 0.25 0.28 0.3 0.18 ?19017 ?20120 1.97 0.52 0.48 0.63 0.35 ?44 45 0.48 0.03 0.52 43
ONLINE-B 0.24 0.24 0.24 0.29 0.30 0.19 ?11980 ?18589 1.97 0.50 0.49 0.62 0.34 ?45 45 0.49 0.03 0.53 44
RBMT-1 0.24 0.24 0.25 0.28 0.28 0.17 ?31202 ?26151 1.97 0.57 0.46 0.61 0.34 ?44 45 0.47 0.03 0.51 43
RBMT-2 0.23 0.23 0.24 0.24 0.25 0.15 ?35157 ?31405 1.96 0.6 0.44 0.59 0.33 ?44 42 0.44 0.02 0.49 41
RBMT-3 0.23 0.23 0.24 0.25 0.26 0.15 ?28289 ?26082 1.97 0.59 0.45 0.6 0.33 ?43 43 0.46 0.03 0.51 42
RBMT-4 0.24 0.23 0.24 0.25 0.26 0.16 ?27892 ?25546 1.97 0.59 0.46 0.60 0.33 ?43 43 0.46 0.03 0.52 42
RBMT-5 0.24 0.23 0.24 0.27 0.26 0.16 ?36770 ?31613 1.96 0.58 0.45 0.6 0.33 ?45 43 0.45 0.03 0.50 42
RWTH-LEUSCH-COMBO 0.25 0.25 0.26 0.32 0.32 0.21 ?15172 ?15261 1.98 0.49 0.5 0.64 0.35 ?43 48 0.50 0.03 0.54 47
SYSTRAN 0.24 0.23 0.24 0.27 0.28 0.17 ?20129 ?26051 1.97 0.53 0.47 0.60 0.33 ?46 44 0.46 0.03 0.51 42
UEDIN 0.22 0.22 0.23 0.22 0.25 0.14 ?25462 ?31678 1.96 0.58 0.45 0.57 0.32 ?47 40 0.44 0.03 0.48 39
UFAL-UM 0.23 0.22 0.23 0.23 0.24 0.14 ?42123 ?37765 1.96 0.60 0.43 0.58 0.32 ?43 41 0.43 0.02 0.48 40
UPM 0.22 0.22 0.23 0.22 0.24 0.14 ?39748 ?38433 1.95 0.58 0.43 0.57 0.32 ?45 40 0.42 0.02 0.46 38
UPV-PRHLT-COMBO 0.25 0.25 0.26 0.32 0.32 0.20 ?16094 ?17723 1.98 0.50 0.49 0.64 0.35 ?43 47 0.5 0.03 0.54 46
Table 42: Automatic evaluation metric scores for systems in the WMT11 Spanish-English News Task
(newssyscombtest2011)
59
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
W
M
P
F
English-Czech News Task
CMU-HEAFIELD-COMBO 0.2 0.19 0.20 0.19 0.22 0.12 2.03 0.62 0.24 ?62 29 27
COMMERCIAL1 0.16 0.15 0.16 0.11 0.16 0.08 2.01 0.70 0.19 ?65 22 21
COMMERCIAL2 0.12 0.10 0.13 0.09 0.15 0.06 2.00 0.73 0.18 ?65 21 19
CU-BOJAR 0.18 0.17 0.18 0.16 0.2 0.1 2.02 0.65 0.23 ?63 26 24
CU-MARECEK 0.18 0.17 0.18 0.16 0.2 0.1 2.02 0.65 0.22 ?63 26 24
CU-POPEL 0.17 0.16 0.18 0.14 0.19 0.1 2.02 0.66 0.21 ?64 25 23
CU-TAMCHYNA 0.18 0.17 0.18 0.15 0.2 0.1 2.02 0.65 0.22 ?63 26 24
CU-ZEMAN 0.17 0.16 0.17 0.13 0.18 0.09 2.02 0.66 0.21 ?63 23 22
JHU 0.18 0.18 0.18 0.16 0.21 0.11 2.02 0.63 0.22 ?63 26 24
ONLINE-B 0.2 0.19 0.20 0.2 0.22 0.12 2.03 0.62 0.24 ?63 29 27
UEDIN 0.19 0.18 0.19 0.17 0.21 0.11 2.03 0.63 0.23 ?63 27 26
UPV-PRHLT-COMBO 0.2 0.19 0.20 0.20 0.23 0.13 2.03 0.61 0.24 ?63 29 28
Table 43: Automatic evaluation metric scores for systems in the WMT11 English-Czech News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
English-German News Task
CMU-HEAFIELD-COMBO 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.66 0.39 ?46 36 0.41 0.03 0.45 35
COPENHAGEN 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?47 33 0.38 0.02 0.42 32
CU-TAMCHYNA 0.17 0.17 0.18 0.11 0.18 0.09 1.94 0.70 0.36 ?48 31 0.36 0.02 0.4 30
CU-ZEMAN 0.16 0.15 0.16 0.05 0.17 0.08 1.92 0.71 0.34 ?51 25 0.31 0.02 0.34 25
DFKI-FEDERMANN 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.71 0.34 ?47 33 0.38 0.03 0.44 32
DFKI-XU 0.18 0.17 0.18 0.15 0.19 0.1 1.96 0.68 0.37 ?47 35 0.39 0.03 0.43 34
ILLC-UVA 0.15 0.14 0.15 0.12 0.18 0.08 1.95 0.68 0.33 ?49 32 0.36 0.02 0.4 31
JHU 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.68 0.35 ?47 33 0.37 0.02 0.42 32
KIT 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.68 0.37 ?47 35 0.39 0.03 0.43 34
KOC 0.17 0.16 0.17 0.12 0.17 0.08 1.95 0.69 0.35 ?47 32 0.36 0.02 0.40 31
KOC-COMBO 0.18 0.17 0.18 0.15 0.2 0.1 1.95 0.67 0.37 ?47 34 0.38 0.02 0.42 33
LIMSI 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.67 0.36 ?47 35 0.39 0.03 0.44 33
LIU 0.17 0.17 0.18 0.15 0.19 0.09 1.95 0.68 0.36 ?47 34 0.38 0.02 0.43 33
ONLINE-A 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.67 0.37 ?47 35 0.40 0.03 0.45 33
ONLINE-B 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.65 0.38 ?46 36 0.42 0.03 0.46 35
RBMT-1 0.17 0.17 0.18 0.13 0.18 0.08 1.95 0.7 0.35 ?46 34 0.39 0.03 0.45 33
RBMT-2 0.16 0.16 0.17 0.12 0.16 0.08 1.94 0.73 0.33 ?47 32 0.37 0.03 0.43 31
RBMT-3 0.18 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?46 35 0.39 0.03 0.46 34
RBMT-4 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.70 0.34 ?47 33 0.38 0.03 0.45 32
RBMT-5 0.17 0.16 0.17 0.12 0.17 0.08 1.95 0.71 0.34 ?47 33 0.38 0.03 0.44 32
RWTH-FREITAG 0.17 0.17 0.17 0.15 0.19 0.09 1.95 0.68 0.36 ?47 34 0.37 0.02 0.41 33
UEDIN 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?47 34 0.38 0.02 0.42 33
UOW 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.7 0.35 ?47 33 0.37 0.02 0.42 32
UPPSALA 0.17 0.16 0.17 0.14 0.18 0.09 1.95 0.68 0.35 ?47 33 0.37 0.02 0.42 32
UPV-PRHLT-COMBO 0.18 0.18 0.19 0.17 0.20 0.10 1.96 0.66 0.38 ?46 36 0.4 0.03 0.44 35
UZH-COMBO 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.66 0.38 ?46 36 0.40 0.03 0.44 35
Table 44: Automatic evaluation metric scores for systems in the WMT11 English-German News Task
(newssyscombtest2011)
60
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
English-French News Task
CMU-HEAFIELD-COMBO 0.25 0.25 0.26 0.34 0.35 0.23 2.02 0.5 0.57 ?41 52 0.54 ?0.01 0.60 50
CU-ZEMAN 0.18 0.17 0.18 0.13 0.19 0.09 1.96 0.68 0.39 ?46 35 0.34 ?0.03 0.40 33
JHU 0.23 0.23 0.24 0.27 0.31 0.19 2.01 0.53 0.52 ?43 47 0.49 ?0.01 0.55 45
KIT 0.24 0.23 0.24 0.29 0.31 0.19 2.01 0.52 0.53 ?42 49 0.51 ?0.01 0.57 47
LATL-GENEVA 0.20 0.2 0.21 0.19 0.23 0.12 1.99 0.62 0.44 ?43 41 0.44 ?0.02 0.51 39
LIMSI 0.24 0.24 0.24 0.3 0.31 0.19 2.01 0.53 0.53 ?41 49 0.51 ?0.01 0.58 48
LIUM 0.24 0.23 0.24 0.29 0.31 0.19 2.01 0.53 0.53 ?42 49 0.51 ?0.01 0.57 47
ONLINE-A 0.24 0.23 0.24 0.27 0.3 0.18 2.01 0.53 0.52 ?42 47 0.5 ?0.01 0.56 46
ONLINE-B 0.25 0.25 0.25 0.33 0.35 0.23 2.02 0.5 0.56 ?42 51 0.53 ?0.01 0.59 50
RBMT-1 0.23 0.22 0.23 0.24 0.27 0.16 2.00 0.56 0.5 ?41 45 0.48 ?0.02 0.56 44
RBMT-2 0.22 0.21 0.22 0.22 0.25 0.14 1.99 0.58 0.47 ?42 44 0.46 ?0.02 0.53 42
RBMT-3 0.23 0.22 0.23 0.25 0.28 0.16 2.00 0.56 0.5 ?41 46 0.48 ?0.02 0.56 44
RBMT-4 0.22 0.21 0.22 0.23 0.26 0.15 1.99 0.58 0.47 ?42 43 0.45 ?0.02 0.51 42
RBMT-5 0.22 0.22 0.23 0.23 0.27 0.15 2 0.57 0.49 ?41 45 0.47 ?0.02 0.55 43
RWTH-HUCK 0.23 0.23 0.24 0.29 0.30 0.18 2.01 0.54 0.52 ?42 48 0.5 ?0.01 0.56 47
UEDIN 0.23 0.22 0.23 0.27 0.3 0.18 2.01 0.54 0.51 ?42 47 0.49 ?0.01 0.55 46
UPPSALA 0.23 0.22 0.23 0.27 0.29 0.17 2.00 0.55 0.51 ?42 46 0.48 ?0.01 0.55 45
UPPSALA-FBK 0.23 0.23 0.23 0.28 0.29 0.18 2.01 0.55 0.51 ?42 47 0.49 ?0.01 0.55 46
UPV-PRHLT-COMBO 0.25 0.24 0.25 0.32 0.34 0.22 2.02 0.50 0.55 ?41 51 0.53 ?0.01 0.59 49
Table 45: Automatic evaluation metric scores for systems in the WMT11 English-French News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
English-Spanish News Task
CEU-UPV 0.24 0.24 0.24 0.29 0.3 0.18 2.01 0.51 0.55 ?45 46 0.45 0.01 0.45 45
CMU-HEAFIELD-COMBO 0.26 0.25 0.26 0.35 0.34 0.22 2.02 0.47 0.58 ?44 50 0.49 0.01 0.49 49
CU-ZEMAN 0.23 0.22 0.23 0.22 0.27 0.15 1.99 0.55 0.52 ?48 39 0.41 0.00 0.41 38
KOC 0.23 0.23 0.23 0.25 0.27 0.16 2 0.54 0.52 ?46 43 0.42 0.00 0.43 42
KOC-COMBO 0.25 0.24 0.25 0.31 0.32 0.2 2.01 0.5 0.56 ?44 47 0.46 0.01 0.47 46
ONLINE-A 0.25 0.24 0.25 0.31 0.32 0.2 2.01 0.49 0.56 ?44 48 0.46 0.01 0.47 46
ONLINE-B 0.25 0.25 0.25 0.33 0.32 0.2 2.02 0.50 0.57 ?44 49 0.47 0.01 0.47 48
PROMT 0.24 0.23 0.24 0.28 0.28 0.17 2.00 0.53 0.52 ?45 45 0.44 0.01 0.46 43
RBMT-1 0.23 0.23 0.23 0.25 0.27 0.16 2 0.55 0.51 ?45 43 0.42 0.00 0.44 42
RBMT-2 0.23 0.22 0.23 0.25 0.26 0.15 1.99 0.55 0.5 ?44 43 0.41 0.00 0.42 41
RBMT-3 0.24 0.23 0.24 0.28 0.28 0.17 2.00 0.53 0.52 ?44 45 0.43 0.00 0.45 43
RBMT-4 0.23 0.22 0.23 0.26 0.26 0.16 1.99 0.54 0.51 ?44 44 0.42 0.00 0.43 42
RBMT-5 0.23 0.22 0.23 0.24 0.26 0.15 1.99 0.57 0.49 ?45 42 0.41 0.00 0.43 41
UEDIN 0.24 0.24 0.24 0.31 0.3 0.18 2.01 0.51 0.55 ?45 47 0.45 0.01 0.45 46
UOW 0.23 0.23 0.24 0.28 0.28 0.16 2.00 0.53 0.53 ?45 45 0.42 0.01 0.43 44
UOW-COMBO 0.25 0.25 0.25 0.33 0.32 0.2 2.01 0.50 0.56 ?44 49 0.47 0.01 0.47 47
UPM 0.21 0.21 0.21 0.21 0.22 0.12 1.98 0.61 0.47 ?47 39 0.37 0.00 0.37 38
UPPSALA 0.24 0.24 0.24 0.3 0.29 0.18 2.01 0.51 0.54 ?45 46 0.44 0.01 0.44 45
UPV-PRHLT-COMBO 0.25 0.25 0.25 0.33 0.32 0.21 2.02 0.49 0.57 ?44 49 0.47 0.01 0.48 48
Table 46: Automatic evaluation metric scores for systems in the WMT11 English-Spanish News Task
(newssyscombtest2011)
61
B
L
E
U
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Haitian Creole (clean)-English Haitian Creole SMS Emergency Response Featured Translation Task
BM-I2R 0.33 ?6798 ?4575 1.96 0.51 0.62 0.34 43 0.44 0.03 0.46 43
CMU-DENKOWSKI 0.29 ?6849 ?6172 1.95 0.53 0.58 0.32 40 0.39 0.02 0.40 39
CMU-HEAFIELD-COMBO 0.32 ?6188 ?4347 1.96 0.51 0.61 0.34 42 0.43 0.03 0.45 42
CMU-HEWAVITHARANA 0.28 ?6523 ?6341 1.95 0.57 0.57 0.32 39 0.38 0.02 0.40 38
HYDERABAD 0.14 ?7548 ?8502 1.92 0.66 0.50 0.28 26 0.3 0.02 0.30 26
KOC 0.23 ?6490 ?9020 1.94 0.67 0.49 0.27 36 0.32 0.02 0.34 35
KOC-COMBO 0.29 ?4901 ?5349 1.95 0.57 0.56 0.31 39 0.38 0.02 0.4 39
LIU 0.27 ?6526 ?6078 1.95 0.59 0.56 0.31 38 0.38 0.02 0.39 37
UMD-EIDELMAN 0.26 ?4407 ?6215 1.95 0.57 0.55 0.31 38 0.37 0.02 0.4 37
UMD-HU 0.22 ?6379 ?7460 1.94 0.59 0.51 0.28 35 0.36 0.02 0.39 34
UPPSALA 0.27 ?5497 ?6754 1.95 0.59 0.54 0.3 38 0.36 0.02 0.39 37
UPV-PRHLT-COMBO 0.32 ?6896 ?5968 1.96 0.53 0.6 0.33 42 0.41 0.02 0.43 41
Table 47: Automatic evaluation metric scores for systems in the WMT11 Haitian Creole (clean)-English Haitian
Creole SMS Emergency Response Featured Translation Task (newssyscombtest2011)
B
L
E
U
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Haitian Creole (raw)-English Haitian Creole SMS Emergency Response Featured Translation Task
BM-I2R 0.29 ?3885 ?3017 1.96 0.57 0.57 0.32 39 0.42 0.02 0.44 38
CMU-DENKOWSKI 0.25 ?3965 ?3905 1.95 0.60 0.53 0.3 35 0.38 0.02 0.4 35
CMU-HEAFIELD-COMBO 0.28 ?3057 ?2588 1.96 0.57 0.57 0.32 39 0.42 0.02 0.44 38
CMU-HEWAVITHARANA 0.25 ?3701 ?3824 1.95 0.61 0.53 0.3 35 0.37 0.02 0.39 35
JHU 0.14 ?3207 ?4279 1.92 0.74 0.43 0.24 26 0.30 0.02 0.32 26
LIU 0.25 ?3447 ?3445 1.95 0.60 0.54 0.30 36 0.38 0.02 0.4 35
UMD-EIDELMAN 0.24 ?2826 ?3754 1.94 0.64 0.52 0.29 34 0.36 0.02 0.39 34
UPV-PRHLT-COMBO 0.28 ?3591 ?3370 1.95 0.58 0.56 0.32 38 0.4 0.02 0.42 38
Table 48: Automatic evaluation metric scores for systems in the WMT11 Haitian Creole (raw)-English Haitian Creole
SMS Emergency Response Featured Translation Task (newssyscombtest2011)
62
INTER-ANNOTATOR AGREEMENT (I.E. ACROSS ANNOTATORS)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
Czech-English, individual systems 0.591 0.354 0.367 0.535 0.343 0.293
English-Czech, individual systems 0.608 0.359 0.388 0.552 0.350 0.312
German-English, individual systems 0.562 0.377 0.298 0.536 0.370 0.264
English-German, individual systems 0.564 0.352 0.327 0.528 0.348 0.276
Spanish-English, individual systems 0.695 0.398 0.493 0.683 0.393 0.477
English-Spanish, individual systems 0.574 0.343 0.352 0.548 0.339 0.317
French-English, individual systems 0.616 0.367 0.393 0.584 0.361 0.349
English-French, individual systems 0.631 0.382 0.403 0.603 0.376 0.363
European languages, individual systems 0.601 0.362 0.375 0.561 0.355 0.320
Czech-English, system combinations 0.700 0.334 0.549 0.577 0.369 0.329
English-Czech, system combinations 0.812 0.348 0.711 0.696 0.392 0.500
German-English, system combinations 0.675 0.353 0.498 0.629 0.341 0.437
English-German, system combinations 0.608 0.346 0.401 0.547 0.334 0.320
Spanish-English, system combinations 0.638 0.335 0.456 0.604 0.359 0.382
English-Spanish, system combinations 0.657 0.335 0.485 0.603 0.371 0.369
French-English, system combinations 0.654 0.336 0.479 0.608 0.336 0.410
English-French, system combinations 0.678 0.352 0.503 0.595 0.339 0.388
European languages, system combinations 0.671 0.335 0.505 0.598 0.342 0.389
Haitian (Clean)-English, individual systems 0.693 0.364 0.517 0.640 0.353 0.443
Haitian (Raw)-English, individual systems 0.689 0.357 0.517 0.639 0.344 0.450
Haitian-English, individual systems 0.691 0.362 0.516 0.639 0.350 0.446
Haitian (Clean)-English, system combinations 0.770 0.367 0.636 0.645 0.333 0.468
Haitian (Raw)-English, system combinations 0.745 0.345 0.611 0.753 0.361 0.613
Haitian-English, system combinations 0.761 0.358 0.628 0.674 0.335 0.509
Tunable metrics task (Urdu-English) 0.692 0.337 0.535 0.641 0.363 0.437
WMT10 (European languages, individual vs. individual) 0.663 0.394 0.445 0.620 0.385 0.382
WMT10 (European languages, combo vs. combo) 0.728 0.344 0.586 0.629 0.334 0.443
WMT10 (European languages, individual vs. combo) N/A N/A N/A 0.634 0.360 0.428
WMT10 (European languages, all systems) 0.658 0.374 0.454 0.626 0.367 0.409
Table 49: Inter-annotator agreement rates, for the various manual evaluation tracks of WMT11, broken down by
language pair. The highlighted rows correspond to rows in the top half of Table 7. See Table 50 below for detailed
intra-annotator agreement rates.
63
INTRA-ANNOTATOR AGREEMENT (I.E. SELF-CONSISTENCY)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
Czech-English, individual systems 0.762 0.354 0.632 0.713 0.343 0.564
English-Czech, individual systems 0.743 0.359 0.598 0.700 0.350 0.539
German-English, individual systems 0.675 0.377 0.478 0.670 0.370 0.475
English-German, individual systems 0.704 0.352 0.543 0.700 0.348 0.541
Spanish-English, individual systems 0.750 0.398 0.585 0.719 0.393 0.537
English-Spanish, individual systems 0.644 0.343 0.458 0.601 0.339 0.396
French-English, individual systems 0.829 0.367 0.730 0.816 0.361 0.712
English-French, individual systems 0.716 0.382 0.541 0.681 0.376 0.488
European languages, individual systems 0.722 0.362 0.564 0.685 0.355 0.512
Czech-English, system combinations 0.756 0.334 0.633 0.657 0.369 0.457
English-Czech, system combinations 0.923 0.348 0.882 0.842 0.392 0.740
German-English, system combinations 0.732 0.353 0.586 0.716 0.341 0.569
English-German, system combinations 0.722 0.346 0.575 0.676 0.334 0.513
Spanish-English, system combinations 0.783 0.335 0.673 0.720 0.359 0.562
English-Spanish, system combinations 0.741 0.335 0.610 0.711 0.371 0.540
French-English, system combinations 0.772 0.336 0.657 0.659 0.336 0.487
English-French, system combinations 0.841 0.352 0.755 0.714 0.339 0.568
European languages, system combinations 0.787 0.335 0.680 0.717 0.342 0.571
Haitian (Clean)-English, individual systems 0.758 0.364 0.619 0.686 0.353 0.515
Haitian (Raw)-English, individual systems 0.783 0.357 0.663 0.756 0.344 0.628
Haitian-English, individual systems 0.763 0.362 0.628 0.700 0.350 0.539
Haitian (Clean)-English, system combinations 0.882 0.367 0.813 0.778 0.333 0.667
Haitian (Raw)-English, system combinations 0.882 0.345 0.820 0.802 0.361 0.690
Haitian-English, system combinations 0.882 0.358 0.816 0.784 0.335 0.675
Tunable metrics task (Urdu-English) 0.857 0.337 0.784 0.856 0.363 0.774
WMT10 (European languages, individual vs. individual) 0.757 0.394 0.599 0.728 0.385 0.557
WMT10 (European languages, combo vs. combo) 0.783 0.344 0.670 0.719 0.334 0.578
WMT10 (European languages, individual vs. combo) N/A N/A N/A 0.746 0.360 0.603
WMT10 (European languages, all systems) 0.755 0.374 0.609 0.734 0.367 0.580
Table 50: Intra-annotator agreement rates, for the various manual evaluation tracks of WMT11, broken down by
language pair. The highlighted rows correspond to rows in the bottom half of Table 7. See Table 49 above for detailed
inter-annotator agreement rates.
64
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 10?51,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Findings of the 2012 Workshop on Statistical Machine Translation
Chris Callison-Burch
Johns Hopkins University
Philipp Koehn
University of Edinburgh
Christof Monz
University of Amsterdam
Matt Post
Johns Hopkins University
Radu Soricut
SDL Language Weaver
Lucia Specia
University of Sheffield
Abstract
This paper presents the results of the WMT12
shared tasks, which included a translation
task, a task for machine translation evaluation
metrics, and a task for run-time estimation of
machine translation quality. We conducted a
large-scale manual evaluation of 103 machine
translation systems submitted by 34 teams.
We used the ranking of these systems to mea-
sure how strongly automatic metrics correlate
with human judgments of translation quality
for 12 evaluation metrics. We introduced a
new quality estimation task this year, and eval-
uated submissions from 11 teams.
1 Introduction
This paper presents the results of the shared tasks
of the Workshop on statistical Machine Translation
(WMT), which was held at NAACL 2012. This
workshop builds on six previous WMT workshops
(Koehn and Monz, 2006; Callison-Burch et al,
2007; Callison-Burch et al, 2008; Callison-Burch
et al, 2009; Callison-Burch et al, 2010; Callison-
Burch et al, 2011). In the past, the workshops have
featured a number of shared tasks: a translation task
between English and other languages, a task for au-
tomatic evaluation metrics to predict human judg-
ments of translation quality, and a system combina-
tion task to get better translation quality by combin-
ing the outputs of multiple translation systems. This
year we discontinued the system combination task,
and introduced a new task in its place:
? Quality estimation task ? Structured predic-
tion tasks like MT are difficult, but the dif-
ficulty is not uniform across all input types.
It would thus be useful to have some mea-
sure of confidence in the quality of the output,
which has potential usefulness in a range of set-
tings, such as deciding whether output needs
human post-editing or selecting the best trans-
lation from outputs from a number of systems.
This shared task focused on sentence-level es-
timation, and challenged participants to rate
the quality of sentences produced by a stan-
dard Moses translation system on an English-
Spanish news corpus in one of two tasks:
ranking and scoring. Predictions were scored
against a blind test set manually annotated with
relevant quality judgments.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dissem-
inate common test sets and public training data with
published performance numbers, and to refine eval-
uation methodologies for machine translation. As
with previous workshops, all of the data, transla-
tions, and collected human judgments are publicly
available.1 We hope these datasets form a valuable
resource for research into statistical machine transla-
tion, system combination, and automatic evaluation
or automatic prediction of translation quality.
2 Overview of the Shared Translation Task
The recurring task of the workshop examines trans-
lation between English and four other languages:
German, Spanish, French, and Czech. We created a
1http://statmt.org/wmt12/results.html
10
test set for each language pair by translating newspa-
per articles. We additionally provided training data
and two baseline systems.
2.1 Test data
The test data for this year?s task was created by hir-
ing people to translate news articles that were drawn
from a variety of sources from November 15, 2011.
A total of 99 articles were selected, in roughly equal
amounts from a variety of Czech, English, French,
German, and Spanish news sites:2
Czech: Blesk (1), CTK (1), E15 (1), den??k (4),
iDNES.cz (3), iHNed.cz (3), Ukacko (2),
Zheny (1)
French: Canoe (3), Croix (3), Le Devoir (3), Les
Echos (3), Equipe (2), Le Figaro (3), Libera-
tion (3)
Spanish: ABC.es (4), Milenio (4), Noroeste (4),
Nacion (3), El Pais (3), El Periodico (3), Prensa
Libre (3), El Universal (4)
English: CNN (3), Fox News (2), Los Angeles
Times (3), New York Times (3), Newsweek (1),
Time (3), Washington Post (3)
German: Berliner Kurier (1), FAZ (3), Giessener
Allgemeine (2), Morgenpost (3), Spiegel (3),
Welt (3)
The translations were created by the professional
translation agency CEET.3 All of the translations
were done directly, and not via an intermediate lan-
guage.
Although the translations were done profession-
ally, we observed a number of errors. These errors
ranged from minor typographical mistakes (I was
terrible. . . instead of It was terrible. . . ) to more
serious errors of incorrect verb choices and nonsen-
sical constructions. An example of the latter is the
French sentence (translated from German):
Il a gratte? une planche de be?ton, perdit des
pie`ces du ve?hicule.
(He scraped against a concrete crash bar-
rier and lost parts of the car.)
2For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
3http://www.ceet.eu/
Here, the French verb gratter is incorrect, and the
phrase planche de be?ton does not make any sense.
We did not quantify errors, but collected a number
of examples during the course of the manual evalua-
tion. These errors were present in the data available
to all the systems and therefore did not bias the re-
sults, but we suggest that next year a manual review
of the professionally-collected translations be taken
prior to releasing the data in order to correct mis-
takes and provide feedback to the translation agency.
2.2 Training data
As in past years we provided parallel corpora to train
translation models, monolingual corpora to train lan-
guage models, and development sets to tune system
parameters. Some statistics about the training mate-
rials are given in Figure 1.
2.3 Submitted systems
We received submissions from 34 groups across 18
institutions. The participants are listed in Table 1.
We also included two commercial off-the-shelf MT
systems, three online statistical MT systems, and
three online rule-based MT systems. Not all systems
supported all language pairs. We note that the eight
companies that developed these systems did not sub-
mit entries themselves, but were instead gathered by
translating the test data via their interfaces (web or
PC).4 They are therefore anonymized in this paper.
The data used to construct these systems is not sub-
ject to the same constraints as the shared task partic-
ipants. It is possible that part of the reference trans-
lations that were taken from online news sites could
have been included in the systems? models, for in-
stance. We therefore categorize all commercial sys-
tems as unconstrained when evaluating the results.
3 Human Evaluation
As with past workshops, we placed greater empha-
sis on the human evaluation than on the automatic
evaluation metric scores. It is our contention that
automatic measures are an imperfect substitute for
human assessment of translation quality. Therefore,
we define the manual evaluation to be primary, and
4We would like to thank Ondr?ej Bojar for harvesting the
commercial entries, Christian Federmann for the statistical MT
entries, and Herve? Saint-Amand for the rule-based MT entries.
11
Europarl Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,965,734 2,007,723 1,920,209 646,605
Words 56,895,229 54,420,026 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 176,258 117,481 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 157,302 137,097 158,840 136,151
Words 4,449,786 3,903,339 3,915,218 3,403,043 3,950,394 3,856,795 2,938,308 3,264,812
Distinct words 78,383 57,711 63,805 53,978 130,026 57,464 136,392 52,488
United Nations Training Corpus
Spanish? English French? English
Sentences 11,196,913 12,886,831
Words 318,788,686 365,127,098 411,916,781 360,341,450
Distinct words 593,567 581,339 565,553 666,077
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,218,201 2,123,835 2,190,579 2,176,537 668,595
Words 59,848,044 60,476,282 63,439,791 53,534,167 14,946,399
Distinct words 123,059 181,837 145,496 394,781 172,461
News Language Model Data
English Spanish French German Czech
Sentence 51,827,706 8,627,438 16,708,622 30,663,107 18,931,106
Words 1,249,883,955 247,722,726 410,581,568 576,833,910 315,167,472
Distinct words 2,265,254 926,999 1,267,582 3,336,078 2,304,933
News Test Set
English Spanish French German Czech
Sentences 3003
Words 73,785 78,965 81,478 73,433 65,501
Distinct words 9,881 12,137 11,441 14,252 17,149
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of
distinct words (case-insensitive) is based on the provided tokenizer.
12
ID Participant
CMU Carnegie Mellon University (Denkowski et al, 2012)
CU-BOJAR Charles University - Bojar (Bojar et al, 2012)
CU-DEPFIX Charles University - DEPFIX (Rosa et al, 2012)
CU-POOR-COMB Charles University - Bojar (Bojar et al, 2012)
CU-TAMCH Charles University - Tamchyna (Tamchyna et al, 2012)
CU-TECTOMT Charles University - TectoMT (Dus?ek et al, 2012)
DFKI-BERLIN German Research Center for Artificial Intelligence (Vilar, 2012)
DFKI-HUNSICKER German Research Center for Artificial Intelligence - Hunsicker (Hunsicker et al, 2012)
GTH-UPM Technical University of Madrid (Lo?pez-Luden?a et al, 2012)
ITS-LATL Language Technology Laboratory @ University of Geneva (Wehrli et al, 2009)
JHU Johns Hopkins University (Ganitkevitch et al, 2012)
KIT Karlsruhe Institute of Technology (Niehues et al, 2012)
LIMSI LIMSI (Le et al, 2012)
LIUM University of Le Mans (Servan et al, 2012)
PROMT ProMT (Molchanov, 2012)
QCRI Qatar Computing Research Institute (Guzman et al, 2012)
QUAERO The QUAERO Project (Markus et al, 2012)
RWTH RWTH Aachen (Huck et al, 2012)
SFU Simon Fraser University (Razmara et al, 2012)
UEDIN-WILLIAMS University of Edinburgh - Williams (Williams and Koehn, 2012)
UEDIN University of Edinburgh (Koehn and Haddow, 2012)
UG University of Toronto (Germann, 2012)
UK Charles University - Zeman (Zeman, 2012)
UPC Technical University of Catalonia (Formiga et al, 2012)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C] Three online statistical machine translation systems
RBMT-[1,3,4] Three rule-based statistical machine translation systems
Table 1: Participants in the shared translation task. Not all teams participated in all language pairs. The translations
from the commercial, online, and rule-based systems were crawled by us, not submitted by the respective companies,
and are therefore anonymized. Anonymized identifiers were chosen so as to correspond with the WMT11 systems.
13
Language Pair Num Label Labels per
Systems Count System
Czech-English 6 6,470 1,078.3
English-Czech 13 11,540 887.6
German-English 16 7,135 445.9
English-German 15 8,760 584.0
Spanish-English 12 5,705 475.4
English-Spanish 11 7,375 670.4
French-English 15 6,975 465.0
English-French 15 7,735 515.6
Overall 103 61,695 598
Table 2: A summary of the WMT12 ranking task, show-
ing the number of systems and number of labels (rank-
ings) collected for each of the language translation tasks.
use the human judgments to validate automatic met-
rics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct on the scale of our
workshop. We distributed the workload across a
number of people, beginning with shared-task par-
ticipants and interested volunteers. This year, we
also opened up the evaluation to non-expert anno-
tators hired on Amazon Mechanical Turk (Callison-
Burch, 2009). To ensure that the Turkers provided
high quality annotations, we used controls con-
structed from the machine translation ranking tasks
from prior years. Control items were selected such
that there was high agreement across the system de-
velopers who completed that item. In all, there were
229 people who participated in the manual evalua-
tion, with 91 workers putting in more than an hour?s
worth of effort, and 21 putting in more than four
hours. After filtering Turker rankings against the
controls to discard Turkers who fell below a thresh-
old level of agreement on the control questions,
there was a collective total of 336 hours of usable
labor. This is similar to the total of 361 hours of
labor collected for WMT11.
We asked annotators to evaluate system outputs
by ranking translated sentences relative to each
other. This was our official determinant of trans-
lation quality. The total number of judgments col-
lected for each of the language pairs is given in Ta-
ble 2.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the instruc-
tions simple:
You are shown a source sentence followed
by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions, and asked to rank them.
We refer to each of these as ranking tasks or some-
times blocks.
Every language task had more than five partici-
pating systems ? up to a maximum of 16 for the
German-English task. Rather than attempting to get
a complete ordering over the systems in each rank-
ing task, we instead relied on random selection and
a reasonably large sample size to make the compar-
isons fair.
We use the collected rank labels to assign each
system a score that reflects how highly that system
was usually ranked by the annotators. The score for
some systemA reflects how frequently it was judged
to be better than other systems. Specifically, each
block in whichA appears includes four implicit pair-
wise comparisons (against the other presented sys-
tems). A is rewarded once for each of the four com-
parisons in which A wins, and its score is the num-
ber of such winning pairwise comparisons, divided
by the total number of non-tying pairwise compar-
isons involving A.
This scoring metric is different from that used in
prior years in two ways. First, the score previously
included ties between system rankings. In that case,
the score for A reflected how often A was rated as
better than or equal to other systems, and was nor-
malized by all comparisons involving A. However,
this approach unfairly rewards systems that are sim-
ilar (and likely to be ranked as tied). This is prob-
lematic since many of the systems use variations of
the same underlying decoder (Bojar et al, 2011).
A second difference is that this year we no longer
include comparisons against reference translations.
In the past, reference translations were included
14
among the systems to be ranked as controls, and
the pairwise comparisons were used in determin-
ing the best system. However, workers have a very
clear preference for reference translations, so includ-
ing them unduly penalized systems that, through
(un)luck of the draw, were pitted against the ref-
erences more often. These changes are part of a
broader discussion of the best way to produce the
system ranking, which we discuss at length in Sec-
tion 4.
The system scores are reported in Section 3.3.
Appendix A provides detailed tables that contain
pairwise head-to-head comparisons between pairs of
systems.
3.2 Inter- and Intra-annotator agreement in
the ranking task
Each year we calculate the inter- and intra-annotator
agreement for the human evaluation, since a reason-
able degree of agreement must exist to support our
process as a valid evaluation setup. To ensure we
had enough data to measure agreement, we occa-
sionally showed annotators items that were repeated
from previously completed items. These repeated
items were drawn from ones completed by the same
annotator and from different annotators.
We measured pairwise agreement among anno-
tators using Cohen?s kappa coefficient (?) (Cohen,
1960), which is defined as
? =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the anno-
tators agree, and P (E) is the proportion of time that
they would agree by chance. Note that ? is basically
a normalized version of P (A), one which takes into
account how meaningful it is for annotators to agree
with each other, by incorporating P (E). Note also
that ? has a value of at most 1 (and could possibly
be negative), with higher rates of agreement result-
ing in higher ?.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate at
which annotators agree, in the context of pairwise
comparisons. P (A) is computed similarly for intra-
annotator agreement (i.e. self-consistency), but over
pairwise comparisons that were annotated more than
once by a single annotator.
As for P (E), it should capture the probability that
two annotators would agree randomly. Therefore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is com-
puted empirically, by observing how often annota-
tors actually rank two systems as being tied. We
note here that this empirical computation is a depar-
ture from previous years? analyses, where we had
assumed that the three categories are equally likely
(yielding P (E) = 19 +
1
9 +
1
9 =
1
3 ). We believe that
this is a more principled approach, which faithfully
reflects the motivation of accounting for P (E) in the
first place.
Table 3 gives ? values for inter-annotator and
intra-annotator agreement. These give an indica-
tion of how often different judges agree, and how
often single judges are consistent for repeated judg-
ments, respectively. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0 ? 0.2 is slight, 0.2 ? 0.4
is fair, 0.4 ? 0.6 is moderate, 0.6 ? 0.8 is sub-
stantial, and 0.8 ? 1.0 is almost perfect. Based on
these interpretations, the agreement for sentence-
level ranking is fair for inter-annotator and moder-
ate for intra-annotator agreement. Consistent with
previous years, intra-annotator agreement is higher
than inter-annotator agreement, except for English?
Czech.
An important difference from last year is that the
evaluations were not constrained only to workshop
participants, but were made available to all Turk-
ers. The workshop participants were trusted to com-
plete the tasks in good faith, and we have multiple
years of data establishing general levels of inter- and
intra-annotator agreement. Their HITs were unpaid,
and access was limited with the use of a qualifica-
tion. The Turkers completed paid tasks, and we used
controls to filter out fraudulent and unconscientious
workers.
15
INTER-ANNOTATOR AGREEMENT INTRA-ANNOTATOR AGREEMENT
LANGUAGE PAIRS P (A) P (E) ? P (A) P (E) ?
Czech-English 0.567 0.405 0.272 0.660 0.405 0.428
English-Czech 0.576 0.383 0.312 0.566 0.383 0.296
German-English 0.595 0.401 0.323 0.733 0.401 0.554
English-German 0.598 0.394 0.336 0.732 0.394 0.557
Spanish-English 0.540 0.408 0.222 0.792 0.408 0.648
English-Spanish 0.504 0.398 0.176 0.566 0.398 0.279
French-English 0.568 0.406 0.272 0.719 0.406 0.526
English-French 0.519 0.388 0.214 0.634 0.388 0.401
WMT12 0.568 0.396 0.284 0.671 0.396 0.455
WMT11 0.601 0.362 0.375 0.722 0.362 0.564
Table 3: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11
rows contain the results from the European languages individual systems task (Callison-Burch et al (2011), Table 7).
Agreement rates vary widely across languages.
For inter-annotator agreements, the range is 0.176 to
0.336, while intra-annotator agreement ranges from
0.279 to 0.648. We note in particular the low agree-
ment rates among judgments in the English-Spanish
task, which is reflected in the relative lack of statis-
tical significance Table 4. The agreement rates for
this year were somewhat lower than last year.
3.3 Results of the Translation Task
We used the results of the manual evaluation to an-
alyze the translation quality of the different systems
that were submitted to the workshop. In our analy-
sis, we aimed to address the following questions:
? Which systems produced the best translation
quality for each language pair?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 4 shows the system ranking for each of the
translation tasks. For each language pair, we define
a system as ?winning? if no other system was found
statistically significantly better (using the Sign Test,
at p ? 0.10). In some cases, multiple systems are
listed as winners, either due to a large number of par-
ticipants or a low number of judgments per system
pair, both of which are factors that make it difficult
to achieve statistical significance.
As in prior years, unconstrained online systems
A and B are among the best for many tasks, with
a few notable exceptions. CU-DEPFIX, which post-
processes the output of ONLINE-B, was judged as
the best system for English-Czech. For the French-
English and English-French tasks, constrained sys-
tems came out on top, with LIMSI appearing both
times. Consistent with prior years, the rule-based
systems performed very well on the English-German
task. A rule-based system also had a good showing
for English-Spanish, but not really anywhere else.
Among the systems competing in all tasks, no sin-
gle system consistently appeared among the top en-
trants. Participants that competed in all tasks tended
to fair worse, with the exception of UEDIN. Addi-
tionally, KIT appeared in four tasks and was a con-
strained winner each time.
4 Methods for Overall Ranking
Last year one of the long papers published at WMT
criticized our method for compiling the overall rank-
ing for systems in the translation task (Bojar et
al., 2011). This year another paper shows some
additional potential inconsistencies in the rankings
(Lopez, 2012). In this section we delve into a de-
tailed analysis of a variety of methods that use the
human evaluation to create an overall ranking of sys-
tems.
In the human evaluation, we collect ranking judg-
ments for output from five systems at a time. We in-
terpret them as 10 ?
(
5?4
2
)
pairwise judgments over
systems and use these to analyze how each system
faired compared against each of the others. Not all
16
Czech-English
3,603?3,718 comparisons/system
System C? >others
ONLINE-B ? N 0.65
UEDIN ? Y 0.60
CU-BOJAR Y 0.53
ONLINE-A N 0.53
UK Y 0.37
JHU Y 0.32
Spanish-English
1,527?1,775 comparisons/system
System C? >others
ONLINE-A ? N 0.62
ONLINE-B ? N 0.61
QCRI ? Y 0.60
UEDIN ?? Y 0.58
UPC Y 0.57
GTH-UPM Y 0.52
RBMT-3 N 0.51
JHU Y 0.48
RBMT-4 N 0.46
RBMT-1 N 0.42
ONLINE-C N 0.42
UK Y 0.19
French-English
1,437?1,701 comparisons/system
System C? >others
LIMSI ?? Y 0.63
KIT ?? Y 0.61
ONLINE-A ? N 0.59
CMU ?? Y 0.57
ONLINE-B ? N 0.57
UEDIN Y 0.55
LIUM Y 0.52
RWTH Y 0.52
RBMT-1 N 0.46
RBMT-3 N 0.46
UK Y 0.44
SFU Y 0.44
RBMT-4 N 0.43
JHU Y 0.41
ONLINE-C N 0.32
English-Czech
2,652?3,146 comparisons/system
System C? >others
CU-DEPFIX ? N 0.66
ONLINE-B N 0.63
UEDIN ? Y 0.56
CU-TAMCH N 0.56
CU-BOJAR ? Y 0.54
CU-TECTOMT ? Y 0.53
ONLINE-A N 0.53
COMMERCIAL-1 N 0.48
COMMERCIAL-2 N 0.46
CU-POOR-COMB Y 0.44
UK Y 0.44
SFU Y 0.36
JHU Y 0.32
English-Spanish
2,013?2,294 comparisons/system
System C? >others
ONLINE-B ? N 0.65
RBMT-3 N 0.58
ONLINE-A ? N 0.56
PROMT N 0.55
UPC ? Y 0.52
UEDIN ? Y 0.52
RBMT-4 N 0.46
RBMT-1 N 0.45
ONLINE-C N 0.43
UK Y 0.41
JHU Y 0.36
English-French
1,410?1,697 comparisons/system
System C? >others
LIMSI ?? Y 0.66
RWTH Y 0.62
ONLINE-B N 0.60
KIT ?? Y 0.59
LIUM Y 0.55
UEDIN Y 0.53
RBMT-3 N 0.52
ONLINE-A N 0.51
PROMT N 0.51
RBMT-1 N 0.48
JHU Y 0.44
UK Y 0.40
RBMT-4 N 0.39
ONLINE-C N 0.39
ITS-LATL N 0.36
German-English
1,386?1,567 comparisons/system
System C? >others
ONLINE-A ? N 0.65
ONLINE-B ? N 0.65
QUAERO Y 0.61
RBMT-3 N 0.60
UEDIN ? Y 0.60
RWTH ? Y 0.56
KIT ? Y 0.55
LIMSI Y 0.54
QCRI Y 0.52
RBMT-1 N 0.51
RBMT-4 N 0.50
ONLINE-C N 0.43
DFKI-BERLIN Y 0.40
UK Y 0.37
JHU Y 0.34
UG Y 0.17
English-German
1,777?2,160 comparisons/system
System C? >others
ONLINE-B ? N 0.64
RBMT-3 N 0.63
RBMT-4 ? N 0.58
RBMT-1 N 0.56
LIMSI ? Y 0.55
ONLINE-A N 0.54
UEDIN-WILLIAMS ? Y 0.51
KIT ? Y 0.50
DFKI-HUNSICKER N 0.48
UEDIN ? Y 0.47
RWTH ? Y 0.47
ONLINE-C N 0.47
UK Y 0.45
JHU Y 0.43
DFKI-BERLIN Y 0.25
C? indicates whether system is constrained (unhighlighted rows): trained only using supplied training data, standard
monolingual linguistic tools, and, optionally, LDC?s English Gigaword.
? indicates a win: no other system is statistically significantly better at p-level ? 0.10 in pairwise comparison.
? indicates a constrained win: no other constrained system is statistically better.
Table 4: Official results for the WMT12 translation task. Systems are ordered by their > others score, reflecting how
often their translations won in pairwise comparisons. For detailed head-to-head comparisons, see Appendix A.
17
pairwise comparisons detect statistical significantly
superior quality of either system, and we note this
accordingly.
It is desirable to additionally produce an overall
ranking. In the past evaluation campaigns, we used
two different methods to obtain such a ranking, and
this year we use yet another one. In this section, we
discuss each of these overall ranking methods and a
few more.
4.1 Rank Ranges
In the first human evaluation, we use fluency and
adequacy judgments on a scale from 1 to 5 (Koehn
and Monz, 2006). We normalized the scores on a
per-sentence basis, thus converting them to a rela-
tive ranking in a 5-system comparison. We listed
systems by the average of these scores over all sen-
tences, in which they were judged.
We did not report ranks, but rank ranges. To
give an example: if a system scored neither sta-
tistically significantly better nor statistically signif-
icantly worse than 3 other systems, we assign it the
rank range 1?4. The given evidence is not sufficient
to rank it exactly, but it does rank somewhere in the
top 4.
In subsequent years, we did not continue the re-
porting of rank ranges (although they can be ob-
tained by examining the pairwise comparison ta-
bles), but we continued to report systems as win-
ners whenever there was not statistically signifi-
cantly outperformed by any other system.
4.2 Ratio of Wins and Ties
In the following years (Callison-Burch et al, 2007;
Callison-Burch et al, 2008; Callison-Burch et al,
2009; Callison-Burch et al, 2010; Callison-Burch et
al., 2011), we abandoned the idea of using fluency
and adequacy judgments, since they showed to be
less reliable than simple ranking of system transla-
tions. We also started to interpret the 5-system com-
parison as a set of pairwise comparisons.
Systems were then ranked by the ratio of how of-
ten they were ranked better or equal to any of the
other systems.
Given a set J of sentence-level judgments
(s1, s2, c) where s1 ? S and s2 ? S are two sys-
tems and
c =
?
??
??
win if s1 better than s2
tie if s1 equal to s2
loss if s1 worse than s2
(1)
then we can count the total number of wins and ties
of a system s as
win(s) = |{(s1, s2, c) ? J : s = s1, c = win}|+
|{(s1, s2, c) ? J : s = s2, c = loss}|
loss(s) = |{(s1, s2, c) ? J : s = s1, c = loss}|+
|{(s1, s2, c) ? J : s = s2, c = win}|
tie(s) = |{(s1, s2, c) ? J : s = s1, c = tie}|+
|{(s1, s2, c) ? J : s = s2, c = tie}|
(2)
and rank systems by the ratio
score(s) =
win(s) + tie(s)
win(s) + loss(s) + tie(s)
(3)
This ratio was used for the official rankings over
the last five years.
4.3 Ratio of Wins (Ignoring Ties)
Bojar et al (2011) present a persuasive argument
that our ranking scheme is biased towards systems
that are similar to many other systems. Given that
most of the systems are based on phrase-based mod-
els trained on the same training data, this is indeed a
valid concern.
They suggest ignoring ties, and using as ranking
score instead the following ratio:
score(s) =
win(s)
win(s) + loss(s)
(4)
This ratio is used for the official ranking this year.
4.4 Minimizing Pairwise Ranking Violations
Lopez (2012, in this volume) argues against using
aggregate statistics over a set of very diverse judg-
ments. Instead, a ranking that has the least number
of pairwise ranking violations is said to be preferred.
If we define the number of pairwise wins as
win(s1, s2) = |{(s1, s2, c) ? J : c = win}|+
|{(s2, s1, c) ? J : c = loss}|
(5)
then we define a count function for pairwise order
violations as
18
score(s1, s2) = max(0,win(s2, s1)? win(s1, s2))
(6)
Given a bijective ranking function R(s)? i with
the codomain of consecutive integers starting at 1,
the total number of pairwise ranking violations is de-
fined as
score(R) =
?
R(si)<R(sj)
score(si, sj) (7)
Finding the optimal rankingR that minimizes this
score is not trivial, but given the number of systems
involved in this evaluation campaign, it is quite man-
ageable.
4.5 Most Probable Ranking
We now introduce a variant to Lopez?s ranking
method. We motivate it first.
Consider the following scenario:
win(A,B) = 20 win(B,A) = 0
win(B,C) = 40 win(C,B) = 20
win(C,A) = 60 win(A,C) = 40
Since this constitutes a circle, there are three
rankings with the minimum number of 20 violation
(ABC, BCA, CAB).
However, we may want to take the ratio of wins
and losses for each pairwise ranking into account.
Using maximum likelihood estimation, we can de-
fine the probability that system s1 is better than sys-
tem s2 on a randomly drawn sentence as
p(s1 > s2) =
win(s1, s2)
win(s1, s2) + win(s2, s1)
(8)
We can then go on to define5 the probability of a
5Sketch of derivation:
p(s1 > s2 > s3) = p(s1 first)p(s2 second|s1 first)
(chain rule)
p(s1 first) = p(s1 > s2 and s1 > s3)
= p(s1 > s2)p(s1 > s3)
(independence assumption)
p(s2 sec.|s1 first) = p(s2 second)
(independence assumption)
= p(s2 > s3)
ranking of three systems as:
p(s1 > s2 > s3) = p(s1 > s2)p(s1 > s3)p(s2 > s3)
(9)
This function scores the three rankings in the ex-
ample above as follows:
p(A > B > C) = 2020
40
100
40
60 = 0.27
p(B > C > A) = 4060
0
20
60
100 = 0
p(C > A > B) = 60100
20
60
20
20 = 0.20
One disadvantage of this and the previous rank-
ing method is that they do not take advantage of all
available evidence. Consider the example:
win(A,B) = 100 win(B,A) = 0
win(A,C) = 60 win(C,A) = 40
win(B,C) = 50 win(C,B) = 50
Here, system A is clearly ahead, but how about B
and C? They are tied in their pairwise comparison.
So, both ABC and ACB have no pairwise ranking
violations and their most probable ranking score, as
defined above, is the same.
B is clearly worse than A, but C has a fighting
chance, and this should be reflected in the ranking.
The following two overall ranking methods over-
come this problem.
4.6 Monte Carlo Playoffs
The sports world is accustomed to the problem of
finding a ranking of sports teams, but being only able
to have pairwise competitions (think basketball or
football). One strategy is to stage playoffs.
Let?s say there are 4 systems: A,B, C, andD. As
in well-known play-off fashion, they are first seeded.
In our case, this happens randomly, say, 1:A, 2:B,
3:C, 4:D (for simplicity?s sake).
First round: A plays against D, B plays against
C. How do they play? We randomly select a sen-
tence on which they were compared (no ties). If A
is better according to human judgment than D, then
A wins.
Let?s say, A wins against D, and B loses against
C. This leads us to the final A against C and the
3rd place game D against B, in which, say, A and D
win. The resulting final ranking is ACDB.
We repeat this a million times with a different ran-
dom seeding every time, and compute the average
rank, which is then used for overall ranking.
19
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.641: ONLINE-B RBMT-4 RBMT-4 6.16: ONLINE-B 0.640 (1-2): ONLINE-B
2 0.627: RBMT-3 ONLINE-B ONLINE-B 6.39: RBMT-3 0.622 (1-2): RBMT-3
3 0.577: RBMT-4 RBMT-3 RBMT-3 6.98: RBMT-4 0.578 (3-5): RBMT-4
4 0.557: RBMT-1 RBMT-1 RBMT-1 7.32: RBMT-1 0.553 (3-6): RBMT-1
5 0.547: LIMSI ONLINE-A ONLINE-A 7.46: LIMSI 0.543 (3-7): LIMSI
6 0.537: ONLINE-A UEDIN-WILLIAMS LIMSI 7.57: ONLINE-A 0.534 (4-8): ONLINE-A
7 0.509: UEDIN-WILLIAMS LIMSI UEDIN-WILLIAMS 7.87: UEDIN-WILLIAMS 0.511 (5-9): UEDIN-WILLIAMS
8 0.503: KIT KIT KIT 7.98: KIT 0.503 (6-11): KIT
9 0.476: DFKI-HUNSICKER DFKI-HUNSICKER DFKI-HUNSICKER 8.32: UEDIN 0.477 (7-13): UEDIN
10 0.475: UEDIN ONLINE-C ONLINE-C 8.38: DFKI-HUNSICKER 0.472 (8-13): DFKI-HUNSICKER
11 0.470: RWTH UEDIN UEDIN 8.41: ONLINE-C 0.470 (8-13): ONLINE-C
12 0.470: ONLINE-C UK UK 8.44: RWTH 0.468 (8-13): RWTH
13 0.448: UK RWTH RWTH 8.72: UK 0.447 (10-14): UK
14 0.435: JHU JHU JHU 8.87: JHU 0.434 (12-14): JHU
15 0.249: DFKI-BERLIN DFKI-BERLIN DFKI-BERLIN 11.15: DFKI-BERLIN 0.249 (15): DFKI-BERLIN
Table 5: Overall ranking with different methods (English?German)
4.7 Expected Wins
In European national football competitions, each
team plays against each other team, and at the end
the number of wins decides the rankings.6 We can
simulate this type of tournament as well with Monte
Carlo methods. However, in the limit, each team will
be on average ranked based on its expected number
of wins in the competition. We can compute the ex-
pected number of wins straightforward as
score(si) =
1
|S| ? 1
?
j,j 6=i
p(si > sj) (10)
Note that this is very similar to Bojar?s method of
ranking systems, with one additional and important
twist. We can rewrite Equation 4, the variant that
ignores ties, as:
score(si) =
win(si)
win(si)+loss(si)
(11)
=
?
j,j 6=i win(si,sj)?
j,j 6=i win(si,sj)+loss(si,sj)
(12)
This section?s Equation 10 can be rewritten as:
score(si) =
1
|S|
?
j,j 6=i
win(si, sj)
win(si, sj) + loss(si, sj)
(13)
The difference is that the new overall ranking
method normalizes the win ratios per pairwise rank-
ing. And this makes sense, since it overcomes one
6They actually play twice against each other, to balance out
home field advantage, which is not a concern here.
problem with our traditional and Bojar?s ranking
method.
Previously, some systems were put at an dis-
advantage, if they are compared more frequently
against good systems than against bad systems. This
could happen, if participants were not allowed to
rank their own systems (a constraint we enforced
in the past, but no longer). This was noticed by
judges a few years ago, when we had instant re-
porting of rankings during the evaluation period. If
you have one of the best systems and carry out a lot
of human judgments, then competitors? systems will
creep up higher, since they are not compared against
your own (very good) system anymore, but more fre-
quently against bad systems.
4.8 Comparison
Table 5 shows the different rankings for English?
German, a rather typical example. The table dis-
plays the ranking of the systems according to five
different methods, alongside with system scores ac-
cording to the ranking method: the win ratio (Bo-
jar), the average rank (MC Playoffs), and the ex-
pected win ratio (Expected Wins). For the latter, we
performed bootstrap resampling and computed rank
ranges that lie in a 95% confidence interval. You
can find the tables for the other language pairs in the
annex.
The win-based methods (Bojar, MC Playoffs, Ex-
pected Wins) give very similar rankings ? exhibit-
ing mostly just the occasional pairwise flip or for
20
many language pairs the ranking is identical. The
same is true for the two methods based on pairwise
rankings (Lopez, Most Probable). However, the two
types of ranking lead to significantly different out-
comes.
For instance, the win-based methods are pretty
sure that ONLINE-B and RBMT-3 are the two top
performers. Bootstrap resampling of rankings ac-
cording to Expected Wins ranking draws a clear
line between them and the rest. However, Lopez?s
method ranks RBMT-4 first. Why? In direct com-
parison of the three systems, RBMT-4 beats statis-
tically insignificantly ONLINE-B 45% wins against
42% wins and essentially ties with RBMT-3 41%
wins against 41% wins (ONLINE-B beats RBMT-3
49%?35%, p ? 0.01).
We use Bojar?s method as our official method for
ranking in Table 4 and as the human judgments that
we used when calculating how well automatic eval-
uation metrics correlate with human judgments.
4.9 Number of Judgments Needed
In general, there are not enough judgments to rank
systems unambiguously. How many judgments do
we need?
We may extrapolate this number from the num-
ber of judgments we have. Figure 2 provides some
hints. The outlier is Czech?English, for which only
6 systems were submitted and we can separate them
almost completely even at p-level 0.01. For all the
other language pairs, we can only draw for around
40% of the pairwise comparisons conclusions with
that level of statistical significance.
Since the plots also contains the ratio of signifi-
cant conclusions when sub-sampling the number of
judgments, we obtain curves with a clear upward
slope. For English?Czech, for which we were able
to collect much more judgments, we can draw over
60% significant conclusions. The curve for this lan-
guage pair does not look much different than the
other languages, suggesting that doubling the num-
ber of judgments should allow similar levels for
them as well.
5 Metrics Task
In addition to allowing us to analyze the translation
quality of different systems, the data gathered during
p-level 0.01
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
p-level 0.05
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
p-level 0.10
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
Figure 2: Ratio of statistically significant pairwise com-
parisons at different p-levels, based on number of pair-
wise judgments collected.
21
Metric IDs Participant
AMBER National Research Council Canada (Chen et al, 2012)
METEOR CMU (Denkowski and Lavie, 2011)
SAGAN-STS FaMAF, UNC, Argentina (Castillo and Estrella, 2012)
SEMPOS Charles University (Macha?c?ek and Bojar, 2011)
SIMBLEU University of Sheffield (Song and Cohn, 2011)
SPEDE Stanford University (Wang and Manning, 2012)
TERRORCAT University of Zurich, DFKI, Charles U (Fishel et al, 2012)
BLOCKERRCATS, ENXERRCATS, WORD-
BLOCKERRCATS, XENERRCATS, POSF
DFKI (Popovic, 2012)
Table 6: Participants in the metrics task.
the manual evaluation is useful for validating auto-
matic evaluation metrics. Table 6 lists the partici-
pants in this task, along with their metrics.
A total of 12 metrics and their variants were sub-
mitted to the metrics task by 8 research groups. We
provided BLEU and TER scores as baselines. We
asked metrics developers to score the outputs of
the machine translation systems and system com-
binations at the system-level and at the segment-
level. The system-level metrics scores are given in
the Appendix in Tables 29?36. The main goal of
the metrics shared task is not to score the systems,
but instead to validate the use of automatic metrics
by measuring how strongly they correlate with hu-
man judgments. We used the human judgments col-
lected during the manual evaluation for the transla-
tion task and the system combination task to calcu-
late how well metrics correlate at system-level and
at the segment-level.
5.1 System-Level Metric Analysis
We measured the correlation of the automatic met-
rics with the human judgments of translation qual-
ity at the system-level using Spearman?s rank cor-
relation coefficient ?. We converted the raw scores
assigned to each system into ranks. We assigned a
human ranking to the systems based on the percent
of time that their translations were judged to be bet-
ter than the translations of any other system in the
manual evaluation (Equation 4).
When there are no ties, ? can be calculated using
the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
C
S
-E
N
-
6
S
Y
S
T
E
M
S
D
E
-E
N
-
16
S
Y
S
T
E
M
S
E
S
-E
N
-
12
S
Y
S
T
E
M
S
F
R
-E
N
-
15
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
System-level correlation for translations into English
SEMPOS .94 .92 .94 .80 .90
AMBER .83 .79 .97 .85 .86
METEOR .66 .89 .95 .84 .83
TERRORCAT .71 .76 .97 .88 .83
SIMPBLEU .89 .70 .89 .82 .82
TER -.89 -.62 -.92 -.82 .81
BLEU .89 .67 .87 .81 .81
POSF .66 .66 .87 .83 .75
BLOCKERRCATS -.64 -.75 -.88 -.74 .75
WORDBLOCKEC -.66 -.67 -.85 -.77 .74
XENERRCATS -.66 -.64 -.87 -.77 .74
SAGAN-STS .66 n/a .91 n/a n/a
Table 7: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average absolute
value.
22
E
N
-C
Z
-
10
S
Y
S
T
E
M
S
E
N
-D
E
-
22
S
Y
S
T
E
M
S
E
N
-E
S
-
15
S
Y
S
T
E
M
S
E
N
-F
R
-
17
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
System-level correlation for translations out of English
SIMPBLEU .83 .46 .42 .94 .66
BLOCKERRCATS -.65 -.53 -.47 -.93 .64
ENXERRCATS -.74 -.38 -.47 -.93 .63
POSF .80 .54 .37 .69 .60
WORDBLOCKEC -.71 -.37 -.47 -.81 .59
TERRORCAT .65 .48 .58 .53 .56
AMBER .71 .25 .50 .75 .55
TER -.69 -.41 -.45 -.66 .55
METEOR .73 .18 .45 .82 .54
BLEU .80 .22 .40 .71 .53
SEMPOS .52 n/a n/a n/a n/a
Table 8: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average abso-
lute value.
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher absolute value
for ? is making predictions that are more similar to
the human judgments than an automatic evaluation
metric with a lower absolute ?.
The system-level correlations are shown in Ta-
ble 7 for translations into English, and Table 8 out
of English, sorted by average correlation across the
language pairs. The highest correlation for each
language pair and the highest overall average are
bolded. Once again this year, many of the metrics
had stronger correlation with human judgments than
BLEU. The metrics that had the strongest correlation
this year were SEMPOS for the into English direc-
tion and SIMPBLEU for the out of English direc-
tion.
5.2 Segment-Level Metric Analysis
We measured the metrics? segment-level scores with
the human rankings using Kendall?s tau rank corre-
F
R
-E
N
(1
15
94
PA
IR
S
)
D
E
-E
N
(1
19
34
PA
IR
S
)
E
S
-E
N
(9
79
6
PA
IR
S
)
C
S
-E
N
(1
10
21
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations into English
SPEDE07-PP .26 .28 .26 .21 .25
METEOR .25 .27 .25 .21 .25
AMBER .24 .25 .23 .19 .23
SIMPBLEU .19 .17 .19 .13 .17
TERRORCAT .18 .19 .18 .19 .19
XENERRCATS .17 .18 .18 .13 .17
POSF .16 .18 .15 .12 .15
WORDBLOCKEC .15 .16 .17 .13 .15
BLOCKERRCATS .07 .08 .08 .06 .07
SAGAN-STS n/a n/a .21 .20 n/a
Table 9: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average correla-
tion.
E
N
-F
R
(1
15
62
PA
IR
S
)
E
N
-D
E
(1
45
53
PA
IR
S
)
E
N
-E
S
(1
18
34
PA
IR
S
)
E
N
-C
S
(1
88
05
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations out of English
METEOR .26 .18 .21 .16 .20
AMBER .23 .17 .22 .15 .19
TERRORCAT .18 .19 .18 .18 .18
SIMPBLEU .2 .13 .18 .10 .15
ENXERRCATS .20 .11 .17 .09 .14
POSF .15 .13 .15 .13 .14
WORDBLOCKEC .19 .1 .17 .1 .14
BLOCKERRCATS .13 .04 .12 .01 .08
Table 10: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average corre-
lation.
23
lation coefficient. We calculated Kendall?s tau as:
? =
num concordant pairs - num discordant pairs
total pairs
where a concordant pair is a pair of two translations
of the same segment in which the ranks calculated
from the same human ranking task and from the cor-
responding metric scores agree; in a discordant pair,
they disagree. In order to account for accuracy- vs.
error-based metrics correctly, counts of concordant
vs. discordant pairs were calculated specific to these
two metric types. The possible values of ? range
between 1 (where all pairs are concordant) and ?1
(where all pairs are discordant). Thus an automatic
evaluation metric with a higher value for ? is mak-
ing predictions that are more similar to the human
judgments than an automatic evaluation metric with
a lower ? .
We did not include cases where the human rank-
ing was tied for two systems. As the metrics produce
absolute scores, compared to five relative ranks in
the human assessment, it would be potentially un-
fair to the metric to count a slightly different met-
ric score as discordant with a tie in the relative hu-
man rankings. A tie in automatic metric rank for
two translations was counted as discordant with two
corresponding non-tied human judgments.
The correlations are shown in Table 9 for trans-
lations into English, and Table 10 out of English,
sorted by average correlation across the four lan-
guage pairs. The highest correlation for each lan-
guage pair and the highest overall average are
bolded. For the into English direction SPEDE and
METEOR tied for the highest segment-level correla-
tion. METEOR performed the best for the out of En-
glish direction, with AMBER doing admirably well
in both the into- and the out-of-English directions.
6 Quality Estimation task
Quality estimation aims to provide a quality indica-
tor for machine translated sentences at various gran-
ularity levels. It differs from MT evaluation, because
quality estimation techniques do not rely on refer-
ence translations. Instead, quality estimation is gen-
erally addressed using machine learning techniques
to predict quality scores. Potential applications of
quality estimation include:
? Deciding whether a given translation is good
enough for publishing as is
? Informing readers of the target language only
whether or not they can rely on a translation
? Filtering out sentences that are not good
enough even for post-editing by professional
translators
? Selecting the best translation among options
from multiple systems.
This shared-task provides a first common ground
for development and comparison of quality estima-
tion systems, focusing on sentence-level estimation.
It provides training and test datasets, along with
evaluation metrics and a baseline system. The goals
of this shared task are:
? To identify new and effective quality indicators
(features)
? To identify alternative machine learning tech-
niques for the problem
? To test the suitability of the proposed evalua-
tion metrics for quality estimation systems
? To establish the state of the art performance in
the field
? To contrast the performance of regression and
ranking techniques.
The task provides datasets for a single language
pair, text domain and MT system: English-Spanish
news texts produced by a phrase-based SMT sys-
tem (Moses) trained on Europarl and News Com-
mentaries corpora provided in the WMT10 transla-
tion task. As training data, translations were man-
ually annotated for quality in terms of post-editing
effort (1-5 scores) and were provided together with
their source sentences, reference translations, and
post-edited translations (Section 6.1). The shared-
task consisted on automatically producing quality-
estimations for a blind test-set, where English source
sentences and their MT-translations were used as in-
puts. Hidden (and subsequently publicly-released)
manual effort-annotations of those translations (ob-
tained in the same fashion as for the training data)
24
were used as reference labels to evaluate the per-
formance of the participating systems (Section 6.1).
Participants also had full access to the translation
engine-related resources (Section 6.1) and could use
any additional external resources. We have also pro-
vided a software package to extract baseline quality
estimation features (Section 6.3).
Participants could submit up to two systems for
two variations of the task: ranking, where par-
ticipants submit a ranking of translations (no ties
allowed), without necessarily giving any explicit
scores for translations, and scoring, where partici-
pants submit a score for each sentence (in the [1,5]
range). Each of these subtasks is evaluated using
specific metrics (Section 6.2).
6.1 Datasets and resources
Training data
The training data used was selected from data
available from previous WMT shared-tasks for
machine-translation: a subset of the WMT10
English-Spanish test set, and a subset of the WMT09
English-Spanish test set, for a total of 1832 sen-
tences.
The training data consists of the following re-
sources:
? English source sentences
? Spanish machine-translation outputs, created
using the SMT Moses engine
? Effort scores, created by using three profes-
sional post-editors using guidelines describ-
ing Post-Editing (PE) effort from highest effort
(score 1) to lowest effort (score 5)
? Post-Editing output, created by a pool of pro-
fessional post-editors starting from the source
sentences and the Moses translations; these PE
outputs were created before the effort scores
were elicited, and were shown to the PE-effort
judges to facilitate their effort estimates
? Spanish translation outputs, created as part of
the WMT machine-translation shared-task as
reference translations for the English source
sentences (independent of any MT output).
The guidelines used by the PE-effort judges to as-
sign scores 1-5 for each of the ?source, MT-output,
PE-output? triplets are the following:
[1] The MT output is incomprehensible, with lit-
tle or no information transferred accurately. It
cannot be edited, needs to be translated from
scratch.
[2] About 50-70% of the MT output needs to be
edited. It requires a significant editing effort in
order to reach publishable level.
[3] About 25-50% of the MT output needs to be
edited. It contains different errors and mis-
translations that need to be corrected.
[4] About 10-25% of the MT output needs to be
edited. It is generally clear and intelligible.
[5] The MT output is perfectly clear and intelligi-
ble. It is not necessarily a perfect translation,
but requires little or no editing.
Providing reliable effort estimates turned out to
be a difficult task for the PE-effort judges, even in
the current set-up (with post edited outputs available
for consultation). To eliminate some of the noise
from these judgments, we performed an intermedi-
ate cleaning step, in which we eliminated the sen-
tences for which the difference between the max-
imum score and the minimum score assigned be-
tween the three judges was > 1. We started the
data-creation process from a total of 2000 sentences
for the training set, and the final 1832 sentences we
selected as training data were the ones that passed
through this intermediate cleaning step.
Besides score disagreement, we noticed another
trend on the human judgements of PE-effort. Some
judges tend to give more moderate scores (in the
middle of available range), while others like to com-
mit also to scores that are more in the extremes of
the available range. Since the quality estimation task
would be negatively influenced by having most of
the scores in the middle of the range, we have chosen
to compute the final effort scores as an weighted av-
erage between the three PE-effort scores, with more
weight given to the judges with higher standard de-
viation from their own mean score. We have used
25
weights 3, 2, and 1 for the three PE-effort judges ac-
cording to this criterion. There is an additional ad-
vantage resulting from this weighted average score:
instead of obtaining average numbers only at val-
ues x.0, x.33, and x.66 (for unweighted average)7,
the weighted averages are spread more evenly in the
range [1, 5].
A few variations of the training data were pro-
vided, including version with cases restored and a
version detokenized. In addition, engine-internal
information from Moses such as phrase and word
alignments, detailed model scores, etc. (parameter
-trace), n-best lists and stack information from the
search graph as a word graph (parameter -output-
word-graph) as produced by the Moses engine were
provided.
The rationale behind releasing this engine-
internal data was to make it possible for this shared-
task to address quality estimation using a glass-box
approach, that is, making use of information from
the internal workings of the MT engine.
Test data
The test data was a subset of the WMT12 English-
Spanish test set, consisting of 442 sentences. The
test data consists of the following files:
? English source sentences
? Spanish machine-translation outputs, created
using the same SMT Moses engine used to cre-
ate the training data
? Effort scores, created by using three profes-
sional post-editors8 using guidelines describing
PE effort from highest effort (score 1) to lowest
effort (score 5)
The first two files were the input for the quality-
estimation shared-task participating systems. Since
the Moses engine used to create the MT outputs was
the same as the one used for generating the train-
ing data, the engine-internal resources are the same
7These three values are the only ones possible given the
cleaning step we perform prior to averaging the scores, which
ensures that the difference between the maximum score and the
minimum score is at most 1.
8The same post-editors that were used to create the training
data were used to create the test data.
as the ones we released as part of the training data
package.
The effort scores were released after the partic-
ipants submitted their shared-task submission, and
were solely used to evaluate the submissions accord-
ing to the established metrics. The guidelines used
by the PE-effort judges to assign 1-5 scores were the
same as the ones used for creating the training data.
We have used the same criteria to ensure the con-
sistency of the human judgments. The initial set of
candidates consisted of 604 sentences, of which only
442 met this criteria. The final scores used as gold-
values have been obtained using the same weighted-
average scheme as for the training data.
Resources
In addition to the training and test materials, we
made several additional resources that were used for
the baseline QE system and/or the SMT system that
produced the training and test datasets:
? The SMT training corpus: source and target
sides of the corpus used to train the Moses en-
gine. These are a concatenation of the Eu-
roparl and the news-commentary data sets from
WMT10 that were tokenized, cleaned (remov-
ing sentences longer than 80 tokens) and true-
cased.
? Two Language models: 5-gram LM generated
from the interpolation of the two target cor-
pora after tokenization and truecasing (used
by Moses) and a trigram LM generated from
the two source corpora and filtered to remove
singletons (used by the baseline QE system).
We also provided unigram, bigram and trigram
counts (used in the baseline QE system).
? An IBM Model 1 table that generated by
Giza++ using the SMT training corpora.
? A word-alignment file as produced by the
grow-diag-final heuristic in Moses for the SMT
training set.
? A phrase table with word alignment informa-
tion generated from the parallel corpora.
? The Moses configuration file used for decod-
ing.
26
6.2 Evaluation metrics
Ranking metrics
For the ranking task, we defined a novel met-
ric that provides some advantages over a more tra-
ditional ranking metrics like Spearman correlation.
Our metric, called DeltaAvg, assumes that the refer-
ence test set has a number associated with each en-
try that represents its extrinsic value. For instance,
using the effort scale we described in Section 6.1,
we associate a value between 1 and 5 with each
sentence, representing the quality of that sentence.
Given these values, our metric does not need an ex-
plicit reference ranking, the way the Spearman rank-
ing correlation does.9 The goal of the DeltaAvg met-
ric is to measure how valuable a proposed ranking
(which we call a hypothesis ranking) is according to
the extrinsic values associated with the test entries.
We first define a parameterized version of this
metric, called DeltaAvg[n]. The following notations
are used: for a given entry sentence s, V (s) repre-
sents the function that associates an extrinsic value
to that entry; we extend this notation to a set S, with
V (S) representing the average of all V (s), s ? S.
Intuitively, V (S) is a quantitative measure of the
?quality? of the set S, as induced by the extrinsic
values associated with the entries in S. For a set
of ranked entries S and a parameter n, we denote
by S1 the first quantile of set S (the highest-ranked
entries), S2 the second quantile, and so on, for n
quantiles of equal sizes.10 We also use the notation
Si,j =
?j
k=i Sk. Using these notations, we define:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1
? V (S) (14)
When the valuation function V is clear from the con-
text, we write DeltaAvg[n] for DeltaAvgV [n]. The
parameter n represents the number of quantiles we
want to split the set S into. For instance, n = 2
gives DeltaAvg[2] = V (S1) ? V (S), hence it mea-
sures the difference between the quality of the top
9A reference ranking can be implicitly induced according to
these values; if, as in our case, higher values mean better sen-
tences, then the reference ranking is defined such that higher-
scored sentences rank higher than lower-scored sentences.
10If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
quantile (top half) S1 and the overall quality (rep-
resented by V (S)). For n = 3, DeltaAvg[3] =
(V (S1)+V (S1,2)/2?V (S) = ((V (S1)?V (S))+
(V (S1,2 ? V (S)))/2, hence it measures an aver-
age difference across two cases: between the quality
of the top quantile (top third) and the overall qual-
ity, and between the quality of the top two quan-
tiles (S1?S2, top two-thirds) and the overall quality.
In general, DeltaAvg[n] measures an average differ-
ence in quality across n ? 1 cases, with each case
measuring the impact in quality of adding an addi-
tional quantile, from top to bottom. Finally, we de-
fine:
DeltaAvgV =
?N
n=2 DeltaAvgV [n]
N ? 1
(15)
whereN = |S|/2. As before, we write DeltaAvg for
DeltaAvgV when the valuation function V is clear
from the context. The DeltaAvg metric is an aver-
age across all DeltaAvg[n] values, for those n values
for which the resulting quantiles have at least 2 en-
tries (no singleton quantiles). The DeltaAvg metric
has some important properties that are desired for a
ranking metric (see Section 6.4 for the results of the
shared-task that substantiate these claims):
? it is non-parametric (i.e., it does not depend on
setting particular parameters)
? it is automatic and deterministic (and therefore
consistent)
? it measures the quality of a hypothesis rank-
ing from an extrinsic perspective (as offered by
function V )
? its values are interpretable: for a given set of
ranked entries, a value DeltaAvg of 0.5 means
that, on average, the difference in quality be-
tween the top-ranked quantiles and the overall
quality is 0.5
? it has a high correlation with the Spearman rank
correlation coefficient, which makes it as use-
ful as the Spearman correlation, with the added
advantage of its values being extrinsically in-
terpretable.
27
In the rest of this paper, we present results for
DeltaAvg using as valuation function V the Post-
Editing effort scores, as defined in Section 6.1.
We also report the results of the ranking task using
the more-traditional Spearman correlation.
Scoring metrics
For the scoring task, we use two metrics that have
been traditionally used for measuring performance
for regression tasks: Mean Absolute Error (MAE) as
a primary metric, and Root of Mean Squared Error
(RMSE) as a secondary metric. For a given test set
S with entries si, 1 ? i ? |S|, we denote by H(si)
the proposed score for entry si (hypothesis), and by
V (si) the reference value for entry si (gold-standard
value). We formally define our metrics as follows:
MAE =
?N
i=1 |H(si)? V (si)|
N
(16)
RMSE =
?
?N
i=1(H(si)? V (si))
2
N
(17)
where N = |S|. Both these metrics are non-
parametric, automatic and deterministic (and there-
fore consistent), and extrinsically interpretable. For
instance, a MAE value of 0.5 means that, on aver-
age, the absolute difference between the hypothe-
sized score and the reference score value is 0.5. The
interpretation of RMSE is similar, with the differ-
ence that RMSE penalizes larger errors more (via
the square function).
6.3 Participants
Eleven teams (listed in Table 11) submitted one or
more systems to the shared task, with most teams
submitting for both ranking and scoring subtasks.
Each team was allowed up to two submissions (for
each subtask). In the descriptions below participa-
tion in the ranking is denoted (R) and scoring is de-
noted (S).
Baseline system (R, S): the baseline system used
the feature extraction software (also provided
to all participants). It analyzed the source and
translation files and the SMT training corpus
to extract the following 17 system-independent
features that were found to be relevant in previ-
ous work (Specia et al, 2009):
? number of tokens in the source and target
sentences
? average source token length
? average number of occurrences of the tar-
get word within the target sentence
? number of punctuation marks in source
and target sentences
? LM probability of source and target sen-
tences using language models described in
Section 6.1
? average number of translations per source
word in the sentence: as given by IBM 1
model thresholded so that P (t|s) > 0.2,
and so that P (t|s) > 0.01 weighted by
the inverse frequency of each word in the
source side of the SMT training corpus
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower fre-
quency words) and 4 (higher frequency
words) in the source side of the SMT train-
ing corpus
? percentage of unigrams in the source sen-
tence seen in the source side of the SMT
training corpus
These features are used to train a Support Vec-
tor Machine (SVM) regression algorithm using
a radial basis function kernel with the LIBSVM
package (Chang and Lin, 2011). The ?,  and C
parameters were optimized using a grid-search
and 5-fold cross validation on the training set.
We note that although the system is referred to
as a ?baseline?, it is in fact a strong system.
Although it is simple it has proved to be ro-
bust across a range of language pairs, MT sys-
tems, and text domains. It is a simpler variant
of the system used in (Specia, 2011). The ratio-
nale behind having such a strong baseline was
to push systems to exploit alternative sources
of information and combination / learning ap-
proaches.
SDLLW (R, S): Both systems use 3 sets of fea-
tures: the 17 baseline features, 8 system-
dependent features from the decoder logs of
Moses, and 20 features developed internally.
Some of these features made use of additional
data and/or resources, such as a secondary
28
ID Participating team
PRHLT-UPV Universitat Politecnica de Valencia, Spain (Gonza?lez-Rubio et al, 2012)
UU Uppsala University, Sweden (Hardmeier et al, 2012)
SDLLW SDL Language Weaver, USA (Soricut et al, 2012)
Loria LORIA Institute, France (Langlois et al, 2012)
UPC Universitat Politecnica de Catalunya, Spain (Pighin et al, 2012)
DFKI DFKI, Germany (Avramidis, 2012)
WLV-SHEF University of Wolverhampton & University of Sheffield, UK (Felice and Specia, 2012)
SJTU Shanghai Jiao Tong University, China (Wu and Zhao, 2012)
DCU-SYMC Dublin City University, Ireland & Symantec, Ireland (Rubino et al, 2012)
UEdin University of Edinburgh, UK (Buck, 2012)
TCD Trinity College Dublin, Ireland (Moreau and Vogel, 2012)
Table 11: Participants in the WMT12 Quality Evaluation shared task.
MT system that was used as pseudo-reference
for the hypothesis, and POS taggers for both
languages. Feature-selection algorithms were
used to select subsets of features that directly
optimize the metrics used in the task. System
?SDLLW M5PbestAvgDelta? uses a resulting
15-feature set optimized towards the AvgDelta
metric. It employs an M5P model to learn a
decision-tree with only two linear equations.
System ?SDLLW SVM? uses a 20-feature set
and an SVM epsilon regression model with ra-
dial basis function kernel with parameters C,
gamma, and epsilon tuned on a development
set (305 training instances). The model was
trained with 10-fold cross validation and the
tuning process was restarted several times us-
ing different starting points and step sizes to
avoid overfitting. The final model was selected
based on its performance on the development
set and the number of support vectors.
UU (R, S): System ?UU best? uses the 17 base-
line features, plus 82 features from Hardmeier
(2011) (with some redundancy and some over-
lap with baseline features), and constituency
trees over input sentences generated by the
Stanford parser and dependency trees over both
input and output sentences generated by the
MaltParser. System ?UU bltk? uses only the
17 baseline features plus constituency and de-
pendency trees as above. The machine learn-
ing component in both cases is SVM regres-
sion (SVMlight software). For the ranking task,
the ranking induced by the regression output
is used. The system uses polynomial kernels
of degree 2 (UU best) and 3 (UU bltk) as well
as two different types of tree kernels for con-
stituency and dependency trees, respectively.
The SVM margin/error trade-off, the mixture
proportion between tree kernels and polyno-
mial kernels and the degree of the polynomial
kernels were optimised using grid search with
5-fold cross-validation over the training set.
TCD (R, S): ?TCD M5P-resources-only? uses
only the baseline features, while ?TCD M5P-
all? uses the baseline and additional features.
A number of metrics (used as features in
TCD M5P-all) were proposed which work in
the following way: given a sentence to eval-
uate (source sentence for complexity or target
sentence for fluency), it is compared against
some reference data using similarity mea-
sures (various metrics which compare distri-
butions of n-grams). The training data was
used as reference, along with the Google n-
grams dataset. Several learning methods were
tested using Weka on the training data (10-
fold cross-validation). The system submission
uses the M5P (regression with decision trees)
algorithm which performed best. Contrary to
what had been observed on the training data
using cross-validation, ?TCD M5P-resources-
only? performs better than ?TCD M5P-all? on
the test data.
29
PRHLT-UPV (R, S): The system addresses the
task using a regression algorithm with 475 fea-
tures, including the 17 the baseline features.
Most of the features are defined as word scores.
Among them, the features obtained form a
smoothed naive Bayes classifier have shown to
be particularly interesting. Different methods
to combine word-level scores into sentence-
level features were investigated. For model
building, SVM regression was used. Given
the large number of features, the training data
provided as part of the task was insufficient
yielding unstable systems with not so good per-
formance. Different feature selection methods
were implemented to determine a subset of rel-
evant features. The final submission used these
relevant features to train an SVM system whose
parameters were optimized with respect to the
final evaluation metrics.
UEDIN (R, S): The system uses the baseline fea-
tures along with some additional features: bi-
nary features for named entities in source using
Stanford NER Tagger; binary indicators for oc-
currence of quotes or parenthetical segments,
words in upper case and numbers; geometric
mean of target word probabilities and proba-
bility of worst scoring word under a Discrim-
inative Word Lexicon Model; Sparse Neural
Network directly mapping from source to tar-
get (using the vector space model) with source
and target side either filtered to relevant words
or hashed to reduce dimensionality; number of
times at least a 3-gram is seen normalized by
sentence length; and Levenshtein distance of
either source or translation to closest entry of
the SMT training corpus on word or character
level. An ensemble of neural networks opti-
mized for RMSE was used for prediction (scor-
ing) and ranking. The contribution of new fea-
tures was tested by adding them to the baseline
features using 5-fold cross-validation. Most
features did not result in any improvement over
the baseline. The final submission was a com-
bination of all feature sets that showed im-
provement.
SJTU (R, S): The task is treated as a regression
problem using the epsilon-SVM method. All
features are extracted from the official data, in-
volving no external NLP tools/resources. Most
of them come from the phrase table, decod-
ing data and SMT training data. The focus
is on special word relations and special phrase
patterns, thus several feature templates on this
topic are extracted. Since the training data is
not large enough to assign weights to all fea-
tures, methods for estimating common strings
or sequences of words are used. The training
data is divided in 3/4 for training and 1/4 for
development to filter ineffective features. Be-
sides the baseline features, the final submission
contains 18 feature templates and about 4 mil-
lion features in total.
WLV-SHEF (R, S): The systems integrates novel
linguistic features from the source and target
texts in an attempt to overcome the limitations
of existing shallow features for quality estima-
tion. These linguistically-informed features in-
clude part-of-speech information, phrase con-
stituency, subject-verb agreement and target
lexicon analysis, which are extracted using
parsers, corpora and auxiliary resources. Sys-
tems are built using epsilon-SVM regression
with parameters optimised using 5-fold cross-
validation on the training set and two differ-
ent feature sets: ?WLV-SHEF BL? uses the 17
baseline features plus 70 linguistically inspired
features, while ?WLV-SHEF FS? uses a larger
set of 70 linguistic plus 77 shallow features (in-
cluding the baseline). Although results indicate
that the models fall slightly below the baseline,
further analysis shows that linguistic informa-
tion is indeed informative and complementary
to shallow indicators.
DFKI (R, S): ?DFKI morphPOSibm1LM? (R) is
a simple linear interpolation of POS 6-gram
language model scores, morpheme 6-gram lan-
guage model scores, IBM 1 scores (both ?di-
rect? and ?inverse?) for POS 4-grams and for
morphemes. The parallel News corpora from
WMT10 is used as extra data to train the lan-
guage model and the IBM 1 model. ?DFKI cfs-
30
plsreg? and ?DFKI grcfs-mars? (S) use a col-
lection of 264 features generated containing
the baseline features and additional resources.
Numerous methods of feature selection were
tested using 10-fold cross validation on the
training data, reducing these to 23 feature sets.
Several regression and (discretized) classifica-
tion algorithms were employed to train predic-
tion models. The best-performing models in-
cluded features derived from PCFG parsing,
language quality checking and LM scoring, of
both source and target, besides features from
the SMT search graph and a few baseline fea-
tures. ?DFKI cfs-plsreg? uses a Best First
correlation-based feature selection technique,
trained with Partial Least Squares Regression,
while ?DFKI grcfs-mars? uses a Greedy Step-
wise correlation-based feature selection tech-
nique, trained with multivariate adaptive re-
gression splines.
DCU-SYMC (R, S): Systems are based on a clas-
sification approach using a set of features that
includes the baseline features. The manually
assigned quality scores provided for each MT
output in the training set were rounded in or-
der to apply classification algorithms on a lim-
ited set of classes (integer values from 1 to 5).
Three classifiers were combined by averaging
the predicted classes: SVM using sequential
minimal optimization and RBF kernel (parame-
ters optimized by grid search), Naive Bayes and
Random Forest. ?DCU-SYMC constrained? is
based on a set of 70 features derived only from
the data provided for the task. These include
a set of features which attempt to model trans-
lation adequacy using a bilingual topic model
built using Latent Dirichlet Allocation. ?DCU-
SYMC unconstrained? is based on 308 fea-
tures including the constrained ones and oth-
ers extracted using external tools: grammatical-
ity features extracted from the source segments
using the TreeTagger part-of-speech tagger, an
English precision grammar, the XLE parser and
the Brown re-ranking parser and features based
on part-of-speech tag counts extracted from the
MT output using a Spanish TreeTagger model.
Loria (S): Several numerical or boolean features
are computed from the source and target sen-
tences and used to train an SVM regression al-
gorithm with linear (?Loria SVMlinear?) and
radial basis function (?Loria SVMrbf?) as ker-
nel. For the radial basis function, a grid search
is performed to optimise the parameter ?. The
official submission use the baseline features
and a number of features proposed in previous
work (Raybaud et al, 2011), amounting to 66
features. A feature selection algorithm is used
in order to remove non-informative features.
No additional data other than that provided for
the shared task is considered. The training data
is split into a training part (1000 sentences) and
a development part (832 sentences) to learn the
regression model and optimise the parameters
of the regression and for feature selection.
UPC (R, S): The systems use several features on
top of the baseline features. These are mostly
based on different language models estimated
on reference and automatic Spanish transla-
tions of the news-v7 corpus. The automatic
translations are generated by the system used
for the shared task. N-gram LMs are esti-
mated on word forms, POS tags, stop words
interleaved by POS tags, stop-word patterns,
plus variants in which the POS tags are re-
placed with the stem or root of each target
word. The POS tags on the target side are ob-
tained by projecting source side annotations via
automatic alignments. The resulting features
are: the perplexity of each additional language
model, according to the two translations, and
the ratio between the two perplexities. Addi-
tionally, features that estimate the likelihood
of the projection of dependency parses on the
two translations are encoded. For learning, lin-
ear SVM regression is used. Optimization was
done via 5-fold cross-validation on a develop-
ment data. Features are encoded by means of
their z-scores, i.e. how many standard devia-
tions the observed value is above or below the
mean. A variant of the system, ?UPC-2? uses
an option of SVMLight that removes inconsis-
tent points from the training set and retrains the
model until convergence.
31
6.4 Results
Here we give the official results for the ranking and
scoring subtasks followed by a discussion that high-
lights the main findings of the task.
Ranking subtask
Table 12 gives the results for the ranking sub-
task. The table is sorted from best to worse using
the DeltaAvg metric scores (Equation 15) as pri-
mary key and the Spearman correlation scores as
secondary key.
The winning submissions for the ranking subtask
are SDLLW?s M5PbestDeltaAvg and SVM entries,
which have DeltaAvg scores of 0.63 and 0.61, re-
spectively. The difference with respect to all the
other submissions is statistically significant at p =
0.05, using pairwise bootstrap resampling (Koehn,
2004). The state-of-the-art baseline system has a
DeltaAvg score of 0.55 (Spearman rank correla-
tion of 0.58). Five other submissions have perfor-
mances that are not different from the baseline at a
statistically-significant level (p = 0.05), as shown
by the gray area in the middle of Table 12. Three
submissions scored higher than the baseline system
at p = 0.05 (systems above the middle gray area),
which indicates that this shared-task succeeded in
pushing the state-of-the-art performance to new lev-
els. The range of performance for the submissions
in the ranking task varies from a DeltaAvg of 0.65
down to a DeltaAvg of 0.15 (with Spearman values
varying from 0.64 down to 0.19).
In addition to the performance of the official sub-
mission, we report here results obtained by var-
ious oracle methods. The oracle methods make
use of various metrics that are associated in a or-
acle manner to the test input: the gold-label Ef-
fort metric for ?Oracle Effort?, the HTER metric
computed against the post-edited translations as ref-
erence for ?Oracle HTER?, and the BLEU metric
computed against the same post-edited translations
as reference for ?Oracle (H)BLEU?.11 The ?Oracle
Effort? DeltaAvg score of 0.95 gives an upperbound
in terms of DeltaAvg for the test set used in this
evaluation. It basically indicates that, for this set,
11We use the (H)BLEU notation to underscore the use of
Post-Edited translations as reference, as opposed to using ref-
erences that are not the product of a Post-Editing process, as for
the traditional BLEU metric.
the difference in PE effort between the top-quality
quantiles and the overall quality is 0.95 on average.
We would like to emphasize here that the DeltaAvg
metric does not have any a-priori range for its values.
The upperbound, for instance, is test-dependent, and
therefore an ?Oracle Effort? score is useful for un-
derstanding the performance level of real system-
submissions. The ?Oracle HTER? DeltaAvg score
of 0.77 is a more realistic upperbound for the cur-
rent set. Since the HTER metric is considered a
good approximation for the effort required in post-
editing, ranking the test set based on the HTER
scores (from lowest HTER to highest HTER) pro-
vides a good oracle comparison point. The oracle
based on (H)BLEU gives a lower DeltaAvg score,
which can be interpreted to mean that the BLEU
metric provides a lower correlation to post-editing
effort compared to HTER. We also note here that
there is room for improvement between the highest-
scoring submission (at DeltaAvg 0.63) and the ?Ora-
cle HTER? DeltaAvg score of 0.77. We are not sure
if this difference can be bridged completely, but hav-
ing measured a quantitative difference between the
current best-performance and a realistic upperbound
is an important achievement of this shared-task.
Scoring subtask
The results for the scoring task are presented in
Table 13, sorted from best to worse by using the
MAE metric scores (Equation 16) as primary key
and the RMSE metric scores (Equation 17) as sec-
ondary key.
The winning submission is SDLLW?s
M5PbestDeltaAvg, with an MAE of 0.61 and
an RMSE of 0.75 (the difference with respect to
all the other submissions is statistically significant
at p = 0.05, using pairwise bootstrap resam-
pling (Koehn, 2004)). The strong, state-of-the-art
quality-estimation baseline system is measured to
have an MAE of 0.69 and RMSE of 0.82, with six
other submissions having performances that are
not different from the baseline at a statistically-
significant level (p = 0.05), as shown by the gray
area in the middle of Table 13). Five submissions
scored higher than the baseline system at p = 0.05
(systems above the middle gray area), which
indicates that this shared-task also succeeded in
pushing the state-of-the-art performance to new
32
System ID DeltaAvg Spearman Corr
? SDLLW M5PbestDeltaAvg 0.63 0.64
? SDLLW SVM 0.61 0.60
UU bltk 0.58 0.61
UU best 0.56 0.62
TCD M5P-resources-only* 0.56 0.56
Baseline (17FFs SVM) 0.55 0.58
PRHLT-UPV 0.55 0.55
UEdin 0.54 0.58
SJTU 0.53 0.53
WLV-SHEF FS 0.51 0.52
WLV-SHEF BL 0.50 0.49
DFKI morphPOSibm1LM 0.46 0.46
DCU-SYMC unconstrained 0.44 0.41
DCU-SYMC constrained 0.43 0.41
TCD M5P-all* 0.42 0.41
UPC 1 0.22 0.26
UPC 2 0.15 0.19
Oracle Effort 0.95 1.00
Oracle HTER 0.77 0.70
Oracle (H)BLEU 0.71 0.62
Table 12: Official results for the ranking subtask of the WMT12 Quality Evaluation shared-task. The winning submis-
sions are indicated by a ? (the difference with respect to other systems is statistically significant with p = 0.05). The
systems in the gray area are not significantly different from the baseline system. Entries with * represent submissions
for which a bug-fix was applied after the submission deadline.
33
System ID MAE RMSE
? SDLLW M5PbestDeltaAvg 0.61 0.75
UU best 0.64 0.79
SDLLW SVM 0.64 0.78
UU bltk 0.64 0.79
Loria SVMlinear 0.68 0.82
UEdin 0.68 0.82
TCD M5P-resources-only* 0.68 0.82
Baseline (17FFs SVM) 0.69 0.82
Loria SVMrbf 0.69 0.83
SJTU 0.69 0.83
WLV-SHEF FS 0.69 0.85
PRHLT-UPV 0.70 0.85
WLV-SHEF BL 0.72 0.86
DCU-SYMC unconstrained 0.75 0.97
DFKI grcfs-mars 0.82 0.98
DFKI cfs-plsreg 0.82 0.99
UPC 1 0.84 1.01
DCU-SYMC constrained 0.86 1.12
UPC 2 0.87 1.04
TCD M5P-all 2.09 2.32
Oracle Effort 0.00 0.00
Oracle HTER (linear mapping into [1.5-5.0]) 0.56 0.73
Oracle (H)BLEU (linear mapping into [1.5-5.0]) 0.61 0.84
Table 13: Official results for the scoring subtask of the WMT12 Quality Evaluation shared-task. The winning submis-
sion is indicated by a ? (the difference with respect to the other submissions is statistically significant at p = 0.05).
The systems in the gray area are not different from the baseline system at a statistically significant level (p = 0.05).
Entries with * represent submissions for which a bug-fix was applied after the submission deadline.
34
levels in terms of absolute scoring. The range of
performance for the submissions in the scoring task
varies from an MAE of 0.61 up to an MAE of 0.87
(the outlier MAE of 2.09 is reportedly due to bugs).
We also calculate scoring Oracles using the meth-
ods used for the ranking Oracles. The difference is
that the HTER and (H)BLEU oracles need a way
of mapping their scores (which are usually in the
[0, 100] range) into the [1, 5] range. For the compar-
ison here, we did the mapping by excluding the 5%
top and bottom outlier scores, and then linearly map-
ping the remaining range into the [1.5, 5] range. The
?Oracle Effort? scores are not very indicative in this
case. However, the ?Oracle HTER? MAE score of
0.56 is a somewhat realistic lowerbound for the cur-
rent set (although the score could be decreased by a
smarter mapping from the HTER range to the Effort
range). We argue that since the HTER metric is con-
sidered a good approximation for the effort required
in post-editing, effort-like scores derived from the
HTER score provide a good way to compute oracle
scores in a deterministic manner. Note that again
the oracle based on (H)BLEU gives a worse MAE
score at 0.61, which support the interpretation that
the (H)BLEU metric provides a lower correlation
to post-editing effort compared to (H)TER. Over-
all, we consider the MAE values for these HTER
and (H)BLEU-based oracles to indicate high error
margins. Most notably the performance of the best
system gets the same MAE score as the (H)BLEU
oracle, at 0.61 MAE. We take this to mean that the
scoring task is more difficult compared to the rank-
ing task, since even oracle-based solutions get high
error scores.
6.5 Discussion
When looking back at the goals that we identified for
this shared-task, most of them have been success-
fully accomplished. In addition, we have achieved
additional ones that were not explicitly stated from
the beginning. In this section, we discuss the accom-
plishments of this shared-task in more detail, start-
ing from the defined goals and beyond.
Identify new and effective quality indicators
The vast majority of the participating systems use
external resources in addition to those provided for
the task, such as parsers, part-of-speech taggers,
named entity recognizers, etc. This has resulted in
a wide variety of features being used. Many of the
novel features have tried to exploit linguistically-
oriented features. While some systems did not
achieve improvements over the baseline while ex-
ploiting such features, others have (the ?UU? sub-
missions, for instance, exploiting both constituency
and dependency trees).
Another significant set of features that has been
previously overlooked is the feature set of the MT
decoder. Considering statistical engines, these fea-
tures are immediately available for quality predic-
tion from the internal trace of the MT decoder (in
a glass-box prediction scenario), and its contribu-
tion is significant. These features, which reflect the
?confidence? of the SMT system on the translations
it produces, have been shown to be complemen-
tary to other, system-independent (black-box) fea-
tures. For example, the ?SDLLW? submissions in-
corporate these features, and their feature selection
strategy consistently favored this feature set. The
power of this set of features alone is enough to yield
(when used with an M5P model) outputs that would
have been placed 4th in the ranking task and 5th
in the scoring task, a remarkable achievement. An-
other interesting feature used by the ?SDLLW? sub-
missions rely on pseudo-references, i.e., translations
produced by other MT systems for the same input
sentence.
Identify alternative machine learning techniques
Although SVM regression was used to compute the
baseline performance, the baseline ?system? pro-
vided for the task consisted solely of a software to
extract features, as opposed to a model built us-
ing the regression algorithm. The rationale behind
this decision was to encourage participants to exper-
iment with alternative methods for combining differ-
ent quality indicators. This was achieved to a large
extent.
The best-performing machine learning techniques
were found to be the M5P Regression Trees and the
SVM Regression (SVR) models. The merit of the
M5P Regression Trees is that it provides compact
models that are less prone to overfitting. In contrast,
the SVR models can easily overfit given the small
amount of training data available and the large num-
bers of features commonly used. Indeed, many of
35
the submissions that fell below the baseline perfor-
mance can blame overfitting for (part of) their sub-
optimal performance. However, SVR models can
achieve high performance through the use of tun-
ing and feature selection techniques to avoid overfit-
ting. Structured learning techniques were success-
fully used by the ?UU? submissions ? the second
best performing team ? to represent parse trees. This
seems an interesting direction to encode other sorts
of linguistic information about source and trans-
lation texts. Other interesting learning techniques
have been tried, such as Neural Networks, Par-
tial Least Squares Regression, or multivariate adap-
tive regression splines, but their performance does
not suggest they are strong candidates for learning
highly-performing quality-estimation models.
Test the suitability of evaluation metrics for qual-
ity estimation DeltaAvg, our proposed metric for
measuring ranking performance, proved suitable for
scoring the ranking subtask. Its high correlation with
the Spearman ranking metric, coupled with its ex-
trinsic interpretability, makes it a preferred choice
for future measurements. It is also versatile, in the
sense that the its valuation function V can change to
reflect different extrinsic measures of quality.
Establish the state of the art performance The
results on both the ranking and the scoring subtasks
established new state of the art levels on the test set
used in this shared task. In addition to these lev-
els, the oracle performance numbers also help under-
stand the current performance level, and how much
of a gap in performance there still exists. Addi-
tional data points regarding quality estimation per-
formance are needed to establish how stable this
measure of the performance gap is.
Contrast the performance of regression and
ranking techniques Most of the submissions in
the ranking task used the results provided by a re-
gression solution (submitted for the scoring task) to
infer the rankings. Also, optimizing for ranking per-
formance via a regression solution seems to result in
regression models that perform very well, as in the
case of the top-ranked submission.
6.6 Quality Estimation Conclusions
There appear to be significant differences between
considering the quality estimation task as a ranking
problem versus a scoring problem. The ranking-
based approach appears to be somewhat simpler
and more easily amenable to automatic solutions,
and at the same time provides immediate benefits
when integrated into larger applications (see, for in-
stance, the post-editing application described in Spe-
cia (2011)). The scoring-based approach is more dif-
ficult, as the high error rate even of oracle-based so-
lutions indicates. It is also well-known from human
evaluations of MT outputs that human judges also
have a difficult time agreeing on absolute-number
judgements to translations.
Our experience in creating the current datasets
confirms that, even with highly-trained profession-
als, it is difficult to arrive at consistent judge-
ments. We plan to have future investigations on
how to achieve more consistent ways of generating
absolute-number scores that reflect the quality of au-
tomated translations.
7 Summary
As in previous incarnations of this workshop we car-
ried out an extensive manual and automatic evalu-
ation of machine translation performance, and we
used the human judgements that we collected to val-
idate automatic metrics of translation quality. This
year was also the debut of a new quality estimation
task, which tries to predict the effort involved in hav-
ing post editors correct MT output. The quality es-
timation task differs from the metrics task in that it
does not involve reference translations.
As in previous years, all data sets generated by
this workshop, including the human judgments, sys-
tem translations and automatic scores, are publicly
available for other researchers to analyze.12
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-0022,
12http://statmt.org/wmt12/results.html
36
the US National Science Foundation under grant
IIS-0713448, and the CoSyne project FP7-ICT-4-
248531 funded by the European Commission. The
views and findings are the authors? alone. Thanks
for Adam Lopez for discussions about alternative
ways of ranking the overall system scores. The
Quality Estimation shared task organizers thank
Wilker Aziz for his help with the SMT models and
resources, and Mariano Felice for his help with the
system for the extraction of baseline features.
References
Eleftherios Avramidis. 2012. Quality estimation for
machine translation output using linguistic analysis
and decoding features. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A grain of salt for the wmt man-
ual evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 1?11, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ondrej Bojar, Bushra Jawaid, and Amir Kamran. 2012.
Probes in a taxonomy of factored phrase-based mod-
els. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Christian Buck. 2012. Black box features for the WMT
2012 quality estimation shared task. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion (WMT07), Prague, Czech Republic.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation (WMT09), Athens, Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT10), Uppsala, Swe-
den.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2009), Singapore.
Julio Castillo and Paula Estrella. 2012. Semantic tex-
tual similarity for MT evaluation. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27. Software available at http://www.
csie.ntu.edu.tw/?cjlin/libsvm.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving amber, an MT evaluation metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measur-
ment, 20(1):37?46.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic metric for reliable optimization and evalu-
ation of machine translation systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The CMU-avenue French-English translation
system. In Proceedings of the Seventh Workshop on
Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Ondr?ej Dus?ek, Zdene?k Z?abokrtsky?, Martin Popel, Mar-
tin Majlis?, Michal Nova?k, and David Marec?ek. 2012.
Formemes in English-Czech deep syntactic mt. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Mariano Felice and Lucia Specia. 2012. Linguistic fea-
tures for quality estimation. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
37
Mark Fishel, Rico Sennrich, Maja Popovic?, and Ondr?ej
Bojar. 2012. TerrorCat: a translation error
categorization-based MT quality metric. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Lluis Formiga, Carlos A. Henr??quez Q., Adolfo
Herna?ndez, Jose? B. Marin?o, Enric Monte, and Jose?
A. R. Fonollosa. 2012. The TALP-UPC phrase-based
translation systems for WMT12: Morphology simpli-
fication and domain adaptation. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
and Chris Callison-Burch. 2012. Joshua 4.0: Packing,
PRO, and paraphrases. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Ulrich Germann. 2012. Syntax-aware phrase-based sta-
tistical machine translation: System description. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Jesu?s Gonza?lez-Rubio, Alberto Sanch??s, and Francisco
Casacuberta. 2012. PRHLT submission to the
WMT12 quality estimation task. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
Francisco Guzman, Preslav Nakov, Ahmed Thabet, and
Stephan Vogel. 2012. QCRI at WMT12: Exper-
iments in Spanish-English and German-English ma-
chine translation of news text. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiedemann.
2012. Tree kernels for machine translation quality
estimation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Christian Hardmeier. 2011. Improving machine transla-
tion quality prediction with syntactic tree kernels. In
Proceedings of the 15th conference of the European
Association for Machine Translation, pages 233?240,
Leuven, Belgium.
Matthias Huck, Stephan Peitz, Markus Freitag, Malte
Nuhn, and Hermann Ney. 2012. The RWTH aachen
machine translation system for WMT 2012. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Sabine Hunsicker, Chen Yu, and Christian Federmann.
2012. Machine learning for hybrid machine transla-
tion. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards effec-
tive use of training data in statistical machine transla-
tion. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation, New
York, New York.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the Empirical Methods in Natural Language Process-
ing Conference.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
David Langlois, Sylvain Raybaud, and Kamel Sma??li.
2012. LORIA system for the WMT12 quality esti-
mation shared task. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,
Marianna Apidianaki, Li Gong, Aure?lien Max, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois Yvon.
2012. LIMSI @ WMT12. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Vero?nica Lo?pez-Luden?a, Rube?n San-Segundo, and
Juan M. Montero. 2012. UPM system for WMT 2012.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Adam Lopez. 2012. Putting human assessments of ma-
chine translation systems in order. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Matous? Macha?c?ek and Ondej Bojar. 2011. Approxi-
mating a deep-syntactic metric for mt evaluation and
tuning. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 373?379, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Freitag Markus, Peitz Stephan, Huck Matthias, Ney Her-
mann, Niehues Jan, Herrmann Teresa, Waibel Alex,
38
Hai-son Le, Lavergne Thomas, Allauzen Alexandre,
Buschbeck Bianka, Crego Joseph Maria, and Senel-
lart Jean. 2012. Joint WMT 2012 submission of
the QUAERO project. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Alexander Molchanov. 2012. PROMT deephybrid sys-
tem for WMT12 shared translation task. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Erwan Moreau and Carl Vogel. 2012. Quality estima-
tion: an experimental study using unsupervised simi-
larity measures. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, Montreal,
Canada, June. Association for Computational Linguis-
tics.
Jan Niehues, Yuqi Zhang, Mohammed Mediani, Teresa
Herrmann, Eunah Cho, and Alex Waibel. 2012. The
karlsruhe institute of technology translation systems
for the WMT 2012. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Daniele Pighin, Meritxell Gonza?lez, and Llu??s Ma`rquez.
2012. The upc submission to the WMT 2012 shared
task on quality estimation. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Sylvain Raybaud, David Langlois, and Kamel Sma??li.
2011. ?This sentence is wrong.? Detecting errors in
machine-translated sentences. Machine Translation,
25(1):1?34.
Majid Razmara, Baskaran Sankaran, Ann Clifton, and
Anoop Sarkar. 2012. Kriya - the SFU system for
translation task at WMT-12. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek. 2012.
DEPFIX: A system for automatic correction of czech
MT outputs. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. DCU-Symantec submission for the
WMT 2012 quality estimation task. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Christophe Servan, Patrik Lambert, Anthony Rousseau,
Holger Schwenk, and Lo??c Barrault. 2012. LIUM?s
smt machine translation systems for WMT 2012. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence level MT eval-
uation. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver systems in the WMT12
quality estimation shared task. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the Sentence-Level Quality of Machine Transla-
tion Systems. In Proceedings of the 13th Conference
of the European Association for Machine Translation,
pages 28?37, Barcelona.
Lucia Specia. 2011. Exploiting Objective Annotations
for Measuring Translation Post-editing Effort. In Pro-
ceedings of the 15th Conference of the European As-
sociation for Machine Translation, pages 73?80, Leu-
ven.
Ales? Tamchyna, Petra Galus?c?a?kova?, Amir Kamran, Milos?
Stanojevic?, and Ondr?ej Bojar. 2012. Selecting data for
English-to-Czech machine translation. In Proceedings
of the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
David Vilar. 2012. DFKI?s smt system for WMT 2012.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Mengqiu Wang and Christopher Manning. 2012.
SPEDE: Probabilistic edit distance metrics for MT
evaluation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Eric Wehrli, Luka Nerima, and Yves Scherrer. 2009.
Deep linguistic multilingual translation and bilingual
dictionaries. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 90?94.
Philip Williams and Philipp Koehn. 2012. GHKM rule
extraction and scope-3 parsing in Moses. In Proceed-
ings of the Seventh Workshop on Statistical Machine
39
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Chunyang Wu and Hai Zhao. 2012. Regression with
phrase indicators for estimating MT quality. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Daniel Zeman. 2012. Data issues of the multilingual
translation matrix. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
40
C
U
-B
O
JA
R
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
U
E
D
IN
U
K
CU-BOJAR ? .29? .43 .53? .47? .31?
JHU .59? ? .59? .67? .65? .44?
ONLINE-A .44 .28? ? .52? .46? .32?
ONLINE-B .36? .23? .34? ? .38? .25?
UEDIN .36? .23? .36? .48? ? .27?
UK .56? .33? .56? .63? .60? ?
> others 0.53 0.32 0.53 0.65 0.60 0.37
Table 14: Head to head comparison for Czech-English systems
A Pairwise System Comparisons by Human Judges
Tables 14?21 show pairwise comparisons between systems for each language pair. The numbers in each of
the tables? cells indicate the percentage of times that the system in that column was judged to be better than
the system in that row. Bolding indicates the winner of the two systems. The difference between 100 and
the sum of the complementary cells is the percent of time that the two systems were judged to be equal.
Because there were so many systems and data conditions the significance of each pairwise comparison
needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differences
(rather than differences that are attributable to chance). In the following tables ? indicates statistical signif-
icance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical significance at
p ? 0.01, according to the Sign Test.
Each table contains a final row showing how often a system was ranked to be > than the others. As
suggested by Bojar et al (2011) present, this is calculated ignoring ties as:
score(s) =
win(s)
win(s) + loss(s)
(18)
B Automatic Scores
Tables 29?36 give the automatic scores for each of the systems.
41
C
O
M
M
E
R
C
IA
L
2
C
U
-B
O
JA
R
C
U
-D
E
P
F
IX
C
U
-P
O
O
R
-C
O
M
B
C
U
-T
A
M
C
H
C
U
-T
E
C
T
O
M
T
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
C
O
M
M
E
R
C
IA
L
1
S
F
U
U
E
D
IN
U
K
COMMERCIAL2 ? .48? .56? .43 .49? .50? .32? .49? .54? .36 .38? .50? .42
CU-BOJAR .33? ? .49? .29? .26 .39 .26? .40 .51? .37? .27? .43 .33?
CU-DEPFIX .28? .36? ? .26? .30? .32? .18? .31? .13? .33? .21? .31? .25?
CU-POOR-COMB .42 .40? .59? ? .41? .51? .34? .49? .57? .45 .33? .47? .42
CU-TAMCH .38? .24 .51? .27? ? .39 .22? .42 .47? .38? .28? .39 .28?
CU-TECTOMT .32? .42 .49? .33? .47 ? .24? .42 .46? .36? .33? .46 .40
JHU .54? .59? .69? .50? .62? .60? ? .59? .61? .52? .44 .62? .48?
ONLINE-A .36? .41 .51? .36? .43 .43 .24? ? .51? .40 .26? .45 .32?
ONLINE-B .32? .34? .24? .28? .35? .35? .22? .33? ? .31? .23? .33? .22?
COMMERCIAL1 .41 .48? .55? .41 .50? .49? .36? .46 .54? ? .30? .48 .41
SFU .47? .56? .64? .47? .55? .52? .36 .53? .64? .56? ? .58? .48?
UEDIN .36? .36 .50? .29? .38 .43 .24? .37 .48? .40 .25? ? .30?
UK .43 .47? .59? .43 .52? .44 .26? .50? .59? .47 .35? .52? ?
> others 0.46 0.54 0.66 0.44 0.56 0.53 0.32 0.53 0.63 0.48 0.36 0.56 0.44
Table 15: Head to head comparison for English-Czech systems
IT
S
-L
A
T
L
JH
U
K
IT
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
P
R
O
M
T
R
W
T
H
U
E
D
IN
U
K
ITS-LATL ? .49? .54? .55? .53? .59? .58? .38 .47? .32 .45? .47? .62? .53? .48
JHU .35? ? .47? .55? .42 .45 .55? .36 .49? .37 .46 .46? .47? .46? .29
KIT .25? .25? ? .37 .29? .28? .39 .27? .35 .30? .32? .33 .36 .24? .22?
LIMSI .23? .23? .34 ? .26 .21? .29? .25? .29? .19? .19? .32? .22? .29 .16?
LIUM .25? .36 .42? .34 ? .27? .46? .21? .40 .25? .37 .34 .35 .29 .30?
ONLINE-A .22? .33 .40? .45? .42? ? .44? .26? .43 .33? .38 .33 .47? .35 .30?
ONLINE-B .20? .22? .33 .43? .32? .29? ? .27? .36 .26? .33 .34 .39 .29? .24?
RBMT-4 .37 .47 .56? .60? .60? .55? .52? ? .41 .36 .39 .40 .58? .51? .42
RBMT-3 .30? .35? .43 .45? .40 .39 .37 .34 ? .27? .29 .23 .55? .42 .34?
ONLINE-C .36 .46 .46? .55? .49? .50? .58? .38 .48? ? .45? .43 .62? .45 .39
RBMT-1 .28? .36 .49? .58? .40 .42 .44 .35 .38 .31? ? .41 .45 .37 .30?
PROMT .20? .34? .41 .50? .46 .40 .40 .34 .22 .33 .32 ? .48? .41 .27?
RWTH .22? .28? .34 .37? .31 .28? .32 .27? .26? .22? .34 .31? ? .29 .17?
UEDIN .28? .29? .40? .39 .34 .35 .42? .31? .39 .34 .36 .34 .34 ? .27?
UK .37 .36 .53? .53? .44? .43? .48? .38 .52? .39 .44? .46? .52? .46? ?
> others 0.36 0.44 0.59 0.66 0.55 0.51 0.6 0.39 0.52 0.39 0.48 0.51 0.62 0.53 0.4
Table 16: Head to head comparison for English-French systems
42
D
F
K
I-
B
E
R
L
IN
D
F
K
I-
H
U
N
S
IC
K
E
R
JH
U
K
IT
L
IM
S
I
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
R
W
T
H
U
E
D
IN
-W
IL
L
IA
M
S
U
E
D
IN
U
K
DFKI-BERLIN ? .62? .58? .64? .71? .68? .80? .68? .71? .58? .65? .62? .64? .61? .60?
DFKI-HUNSICKER .28? ? .42 .48 .51? .47 .52? .49? .57? .38 .53? .39 .39 .41 .39
JHU .24? .45 ? .43? .43 .47? .62? .56? .60? .46 .47? .46? .47? .39 .42
KIT .22? .41 .27? ? .39 .45 .60? .54? .58? .37 .47 .33 .43 .39 .26?
LIMSI .15? .37? .34 .36 ? .47 .49? .43 .43 .35 .48 .36 .37 .32 .31?
ONLINE-A .20? .37 .35? .41 .39 ? .45? .42 .51? .38 .49 .42 .40 .37 .36?
ONLINE-B .15? .35? .26? .27? .35? .30? ? .45 .35? .29? .41 .30? .34? .30? .18?
RBMT-4 .25? .22? .31? .31? .45 .45 .42 ? .41 .38 .40 .44 .35? .36? .36?
RBMT-3 .18? .27? .24? .28? .38 .36? .49? .41 ? .33? .26? .29? .28? .31? .34?
ONLINE-C .27? .47 .35 .49 .46 .44 .63? .48 .55? ? .49? .40 .43 .43 .46
RBMT-1 .19? .30? .33? .41 .41 .39 .45 .45 .50? .32? ? .34? .40 .39 .39
RWTH .20? .43 .30? .45 .45 .44 .58? .50 .58? .43 .53? ? .41 .40 .41
UEDIN-WILLIAMS .20? .46 .30? .36 .36 .45 .54? .52? .54? .41 .46 .38 ? .32 .30?
UEDIN .20? .45 .40 .38 .43 .48 .56? .56? .53? .47 .48 .29 .39 ? .35
UK .25? .49 .40 .45? .51? .49? .64? .51? .52? .44 .47 .34 .48? .40 ?
> others 0.25 0.48 0.43 0.50 0.55 0.54 0.64 0.58 0.63 0.47 0.56 0.47 0.51 0.47 0.45
Table 17: Head to head comparison for English-German systems
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
P
R
O
M
T
U
E
D
IN
U
K
U
P
C
JHU ? .52? .59? .50? .58? .48? .49? .56? .48? .44? .52?
ONLINE-A .27? ? .45 .34? .44 .31? .31? .44 .37 .28? .37
ONLINE-B .21? .37 ? .28? .35? .25? .28? .31? .30? .23? .31?
RBMT-4 .35? .52? .56? ? .49? .39 .40 .46? .45 .38? .45
RBMT-3 .26? .39 .46? .34? ? .32? .28? .24 .34? .32? .37
ONLINE-C .33? .54? .61? .40 .47? ? .43 .50? .50? .42 .48
RBMT-1 .39? .51? .61? .39 .49? .34 ? .47? .50? .39 .46
PROMT .28? .41 .51? .33? .29 .33? .34? ? .42 .32? .40
UEDIN .25? .41 .48? .38 .47? .30? .35? .43 ? .28? .39
UK .31? .52? .57? .48? .53? .42 .44 .52? .42? ? .50?
UPC .24? .40 .53? .40 .43 .39 .39 .46 .36 .28? ?
> others 0.36 0.56 0.65 0.46 0.58 0.43 0.45 0.55 0.52 0.41 0.52
Table 18: Head to head comparison for English-Spanish systems
43
C
M
U
JH
U
K
IT
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
R
W
T
H
S
F
U
U
E
D
IN
U
K
CMU ? .34? .32 .46 .35 .41 .39 .30? .36 .29? .35? .32 .28? .45 .33?
JHU .50? ? .63? .55? .53? .63? .57? .43 .42 .31? .46 .52? .43 .53? .43
KIT .40 .21? ? .36 .30 .35 .44 .33? .33? .23? .31? .25? .28? .23? .30?
LIMSI .35 .26? .37 ? .31? .35 .40 .29? .32? .23? .33? .29? .28? .29 .23?
LIUM .47 .25? .43 .53? ? .44 .42 .36 .43 .28? .38 .38 .32? .40 .42
ONLINE-A .45 .22? .41 .47 .40 ? .41 .30? .25? .28? .23? .40 .27? .40 .25?
ONLINE-B .45 .32? .38 .42 .41 .39 ? .34? .39 .30? .33? .30? .34? .44 .32?
RBMT-4 .56? .40 .54? .61? .48 .54? .54? ? .43 .31? .48? .45 .42 .52? .46
RBMT-3 .50 .46 .53? .53? .46 .54? .47 .33 ? .28? .40 .53? .52 .50 .48
ONLINE-C .59? .57? .72? .66? .59? .60? .61? .45? .54? ? .58? .65? .53? .66? .58?
RBMT-1 .54? .43 .58? .54? .48 .62? .55? .31? .44 .20? ? .47 .41 .56? .38
RWTH .39 .35? .50? .52? .43 .50 .55? .42 .37? .23? .40 ? .34? .36 .29?
SFU .57? .38 .55? .54? .48? .55? .51? .42 .38 .35? .45 .50? ? .41 .46
UEDIN .37 .32? .42? .42 .40 .43 .40 .34? .40 .24? .36? .39 .41 ? .29?
UK .50? .40 .48? .59? .44 .58? .50? .42 .41 .35? .49 .53? .35 .51? ?
> others 0.57 0.41 0.61 0.63 0.52 0.59 0.57 0.43 0.46 0.32 0.46 0.52 0.44 0.55 0.44
Table 19: Head to head comparison for French-English systems
D
F
K
I-
B
E
R
L
IN
JH
U
K
IT
L
IM
S
I
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
Q
C
R
I
Q
U
A
E
R
O
R
W
T
H
U
E
D
IN
U
G
U
K
DFKI-BERLIN ? .38 .49 .52? .57? .65? .55? .62? .50 .49 .51? .66? .53? .61? .17? .37
JHU .45 ? .60? .66? .66? .69? .57? .60? .52 .62? .58? .67? .59? .62? .21? .37
KIT .36 .16? ? .47 .60? .50 .41 .50 .31? .39 .32 .36 .32 .39 .15? .26?
LIMSI .30? .14? .35 ? .49? .57? .49 .54 .34? .33? .43 .31 .44 .49? .14? .30?
ONLINE-A .32? .20? .22? .32? ? .39 .30? .44 .20? .30? .37 .35? .32? .31? .16? .29?
ONLINE-B .25? .21? .38 .29? .38 ? .27? .39 .31? .37 .30? .43 .34 .33? .12? .24?
RBMT-4 .33? .33? .49 .44 .57? .63? ? .46 .26? .40 .53? .51? .56? .48 .21? .32?
RBMT-3 .26? .30? .39 .40 .45 .45 .32 ? .35 .36 .34? .48 .33? .41 .13? .23?
ONLINE-C .36 .37 .58? .54? .70? .62? .57? .50 ? .53? .48 .57? .55? .58? .14? .45
RBMT-1 .41 .32? .48 .55? .64? .52 .42 .47 .34? ? .51 .49 .48 .45 .15? .25?
QCRI .31? .26? .43 .37 .48 .51? .36? .52? .43 .38 ? .48? .48? .45? .14? .23?
QUAERO .18? .19? .29 .33 .51? .43 .33? .42 .31? .37 .23? ? .34 .48? .09? .16?
RWTH .29? .25? .38 .34 .51? .48 .37? .58? .38? .40 .29? .39 ? .44 .20? .24?
UEDIN .24? .20? .38 .30? .55? .52? .42 .44 .35? .37 .29? .32? .38 ? .08? .22?
UG .68? .61? .72? .76? .76? .82? .72? .80? .70? .76? .73? .76? .73? .84? ? .57?
UK .43 .37 .48? .48? .54? .62? .57? .64? .44 .59? .49? .58? .51? .56? .20? ?
> others 0.40 0.34 0.55 0.54 0.65 0.65 0.50 0.60 0.43 0.51 0.52 0.61 0.56 0.6 0.17 0.37
Table 20: Head to head comparison for German-English systems
44
G
T
H
-U
P
M
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
Q
C
R
I
U
E
D
IN
U
K
U
P
C
GTH-UPM ? .41 .50? .52? .38 .46 .32? .35? .44? .46 .17? .41
JHU .37 ? .54? .56? .44 .48 .39 .39 .47? .50? .15? .47?
ONLINE-A .34? .31? ? .43 .28? .38? .29? .29? .40 .39 .16? .41
ONLINE-B .36? .30? .44 ? .34? .38 .30? .32? .37? .38 .18? .41
RBMT-4 .50 .45 .61? .57? ? .46 .41 .40 .53? .57? .21? .56?
RBMT-3 .42 .40 .53? .51 .36 ? .36? .31? .60? .54? .14? .54?
ONLINE-C .54? .48 .58? .62? .49 .50? ? .40 .58? .59? .23? .55?
RBMT-1 .56? .50 .59? .57? .40 .53? .41 ? .57? .59? .23? .58?
QCRI .28? .31? .45 .50? .38? .32? .29? .34? ? .31 .12? .33?
UEDIN .39 .27? .49 .49 .33? .38? .31? .31? .34 ? .15? .38
UK .74? .71? .81? .76? .73? .76? .69? .66? .76? .75? ? .77?
UPC .42 .32? .49 .49 .38? .36? .33? .35? .44? .36 .14? ?
> others 0.52 0.48 0.62 0.61 0.46 0.51 0.42 0.42 0.60 0.58 0.19 0.57
Table 21: Head to head comparison for Spanish-English systems
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.643: ONLINE-B ONLINE-B ONLINE-B 2.88: ONLINE-B 0.642 (1): ONLINE-B
2 0.606: UEDIN UEDIN UEDIN 3.07: UEDIN 0.603 (2): UEDIN
3 0.530: ONLINE-A CU-BOJAR CU-BOJAR 3.40: CU-BOJAR 0.528 (3-4): ONLINE-A
4 0.530: CU-BOJAR ONLINE-A ONLINE-A 3.40: ONLINE-A 0.528 (3-4): CU-BOJAR
5 0.375: UK UK UK 4.01: UK 0.379 (5): UK
6 0.318: JHU JHU JHU 4.24: JHU 0.320 (6): JHU
Table 22: Overall ranking with different methods (Czech?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.646: ONLINE-A ONLINE-B ONLINE-B 6.35: ONLINE-A 0.647 (1-3): ONLINE-A
2 0.645: ONLINE-B ONLINE-A ONLINE-A 6.44: ONLINE-B 0.642 (1-3): ONLINE-B
3 0.612: QUAERO UEDIN UEDIN 6.94: QUAERO 0.609 (2-5): QUAERO
4 0.599: RBMT-3 QUAERO QUAERO 7.04: RBMT-3 0.600 (2-6): RBMT-3
5 0.597: UEDIN RBMT-3 RBMT-3 7.16: UEDIN 0.593 (3-6): UEDIN
6 0.558: RWTH KIT KIT 7.76: RWTH 0.551 (5-9): RWTH
7 0.545: LIMSI RWTH RWTH 7.83: KIT 0.547 (5-10): KIT
8 0.544: KIT QCRI QCRI 7.85: LIMSI 0.545 (6-10): LIMSI
9 0.524: QCRI RBMT-4 RBMT-4 8.20: QCRI 0.521 (7-11): QCRI
10 0.505: RBMT-1 LIMSI LIMSI 8.40: RBMT-4 0.506 (8-11): RBMT-1
11 0.502: RBMT-4 RBMT-1 RBMT-1 8.42: RBMT-1 0.506 (8-11): RBMT-4
12 0.434: ONLINE-C ONLINE-C ONLINE-C 9.43: ONLINE-C 0.434 (12-13): ONLINE-C
13 0.402: DFKI-BERLIN DFKI-BERLIN DFKI-BERLIN 9.86: DFKI-BERLIN 0.405 (12-14): DFKI-BERLIN
14 0.374: UK UK UK 10.25: UK 0.377 (13-15): UK
15 0.337: JHU JHU JHU 10.81: JHU 0.338 (14-15): JHU
16 0.179: UG UG UG 13.26: UG 0.180 (16): UG
Table 23: Overall ranking with different methods (German?English)
45
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.630: LIMSI LIMSI LIMSI 6.33: LIMSI 0.626 (1-3): LIMSI
2 0.613: KIT CMU CMU 6.55: KIT 0.610 (1-4): KIT
3 0.593: ONLINE-A ONLINE-B ONLINE-B 6.80: ONLINE-A 0.592 (1-5): ONLINE-A
4 0.573: CMU KIT KIT 7.06: CMU 0.571 (2-6): CMU
5 0.569: ONLINE-B ONLINE-A ONLINE-A 7.12: ONLINE-B 0.567 (3-7): ONLINE-B
6 0.546: UEDIN LIUM LIUM 7.51: UEDIN 0.538 (5-8): UEDIN
7 0.523: LIUM RWTH RWTH 7.73: LIUM 0.522 (5-8): LIUM
8 0.515: RWTH UEDIN UEDIN 7.88: RWTH 0.510 (6-9): RWTH
9 0.459: RBMT-1 RBMT-1 RBMT-1 8.51: RBMT-1 0.463 (8-12): RBMT-1
10 0.457: RBMT-3 UK UK 8.56: RBMT-3 0.458 (9-13): RBMT-3
11 0.444: UK SFU SFU 8.75: SFU 0.444 (9-14): SFU
12 0.444: SFU RBMT-3 RBMT-3 8.78: UK 0.441 (9-14): UK
13 0.429: RBMT-4 RBMT-4 RBMT-4 8.92: RBMT-4 0.430 (10-14): RBMT-4
14 0.412: JHU JHU JHU 9.19: JHU 0.409 (12-14): JHU
15 0.321: ONLINE-C ONLINE-C ONLINE-C 10.31: ONLINE-C 0.319 (15): ONLINE-C
Table 24: Overall ranking with different methods (French?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.617: ONLINE-A ONLINE-A ONLINE-A 5.38: ONLINE-A 0.617 (1-4): ONLINE-A
2 0.612: ONLINE-B ONLINE-B ONLINE-B 5.43: ONLINE-B 0.611 (1-4): ONLINE-B
3 0.603: QCRI QCRI QCRI 5.56: QCRI 0.600 (1-4): QCRI
4 0.585: UEDIN UPC UPC 5.75: UEDIN 0.581 (2-5): UEDIN
5 0.565: UPC UEDIN UEDIN 5.89: UPC 0.567 (3-6): UPC
6 0.528: GTH-UPM RBMT-3 RBMT-3 6.29: GTH-UPM 0.526 (5-7): GTH-UPM
7 0.512: RBMT-3 JHU JHU 6.37: RBMT-3 0.518 (6-8): RBMT-3
8 0.477: JHU GTH-UPM GTH-UPM 6.73: JHU 0.480 (7-9): JHU
9 0.461: RBMT-4 RBMT-4 RBMT-4 6.92: RBMT-4 0.460 (8-10): RBMT-4
10 0.423: RBMT-1 ONLINE-C ONLINE-C 7.19: RBMT-1 0.429 (9-11): RBMT-1
11 0.420: ONLINE-C RBMT-1 RBMT-1 7.24: ONLINE-C 0.423 (9-11): ONLINE-C
12 0.189: UK UK UK 9.25: UK 0.188 (12): UK
Table 25: Overall ranking with different methods (Spanish?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.662: CU-DEPFIX CU-DEPFIX CU-DEPFIX 5.25: CU-DEPFIX 0.660 (1): CU-DEPFIX
2 0.628: ONLINE-B ONLINE-B ONLINE-B 5.78: ONLINE-B 0.616 (2): ONLINE-B
3 0.557: UEDIN UEDIN UEDIN 6.42: UEDIN 0.557 (3-6): UEDIN
4 0.555: CU-TAMCH CU-TAMCH CU-TAMCH 6.45: CU-TAMCH 0.555 (3-6): CU-TAMCH
5 0.543: CU-BOJAR CU-BOJAR CU-BOJAR 6.58: CU-BOJAR 0.541 (3-7): CU-BOJAR
6 0.531: CU-TECTOMT CU-TECTOMT CU-TECTOMT 6.69: CU-TECTOMT 0.532 (4-7): CU-TECTOMT
7 0.528: ONLINE-A ONLINE-A ONLINE-A 6.72: ONLINE-A 0.529 (4-7): ONLINE-A
8 0.478: COMMERCIAL1 COMMERCIAL2 COMMERCIAL2 7.27: COMMERCIAL1 0.477 (8-10): COMMERCIAL1
9 0.459: COMMERCIAL2 COMMERCIAL1 COMMERCIAL1 7.46: COMMERCIAL2 0.459 (8-11): COMMERCIAL2
10 0.442: CU-POOR-COMB CU-POOR-COMB CU-POOR-COMB 7.61: CU-POOR-COMB 0.443 (9-11): CU-POOR-COMB
11 0.437: UK UK UK 7.65: UK 0.440 (9-11): UK
12 0.360: SFU SFU SFU 8.40: SFU 0.362 (12): SFU
13 0.326: JHU JHU JHU 8.72: JHU 0.328 (13): JHU
Table 26: Overall ranking with different methods (English?Czech)
46
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.655: LIMSI LIMSI LIMSI 5.98: LIMSI 0.651 (1-2): LIMSI
2 0.615: RWTH RWTH RWTH 6.57: RWTH 0.609 (2-4): RWTH
3 0.595: ONLINE-B ONLINE-B ONLINE-B 6.84: ONLINE-B 0.589 (2-5): ONLINE-B
4 0.590: KIT KIT KIT 6.86: KIT 0.587 (2-5): KIT
5 0.554: LIUM LIUM LIUM 7.36: LIUM 0.550 (4-8): LIUM
6 0.534: UEDIN UEDIN UEDIN 7.67: UEDIN 0.526 (5-9): UEDIN
7 0.516: RBMT-3 RBMT-3 RBMT-3 7.85: RBMT-3 0.514 (5-10): RBMT-3
8 0.513: ONLINE-A ONLINE-A ONLINE-A 7.92: PROMT 0.507 (6-10): ONLINE-A
9 0.506: PROMT PROMT PROMT 7.92: ONLINE-A 0.507 (6-10): PROMT
10 0.483: RBMT-1 RBMT-1 RBMT-1 8.23: RBMT-1 0.483 (8-11): RBMT-1
11 0.436: JHU JHU JHU 8.85: JHU 0.436 (10-12): JHU
12 0.396: UK UK RBMT-4 9.34: RBMT-4 0.397 (11-15): RBMT-4
13 0.394: ONLINE-C RBMT-4 ITS-LATL 9.38: ONLINE-C 0.393 (12-15): ONLINE-C
14 0.394: RBMT-4 ITS-LATL ONLINE-C 9.41: UK 0.391 (12-15): UK
15 0.360: ITS-LATL ONLINE-C UK 9.81: ITS-LATL 0.360 (13-15): ITS-LATL
Table 27: Overall ranking with different methods (English?French)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.648: ONLINE-B ONLINE-B ONLINE-B 4.70: ONLINE-B 0.646 (1): ONLINE-B
2 0.579: RBMT-3 RBMT-3 RBMT-3 5.35: RBMT-3 0.577 (2-4): RBMT-3
3 0.561: ONLINE-A PROMT PROMT 5.49: ONLINE-A 0.561 (2-5): ONLINE-A
4 0.545: PROMT ONLINE-A ONLINE-A 5.66: PROMT 0.542 (3-6): PROMT
5 0.526: UEDIN UPC UPC 5.78: UEDIN 0.528 (4-6): UEDIN
6 0.524: UPC UEDIN UEDIN 5.81: UPC 0.525 (4-6): UPC
7 0.463: RBMT-4 RBMT-1 RBMT-1 6.33: RBMT-4 0.464 (7-9): RBMT-4
8 0.452: RBMT-1 RBMT-4 RBMT-4 6.42: RBMT-1 0.452 (7-9): RBMT-1
9 0.430: ONLINE-C UK ONLINE-C 6.57: ONLINE-C 0.434 (8-10): ONLINE-C
10 0.412: UK ONLINE-C UK 6.73: UK 0.415 (9-10): UK
11 0.357: JHU JHU JHU 7.17: JHU 0.357 (11): JHU
Table 28: Overall ranking with different methods (English?Spanish)
47
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
A
G
A
N
-S
T
S
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
Czech-English News Task
CU-BOJAR 0.17 0.2 39 0.31 44 0.66 0.50 0.21 0.65 0.2 50 639
JHU 0.16 0.18 41 0.28 41 0.63 0.47 0.19 0.65 0.10 53 692
ONLINE-A 0.18 0.21 40 0.31 43 0.68 0.51 0.21 0.62 0.22 50 648
ONLINE-B 0.18 0.23 40 0.30 42 0.67 0.53 0.23 0.59 0.20 52 660
UEDIN 0.18 0.22 39 0.32 45 0.69 0.53 0.23 0.60 0.25 49 627
UK 0.16 0.18 41 0.29 41 0.63 0.49 0.19 0.67 0.17 53 682
Table 29: Automatic evaluation metric scores for systems in the WMT12 Czech-English News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
German-English News Task
DFKI-BERLIN 0.17 0.21 40 0.3 43 0.46 0.18 0.61 0.25 50 653
JHU 0.17 0.2 41 0.29 42 0.42 0.21 0.61 0.20 52 672
KIT 0.18 0.23 39 0.31 45 0.46 0.23 0.58 0.28 49 630
LIMSI 0.18 0.23 39 0.31 45 0.48 0.23 0.6 0.30 49 628
ONLINE-A 0.18 0.21 40 0.32 44 0.50 0.22 0.6 0.27 50 645
ONLINE-B 0.19 0.24 39 0.31 44 0.53 0.24 0.59 0.29 50 636
RBMT-4 0.16 0.16 41 0.29 42 0.44 0.18 0.68 0.24 53 690
RBMT-3 0.16 0.17 40 0.3 42 0.47 0.19 0.66 0.29 52 677
ONLINE-C 0.15 0.14 42 0.28 40 0.43 0.17 0.70 0.26 54 711
RBMT-1 0.15 0.15 43 0.29 40 0.45 0.17 0.69 0.24 54 711
QCRI 0.18 0.23 40 0.31 44 0.46 0.23 0.59 0.26 50 639
QUAERO 0.19 0.24 38 0.32 46 0.49 0.24 0.57 0.3 48 613
RWTH 0.18 0.23 39 0.31 45 0.48 0.24 0.58 0.27 49 626
UEDIN 0.18 0.23 39 0.31 46 0.51 0.23 0.59 0.32 49 630
UG 0.11 0.11 45 0.24 35 0.38 0.14 0.77 0.10 59 768
UK 0.16 0.18 42 0.29 40 0.42 0.2 0.65 0.27 53 683
Table 30: Automatic evaluation metric scores for systems in the WMT12 German-English News Task
48
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
French-English News Task
CMU 0.20 0.29 36 0.34 51 0.54 0.29 0.52 0.25 44 561
JHU 0.19 0.26 37 0.33 47 0.50 0.26 0.54 0.20 46 596
KIT 0.21 0.30 35 0.34 51 0.54 0.3 0.51 0.25 43 551
LIMSI 0.21 0.30 35 0.34 52 0.55 0.3 0.51 0.25 43 546
LIUM 0.20 0.29 36 0.34 50 0.54 0.29 0.52 0.24 44 558
ONLINE-A 0.2 0.27 37 0.34 48 0.52 0.27 0.53 0.24 45 584
ONLINE-B 0.20 0.30 36 0.33 48 0.55 0.29 0.51 0.22 46 582
RBMT-4 0.18 0.20 38 0.32 45 0.49 0.21 0.64 0.15 48 622
RBMT-3 0.18 0.21 39 0.31 46 0.49 0.22 0.61 0.15 48 637
ONLINE-C 0.18 0.19 38 0.31 45 0.45 0.21 0.64 0.10 48 633
RBMT-1 0.18 0.21 39 0.32 47 0.5 0.22 0.62 0.15 48 626
RWTH 0.20 0.29 36 0.34 50 0.53 0.28 0.53 0.20 44 563
SFU 0.2 0.25 37 0.33 48 0.51 0.26 0.54 0.17 46 596
UEDIN 0.20 0.30 35 0.34 51 0.54 0.3 0.51 0.25 43 549
UK 0.19 0.25 38 0.33 47 0.52 0.25 0.57 0.17 47 602
Table 31: Automatic evaluation metric scores for systems in the WMT12 French-English News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
A
G
A
N
-S
T
S
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
Spanish-English News Task
GTH-UPM 0.21 0.29 35 0.35 51 0.7 0.55 0.29 0.51 0.31 43 565
JHU 0.21 0.29 35 0.35 51 0.7 0.56 0.29 0.51 0.31 43 560
ONLINE-A 0.22 0.31 34 0.36 52 0.72 0.58 0.31 0.49 0.36 42 535
ONLINE-B 0.22 0.38 33 0.36 53 0.70 0.60 0.35 0.45 0.35 41 523
RBMT-4 0.19 0.23 36 0.33 49 0.69 0.54 0.24 0.60 0.29 45 591
RBMT-3 0.19 0.23 36 0.33 49 0.69 0.54 0.23 0.60 0.29 45 590
ONLINE-C 0.19 0.22 37 0.33 47 0.68 0.5 0.23 0.61 0.24 46 598
RBMT-1 0.18 0.22 38 0.33 48 0.67 0.52 0.23 0.62 0.23 47 607
QCRI 0.22 0.33 33 0.36 54 0.71 0.6 0.32 0.49 0.32 40 523
UEDIN 0.22 0.33 33 0.36 54 0.71 0.59 0.32 0.48 0.32 40 519
UK 0.18 0.22 37 0.30 44 0.6 0.48 0.23 0.60 0.10 48 634
UPC 0.22 0.32 34 0.36 54 0.71 0.57 0.31 0.49 0.33 41 531
Table 32: Automatic evaluation metric scores for systems in the WMT12 Spanish-English News Task
49
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-Czech News Task
COMMERCIAL-2 0.01 0.08 47 693 0.17 23 0.38 0.1 0.76 0.17 61
CU-BOJAR 0.17 0.13 45 644 0.21 28 0.4 0.13 0.69 0.26 57
CU-DEPFIX 0.19 0.16 44 623 0.22 28 0.45 0.15 0.66 0.30 55
CU-POOR-COMB 0.14 0.12 48 710 0.19 27 0.35 0.12 0.67 0.23 60
CU-TAMCH 0.17 0.13 45 647 0.21 28 0.38 0.13 0.69 0.29 57
CU-TECTOMT 0.16 0.12 48 690 0.19 26 0.36 0.12 0.68 0.22 60
JHU 0.16 0.1 47 691 0.2 23 0.39 0.11 0.69 0.10 60
ONLINE-A 0.17 0.13 n/a n/a 0.21 n/a 0.42 0.13 0.67 0.25 n/a
ONLINE-B 0.19 0.16 44 623 0.21 28 0.45 0.15 0.66 0.30 55
COMMERCIAL-1 0.11 0.09 48 692 0.18 22 0.38 0.10 0.74 0.21 61
SFU 0.15 0.11 47 674 0.19 23 0.39 0.11 0.71 0.21 60
UEDIN 0.18 0.15 45 639 0.21 27 0.41 0.14 0.66 0.40 56
UK 0.15 0.11 47 669 0.19 25 0.39 0.12 0.71 0.35 59
Table 33: Automatic evaluation metric scores for systems in the WMT12 English-Czech News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-German News Task
DFKI-BERLIN 0.18 0.14 46 628 0.35 41 0.13 0.69 0.10 57
DFKI-HUNSICKER 0.18 0.14 45 621 0.35 42 0.15 0.69 0.17 57
JHU 0.2 0.15 45 618 0.37 42 0.16 0.68 0.17 56
KIT 0.20 0.17 45 606 0.38 43 0.17 0.66 0.14 55
LIMSI 0.2 0.17 45 615 0.37 43 0.17 0.65 0.15 56
ONLINE-A 0.20 0.16 45 617 0.38 43 0.17 0.65 0.36 55
ONLINE-B 0.22 0.18 43 589 0.38 42 0.18 0.64 0.35 55
RBMT-4 0.18 0.14 45 623 0.35 42 0.15 0.69 0.35 57
RBMT-3 0.19 0.15 44 608 0.36 44 0.16 0.68 0.37 56
ONLINE-C 0.16 0.11 47 655 0.32 39 0.13 0.74 0.37 60
RBMT-1 0.17 0.13 47 643 0.34 42 0.15 0.70 0.36 58
RWTH 0.2 0.16 44 609 0.37 43 0.16 0.67 0.25 56
UEDIN-WILLIAMS 0.19 0.16 45 628 0.37 43 0.17 0.66 0.33 57
UEDIN 0.20 0.16 45 611 0.37 43 0.17 0.66 0.29 55
UK 0.18 0.14 46 632 0.36 40 0.15 0.71 0.27 58
Table 34: Automatic evaluation metric scores for systems in the WMT12 English-German News Task
50
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-French News Task
ITS-LATL 0.24 0.21 41 548 0.45 48 0.21 0.61 0.15 50
JHU 0.26 0.25 38 511 0.49 51 0.25 0.57 0.15 47
KIT 0.28 0.28 36 480 0.52 55 0.28 0.54 0.22 44
LIMSI 0.28 0.29 36 472 0.52 55 0.28 0.54 0.22 44
LIUM 0.28 0.28 37 480 0.51 54 0.28 0.55 0.20 45
ONLINE-A 0.26 0.25 39 512 0.5 52 0.26 0.57 0.17 47
ONLINE-B 0.24 0.21 36 473 0.48 45 0.26 0.77 0.10 49
RBMT-4 0.24 0.21 40 539 0.46 48 0.22 0.60 0.10 49
RBMT-3 0.26 0.24 39 511 0.48 52 0.24 0.58 0.14 47
ONLINE-C 0.23 0.2 41 550 0.45 50 0.21 0.62 0.10 50
RBMT-1 0.25 0.22 40 531 0.47 51 0.23 0.6 0.13 49
PROMT 0.26 0.24 38 502 0.49 52 0.25 0.58 0.18 46
RWTH 0.28 0.29 36 478 0.52 54 0.28 0.54 0.22 44
UEDIN 0.28 0.28 36 479 0.52 54 0.28 0.55 0.27 45
UK 0.25 0.23 39 523 0.48 51 0.24 0.6 0.17 48
Table 35: Automatic evaluation metric scores for systems in the WMT12 English-French News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-Spanish News Task
JHU 0.29 0.29 37 494 0.54 52 0.29 0.51 0.14 45
ONLINE-A 0.31 0.31 36 475 0.56 54 0.31 0.48 0.2 43
ONLINE-B 0.33 0.36 34 431 0.57 54 0.34 0.48 0.25 42
RBMT-4 0.27 0.24 39 528 0.5 50 0.25 0.55 0.14 48
RBMT-3 0.28 0.26 39 510 0.51 51 0.26 0.54 0.13 46
ONLINE-C 0.26 0.24 40 532 0.5 49 0.25 0.55 0.10 48
RBMT-1 0.26 0.23 40 534 0.50 49 0.25 0.57 0.13 49
PROMT 0.29 0.27 38 497 0.52 52 0.28 0.53 0.18 45
UEDIN 0.31 0.32 35 466 0.56 55 0.32 0.49 0.19 42
UK 0.29 0.28 38 510 0.54 51 0.28 0.52 0.17 46
UPC 0.31 0.32 36 476 0.56 54 0.31 0.49 0.19 43
Table 36: Automatic evaluation metric scores for systems in the WMT12 English-Spanish News Task
51
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Findings of the 2013 Workshop on Statistical Machine Translation
Ondr?ej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Chris Callison-Burch
University of Pennsylvania
Christian Federmann
Saarland University
Barry Haddow
University of Edinburgh
Philipp Koehn
University of Edinburgh
Christof Monz
University of Amsterdam
Matt Post
Johns Hopkins University
Radu Soricut
Google
Lucia Specia
University of Sheffield
Abstract
We present the results of the WMT13
shared tasks, which included a translation
task, a task for run-time estimation of ma-
chine translation quality, and an unoffi-
cial metrics task. This year, 143 machine
translation systems were submitted to the
ten translation tasks from 23 institutions.
An additional 6 anonymized systems were
included, and were then evaluated both au-
tomatically and manually, in our largest
manual evaluation to date. The quality es-
timation task had four subtasks, with a to-
tal of 14 teams, submitting 55 entries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2013. This workshop builds
on seven previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al, 2007, 2008,
2009, 2010, 2011, 2012).
This year we conducted three official tasks: a
translation task, a human evaluation of transla-
tion results, and a quality estimation task.1 In
the translation task (?2), participants were asked
to translate a shared test set, optionally restrict-
ing themselves to the provided training data. We
held ten translation tasks this year, between En-
glish and each of Czech, French, German, Span-
ish, and Russian. The Russian translation tasks
were new this year, and were also the most popu-
lar. The system outputs for each task were evalu-
ated both automatically and manually.
The human evaluation task (?3) involves ask-
ing human judges to rank sentences output by
anonymized systems. We obtained large numbers
of rankings from two groups: researchers (who
1The traditional metrics task is evaluated in a separate pa-
per (Macha?c?ek and Bojar, 2013).
contributed evaluations proportional to the number
of tasks they entered) and workers on Amazon?s
Mechanical Turk (who were paid). This year?s ef-
fort was our largest yet by a wide margin; we man-
aged to collect an order of magnitude more judg-
ments than in the past, allowing us to achieve sta-
tistical significance on the majority of the pairwise
system rankings. This year, we are also clustering
the systems according to these significance results,
instead of presenting a total ordering over systems.
The focus of the quality estimation task (?6)
is to produce real-time estimates of sentence- or
word-level machine translation quality. This task
has potential usefulness in a range of settings, such
as prioritizing output for human post-editing, or
selecting the best translations from a number of
systems. This year the following subtasks were
proposed: prediction of percentage of word edits
necessary to fix a sentence, ranking of up to five al-
ternative translations for a given source sentence,
prediction of post-editing time for a sentence, and
prediction of word-level scores for a given trans-
lation (correct/incorrect and types of edits). The
datasets included English-Spanish and German-
English news translations produced by a number
of machine translation systems. This marks the
second year we have conducted this task.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation methodologies for machine trans-
lation. As before, all of the data, translations,
and collected human judgments are publicly avail-
able.2 We hope these datasets serve as a valu-
able resource for research into statistical machine
translation, system combination, and automatic
evaluation or prediction of translation quality.
2http://statmt.org/wmt13/results.html
1
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and five other lan-
guages: German, Spanish, French, Czech, and ?
new this year ? Russian. We created a test set for
each language pair by translating newspaper arti-
cles and provided training data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources. A total of 52
articles were selected, in roughly equal amounts
from a variety of Czech, English, French, German,
Spanish, and Russian news sites:3
Czech: aktua?lne?.cz (1), CTK (1), den??k (1),
iDNES.cz (3), lidovky.cz (1), Novinky.cz (2)
French: Cyber Presse (3), Le Devoir (1), Le
Monde (3), Liberation (2)
Spanish: ABC.es (2), BBC Spanish (1), El Peri-
odico (1), Milenio (3), Noroeste (1), Primera
Hora (3)
English: BBC (2), CNN (2), Economist (1),
Guardian (1), New York Times (2), The Tele-
graph (1)
German: Der Standard (1), Deutsche Welle (1),
FAZ (1), Frankfurter Rundschau (2), Welt (2)
Russian: AIF (2), BBC Russian (2), Izvestiya (1),
Rosbalt (1), Vesti (1)
The stories were translated by the professional
translation agency Capita, funded by the EU
Framework Programme 7 project MosesCore, and
by Yandex, a Russian search engine.4 All of the
translations were done directly, and not via an in-
termediate language.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
train language models, and development sets to
tune system parameters. Some training corpora
were identical from last year (Europarl5, United
Nations, French-English 109 corpus, CzEng),
some were updated (News Commentary, mono-
lingual data), and new corpora were added (Com-
mon Crawl (Smith et al, 2013), Russian-English
3For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
4http://www.yandex.com/
5As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
parallel data provided by Yandex, Russian-English
Wikipedia Headlines provided by CMU).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their en-
try names are listed in Table 1; each system did
not necessarily appear in all translation tasks. We
also included three commercial off-the-shelf MT
systems and three online statistical MT systems,6
which we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
We run the evaluation campaign using an up-
dated version of Appraise (Federmann, 2012); the
tool has been extended to support collecting judg-
ments using Amazon?s Mechanical Turk, replac-
ing the annotation system used in previous WMTs.
The software, including all changes made for this
year?s workshop, is available from GitHub.7
This year differs from prior years in a few im-
portant ways:
? We collected about ten times more judgments
that we have in the past, using judgments
from both participants in the shared task and
non-experts hired on Amazon?s Mechanical
Turk.
? Instead of presenting a total ordering of sys-
tems for each pair, we cluster them and report
a ranking over the clusters.
6Thanks to Herve? Saint-Amand and Martin Popel for har-
vesting these entries.
7https://github.com/cfedermann/Appraise
2
Europarl Parallel Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,965,734 2,007,723 1,920,209 646,605
Words 56,895,229 54,420,026 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 176,258 117,481 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 174,441 157,168 178,221 140,324 150,217
Words 5,116,388 4,520,796 4,928,135 4,066,721 4,597,904 4,541,058 3,206,423 3,507,249 3,841,950 4,008,949
Distinct words 84,273 61,693 69,028 58,295 142,461 61,761 138,991 54,270 145,997 57,991
Common Crawl Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 1,845,286 3,244,152 2,399,123 161,838 878,386
Words 49,561,060 46,861,758 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 710,755 640,778 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
Spanish? English French? English
Sentences 11,196,913 12,886,831
Words 318,788,686 365,127,098 411,916,781 360,341,450
Distinct words 593,567 581,339 565,553 666,077
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English
Sentences 514,859
Words 1,191,474 1,230,644
Distinct words 282,989 251,328
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,218,201 2,123,835 2,190,579 2,176,537 668,595
Words 59,848,044 60,476,282 63,439,791 53,534,167 14,946,399
Distinct words 123,059 181,837 145,496 394,781 172,461
News Language Model Data
English Spanish French German Czech Russian
Sentence 68,521,621 13,384,314 21,195,476 54,619,789 27,540,749 19,912,911
Words 1,613,778,461 386,014,234 524,541,570 983,818,841 456,271,247 351,595,790
Distinct words 3,392,137 1,163,825 1,590,187 6,814,953 2,655,813 2,195,112
News Test Set
English Spanish French German Czech Russian
Sentences 3000
Words 64,810 73,659 73,659 63,412 57,050 58,327
Distinct words 8,935 10,601 11,441 12,189 15,324 15,736
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
3
ID Institution
BALAGUR Yandex School of Data Analysis (Borisov et al, 2013)
CMU
CMU-TREE-TO-TREE
Carnegie Mellon University (Ammar et al, 2013)
CU-BOJAR,
CU-DEPFIX,
CU-TAMCHYNA
Charles University in Prague (Bojar et al, 2013)
CU-KAREL, CU-ZEMAN Charles University in Prague (B??lek and Zeman, 2013)
CU-PHRASEFIX,
CU-TECTOMT
Charles University in Prague (Galus?c?a?kova? et al, 2013)
DCU Dublin City University (Rubino et al, 2013a)
DCU-FDA Dublin City University (Bicici, 2013a)
DCU-OKITA Dublin City University (Okita et al, 2013)
DESRT Universita` di Pisa (Miceli Barone and Attardi, 2013)
ITS-LATL University of Geneva
JHU Johns Hopkins University (Post et al, 2013)
KIT Karlsruhe Institute of Technology (Cho et al, 2013)
LIA Universite? d?Avignon (Huet et al, 2013)
LIMSI LIMSI (Allauzen et al, 2013)
MES-* Munich / Edinburgh / Stuttgart (Durrani et al, 2013a; Weller et al, 2013)
OMNIFLUENT SAIC (Matusov and Leusch, 2013)
PROMT PROMT Automated Translations Solutions
QCRI-MES Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al, 2013)
QUAERO QUAERO (Peitz et al, 2013a)
RWTH RWTH Aachen (Peitz et al, 2013b)
SHEF University of Sheffield
STANFORD Stanford University (Green et al, 2013)
TALP-UPC TALP Research Centre (Formiga et al, 2013a)
TUBITAK TU?BI?TAK-BI?LGEM (Durgar El-Kahlout and Mermer, 2013)
UCAM University of Cambridge (Pino et al, 2013)
UEDIN,
UEDIN-HEAFIELD
University of Edinburgh (Durrani et al, 2013b)
UEDIN-SYNTAX University of Edinburgh (Nadejde et al, 2013)
UMD University of Maryland (Eidelman et al, 2013)
UU Uppsala University (Stymne et al, 2013)
COMMERCIAL-1,2,3 Anonymized commercial systems
ONLINE-A,B,G Anonymized online systems
Table 1: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
4
3.1 Ranking translations of sentences
The ranking among systems is produced by col-
lecting a large number of rankings between the
systems? translations. Every language task had
many participating systems (the largest was 19,
for the Russian-English task). Rather than asking
judges to provide a complete ordering over all the
translations of a source segment, we instead ran-
domly select five systems and ask the judge to rank
just those. We call each of these a ranking task.
A screenshot of the ranking interface is shown in
Figure 2.
For each ranking task, the judge is presented
with a source segment, a reference translation,
and the outputs of five systems (anonymized and
randomly-ordered). The following simple instruc-
tions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
The rankings of the systems are numbered from 1
to 5, with 1 being the best translation and 5 be-
ing the worst. Each ranking task has the potential
to provide 10 pairwise rankings, and fewer if the
judge chooses any ties. For example, the ranking
{A:1, B:2, C:4, D:3, E:5}
provides 10 pairwise rankings, while the ranking
{A:3, B:3, C:4, D:3, E:1}
provides just 7. The absolute value of the ranking
or the degree of difference is not considered.
We use the collected pairwise rankings to assign
each system a score that reflects how highly that
system was usually ranked by the annotators. The
score for some system A reflects how frequently it
was judged to be better than other systems when
compared on the same segment; its score is the
number of pairwise rankings where it was judged
to be better, divided by the total number of non-
tying pairwise comparisons. These scores were
used to compute clusters of systems and rankings
between them (?3.4).
3.2 Collecting the data
A goal this year was to collect enough data to
achieve statistical significance in the rankings. We
distributed the workload among two groups of
judges: researchers and Turkers. The researcher
group comprised partipants in the shared task, who
were asked to contribute judgments on 300 sen-
tences for each system they contributed. The re-
searcher evaluation was held over three weeks
from May 17?June 7, and yielded about 280k pair-
wise rankings.
The Turker group was composed of non-expert
annotators hired on Amazon?s Mechanical Turk
(MTurk). A basic unit of work on MTurk is called
a Human Intelligence Task (HIT) and included
three ranking tasks, for which we paid $0.25. To
ensure that the Turkers provided high quality an-
notations, this portion of the evaluation was be-
gun after the researcher portion had completed,
enabling us to embed controls in the form of high-
consensus pairwise rankings in the Turker HITs.
To build these controls, we collected ranking tasks
containing pairwise rankings with a high degree of
researcher consensus. An example task is here:
SENTENCE 504
SOURCE Vor den heiligen Sta?tten verbeugen
REFERENCE Let?s worship the holy places
SYSTEM A Before the holy sites curtain
SYSTEM B Before we bow to the Holy Places
SYSTEM C To the holy sites bow
SYSTEM D Bow down to the holy sites
SYSTEM E Before the holy sites pay
MATRIX
A B C D E
A - 0 0 0 3
B 5 - 0 1 5
C 6 6 - 0 6
D 6 8 5 - 6
E 0 0 0 0 -
Matrix entry Mi,j records the number of re-
searchers who judged System i to be better than
System j. We use as controls pairwise judgments
for which |Mi,j?Mj,i| > 5, i.e., judgments where
the researcher consensus ran strongly in one direc-
tion. We rejected HITs from Turkers who encoun-
tered at least 10 of these controls and failed more
than 50% of them.
There were 463 people who participated in the
Turker portion of the manual evaluation, contribut-
ing 664k pairwise rankings from Turkers who
passed the controls. Together with the researcher
judgments, we collected close to a million pair-
wise rankings, compared to 101k collected last
year: a ten-fold increase. Table 2 contains more
detail.
5
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly-ordered) and has to rank
these according to their translation quality, ties are allowed. For technical reasons, annotators on Amazon?s Mechanical Turk
received all three ranking tasks for a single HIT on a single page, one upon the other.
3.3 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960), which is de-
fined as
? = P (A)? P (E)1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. Note that ? is ba-
sically a normalized version of P (A), one which
takes into account how meaningful it is for anno-
tators to agree with each other, by incorporating
P (E). The values for ? range from 0 to 1, with
zero indicating no agreement and 1 perfect agree-
ment.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it should capture the probability
that two annotators would agree randomly. There-
fore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 3 gives ? values for inter-annotator agree-
ment for WMT11?WMT13 while Table 4 de-
tails intra-annotator agreement scores. Due to the
change of annotation software, we used a slightly
different way of computing annotator agreement
scores. Therefore, we chose to re-compute values
for previous WMTs to allow for a fair comparison.
The exact interpretation of the kappa coefficient is
difficult, but according to Landis and Koch (1977),
0?0.2 is slight, 0.2?0.4 is fair, 0.4?0.6 is moderate,
6
LANGUAGE PAIR Systems Rankings Average
Czech-English 11 85,469 7,769.91
English-Czech 12 102,842 8,570.17
German-English 17 128,668 7,568.71
English-German 15 77,286 5,152.40
Spanish-English 12 67,832 5,652.67
English-Spanish 13 60,464 4,651.08
French-English 13 80,741 6,210.85
English-French 17 100,783 5,928.41
Russian-English 19 151,422 7,969.58
English-Russian 14 87,323 6,237.36
Total 148 942,840 6,370.54
WMT12 103 101,969 999.69
WMT11 133 63,045 474.02
Table 2: Amount of data collected in the WMT13 manual evaluation. The final two rows report summary information from the
previous two workshops.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.400 0.311 0.244 0.342 0.279
English-Czech 0.460 0.359 0.168 0.408 0.075
German-English 0.324 0.385 0.299 0.443 0.324
English-German 0.378 0.356 0.267 0.457 0.239
Spanish-English 0.494 0.298 0.277 0.415 0.295
English-Spanish 0.367 0.254 0.206 0.333 0.249
French-English 0.402 0.272 0.275 0.405 0.321
English-French 0.406 0.296 0.231 0.434 0.237
Russian-English ? ? 0.278 0.315 0.324
English-Russian ? ? 0.243 0.416 0.207
Table 3: ? scores measuring inter-annotator agreement. The WMT13r and WMT13m columns provide breakdowns for re-
searcher annotations and MTurk annotations, respectively. See Table 4 for corresponding intra-annotator agreement scores.
0.6?0.8 is substantial, and 0.8?1.0 is almost per-
fect. We find that the agreement rates are more or
less the same as in prior years.
The WMT13 column contains both researcher
and Turker annotations at a roughly 1:2 ratio. The
final two columns break out agreement numbers
between these two groups. The researcher agree-
ment rates are similar to agreement rates from past
years, while the Turker agreement are well below
researcher agreement rates, varying widely, but of-
ten comparable to WMT11 and WMT12. Clearly,
researchers are providing us with more consistent
opinions, but whether these differences are ex-
plained by Turkers racing through jobs, the partic-
ularities that inform researchers judging systems
they know well, or something else, is hard to tell.
Intra-annotator agreement scores are also on par
from last year?s level, and are often much better.
We observe better intra-annotator agreement for
researchers compared to Turkers.
As a small test, we varied the threshold of ac-
ceptance against the controls for the Turker data
alone and computed inter-annotator agreement
scores on the datasets for the Russian?English task
(the only language pair where we had enough data
at high thresholds). Table 5 shows that higher
thresholds do indeed give us better agreements,
but not monotonically. The increasing ?s sug-
gests that we can find a segment of Turkers who
do a better job and that perhaps a slightly higher
threshold of 0.6 would serve us better, while the
remaining difference against the researchers sug-
gests there may be different mindsets informing
the decisions. In any case, getting the best perfor-
mance out of the Turkers remains difficult.
3.4 System Score
Given the multitude of pairwise comparisons, we
would like to rank the systems according to a
single score computed for each system. In re-
7
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.597 0.454 0.479 0.483 0.478
English-Czech 0.601 0.390 0.290 0.547 0.242
German-English 0.576 0.392 0.535 0.643 0.515
English-German 0.528 0.433 0.498 0.649 0.452
Spanish-English 0.574 1.000 0.575 0.605 0.537
English-Spanish 0.426 0.329 0.492 0.468 0.492
French-English 0.673 0.360 0.578 0.585 0.565
English-French 0.524 0.414 0.495 0.630 0.486
Russian-English ? ? 0.450 0.363 0.477
English-Russian ? ? 0.513 0.582 0.500
Table 4: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation. The WMT13r and WMT13m columns provide breakdowns for researcher annotations and MTurk annota-
tions, respectively. The perfect inter-annotator agreement for Spanish-English is a result of there being very little data for that
language pair.
thresh. rankings ?
0.5 16,605 0.234
0.6 9,999 0.337
0.7 3,219 0.360
0.8 1,851 0.395
0.9 849 0.336
Table 5: Agreement as a function of threshold for Turkers on
the Russian?English task. The threshold is the percentage of
controls a Turker must pass for her rankings to be accepted.
cent evaluation campaigns, we tweaked the metric
and now arrived at a intuitive score that has been
demonstrated to be accurate in ranking systems ac-
cording to their true quality (Koehn, 2012).
The score, which we call EXPECTED WINS, has
an intuitive explanation. If the system is compared
against a randomly picked opposing system, on a
randomly picked sentence, by a randomly picked
judge, what is the probability that its translation is
ranked higher?
Formally, the score for a system Si among a set
of systems {Sj} given a pool of pairwise rankings
summarized as win(A,B) ? the number of times
system A is ranked higher than system B ? is
defined as follows:
score(Si) = 1|{Sj}|
?
j,j 6=i
win(Si, Sj)
win(Si, Sj) + win(Sj , Si)
Note that this score ignores ties.
3.5 Rank Ranges and Clusters
Given the scores, we would like to rank the sys-
tems, which is straightforward. But we would also
like to know, if the obtained system ranking is
statistically significant. Typically, given the large
number of systems that participate, and the simi-
larity of the systems given a common training data
condition and often common toolsets, there will be
some systems that will be very close in quality.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute the expected wins score for each system
based on this sample, and rank each system. By
repeating this procedure a 1,000 times, we can de-
termine a range of ranks, into which system falls
at least 95% of the time (i.e., at least 950 times) ?
corresponding to a p-level of p ? 0.05.
Furthermore, given the rank ranges for each sys-
tem, we can cluster systems with overlapping rank
ranges.8
For all language pairs and all systems, Table 6
reports all system scores, rank ranges, and clus-
ters. The official interpretation of these results
is that systems in the same cluster are considered
tied. Given the large number of judgements that
we collected, it was possible to group on average
about two systems in a cluster, even though the
systems in the middle are typically in larger clus-
ters.
8Formally, given ranges defined by start(Si) and end(Si),
we seek the largest set of clusters {Cc} that satisfies:
?S ?C : S ? C
S ? Ca, S ? Cb ? Ca = Cb
Ca 6= Cb ? ?Si ? Ca, Sj ? Cb :
start(Si) > end(Sj) or start(Sj) > end(Si)
8
Czech-English
# score range system
1 0.607 1 UEDIN-HEAFIELD
2 0.582 2-3 ONLINE-B
0.573 2-4 MES
0.562 3-5 UEDIN
0.547 4-7 ONLINE-A
0.542 5-7 UEDIN-SYNTAX
0.534 6-7 CU-ZEMAN
8 0.482 8 CU-TAMCHYNA
9 0.458 9 DCU-FDA
10 0.321 10 JHU
11 0.297 11 SHEF-WPROA
English-Czech
# score range system
1 0.580 1-2 CU-BOJAR
0.578 1-2 CU-DEPFIX
3 0.562 3 ONLINE-B
4 0.525 4 UEDIN
5 0.505 5-7 CU-ZEMAN
0.502 5-7 MES
0.499 5-8 ONLINE-A
0.484 7-9 CU-PHRASEFIX
0.476 8-9 CU-TECTOMT
10 0.457 10-11 COMMERCIAL-1
0.450 10-11 COMMERCIAL-2
12 0.389 12 SHEF-WPROA
Spanish-English
# score range system
1 0.624 1 UEDIN-HEAFIELD
2 0.595 2 ONLINE-B
3 0.570 3-5 UEDIN
0.570 3-5 ONLINE-A
0.567 3-5 MES
6 0.537 6 LIMSI-SOUL
7 0.514 7 DCU
8 0.488 8-9 DCU-OKITA
0.484 8-9 DCU-FDA
10 0.462 10 CU-ZEMAN
11 0.425 11 JHU
12 0.169 12 SHEF-WPROA
English-Spanish
# rank range system
1 0.637 1 ONLINE-B
2 0.582 2-4 ONLINE-A
0.578 2-4 UEDIN
0.567 3-4 PROMT
5 0.535 5-6 MES
0.528 5-6 TALP-UPC
7 0.491 7-8 LIMSI
0.474 7-9 DCU
0.472 8-10 DCU-FDA
0.455 9-11 DCU-OKITA
0.446 10-11 CU-ZEMAN
12 0.417 12 JHU
13 0.324 13 SHEF-WPROA
German-English
# rank range system
1 0.660 1 ONLINE-B
2 0.620 2-3 ONLINE-A
0.608 2-3 UEDIN-SYNTAX
4 0.586 4-5 UEDIN
0.584 4-5 QUAERO
0.571 5-7 KIT
0.562 6-7 MES
8 0.543 8-9 RWTH-JANE
0.533 8-10 MES-REORDER
0.526 9-10 LIMSI-SOUL
11 0.480 11 TUBITAK
12 0.462 12-13 UMD
0.462 12-13 DCU
14 0.396 14 CU-ZEMAN
15 0.367 15 JHU
16 0.311 16 SHEF-WPROA
17 0.238 17 DESRT
English-German
# rank range system
1 0.637 1-2 ONLINE-B
0.636 1-2 PROMT
3 0.614 3 UEDIN-SYNTAX
0.587 3-5 ONLINE-A
0.571 4-6 UEDIN
0.554 5-6 KIT
7 0.523 7 STANFORD
8 0.507 8 LIMSI-SOUL
9 0.477 9-11 MES-REORDER
0.476 9-11 JHU
0.460 10-12 CU-ZEMAN
0.453 11-12 TUBITAK
13 0.361 13 UU
14 0.329 14-15 SHEF-WPROA
0.323 14-15 RWTH-JANE
English-Russian
# rank range system
1 0.641 1 PROMT
2 0.623 2 ONLINE-B
3 0.556 3-4 CMU
0.542 3-6 ONLINE-G
0.538 3-7 ONLINE-A
0.531 4-7 UEDIN
0.520 5-7 QCRI-MES
8 0.498 8 CU-KAREL
9 0.478 9-10 MES-QCRI
0.469 9-10 JHU
11 0.434 11-12 COMMERCIAL-3
0.426 11-13 LIA
0.419 12-13 BALAGUR
14 0.331 14 CU-ZEMAN
French-English
# rank range system
1 0.638 1 UEDIN-HEAFIELD
2 0.604 2-3 UEDIN
0.591 2-3 ONLINE-B
4 0.573 4-5 LIMSI-SOUL
0.562 4-5 KIT
0.541 5-6 ONLINE-A
7 0.512 7 MES-SIMPLIFIED
8 0.486 8 DCU
9 0.439 9-10 RWTH
0.429 9-11 CMU-T2T
0.420 10-11 CU-ZEMAN
12 0.389 12 JHU
13 0.322 13 SHEF-WPROA
English-French
# rank range system
1 0.607 1-2 UEDIN
0.600 1-3 ONLINE-B
0.588 2-4 LIMSI-SOUL
0.584 3-4 KIT
5 0.553 5-7 PROMT
0.551 5-8 STANFORD
0.547 5-8 MES
0.537 6-9 MES-INFLECTION
0.533 7-10 RWTH-PB
0.516 9-11 ONLINE-A
0.499 10-11 DCU
12 0.427 12 CU-ZEMAN
13 0.408 13 JHU
14 0.382 14 OMNIFLUENT
15 0.350 15 ITS-LATL
16 0.326 16 ITS-LATL-PE
Russian-English
# rank range system
1 0.657 1 ONLINE-B
2 0.604 2-3 CMU
0.588 2-3 ONLINE-A
4 0.562 4-6 ONLINE-G
0.561 4-6 PROMT
0.550 5-7 QCRI-MES
0.546 5-7 UCAM
8 0.527 8-9 BALAGUR
0.519 8-10 MES-QCRI
0.507 9-11 UEDIN
0.497 10-12 OMNIFLUENT
0.492 11-14 LIA
0.483 12-15 OMNIFLUENT-C
0.481 12-15 UMD
0.476 13-15 CU-KAREL
16 0.432 16 COMMERCIAL-3
17 0.417 17 UEDIN-SYNTAX
18 0.396 18 JHU
19 0.215 19 CU-ZEMAN
Table 6: Official results for the WMT13 translation task. Systems are ordered by the expected win score. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05. This method is also used to determine the
range of ranks into which system falls. Systems with grey background indicate use of resources that fall outside the constraints
provided for the shared task.
9
4 Understandability of English?Czech
For the English-to-Czech translation, we con-
ducted a variation of the ?understandability? test
as introduced in WMT09 (Callison-Burch et al,
2009) and used in WMT10. In order to obtain
additional reference translations, we conflated this
test with post-editing. The procedure was as fol-
lows:
1. Monolingual editing (also called blind edit-
ing). The first annotator is given just the MT
output and requested to correct it. Given er-
rors in MT outputs, some guessing of the
original meaning is often inevitable and the
annotators are welcome to try. If unable, they
can mark the sentences as incomprehensible.
2. Review. A second annotator is asked to
validate the monolingual edit given both the
source and reference translations. Our in-
structions specify three options:
(a) If the monolingual edit is an adequate
translation and acceptably fluent Czech,
confirm it without changes.
(b) If the monolingual edit is adequate but
needs polishing, modify the sentence
and prefix it with the label ?OK:?.
(c) If the monolingual edit is wrong, cor-
rect it. You may start from the origi-
nal unedited MT output, if that is eas-
ier. Avoid using the reference directly,
prefer words from MT output whenever
possible.
The motivation behind this procedure is that we
want to save the time necessary for reading the
sentence. If the reviewer has already considered
whether the sentence is an acceptable translation,
they do not need to read the MT output again in
order to post-edit it. Our approach is thus some-
what the converse of Aziz et al (2013) who ana-
lyze post-editing effort to obtain rankings of MT
systems. We want to measure the understandabil-
ity of MT outputs and obtain post-edits at the same
time.
Both annotation steps were carried out in
the CASMACAT/Matecat post-editing user inter-
face.9, modified to provide the relevant variants of
the sentence next to the main edit box. Screen-
shots of the two annotation phases are given in
Figure 3 and Figure 4.
9http://www.casmacat.eu/index.php?n=Workbench
Occurrence GOOD ALMOST BAD EMPTY Total
First 34.7 0.1 42.3 11.0 4082
Repeated 41.1 0.1 41.0 6.1 805
Overall 35.8 0.1 42.1 10.2 4887
Table 7: Distribution of review statuses.
Similarly to the traditional ranking task, we pro-
vided three consecutive sentences from the origi-
nal text, each translated with a different MT sys-
tem. The annotators are free to use this contex-
tual information when guessing the meaning or re-
viewing the monolingual edits. Each ?annotation
HIT? consists of 24 sentences, i.e. 8 snippets of 3
consecutive sentences.
4.1 Basic Statistics on Editing
In total, 21 annotators took part in the exercise, 20
of them contributed to monolingual editing and 19
contributed to the reviews.
Connecting each review with the monolingual
edit (some edits received multiple reviews), we ob-
tain one data row. We collected 4887 data rows
(i.e. sentence revisions) for 3538 monolingual ed-
its, covering 1468 source sentences as translated
by 12 MT systems (including the reference).
Not all MT systems were considered for each
sentence, we preferred to obtain judgments for
more source sentences.
Based on the annotation instructions, each data
row has one of the four possible statuses: GOOD,
ALMOST, BAD, and EMPTY. GOOD rows are
those where the reviewer accepted the monolin-
gual edit without changes, ALMOST edits were
modified by the reviewer but they were marked as
?OK?. BAD edits were changed by the reviewer
and no ?OK? mark was given. Finally, the sta-
tus EMPTY is assigned to rows where the mono-
lingual editor refused to edit the sentence. The
EMPTY rows nevertheless contain the (?regular?)
post-edit of the reviewer, so they still provide a
new reference translation for the sentence.
Table 7 summarizes the distribution of row sta-
tuses depending on one more significant distinc-
tion: whether the monolingual editor has seen the
sentence before or not. We see that EMPTY and
BAD monolingual edits together drop by about
6% absolute when the sentence is not new to the
monolingual editor. The occurrence is counted as
?repeated? regardless whether the annotator has
previously seen the sentence in an editing or re-
viewing task. Unless stated otherwise, we exclude
repeated edits from our calculations.
10
Figure 3: In this screen, the annotator is expected to correct the MT output given only the context of at most two neighbouring
machine-translated sentences.
ALMOST Pairwise
treated Comparisons Agreement ?
inter
separate 2690 56.0 0.270
as BAD 2690 67.9 0.351
as GOOD 2690 65.2 0.289
intra
separate 170 65.3 0.410
as BAD 170 69.4 0.386
as GOOD 170 71.8 0.422
Table 8: Annotator agreement when reviewing monolingual
edits.
4.2 Agreement on Understandability
Before looking at individual system results, we
consider annotator agreement in the review step.
Details are given in Table 8. Given a (non-
EMPTY) string from a monolingual edit, we
would like to know how often two acceptability
judgments by two different reviewers (inter-) or
the same reviewer (intra-) agree. The repeated ed-
its remain in this analysis because we are not in-
terested in the origin of the string.
Our annotation setup leads to three possible la-
bels: GOOD, ALMOST, and BAD. The agree-
ment on one of three classes is bound to be lower
than the agreement on two classes, so we also re-
interpret ALMOST as either GOOD or BAD. Gen-
erally speaking, ALMOST is a positive judgment,
so it would be natural to treat it as GOOD. How-
ever, in our particular setup, when the reviewer
modified the sentence and forgot to add the label
?OK:?, the item ended up in the BAD class. We
conclude that this is indeed the case: the inter-
annotator agreement appears higher if ALMOST
is treated as BAD. Future versions of the review-
ing interface should perhaps first ask for the yes/no
judgment and only then allow to post-edit.
The ? values in Table 8 are the Fleiss?
kappa (Fleiss, 1971), accounting for agreement by
chance given the observed label distributions.
In WMT09, the agreements for this task were
higher: 77.4 for inter-AA and 86.6 for intra-AA.
(In 2010, the agreements for this task were not re-
ported.) It is difficult to say whether the differ-
ence lies in the particular language pair, the dif-
ferent set of annotators, or the different user in-
terface for our reviewing task. In 2009 and 2010,
the reviewers were shown 5 monolingual edits at
once and they were asked to judge each as accept-
able or not acceptable. We show just one segment
and they have probably set their minds on the post-
editing rather than acceptability judgment. We be-
lieve that higher agreements can be reached if the
reviewers first validate one or more of the edits and
only then are allowed to post-edit it.
4.3 Understandability of English?Czech
Table 9 brings about the first main result of our
post-editing effort. For each system (including
the reference translation), we check how often a
monolingual edit was marked OK or ALMOST
by the subsequent reviewer. The average under-
standability across all MT systems into Czech is
44.2?1.6%. This is a considerable improvement
compared to 2009 where the best systems pro-
duced about 32% understandable sentences. In
11
Figure 4: In this screen, the annotator is expected to validate the monolingual edit, correcting it if necessary. The annotator is
expected to add the prefix ?OK:? if the correction was more or less cosmetic.
Rank System Total Observations % Understandable
Overall incl. ref. 4082 46.7?1.6
Overall without ref. 3808 44.2?1.6
1 Reference 274?31 80.3?4.8
2-6 CU-ZEMAN 348?34 51.7?5.1
2-6 UEDIN 332?33 51.5?5.4
2-6 ONLINE-B 337?34 50.7?5.3
2-6 CU-BOJAR 341?35 50.7?5.2
2-7 CU-DEPFIX 350?34 48.0?5.3
6-10 COMMERCIAL-2 358?36 43.6?5.2
6-11 COMMERCIAL-1 316?34 41.5?5.5
7-12 CU-TECTOMT 338?34 39.4?5.2
8-12 MES 346?36 38.4?5.2
8-12 CU-PHRASEFIX 394?40 38.1?4.8
10-12 SHEF-WPROA 348?32 34.2?5.1
2009 Reference 91
2009 Best System 32
2010 Reference 97
2010 Best System 58
Table 9: Understandability of English?Czech systems. The
? values indicate empirical confidence bounds at 95%. Rank
ranges were also obtained in the same resampling: in 95% of
observations, the system was ranked in the given range.
2010, the best systems or system combinations
reached 55%?58%. The test set across years and
the quality of references and judgments also play a
role. In our annotation setup, the references appear
to be correctly understandable only to 80.3?4.8%.
To estimate the variance of these results due
to the particular sentences chosen, we draw 1000
random samples from the dataset, preserving the
dataset size and repeating some. The exact num-
ber of judgments per system can thus vary. We
report the 95% empirical confidence interval after
the ??? signs in Table 9 (the systems range from
?4.8 to?5.5). When we drop individual blind ed-
itors or reviewers, the understandability judgments
differ by about ?2 to ?4. In other words, the de-
pendence on the test set appears higher than the
dependence on the annotators.
The limited size of our dataset alows us only
to separate two main groups of systems: those
ranking 2?6 and those ranking worse. This rough
grouping vaguely matches with WMT13 ranking
results as given in Table 6. A somewhat surpris-
ing observation is that two automatic corrections
ranked better in WMT13 ranking but score worse
in understandability: CU-DEPFIX fixes some lost
negation and some agreement errors of CU-BOJAR
and CU-PHRASEFIX is a standard statistical post-
editing of a transfer-based system CU-TECTOMT.
A detailed inspection of the data is necessary to
explain this.
5 More Reference Translations for Czech
Our annotation procedure described in Section 4
allowed us to obtain a considerable number of ad-
ditional reference translations on top of official
single reference.
12
Refs 1 2 3 4 5 6 7 8 9 10-16
Sents 233 709 174 123 60 48 40 27 25 29
Table 10: Number of source sentences with the given number
of distinct reference translations.
In total, our edits cover 1468 source sentences,
i.e. about a half of the official test set size, and pro-
vide 4311 unique references. On average, one sen-
tence in our set has 2.94?2.17 unique reference
translations. Table 10 provides a histogram.
It is well known that automatic MT evalua-
tion methods perform better with more references,
because a single one may not confirm a correct
part of MT output. This issue is more severe
for morphologically rich languages like Czech
where about 1/3 of MT output was correct but not
confirmed by the reference (Bojar et al, 2010).
Advanced evaluation methods apply paraphras-
ing to smooth out some of the lexical divergence
(Kauchak and Barzilay, 2006; Snover et al, 2009;
Denkowski and Lavie, 2010). Simpler techniques
such as lemmatizing are effective for morphologi-
cally rich languages (Tantug et al, 2008; Kos and
Bojar, 2009) but they will lose resolution once the
systems start performing generally well.
WMTs have taken the stance that a big enough
test set with just a single reference should compen-
sate for the lack of other references. We use our
post-edited reference translations to check this as-
sumption for BLEU and NIST as implemented in
mteval-13a (international tokenization switched
on, which is not the default setting).
We run many probes, randomly picking the test
set size (number of distinct sentences) and the
number of distinct references per sentence. Note
that such test sets are somewhat artificially more
diverse; in narrow domains, source sentences can
repeat and even appear verbatim in the training
data, and in natural test sets with multiple refer-
ences, short sentences can receive several identical
translations.
For each probe, we measure the Spearman?s
rank correlation coefficient ? of the ranks pro-
posed by BLEU or NIST and the manual ranks.
We use the same implementation as applied in the
WMT13 Shared Metrics Task (Macha?c?ek and Bo-
jar, 2013). Note that the WMT13 metrics task still
uses the WMT12 evaluation method ignoring ties,
not the expected wins. As Koehn (2012) shows,
the two methods do not differ much.
Overall, the correlation is strongly impacted by
Figure 5: Correlation of BLEU and WMT13 manual ranks
for English?Czech translation
Figure 6: Correlation of NIST and WMT13 manual ranks
for English?Czech translation
the particular choice of test sentences and refer-
ence translations. By picking sentences randomly,
similarly or equally sized test sets can reach dif-
ferent correlations. Indeed, e.g. for a test set of
about 1500 distinct sentences selected from the
3000-sentence official test set (1 reference trans-
lation), we obtain correlations for BLEU between
0.86 and 0.94.
Figure 5 plots the correlations of BLEU and the
system rankings, Figure 6 provides the same pic-
ture for NIST. The upper triangular part of the plot
contains samples from our post-edited reference
translations, the lower rectangular part contains
probes from the official test set of 3000 sentences
with 1 reference translation.
To interpret the observations, we also calculate
the average and standard deviation of correlations
for each cell in Figures 5 and 6. Figures 7 and
8 plot the values for 1, 6, 7 and 8 references for
13
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
B
L
E
U
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 7: Projections from Figure 5 of BLEU and WMT13
manual ranks for English?Czech translation
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
N
I
S
T
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 8: Projections from Figure 6 of NIST and WMT13
manual ranks for English?Czech translation
BLEU and NIST, resp. The projections confirm
that the average correlations grow with test set
size, the growth is however sub-logarithmic.
Starting from as few as a dozen of sentences, we
see that using more references is better than using
a larger test set. For BLEU, we however already
seem to reach false positives at 7 references for
one or two hundred sentences: larger sets with just
one reference may correlate slightly better.
Using one reference obtained by post-editing
seems better than using the official (independent)
reference translations. BLEU is more affected
than NIST by this difference even at relatively
large test set size. Note that our post-edits are in-
spired by all MT systems, the good as well as the
bad ones. This probably provides our set with a
certain balance.
Overall, the best balance between the test set
size and the number of references seems to lie
somewhere around 7 references and 100 or 200
sentences. Creating such a test set could be even
cheaper than the standard 3000 sentences with just
one reference. However, the wide error bars re-
mind us that even this setting can lead to correla-
tions anywhere between 0.86 and 0.96. For other
languages, data sets types or other MT evaluation
methods, the best setting can be quite different and
has to be sought for.
6 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
second edition of the WMT shared task on qual-
ity estimation builds on the previous edition of the
task (Callison-Burch et al, 2012), with variants to
this previous task, including both sentence-level
and word-level estimation, with new training and
test datasets, along with evaluation metrics and
baseline systems.
The motivation to include both sentence- and
word-level estimation come from the different po-
tential applications of these variants. Some inter-
esting uses of sentence-level quality estimation are
the following:
? Decide whether a given translation is good
enough for publishing as is.
? Inform readers of the target language only
whether or not they can rely on a translation.
? Filter out sentences that are not good enough
for post-editing by professional translators.
? Select the best translation among options
from multiple MT and/or translation memory
systems.
Some interesting uses of word-level quality es-
timation are the following:
? Highlight words that need editing in post-
editing tasks.
? Inform readers of portions of the sentence
which are not reliable.
? Select the best segments among options from
multiple translation systems for MT system
combination.
The goals of this year?s shared task were:
14
? To explore various granularity levels for the
task (sentence-level and word-level).
? To explore the prediction of more objective
scores such as edit distance and post-editing
time.
? To explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics in the task of ranking alternative
translations generated by different MT sys-
tems.
? To identify new and effective quality indica-
tors (features) for all variants of the quality
estimation task.
? To identify effective machine learning tech-
niques for all variants of the quality estima-
tion task.
? To establish the state of the art performance
in the field.
Four subtasks were proposed, as we discuss in
Sections 6.1 and 6.2. Each subtask provides spe-
cific datasets, annotated for quality according to
the subtask (Section 6.3), and evaluates the system
submissions using specific metrics (Section 6.6).
When available, external resources (e.g. SMT
training corpus) and translation engine-related re-
sources were given to participants (Section 6.4),
who could also use any additional external re-
sources (no distinction between open and close
tracks is made). Participants were also provided
with a software package to extract quality esti-
mation features and perform model learning (Sec-
tion 6.5), with a suggested list of baseline features
and learning method (Section 6.7). Participants
could submit up to two systems for each subtask.
6.1 Sentence-level Quality Estimation
Task 1.1 Predicting Post-editing Distance This
task is similar to the quality estimation task in
WMT12, but with one important difference in the
scoring variant: instead of using the post-editing
effort scores in the [1-5] range, we use HTER
(Snover et al, 2006) as quality score. This score
is to be interpreted as the minimum edit distance
between the machine translation and its manually
post-edited version, and its range is [0, 1] (0 when
no edit needs to be made, and 1 when all words
need to be edited). Two variants of the results
could be submitted in the shared task:
? Scoring: A quality score for each sentence
translation in [0,1], to be interpreted as an
HTER score; lower scores mean better trans-
lations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning). The reference ranking is de-
fined based on the true HTER scores.
Task 1.2 Selecting Best Translation This task
consists in ranking up to five alternative transla-
tions for the same source sentence produced by
multiple MT systems. We use essentially the same
data provided to participants of previous years
WMT?s evaluation metrics task ? where MT eval-
uation metrics are assessed according to how well
they correlate with human rankings. However, ref-
erence translations produced by humans are not be
used in this task.
Task 1.3 Predicting Post-editing Time For this
task systems are required to produce, for each
translation, the expected time (in seconds) it
would take a translator to post-edit such an MT
output. The main application for predictions of
this type is in computer-aided translation where
the predicted time can be used to select among dif-
ferent hypotheses or even to omit any MT output
in cases where no good suggestion is available.
6.2 Word-level Quality Estimation
Based on the data of Task 1.3, we define Task 2, a
word-level annotation task for which participants
are asked to produce a label for each token that
indicates whether the word should be changed by
a post-editor or kept in the final translation. We
consider the following two sets of labels for pre-
diction:
? Binary classification: a keep/change label,
the latter meaning that the token should be
corrected in the post-editing process.
? Multi-class classification: a label specifying
the edit action that should be performed on
the token (keep as is, delete, or substitute).
6.3 Datasets
Task 1.1 Predicting post-editing distance For
the training of models, we provided the WMT12
15
quality estimation dataset: 2,254 English-
Spanish news sentences extracted from previous
WMT translation task English-Spanish test sets
(WMT09, WMT10, and WMT12). These were
translated by a phrase-based SMT Moses system
trained on Europarl and News Commentaries cor-
pora as provided by WMT, along with their source
sentences, reference translations, post-edited
translations, and HTER scores. We used TERp
(default settings: tokenised, case insensitive,
etc., but capped to 1)10 to compute the HTER
scores. Likert scores in [1,5] were also provided,
as participants may choose to use them for the
ranking variant.
As test data, we use a subset of the WMT13
English-Spanish news test set with 500 sentences,
whose translations were produced by the same
SMT system used for the training set. To com-
pute the true HTER labels, the translations were
post-edited under the same conditions as those on
the training set. As in any blind shared task, the
HTER scores were solely used to evaluate the sub-
missions, and were only released to participants
after they submitted their systems.
A few variations of the training and test data
were provided, including a version with cases re-
stored and a version detokenized. In addition,
we provided a number of engine-internal informa-
tion from Moses for glass-box feature extraction,
such as phrase and word alignments, model scores,
word graph, n-best lists and information from the
decoder?s search graph.
Task 1.2 Selecting best translation As training
data, we provided a large set of up to five alter-
native machine translations produced by different
MT systems for each source sentence and ranked
for quality by humans. This was the outcome of
the manual evaluation of the translation task from
WMT09-WMT12. It includes two language pairs:
German-English and English-Spanish, with 7,098
and 4,592 source sentences and up to five ranked
translations, totalling 32,922 and 22,447 transla-
tions, respectively.
As test data, a set of up to five alternative ma-
chine translations per source sentence from the
WMT08 test sets was provided, with 365 (1,810)
and 264 (1,315) source sentences (translations)
for German-English and English-Spanish, respec-
tively. We note that there was some overlap be-
tween the MT systems used in the training data
10http://www.umiacs.umd.edu/?snover/terp/
and test datasets, but not all systems were the
same, as different systems participate in WMT
over the years.
Task 1.3 and Task 2 Predicting post-editing
time and word-level edits For Tasks 1.3 and 2
we provides a new dataset consisting of 22 English
news articles which were translated into Span-
ish using Moses and post-edited during a CAS-
MACAT11 field trial. Of these, 15 documents have
been processed repeatedly by at least 2 out of 5
translators, resulting in a total of 1,087 segments.
For each segment we provided:
? English source and Spanish translation.
? Spanish MT output which was used as basis
for post-editing.
? Document and translator ID.
? Position of the segment within the document.
The metadata about translator and document was
made available as we expect that translator perfor-
mance and normalisation over document complex-
ity can be helpful when predicting the time spend
on a given segment.
For the training portion of the data we also pro-
vided:
? Time to post-edit in seconds (Task 1.3).
? Binary (Keep, Change) and multiclass (Keep,
Substitute, Delete) labels on word level along
with explicit tokenization (Task 2).
The labels in Task 2 are derived by comput-
ing WER between the original machine translation
and its post-edited version.
6.4 Resources
For all tasks, we provided resources to extract
quality estimation features when these were avail-
able:
? The SMT training corpus (WMT News and
Europarl): source and target sides of the cor-
pus used to train the SMT engines for Tasks
1.1, 1.3, and 2, and truecase models gener-
ated from these. These corpora can also be
used for Task 1.2, but we note that some of
the MT systems used in the datasets of this
task were not statistical or did not use (only)
the training corpus provided by WMT.
11http://casmacat.eu/
16
? Language models: n-gram language models
of source and target languages generated us-
ing the SMT training corpora and standard
toolkits such as SRILM Stolcke (2002), and
a language model of POS tags for the target
language. We also provided unigram, bigram
and trigram counts.
? IBM Model 1 lexical tables generated by
GIZA++ using the SMT training corpora.
? Phrase tables with word alignment informa-
tion generated by scripts provided by Moses
from the parallel corpora.
? For Tasks 1.1, 1.3 and 2, the Moses config-
uration file used for decoding or the code to
re-run the entire Moses system.
? For Task 1.1, both English and Spanish re-
sources for a number of advanced features
such as pre-generated PCFG parsing models,
topic models, global lexicon models and mu-
tual information trigger models.
We refer the reader to the QUEST website12 for
a detailed list of resources provided for each task.
6.5 QUEST Framework
QUEST (Specia et al, 2013) is an open source
framework for quality estimation which provides a
wide variety of feature extractors from source and
translation texts and external resources and tools.
These range from simple, language-independent
features, to advanced, linguistically motivated fea-
tures. They include features that rely on informa-
tion from the MT system that generated the trans-
lations (glass-box features), and features that are
oblivious to the way translations were produced
(black-box features).
QUEST also integrates a well-known machine
learning toolkit, scikit-learn,13 and other algo-
rithms that are known to perform well on this task
(e.g. Gaussian Processes), providing a simple and
effective way of experimenting with techniques
for feature selection and model building, as well
as parameter optimisation through grid search.
From QUEST, a subset of 17 features and an
SVM regression implementation were used as
baseline for Tasks 1.1, 1.2 and 1.3. The software
was made available to all participants.
12http://www.quest.dcs.shef.ac.uk/
13http://scikit-learn.org/
6.6 Evaluation Metrics
Task 1.1 Predicting post-editing distance
Evaluation is performed against the HTER and/or
ranking of translations using the same metrics as
in WMT12. For the scoring variant of the task,
we use two standard metrics for regression tasks:
Mean Absolute Error (MAE) as a primary metric,
and Root of Mean Squared Error (RMSE) as a
secondary metric. To improve readability, we
report these error numbers by first mapping the
HTER values to the [0, 100] interval, to be read
as percentage-points of the HTER metric. For a
given test set S with entries si, 1 ? i ? |S|, we
denote by H(si) the proposed score for entry si
(hypothesis), and by V (si) the reference value for
entry si (gold-standard value):
MAE =
?N
i=1 |H(si)? V (si)|
|S|
RMSE =
??N
i=1(H(si)? V (si))2
|S|
Both these metrics are non-parametric, auto-
matic and deterministic (and therefore consistent),
and extrinsically interpretable. For instance, a
MAE value of 10 means that, on average, the ab-
solute difference between the hypothesized score
and the reference score value is 10 percentage
points (i.e., 0.10 difference in HTER scores). The
interpretation of RMSE is similar, with the differ-
ence that RMSE penalises larger errors more (via
the square function).
For the ranking variant of the task, we use the
DeltaAvg metric proposed in the 2012 edition of
the task (Callison-Burch et al, 2012) as our main
metric. This metric assumes that each reference
test instance has an extrinsic number associated
with it that represents its ranking with respect to
the other test instances. For completeness, we
present here again the definition of DeltaAvg.
The goal of the DeltaAvg metric is to measure
how valuable a proposed ranking (which we call a
hypothesis ranking) is, according to the true rank-
ing values associated with the test instances. We
first define a parametrised version of this metric,
called DeltaAvg[n]. The following notations are
used: for a given entry sentence s, V (s) represents
the function that associates an extrinsic value to
that entry; we extend this notation to a set S, with
V (S) representing the average of all V (s), s ? S.
17
Intuitively, V (S) is a quantitative measure of the
?quality? of the set S, as induced by the extrinsic
values associated with the entries in S. For a set
of ranked entries S and a parameter n, we denote
by S1 the first quantile of set S (the highest-ranked
entries), S2 the second quantile, and so on, for n
quantiles of equal sizes.14 We also use the nota-
tion Si,j = ?jk=i Sk. Using these notations, wedefine:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1 ? V (S)
When the valuation function V is clear from the
context, we write DeltaAvg[n] for DeltaAvgV [n].
The parameter n represents the number of quan-
tiles we want to split the set S into. For instance,
n = 2 gives DeltaAvg[2] = V (S1)?V (S), hence it
measures the difference between the quality of the
top quantile (top half) S1 and the overall quality
(represented by V (S)). For n = 3, DeltaAvg[3] =
(V (S1)+V (S1,2)/2?V (S) = ((V (S1)?V (S))+
(V (S1,2?V (S)))/2, hence it measures an average
difference across two cases: between the quality of
the top quantile (top third) and the overall quality,
and between the quality of the top two quantiles
(S1 ? S2, top two-thirds) and the overall quality.
In general, DeltaAvg[n] measures an average dif-
ference in quality across n ? 1 cases, with each
case measuring the impact in quality of adding an
additional quantile, from top to bottom. Finally,
we define:
DeltaAvgV =
?N
n=2 DeltaAvgV [n]
N ? 1
where N = |S|/2. As before, we write DeltaAvg
for DeltaAvgV when the valuation function V is
clear from the context. The DeltaAvg metric is an
average across all DeltaAvg[n] values, for those
n values for which the resulting quantiles have at
least 2 entries (no singleton quantiles).
We present results for DeltaAvg using as valu-
ation function V the HTER scores, as defined in
Section 6.3. We also use Spearman?s rank correla-
tion coefficient ? as a secondary metric.
Task 1.2 Selecting best translation The perfor-
mance on the task of selecting the best transla-
tion from a pool of translation candidates is mea-
14If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
sured by comparing proposed (hypothesis) rank-
ings against human-produced rankings. The met-
ric used is Kendall?s ? rank correlation coefficient,
computed as follows:
? = |concordant pairs| ? |discordant pairs||total pairs|
where a concordant pair is a pair of two transla-
tions for the same source segment in which the
ranking order proposed by a human annotator and
the ranking order of the hypothesis agree; in a dis-
cordant pair, they disagree. The possible values of
? range between 1 (where all pairs are concordant)
and ?1 (where all pairs are discordant). Thus a
system with ranking predictions having a higher
? value makes predictions that are more similar
to human judgements than a system with ranking
predictions having a lower ? . Note that, in general,
being able to predict rankings with an accuracy
of ? = ?1 is as difficult as predicting rankings
with an accuracy of ? = 1, whereas a completely
random ranking would have an expected value of
? = 0. The range is therefore said to be symmet-
ric.
However, there are two distinct ways of mea-
suring rank correlation using Kendall?s ? , related
to the way ties are treated. They greatly affect how
Kendall?s ? numbers are to be interpreted, and es-
pecially the symmetry property. We explain the
difference in detail in what follows.
Kendall?s ? with ties penalised If the goal is
to measure to what extent the difference in qual-
ity visible to a human annotator has been captured
by an automatically produced hypothesis (recall-
oriented view), then proposing a tie between t1
and t2 (t1-equal-to-t2) when the pair was judged
(in the reference) as t1-better-than-t2 is treated as
a failure-to-recall. In other words, it is as bad as
proposing t1-worse-than-t2. Henceforth, we call
this recall-oriented measure ?Kendall?s ? with ties
penalised?. This metric has the following proper-
ties:
? it is completely fair when comparing differ-
ent methods to produce ranking hypotheses,
because the denominator (number of total
pairs) is the same (it is the number of non-
tied pairs under the human judgements).
? it is non-symmetric, in the sense that a value
of ? = ?1 is not as difficult to obtain as ? =
18
1 (simply proposing only ties gets a ? = ?1);
hence, the sign of the ? value matters.
? the expected value of a completely random
ranking is not necessarily ? = 0, but rather
depends on the number of ties in the refer-
ence rankings (i.e., it is test set dependent).
Kendall?s ? with ties ignored If the goal
is to measure to what extent the difference in
quality signalled by an automatically produced
hypothesis is reflected in the human annota-
tion (precision-oriented view), then proposing t1-
equal-to-t2 when the pair was judged differently
in the reference does no harm the metric.
Henceforth, we call this precision-oriented
measure ?Kendall?s ? with ties ignored?. This
metric has the following properties:
? it is not completely fair when comparing dif-
ferent methods to produce ranking hypothe-
ses, because the denominator (number of to-
tal pairs) may not be the same (it is the num-
ber of non-tied pairs under each system?s pro-
posal).
? it is symmetric, in the sense that a value of
? = ?1 is as difficult to obtain as ? = 1;
hence, the sign of the ? value may not mat-
ter. 15
? the expected value of a completely random
ranking is ? = 0 (test-set independent).
The first property is the most worrisome from
the perspective of reporting the results of a shared
task, because a system may fare very well on this
metric simply because it choses not to commit
(proposes ties) most of the time. Therefore, to
give a better understanding of the systems? perfor-
mance, for Kendall?s ? with ties ignored we also
provide the number of non-ties proposed by each
system.
Task 1.3 Predicting post-editing time Submis-
sions are evaluated in terms of Mean Average Er-
ror (MAE) against the actual time spent by post-
editors (in seconds). By using a linear error mea-
sure we limit the influence of outliers: sentences
that took very long to edit or where the measure-
ment taken is questionable.
15In real life applications this distinction matters. Even
if, from a computational perspective, it is as hard to get ?
close to?1 as it is to get it close to 1, knowing the sign is the
difference between selecting the best or the worse translation.
To further analyse the influence of extreme val-
ues, we also compute Spearman?s rank correlation
? coefficient which does not depend on the abso-
lute values of the predictions.
We also give RMSE and Pearson?s correlation
coefficient r for reference.
Task 2 Predicting word-level scores The word-
level task is primarily evaluated by macro-
averaged F-measure. Because the class distribu-
tion is skewed ? in the test data about one third
of the tokens are marked as correct ? we compute
precision and recall and F1 for each class individ-
ually. Consider the following confusion matrix for
the two classes Keep and Change:
predicted
(K)eep (C)hange
expected (K)eep 10 20(C)hange 30 40
For the given example we derive true-positive
(tp), true-negative (tn), false-positive (fp), and
false-negative (fn) counts:
tpK = 10 fpK = 30 fnK = 20
tpC = 40 fpC = 20 fnC = 30
precisionK =
tpK
tpK + fpK
= 10/40
recallK =
tpK
tpK + fnK
= 10/30
F1,K =
2 ? precisionK ? recallK
precisionK +recallK
A single cumulative statistic can be computed
by averaging the resulting F-measures (macro av-
eraging) or by micro averaging in which case pre-
cision and recall are first computed by accumulat-
ing the relevant values for all classes (O?zgu?r et al,
2005), e.g.
precision = tpK + tpC(tpK + fpK) + (tpC + fpC)
The latter gives equal weight to each exam-
ple and is therefore dominated by performance on
the largest class while macro-averaged F-measure
gives equal weight to each class.
The same setup is used to evaluate the perfor-
mance in the multiclass setting. Please note that
here the test data only contains 4% examples for
class (D)elete.
19
ID Participating team
CMU Carnegie Mellon University, USA (Hildebrand and Vogel, 2013)
CNGL Centre for Next Generation Localization, Ireland (Bicici, 2013b)
DCU Dublin City University, Ireland (Almaghout and Specia, 2013)
DCU-SYMC Dublin City University & Symantec, Ireland (Rubino et al, 2013b)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis and
Popovic, 2013)
FBK-UEdin Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de
Souza et al, 2013)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al, 2013)
LIMSI Laboratoire d?Informatique pour la Me?canique et les Sciences de l?Inge?nieur,
France (Singh et al, 2013)
LORIA Lorraine Laboratory of Research in Computer Science and its Applications,
France (Langlois and Smaili, 2013)
SHEF University of Sheffield, UK (Beck et al, 2013)
TCD-CNGL Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013)
TCD-DCU-CNGL Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and
Rubino, 2013)
UMAC University of Macau, China (Han et al, 2013)
UPC Universitat Politecnica de Catalunya, Spain (Formiga et al, 2013b)
Table 11: Participants in the WMT13 Quality Estimation shared task.
6.7 Participants
Table 11 lists all participating teams submitting
systems to any subtask in this shared task. Each
team was allowed up to two submissions for each
subtask. In the descriptions below participation in
specific tasks is denoted by a task identifier: T1.1,
T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.3):
QUEST was used to extract 17 system-
independent features from the source and
translation files and the SMT training cor-
pus that were found to be relevant in previous
work (same features as in the WMT12 shared
task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? Language model probability of source
and target sentences using language
models provided by the task.
? average number of translations per
source word in the sentence: as given
by IBM 1 model thresholded so that
P (t|s) > 0.2, and so that P (t|s) > 0.01
weighted by the inverse frequency of
each word in the source side of the SMT
training corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source side of the
SMT training corpus
? percentage of unigrams in the source
sentence seen in the source side of the
SMT training corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within the
SCIKIT-LEARN toolkit. The ?,  and C pa-
rameters were optimized using a grid-search
and 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as a ?baseline?, it is in fact a strong
system. For tasks of the same type as 1.1
and 1.3, it has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting post-editing effort, as it
has also been shown in the previous edition
of the task (Callison-Burch et al, 2012).
The same features could be useful for a base-
line system for Task 1.2. In our official re-
20
sults, however, the baseline for Task 1.2 is
simpler than that: it proposes random ranks
for each pair of alternative translations for a
given source sentence, as we will discuss in
Section 6.8.
CMU (T1.1, T1.2, T1.3): The CMU quality
estimation system was trained on features
based on language models, the MT sys-
tem?s distortion model and phrase table fea-
tures, statistical word lexica, several sentence
length statistics, source language word and
bi-gram frequency statistics, n-best list agree-
ment and diversity, source language parse,
source-target word alignment and a depen-
dency parse based cohesion penalty. These
features were extracted using GIZA++, a
forced alignment algorithm and the Stanford
parser (de Marneffe et al, 2006). The pre-
diction models were trained using four clas-
sifiers in the Weka toolkit (Hall et al, 2009):
linear regression, M5P trees, multi layer per-
ceptron and SVM regression. In addition to
main system submission, a classic n-best list
re-ranking approach was used for Task 1.2.
CNGL (T1.1, T1.2, T1.3, T2): CNGL systems
are based on referential translation machines
(RTM) (Bic?ici and van Genabith, 2013), par-
allel feature decay algorithms (FDA) (Bicici,
2013a), and machine translation performance
predictor (MTPP) (Bic?ici et al, 2013), all
of which allow to obtain language and MT
system-independent predictions. For each
task, RTM models were developed using the
parallel corpora and the language model cor-
pora distributed by the WMT13 translation
task and the language model corpora pro-
vided by LDC for English and Spanish.
The sentence-level features are described in
MTPP (Bic?ici et al, 2013); they include
monolingual or bilingual features using n-
grams defined over text or common cover
link (CCL) (Seginer, 2007) structures as the
basic units of information over which sim-
ilarity calculations are made. RTMs use
308 features about coverage and diversity,
IBM1, and sentence translation performance,
retrieval closeness and minimum Bayes re-
trieval risk, distributional similarity and en-
tropy, IBM2 alignment, character n-grams,
and sentence readability. The learning mod-
els are Support Vector Machines (SVR) and
SVR with partial least squares (SVRPLS).
The word-level features include CCL links,
word length, location, prefix, suffix, form,
context, and alignment, totalling 511K fea-
tures for binary classification, and 637K for
multiclass classification. Generalised lin-
ear models (GLM) (Collins, 2002) and GLM
with dynamic learning (GLMd) were used.
DCU (T1.2): The main German-English submis-
sion uses six Combinatory Categorial Gram-
mar (CCG) features: CCG supertag lan-
guage model perplexity and log probability,
the number of maximal CCG constituents in
the translation output which are the highest-
probability minimum number of CCG con-
stituents that span the translation output, the
percentage of CCG argument mismatches be-
tween each subsequent CCG supertags, the
percentage of CCG argument mismatches be-
tween each subsequent CCG maximal cate-
gories and the minimum number of phrases
detected in the translation output. A second
submission uses the aforementioned CCG
features combined with 80 features from
QUEST as described in (Specia, 2011). For
the CCG features, the C&C parser was used
to parse the translation output. Moses was
used to build the phrase table from the SMT
training corpus with maximum phrase length
set to 7. The language model of supertags
was built using the SRILM toolkit. As learn-
ing algorithm, Logistic Regression as pro-
vided by the SCIKIT-LEARN toolkit was used.
The training data was prepared by converting
each ranking of translation outputs to a set
of pairwise comparisons according to the ap-
proach proposed by Avramidis et al (2011).
The rankings were generated back from pair-
wise comparisons predicted by the model.
DCU-SYMC (T1.1): The DCU-Symantec team
employed a wide set of features which in-
cluded language model, n-gram counts and
word-alignment features as well as syntac-
tic features, topic model features and pseudo-
reference features. The main learning algo-
rithm was SVR, but regression tree learning
was used to perform feature selection, re-
ducing the initial set of 442 features to 96
features (DCU-Symantec alltypes) and 134
21
(DCU-Symantec combine). Two methods
for feature selection were used: a best-first
search in the feature space using regression
trees to evaluate the subsets, and reading bi-
narised features directly from the nodes of
pruned regression trees.
The following NLP tools were used in feature
extraction: the Brown English Wall-Street-
Journal-trained statistical parser (Charniak
and Johnson, 2005), a Lexical Functional
Grammar parser (XLE), together with a
hand-crafted Lexical Functional Grammar,
the English ParGram grammar (Kaplan et al,
2004), and the TreeTagger part-of-speech
tagger (Schmidt, 1994) with off-the-shelf
publicly available pre-trained tagging mod-
els for English and Spanish. For pseudo-
reference features, the Bing, Moses and Sys-
tran translation systems were used. The Mal-
let toolkit (McCallum, 2002) was used to
build the topic models and features based on
a grammar checker were extracted with Lan-
guageTool.16
DFKI (T1.2, T1.3): DFKI?s submission for Task
1.2 was based on decomposing rankings into
pairs (Avramidis, 2012), where the best sys-
tem for each pair was predicted with Lo-
gistic Regression (LogReg). For German-
English, LogReg was trained with Stepwise
Feature Selection (Hosmer, 1989) on two
feature sets: Feature Set 24 includes ba-
sic counts augmented with PCFG parsing
features (number of VPs, alternative parses,
parse probability) on both source and tar-
get sentences (Avramidis et al, 2011), and
pseudo-reference METEOR score; the most
successful set, Feature Set 33 combines those
24 features with the 17 baseline features. For
English-Spanish, LogReg was used with L2
Regularisation (Lin et al, 2007) and two fea-
ture sets were devised after scoring features
with ReliefF (Kononenko, 1994) and Infor-
mation Gain (Hunt et al, 1966). Feature Set
431 combines 30 features with highest abso-
lute Relief-F and Information Gain (15 from
each). features with the highest
Task 1.3 was modelled using feature sets
selected after Relief-F scoring of external
black-box and glass-box features extracted
16http://www.languagetool.org/
from the SMT decoding process. The most
successful submission (linear6) was trained
with Linear Regression including the 17 fea-
tures with highest positive Relief-F. Most
prominent features include the alternative
possible parses of the source and target sen-
tence, the positions of the phrases with the
lowest and highest probability and future
cost estimate in the translation, the counts of
phrases in the decoding graph whose prob-
ability or whether the future cost estimate
is higher/lower than their standard deviation,
counts of verbs and determiners, etc. The
second submission (pls8) was trained with
Partial Least Squares regression (Stone and
Brooks, 1990) including more glass-box fea-
tures.
FBK-Uedin (T1.1, T1.3):
The submissions explored features built on
MT engine resources including automatic
word alignment, n-best candidate translation
lists, back-translations and word posterior
probabilities. Information about word align-
ments is used to extract quantitative (amount
and distribution of the alignments) and qual-
itative (importance of the aligned terms) fea-
tures under the assumption that alignment
information can help tasks where sentence-
level semantic relations need to be identified
(Souza et al, 2013). Three similar English-
Spanish systems are built and used to provide
pseudo-references (Soricut et al, 2012) and
back-translations, from which automatic MT
evaluation metrics could be computed and
used as features.
All features were computed over a concatena-
tion of several publicly available parallel cor-
pora for the English-Spanish language pair
such as Europarl, News Commentary, and
MultiUN. The models were developed using
supervised learning algorithms: SVMs (with
feature selection step prior to model learning)
and extremely randomized trees.
LIG (T2): The LIG systems are designed to
deal with both binary and multiclass variants
of the word level task. They integrate sev-
eral features including: system-based (graph
topology, language model, alignment con-
text, etc.), lexical (Part-of-Speech tags), syn-
tactic (constituent label, distance to the con-
22
stituent tree root) and semantic (target and
source polysemy count). Besides the exist-
ing components of the SMT system, feature
extraction requires further external tools and
resources, such as: TreeTagger (for POS tag-
ging), Bekerley Parser trained with AnCora
treebank (for generating constituent trees in
Spanish), WordNet and BabelNet (for pol-
ysemy count), Google Translate. The fea-
ture set is then combined and trained using
a Conditional Random Fields (CRF) learn-
ing method. During the labelling phase, the
optimal threshold is tuned using a small de-
velopment set split from the original training
set. In order to retain the most informative
features and eliminate the redundant ones, a
Sequential Backward Selection algorithm is
employed over the all-feature systems. With
the binary classifier, the Boosting technique
is applied to allow a number of sub feature
sets to complement each other, resulting in
the ?stronger? combined system.
LIMSI (T1.1, T1.3): The two tasks were treated
as regression problems using a simple elas-
tic regression, a linear model trained with L1
and L2 regularisers. Regarding features, the
submissions mainly aimed at evaluating the
usefulness for quality estimation of n-gram
posterior probabilities (Gispert et al, 2013)
that quantify the probability for a given n-
gram to be part of the system output. Their
computation relies on all the hypotheses con-
sidered by a SMT system during decoding:
intuitively, the more hypotheses a n-gram ap-
pears in, the more confident the system is
that this n-gram is part of the correct trans-
lation, and the higher its posterior probabil-
ity is. The feature set contains 395 other fea-
tures that differs, in two ways, from the tra-
ditional features used in quality estimation.
First, it includes several features based on
large span continuous space language mod-
els (Le et al, 2011) that have already proved
their efficiency both for the translation task
and the quality estimation task. Second, each
feature was expanded into two ?normalized
forms? in which their value was divided ei-
ther by the source length or the target length
and, when relevant, into a ?ratio form? in
which the feature value computed on the tar-
get sentence is divided by its value computed
in the source sentence.
LORIA (T1.1): The system uses the 17 baseline
features, plus several numerical and boolean
features computed from the source and target
sentences (Langlois et al, 2012). These are
based on language model information (per-
plexity, level of back-off, intra-lingual trig-
gers), translation table (IBM1 table, inter-
lingual triggers). For language models, for-
ward and backward models are built. Each
feature gives a score to each word in the sen-
tence, and the score of the sentence is the av-
erage of word scores. For several features,
the score of a word depends on the score of its
neighbours. This leads to 66 features. Sup-
port Vector Machines are used to learn a re-
gression model. In training is done in a multi-
stage procedure aimed at increasing the size
of the training corpus. Initially, the train-
ing corpus with machine translated sentences
provided by the task is used to train an SVM
model. Then this model is applied to the post-
edited and reference sentences (also provided
as part of the task). These are added to the
quality estimation training corpus using as la-
bels the SVM predictions. An algorithm to
tune the predicted scores on a development
corpus is used.
SHEF (T1.1, T1.3): These submissions use
Gaussian Processes, a non-parametric prob-
abilistic learning framework for regression,
along with two techniques to improve predic-
tion performance and minimise the amount
of resources needed for the problem: feature
selection based on optimised hyperparame-
ters and active learning to reduce the training
set size (and therefore the annotation effort).
The initial set features contains all black box
and glass box features available within the
QUEST framework (Specia et al, 2013) for
the dataset at hand (160 in total for Task 1.1,
and 80 for Task 1.3). The query selection
strategy for active learning is based on the
informativeness of the instances using Infor-
mation Density, a measure that leverages be-
tween the variance among instances and how
dense the region (in the feature space) where
the instance is located is. To perform fea-
ture selection, following (Shah et al, 2013)
features are ranked by the Gaussian Process
23
algorithm according to their learned length
scales, which can be interpreted as the rel-
evance of such feature for the model. This
information was used for feature selection
by discarding the lowest ranked (least use-
ful) ones. based on empirical results found
in (Shah et al, 2013), the top 25 features for
both models were selected and used to retrain
the same regression algorithm.
UPC (T1.2): The methodology used a broad set
of features, mainly available through the last
version of the Asiya toolkit for MT evalua-
tion (Gonza`lez et al, 2012)17. Concretely,
86 features were derived for the German-to-
English and 97 features for the English-to-
Spanish tasks. These features cover differ-
ent approaches and include standard qual-
ity estimation features, as provided by the
above mentioned Asiya and QUEST toolk-
its, but also a variety of features based on
pseudo-references, explicit semantic analy-
sis and specialised language models trained
on the parallel and monolingual corpora pro-
vided by the WMT Translation Task.
The system selection task is approached by
means of pairwise ranking decisions. It uses
Random Forest classifiers with ties, expand-
ing the work of 402013cFormiga et al), from
which a full ranking can be derived and the
best system per sentence is identified. Once
the classes are given by the Random Forest,
one can build a graph by means of the adja-
cency matrix of the pairwise decision. The fi-
nal ranking is assigned through a dominance
scheme similar to Pighin et al (2012).
An important remark of the methodology is
the feature selection process, since it was no-
ticed that the learner was sensitive to the fea-
tures used. Selecting the appropriate set of
features was crucial to achieve a good per-
formance. The best feature combination was
composed of: i) a baseline quality estimation
feature set (Asiya or Quest) but not both of
them, ii) Length Model, iii) Pseudo-reference
aligned based features, and iv) adapted lan-
guage models. However, within the de-en
task, substituting Length Model and Aligned
Pseudo-references by the features based on
17http://asiya.lsi.upc.edu/
Semantic Roles could bring marginally bet-
ter accuracy.
TCD-CNGL (T1.1) and TCD-DCU-CNGL
(T1.3): The system is based on features
which are commonly used for style classifi-
cation (e.g. author identification). The as-
sumption is that low/high quality translations
can be characterised by some patterns which
are frequent and/or differ significantly from
the opposite category. Such features are in-
tended to focus on striking patterns rather
than to capture the global quality in a sen-
tence, but they are used in conjunction with
classical features for quality estimation (lan-
guage modelling, etc.). This requires two
steps in the training process: first the refer-
ence categories against which sentences will
be compared are built, then the standard qual-
ity estimation model training stage is per-
formed. Both datasets (Tasks 1.1 and 1.3)
were used for both tasks. Since the number
of features can be very high (up to 65,000),
a combination of various heuristics for se-
lecting features was used before the training
stage (the submitted systems were trained us-
ing SVM with RBF kernels).
UMAC (T1.1, T1.2, T2): For Task 1.1, the fea-
ture set consists in POS sequences of the
source and target languages, using 12 uni-
versal tags that are common in both lan-
guages. The algorithm is an enhanced ver-
sion of the BLEU metric (EBLEU) designed
with a modified length penalty and added re-
call factor, and having the precision and re-
call components grouped using the harmonic
mean. For Task 1.2, in addition to the uni-
versal POS sequences of the source and tar-
get languages, features include the scores of
length penalty, precision, recall and rank.
Variants of EBLEU with different strategies
for alignment are used, as well as a Na??ve
Bayes classification algorithm. For Task 2,
the features used are unigrams (from previous
4th to following 3rd tokens), bigrams (from
previous 2nd to following 2nd tokens), skip
bigrams (previous and next token), trigrams
(from previous 2nd to following 2nd tokens).
The learning algorithms are Conditional Ran-
dom Fields and Na??ve Bayes.
24
6.8 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing distance
Table 12 summarises the results for the ranking
variant of the task. They are sorted from best to
worse using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are CNGL SVRPLS, with a
DeltaAvg score of 11.09, and DCU-SYMC all-
types, with a DeltaAvg score of 10.13. While the
former holds the higher score, the difference is not
significant at the p ? 0.05 level as estimated by a
bootstrap resampling test.
Both submissions are better than the baseline
system by a very wide margin, a larger relative im-
provement than that obtained in the corresponding
WMT12 task. In addition, five submissions (out
of 12 systems) scored significantly higher than the
baseline system (systems above the middle gray
area), which is a larger proportion than that in last
year?s task (only 3 out of 16 systems), indicat-
ing that this shared task succeeded in pushing the
state-of-the-art performance to new levels.
In addition to the performance of the official
submission, we report results obtained by two or-
acle methods: the gold-label HTER metric com-
puted against the post-edited translations as ref-
erence (Oracle HTER), and the BLEU metric (1-
BLEU to obtain the same range as HTER) com-
puted against the same post-edited translations as
reference (Oracle HBLEU). The ?Oracle HTER?
DeltaAvg score of 16.38 gives an upperbound in
terms of DeltaAvg for the test set used in this eval-
uation. It indicates that, for this set, the differ-
ence in post-editing effort between the top quality
quantiles and the overall quality is 16.38 on aver-
age. The oracle based on HBLEU gives a lower
DeltaAvg score, which is expected since HTER
was our actual gold label. However, it is still
significantly higher than the score of the winning
submission, which shows that there is significant
room for improvement even by the highest scor-
ing submissions.
The results for the scoring variant of the task
are presented in Table 13, sorted from best to
worse by using the MAE metric scores as primary
key and the RMSE metric scores as secondary key.
According to MAE scores, the winning submis-
sion is SHEF FS (MAE = 12.42), which uses fea-
ture selection and a novel learning algorithm for
the task, Gaussian Processes. The baseline sys-
tem is measured to have an MAE of 14.81, with
six other submissions having performances that
are not different from the baseline at a statisti-
cally significant level, as shown by the gray area
in the middle of Table 13). Nine submissions (out
of 16) scored significantly higher than the base-
line system (systems above the middle gray area),
a considerably higher proportion of submissions
as compared to last year (5 out of 19), which indi-
cates that this shared task also succeeded in push-
ing the state-of-the-art performance to new levels
in terms of absolute scoring. Only one (6%) sys-
tem scored significantly lower than the baseline,
as opposed to 8 (42%) in last year?s task.
For the sake of completeness, we also show or-
acles figures using the same methods as for the
ranking variant of the task. Here the lowerbound
in error (Oracle HTER) will clearly be zero, as
both MAE and RMSE are measured against the
same gold label used for the oracle computation.
?Oracle HBLEU? is also not indicative in this
case, as the although the values for the two metrics
(HTER and HBLEU) are within the same ranges,
they are not directly comparable. This explains the
larger MAE/RMSE figures for ?Oracle HBLEU?
than those for most submissions.
Task 1.2 Selecting the best translation
Below we present the results for this task for each
of the two Kendall?s ? flavours presented in Sec-
tion 6.6, for the German-English test set (Tables 14
and 16) and the English-Spanish test set (Tables 15
and 17). The results are sorted from best to worse
using each of the Kendall?s ? metric flavours.
For German-English, the winning submission is
DFKI?s logRegFss33 entry, for both Kendall?s ?
with ties penalised and ties ignored, with ? = 0.31
(since this submission has no ties, the two met-
rics give the same ? value). A trivial baseline that
proposes random ranks (with ties allowed) has a
Kendall?s ? with ties penalised of -0.12 (as this
metric penalises the system?s ties that were non-
ties in the reference), and a Kendall?s ? with ties
ignored of 0.08. Most of the submissions per-
formed better than this simple baseline. More in-
terestingly perhaps is the comparison between the
best submission and the performance by an ora-
25
System ID DeltaAvg Spearman ?
? CNGL SVRPLS 11.09 0.55
? DCU-SYMC alltypes 10.13 0.59
SHEF FS 9.76 0.57
CNGL SVR 9.88 0.51
DCU-SYMC combine 9.84 0.59
CMU noB 8.98 0.57
SHEF FS-AL 8.85 0.50
Baseline bb17 SVR 8.52 0.46
CMU full 8.23 0.54
LIMSI 8.15 0.44
TCD-CNGL open 6.03 0.33
TCD-CNGL restricted 5.85 0.31
UMAC 2.74 0.11
Oracle HTER 16.38 1.00
Oracle HBLEU 15.74 0.93
Table 12: Official results for the ranking variant of the WMT13 Quality Estimation Task 1.1. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test. Oracle results that use human-references are also shown for comparison purposes.
System ID MAE RMSE
? SHEF FS 12.42 15.74
SHEF FS-AL 13.02 17.03
CNGL SVRPLS 13.26 16.82
LIMSI 13.32 17.22
DCU-SYMC combine 13.45 16.64
DCU-SYMC alltypes 13.51 17.14
CMU noB 13.84 17.46
CNGL SVR 13.85 17.28
FBK-UEdin extra 14.38 17.68
FBK-UEdin rand-svr 14.50 17.73
LORIA inctrain 14.79 18.34
Baseline bb17 SVR 14.81 18.22
TCD-CNGL open 14.81 19.00
LORIA inctraincont 14.83 18.17
TCD-CNGL restricted 15.20 19.59
CMU full 15.25 18.97
UMAC 16.97 21.94
Oracle HTER 0.00 0.00
Oracle HBLEU (1-HBLEU) 16.85 19.72
Table 13: Official results for the scoring variant of the WMT13 Quality Estimation Task 1.1. The winning submission is
indicated by a ? (it is significantly better than the other submissions according to bootstrap resampling (10k times) with 95%
confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant level
according to the same test. Oracle results that use human-references are also shown for comparison purposes.
26
German-English System ID Kendall?s ? with ties penalised
? DFKI logRegFss33 0.31
DFKI logRegFss24 0.28
CNGL SVRPLSF1 0.17
CNGL SVRF1 0.17
DCU CCG 0.15
UPC AQE+SEM+LM 0.11
UPC AQE+LeM+ALGPR+LM 0.10
DCU baseline+CCG 0.00
Baseline Random-ranks-with-ties -0.12
UMAC EBLEU-I -0.39
UMAC NB-LPR -0.49
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.19
Oracle BLEU (margin 0.01) 0.05
Oracle METEOR-ex (margin 0.00) 0.23
Oracle METEOR-ex (margin 0.01) 0.06
Table 14: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties penalised
? CNGL SVRPLSF1 0.15
CNGL SVRF1 0.13
DFKI logRegL2-411 0.09
DFKI logRegL2-431 0.04
UPC QQE+LeM+ALGPR+LM -0.03
UPC AQE+LeM+ALGPR+LM -0.06
CMU BLEUopt -0.11
Baseline Random-ranks-with-ties -0.23
UMAC EBLEU-A -0.27
UMAC EBLEU-I -0.35
CMU cls -0.63
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.17
Oracle BLEU (margin 0.02) -0.06
Oracle METEOR-ex (margin 0.00) 0.19
Oracle METEOR-ex (margin 0.02) 0.05
Table 15: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
27
German-English System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? DFKI logRegFss33 0.31 882/882
DFKI logRegFss24 0.28 882/882
UPC AQE+SEM+LM 0.27 768/882
UPC AQE+LeM+ALGPR+LM 0.24 788/882
DCU CCG 0.18 862/882
CNGL SVRPLSF1 0.17 882/882
CNGL SVRF1 0.17 881/882
Baseline Random-ranks-with-ties 0.08 718/882
DCU baseline+CCG 0.01 874/882
UMAC NB-LPR 0.01 447/882
UMAC EBLEU-I -0.03 558/882
Oracle Human 1.00 882/882
Oracle BLEU (margin 0.00) 0.22 859/882
Oracle BLEU (margin 0.01) 0.27 728/882
Oracle METEOR-ex (margin 0.00) 0.20 869/882
Oracle METEOR-ex (margin 0.01) 0.24 757/882
Table 16: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? CMU cls 0.23 192/633
CNGL SVRPLSF1 0.16 632/633
CNGL SVRF1 0.13 631/633
DFKI logRegL2-411 0.13 610/633
UPC QQE+LeM+ALGPR+LM 0.11 554/633
UPC AQE+LeM+ALGPR+LM 0.08 554/633
UMAC EBLEU-A 0.07 430/633
DFKI logRegL2-431 0.04 633/633
Baseline Random-ranks-with-ties 0.03 507/633
UMAC EBLEU-I 0.02 407/633
CMU BLEUopt -0.11 633/633
Oracle Human 1.00 633/633
Oracle BLEU (margin 0.00) 0.19 621/633
Oracle BLEU (margin 0.02) 0.26 474/633
Oracle METEOR-ex (margin 0.00) 0.25 623/633
Oracle METEOR-ex (margin 0.02) 0.28 517/633
Table 17: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
28
cle method that has access to human-created refer-
ences. This oracle uses human references to com-
pute BLEU and METEOR scores for each trans-
lation segment, and consequently computes rank-
ings for the competing translations based on these
scores. To reflect the impact of ties on the two
versions of Kendall?s ? metric we use, we allow
these ranks to be tied if the difference between the
oracle BLEU or METEOR scores is smaller than
a margin (see lower section of Tables 14 and 16,
with margins of 0 and 0.01 for the scores). For ex-
ample, under a regime of BLEU with margin 0.01,
a translation with BLEU score of 0.172 would get
the same rank as a translation with BLEU score of
0.164 (difference of 0.008), but a higher rank than
a translation with BLEU score of 0.158 (difference
of 0.014). Not surprisingly, under the Kendall?s
? with ties penalised the best Oracle BLEU or
METEOR performance happens for a 0.0 mar-
gin (which makes ties possible only for exactly-
matching scores), for a value of ? = 0.19 and
? = 0.23, respectively. Under the Kendall?s ? with
ties ignored, the Oracle BLEU performance for a
0.01 margin (i.e, translations under 1 BLEU point
should be considered as having the same rank)
achieves ? = 0.27, while Oracle METEOR for a
0.01 margin achieves ? = 0.24. These values are
lower than the ? = 0.31 of the winning submis-
sion without access to reference translations, sug-
gesting that quality estimation models are capable
of better modelling translation differences com-
pared to traditional, human reference-based MT
evaluation metrics.
For English-Spanish, under Kendall?s ? with
ties penalised the winning submission is CNGL?s
SVRPLSF1, with ? = 0.15. Under Kendall?s ?
with ties ignored, the best scoring submission is
CMU?s cls with ? = 0.23, but this is achieved
by offering non-tie judgements only for 192 of the
633 total judgements (30% of them). As we dis-
cussed in Section 6.6, the ?Kendall?s ? with ties
ignored? metric is weak with respect to compar-
ing different submissions, since it favours systems
that are do not commit to a given rank and rather
produce a large number of ties. This becomes even
clearer when we look at the performance of the or-
acle methods (Tables 15 and 17). Under Kendall?s
? with ties penalised, ?Oracle BLEU? (margin
0.00) achieves ? = 0.17, while under Kendall?s
? with ties ignored, ?Oracle BLEU? (margin 0.02)
has a ? = 0.26. This results in 474 non-tie deci-
sions (75% of them), and a better ? value com-
pared to ?Oracle BLEU? (margin 0.00), with a
? = 0.19 under the same metric. The oracle values
for both BLEU and METEOR are close to the ?
values of the winning submissions, supporting the
conclusion that quality estimation techniques can
successfully replace traditional, human reference-
based MT evaluation metrics.
Task 1.3 Predicting post-editing time
Results for this task are presented in Table 18.
A third of the submissions was able to beat the
baseline. Among these FBK-UEDIN?s submission
ranked best in terms of MAE, our main metric for
this task, and also achieved the lowest RMSE.
Only three systems were able to beat our base-
line in terms of MAE. Please note that while all
features were available to the participants, our
baseline is actually a competitive system.
The second-best entry, CNGL SVR, reached
the highest Spearman?s rank correlation, our sec-
ondary metric. Furthermore, in terms of this met-
ric all four top-ranking entries, two by CNGL and
FBK-UEDIN respectively, are significantly better
than the baseline (10k bootstrap resampling test
with 95% confidence intervals). As high ranking
submissions also yield strong rank correlation to
the observed post-editing time, we can be confi-
dent that improvements in MAE are not only due
to better handling of extreme cases.
Many participants submitted two variants of
their systems with different numbers of features
and/or machine learning approaches. In Table 18
we can see these are grouped closely together giv-
ing rise to the assumption that the general pool of
available features and thereby the used resources
and strongest features are most relevant for a sys-
tem?s performance. Another hint in that direction
is the observation the top-ranked systems rely on
additional data and resources to generate their fea-
tures.
Task 2 Predicting word-level scores
Results for this task are presented in Table 19 and
20, sorted by macro average F1. Since this is a
new task, we have yet to establish a strong base-
line. For reference we provide a trivial baseline
that predicts the dominant class ? (K)eep ? for ev-
ery token.
The first observation in Table 19 is that this triv-
ial baseline is difficult to beat in terms of accuracy.
However, considering our main metric ? macro-
29
System ID MAE RMSE Pearson?s r Spearman?s ?
? FBK-UEDIN Extra 47.5 82.6 0.65 0.75
? FBK-UEDIN Rand-SVR 47.9 86.7 0.66 0.74
CNGL SVR 49.2 90.4 0.67 0.76
CNGL SVRPLS 49.6 86.6 0.68 0.74
CMU slim 51.6 84.7 0.63 0.68
Baseline bb17 SVR 51.9 93.4 0.61 0.70
DFKI linear6 52.4 84.3 0.64 0.68
CMU full 53.6 92.2 0.58 0.60
DFKI pls8 53.6 88.3 0.59 0.67
TCD-DCU-CNGL SVM2 55.8 98.9 0.47 0.60
TCD-DCU-CNGL SVM1 55.9 99.4 0.48 0.60
SHEF FS 55.9 103.1 0.42 0.61
SHEF FS-AL 64.6 99.1 0.57 0.60
LIMSI elastic 70.6 114.4 0.58 0.64
Table 18: Official results for the Task 1.3 of the WMT13 Quality Estimation shared-task. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test.
Keep Change
System ID Accuracy Prec. Recall F1 Prec. Recall F1 Macro F1
? LIG FS BIN 0.74 0.79 0.86 0.82 0.56 0.43 0.48 0.65
? LIG BOOST BIN 0.74 0.78 0.88 0.83 0.57 0.37 0.45 0.64
CNGL GLM 0.70 0.76 0.86 0.80 0.47 0.31 0.38 0.59
UMAC NB 0.56 0.82 0.49 0.62 0.37 0.73 0.49 0.55
CNGL GLMd 0.71 0.74 0.93 0.82 0.51 0.19 0.28 0.55
UMAC CRF 0.71 0.72 0.98 0.83 0.49 0.04 0.07 0.45
Baseline (one class) 0.71 0.71 1.00 0.83 0.00 0.00 0.00 0.42
Table 19: Official results for Task 2: binary classification on word level of the WMT13 Quality Estimation shared-task. The
winning submissions are indicated by a ?.
System ID F1 Keep F1 Substitute F1 Delete Micro-F1 Macro-F1
? LIG FS MULT 0.83 0.44 0.072 0.72 0.45
? LIG ALL MULT 0.83 0.45 0.064 0.72 0.45
UMAC NB 0.62 0.43 0.042 0.52 0.36
CNGL GLM 0.83 0.18 0.028 0.71 0.35
CNGL GLMd 0.83 0.14 0.034 0.72 0.34
UMAC CRF 0.83 0.04 0.012 0.71 0.29
Baseline (one class) 0.83 0.00 0.000 0.71 0.28
Table 20: Official results for Task 2: multiclass classification on word level of the WMT13 Quality Estimation shared-task.
The winning submissions are indicated by a ?.
30
average F1 ? it is clear that all systems outperform
the baseline. The winning systems by LIG for the
binary task are also the top ranking systems on the
multiclass task.
While promising results are found for the bi-
nary variant of the task where systems are able to
achieve an F1 of almost 0.5 for the relevant class
? Change, the multiclass prediction variant of the
task seem to suffer from its severe class imbalance.
In fact, none of the systems shows good perfor-
mance when predicting deletions.
6.9 Discussion
In what follows, we discuss the main accomplish-
ments of this shared task starting from the goals
we had previously identified for it.
Explore various granularity levels for the
quality-prediction task The decision on which
level of granularity quality estimation is applied
depends strongly on the intended application. In
Task 2 we tested binary word-level classification
in a post-editing setting. If such annotation is pre-
sented through a user interface we imagine that
words marked as incorrect would be hidden from
the editor, highlighted as possibly wrong or that a
list of alternatives would we generated.
With respect to the poor improvements over
trivial baselines, we consider that the results for
word-level prediction could be mostly connected
to limitations of the datasets provided, which are
very small for word-level prediction, as compared
to successful previous work such as (Bach et al,
2011). Despite the limited amount of training
data, several systems were able to predict dubious
words (binary variant of the task), showing that
this can be a promising task. Extending the granu-
larity even further by predicting the actual editing
action necessary for a word yielded less positive
results than the binary setting.
We cannot directly compare sentence- and
word-level results. However, since sentence-level
predictions can benefit from more information
available and therefore more signal on which the
prediction is based, the natural conclusion is that,
if there is a choice in the prediction granularity,
to opt for the coarser one possible (i.e., sentence-
level over word-level). But certain applications
may require finer granularity levels, and therefore
word-level predictions can still be very valuable.
Explore the prediction of more objective scores
Given the multitude of possible applications for
quality estimation we must decide which predicted
values are both useful and accurate. In this year?s
task we have attempted to address the useful-
ness criterion by moving from the subjective, hu-
man judgement-based scores, to the prediction of
scores that can be more easily interpreted for prac-
tical applications: post-editing distance or types of
edits (word-level), post-editing time, and ranking
of alternative translations.
The general promise of using objective scores is
that predicting a value that is related to the use case
will make quality estimation more applicable and
yield lower deviance compared to the use of proxy
metrics. The magnitude of this benefit should be
sufficient to account for the possible additional ef-
fort related to collecting such scores.
While a direct comparison between the differ-
ent types of scores used for this year?s tasks is not
possible as they are based on different datasets, if
we compare last year?s task on predicting 1-5 lik-
ert scores (and generating an overall ranking of all
translations in the test set) with this year?s Task
1.1, which is virtually the same, but using post-
editing distance as gold-label, we see that the num-
ber of systems that outperform the baseline 18 is
proportionally larger this year. We can also notice
a higher relative improvement of these submis-
sions over the baseline system. While this could
simply be a consequence of progress in the field, it
may also provide an indication that objective met-
rics are more suitable for the problem.
Particularly with respect to post-editing time,
given that this label has a long tailed distribution
and is not trivial to measure even in a controlled
environment, the results of Task 1.3 are encour-
aging. Comparison with the better results seen
on Tasks 1.1 and 1.2, however, suggests that, for
Task 1.3, additional data processing, filtering, and
modelling (including modelling translator-specific
traits such as their variance in time) is required, as
evidenced in (Cohn and Specia, 2013).
Explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics When it comes to the task of au-
tomatically ranking alternative translations gener-
ated by different MT systems, the traditional use
of reference-based MT evaluation metrics is chal-
lenged by the findings of this task.
The top ranking quality estimation submissions
18The two baselines are exactly the same, and therefore the
comparison is meaningful.
31
to Task 1.2 have performances that outperform or
are at least at the same level with the ones that
involve the use of human references. The most in-
teresting property of these techniques is that, be-
ing reference-free, they can be used for any source
sentences, and therefore are ready to be deployed
for arbitrary texts.
An immediate application for this capability is
a procedure by which MT system-selection is per-
formed, based on the output of such quality esti-
mators. Additional measurements are needed to
determine the level of improvement in translation
quality that the current performance of these tech-
niques can achieve in a system-selection scenario.
Identify new and effective quality indicators
Quality indicators, or features, are core to the
problem of quality estimation. One significant dif-
ference this year with respect to previous year was
the availability of QUEST, a framework for the ex-
traction of a large number of features. A few sub-
missions used these larger sets ? as opposed to the
17 baseline features used in the 2012 edition ? as
their starting point, to which they added other fea-
tures. Most features available in this framework,
however, had already been used in previous work.
Novel families of features used this year which
seems to have played an important role are those
proposed by CNGL. They include a number of
language and MT-system independent monolin-
gual and bilingual similarity metrics between the
sentences for prediction and corpora of the lan-
guage pair under consideration. Based on standard
regression algorithm (the same used by the base-
line system), the submissions from CNGL using
such feature families topped many of the tasks.
Another interesting family of features is that
used by TCD-CNGL and TCD-DCU-CNGL for
Tasks 1.1 and 1.3. These were borrowed from
work on style or authorship identification. The as-
sumption is that low/high quality translations can
be characterised by some patterns which are fre-
quent and/or differ significantly from patterns be-
longing to the opposite category.
Like in last year?s task, the vast majority of
the participating systems used external resources
in addition to those provided for the task, par-
ticularly for linguistically-oriented features, such
as parsers, part-of-speech taggers, named entity
recognizers, etc. A novel set of syntactic fea-
tures based on Combinatory Categorial Grammar
(CCG) performed reasonably well in Task 1.2:
with six CCG-based features and no additional
features, the system outperformed the baseline
system and also a second submission where the
17 baseline features were added. This highlights
the potential of linguistically-motivated features
for the problem.
As expected, different feature sets were used
for different tasks. This is essential for Task 2,
where word-level features are certainly necessary.
For example, LIG used a number of lexical fea-
tures such as part-of-speech tag, word-posterior
probabilities, syntactic (constituent label, distance
to the constituent tree root, and target and source
polysemy count). For submissions where a se-
quence labelling algorithm such as a Conditional
Random Fields was used for prediction, the inter-
dependencies between adjacent words and labels
was also modelled though features.
Pseudo-references, i.e., scores from standard
evaluation metrics such as BLEU based on trans-
lations generated by an alternative MT system as
?reference?, featured in more than half of the sub-
missions for sentence-level tasks. This is not sur-
prising given their performance in previous work
on quality estimation.
Identify effective machine learning techniques
for all variants of the quality estimation task
For the sentence-level tasks, standard regression
methods such as SVR performed well as in the
previous edition of the shared task, topping the
results for the ranking variant of Task 1.1, both
first and second place. In fact this algorithm was
used by most submissions that outperformed the
baseline. An alternative algorithm to SVR with
very promising results and which was introduced
for the problem this year is that of Gaussian Pro-
cesses. It was used by SHEF, the winning submis-
sion in the scoring variant of Task 1.1, which also
performed well in the ranking variant, despite its
hyperparameters having been optimised for scor-
ing only. Algorithms behave similarly for Task
1.3, with SVR performing particularly well.
For Task 1.2, logistic regression performed the
best or among the best, along with SVR. One of
the most effective approach for this task, however,
appears to be one that is better tailored for the
task, namely pair-wise decomposition for ranking.
This approach benefits from transforming a k-way
ranking problem into a series of simpler, 2-way
ranking problems, which can be more accurately
solved. Another approach that shows promise is
32
that of ensemble of regressors, in which the output
is the results combining the predictions of differ-
ent regression models.
Linear-chain Conditional Random Fields are a
popular model of choice for sequence labelling
tasks and have been successfully used by several
participants in Task 2, along with discriminatively
trained Hidden Markov Models and Na??ve Bayes.
As in the previous edition, feature engineer-
ing and feature selection prior to model learning
were important components in many submissions.
However, the role of individual features is hard
to judge separately from the role of the machine
learning techniques employed.
Establish the state of the art performance All
four tasks addressed in this shared task have
achieved a dual role that is important for the re-
search community: (i) to make publicly available
new data sets that can serve to compare different
approaches and contributions; and (ii) to estab-
lish the present state-of-the-art performance in the
field, so that progress can be easily measured and
tracked. In addition, the public availability of the
scoring scripts makes evaluation and direct com-
parison straightforward.
Many participants submitted predictions for
several tasks. Comparison of the results shows
that there is little overlap between the best sys-
tems when the predicted value is varied. While
we did not formally require the participants to use
similar systems across tasks, these results indicate
that specialised systems with features selected de-
pending on the predicted variable can in fact be
beneficial.
As we mentioned before, compared to the pre-
vious edition of the task, we noticed (for Task
1.1) a larger relative improvement of scores over
the baseline system, as well as a larger propor-
tion of systems outperforming the baseline sys-
tems, which are a good indication that the field is
progressing over the years. For example, in the
scoring variant of Task 1.1, last year only 5 out of
20 systems (i.e. 25% of the systems) were able to
significantly outperform the baseline. This year, 9
out 16 systems (i.e. 56%) outperformed the same
baseline. Last year, the relative improvement of
the winning submission with respect to the base-
line system was 13%, while this year the relative
improvement is of 19%.
Overall, the tables of results presented in Sec-
tion 6.8 give a comprehensive view of the current
state-of-the-art on the data sets used for this shared
task, as well as indications on how much room
there still is for improvement via figures from ora-
cle methods. As a result, people interested in con-
tributing to research in these machine translation
quality estimation tasks will be able to do so in a
principled way, with clearly established state-of-
the-art levels and straightforward means of com-
parison.
7 Summary
As in previous incarnations of this workshop we
carried out an extensive manual and automatic
evaluation of machine translation performance,
and we used the human judgements that we col-
lected to validate automatic metrics of translation
quality. We also refined last year?s quality estima-
tion task, asking for methods that predict sentence-
level post-editing effort and time, rank translations
from alternative systems, and pinpoint words in
the output that are more likely to be wrong.
As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.19
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Google, Microsoft and Yandex.
We would also like to thank our colleagues Ma-
tous? Macha?c?ek and Martin Popel for detailed dis-
cussions.
References
Allauzen, A., Pe?cheux, N., Do, Q. K., Dinarelli,
M., Lavergne, T., Max, A., Le, H.-S., and Yvon,
F. (2013). LIMSI @ WMT13. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 60?67, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Almaghout, H. and Specia, L. (2013). A CCG-
based Quality Estimation Metric for Statistical
Machine Translation. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Ammar, W., Chahuneau, V., Denkowski, M., Han-
neman, G., Ling, W., Matthews, A., Murray,
19http://statmt.org/wmt13/results.html
33
K., Segall, N., Lavie, A., and Dyer, C. (2013).
The CMU machine translation systems at WMT
2013: Syntax, synthetic translation options, and
pseudo-references. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 68?75, Sofia, Bulgaria. Association for
Computational Linguistics.
Avramidis, E. (2012). Comparative Quality Es-
timation: Automatic Sentence-Level Ranking
of Multiple Machine Translation Outputs. In
Proceedings of 24th International Conference
on Computational Linguistics, pages 115?132,
Mumbai, India.
Avramidis, E. and Popovic, M. (2013). Selecting
feature sets for comparative and time-oriented
quality estimation of machine translation out-
put. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 327?
334, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Avramidis, E., Popovic?, M., Vilar, D., and Bur-
chardt, A. (2011). Evaluate with confidence es-
timation: Machine ranking of translation out-
puts using grammatical features. In Proceed-
ings of the Sixth Workshop on Statistical Ma-
chine Translation.
Aziz, W., Mitkov, R., and Specia, L. (2013).
Ranking Machine Translation Systems via Post-
Editing. In Proc. of Text, Speech and Dialogue
(TSD), Lecture Notes in Artificial Intelligence,
Berlin / Heidelberg. Za?padoc?eska? univerzita v
Plzni, Springer Verlag.
Bach, N., Huang, F., and Al-Onaizan, Y. (2011).
Goodness: A method for measuring machine
translation confidence. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 211?219, Portland, Ore-
gon, USA.
Beck, D., Shah, K., Cohn, T., and Specia, L.
(2013). SHEF-Lite: When less is more for
translation quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 335?340, Sofia, Bulgaria.
Association for Computational Linguistics.
Bic?ici, E., Groves, D., and van Genabith, J. (2013).
Predicting sentence translation quality using ex-
trinsic and language independent features. Ma-
chine Translation.
Bic?ici, E. and van Genabith, J. (2013). CNGL-
CORE: Referential translation machines for
measuring semantic similarity. In *SEM 2013:
The Second Joint Conference on Lexical and
Computational Semantics, Atlanta, Georgia,
USA. Association for Computational Linguis-
tics.
Bicici, E. (2013a). Feature decay algorithms
for fast deployment of accurate statistical ma-
chine translation systems. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 76?82, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Bicici, E. (2013b). Referential translation ma-
chines for quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 341?349, Sofia, Bulgaria.
Association for Computational Linguistics.
B??lek, K. and Zeman, D. (2013). CUni multilin-
gual matrix in the WMT 2013 shared task. In
Proceedings of the Eighth Workshop on Statis-
tical Machine Translation, pages 83?89, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Bojar, O., Kos, K., and Marec?ek, D. (2010). Tack-
ling Sparse Data Issue in Machine Translation
Evaluation. In Proceedings of the ACL 2010
Conference Short Papers, pages 86?91, Upp-
sala, Sweden. Association for Computational
Linguistics.
Bojar, O., Rosa, R., and Tamchyna, A. (2013).
Chimera ? three heads for English-to-Czech
translation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
90?96, Sofia, Bulgaria. Association for Compu-
tational Linguistics.
Borisov, A., Dlougach, J., and Galinskaya, I.
(2013). Yandex school of data analysis ma-
chine translation systems for WMT13. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, pages 97?101, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
34
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montre?al, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Buck, C., Turchi, M.,
and Negri, M. (2013). FBK-UEdin participa-
tion to the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 350?
356, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Charniak, E. and Johnson, M. (2005). Coarse-to-
fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual
Meeting on Association for Computational Lin-
guistics, pages 173?180. Association for Com-
putational Linguistics.
Cho, E., Ha, T.-L., Mediani, M., Niehues, J., Her-
rmann, T., Slawik, I., and Waibel, A. (2013).
The Karlsruhe Institute of Technology transla-
tion systems for the WMT 2013. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 102?106, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling Anno-
tator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality
Estimation. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (to appear).
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
de Marneffe, M.-C., MacCartney, B., and Man-
ning, C. D. (2006). Generating typed depen-
dency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Denkowski, M. and Lavie, A. (2010). Meteor-next
and the meteor paraphrase tables: improved
evaluation support for five target languages. In
Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR,
WMT ?10, pages 339?342, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.
Durgar El-Kahlout, I. and Mermer, C. (2013).
TU?btak-blgem german-english machine trans-
lation systems for w13. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 107?111, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Durrani, N., Fraser, A., Schmid, H., Sajjad, H.,
and Farkas, R. (2013a). Munich-Edinburgh-
Stuttgart submissions of OSM systems at
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
120?125, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Durrani, N., Haddow, B., Heafield, K., and Koehn,
P. (2013b). Edinburgh?s machine translation
systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 112?119, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
35
Eidelman, V., Wu, K., Ture, F., Resnik, P., and Lin,
J. (2013). Towards efficient large-scale feature-
rich statistical machine translation. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 126?131, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Fleiss, J. L. (1971). Measuring nominal scale
agreement among many raters. Psychological
Bulletin, 76(5):378?382.
Formiga, L., Costa-jussa`, M. R., Marin?o, J. B.,
Fonollosa, J. A. R., Barro?n-Ceden?o, A., and
Marquez, L. (2013a). The TALP-UPC phrase-
based translation systems for WMT13: System
combination with morphology generation, do-
main adaptation and corpus filtering. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 132?138, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Formiga, L., Gonza`lez, M., Barro?n-Ceden?o, A.,
Fonollosa, J. A. R., and Marquez, L. (2013b).
The TALP-UPC approach to system selection:
Asiya features and pairwise classification using
random forests. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 357?362, Sofia, Bulgaria. Association for
Computational Linguistics.
Formiga, L., Ma`rquez, L., and Pujantell, J.
(2013c). Real-life translation quality estimation
for mt system selection. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Galus?c?a?kova?, P., Popel, M., and Bojar, O. (2013).
PhraseFix: Statistical post-editing of TectoMT.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 139?145,
Sofia, Bulgaria. Association for Computational
Linguistics.
Gispert, A., Blackwood, G., Iglesias, G., and
Byrne, W. (2013). N-gram posterior probabil-
ity confidence measures for statistical machine
translation: an empirical study. Machine Trans-
lation, 27:85?114.
Gonza`lez, M., Gime?nez, J., and Ma`rquez, L.
(2012). A graphical interface for mt evaluation
and error analysis. In Proceedings of the ACL
2012 System Demonstrations, pages 139?144,
Jeju Island, Korea.
Green, S., Cer, D., Reschke, K., Voigt, R., Bauer,
J., Wang, S., Silveira, N., Neidert, J., and Man-
ning, C. D. (2013). Feature-rich phrase-based
translation: Stanford University?s submission to
the WMT 2013 translation task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 146?151, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hall, M., Frank, E., Holmes, G., Pfahringer,
B., Reutemann, P., and Witten, I. H. (2009).
The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Han, A. L.-F., Wong, D. F., Chao, L. S., Lu, Y., He,
L., Wang, Y., and Zhou, J. (2013). A descrip-
tion of tunable machine translation evaluation
systems in WMT13 metrics task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 412?419, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hildebrand, S. and Vogel, S. (2013). MT quality
estimation: The CMU system for WMT?13. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 371?377, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Hosmer, D. (1989). Applied logistic regression.
Wiley, New York, 8th edition.
Huet, S., Manishina, E., and Lefe`vre, F.
(2013). Factored machine translation systems
for Russian-English. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 152?155, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Hunt, E., Martin, J., and Stone, P. (1966). Experi-
ments in Induction. Academic Press, New York.
Kaplan, R., Riezler, S., King, T., Maxwell, J.,
Vasserman, A., and Crouch, R. (2004). Speed
and accuracy in shallow and deep stochastic
parsing. In Proceedings of the Human Lan-
guage Technology Conference and the 4th An-
nual Meeting of the North American Chapter of
the Association for Computational Linguistics
(HLT/NAACL 04).
36
Kauchak, D. and Barzilay, R. (2006). Paraphras-
ing for automatic evaluation. In Proceedings
of the main conference on Human Language
Technology Conference of the North American
Chapter of the Association of Computational
Linguistics, HLT-NAACL ?06, pages 455?462,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Koehn, P. (2012). Simulating human judgment in
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Kononenko, I. (1994). Estimating attributes: anal-
ysis and extensions of RELIEF. In Proceedings
of the European conference on machine learn-
ing on Machine Learning, pages 171?182, Se-
caucus, NJ, USA. Springer-Verlag New York,
Inc.
Kos, K. and Bojar, O. (2009). Evaluation of Ma-
chine Translation Metrics for Czech as the Tar-
get Language. Prague Bulletin of Mathematical
Linguistics, 92:135?147.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Langlois, D., Raybaud, S., and Sma??li, K. (2012).
Loria system for the wmt12 quality estimation
shared task. In Proceedings of the Seventh
Workshop on Statistical Machine Translation,
pages 114?119, Montre?al, Canada.
Langlois, D. and Smaili, K. (2013). LORIA sys-
tem for the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 378?
383, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Le, H. S., Oparin, I., Allauzen, A., Gauvain, J.-L.,
and Yvon, F. (2011). Structured output layer
neural network language model. In ICASSP,
pages 5524?5527.
Lin, C.-J., Weng, R. C., and Keerthi, S. S. (2007).
Trust region Newton methods for large-scale lo-
gistic regression. In Proceedings of the 24th
international conference on Machine learning
- ICML ?07, pages 561?568, New York, New
York, USA. ACM Press.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Macha?c?ek, M. and Bojar, O. (2013). Results of the
WMT13 metrics shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 43?49, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Matusov, E. and Leusch, G. (2013). Omnifluent
English-to-French and Russian-to-English sys-
tems for the 2013 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 156?161, Sofia, Bulgaria. Association for
Computational Linguistics.
McCallum, A. K. (2002). MALLET: a machine
learning for language toolkit.
Miceli Barone, A. V. and Attardi, G. (2013).
Pre-reordering for machine translation using
transition-based walks on dependency parse
trees. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 162?
167, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Moreau, E. and Rubino, R. (2013). An approach
using style classification features for quality es-
timation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
427?432, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Nadejde, M., Williams, P., and Koehn, P. (2013).
Edinburgh?s syntax-based machine translation
systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
168?174, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Okita, T., Liu, Q., and van Genabith, J. (2013).
Shallow semantically-informed PBSMT and
HPBSMT. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
175?182, Sofia, Bulgaria. Association for Com-
putational Linguistics.
O?zgu?r, A., O?zgu?r, L., and Gu?ngo?r, T. (2005). Text
37
categorization with class-based and corpus-
based keyword selection. In Proceedings of
the 20th International Conference on Computer
and Information Sciences, ISCIS?05, pages
606?615, Berlin, Heidelberg. Springer.
Peitz, S., Mansour, S., Huck, M., Freitag, M.,
Ney, H., Cho, E., Herrmann, T., Mediani,
M., Niehues, J., Waibel, A., Allauzen, A.,
Khanh Do, Q., Buschbeck, B., and Wand-
macher, T. (2013a). Joint WMT 2013 submis-
sion of the QUAERO project. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 183?190, Sofia, Bulgaria.
Association for Computational Linguistics.
Peitz, S., Mansour, S., Peter, J.-T., Schmidt, C.,
Wuebker, J., Huck, M., Freitag, M., and Ney,
H. (2013b). The RWTH aachen machine trans-
lation system for WMT 2013. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 191?197, Sofia, Bulgaria.
Association for Computational Linguistics.
Pighin, D., Formiga, L., and Ma`rquez, L.
(2012). A graph-based strategy to streamline
translation quality assessments. In Proceed-
ings of the Tenth Conference of the Associa-
tion for Machine Translation in the Americas
(AMTA?2012), San Diego, USA.
Pino, J., Waite, A., Xiao, T., de Gispert, A., Flego,
F., and Byrne, W. (2013). The University of
Cambridge Russian-English system at WMT13.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 198?203,
Sofia, Bulgaria. Association for Computational
Linguistics.
Post, M., Ganitkevitch, J., Orland, L., Weese, J.,
Cao, Y., and Callison-Burch, C. (2013). Joshua
5.0: Sparser, better, faster, server. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 204?210, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Rubino, R., Toral, A., Corte?s Va??llo, S., Xie, J.,
Wu, X., Doherty, S., and Liu, Q. (2013a). The
CNGL-DCU-Prompsit translation systems for
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
211?216, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Rubino, R., Wagner, J., Foster, J., Roturier, J.,
Samad Zadeh Kaljahi, R., and Hollowood, F.
(2013b). DCU-Symantec at the WMT 2013
quality estimation shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 390?395, Sofia, Bulgaria.
Association for Computational Linguistics.
Sajjad, H., Smekalova, S., Durrani, N., Fraser, A.,
and Schmid, H. (2013). QCRI-MES submis-
sion at WMT13: Using transliteration mining
to improve statistical machine translation. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 217?222, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Schmidt, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods
in Natural Language Processing.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An In-
vestigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings
of MT Summit XIV (to appear), Nice, France.
Singh, A. K., Wisniewski, G., and Yvon, F.
(2013). LIMSI submission for the WMT?13
quality estimation task: an experiment with n-
gram posteriors. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 396?402, Sofia, Bulgaria. Association for
Computational Linguistics.
Smith, J., Saint-Amand, H., Plamada, M., Koehn,
P., Callison-Burch, C., and Lopez, A. (2013).
Dirt cheap web-scale parallel text from the
Common Crawl. In Proceedings of the 2013
Conference of the Association for Computa-
tional Linguistics (ACL 2013), Sofia, Bulgaria.
Association for Computational Linguistics.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Snover, M., Madnani, N., Dorr, B. J., and
Schwartz, R. (2009). Fluency, adequacy, or
hter?: exploring different human judgments
with a tunable mt metric. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
38
lation, StatMT ?09, pages 259?268, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Soricut, R., Bach, N., and Wang, Z. (2012). The
SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceed-
ings of the 7th Workshop on Statistical Machine
Translation, pages 145?151.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L. (2011). Exploiting Objective Annota-
tions for Measuring Translation Post-editing Ef-
fort. In Proceedings of the 15th Conference of
the European Association for Machine Transla-
tion, pages 73?80, Leuven.
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Stolcke, A. (2002). SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the
7th International Conference on Spoken Lan-
guage Processing (ICSLP 2002), pages 901?
904.
Stone, M. and Brooks, R. J. (1990). Contin-
uum regression: cross-validated sequentially
constructed prediction embracing ordinary least
squares, partial least squares and principal com-
ponents regression. Journal of the Royal
Statistical Society Series B Methodological,
52(2):237?269.
Stymne, S., Hardmeier, C., Tiedemann, J., and
Nivre, J. (2013). Tunable distortion limits and
corpus cleaning for SMT. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 223?229, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Tantug, A. C., Oflazer, K., and El-Kahlout, I. D.
(2008). BLEU+: a Tool for Fine-Grained BLEU
Computation. In (ELRA), E. L. R. A., edi-
tor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08),
Marrakech, Morocco.
Weller, M., Kisselew, M., Smekalova, S., Fraser,
A., Schmid, H., Durrani, N., Sajjad, H., and
Farkas, R. (2013). Munich-Edinburgh-Stuttgart
submissions at WMT13: Morphological and
syntactic processing for SMT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 230?237, Sofia, Bulgaria.
Association for Computational Linguistics.
39
A Pairwise System Comparisons by Human Judges
Tables 21?30 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according bootstrap resampling (p ?
0.05). Gray lines separate clusters based on non-overlapping rank ranges.
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
M
ES
UE
DI
N
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
CU
-ZE
M
AN
CU
-TA
M
CH
YN
A
DC
U-
FD
A
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .50 .48? .43? .47? .43? .44? .38? .32? .25? .26?
ONLINE-B .50 ? .46? .48? .47? .49 .44? .40? .39? .29? .27?
MES .52? .54? ? .49 .47? .44? .45? .42? .41? .27? .25?
UEDIN .57? .52? .51 ? .51 .48? .47? .42? .39? .28? .25?
ONLINE-A .53? .53? .53? .49 ? .48 .51 .44? .42? .31? .30?
UEDIN-SYNTAX .57? .51 .56? .52? .52 ? .51 .43? .41? .29? .26?
CU-ZEMAN .56? .56? .55? .53? .49 .49 ? .45? .42? .32? .29?
CU-TAMCHYNA .62? .60? .58? .58? .56? .57? .55? ? .46? .35? .32?
DCU-FDA .68? .61? .59? .61? .58? .59? .58? .54? ? .32? .32?
JHU .75? .71? .73? .72? .69? .71? .68? .65? .68? ? .46?
SHEF-WPROA .74? .73? .75? .75? .70? .74? .71? .68? .68? .54? ?
score .60 .58 .57 .56 .54 .54 .53 .48 .45 .32 .29
rank 1 2-3 2-4 3-5 4-7 5-7 6-7 8 9 10 11
Table 21: Head to head comparison, ignoring ties, for Czech-English systems
CU
-B
OJ
AR
CU
-D
EP
FI
X
ON
LI
NE
-B
UE
DI
N
CU
-ZE
M
AN
M
ES
ON
LI
NE
-A
CU
-PH
RA
SE
FI
X
CU
-TE
CT
OM
T
CO
M
M
ER
CI
AL
-1
CO
M
M
ER
CI
AL
-2
SH
EF
-W
PR
OA
CU-BOJAR ? .51 .47? .44? .42? .43? .48 .41? .37? .39? .38? .33?
CU-DEPFIX .49 ? .48? .42? .43? .41? .47? .42? .40? .40? .39? .34?
ONLINE-B .53? .52? ? .47? .44? .44? .44? .44? .44? .41? .36? .34?
UEDIN .56? .58? .53? ? .47? .47? .48 .45? .44? .42? .43? .38?
CU-ZEMAN .58? .57? .56? .53? ? .49 .49 .48? .46? .47? .47? .35?
MES .57? .59? .56? .53? .51 ? .50 .47? .46? .43? .44? .42?
ONLINE-A .52 .53? .56? .52 .51 .50 ? .52 .47? .47? .47? .46?
CU-PHRASEFIX .59? .58? .56? .55? .52? .53? .48 ? .49 .48? .49 .42?
CU-TECTOMT .63? .60? .56? .56? .54? .54? .53? .51 ? .46? .46? .40?
COMMERCIAL-1 .61? .60? .59? .58? .53? .57? .53? .52? .54? ? .49 .42?
COMMERCIAL-2 .62? .61? .64? .57? .53? .56? .53? .51 .54? .51 ? .43?
SHEF-WPROA .67? .66? .66? .62? .65? .58? .54? .58? .60? .58? .57? ?
score .58 .57 .56 .52 .50 .50 .49 .48 .47 .45 .45 .38
rank 1-2 1-2 3 4 5-7 5-7 5-8 7-9 8-9 10-11 10-11 12
Table 22: Head to head comparison, ignoring ties, for English-Czech systems
40
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
UE
DI
N
QU
AE
RO
KI
T
M
ES
RW
TH
-JA
NE
M
ES
-SZ
EG
ED
-R
EO
RD
ER
-SP
LI
T
LI
M
SI
-N
CO
DE
-SO
UL
TU
BI
TA
K
UM
D
DC
U
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
DE
SR
T
ONLINE-B ? .48 .44? .37? .44? .41? .42? .40? .35? .37? .32? .31? .31? .27? .23? .18? .16?
ONLINE-A .52 ? .47 .45? .47 .43? .42? .41? .44? .40? .35? .36? .34? .31? .27? .25? .21?
UEDIN-SYNTAX .56? .53 ? .48 .46? .48? .46? .46? .45? .45? .35? .35? .34? .28? .25? .20? .19?
UEDIN .63? .55? .52 ? .51 .46? .47? .49 .44? .43? .39? .34? .35? .32? .28? .24? .22?
QUAERO .56? .53 .54? .49 ? .49 .52 .44? .46? .44? .39? .38? .37? .30? .31? .25? .21?
KIT .59? .57? .52? .54? .51 ? .45? .51 .43? .46? .37? .38? .41? .35? .31? .25? .21?
MES .58? .58? .54? .53? .48 .55? ? .49 .49 .46? .44? .37? .40? .34? .30? .26? .20?
RWTH-JANE .60? .59? .54? .51 .56? .49 .51 ? .46? .50 .45? .46? .47? .38? .33? .28? .20?
MES-SZEGED-REORDER-SPLIT .65? .56? .55? .56? .54? .57? .51 .54? ? .53? .44? .41? .41? .36? .34? .31? .21?
LIMSI-NCODE-SOUL .63? .60? .55? .57? .56? .54? .54? .50 .47? ? .51 .45? .43? .37? .34? .30? .22?
TUBITAK .68? .65? .65? .61? .61? .63? .56? .55? .56? .49 ? .48? .49 .39? .41? .30? .25?
UMD .69? .64? .65? .66? .62? .62? .63? .54? .59? .55? .52? ? .48? .41? .40? .33? .27?
DCU .69? .66? .66? .65? .63? .59? .60? .53? .59? .57? .51 .52? ? .41? .38? .37? .25?
CU-ZEMAN .73? .69? .72? .68? .70? .65? .66? .62? .64? .63? .61? .59? .59? ? .44? .43? .29?
JHU .77? .73? .75? .72? .69? .69? .70? .67? .66? .66? .59? .60? .62? .56? ? .43? .30?
SHEF-WPROA .82? .75? .80? .76? .75? .75? .74? .72? .69? .70? .70? .67? .63? .57? .57? ? .41?
DESRT .84? .79? .81? .78? .79? .79? .80? .80? .79? .78? .75? .73? .75? .71? .70? .59? ?
score .66 .62 .60 .58 .58 .57 .56 .54 .53 .52 .48 .46 .46 .39 .36 .31 .23
rank 1 2-3 2-3 4-5 4-5 5-7 6-7 8-9 8-10 9-10 11 12-13 12-13 14 15 16 17
Table 23: Head to head comparison, ignoring ties, for German-English systems
ON
LI
NE
-B
PR
OM
T
UE
DI
N-
SY
NT
AX
ON
LI
NE
-A
UE
DI
N
KI
T
ST
AN
FO
RD
LI
M
SI
-N
CO
DE
-SO
UL
M
ES
-R
EO
RD
ER
JH
U
CU
-ZE
M
AN
TU
BI
TA
K
UU SH
EF
-W
PR
OA
RW
TH
-JA
NE
ONLINE-B ? .55? .50 .45? .45? .34? .37? .37? .37? .32? .32? .33? .24? .21? .26?
PROMT .45? ? .48? .50 .43? .40? .39? .36? .37? .31? .31? .32? .27? .24? .27?
UEDIN-SYNTAX .50 .52? ? .57? .45? .43? .38? .41? .39? .38? .33? .33? .26? .25? .22?
ONLINE-A .55? .50 .43? ? .51 .42? .48 .41? .36? .44? .44? .38? .32? .27? .29?
UEDIN .55? .57? .55? .49 ? .52 .45? .45? .42? .43? .37? .34? .29? .27? .31?
KIT .66? .60? .57? .58? .48 ? .48 .45? .42? .36? .39? .40? .30? .29? .26?
STANFORD .63? .61? .62? .52 .55? .52 ? .50 .44? .48 .44? .43? .34? .29? .32?
LIMSI-NCODE-SOUL .63? .64? .59? .59? .55? .55? .50 ? .44? .44? .44? .47? .40? .34? .33?
MES-REORDER .63? .63? .61? .64? .58? .58? .56? .56? ? .50 .46? .49 .38? .37? .34?
JHU .68? .69? .62? .56? .57? .64? .52 .56? .50 ? .48? .45? .36? .37? .34?
CU-ZEMAN .68? .69? .67? .56? .63? .61? .56? .56? .54? .52? ? .48 .40? .33? .34?
TUBITAK .67? .68? .67? .62? .66? .60? .57? .53? .51 .55? .52 ? .38? .40? .32?
UU .76? .73? .74? .68? .71? .70? .66? .60? .62? .64? .60? .62? ? .44? .46?
SHEF-WPROA .79? .76? .75? .73? .73? .71? .71? .66? .63? .63? .67? .60? .56? ? .47?
RWTH-JANE .74? .73? .78? .71? .69? .74? .68? .67? .66? .66? .66? .68? .54? .53? ?
score .63 .63 .61 .58 .57 .55 .52 .50 .47 .47 .46 .45 .36 .32 .32
rank 1-2 1-2 3 3-5 4-6 5-6 7 8 9-11 9-11 10-12 11-12 13 14-15 14-15
Table 24: Head to head comparison, ignoring ties, for English-German systems
41
UE
DI
N-
HE
AF
IE
LD
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
ON
LI
NE
-A
M
ES
-SI
M
PL
IF
IE
DF
RE
NC
H
DC
U
RW
TH
CM
U-
TR
EE
-TO
-TR
EE
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .45? .46? .46? .42? .42? .34? .34? .29? .33? .31? .28? .24?
UEDIN .55? ? .52? .43? .45? .46? .40? .38? .33? .36? .33? .32? .23?
ONLINE-B .54? .48? ? .49 .46? .44? .45? .40? .38? .34? .36? .31? .26?
LIMSI-NCODE-SOUL .54? .57? .51 ? .52? .47 .45? .42? .38? .36? .34? .31? .28?
KIT .58? .55? .54? .48? ? .47 .46? .44? .39? .38? .37? .33? .28?
ONLINE-A .58? .54? .56? .53 .53 ? .47 .45? .40? .40? .39? .34? .32?
MES-SIMPLIFIEDFRENCH .66? .60? .55? .55? .54? .53 ? .48? .44? .40? .39? .39? .32?
DCU .66? .62? .60? .58? .56? .55? .52? ? .45? .45? .42? .41? .36?
RWTH .71? .67? .62? .62? .61? .60? .56? .55? ? .48? .47? .47? .38?
CMU-TREE-TO-TREE .67? .64? .66? .64? .62? .60? .60? .55? .52? ? .50 .48 .37?
CU-ZEMAN .69? .67? .64? .66? .63? .61? .61? .58? .53? .50 ? .47? .39?
JHU .72? .68? .69? .69? .67? .66? .61? .59? .53? .52 .53? ? .45?
SHEF-WPROA .76? .77? .74? .72? .72? .68? .68? .64? .62? .63? .61? .55? ?
score .63 .60 .59 .57 .56 .54 .51 .48 .43 .42 .42 .38 .32
rank 1 2-3 2-3 4-5 4-5 5-6 7 8 9-10 9-11 10-11 12 13
Table 25: Head to head comparison, ignoring ties, for French-English systems
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
PR
OM
T
ST
AN
FO
RD
M
ES
M
ES
-IN
FL
EC
TI
ON
RW
TH
-PH
RA
SE
-B
AS
ED
-JA
NE
ON
LI
NE
-A
DC
U
CU
-ZE
M
AN
JH
U
OM
NI
FL
UE
NT
IT
S-L
AT
L
IT
S-L
AT
L-P
E
UEDIN ? .49 .47? .48 .50 .44? .41? .40? .47? .39? .41? .35? .29? .30? .27? .24?
ONLINE-B .51 ? .46? .47? .47? .44? .49 .43? .43? .43? .38? .35? .36? .28? .25? .25?
LIMSI-NCODE-SOUL .53? .54? ? .45? .48 .48 .45? .43? .44? .45? .41? .32? .34? .30? .27? .27?
KIT .52 .53? .55? ? .48 .46? .45? .43? .45? .46? .38? .30? .33? .31? .29? .29?
PROMT .50 .53? .52 .52 ? .50 .48 .52? .45? .47 .48? .38? .36? .36? .34? .31?
STANFORD .56? .56? .52 .54? .50 ? .52 .48 .44? .49 .44? .39? .34? .36? .30? .29?
MES .59? .51 .55? .55? .52 .48 ? .52 .51 .45? .45? .36? .37? .34? .29? .29?
MES-INFLECTION .60? .57? .57? .57? .48? .52 .48 ? .54? .51 .46? .37? .35? .31? .33? .31?
RWTH-PHRASE-BASED-JANE .53? .57? .56? .55? .55? .56? .49 .46? ? .53 .49 .38? .36? .34? .35? .31?
ONLINE-A .61? .57? .55? .54? .53 .51 .55? .49 .47 ? .50 .45? .38? .38? .39? .35?
DCU .59? .62? .59? .62? .52? .56? .55? .54? .51 .50 ? .42? .40? .40? .36? .35?
CU-ZEMAN .65? .65? .68? .70? .62? .61? .64? .63? .62? .55? .58? ? .50 .42? .41? .37?
JHU .71? .64? .66? .67? .64? .66? .63? .65? .64? .62? .60? .50 ? .47? .42? .38?
OMNIFLUENT .70? .72? .70? .69? .64? .64? .66? .69? .66? .62? .60? .58? .53? ? .43? .42?
ITS-LATL .73? .75? .72? .71? .66? .70? .71? .67? .65? .61? .64? .59? .58? .57? ? .45?
ITS-LATL-PE .76? .75? .73? .71? .69? .71? .71? .69? .69? .65? .65? .63? .62? .58? .55? ?
score .60 .60 .58 .58 .55 .55 .54 .53 .53 .51 .49 .42 .40 .38 .35 .32
rank 1-2 1-3 2-4 3-4 5-7 5-8 5-8 6-9 7-10 9-11 10-11 12 13 14 15 16
Table 26: Head to head comparison, ignoring ties, for English-French systems
42
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
UE
DI
N
ON
LI
NE
-A
M
ES
LI
M
SI
-N
CO
DE
-SO
UL
DC
U
DC
U-
OK
IT
A
DC
U-
FD
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .49 .42? .45? .43? .40? .34? .43? .37? .34? .31? .15?
ONLINE-B .51 ? .49 .44? .46? .47? .42? .39? .40? .37? .37? .16?
UEDIN .58? .51 ? .55? .50 .47? .43? .42? .39? .39? .35? .14?
ONLINE-A .55? .56? .45? ? .50 .44? .45? .42? .42? .41? .37? .18?
MES .57? .54? .50 .50 ? .47? .45? .41? .41? .40? .38? .15?
LIMSI-NCODE-SOUL .60? .53? .53? .56? .53? ? .46? .45? .44? .43? .38? .18?
DCU .66? .58? .57? .55? .55? .54? ? .44? .47? .42? .41? .16?
DCU-OKITA .57? .61? .58? .58? .59? .55? .56? ? .49 .46? .46? .18?
DCU-FDA .63? .60? .61? .58? .59? .56? .53? .51 ? .48? .43? .18?
CU-ZEMAN .66? .63? .61? .59? .60? .57? .58? .54? .52? ? .43? .18?
JHU .69? .63? .65? .63? .62? .62? .59? .54? .57? .57? ? .22?
SHEF-WPROA .85? .84? .86? .82? .85? .82? .84? .82? .82? .82? .78? ?
score .62 .59 .57 .57 .56 .53 .51 .48 .48 .46 .42 .16
rank 1 2 3-5 3-5 3-5 6 7 8-9 8-9 10 11 12
Table 27: Head to head comparison, ignoring ties, for Spanish-English systems
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N
PR
OM
T
M
ES
TA
LP
-U
PC
LI
M
SI
-N
CO
DE
DC
U
DC
U-
FD
A
DC
U-
OK
IT
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
ONLINE-B ? .49 .45? .43? .38? .35? .34? .35? .37? .34? .33? .32? .23?
ONLINE-A .51 ? .49 .48 .38? .46? .42? .41? .43? .38? .38? .37? .31?
UEDIN .55? .51 ? .49 .46? .45? .43? .42? .36? .38? .38? .38? .26?
PROMT .57? .52 .51 ? .46? .48 .43? .43? .40? .37? .39? .34? .29?
MES .62? .62? .54? .54? ? .46? .44? .44? .41? .40? .43? .36? .32?
TALP-UPC .65? .54? .55? .52 .54? ? .50 .45? .44? .40? .40? .37? .32?
LIMSI-NCODE .66? .58? .57? .57? .56? .50 ? .46? .51 .48 .44? .45? .35?
DCU .65? .59? .58? .57? .56? .55? .54? ? .50 .48 .48 .45? .36?
DCU-FDA .63? .57? .64? .60? .59? .56? .49 .50 ? .53? .49 .42? .32?
DCU-OKITA .66? .62? .62? .63? .60? .60? .52 .52 .47? ? .50 .47? .36?
CU-ZEMAN .67? .62? .62? .61? .57? .60? .56? .52 .51 .50 ? .46? .40?
JHU .68? .63? .62? .66? .64? .63? .55? .55? .58? .53? .54? ? .37?
SHEF-WPROA .77? .69? .74? .71? .68? .68? .65? .64? .68? .64? .60? .63? ?
score .63 .58 .57 .56 .53 .52 .49 .47 .47 .45 .44 .41 .32
rank 1 2-4 2-4 3-4 5-6 5-6 7-8 7-9 8-10 9-11 10-11 12 13
Table 28: Head to head comparison, ignoring ties, for English-Spanish systems
43
ON
LI
NE
-B
CM
U
ON
LI
NE
-A
ON
LI
NE
-G
PR
OM
T
QC
RI
-M
ES
UC
AM
-M
UL
TI
FR
ON
TE
ND
BA
LA
GU
R
M
ES
-Q
CR
I
UE
DI
N
OM
NI
FL
UE
NT
-U
NC
NS
TR
LI
A
OM
NI
FL
UE
NT
-C
NS
TR
UM
D
CU
-K
AR
EL
CO
M
M
ER
CI
AL
-3
UE
DI
N-
SY
NT
AX
JH
U
CU
-ZE
M
AN
ONLINE-B ? .40? .42? .41? .37? .37? .41? .33? .33? .37? .33? .33? .35? .38? .34? .33? .29? .28? .14?
CMU .60? ? .50 .46? .43? .47? .42? .42? .39? .43? .41? .41? .40? .38? .36? .30? .30? .29? .17?
ONLINE-A .58? .50 ? .50 .51 .43? .47? .44? .40? .41? .43? .38? .40? .38? .38? .39? .34? .30? .19?
ONLINE-G .59? .54? .50 ? .55? .50 .51 .48 .42? .41? .44? .43? .46? .40? .44? .36? .34? .33? .19?
PROMT .63? .57? .49 .45? ? .43? .47? .43? .47? .47? .43? .39? .44? .43? .37? .41? .40? .38? .25?
QCRI-MES .63? .53? .57? .50 .57? ? .48 .46? .47? .45? .43? .45? .45? .38? .42? .37? .33? .40? .19?
UCAM-MULTIFRONTEND .59? .58? .53? .49 .53? .52 ? .47? .48 .46? .46? .42? .45? .46? .45? .40? .39? .33? .17?
BALAGUR .67? .58? .56? .52 .57? .54? .53? ? .47? .49 .45? .53? .40? .44? .44? .41? .36? .33? .23?
MES-QCRI .67? .61? .60? .58? .53? .53? .52 .53? ? .49 .47? .47? .43? .43? .44? .38? .42? .39? .17?
UEDIN .63? .57? .59? .59? .53? .55? .54? .51 .51 ? .48 .52 .44? .52 .49 .42? .43? .35? .21?
OMNIFLUENT-UNCNSTR .67? .59? .57? .56? .57? .57? .54? .55? .53? .52 ? .51 .46? .48 .48 .44? .40? .39? .25?
LIA .67? .59? .62? .57? .61? .55? .58? .47? .53? .48 .49 ? .51 .49 .48 .50 .41? .39? .20?
OMNIFLUENT-CNSTR .65? .60? .60? .54? .56? .55? .55? .60? .57? .56? .54? .49 ? .51 .48 .47? .40? .40? .25?
UMD .62? .62? .62? .60? .57? .62? .54? .56? .57? .48 .52 .51 .49 ? .53? .42? .46? .42? .19?
CU-KAREL .66? .64? .62? .56? .63? .58? .55? .56? .56? .51 .52 .52 .52 .47? ? .44? .40? .47? .24?
COMMERCIAL-3 .67? .70? .61? .64? .59? .63? .60? .59? .62? .58? .56? .50 .53? .58? .56? ? .51 .44? .32?
UEDIN-SYNTAX .71? .70? .66? .66? .60? .67? .61? .64? .58? .57? .60? .59? .60? .54? .60? .49 ? .45? .25?
JHU .72? .71? .70? .67? .62? .60? .67? .67? .61? .65? .61? .61? .60? .58? .53? .56? .55? ? .24?
CU-ZEMAN .86? .83? .81? .81? .75? .81? .83? .77? .83? .79? .75? .80? .75? .81? .76? .68? .75? .76? ?
score .65 .60 .58 .56 .56 .55 .54 .52 .51 .50 .49 .49 .48 .48 .47 .43 .41 .39 .21
rank 1 2-3 2-3 4-6 4-6 5-7 5-7 8-9 8-10 9-11 10-12 11-14 12-15 12-15 13-15 16 17 18 19
Table 29: Head to head comparison, ignoring ties, for Russian-English systems
PR
OM
T
ON
LI
NE
-B
CM
U
ON
LI
NE
-G
ON
LI
NE
-A
UE
DI
N
QC
RI
-M
ES
CU
-K
AR
EL
M
ES
-Q
CR
I
JH
U
CO
M
M
ER
CI
AL
-3
LI
A
BA
LA
GU
R
CU
-ZE
M
AN
PROMT ? .44? .39? .47 .46? .36? .37? .37? .32? .35? .28? .30? .32? .24?
ONLINE-B .56? ? .44? .41? .44? .38? .37? .35? .33? .39? .33? .31? .35? .24?
CMU .61? .56? ? .52 .49 .47? .43? .41? .39? .44? .44? .40? .35? .28?
ONLINE-G .53 .59? .48 ? .48 .50 .48 .46 .46? .42? .38? .43? .38? .36?
ONLINE-A .54? .56? .51 .52 ? .47 .49 .49 .48 .44? .38? .40? .40? .34?
UEDIN .64? .62? .53? .50 .53 ? .49 .46? .42? .39? .44? .41? .38? .29?
QCRI-MES .63? .63? .57? .52 .51 .51 ? .48 .45? .44? .42? .39? .40? .29?
CU-KAREL .63? .65? .59? .54 .51 .54? .52 ? .50 .46? .43? .40? .42? .34?
MES-QCRI .68? .67? .61? .54? .52 .58? .55? .50 ? .48? .47? .43? .45? .34?
JHU .65? .61? .56? .58? .56? .61? .56? .54? .52? ? .51 .44? .44? .33?
COMMERCIAL-3 .72? .67? .56? .62? .62? .56? .58? .57? .53? .49 ? .52 .48 .44?
LIA .70? .69? .60? .57? .60? .59? .61? .60? .57? .56? .48 ? .47? .41?
BALAGUR .68? .65? .65? .62? .60? .62? .60? .58? .55? .56? .52 .53? ? .41?
CU-ZEMAN .76? .76? .72? .64? .66? .71? .71? .66? .66? .67? .56? .59? .59? ?
score .64 .62 .55 .54 .53 .53 .52 .49 .47 .46 .43 .42 .41 .33
rank 1 2 3-4 3-6 3-7 4-7 5-7 8 9-10 9-10 11-12 11-13 12-13 14
Table 30: Head to head comparison, ignoring ties, for English-Russian systems
44
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 93?101,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Investigating Connectivity and Consistency Criteria for Phrase Pair
Extraction in Statistical Machine Translation
Spyros Martzoukos, Christophe Costa Flore?ncio and Christof Monz
Intelligent Systems Lab Amsterdam, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{S.Martzoukos, C.Monz}@uva.nl, chriscostafl@gmail.com
Abstract
The consistency method has been estab-
lished as the standard strategy for extract-
ing high quality translation rules in statis-
tical machine translation (SMT). However,
no attention has been drawn to why this
method is successful, other than empiri-
cal evidence. Using concepts from graph
theory, we identify the relation between
consistency and components of graphs that
represent word-aligned sentence pairs. It
can be shown that phrase pairs of interest
to SMT form a sigma-algebra generated
by components of such graphs. This con-
struction is generalized by allowing seg-
mented sentence pairs, which in turn gives
rise to a phrase-based generative model. A
by-product of this model is a derivation of
probability mass functions for random par-
titions. These are realized as cases of con-
strained, biased sampling without replace-
ment and we provide an exact formula for
the probability of a segmentation of a sen-
tence.
1 Introduction
A parallel corpus, i.e., a collection of sentences in
a source and a target language, which are trans-
lations of each other, is a core ingredient of ev-
ery SMT system. It serves the purpose of training
data, i.e., data from which translation rules are ex-
tracted. In its most basic form, SMT does not re-
quire the parallel corpus to be annotated with lin-
guistic information, and human supervision is thus
restricted to the construction of the parallel corpus.
The extraction of translation rules is done by ap-
propriately collecting statistics from the training
data. The pioneering work of (Brown et al, 1993)
identified the minimum assumptions that should
be made in order to extract translation rules and
developed the relevant models that made such ex-
tractions possible.
These models, known as IBMmodels, are based
on standard machine learning techniques. Their
output is a matrix of word alignments for each sen-
tence pair in the training data. These word align-
ments provide the input for later approaches that
construct phrase-level translation rules which may
(Wu, 1997; Yamada and Knight, 2001) or may not
(Och et al, 1999; Marcu and Wong, 2002) rely on
linguistic information.
The method developed in (Och et al, 1999),
known as the consistency method, is a simple yet
effective method that has become the standard way
of extracting (source, target)-pairs of phrases as
translation rules. The development of consistency
has been done entirely on empirical evidence and
it has thus been termed a heuristic.
In this work we show that the method of (Och
et al, 1999) actually encodes a particular type of
structural information induced by the word align-
ment matrices. Moreover, we show that the way in
which statistics are extracted from the associated
phrase pairs is insufficient to describe the underly-
ing structure.
Based on these findings we suggest a phrase-
level model in the spirit of the IBM models. A key
aspect of the model is that it identifies the most
likely partitions, rather than alignment maps, asso-
ciated with appropriately chosen segments of the
training data. For that reason, we provide a gen-
eral construction of probability mass functions for
partitions and, in particular, an exact formula for
the probability of a segmentation of a sentence.
93
2 Definition of Consistency
In this section we provide the definition of consis-
tency, which was introduced in (Och et al, 1999),
refined in (Koehn et al, 2003), and we follow
(Koehn, 2009) in our description. We start with
some preliminary definitions.
Let S = s1...s|S| be a source sentence, i.e., a
string that consists of consecutive source words;
each word si is drawn from a source language vo-
cabulary and i indicates the position of the word
in S. The operation of string extraction from the
words of S is defined as the construction of the
string s = si1 ...sin from the words of S, with
1 ? i1 < ... < in ? |S|. If i1, ..., in are consecu-
tive, which implies that s is a substring of S, then
s is called a source phrase and we write s ? S.
As a shorthand we also write sini1 for the phrase
si1 ...sin . Similar definitions apply to the target
side and we denote by T, tj and t a target sen-
tence, word and phrase respectively.
Let (S = s1s2...s|S|, T = t1t2...t|T |) be a sen-
tence pair and letA denote the |S|?|T |matrix that
encodes the existence/absence of word alignments
in (S, T ) as
A(i, j) =
{
1, if si and tj are aligned
0, otherwise,
(1)
for all i = 1, ..., |S| and j = 1, ..., |T |. Un-
aligned words are allowed. A pair of strings (s =
si1 ...si|s| , t = tj1 ...tj|t|) that is extracted from
(S, T ) is termed consistent with A, if the follow-
ing conditions are satisfied:
1. s ? S and t ? T .
2. ?k ? {1, ..., |s|} such that A(ik, j) = 1, then
j ? {j1, ..., j|t|}.
3. ?l ? {1, ..., |t|} such that A(i, jl) = 1, then
i ? {i1, ..., i|s|}.
4. ?k ? {1, ..., |s|} and ?l ? {1, ..., |t|} such
that A(ik, jl) = 1.
Condition 1 guarantees that (s, t) is a phrase
pair and not just a pair of strings. Condition 2 says
that if a word in s is aligned to one or more words
in T , then all such target words must appear in t.
Condition 3 is the equivalent of Condition 2 for the
target words. Condition 4 guarantees the existence
of at least one word alignment in (s, t).
For a sentence pair (S, T ), the set of all consis-
tent pairs with an alignment matrix A is denoted
by P (S, T ). Figure 1(a) shows an example of a
sentence pair with an alignment matrix together
with all its consistent pairs.
In SMT the extraction of each consistent pair
(s, t) from (S, T ) is followed by a statistic
f(s, t;S, T ). Typically f(s, t;S, T ) counts the oc-
currences of (s, t) in (S, T ). By considering all
sentence pairs in the training data, the translation
probability is constructed as
p(t|s) =
?
(S,T ) f(s, t;S, T )
?
(S,T )
?
t? f(s, t
?;S, T )
, (2)
and similarly for p(s|t). Finally, the entries of the
phrase table consist of all extracted phrase pairs,
their corresponding translation probabilities and
other models which we do not discuss here.
3 Consistency and Components
For a given sentence pair (S, T ) and a fixed word
alignment matrixA, our aim is to show the equiva-
lence between consistency and connectivity prop-
erties of the graph formed by (S, T ) and A. More-
over, we explain that the way in which measure-
ments are performed is not compatible , in princi-
ple, with the underlying structure. We start with
some basic definitions from graph theory (see for
example (Harary, 1969)).
Let G = (V,E) be a graph with vertex set V
and edge set E. Throughout this work, vertices
represent words and edges represent word align-
ments, but the latter will be further generalized in
Section 4. A subgraph H = (V ?, E?) of G is a
graph with V ? ? V , E? ? E and the property
that for each edge in E?, both its endpoints are in
V ?. A path in G is a sequence of edges which con-
nect a sequence of distinct vertices. Two vertices
u, v ? V are called connected if G contains a path
from u to v. G is said to be connected if every pair
of vertices in G is connected.
A connected component, or simply component,
of G is a maximal connected subgraph of G. G
is called bipartite if V can be partitioned in sets
VS and VT , such that every edge in E connects a
vertex in VS to one in VT . The disjoint union of
graphs, or simply union, is an operation on graphs
defined as follows. For n graphs with disjoint ver-
tex sets V1, ..., Vn (and hence disjoint edge sets),
their union is the graph (?ni=1Vi,?
n
i=1Ei).
Consider the graph G whose vertices are the
words of the source and target sentences, and
whose edges are induced by the non-zero entries
94
  
t1 t2 t3 t4 t5 t6 t7s1s2s3s4s5 s1 s2
t1 t2 t3
s3 s4
t4 t5 t6
s5
t7s1 s3t1 t4 t5 s2 s4t2 t3 t6 s5t7,{ }C1= G
s1 s3t1 t4 t5 s2 s4t2 t3 t6 s5t7{ }C2= s1 s3t1 t4 t5 t6 s5t7 s2 s4t2 t3 t6 s2 s4t2 t3 s5t7s1 s3t1 t4 t5
s1 s3t1 t4 t5 s2 s4t2 t3 s5t7{ }C3= s1 s3t1 t4 t5 t6 s5t7s2 s4t2 t3 t6s2 s4t2 t3 s5t7s1 s3t1 t4 t5t6
s1 s3t1 t4 t5 s2 s4t2 t3{ }C 4= t6 s5t7
(s5 , t7) ,(s14 , t15) ,(s5 , t67) ,(s14 , t16) ,(S ,T )
(a)
(b)
P (S ,T )= { }
, ,, , , , ,, , ,
Figure 1: (a) Left: Sentence pair with an alignment matrix. Dots indicate existence of word alignments.
Right: All consistent pairs. (b) The graph representation of the matrix in (a), and the sets generated by
components of the graph. Dark shading indicates consistency.
of the matrix A. There are no edges between
any two source-type vertices nor between any two
target-type vertices. Moreover, the source and tar-
get language vocabularies are assumed to be dis-
joint and thus G is bipartite. The set of all com-
ponents of G is defined as C1 and let k denote its
cardinality, i.e., |C1| = k. From the members of
C1 we further construct sets C2, ..., Ck as follows:
For each i, 2 ? i ? k, any member ofCi is formed
by the union of any i distinct members of C1. In
other words, any member of Ci is a graph with i
components and each such component is a mem-
ber of C1. The cardinality of Ci is clearly
(k
i
)
, for
every i, 1 ? i ? k.
Note that Ck = {G}, since G is the union of
all members of C1. Moreover, observe that C? =
?ki=1Ci is the set of graphs that can be generated
by all possible unions of G?s components. In that
sense
C = {?} ? C? (3)
is the power set of G. Indeed we have |C| = 1 +
?k
i=1
(k
i
)
= 2k as required.1
Figure 1(b) shows the graph G and the associ-
ated sets Ci of (S, T ) and A in Figure 1(a). Note
the bijective correspondence between consistent
1Here we used the fact that for any set X with |X| =
n, the set of all subsets of X , i.e., the power set of X , has
cardinality
Pn
i=0
`n
i
?
= 2n.
pairs and the phrase pairs that can be extracted
from the vertices of the members of the sets Ci.
This is a consequence of consistency Conditions 2
and 3, since they provide the sufficient conditions
for component formation.
In general, if a pair of strings (s, t) satisfies the
consistency Conditions 2 and 3, then it can be ex-
tracted from the vertices of a graph inCi, for some
i. Moreover, if Conditions 1 and 4 are also satis-
fied, i.e., if (s, t) is consistent, then we can write
P (S, T ) =
k?
i=1
{
(SH , TH) : H ? Ci,
SH ? S, TH ? T
}
,
(4)
where SH denotes the extracted string from the
source-type vertices of H , and similarly for TH .
Having established this relationship, when refer-
ring to members of C, we henceforth mean either
consistent pairs or inconsistent pairs. The latter
are pairs (SH , TH) for some H ? C such that at
least either SH 6? S or TH 6? T .
The construction above shows that phrase pairs
of interest to SMT are part of a carefully con-
structed subclass of all possible string pairs that
can be extracted from (S, T ). The power set C
of G gives rise to a small, possibly minimal, set
95
in which consistent and inconsistent pairs can be
measured.1 In other words, since C is (by con-
struction) a sigma-algebra, the pair (C1, C) is a
measurable space. Furthermore, one can construct
a measure space (C1, C, f), with an appropriately
chosen measure f : C ? [0,?).
Is the occurrence-counting measure f of Sec-
tion 2 a good choice? Fix an ordering for Ci, and
let Ci,j denote the jth member of Ci, for all i,
1 ? i ? k. Furthermore, let ?(x, y) = 1, if x = y
and 0, otherwise. We argue by contradiction that
the occurrence-counting measure
f(H) =
?
{H?: H??C, H? is consistent}
?(H,H ?), (5)
fails to form a measure space. Suppose that more
than one component of G is consistent, i.e., sup-
pose that
1 <
k?
j=1
f(C1,j) ? k. (6)
By construction of C, it is guaranteed that
1 = f(G) = f(Ck,1) = f(?
k
j=1 C1,j). (7)
The members of C1 are pairwise disjoint, because
each of them is a component ofG. Thus, since f is
assumed to be a measure, sigma-additivity should
be satisfied, i.e., we must have
f(?kj=1 C1,j) =
k?
j=1
f(C1,j) > 1, (8)
which is a contradiction.
In practice, the deficiency of using eq. 5 as
a statistic could possibly be explained by the
fact that the so-called lexical weights are used as
smoothing.
4 Consistency, Components and
Segmentations
In Section 3 the only relation that was assumed
among source (target) words/vertices was the or-
der of appearance in the source (target) sentence.
As a result, the graph representation G of (S, T )
and A was bipartite. There are several, linguisti-
cally motivated, ways in which a general graph can
be obtained from the bipartite graph G. We ex-
plain that the minimal linguistic structure, namely
1See Appendix for definitions.
sentence segmentations, can provide a generaliza-
tion of the construction introduced in Section 3.
Let X be a finite set of consecutive integers. A
consecutive partition of X is a partition of X such
that each part consists of integers consecutive in
X . A segmentation ? of a source sentence S is a
consecutive partition of {1, ..., |S|}. A part of ?,
i.e., a segment, is intuitively interpreted as a phrase
in S. In the graph representation G of (S, T ) and
A, a segmentation ? of S is realised by the ex-
istence of edges between consecutive source-type
vertices whose labels, i.e., word positions in S, ap-
pear in the same segment of ?. The same argument
holds for a target sentence and its words; a target
segmentation is denoted by ? .
Clearly, there are 2|S|?1 possible ways to seg-
ment S and, given a fixed alignment matrix A,
the number of all possible graphs that can be con-
structed is thus 2|S|+|T |?2. The bipartite graph
of Section 3 is just one possible configuration,
namely the one in which each segment of ? con-
sists of exactly one word, and similarly for ? . We
denote this segmentation pair by (?0, ?0).
We now turn to extracting consistent pairs in
this general setting from all possible segmenta-
tions (?, ?) for a sentence pair (S, T ) and a fixed
alignment matrix A. As in Section 3, we con-
struct graphs G?,? , associated sets C?,?i , for all i,
1 ? i ? k?,? , and C?,? , for all (?, ?). Consistent
pairs are extracted in lieu of eq. 4, i.e.,
P ?,? (S, T ) =
k?,??
i=1
{
(SH , TH) : H ? C
?,?
i ,
SH ? S, TH ? T
}
, (9)
and it is trivial to see that
{(S, T )} ? P ?,? (S, T ) ? P (S, T ), (10)
for all (?, ?). Note that P (S, T ) = P ?0,?0(S, T )
and, depending on the details of A, it is possible
for other pairs (?, ?) to attain equality. Moreover,
each consistent pair in P (S, T ) can be be extracted
from a member of at least one C?,? .
We focus on the sets C?,?1 , i.e., the components
of G?,? , for all (?, ?). In particular, we are inter-
ested in the relation between P (S, T ) and C?,?1 ,
for all (?, ?). Each consistent H ? C?0,?0 can
be converted into a single component by appropri-
ately forming edges between consecutive source-
type vertices and/or between consecutive target-
type vertices. The resulting component will evi-
dently be a member of C?,?1 , for some (?, ?). It
96
is important to note that the conversion of a con-
sistent H ? C?0,?0 into a single component need
not be unique; see Figure 2 for a counterexam-
ple. Since (a) such conversions are possible for
all consistent H ? C?0,?0 and (b) P (S, T ) =
P ?0,?0(S, T ), it can be deduced that all possible
consistent pairs can be traced in the sets C?,?1 , for
all (?, ?). In other words, we have:
P (S, T ) =
?
?,?
{
(SH , TH) : H ? C
?,?
1 ,
SH ? S, TH ? T
}
. (11)
The above equation says that by taking sen-
tence segmentations into account, we can recover
all possible consistent pairs, by inspecting only the
components of the underlying graphs.
It would be interesting to investigate the re-
lation between measure spaces (C?,?1 , C
?,? , f?,? )
and different configurations for A. We leave that
for future work and focus on the advantages pro-
vided by eq. 11.
  
t 1t 2 t 3 t 4s2 s1 s3t 1t 2 t 3 t 4s2 s1 s3 t 1t 2 t 3 t 4s2 s1 s3t 1t 2 t 3 t 4s2 s1 s3 t 1t 2 t 3 t 4s2 s1 s3
Figure 2: A graph with three components (top),
and four possible conversions into a single compo-
nent by forming edges between contiguous words.
5 Towards a phrase-level model that
respects consistency
The aim of this section is to exploit the relation
established in eq. 11 between consistent pairs and
components of segmented sentence pairs. It was
also shown in Section 2 that the computation of the
translation models is inappropriate to describe the
underlying structure. We thus suggest a phrase-
based generative model in the spirit of the IBM
word-based models, which is compatible with the
construction of the previous sections.
5.1 Hidden variables
All definitions from the previous sections are car-
ried over, and we introduce a new quantity that is
associated with components. Let G?,? and C?,?1 ,
for some (?, ?) be as in Section 4, then the set
K is defined as follows: Each member of K is
a pair of (source, target) sets of segments that cor-
responds to the pair of (source, target) vertices of
a consistent member of C?,?1 . In other words, K is
a bisegmentation of a pair of segmented sentences
that respects consistency.
Figure 3 shows three possible ways to con-
struct consistent graphs from (S, T ) = (s41, t
6
1),
? = {{1, 2}, {3}, {4}} ? {x1, x2, x3} and ? =
{{1}, {2, 3, 4}, {5}, {6}} ? {y1, y2, y3, y4}. In
each case the exact alignment information is un-
known and we have:
(a) K =
{ (
{x1}, {y1}
)
,
(
{x2}, {y2}
)
,
(
{x3}, {y3, y4}
) }
.
(b) K =
{ (
{x1, x2}, {y1, y2, y3}
)
,
(
{x3}, {y4}
)}
.
(c) K =
{ (
{x1}, {y3, y4}
)
,
(
{x2, x3}, {y1, y2}
)}
.
  
t1s1 s2 s3t 2 t3 t4 s5t 5 t6
t1 s1 s2 s3t 2 t3 t4t 5 t6s5
t1s1 s2 s3t 2 t3t4 t 5t6 s5
(a)
(b)
(c)
Figure 3: Three possible ways to construct con-
sistent graphs for (s41, t
6
1) and a given segmenta-
tion pair. Exact word alignment information is un-
known.
In the proposed phrase-level generative model
the random variables whose instances are ?, ? and
97
K are hidden variables. As with the IBM mod-
els, they are associated with the positions of words
in a sentence, rather than the words themselves.
Alignment information is implicitly identified via
the consistent bisegmentation K.
Suppose we have a corpus that consists of pairs
of parallel sentences (S, T ), and let fS,T denote
the occurrence count of (S, T ) in the corpus. Also,
let lS = |S| and lT = |T |. The aim is to maximize
the corpus log-likelihood function
` =
?
S,T
fS,T log p?(T |S)
=
?
S,T
fS,T log
?
?,?,K
p?(T, ?, ?,K|S), (12)
where ?, ? and K are hidden variables parameter-
ized by a vector ? of unknown weights, whose val-
ues are to be determined. The expectation max-
imization algorithm (Dempster et al, 1977) sug-
gests that an iterative application of
?n+1 = argmax
?
?
S,T
fS,T
?
?,?,K
p?n(?, ?,K|S, T )?
log p?(T, ?, ?,K|S),
(13)
provides a good approximation for the maximum
value of `. As with the IBM models we seek prob-
ability mass functions (PMFs) of the form
p?(T, ?, ?,K|S) = p?(lT |S)p?(?, ?,K|lT , S)?
p?(T |?, ?,K, lT , S),
(14)
and decompose further as
p?(?, ?,K|lT , S) = p?(?, ? |lT , S)p?(K|?, ?, lT , S)
(15)
A further simplification of p?(?, ? |lT , S) =
p?(?|S)p?(? |lT ) may not be desirable, but will
help us understand the relation between ? and the
PMFs. In particular, we give a formal description
of p?(?|S) and then explain that p?(K|?, ?, lT , S)
and p?(T |?, ?,K, lT , S) can be computed in a
similar way.
5.2 Constrained, biased sampling without
replacement
The probability of a segmentation given a sentence
can be realised in two different ways. We first pro-
vide a descriptive approach which is more intu-
itive, and we use the sentence S = s41 as an ex-
ample whenever necessary. The set of all possi-
ble segments of S is denoted by seg(S) and triv-
ially |seg(S)| = |S|
(
|S| + 1
)
/2. Each segment
x ? seg(S) has a nonnegative weight ?(x|lS) such
that ?
x?seg(S)
?(x|lS) = 1. (16)
Suppose we have an urn that consists of
|seg(S)| weighted balls; each ball corresponds to
a segment of S. We sample without replacement
with the aim of collecting enough balls to form a
segmentation of S. When drawing a ball x we si-
multaneously remove from the urn all other balls
x? such that x ? x? 6= ?. We stop when the urn
is empty. In our example, let the urn contain 10
balls and suppose that the first draw is {1, 2}. In
the next draw, we have to choose from {3}, {4}
and {3, 4} only, since all other balls contain a ?1?
and/or a ?2? and are thus removed. The sequence
of draws that leads to a segmentation is thus a path
in a decision tree. Since ? is a set, there are |?|!
different paths that lead to its formation. The set
of all possible segmentations, in all possible ways
that each segmentation can be formed, is encoded
by the collection of all such decision trees.
The second realisation, which is based on the
notions of cliques and neighborhoods, is more
constructive and will give rise to the desired PMF.
A clique in a graph is a subset U of the vertex set
such that for every two vertices u, v ? U , there ex-
ists an edge connecting u and v. For any vertex u
in a graph, the neighborhood of u is defined as the
set N(u) = {v : {u, v} is an edge}. A maximal
clique is a clique U that is not a subset of a larger
clique: For each u ? U and for each v ? N(u) the
set U ? {v} is not a clique.
Let G be the graph whose vertices are all seg-
ments of S and whose edges satisfy the condition
that any two vertices x and x? form an edge iff
x ? x? = ?; see Figure 4 for an example. G es-
sentially provides a compact representation of the
decision trees discussed above.
It is not difficult to see that a maximal clique
also forms a segmentation. Moreover, the set of all
maximal cliques in G is exactly the set of all pos-
sible segmentations for S. Thus, p?(?|S) should
satisfy
p?(?|S) = 0, if ? is not a clique in G, (17)
and ?
?
p?(?|S) = 1, (18)
98
  
{1}
{2}
{3}
{4}
{253}{154}
{351}{35154} {25351}
{2535154}
Figure 4: The graph whose vertices are the seg-
ments of s41 and whose edges are formed by non-
overlapping vertices.
where the sum is over all maximal cliques in G.
In our example p?
(
{ {1}, {1, 2} }|S
)
= 0, be-
cause there is no edge connecting segments {1}
and {1, 2} so they are not part of any clique.
In order to derive an explicit formula for
p?(?|S) we focus on a particular type of paths
in G. A path is called clique-preserving, if ev-
ery vertex in the path belongs to the same clique.
Our construction should be such that each clique-
preserving path has positive probability of occur-
ring, and all other paths should have probability
0. We proceed with calculating probabilities of
clique-preserving paths based on the structure of
G and the constraint of eq. 16.
The probability p?(?|S) can be viewed as
the probability of generating all clique-preserving
paths on the maximal clique ? in G. Since
? is a clique, there are |?|! possible paths that
span its vertices. Let ? = {x1, ..., x|?|},
and let pi denote a permutation of {1, ..., |?|}.
We are interested in computing the probabil-
ity q?(xpi(1), ..., xpi(|?|)) of generating a clique-
preserving path xpi(1), ..., xpi(|?|) in G. Thus,
p?(?|S) = p?({x1, ..., x|?|}|S)
=
?
pi
q?(xpi(1), ..., xpi(|?|))
=
?
pi
q?(xpi(1)) q?(xpi(2)|xpi(1))? ...
...? q?(xpi(|?|)|xpi(1), ..., xpi(|?|?1)).
(19)
The probabilities q?(?) can be explicitly calcu-
lated by taking into account the following ob-
servation. A clique-preserving path on a clique
? can be realised as a sequence of vertices
xpi(1), ..., xpi(i), ..., xpi(|?|) with the following con-
straint: If at step i ? 1 of the path we are at ver-
tex xpi(i?1), then the next vertex xpi(i) should be a
neighbor of all of xpi(1), ..., xpi(i?1). In other words
we must have
xpi(i) ? Npi,i ?
i?1?
l=1
N(xpi(l)). (20)
Thus, the probability of choosing xpi(i) as the next
vertex of the path is given by
q?(xpi(i)|xpi(1), ..., xpi(i?1)) =
?(xpi(i)|lS)
?
x?Npi,i
?(x|lS)
,
(21)
if xpi(i) ? Npi,i and 0, otherwise. When choosing
the first vertex of the path (the root in the deci-
sion tree) we have Npi,1 = seg(S), which gives
q?(xpi(1)) = ?(xpi(1)|lS), as required. Therefore
eq. 19 can be written compactly as
p?(?|S) =
?
?
|?|?
i=1
?(xi|lS)
?
?
?
pi
1
Q?(?, pi;S)
,
(22)
where
Q?(?, pi;S) =
|?|?
i=1
?
x?Npi,i
?(x|lS) . (23)
The construction above can be generalized in
order to derive a PMF for any random variable
whose values are partitions of a set. Indeed, by al-
lowing the vertices of G to be a subset of a power
set, and keeping the condition of edge formation
the same, probabilities of clique-preserving paths
can be calculated in the same way. Figure 5 shows
the graph G that represents all possible instances of
K with (S, T ) = (s41, t
5
1), ? =
{
{1, 2}, {3}, {4}
}
and ? =
{
{1}, {2, 3, 4}, {5}
}
. Again each maxi-
mal clique is a possible consistent bisegmentation.
In order for this model to be complete, one
should solve the maximization step of eq. 13 and
calculate the posterior p?n(?, ?,K|S, T ). We are
not bereft of hope, as relevant techniques have
been developed (see Section 6).
6 Related Work
To our knowledge, this is the first attempt to inves-
tigate formal motivations behind the consistency
method.
99
  
t 12 s, 23t 4 s,5 t 3 s, 5
t 12 s, 1
t 4 s,1 t 3 s, 1
t 12 s, 5
t 3 s, 23 t 4 s, 23t 14 s,13
t 14 s, 25
t 43 s, 25
t 13 s, 15
t 14 s,1 t 3 s, 25 t 14 s,5 t 3 s, 13
t 43 s, 13
Figure 5: Similar to Figure 4 but for consistent
bisegmentations with (S, T ) = (s41, t
5
1) and a
given segmentation pair (see text). For clarity, we
show the phrases that are formed from joining con-
tiguous segments in each pair, rather than the seg-
ments themselves.
Several phrase-level generative models have
been proposed, almost all relying on multinomial
distributions for the phrase alignments (Marcu and
Wong, 2002; Zhang et al, 2003; Deng and Byrne
2005; DeNero et al, 2006; Birch et al, 2006).
This is a consequence of treating alignments as
functions rather than partitions.
Word alignment and phrase extraction via In-
version Transduction Grammars (Wu, 1997), is a
linguistically motivated method that relies on si-
multaneous parsing of source and target sentences
(DeNero and Klein, 2010; Cherry and Lin 2007;
Neubig et al, 2012).
The partition probabilities we introduced in
Section 5.2 share the same tree structure discussed
in (Dennis III, 1991), which has found applica-
tions in Information Retrieval (Haffari and Teh,
2009).
7 Conclusions
We have identified the relation between consis-
tency and components of graphs that represent
word-aligned sentence pairs. We showed that
phrase pairs of interest to SMT form a sigma-
algebra generated by components of such graphs,
but the existing occurrence-counting statistics are
inadequate to describe this structure. A general-
ization of our construction via sentence segmenta-
tions lead to a realisation of random partitions as
cases of constrained, biased sampling without re-
placement. As a consequence, we derived an exact
formula for the probability of a segmentation of a
sentence.
Appendix: Measure Space
The following standard definitions can be found
in, e.g., (Feller, 1971). LetX be a set. A collection
B of subsets of X is called a sigma-algebra if the
following conditions hold:
1. ? ? B.
2. If E is in B, then so is its complement X \E.
3. If {Ei} is a countable collection of sets in B,
then so is their union ?iEi.
Condition 1 guarantees that B is non-empty and
Conditions 2 and 3 say thatB is closed under com-
plementation and countable unions respectively.
The pair (X,B) is called a measurable space.
A function f : B ? [0,?) is called a measure
if the following conditions hold:
1. f(?) = 0.
2. If {Ei} is a countable collection of pairwise
disjoint sets in B, then
f(?iEi) =
?
i
f(Ei).
Condition 2 is known as sigma-additivity. The
triple (X,B, f) is called a measure space.
Acknowledgments
This research was supported by the European
Union?s ICT Policy Support Programme as part
of the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430 (GALATEAS) and by the EC funded
project CoSyne (FP7-ICT-4-24853).
References
Alexandra Birch, Chris Callison-Burch, Miles Os-
borne and Philipp Koehn. 2006. Constraining the
Phrase-Based, Joint Probability Statistical Transla-
tion Model. In Proc. of the Workshop on Statistical
Machine Translation, pages 154?157.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation.
Computational Linguistics, vol.19(2), pages 263?
312.
100
Colin Cherry and Dekang Lin. 2007. Inversion Trans-
duction Grammar for Joint Phrasal TranslationMod-
eling. In Proc. of SSST, NAACL-HLT / AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 17?24.
A.P. Dempster, N.M. Laird and D.B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical So-
ciety, Series B (Methodological) 39(1), pages 1?38.
John DeNero, Dan Gillick, James Zhang and Dan
Klein. 2006. Why Generative Phrase Models Un-
derperform Surface Heuristics. In Proc. of the Work-
shop on Statistical Machine Translation, pages 31?
38.
John DeNero and Dan Klein. 2010. Discriminative
Modeling of Extraction Sets for Machine Transla-
tion. In Proc. of the Association for Computational
Linguistics (ACL), pages 1453?1463.
Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. In Proc. of the Conference on Empir-
ical Methods in Natural Language Processing and
Human Language Technology (HLT-EMNLP), pages
169?176.
Samuel Y. Dennis III. 1991. On the Hyper-Dirichlet
Type 1 and Hyper-Liouville Distributions. Commu-
nications in Statistics - Theory and Methods, 20(12),
pages 4069?4081.
William Feller. 1971. An Introduction to Probability
Theory and its Applications, Volume II. John Wiley,
New York.
Gholamreza Haffari and Yee Whye Teh. 2009. Hi-
erarchical Dirichlet Trees for Information Retrieval.
In Proc. of the Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL), pages 173?181.
Frank Harary. 1969. Graph Theory. Addison?Wesley,
Reading, MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology (HLT-NAACL), pages
48?54.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press, Cambridge, UK.
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proc. of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 133?139.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori and Tatsuya Kawahara. 2012. Joint
Phrase Alignment and Extraction for Statistical Ma-
chine Translation. Journal of Information Process-
ing, vol. 20(2), pages 512?523.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical
Machine Translation. In Proc. of the Joint Con-
ference of Empirical Methods in Natural Language
Processing and Very Large Corpora (EMNLP-VLC),
pages 20?28.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23, pages 377?
404.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of the
Association for Computational Linguistics (ACL),
pages 523?530.
Ying Zhang, Stephan Vogel and Alex Waibel. 2003.
Integrated Phrase Segmentation and Alignment Al-
gorithm for Statistical Machine Translation. In
Proc. of the International Conference on Natural
Language Processing and Knowledge Engineering
(NLP-KE).
101
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12?58,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Findings of the 2014 Workshop on Statistical Machine Translation
Ond
?
rej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Christian Federmann
Microsoft Research
Barry Haddow
University of Edinburgh
Philipp Koehn
JHU / Edinburgh
Johannes Leveling
Dublin City University
Christof Monz
University of Amsterdam
Pavel Pecina
Charles University in Prague
Matt Post
Johns Hopkins University
Herve Saint-Amand
University of Edinburgh
Radu Soricut
Google
Lucia Specia
University of Sheffield
Ale
?
s Tamchyna
Charles University in Prague
Abstract
This paper presents the results of the
WMT14 shared tasks, which included a
standard news translation task, a sepa-
rate medical translation task, a task for
run-time estimation of machine translation
quality, and a metrics task. This year, 143
machine translation systems from 23 insti-
tutions were submitted to the ten transla-
tion directions in the standard translation
task. An additional 6 anonymized sys-
tems were included, and were then evalu-
ated both automatically and manually. The
quality estimation task had four subtasks,
with a total of 10 teams, submitting 57 en-
tries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2014. This workshop builds
on eight previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al., 2007, 2008,
2009, 2010, 2011, 2012; Bojar et al., 2013).
This year we conducted four official tasks: a
translation task, a quality estimation task, a met-
rics task
1
and a medical translation task. In the
translation task (?2), participants were asked to
translate a shared test set, optionally restricting
themselves to the provided training data. We held
ten translation tasks this year, between English and
each of Czech, French, German, Hindi, and Rus-
sian. The Hindi translation tasks were new this
year, providing a lesser resourced data condition
on a challenging language pair. The system out-
puts for each task were evaluated both automati-
cally and manually.
1
The metrics task is reported in a separate paper
(Mach?a?cek and Bojar, 2014).
The human evaluation (?3) involves asking
human judges to rank sentences output by
anonymized systems. We obtained large num-
bers of rankings from researchers who contributed
evaluations proportional to the number of tasks
they entered. Last year, we dramatically increased
the number of judgments, achieving much more
meaningful rankings. This year, we developed a
new ranking method that allows us to achieve the
same with fewer judgments.
The quality estimation task (?4) this year
included sentence- and word-level subtasks:
sentence-level prediction of 1-3 likert scores,
sentence-level prediction of percentage of word
edits necessary to fix a sentence, sentence-level
prediction of post-editing time, and word-level
prediction of scores at different levels of granular-
ity (correct/incorrect, accuracy/fluency errors, and
specific types of errors). Datasets were released
with English-Spanish, English-German, Spanish-
English and German-English news translations
produced by 2-3 machine translation systems and,
for some subtasks, a human translation.
The medical translation task (?5) was intro-
duced this year. Unlike the ?standard? translation
task, the test sets come from the very specialized
domain of medical texts. The aim of this task was
not only domain adaptation but also the utilization
of translation systems in a larger scenario, namely
cross-lingual information retrieval (IR). Extrinsic
evaluation in an IR setting was a part of this task
(on the other hand, manual evaluation of transla-
tion quality was not carried out).
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation and estimation methodologies for
machine translation. As before, all of the data,
12
translations, and collected human judgments are
publicly available.
2
We hope these datasets serve
as a valuable resource for research into statistical
machine translation and automatic evaluation or
prediction of translation quality.
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and other languages.
As in the previous years, the other languages in-
clude German, French, Czech and Russian.
We dropped Spanish and added Hindi this year.
From a linguistic point of view, Spanish poses
similar problems as French, making its prior in-
clusion less valuable. Hindi is not only interest-
ing since it is a more distant language than the
European languages we include, but also because
we have much less training data, thus forcing re-
searchers to deal with low resource conditions, but
also providing them with a language pair that does
not suffer from the computational complexities of
having to deal with massive amounts of training
data.
We created a test set for each language pair by
translating newspaper articles and provided train-
ing data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources, as before. How-
ever, we changed our method to create the test sets.
In previous years, we took equal amounts of
source sentences from all six languages involved
(around 500 sentences each), and translated them
into all other languages. While this produced a
multi-parallel test corpus that could be also used
for language pairs (such as Czech-Russian) that
we did not include in the evaluation, it did suf-
fer from artifacts from the larger distance between
source and target sentences. Most test sentences
involved the translation a source sentence that
was translated from a their language into a tar-
get sentence (which was compared against a trans-
lation from that third language as well). Ques-
tions have been raised, if the evaluation of, say,
French-English translation is best served when
testing on sentences that have been originally writ-
ten in, say, Czech. For discussions about trans-
lationese please for instance refer to Koppel and
Ordan (2011).
2
http://statmt.org/wmt14/results.html
This year, we took about 1500 English sen-
tences and translated them into the other 5 lan-
guages, and then additional 1500 sentences from
each of the other languages and translated them
into English. This gave us test sets of about 3000
sentences for our English-X language pairs, which
have been either written originally written in En-
glish and translated into X, or vice versa.
The composition of the test documents is shown
in Table 1. The stories were translated by the pro-
fessional translation agency Capita, funded by the
EU Framework Programme 7 project MosesCore,
and by Yandex, a Russian search engine com-
pany.
3
All of the translations were done directly,
and not via an intermediate language.
2.2 Training data
As in past years we provided parallel corpora
to train translation models, monolingual cor-
pora to train language models, and development
sets to tune system parameters. Some train-
ing corpora were identical from last year (Eu-
roparl
4
, United Nations, French-English 10
9
cor-
pus, CzEng, Common Crawl, Russian-English
Wikipedia Headlines provided by CMU), some
were updated (Russian-English parallel data pro-
vided by Yandex, News Commentary, monolin-
gual data), and a new corpus was added (Hindi-
English corpus, Bojar et al. (2014)), Hindi-English
Wikipedia Headline corpus).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their entry
names are listed in Table 2; each system did not
necessarily appear in all translation tasks. We also
included four commercial off-the-shelf MT sys-
tems and four online statistical MT systems, which
we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3
http://www.yandex.com/
4
As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
13
Europarl Parallel Corpus
French? English German? English Czech? English
Sentences 2,007,723 1,920,209 646,605
Words 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 183,251 201,288 146,549 165,602
Words 5,688,656 4,659,619 5,105,101 5,046,157 3,288,645 3,590,287 4,153,847 4,339,974
Distinct words 72,863 62,673 150,760 65,520 139,477 55,547 151,101 60,801
Common Crawl Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 3,244,152 2,399,123 161,838 878,386
Words 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
French? English
Sentences 12,886,831
Words 411,916,781 360,341,450
Distinct words 565,553 666,077
10
9
Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Hindi-English Parallel Corpus
Hindi? English
Sentences 287,202
Words 6,002,418 3,953,851
Distinct words 121,236 105,330
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English Hindi? English
Sentences 514,859 32,863
Words 1,191,474 1,230,644 141,042 70,075
Distinct words 282,989 251,328 25,678 26,989
Europarl Language Model Data
English French German Czech
Sentence 2,218,201 2,190,579 2,176,537 668,595
Words 59,848,044 63,439,791 53,534,167 14,946,399
Distinct words 123,059 145,496 394,781 172,461
News Language Model Data
English French German Czech Russian Hindi
Sentence 90,209,983 30,451,749 89,634,193 36,426,900 32,245,651 1,275,921
Words 2,109,603,244 748,852,739 1,606,506,785 602,950,410 575,423,682 36,297,394
Distinct words 4,089,792 1,906,470 10,248,707 3,101,846 2,860,837 258,759
News Test Set
French? English German? English Czech? English Russian? English Hindi? English
Sentences 3003 3003 3003 3003 2507
Words 81,194 71,147 63,078 67,624 60,240 68,866 62,107 69,329 86,974 55,822
Distinct words 11,715 10,610 13,930 10,458 16,774 9,893 17,009 9,938 8,292 9,217
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
14
Language Sources (Number of Documents)
Czech aktu?aln?e.cz (2), blesk.cz (3), blisty.cz (1), den??k.cz (9), e15.cz (1), iDNES.cz (17), ihned.cz (14), lidovky.cz (8), medi-
afax.cz (2), metro.cz (1), Novinky.cz (5), pravo.novinky.cz (6), reflex.cz (2), tyden.cz (1), zdn.cz (1).
French BBC French Africa (1), Canoe (9), Croix (4), Cyber Presse (12), Dernieres Nouvelles (1), dhnet.be (5), Equipe (1),
Euronews (6), Journal Metro.com (1), La Libre.be (2), La Meuse.be (2), Le Devoir (3), Le Figaro (8), Le Monde (3),
Les Echos (15), Lexpress.fr (3), Liberation (1), L?independant (2), Metro France (1), Nice-Matin (6), Le Nouvel Ob-
servateur (3), Radio Canada (6), Reuters (7).
English ABC News (5), BBC (5), CBS News (5), CNN (5), Daily Mail (5), Financial Times (5), Fox News (2), Globe and
Mail (1), Independent (1), Los Angeles Times (1), New Yorker (1), News.com Australia (16), Reuters (3), Scotsman (2),
smh.com.au (2), stv.tv (1), Telegraph (6), UPI (2).
German Abendzeitung N?urnberg (1), all-in.de (2), Augsburger Allgemeine (1), AZ Online (1), B?orsenzeitung (1), come-
on.de (1), Der Westen (2), DZ Online (1), Reutlinger General-Anzeiger (1), Generalanzeiger Bonn (1), Giessener
Anzeiger (1), Goslarsche Zeitung (1), Hersfelder Zeitung (1), J?udische Allgemeine (1), Kreisanzeiger (2),
Kreiszeitung (2), Krone (1), Lampertheimer Zeitung (2), Lausitzer Rundschau (1), Mittelbayerische (1), Morgen-
post (1), nachrichten.at (1), Neue Presse (1), OP Online (1), Potsdamer Neueste Nachrichten (1), Passauer Neue
Presse (1), Recklingh?auser Zeitung (1), Rhein Zeitung (1), salzburg.com (1), Schwarzw?alder Bote (29), Segeberger
Zeitung (1), Soester Anzeiger (1), S?udkurier (17), svz.de (1), Tagesspiegel (1), Usinger Anzeiger (3), Volksblatt.li (1),
Westf?alischen Anzeiger (3), Wiener Zeitung (1), Wiesbadener Kurier (1), Westdeutsche Zeitung (1), Wilhelmshavener
Zeitung (1), Yahoo Deutschland (1).
Hindi Bhaskar (24), Jagran (61), Navbharat Times / India Times (4), ndtv (2).
Russian 168.ru (1), aif (3), altapress.ru (2), argumenti.ru (2), BBC Russian (3), belta.by (2), communa.ru (1), dp.ru (1), eg-
online.ru (1), Euronews (2), fakty.ua (2), gazeta.ru (1), inotv.rt.com (1), interfax (1), Izvestiya (1), Kommersant (7),
kp (2), lenta.ru (4), lgng (1), litrossia.ru (1), mirnov.ru (5), mk (8), mn.ru (2), newizv (2), nov-pravda.ru (1), no-
vayagazeta (1), nr2.ru (8), pnp.ru (1), rbc.ru (3), ria.ru (4), rosbalt.ru (1), sovsport.ru (6), Sport Express (10), trud.ru (4),
tumentoday.ru (1), vesti.ru (10), zr.ru (1).
Table 1: Composition of the test set. For more details see the XML test files. The docid tag gives the source and the date for
each document in the test set, and the origlang tag indicates the original source language.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
This year?s evaluation was conducted a bit dif-
ferently. The main differences are:
? In contrast to the past two years, we collected
judgments entirely from researchers partici-
pating in the shared tasks and trusted friends
of the community. Last year, about two thirds
of the data were solicited from random volun-
teers on the Amazon Mechanical Turk. For
some language pairs, the Turkers data had
much lower inter-annotator agreement com-
pared to the researchers.
? As a result, we collected about seventy-five
percent less data, but were able to obtain
good confidence intervals on the clusters with
the use of new approaches to ranking.
? We compared three different ranking method-
ologies, selecting the one with the highest ac-
curacy on held-out data.
We also maintain many of our customs from
prior years, including the presentation of the re-
sults in terms of a partial ordering (clustering) of
the systems. Systems in the same cluster could not
be meaningfully distinguished and should be con-
sidered ties.
3.1 Data collection
The system ranking is produced from a large set of
pairwise annotations between system pairs. These
pairwise annotations are collected in an evaluation
campaign that enlists participants in the shared
task to contribute one hundred ?Human Intelli-
gence Tasks? (HITs) per system submitted. Each
HIT consists of three ranking tasks. In a rank-
ing task, an annotator is presented with a source
segment, a human reference translation, and the
outputs of five anonymized systems, randomly se-
lected from the set of participating systems, and
randomly ordered.
To run the evaluation, we use Appraise
5
(Fe-
dermann, 2012), an open-source tool built on
Python?s Django framework. At the top of each
HIT, the following instructions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
5
https://github.com/cfedermann/Appraise
15
ID Institution
AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014)
CIMS University of Stuttgart / University of Munich (Cap et al., 2014)
CMU Carnegie Mellon University (Matthews et al., 2014)
CU-* Charles University, Prague (Tamchyna et al., 2014)
DCU-FDA Dublin City University (Bicici et al., 2014)
DCU-ICTCAS Dublin City University (Li et al., 2014b)
DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014)
EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014)
KIT Karlsruhe Institute of Technology (Herrmann et al., 2014)
IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014)
IIIT-HYDERABAD IIIT Hyderabad
IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014)
IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014)
KAZNU Amandyk Kartbayev, FBK
LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014)
MANAWI-* Universit?at des Saarlandes (Tan and Pal, 2014)
MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014)
PROMT-RULE,
PROMT-HYBRID
PROMT
RWTH RWTH Aachen (Peitz et al., 2014)
STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014)
UA-* University of Alicante (S?anchez-Cartagena et al., 2014)
UEDIN-PHRASE,
UEDIN-UNCNSTR
University of Edinburgh (Durrani et al., 2014b)
UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014)
UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014)
YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C,G] Four online statistical machine translation systems
RBMT-[1,4] Two rule-based statistical machine translation systems
Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
16
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly ordered), and is asked to
rank these according to their translation quality, with ties allowed.
A screenshot of the ranking interface is shown in
Figure 2. Annotators are asked to rank the sys-
tems from 1 (best) to 5 (worst), with ties permit-
ted. Note that a lower rank is better. The rankings
provided by a ranking task are then reduced to a
set of ten pairwise rankings produced by consider-
ing all
(
5
2
)
combinations of systems in the ranking
task. For example, consider the following annota-
tion provided among systems A,B, F,H , and J :
1 2 3 4 5
F ?
A ?
B ?
J ?
H ?
This is reduced to the following set of pairwise
judgments:
A > B,A = F,A > H,A < J
B < F,B < H,B < J
F > H,F < J
H < J
Here,A > B should be read is ?A is ranked higher
than (worse than) B?. Note that by this procedure,
the absolute value of ranks and the magnitude of
their differences are discarded.
For WMT13, nearly a million pairwise anno-
tations were collected from both researchers and
paid workers on Amazon?s Mechanical Turk, in
a roughly 1:2 ratio. This year, we collected data
from researchers only, an ability that was enabled
by the use of a new technique for producing the
partial ranking for each task (?3.3.3). Table 3 con-
tains more detail.
3.2 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960). If P (A) be
the proportion of times that the annotators agree,
and P (E) is the proportion of time that they would
17
LANGUAGE PAIR Systems Rankings Average
Czech?English 5 21,130 4,226.0
English?Czech 10 55,900 5,590.0
German?English 13 25,260 1,943.0
English?German 18 54,660 3,036.6
French?English 8 26,090 3,261.2
English?French 13 33,350 2,565.3
Russian?English 13 34,460 2,650.7
English?Russian 9 28,960 3,217.7
Hindi?English 9 20,900 2,322.2
English?Hindi 12 28,120 2,343.3
TOTAL WMT 14 110 328,830 2,989.3
WMT13 148 942,840 6,370.5
WMT12 103 101,969 999.6
WMT11 133 63,045 474.0
Table 3: Amount of data collected in the WMT14 manual evaluation. The final three rows report summary information from
the previous two workshops.
agree by chance, then Cohen?s kappa is:
? =
P (A)? P (E)
1? P (E)
Note that ? is basically a normalized version of
P (A), one which takes into account how mean-
ingful it is for annotators to agree with each other
by incorporating P (E). The values for ? range
from 0 to 1, with zero indicating no agreement and
1 perfect agreement.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A < B, A = B, or A > B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it captures the probability that two
annotators would agree randomly. Therefore:
P (E) = P (A<B)
2
+ P (A=B)
2
+ P (A>B)
2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 4 gives ? values for inter-annotator agree-
ment for WMT11?WMT14 while Table 5 de-
tails intra-annotator agreement scores, including
the division of researchers (WMT13
r
) and MTurk
(WMT13
m
) data. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0?0.2 is slight, 0.2?0.4 is
fair, 0.4?0.6 is moderate, 0.6?0.8 is substantial,
and 0.8?1.0 is almost perfect. The agreement rates
are more or less in line with prior years: worse for
some tasks, better for others, and on average, the
best since WMT11 (where agreement scores were
likely inflated due to inclusion of reference trans-
lations in the comparisons).
3.3 Models of System Rankings
The collected pairwise rankings are used to pro-
duce a ranking of the systems. Machine transla-
tion evaluation has always been a subject of con-
tention, and no exception to this rule exists for the
WMT manual evaluation. While the precise met-
ric has varied over the years, it has always shared
a common idea of computing the average num-
ber of times each system was judged better than
other systems, and ranking from highest to low-
est. For example, in WMT11 Callison-Burch et al.
(2011), the metric computed the percentage of the
time each system was ranked better than or equal
to other systems, and included comparisons to hu-
man references. In WMT12 Callison-Burch et al.
(2012), comparisons to references were dropped.
In WMT13, rankings were produced over 1,000
bootstrap-resampled sets of the training data. A
rank range was collected for each system across
these folds; the average value was used to order
the systems, and a 95% confidence interval across
these ranks was used to organize the systems into
equivalence classes containing systems with over-
18
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.400 0.311 0.244 0.342 0.279 0.305
English?Czech 0.460 0.359 0.168 0.408 0.075 0.360
German?English 0.324 0.385 0.299 0.443 0.324 0.368
English?German 0.378 0.356 0.267 0.457 0.239 0.427
French?English 0.402 0.272 0.275 0.405 0.321 0.357
English?French 0.406 0.296 0.231 0.434 0.237 0.302
Hindi?English ? ? ? ? ? 0.400
English?Hindi ? ? ? ? ? 0.413
Russian?English ? ? 0.278 0.315 0.324 0.324
English?Russian ? ? 0.243 0.416 0.207 0.418
MEAN 0.395 0.330 0.260 0.367
Table 4: ? scores measuring inter-annotator agreement. See Table 5 for corresponding intra-annotator agreement scores.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.597 0.454 0.479 0.483 0.478 0.382
English?Czech 0.601 0.390 0.290 0.547 0.242 0.448
German?English 0.576 0.392 0.535 0.643 0.515 0.344
English?German 0.528 0.433 0.498 0.649 0.452 0.576
French?English 0.673 0.360 0.578 0.585 0.565 0.629
English?French 0.524 0.414 0.495 0.630 0.486 0.507
Hindi?English ? ? ? ? ? 0.605
English?Hindi ? ? ? ? ? 0.535
Russian?English ? ? 0.450 0.363 0.477 0.629
English?Russian ? ? 0.513 0.582 0.500 0.570
MEAN 0.583 0.407 0.479 0.522
Table 5: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation.
lapping ranges.
This year, we introduce two new changes. First,
we pit the WMT13 method against two new ap-
proaches: that of Hopkins and May (2013, ?3.3.2),
and another based on TrueSkill (Sakaguchi et al.,
2014, ?3.3.3). Second, we compare these two
methods against WMT13?s ?Expected Wins? ap-
proach, and then select among them by determin-
ing which of them has the highest accuracy in
terms of predicting annotations on a held-out set
of pairwise judgments.
3.3.1 Method 1: Expected Wins (EW)
Introduced for WMT13, the EXPECTED WINS has
an intuitive score demonstrated to be accurate in
ranking systems according to an underlying model
of ?relative ability? (Koehn, 2012a). The idea is
to gauge the probability that a system S
i
will be
ranked better than another system randomly cho-
sen from a pool of opponents {S
j
: j 6= i}. If
we define the function win(A,B) as the number
of times system A is ranked better than system B,
then we can define this as follows:
score
EW
(S
i
) =
1
|{S
j
}|
?
j,j 6=i
win(S
i
, S
j
)
win(S
i
, S
j
) + win(S
j
, S
i
)
Note that this score ignores ties.
3.3.2 Method 2: Hopkins and May (HM)
Hopkins and May (2013) introduced a graphical
model formulation of the task, which makes the
notion of underlying system ability even more ex-
plicit. Each system S
J
in the pool {S
j
} is repre-
sented by an associated relative ability ?
j
and a
variance ?
2
a
(fixed across all systems) which serve
as the parameters of a Gaussian distribution. Sam-
ples from this distribution represent the quality
of sentence translations, with higher quality sam-
ples having higher values. Pairwise annotations
(S
1
, S
2
, pi) are generated according to the follow-
ing process:
19
1. Select two systems S
1
and S
2
from the pool
of systems {S
j
}
2. Draw two ?translations?, adding random
Gaussian noise with variance ?
2
obs
to simulate
the subjectivity of the task and the differences
among annotators:
q
1
? N (?
S
1
, ?
2
a
) +N (0, ?
2
obs
)
q
2
? N (?
S
2
, ?
2
a
) +N (0, ?
2
obs
)
3. Let d be a nonzero real number that defines
a fixed decision radius. Produce a rating pi
according to:
pi =
?
?
?
< q
1
? q
2
> d
> q
2
? q
1
> d
= otherwise
Hopkins and May use Gibbs sampling to infer
the set of system means from an annotated dataset.
Details of this inference procedure can be found in
Sakaguchi et al. (2014). The score used to produce
the rankings is simply the system mean associated
with each system:
score
HM
(S
i
) = ?
S
i
3.3.3 Method 3: TrueSkill (TS)
TrueSkill is an adaptive, online system that em-
ploys a similar model of relative ability Herbrich
et al. (2006). It was initially developed for Xbox
Live?s online player community, where it is used
to model player ability, assign levels, and select
competitive matches. Each player S
j
is modeled
by two parameters: TrueSkill?s current estimate
of each system?s relative ability, ?
S
j
, and a per-
system measure of TrueSkill?s uncertainty of those
estimates, ?
2
S
j
. When the outcome of a match is
observed, TrueSkill uses the relative status of the
two systems to update these estimates. If a trans-
lation from a system with a high mean is judged
better than a system with a greatly lower mean, the
result is not surprising, and the update size for the
corresponding system means will be small. On the
other hand, when an upset occurs in a competition,
the means will receive larger updates. Sakaguchi
et al. (2014) provide an adaptation of this approach
to the WMT manual evaluation, and showed that
it performed well on WMT13 data.
Similar to the Hopkins and May model,
TrueSkill scores systems by their inferred means:
score
TS
(S
i
) = ?
S
i
This score is then used to sort the systems and pro-
duce the ranking.
3.4 Method Selection
We have three methods which, provided with the
collected data, produce different rankings of the
systems. Which of them is correct? More imme-
diately, which one of them should we publish as
the official ranking for the WMT14 manual eval-
uation? As discussed, the method used to com-
pute the ranking has been tweaked a bit each year
over the past few years in response to criticisms
(e.g., Lopez (2012); Bojar et al. (2011)). While the
changes were reasonable (and later corroborated),
Hopkins and May (2013) pointed out that this task
of model selection should be driven by empirical
evaluation on held-out data, and suggested per-
plexity as the metric of choice.
We choose instead a more direct gold-standard
evaluation metric: the accuracy of the rankings
produced by each method in predicting pairwise
judgments. We use each method to produce a par-
tial ordering of the systems, grouping them into
equivalence classes. This partial ordering unam-
biguously assigns a prediction pi
P
between any
pair of systems (S
i
, S
j
). By comparing the pre-
dicted relationship pi
P
to the actual annotation for
each pairwise judgment in the test data (by token),
we can compute an accuracy score for each model.
We predict accuracy in this manner using 100-
fold cross-validation. For each task, we split the
data into a fixed set of 100 randomly-selected
folds. Each fold serves as a test set, with the
remaining ninety-nine folds available as training
data for each method. Note that the total order-
ing over systems provided by the score
?
functions
defined do not predict ties. In order to do enable
the models to predict ties, we produce equivalence
classes using the following procedure:
? Assign S
1
to a cluster
? For each system S
i
, assign it to the current
cluster if score(S
i?1
) ? score(S
i
) ? r; oth-
erwise, assign it to a new cluster
The value of r (the decision radius for ties)
is tuned using accuracy on the entire training
data using grid search over the values r ?
0, 0.01, 0.02, . . . , .25 (26 values in total). This
value is tuned separately for each method on each
fold. Table 6 contains an example partial ordering.
20
System Score Rank
B 0.60 1
D 0.44 2
E 0.39 2
A 0.25 2
F -0.09 3
C -0.22 3
Table 6: The partial ordering computed with the provided
scores when r = 0.15.
Task EW HM TS Oracle
Czech?English 40.4 41.1 41.1 41.2
English?Czech 45.3 45.6 45.9 46.8
French?English 49.0 49.4 49.3 50.3
English?French 44.6 44.4 44.7 46.0
German?English 43.5 43.7 43.7 45.2
English?German 47.3 47.4 47.2 48.2
Hindi?English 62.5 62.2 62.5 62.6
English?Hindi 53.3 53.7 53.5 55.7
Russian?English 47.6 47.7 47.7 50.6
English?Russian 46.5 46.1 46.4 48.2
MEAN 48.0 48.1 48.2 49.2
Table 7: Accuracies for each method across 100 folds, for
each translation task. The oracle uses the most frequent out-
come between each pair of systems, and therefore might not
constitute a feasible ranking.
After training, each model has defined a partial
ordering over systems.
6
This is then used to com-
pute accuracy on all the pairwise judgments in the
test fold. This process yields 100 accuracies for
each method; the average accuracy across all the
folds can then be used to compute the best method.
Table 7 contains accuracy results for the three
methods on the WMT14 tasks. On average, there
is a small improvement in accuracy moving from
Expected Wins to the H&M model, and then again
to the TrueSkill model; however, there is no pat-
tern to the best model for each class. The Oracle
column is computed by selecting the most prob-
able outcome (pi ? {<,=, >}) for each system
pair, and provides an upper bound on accuracy
when predicting outcomes using only system-level
information. Furthermore, this method of oracle
computation might not represent a feasible rank-
ing or clustering,
7
.
The TrueSkill approach was best overall, so we
used it to produce the official rankings for all lan-
6
It is a total ordering when r = 0, or when all the system
scores are outside the decision radius.
7
For example, if there were a cycle of ?better than? judg-
ments among a set of systems.
guage pairs.
3.5 Rank Ranges and Clusters
Above we saw how to produce system scores for
each method, which provides a total ordering of
the systems. But we would also like to know if the
obtained system ranking is statistically significant.
Given the large number of systems that participate,
and the similarity of the underlying systems result-
ing from the common training data condition and
(often) toolsets, there will be some systems that
will be very close in quality. These systems should
be grouped together in equivalence classes.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute a TrueSkill model score for each system
based on this sample, and then rank the systems
from 1..|{S
j
}|. By repeating this procedure 1,000
times, we can determine a range of ranks, into
which system falls at least 95% of the time (i.e.,
at least 950 times) ? corresponding to a p-level
of p ? 0.05. Furthermore, given the rank ranges
for each system, we can cluster systems with over-
lapping rank ranges.
8
Table 8 reports all system scores, rank ranges,
and clusters for all language pairs and all systems.
The official interpretation of these results is that
systems in the same cluster are considered tied.
Given the large number of judgments that we col-
lected, it was possible to group on average about
two systems in a cluster, even though the systems
in the middle are typically in larger clusters.
3.6 Cluster analysis
The official ranking results for English-German
produced clusters compute at the 90% confidence
level due to the presence of a very large cluster
(of nine systems). While there is always the pos-
sibility that this cluster reflects a true ambiguity, it
is more likely due to the fact that we didn?t have
enough data: English?German had the most sys-
8
Formally, given ranges defined by start(S
i
) and end(S
i
),
we seek the largest set of clusters {C
c
} that satisfies:
?S ?C : S ? C
S ? C
a
, S ? C
b
? C
a
= C
b
C
a
6= C
b
? ?S
i
? C
a
, S
j
? C
b
:
start(S
i
) > end(S
j
) or start(S
j
) > end(S
i
)
21
Czech?English
# score range system
1 0.591 1 ONLINE-B
2 0.290 2 UEDIN-PHRASE
3 -0.171 3-4 UEDIN-SYNTAX
-0.243 3-4 ONLINE-A
4 -0.468 5 CU-MOSES
English?Czech
# score range system
1 0.371 1-3 CU-DEPFIX
0.356 1-3 UEDIN-UNCNSTR
0.333 1-4 CU-BOJAR
0.287 3-4 CU-FUNKY
2 0.169 5-6 ONLINE-B
0.113 5-6 UEDIN-PHRASE
3 0.030 7 ONLINE-A
4 -0.175 8 CU-TECTO
5 -0.534 9 COMMERCIAL1
6 -0.950 10 COMMERCIAL2
Russian?English
# score range system
1 0.583 1 AFRL-PE
2 0.299 2 ONLINE-B
3 0.190 3-5 ONLINE-A
0.178 3-5 PROMT-HYBRID
0.123 4-7 PROMT-RULE
0.104 5-8 UEDIN-PHRASE
0.069 5-8 YANDEX
0.066 5-8 ONLINE-G
4 -0.017 9 AFRL
5 -0.159 10 UEDIN-SYNTAX
6 -0.306 11 KAZNU
7 -0.487 12 RBMT1
8 -0.642 13 RBMT4
English?Russian
# score range system
1 0.575 1-2 PROMT-RULE
0.547 1-2 ONLINE-B
2 0.426 3 PROMT-HYBRID
3 0.305 4-5 UEDIN-UNCNSTR
0.231 4-5 ONLINE-G
4 0.089 6-7 ONLINE-A
0.031 6-7 UEDIN-PHRASE
5 -0.920 8 RBMT4
6 -1.284 9 RBMT1
German?English
# score range system
1 0.451 1 ONLINE-B
2 0.267 2-3 UEDIN-SYNTAX
0.258 2-3 ONLINE-A
3 0.147 4-6 LIMSI-KIT
0.146 4-6 UEDIN-PHRASE
0.138 4-6 EU-BRIDGE
4 0.026 7-8 KIT
-0.049 7-8 RWTH
5 -0.125 9-11 DCU-ICTCAS
-0.157 9-11 CMU
-0.192 9-11 RBMT4
6 -0.306 12 RBMT1
7 -0.604 13 ONLINE-C
French?English
# score range system
1 0.608 1 UEDIN-PHRASE
2 0.479 2-4 KIT
0.475 2-4 ONLINE-B
0.428 2-4 STANFORD
3 0.331 5 ONLINE-A
4 -0.389 6 RBMT1
5 -0.648 7 RBMT4
6 -1.284 8 ONLINE-C
English?French
# score range system
1 0.327 1 ONLINE-B
2 0.232 2-4 UEDIN-PHRASE
0.194 2-5 KIT
0.185 2-5 MATRAN
0.142 4-6 MATRAN-RULES
0.120 4-6 ONLINE-A
3 0.003 7-9 UU-DOCENT
-0.019 7-10 PROMT-HYBRID
-0.033 7-10 UA
-0.069 8-10 PROMT-RULE
4 -0.215 11 RBMT1
5 -0.328 12 RBMT4
6 -0.540 13 ONLINE-C
English?German
# score range system
1 0.264 1-2 UEDIN-SYNTAX
0.242 1-2 ONLINE-B
2 0.167 3-6 ONLINE-A
0.156 3-6 PROMT-HYBRID
0.155 3-6 PROMT-RULE
0.155 3-6 UEDIN-STANFORD
3 0.094 7 EU-BRIDGE
4 0.033 8-10 RBMT4
0.031 8-10 UEDIN-PHRASE
0.012 8-10 RBMT1
5 -0.032 11-12 KIT
-0.069 11-13 STANFORD-UNC
-0.100 12-14 CIMS
-0.126 13-15 STANFORD
-0.158 14-16 UU
-0.191 15-16 ONLINE-C
6 -0.307 17-18 IMS-TTT
-0.325 17-18 UU-DOCENT
Hindi?English
# score range system
1 1.326 1 ONLINE-B
2 0.559 2-3 ONLINE-A
0.476 2-4 UEDIN-SYNTAX
0.434 3-4 CMU
3 0.323 5 UEDIN-PHRASE
4 -0.198 6-7 AFRL
-0.280 6-7 IIT-BOMBAY
5 -0.549 8 DCU-LINGO24
6 -2.092 9 IIIT-HYDERABAD
English?Hindi
# score range system
1 1.008 1 ONLINE-B
2 0.915 2 ONLINE-A
3 0.214 3 UEDIN-UNCNSTR
4 0.120 4-5 UEDIN-PHRASE
0.054 4-5 CU-MOSES
5 -0.111 6-7 IIT-BOMBAY
-0.142 6-7 IPN-UPV-CNTXT
6 -0.233 8-9 DCU-LINGO24
-0.261 8-9 IPN-UPV-NODEV
7 -0.449 10-11 MANAWI-H1
-0.494 10-11 MANAWI
8 -0.622 12 MANAWI-RMOOV
Table 8: Official results for the WMT14 translation task. Systems are ordered by their inferred system means. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05, except for English?German, where p ? 0.1.
This method is also used to determine the range of ranks into which system falls. Systems with grey background indicate use
of resources that fall outside the constraints provided for the shared task.
22
tems (18, compared to 13 for the next languages),
yet only an average amount of per-system data.
Here, we look at this language pair in more detail,
in order to justify this decision, and to shed light
on the differences between the ranking methods.
Table 9 presents the 95% confidence-level clus-
terings for English?German computed with each
of the three methods, along with lines that show
the reorderings of the systems between them. Re-
orderings of this type have been used to argue
against the reliability of the official WMT rank-
ing (Lopez, 2012; Hopkins and May, 2013). This
table shows that these reorderings are captured en-
tirely by the clustering approach we used. This rel-
ative consensus of these independently-computed
and somewhat different models suggests that the
published ranking is approaching the true ambigu-
ity underlying systems within the same cluster.
Looking across all language pairs, we find that
the total ordering predicted by EW and TS is ex-
actly the same for eight of the ten language pair
tasks, and is constrained to reorderings within
the official cluster for the other two (German?
English ? just one adjacent swap ? and English?
German, depicted in Table 9).
3.7 Conclusions
The official ranking method employed by WMT
over the past few years has changed a few times as
a result of error analysis and introspection. Until
this year, these results were largely based on the
intuitions of the community and organizers about
deficiencies in the models. In addition to their in-
tuitive appeal, many of these changes (such as the
decision to throw out comparisons against refer-
ences) have been empirically validated Hopkins
and May (2013). The actual effect of the refine-
ments in the ranking metric has been minor pertur-
bations in the permutation of systems. The cluster-
ing method of Koehn (2012b), in which the official
rankings are presented as a partial (instead of to-
tal) ordering, alleviated many of the problems ob-
served by Lopez (2012), and also capture all the
variance across the new systems introduced this
year. In addition, presenting systems as clusters
appeals to intuition. As such, we disagree with
claims that there is a problem with irreproducibil-
ity of the results of the workshop evaluation task,
and especially disagree that there is anything ap-
proaching a ?crisis of confidence? (Hopkins and
May, 2013). These claims seem to us to be over-
stated.
Conducting proper model selection by compar-
ison on held-out data, however, is a welcome sug-
gestion, and our inclusion of this process supports
improved confidence in the ranking results. That
said, it is notable that the different methods com-
pute very similar orderings. This avoids hallu-
cinating distinctions among systems that are not
really there, and captures the intuition that some
systems are basically equivalent. The chief ben-
efit of the TrueSkill model is not in outputting a
better complete ranking of the systems, but lies in
its reduced variance, which allow us to cluster the
systems with less data. There is also the unex-
plored avenue of using TrueSkill to drive the data
collection, steering the annotations of judges to-
wards evenly matched systems during the collec-
tion phase, potentially allowing confident results
to be presented while collecting even less data.
There is, of course, more work to be done.
We have produced this year statistically significant
clusters with a third of the data required last year,
which is an improvement. Models of relative abil-
ity are a natural fit for the manual evaluation, and
the introduction of an online Bayesian approach
to data collection present further opportunities to
reduce the amount of data needed. These methods
also provide a framework for extending the models
in a variety of potentially useful ways, including
modeling annotator bias, incorporating sentence
metadata (such as length, difficulty, or subtopic),
and adding features of the sentence pairs.
4 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
third edition of the WMT shared task on qual-
ity estimation builds on the previous editions of
the task (Callison-Burch et al., 2012; Bojar et al.,
2013), with tasks including both sentence-level
and word-level estimation, with new training and
test datasets.
The goals of this year?s shared task were:
? To investigate the effectiveness of different
quality labels.
? To explore word-level quality prediction at
23
Expected Wins Hopkins & May TrueSkill
UEDIN-SYNTAX UEDIN-SYNTAX UEDIN-SYNTAX
ONLINE-B ONLINE-B ONLINE-B
ONLINE-A UEDIN-STANFORD ONLINE-A
UEDIN-STANFORD PROMT-HYBRID PROMT-HYBRID
PROMT-RULE ONLINE-A PROMT-RULE
PROMT-HYBRID PROMT-RULE UEDIN-STANFORD
EU-BRIDGE EU-BRIDGE EU-BRIDGE
RBMT4 UEDIN-PHRASE RBMT4
UEDIN-PHRASE RBMT4 UEDIN-PHRASE
RBMT1 RBMT1 RBMT1
KIT KIT KIT
STANFORD-UNC STANFORD-UNC STANFORD-UNC
CIMS CIMS CIMS
STANFORD STANFORD STANFORD
UU UU UU
ONLINE-C ONLINE-C ONLINE-C
IMS-TTT UU-DOCENT IMS-TTT
UU-DOCENT IMS-TTT UU-DOCENT
Table 9: A comparison of the rankings produced by Expected Wins, Hopkins & May, and TrueSkill for English?German (the
task with the most systems and the largest cluster). The lines extending all the way across mark the official English?German
clustering (computed from TrueSkill with 90% confidence intervals), while bold entries mark the start of new clusters within
each method or column (computed at the 95% confidence level). The TrueSkill clusterings contain all the system reorderings
across the other two ranking methods.
different levels of granularity.
? To study the effects of training and test
datasets with mixed domains, language pairs
and MT systems.
? To examine the effectiveness of quality pre-
diction methods on human translations.
Four tasks were proposed: Tasks 1.1, 1.2, 1.3
are defined at the sentence-level (Sections 4.1),
while Task 2, at the word-level (Section 4.2). Each
task provides one or more datasets with up to four
language pairs each: English-Spanish, English-
German, German-English, Spanish-English, and
up to four alternative translations generated by:
a statistical MT system (SMT), a rule-based MT
system (RBMT), a hybrid MT system, and a hu-
man. These datasets were annotated with differ-
ent labels for quality by professional translators as
part of the QTLaunchPad
9
project. External re-
sources (e.g. parallel corpora) were provided to
participants. Any additional resources, including
additional quality estimation training data, could
9
http://www.qt21.eu/launchpad/
be used by participants (no distinction between
open and close tracks is made). Participants were
also provided with a software package to extract
quality estimation features and perform model
learning, with a suggested list of baseline features
and learning method for sentence-level prediction.
Participants, described in Section 4.3, could sub-
mit up to two systems for each task.
Data used for building specific MT systems or
internal system information (such as n-best lists)
were not made available this year as multiple MT
systems were used to produced the datasets, in-
cluding rule-based systems. In addition, part of
the translations were produced by humans. Infor-
mation on the sources of translations was not pro-
vided either. Therefore, as a general rule, partici-
pants were only allowed to use black-box features.
4.1 Sentence-level Quality Estimation
For the sentence-level tasks, two variants of the
results could be submitted for each task and lan-
guage pair:
? Scoring: An absolute quality score for each
sentence translation according to the type of
24
prediction, to be interpreted as an error met-
ric: lower scores mean better translations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning).
Evaluation was performed against the true label
and/or HTER ranking using the same metrics as in
previous years:
? Scoring: Mean Average Error (MAE) (pri-
mary metric), Root Mean Squared Error
(RMSE).
? Ranking: DeltaAvg (primary metric) (Bojar
et al., 2013) and Spearman?s rank correlation.
For all sentence-level these tasks, the same 17
features as in WMT12-13 were used to build base-
line systems. The SVM regression algorithm
within QUEST (Specia et al., 2013)
10
was applied
for that with RBF kernel and grid search for pa-
rameter optimisation.
Task 1.1 Predicting post-editing effort
Data in this task is labelled with discrete and
absolute scores for perceived post-editing effort,
where:
? 1 = Perfect translation, no post-editing
needed at all.
? 2 = Near miss translation: translation con-
tains maximum of 2-3 errors, and possibly
additional errors that can be easily fixed (cap-
italisation, punctuation, etc.).
? 3 = Very low quality translation, cannot be
easily fixed.
The datasets were annotated in a ?triage? phase
aimed at selecting translations of type ?2? (near
miss) that could be annotated for errors at the
word-level using the MQM metric (see Task 2, be-
low) for a more fine-grained and systematic trans-
lation quality analysis. Word-level errors in trans-
lations of type ?3? are too difficult if not impos-
sible to annotate and classify, particularly as they
often contain inter-related errors in contiguous or
overlapping word spans.
10
http://www.quest.dcs.shef.ac.uk/
For the training of prediction models, we pro-
vide a new dataset consisting of source sen-
tences and their human translations, as well as
two-three versions of machine translations (by an
SMT system, an RBMT system and, for English-
Spanish/German only, a hybrid system), all in the
news domain, extracted from tests sets of various
WMT years and MT systems that participated in
the translation shared task:
# Source sentences # Target sentences
954 English 3,816 Spanish
350 English 1,400 German
350 German 1,050 English
350 Spanish 1,050 English
As test data, for each language pair and MT sys-
tem (or human translation) we provide a new set
of translations produced by the same MT systems
(and humans) as those used for the training data:
# Source sentences # Target sentences
150 English 600 Spanish
150 English 600 German
150 German 450 English
150 Spanish 450 English
The distribution of true scores in both training
and test sets for each language pair is given in Fig-
ures 3.
0%#
10%#
20%#
30%#
40%#
50%#
60%#
{en-
de-1
}#
{en-
de-2
}#
{en-
de-3
}#
{de-
en-1
}#
{de-
en-2
}#
{de-
en-3
}#
{en-
es-1
}#
{en-
es-2
}##
{en-
es-3
}##
{es-
en-1
}#
{es-
en-2
}#
{es-
en-3
}#
#Training##### #Test####
Figure 3: Distribution of true 1-3 scores by langauge pair.
Additionally, we provide some out of domain
test data. These translations were annotated in
the same way as above, each dataset by one Lan-
guage Service Provider (LSP), i.e, one profes-
sional translator, with two LPSs producing data in-
dependently for English-Spanish. They were gen-
erated using the LSPs? own source data (a different
domain from news), and own MT system (differ-
ent from the three used for the official datasets).
The results on these datasets were not considered
25
for the official ranking of the participating sys-
tems:
# Source sentences # Target sentences
971 English 971 Spanish
297 English 297 German
388 Spanish 388 English
Task 1.2 Predicting percentage of edits
In this task we use HTER (Snover et al., 2006) as
quality score. This score is to be interpreted as
the minimum edit distance between the machine
translation and its manually post-edited version,
and its range is [0, 1] (0 when no edit needs to
be made, and 1 when all words need to be edited).
We used TERp (default settings: tokenised, case
insensitive, etc., but capped to 1)
11
to compute the
HTER scores.
For practical reasons, the data is a subset of
Task 1.1?s dataset: only translations produced
by the SMT system English-Spanish. As train-
ing data, we provide 896 English-Spanish trans-
lation suggestions and their post-editions. As
test data, we provide a new set of 208 English-
Spanish translations produced by the same SMT
system. Each of the training and test translations
was post-edited by a professional translator using
the CASMACAT
12
web-based tool, which also col-
lects post-editing time on a sentence-basis.
Task 1.3 Predicting post-editing time
For this task systems are required to produce, for
each translation, a real valued estimate of the time
(in milliseconds) it takes a translator to post-edit
the translation. The training and test sets are a sub-
set of that uses in Task 1.2 (subject to filtering of
outliers). The difference is that the labels are now
the number of milliseconds that were necessary to
post-edit each translation.
As training data, we provide 650 English-
Spanish translation suggestions and their post-
editions. As test data, we provide a new set of 208
English-Spanish translations (same test data as for
Task 1.2).
4.2 Word-level Quality Estimation
The data for this task is based on a subset of the
datasets used for Task 1.1, for all language pairs,
11
http://www.umiacs.umd.edu/
?
snover/terp/
12
http://casmacat.eu/
human and machine translations: those transla-
tions labelled ?2? (near misses), plus additional
data provided by industry (either on the news do-
main or on other domains, such as technical doc-
umentation, produced using their own MT sys-
tems, and also pre-labelled as ?2?). All seg-
ments were annotated with word-level labels by
professional translators using the core categories
in MQM (Multidimensional Quality Metrics)
13
as
error typology (see Figure 4). Each word or se-
quence of words was annotated with a single error.
For (supposedly rare) cases where a decision be-
tween multiple fine-grained error types could not
be made, annotators were requested to choose a
coarser error category in the hierarchy.
Participants are asked to produce a label for
each token that indicates quality at different lev-
els of granularity:
? Binary classification: an OK / bad label,
where bad indicates the need for editing the
token.
? Level 1 classification: an OK / accuracy /
fluency label, specifying coarser level cate-
gories of errors for each token, or ?OK? for
tokens with no error.
? Multi-class classification: one of the labels
specifying the error type for the token (termi-
nology, mistranslation, missing word, etc.) in
Figure 4, or ?OK? for tokens with no error.
As training data, we provide tokenised transla-
tion output for all language pairs, human and ma-
chine translations, with tokens annotated with all
issue types listed above, or ?OK?. The annotation
was performed manually by professional transla-
tors as part of the QTLaunchPad project. For
the coarser variants, fine-grained errors are gen-
eralised to Accuracy or Fluency, or ?bad? for the
binary variant. The amount of available training
data varies by language pair:
# Source sentences # Target sentences
1,957 English 1,957 Spanish
715 English 715 German
350 German 350 English
900 Spanish 900 English
13
http://www.qt21.eu/launchpad/content/
training
26
Figure 4: MQM metric as error typology.
As test data, we provide additional data points
for all language pairs, human and machine trans-
lations:
# Source sentences # Target sentences
382 English 382 Spanish
150 English 150 German
100 German 100 English
150 Spanish 150 English
In contrast to Tasks 1.1?1.3, no baseline feature
set is provided to the participants.
Similar to last year (Bojar et al., 2013), the
word-level task is primarily evaluated by macro-
averaged F-measure (in %). Because the class dis-
tribution is skewed ? in the test data about 78% of
the tokens are marked as ?OK? ? we compute pre-
cision, recall, and F
1
for each class individually,
weighting F
1
scores by the frequency of the class
in the test data. This avoids giving undue impor-
tance to less frequent classes. Consider the follow-
ing confusion matrix for Level 1 annotation, i.e.
the three classes (O)K, (F)luency, and (A)ccuracy:
reference
O F A
predicted
O 4172 1482 193
F 1819 1333 214
A 198 133 69
For each of the three classes we assume a binary
setting (one-vs-all) and derive true-positive (tp),
false-positive (fp), and false-negative (fn) counts
from the rows and columns of the confusion ma-
trix as follows:
tp
O
= 4172
fp
O
= 1482 + 193 = 1675
fn
O
= 1819 + 198 = 2017
tp
F
= 1333
fp
F
= 1819 + 214 = 2033
fn
F
= 1482 + 133 = 1615
tp
A
= 69
fp
A
= 198 + 133 = 331
fn
A
= 193 + 214 = 407
We continue to compute F
1
scores for each
class c ? {O,F,A}:
precision
c
= tp
c
/(tp
c
+ fp
c
)
recall
c
= tp
c
/(tp
c
+ fn
c
)
F
1,c
=
2 ? precision
c
? recall
c
precision
c
+recall
c
yielding:
precision
O
= 4172/(4172 + 1675) = 0.7135
recall
O
= 4172/(4172 + 2017) = 0.6741
F
1,O
=
2 ? 0.7135 ? 0.6741
0.7135 + 0.6741
= 0.6932
? ? ?
F
1,F
= 0.4222
F
1,A
= 0.1575
Finally, we compute the average of F
1,c
scores
weighted by the occurrence count N(c) of c:
weightedF
1,ALL
=
1
?
c
N(c)
?
c
N
c
? F
1,c
weightedF
1,ERR
=
1
?
c:c 6=O
N(c)
?
c:c 6=O
N
c
? F
1,c
27
which for the above example gives:
weightedF
1,ALL
=
1
6189 + 2948 + 476
?
(6189 ? 0.6932 + 2948 ? 0.4222
+476 ? 0.1575) = 0.5836
weightedF
1,ERR
=
1
2948 + 476
?
(2948 ? 0.4222 + 476 ? 0.1575)
= 0.3854
We choose F
1,ERR
as our primary evaluation mea-
sure because it most closely mimics the common
application of F
1
scores in binary classification:
one is interested in the performance in detecting a
positive class, which in this case would be erro-
neous words. This does, however, ignore the num-
ber of correctly classified words of the OK class,
which is why we also report F
1,ALL
. In addition,
we follow Powers (2011) and report Matthews
Correlation Coefficient (MCC), averaged in the
same way as F
1
, as our secondary metric. Finally,
for contrast we also report Accuracy (ACC).
4.3 Participants
Table 10 lists all participating teams. Each team
was allowed up to two submissions for each task
and language pair. In the descriptions below, par-
ticipation in specific tasks is denoted by a task
identifier: T1.1, T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.2,
T1.3): QUEST is used to extract 17 system-
independent features from source and trans-
lation sentences and parallel corpora (same
features as in the WMT12 shared task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? language model (LM) probability of
source and target sentences based on
models for the WMT News Commen-
tary corpus.
? average number of translations per
source word in the sentence as given by
IBM Model 1 extracted from the WMT
News Commentary parallel corpus, and
thresholded so that P (t|s) > 0.2, or
so that P (t|s) > 0.01 weighted by the
inverse frequency of each word in the
source side of the parallel corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source language
extracted from the WMT News Com-
mentary corpus.
? percentage of unigrams in the source
sentence seen in the source side of the
WMT News Commentary corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within
the SCIKIT-LEARN toolkit. The ?,  and C
parameters were optimised via grid search
with 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as ?baseline?, it is in fact a strong
system. It has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting various forms of post-
editing effort (Callison-Burch et al., 2012;
Bojar et al., 2013).
DCU (T1.1): DCU-MIXED and DCU-SVR use
a selection of features available in QUEST,
such as punctuation statistics, LM perplex-
ity, n-gram frequency quartile statistics and
coarse-grained POS frequency ratios, and
four additional feature types: combined POS
and stop word LM features, source-side
pseudo-reference features, inverse glass-box
features for translating the translation and er-
ror grammar parsing features. For machine
learning, the QUEST framework is expanded
to combine logistic regression and support
vector regression and to handle cross- valida-
tion and randomisation in a way that training
items with the same source side are kept to-
gether. External resources are monolingual
corpora taken from the WMT 2014 transla-
tion task for LMs, the MT system used for the
inverse glass-box features (Li et al., 2014b)
and, for error grammar parsing, the Penn-
Treebank and an error grammar derived from
it (Foster, 2007).
28
ID Participating team
DCU Dublin City University Team 1, Ireland (Hokamp et al., 2014)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis,
2014)
FBK-UPV-UEDIN Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia,
Spain & University of Edinburgh, UK (Camargo de Souza et al., 2014)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al., 2014)
LIMSI Laboratoire d?Informatique pour la M?ecanique et les Sciences de l?Ing?enieur,
France (Wisniewski et al., 2014)
MULTILIZER Multilizer, Finland
RTM-DCU Dublin City University Team 2, Ireland (Bicici and Way, 2014)
SHEF-lite University of Sheffield Team 1, UK (Beck et al., 2014)
USHEFF University of Sheffield Team 2, UK (Scarton and Specia, 2014)
YANDEX Yandex, Russia
Table 10: Participants in the WMT14 Quality Estimation shared task.
DFKI (T1.2): DFKI/SVR builds upon the base-
line system (above) by adding non-redundant
data from the WMT13 task for predicting
the same label (HTER) and additional fea-
tures such as (a) rule-based language cor-
rections (language tool) (b), PCFG parsing
statistics and counts of tree labels, (c) po-
sition statistics of parsing labels, (d) posi-
tion statistics of trigrams with low probabil-
ity. DFKI/SVRxdata uses a similar setting,
with the addition of more training data from
non-minimally post-edited translation out-
puts (references), filtered based on a thresh-
old on the edit distance between the MT out-
put and the freely-translated reference.
FBK-UPV-UEDIN (T1.2, T1.3, T2): The sub-
missions for the word-level task (T2) use fea-
tures extracted from word posterior probabil-
ities and confusion network descriptors com-
puted over the 100k-best hypothesis transla-
tions generated by a phrase-based SMT sys-
tem. They also use features from word lexi-
cons, and POS tags of each word for source
and translation sentences. The predictions of
the Binary model are used as a feature for the
Level 1 and Multi-class settings. Both condi-
tional random fields (CRF) and bidirectional
long short-term memory recurrent neural net-
works (BLSTM-RNNs) are used for the Bi-
nary setting, and BLSTM-RNNs only for the
Level 1 and Multi-class settings.
The sentence-level QE submissions (T1.2
and T1.3) are trained on black-box features
extracted using QUEST in addition to fea-
tures based on word alignments, word poste-
rior probabilities and diversity scores (Souza
et al., 2013). These features are computed
over 100k-best hypothesis translations also
used for task 2. In addition, a set of ratios
computed from the word-level predictions of
the model trained on the binary setting of
task 2 is used. A total of 221 features and
the extremely randomised trees (Geurts et al.,
2006) learning algorithm are used to train re-
gression models.
LIG (T2): Conditional Random Fields classi-
fiers are trained with features used in LIG?s
WMT13 systems (Luong et al., 2013): tar-
get and source words, alignment informa-
tion, source and target alignment context,
LM scores, target and source POS tags,
lexical categorisations (stopword, punctua-
tion, proper name, numerical), constituent
label, depth in the constituent tree, target
polysemy count, pseudo reference. These
are combined with novel features: word
occurrence in multiple translation systems
and POS tag-based LM scores (longest tar-
get/source n-gram length and backoff score
for POS tag). These features require external
NLP tools and resources such as: TreeTag-
ger, GIZA++, Bekerley parser, Link Gram-
mar parser, WordNet and BabelNet, Google
Translate (pseudo-reference). For the binary
task, the optimal classification threshold is
tuned based on a development set split from
the original training set. Feature selection is
employed over the all features (for the binary
29
task only), with the Sequential Backward Se-
lection algorithm. The best performing fea-
ture set is then also used for the Level 1 and
Multi-class variants.
LIMSI (T2): The submission relies on a ran-
dom forest classifier and considers only 16
dense and continuous features. To prevent
sparsity issues, lexicalised information such
as the word or the previous word identities
is not included. The features considered are
mostly classic MT features and can be cat-
egorised into two classes: association fea-
tures, which describe the quality of the as-
sociation between the source sentence and
each target word, and fluency features, which
describe the ?quality? of the translation hy-
potheses. The latter rely on different lan-
guage models (either on POS or on words)
and the former on IBM Model 1 translation
probabilities and on pseudo- references, i.e.
translation produced by an independent MT
system. Random forests are known to per-
form well in tasks like this one, in which
only a few dense and continuous features are
available, possibly because of their ability to
take into account complex interactions be-
tween features and to automatically partition
the continuous feature values into a discrete
set of intervals that achieves the best classifi-
cation performance. Since they predict the
class probabilities, it is possible to directly
optimize the F
1
score during training by find-
ing, with a grid search method, the decision
threshold that achieved the best F
1
score on
the training set.
MULTILIZER (T1.2, T1.3): The 80 black-box
features from QUEST are used in addition to
new features based on using other MT en-
gines for forward and backward translations.
In forward translations, the idea is that dif-
ferent MT engines make different mistakes.
Therefore, when several forward translations
are similar to each other, these translations
are more likely to be correct. This is con-
firmed by the Pearson correlation of similar-
ities between the forward translations against
the true scores (above 0.5). A backward
translation is very error-prone and therefore
it has to be used in combination with for-
ward translations. A single back-translation
similar to original source segment does not
bring much information. Instead, when sev-
eral MT engines give back-translations simi-
lar to this source segment, one can conclude
that the translation is reliable. Those transla-
tions where similarities both in forward trans-
lation and backward translation are high are
intuitively more likely to be good. A simple
feature selection method that omits all fea-
tures with Pearson correlation against the true
scores below 0.2 is used. The systems sub-
mitted are obtained using linear regression
models.
RTM-DCU (T1.1, T1.2, T1.3, T2): RTM-DCU
systems are based on referential translation
machines (RTM) (Bic?ici, 2013) and parallel
feature decay algorithms (ParFDA5) (Bic?ici
et al., 2014), which allow language and MT
system-independent predictions. For each
task, individual RTM models are developed
using the parallel corpora and the language
model corpora distributed by the WMT14
translation task and the language model cor-
pora provided by LDC for English and Span-
ish. RTMs use 337 to 437 sentence-level fea-
tures for coverage and diversity, IBM1 and
sentence translation performance, retrieval
closeness and minimum Bayes retrieval risk,
distributional similarity and entropy, IBM2
alignment, character n-grams, sentence read-
ability, and parse output tree structures. The
features use ngrams defined over text or com-
mon cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over
which similarity calculations are performed.
Learning models include ridge regression
(RR), support vector machines (SVR), and
regression trees (TREE), which are applied
after partial least squares (PLS) or feature
selection (FS). For word-level prediction,
generalised linear models (GLM) (Collins,
2002) and GLM with dynamic learning
(GLMd) (Bic?ici, 2013) are used with word-
level features including CCL links, word
length, location, prefix, suffix, form, context,
and alignment, totalling up to a couple of mil-
lion features.
SHEF-lite (T1.1, T1.2, T1.3): These submis-
sions use the framework of Multi-task Gaus-
sian Processes, where multiple datasets are
30
combined in a multi-task setting similar to
the one used by Cohn and Specia (2013).
For T1.1, data for all language pairs is put
together, and each language is considered a
task. For T1.2 and T1.3, additional datasets
from previous shared task years are used,
each encoded as a different task. For all tasks,
the QUEST framework is used to extract a set
of 80 black-box features (a superset of the 17
baseline features). To cope with the large size
of the datasets, the SHEF-lite-sparse submis-
sion uses Sparse Gaussian Processes, which
provide sensible sparse approximations using
only a subset of instances (inducing inputs)
to speed up training and prediction. For this
?sparse? submission, feature selection is per-
formed following the approach of Shah et al.
(2013) by ranking features according to their
learned length-scales and selecting the top 40
features.
USHEFF (T1.1, T1.2, T1.3): USHEFF submis-
sions exploit the use of consensus among
MT systems by comparing the MT sys-
tem output to several alternative translations
generated by other MT systems (pseudo-
references). The comparison is done using
standard evaluation metrics (BLEU, TER,
METEOR, ROUGE for all tasks, and two
metrics based on syntactic similarities from
shallow and dependency parser information
for T1.2 and T1.3). Figures extracted from
such metrics are used as features to com-
plement prediction models trained on the 17
baseline features. Different from the standard
use of pseudo-reference features, these fea-
tures do not assume that the alternative MT
systems are better than the system of inter-
est. A more realistic scenario is considered
where the quality of the pseudo-references is
not known. For T1, no external systems in
addition to those provided for the shared task
are used: for a given translation, all alter-
native translations for the same source seg-
ment (two or three, depending on the lan-
guage pair) are used as pseudo-references.
For T1.2 and T1.3, for each source sentence,
all alternative translations produced by MT
systems on the same data (WMT12/13) are
used as pseudo-references. The hypothesis
is that by using translations from several MT
systems one can find consensual information
and this can smooth out the effect of ?coinci-
dences? in the similarities between systems?
translations. SVM regression with radial ba-
sis function kernel and hyper-parameters op-
timised via grid search is used to build the
models.
YANDEX (T1.1): Both submissions are based
on the the 80 black-box features, plus an
LM score from a larger language model,
a pseudo-reference, and several additional
features based on POS tags and syntactic
parsers. The first attempt uses an extract
of the top 5 features selected with a greedy
search from the set of all features. SVM re-
gression is used as machine learning algo-
rithm. The second attempt uses the same
features processed with Yandex? implemen-
tation of the gradient tree boosting (Ma-
trixNet).
4.4 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing effort
Table 11 summarises the results for the ranking
variant of Task 1.1. They are sorted from best to
worst using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are as follows: for English-Spanish
it is RTM-DCU/RTM-TREE, with a DeltaAvg
score of 0.26; for Spanish-English it is USH-
EFF, with a DeltaAvg score of 0.23; for English-
German it is again RTM-DCU/RTM-TREE, with a
DeltaAvg score of 0.39; and for German-English it
is RTM-DCU/RTM-RR, with a DeltaAvg score of
0.38. These winning submissions are better than
the baseline system by a large margin, which indi-
cates that current best performance in MT quality
estimation has reached levels that are clearly be-
yond what the baseline system can produce. As for
the other systems, according to DeltaAvg, com-
pared to the previous year results a smaller per-
centage of systems is able to beat the baseline.
This might be a consequence of the use of the met-
ric for the prediction of only three discrete labels.
The results for the scoring task are presented in
Table 12, sorted from best to worst using the MAE
31
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.26 0.38
? RTM-DCU/RTM-TREE 0.26 0.41
? YANDEX/SHAD BOOSTEDTREES2 0.23 0.35
USHEFF 0.21 0.33
SHEFF-lite 0.21 0.33
YANDEX/SHAD SVR1 0.18 0.29
SHEFF-lite-sparse 0.17 0.27
Baseline SVM 0.14 0.22
Spanish-English
? USHEFF 0.23 0.30
? RTM-DCU/RTM-PLS-RR 0.20 0.35
? RTM-DCU/RTM-FS-RR 0.19 0.36
Baseline SVM 0.12 0.21
SHEFF-lite-sparse 0.12 0.17
SHEFF-lite 0.11 0.15
English-German
? RTM-DCU/RTM-TREE 0.39 0.54
RTM-DCU/RTM-PLS-TREE 0.33 0.42
USHEFF 0.26 0.41
SHEFF-lite 0.26 0.36
Baseline SVM 0.23 0.34
SHEFF-lite-sparse 0.23 0.33
German-English
? RTM-DCU/RTM-RR 0.38 0.51
? RTM-DCU/RTM-PLS-RR 0.35 0.45
USHEFF 0.28 0.30
SHEFF-lite 0.24 0.27
Baseline SVM 0.21 0.25
SHEFF-lite-sparse 0.14 0.17
Table 11: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
32
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.49 0.61
? SHEFF-lite 0.49 0.63
? USHEFF 0.49 0.63
? SHEFF-lite/sparse 0.49 0.69
? RTM-DCU/RTM-TREE 0.49 0.61
Baseline SVM 0.52 0.66
YANDEX/SHAD BOOSTEDTREES2 0.56 0.68
YANDEX/SHAD SVR1 0.64 0.81
DCU-Chris/SVR 0.66 0.88
DCU-Chris/MIXED 0.94 1.14
Spanish-English
? RTM-DCU/RTM-FS-RR 0.53 0.64
? SHEFF-lite/sparse 0.54 0.69
? RTM-DCU/RTM-PLS-RR 0.55 0.71
USHEFF 0.57 0.67
Baseline SVM 0.57 0.68
SHEFF-lite 0.62 0.77
DCU-Chris/MIXED 0.65 0.91
English-German
? RTM-DCU/RTM-TREE 0.58 0.68
RTM-DCU/RTM-PLS-TREE 0.60 0.71
SHEFF-lite 0.63 0.74
USHEFF 0.64 0.75
SHEFF-lite/sparse 0.64 0.75
Baseline SVM 0.64 0.76
DCU-Chris/MIXED 0.69 0.98
German-English
? RTM-DCU/RTM-RR 0.55 0.67
? RTM-DCU/RTM-PLS-RR 0.57 0.74
USHEFF 0.63 0.76
SHEFF-lite 0.65 0.77
Baseline SVM 0.65 0.78
Table 12: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
33
metric scores as primary key and the RMSE metric
scores as secondary key.
The winning submissions for the scoring variant
of Task 1.1 are as follows: for English-Spanish it
is RTM-DCU/RTM-TREE with a MAE of 0.49;
for Spanish-English it is RTM-DCU/RTM-FS-
RR with a MAE of 0.53; for English-German
it is again RTM-DCU/RTM-TREE, with a MAE
of 0.58; and for German-English it is RTM-
DCU/RTM-RR with a MAE of 0.55. These sub-
missions are again much better than the baseline
system, which under the scoring variant seems
to perform at a middle-of-the-pack level or lower
compared to the overall pool of submissions.
Overall, more systems are able to outperform the
baseline according to the scoring metric.
The top system for most language pairs are
essentially based on the same core techniques
(RTM-DCU) according to both the DeltaAvg and
MAE metrics. The ranking of other systems, how-
ever, can be substantially different according to the
two metrics.
Task 1.2 Predicting percentage of edits
Table 13 summarises the results for the ranking
variant of Task 1.2. For readability purposes we
have used a multiplication-factor of 100 in the
scoring script, which makes the HTER numbers
(both predicted and gold) to be in the [0, 100]
range. They are sorted from best to worst using
the DeltaAvg metric scores as primary key and the
Spearman?s rank correlation scores as secondary
key.
The winning submission for the ranking vari-
ant of Task 1.2 is RTM-DCU/RTM-SVR, with a
DeltaAvg score of 9.31. There is a large mar-
gin between this score and the baseline score of
DeltaAvg 5.08, which indicates again that current
best performance has reached levels that are much
beyond what this baseline system can produce.
The vast majority of the submissions perform bet-
ter than the baseline (the only exception is the sub-
mission from SHEFF-lite, for which the authors
report a major issue with the learning algorithm).
The results for the scoring variant are presented
in Table 14, sorted from best to worst by using the
MAE metric scores as primary key and the RMSE
metric scores as secondary key.
The winning submission for the scoring variant
of Task 1.2 is FBK-UPV-UEDIN/WP with a MAE
of 12.89, while the baseline system has a MAE
of 15.23. Most of the submissions perform better
than the baseline.
Task 1.3 Predicting post-editing time
Table 15 summarises the results for the ranking
variant of Task 1.3. For readability purposes, we
have used a multiplication-factor of 0.001 in the
scoring script, which makes the time (both pre-
dicted and gold) to be measured in seconds. They
are sorted from best to worst using the DeltaAvg
metric scores as primary key and the Spearman?s
rank correlation scores as secondary key.
The winning submission for the ranking vari-
ant of Task 1.3 is RTM-DCU/RTM-RR, with a
DeltaAvg score of 17.02 (when predicting sec-
onds). The interesting aspect of these results is
that the DeltaAvg numbers have a direct real-
world interpretation, in terms of time spent (or
saved, depending on one?s view-point) for post-
editing machine-produced translations. A more
elaborate discussion on this point can be found in
Section 4.5.
The winning submission for the scoring variant
of Task 1.3 is RTM-DCU/RTM-SVR, with a MAE
of 16.77. Note that all of the submissions perform
significantly better than the baseline, which has a
MAE of 21.49, and that the majority is not signif-
icantly worse than the top scoring submission.
Task 2 Predicting word-level edits
The results for Task 2 are summarised in Tables
17?19. The results are ordered by F
1
score for
the Error (BAD) class. For comparison, two triv-
ial baselines are included, one that marks every
word as correct and that marks every word with
the most common error class found in the training
data. Both baselines are clearly useless for any ap-
plication, but help put the results in perspective.
Most teams submitted systems for a single lan-
guage pair: English-Spanish; only a single team
produced predictions for all four pairs.
Table 17 gives the results of the binary (OK vs.
BAD) classification variant of Task 2. The win-
ning submissions for this variant are as follows:
for English-Spanish it is FBK-UPV-UEDIN/RNN
with a weighted F
1
of 48.73; for Spanish-
English it is RTM-DCU/RTM-GLMd with a
weighted F
1
of 29.14; for English-German it is
RTM-DCU/RTM-GLM with a weighted F
1
of
45.30; and for German-English it is again RTM-
DCU/RTM-GLM with a weighted F
1
of 26.13.
Remarkably, for three out of four language
pairs, the systems fail to beat our trivial baseline of
34
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-SVR 9.31 0.53
? RTM-DCU/RTM-TREE 8.57 0.48
? USHEFF 7.93 0.45
SHEFF-lite/sparse 7.69 0.43
Baseline 5.08 0.31
SHEFF-lite 0.72 0.09
Table 13: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (100k times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
System ID MAE RMSE
English-Spanish
? FBK-UPV-UEDIN/WP 12.89 16.74
? RTM-DCU/RTM-SVR 13.40 16.69
? USHEFF 13.61 17.84
RTM-DCU/RTM-TREE 14.03 17.48
DFKI/SVR 14.32 17.74
FBK-UPV-UEDIN/NOWP 14.38 18.10
SHEFF-lite/sparse 15.04 18.38
MULTILIZER 15.04 20.86
Baseline 15.23 19.48
DFKI/SVRxdata 16.01 19.52
SHEFF-lite 18.15 23.41
Table 14: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-RR 17.02 0.68
? RTM-DCU/RTM-SVR 16.60 0.67
SHEFF-lite/sparse 16.33 0.63
SHEFF-lite 16.08 0.64
USHEFF 14.98 0.59
Baseline 14.71 0.57
Table 15: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
35
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-SVR 16.77 26.17
?MULTILIZER/MLZ2 17.07 25.83
? SHEFF-lite 17.13 27.33
?MULTILIZER/MLZ1 17.31 25.51
? SHEFF-lite/sparse 17.42 27.35
? FBK-UPV-UEDIN/WP 17.48 25.31
RTM-DCU/RTM-RR 17.50 25.97
FBK-UPV-UEDIN/NOWP 18.69 26.58
USHEFF 21.48 34.28
Baseline 21.49 34.28
Table 16: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
weighted F
1
F
1
System ID All Bad ? MCC ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 64.38
Baseline (always Bad) 18.71 52.53 0.00 35.62
? FBK-UPV-UEDIN/RNN 62.00 48.73 18.23 61.62
LIMSI/RF 60.55 47.32 15.44 60.09
LIG/FS 63.55 44.47 19.41 64.67
LIG/BL ALL 63.77 44.11 19.91 65.12
FBK-UPV-UEDIN/RNN+tandem+crf 62.17 42.63 16.32 63.26
RTM-DCU/RTM-GLM 60.68 35.08 13.45 63.74
RTM-DCU/RTM-GLMd 60.24 32.89 12.98 63.97
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 82.37
Baseline (always Bad) 5.28 29.98 0.00 17.63
? RTM-DCU/RTM-GLMd 79.54 29.14 25.47 82.98
RTM-DCU/RTM-GLM 79.42 26.91 25.93 83.43
English-German
Baseline (always OK) 59.39 0.00 0.00 71.33
Baseline (always Bad) 12.78 44.57 0.00 28.67
? RTM-DCU/RTM-GLM 71.51 45.30 28.61 72.97
RTM-DCU/RTM-GLMd 68.73 36.91 21.32 71.41
German-English
Baseline (always OK) 67.82 0.00 0.00 77.60
Baseline (always Bad) 8.20 36.60 0.00 22.40
? RTM-DCU/RTM-GLM 72.41 26.13 16.08 76.14
RTM-DCU/RTM-GLMd 71.42 22.97 12.63 75.46
Table 17: Official results for the binary part of the WMT14 Quality Evaluation Task 2. The winning submissions are indicated
by a ?. All values are given as percentages.
36
marking all the words as wrong. This may either
indicate that the predictions themselves are of low
quality or the chosen evaluation approach is mis-
leading. On the other hand F
1
scores are a com-
mon measure of binary classification performance
and no averaging is performed here.
Table 18 gives the results of the Level 1
classification (OK, Fluency, Accuracy) variant
of Task 2. Here the second baseline is to
always predict Fluency errors, as this is the
most common error category in the training
data. The winning submissions of this vari-
ant are as follows: for English-Spanish it
is FBK-UPV-UEDIN/RNN+tandem+crf with a
weighted F
1
of 23.94 and for Spanish-English,
English-German, and German-English it is RTM-
DCU/RTM-GLMd with weighted F
1
scores of
23.94, 21.94, and 8.57 respectively.
As before, all systems fail to outperform the
single-class baseline for the Spanish-English lan-
guage pair according to our primary metric. How-
ever, for Spanish-English and English-German
both submissions are able to beat the baseline by
large margin. We also observe that the absolute
numbers vary greatly between language pairs.
Table 19 gives the results of the Multi-class
classification variant of Task 2. Again, the sec-
ond baseline is to always predict the most common
error category in the training data, which varies
depending on language pair and produces and in-
creasingly weak baseline as the number of classes
rises.
The winning submissions of this variant are
as follows: for English-Spanish, Spanish-English,
and English-German it is RTM-DCU/RTM-GLM
with weighted F
1
scores of 26.84, 8.75, and 15.02
respectively and and for German-English it is
RTM-DCU/RTM-GLMd with a weighted F
1
of
3.08. Not only do these systems perform above
our baselines for all but the German-English lan-
guage pair, they also outperform all other sub-
missions for English-Spanish. Remarkably, RTM-
DCU/RTM-GLM wins English-Spanish for all of
the proposed metrics by a sizeable margin.
4.5 Discussion
In what follows, we discuss the main accomplish-
ments of this year?s shared task starting from the
goals we had previously identified for it.
Investigating the effectiveness of different
quality labels
For the sentence-level tasks, the results of this
year?s shared task allow us to investigate the ef-
fectiveness of predicting translation quality using
three very different quality labels: perceived post-
editing effort on a scale of [1-3] (Task 1.1); HTER
scores (Task 1.2); and the time that a translator
takes to post-edit the translation (Task 1.3). One of
the ways one can compare the effectiveness across
all these different labels is to look at how well
the models can produce predictions that correlate
with the gold label that we have at our disposal.
A measure of correlation that does not depend
on the value of the labels is Spearman?s ranking
correlation. From this perspective, the label that
seems the most effective appears to be post-editing
time (Task 1.3), with the best system (RTM-
DCU/RTM-RR) producing a Spearman?s ? of 0.68
(English-Spanish translations, see Table 15). In
comparison, when perceived post-editing effort la-
bels are used (Task 1.1), the best systems achieve
a Spearman?s ? of 0.38 and 0.30 for English-
Spanish and Spanish-English translations, respec-
tively, and ? of 0.54 and 0.51 for English-German
and German-English, respectively (Table 11); for
HTER scores (Task 1.2) the best systems achieve
a Spearman?s ? of 0.53 for English-Spanish trans-
lations (Table 13).
This comparison across tasks seems to indicate
that, among the three labels we have proposed,
post-editing time seems to be the most learnable,
in the sense that automatic predictions can vest
match the gold labels (in this case, with respect
to the rankings they induce). A possible reason
for this is that post-editing time correlates with the
length of the source sentence whereas HTER is a
normalised measure.
Compared to the results regarding time predic-
tion in the Quality Evaluation shared task from
2013 (Bojar et al., 2013), we note that this time
all submissions were able to beat the baseline sys-
tem (compared to only 1/3 of the submissions in
2013). In addition, better handling of the data
acquisition reduced the number of outliers in this
year?s dataset allowing for numbers that are more
reliably interpretable. As an example of its in-
terpretability, consider the following: the winning
submission for the ranking variant of Task 1.3 is
RTM-DCU/RTM-RR, with a a Spearman?s ? of
0.68 and a DeltaAvg score of 17.02 (when predict-
37
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always fluency) 14.39 40.41 0.00 0.00 30.67
? FBK-UPV-UEDIN/RNN+tandem+crf 58.36 38.54 16.63 13.89 57.98
FBK-UPV-UEDIN/RNN 60.32 37.25 18.22 15.51 61.75
LIG/BL ALL 58.97 31.79 14.95 11.48 61.13
LIG/FS 58.95 31.78 14.92 11.46 61.10
RTM-DCU/RTM-GLMd 58.23 26.62 12.60 12.76 62.94
RTM-DCU/RTM-GLM 56.47 29.91 8.11 7.96 58.56
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always fluency) 2.67 15.13 0.00 0.00 12.24
? RTM-DCU/RTM-GLMd 78.89 23.94 25.41 25.45 83.17
RTM-DCU/RTM-GLM 78.78 21.96 26.31 26.99 83.69
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always fluency) 3.83 13.35 0.00 0.00 14.82
? RTM-DCU/RTM-GLMd 64.58 21.94 17.69 15.92 69.26
RTM-DCU/RTM-GLM 64.43 21.10 16.99 14.93 69.34
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always fluency) 3.34 14.92 0.00 0.00 13.79
? RTM-DCU/RTM-GLMd 69.17 8.57 10.61 5.76 75.91
RTM-DCU/RTM-GLM 69.09 8.26 9.95 5.76 75.97
Table 18: Official results for the Level 1 classification part of the WMT14 Quality Evaluation Task 2. The winning submissions
are indicated by a ?. All values are given as percentages.
38
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always unintelligible) 7.93 22.26 0.00 0.00 21.99
? RTM-DCU/RTM-GLM 60.52 26.84 23.77 21.45 66.83
FBK-UPV-UEDIN/RNN+tandem+crf 52.96 23.07 15.17 10.74 52.13
LIG/BL ALL 56.66 20.50 18.56 13.39 60.39
LIG/FS 56.66 20.50 18.56 13.39 60.39
FBK-UPV-UEDIN/RNN 52.84 17.09 7.66 4.24 57.18
RTM-DCU/RTM-GLMd 51.87 3.22 10.16 4.04 64.42
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always word order) 0.34 1.96 0.00 0.00 4.24
? RTM-DCU/RTM-GLM 76.34 8.75 19.82 13.43 83.27
RTM-DCU/RTM-GLMd 76.21 8.19 19.35 15.32 83.17
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always mistranslation) 2.48 8.66 0.00 0.00 11.78
? RTM-DCU/RTM-GLM 63.57 15.02 17.57 15.08 70.82
RTM-DCU/RTM-GLMd 63.33 12.48 18.70 13.20 71.45
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always word order) 1.56 6.96 0.00 0.00 9.23
? RTM-DCU/RTM-GLMd 67.62 3.08 7.19 1.48 74.73
RTM-DCU/RTM-GLM 67.86 2.36 7.55 1.79 75.75
Table 19: Official results for the Multi-class classification part of the WMT14 Quality Evaluation Task 2. The winning
submissions are indicated by a ?. All values are given as percentages.
39
ing seconds). This number has a direct real-world
interpretation: using the order proposed by this
system, a human translator would spend, on av-
erage, about 17 seconds less on a sentence taken
from the top of the ranking compared to a sen-
tence picked randomly from the set.
14
To put this
number into perspective, for this dataset the av-
erage time to complete a sentence post-editing is
39 seconds. As such, one has an immediate inter-
pretation for the usefulness of using such a rank-
ing: translating around 100 sentences taken from
the top of the rankings would take around 36min
(at about 22 seconds/sentence), while translating
the same number of sentences extracted randomly
from the same dataset would take around 1h5min
(at about 39 seconds/sentence). It is in this sense
that we consider post-editing time an interpretable
label.
Another desirable property of label predictions
is usefulness; this property, however, it highly
task-dependent and therefore cannot be judged in
the absence of a specific task. For instance, an in-
terpretable label like post-editing time may not be
that useful in a task the requires one to place the
machine translations into ?ready to publish? and
?not ready to publish? bins. For such an appli-
cation, labels such as the ones used by Task 1.1
are clearly more useful, and also very much inter-
pretable within the scope of the task. Our attempt
at presenting the Quality Prediction task with a va-
riety of prediction labels illustrates a good range
of properties for the proposed labels and enables
one to draw certain conclusions depending on the
needs of the specific task at hand.
For the word-level tasks, different quality labels
equate with using different levels of granularity for
the predictions, which we discuss next.
Exploring word-level quality prediction at
different levels of granularity
Previous work on word-level predictions, e.g. (Bo-
jar et al., 2013) has focused on prediction of auto-
matically derived labels, generally due to practical
considerations as the manual annotation is labour
intensive. While easily applicable, automatic an-
notations, using for example TER alignment be-
tween the machine translation and reference (or
post-edition), face the same problems as automatic
14
Note that the 17.02 seconds figure is a difference in real-
time, not predicted time; what is considered in this variant of
Task 1.3 is only the predicted ranking of data points, not the
absolute values of the predictions.
MT evaluation metrics as they fail to account for
different word choices and lack the ability to re-
liably distinguish meaning preserving reorderings
from those that change the semantics of the out-
put. Furthermore, previous automatic annotation
for word-level quality estimation has focused on
binary labels: correct / incorrect, or at most, the
main edit operations that can be captured by align-
ment metrics like TER: correct, insertion, dele-
tion, substitution.
In this year?s task we were able to provide
manual fine-grained annotations at the word-level
produced by humans irrespective of references or
post-editions. Error categories range from fre-
quent ones, such as unintelligible, mistranslation,
and terminology, to rare ones such as additions or
omissions. For example, only 10 out of more than
3,400 errors in the English-Spanish test set fall
into the latter categories, while over 2,000 words
are marked as unintelligible. By hierarchically
grouping errors into coarser categories we aimed
to find a compromise between data sparsity and
the expressiveness of the labels. What marks a
good compromise depends on the use case, which
we do not specify here, and the quality of the finer
grained predictions: if a system is able to predict
even rare errors these may be grouped later if nec-
essary.
Overall, word-level error prediction seems to re-
main a challenging task as evidenced by the fact
that many submissions were unable to beat a triv-
ial baseline. We hypothesise that this is at least
partially due to a mismatch in loss-functions used
in training and testing. We know from the sys-
tem descriptions that some systems were tuned to
optimise squared error or accuracy, while evalua-
tion was performed using weighted F
1
scores. On
the other hand, even a comparison of just accuracy
shows that systems struggle to obtain a lower error
rates than the ?all-OK? baseline.
Such performance problems are consistent over
the three levels of granularity, contrary to the in-
tuition that binary classification would be easier.
A notable exception is the RTM-DCU/RTM-GLM
system, which is able to beat both the baseline and
all other systems on the Multi-Class variant of the
English-Spanish task ? cf. Table 19 ? with regard
to all metrics. For this and most other submis-
sions we observe that labels are not consistent for
different granularities, i.e. at token marked with a
specific error in the multi-class variant may still
40
carry an ?OK? label in binary annotation. Thus,
additional coarse grained annotations may be de-
rived by automatic means. For example, mapping
the multi-class predictions of the above system to
coarser categories improves the F
1,ERR
score in
Table 17 from 35.08 to 37.02 but does not change
the rank with respect to the other entries.
The fact that coarse grained predictions seem
not to be derived from the fine-grained ones leads
us to believe that most participants treated the
different granularities as independent classifica-
tion tasks. The FBK-UPV-UEDIN team trans-
fers information in the opposite direction by using
their binary predictions as features for Level-1 and
multi-class.
Given the current quality of word-level predic-
tion it remains unclear if these systems can already
be employed in a practical setting, e.g. to focus the
attention of post-editors.
Studying the effects of training and test
datasets with mixed domains, language pairs
and MT systems
This year?s shared task made available datasets for
more than one language pair with the same or dif-
ferent types of annotation, 2-3 multiple MT sys-
tems (plus a human translation) per language pair,
and out-of-domain test data (Tasks 1.1 and 2). In-
stances for each language pair were kept in sep-
arate datasets and thus the ?language pair? vari-
able can be analysed independently. However, for
a given language pair, datasets mix translation sys-
tems (and humans) in Task 1.1, and also text do-
mains in Task 2.
Directly comparing the performance across lan-
guage pairs is not possible, given that their
datasets have different numbers of instances (pro-
duced by 3 or 4 systems) and/or different true
score distributions (see Figure 3). For a relative
comparison (although not all systems submitted
results for all language pairs, which is especially
true in Task 2), we observe in Task 1.1 that for all
language pairs generally at least half of the sys-
tems did better than the baseline. To our surprise,
only one submission combined data for multiple
languages together for Task 1.1: SHEF-lite, treat-
ing each language pair data as a different task in
a multi-task learning setting. However, only for
the ?sparse? variant of the submission significant
gains were reported over modelling each task in-
dependently (with the tasks still sharing the same
data kernel and the same hyperparameters).
The interpretation of the results for Task 2 is
very dependent on the evaluation metric used,
but generally speaking a large variation in per-
formance was found between different languages,
with English-Spanish performing the best, possi-
bly given the much larger number of training in-
stances. Data for Task 2 also presented varied true
score distributions (as shown by the performance
of the baseline (e.g. always ?OK?) in Tables 17-
19.
One of the main goals with Task 1.1 (and Task 2
to some extent) was to test the robustness of mod-
els in a blind setting where multiple MT systems
(and human translations) are put together and their
identifiers are now known. All submissions for
these tasks were therefore translation system ag-
nostic, with no submission attempting to perform
meta-identification of the origins of the transla-
tions. For Task 1.1, data from multiple MT sys-
tems was explicitly used by USHEFF though the
idea of consensus translations. Translations from
all but the system of interest for the same source
segment were used as pseudo-references. The
submission significantly outperformed the base-
line for all language pairs and did particularly well
for Spanish-English and English-Spanish.
An in depth analysis of Task 1.1?s datasets on
the difference in prediction performance between
models built and applied for individual transla-
tion systems and models built and tested for all
translations pooled together is presented in (Shah
and Specia, 2014). Not surprisingly, the former
models perform significantly better, with MAE
scores ranging between 0.35 and 0.5 for differ-
ent language pairs and MT systems, and signifi-
cantly lower scores for models trained and tested
on human translations only (MAE scores between
0.2 and 0.35 for different language pairs), against
MAE scores ranging between 0.5 and 0.65 for
models with pooled data.
For Tasks 1.2 and 1.3, two submissions included
English-Spanish data which had been produced by
yet different MT systems (SHEF-lite and DFKI).
While using these additional instances seemed at-
tractive given the small number of instances avail-
able for these tasks, it is not clear what their contri-
bution was. For example, with a reduced set of in-
stances (only 400) from the combined sets, SHEF-
lite/sparse performed significantly better than its
variant SHEF-lite.
Finally, with respect to out-of-domain (different
41
text domain and MT system) test data, for Task
1.1, none of the papers submitted included experi-
ments. (Shah and Specia, 2014) applied the mod-
els trained on pooled datasets (as explained above)
for each language pair to the out-of-domain test
sets. The results were surprisingly positive, with
average MAE score of 0.5, compared to the 0.5-
0.65 range for in-domain data (see above). Further
analysis is necessary to understand the reasons for
that.
In Task 2, the official training and test sets al-
ready include out-of-domain data because of the
very small amount of in-domain data available,
and thus is is hard to isolate the effect of this data
on the results.
Examining the effectiveness of quality
prediction methods on human translations
Datasets for Tasks 1.1 and 2 contain human trans-
lations, in addition to the automatic translations
from various MT systems. Predicting human
translation quality is an area that has been largely
unexplored. Previous work has looked into dis-
tinguishing human from machine translations (e.g.
(Gamon et al., 2005)), but this problem setting is
somehow artificial, and moreover arguably harder
to solve nowadays given the higher general qual-
ity of current MT systems (Shah and Specia,
2014). Although human translations are obviously
of higher quality in general, many segments are
translated by MT systems with the same or similar
levels of quality as human translation. This is par-
ticularly true for Task 2, since data had been pre-
viously categorised and only ?near misses? were
selected for the word-level annotation, i.e., human
and machine translations that were both nearly
perfect in this case.
While no distinction was made between human
and machine translations in our tasks, we believe
the mix of these two types of translations has had
a negative impact in prediction performance. Intu-
itively, one can expect errors in human translation
to be more subtle, and hence more difficult to cap-
ture via standard quality estimation features. For
example, an incorrect lexical choice (due to, e.g.,
ambiguity) which still fits the context and does not
make the translation ungrammatical is unlikely to
be captured. We hoped that participants would de-
sign features for this particular type of translation,
but although linguistically motivated features have
been exploited, they did not seem appropriate for
human translations.
It is interesting to mention the indirect use of
human translations by USHEFF for Tasks 1.1-1.3:
given a translation for a source segment, all other
translations for the same segment were used as
pseudo-references. Apart from when this transla-
tion was actually the human translation, the hu-
man translation was effectively used as a refer-
ence. While this reference was mixed with 2-
3 other pseudo-references (other machine transla-
tions) for the feature computations, these features
led to significant gains in performance over the
baseline features Scarton and Specia (2014).
We believe that more investigation is needed for
human translation quality prediction. Tasks ded-
icated to this type of data at both sentence- and
word-level in the next editions of this shared task
would be a possible starting point. The acquisi-
tion of such data is however much more costly, as
it is arguably hard to find examples of low quality
human translation, unless specific settings, such as
translation learner corpora, are considered.
5 Medical Translation Task
The Medical Translation Task addresses the prob-
lem of domain-specific and genre-specific ma-
chine translation. The task is split into two sub-
tasks: summary translation, focused on transla-
tion of sentences from summaries of medical ar-
ticles, and query translation, focused on transla-
tion of queries entered by users into medical infor-
mation search engines.
In general, texts of specific domains and gen-
res are characterized by the occurrence of special
vocabulary and syntactic constructions which are
rare or even absent in traditional (general-domain)
training data and therefore difficult for MT. Spe-
cific training data (containing such vocabulary and
syntactic constructions) is usually scarce or not
available at all. Medicine, however, is an exam-
ple of a domain for which in-domain training data
(both parallel and monolingual) is publicly avail-
able in amounts which allow to train a complete
SMT system or to adapt an existing one.
5.1 Task Description
In the Medical Translation Task, we provided links
to various medical-domain training resources and
asked participants to use the data to train or adapt
their systems to translate unseen test sets for both
subtasks between English and Czech (CS), Ger-
man (DE), and French (FR), in both directions.
42
The summary translation test data is domain-
specific, but otherwise can be considered as ordi-
nary sentences. On the other hand, the query trans-
lation test data is also specific for its genre (gen-
eral style) ? it contains short sequences of (more
or less) of independent terms rather than complete
and grammatical sentences, the usual target of cur-
rent MT systems.
Similarly to the standard Translation Task, the
participants of the Medical Translation Task were
allowed to use only the provided resources in the
constrained task (in addition to data allowed in
the constrained standard Translation Task), but
could exploit any additional resources in the un-
constrained task. The submissions were expected
with true letter casing and detokenized. The trans-
lation quality was measured using automatic eval-
uation metrics, manual evaluation was not per-
formed.
5.2 Test and Development Data
The test and development data sets for this task
were provided by the EU FP7 project Khres-
moi.
15
This projects develops a multi-lingual
multi-modal search and access system for biomed-
ical information and documents and its MT com-
ponent allows users to use non-English queries to
search in English documents and see summaries
of retrieved documents in their preferred language
(Czech, German, or French). The statistics of the
data sets are presented in Tables 20 and 21.
For the summary translation subtask, 1,000
and 500 sentences were provided for test devel-
opment purposes, respectively. The sentences
were randomly sampled from automatically gen-
erated summaries (extracts) of English documents
(web pages) containing medical information rel-
evant to 50 topics provided for the CLEF 2013
eHealth Task 3.
16
Out-of-domain and ungram-
matical sentences were manually removed. The
sentences were then translated by medical experts
into Czech, German and French, and the transla-
tions were reviewed. Each sentence was provided
with the corresponding document ID and topic ID.
The set also included a description for each of the
50 topics. The data package (Khresmoi Summary
Translation Test Data 1.1) is now available from
the LINDAT/CLARIN repository
17
and more de-
15
http://khresmoi.eu/
16
https://sites.google.com/site/
shareclefehealth/
17
http://hdl.handle.net/11858/
tails can be found in Zde?nka Ure?sov?a and Pecina
(2014).
For the query translation subtask, the main
test set contains 1,000 queries for test and 508
queries for development purposes. The original
English queries were extracted at random from
real user query logs provided by the Health on the
Net foundation
18
(queries by general public) and
the Trip database
19
(queries by medical experts).
Each query was translated into Czech, German,
and French by medical experts and the transla-
tions were reviewed. The data package (Khresmoi
Query Translation Test Data 1.0) is available from
the LINDAT/CLARIN repository.
20
An additional test set for the query translation
subtask was adopted from the CLEF 2013 eHealth
Task 3 (Pecina et al., 2014). It contains 50 queries
constructed from titles of the test topics (originally
in English) translated into Czech, German, and
French by medical experts. The participants were
asked to translate the queries back to English and
the resulting translations were used in an informa-
tion retrieval (IR) experiment for extrinsic evalua-
tion.
5.3 Training Data
This section reviews the in-domain resources
which were allowed for the constrained Medical
Translation Task in addition to resources for the
constrained standard Translation Task (see Section
2). Most of the corpora are available for direct
download, others can be obtained upon registra-
tion. The corpora usually employ their own, more
or less complex data format. To lower the entry
barrier, we provided a set of easy-to-use scripts to
convert the data to a plain text format suitable for
MT training.
5.3.1 Parallel Training Data
The medical-domain parallel data includes the fol-
lowing corpora (see Table 22 for statistics): The
EMEA corpus (Tiedemann, 2009) contains doc-
uments from the European Medicines Agency,
automatically processed and aligned on sentence
level. It is available for many language pairs, in-
cluding those relevant to this task. UMLS is a
multilingual metathesaurus of health and biomed-
00-097C-0000-0023-866E-1
18
http://www.hon.ch/
19
http://www.tripdatabase.com/
20
http://hdl.handle.net/11858/
00-097C-0000-0022-D9BF-5
43
sents tokens
total Czech German French English
dev 500 9,209 9,924 12,369 10,350
test 1,000 19,191 20,831 26,183 21,423
Table 20: Statistics of summary test data.
queries tokens
total general expert Czech German French English
dev 508 249 259 1,128 1,041 1,335 1,084
test 1,000 500 500 2,121 1,951 2,490 2,067
Table 21: Statistics of query test data.
L1?L2 Czech?English DE?EN FR?EN
data set sents L1 tokens L2 tokens sents L1 tokens L2 tokens sents L1 tokens L2 tokens
EMEA 1,053 13,872 14,378 1,108 13,946 14,953 1,092 17,605 14,786
UMLS 1,441 4,248 5,579 2,001 6,613 8,153 2,171 8,505 8,524
Wiki 3 5 6 10 19 22 8 19 17
MuchMore 29 688 740
PatTr 1,848 102,418 106,727 2,201 127,098 108,665
COPPA 664 49,016 39,933
Table 22: Statistics of the in-domain parallel training data allowed for the constrained task (in thousands).
data set English Czech German French
PatTR 121,592 53,242 54,608
UMLS 7,991 63 24 37
Wiki 26,945 1,784 10,232 8,376
AACT 13,341
DrugBank 953
FMA 884
GENIA 557
GREC 62
PIL 662
Table 23: Sizes of monolingual training data allowed for the
constrained tasks (in thousands of tokens).
ical vocabularies and standards (U.S. National Li-
brary of Medicine, 2009). The UMLS dataset
was constructed by selecting the concepts which
have translations in the respective languages. The
Wiki dataset contains bilingual pairs of titles of
Wikipedia articles belonging to the categories
identified to be medical-domain within the Khres-
moi project. It is available for all three lan-
guage pairs. The MuchMore Springer Corpus
is a German?English parallel corpus of medical
journals abstracts published by Springer (Buitelaar
et al., 2003). PatTR is a parallel corpus extracted
from the MAREC patent collection (W?aschle and
Riezler, 2012). It is available for German?English
and French?English. For the medical domain,
we only consider text from patents indicated to
be from the medicine-related categories (A61,
C12N, C12P). COPPA (Corpus of Parallel Patent
Applications (Pouliquen and Mazenc, 2011) is a
French?English parallel corpus extracted from the
MAREC patent collection (W?aschle and Riezler,
2012). The medical-domain subset is identified by
the same categories as in PatTR.
5.3.2 Monolingual Training Data
The medical-domain monolingual data consists of
the following corpora (statistics are presented in
Table 23): The monolingual UMLS dataset con-
tains concept descriptions in CS, DE, and FR ex-
tracted from the UMLS Metathesaurus (see Sec-
tion 5.3.1). The monolingual Wiki dataset con-
sists of articles belonging to the categories iden-
tified to be medical-domain within the Khresmoi
project. The PatTR dataset contains non-parallel
data extracted from the medical patents included
in the PatTR corpus (see Section 5.3.1). AACT is a
collection of restructured and reformatted English
texts publicly available and downloadable from
ClinicalTrials.gov, containing clinical studies con-
ducted around the world. DrugBank is a bioin-
formatics and cheminformatics resource contain-
ing drug descriptions (Knox et al., 2011). GENIA
is a corpus of biomedical literature compiled and
annotated within the GENIA project (Kim et al.,
2003). FMA stands for the Foundational Model
of Anatomy Ontology, a knowledge source for
biomedical informatics concerned with symbolic
representation of the phenotypic structure of the
human body (Rosse and Mejino Jr., 2008). GREC
(Gene Regulation Event Corpus) is a semantically
annotated English corpus of abstracts of biomedi-
cal papers (Thompson et al., 2009). The PIL cor-
pus is a collection of documents giving instruc-
tions to patients about their medication (Bouayad-
Agha et al., 2000).
5.4 Participants
A total of eight teams participated in the Medical
Translation Task by submitting their systems to at
least one subtask for one or more translation direc-
tions. A list of the participants is given in Table 24;
we provide short descriptions of their systems in
the following.
CUNI was involved in the organization of the task,
and their primary goal was to set up a baseline for
both the subtasks and for all translation directions.
44
ID Participating team
CUNI Charles University in Prague (Du?sek et al., 2014)
DCU-Q Dublin City University (Okita et al., 2014)
DCU-S Dublin City University (Zhang et al., 2014)
LIMSI Laboratoire dInformatique pour la Mecanique et les Sciences de lIng?enieur (P?echeux et al., 2014)
POSTECH Pohang University of Science and Technology (Li et al., 2014a)
UEDIN University of Edinburgh (Durrani et al., 2014a)
UM-DA University of Macau (Wang et al., 2014)
UM-WDA University of Macau (Lu et al., 2014)
Table 24: Participants in the WMT14 Medical Translation Task.
Their systems are based on the Moses phrase-
based toolkit and linear interpolation of in-domain
and out-of-domain language models and phrase ta-
bles. The constrained/unconstrained systems dif-
fer in the training data only. The constrained
ones are built using all allowed training data; the
unconstrained ones take advantage of additional
web-crawled monolingual data used for training of
the language models, and additional parallel non-
medical data from the PatTr and COPPA patent
collections.
DCU-Q submitted a system designed specifically
for terminology translation in the query translation
task for EN?FR and FR?EN. This system supports
six terminology extraction methods and is able to
detect rare word pairs including zero-appearance
word pairs. It uses monotonic decoding with lat-
tice inputs, avoiding unnecessary hypothesis ex-
pansions by the reordering model.
DCU-S submitted a system to the FR?EN sum-
mary translation subtask only. The system is
similar to DCU?s system for patent translation
(phrased-based using Moses) but adapted to trans-
late medical summaries and reports.
LIMSI took part in the summary translation sub-
task for English to French.Their primary submis-
sion uses a combination of two translation sys-
tems: NCODE, based on bilingual n-gram trans-
lation models; and an on-the-fly estimation of
the parameters of Moses along with a vector
space model to perform domain adaptation. A
continuous-space language model is also used in
a post-processing step for each system.
POSTECH submitted a phrase-based SMT sys-
tem and query translation system for the DE?EN
language pair in both subtasks. They analysed
three types of query formation, generated query
translation candidates using term-to-term dictio-
naries and a phrase-based system, and then scored
them using a co-occurrence word frequency mea-
sure to select the best candidate.
UEDIN applied the Moses phrase-based system to
all language pairs and both subtasks. They used
the hierarchical reordering model and the OSM
feature, same as in UEDIN?s news translation sys-
tem, and applied compound splitting to German
input. They used separate language models built
on in-domain and out-of-domain data with linear
interpolation. For all language pairs except CS-
EN and DE-EN, they selected data for the transla-
tion model using modified Moore-Lewis filtering.
For DE-EN and CS-EN, they concatenated all the
supplied parallel training data.
UM-DA submitted systems for all language pairs
in the summary translation subtask based on a
combination of different adaptation steps, namely
domain-specific pre-processing, language model
adaptation, translation model adaptation, numeric
adaptation, and hyphenated word adaptation. Data
for the domain-adapted language and translation
models were selected using various data selection
techniques.
UM-WDA submitted systems for all language
pairs in the summary translation subtask. Their
systems are domain-adapted using web-crawled
in-domain resources: bilingual dictionaries and
monolingual data. The translation model and lan-
guage model trained on the crawled data were in-
terpolated with the best-performing language and
translation model employed in the UM-DA sys-
tems.
5.5 Results
MT quality in the Medical Translation Task
is evaluated using automatic evaluation metrics:
BLEU (Papineni et al., 2002), TER (Snover et al.,
2006), PER (Tillmann et al., 1997), and CDER
(Leusch et al., 2006). BLEU scores are reported as
percentage and all error rates are reported as one
minus the original value, also as percentage, so
that all metrics are in the 0-100 range, and higher
scores indicate better translations.
The main reason for not conducting human
evaluation, as it happens in the standard Trans-
45
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 29.64 29.79
?
1.07 47.45
?
1.15 61.64
?
1.06 52.18
?
0.98 31.68
?
1.14 49.84
?
1.10 64.38
?
1.06 54.10
?
0.96
CUNI 22.44 22.57
?
0.95 41.43
?
1.16 55.46
?
1.09 46.42
?
0.96 32.34
?
1.12 50.24
?
1.20 65.07
?
1.10 54.42
?
0.96
UEDIN 36.65 36.87
?
1.23 54.35
?
1.19 67.16
?
1.00 57.61
?
1.01 38.02
?
1.24 56.14
?
1.17 69.24
?
1.01 58.96
?
0.96
UM-DA 37.62 37.79
?
1.26 54.55
?
1.20 68.29
?
0.88 57.28
?
1.03 38.81
?
1.28 56.04
?
1.20 70.06
?
0.82 58.45
?
1.05
CUNI 22.92 23.06
?
0.97 42.49
?
1.10 56.10
?
1.12 47.13
?
0.95 33.18
?
1.15 51.48
?
1.15 66.00
?
1.03 55.30
?
0.96
CUNI 22.69 22.84
?
0.98 42.21
?
1.14 56.01
?
1.11 46.79
?
0.94 32.84
?
1.13 51.10
?
1.11 65.79
?
1.07 54.81
?
0.96
UM-WDA 37.35 37.53
?
1.26 54.39
?
1.19 68.21
?
0.83 57.16
?
1.07 38.61
?
1.27 55.92
?
1.17 70.02
?
0.81 58.36
?
1.07
ONLINE 39.57
?
1.21 58.24
?
1.14 70.16
?
0.78 60.04
?
1.02 40.62
?
1.23 59.72
?
1.11 71.94
?
0.74 61.26
?
1.01
German?English
CUNI 28.20 28.34
?
1.12 46.66
?
1.13 61.53
?
1.03 50.57
?
0.93 30.69
?
1.19 48.91
?
1.16 64.12
?
1.04 52.52
?
0.95
CUNI 28.85 28.99
?
1.15 47.12
?
1.15 61.98
?
1.07 50.72
?
0.98 31.37
?
1.21 49.29
?
1.13 64.53
?
1.05 52.64
?
0.98
POSTECH 25.92 25.99
?
1.06 43.66
?
1.14 59.62
?
0.92 47.13
?
0.90 26.97
?
1.06 45.13
?
1.12 61.53
?
0.89 48.37
?
0.88
UEDIN 37.31 37.53
?
1.19 55.72
?
1.14 68.82
?
0.99 58.35
?
0.95 38.60
?
1.25 57.18
?
1.12 70.46
?
0.98 59.53
?
0.94
UM-DA 35.71 35.81
?
1.23 53.08
?
1.16 66.82
?
0.98 55.91
?
0.96 36.55
?
1.27 54.01
?
1.13 68.05
?
0.97 56.78
?
0.95
CUNI 30.58 30.71
?
1.10 48.68
?
1.09 63.19
?
1.08 52.72
?
0.94 33.14
?
1.19 50.98
?
1.06 65.88
?
1.04 54.74
?
0.94
CUNI 30.22 30.32
?
1.12 47.71
?
1.18 62.20
?
1.10 52.17
?
0.91 32.75
?
1.20 50.00
?
1.14 64.87
?
1.06 54.19
?
0.92
UM-WDA 32.70 32.88
?
1.19 49.60
?
1.18 63.74
?
1.01 53.50
?
0.96 33.95
?
1.23 51.05
?
1.19 65.54
?
0.98 54.73
?
0.96
ONLINE 41.18
?
1.24 59.33
?
1.09 70.95
?
0.92 61.92
?
1.01 42.29
?
1.23 60.76
?
1.08 72.51
?
0.88 63.06
?
0.96
French?English
CUNI 34.42 34.55
?
1.20 52.24
?
1.17 64.52
?
1.03 56.48
?
0.91 36.52
?
1.23 54.35
?
1.12 67.07
?
1.00 58.34
?
0.91
CUNI 33.67 33.59
?
1.16 50.39
?
1.23 61.75
?
1.16 56.74
?
0.97 35.55
?
1.21 52.55
?
1.26 64.45
?
1.13 58.63
?
0.91
DCU-B 44.85 45.01
?
1.24 62.57
?
1.12 74.11
?
0.78 64.33
?
0.99 46.12
?
1.26 64.04
?
1.06 75.84
?
0.74 65.55
?
0.94
UEDIN 46.44 46.68
?
1.26 64.12
?
1.16 74.47
?
0.87 66.40
?
0.96 48.01
?
1.29 65.70
?
1.15 76.30
?
0.86 67.76
?
0.91
UM-DA 47.08 47.22
?
1.33 64.08
?
1.16 75.41
?
0.88 66.15
?
0.96 48.23
?
1.31 65.36
?
1.10 76.95
?
0.89 67.18
?
0.93
CUNI 34.74 34.89
?
1.12 52.39
?
1.16 63.76
?
1.09 57.29
?
0.94 36.84
?
1.17 54.56
?
1.13 66.43
?
1.07 59.14
?
0.90
CUNI 35.04 34.99
?
1.18 52.11
?
1.24 63.24
?
1.09 57.51
?
0.97 37.04
?
1.18 54.38
?
1.17 66.02
?
1.05 59.55
?
0.93
UM-WDA 43.84 44.06
?
1.32 61.14
?
1.18 73.13
?
0.87 63.09
?
1.00 45.17
?
1.36 62.63
?
1.15 74.94
?
0.84 64.37
?
0.99
ONLINE 46.99
?
1.35 64.31
?
1.12 76.07
?
0.78 66.09
?
1.00 47.99
?
1.33 65.65
?
1.07 77.65
?
0.75 67.20
?
0.96
English?Czech
CUNI 17.36 17.65
?
0.96 37.17
?
1.02 49.13
?
0.98 40.31
?
0.95 18.75
?
0.96 38.32
?
1.02 50.82
?
0.91 41.39
?
0.94
CUNI 16.64 16.89
?
0.93 36.57
?
1.05 48.79
?
0.98 39.46
?
0.90 17.94
?
0.96 37.74
?
1.03 50.50
?
0.97 40.59
?
0.91
UEDIN 23.45 23.74
?
1.00 44.20
?
1.10 55.38
?
0.88 46.23
?
0.99 24.20
?
1.00 44.92
?
1.08 56.38
?
0.90 46.78
?
1.00
UM-DA 22.61 22.72
?
0.98 42.73
?
1.16 54.12
?
0.93 44.73
?
1.01 23.12
?
1.01 43.41
?
1.14 55.11
?
0.93 45.32
?
1.02
CUNI 20.56 20.84
?
1.01 39.98
?
1.09 51.98
?
0.99 42.86
?
1.00 22.03
?
1.05 41.19
?
1.08 53.66
?
0.97 43.93
?
1.01
CUNI 19.50 19.72
?
0.97 38.09
?
1.10 50.12
?
1.06 41.50
?
0.96 20.91
?
1.02 39.26
?
1.12 51.79
?
1.04 42.59
?
0.96
UM-WDA 22.14 22.33
?
0.96 42.30
?
1.11 53.89
?
0.92 44.48
?
1.01 22.72
?
0.97 43.02
?
1.09 54.89
?
0.95 45.08
?
0.99
ONLINE 33.45
?
1.28 51.64
?
1.28 61.82
?
1.10 53.97
?
1.18 34.02
?
1.31 52.35
?
1.22 62.84
?
1.08 54.52
?
1.18
English?German
CUNI 12.52 12.64
?
0.77 29.84
?
0.99 45.38
?
1.14 34.69
?
0.81 16.63
?
0.91 33.63
?
1.07 50.03
?
1.24 38.43
?
0.87
CUNI 12.42 12.53
?
0.77 29.02
?
1.05 44.27
?
1.16 34.62
?
0.78 16.41
?
0.91 32.87
?
1.08 48.99
?
1.21 38.37
?
0.86
POSTECH 15.46 15.59
?
0.91 34.41
?
1.01 49.00
?
0.83 37.11
?
0.90 15.98
?
0.92 34.98
?
1.00 49.94
?
0.81 37.60
?
0.87
UEDIN 20.88 21.01
?
1.03 40.03
?
1.08 55.54
?
0.91 42.95
?
0.90 21.40
?
1.03 40.55
?
1.08 56.33
?
0.92 43.41
?
0.90
UM-DA 20.89 21.09
?
1.07 40.76
?
1.03 55.45
?
0.89 43.02
?
0.93 21.52
?
1.08 41.31
?
1.01 56.38
?
0.90 43.58
?
0.91
CUNI 14.29 14.42
?
0.81 31.82
?
1.03 47.01
?
1.13 36.81
?
0.79 18.87
?
0.90 35.76
?
1.11 51.76
?
1.17 40.65
?
0.87
CUNI 13.44 13.58
?
0.75 30.37
?
1.03 45.80
?
1.14 35.80
?
0.76 17.84
?
0.89 34.41
?
1.13 50.75
?
1.18 39.85
?
0.78
UM-WDA 18.77 18.91
?
1.00 37.92
?
1.02 53.59
?
0.85 40.90
?
0.86 19.30
?
1.02 38.42
?
1.01 54.40
?
0.85 41.34
?
0.86
ONLINE 23.92
?
1.06 44.33
?
0.97 57.47
?
0.80 46.35
?
0.91 24.29
?
1.07 44.83
?
0.98 58.20
?
0.80 46.71
?
0.92
English?French
CUNI 30.30 30.67
?
1.11 46.59
?
1.09 59.83
?
1.04 50.51
?
0.93 32.06
?
1.12 48.01
?
1.09 61.66
?
1.00 51.83
?
0.94
CUNI 29.35 29.71
?
1.10 45.84
?
1.07 58.81
?
1.04 50.00
?
0.96 31.02
?
1.10 47.24
?
1.09 60.57
?
1.02 51.31
?
0.94
LIMSI 40.14 43.54
?
1.22 59.70
?
1.04 69.45
?
0.86 61.35
?
0.96 44.04
?
1.22 60.32
?
1.03 70.20
?
0.85 61.90
?
0.94
LIMSI 38.83 42.21
?
1.13 58.88
?
1.01 68.70
?
0.81 60.59
?
0.93 42.69
?
1.12 59.53
?
0.98 69.50
?
0.80 61.17
?
0.91
UEDIN 40.74 44.24
?
1.16 60.66
?
1.07 70.35
?
0.82 62.28
?
0.95 44.85
?
1.17 61.43
?
1.05 71.27
?
0.81 62.94
?
0.91
UM-DA 41.24 41.68
?
1.12 58.72
?
1.06 69.37
?
0.78 60.12
?
0.95 42.16
?
1.11 59.39
?
1.05 70.21
?
0.77 60.71
?
0.92
CUNI 32.23 32.61
?
1.09 48.48
?
1.08 61.13
?
1.01 52.24
?
0.93 34.08
?
1.10 49.93
?
1.11 62.92
?
0.99 53.65
?
0.92
CUNI 32.45 32.84
?
1.06 48.68
?
1.06 61.32
?
0.98 52.35
?
0.94 34.22
?
1.07 50.09
?
1.04 63.04
?
0.96 53.67
?
0.91
UM-WDA 40.78 41.16
?
1.13 58.20
?
0.99 68.93
?
0.84 59.64
?
0.94 41.79
?
1.12 59.10
?
0.96 70.01
?
0.84 60.39
?
0.91
ONLINE 58.63
?
1.26 70.70
?
1.12 78.22
?
0.81 71.89
?
0.96 59.27
?
1.26 71.50
?
1.10 79.16
?
0.81 72.63
?
0.94
Table 25: Official results of translation quality evaluation in the medical summary translation subtask.
46
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 10.71 10.57
?
3.42 15.72
?
2.77 23.37
?
3.03 18.68
?
2.42 30.13
?
4.85 53.38
?
3.01 62.53
?
2.84 55.44
?
2.87
CUNI 9.92 9.78
?
3.04 16.84
?
2.84 23.80
?
3.08 19.85
?
2.40 28.21
?
4.56 54.15
?
3.04 62.56
?
2.99 55.91
?
2.79
UEDIN 24.66 24.68
?
4.52 39.88
?
3.05 49.97
?
3.29 41.81
?
2.80 28.25
?
4.94 45.31
?
3.14 55.66
?
3.06 46.67
?
2.77
CUNI 12.00 11.86
?
3.42 18.49
?
2.74 24.67
?
2.85 21.08
?
2.29 31.91
?
4.81 57.61
?
3.13 65.02
?
2.99 59.24
?
2.69
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
ONLINE 28.88
?
4.96 47.31
?
3.35 55.19
?
3.21 49.88
?
2.89 35.33
?
5.20 55.80
?
3.20 64.05
?
2.97 57.94
?
2.85
German?English
CUNI 10.90 10.74
?
3.41 18.89
?
2.39 26.09
?
2.00 20.29
?
2.07 32.15
?
5.23 55.56
?
2.90 63.68
?
2.34 56.45
?
2.62
CUNI 10.71 10.55
?
3.47 18.40
?
2.35 25.45
?
2.04 19.84
?
2.07 32.06
?
5.19 54.85
?
2.91 62.87
?
2.39 55.52
?
2.61
POSTECH 18.06 17.97
?
4.38 28.57
?
3.30 40.38
?
2.77 31.79
?
2.80 21.99
?
4.65 35.76
?
3.35 47.84
?
2.82 38.84
?
2.92
POSTECH 17.99 17.88
?
4.72 29.79
?
3.04 41.15
?
2.48 32.49
?
2.63 24.41
?
4.83 41.72
?
3.19 53.33
?
2.55 44.06
?
2.88
UEDIN 23.33 23.39
?
4.37 38.55
?
3.65 48.21
?
3.43 40.75
?
3.05 27.17
?
4.63 43.87
?
3.52 53.76
?
3.48 45.72
?
3.03
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
CUNI 8.75 8.49
?
3.60 19.10
?
2.27 24.98
?
1.95 19.95
?
2.02 30.00
?
5.59 56.07
?
2.92 62.92
?
2.32 56.27
?
2.56
ONLINE 19.97
?
4.46 37.03
?
3.26 43.91
?
3.22 40.95
?
2.93 33.86
?
4.87 53.28
?
3.28 60.86
?
3.22 56.33
?
2.98
French?English
CUNI 13.90 13.79
?
3.61 18.49
?
2.55 28.35
?
2.81 20.36
?
2.20 34.97
?
5.34 59.54
?
2.94 72.30
?
2.63 58.86
?
2.76
CUNI 12.10 11.95
?
3.41 17.23
?
2.57 27.12
?
2.88 19.15
?
2.28 33.74
?
5.01 58.95
?
2.96 71.25
?
2.76 58.20
?
2.81
DCU-Q 30.85 31.24
?
5.08 58.88
?
2.97 67.94
?
2.62 59.19
?
2.62 36.88
?
5.07 66.38
?
2.85 75.86
?
2.37 66.29
?
2.55
DCU-Q 26.51 26.16
?
4.40 48.02
?
3.72 57.34
?
3.24 53.56
?
2.79 28.61
?
4.52 53.65
?
3.73 63.51
?
3.21 59.07
?
2.79
UEDIN 27.20 27.60
?
3.98 38.54
?
3.22 48.81
?
3.26 39.77
?
2.95 32.23
?
4.27 43.66
?
3.20 54.31
?
3.17 44.53
?
2.79
CUNI 14.03 14.00
?
3.30 20.11
?
2.38 29.00
?
2.71 21.62
?
2.22 38.98
?
5.08 62.90
?
2.87 74.49
?
2.45 62.12
?
2.64
CUNI 13.38 13.16
?
3.52 17.79
?
2.56 28.84
?
2.81 19.17
?
2.23 35.00
?
5.20 59.52
?
2.98 73.08
?
2.57 58.41
?
2.68
ONLINE 32.96
?
5.04 53.68
?
3.21 64.27
?
2.80 54.40
?
2.66 38.09
?
5.52 61.44
?
3.08 72.59
?
2.61 61.60
?
2.78
English?Czech
CUNI 8.37 8.00
?
3.65 17.74
?
2.23 26.46
?
1.96 19.48
?
2.10 19.49
?
4.60 41.53
?
2.94 51.34
?
2.51 42.54
?
2.74
CUNI 9.04 8.75
?
3.64 18.25
?
2.27 26.97
?
1.92 19.69
?
2.11 21.46
?
5.05 42.36
?
3.09 51.99
?
2.40 43.18
?
2.68
UEDIN 12.57 12.40
?
3.61 21.15
?
2.96 33.56
?
2.80 22.30
?
2.67 14.06
?
3.80 24.92
?
2.90 37.85
?
2.72 25.58
?
2.70
UEDIN 6.64 6.21
?
4.73 -2.35
?
3.06 5.95
?
3.48 -0.97
?
3.12 14.35
?
3.52 14.51
?
3.19 24.96
?
3.50 15.11
?
3.10
CUNI 9.06 8.64
?
3.82 19.92
?
2.24 26.97
?
1.94 20.82
?
2.06 22.42
?
5.24 44.89
?
2.94 52.89
?
2.40 45.36
?
2.78
CUNI 8.49 8.01
?
6.05 18.13
?
2.28 25.19
?
1.86 19.19
?
2.01 21.04
?
4.80 42.66
?
2.87 50.34
?
2.47 43.30
?
2.74
ONLINE 21.09
?
4.60 48.56
?
2.82 54.72
?
2.51 48.30
?
2.83 24.37
?
4.80 51.93
?
2.74 58.10
?
2.50 51.62
?
2.80
English?German
CUNI 10.17 10.01
?
3.92 26.48
?
3.24 36.71
?
3.37 29.26
?
2.96 13.02
?
4.17 31.96
?
3.41 42.39
?
3.21 34.61
?
2.95
CUNI 9.98 9.69
?
3.94 26.16
?
3.19 35.50
?
3.23 28.86
?
2.94 12.90
?
4.28 31.75
?
3.33 41.24
?
3.21 34.38
?
3.05
POSTECH 13.43 13.01
?
5.91 26.38
?
3.09 35.75
?
3.16 27.86
?
2.82 15.05
?
5.71 30.45
?
3.10 39.89
?
3.14 31.79
?
3.00
POSTECH 13.41 13.15
?
5.21 22.18
?
3.09 30.89
?
3.31 24.17
?
3.06 14.96
?
5.15 26.13
?
3.19 34.92
?
3.40 27.98
?
3.12
UEDIN 10.45 10.14
?
3.86 23.44
?
3.43 34.55
?
3.34 25.46
?
3.17 11.91
?
4.42 27.91
?
3.45 39.08
?
3.42 29.63
?
3.31
CUNI 8.91 7.72
?
6.48 30.05
?
3.22 40.65
?
2.71 31.91
?
2.88 13.66
?
5.37 35.51
?
3.28 46.12
?
2.74 37.27
?
3.01
CUNI 9.14 8.69
?
6.44 27.66
?
3.31 37.95
?
3.45 31.00
?
2.82 14.03
?
5.92 33.53
?
3.45 44.03
?
3.53 36.73
?
3.00
ONLINE 20.07
?
6.06 41.07
?
3.23 47.41
?
2.86 41.61
?
3.02 21.67
?
6.23 43.78
?
3.23 50.18
?
2.95 44.26
?
3.06
English?French
CUNI 13.12 12.92
?
2.84 21.95
?
2.41 33.19
?
2.09 23.70
?
2.24 28.42
?
3.98 51.43
?
2.90 63.74
?
2.35 52.64
?
2.58
CUNI 12.80 12.65
?
2.81 19.16
?
2.61 31.61
?
2.21 21.91
?
2.32 27.52
?
4.05 47.47
?
3.08 61.43
?
2.37 49.82
?
2.72
DCU-Q 27.69 27.84
?
4.11 48.97
?
3.06 60.90
?
2.55 51.84
?
2.83 28.98
?
4.16 51.73
?
3.10 63.84
?
2.47 54.43
?
2.76
UEDIN 20.16 21.76
?
3.42 31.66
?
4.23 44.37
?
4.13 44.29
?
2.73 23.25
?
3.49 35.38
?
4.19 48.52
?
4.07 47.94
?
2.75
CUNI 13.78 13.57
?
3.00 21.92
?
2.51 33.47
?
2.03 24.16
?
2.32 30.07
?
4.10 51.12
?
3.08 63.61
?
2.45 52.96
?
2.67
CUNI 15.27 15.24
?
3.12 23.58
?
2.54 34.39
?
2.54 25.79
?
2.32 31.40
?
4.15 53.60
?
2.96 65.39
?
2.57 55.47
?
2.69
ONLINE 28.93
?
3.66 49.20
?
3.08 60.85
?
2.69 51.68
?
2.78 30.88
?
3.66 52.25
?
3.08 64.06
?
2.62 54.59
?
2.68
Table 26: Official results of translation quality evaluation in the medical query translation subtask.
source lang. ID P@5 P@10 NDCG@5 NDCG@10 MAP Rprec bpref rel
Czech?English CUNI 0.3280 0.3340 0.2873 0.2936 0.2217 0.2362 0.3473 1461
German?English CUNI 0.2800 0.3000 0.2467 0.2630 0.2057 0.2077 0.3310 1426
French?English CUNI 0.3280 0.3380 0.2811 0.2882 0.2206 0.2284 0.3504 1481
DCU-Q 0.3480 0.3460 0.3060 0.3072 0.2252 0.2358 0.3659 1524
UEDIN 0.4440 0.4300 0.3793 0.3826 0.2843 0.2935 0.3936 1544
English (monolingual) 0.4600 0.4700 0.4091 0.4205 0.3035 0.3198 0.3858 1638
Table 27: Official results of retrieval evaluation in the query translation subtask.
47
lation Task, was the lack of domain expertise of
prospective raters. While in the standard task, the
only requirement for the raters was to be a na-
tive speaker of the target language, in the Med-
ical Translation Task, a very good knowledge of
the domain would be necessary to provide reli-
able judgements and the raters with such an ex-
pertise (medical doctors and native speakers) were
not available.
The complete results of the task are presented
in Table 25 (for summary translation) and Ta-
bles 26 and 27 (for query translation). Partici-
pant IDs given in bold indicate primary submis-
sions, IDs in normal font refer to contrastive sub-
missions. The first section for each translation di-
rection (white background) refers to constrained
submissions and the second one (light-gray back-
ground) to unconstrained submissions. The col-
umn denoted as ?original? contains BLEU scores
as reported by the Matrix submission system ob-
tained on the original submitted translations. Due
to punctuation inconsistency in the original refer-
ence translations, we decided to perform punctu-
ation normalization before calculating the official
scores. The columns denoted as ?normalized true-
cased? contain scores obtained on the submitted
translations after punctuation normalization and
the columns denoted as ?normalized lowercased?
contain scores obtained after punctuation normal-
ization and lowercasing. The normalization script
is available in the package with summary transla-
tion test data. The confidence intervals were ob-
tained by bootstrap resampling with a confidence
level of 95%. Figures in bold denote the best con-
strained system and, if its score is higher, the best
unconstrained system for each translation direc-
tion and each metric. For comparison, we also
present results of a major on-line translation sys-
tem (denoted as ONLINE).
The results of the extrinsic evaluation of query
translation submissions are given in 27. We used
the CLEF 2013 eHealth Task 3 test collection con-
taining about 1 million web pages (in English),
50 test queries (originally in English and trans-
lated to Czech, German, and French), and their
relevance assessments. Some of the participants
of the WMT Medical Task (three teams with five
submissions in total) submitted translations of the
queries (from Czech, German, and French) into
English and these translations were used to query
the CLEF 2013 eHealth Task 3 test collection us-
ing a state-of-the-art system based on a BM25
model, described in Pecina et al. (2014). Origi-
nally, we asked for 10 best translations for each
query, but only the best one were used for the
evaluation. The results are provided in terms of
standard IR evaluation measures: precision at a
cut-off of 5 and 10 documents (P@5, P@10),
normalized discounted cumulative gain (J?arvelin
and Kek?al?ainen, 2002) at 5 and 10 documents
(NDCG@5, NDCG@10), mean average precision
(MAP) (Voorhees and Harman, 2005), precision
reached after R documents retrieved, where R in-
dicates the number of the relevant documents for
each query in the entire collection (Rprec), binary
preference (bpref) (Buckley and Voorhees, 2004),
and number or relevant documents retrieved (rel).
The cross-lingual results are also compared with
the monolingual one (obtained by using the refer-
ence (English) translations of the test topics) to see
how the system would perform if the queries were
translated perfectly.
5.6 Discussion and Conclusion
Both the subtasks turned out to be quite challeng-
ing not only because of the specific domain ? in
summary sentences, we can observe much higher
density of terminology than in ordinary sentences;
the queries, which are also rich in terminology, do
not form sentences at all.
Most submissions were based on systems par-
ticipating in the standard Translation Task and
trained on the provided data or its subsets CUNI
provided baseline systems for all language pairs in
both subtasks, which turned to be relatively strong
for the query translation task, especially in trans-
lation to English, but only in terms of scores ob-
tained on normalized and lowercased translations
since their truecasing component did not perform
well.
In the summary translation subtask, the best
overall results were achieved by the UEDIN team
which won for DE?EN, EN?CS, and EN?FR, fol-
lowed by the UM-DA team, which performed on
par with UEDIN in all other translation.
The unconstrained submissions in almost all
cases did not outperform the results of the con-
strained submissions. Some improvements were
observed in the query translations subtasks by the
CUNI?s unconstrained system with language mod-
els trained on larger in-domain data.
The ONLINE system outperforms all other sub-
48
missions with only two exceptions ? the UM-DA?s
and UEDIN?s systems for the summary translation
in the FR?EN direction, though the score differ-
ences are within the 95% confidence interval.
In the query translation subtask, DCU-Q built
a system designed specifically for terminology
translation between French and English and out-
performed all other participants in translation into
English; however, the confidence intervals in the
query translation task are much wider and most of
the differences in scores of the automatic metrics
are not statistically significant.
The extrinsic evaluation in the cross-lingual in-
formation retrieval was conducted for translations
into English only. CUNI provided the baselines
for all directions, but other submissions were done
for FR?EN only. Here, the winner is UEDIN, who
outperformed both CUNI and DCU-Q, and their
scores are very close to those obtained using the
reference English translations.
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Yandex.
We would also like to thank our colleagues Ma-
tou?s Mach?a?cek and Martin Popel for detailed dis-
cussions.
References
Avramidis, E. (2014). Efforts on machine learning
over human-mediated translation edit rate. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Beck, D., Shah, K., and Specia, L. (2014). Shef-
lite 2.0: Sparse multi-task gaussian processes
for translation quality estimation. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Bic?ici, E. (2013). Referential translation machines
for quality estimation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, Sofia, Bulgaria.
Bic?ici, E., Liu, Q., and Way, A. (2014). Parallel
FDA5 for fast deployment of accurate statisti-
cal machine translation systems. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, USA. Association
for Computational Linguistics.
Bicici, E., Liu, Q., and Way, A. (2014). Parallel
fda5 for fast deployment of accurate statistical
machine translation systems. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bicici, E. and Way, A. (2014). Referential transla-
tion machines for predicting translation quality.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Bojar, O., Buck, C., Callison-Burch, C., Feder-
mann, C., Haddow, B., Koehn, P., Monz, C.,
Post, M., Soricut, R., and Specia, L. (2013).
Findings of the 2013 Workshop on Statistical
Machine Translation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 1?42, Sofia, Bulgaria. Association
for Computational Linguistics.
Bojar, O., Diatka, V., Rychl?y, P., Stra?n?ak, P.,
Tamchyna, A., and Zeman, D. (2014). Hindi-
English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth Inter-
national Language Resources and Evaluation
Conference, Reykjavik, Iceland. ELRA.
Bojar, O., Ercegov?cevi?c, M., Popel, M., and
Zaidan, O. (2011). A grain of salt for the WMT
manual evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation,
pages 1?11, Edinburgh, Scotland. Association
for Computational Linguistics.
Borisov, A. and Galinskaya, I. (2014). Yandex
school of data analysis russian-english machine
translation system for wmt14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bouayad-Agha, N., Scott, D. R., and Power, R.
(2000). Integrating content and style in doc-
uments: A case study of patient information
leaflets. Information Design Journal, 9(2?
3):161?176.
Buckley, C. and Voorhees, E. M. (2004). Re-
trieval evaluation with incomplete information.
49
In Proceedings of the 27th Annual International
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 25?
32, Sheffield, United Kingdom.
Buitelaar, P., Sacaleanu, B.,
?
Spela Vintar, Stef-
fen, D., Volk, M., Dejean, H., Gaussier, E.,
Widdows, D., Weiser, O., and Frederking, R.
(2003). Multilingual concept hierarchies for
medical information organization and retrieval.
Public deliverable, MuchMore project.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Gonz?alez-Rubio, J.,
Buck, C., Turchi, M., and Negri, M. (2014).
Fbk-upv-uedin participation in the wmt14 qual-
ity estimation shared-task. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Cap, F., Weller, M., Ramm, A., and Fraser, A.
(2014). Cims ? the cis and ims joint submis-
sion to wmt 2014 translating from english into
german. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling an-
notator bias with multi-task gaussian processes:
An application to machine translation quality
estimation. In Proceedings of the 51st An-
nual Meeting of the Association for Compu-
tational Linguistics, ACL-2013, pages 32?42,
Sofia, Bulgaria.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Costa-juss`a, M. R., Gupta, P., Rosso, P., and
Banchs, R. E. (2014). English-to-hindi sys-
tem description for wmt 2014: Deep source-
context features for moses. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Do, Q. K., Herrmann, T., Niehues, J., Allauzen,
A., Yvon, F., and Waibel, A. (2014). The
kit-limsi translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Dungarwal, P., Chatterjee, R., Mishra, A.,
Kunchukuttan, A., Shah, R., and Bhattacharyya,
P. (2014). The iit bombay hindi-english transla-
tion system at wmt 2014. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
50
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014a). Edinburgh?s phrase-based machine
translation systems for wmt-14. In Proceedings
of the ACL 2014 Ninth Workshop of Statistical
Machine Translation, Baltimore, USA.
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014b). Edinburghs phrase-based machine
translation systems for wmt-14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Du?sek, O., Haji?c, J., Hlav?a?cov?a, J., Nov?ak, M.,
Pecina, P., Rosa, R., Tamchyna, A., Ure?sov?a,
Z., and Zeman, D. (2014). Machine transla-
tion of medical texts in the khresmoi project. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Foster, J. (2007). Treebanks gone bad: Parser eval-
uation and retraining using a treebank of un-
grammatical sentences. International Journal
on Document Analysis and Recognition, 10(3-
4):129?145.
Freitag, M., Peitz, S., Wuebker, J., Ney, H., Huck,
M., Sennrich, R., Durrani, N., Nadejde, M.,
Williams, P., Koehn, P., Herrmann, T., Cho,
E., and Waibel, A. (2014). Eu-bridge mt:
Combined machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Gamon, M., Aue, A., and Smets, M. (2005).
Sentence-level MT evaluation without reference
translations: beyond language modeling. In
Proceedings of the Annual Conference of the
European Association for Machine Translation,
Budapest.
Geurts, P., Ernst, D., and Wehenkel, L. (2006). Ex-
tremely randomized trees. Machine Learning,
63(1):3?42.
Green, S., Cer, D., and Manning, C. (2014).
Phrasal: A toolkit for new directions in statis-
tical machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hardmeier, C., Stymne, S., Tiedemann, J., Smith,
A., and Nivre, J. (2014). Anaphora models and
reordering for phrase-based smt. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Herbrich, R., Minka, T., and Graepel, T. (2006).
TrueSkill
TM
: A Bayesian Skill Rating Sys-
tem. In Proceedings of the Twentieth Annual
Conference on Neural Information Processing
Systems, pages 569?576, Vancouver, British
Columbia, Canada. MIT Press.
Herrmann, T., Mediani, M., Cho, E., Ha, T.-L.,
Niehues, J., Slawik, I., Zhang, Y., and Waibel,
A. (2014). The karlsruhe institute of technol-
ogy translation systems for the wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Hokamp, C., Calixto, I., Wagner, J., and Zhang,
J. (2014). Target-centric features for transla-
tion quality estimation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hopkins, M. and May, J. (2013). Models of trans-
lation competitions. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1416?1424, Sofia, Bulgaria.
J?arvelin, K. and Kek?al?ainen, J. (2002). Cumu-
lated gain-based evaluation of ir techniques.
ACM Transactions on Information Systems,
20(4):422?446.
Kim, J.-D., Ohta, T., Tateisi, Y., and Tsujii, J.
(2003). GENIA corpus ? a semantically anno-
tated corpus for bio-textmining. Bioinformatics,
19(suppl 1):i180?i182.
Knox, C., Law, V., Jewison, T., Liu, P., Ly,
S., Frolkis, A., Pon, A., Banco, K., Mak, C.,
Neveu, V., Djoumbou, Y., Eisner, R., Guo,
A. C., and Wishart, D. S. (2011). DrugBank 3.0:
a comprehensive resource for Omics research
on drugs. Nucleic acids research, 39(suppl
1):D1035?D1041.
Koehn, P. (2012a). Simulating human judgment in
51
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. (2012b). Simulating Human Judgment
in Machine Translation Evaluation Campaigns.
In Proceedings of the Ninth International Work-
shop on Spoken Language Translation, pages
179?184, Hong Kong, China.
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Koppel, M. and Ordan, N. (2011). Translationese
and its dialects. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Techolo-
gies, pages 1318?1326, Portland, Oregon.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Leusch, G., Ueffing, N., and Ney, H. (2006). Cder:
Efficient mt evaluation using block movements.
In Proceedings of the 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 241?248, Trento,
Italy.
Li, J., Kim, S.-J., Na, H., and Lee, J.-H. (2014a).
Postech?s system description for medical text
translation task. In Proceedings of the ACL
2014 Ninth Workshop of Statistical Machine
Translation, Baltimore, USA.
Li, L., Wu, X., Vaillo, S. C., Xie, J., Way, A., and
Liu, Q. (2014b). The dcu-ictcas mt system at
wmt 2014 on german-english translation task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Lopez, A. (2012). Putting Human Assessments of
Machine Translation Systems in Order. In Pro-
ceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 1?9, Montr?eal,
Canada. Association for Computational Lin-
guistics.
Lu, Y., Wang, L., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira, F. (2014). Domain adapta-
tion for medical text translation using web re-
sources. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
Luong, N. Q., Besacier, L., and Lecouteux, B.
(2014). Lig system for word level qe task at
wmt14. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Mach?a?cek, M. and Bojar, O. (2014). Results of
the wmt14 metrics shared task. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Matthews, A., Ammar, W., Bhatia, A., Feely, W.,
Hanneman, G., Schlinger, E., Swayamdipta, S.,
Tsvetkov, Y., Lavie, A., and Dyer, C. (2014).
The cmu machine translation systems at wmt
2014. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Neidert, J., Schuster, S., Green, S., Heafield, K.,
and Manning, C. (2014). Stanford universitys
submissions to the wmt 2014 translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Okita, T., Vahid, A. H., Way, A., and Liu, Q.
(2014). Dcu terminology translation system for
medical query subtask at wmt14. In Proceed-
ings of the ACL 2014 Ninth Workshop of Statis-
tical Machine Translation, Baltimore, USA.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a method for automatic eval-
uation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 311?318,
Philadelphia, PA, USA. Association for Com-
putational Linguistics.
P?echeux, N., Gong, L., Do, Q. K., Marie, B.,
Ivanishcheva, Y., Allauzen, A., Lavergne, T.,
52
Niehues, J., Max, A., and Yvon, Y. (2014).
LIMSI @ WMT?14 Medical Translation Task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, USA.
Pecina, P., Du?sek, O., Goeuriot, L., Haji?c, J.,
Hlav?a?cov?a, J., Jones, G., Kelly, L., Leveling, J.,
Mare?cek, D., Nov?ak, M., Popel, M., Rosa, R.,
Tamchyna, A., and Ure?sov?a, Z. (2014). Adapta-
tion of machine translation for multilingual in-
formation retrieval in the medical domain. Arti-
ficial Intelligence in Medicine, (0):?.
Peitz, S., Wuebker, J., Freitag, M., and Ney, H.
(2014). The rwth aachen german-english ma-
chine translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Pouliquen, B. and Mazenc, C. (2011). COPPA,
CLIR and TAPTA: three tools to assist in over-
coming the patent barrier at WIPO. In Pro-
ceedings of the Thirteenth Machine Translation
Summit, pages 24?30, Xiamen, China. Asia-
Pacific Association for Machine Translation.
Powers, D. M. W. (2011). Evaluation: from preci-
sion, recall and f-measure to roc, informedness,
markedness & correlation. Journal of Machine
Learning Technologies.
Quernheim, D. and Cap, F. (2014). Large-scale ex-
act decoding: The ims-ttt submission to wmt14.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Rosse, C. and Mejino Jr., J. L. V. (2008). The
foundational model of anatomy ontology. In
Burger, A., Davidson, D., and Baldock, R., ed-
itors, Anatomy Ontologies for Bioinformatics,
volume 6 of Computational Biology, pages 59?
117. Springer London.
Rubino, R., Toral, A., S?anchez-Cartagena, V. M.,
Ferr?andez-Tordera, J., Ortiz Rojas, S., Ram??rez-
S?anchez, G., S?anchez-Mart??nez, F., and Way,
A. (2014). Abu-matran at wmt 2014 transla-
tion task: Two-step data selection and rbmt-
style synthetic rules. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Sakaguchi, K., Post, M., and Van Durme, B.
(2014). Efficient elicitation of annotations for
human evaluation of machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland.
S?anchez-Cartagena, V. M., P?erez-Ortiz, J. A., and
S?anchez-Mart??nez, F. (2014). The ua-prompsit
hybrid machine translation system for the 2014
workshop on statistical machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Scarton, C. and Specia, L. (2014). Exploring con-
sensus in machine translation for quality esti-
mation. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Schwartz, L., Anderson, T., Gwinnup, J., and
Young, K. (2014). Machine translation and
monolingual postediting: The afrl wmt-14 sys-
tem. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An
investigation on the effectiveness of features for
translation quality estimation. In Proceedings
of the Machine Translation Summit XIV, pages
167?174, Nice, France.
Shah, K. and Specia, L. (2014). Quality estimation
for translation selection. In Proceedings of the
17th Annual Conference of the European As-
sociation for Machine Translation, Dubrovnik,
Croatia.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
53
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria.
Tamchyna, A., Popel, M., Rosa, R., and Bojar, O.
(2014). Cuni in wmt14: Chimera still awaits
bellerophon. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, Balti-
more, Maryland, USA. Association for Compu-
tational Linguistics.
Tan, L. and Pal, S. (2014). Manawi: Using
multi-word expressions and named entities to
improve machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Thompson, P., Iqbal, S., McNaught, J., and Ana-
niadou, S. (2009). Construction of an annotated
corpus to support biomedical information ex-
traction. BMC bioinformatics, 10(1):349.
Tiedemann, J. (2009). News from OPUS ? a
collection of multilingual parallel corpora with
tools and interfaces. In Recent Advances in
Natural Language Processing, volume 5, pages
237?248, Borovets, Bulgaria. John Benjamins.
Tillmann, C., Vogel, S., Ney, H., Zubiaga, A.,
and Sawaf, H. (1997). Accelerated DP based
search for statistical translation. In Kokki-
nakis, G., Fakotakis, N., and Dermatas, E., edi-
tors, Proceedings of the Fifth European Confer-
ence on Speech Communication and Technol-
ogy, pages 2667?2670, Rhodes, Greece. Inter-
national Speech Communication Association.
U.S. National Library of Medicine (2009). UMLS
reference manual. Metathesaurus. Bethesda,
MD, USA.
Voorhees, E. M. and Harman, D. K., editors
(2005). TREC: Experiment and evaluation in
information retrieval, volume 63 of Digital li-
braries and electronic publishing series. MIT
press Cambridge, Cambridge, MA, USA.
Wang, L., Lu, Y., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira., F. (2014). Combining domain
adaptation approaches for medical text transla-
tion. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
W?aschle, K. and Riezler, S. (2012). Analyz-
ing parallelism and domain similarities in the
MAREC patent corpus. In Salampasis, M. and
Larsen, B., editors, Multidisciplinary Informa-
tion Retrieval, volume 7356 of Lecture Notes
in Computer Science, pages 12?27. Springer
Berlin Heidelberg.
Williams, P., Sennrich, R., Nadejde, M., Huck, M.,
Hasler, E., and Koehn, P. (2014). Edinburghs
syntax-based systems at wmt 2014. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Wisniewski, G., P?echeux, N., Allauzen, A., and
Yvon, F. (2014). Limsi submission for wmt?14
qe task. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
wu, x., Haque, R., Okita, T., Arora, P., Way, A.,
and Liu, Q. (2014). Dcu-lingo24 participation
in wmt 2014 hindi-english translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Zde?nka Ure?sov?a, Ond?rej Du?sek, J. H. and Pecina,
P. (2014). Multilingual test sets for machine
translation of search queries for cross-lingual
information retrieval in the medical domain. In
To appear in Proceedings of the Ninth Interna-
tional Conference on Language Resources and
Evaluation, Reykjavik, Iceland.
Zhang, J., Wu, X., Calixto, I., Vahid, A. H., Zhang,
X., Way, A., and Liu, Q. (2014). Experiments in
medical translation shared task at wmt 2014. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
54
A Pairwise System Comparisons by Human Judges
Tables 28?37 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according the official method used in
Table 8. Gray lines separate clusters based on non-overlapping rank ranges.
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
C
U
-
M
O
S
E
S
ONLINE-B ? .47? .43? .42? .39?
UEDIN-PHRASE .53? ? .44? .44? .41?
UEDIN-SYNTAX .57? .56? ? .49 .48?
ONLINE-A .58? .56? .51 ? .48?
CU-MOSES .61? .59? .52? .52? ?
score .57 .54 .47 .46 .44
rank 1 2 3-4 3-4 5
Table 28: Head to head comparison, ignoring ties, for Czech-English systems
C
U
-
D
E
P
F
I
X
U
E
D
I
N
-
U
N
C
N
S
T
R
C
U
-
B
O
J
A
R
C
U
-
F
U
N
K
Y
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
O
N
L
I
N
E
-
A
C
U
-
T
E
C
T
O
C
O
M
M
E
R
C
I
A
L
1
C
O
M
M
E
R
C
I
A
L
2
CU-DEPFIX ? .50 .42? .48 .44? .43? .41? .35? .30? .24?
UEDIN-UNCNSTR .50 ? .51 .48 .42? .37? .42? .39? .31? .26?
CU-BOJAR .58? .49 ? .49 .45? .44? .40? .36? .32? .24?
CU-FUNKY .52 .52 .51 ? .48 .47? .44? .34? .33? .26?
ONLINE-B .56? .58? .55? .52 ? .48 .47? .41? .31? .26?
UEDIN-PHRASE .57? .63? .56? .53? .52 ? .48 .44? .32? .27?
ONLINE-A .59? .58? .60? .56? .53? .52 ? .45? .37? .30?
CU-TECTO .65? .61? .64? .66? .59? .56? .55? ? .42? .30?
COMMERCIAL1 .70? .69? .68? .67? .69? .68? .63? .58? ? .40?
COMMERCIAL2 .76? .74? .76? .74? .74? .73? .70? .70? .60? ?
score .60 .59 .58 .57 .54 .52 .50 .44 .36 .28
rank 1-3 1-3 1-4 3-4 5-6 5-6 7 8 9 10
Table 29: Head to head comparison, ignoring ties, for English-Czech systems
55
O
N
L
I
N
E
-
B
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
L
I
M
S
I
-
K
I
T
E
U
-
B
R
I
D
G
E
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
R
W
T
H
D
C
U
-
I
C
T
C
A
S
C
M
U
R
B
M
T
4
R
B
M
T
1
O
N
L
I
N
E
-
C
ONLINE-B ? .46 .40? .41? .35? .42? .38? .35? .40? .31? .33? .32? .22?
UEDIN-SYNTAX .54 ? .51 .47 .47 .45 .45? .39? .36? .38? .35? .34? .27?
ONLINE-A .60? .49 ? .42? .44? .51 .41? .38? .44? .42? .38? .31? .20?
LIMSI-KIT .59? .53 .58? ? .55 .53 .31? .45? .39? .41? .37? .35? .29?
EU-BRIDGE .65? .53 .56? .45 ? .45 .44? .48 .40? .37? .39? .37? .30?
UEDIN-PHRASE .58? .55 .49 .47 .55 ? .48 .39? .34? .45? .40? .40? .34?
KIT .62? .55? .59? .69? .56? .52 ? .45? .41? .45? .47 .40? .31?
RWTH .65? .61? .62? .55? .52 .61? .55? ? .54 .44? .44? .38? .37?
DCU-ICTCAS .60? .64? .56? .61? .60? .66? .59? .46 ? .51 .49 .46? .40?
CMU .69? .62? .58? .59? .63? .55? .55? .56? .49 ? .53 .42? .43?
RBMT4 .67? .65? .62? .63? .61? .60? .53 .56? .51 .47 ? .51 .37?
RBMT1 .68? .66? .69? .65? .63? .60? .60? .62? .54? .58? .49 ? .38?
ONLINE-C .78? .73? .80? .71? .70? .66? .69? .63? .60? .57? .63? .62? ?
score .63 .58 .58 .55 .55 .54 .49 .47 .45 .44 .44 .40 .32
rank 1 2-3 2-3 4-6 4-6 4-6 7-8 7-8 9-11 9-11 9-11 12 13
Table 30: Head to head comparison, ignoring ties, for German-English systems
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
S
T
A
N
F
O
R
D
E
U
-
B
R
I
D
G
E
R
B
M
T
4
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
1
K
I
T
S
T
A
N
F
O
R
D
-
U
N
C
C
I
M
S
S
T
A
N
F
O
R
D
U
U
O
N
L
I
N
E
-
C
I
M
S
-
T
T
T
U
U
-
D
O
C
E
N
T
UEDIN-SYNTAX ? .55? .46? .45? .46? .44? .41? .45? .43? .41? .38? .38? .36? .33? .38? .30? .30? .25?
ONLINE-B .45? ? .50 .48 .50 .47 .43? .46? .41? .45? .39? .39? .37? .32? .35? .34? .30? .29?
ONLINE-A .54? .50 ? .44? .52 .50 .45? .43? .43? .42? .39? .41? .42? .42? .37? .44? .38? .33?
PROMT-HYBRID .55? .52 .56? ? .45? .47 .47 .46? .50 .44? .42? .40? .41? .38? .39? .39? .33? .34?
PROMT-RULE .54? .50 .48 .55? ? .51 .47 .47 .45? .38? .42? .40? .43? .41? .43? .38? .35? .29?
UEDIN-STANFORD .56? .53 .50 .53 .49 ? .48 .50 .47 .44? .46 .36? .36? .36? .36? .35? .30? .32?
EU-BRIDGE .59? .57? .55? .53 .53 .52 ? .46? .43? .52 .42? .42? .45? .35? .36? .41? .38? .30?
RBMT4 .55? .54? .57? .54? .53 .50 .54? ? .53 .49 .44? .49 .50 .47 .40? .42? .38? .40?
UEDIN-PHRASE .57? .59? .57? .50 .55? .53 .57? .47 ? .50 .55? .47 .45? .44? .43? .42? .37? .34?
RBMT1 .59? .55? .58? .56? .62? .56? .48 .51 .50 ? .47 .47 .45? .47 .43? .42? .38? .41?
KIT .62? .61? .61? .58? .58? .54 .58? .56? .45? .53 ? .47 .49 .46 .43? .48 .34? .37?
STANFORD-UNC .62? .61? .59? .60? .60? .64? .58? .51 .53 .53 .53 ? .48 .47 .45? .45? .39? .41?
CIMS .64? .63? .58? .59? .57? .64? .55? .50 .55? .55? .51 .52 ? .53 .42? .52 .47 .42?
STANFORD .67? .68? .58? .62? .59? .64? .65? .53 .56? .53 .54 .53 .47 ? .53 .42? .39? .48
UU .62? .65? .62? .61? .57? .64? .64? .60? .57? .57? .57? .55? .58? .47 ? .46? .45? .38?
ONLINE-C .70? .66? .56? .61? .62? .65? .59? .58? .58? .58? .52 .55? .48 .58? .54? ? .48 .47
IMS-TTT .70? .70? .62? .67? .65? .70? .62? .62? .63? .62? .66? .61? .53 .61? .55? .52 ? .49
UU-DOCENT .75? .71? .67? .66? .71? .68? .70? .60? .66? .59? .63? .59? .58? .52 .62? .53 .51 ?
score .60 .59 .56 .56 .56 .56 .54 .51 .51 .50 .48 .47 .46 .44 .43 .42 .38 .37
rank 1-2 1-2 3-6 3-6 3-6 3-6 7 8-10 8-10 8-10 11-12 11-13 12-14 13-15 14-16 15-16 17-18 17-18
Table 31: Head to head comparison, ignoring ties, for English-German systems
56
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
O
N
L
I
N
E
-
B
S
T
A
N
F
O
R
D
O
N
L
I
N
E
-
A
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
UEDIN-PHRASE ? .48 .48 .45? .43? .28? .28? .19?
KIT .52 ? .54? .48 .44? .31? .29? .21?
ONLINE-B .52 .46? ? .51 .47 .31? .30? .24?
STANFORD .55? .52 .49 ? .46? .34? .30? .23?
ONLINE-A .57? .56? .53 .54? ? .32? .29? .21?
RBMT1 .72? .69? .69? .66? .68? ? .42? .33?
RBMT4 .72? .71? .70? .70? .71? .58? ? .39?
ONLINE-C .81? .79? .76? .77? .79? .67? .61? ?
score .63 .60 .59 .58 .57 .40 .35 .25
rank 1 2-4 2-4 2-4 5 6 7 8
Table 32: Head to head comparison, ignoring ties, for French-English systems
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
M
A
T
R
A
N
M
A
T
R
A
N
-
R
U
L
E
S
O
N
L
I
N
E
-
A
U
U
-
D
O
C
E
N
T
P
R
O
M
T
-
H
Y
B
R
I
D
U
A
P
R
O
M
T
-
R
U
L
E
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
ONLINE-B ? .46? .48 .46? .50 .41? .39? .39? .37? .38? .37? .35? .27?
UEDIN-PHRASE .54? ? .50 .47 .46 .46? .42? .41? .46? .42? .35? .34? .33?
KIT .52 .50 ? .53 .51 .50 .43? .49 .41? .42? .35? .37? .29?
MATRAN .54? .53 .47 ? .49 .50 .43? .43? .38? .48 .40? .34? .32?
MATRAN-RULES .50 .54 .49 .51 ? .53 .40? .45? .46? .42? .44? .40? .34?
ONLINE-A .59? .54? .50 .50 .47 ? .44? .49 .47 .45? .42? .37? .34?
UU-DOCENT .61? .58? .57? .57? .60? .56? ? .43? .52 .46? .39? .44? .33?
PROMT-HYBRID .61? .59? .51 .57? .55? .51 .57? ? .50 .41? .46? .44? .35?
UA .63? .54? .59? .62? .54? .53 .48 .50 ? .49 .46? .43? .34?
PROMT-RULE .62? .58? .58? .52 .58? .55? .54? .59? .51 ? .47 .39? .37?
RBMT1 .63? .65? .65? .60? .56? .58? .61? .54? .54? .53 ? .46? .45?
RBMT4 .65? .66? .63? .66? .60? .63? .56? .56? .57? .61? .54? ? .45?
ONLINE-C .73? .67? .71? .67? .66? .66? .67? .65? .66? .63? .55? .55? ?
score .59 .57 .55 .55 .54 .53 .49 .49 .48 .47 .43 .40 .34
rank 1 2-4 2-5 2-5 4-6 4-6 7-9 7-10 7-10 8-10 11 12 13
Table 33: Head to head comparison, ignoring ties, for English-French systems
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
S
Y
N
T
A
X
C
M
U
U
E
D
I
N
-
P
H
R
A
S
E
A
F
R
L
I
I
T
-
B
O
M
B
A
Y
D
C
U
-
L
I
N
G
O
2
4
I
I
I
T
-
H
Y
D
E
R
A
B
A
D
ONLINE-B ? .36? .33? .37? .31? .21? .20? .14? .00
ONLINE-A .64? ? .48 .47? .44? .31? .30? .24? .12?
UEDIN-SYNTAX .67? .52 ? .47 .46? .33? .29? .24? .12?
CMU .63? .53? .53 ? .47 .37? .31? .26? .11?
UEDIN-PHRASE .69? .56? .54? .53 ? .40? .33? .25? .11?
AFRL .79? .69? .67? .63? .60? ? .53 .40? .16?
IIT-BOMBAY .80? .70? .71? .69? .67? .47 ? .44? .19?
DCU-LINGO24 .86? .76? .76? .74? .75? .60? .56? ? .19?
IIIT-HYDERABAD .94? .88? .88? .89? .89? .84? .81? .81? ?
score .75 .62 .61 .60 .57 .44 .41 .34 .13
rank 1 2-3 2-4 3-4 5 6-7 6-7 8 9
Table 34: Head to head comparison, ignoring ties, for Hindi-English systems
57
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
U
N
C
N
S
T
R
U
E
D
I
N
-
P
H
R
A
S
E
C
U
-
M
O
S
E
S
I
I
T
-
B
O
M
B
A
Y
I
P
N
-
U
P
V
-
C
N
T
X
T
D
C
U
-
L
I
N
G
O
2
4
I
P
N
-
U
P
V
-
N
O
D
E
V
M
A
N
A
W
I
-
H
1
M
A
N
A
W
I
M
A
N
A
W
I
-
R
M
O
O
V
ONLINE-B ? .49 .28? .29? .27? .23? .22? .20? .17? .12? .13? .13?
ONLINE-A .51 ? .31? .29? .27? .25? .20? .20? .21? .19? .16? .15?
UEDIN-UNCNSTR .72? .69? ? .44? .49 .39? .40? .34? .39? .29? .30? .27?
UEDIN-PHRASE .71? .71? .56? ? .48 .45? .44? .39? .37? .31? .31? .32?
CU-MOSES .73? .73? .51 .52 ? .47 .42? .40? .45? .36? .35? .33?
IIT-BOMBAY .77? .75? .61? .55? .53 ? .50 .47 .45? .41? .40? .36?
IPN-UPV-CNTXT .78? .80? .60? .56? .58? .50 ? .51 .41? .40? .40? .37?
DCU-LINGO24 .80? .80? .66? .61? .60? .53 .49 ? .52 .41? .41? .39?
IPN-UPV-NODEV .83? .79? .61? .63? .55? .55? .59? .48 ? .46? .44? .38?
MANAWI-H1 .88? .81? .71? .69? .64? .59? .60? .59? .54? ? .35? .34?
MANAWI .87? .84? .70? .69? .65? .60? .60? .59? .56? .65? ? .39?
MANAWI-RMOOV .87? .85? .73? .68? .67? .64? .63? .61? .62? .66? .61? ?
score .77 .75 .57 .54 .52 .47 .46 .43 .42 .38 .35 .31
rank 1 2 3 4-5 4-5 6-7 6-7 8-9 8-9 10-11 10-11 12
Table 35: Head to head comparison, ignoring ties, for English-Hindi systems
A
F
R
L
-
P
E
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
P
H
R
A
S
E
Y
A
N
D
E
X
O
N
L
I
N
E
-
G
A
F
R
L
U
E
D
I
N
-
S
Y
N
T
A
X
K
A
Z
N
U
R
B
M
T
1
R
B
M
T
4
AFRL-PE ? .42? .40? .39? .39? .41? .35? .39? .28? .26? .26? .29? .21?
ONLINE-B .58? ? .42? .43? .45? .45? .42? .43? .46? .37? .33? .29? .31?
ONLINE-A .60? .58? ? .50 .45? .51 .47 .45? .42? .40? .33? .32? .30?
PROMT-HYBRID .61? .57? .50 ? .47 .45? .49 .44? .43? .44? .39? .31? .27?
PROMT-RULE .61? .55? .55? .53 ? .46? .47 .49 .48 .42? .36? .34? .30?
UEDIN-PHRASE .59? .55? .49 .55? .54? ? .49 .50 .47 .44? .32? .37? .29?
YANDEX .65? .58? .53 .51 .53 .51 ? .48 .50 .43? .34? .36? .34?
ONLINE-G .61? .57? .55? .56? .51 .50 .52 ? .48 .43? .39? .35? .30?
AFRL .72? .54? .58? .57? .52 .53 .50 .52 ? .44? .41? .41? .37?
UEDIN-SYNTAX .74? .63? .60? .56? .58? .56? .57? .57? .56? ? .51 .36? .37?
KAZNU .74? .67? .67? .61? .64? .68? .66? .61? .59? .49 ? .44? .38?
RBMT1 .71? .71? .68? .69? .66? .63? .64? .65? .59? .64? .56? ? .47
RBMT4 .79? .69? .70? .73? .70? .71? .66? .70? .63? .63? .62? .53 ?
score .66 .58 .55 .55 .53 .53 .52 .51 .49 .45 .40 .36 .32
rank 1 2 3-5 3-5 4-7 5-8 5-8 5-8 9 10 11 12 13
Table 36: Head to head comparison, ignoring ties, for Russian-English systems
P
R
O
M
T
-
R
U
L
E
O
N
L
I
N
E
-
B
P
R
O
M
T
-
H
Y
B
R
I
D
U
E
D
I
N
-
U
N
C
N
S
T
R
O
N
L
I
N
E
-
G
O
N
L
I
N
E
-
A
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
4
R
B
M
T
1
PROMT-RULE ? .51 .45? .43? .43? .39? .38? .15? .00
ONLINE-B .49 ? .50 .47? .38? .36? .38? .16? .13?
PROMT-HYBRID .55? .50 ? .49 .47 .39? .40? .18? .15?
UEDIN-UNCNSTR .57? .53? .51 ? .50 .44? .36? .25? .18?
ONLINE-G .57? .62? .53 .50 ? .46? .44? .23? .18?
ONLINE-A .61? .64? .61? .56? .54? ? .49 .24? .18?
UEDIN-PHRASE .62? .62? .60? .64? .56? .51 ? .30? .21?
RBMT4 .85? .84? .82? .75? .77? .76? .70? ? .42?
RBMT1 .91? .87? .85? .82? .82? .82? .79? .58? ?
score .64 .64 .61 .58 .55 .51 .49 .26 .19
rank 1-2 1-2 3 4-5 4-5 6-7 6-7 8 9
Table 37: Head to head comparison, ignoring ties, for English-Russian systems
58
