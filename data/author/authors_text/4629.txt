A Transformation-based Sentence Splitting Method for Statistical Ma-
chine Translation 
Jonghoon Lee, Donghyeon Lee and Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science & Technology (POSTECH) 
{jh21983, semko, gblee}@postech.ac.kr 
 
 
 
Abstract 
We propose a transformation based sen-
tence splitting method for statistical ma-
chine translation. Transformations are ex-
panded to improve machine translation 
quality after automatically obtained from 
manually split corpus. Through a series of 
experiments we show that the transforma-
tion based sentence splitting is effective 
pre-processing to long sentence translation. 
1 Introduction 
Statistical approaches to machine translation have 
been studied actively, after the formalism of statis-
tical machine translation (SMT) is proposed by 
Brown et al (1993). Although many approaches of 
them were effective, there are still lots of problems 
to solve. Among others, we have an interest in the 
problems occurring with long sentence decoding. 
Various problems occur when we try to translate 
long input sentences because a longer sentence 
contains more possibilities of selecting translation 
options and reordering phrases. However, reorder-
ing models in traditional phrase-based systems are 
not sufficient to treat such complex cases when we 
translate long sentences (Koehn et al 2003). 
Some methods which can offer powerful reor-
dering policies have been proposed like syntax 
based machine translation (Yamada and Knight, 
2001) and Inversion Transduction Grammar (Wu, 
1997). Although these approaches are effective, 
decoding long sentences is still difficult due to 
their computational complexity. As the length of 
an input sentence becomes longer, the analysis and 
decoding become more complex. The complexity 
causes approximations and errors inevitable during 
the decoding search. 
In order to reduce this kind of difficulty caused 
by the complexity, a long sentence can be paraph-
rased by several shorter sentences with the same 
meaning. Generally, however, decomposing a 
complex sentence into sub-sentences requires in-
formation of the sentence structures which can be 
obtained by syntactic or semantic analysis. Unfor-
tunately, the high level syntactic and semantic 
analysis can be erroneous and costs as expensive as 
SMT itself. So, we don?t want to fully analyze the 
sentences to get a series of sub-sentences, and our 
approach to this problem considers splitting only 
compound sentences. 
In the past years, many research works were 
concerned with sentence splitting methods to im-
prove machine translation quality. This idea had 
been used in speech translation (Furuse et al 1998) 
and example based machine translation (Doi and 
Sumita, 2004). These research works achieved 
meaningful results in terms of machine translation 
quality. Unfortunately, however, the method of 
Doi and Sumita using n-gram is not available if the 
source language is Korean. In Korean language, 
most of sentences have special form of ending 
morphemes at the end. For that reason, we should 
determine not only the splitting position but also 
the ending morphemes that we should replace in-
stead of connecting morphemes. And the Furuse et 
al?s method involves parsing which requires heavy 
cost. 
In this paper we propose a transformation based 
splitting method to improve machine translation 
quality which can be applied to the translation 
tasks with Korean as a source language. 
2 Methods 
Our task is splitting a long compound sentence into 
short sub-sentences to improve the performance of 
phrase-based statistical machine translation system. 
We use a transformation based approach to 
accomplish our goal. 
2.1 A Concept of Transformation 
The transformation based learning (TBL) is a kind 
of rule learning methods. The formalism of TBL is 
introduced by Brill (1995). In past years, the TBL 
approach was used to solve various problems in 
natural language processing such as part of speech 
(POS) tagging and parsing (Brill, 1993). 
A transformation consists of two parts: a trigger-
ing environment and a rewriting rule. And the re-
writing rule consists of a source pattern and a tar-
get pattern. Our consideration is how to get the 
right transformations and apply them to split the 
long sentences. 
A transformation works in the following man-
ner; some portion of the input is changed by the 
rewriting rule if the input meets a condition speci-
fied in the triggering environment. The rewriting 
rule finds the source pattern in the input and rep-
laces it with the target pattern. For example, sup-
pose that a transformation which have a triggering 
environment A, source pattern B and target pattern 
C. We can describe this transformation as a sen-
tence: if a condition A is satisfied by an input sen-
tence, then replace pattern B in the input sentence 
with pattern C. 
2.2 A Transformation Based Sentence Split-
ting Method 
Normally, we have two choices when there are 
two or more transformations available for an input 
pattern at the same time. The first choice is apply-
ing the transformation one by one, and the second 
choice is applying them simultaneously. The 
choice is up to the characteristics of the problem 
that we want to solve. In our problem, we choose 
the former strategy which is applying the transfor-
mations one by one, because it gives direct intui-
tion about the process of splitting sentences. By 
choosing this strategy, we can design splitting 
process as a recursive algorithm. 
At first, we try to split an input sentence into 
two sub-sentences. If the sentence has been split by 
some transformation, the result involves exactly 
two sub-sentences. And then we try to split each 
sub-sentence again. We repeat this process in re-
cursive manner until no sub-sentences are split. 
In the above process, a sentence is split into at 
most two sub-sentences through a single trial. In a 
single trial, a transformation works in the follow-
ing manner:  If an input sentence satisfies the envi-
ronment, we substitute the source pattern into the 
target pattern. That is, replace the connecting mor-
phemes with the proper ending morphemes. And 
then we split the sentence with pre-defined posi-
tion in the transformation. And finally, we insert 
the junction word that is also pre-defined in the 
transformation between the split sentences after the 
sub sentences are translated independently. 
From the above process, we can notice easily 
that a transformation for sentence splitting consists 
of the four components: a triggering environment, 
a rewriting rule, a splitting position and a junction 
type. The contents of each component are as fol-
lows. (1) A triggering environment contains a se-
quence of morphemes with their POS tags. (2) A 
rewriting consists of a pair of sequences of POS 
tagged morphemes. (3) A junction type can have 
one of four types: ?and?, ?or?, ?but? and ?NULL?. 
(4) A splitting position is a non-negative integer 
that means the position of starting word of second 
sub-sentence. 
2.3 Learning the Transformation for Sen-
tence Splitting 
At the training phase, TBL process determines 
the order of application (or rank) of the transforma-
tions to minimize the error-rate defined by a spe-
cific measure. The order is determined by choosing 
the best rule for a given situation and applying the 
best rule for each situation iteratively. In the sen-
tence splitting task, we maximize the machine 
translation quality with BLEU score (Papineni et 
al., 2001) instead of minimizing the error of sen-
tence splitting. 
During the training phase, we determine the or-
der of applying transformation after we build a set 
of transformations. To build the set of transforma-
tions, we need manually split examples to learn the 
transformations. 
Building a transformation starts from extracting 
a rewriting rule by calculating edit-distance matrix 
between an original sentence and its split form 
from the corpus. We can easily extract the different 
parts from the matrix. 
BaseBLEU :=  BLEU score of the baseline system 
S := Split example sentence 
T := Extracted initial transformation  
for each t? T  
    for each s?S 
        while true 
             try to split s with t 
             if mis-splitting is occurred 
                  Expand environment 
             else exit while loop 
             if environment cannot be expanded 
                  exit while loop 
S? := apply t to S 
    Decode S? 
    BLEU := measure BLEU 
    Discard t if BLEU < BaseBLEU 
sort  T w.r.t. BLEU 
From the difference pattern, we can make the 
source pattern of a rewriting rule by taking the dif-
ferent parts of the original sentence side. Similarly, 
the target pattern can be obtained from the differ-
ent parts of split form. And the junction type and 
splitting position are directly obtained from the 
difference pattern. Finally, the transformation is 
completed by setting the triggering environment as 
same to the source pattern. The set of initial trans-
formations is obtained by repeating this process on 
all the examples. 
The Transformations for sentence splitting are 
built from the initial transformations through ex-
panding process. In the expanding process, each 
rule is applied to the split examples. We expand 
the triggering environment with some heuristics (in 
section 2.4), if a sentence is a mis-split. 
And finally, in order to determine the rank of 
each transformation, we sorted the extracted trans-
formations by decreasing order of resulted BLEU 
scores after applying the transformation to each 
training sentence. And some transformations are 
discarded if they decrease the BLEU score. This 
process is different from original TBL. The mod-
ified TBL learning process is described in figure 1. 
2.4 Expanding Triggering Environments 
Expanding environment should be treated very 
carefully. If the environment is too specific, the 
transformation cannot be used in real situation. On 
the other hand, if it is too general, then the trans-
formation becomes erroneous. 
Our main strategy for expanding the environ-
ment is to increase context window size of the 
triggering environment one by one until it causes 
no error on the training sentences. In this manner, 
we can get minimal error-free transformations on 
the sentence splitting corpus. 
We use two different windows to define a trig-
gering environment: one for morpheme and anoth-
er for its part of speech (POS) tag. Figure 2 shows 
this concept of two windows. The circles corres-
pond to sequences of morphemes and POS tags in 
a splitting example. Window 1 represents a mor-
pheme context and window 2 represents a POS tag 
context. The windows are independently expanded 
from the initial environment which consists of a 
morpheme ?A? and its POS tag. In the figure, win-
dow 1 is expanded to one forward morpheme and 
one backward morpheme while window 2 is ex-
panded to two backward POS tags. 
In order to control these windows, we defined 
some heuristics by specifying the following three 
policies of expanding windows: no expansion, 
forward only and forward and backward. From 
those three polices, we have 9 combinations of 
heuristics because we have two windows. By ob-
serving the behavior of these heuristics, we can 
estimate what kind of information is most impor-
tant to determine the triggering environment. 
Figure 1. Modified TBL for sentence splitting 
 
 
 
Figure 2. Window-based heuristics for triggering 
environments 
 
 
 
 
  
 
Test No. Window1 policy Window2 policy 
Test 1 
No expansion 
No expansion 
Test 2 Forward only 
Test 3 Free expansion 
Test 4 
Forward only 
No expansion 
Test 5 Forward only 
Test 6 Free expansion 
Test 7 
Free expansion 
No expansion 
Test 8 Forward only 
Test 9 Free expansion 
 
Table 2.Experimental setup 
 
 
 
 
We have at most 4 choices for a single step of 
the expanding procedure: forward morpheme, 
backward morpheme, forward POS tag, and back-
ward POS tag. We choose one of them in a fixed 
order: forward POS tag, forward morpheme, 
backward POS tag and backward morpheme. 
These choices can be limited by 9 heuristics. For 
example, suppose that we use a heuristic with for-
ward policy on morpheme context window and no 
expansion policy for POS tag context window. In 
this case we have only one choice: forward mor-
pheme. 
3  Experiments 
We performed a series of experiments on Korean 
to English translation task to see how the sentence 
splitting affects machine translation quality and 
which heuristics are the best. Our baseline system 
built with Pharaoh (Koehn, 2004) which is most 
popular phrase-based decoder. And trigram lan-
guage model with KN-discounting (Kneser and 
Ney, 1995) built by SRILM toolkit (Stolcke, 2002) 
is used. 
Test 
No. 
# of  af-
fected sen-
tences 
BLEU score 
Before 
splitting 
After 
splitting 
Test 1 209 0.1778 0.1838 
Test 2 142 0.1564 0.1846 
Test 3 110 0.1634 0.1863 
Test 4 9 0.1871 0.2150 
Test 5 96 0.1398 0.1682 
Test 6 100 0.1452 0.1699 
Test 7 8 0.2122 0.2433 
Test 8 157 0.1515 0.1727 
Test 9 98 0.1409 0.1664 
Table 1 shows the corpus statistics used in the 
experiments. The training corpus for MT system 
has been built by manually translating Korean sen-
tences which are collected from various sources. 
We built 123,425 sentence pairs for training SMT, 
1,577 pairs for splitting and another 1,577 pairs for 
testing. The domain of the text is daily conversa-
tions and travel expressions. The sentence splitting 
corpus has been built by extracting long sentences 
from the source-side mono-lingual corpus. The 
sentences in the splitting corpus have been manual-
ly split. 
The experimental settings for comparing 9 heu-
ristics described in the section 2.4 are listed in ta-
ble 2. Each experiment corresponds to a heuristic. 
To see the effect of sentence splitting on transla-
tion quality, we evaluated BLEU score for affected 
sentenced by the splitting.  The results are shown 
in table 3. Each test number shows the effect of 
transformation-based sentence splitting with dif-
ferent window selection heuristics listed in table 2. 
The scores are consistently increased with signifi-
cant differences. After analyzing the results of ta-
ble 3, we notice that we can expect some perfor-
 
SMT Splitting 
Korean English Before Split After Split 
Train # of Sentences 123,425 1,577 1,906 
# of Words 1,083,912 916,950 19,918 20,243 
Vocabulary 15,002 14,242 1,956 1,952 
Test #of Sentences 1,577 - - 
 
Table 1. Corpus statistics 
Table 3. BLEU scores of affected sentences 
 
mance gain when the average sentence length is 
long. 
The human evaluation shows more promising 
results in table 4. In the table, the superior change 
means that the splitting results in better translation 
and inferior means the opposite case. Two ratios 
are calculated to see the effects of sentence split-
ting. The ratio ?sup/inf? shows the ratio of superior 
over inferior splitting. And ratio trans/change 
shows how many sentences are affected by a trans-
formation in an average. In most of the experi-
ments, the number of superior splitting is over 
three times larger than that of inferior ones. This 
result means that the sentence splitting is a helpful 
pre-processing for machine translation. 
We listed some example translations affected by 
sentence splitting in the table 5. In the three cases, 
junction words don?t appear in the results of trans-
lation after split because their junction types are 
NULL that involves no junction word. Although 
several kinds of improvements are observed in su-
perior cases, the most interesting case occurs in 
out-of-vocabulary (OOV) cases. A translation re-
sult has a tendency to be a word salad when 
OOV?s are included in the input sentence. In this 
case, the whole sentence may lose its original 
meaning in the result of translation. But after split-
ting the input sentence, the OOV?s have a high 
chance to be located in one of the split sub-
sentences. Then the translation result can save at 
least a part of its original meaning. This case oc-
curs easily if an input sentence includes only one 
OOV. The Superior change of table 5 is the case. 
Although both baseline and split are far from the 
reference, split catches some portion of the mean-
ing. 
Test 
No. 
# of trans-
formations 
(rules) 
# of 
changes 
(sentences) 
# of supe-
rior 
changes 
# of infe-
rior 
changes 
# of insig-
nificant 
changes 
Ratio 
Sup/Inf 
Ratio 
trans/chang
e 
1 34 209 60 30 119 2.00 6.15 
2 177 142 43 9 90 4.78 0.802 
3 213 110 29 9 72 3.22 0.516 
4 287 9 4 1 4 4.00 0.031 
5 206 96 25 4 67 6.25 0.466 
6 209 100 23 8 69 2.88 0.478 
7 256 8 3 1 4 3.00 0.031 
8 177 157 42 10 102 4.20 0.887 
9 210 98 21 4 73 5.25 0.467 
Table 4. Human evaluation results 
Superior change 
Reference I saw that some items are on sale on window . what are they ? 
Baseline 
What kind of items do you have this item in OOV some discount, I get a 
discount ? 
Split 
You have this item in OOV some discount . what kind of items do I get 
a discount ? 
Insignificant 
change 
Reference What is necessary to be issued a new credit card? 
Baseline I ?d like to make a credit card . What do I need? 
Split I ?d like to make a credit card . What is necessary? 
Inferior change 
 
Reference 
I ?d like to make a reservation by phone and tell me the phone number 
please . 
Baseline 
I ?d like to make a reservation but can you tell me the phone number , 
please . 
Split I  ?d like to make a reservation . can you tell me the , please . 
Table 5. Example translations (The sentences are manually re-cased for readability) 
Most of the Inferior cases are caused by mis-
splitting. Mis-splitting includes a case of splitting a 
sentence that should not be split or splitting a sen-
tence on the wrong position. This case can be re-
duced by controlling the heuristics described in 
section 2.4. But the problem is that the effort to 
reducing inferior cases also reduces the superior 
cases. To compare the heuristics each other in this 
condition, we calculated the ratio of superior and 
inferior cases. The best heuristic is test no. 5 in 
terms of the ratio of sup/inf. 
The test no. 4 and 7 show that a trans-formation 
becomes very specific when lexical information is 
used alone. Hence the ratio trans/change becomes 
below 0.01 in this case.  And test no. 1 shows that 
the transformations with no environment expan-
sion are erroneous since it has the lowest ratio of 
sup/inf. 
4 Conclusion 
We introduced a transformation based sentence 
splitting method for machine translation as a effec-
tive and efficient pre-processing. A transformation 
consists of a triggering environment and a rewrit-
ing rule with position and junction type informa-
tion. The triggering environment of a transforma-
tion is extended to be error-free with respect to 
training corpus after a rewriting rule is extracted 
from manually split examples. The expanding 
process for the transformation can be generalized 
by adding POS tag information into the triggering 
environment. 
The experimental results show that the effect of 
splitting is clear in terms of both automatic evalua-
tion metric and human evaluation. The results con-
sistently state that the statistical machine transla-
tion quality can be improved by transformation 
based sentence splitting method. 
Acknowledgments 
This research was supported by the MIC (Ministry 
of Information and Communication), Korea, under 
the ITRC (Information Technology Research Cen-
ter) support program supervised by the IITA (Insti-
tute of Information Technology Assessment) (II-
TA-2006-C1090-0603-0045). The parallel corpus 
was courteously provided by Infinity Telecom, Inc. 
References 
Eric Brill. 1993. Transformation-based error-driven 
parsing. In Proc. of third International Workshop on 
Parsing.  
Eric Brill. 1995. Transformation-based error-driven 
learning and natural language processing: A Case 
Study in Part-of-Speech Tagging. Computational 
Linguistics 21(4):543-565. 
Peter F. Brown, Stephen A. Della Pietra, Vincent 
J.Della Pietra and Robert L. Mercer. 1993. The Ma-
thematics of Statistical Machine Translation: Parame-
ter estimation. Computational Linguistics, 19(2):263-
312. 
Takao Doi and Eiichiro Sumita. 2004. Splitting input 
sentence for machine translation using language 
model with sentence similarity. In Proc. of the 20th 
international conference on Computational Linguis-
tics. 
Osamu Furuse, Setsuo Yamada and Kazuhide Yamamo-
to. 1998. Splitting Long or Ill-formed Input for Ro-
bust Spoken-language Translation. In Proc of the 36th 
annual meeting on Association for Computational 
Linguistics. 
Reinhard Kneser and Hermann Ney. 1995. Improved 
backing-off for m-gram lnguage modeling. In Proc. 
of the International Conference on Acoustics, Speech, 
and Signal Processing (ICASSP). 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation mod-
els. In Proc. of the 6th Conference of the Association 
for Machine translation in the Americas. 
Philipp Koehn, Franz Josef Och and Kevin Knight. 
2003. Statistical Phrase-Based Translation. In Proc of 
the of the 2003 Conference of the North American 
Chapter of the Association for Computational Lin-
guistics on Human Language Technology. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic 
evaluation of Machine Translation. Technical Report 
RC22176, IBM. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. In Proc. of the 7th International 
Conference on Spoken Language Processing (ICSLP). 
 Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics 23(3):377-404. 
Kenji Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation Model. In Proc. of the confe-
rence of the Association for Computational Linguis-
tics (ACL). 
 
NAACL HLT Demonstration Program, pages 7?8,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
POSSLT: A Korean to English Spoken Language Translation System 
 
 
Donghyeon Lee, Jonghoon Lee, Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science & Technology (POSTECH) 
San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea 
{semko, jh21983, gblee}@postech.ac.kr 
 
 
 
 
Abstract 
The POSSLT 1  is a Korean to English 
spoken language translation (SLT) system. 
Like most other SLT systems, automatic 
speech recognition (ASR), machine trans-
lation (MT), and text-to-speech (TTS) are 
coupled in a cascading manner in our 
POSSLT. However, several novel tech-
niques are applied to improve overall 
translation quality and speed. Models 
used in POSSLT are trained on a travel 
domain conversational corpus. 
1 Introduction 
Spoken language translation (SLT) has become 
more important due to globalization. SLT systems 
consist of three major components: automatic 
speech recognition (ASR), statistical machine 
translation (SMT), text-to-speech (TTS). Currently, 
most of SLT systems are developed in a cascading 
method. Simple SLT systems translate a single best 
recognizer output, but, translation quality can be 
improved using the N-best hypotheses or lattice 
provided by the ASR (Zhang et. al., 2004; Saleem 
et. al., 2004). 
In POSSLT, we used an N-best hypothesis re-
ranking based on both ASR and SMT features, and 
divided the language model of the ASR according 
to the specific domain situation. To improve the 
Korean-English SMT quality, several new tech-
                                                          
1 POSSLT stands for POSTECH Spoken Language Transla-
tion system 
niques can be applied (Lee et. al., 2006-b). The 
POSSLT applies most of these techniques using a 
preprocessor. 
2 System Description 
The POSSLT was developed by integrating ASR, 
SMT, and TTS. The system has a pipelined archi-
tecture as shown in Fig. 1. LM loader, preproces-
sor and re-ranking module are newly developed to 
improve the translation quality and speed for 
POSSLT. 
 
 
Figure 1: Overview of POSSLT 
2.1 ASR 
The system used HTK-based continuous speech 
recognition engine properly trained for Korean. 
The acoustic model, lexical model and language 
model of Korean are trained for conversational 
corpus. The phonetic set for Korean has 48 pho-
neme-like-units, and we used three-state tri-phone 
hidden Markov models and trigram language mod-
7
els. Pronunciation lexicons are automatically built 
by a Korean grapheme-to-phoneme (G2P) tool 
(Lee et. al., 2006-a). We used an eojeol2 as a basic 
recognition unit for lexical and language models, 
because an eojeol-based recognition unit has the 
higher accuracy than the morpheme-based one. 
The ASR produces the N-best hypotheses deter-
mined through the decoding process, which are 
used as the input of SMT. 
2.2 SMT 
We implemented a Korean-English phrase-based 
SMT decoder based on Pharaoh (Koehn, 2004). 
The decoder needs a phrase translation model for 
the Korean-English pair and a language model for 
English. We used the Pharaoh training module and 
GIZA++ (Och and Ney, 2000) to construct the 
phrase translation table. For language modeling, 
SRILM toolkit (Stolcke, 2002) was used to build a 
trigram language model. 
2.3 TTS 
We used Microsoft SAPI 5.1 TTS engine for Eng-
lish TTS. The final best translation is pronounced 
using the engine. 
2.4 LM Loader 
In cascading SLT systems, SMT coverage depends 
on the used ASR. In order to increase the ASR 
coverage, our system loads and unloads the ASR 
language models dynamically. In our system which 
uses a travel corpus, language models are built for 
ten domain situation categories such as an airport, 
a hotel, a shopping, etc. Besides user utterances, 
user selection of the situation is needed as an input 
to decide which language model have to be loaded 
in advance. By using the divided language models, 
many benefits such as fast decoding, higher accu-
racy and more coverage can be obtained. 
2.5 Preprocessor 
In the Korean-English SMT task, there have been 
developed several techniques for improving the 
translation quality such as changing spacing units 
into morphemes, adding POS tag information, and 
deleting useless words (Lee et. al., 2006-b). 
                                                          
2 Eojeol is a spacing unit in Korean and typically consists of 
more than one morpheme. 
However, for these techniques, Part-Of-Speech 
(POS) tagger is needed. If the final analyzed form 
of an eojeol (in the form of a sequence of mor-
phemes plus POS tags) is defined as a word in the 
ASR lexicon, the transformed sentences are direct-
ly generated by the ASR only, so POS tagger er-
rors can be removed from the system. Preprocessor 
also removes useless words in SMT in the trans-
formed sentences produced by the ASR. 
2.6 Re-ranking Module  
We implemented a re-ranking module to make a 
robust SLT system against the speech recognition 
errors. The re-ranking module uses several fea-
tures: ASR acoustic model scores, ASR language 
model scores, and SMT translation scores. Finally, 
the re-ranking module sorts the N-best lists by 
comparing the total scores. 
Acknowledgements  
This research was supported by the MIC (Ministry of 
Information and Communication), Korea, under the 
ITRC (Information Technology Research Center) sup-
port program supervised by the IITA (Institute of In-
formation Technology Assessment; IITA-2005-C1090-
0501-0018) 
References  
A. Stolcke. 2002. SRILM ? An Extensible Language Modeling 
Toolkit. Proc. of ICSLP. 
F. J. Och and H. Ney. 2000. Improved statistical alignment 
models. Proc. of 38th Annual Meeting of the ACL, page 
440-447, Hongkong, China, October 2000. 
Jinsik Lee, Seungwon Kim, Gary Geunbae Lee. 2006-a. Gra-
pheme-to-Phoneme Conversion Using Automatically Ex-
tracted Associative Rules for Korean TTS System. Proc. of 
Interspeech-ICSLP. 
Jonghoon Lee, Donghyeon Lee, Gary Geunbae Lee. 2006-b. 
Improving Phrase-based Korean-English Statistical Ma-
chine Translation. Proc. of Interspeech-ICSLP. 
P. Koehn. 2004. Pharaoh: A Beam Search Decoder for 
Phrase-based Statistical Machine Translation Models. 
Proc. of AMTA, Washington DC. 
R. Zhang, G. Kikui, H. Yamamoto, T. Watanabe, F. Soong, 
and W. K. Lo. 2004. A unified approach in speech-to-
speech translation: Integrating features of speech recogni-
tion and machine translation. Proc. of Coling 2004, Geve-
va. 
S. Saleem, S. Chen Jou, S. Vogel, and T.Schultz. 2004. Using 
word lattice information for a tighter coupling in speech 
translation systems. Proc. of ICSLP 2004, Jeju, Korea. 
8
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 564?571,
Beijing, August 2010
A Cross-lingual Annotation Projection Approach
for Relation Detection
Seokhwan Kim?, Minwoo Jeong?, Jonghoon Lee?, Gary Geunbae Lee?
?Department of Computer Science and Engineering,
Pohang University of Science and Technology
{megaup|jh21983|gblee}@postech.ac.kr
?Saarland University
m.jeong@mmci.uni-saarland.de
Abstract
While extensive studies on relation ex-
traction have been conducted in the last
decade, statistical systems based on su-
pervised learning are still limited because
they require large amounts of training data
to achieve high performance. In this pa-
per, we develop a cross-lingual annota-
tion projection method that leverages par-
allel corpora to bootstrap a relation detec-
tor without significant annotation efforts
for a resource-poor language. In order to
make our method more reliable, we intro-
duce three simple projection noise reduc-
tion methods. The merit of our method is
demonstrated through a novel Korean re-
lation detection task.
1 Introduction
Relation extraction aims to identify semantic re-
lations of entities in a document. Many rela-
tion extraction studies have followed the Rela-
tion Detection and Characterization (RDC) task
organized by the Automatic Content Extraction
project (Doddington et al, 2004) to make multi-
lingual corpora of English, Chinese and Ara-
bic. Although these datasets encourage the de-
velopment and evaluation of statistical relation
extractors for such languages, there would be a
scarcity of labeled training samples when learn-
ing a new system for another language such as
Korean. Since manual annotation of entities and
their relations for such resource-poor languages
is very expensive, we would like to consider in-
stead a weakly-supervised learning technique in
order to learn the relation extractor without sig-
nificant annotation efforts. To do this, we propose
to leverage parallel corpora to project the relation
annotation on the source language (e.g. English)
to the target (e.g. Korean).
While many supervised machine learning ap-
proaches have been successfully applied to the
RDC task (Kambhatla, 2004; Zhou et al, 2005;
Zelenko et al, 2003; Culotta and Sorensen, 2004;
Bunescu and Mooney, 2005; Zhang et al, 2006),
few have focused on weakly-supervised relation
extraction. For example, (Zhang, 2004) and (Chen
et al, 2006) utilized weakly-supervised learning
techniques for relation extraction, but they did
not consider weak supervision in the context of
cross-lingual relation extraction. Our key hypoth-
esis on the use of parallel corpora for learning
the relation extraction system is referred to as
cross-lingual annotation projection. Early stud-
ies of cross-lingual annotation projection were ac-
complished for lexically-based tasks; for exam-
ple part-of-speech tagging (Yarowsky and Ngai,
2001), named-entity tagging (Yarowsky et al,
2001), and verb classification (Merlo et al, 2002).
Recently, there has been increasing interest in ap-
plications of annotation projection such as depen-
dency parsing (Hwa et al, 2005), mention de-
tection (Zitouni and Florian, 2008), and semantic
role labeling (Pado and Lapata, 2009). However,
to the best of our knowledge, no work has reported
on the RDC task.
In this paper, we apply a cross-lingual anno-
tation projection approach to binary relation de-
tection, a task of identifying the relation between
two entities. A simple projection method propa-
gates the relations in source language sentences to
564
word-aligned target sentences, and a target rela-
tion detector can bootstrap from projected annota-
tion. However, this automatic annotation is unre-
liable because of mis-classification of source text
and word alignment errors, so it causes a critical
falling-off in annotation projection quality. To al-
leviate this problem, we present three noise reduc-
tion strategies: a heuristic filtering; an alignment
correction with dictionary; and an instance selec-
tion based on assessment, and combine these to
yield a better result.
We provide a quantitive evaluation of our
method on a new Korean RDC dataset. In our
experiment, we leverage an English-Korean par-
allel corpus collected from the Web, and demon-
strate that the annotation projection approach and
noise reduction method are beneficial to build an
initial Korean relation detection system. For ex-
ample, the combined model of three noise reduc-
tion methods achieves F1-scores of 36.9% (59.8%
precision and 26.7% recall), favorably comparing
with the 30.5% shown by the supervised base-
line.1
The remainder of this paper is structured as fol-
lows. In Section 2, we describe our cross-lingual
annotation projection approach to relation detec-
tion task. Then, we present the noise reduction
methods in Section 3. Our experiment on the pro-
posed Korean RDC evaluation set is shown in Sec-
tion 4 and Section 5, and we conclude this paper
in Section 6.
2 Cross-lingual Annotation Projection
for Relation Detection
The annotation projection from a resource-rich
language L1 to a resource-poor language L2 is
performed by a series of three subtasks: annota-
tion, projection and assessment.
The annotation projection for relation detection
can be performed as follows:
1) For a given pair of bi-sentences in parallel cor-
pora between a resource-rich language L1 and
a target language L2, the relation detection task
is carried out for the sentence in L1.
1The dataset and the parallel corpus are available on the
author?s website,
http://isoft.postech.ac.kr/?megaup/research/resources/.
2) The annotations obtained by analyzing the sen-
tence in L1 are projected onto the sentence in
L2 based on the word alignment information.
3) The projected annotations on the sentence in
L2 are utilized as resources to perform the re-
lation detection task for the language L2.
2.1 Annotation
The first step to projecting annotations from L1
onto L2 is obtaining annotations for the sentences
in L1. Since each instance for relation detection
is composed of a pair of entity mentions, the in-
formation about entity mentions on the given sen-
tences should be identified first. We detect the
entities in the L1 sentences of the parallel cor-
pora. Entity identification generates a number of
instances for relation detection by coupling two
entities within each sentence. For each instance,
the existence of semantic relation between entity
mentions is explored, which is called relation de-
tection. We assume that there exist available mod-
els or systems for all annotation processes, includ-
ing not only an entity tagger and a relation de-
tector themselves, but also required preprocessors
such as a part-of-speech tagger, base-phrase chun-
ker, and syntax parser for analyzing text in L1.
Figure 1 shows an example of annotation pro-
jection for relation detection of a bitext in En-
glish and Korean. The annotation of the sentence
in English shows that ?Jan Mullins? and ?Com-
puter Recycler Incorporated? are entity mentions
of a person and an organization, respectively. Fur-
thermore, the result indicates that the pair of en-
tities has a semantic relationship categorized as
?ROLE.Owner? type.
2.2 Projection
In order to project the annotations from the sen-
tences in L1 onto the sentences in L2, we utilize
the information of word alignment which plays
an important role in statistical machine transla-
tion techniques. The word alignment task aims
to identify translational relationships among the
words in a bitext and produces a bipartite graph
with a set of edges between words with transla-
tional relationships as shown in Figure 1. In the
same manner as the annotation in L1, entities are
565
????????
(keom-pyu-teo-ri-sa-i-keul-reo)
?
(ui)
??
(sa-jang)
?
(eun)
? ??
(ra-go)
???
(mal-haet-da)
Mullins, owner of Incorporated said that ...
?
(jan)
???
(meol-rin-seu)
Jan Computer Recycler
ROLE.Owner
PER ORG
ORG PER
ROLE.Owner
Figure 1: An example of annotation projection for relation detection of a bitext in English and Korean
considered as the first units to be projected. We as-
sume that the words of the sentences in L2 aligned
with a given entity mention in L1 inherit the infor-
mation about the original entity in L1.
After projecting the annotations of entity men-
tions, the projections for relational instances fol-
low. A projection is performed on a projected in-
stance in L2 which is a pair of projected entities
by duplicating annotations of the original instance
in L1.
Figure 1 presents an example of projection of a
positive relational instance between ?Jan Mullins?
and ?Computer Recycler Incorporated? in the
English sentence onto its translational counter-
part sentence in Korean. ?Jan meol-rin-seu? and
?keom-pyu-teo-ri-sa-i-keul-reo? are labeled as en-
tity mentions with types of a person?s name and an
organization?s name respectively. In addition, the
instance composed of the two projected entities is
annotated as a positive instance, because its orig-
inal instance on the English sentence also has a
semantic relationship.
As the description suggests, the annotation pro-
jection approach is highly dependant on the qual-
ity of word alignment. However, the results of au-
tomatic word alignment may include several noisy
or incomplete alignments because of technical dif-
ficulties. We present details to tackle the problem
by relieving the influence of alignment errors in
Section 3.
2.3 Assessment
The most important challenge for annotation pro-
jection approaches is how to improve the robust-
ness against the erroneous projections. The noise
produced by not only word alignment but also
mono-lingual annotations in L1 accumulates and
brings about a drastic decline in the quality of pro-
jected annotations.
The simplest policy of utilizing the projected
annotations for relation detection in L2 is to con-
sider that all projected instances are equivalently
reliable and to employ entire projections as train-
ing instances for the task without any filtering. In
contrast with this policy, which is likely to be sub-
standard, we propose an alternative policy where
the projected instances are assessed and only the
instances judged as reliable by the assessment are
utilized for the task. Details about the assessment
are provided in Section 3.
3 Noise Reduction Strategies
The efforts to reduce noisy projections are consid-
ered indispensable parts of the projection-based
relation detection method in a resource-poor lan-
guage. Our noise reduction approach includes the
following three strategies: heuristic-based align-
ment filtering, dictionary-based alignment correc-
tion, and assessment-based instance selection.
3.1 Heuristic-based Alignment Filtering
In order to improve the performance of annotation
projection approaches, we should break the bottle-
neck caused by the low quality of automatic word
alignment results. As relation detection is carried
out for each instance consisting of two entity men-
tions, the annotation projection for relation detec-
tion concerns projecting only entity mentions and
566
their relational instances. Since this is different
from other shallower tasks such as part-of-speech
tagging, base phrase chunking, and dependency
parsing which should consider projections for all
word units, we define and apply some heuristics
specialized to projections of entity mentions and
relation instances to improve robustness of the
method against erroneous alignments, as follows:
? A projection for an entity mention should
be based on alignments between contiguous
word sequences. If there are one or more
gaps in the word sequence in L2 aligned
with an entity mention in the sentence in
L1, we assume that the corresponding align-
ments are likely to be erroneous. Thus, the
alignments of non-contiguous words are ex-
cluded in projection.
? Both an entity mention in L1 and its projec-
tion in L2 should include at least one base
noun phrase. If no base noun phrase oc-
curs in the original entity mention in L1, it
may suggest some errors in annotation for
the sentence in L1. The same case for the
projected instance raises doubts about align-
ment errors. The alignments between word
sequences without any base noun phrase are
filtered out.
? The projected instance in L2 should sat-
isfy the clausal agreement with the original
instance in L1. If entities of an instance
are located in the same clause (or differ-
ent clauses), its projected instance should be
in the same manner. The instances without
clausal agreement are ruled out.
3.2 Dictionary-based Alignment Correction
The errors in word alignment are composed of
not only imprecise alignments but also incomplete
alignments. If an alignment of an entity among
two entities of a relation instance is not provided
in the result of the word alignment task, the pro-
jection for the corresponding instance is unavail-
able. Unfortunately, the above-stated alignment
filtering heuristics for improving the quality of
projections make the annotation loss problems
worse by filtering out several alignments likely to
be noisy.
In order to solve this problem, a dictionary-
based alignment correction strategy is incorpo-
rated in our method. The strategy requires a bilin-
gual dictionary for entity mentions. Each entry of
the dictionary is a pair of entity mention in L1 and
its translation or transliteration in L2. For each
entity to be projected from the sentence in L1,
its counterpart in L2 is retrieved from the bilin-
gual dictionary. Then, we seek the retrieved entity
mention from the sentence in L2 by finding the
longest common subsequence. If a subsequence
matched to the retrieved mention is found in the
sentence in L2, we make a new alignment between
it and its original entity on the L1 sentence.
3.3 Assessment-based Instance Selection
The reliabilities of instances projected via a series
of independent modules are different from each
other. Thus, we propose an assessment strategy
for each projected instance. To evaluate the reli-
ability of a projected instance in L2, we use the
confidence score of monolingual relation detec-
tion for the original counterpart instance in L1.
The acceptance of a projected instance is deter-
mined by whether the score of the instance is
larger than a given threshold value ?. Only ac-
cepted instances are considered as the results of
annotation projection and applied to solve the re-
lation detection task in target language L2.
4 Experimental Setup
To demonstrate the effectiveness of our cross-
lingual annotation projection approach for rela-
tion detection, we performed an experiment on
relation detection in Korean text with propagated
annotations from English resources.
4.1 Annotation
The first step to evaluate our method was annotat-
ing the English sentences in a given parallel cor-
pus. We use an English-Korean parallel corpus
crawled from an English-Korean dictionary on the
web. The parallel corpus consists of 454,315 bi-
sentence pairs in English and Korean 2. The En-
glish sentences in the parallel corpus were prepro-
2The parallel corpus collected and other resources are all
available in our website
http://isoft.postech.ac.kr/?megaup/research/resources/
567
cessed by the Stanford Parser 3 (Klein and Man-
ning, 2003) which provides a set of analyzed re-
sults including part-of-speech tag sequences, a de-
pendency tree, and a constituent parse tree for a
sentence.
The annotation for English sentences is di-
vided into two subtasks: entity mention recogni-
tion and relation detection. We utilized an off-
the-shelf system, Stanford Named Entity Recog-
nizer 4 (Finkel et al, 2005) for detecting entity
mentions on the English sentences. The total
number of English entities detected was 285,566.
Each pair of recognized entities within a sentence
was considered as an instance for relation detec-
tion.
A classification model learned with the train-
ing set of the ACE 2003 corpus which con-
sists of 674 documents and 9,683 relation in-
stances was built for relation detection in English.
In our implementation, we built a tree kernel-
based SVM model using SVM-Light 5 (Joachims,
1998) and Tree Kernel Tools 6 (Moschitti, 2006).
The subtree kernel method (Moschitti, 2006) for
shortest path enclosed subtrees (Zhang et al,
2006) was adopted in our model. Our rela-
tion detection model achieved 81.2/69.8/75.1 in
Precision/Recall/F-measure on the test set of the
ACE 2003 corpus, which consists of 97 docu-
ments and 1,386 relation instances.
The annotation of relations was performed by
determining the existence of semantic relations
for all 115,452 instances with the trained model
for relation detection. The annotation detected
22,162 instances as positive which have semantic
relations.
4.2 Projection
The labels about entities and relations in the En-
glish sentences of the parallel corpora were propa-
gated into the corresponding sentences in Korean.
The Korean sentences were preprocessed by our
part-of-speech tagger 7 (Lee et al, 2002) and a de-
pendency parser implemented by MSTParser with
3http://nlp.stanford.edu/software/lex-parser.shtml
4http://nlp.stanford.edu/software/CRF-NER.shtml
5http://svmlight.joachims.org/
6http://disi.unitn.it/?moschitt/Tree-Kernel.htm
7http://isoft.postech.ac.kr/?megaup/research/postag/
Filter Without assessing With assessing
none 97,239 39,203
+ heuristics 31,652 12,775
+ dictionary 39,891 17,381
Table 1: Numbers of projected instances
a model trained on the Sejong corpus (Kim, 2006).
The annotation projections were performed on
the bi-sentences of the parallel corpus followed
by descriptions mentioned in Section 2.2. The
bi-sentences were processed by the GIZA++ soft-
ware (Och and Ney, 2003) in the standard con-
figuration in both English-Korean and Korean-
English directions. The bi-direcional alignments
were joined by the grow-diag-final algorithm,
which is widely used in bilingual phrase extrac-
tion (Koehn et al, 2003) for statistical machine
translation. This system achieved 65.1/41.6/50.8
in Precision/Recall/F-measure in our evaluation
of 201 randomly sampled English-Korean bi-
sentences with manually annotated alignments.
The number of projected instances varied with
the applied strategies for reducing noise as shown
in Table 1. Many projected instances were fil-
tered out by heuristics, and only 32.6% of the in-
stances were left. However, several instances were
rescued by dictionary-based alignment correction
and the number of projected instances increased
from 31,652 to 39,891. For all cases of noise re-
duction strategies, we performed the assessment-
based instance selection with a threshold value ?
of 0.7, which was determined empirically through
the grid search method. About 40% of the pro-
jected instances were accepted by instance selec-
tion.
4.3 Evaluation
In order to evaluate our proposed method, we pre-
pared a dataset for the Korean RDC task. The
dataset was built by annotating the information
about entities and relations in 100 news docu-
ments in Korean. The annotations were performed
by two annotators following the guidelines for the
ACE corpus processed by LDC. Our Korean RDC
corpus consists of 835 sentences, 3,331 entity
mentions, and 8,354 relation instances. The sen-
568
Model w/o assessing with assessingP R F P R F
Baseline 60.5 20.4 30.5 - - -
Non-filtered 22.5 6.5 10.0 29.1 13.2 18.2
Heuristic 51.4 15.5 23.8 56.1 22.9 32.5
Heuristic + Dictionary 55.3 19.4 28.7 59.8 26.7 36.9
Table 2: Experimental Results
tences of the corpus were preprocessed by equiva-
lent systems used for analyzing Korean sentences
for projection. We randomly divided the dataset
into two subsets with the same number of in-
stances for use as a training set to build the base-
line system and for evaluation.
For evaluating our approach, training instance
sets to learn models were prepared for relation
detection in Korean. The instances of the train-
ing set (half of the manually built Korean RDC
corpufs) were used to train the baseline model.
All other sets of instances include these baseline
instances and additional instances propagated by
the annotation projection approach. The train-
ing sets with projected instances are categorized
into three groups by the level of applied strategies
for noise reduction. While the first set included
all projections without any noise reduction strate-
gies, the second included only the instances ac-
cepted by the heuristics. The last set consisted of
the results of a series of heuristic-based filtering
and dictionary-based correction. For each training
set with projected instances, an additional set was
derived by performing assessment-based instance
selection.
We built the relation detection models for all
seven training sets (a baseline set, three pro-
jected sets without assessing, and three pro-
jected sets with assessing). Our implementations
are based on the SVM-Light and Tree Kernel
Tools described in the former subsection. The
shortest path dependency kernel (Bunescu and
Mooney, 2005) implemented by the subtree kernel
method (Moschitti, 2006) was adopted to learn all
models.
The performance for each model was evaluated
with the predictions of the model on the test set,
which was the other half of Korean RDC corpus.
We measured the performances of the models on
true entity mentions with true chaining of coref-
erence. Precision, Recall and F-measure were
adopted for our evaluation.
5 Experimental Results
Table 2 compares the performances of the differ-
ent models which are distinguished by the applied
strategies for noise reduction. It shows that:
? The model with non-filtered projections
achieves extremely poor performance due to
a large number of erroneous instances. This
indicates that the efforts for reducing noise
are urgently needed.
? The heuristic-based alignment filtering helps
to improve the performance. However, it is
much worse than the baseline performance
because of a falling-off in recall.
? The dictionary-based correction to our pro-
jections increased both precision and recall
compared with the former models with pro-
jected instances. Nevertheless, it still fails to
achieve performance improvement over the
baseline model.
? For all models with projection, the
assessment-based instance selection boosts
the performances significantly. This means
that this selection strategy is crucial in
improving the performance of the models
by excluding unreliable instances with low
confidence.
? The model with heuristics and assessments
finally achieves better performance than the
baseline model. This suggests that the pro-
jected instances have a beneficial influence
569
on the relation detection task when at least
these two strategies are adopted for reducing
noises.
? The final model incorporating all proposed
noise reduction strategies outperforms the
baseline model by 6 in F-measure. This is
due to largely increased recall by absorbing
more useful features from the well-refined
set of projected instances.
The experimental results show that our pro-
posed techniques effectively improve the perfor-
mance of relation detection in the resource-poor
Korean language with a set of annotations pro-
jected from the resource-rich English language.
6 Conclusion
This paper presented a novel cross-lingual annota-
tion projection method for relation extraction in a
resource-poor language. We proposed methods of
propagating annotations from a resource-rich lan-
guage to a target language via parallel corpora. In
order to relieve the bad influence of noisy projec-
tions, we focused on the strategies for reducing the
noise generated during the projection. We applied
our methods to the relation detection task in Ko-
rean. Experimental results show that the projected
instances from an English-Korean parallel corpus
help to improve the performance of the task when
our noise reduction strategies are adopted.
We would like to introduce our method to the
other subtask of relation extraction, which is re-
lation categorization. While relation detection is
a binary classification problem, relation catego-
rization can be solved by a classifier for multi-
ple classes. Since the fundamental approaches
of the two tasks are similar, we expect that our
projection-based relation detection methods can
be easily adapted to the relation categorization
task.
For this further work, we are concerned about
the problem of low performance for Korean,
which was below 40 for relation detection. The re-
lation categorization performance is mostly lower
than detection because of the larger number of
classes to be classified, so the performance of
projection-based approaches has to be improved
in order to apply them. An experimental result
of this work shows that the most important factor
in improving the performance is how to select the
reliable instances from a large number of projec-
tions. We plan to develop more elaborate strate-
gies for instance selection to improve the projec-
tion performance for relation extraction.
Acknowledgement
This research was supported by the MKE (The
Ministry of Knowledge Economy), Korea, un-
der the ITRC (Information Technology Research
Center) support program supervised by the NIPA
(National IT Industry Promotion Agency) (NIPA-
2010-C1090-1031-0009).
References
Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, page 724731.
Chen, Jinxiu, Donghong Ji, Chew Lim Tan, and
Zhengyu Niu. 2006. Relation extraction using la-
bel propagation based semi-supervised learning. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 129?136, Sydney, Australia. Associ-
ation for Computational Linguistics.
Culotta, Aron and Jaffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In Pro-
ceedings of ACL, volume 4.
Doddington, George, Alexis Mitchell, Mark Przy-
bocki, Lance Ramshaw, Stephanie Strassel, and
Ralph Weischedel. 2004. The automatic content
extraction (ACE) programtasks, data, and evalua-
tion. In Proceedings of LREC, volume 4, page
837840.
Finkel, Jenny R., Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
volume 43, page 363.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
570
Joachims, Thorsten. 1998. Text categorization with
support vector machines: Learning with many rele-
vant features. In Proceedings of the European Con-
ference on Machine Learning, pages 137?142.
Kambhatla, Nanda. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the ACL 2004 on Interactive poster and demonstra-
tion sessions, page 22, Barcelona, Spain. Associa-
tion for Computational Linguistics.
Kim, Hansaem. 2006. Korean national corpus in the
21st century sejong project. In Proceedings of the
13th NIJL International Symposium, page 4954.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 423?430, Sap-
poro, Japan. Association for Computational Lin-
guistics.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, vol-
ume 1, pages 48?54.
Lee, Gary Geunbae, Jeongwon Cha, and Jong-Hyeok
Lee. 2002. Syllable pattern-based unknown mor-
pheme segmentation and estimation for hybrid part-
of-speech tagging of korean. Computational Lin-
guistics, 28(1):53?70.
Merlo, Paola, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multilingual paradigm
for automatic verb classification. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 207?214, Philadelphia,
Pennsylvania. Association for Computational Lin-
guistics.
Moschitti, Alessandro. 2006. Making tree kernels
practical for natural language learning. In Proceed-
ings of EACL06.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Pado, Sebastian and Mirella Lapata. 2009.
Cross-lingual annotation projection of semantic
roles. Journal of Artificial Intelligence Research,
36(1):307340.
Yarowsky, David and Grace Ngai. 2001. Inducing
multilingual POS taggers and NP bracketers via ro-
bust projection across aligned corpora. In Second
meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics on Language
technologies 2001, pages 1?8, Pittsburgh, Pennsyl-
vania. Association for Computational Linguistics.
Yarowsky, David, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, pages 1?
8, San Diego. Association for Computational Lin-
guistics.
Zelenko, Dmitry, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. J. Mach. Learn. Res., 3:1083?1106.
Zhang, Min, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 825?832, Sydney, Australia. Associ-
ation for Computational Linguistics.
Zhang, Zhu. 2004. Weakly-supervised relation clas-
sification for information extraction. In Proceed-
ings of the thirteenth ACM international conference
on Information and knowledge management, pages
581?588, Washington, D.C., USA. ACM.
Zhou, Guodong, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, page
434.
Zitouni, Imed and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 600?609, Honolulu,
Hawaii. Association for Computational Linguistics.
571
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 328?332,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Meta Learning Approach to Grammatical Error Correction 
 Hongsuck Seo1, Jonghoon Lee1, Seokhwan Kim2, Kyusong Lee1 Sechun Kang1, Gary Geunbae Lee1 1Pohang University of Science and Technology 2Institute for Infocomm Research {hsseo, jh21983}@postech.ac.kr, kims@i2r.a-star.edu.sg {kyusonglee, freshboy, gblee}@postech.ac.kr     Abstract We introduce a novel method for grammatical error correction with a number of small corpora. To make the best use of several corpora with different characteristics, we employ a meta-learning with several base classifiers trained on different corpora. This research focuses on a grammatical error correction task for article errors. A series of experiments is presented to show the effectiveness of the proposed approach on two different grammatical error tagged corpora. 1. Introduction As language learning has drawn significant attention in the community, grammatical error correction (GEC), consequently, has attracted a fair amount of attention. Several organizations have built diverse resources including grammatical error (GE) tagged corpora. Although there are some publicly released GE tagged corpora, it is still challenging to train a good GEC model due to the lack of large GE tagged learner corpus. The available GE tagged corpora are mostly small datasets having different characteristics depending on the development methods, e.g. spoken corpus vs. written corpus. This situation forced researchers to utilize native corpora rather than GE tagged learner corpora for the GEC task. The native corpus approach consists of learning a model that predicts the correct form of an article given the surrounding context. Some researchers 
focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al, 2000; Lee, 2004; Nagata et al, 2006; Han et al, 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al, 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE tagged corpora have received less focus. In this paper, we present a novel approach to the GEC task using meta-learning. We focus mainly on article errors for two reasons. First, articles are one of the most significant sources of GE for the learners with various L1 backgrounds. Second, the effective features for article error correction are already well engineered allowing for quick analysis of the method. Our approach is distinguished from others by integrating the predictive models trained on several GE tagged learner corpora, rather than just one GE tagged corpus. Moreover, the framework is compatible to any classification technique. In this study, we also use a native corpus employing Dahlmeier and Ng?s approach. We demonstrate the effectiveness of the proposed method against baseline models in article error correction tasks. 
328
The remainder of this paper is organized as follows: Section 2 explains our proposed method. The experiments are presented in Section 3. Finally, Section 4 concludes the paper. 2. Method Our method predicts the type of article for a noun phrase within three classes: null, definite, and indefinite. A correction arises when the prediction disagrees with the observed article. The meta-learning technique is applied to this task to deal with multiple corpora obtained from different sources. A meta-classifier decides the final output based on the intermediate results obtained from several base classifiers. Each base classifier is trained on a different corpus than are the other classifiers. In this work, the feature extraction processes used for the base classifiers are identical to each other for simplicity, although they need not necessarily be identical. The meta-classifier takes the output scores of the base classifiers as its input and is trained on the held-out development data (Figure 1a). During run time, the trained classifiers are organized in the same manner. For the given features, the base classifiers independently calculate the score, then the meta-classifier makes the final decision based on the scores (Figure 1b). 2.1. Meta-learning Meta-learning is a sequential learning process following the output of other base learners (classifiers). Normally, different classifiers successfully predict results on different parts of the 
input space, so researchers have often tried to combine different classifiers together (Breiman, 1996; Cohen et al, 2007; Zhang, 2007; Ayd?n, 2009; Menahem et al, 2009). To capitalize on the strengths and compensate for the weaknesses of each classifier, we build a meta-learner that takes an input vector consisting of the outputs of the base classifiers. The performance of meta-learning can be improved using output probabilities for every class label from the base classifiers. The meta-classifier for the proposed method consists of multiple linear classifiers. Each classifier takes an input vector consisting of the output scores of each base classifier and calculates a score for each type of article. The meta-classifier finally takes the class having the maximum score. A common design of an ensemble is to train different base classifiers with the same dataset, but in this work one classification technique was used with different datasets each having different characteristics. Although only one classification method was used in this work, different methods each well-tuned to the individual corpora may be used to improve the performance. We employed the meta-learning method to generate synergy among corpora with diverse characteristics. More specifically, it is shown by cross validation that meta-learning performs at a level that is comparable to the best base classifier (Dzeroski and Zenko, 2004). 2.2. Base Classifiers In the meta-learning framework, the performance of the base classifiers is important because the improvement in base classification generally enha-
Figure 1: Overview of the proposed method 
329
nces the overall performance. The base classifiers can be expected to become more informative as more data are provided. We followed the structural learning approach (Ando and Zhang, 2005), which trains a model from both a native corpus and a GE tagged corpus (Dahlmeire and Ng, 2011), to improve the base classifiers by the additional information extracted from a native corpus. Structural learning is a technique which trains multiple classifiers with common structure. The common structure chooses the hypothesis space of each individual classifier and the individual classifiers are trained separately once the hypothesis space is determined. The common structure can be obtained from auxiliary problems which are closely related to the main problems. A word selection problem is a task to predict the appropriate word given the surrounding context in a native corpus and is a closely related auxiliary problem of the GEC task. We can obtain the common structure from the article selection problem and use it for the correction problem. In this work, all the base classifiers used the same least squares loss function for structural learning.  We adopted the feature set investigated in De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al 2005) to extract the features. 2.3. Evaluation Metric The effectiveness of the proposed method is evaluated in terms of accuracy, precision, recall, and F1-score (Dahlmeire and Ng, 2011). Accuracy is the number of correct predictions divided by the total number of instances. Precision is the ratio of the suggested corrections that agree with the tagged answer to the total number of the suggested corrections whereas recall is the ratio of the suggested corrections that agree with the tagged answer to the total number of corrections in the corpus. 3. Experiments 3.1. Datasets In this work we used a native corpus and two GE tagged corpora. For the native corpus, we used                                                             1 http://nlp.stanford.edu/software/corenlp.shtml 
news data2 which is a large English text extracted from news articles. The First Certificate in English exams in the Cambridge Learner Corpus 3 (hereafter, CLC-FCE; Yannakoudakis et al, 2011) and the Japanese Learner English corpus (Izumi et. al., 2005) were used for the GE tagged corpora. We extracted noun phrases from each corpus by parsing the text of the respective corpora. (1) We parsed the native corpus from the beginning until approximately a million noun phrases are extracted. (2) About 90k noun phrases containing ~3,300 mistakes in article usage were extracted from the entire CLC-FCE corpus, and (3) about 30k noun phrases containing ~2,500 mistakes were extracted from the JLE corpus.  The extracted noun phrases were used for our training and test data. We hold out 10% of the data for the test. We applied 20% under-sampling to the training instances that do not have any errors to alleviate data imbalance in the training set. We emphasize the fact that the two learner corpora differ from each other in three aspects. The first aspect is the styles of the texts: the CLC is literary whereas the JLE is colloquial. The second is the error rate: about 3.5% for CLC-FCE and   8.5% for JLE. Finally, the third is the distribution of L1 languages of the learners: the learners of the CLC corpus have various L1 backgrounds whereas the learners of the JLE consist of only Japanese. These experiments demonstrate the effectiveness of the proposed method relying on the diversity of the corpora. The native corpus was used to find the common structure using structural learning and two GE tagged learner corpora are used to train the base classifiers by structural learning with the common structure obtained from the news corpus. We trained three classifiers for comparison; (1) the classifier (INTEG) trained with the integrated training set of the two GE tagged corpora, and two base classifiers used for the ensemble: (2) the base classifier (CB) trained only with the CLC-FCE and (3) the other base classifier (JB) trained with the JLE. 3.2. Results The accuracy obtained from the word selection task with the news corpus was 76.10%. Upon                                                             2 http://www.statmt.org/wmt09/translation-task.html 3 http://www.ilexir.com/ 
330
obtaining the parameters of the word selection task, the structural parameter ?  was calculated by singular value decomposition and was used for the structural learning of the main GEC task. We used three different test data sets: the CLC-FCE, the JLE and an integrated test set of the two. The accuracy (Acc.) and the precision (Prec.) of the INTEG was poorer than CB on the CLC-FCE test set (Table 1), whereas INTEG outperformed JB on the JLE test (Table 2).  Some instances extracted from the CLC-FCE corpus have similar characteristics to the instances from the JLE corpus. This overlap of instances affected the performance in both positive and negative ways. Prediction of instances similar to those in the JLE was enhanced. Consequently, INTEG model demonstrated better accuracy and precision for the JLE test set. Unfortunately, for the CLC test set, the instances resulted in lower accuracy and precision. The proposed model is able to alleviate this model bias due to similar instances observed in the INTEG model. The accuracy of the proposed model consistently increased by over 10% for all three data sets. The relative performance gain in terms of F1-score (F1) was 15% on the integrated set. This performance gain stems from the over   25% relative improvement of the precision (Table 1, 2 and 3). We believe the improvement comes from the contribution of reconfirming procedures performed 
by the meta-classifier. When the prediction of the two base classifiers conflicts with each other, the meta-classifier tends to choose the one with a higher confidence score; this choice improves the accuracy and precision because known features generate a higher confidence whereas unseen or less-weighted features generate a lower score. Although the proposed model introduced a tradeoff between precision and recall (Rec.), this tradeoff was tolerable in order to improve the overall F1-score. Since GEC is a task where false alarm is critical, obtaining high precision is very important. The low precision on the whole experiments is due to the data imbalance. Instances in the dataset are mostly not erroneous, e.g., only 3.5% of erroneous instances for the CLC corpus. The standard for correct prediction is also very strict and does not allow multiple answers. Performance can be evaluated in a more realistic way by applying a softer standard, e.g., by evaluating manually. 4. Conclusion We have presented a novel approach to grammatical error correction by building a meta-classifier using multiple GE tagged corpora with different characteristics in various aspects. The experiments showed that building a meta-classifier overcomes the interference that occurs when training with a set of heterogeneous corpora. The proposed method also outperforms the base classifier themselves tested on the same class of test set as the training set with which the base classifiers are trained. A better automatic evaluation metric would be needed as further research. Acknowledgments Industrial Strategic technology development program, 10035252, development of dialog-based spontaneous speech interface technology on mobile platform, funded by the Ministry of Knowledge Economy (MKE, Korea).   
Model Acc. Prec. Rec. F1 INTEG 73.37 4.69 72.39 8.82 CB 77.20 5.39 71.17 10.03 Proposed 86.99 6.17 45.77 10.88 Table 1: Best results for GEC task on CLC-FCE test set.  Model Acc. Prec. Rec. F1 INTEG 78.87 14.88 85.47 25.35 JB 78.02 14.49 86.32 24.82 Proposed 89.61 19.28 46.60 27.27 Table 2: Best results for GEC task on JLE test set. Model Acc. Prec. Rec. F1 INTEG 74.64 6.84 77.86 12.58 Proposed 87.50 8.61 46.12 14.52 Table 3: Best results for GEC task on the integrated set of CLC-FCE and JLE test sets.  
331
References  R.K. Ando and T. Zhang. 2005. A framework for learn- ing predictive structures from multiple tasks and un- labeled data. Journal of Machine Learning Research, 6, pp. 1817-1853. U. Ayd?n, S. Murat, Olcay T Y?ld?z, A. Ethem, 2009, Incremental construction of classifier and discriminant ensembles, Information Science, 179 (9), pp. 144-152. L. Breiman, 1996, Bagging predictors, Machine Learning, pp. 123?140. S. Cohen, L. Rokach, O. Maimon, 2007, Decision tree instance space decomposition with grouped gain-ratio, Information Science, 177 (17), pp. 3592?3612. D. Dahlmeier, H. T. Ng, 2011, Grammatical error correction with alternating structure optimization, In Proceedings of the 49th Annual Meeting of the ACL-HLT 2011, pp. 915-923. R. De Felice. 2008. Automatic Error Detection in Non- native English. Ph.D. thesis, University of Oxford. S. Dzeroski, B. Zenko, 2004, Is combining classifiers with stacking better than selecting the best one?, Machine Learning, 54 (3), pp. 255?273. J. R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43nd Annual Meeting of the ACL, pp. 363-370. N.R. Han, M. Chodorow, and C. Leacock. 2006. De- tecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(02), pp. 115-129. N.R. Han, J. Tetreault, S.H. Lee, and J.Y. Ha. 2010. Using an error-annotated learner corpus to develop an ESL/EFL error correction system. In Proceedings of LREC. D. Klein and C.D. Manning. 2003a. Accurate unlexical- ized parsing. In Proceedings of ACL, pp. 423-430. D. Klein and C.D. Manning. 2003b. Fast exact inference with a factored model for natural language processing. Advances in Neural Information Processing Systems (NIPS 2002), 15, pp. 3-10. K. Knight and I. Chander. 1994. Automated postediting of documents. In Proceedings of AAAI, pp. 779-784. J. Lee. 2004. Automatic article restoration. In Proceed- ings of HLT-NAACL, pp. 31-36. R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A feedback-augmented method for detecting errors in 
the writing of learners of English. In Proceedings of COLING-ACL, pp. 241--248. A. Mariko, 2007, Grammatical errors across proficiency levels in L2 spoken and written English, The Economic Journal of Takasaki City University of Economics, 49 (3, 4), pp. 117-129. E. Menahem, L. Rokach, Y. Elovici, 2009, Troika-An imporoved stacking schema for classification tasks, Information Science, 179 (24), pp. 4097-4122. G. Minnen, F. Bond, and A. Copestake. 2000. Memory- based learning for article generation. In Proceedings of CoNLL, pp. 43-48. E. Izumi, K. Uchimoto, H. Isahara, 2005, Error annotation for corpus of Japanese learner English, In Proceedings of the 6th International Workshop on Linguistically Interpreted Corpora, pp. 71-80. A. Rozovskaya and D. Roth. 2010. Training paradigms for correcting errors in grammar and usage. In Pro- ceedings of HLT-NAACL, pp. 154-162. K. Toutanova and C. D. Manning. 2000. Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In Proceedings of the Joint SIGDAT Conference on EMNLP/VLC-2000, pp. 63-70. H.Yannakoudakis, T. Briscoe, B. Medlock, 2011, A new dataset and method for automatically grading ESOL texts, In Proceedings of ACL, pp. 180-189. G. P. Zhang, 2007, A neural network ensemble method with jittered training data for time series forecasting, Information Sciences: An International Journal, 177 (23), pp. 5329?5346. 
332
