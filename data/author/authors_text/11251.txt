R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 827 ? 837, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Principles of Non-stationary Hidden Markov Model  
and Its Applications to Sequence Labeling Task 
Xiao JingHui, Liu BingQuan, and Wang XiaoLong 
School of Computer Science and Techniques,  
Harbin Institute of Technology, Harbin, 150001, China 
{xiaojinghui, liubq, wangxl}@insun.hit.edu.cn 
Abstract. Hidden Markov Model (Hmm) is one of the most popular language 
models. To improve its predictive power, one of Hmm hypotheses, named 
limited history hypothesis, is usually relaxed. Then Higher-order Hmm is built 
up. But there are several severe problems hampering the applications of high-
order Hmm, such as the problem of parameter space explosion, data sparseness 
problem and system resource exhaustion problem. From another point of view, 
this paper relaxes the other Hmm hypothesis, named stationary (time invariant) 
hypothesis, makes use of time information and proposes a non-stationary Hmm 
(NSHmm). This paper describes NSHmm in detail, including its definition, the 
representation of time information, the algorithms and the parameter space and 
so on. Moreover, to further reduce the parameter space for mobile applications, 
this paper proposes a variant form of NSHmm (VNSHmm). Then NSHmm and 
VNSHmm are applied to two sequence labeling tasks: pos tagging and pinyin-to-
character conversion. Experiment results show that compared with Hmm, 
NSHmm and VNSHmm can greatly reduce the error rate in both of the two 
tasks, which proves that they have much more predictive power than Hmm does. 
1   Introduction 
Statistical language model plays an important role in natural language processing and 
great efforts are devoted to the research of language modeling. Hidden Markov Model 
(Hmm) is one of the most popular language models. It was first proposed by IBM in 
speech recognition [1] and achieved great success. Then Hmm has a wide range of 
applications in many domains, such as OCR [2], handwriting recognition [3], machine 
translation [4], Chinese pinyin-to-character conversion [5] and so on. 
To improve Hmm?s predictive power, one of Hmm hypotheses [6] named limited 
history hypothesis, is usually relaxed and higher-order Hmm is proposed. But as the 
order of Hmm increases, its parameter space explodes at an exponential rate, which 
may result in several severe problems, such as data sparseness problem [7], system 
resource exhaustion problem and so on. From another point of view, this paper 
relaxes the other Hmm hypothesis, named stationary hypothesis, makes use of time 
information and proposes non-stationary Hmm (NSHmm). This paper first defines 
NSHmm in a formalized form, and then discusses how to represent time information 
in NSHmm. After that, the algorithms of NSHmm are provided and the parameter 
space complexity is calculated. Moreover, to further reduce the parameter space, a 
828 J. Xiao, B. Liu, and X. Wang 
variant form of NSHmm (VNSHmm) is proposed later. At last, NSHmm and 
VNSHmm are applied to two sequence labeling tasks: pos tagging and pinyin-to-
character conversion. As the experiment results show, compared with Hmm, NSHmm 
and VNSHmm can greatly reduce the error rate in the both two tasks.  
The rest of this paper is structured as follows: in section 2 we briefly review the 
definition of standard Hmm. In section 3, NSHmm is proposed and the relative 
questions are discussed in detail. Experiments and results are discussed in section 4. 
Finally, we give our conclusions in section 5 and plan the further work in section 6. 
2   Hidden Markov Model 
Hmm is a function of Markov process and can be mathematically defined as a five-
tuple M = <?, ?, ?, ?, ?> which consists of: 
1. A finite set of (hidden) states ?. 
2. A finite set of (observed) symbols ?. 
3. A state transition function ?: ?? ?-> [0, 1]. 
4. A symbol emission function ?: ?? ?-> [0, 1]. 
5. And an initial state probability function ?: Omega -> [0, 1]. 
The functions of ?, ? and ? are usually estimated by MLE principle on large scale 
corpus. Based on the above definition, Hmm makes two hypotheses at the same time: 
1. Limited history hypothesis: the current state is completely decided by the last 
state before, but irrelative to the entire state history. 
2. Stationary hypothesis: the state transition function ? is completely determined 
by states, but irrelative to the time when state transition occurs. So it is with the 
symbol emission function. 
There are three fundamental questions and a series of corresponding algorithms for 
Hmm: 
1. Given Hmm, how to calculate the probability of a sequence observation? 
Forward algorithm and backward algorithm can handle that question. 
2. Given Hmm and an observation sequence, how to find the best state sequence to 
explain the observation? Viterbi algorithm can fulfill that task. 
3. Given an observation sequence, how to estimate the parameters of Hmm to best 
explain the observed data? Baum-Welch algorithm can solve that problem. 
Hmm is a popular language model and has been applied to many tasks in natural 
language processing. For example, in pos tagging, the word sequence is taken as the 
observation of Hmm, and the pos sequence as the hidden state chain. Viterbi 
algorithm can find the best pos sequence corresponding to the word sequence. 
3   Non-stationary Hidden Markov Model 
3.1   Motivation 
There are many approaches to improve the predictive power of Hmm in practice. For 
example, factorial Hmm [8] is proposed by decomposing the hidden state 
 Principles of Non-stationary Hidden Markov Model and Its Applications 829 
representation into multiple independent Markov chains. In speech recognition, a 
factorial Hmm can represent the combination of multiple signals which are produced 
independently and the characteristics of each signal are described by a distinct 
Markov chain. And some Hmms use neural networks to estimate phonetic posterior 
probability in speech recognition [9]. The input layer of the network typically covers 
both the past states and the further states. However, from the essential definition of 
Hmm, there are two ways to improve the predictive power of Hmm. One approach is 
to relax the limited history hypothesis and involve more history information into 
language model. The other is to relax the stationary hypothesis and make use of time 
information. In recent years, much research focuses on the first approach [10] and 
higher-order Hmm is built up. But as the order increases, the parameter space 
explodes at such an exponential rate that training corpus becomes too sparse and 
system resource exhausts soon. This paper adopts the second approach and tries to 
make good use of time information. Then NSHmm is proposed. Since there is no 
theoretical conflict between NSHmm and high-order Hmm, the two models can be 
combined together in proper conditions.  
3.2   Definition for NSHmm 
Similarly with Hmm, NSHmm is also mathematically defined as a five-tuple M = 
<?, ?, ??, ??, ??> which consists of: 
1. A finite set of (hidden) states ?. 
2. A finite set of (observed) symbols ?. 
3. A state transition function ??: ?? ?? t -> [0, 1]. 
4. A symbol emission function ??: ?? ?? t -> [0, 1]. 
5. And an initial state probability function ??: ?? t -> [0, 1]. 
In the above definition, t is the time variable indicating when state transition or 
symbol emission occurs. Different from Hmm?s definition, ??, ?? and ?? are all the 
functions of t. And they can still be estimated by MLE principle on large scale corpus. 
This key question of NSHmm is how to represent time information. We?ll discuss that 
question in the next section.  
3.3   Representation of Time Information 
Since time information is to describe when the events of Hmm (e.g. state transition or 
symbol emission) occur, a natural way is to use the event index in Markov chain to 
represent the time information. But there are two serious problems with that method. 
Firstly, index has different meanings in the Markov chains of different length. 
Secondly, since a Markov chain may have arbitrary length, the event index can be any 
natural number. However, computer system can only deal with finite value. A refined 
method is to use the ratio of the event index and the length of Markov chain which is 
a real number of the range [0, 1]. But there are infinite real numbers in the range [0, 
1]. In this paper, we divide the range [0, 1] into several equivalence classes (bins) and 
each class share the same time information. When training NSHmm, the functions of 
??, ?? and ?? should be estimated in each bin respectively according to their time 
information. And when they are accessed, they should also get the value in the 
830 J. Xiao, B. Liu, and X. Wang 
according bin. For example, the state transition function ?? can be estimated by the 
formula below:  
( , , )
( , )ijt
C i j tp
C i t
=  (1) 
where ( , , )C i j t is the co-occurrence frequency of state i and state j at time t and it can 
be estimated by counting the co-occurrence times of state i and state j in the tth bin in 
each sentence of corpus. ( , )C i t is the frequency of state i at time t and can be 
estimated by counting the occurrence times of state i in the tth bin in the sentence of 
corpus. And the result ijtP is the transition probability between state i and j at time t. 
It?s similar to estimate the functions of ?? and ??.  
3.4   Algorithms on Non-stationary Hidden Markov Model 
The three fundamental questions of Hmm also exist in NSHmm. The corresponding 
algorithms, such as forward algorithm, viterbi algorithm and Baum-Welch algorithm, 
can work well in NSHmm, except that they have to first calculate the time 
information and then compute the function values of ??, ?? and ?? according to the 
statistical information in the corresponding bins. 
3.5   Space Complexity Analysis 
In this section, we will analyze the space complexity of NSHmm. Compared with 
Hmm, some conclusions can be drawn at the end of this section. For simplicity and 
convenience, we define some notations below: 
? The hidden state number n 
? The observed symbol number m 
? The bin number for NSHmm k 
In Hmm and NSHmm, all system parameters are devoted to simulate the three 
functions of ?, ? and ?. For Hmm, a vector of size n is usually used to store the initial 
probability of each state. An n ? n matrix is adopted to store the transition 
probabilities between every two states, and n ? m matrix to record the emission 
probabilities between states and observed symbols. The space complexity for Hmm is 
the sum of these three parts which is ( )n n n n m? + ? + ? . For NSHmm, since ??, ?? 
and ?? are all the functions of time t, time information should be counted in. An n ? k 
matrix is used to store the initial probability of each state at different time. An n ? n? 
k matrix is used to store the transition probability between each state at different time 
and n ? m ? k matrix to keep the emission probability. Thus, the space complexity of 
NSHmm is (( ) )n n n n m k? + ? + ? ? which is k times than that of Hmm. As the 
analysis shows, the space complexity of NSHmm increases at a linear speed with k, 
rather than at an exponential speed as high-order Hmm dose. Moreover, as k is 
usually far below than n, NSHmm is much easier to avoid the problem of parameter 
space explosion. 
 Principles of Non-stationary Hidden Markov Model and Its Applications 831 
3.6   Variant Form of NSHmm 
In this section, this paper proposes a variant form of NSHmm (VNSHmm). It?s based 
on these facts: for some applications, such as on mobile platform, there is not enough 
system resource to build up a whole NSHmm. Then NSHmm has to be compressed. 
This paper constructs some statistical variables for time information and uses these 
statistical variables to substitute concrete time information in NSHmm. When 
computing the probability in VNSHmm, these statistical variables are combined 
together to calculate a coefficient for normal probability of Hmm. 
Two statistical variables, expectation and variance of time information, are adopted 
in VNSHmm. And such assumptions are made that more weight should be awarded if 
the time of event occurring fits better with the training corpus, and less weight vice 
versa. The probability function in VNSHmm is defined as below: 
2(( ) )1 V t E
tp e pZ
? ?? ? +
= ?  (2) 
where Z is a normalizing factor, and is defined as: 
2(( ) )
1
t k
V t E
t
Z e p? ?
=
? ? +
=
= ??  (3) 
The notations in the formulation (2) and (3) are described in the following: 
? Current time information t 
? Expectation of time information E 
? Variance of time information V 
? State transition probability ( or symbol emission probability ) p 
? Adjusted coefficients ? and  ? 
pt is descendent with the term t-E which defines the difference between current 
time and time expectation in training corpus. As the value of t-E decreases, t fits for 
training corpus better and more weight is added to pt. For example, we take a 
Chinese sentence as a state chain of Markov process. The word ????(first of all) 
usually leads a sentence in training corpus. For test corpus, more weight should be 
given to pt if ??(first of all) appears at the beginning of the sentence, whereas less 
weight if at the sentence end. pt is ascendant with the variance V. The item V is 
mainly used to balance the value of term t-E for some active states. For example, in 
Chinese, some adjectives, such as ????(beautiful), can appear at any position of 
the sentence. Then it?s unreasonable to decrease pt just because the term t-E 
increases. In such a situation, the value of item V for ????(beautiful) is usually 
bigger than that of those inactive states (e.g.??(first of all)). Then the item V can 
provide a balance for the value of t-E.  
Since VNSHmm just makes use of expectation and variance, rather than the whole 
time information, its space complexity is equal to that of the NSHmm with only two 
bins, which is (( ) 2)n n n n m? + ? + ? ? . 
832 J. Xiao, B. Liu, and X. Wang 
4   Experiments 
In the experiments, NSHmm and VNSHmm have been applied to two sequence 
labeling tasks: pos tagging and pinyin-to-character conversion. This paper will 
describe them in detail in the following two sections. 
4.1   Pos Tagging 
For pos tagging, this paper chooses the People?s Daily corpus in 1998 which has been 
labeled by Peking University [11]. The first 5 month corpus is taken as training 
corpus and the 6th month as test corpus. Since most of pos-taggers are based on 2-
order Hmm (trigram), 2-order NSHmm and 2-order VNSHmm are constructed 
respectively in the experiments.  
We first calculate KL distances between the emission probability distribution of 
Hmm and the distributions of NSHmm at different time. Only when the distances are 
great, could NSHmm be expected to outperform Hmm; otherwise NSHmm would 
have similar performance as Hmm has. Since there are totally k different distance 
values for NSHmm with k bins, we just calculate the average distance for each 
NSHmm. The results are presented in table 1 as below: 
Table 1. Average KL Distances between Emission Probability Distributions of NSHmm and 
Hmm  
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Aver KL Dis 0 0.08 0.12 0.15 0.17 0.19 0.21 0.22 
From table 1 we can see that as the bin number increases, the average KL distance 
become bigger and bigger, which indicates there is more and more difference between 
the emission probability distributions of Hmm and that of NSHmm. Similar results can 
be gotten by comparing state-transition-probability distributions of the two models. 
And as time information increases, we expect more predictive power for NSHmm.  
To prove the effectiveness of NSHmm and VNSHmm, in the rest of this section, 
two sets of experiments, close test and open test, are performed. The results of close 
test are showed in table 2, figure 1 and the results of open test are presented in table 3, 
figure 2 as below. 
Table 2. Pos Tagging Close Test 
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Hmm (baseline) 6.04% --- --- --- --- --- --- --- 
Error 
Rate 
6.04% 5.63% 5.55% 5.52% 5.47% 5.44% 5.42% 5.47% NSHmm 
Reduction --- 6.79% 8.11% 8.61% 9.44% 9.93% 10.26% 9.43% 
Error 
Rate 
6.04% 5.85% 5.85% 5.85% 5.85% 5.85% 5.85% 5.85% VNSHmm 
Reduction --- 3.15% 3.15% 3.15% 3.15% 3.15% 3.15% 3.15% 
 Principles of Non-stationary Hidden Markov Model and Its Applications 833 
0 1 2 3 4 5 6 7 8 9
5.2
5.4
5.6
5.8
6.0
6.2
Er
ro
r 
Ra
te
 
(%
)
Bins Number: K
 Hmm
 NSHmm
 VNSHmm
 
Fig. 1. Pos Tagging Close Test 
Table 3. Pos Tagging Open Test 
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Hmm (baseline) 6.99% --- --- --- --- --- --- --- 
Error  
Rate 
6.99% 6.44% 6.39% 6.42% 6.40% 6.43% 6.47% 6.58% NSHmm 
Reduction --- 7.87% 8.58% 8.15% 8.44% 8.01% 7.44% 5.87% 
Error  
Rate 
6.99% 6.59% 6.59% 6.59% 6.59% 6.59% 6.59% 6.59% VNSHmm 
Reduction --- 5.72% 5.72% 5.72% 5.72% 5.72% 5.72% 5.72% 
0 1 2 3 4 5 6 7 8 9
6.4
6.6
6.8
7.0
7.2
Er
ro
r R
at
e 
(%
)
Bins Number: K
 Hmm
 NSHmm
 VNSHmm
 
Fig. 2. Pos Tagging Open Test 
As table 2 and table 3 have showed, no matter in close test or in open test, NSHmm 
and VNSHmm achieve much lower error rates than Hmm. NSHmm gets at most 
10.26% error rate reduction and VNSHmm obtains 3.15% reduction in close test; and 
834 J. Xiao, B. Liu, and X. Wang 
they achieve 8.58% and 5.72% reductions respectively in open test. These facts prove 
that NSHmm and VNSHmm have much more predictive power than Hmm has. From 
figure 1 we can see that in close test, as the bin number increases, the error rate of 
NSHmm is decreased constantly, which proves that the improvement of NSHmm is 
due to the increasing time information. But in the open test as figure 2 shows, the 
error rate stops decreasing after k=3. That is because of the overfitting problem. As a 
consequence, this paper suggests k=3 in NSHmm for pos tagging task. From figure 1 
and figure 2, VNSHmm performs stably after k=2, which indicates a small number of 
parameters are enough to stat reliable statistical variables for VNSHmm and get 
improved performance.  
4.2   Pinyin-to-Character Conversion 
For the experiments of pinyin-to-character conversion, this paper adopts the same 
training corpus and test corpus as in pos tagging experiments. And 6763 Chinese 
frequent characters are chosen as the lexicon. This paper firstly converts all raw 
Chinese corpuses to the pinyin corpuses. Then based on the both kinds of corpuses, 
Hmm, NSHmm and VNSHmm are built up.  
In the experiments, we first calculate KL distances between the state-transition-
probability distributions of Hmm and the distributions of NSHmm at different time. 
As we have done in the pos tagging experiments, we just calculate the average KL 
distance for each NSHmm. The results are presented in table 4. 
Table 4. Average KL Distances between State-Transition-Probability Distributions of NSHmm 
and Hmm 
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Aver KL Dis 0 0.08 0.12 0.15 0.17 0.18 0.19 0.21 
From table 4 we can see that as the bin number increases, the average KL distance 
become bigger and bigger and more predictive power is expected for NSHmm. And 
similar results can be gotten by comparing emission probability distributions of the 
two models. Then in the rest of this section, we perform the pinyin-to-character 
conversion experiments. Close test and open test are performed respectively. The 
results of close test are showed in table 5, figure 3 and the results of open test are 
presented in table 6, figure 4 respectively. 
Table 5. Pinyin-to-Character Conversion Close Test 
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Hmm (baseline) 8.30% --- --- --- --- --- --- --- 
Error  
Rate 
8.30% 7.17% 6.55% 6.08% 5.74% 5.43% 5.19% 4.98% NSHmm 
Reduction --- 13.61% 21.08% 26.75% 30.84% 34.58% 37.47% 40.00% 
Error  
Rate 
8.30% 8.28% 8.27% 8.28% 8.28% 8.28% 8.28% 8.28% VNSHmm 
Reduction --- 0.24% 0.24% 0.24% 0.24% 0.24% 0.24% 0.24% 
 Principles of Non-stationary Hidden Markov Model and Its Applications 835 
0 1 2 3 4 5 6 7 8 9
4.5
5.0
5.5
6.0
6.5
7.0
7.5
8.0
8.5
9.0
Er
ro
r 
R
at
e 
(%
)
Bins Number: K
 Hmm
 NSHmm
 VNSHmm
 
Fig. 3. Pinyin-to-Character Conversion Close Test 
Table 6. Pinyin-to-Character Conversion Open Test 
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Hmm (baseline) 14.97% --- --- --- --- --- --- --- 
Error  
Rate 
14.97%12.62% 13.16% 13.61% 13.93% 14.23% 14.52% 14.81% NSHmm 
Reduction --- 15.70% 12.09% 9.08% 6.95% 4.94% 3.01% 1.07% 
Error  
Rate 
14.97% 11.98% 11.96% 11.96% 11.96% 11.97% 11.97% 11.97% VNSHmm 
Reduction --- 19.97% 20.11% 20.11% 20.11% 20.04% 20.04% 20.04% 
0 1 2 3 4 5 6 7 8 9
12
13
14
15
16
Er
ro
r R
at
e 
(%
)
Bins Number: K
 Hmm
 NSHmm
 VNSHmm
 
Fig. 4. Pinyin-to-Character Conversion Open Test 
836 J. Xiao, B. Liu, and X. Wang 
In the experiments of pinyin-to-character conversion, the results are very similar to 
those in the pos tagging experiments. NSHmm and VNSHmm show much more 
predictive power than Hmm does. NSHmm gets at most 40% error rate reduction and 
VNSHmm obtains 0.24% reduction in close test; and they achieve 15.7% and 20.11% 
reductions respectively in open test. As time information increases, the error rate of 
NSHmm decreases drastically in close test as it dose in pos tagging task. And the 
overfitting problem arises after k=2 in open test.  
However, different from the results of pos tagging experiments, VNSHmm 
outperforms NSHmm in open test. Since 6763 characters are adopted as states set in 
pinyin-to-character conversion system, which is much larger than the states set in pos 
tagging system, data sparseness problem is more likely to occur. VNSHmm can be 
view as a natural smoothing technique for NSHmm. Thus it works better. We also 
notice that the improvements in pinyin-to-character conversion experiments are more 
significant than those in pos-tagging experiments. In pinyin-to-character conversion 
task, the state chain is the Chinese sentence. Intuitively, some Chinese characters and 
words are much more likely to occur at some certain positions in the sentence, for 
instance, the beginning or the end of a sentence. As we discuss in section 3.3, in 
practice the time information of events in NSHmm is defined as the position 
information where the events occur. Then NSHmm and VNSHmm can capture those 
characteristics straightforwardly. But in pos-tagging, the state chain is the pos tag 
stream. Pos is a more abstract concept than word, and their positional characteristics 
are not as apparent as words?. Henceforth, the improvements in pos-tagging 
experiments are less significant than those in pinyin-to-character conversion 
experiments. But NSHmm and VNSHmm can still model and make good use of those 
positional characteristics, and notable improvements have been achieved. 
In a word, NSHmm and VNSHmm achieve much lower error rates in both of the 
two sequence labeling tasks and show much more predictive power than Hmm. 
5   Conclusions 
To improve Hmm?s predictive power and meanwhile avoid the problems of high-
order Hmm, this paper relaxes the stationary hypothesis of Hmm, makes use of time 
information and proposes NSHmm. Moreover, to further reduce NSHmm?s parameter 
space for mobile applications, VNSHmm is proposed by constructing statistical 
variables on the time information of NSHmm. Then NSHmm and VNSHmm are 
applied to two sequence labeling tasks: pos tagging and pinyin-to-character 
conversion. From the experiment results, we can draw three conclusions in this paper: 
? Firstly, NSHmm and VNSHmm achieve much lower error rates than Hmm in 
both of the two tasks and thus have more predictive power. 
? Secondly, the improvement of NSHmm is due to the increasing time 
information. 
? Lastly, a small number of parameters are enough to stat the statistical variables 
for VNSHmm. 
 Principles of Non-stationary Hidden Markov Model and Its Applications 837 
6   Further Research 
Since NSHmm is an enhanced Hmm, some problems of Hmm also exist in NSHmm. 
For example, data sparseness problem is arising as time information increases in 
NSHmm. Some smoothing algorithms should be designed to solve it in our further 
work. Also it?s difficult to describe long distance constraint for NSHmm and further 
research should be devoted to this problem. To construct more compact NSHmm, 
proper prone techniques should be further studied and be compared with VNSHmm. 
Acknowledgements 
This investigation was supported emphatically by the National Natural Science 
Foundation of China (No.60435020) and the High Technology Research and 
Development Programme of China (2002AA117010-09). 
We especially thank the three anonymous reviewers for their valuable suggestions 
and comments.  
References 
1. F. Jelinek. Self-Organized Language Modeling for Speech Recognition. IEEE ICASSP, 
1989.  
2. George Nagy. At the Frontier of OCR. Processing of IEEE. 1992, 80(7).  
3. ZhiMing Xu, XiaoLong Wang, Kai Zhang, Yi Guan. A Post Processing Method for Online 
Handwritten Chinese Character recognition. Journal of Computer Research and 
Development. Vol.36, No. 5, May 1999. 
4. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 
The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational 
Linguistics. 1992, 19(2). 
5. Liu Bingquan, Wang Xiaolong and Wang Yuying, Incorporating Linguistic Rules in 
Statistical Chinese Language Model for Pinyin-to-Character Conversion. High Technology 
Letters. Vol.7 No.2, June 2001, P:8-13 
6. Christopher D. Manning and Hinrich Schutze. Foundation of Statistic Natural Language 
Processing. The MIT Press. 1999.  
7. Brown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. 
Mercer. Class-based n-gram models of natural language. Computational Linguistics, 
18(4):467-479. 1992.  
8. Z. Ghahramani and M. Jordan. Factorial hidden Markov models. Machine Learning, 29, 
1997.  
9. J. Fritsch. ACID/HNN: A framework for hierarchical connectionist acoustic modeling. In 
Proc. IEEE ASRU, Santa Barbara, December 1997.  
10. Goodman, J. A bit of progress in language modeling. Computer Speech and Language, 
403-434. 2001.  
11. http://www.icl.pku.edu.cn 
Detecting Segmentation Errors in Chinese Annotated Corpus 
  Chengjie Sun   Chang-Ning Huang   Xiaolong Wang     Mu Li
Harbin Institute of  Microsoft Research,  Harbin Institute of  Microsoft Research, 
Technology, Harbin,   Asia, Beijing,     Technology, Harbin,    Asia, Beijing, 
150001, China     100080, China       150001, China      100080, China 
{cjsun, wangxl}@insun.hit.edu.cn     cnhuang@msrchina.research.microsoft.com
muli@microsoft.com
                      
Abstract
          
This paper proposes a 
semi-automatic method to detect 
segmentation errors in a manually 
annotated Chinese corpus in order 
to improve its quality further. A 
particular Chinese character string 
occurring more than once in a 
corpus may be assigned different 
segmentations during a 
segmentation process. Based on 
these differences our approach 
outputs the segmentation error 
candidates found in a segmented 
corpus and then on which the 
segmentation errors are identified 
manually. Segmentation error rate 
of a gold standard corpus can be 
given using our method. In Peking 
University (PK) and Academic 
Sinica (AS) test corpora of Special 
Interest Group for Chinese 
Language Processing (SIGHAN) 
Bakeoff1, 1.29% and 2.26% 
segmentation error rates are 
detected by our method. These 
errors decrease the F-measure of 
SIGHAN Bakeoff1 baseline test by 
1.36% in PK test data and 1.93% in 
AS test data respectively.  
                                                          
                                                           This work was done while Chengjie Sun was visiting 
Microsoft Research Asia. 
1 Introduction 
SIGHAN Bakeoff11 proposed an automatic 
method to evaluate the performance of 
different Chinese word segmentation 
systems on four distinct data sets. This 
method makes the performance of different 
Chinese word segmentation systems 
comparable and greatly promotes the 
technology of Chinese Word Segmentation. 
However, the quality of the reference 
corpora in the evaluation should be paid 
more attention because they provide training 
material for participants and they serve as a 
gold standard for evaluating the 
performance of participant systems.  
This paper presents a semi-automatic 
method to detect segmentation errors in a 
manually annotated Chinese corpus in order 
to improve its quality further. Especially a 
segmentation error rate of a gold standard 
corpus could be obtained with our approach. 
As we know a particular Chinese character 
string occurring more than once in a corpus 
may be assigned different segmentations. 
Those differences are considered as   
segmentation inconsistencies by some 
researchers (Wu, 2003; Chen, 2003).  
Segmentation consistency is also considered 
as one of the quality criteria of an annotated 
Chinese corpus (Sun, 1999). But in order to 
provide a more clearer description of those 
segmentation differences we define a new 
1 http://www.sighan.org/bakeoff2003/ 
1
term, segmentation variation, to replace the 
original one, segmentation inconsistency.
Our approach of spotting segmentation 
errors within an annotated corpus consists of 
two steps: (1) automatically listing the 
segmentation error candidates with 
segmentation variations found in an 
annotated corpus, (2) spotting segmentation 
errors within those candidates manually. 
The target of this approach is to count the 
number of error tokens in the corpus and 
give the segmentation error rate of the 
corpus, which is not given for any gold 
standard corpus in Bakeoff1. 
The remainder of this paper is structured 
as follows. In section 2, we discriminate the 
kinds of segmentation inconsistencies in test 
sets of SIGHAN Bakeoff1. In section 3, 
segmentation variation is defined and our 
approach to detect segmentation errors in a 
manually annotated corpus is proposed. In 
section 4 we conduct baseline experiments 
of PK and AS corpora with revised test sets 
in order to show exactly the impact of 
segmentation errors in the test sets of 
Bakeoff1. Section 5 is a brief conclusion. 
2 Segmentation inconsistency
In the close test of Bakeoff1, participants 
could only use training material from the 
training data for the particular corpus being 
testing on. No other material was allowed 
(Sproat and Emerson, 2003). As we know 
that the test data should be consistent with 
the training data based on a general 
definition of Chinese words. That is if we 
collect all words seen in the training data 
and store them into a lexicon, then each 
word in a test set is either a lexicon word or 
an OOV (out of vocabulary) word (Gao et 
al., 2003). In another word, if a character 
string has been treated as one word, i.e. a 
lexicon word, in the training data, the same 
occurrence should be taken in the 
corresponding test data unless it is a CAS 
(combination ambiguity string) and vice 
versa.
As we all know that a CAS like "??
[cai2-neng2]" may be segmented into one 
word or two words depending on different 
contexts. Thus segmentation inconsistency 
like "??" (talent) and "??" (only can) 
could both be correct segmentations in a text. 
Therefore ?segmentation inconsistency? 
should not be regarded as incorrect 
segmentations in general and should be 
clarified further. If one wants to discuss the 
segmentation errors based on segmentation 
inconsistencies, then from which those CAS 
instances should be excluded. 
If we exclude CAS words in our 
investigation temporary then for a non-CAS 
character string, there are four kinds of 
situations violating the general definition of 
Chinese word, also called lexicon driven 
principle in automatic word segmentation 
technology: 
S1. A character string is segmented 
inconsistently within a training data; 
S2. A character string is segmented 
inconsistently within a test data; 
S3. A character string is segmented 
inconsistently between a test data and its training 
data. This situation could be divided into the 
following two cases further: 
S3.1 A word identified in a training data has 
been segmented into multiple words in 
corresponding test data; 
S3.2 A word identified in a test data has been 
segmented into multiple words in 
corresponding training data. 
Chen (2003) describes inconsistency 
problem found in cases S1, S2 and S3.1 of 
PK corpora. For example, he gives the 
amount of unique text fragments that have 
two or more segmentations within PK 
training data, within PK test data and also 
between PK training data and PK test data. 
2
But those CAS words have not been 
excluded in his description. Ignoring the 
content of inconsistencies the influence 
about the number of segmentation 
inconsistencies of a particular corpus will be 
exaggerated greatly. In addition, Chen didn?t 
consider the case of S3.2 which could also 
affect the evaluation significantly according 
to the lexicon driven principle. 53 word 
types found in case 3.2 (refer to Appendix 
part 2) were totally treated as OOV words in 
Bakeoff1 which impacts the identification of 
those authentic new words in the task. So 
the issue of segmentation inconsistency in 
reference corpora needs further 
investigation.
As mentioned before, in common 
knowledge "segmentation inconsistency" is 
a derogatory term. But our investigation 
shows that most of segmentation 
inconsistencies found in an annotated corpus 
turned out to be correct segmentations of 
CASs. Therefore it is not an appropriate 
technique term to assess the quality of an 
annotated corpus. Besides, with the concept 
of "segmentation inconsistency" it is hard to 
distinguish the different inconsistent 
components within an annotated corpus and 
finally count up the number of segmentation 
errors exactly.  In the next section we 
propose a new term "segmentation 
variation" to replace the original one, 
"segmentation inconsistency". 
3 Segmentation variation  
3.1 Definition
Definition 1: In annotated corpora C, a set 
of f(W, C) is defined as: f(W, C) = {all 
possible segmentations that word W has in 
corpora C}.
Definition 2: W is a segmentation 
variation type (segmentation variation
in short, hereafter) with respect to C iff 
|f(W, C)|>1.
Definition 3: An instance of element in f(W,
C) is called a variation instance. Thus a 
segmentation variation (type) consists of 
more than one variation instances in 
corpora C. And a variation instance may 
include one or more than one tokens.
Definition 4: If a variation instance is an 
incorrect segmentation, it is called an 
error instance (EI).
The definitions of segmentation variation, 
variation instance and error instance (EI) 
clearly distinguish those inconsistent 
components, so we can count the number of 
segmentation errors (in tokens) exactly.  
The term variation is also used to express 
other annotation inconsistency in a corpus 
by other researchers. For example, 
Dickinson and Meurers (2003) used 
variation to describe POS (Part-of-Speech) 
inconsistency in an annotated corpus. 
Example 1: Segmentation variations 
(Bakeoff1 PK corpus):  
Word "??[deng3-tong2]" is segmented as 
"??" (equal) and "??" (et al with).  
Word "???[huang2-jin1-zhou1]" is 
segmented as "???" (golden week) and "
???" (gold week). 
Word "????[bing1-qing1-yu4-jie2]" is 
segmented as "????" (pure and noble) 
and "????" (ice clear jade clean). 
In example 1, Words like ????, ???
? ? and ????? ? are segmentation 
variation types. Segmentations ???? and 
??? ? are two variation instances of 
segmentation variation ????. Besides, the 
variation instance ???? consists of two 
tokens ??? and ???. While the variation 
instance "? ? ? ?" consists of four 
tokens "?", "?", "?" and "?".
The existence of segmentation variations 
in corpora lies in two reasons: 1) ambiguity: 
variation type W has multiple possible 
segmentations in different contexts, or 2) 
3
error: W has been wrongly segmented which 
could be judged by a given lexicon. 
Example 2: A segmentation variation 
caused by ambiguity (Bakeoff1 PK corpus): 
Segmentation variation: "??[guo2-du1]" 
Variation instances: "??" (capital) and "?
[guo2] ?[dou1]" (countries all). They are 
both correct segmentations in following 
sentences:
?????????????
(Constantinople became the capital of 
Byzantium.) 
??????????
????????
(Both countries all advocate solving 
disagreements by conversation and 
negotiation.) 
Example 3: Segmentation variations caused 
by error (Bakeoff1 PK corpus): 
Segmentation variation: "????
[jin4-guan3-ru2-ci4]" 
Variation instances: "????" (still) and "
????" (despite so). 
Segmentation variation: ??????
Variation instances: ?????? and ??
????
In the rest of the paper, a segmentation 
variation caused by ambiguity is called a 
CAS variation and a segmentation variation 
caused by error is called a non-CAS 
variation. Each kind of segmentation 
variations may include error instances (EIs). 
*: The number in the bracket is the amount caused by CAS.
Table 1 segmentation variations types, instances and EIs in PK test data 
3.2 Finding error instances (EIs) 
How to find the segmentation variations in 
corpora? Following is the algorithm of 
finding segmentation variations. According 
to our definition, the algorithm is quite 
straightforward. It takes two segmented 
Chinese corpora (reference corpus and 
corpus to be checked) and outputs a list of 
segmentation variation instances between 
the two corpora2.
Algorithm steps:
                                                          
2 These two corpora could be also regarded as one 
unique corpus: the corpus to be checked. A large scale 
reference corpus is always helpful in spotting more 
variations in the corpus to be checked. 
1. Extract all the multi-character words 
in reference corpus and store their positions 
in reference corpus respectively; 
2. Find the words that be segmented into 
N parts (N is from 2 to the length of current 
word) in the corpus to be checked. Store the 
positions of those segmentations found in 
the corpus to be checked; 
3. Output a list of variation instances 
with their contexts between two corpora. 
We use ?AutoCheck? to stand for the 
processing using the algorithm above. In 
order to find the segmentation variations 
within one corpus, we can also make the 
reference corpus and the corpus to be 
checked be the same corpus. Data in Table 1 
are obtained through ?AutoCheck + manual 
Situation
Within test 
data
Between:
One-to-Mult
Between:
Mult-to-One
# of variation type 21 92 228
# of variation instances 87 129 506
# of EIs* 12(3) 68(4) 77
# of error tokens* 28(6) 142(8) 77
4
checking?. That is firstly running 
?AutoCheck? 3 times as shown in Table 2 to 
get the list of variation types and instances 
in each situation respectively, and then EIs 
are found through manual checking.  
In Table 1, situations ?within test data?, 
?Between: One-to-Mult? and ?Between: 
Mult-to-One? correspond to the Situations 
S2, S3.1 and S3.2 described in Section 2. 
Here we still include CAS segmentations in 
order to take a close look at the distribution 
of EIs in each kind of segmentation 
variation. We can see that in situation 
?Between: One-to-Mult?, there are only 4 
EIs caused by CAS among 68 EIs. It is a 
very small fraction, so most of CAS 
variation instances are correct segmentations 
in a manually checked corpus. 
Situation Reference 
corpus
Corpus to be 
checked
Within test 
data 
PK test data PK test data 
Between: 
One-to-Mult 
PK training 
data 
PK test data 
Between: 
Mult-to-One 
PK test data PK training 
data 
Table 2 Inputs of different AutoCheck runs 
Except ??? ?? (gold week) most of 
the EIs in S2: ?within test data? are also 
found in S3.1: ?Between Rne-to-Pult?3. This 
is because in S3.1 the size of the reference 
corpus (training set) is much greater than the 
corpus to be checked (test set) so variations 
found in this case almost cover all of those 
found in S2 (test set only). EIs in S3.2: 
?between: Pult-to-Rne? are such strings that 
they are never considered as one word in PK 
training data while always identified as one 
                                                          
3 ??? ?? is considered as a segmentation 
error according to its variation instance ?????
(golden week). 
word in PK test data. For example, the 
segmentation variation (type) " ? ?
[shang4-tu2]" occurs four times as one word 
???? (above picture) in test data, but three 
of its variation instances "? ?" (upper 
picture) have been found in the training set. 
Thus, variation type "?? " should be 
identified as a segmentation error rather than 
an OOV word as in Bakeoff1. From Table 1, 
we can find 221 error tokens in all error 
instances (EIs) after removing the 26 
redundant ones in PK test data (17194 
tokens). So, the error rate of PK test data is 
1.29%. 
Using the same method, we also find out 
the 139 error instances (271 error tokens) in 
AS test data. The error rate of AS test data is 
2.26% as shown in table 3. 
Table 3 shows the error rate of AS test set 
is 2.26% and it is higher than PK test data 
which is 1.29%. So we believe that the 
reason why the evaluation result on AS 
corpus are higher than those on PK corpus 
of Bakeoff1 is not due to the segmentation 
quality of AS test data but because RI the 
OOV rate (0.022) in AS test data ZKLFK is 
much lower than PK test data (0.069). 
data PK test data AS test data 
Total tokens 17194 11985 
Error tokens 221 271 
Error rate 1.29% 2.26% 
Table 3 Segmentation errors in PK and AS 
test data 
?AutoCheck? outputs a list of all 
variation instances found in the corpus but it 
can not judge whether a variation instance is 
EI or not. Besides, the output of 
?AutoCheck? doesn?t include those 
segmentation errors which are not instances 
of any segmentation variation in a corpus. 
Two examples are given in Example 4. It 
5
means that ?AutoCheck+manual checking?4
can not spot all segmentation errors in a 
corpus. Despite of these disadvantages of 
?AutoCheck?, it is still a necessary assistant 
to find out almost all of the segmentation 
errors in an annotated corpus for its effective 
in finding segmentation error candidates. 
Example 4: Segmentation errors which are 
not instances of any segmentation variation 
(Bakeoff1 PK corpus): 
??????g?????
?g????????????
(Archon Marino Zanotti held a ceremony on 
the morning of 16th) 
????????????
?????
(?has become the largest scale agency 
system in the world) 
 AutoCheck has been applied in 
preparing the MSRA (Microsoft Research 
Asia) annotated corpora of Chinese word 
segmentation (MS corpora, hereafter) that 
were submitted to SIGHAN Bakeoff2 as one 
of the data sets. "AutoCheck+manual 
checking" is applied as the principal way of 
quality control on MS corpora. Even only 
taking a manual check on those variations 
output by the AutoCheck could provide an 
approximate assessment about the quality of 
the annotated corpus. The lower the number 
of error instances (EIs) found in the output 
list the lower the segmentation error rate the 
annotated corpus reaches. For example, 
there are 37 variation instances output by 
AutoCheck in an annotated document #25 
with 26K tokens in MS Corpora, in which 
no EIs has been found manually. Then the 
whole document was reviewed thoroughly 
by a person in which only two segmentation 
errors (shown in Example 5) have been 
found. Our practice shows that with the 
                                                          
4 ?manual checking? is restricted on the output list 
only. Therefore it is a very effective way to assess 
approximately the quality of an annotated corpus. 
quality control method above the 
segmentation error rate of MS corpora 
reaches 0.1% in average at the worst cases.  
Example 5: Segmentation errors in #25 
Error 1: ? ? ?????? ?? ?
???? ?? ?? ? ?? ?
(There are more than 360 leaders of 
corps and division working in grass roots of 
army and college.) 
The string "??[jun1-shi1]" (military 
counselor) should be corrected as "? ?"
(corps and division).
Error 2: ? ? ?? ? ?? ??? ?
?? ? ?? ?
(It is like a prism reflecting the style and 
features of the age) 
The string "??[yi1-mian4] (at the same 
time) should be corrected as "? ?" (a).
4 The impact to the evaluation 
caused by segmentation errors in 
corpora of Bakeoff1 
In order to show the impact to the 
evaluation result caused by EIs existing in 
test data of Bakeoff1, we conduct the 
baseline close test with PK and AS corpora, 
i.e. we compile lexicons only containing 
words in their training data and then use the 
lexicons with a forward maximum matching 
algorithm to segment their test data 
respectively (Sproat and Emerson, 2003). 
Original and modified test data are used as 
gold standard in our baseline test.  
In table 4, reference data PK1 and AS1 
are the original PK test data and AS test data. 
Reference data PK2 and AS2 are obtained 
after correcting all segmentation errors 
found in their original data (Table 3). 
Results in Table 4 are the output of 
Bakeoff1 evaluation program. Word count is 
the number of tokens in reference data and 
the change in word count is caused by our 
modification. Table 4 shows the impact of 
EIs in test data to the evaluation results. We 
6
can see the F measure increase to 0.879 
(0.933) from 0.867 (0.915) and the OOV 
ratio is decrease to 0.065 (0.020) from 0.069 
(0.022) when all EIs are corrected in PK 
(AS) test data. 
Table 4 Baseline test results with original and revised PK and AS test data 
5 Conclusion
A semi-automatic method to detect 
segmentation errors in a manually annotated 
Chinese corpus is presented in this paper. 
The main contributions of this research are:  
? Offer an effective way to spot the 
segmentation errors in a manually 
annotated corpus and give the 
segmentation error rate of the 
corpus.
? Point out that segmentation 
inconsistency is not an appropriate 
technique term to assess the 
segmentation quality of an 
annotated corpus and define the 
concept of segmentation variation 
instead to get the segmentation error 
rate of a gold standard corpus. 
? Show the influence to the evaluation 
result caused by the segmentation 
errors in a gold standard corpus. 
1.29% error rate of PK test data and 
2.26% error rate of AS test data 
decrease the F-measure of the 
SIGHAN Bakeoff1 baseline test by 
1.36% and 1.93% respectively. 
Acknowledgements 
We would like to thank the members of the 
Natural Language Computing Group at 
Microsoft Research Asia, especially to 
acknowledge Jianfeng Gao, John Chen, and 
the two anonymous reviewers for their 
insightful comments and suggestions. 
References 
Aitao Chen. 2003. Chinese word segmentation 
using minimal linguistic knowledge. In 
Proceedings of the Second SIGHAN 
Workshop on Chinese Language Processing, 
July 11-12, 2003, Sapporo, Japan. 
Andi Wu. 2003. Chinese word segmentation in 
MSR-NLP. In Proceedings of the Second 
SIGHAN Workshop on Chinese Language 
Processing, July 11-12, 2003, Sapporo, Japan. 
Jianfeng Gao, Mu Li and Chang-Ning Huang. 
2003. Improved source-channel models for 
Chinese word segmentation. In Proceedings 
of ACL-2003. July 7-12, 2003. Sapporo, 
Japan. 
Markus Dickinson, W. Detmar Meurers. 2003. 
Detecting errors in Part-of-Speech annotation. 
In Proceedings of the 11th Conference of the 
European Chapter of the Association for 
Referen
ce data 
Word
count
R P F OOV ROOV RIV
PK1 17,194 0.909 0.829 0.867 0.069 0.050 0.972 
PK2 17,200 0.920 0.841 0.879 0.065 0.053 0.980 
AS1 11,985 0.917 0.912 0.915 0.022 0.000 0.938 
AS2 11.886 0.939 0.926 0.933 0.020 0.000 0.958 
7
Computational Linguistics (EACL-03), 2003, 
Budapest, Hungary 
Richard Sproat, Thomas Emerson. 2003. The 
first international Chinese word Segmentation 
Bakeoff. In Proceedings of the Second 
SIGHAN Workshop on Chinese Language 
Processing, July 11-12, 2003, Sapporo, Japan. 
 Sun Maosong. 1999. On the consistency of 
word-segmented Chinese corpus. (In Chinese) 
Applied Linguistics, (2):88-91, 1999.  
Appendix: Modified EIs in PK test data 
1) EI found in CAS variations: 
Original: ??????????
????
Modified: ?????????
?????
2) Some EIs in S3.2 which are considered as 
new words in Bakeoff1: 
??? ???? ?? ??? ?? ?
? ?? ?? ??? ??? ?? ?
?? ??? ???? ?? ?? ??
? ?? ?? ?? ?? ?? ?? ?
? ?? ?? ?? ?? ?? ?? ?
? ?? ??? ?? ?? ????
?? ?? ?? ?? ?? ??? ?
? ?? ??? ?? ??? ????
?? ?? ?? ?? ??
3) Some EIs in S3.1 in which their variation 
types should be lexicon words 
Original Modified
?? ? ? ????
???? ? ?
?
???????
? ?? ???
?? ?? ????
?? ?? ????
?? ?? ????
? ? ? ? ????
?? ?? ????
?? ?? ????
? ? ??
? ? ??
? ? ??
? ? ??
8
  
Name Origin Recognition Using Maximum Entropy Model  
and Diverse Features 
Min Zhang1, Chengjie Sun2, Haizhou Li1, Aiti Aw1, Chew Lim Tan3, Xiaolong Wang2 
1Institute for Infocomm 
Research, Singapore 
{mzhang,hli,aaiti} 
@i2r.a-star.edu.sg 
2Harbin Institute of 
Technology, China 
{cjsun,wangxl} 
@insun.hit.edu.cn 
 
3National University of 
Singapore, Singapore 
tancl@comp.
nus.edu.sg 
Abstract 
Name origin recognition is to identify the 
source language of a personal or location 
name.  Some early work used either rule-
based or statistical methods with single 
knowledge source. In this paper, we cast the 
name origin recognition as a multi-class 
classification problem and approach the 
problem using Maximum Entropy method. 
In doing so, we investigate the use of differ-
ent features, including phonetic rules, n-
gram statistics and character position infor-
mation for name origin recognition. Ex-
periments on a publicly available personal 
name database show that the proposed ap-
proach achieves an overall accuracy of 
98.44% for names written in English and 
98.10% for names written in Chinese, which 
are significantly and consistently better than 
those in reported work.  
1 Introduction 
Many technical terms and proper names, such as 
personal, location and organization names, are 
translated from one language into another with 
approximate phonetic equivalents. The phonetic 
translation practice is referred to as transliteration; 
conversely, the process of recovering a word in its 
native language from a transliteration is called as 
back-transliteration (Zhang et al 2004; Knight 
and Graehl, 1998).  For example, English name 
?Smith? and ????  (Pinyin 1 : Shi-Mi-Si)? in 
                                                 
1 Hanyu Pinyin, or Pinyin in short, is the standard romaniza-
tion system of Chinese. In this paper, Pinyin is given next to 
Chinese form a pair of transliteration and back-
transliteration. In many natural language process-
ing tasks, such as machine translation and cross-
lingual information retrieval, automatic name 
transliteration has become an indispensable com-
ponent.  
Name origin refers to the source language of a 
name where it originates from. For example, the 
origin of the English name ?Smith? and its Chi-
nese transliteration ???? (Shi-Mi-Si)? is Eng-
lish, while both ?Tokyo? and ??? (Dong-Jing)? 
are of Japanese origin. Following are examples of 
different origins of a collection of English-Chinese 
transliterations. 
 
English: Richard-??? (Li-Cha-De) 
Hackensack-????(Ha-Ken-
Sa-Ke) 
Chinese: Wen JiaBao-???(Wen-Jia-
Bao) 
ShenZhen???(Shen-Zhen) 
Japanese: Matsumoto-?? (Song-Ben) 
Hokkaido-???(Bei-Hai-Dao) 
Korean: Roh MooHyun-???(Lu-Wu-
Xuan) 
Taejon-??(Da-Tian) 
Vietnamese: Phan Van Khai-???(Pan-
Wen-Kai) 
Hanoi-??(He-Nei) 
 
In the case of machine transliteration, the name 
origins dictate the way we re-write a foreign word. 
For example, given a name written in English or 
Chinese for which we do not have a translation in 
                                                                            
Chinese characters in round brackets for ease of reading. 
56
  
a English-Chinese dictionary, we first have to de-
cide whether the name is of Chinese, Japanese, 
Korean or some European/English origins. Then 
we follow the transliteration rules implied by the 
origin of the source name. Although all English 
personal names are rendered in 26 letters, they 
may come from different romanization systems. 
Each romanization system has its own rewriting 
rules. English name ?Smith? could be directly 
transliterated into Chinese as ????(Shi-Mi-Si)? 
since it follows the English phonetic rules, while 
the Chinese translation of Japanese name ?Koi-
zumi? becomes ???(Xiao-Quan)? following the 
Japanese phonetic rules. The name origins are 
equally important in back-transliteration practice. 
Li et al (2007) incorporated name origin recogni-
tion to improve the performance of personal name 
transliteration. Besides multilingual processing, 
the name origin also provides useful semantic in-
formation (regional and language information) for 
common NLP tasks, such as co-reference resolu-
tion and name entity recognition. 
Unfortunately, little attention has been given to 
name origin recognition (NOR) so far in the litera-
ture. In this paper, we are interested in two kinds 
of name origin recognition: the origin of names 
written in English (ENOR) and the origin of 
names written in Chinese (CNOR). For ENOR, 
the origins include English (Eng), Japanese (Jap), 
Chinese Mandarin Pinyin (Man) and Chinese Can-
tonese Jyutping (Can). For CNOR, they include 
three origins: Chinese (Chi, for both Mandarin and 
Cantonese), Japanese and English (refer to Latin-
scripted language). 
Unlike previous work (Qu and Grefenstette, 
2004; Li et al, 2006; Li et al, 2007) where NOR 
was formulated with a generative model, we re-
gard the NOR task as a classification problem. We 
further propose using a discriminative learning 
algorithm (Maximum Entropy model: MaxEnt) to 
solve the problem. To draw direct comparison, we 
conduct experiments on the same personal name 
corpora as that in the previous work by Li et al 
(2006). We show that the MaxEnt method effec-
tively incorporates diverse features and outper-
forms previous methods consistently across all test 
cases. 
The rest of the paper is organized as follows: in 
section 2, we review the previous work. Section 3 
elaborates our proposed approach and the features. 
Section 4 presents our experimental setup and re-
ports our experimental results. Finally, we con-
clude the work in section 5. 
2 Related Work 
Most of previous work focuses mainly on ENOR 
although same methods can be extended to CNOR. 
We notice that there are two informative clues that 
used in previous work in ENOR. One is the lexical 
structure of a romanization system, for example, 
Hanyu Pinyin, Mandarin Wade-Giles, Japanese 
Hepbrun or Korean Yale, each has a finite set of 
syllable inventory (Li et al, 2006). Another is the 
phonetic and phonotactic structure of a language, 
such as phonetic composition, syllable structure. 
For example, English has unique consonant 
clusters such as /str/ and /ks/ which Chinese, 
Japanese and Korean (CJK) do not have. 
Considering the NOR solutions by the use of these 
two clues, we can roughly group them into two 
categories: rule-based methods (for solutions 
based on lexical structures) and statistical methods 
(for solutions based on phonotactic structures). 
Rule-based Method  
Kuo and Yang (2004) proposed using a rule-
based method to recognize different romanization 
system for Chinese only. The left-to-right longest 
match-based lexical segmentation was used to 
parse a test word. The romanization system is con-
firmed if it gives rise to a successful parse of the 
test word. This kind of approach (Qu and Grefen-
stette, 2004) is suitable for romanization systems 
that have a finite set of discriminative syllable in-
ventory, such as Pinyin for Chinese Mandarin. For 
the general tasks of identifying the language origin 
and romanization system, rule based approach 
sounds less attractive because not all languages 
have a finite set of discriminative syllable inven-
tory. 
Statistical Method 
1) N-gram Sum Method (SUM): Qu and Gre-
fenstette (2004) proposed a NOR identifier using a 
trigram language model (Cavnar and Trenkle, 
1994) to distinguish personal names of three lan-
guage origins, namely Chinese, Japanese and Eng-
lish. In their work, the training set includes 11,416 
Chinese name entries, 83,295 Japanese name en-
tries and 88,000 English name entries. However, 
the trigram is defined as the joint probabil-
57
  
ity 1 2( )i i ip c c c? ? for 3-character 1 2i i ic c c? ?  rather than 
the commonly used conditional probabil-
ity 1 2( | )i i ip c c c? ? . Therefore, the so-called trigram 
in Qu and Grefenstette (2004) is basically a sub-
string unigram probability, which we refer to as 
the n-gram (n-character) sum model (SUM) in this 
paper. Suppose that we have the unigram count 
1 2( )i i iC c c c? ? for character substring 1 2i i ic c c? ? , the 
unigram is then computed as: 
1 2
1 2
1 2
1 2,
( )
( )
( )
i i i
i i i
i i i
i i ii c c c
C c c c
p c c c
C c c c
? ?
? ?
? ?
? ?
= ?           (1) 
which is the count of character substring 1 2i i ic c c? ?  
normalized by the sum of all 3-character string 
counts in the name list for the language of interest.  
For origin recognition of Japanese names, this 
method works well with an accuracy of 92%. 
However, for English and Chinese, the results are 
far behind with a reported accuracy of 87% and 
70% respectively. 
2) N-gram Perplexity Method (PP): Li et al 
(2006) proposed using n-gram character perplexity 
cPP  to identify the origin of a Latin-scripted name. 
Using bigram, the cPP is defined as: 
1
1 log ( | )
2
Nc
i i 1ic
p c cN
cPP
?
=
? ?
=   (2) 
where cN is the total number of characters in the 
test name, ic is the i
th character in the test name. 
1( | )i ip c c ? is the bigram probability which is 
learned from each name list respectively. As a 
function of model, cPP  measures how good the 
model matches the test data. Therefore, cPP can be 
used to measure how good a test name matches a 
training set. A test name is identified to belong to 
a language if the language model gives rise to the 
minimum perplexity. Li et al (2006) shown that 
the PP method gives much better performance 
than the SUM method. This may be due to the fact 
that the PP measures the normalized conditional 
probability rather than the sum of joint probability. 
Thus, the PP method has a clearer mathematical 
interpretation than the SUM method. 
The statistical methods attempt to overcome the 
shortcoming of rule-based method, but they suffer 
from data sparseness, especially when dealing 
with a large character set, such as in Chinese (our 
experiments will demonstrate this point empiri-
cally). In this paper, we propose using Maximum 
Entropy (MaxEnt) model as a general framework 
for both ENOR and CNOR. We explore and inte-
grate multiple features into the discriminative clas-
sifier and use a common dataset for benchmarking. 
Experimental results show that the MaxEnt model 
effectively incorporates diverse features to demon-
strate competitive performance.   
3 MaxEnt Model and Features 
3.1 MaxEnt Model for NOR 
The principle of maximum entropy (MaxEnt) 
model is that given a collection of facts, choose a 
model consistent with all the facts, but otherwise 
as uniform as possible (Berger et al, 1996). Max-
Ent model is known to easily combine diverse fea-
tures. For this reason, it has been widely adopted 
in many natural language processing tasks. The 
MaxEnt model is defined as: 
( , )
1
1
( | ) j i
K
f c x
i j
j
p c x
Z
?
=
= ?           (3) 
      ( , )
1 1 1
( | ) j i
KN N
f c x
i j
i i j
Z p c x ?
= = =
= =? ??          (4) 
where ic is the outcome label, x is the given obser-
vation, also referred to as an instance. Z is a nor-
malization factor. N  is the number of outcome 
labels, the number of language origins  in our case. 
1 2, , , Kf f fL are feature functions and 
1 2, , , K? ? ?L are the model parameters. Each pa-
rameter corresponds to exactly one feature and can 
be viewed as a ?weight? for the corresponding fea-
ture.  
In the NOR task, c is the name origin label; x is 
a personal name, if is a feature function. All fea-
tures used in the MaxEnt model in this paper are 
binary. For example: 
 
1,    " "& (" ")
( , )
0,  j
if c Eng x contains str
f c x
otherwise
=?
= ??
 
In our implementation, we used Zhang?s maxi-
mum entropy package2. 
3.2 Features 
Let us use English name ?Smith? to illustrate the 
features that we define. All characters in a name 
                                                 
2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html 
58
  
are first converted into upper case for ENOR be-
fore feature extraction. 
N-gram Features: N-gram features are de-
signed to capture both phonetic and orthographic 
structure information for ENOR and orthographic 
information only for CNOR. This is motivated by 
the facts that: 1) names written in English but from 
non-English origins follow different phonetic rules 
from the English one; they also manifest different 
character usage in orthographic form; 2) names 
written in Chinese follows the same pronunciation 
rules (Pinyin), but the usage of Chinese characters 
is distinguishable between different language ori-
gins as reported in Table 2 of (Li et al, 2007).  
The N-gram related features include: 
1) FUni: character unigram <S, M, I, T, H> 
2) FBi: character bigram <SM, MI, IT, TH> 
3) FTri: character trigram <SMI, MIT, ITH > 
Position Specific n-gram Features: We in-
clude position information into the n-gram fea-
tures. This is mainly to differentiate surname from 
given name in recognizing the origin of CJK per-
sonal names written in Chinese. For example, the 
position specific n-gram features of a Chinese 
name ????(Wen-Jia-Bao)? are as follows: 
1) FPUni: position specific unigram  
<0?(Wen), 1?(Jia), 2?(Bao)> 
2) FPBi: position specific bigram  
<0??(Wen-Jia), 1??(Jia-Bao)> 
3) FPTri: position specific trigram  
<0???(Wen-Jia-Bao)> 
Phonetic Rule-based Features: These features 
are inspired by the rule-based methods (Kuo and 
Yang, 2004; Qu and Grefenstette, 2004) that check 
whether an English name is a sequence of sylla-
bles of CJK languages in ENOR task. We use the 
following two features in ENOR task as well. 
1) FMan: a Boolean feature to indicate 
whether a name is a sequence of Chinese 
Mandarin Pinyin.   
2) FCan: a Boolean feature to indicate whether 
a name is a sequence of Cantonese Jyutping. 
Other Features:  
1) FLen: the number of Chinese characters in a 
given name. This feature is for CNOR only.  
The numbers of Chinese characters in per-
sonal names vary with their origins. For ex-
ample, Chinese and Korean names usually 
consist of 2 to 3 Chinese characters while 
Japanese names can have up to 4 or 5 Chi-
nese characters 
2) FFre: the frequency of n-gram in a given 
name. This feature is for ENOR only. In 
CJK names, some consonants or vowels 
usually repeat in a name as the result of the 
regular syllable structure. For example, in 
the Chinese name ?Zhang Wanxiang?, the 
bigram ?an? appears three times 
Please note that the trigram and position spe-
cific trigram features are not used in CNOR due to 
anticipated data sparseness in CNOR3.  
4 Experiments 
We conduct the experiments to validate the effec-
tiveness of the proposed method for both ENOR 
and CNOR tasks. 
4.1 Experimental Setting 
 
Origin #  entries Romanization System 
Eng4 88,799 English 
Man5 115,879 Pinyin 
Can 115,739 Jyutping 
Jap6 123,239 Hepburn 
 
Table 1: DE: Latin-scripted personal name corpus for 
ENOR 
 
 
Origin #  entries 
Eng7 37,644 
Chi8 29,795 
Jap9 33,897 
 
Table 2: DC: Personal name corpus written in Chinese 
characters for CNOR 
 
                                                 
3 In the test set of CNOR, 1080 out of 2980 names of Chinese 
origin do not consist of any bigrams learnt from training data, 
while 2888 out of 2980 names do not consist of any learnt 
trigrams. This is not surprising as most of Chinese names only 
have two or three Chinese characters and in our open testing, 
the train set is exclusive of all entries in the test set.  
4 http://www.census.gov/genealogy/names/ 
5 http://technology.chtsai.org/namelist/  
6 http://www.csse.monash.edu.au/~jwb/enamdict_doc.html 
7 Xinhua News Agency (1992)  
8 http://www.ldc.upenn.edu LDC2005T34 
9 www.cjk.org 
59
  
Datasets: We prepare two data sets which are col-
lected from publicly accessible sources: DE and DC 
for the ENOR and CNOR experiment respectively. 
DE is the one used in (Li et al, 2006), consisting of 
personal names of Japanese (Jap), Chinese (Man), 
Cantonese (Can) and English (Eng) origins. DC 
consists of personal names of Japanese (Jap), Chi-
nese (Chi, including both Mandarin and Canton-
ese) and English (Eng) origins. Table 1 and Table 
2 list their details. In the experiments, 90% of en-
tries in Table 1 (DE) and Table 2 (DC) are ran-
domly selected for training and the remaining 10% 
are kept for testing for each language origin. Col-
umns 2 and 3 in Tables 7 and 8 list the numbers of 
entries in the training and test sets.  
 
Evaluation Methods: Accuracy is usually used to 
evaluate the recognition performance (Qu and 
Gregory, 2004; Li et al, 2006; Li et al, 2007). 
However, as we know, the individual accuracy 
used before only reflects the performance of recall 
and does not give a whole picture about a multi-
class classification task. Instead, we use precision 
(P), recall (R) and F-measure (F) to evaluate the 
performance of each origin. In addition, an overall 
accuracy (Acc) is also given to describe the whole 
performance. The P, R, F and Acc are calculated 
as following: 
 
#        
#          
correctly recognized entries of the given origin
P
entries recognized as the given origin by the system
=
 
 
#        
#      
correctly recognized entries of the given origin
R
entries of the given origin
=
 
 
2PR
F
P R
=
+
     #     
#   
all correctly recognized entries
Acc
all entries
=
 
4.2 Experimental Results and Analysis 
Table 3 reports the experimental results of ENOR. 
It shows that the MaxEnt approach achieves the 
best result of 98.44% in overall accuracy when 
combining all the diverse features as listed in Sub-
section 3.2. Table 3 also measures the contribu-
tions of different features for ENOR by gradually 
incorporating the feature set. It shows that:  
1) All individual features are useful since the 
performance increases consistently when 
more features are being introduced. 
2) Bigram feature presents the most informa-
tive feature that gives rise to the highest 
performance gain, while the trigram feature  
further boosts performance too. 
3) MaxEnt method can integrate the advan-
tages of previous rule-based and statistical 
methods and easily integrate other features. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
    
R(
%)
 
F 
Ac
c(%
) 
Eng 91.40 80.76 85.75
Man 83.05 81.90 82.47
Can 81.13 82.76 81.94
FUni 
Jap 87.31 94.11 90.58
85.29
Eng 97.54 91.10 94.21
Man 97.51 98.10 97.81
Can 97.68 98.05 97.86
+FBi 
Jap 94.62 98.24 96.39
96.72
Eng 97.71 93.79 95.71
Man 98.94 99.37 99.16
Can 99.12 99.19 99.15
+FTri 
Jap 96.19 98.52 97.34
97.97
Eng 97.53 94.64 96.06
Man 99.21 99.43 99.32
Can 99.41 99.24 99.33
+FPUni 
Jap 96.48 98.49 97.47
98.16
Eng 97.68 94.98 96.31
Man 99.32 99.50 99.41
Can 99.53 99.34 99.44
+FPBi 
Jap 96.59 98.52 97.55
98.28
Eng 97.62 94.97 96.27
Man 99.34 99.58 99.46
Can 99.63 99.37 99.50
+FPTri 
Jap 96.61 98.45 97.52
98.30
Eng 97.74 95.06 96.38
Man 99.37 99.59 99.48
Can 99.61 99.41 99.51
+FFre 
Jap 96.66 98.56 97.60
98.35
Eng 97.82 95.11 96.45
Man 99.52 99.68 99.60
Can 99.71 99.59 99.65
 + FMan 
+ FCan 
Jap 96.69 98.59 97.63
98.44
 
Table 3: Contribution of each feature for ENOR 
 
 
60
  
Features Eng Jap Man Can 
FMan -0.357 0.069 0.072 -0.709 
FCan -0.424 -0.062 -0.775 0.066 
 
Table 4: Features weights in ENOR task. 
 
F
ea
tu
re
 
O
ri
gi
n 
P(
%
) 
R(
%
) 
F 
   A
cc(
%
) 
Eng 97.89 98.43 98.16
Chi 95.80 95.03 95.42FUni 
Jap 96.96 97.05 97.00
96.97 
Eng 96.99 98.27 97.63
Chi 96.86 92.11 94.43+FBi 
Jap 95.04 97.73 96.36
96.28 
Eng 97.35 98.38 97.86
Chi 97.29 95.00 96.13+FLen 
Jap 96.78 97.64 97.21
97.14 
Eng 97.74 98.65 98.19
Chi 97.65 96.34 96.99+FPUni 
Jap 97.91 98.05 97.98
97.77 
Eng 97.50 98.43 97.96
Chi 97.61 96.04 96.82+FPBi 
Jap 97.59 97.94 97.76
97.56 
Eng 98.08 99.04 98.56
Chi 97.57 96.88 97.22
FUni 
+FLen 
+ 
FPUni Jap 98.58 98.11 98.34
98.10 
 
Table 5: Contribution of each feature for CNOR 
 
Table 4 reports the feature weights of two fea-
tures ?FMan? and ?FCan? with regard to different 
origins in ENOR task. It shows that ?FCan? has 
positive weight only for origin ?Can? while 
?FMan? has positive weights for both origins 
?Man? and ?Jap?, although the weight for ?Man? 
is higher. This agrees with our observation that the 
two features favor origins ?Man? or ?Can?. The 
feature weights also reflect the fact that some 
Japanese names can be successfully parsed by the 
Chinese Mandarin Pinyin system due to their simi-
lar syllable structure. For example, the Japanese 
name ?Tanaka Miho? is also a sequence of Chi-
nese Pinyin: ?Ta-na-ka Mi-ho?.  
Table 5 reports the contributions of different 
features in CNOR task by gradually incorporating 
the feature set. It shows that:  
1) Unigram features are the most informative 
2) Bigram features degrade performance. This 
is largely due to the data sparseness prob-
lem as discussed in Section 3.2.   
3) FLen is also useful that confirms our intui-
tion about name length. 
Finally the combination of the above three use-
ful features achieves the best performance of 
98.10% in overall accuracy for CNOR as in the 
last row of Table 5. 
In Tables 3 and 5, the effectiveness of each fea-
ture may be affected by the order in which the fea-
tures are incorporated, i.e., the features that are 
added at a later stage may be underestimated. 
Thus, we conduct another experiment using "all-
but-one" strategy to further examine the effective-
ness of each kind of features. Each time, one type 
of the n-gram (n=1, 2, 3) features (including or-
thographic n-gram, position-specific and n-gram 
frequency features) is removed from the whole 
feature set. The results are shown in Table 6. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
 
R(
%)
 
F 
Ac
c(%
) 
Eng 97.81 95.01 96.39
Man 99.41 99.58 99.49
Can 99.53 99.48 99.50
w/o 
Uni-
gram 
Jap 96.63 98.52 97.57
98.34
Eng 97.34 95.17 96.24
Man 99.30 99.48 99.39
Can 99.54 99.33 99.43
w/o Bi-
gram 
Jap 96.73 98.32 97.52
98.26
Eng 97.57 94.10 95.80
Man 98.98 99.23 99.10
Can 99.20 99.08 99.14
w/o 
Tri-
gram 
Jap 96.06 98.42 97.23
97.94
 
Table 6: Effect of n-gram feature for ENOR 
 
Table 6 reveals that removing trigram features 
affects the performance most. This suggests that 
trigram features are much more effective for 
ENOR than other two types of features. It also 
shows that trigram features in ENOR does not suf-
fer from the data sparseness issue. 
As observed in Table 5, in CNOR task, 93.96% 
61
  
accuracy is obtained when removing unigram fea-
tures, which is much lower than 98.10% when bi-
gram features are removed. This suggests that uni-
gram features are very useful in CNOR, which is 
mainly due to the data sparseness problem that 
bigram features may have encountered. 
4.3 Model Complexity and Data Sparseness 
Table 7 (ENOR) and Table 8 (CNOR) compare 
our MaxEnt model with the SUM model (Qu and 
Gregory, 2004) and the PP model (Li et al, 2006). 
All the experiments are conducted on the same 
data sets as described in section 4.1. Tables 7 and 
8 show that the proposed MaxEnt model outper-
forms other models. The results are statistically 
significant ( 2? test with p<0.01) and consistent 
across all tests. 
Model Complexity: 
We look into the complexity of the models and 
their effects. Tables 7 and 8 summarize the overall 
accuracy of three models. Table 9 reports the 
numbers of parameters in each of the models. We 
are especially interested in a comparison between 
the MaxEnt and PP models because their perform-
ance is close.  We observe that, using trigram fea-
tures, the MaxEnt model has many more parame-
ters than the PP model does. Therefore, it is not 
surprising if the MaxEnt model outperforms when 
more training data are available. However, the ex-
periment results also show that the MaxEnt model 
consistently outperforms the PP model even with 
the same size of training data. This is largely at-
tributed to the fact that MaxEnt incorporates more 
robust features than the PP model does, such as 
rule-based, length of names features.  
One also notices that PP clearly outperforms 
SUM by using the same number of parameters in 
ENOR and shows comparable performance in 
CNOR tasks. Note that SUM and PP are different 
in two areas: one is the PP model employs word 
length normalization while SUM doesn?t; another 
that the PP model uses n-gram conditional prob-
ability while SUM uses n-character joint probabil-
ity. We believe that the improved performance of 
PP model can be attributed to the effect of usage 
of conditional probability, rather than length nor-
malization since length normalization does not 
change the order of probabilities. 
Data Sparesness: 
We understand that we can only assess the ef-
fectiveness of a feature when sufficient statistics is 
available. In CNOR (see Table 8), we note that the 
Chinese transliterations of English origin use only 
377 Chinese characters, so data sparseness is not a 
big issue. Therefore, bigram SUM and bigram PP 
methods easily achieve good performance for Eng-
lish origin. However, for Japanese origin (repre-
sented by 1413 Chinese characters) and Chinese 
origin (represented by 2319 Chinese characters), 
the data sparseness becomes acute and causes per-
formance degradation in SUM and PP models. We 
are glad to find that MaxEnt still maintains a good 
performance benefiting from other robust features. 
Table 10 compares the overall accuracy of the 
three methods using unigram and bigram features 
in CNOR task, respectively. It shows that the 
MaxEnt method achieves best performance. An-
other interesting finding is that unigram features 
perform better than bigram features for PP and  
MaxEnt models, which shows that  data sparseness 
remains an issue even for MaxEnt model.  
5 Conclusion 
We propose using MaxEnt model to explore di-
verse features for name origin recognition. Ex-
periment results show that our method is more ef-
fective than previously reported methods. Our 
contributions include: 
1) Cast the name origin recognition problem as 
a multi-class classification task and propose 
a MaxEnt solution to it; 
2) Explore and integrate diverse features for 
name origin recognition and propose the 
most effective feature sets for ENOR and 
for CNOR 
In the future, we hope to integrate our name 
origin recognition method with a machine translit-
eration engine to further improve transliteration 
performance. We also hope to study the issue of 
name origin recognition in context of sentence and 
use contextual words as additional features. 
References 
Adam L. Berger, Stephen A. Della Pietra and Vincent J. 
Della Pietra. 1996. A Maximum Entropy Approach 
to Natural Language Processing. Computational Lin-
guistics. 22(1):39?71. 
William B. Cavnar and John M. Trenkle. 1994. Ngram 
based text categorization. In 3rd Annual Symposium 
62
  
on Document Analysis and Information Retrieval, 
275?282. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics. 24(4), 
599-612. 
Jin-Shea Kuo and Ying-Kuei Yan. 2004. Generating 
Paired Transliterated-Cognates Using Multiple Pro-
nunciation Characteristics from Web Corpora. PA-
CLIC 18, December 8th-10th, Waseda University, 
Tokyo, Japan, 275?282. 
Haizhou Li, Shuanhu Bai and Jin-Shea Kuo. 2006. 
Transliteration. Advances in Chinese Spoken Lan-
guage Processing. World Scientific Publishing Com-
pany, USA, 341?364. 
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo and Minghui 
Dong. 2007. Semantic Transliteration of Personal 
Names. ACL-2007. 120?127. 
Xinhua News Agency. 1992. Chinese Transliteration of 
Foreign Personal Names. The Commercial Press  
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in 
Latin script via language identification and corpus 
validation. ACL-2004. 183?190. 
Min Zhang, Jian Su and Haizhou Li. 2004. Direct Or-
thographical Mapping for Machine Translation. 
COLING-2004. 716-722. 
 
Trigram SUM Trigram PP MaxEnt Origin # training 
entries 
# test 
entries P (%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 79,920 8,879 94.66 72.50 82.11 95.84 94.72 95.28 97.82 95.11 96.45
Man 104,291 11,588 86.79 94.87 90.65 98.99 98.33 98.66 99.52 99.68 99.60
Can 104,165 11,574 90.03 93.87 91.91 96.17 99.67 97.89 99.71 99.59 99.65
Jap 110,951 12,324 89.17 92.84 90.96 98.20 96.29 97.24 96.69 98.59 97.63
Overall Acc (%) 89.57 97.39 98.44 
Table 7: Benchmarking different methods in ENOR task 
Bigram SUM  Bigram PP  MaxEnt Origin # training 
entries 
# test 
entries P(%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 37,644 3,765 95.94 98.65 97.28 97.58 97.61 97.60 98.08 99.04 98.56 
Chi 29,795 2,980 96.26 87.35 91.59 95.10 87.35 91.06 97.57 96.88 97.22 
Jap 33,897 3,390 93.01 97.67 95.28 90.94 97.43 94.07 98.58 98.11 98.34 
Overall Acc (%) 95.00 94.53 98.10 
Table 8: Benchmarking different methods in CNOR task 
# of parameters for ENOR # of parameters for CNOR 
Methods 
Trigram Unigram Bigram 
MaxEnt  124,692 13,496  182,116 
PP 16,851 4,045 86,490 
SUM  16,851 4,045 86,490 
 
Table 9: Numbers of parameters used in different methods 
 
 SUM PP MaxEnt 
Unigram Features 90.55 97.09 98.10 
Bigram Features 95.00 94.53 97.56 
 
Table 10: Overall accuracy using unigram and bigram features in CNOR task 
63
A Study of Chinese Lexical Analysis Based on Discriminative Models 
Guang-Lu Sun  Cheng-Jie Sun  Ke Sun and Xiao-Long Wang 
Intelligent Technology & Natural Language Processing Laboratory, School of Computer 
Science and Technology, Harbin Institute of Technology, 150001, Harbin, China 
{glsun, cjsun, ksun, wangxl}@insun.hit.edu.cn 
 
Abstract 
This paper briefly describes our system in 
The Fourth SIGHAN Bakeoff. 
Discriminative models including maximum 
entropy model and conditional random 
fields are utilized in Chinese word 
segmentation and named entity recognition 
with different tag sets and features. 
Transformation-based learning model is 
used in part-of-speech tagging. Evaluation 
shows that our system achieves the 
F-scores: 92.64% and 92.73% in NCC 
Word Segmentation close and open tests, 
89.11% in MSRA name entity recognition 
open test, 91.13% and 91.97% in PKU 
part-of-speech tagging close and open tests. 
All the results get medium performances 
on the bakeoff tracks. 
1 Introduction 
Lexical analysis is the basic step in natural 
language processing. It is prerequisite to many 
further applications, such as question answer 
system, information retrieval and machine 
translation. Chinese lexical analysis chiefly 
consists of word segmentation (WS), name entity 
recognition (NER) and part-of-speech (POS) 
tagging. Because Chinese does not have explicit 
word delimiters to mark word boundaries like 
English, WS is essential process for Chinese. POS 
tagging and NER are just like those of English.  
Our system participated in The Fourth SIGHAN 
Bakeoff which held in 2007. Different approaches 
are applied to solve all the three tasks which are 
integrated into a unified system (ITNLP-IsLex). 
For WS task, conditional random fields (CRF) are 
used. For NER, maximum entropy model (MEM) 
is applied. And transformation-based learning 
(TBL) algorithm is utilized to solve POS tagging 
problem. The reasons using different models are 
listed in the rest sections of this paper. We give a 
brief introduction to our system sequentially. 
Section 2 describes WS. Section 3 and section 4 
introduce NER and POS tagging respectively. We 
give some experimental results in section 5. Finally 
we draw some conclusions. 
2 Chinese word segmentation 
For WS task, NCC corpus is chosen both in close 
test and open test.  
2.1 Conditional random fields 
Conditional random fields are undirected graphical 
models defined by Lafferty (2001). There are two 
advantages of CRF. One is their great flexibility to 
incorporate various types of arbitrary, 
non-independent features of the input, the other is 
their ability to overcome the label bias problem.  
Given the observation sequence X, on the basis 
of CRF, the conditional probability of the state 
sequence Y is: 
( ) (k k i-1 i
k
1
p Y X = exp l f y , y ,X,i
Z(X)
)? ?? ?? ??      (1)
(k k i-1 i
y Y k
)Z(X)= exp l f y , y ,X,i
?
? ?? ?? ?? ?          (2) 
Z(x) is the normalization factor. ( )1, , ,k i if y y X i?  
is the universal definition of features in CRF. 
2.2 Word segmentation based on CRF 
Inspired by Zhao (2006), the Chinese WS task is 
considered as a sequential labeling problem, i.e., 
assigning a label to each character in a sentence 
given its contexts. CRF model is adopted to do 
labeling. 
6 tags are utilized in this work: B, B1, B2, I, E, S. 
The meaning of each tag is listed in Table 1. The 
147
Sixth SIGHAN Workshop on Chinese Language Processing
raw training file format from NCC can be easily to 
convert to this 6 tags format. 
? ? ? ? ? ?An example: /S /B /B1 /B2 /I /I 
? ? ? ? ? ? ? ? ?/I /I /I /E /B /E /B /E /S.  
 
Table 1 Tags of character-based labeling 
Tag Meaning 
B The 1st character of a multi-character word
B1 The 2nd character of a multi-character word 
B2 The 3rd character of a multi-character word 
I Other than B, B1, B2 and last character in a multi-character word 
E The last character of a multi-character word 
S Single character word 
 
The contexts window size for each character is 5: 
C-2, C-1, C0, C1, and C2. There are 10 feature 
templates used to generate features for CRF model 
including uni-gram, bi-gram and tri-gram: C-2, C-1, 
C0, C1, C2, C-1C0, C0C1, C-2C-1C0, C-1C0C1, and 
C0C1C2. 
For the parameters in CRF model, we only do 
work to choose cut-off value for features. Our 
experiments show that the best performance can be 
achieved when cut-off value is set to 2. 
Maximum likelihood estimation and L-BFGS 
algorithm is used to estimate the weight of 
parameters in the training module. Baum-Welch 
algorithm is used to search the best sequence of 
test data. 
For close test, we only used CRF to do 
segmentation, no more post-processing, such as 
time and date finding, was done. So the 
performance could be further improved. 
For open test, we just use our NER system to tag 
the output of our close segmentation result, no more 
other resources were involved. 
3 Chinese name entity recognition 
For NER task, MSRA is chosen in open test. 
Chinese name dictionary, foreign name dictionary, 
Chinese place dictionary and organization 
dictionary are used in the model. 
3.1 Maximum entropy model 
Maximum entropy model is an exponential 
model that offers the flexibility of integrating 
multiple sources of knowledge into a model 
(Berger, 1996). It focuses on the modeling of 
tagging sequence, replacing the modeling of 
observation sequence. 
Given the observations sequence X, on the basis 
of MEM, the conditional probability of the state 
sequence Y is: 
1
( | ) exp ( , )
( ) j jj
p Y X f Y X
Z X
?? ?= ??? ?
? ??         (3) 
( ) exp ( , )j j
Y j
Z X f?? ?= ??? ?
? ? Y X ??              (4) 
 
Table 2 Feature templates of NER 
Feature 
template Description 
Ci 
The word tokens in the 
window 
i =-2, -1, 0, 1, 2 
Ti 
The NE tags 
i = -1  
CiCi-1 
The bigram of Ci 
i = -1, 1 
Pi 
The POS tags of word 
tokens 
i = -1, 0, 1 
P-1P1 
The combination of POS 
tags 
T-1C0 
The previous tag and the 
current word token 
B Ci is Chinese family name 
C Ci is part of Chinese first name 
W Ci is Chinese whole name 
F Ci is foreign name 
S Ci is Chinese first name 
W(Ci) 
O other 
W(Ci-1)W(Ci) 
The bigram of W(Ci) 
i = -1, 1 
IsInOrgDict(C0)
The current word token is in 
organization dictionary 
IsInPlaceDict(C0)
The current word token is in 
place dictionary 
148
Sixth SIGHAN Workshop on Chinese Language Processing
Being Similar to the definition of CRF, Z(x) is 
the normalization factor. ( ),jf Y X is the universal 
definition of features. 
3.2 Name entity recognition based on MEM 
Firstly, we use a segmentation tool to split both 
training and test corpus into word-token-based 
texts. Characters that are not in the dictionary are 
scattered in the texts. NE tags using in the model 
follow the tags in training corpus. Other word 
tokens that do not belong to NE are tagged as O. 
Based on the segmented text, the context window 
is also set as 5. Inspired by Zhang?s (2006) work, 
there are 10 types of feature templates for 
generating features for NER model in Table 2. 
When training our ME Model, the best 
performance can be achieved when cut-off value is 
set to 1. 
Maximum likelihood estimation and GIS 
algorithm is used to estimate the weight of 
parameters in the model. The iteration time is 500.  
4 Chinese part-of-speech tagging 
For POS tagging task, NCC corpus and PKU 
corpus are chosen both in the close test and open 
test. 
4.1 Transformation-based learning 
The formalism of Transformation-based learning is 
first introduced in 1992. It starts with the correctly 
tagged training corpus. A baseline heuristic for 
initial tag and a set of rule templates that specify the 
transformation rules match the context of a word. 
By transformating the error initial tags to the correct 
ones, a set of candidate rules are built to be the 
conditional pattern based on which the 
transformation is applied. Then, the candidate rule 
which has the best transformation effect is selected 
and stored as the first transformation rules in the 
TBL model. The training process is repeated until 
no more candidate rule has the positive effect. The 
selected rules are stored in the learned rule sequence 
in turn for the purpose of template correction 
learning. 
4.2 Part-of-speech tagging based on TBL 
POS tagging is a standard sequential labeling 
problem. CRF has some advantages to solve it. 
Because both corpora have relative many POS tags, 
our computational ability can not afford the CRF 
model in condition of these tags. TBL model is 
utilized to replace with CRF. 
We compute the max probability of current 
word?s POS tag in training corpus. The POS tag 
which has max occurrence probability for each 
word is used to tag its word token. By this method, 
we got the initial POS tag for each word.  
The rule templates which are formed from 
conjunctions of words match to particular 
combinations in the histories of the current 
position. 40 types of rule templates are built using 
the patterns. The cut-off value of the 
transformation rules is set to 3 (Sun, 2007). 
For open test, our NER system is used to tag the 
output of our POS tagging result. Parts of NE tags 
are corrected. 
5 Evaluation 
Following the measurement approach adopted in 
SIGHAN, we measure the performance of the three 
tasks in terms of the precision (P), recall (R), and 
F-score (F). 
5.1 Word segmentation results 
Table 3 Word segmentation results on NCC corpus 
NCC close test open test 
R .9268 .9268 
Cr .00133447 .00133458 
P .926 .928 
Cp .00134119 .00132534 
F .9264 .9273 
Roov .6094 .6265 
Poov .4948 .5032 
Foov .5462 .5581 
Riv .9426 .9417 
Piv .9527 .9546 
Fiv .9476 9481 
 
The WS results are listed on the Table 3. Some 
errors could be caused by the annotation 
differences between the training data and test data.  
For example, ???? (A Zhen) was considered as a 
whole word in training data, while ???? (A Lan)  
was annotated as two separate word ??? (A) and 
??? (Lan) in the test data. Some post-processing 
rules for English words, money unit and 
morphology can improve the performance further, 
Following are such errors in our results: ?vid eo?, 
149
Sixth SIGHAN Workshop on Chinese Language Processing
?? ?? (Japan yen), ?? ? ? ?? (not three 
not four). 
For open test, we hoped to use NER module to 
increase the OOV recall. But the NER module 
didn?t prompt the performance very much because 
it was trained by the MSRA NER data in Bakeoff3. 
The difference between two corpora may depress 
the NER modules effect. Also, the open test was 
done on the output of close test and all the errors 
were passed. 
5.2 Name entity recognition results 
The official results of our NER system on MSRA 
corpus for open track are showed in Table 4. As it 
shows, our system achieves a relatively high score 
on both PER and LOC task, but the performance of 
ORG is not so good, and the Avg1 performance is 
decreased by it. The reasons are: (1) The ORG 
sequences are often very long and our system is 
unable to deal with the long term, a MEMM or 
CRF model may perform better. (2) The resource 
for LOC and ORG are much smaller than that of 
PER. More sophisticated features such like 
?W(Ci)? may provide more useful information for 
the system. 
 
Table 4 NER results on MSRA corpus 
MSRA P R F 
PER .9498 .9549 .9524 
LOC .9129 .9194 .9161 
ORG .8408 .7469 .7911 
Avg1 .9035 .8791 .8911 
5.3 Part-of-speech tagging results 
We evaluate our POS tagging model on the PKU 
corpus for close and open track and NCC corpus 
for close track based on TBL. Table 5 is the 
official result of our system. In PKU open test, 
NER is used to recognize name entity of text, so its 
result is better than that of close test. The IV-R 
result is relative good, but the OOV-R is not so 
good, which drops the total performance. The 
reasons lie in: (1) TBL model is not good at 
tagging out of vocabulary words. CRF model may 
be a better selection if our computer can meet its 
huge memory requirements. (2) Our NER system 
is trained by MSRA corpus. It does not fit the PKU 
and NCC corpus. 
 
Table 5 POS results on PKU and NCC corpus 
Corpus Total-A IV-R OOV-R MT-R
PKU close 
test .9113 .9518 .2708 .8958
PKU open 
test .9197 .9512 .4222 .899 
NCC close 
test .9277 .9664 .2329 .9 
6 Conclusions 
Chinese lexical analysis system is built for the 
SIGHAN tracks which consists of Chinese word 
segmentation, name entity recognition and 
part-of-speech tagging. Conditional random fields, 
maximum entropy model and transformation-based 
learning model are utilized respectively. Our 
system achieves the medium results in all the three 
tasks. 
References 
A. Berger, S. A. Della Pietra and V. J. Della Pietra. A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, 1996. 22(1), 
pages 39-71. 
 
G. Sun, Y. Guan and X. Wang. A Maximum Entropy 
Chunking Model With N-fold Template Correction. 
Journal of Electronics, 2007. 24(5), pages 690-695. 
 
J. Lafferty, A. McCallum and F. Pereira. Conditional 
random fields: Probabilistic models for segmenting 
and labeling sequence data. In Proceedings of 
ICML-2001, Williams College, Massachusetts, USA. 
2001. pages 282-289. 
 
S. Zhang, Y. Qin, J. Wen, X. Wang. Word 
Segmentation and Named Entity Recognition for 
SIGHAN Bakeoff3. In Proceedings of the Fifth 
SIGHAN Workshop on Chinese Language Processing. 
Sydney, Australia. 2006. pages 158?161. 
 
H. Zhao, C. Huang, and M. Li. An improved Chinese 
word segmentation system with conditional random 
field. In Proceedings of the Fifth SIGHAN Workshop 
on Chinese Language Processing, Sydney, Australia. 
2006. pages 162?165.  
 
150
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 189?192,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Pragmatic Chinese Word Segmentation System 
Wei Jiang, Yi Guan, Xiao-Long Wang 
School of Computer Science and Technology, Harbin Institute of Technology,  
Heilongjiang Province, 150001, P.R.China 
jiangwei@insun.hit.edu.cn 
 
Abstract 
This paper presents our work for partici-
pation in the Third International Chinese 
Word Segmentation Bakeoff. We apply 
several processing approaches according 
to the corresponding sub-tasks, which are 
exhibited in real natural language. In our 
system, Trigram model with smoothing 
algorithm is the core module in word 
segmentation, and Maximum Entropy 
model is the basic model in Named En-
tity Recognition task. The experiment 
indicates that this system achieves F-
measure 96.8% in MSRA open test in the 
third SIGHAN-2006 bakeoff. 
1 Introduction 
Word is a logical semantic and syntactic unit in 
natural language. Unlike English, there is no de-
limiter to mark word boundaries in Chinese lan-
guage, so in most Chinese NLP tasks, word seg-
mentation is a foundation task, which transforms 
Chinese character string into word sequence. It is 
prerequisite to POS tagger, parser or further ap-
plications, such as Information Extraction, Ques-
tion Answer system. 
Our system participated in the Third Interna-
tional Chinese Word Segmentation Bakeoff, 
which held in 2006. Compared with our system 
in the last bakeoff (Jiang 2005A), the system in 
the third bakeoff is adjusted intending to have a 
better pragmatic performance. This paper mainly 
focuses on describing two sub-tasks: (1) The ba-
sic Word Segmentation; (2) Named entities rec-
ognition. We apply different approaches to solve 
above two tasks, and all the modules are inte-
grated into a pragmatic system (ELUS). 
2 System Description 
All the words in our system are categorized into 
five types: Lexicon words (LW), Factoid words 
(FT), Morphologically derived words (MDW), 
Named entities (NE), and New words (NW). 
Figure 1 demonstrates our system structure.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The input character sequence is converted into 
one or several sentences, which is the basic deal-
ing unit. The ?Basic Segmentation? is used to 
identify the LW, FT, MDW words, and ?Named 
Entity Recognition? is used to detect NW words. 
We don?t adopt the New Word detection algo-
rithm in our system in this bakeoff. The ?Disam-
biguation? module performs to classify compli-
cated ambiguous words, and all the above results 
are connected into the final result, which is de-
noted by XML format. 
2.1 Trigram and Smoothing Algorithm 
We apply the trigram model to the word segmen-
tation task (Jiang 2005A), and make use of Ab-
solute Smoothing algorithm to overcome the 
sparse data problem. 
Trigram model is used to convert the sentence 
into a word sequence. Let w = w1 w2 ?wn be a 
word sequence, then the most likely word se-
quence w* in trigram is: 
?
=
??=
n
i
iii
www
wwwP
n 1
12 )|(maxarg*
21 L
w                   (1) 
where let P(w0|w-2 w-1) be P(w0) and let P(w1|w-1 
w0) be P(w1|w0), and wi represents LW or a type 
of FT or MDW. In order to search the best seg-
mentation way, all the word candidates are filled 
in the word lattice (Zhao 2005). And the Viterbi 
Basic 
Segmentation
Disambiguation
Sentence
Named Entity 
Recognition
Word 
Sequence
Factoid Detect
Lexicon Words
Morphological 
Word
Input 
Sequence
Figure 1 ELUS Segmenter and NER 
189
algorithm is used to search the best word seg-
mentation path.  
FT and MDW need to be detected when con-
structing word lattice (detailed in section 2.2). 
The data structure of lexicon can affect the effi-
ciency of word segmentation, so we represent 
lexicon words as a set of TRIEs, which is a tree-
like structure. Words starting with the same 
character are represented as a TRIE, where the 
root represents the first Chinese character, and 
the children of the root represent the second 
characters, and so on (Gao 2004). 
When searching a word lattice, there is the 
zero-probability phenomenon, due to the sparse 
data problem. For instance, if there is no cooc-
curence pair ???/?/???(we eat bananas) in 
the training corpus, then P(??|????) = 0. 
According to formula (1), the probability of the 
whole candidate path, which includes ???/?/
??? is zero, as a result of the local zero prob-
ability. In order to overcome the sparse data 
problem, our system has applied Absolute Dis-
counting Smoothing algorithm (Chen, 1999). 
|}0)(:{|)( 1 1
1
11 >=? ? +?? +?+ ii niii ni wwcwwN             (2) 
The notation N1+ is meant to evoke the number 
of words that have one or more counts, and the ?  
is meant to evoke a free variable that is summed 
over. The function ()c  represents the count of 
one word or the cooccurence count of multi-
words. In this case, the smoothing probability 
? +?+?? +?
?=
iw
i
ni
i
nii
nii
wc
Dwc
wwp
)(
}0,)(max{
)|(
1
11
1  
)|()1( 1 2
? +??+ i nii wwp?          (3)    
where, 
??
?
?
?
??
?
?
?
?=? ? +?+
+?? )()(1 1 111 i niw i ni wNwc
D
i
?     (4) 
Because we use trigram model, so the maxi-
mum n may be 3. A fixed discount D (0 ?D ? 1) 
can be set through the deleted estimation on the 
training data. They arrive at the estimate 
21
1
2nn
nD +=                                              (5) 
where n1 and n2 are the total number of n-grams 
with exactly one and two counts, respectively. 
After the basic segmentation, some compli-
cated ambiguous segmentation can be further 
disambiguated. In trigram model, only the previ-
ous two words are considered as context features, 
while in disambiguation processing, we can use 
the Maximum Entropy model fused more fea-
tures (Jiang 2005B) or rule based method. 
2.2 Factoid and Morphological words 
All the Factoid words can be represented as regu-
lar expressions. So the detection of factoid words 
can be achieved by Finite State Automaton(FSA). 
In our system, the following categories of factoid 
words can be detected, as shown in table 1. 
Table 1 Factoid word categories 
FT type Factoid word Example 
Number
Integer, real, 
percent  etc. 
2910, 46.12%, ??
?, ?????? 
Date Date 2005? 5? 12? 
Time Time 8:00, ????? 
English English word, How, are, you 
www Website, IP address  
http://www.hit.edu.cn
192.168.140.133 
email Email elus@google.com 
phone Phone, fax 0451-86413322 
Deterministic FSA (DFA) is efficient because 
a unique ?next state? is determined, when given 
an input symbol and the current state. While it is 
common for a linguist to write rule, which can be 
represented directly as a non-deterministic FSA 
(NFA), i.e. which allows several ?next states? to 
follow a given input and state.  
Since every NFA has an equivalent DFA, we 
build a FT rule compiler to convert all the FT 
generative rules into a DFA. e.g.  
z ?< digit > -> [0..9]; 
z < year > ::= < digit >{< digit >+}??; 
z < integer > ::= {< digit >+}; 
where ?->? is a temporary generative rule, and 
?::=? is a real generative rule. 
As for the morphological words, we erase the 
dealing module, because the word segmentation 
definition of our system adopts the PKU standard. 
3 Named Entity Recognition 
We adopt Maximum Entropy model to perform 
the Named Entity Recognition. The extensive 
evaluation on NER systems in recent years (such 
as CoNLL-2002 and CoNLL-2003) indicates the 
best statistical systems are typically achieved by 
using a linear (or log-linear) classification algo-
rithm, such as Maximum Entropy model, to-
gether with a vast amount of carefully designed 
linguistic features. And this seems still true at 
present in terms of statistics based methods. 
Maximum Entropy model (ME) is defined 
over H? T in segmentation disambiguation, 
where H is the set of possible contexts around 
target word that will be tagged, and T is the set of 
allowable tags, such as B-PER, I-PER, B-LOC, 
I-LOC etc. in our NER task. Then the model?s 
conditional probability is defined as 
190
? ?= Tt thp
thphtp
'
)',(
),()|(                                (6) 
where ?
=
=
k
j
thf
j
jthp
1
),(),( ???                              (7) 
where h is the current context and t is one of the 
possible tags.  
The several typical kinds of features can be 
used in the NER system. They usually include 
the context feature, the entity feature, and the 
total resource or some additional resources.  
Table 2 shows the context feature templates. 
Table2 NER feature template1 
Type Feature Template 
One order feature wi-2, wi-1, wi, wi+1, wi+2 
Two order feature wi-1:i, wi:i+1 
NER tag feature t i-1 
While, we only point out the local feature 
template, some other feature templates, such as 
long distance dependency templates, are also 
helpful to NER performance. These trigger fea-
tures can be collected by Average Mutual Infor-
mation or Information Gain algorithm etc. 
Besides context features, entity features is an-
other important factor, such as the suffix of Lo-
cation or Organization. The following 8 kinds of 
dictionaries are usually useful (Zhao 2006): 
Table 3 NER resource dictionary2  
List Type Lexicon Example 
Place lexicon ??, ??, ???Word list 
Chinese surname ?, ?, ?, ?? 
Prefix of PER ?, ?, ? 
Suffix of PLA ?, ?, ?, ?, ? String list 
Suffix of ORG ?, ??, ??, ? 
Character for CPER ?,?, ?, ?, ? 
Character for FPER ?, ?, ?, ?, ? Character list 
Rare character ?, ?, ? 
In addition, some external resources may im-
prove the NER performance too, e.g. we collect a 
lot of entities for Chinese Daily Newspaper in 
2000, and total some entity features. 
However, our system is based on Peking Uni-
versity (PKU) word segmentation definition and 
PKU NER definition, so we only used the basic 
features in table 2 in this bakeoff.  Another effect 
is the corpus: our system is training by the Chi-
nese Peoples? Daily Newspaper corpora in 1998, 
which conforms to PKU NER definition. In the 
section 4, we will give our system performance 
with the basic features in Chinese Peoples? Daily 
Newspaper corpora. 
                                                 
1 wi ? current word, wi-1 ? previous word, ti ? current tag. 
2 Partial translation: ?? BeiJing,?? New york;? Zhang, 
?Wang; ? Old;? mountain,? lake;? bureau. 
4 Performance and analysis 
4.1 The Evaluation in Word Segmentation 
The performance of our system in the third bake-
off is presented in table 4 in terms of recall(R), 
precision(P) and F score in percentages. The 
score software is standard and open by SIGHAN. 
Table 4 MSRA test in SIGHAN2006 (%) 
MSRA R P F OOV Roov Riv
Close 96.3 91.8 94.0 3.4 17.5 99.1
Open 97.7 96.0 96.8 3.4 62.4 98.9
Our system has good performance in terms of 
Riv measure. The Riv measure in close test and in 
open test are 99.1% and 98.9% respectively. This 
good performance owes to class-based trigram 
with the absolute smoothing and word disam-
biguation algorithm. 
In our system, it is the following reasons that 
the open test has a better performance than the 
close test: 
(1) Named Entity Recognition module is 
added into the open test system. And Named En-
tities, including PER, LOC, ORG, occupy the 
most of the out-of-vocabulary words. 
(2) The system of close test can only use the 
dictionary that is collected from the given train-
ing corpus, while the system of open test can use 
a better dictionary, which includes the words that 
exist in MSRA training corpus in SIGHAN2005. 
And we know, the dictionary is the one of impor-
tant factors that affects the performance, because 
the LW candidates in the word lattice are gener-
ated from the dictionary. 
As for the dictionary, we compare the two col-
lections in SIGHAN2005 and SIGHAN2006, and 
evaluating in SIGHAN2005 MSRA close test. 
There are less training sentence in SIGHAN2006, 
as a result, there is at least 1.2% performance 
decrease. So this result indicates that the diction-
ary can bring an important impact in our system. 
Table 5 gives our system performance in the 
second bakeoff. We?ll make brief comparison. 
Table 5 MSRA test in SIGHAN 2005 (%) 
MSRA R P F OOV Roov Riv
Close 97.3 94.5 95.9 2.6 32.3 99.1
Open 98.0 96.5 97.2 2.6 59.0 99.0
Comparing table 4 with table 5, we find that 
the OOV is 3.4 in third bakeoff, which is higher 
than the value in the last bakeoff. Obviously, it is 
one of reasons that affect our performance. 
In addition, based on pragmatic consideration, 
our system has been made some simplifier, for 
instance, we erase the new word detection algo-
rithm and the is no morphological word detection. 
191
4.2 Named Entity Recognition 
In MSRA NER open test, our NER system is 
training in prior six-month corpora of Chinese 
Peoples? Daily Newspaper in 1998, which were 
annotated by Peking University. Table 6 shows 
the NER performance in the MSRA open test. 
Table 6 The NER performance in MSRA Open test 
MSRA NER Precision Recall  F Score 
PER 93.68% 86.37% 89.87 
LOC 85.50% 59.67% 70.29 
ORG 75.87% 47.48% 58.41 
Overall 86.97% 65.56% 74.76 
As a result of insufficiency in preparing bake-
off, our system is only trained in Chinese Peo-
ples? Daily Newspaper, in which the NER is de-
fined according to PKU standard. However, the 
NER definition of MSRA is different from that 
of PKU, e.g, ???/LOC ???, ??/PER ?
/PER??? in MSRA, are not entities in PKU. 
So the training corpus becomes a main handicap 
to decrease the performance of our system, and it 
also explains that there is much difference be-
tween the recall rate and the precision in table 6. 
Table 7 gives the evaluation of our NER sys-
tem in Chinese Peoples? Daily Newspaper, train-
ing in prior five-month corpora and testing in the 
sixth month corpus. We also use the feature tem-
plates in table 2, in order to make comparison 
with table 6. 
Table 7 The NER test in Chinese Peoples? Daily 
MSRA NER Precision Recall  F Score 
CPN 93.56 90.96 92.24 
FPN 90.42 86.47 88.40 
LOC 91.94 90.52 91.22 
ORG 88.38 84.52 86.40 
Overall 91.35 88.85 90.08 
This experiment indicates that our system can 
have a good performance, if the test corpus and 
the training corpora conform to the condition of 
independent identically distributed attribution. 
4.3 Analysis and Discussion 
Some points need to be further considered: 
(1) The dictionary can bring a big impact to 
the performance, as the LW candidates come 
from the dictionary. However a big dictionary 
can be easily acquired in the real application. 
(2) Due to our technical and insufficiently 
preparing problem, we use the PKU NER defini-
tion, however they seem not unified with the 
MSRA definition. 
(3) Our NER system is a word-based model, 
and we have find out that the word segmentation 
with two different dictionaries can bring a big 
impact to the NER performance. 
(4) We erase the new word recognition algo-
rithm in our system. While, we should explore 
the real annotated corpora, and add new word 
detection algorithm, if it has positive effect. e.g. 
???  ??(lotus prize) can be recognized as one 
word by the conditional random fields model. 
5 Conclusion 
We have briefly described our word segmenta-
tion system and NER system. We use word-
based features in the whole processing. Our sys-
tem has a good performance in terms of Riv 
measure, so this means that the trigram model 
with the smoothing algorithm can deal with the 
basic segmentation task well. However, the result 
in the bakeoff indicates that detecting out-of-
vocabulary word seems to be a harder task than 
dealing with the segmentation-ambiguity task. 
The work in the future will concentrate on two 
sides: improving the NER performance and add-
ing New Word Detection Algorithm. 
References 
HuaPing Zhang, Qun Liu etc. 2003. Chinese Lexical 
Analysis Using Hierarchical Hidden Markov 
Model, Second SIGHAN workshop affiliated with 
4th ACL, Sapporo Japan, pp.63-70. 
Jianfeng Gao, Mu Li et al 2004. Chinese Word Seg-
mentation: A Pragmatic Approach. MSR-TR-2004-
123, November 2004. 
Peng Fuchun, Fangfang Feng and Andrew McCallum. 
Chinese segmentation and new word detection us-
ing conditional random fields. In:COLING 2004. 
Stanley F.Chen and J. Goodman. 1999. An empirical 
study of smoothing techniques for language model-
ing. Computer Speech and Language. 13:369-394. 
Wei Jiang, Jian Zhao et al 2005A.Chinese Word 
Segmentation based on Mixing Model. 4th 
SIGHAN Workshop. pp. 180-182.  
Wei Jiang, Xiao-Long Wang, Yi Guan et al 2005B. 
applying rough sets in word segmentation disam-
biguation based on maximum entropy model. Jour-
nal of Harbin Institute of Technology (New Series). 
13(1): 94-98. 
Zhao Jian. 2006. Research on Conditional Probabilis-
tic Model and Its Application in Chinese Named 
Entity Recognition. Ph.D. Thesis. Harbin Institute 
of Technology, China. 
Zhao Yan. 2005. Research on Chinese Morpheme 
Analysis Based on Statistic Language Model. Ph.D. 
Thesis. Harbin Institute of Technology, China. 
192
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 109?113,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Joint Syntactic and Semantic Dependency Parsing System based on
Maximum Entropy Models
Buzhou Tang1 Lu Li2 Xinxin Li1 Xuan Wang2 Xiaolong Wang2
Shenzhen Graduate School
Harbin Institute of Technology
Shenzhen,518055, China
1{tangbuzhou,lixxin2}@gmail.com
2{lli,wangxuan,wangxl}@insun.hit.edu.cn
Abstract
A joint syntactic and semantic dependency
parsing system submitted to the CoNLL-2009
shared task is presented in this paper. The
system is composed of three components: a
syntactic dependency parser, a predicate clas-
sifier and a semantic parser. The first-order
MSTParser is used as our syntactic depen-
dency pasrser. Projective and non-projective
MSTParsers are compared with each other on
seven languages. Predicate classification and
semantic parsing are both recognized as clas-
sification problem, and the Maximum Entropy
Models are used for them in our system. For
semantic parsing and predicate classifying, we
focus on finding optimized features on multi-
ple languages. The average Macro F1 Score
of our system is 73.97 for joint task in closed
challenge.
1 Introduction
The task for CoNLL-2009 is an extension of the
CoNLL-2008 shared task to multiple languages: En-
glish (Surdeanu et al, 2008), Catalan plus Span-
ish (Mariona Taule? et al, 2008), Chinese (Martha
Palmer et al, 2009), Czech (Jan Hajic? et al,
2006), German (Aljoscha Burchardt et al, 2006) and
Japanese (Daisuke Kawahara et al, 2002). Com-
pared to the CoNLL-2008 shared task, the predi-
cates are given for us in semantic dependencies task.
Therefore, we have only need to label the semantic
roles of nouns and verbs, and the frames of predi-
cates.
In this paper, a joint syntactic and semantic de-
pendency parsing system submitted to the CoNLL-
2009 shared task is presented. The system is com-
posed of three components: a syntactic dependency
parser, a predicate classifier and a semantic parser.
The first-order MSTParser is used as our syntactic
dependency parser. Projective and non-projective
MSTParsers are compared with each other on seven
languages. The predicate classifier labeling the
frames of predicates and the semantic parser label-
ing the semantic roles of nouns and verbs for each
predicate are both recognized as classification prob-
lem, and the Maximum Entropy Models (MEs) are
used for them in our system. Among three com-
ponents, we mainly focus on the predicate classifier
and the semantic parser.
For semantic parsing and predicate classifying,
features of different types are selected to our sys-
tem. The effect of them on multiple languages will
be described in the following sections in detail.
2 System Description
Generally Speaking, a syntactic and semantic de-
pendency parsing system is usually divided into four
separate subtasks: syntactic parsing, predicate iden-
tification, predicate classification, and semantic role
labeling. In the CoNLL-2009 shared task, the pred-
icate identification is not required, since the pred-
icates are given for us. Therefore, the system we
present is only composed of three components: a
syntactic dependency parser, a predicate classifier
and a semantic parser. The syntactic dependencies
are processed with the MSTParser 0.4.3b. The pred-
icates identification and semantic role label are pro-
cessed with MEs-based classifier respectively. Un-
like conventional systems, the predicates identifica-
109
tion and the semantic parser are independent with
each other. Figure 1 is the architecture of our sys-
tem.
Figure 1: System Architecture
In our system, we firstly select an appropriate
mode (projective or non-projective) of Graph-based
Parser (MSTParser) for each language, then con-
struct the MEs-based predicates classification and
the MEs-based semantic parser with syntactic de-
pendency relationships and predicate classification
respectively.
2.1 Syntactic Dependency Parsing
MSTParser (McDonald, 2008) is used as our syn-
tactic dependency parser. It is a state-of-the-art de-
pendency parser that searches for maximum span-
ning trees (MST) over directed graph. Both of pro-
jective and non-projective are supported by MST-
Parser. Our system employs the first-order frame-
work with projective and non-projective modes on
seven given languages.
2.2 Predicate Classification
In this phase, we label the sense of each predicate
and the MEs are adopted for classification. Features
of different types are extracted for each predicate,
and an optimized combination of them is adopted in
our final system. Table 1 lists all features. 1-20 are
the features used in Li?s system (Lu Li et al, 2008),
No Features No Features
1 w0 20 Lemma
2 p0 21 DEPREL
3 p?1 22 CHD POS
4 p1 23 CHD POS U
5 p?1p0 24 CHD REL
6 p0p1 25 CHD REL U
7 p?2p0 26 SIB REL
8 p0p2 27 SIB REL U
9 p?3p0 28 SIB POS
10 p0p3 29 SIB POS U
11 p?1p0p1 30 VERB V
12 w0p0 31 4+11
13 w0p?1p0 32 Indegree
14 w0p0p1 33 Outdegree
15 w0p?2p0 34 Degree
16 w0p0p2 35 ARG IN
17 w0p?3p0 36 ARG OUT
18 w0p0p3 37 ARG Degree
19 w0p?1p0p1 38 Span
Table 1: Features for Predicate Classification.
and 21-31 are a part of the optimized features pre-
sented in Che?s system (Wanxiang Che et al, 2008)
In Table 1, ?w? denotes the word and ?p? de-
notes POS of the words. Features in the form of
part1 part2 denote the part2 of the part1, while fea-
tures in the form of part1+part2 denote the combi-
nation of the part1 and part2. ?CHD? and ?SIB? de-
note a sequence of the child and the sibling words
respectively, ?REL? denotes the type of relations,
?U? denotes the result after reducing the adjacent
duplicate tags to one, ?V? denotes whether the part
is a voice, ?In? and ?OUT? denote the in degree and
out degree, which denotes how many dependency
relations coming into this word and going away from
this word,and ?ARG? denotes the semantic roles of
the predicate. The ?Span? denotes the maximum
length between the predicate and its arguments. The
final optimized feature combination is :1-31 and 33-
37.
2.3 Semantic Role Labeling
The semantic role labeling usually contains two sub-
tasks: argument identification and argument classi-
fication. In our system, we perform them in a single
110
stage through one classifier, which specifies a par-
ticular role label to the argument candidates directly
and assigns ?NONE? label to the argument candi-
dates with no role. MEs are also adopted for classifi-
cation. For each word in a sentence, MEs gives each
candidate label (including semantic role labels and
none label) a probability for the predicate. The fea-
tures except for the feature (lemma plus sense num-
ber of the predicate in (Lu Li et al, 2008)) and the
features 32-38 in Table 1 are selected in our system.
3 Experiments and Results
We train the first-order MSTParser 1 with projective
and non-projective modes in terms of default param-
eters respectively. Our maximum entropy classifiers
are implemented with the Maximum Entropy Mod-
eling Toolkit 2 . The default classifier parameters are
used in our system except for iterations. All mod-
els are trained using all training data, and tested on
the whole development data and test data, with 64-
bit 3.00GHz Intel(R) Pentium(R) D CPU and 4.0G
memory.
3.1 Syntactic Dependency Parsing
Table 2 is a performance comparison between pro-
jective parser and non-projective parser on the devel-
opment data of seven languages. In Table 2, ?LAS?,
?ULAS? and ?LCS? denote as Labeled attachment
score, Unlabeled attachment score and Label accu-
racy score respectively.
The experiments show that Catalan, Chinese and
Spanish have projective property and others have
non-projective property.
3.2 Predicate Classification
To get the optimized system, three group features are
used for comparison.
? group 1: features 1-20 in Table 1.
? group 2: features 1-31 in Table 1.
? group 3: all features in Table 1.
The performance of predicate classification on the
development data of the six languages, which con-
tain this subtask, are given in Table 3. The results
1http://sourceforge.net/projects/mstparser.
2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
LAS(%) ULAS(%) LCS(%)
Catalan 84.18 88.18 91.76
83.69 87.74 91.59
Chinese 72.58 77.06 82.07
62.85 69.47 73.00
Czech 72.79 81.40 80.93
73.18 81.86 81.30
English 86.89 90.29 91.50
86.88 90.34 91.58
German 83.43 86.89 90.24
84.00 87.40 90.61
Japanese 92.23 93.16 98.38
92.23 93.14 98.45
Spanish 83.88 87.93 91.36
83.46 87.46 91.37
Table 2: Performance of Syntactic Dependency
Parsing with different modes. The above line is the
performance of projective mode, while the below
one is the performance of non-projective mode for
each language.
group 1 group 2 group 3
Catalan 75.51 80.90 82.23
Chinese 93.79 94.99 94.75
Czech 91.83 91.77 91.86
English 92.12 92.48 93.20
German 74.49 74.14 75.85
Spanish 74.01 76.22 76.53
Table 3: Performance of predicate classification (F1
scores) for different group features on the develop-
ment data of the six languages.
show that Che?s features and the degrees of the pred-
icate and its arguments are useful for all languages,
the former improves the labeled F1 measure by 0.3%
to 5.4%, and the latter by 0.3% to 1.7%.
3.3 Semantic Role Labeling
In this phase, feature selection and performance lose
caused by P-columns are studied. Firstly, we com-
pare the following two group features:
? group 1: The features except for the lemma
plus sense number of the predicate in (Lu Li
et al, 2008).
111
LF1 ULF1 PF1
Catalan 73.25 92.69 38.41
72.71 91.93 35.22
83.23 100.00 61.88
Chinese 69.60 82.15 28.35
71.49 81.71 29.41
85.44 95.21 58.20
Czech 80.62 92.49 70.04
79.10 91.44 68.34
85.42 96.93 77.78
English 73.91 87.26 33.16
76.10 88.58 36.28
79.35 91.74 43.32
German 64.85 88.05 27.21
65.36 88.63 26.70
72.78 94.54 41.50
Japanese 69.43 82.79 29.27
69.87 83.31 29.69
72.80 87.13 34.96
Spanish 73.49 93.15 39.64
78.18 91.68 33.57
81.96 99.98 59.20
Table 4: Performance of Semantic Role Labeling
(F1 score) with different features.
? group 2: group1+the degrees of the predicate
and its arguments presented in the last section.
Secondly, features extracted from golden-columns
and P-columns are both used for testing.
The performance of them are given in Table 4,
where ?LF1?, ?ULF1? and ?PF1? denote as Labeled
F1 score, Unlabeled F1 score and Proposition F1
score respectively. The above line is the F1 scores of
Semantic Role Labeling with different features. The
uppermost line is the result of group1 features, the
middle line is the result of group2 features extracted
from P-columns, and the downmost one is the result
of group2 features extracted from golden-columns
for each language.
The results show that the features of degree also
improves the labeled F1 measure by 3.4% to 15.8%,
the different labeled F1 between golden-columns
and P-columns is about 2.9%?13.9%.
LAS LF1 M LF1
Catalan 84.18 72.71 81.46
75.68 66.95 71.32
Chinese 72.58 71.49 72.20
63.95 67.06 65.53
Czech 73.18 79.10 76.37
72.60 79.08 75.85
Czech-ood 69.81 79.80 74.81
English 86.88 76.10 82.89
86.61 77.17 81.92
English-ood 80.09 67.21 73.69
German 84.00 65.36 83.06
79.85 61.98 70.93
German-ood 71.86 61.83 66.86
Japanese 92.23 69.87 83.77
91.26 69.58 80.49
Spanish 83.88 71.18 80.74
77.21 66.23 71.72
Table 5: Overall performance of our final joint sys-
tem.
3.4 Overall Performance
In the final system, we select the optimized feature
subset discussed in the former sections. The overall
performance of the system on the development data ,
test data and Out-of-domain data are shown in Table
5 (all features are extracted from P-columns). The
average Macro F1 Scores of our system are 73.97
on test data and 71.79 on Out-of-domain data.
In Table 5, ?LAS?, ?LF1? and ?M LF1? denote
as Labeled accuracy score for Syntactic Dependency
Parsing, Labeled F1 score for Semantic Role Label-
ing, and Overall Macro Labeled F1 score respec-
tively. The topmost line is the result on the devel-
opment data, the middle one is the result on the test
data for each language and the downmost one is the
result on the Out-of-domain data if the data exist.
4 Conclusion and Discussion
We present a joint syntactic and semantic depen-
dency parsing system for CoNLL2009 Shared Task,
which composed of three components: a syntac-
tic dependency parser, a predicate classifier and a
semantic parser. All of them are built with some
state-of-the-art methods. For the predicate classifier
and the semantic parser, a new kind of features?
112
degrees, which reflect the activeness of the words
in a sentence improves their performance. In order
to improve the performance further, we will study
new machine learning methods for semantic depen-
dency parsing, especially the joint learning methods,
which can avoid the information loss problem of our
system.
Acknowledgments
We would like to thank McDonald for providing
the MSTParser program, to Zhang Le for provid-
ing the Maxent program. This research has been
partially supported by the National Natural Science
Foundation of China(No.60703015) and the Na-
tional 863 Program of China (No.2006AA01Z197,
No.2007AA01Z194).
References
Jan Hajic? and Massimiliano Ciaramita and Richard Jo-
hansson and Daisuke Kawahara and Maria Anto`nia
Mart?? and Llu??s Ma`rquez and Adam Meyers and
Joakim Nivre and Sebastian Pado? and Jan S?te?pa?nek
and Pavel Stran?a?k and Miahi Surdeanu and Nianwen
Xue and Yi Zhang. 2009. The CoNLL-2009 Shared
Task: Syntactic and Semantic Dependencies in Multi-
ple Languages. Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5. Boulder, Colorado, USA.
Mariona Taule? and Maria Anto`nia Mart?? and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Cor-
pora for Catalan and Spanish. Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008). Marrakesh, Morroco.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1),pages 143?172.
Jan Hajic? and Jarmila Panevova? and Eva Hajic?ova? and
Petr Sgall and Petr Pajas and Jan S?te?pa?nek and Jir???
Havelka and Marie Mikulova? and Zdene?k Z?abokrtsky?.
2006. Prague Dependency Treebank 2.0. CD-ROM,
Cat. No. LDC2006T01, ISBN 1-58563-370-4. Lin-
guistic Data Consortium, Philadelphia, Pennsylvania,
USA. URL: http://ldc.upenn.edu.
Surdeanu, Mihai and Johansson, Richard and Meyers,
Adam and Ma`rquez, Llu??s and Nivre, Joakim. 2008.
The CoNLL-2008 Shared Task on Joint Parsing of
Syntactic and Semantic Dependencies. Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning(CoNLL-2008).
Aljoscha Burchardt and Katrin Erk and Anette Frank and
Andrea Kowalski and Sebastian Pado? and Manfred
Pinkal. 2006. The SALSA corpus: a German corpus
resource for lexical semantics. Proceedings of the 5rd
International Conference on Language Resources and
Evaluation (LREC-2006), pages 2008?2013. Genoa,
Italy.
Daisuke Kawahara and Sadao Kurohashi and Ko?iti
Hasida. 2002. Construction of a Japanese Relevance-
tagged Corpus. Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013. Las Palmas, Canary
Islands.
McDonald and Ryan. 2006. Discriminative Learning
and Spanning Tree Algorithms for Dependency Pars-
ing, Ph.D. thesis. University of Pennsylvania.
Lu Li, Shixi Fan, Xuan Wang, XiaolongWang. 2008.
Discriminative Learning of Syntactic and Semantic
Dependencies. CoNLL 2008: Proceedings of the
12th Conference on Computational Natural Language
Learning, pages 218?222. Manchester.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,
Bing Qin, Ting Liu, Sheng Li. 2008. A Cascaded
Syntactic and Semantic Dependency Parsing System.
CoNLL 2008: Proceedings of the 12th Conference
on Computational Natural Language Learning, pages
238?242. Manchester.
113
Coling 2010: Poster Volume, pages 1515?1523,
Beijing, August 2010
Active Deep Networks for Semi-Supervised                              
Sentiment Classification 
Shusen Zhou, Qingcai Chen and Xiaolong Wang 
Shenzhen Graduate School, Harbin Institute of Technology 
zhoushusen@hitsz.edu.cn, qincai.chen@hitsz.edu.cn, 
wangxl@insun.hit.edu.cn 
 
 
Abstract 
This paper presents a novel semi-
supervised learning algorithm called Ac-
tive Deep Networks (ADN), to address 
the semi-supervised sentiment classifica-
tion problem with active learning. First, 
we propose the semi-supervised learning 
method of ADN. ADN is constructed by 
Restricted Boltzmann Machines (RBM) 
with unsupervised learning using labeled 
data and abundant of unlabeled data. 
Then the constructed structure is fine-
tuned by gradient-descent based super-
vised learning with an exponential loss 
function. Second, we apply active learn-
ing in the semi-supervised learning 
framework to identify reviews that 
should be labeled as training data. Then 
ADN architecture is trained by the se-
lected labeled data and all unlabeled data. 
Experiments on five sentiment classifica-
tion datasets show that ADN outper-
forms the semi-supervised learning algo-
rithm and deep learning techniques ap-
plied for sentiment classification. 
1 Introduction 
In recent years, sentiment analysis has received 
considerable attentions in Natural Language 
Processing (NLP) community (Blitzer et al, 
2007; Dasgupta and Ng, 2009; Pang et al, 2002). 
Polarity classification, which determine whether 
the sentiment expressed in a document is posi-
tive or negative, is one of the most popular tasks 
of sentiment analysis (Dasgupta and Ng, 2009). 
Sentiment classification is a special type of text 
categorization, where the criterion of classifica-
tion is the attitude expressed in the text, rather 
than the subject or topic. Labeling the reviews 
with their sentiment would provide succinct 
summaries to readers, which makes it possible to 
focus the text mining on areas in need of im-
provement or on areas of success (Gamon, 2004) 
and is helpful in business intelligence applica-
tions, recommender systems, and message filter-
ing (Pang, et al, 2002). 
While topics are often identifiable by key-
words alone, sentiment classification appears to 
be a more challenge task (Pang, et al, 2002). 
First, sentiment is often conveyed with subtle 
linguistic mechanisms such as the use of sar-
casm and highly domain-specific contextual 
cues (Li et al, 2009). For example, although the 
sentence ?The thief tries to protect his excellent 
reputation? contains the word ?excellent?, it tells 
us nothing about the author?s opinion and in fact 
could be well embedded in a negative review. 
Second, sentiment classification systems are typ-
ically domain-specific, which makes the expen-
sive process of annotating a large amount of data 
for each domain and is a bottleneck in building 
high quality systems (Dasgupta and Ng, 2009). 
This motivates the task of learning robust senti-
ment models from minimal supervision (Li, et 
al., 2009).  
Recently, semi-supervised learning, which 
uses large amount of unlabeled data together 
with labeled data to build better learners (Raina 
et al, 2007; Zhu, 2007), has drawn more atten-
tion in sentiment analysis (Dasgupta and Ng, 
2009; Li, et al, 2009). As argued by several re-
searchers (Bengio, 2007; Salakhutdinov and 
Hinton, 2007), deep architecture, composed of 
multiple levels of non-linear operations (Hinton 
et al, 2006), is expected to perform well in 
semi-supervised learning because of its capabili-
ty of modeling hard artificial intelligent tasks. 
Deep Belief Networks (DBN) is a representative 
1515
deep learning algorithm achieving notable suc-
cess for semi-supervised learning (Hinton, et al, 
2006).  Ranzato and Szummer (2008) propose an 
algorithm to learn text document representations 
based on semi-supervised auto-encoders that are 
combined to form a deep network. 
Active learning is another way that can mi-
nimize the number of required labeled data 
while getting competitive result. Usually, the 
training set is chosen randomly. However, active 
learning choose the training data actively, which 
reduce the needs of labeled data (Tong and 
Koller, 2002). Recently, active learning had 
been applied in sentiment classification 
(Dasgupta and Ng, 2009). 
Inspired by the study of semi-supervised 
learning, active learning and deep architecture, 
this paper proposes a novel semi-supervised po-
larity classification algorithm called Active 
Deep Networks (ADN) that is based on a repre-
sentative deep learning algorithm Deep Belief 
Networks (DBN) (Hinton, et al, 2006) and ac-
tive learning (Tong and Koller, 2002). First, we 
propose the ADN architecture, which utilizes a 
new deep architecture for classification, and an 
exponential loss function aiming to maximize 
the separability of the classifier. Second, we 
propose the ADN algorithm. It firstly identifies a 
small number of manually labeled reviews by an 
active learner, and then trains the ADN classifier 
with the identified labeled data and all of the 
unlabeled data.  
Our paper makes several important contribu-
tions: First, this paper proposes a novel ADN 
architecture that integrates the abstraction ability 
of deep belief nets and the classification ability 
of backpropagation strategy. It improves the ge-
neralization capability by using abundant unla-
beled data, and directly optimizes the classifica-
tion results in training dataset using back propa-
gation strategy, which makes it possible to 
achieve attractive classification performance 
with few labeled data. Second, this paper pro-
poses an effective active learning method that 
integrates the labeled data selection ability of 
active learning and classification ability of ADN 
architecture. Moreover, the active learning is 
also based on the ADN architecture, so the la-
beled data selector and the classifier are based 
on the same architecture, which provides an uni-
fied framework for semi-supervised classifica-
tion task. Third, this paper applies semi-
supervised learning and active learning to senti-
ment classification successfully and gets com-
petitive performance. Our experimental results 
on five sentiment classification datasets show 
that ADN outperforms previous sentiment clas-
sification methods and deep learning methods. 
The rest of the paper is organized as follows. 
Section 2 gives an overview of sentiment classi-
fication. The proposed semi-supervised learning 
method ADN is described in Section 3. Section 
4 shows the empirical validation of ADN by 
comparing its classification performance with 
previous sentiment classifiers and deep learning 
methods on sentiment datasets. The paper is 
closed with conclusion. 
2 Sentiment Classification 
Sentiment classification can be performed on 
words, sentences or documents, and is generally 
categorized into lexicon-based and corpus-based 
classification method (Wan, 2009). The detailed 
survey about techniques and approaches of 
sentiment classification can be seen in the book 
(Pang and Lee, 2008). In this paper we focus on 
corpus-based classification method. 
Corpus-based methods use a labeled corpus to 
train a sentiment classifier (Wan, 2009). Pang et 
al. (2002) apply machine learning approach to 
corpus-based sentiment classification firstly. 
They found that standard machine learning tech-
niques outperform human-produced baselines. 
Pang and Lee (2004) apply text-categorization 
techniques to the subjective portions of the sen-
timent document. These portions are extracted 
by efficient techniques for finding minimum cuts 
in graphs. Gamon (2004) demonstrate that using 
large feature vectors in combination with feature 
reduction, high accuracy can be achieved in the 
very noisy domain of customer feedback data. 
Xia et al (2008) propose the sentiment vector 
space model to represent song lyric document, 
assign the sentiment labels such as light-hearted 
and heavy-hearted. 
Supervised sentiment classification systems 
are domain-specific and annotating a large scale 
corpus for each domain is very expensive 
(Dasgupta and Ng, 2009). There are several so-
lutions for this corpus annotation bottleneck.   
The first type of solution is using old domain 
labeled examples to new domain sentiment clas-
1516
sification. Blitzer et al (2007) investigate do-
main adaptation for sentiment classifiers, which 
could be used to select a small set of domains to 
annotate and their trained classifiers would 
transfer well to many other domains. Li and 
Zong (2008) study multi-domain sentiment clas-
sification, which aims to improve performance 
through fusing training data from multiple do-
mains. 
The second type of solution is semi-
supervised sentiment classification. Sindhwani 
and Melville (2008) propose a semi-supervised 
sentiment classification algorithm that utilizes 
lexical prior knowledge in conjunction with un-
labeled data. Dasgupta and Ng (2009) firstly 
mine the unambiguous reviews using spectral 
techniques, and then exploit them to classify the 
ambiguous reviews via a novel combination of 
active learning, transductive learning, and en-
semble learning. 
The third type of solution is unsupervised sen-
timent classification. Zagibalov and Carroll 
(2008) describe an automatic seed word selec-
tion for unsupervised sentiment classification of 
product reviews in Chinese. 
However, unsupervised learning of sentiment 
is difficult, partially because of the prevalence of 
sentimentally ambiguous reviews (Dasgupta and 
Ng, 2009). Using multi-domain sentiment cor-
pus to sentiment classification is also hard to 
apply, because each domain has a very limited 
amount of training data, due to annotating a 
large corpus is difficult and time-consuming (Li 
and Zong, 2008). So in this paper we focus on 
semi-supervised approach to sentiment classifi-
cation. 
3 Active Deep Networks 
In this part, we propose a semi-supervised 
learning algorithm, Active Deep Networks 
(ADN), to address the sentiment classification 
problem with active learning. Section 3.1 
formulates the ADN problem. Section 3.2 
proposes the semi-supervised learning of ADN 
without active learning. Section 3.3 proposes the 
active learning method of ADN. Section 3.4 
gives the ADN procedure. 
3.1 Problem Formulation 
There are many review documents in the dataset. 
We preprocess these reviews to be classified, 
which is similar with Dasgupta and Ng (2009).  
Each review is represented as a vector of uni-
grams, using binary weight equal to 1 for terms 
present in a vector. Moreover, the punctuations, 
numbers, and words of length one are removed 
from the vector. Finally, we sort the vocabulary 
by document frequency and remove the top 
1.5%. It is because that many of these high doc-
ument frequency words are stopwords or domain 
specific general-purpose words. 
After preprocess, every review can be 
represented by a vector. Then the dataset can be 
represented as a matrix: 
? ?
1 2
1 1 1
1 2
1 2 2 2 2
1 2
, , ,
, , ,
, , , 1
, , ,
, , ,
R T
R T
R T
R T
D D D
x x x
x x x
x x x
?
?
?
?
? ?
? ?
? ?? ?? ?? ? ? ?
? ?
? ?? ?
X x x x
?
?
?
? ? ? ?
?
 
where R is the number of training samples, T is 
the number of test samples, D is the number of 
feature words in the dataset. Every column of X 
corresponds to a sample x, which is a representa-
tion of a review. A sample that has all features is 
viewed as a vector in D, where the ith coordi-
nate corresponds to the ith feature. 
The L labeled samples are chosen randomly 
from R training samples, or chosen actively by 
active learning, which can be seen as: 
? ? ? ?1 2, [ , ,..., ] 1 2L R L is s s s R? ? ? ?X X S S 
where S is the index of selected training reviews 
to be labeled manually. 
Let Y be a set of labels corresponds to L la-
beled training samples and is denoted as: 
? ?
1 2
1 1 1
1 2
1 2 2 2 2
1 2
, , ,
, , ,
, , , 3
, , ,
, , ,
L
L
L L
L
C C C
y y y
y y y
y y y
? ?
? ?
? ?? ?? ?? ? ? ?
? ?
? ?? ?
Y y y y
?
?
?
? ? ? ?
?
 
where C is the number of classes. Every column 
of Y is a vector in C, where the jth coordinate 
corresponds to the jth class.  
? ?thth1    if  class 4-1  if  class
i
i
j i
jy j
? ??? ? ???
x
x  
For example, if a review x is positive, y=[1, -
1]?; else, y = [-1, 1]?. 
We intend to seek the mapping function 
L L?X Y  using the L labeled data and R+T-L 
unlabeled data. After training, we can determine 
y by the trained ADN while a new sample x is 
fed. 
1517
3.2 Semi-Supervised Learning 
To address the problem formulated in section 3.1, 
we propose a novel deep architecture for ADN 
method, as show in Figure 1. The deep architec-
ture is a fully interconnected directed belief nets 
with one input layer h0, N hidden layers h1, 
h2, ?, hN, and one label layer at the top. The 
input layer h0 has D units, equal to the number of 
features of sample data x. The label layer has C 
units, equal to number of classes of label vector 
y. The numbers of units for hidden layers, cur-
rently, are pre-defined according to the expe-
rience or intuition. The seeking of the mapping 
function L L?X Y , here, is transformed to the 
problem of finding the parameter space W={w1, 
w2,?,wN} for the deep architecture. 
The semi-supervised learning method based 
on ADN architecture can be divided into two 
stages: First, AND architecture is constructed by 
greedy layer-wise unsupervised learning using 
RBMs as building blocks. All the unlabeled data 
together with L labeled data are utilized to find 
the parameter space W with N layers. Second, 
ADN architecture is trained according to the ex-
ponential loss function using gradient descent 
method. The parameter space W is retrained by 
an exponential loss function using L labeled data.  
 
x
1
x
2
x
D
? ? ? ? ?
? ? ? ?
RBM
h
0
h
1
w
1
? ? ?h
2
RBMw
2
?
 
?
 
?
 
? ?h
N
f(h
N
(x), y)
? ?
y
1
y
2
y
C
labels
Minimize 
Loss
Figure 1. Architecture of Active Deep Networks 
 
For unsupervised learning, we define the 
energy of the state (hk-1, hk) as: 
? ?
? ?
1
1
1
1 1
1 1
1 1
, ;
5
k k
k k
D D
k 1 k k k k
st s t
s t
D D
k k k k
s s t t
s t
E w h h
b h c h
?
?
?
? ?
? ?
? ?
? ?
? ?
? ?
??
? ?
h h
where ? ?cbw ,,??  are the model parameters: 
kstw is the symmetric interaction term between 
unit s in the layer hk-1 and unit t in the layer hk, k 
=1,?, N-1. 
1ksb ? is the s
th bias of layer hk-1 and 
ktc  
is the tth bias of layer hk. Dk is the number of unit 
in the kth layer.  
The probability that the model assigns to hk-1 
is: 
? ? ? ? ? ?? ? ? ?1 11; exp , ; 6kk k kP EZ? ??? ?? ??hh h h
 
? ? ? ?? ? ? ?1 1exp , ; 7k k k kZ E? ?? ?? ???h h h h
 
where ? ??Z  denotes the normalizing constant. 
The conditional distributions over hk and hk-1 are: 
? ? ? ? ? ?1 1| | 8k k k kttp p h? ???h h h
 
? ? ? ? ? ?1 1| | 9k k k kssp p h? ???h h h
 
the probability of turning on unit t is a logistic 
function of the states of hk-1 and 
kstw : 
? ? ? ?1 11| sigm 10k k k k kt t st ssp h c w h? ?? ?? ? ?? ?? ??h
 
the probability of turning on unit s is a logistic 
function of the states of hk and 
kstw : 
 
? ? ? ?1 11| sigm 11k k k k ks s st ttp h b w h? ?? ?? ? ?? ?? ??h
 
where the logistic function is: 
? ? ? ?sigm 1 1 12e ?? ?? ? 
The derivative of the log-likelihood with re-
spect to the model parameter wk can be obtained 
by the CD method (Hinton, 2002): 
? ?
0
1 1 1log ( ) 13
M
k k k k ks t s tP Pst
p h h h hw
? ? ?? ? ??
h
 
where 
0P?
denotes an expectation with respect to 
the data distribution and  
MP?
denotes a distribu-
tion of samples from running the Gibbs sampler 
initialized at the data, for M full steps. 
The above discussion is based on the training 
of the parameters between two hidden layers 
with one sample data x. For unsupervised learn-
ing, we construct the deep architecture using all 
labeled data with unlabeled data by inputting 
them one by one from layer h0, train the parame-
ter between h0 and h1. Then h1 is constructed, we 
1518
can use it to construct the up one layer h2. The 
deep architecture is constructed layer by layer 
from bottom to top, and in each time, the para-
meter space wk is trained by the calculated data 
in the k-1th layer. 
According to the wk calculated above, the 
layer hk can be got as below when a sample x is 
fed from layer h0: 
 
? ?
1
1
1
( ) sigm( )    1, ,
1, , 1 14
kD
k k k k
t t st s k
s
h c w h t D
k N
?
?
?
? ? ?
? ?
?x x ?
?
? ? 
The parameter space wN is initialized random-
ly, just as backpropagation algorithm. Then 
ADN architecture is constructed. The top hidden 
layer is formulated as: 
? ?1 1
1
( ) 1, , 15NDN N N Nt t st s Nsh c w h t D
? ?
?
? ? ??x x ?? ?
 
For supervised learning, the ADN architecture 
is trained by L labeled data. The optimization 
problem is formulized as: 
? ?? ? ? ?harg min f h , 16N N L LX Y
 
where 
? ?? ? ? ?? ? ? ?
1 1
f h , T h 17L CN L L N i ij ji j y? ????X Y x
  
and the loss function is defined as 
? ?T( ) exp( ) 18r r? ?  
In the supervised learning stage, the stochastic 
activities are replaced by deterministic, real va-
lued probabilities. We use gradient-descent 
through the whole deep architecture to retrain 
the weights for optimal classification. 
3.3 Active Learning 
Semi-supervised learning allows us to classify 
reviews with few labeled data. However, anno-
tating the reviews manually is expensive, so we 
want to get higher performance with fewer la-
beled data. Active learning can help to choose 
those reviews that should be labeled manually in 
order to achieving higher classification perfor-
mance with the same number of labeled data. 
For such purpose, we incorporate pool-based 
active learning with the ADN method, which 
accesses to a pool of unlabeled instances and 
requests the labels for some number of them 
(Tong and Koller, 2002). 
Given an unlabeled pool XR and a initial la-
beled data set XL (one positive, one negative), 
the ADN architecture hN  will decide which in-
stance in XR to query next. Then the parameters 
of hN are adjusted after new reviews are labeled 
and inserted into the labeled data set. The main 
issue for an active learner is the choosing of next 
unlabeled instance to query. In this paper, we 
choose the reviews whose labels are most uncer-
tain for the classifier. Following previous work 
on active learning for SVMs (Dasgupta and Ng, 
2009; Tong and Koller, 2002), we define the 
uncertainty of a review as its distance from the 
separating hyperplane. In other words, reviews 
that are near the separating hyperplane are cho-
sen as the labeled training data.  
After semi-supervised learning, the parame-
ters of ADN are adjusted. Given an unlabeled 
pool XR, the next unlabeled instance to be que-
ried are chosen according to the location of 
hN(XR). The distance of a point hN(xi) and the 
classes separation line 
1 2N Nh h? is: 
? ? ? ? ? ?1 2 2 19i N i N ih h? ?d x x 
The selected training reviews to be labeled 
manually are given by:  
? ?? ? ? ?: min 20js j? ?d d 
We can select a group of most uncertainty re-
views to label at each time.  
The experimental setting is similar with 
Dasgupta & Ng (2009). We perform active 
learning for five iterations and select twenty of 
the most uncertainty reviews to be queried each 
time. Then the ADN is re-trained on all of la-
beled and unlabeled reviews so far with semi-
supervised learning. At last, we can decide the 
label of reviews x according to the output hN(x) 
of the ADN architecture as below: 
? ? ? ?? ?
? ? ? ?? ? ? ?
1    if max
21
-1  if max
N N
j
j N N
j
h
y
h
? ??? ? ???
x h x
x h x
 
As shown by Tong and Koller (2002), the Ba-
lanceRandom method, which randomly sample 
an equal number of positive and negative in-
stances from the pool, has much better perfor-
mance than the regular random method. So we 
incorporate this ?Balance? idea with ADN me-
thod. However, to choose equal number of posi-
tive and negative instances without labeling the 
entire pool of instances in advance may not be 
practicable. So we present a simple way to ap-
proximate the balance of positive and negative 
reviews. At first, count the number of positive 
and negative labeled data respectively. Second, 
1519
for each iteration, classify the unlabeled reviews 
in the pool and choose the appropriate number of 
positive and negative reviews to let them equally. 
3.4 ADN Procedure 
The procedure of ADN is shown in Figure 2. For 
the training of ADN architecture, the parameters 
are random initialized with normal distribution. 
All the training data and test data are used to 
train the ADN with unsupervised learning. The 
training set XR can be seen as an unlabeled pool. 
We randomly select one positive and one nega-
tive review in the pool to input as the initial la-
beled training set that are used for supervised 
learning. The number of units in hidden layer 
D1?DN and the number of epochs Q are set ma-
nually based on the dimension of the input data 
and the size of training dataset. The iteration 
times I and the number G of active choosing da-
ta for each iteration can be set manually based 
on the number of labeled data in the experiment. 
For each iteration, the ADN architecture is 
trained by all the unlabeled data and labeled data 
in existence with unsupervised learning and su-
pervised learning firstly. Then we choose G re-
views from the unlabeled pool based on the dis-
tance of these data from the separating line. At 
last, label these data manually and add them to 
the labeled data set. For the next iteration, the 
ADN architecture can be trained on the new la-
beled data set. At last, ADN architecture is re-
trained by all the unlabeled data and existing 
labeled data. After training, the ADN architec-
ture is tested based on Equation (21). 
The proposed ADN method can active choose 
the labeled data set and classify the data with the 
same architecture, which avoid the barrier be-
tween choosing and training with different archi-
tecture. More importantly, the parameters of 
ADN are trained iteratively on the label data se-
lection process, which improve the performance 
of ADN. For the ADN training process: in unsu-
pervised learning stage, the reviews can be ab-
stracted; in supervised learning stage, ADN is 
trained to map the samples belong to different 
classes into different regions. We combine the 
unsupervised and supervised learning, and train 
parameter space of ADN iteratively. The proper 
data that should be labeled are chosen in each 
iteration, which improves the classification per-
formance of ADN. 
 
 
Figure 2. Active Deep Networks Procedure. 
4 Experiments 
4.1 Experimental Setup 
We evaluate the performance of the proposed 
ADN method using five sentiment classification 
datasets. The first dataset is MOV (Pang, et al, 
2002), which is a widely-used movie review da-
taset. The other four dataset contain reviews of 
four different types of products, including books 
(BOO), DVDs (DVD), electronics (ELE), and 
kitchen appliances (KIT) (Blitzer, et al, 2007; 
Dasgupta and Ng, 2009). Each dataset includes 
1,000 positive and 1,000 negative reviews. 
Similar with Dasgupta and Ng (2009), we di-
vide the 2,000 reviews into ten equal-sized folds 
randomly and test all the algorithms with cross-
validation. In each folds, 100 reviews are ran-
dom selected as training data and the remaining 
100 data are used for test. Only the reviews in 
the training data set are used for the selection of 
labeled data by active learning.   
The ADN architecture has different number of 
hidden units for each hidden layer. For greedy 
Active Deep Networks Procedure 
 
Input:  data X 
number of units in every hidden layer D1?DN   
number of epochs Q 
number of training data R 
number of test data T 
number of iterations I 
number of active choose data for every iteration G 
Initialize: W = normally distributed random numbers 
                  XL = one positive and one negative reviews 
 
for i = 1 to I 
Step 1. Greedy layer-wise training hidden layers using RBM 
for  n = 1 to N-1 
for  q = 1 to Q 
    for k = 1 to R+T 
      Calculate the non-linear positive and negative phase 
according to (10) and (11). 
        Update the weights and biases by (13). 
    end for 
end for  
end for  
Step 2. Supervised learning the ADN with gradient descent  
Minimize f(hN(X),Y) on labeled data set XL, update the  
parameter space W according to (16). 
Step 3. Choose instances for labeled data set 
Choose G instances which near the separating line by (20) 
Add  G instances into the labeled data set XL 
end 
Train ADN with Step 1 and Step 2. 
 
Output: ADN  hN(x) 
1520
layer-wise unsupervised learning, we train the 
weights of each layer independently with the 
fixed number of epochs equal to 30 and the 
learning rate is set to 0.1. The initial momentum 
is 0.5 and after 5 epochs, the momentum is set to 
0.9. For supervised learning, we run 10 epochs, 
three times of linear searches are performed in 
each epoch.  
We compare the classification performance of 
ADN with five representative classifiers, i.e., 
Semi-supervised spectral learning (Spectral) 
(Kamvar et al, 2003), Transductive SVM 
(TSVM), Active learning (Active) (Tong and 
Koller, 2002), Mine the Easy Classify the Hard 
(MECH) (Dasgupta and Ng, 2009), and Deep 
Belief Networks (DBN) (Hinton, et al, 2006). 
Spectral learning, TSVM, and Active learning 
method are three baseline methods for sentiment 
classification. MECH is a new semi-supervised 
method for sentiment classification (Dasgupta 
and Ng, 2009). DBN (Hinton, et al, 2006) is the 
classical deep learning method proposed recent-
ly.  
4.2 ADN Performance 
For MOV dataset, the ADN structure used in 
this experiment is 100-100-200-2, which 
represents the number of units in output layer is 
2, in 3 hidden layers are 100, 100, and 200 re-
spectively. For the other four data sets, the ADN 
structure is 50-50-200-2. The number of unit in 
input layer is the same as the dimensions of each 
datasets. All theses parameters are set based on 
the dimension of the input data and the scale of 
the dataset. Because that the number of vocabu-
lary in MOV dataset is more than other four da-
tasets, so the number of units in previous two 
hidden layers for MOV dataset are more than 
other four datasets. We perform active learning 
for 5 iterations. In each iteration, we select and 
label 20 of the most uncertain points, and then 
re-train the ADN on all of the unlabeled data  
and labeled data annotated so far. After 5 itera-
tions, 100 labeled data are used for training. 
The classification accuracies on test data in 
cross validation for five datasets and six me-
thods are shown in Table 1. The results of pre-
vious four methods are reported by Dasgupta 
and Ng (2009). For ADN method, the initial two 
labeled data are selected randomly, so we repeat 
thirty times for each fold and the results are av-
eraged. For the randomness involved in the 
choice of labeled data, all the results of other 
five methods are achieved by repeating ten times 
for each fold and then taking average on results.  
Through Table 1, we can see that the perfor-
mance of DBN is competitive with MECH. 
Since MECH is the combination of spectral clus-
tering, TSVM and Active learning, DBN is just a 
classification method based on deep neural net-
work, this result proves the good learning ability 
of deep architecture. ADN is a combination of 
semi-supervised learning and active learning 
based on deep architecture, the performance of 
ADN is better than all other five methods on five 
datasets. This could be contributed by: First, 
ADN uses a new architecture to guide the output 
vector of samples belonged to different regions 
of new Euclidean space, which can abstract the 
useful information that are not accessible to oth-
er learners; Second, ADN use an exponential 
loss function to maximize the separability of 
labeled data in global refinement for better dis-
criminability; Third, ADN fully exploits the em-
bedding information from the large amount of 
unlabeled data to improve the robustness of the 
classifier; Fourth, ADN can choose the useful 
training data actively, which also improve the 
classification performance. 
 
Type MOV KIT ELE BOO DVD 
Spectral 67.3 63.7 57.7 55.8 56.2 
TSVM 68.7 65.5 62.9 58.7 57.3 
Active 68.9 68.1 63.3 58.6 58.0 
MECH 76.2 74.1 70.6 62.1 62.7 
DBN 71.3 72.6 73.6 64.3 66.7 
ADN 76.3 77.5 76.8 69.0 71.6 
 
Table 1. Test Accuracy with 100 Labeled Data 
for Five Datasets and Six Methods. 
4.3 Effect of Active Learning 
To test the performance of our proposed active 
learning method, we conduct following addi-
tional experiments.  
Passive learning: We random select 100 re-
views from the training fold and use them as 
labeled data. Then the proposed semi-supervised 
1521
learning method of ADN is used to train and test 
the performance. Because of randomness, we 
repeat 30 times for each fold and take average 
on results. The test accuracies of passive learn-
ing for five datasets are shown in Table 2. In 
comparison with ADN method in Table 1, we 
can see that the proposed active learning method 
yields significantly better results than randomly 
chosen points, which proves effectiveness of 
proposed active learning method. 
Fully supervised learning: We train a fully 
supervised classifier using all 1,000 training re-
views based on the ADN architecture, results are 
also shown in Table 2. Comparing with the 
ADN method in Table 1, we can see that em-
ploying only 100 active learning points enables 
us to almost reach fully-supervised performance 
for three datasets. 
 
Type MOV KIT ELE BOO DVD 
Passive 72.2 75.0 75.0 66.0 67.9 
Supervised 77.2 79.4 79.1 69.3 72.1 
 
Table 2. Test Accuracy of ADN with different 
experiment setting for Five Datasets. 
4.4 Semi-Supervised Learning with Va-
riance of Labeled Data 
To verify the performance of semi-supervised 
learning with different number of labeled data, 
we conduct another series of experiments on five 
datasets and show the results on Figure 3. We 
run ten-fold cross validation for each dataset. 
Each fold is repeated ten times and the results 
are averaged. 
We can see that ADN can also get a relative 
high accuracy even by using just 20 labeled re-
views for training. For most of the sentiment 
datasets, the test accuracy is increasing slowly 
while the number of labeled review is growing. 
This proves that ADN reaches good performance 
even with few labeled reviews. 
5 Conclusions  
This paper proposes a novel semi-supervised 
learning algorithm ADN to address the senti-
ment classification problem with a small number 
of  labeled  data.   ADN  can  choose  the  proper  
 
 
20 30 40 50 60 70 80 90 100
60
62
64
66
68
70
72
74
76
78
80
Number of labeled review
T
e
s
t
 
a
c
c
u
r
a
c
y
 
(
%
)
 
 
MOV
KIT
ELE
BOO
DVD
 
Figure 3. Test Accuracy of ADN with Different 
Number of Labeled Reviews for Five Datasets. 
 
training data to be labeled manually, and fully 
exploits the embedding information from the 
large amount of unlabeled data to improve the 
robustness of the classifier. We propose a new 
architecture to guide the output vector of sam-
ples belong to different regions of new Eucli-
dean space, and use an exponential loss function 
to maximize the separability of labeled data in 
global refinement for better discriminability. 
Moreover, ADN can make the right decision 
about which training data should be labeled 
based on existing unlabeled and labeled data. By 
using unsupervised and supervised learning ite-
ratively, ADN can choose the proper training 
data to be labeled and train the deep architecture 
at the same time. Finally, the deep architecture is 
re-trained using the chosen labeled data and all 
the unlabeled data. We also conduct experiments 
to verify the effectiveness of ADN method with 
different number of labeled data, and demon-
strate that ADN can reach very competitive clas-
sification performance just by using few labeled 
data. This results show that the proposed ADN 
method, which only need fewer manual labeled 
reviews to reach a relatively higher accuracy, 
can be used to train a high performance senti-
ment classification system. 
Acknowledgement 
This work is supported in part by the National 
Natural Science Foundation of China (No. 
60703015 and No. 60973076). 
1522
References 
Bengio, Yoshua. 2007. Learning deep architectures 
for AI. Montreal: IRO, Universite de Montreal. 
Blitzer, John, Mark Dredze, and Fernando Pereira. 
2007. Biographies, Bollywood, Boom-boxes and 
Blenders: Domain Adaptation for Sentiment 
Classification. In 45th Annual Meeting of the 
Association of Computational Linguistics.  
Dasgupta, Sajib, and Vincent Ng. 2009. Mine the 
Easy, Classify the Hard: A Semi-Supervised 
Approach to Automatic Sentiment Classification. 
In Joint Conference of the 47th Annual Meeting of 
the Association for Computational Linguistics and 
4th International Joint Conference on Natural 
Language Processing of the Asian Federation of 
Natural Language Processing.  
Gamon, Michael. 2004. Sentiment classification on 
customer feedback data: noisy data, large feature 
vectors, and the role of linguistic analysis. In 
International Conference on Computational 
Linguistics.  
Hinton, Geoffrey E. . 2002. Training products of 
experts by minimizing contrastive divergence. 
Neural Computation, 14(8): 1771-1800. 
Hinton, Geoffrey E. , Simon Osindero, and Yee-
Whye Teh. 2006. A Fast Learning Algorithm for 
Deep Belief Nets. Neural Computation, 18: 1527-
1554. 
Kamvar, Sepandar, Dan Klein, and Christopher 
Manning. 2003. Spectral Learning. In 
International Joint Conferences on Artificial 
Intelligence.  
Li, Shoushan, and Chengqing Zong. 2008. Multi-
domain Sentiment Classification. In 46th Annual 
Meeting of the Association of Computational 
Linguistics.  
Li, Tao, Yi Zhang, and Vikas Sindhwani. 2009. A 
Non-negative Matrix Tri-factorization Approach to 
Sentiment Classification with Lexical Prior 
Knowledge. In Joint Conference of the 47th 
Annual Meeting of the Association for 
Computational Linguistics and 4th International 
Joint Conference on Natural Language Processing 
of the Asian Federation of Natural Language 
Processing.  
Pang, Bo, and Lillian Lee. 2004. A Sentimental 
Education: Sentiment Analysis Using Subjectivity 
Summarization Based on Minimum Cuts. In 42th 
Annual Meeting of the Association of 
Computational Linguistics.  
Pang, Bo, and Lillian Lee. 2008. Opinion mining and 
sentiment analysis (Vol. 2). 
Pang, Bo, Lillian Lee, and Shivakumar 
Vaithyanathan. 2002. Thumbs up? Sentiment 
Classification using Machine Learning Techniques. 
In Conference on Empirical Methods in Natural 
Language Processing.  
Raina, Rajat, Alexis Battle, Honglak Lee, Benjamin 
Packer, and Andrew Y. Ng. 2007. Self-taught 
learning: transfer learning from unlabeled data. In 
International conference on Machine learning.  
Ranzato, Marc'Aurelio, and Martin Szummer. 2008. 
Semi-supervised learning of compact document 
representations with deep networks. In 
International Conference on Machine learning.  
Salakhutdinov, Ruslan, and Geoffrey E. Hinton. 2007. 
Learning a Nonlinear Embedding by Preserving 
Class Neighbourhood Structure. In Proceedings of 
Eleventh International Conference on Artificial 
Intelligence and Statistics.  
Sindhwani, Vikas, and Prem Melville. 2008. 
Document-Word Co-regularization for Semi-
supervised Sentiment Analysis. In International 
Conference on Data Mining.  
Tong, Simon, and Daphne Koller. 2002. Support 
vector machine active learning with applications to 
text classification. Journal of Machine Learning 
Research, 2: 45-66. 
Wan, Xiaojun. 2009. Co-Training for Cross-Lingual 
Sentiment Classification. In Joint Conference of 
the 47th Annual Meeting of the Association for 
Computational Linguistics and 4th International 
Joint Conference on Natural Language Processing 
of the Asian Federation of Natural Language 
Processing.  
Xia, Yunqing, Linlin Wang, Kam-Fai Wong, and 
Mingxing Xu. 2008. Lyric-based Song Sentiment 
Classification with Sentiment Vector Space Model. 
In 46th Annual Meeting of the Association of 
Computational Linguistics.  
Zagibalov, Taras, and John Carroll. 2008. Automatic 
Seed Word Selection for Unsupervised Sentiment 
Classification of Chinese Text. In International 
Conference on Computational Linguistics.  
Zhu, Xiaojin. 2007. Semi-supervised learning 
literature survey. University of Wisconsin 
Madison. 
1523
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1230?1238,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Modeling Semantic Relevance for Question-Answer Pairs
in Web Social Communities
Baoxun Wang, Xiaolong Wang, Chengjie Sun, Bingquan Liu, Lin Sun
School of Computer Science and Technology
Harbin Institute of Technology
Harbin, China
{bxwang, wangxl, cjsun, liubq, lsun}@insun.hit.edu.cn
Abstract
Quantifying the semantic relevance be-
tween questions and their candidate an-
swers is essential to answer detection in
social media corpora. In this paper, a deep
belief network is proposed to model the
semantic relevance for question-answer
pairs. Observing the textual similarity
between the community-driven question-
answering (cQA) dataset and the forum
dataset, we present a novel learning strat-
egy to promote the performance of our
method on the social community datasets
without hand-annotating work. The ex-
perimental results show that our method
outperforms the traditional approaches on
both the cQA and the forum corpora.
1 Introduction
In natural language processing (NLP) and infor-
mation retrieval (IR) fields, question answering
(QA) problem has attracted much attention over
the past few years. Nevertheless, most of the QA
researches mainly focus on locating the exact an-
swer to a given factoid question in the related doc-
uments. The most well known international evalu-
ation on the factoid QA task is the Text REtrieval
Conference (TREC)1, and the annotated questions
and answers released by TREC have become im-
portant resources for the researchers. However,
when facing a non-factoid question such as why,
how, or what about, however, almost no automatic
QA systems work very well.
The user-generated question-answer pairs are
definitely of great importance to solve the non-
factoid questions. Obviously, these natural QA
pairs are usually created during people?s com-
munication via Internet social media, among
which we are interested in the community-driven
1http://trec.nist.gov
question-answering (cQA) sites and online fo-
rums. The cQA sites (or systems) provide plat-
forms where users can either ask questions or de-
liver answers, and best answers are selected man-
ually (e.g., Baidu Zhidao2 and Yahoo! Answers3).
Comparing with cQA sites, online forums have
more virtual society characteristics, where people
hold discussions in certain domains, such as tech-
niques, travel, sports, etc. Online forums contain
a huge number of QA pairs, and much noise infor-
mation is involved.
To make use of the QA pairs in cQA sites and
online forums, one has to face the challenging
problem of distinguishing the questions and their
answers from the noise. According to our investi-
gation, the data in the community based sites, es-
pecially for the forums, have two obvious charac-
teristics: (a) a post usually includes a very short
content, and when a person is initializing or re-
plying a post, an informal tone tends to be used;
(b) most of the posts are useless, which makes
the community become a noisy environment for
question-answer detection.
In this paper, a novel approach for modeling the
semantic relevance for QA pairs in the social me-
dia sites is proposed. We concentrate on the fol-
lowing two problems:
1. How to model the semantic relationship be-
tween two short texts using simple textual fea-
tures? As mentioned above, the user generated
questions and their answers via social media are
always short texts. The limitation of length leads
to the sparsity of the word features. In addition,
the word frequency is usually either 0 or 1, that is,
the frequency offers little information except the
occurrence of a word. Because of this situation,
the traditional relevance computing methods based
on word co-occurrence, such as Cosine similarity
and KL-divergence, are not effective for question-
2http://zhidao.baidu.com
3http://answers.yahoo.com
1230
answer semantic modeling. Most researchers try
to introduce structural features or users? behavior
to improve the models performance, by contrast,
the effect of textual features is not obvious.
2. How to train a model so that it has good per-
formance on both cQA and forum datasets? So
far, people have been doing QA researches on the
cQA and the forum datasets separately (Ding et
al., 2008; Surdeanu et al, 2008), and no one has
noticed the relationship between the two kinds of
data. Since both the cQA systems and the online
forums are open platforms for people to commu-
nicate, the QA pairs in the cQA systems have sim-
ilarity with those in the forums. In this case, it is
highly valuable and desirable to propose a train-
ing strategy to improve the model?s performance
on both of the two kinds of datasets. In addition,
it is possible to avoid the expensive and arduous
hand-annotating work by introducing the method.
To solve the first problem, we present a deep
belief network (DBN) to model the semantic rel-
evance between questions and their answers. The
network establishes the semantic relationship for
QA pairs by minimizing the answer-to-question
reconstructing error. Using only word features,
our model outperforms the traditional methods on
question-answer relevance calculating.
For the second problem, we make our model
to learn the semantic knowledge from the solved
question threads in the cQA system. Instead of
mining the structure based features from cQA
pages and forum threads individually, we con-
sider the textual similarity between the two kinds
of data. The semantic information learned from
cQA corpus is helpful to detect answers in forums,
which makes our model show good performance
on social media corpora. Thanks to the labels for
the best answers existing in the threads, no manual
work is needed in our strategy.
The rest of this paper is organized as follows:
Section 2 surveys the related work. Section 3 in-
troduces the deep belief network for answer de-
tection. In Section 4, the homogenous data based
learning strategy is described. Experimental result
is given in Section 5. Finally, conclusions and fu-
ture directions are drawn in Section 6.
2 Related Work
The value of the naturally generated question-
answer pairs has not been recognized until recent
years. Early studies mainly focus on extracting
QA pairs from frequently asked questions (FAQ)
pages (Jijkoun and de Rijke, 2005; Riezler et al,
2007) or service call-center dialogues (Berger et
al., 2000).
Judging whether a candidate answer is seman-
tically related to the question in the cQA page
automatically is a challenging task. A frame-
work for predicting the quality of answers has
been presented in (Jeon et al, 2006). Bernhard
and Gurevych (2009) have developed a transla-
tion based method to find answers. Surdeanu et
al. (2008) propose an approach to rank the an-
swers retrieved by Yahoo! Answers. Our work is
partly similar to Surdeanu et al (2008), for we also
aim to rank the candidate answers reasonably, but
our ranking algorithm needs only word informa-
tion, instead of the combination of different kinds
of features.
Because people have considerable freedom to
post on forums, there are a great number of irrel-
evant posts for answering questions, which makes
it more difficult to detect answers in the forums.
In this field, exploratory studies have been done by
Feng et al (2006) and Huang et al (2007), who ex-
tract input-reply pairs for the discussion-bot. Ding
et al(2008) and Cong et al(2008) have also pre-
sented outstanding research works on forum QA
extraction. Ding et al (2008) detect question con-
texts and answers using the conditional random
fields, and a ranking algorithm based on the au-
thority of forum users is proposed by Cong et al
(2008). Treating answer detection as a binary clas-
sification problem is an intuitive idea, thus there
are some studies trying to solve it from this view
(Hong and Davison, 2009; Wang et al, 2009). Es-
pecially Hong and Davison (2009) have achieved
a rather high precision on the corpora with less
noise, which also shows the importance of ?social?
features.
In order to select the answers for a given ques-
tion, one has to face the problem of lexical gap.
One of the problems with lexical gap embedding
is to find similar questions in QA achieves (Jeon et
al., 2005). Recently, the statistical machine trans-
lation (SMT) strategy has become popular. Lee et
al. (2008) use translate models to bridge the lexi-
cal gap between queries and questions in QA col-
lections. The SMT based methods are effective on
modeling the semantic relationship between ques-
tions and answers and expending users? queries in
answer retrieval (Riezler et al, 2007; Berger et al,
1231
2000; Bernhard and Gurevych, 2009). In (Sur-
deanu et al, 2008), the translation model is used
to provide features for answer ranking.
The structural features (e.g., authorship, ac-
knowledgement, post position, etc), also called
non-textual features, play an important role in an-
swer extraction. Such features are used in (Ding
et al, 2008; Cong et al, 2008), and have signifi-
cantly improved the performance. The studies of
Jeon et al (2006) and Hong et al (2009) show that
the structural features have even more contribution
than the textual features. In this case, the mining
of textual features tends to be ignored.
There are also some other research topics in this
field. Cong et al (2008) and Wang et al (2009)
both propose the strategies to detect questions in
the social media corpus, which is proved to be a
non-trivial task. The deep research on question
detection has been taken by Duan et al (2008).
A graph based algorithm is presented to answer
opinion questions (Li et al, 2009). In email sum-
marization field, the QA pairs are also extracted
from email contents as the main elements of email
summarization (Shrestha and McKeown, 2004).
3 The Deep Belief Network for QA pairs
Due to the feature sparsity and the low word fre-
quency of the social media corpus, it is difficult
to model the semantic relevance between ques-
tions and answers using only co-occurrence fea-
tures. It is clear that the semantic link exists be-
tween the question and its answers, even though
they have totally different lexical representations.
Thus a specially designed model may learn se-
mantic knowledge by reconstructing a great num-
ber of questions using the information in the cor-
responding answers. In this section, we propose
a deep belief network for modeling the seman-
tic relationship between questions and their an-
swers. Our model is able to map the QA data into
a low-dimensional semantic-feature space, where
a question is close to its answers.
3.1 The Restricted Boltzmann Machine
An ensemble of binary vectors can be modeled us-
ing a two-layer network called a ?restricted Boltz-
mann machine? (RBM) (Hinton, 2002). The di-
mension reducing approach based on RBM ini-
tially shows good performance on image process-
ing (Hinton and Salakhutdinov, 2006). Salakhut-
dinov and Hinton (2009) propose a deep graphical
model composed of RBMs into the information re-
trieval field, which shows that this model is able to
obtain semantic information hidden in the word-
count vectors.
As shown in Figure 1, the RBM is a two-layer
network. The bottom layer represents a visible
vector v and the top layer represents a latent fea-
ture h. The matrix W contains the symmetric in-
teraction terms between the visible units and the
hidden units. Given an input vector v, the trained
Figure 1: Restricted Boltzmann machine
RBM model provides a hidden feature h, which
can be used to reconstruct v with a minimum er-
ror. The training algorithm for this paper will be
described in the next subsection. The ability of the
RBM suggests us to build a deep belief network
based on RBM so that the semantic relevance be-
tween questions and answers can be modeled.
3.2 Pretraining a Deep Belief Network
In the social media corpora, the answers are al-
ways descriptive, containing one or several sen-
tences. Noticing that an answer has strong seman-
tic association with the question and involves more
information than the question, we propose to train
a deep belief network by reconstructing the ques-
tion using its answers. The training object is to
minimize the error of reconstruction, and after the
pretraining process, a point that lies in a good re-
gion of parameter space can be achieved.
Firstly, the illustration of the DBN model is
given in Figure 2. This model is composed of
three layers, and here each layer stands for the
RBM or its variant. The bottom layer is a variant
form of RBM?s designed for the QA pairs. This
layer we design is a little different from the classi-
cal RBM?s, so that the bottom layer can generate
the hidden features according to the visible answer
vector and reconstruct the question vector using
the hidden features. The pre-training procedure of
this architecture is practically convergent. In the
bottom layer, the binary feature vectors based on
the statistics of the word occurrence in the answers
are used to compute the ?hidden features? in the
1232
Figure 2: The Deep Belief Network for QA Pairs
hidden units. The model can reconstruct the ques-
tions using the hidden features. The processes can
be modeled as follows:
p(h j = 1|a) = ?(b j +
?
i
wi jai) (1)
p(qi = 1|h) = ?(bi +
?
j
wi jh j) (2)
where ?(x) = 1/(1 + e?x), a denotes the visible
feature vector of the answer, qi is the ith element
of the question vector, and h stands for the hid-
den feature vector for reconstructing the questions.
wi j is a symmetric interaction term between word
i and hidden feature j, bi stands for the bias of the
model for word i, and b j denotes the bias of hidden
feature j.
Given the training set of answer vectors, the bot-
tom layer generates the corresponding hidden fea-
tures using Equation 1. Equation 2 is used to re-
construct the Bernoulli rates for each word in the
question vectors after stochastically activating the
hidden features. Then Equation 1 is taken again
to make the hidden features active. We use 1-step
Contrastive Divergence (Hinton, 2002) to update
the parameters by performing gradient ascent:
?wi j = (< qih j >qData ? < qih j >qRecon) (3)
where < qih j >qData denotes the expectation of
the frequency with which the word i in a ques-
tion and the feature j are on together when the
hidden features are driven by the question data.
< qih j >qRecon defines the corresponding expec-
tation when the hidden features are driven by the
reconstructed question data.  is the learning rate.
The classical RBM structure is taken to build
the middle layer and the top layer of the network.
The training method for the higher two layer is
similar to that of the bottom one, and we only have
to make each RBM to reconstruct the input data
using its hidden features. The parameter updates
still obeying the rule defined by gradient ascent,
which is quite similar to Equation 3. After train-
ing one layer, the h vectors are then sent to the
higher-level layer as its ?training data?.
3.3 Fine-tuning the Weights
Notice that a greedy strategy is taken to train each
layer individually during the pre-training proce-
dure, it is necessary to fine-tune the weights of the
entire network for optimal reconstruction. To fine-
tune the weights, the network is unrolled, taking
the answers as the input data to generate the corre-
sponding questions at the output units. Using the
cross-entropy error function, we can then tune the
network by performing backpropagation through
it. The experiment results in section 5.2 will show
fine-tuning makes the network performs better for
answer detection.
3.4 Best answer detection
After pre-training and fine-tuning, a deep belief
network for QA pairs is established. To detect the
best answer to a given question, we just have to
send the vectors of the question and its candidate
answers into the input units of the network and
perform a level-by-level calculation to obtain the
corresponding feature vectors. Then we calculate
the distance between the mapped question vector
and each candidate answer vector. We consider the
candidate answer with the smallest distance as the
best one.
4 Learning with Homogenous Data
In this section, we propose our strategy to make
our DBN model to detect answers in both cQA and
forum datasets, while the existing studies focus on
one single dataset.
4.1 Homogenous QA Corpora from Different
Sources
Our motivation of finding the homogenous
question-answer corpora from different kind of so-
cial media is to guarantee the model?s performance
and avoid hand-annotating work.
In this paper, we get the ?solved question? pages
in the computer technology domain from Baidu
Zhidao as the cQA corpus, and the threads of
1233
Figure 3: Comparison of the post content lengths in the cQA and the forum datasets
ComputerFansClub Forum4 as the online forum
corpus. The domains of the corpora are the same.
To further explain that the two corpora are ho-
mogenous, we will give the detail comparison on
text style and word distribution.
As shown in Figure 3, we have compared the
post content lengths of the cQA and the forum
in our corpora. For the comparison, 5,000 posts
from the cQA corpus and 5,000 posts from the fo-
rum corpus are randomly selected. The left panel
shows the statistical result on the Baidu Zhidao
data, and the right panel shows the one on the fo-
rum data. The number i on the horizontal axis de-
notes the post contents whose lengths range from
10(i? 1) + 1 to 10i bytes, and the vertical axis rep-
resents the counts of the post contents. From Fig-
ure 3 we observe that the contents of most posts
in both the cQA corpus and the forum corpus are
short, with the lengths not exceeding 400 bytes.
The content length reflects the text style of the
posts in cQA systems and online forums. From
Figure 3 it can be also seen that the distributions
of the content lengths in the two figures are very
similar. It shows that the contents in the two cor-
pora are both mainly short texts.
Figure 4 shows the percentage of the concurrent
words in the top-ranked content words with high
frequency. In detail, we firstly rank the words by
frequency in the two corpora. The words are cho-
sen based on a professional dictionary to guarantee
that they are meaningful in the computer knowl-
edge field. The number k on the horizontal axis in
Figure 4 represents the top k content words in the
4http://bbs.cfanclub.net/
corpora, and the vertical axis stands for the per-
centage of the words shared by the two corpora in
the top k words.
Figure 4: Distribution of concurrent content words
Figure 4 shows that a large number of meaning-
ful words appear in both of the two corpora with
high frequencies. The percentage of the concur-
rent words maintains above 64% in the top 1,400
words. It indicates that the word distributions of
the two corpora are quite similar, although they
come from different social media sites.
Because the cQA corpus and the forum corpus
used in this study have homogenous characteris-
tics for answer detecting task, a simple strategy
may be used to avoid the hand-annotating work.
Apparently, in every ?solved question? page of
Baidu Zhidao, the best answer is selected by the
user who asks this question. We can easily extract
the QA pairs from the cQA corpus as the training
1234
set. Because the two corpora are similar, we can
apply the deep belief network trained by the cQA
corpus to detect answers on both the cQA data and
the forum data.
4.2 Features
The task of detecting answers in social media cor-
pora suffers from the problem of feature sparsity
seriously. High-dimensional feature vectors with
only several non-zero dimensions bring large time
consumption to our model. Thus it is necessary to
reduce the dimension of the feature vectors.
In this paper, we adopt two kinds of word fea-
tures. Firstly, we consider the 1,300 most fre-
quent words in the training set as Salakhutdinov
and Hinton (2009) did. According to our statis-
tics, the frequencies of the rest words are all less
then 10, which are not statistically significant and
may introduce much noise.
We take the occurrence of some function words
as another kind of features. The function words
are quite meaningful for judging whether a short
text is an answer or not, especially for the non-
factoid questions. For example, in the answers to
the causation questions, the words such as because
and so are more likely to appear; and the words
such as firstly, then, and should may suggest the
answers to the manner questions. We give an ex-
ample for function word selection in Figure 5.
Figure 5: An example for function word selection
For this reason, we collect 200 most frequent
function words in the answers of the training set.
Then for every short text, either a question or an
answer, a 1,500-dimensional vector can be gener-
ated. Specifically, all the features we have adopted
are binary, for they only have to denote whether
the corresponding word appears in the text or not.
5 Experiments
To evaluate our question-answer semantic rele-
vance computing method, we compare our ap-
proach with the popular methods on the answer
detecting task.
5.1 Experiment Setup
Architecture of the Network: To build the deep
belief network, we use a 1500-1500-1000-600 ar-
chitecture, which means the three layers of the net-
work have individually 1,500?1,500, 1,500?1,000
and 1,000?600 units. Using the network, a 1,500-
dimensional binary vector is finally mapped to a
600-dimensional real-value vector.
During the pretraining stage, the bottom layer
is greedily pretrained for 200 passes through the
entire training set, and each of the rest two layers is
greedily pretrained for 50 passes. For fine-tuning
we apply the method of conjugate gradients5, with
three line searches performed in each pass. This
algorithm is performed for 50 passes to fine-tune
the network.
Dataset: we have crawled 20,000 pages of
?solved question? from the computer and network
category of Baidu Zhidao as the cQA corpus. Cor-
respondingly we obtain 90,000 threads from Com-
puterFansClub, which is an online forum on com-
puter knowledge. We take the forum threads as
our forum corpus.
From the cQA corpus, we extract 12,600 human
generated QA pairs as the training set without any
manual work to label the best answers. We get the
contents from another 2,000 cQA pages to form
a testing set, each content of which includes one
question and 4.5 candidate answers on average,
with one best answer among them. To get another
testing dataset, we randomly select 2,000 threads
from the forum corpus. For this training set, hu-
man work are necessary to label the best answers
in the posts of the threads. There are 7 posts in-
cluded in each thread on average, among which
one question and at least one answer exist.
Baseline: To show the performance of our
method, three main popular relevance computing
methods for ranking candidate answers are con-
sidered as our baselines. We will briefly introduce
them:
Cosine Similarity. Given a question q and its
candidate answer a, their cosine similarity can be
computed as follows:
cos(q, a) =
?n
k=1 wqk ? wak??n
k=1 w2qk ?
??n
k=1 w2ak
(4)
where wqk and wak stand for the weight of the kth
word in the question and the answer respectively.
5Code is available at
http://www.kyb.tuebingen.mpg.de/bs/people/carl/code/minimize/
1235
The weights can be get by computing the product
of term frequency (tf ) and inverse document fre-
quency (idf )
HowNet based Similarity. HowNet6 is an elec-
tronic world knowledge system, which serves as
a powerful tool for meaning computation in hu-
man language technology. Normally the similar-
ity between two passages can be calculated by
two steps: (1) matching the most semantic-similar
words in each passages greedily using the API?s
provided by HowNet; (2) computing the weighted
average similarities of the word pairs. This strat-
egy is taken as a baseline method for computing
the relevance between questions and answers.
KL-divergence Language Model. Given a ques-
tion q and its candidate answer a, we can con-
struct unigram language model Mq and unigram
language model Ma. Then we compute KL-
divergence between Mq and Ma as below:
KL(Ma||Mq) =
?
w
p(w|Ma) log(p(w|Ma)/p(w|Mq))
(5)
5.2 Results and Analysis
We evaluate the performance of our approach for
answer detection using two metrics: Precision@1
(P@1) and Mean Reciprocal Rank (MRR). Ap-
plying the two metrics, we perform the baseline
methods and our DBN based methods on the two
testing set above.
Table 1 lists the results achieved on the forum
data using the baseline methods and ours. The ad-
ditional ?Nearest Answer? stands for the method
without any ranking strategies, which returns the
nearest candidate answer from the question by po-
sition. To illustrate the effect of the fine-tuning for
our model, we list the results of our method with-
out fine-tuning and the results with fine-tuning.
As shown in Table 1, our deep belief network
based methods outperform the baseline methods
as expected. The main reason for the improve-
ments is that the DBN based approach is able to
learn semantic relationship between the words in
QA pairs from the training set. Although the train-
ing set we offer to the network comes from a dif-
ferent source (the cQA corpus), it still provide
enough knowledge to the network to perform bet-
ter than the baseline methods. This phenomena in-
dicates that the homogenous corpora for training is
6Detail information can be found in:
http://www.keenage.com/
effective and meaningful.
Method P@1 (%) MRR (%)
Nearest Answer 21.25 38.72
Cosine Similarity 23.15 43.50
HowNet 22.55 41.63
KL divergence 25.30 51.40
DBN (without FT) 41.45 59.64
DBN (with FT) 45.00 62.03
Table 1: Results on Forum Dataset
We have also investigated the reasons for the un-
satisfying performance of the baseline approaches.
Basically, the low precision is ascribable to the
forum corpus we have obtained. As mentioned
in Section 1, the contents of the forum posts are
short, which leads to the sparsity of the features.
Besides, when users post messages in the online
forums, they are accustomed to be casual and use
some synonymous words interchangeably in the
posts, which is believed to be a significant situ-
ation in Chinese forums especially. Because the
features for QA pairs are quite sparse and the con-
tent words in the questions are usually morpholog-
ically different from the ones with the same mean-
ing in the answers, the Cosine Similarity method
become less powerful. For HowNet based ap-
proaches, there are a large number of words not
included by HowNet, thus it fails to compute the
similarity between questions and answers. KL-
divergence suffers from the same problems with
the Cosine Similarity method. Compared with
the Cosine Similarity method, this approach has
achieved the improvement of 9.3% in P@1, but
it performs much better than the other baseline
methods in MRR.
The baseline results indicate that the online fo-
rum is a complex environment with large amount
of noise for answer detection. Traditional IR
methods using pure textual features can hardly
achieve good results. The similar baseline results
for forum answer ranking are also achieved by
Hong and Davison (2009), which takes some non-
textual features to improve the algorithm?s perfor-
mance. We also notice that, however, the baseline
methods have obtained better results on forum cor-
pus (Cong et al, 2008). One possible reason is that
the baseline approaches are suitable for their data,
since we observe that the ?nearest answer? strat-
egy has obtained a 73.5% precision in their work.
Our model has achieved the precision of
1236
45.00% in P@1 and 62.03% in MRR for answer
detecting on forum data after fine-tuning, while
some related works have reported the results with
the precision over 90% (Cong et al, 2008; Hong
and Davison, 2009). There are mainly two rea-
sons for this phenomena: Firstly, both of the pre-
vious works have adopt non-textual features based
on the forum structure, such as authorship, po-
sition and quotes, etc. The non-textual (or so-
cial based) features have played a significant role
in improving the algorithms? performance. Sec-
ondly, the quality of corpora influences the results
of the ranking strategies significantly, and even
the same algorithm may perform differently when
the dataset is changed (Hong and Davison, 2009).
For the experiments of this paper, large amount of
noise is involved in the forum corpus and we have
done nothing extra to filter it.
Table 2 shows the experimental results on the
cQA dataset. In this experiment, each sample is
composed of one question and its following sev-
eral candidate answers. We delete the ones with
only one answer to confirm there are at least two
candidate answers for each question. The candi-
date answers are rearranged by post time, so that
the real answers do not always appear next to the
questions. In this group of experiment, no hand-
annotating work is needed because the real an-
swers have been labeled by cQA users.
Method P@1 (%) MRR (%)
Nearest Answer 36.05 56.33
Cosine Similarity 44.05 62.84
HowNet 41.10 58.75
KL divergence 43.75 63.10
DBN (without FT) 56.20 70.56
DBN (with FT) 58.15 72.74
Table 2: Results on cQA Dataset
From Table 2 we observe that all the approaches
perform much better on this dataset. We attribute
the improvements to the high quality QA corpus
Baidu Zhidao offers: the candidate answers tend to
be more formal than the ones in the forums, with
less noise information included. In addition, the
?Nearest Answer? strategy has reached 36.05% in
P@1 on this dataset, which indicates quite a num-
ber of askers receive the real answers at the first
answer post. This result has supported the idea of
introducing position features. What?s more, if the
best answer appear immediately, the asker tends
to lock down the question thread, which helps to
reduce the noise information in the cQA corpus.
Despite the baseline methods? performances
have been improved, our approaches still outper-
form them, with a 32.0% improvement in P@1
and a 15.3% improvement in MRR at least. On
the cQA dataset, our model shows better perfor-
mance than the previous experiment, which is ex-
pected because the training set and the testing set
come from the same corpus, and the DBN model
is more adaptive to the cQA data.
We have observed that, from both of the two
groups of experiments, fine-tuning is effective for
enhancing the performance of our model. On the
forum data, the results have been improved by
8.6% in P@1 and 4.0% in MRR, and the improve-
ments are 3.5% and 3.1% individually.
6 Conclusions
In this paper, we have proposed a deep belief net-
work based approach to model the semantic rel-
evance for the question answering pairs in social
community corpora.
The contributions of this paper can be summa-
rized as follows: (1) The deep belief network we
present shows good performance on modeling the
QA pairs? semantic relevance using only word fea-
tures. As a data driven approach, our model learns
semantic knowledge from large amount of QA
pairs to represent the semantic relevance between
questions and their answers. (2) We have stud-
ied the textual similarity between the cQA and the
forum datasets for QA pair extraction, and intro-
duce a novel learning strategy to make our method
show good performance on both cQA and forum
datasets. The experimental results show that our
method outperforms the traditional approaches on
both the cQA and the forum corpora.
Our future work will be carried out along two
directions. Firstly, we will further improve the
performance of our method by adopting the non-
textual features. Secondly, more research will be
taken to put forward other architectures of the deep
networks for QA detection.
Acknowledgments
The authors are grateful to the anonymous re-
viewers for their constructive comments. Special
thanks to Deyuan Zhang, Bin Liu, Beidong Liu
and Ke Sun for insightful suggestions. This work
is supported by NSFC (60973076).
1237
References
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the lexi-
cal chasm: Statistical approaches to answer-finding.
In In Proceedings of the 23rd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 192?199.
Delphine Bernhard and Iryna Gurevych. 2009. Com-
bining lexical semantic resources with question &
answer archives for translation-based answer find-
ing. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 728?736, Suntec,
Singapore, August. Association for Computational
Linguistics.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
pairs from online forums. In SIGIR ?08: Proceed-
ings of the 31st annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 467?474, New York, NY,
USA. ACM.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random fields to ex-
tract contexts and answers of questions from online
forums. In Proceedings of ACL-08: HLT, pages
710?718, Columbus, Ohio, June. Association for
Computational Linguistics.
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong
Yu. 2008. Searching questions by identifying ques-
tion topic and question focus. In Proceedings of
ACL-08: HLT, pages 156?164, Columbus, Ohio,
June. Association for Computational Linguistics.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard H.
Hovy. 2006. An intelligent discussion-bot for an-
swering student queries in threaded discussions. In
Ccile Paris and Candace L. Sidner, editors, IUI,
pages 171?177. ACM.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
Georey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14.
Liangjie Hong and Brian D. Davison. 2009. A
classification-based approach to question answering
in discussion boards. In SIGIR ?09: Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 171?178, New York, NY, USA. ACM.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbot knowledge from online discussion
forums. In IJCAI?07: Proceedings of the 20th in-
ternational joint conference on Artifical intelligence,
pages 423?428, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM ?05, pages 84?90, New
York, NY, USA. ACM.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A framework to predict the quality of
answers with non-textual features. In SIGIR ?06,
pages 228?235, New York, NY, USA. ACM.
Valentin Jijkoun and Maarten de Rijke. 2005. Retriev-
ing answers from frequently asked questions pages
on the web. In CIKM ?05, pages 76?83, New York,
NY, USA. ACM.
Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and
Hae-Chang Rim. 2008. Bridging lexical gaps be-
tween queries and questions on large online q&a
collections with compact translation models. In
EMNLP ?08: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 410?418, Morristown, NJ, USA. Association
for Computational Linguistics.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with
random walks on graphs. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
737?745, Suntec, Singapore, August. Association
for Computational Linguistics.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In Proceedings of the 45th
Annual Meeting of the Association of Computa-
tional Linguistics, pages 464?471, Prague, Czech
Republic, June. Association for Computational
Linguistics.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Semantic hashing. Int. J. Approx. Reasoning,
50(7):969?978.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. In Proceedings of Coling 2004, pages 889?
895, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large
online QA collections. In Proceedings of ACL-08:
HLT, pages 719?727, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Baoxun Wang, Bingquan Liu, Chengjie Sun, Xiao-
long Wang, and Lin Sun. 2009. Extracting chinese
question-answer pairs from online forums. In SMC
2009: Proceedings of the IEEE International Con-
ference on Systems, Man and Cybernetics, 2009.,
pages 1159?1164.
1238
Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 1?8
Manchester, August 2008
Semantic Chunk Annotation for complex questions using Conditional 
Random Field 
Shixi Fan 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
fanshixi@hit.edu.cn 
Yaoyun Zhang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
Xiaoni5122@gmail.com 
Wing W. Y. Ng 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wing@hitsz.edu.cn 
Xuan Wang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wangxuan@insun.hit.edu.cn 
Xiaolong Wang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wangxl@insun.hit.edu.cn 
 
 
Abstract 
This paper presents a CRF (Conditional 
Random Field) model for Semantic 
Chunk Annotation in a Chinese Question 
and Answering System (SCACQA). The 
model was derived from a corpus of real 
world questions, which are collected 
from some discussion groups on the 
Internet. The questions are supposed to 
be answered by other people, so some of 
the questions are very complex. Mutual 
information was adopted for feature se-
lection.  The training data collection con-
sists of 14000 sentences and the testing 
data collection consists of 4000 sentences. 
The result shows an F-score of 93.07%. 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
1 Introduction 
1.1 Introduction of Q&A System 
Automated question answering has been a hot 
topic of research and development since the ear-
liest AI applications (A.M. Turing, 1950). Since 
then there has been a continual interest in proc-
essing knowledge and retrieving it efficiently to 
users automatically. The end of the 1980s saw a 
boost in information retrieval technologies and 
applications, with an unprecedented growth in 
the amount of digital information available, an 
explosion of growth in the use of computers for 
communications, and the increasing number of 
users that have access to all this information 
(Diego Moll?and Jose?Luis Vicedo, 2007).  
Search engines such as Google, Yahoo, Baidu 
and etc have made a great success for people?s 
information need. 
Anyhow, search engines are keywords-based 
which can only return links of relevant web 
pages, failing to provide a friendly user-interface 
with queries expressed in natural language sen-
tences or questions, or to return precise answers 
to users. Especially from the end of the 1990s, as 
1
information retrieval technologies and method-
ologies became mature and grew more slowly in 
pace, automated question answering(Q&A) sys-
tems which accept questions in free natural lan-
guage formations and return exactly the answer 
or a short paragraph containing relevant informa-
tion has become an urgent necessity. Major in-
ternational evaluations such as TREC, CLEF and 
NTCIR have attracted the participation of many 
powerful systems.  
The architecture of a Q&A system generally in-
cludes three modules: question processing, can-
didate answer/document retrieval, and answer 
extraction and re-ranking.      
1.2 Introduction of Question Analyzing      
Question Analyzing, as the premise and founda-
tion of the latter two modules, is of paramount 
importance to the integrated performance of a 
Q&A system. The reason is quite intuitive: a 
question contains all the information to retrieve 
the corresponding answer. Misinterpretation or 
too much loss of information during the process-
ing will inevitably lead to poor precision of the 
system. 
The early research efforts and evaluations in 
Q&A were focused mainly on factoid questions 
asking for named entities, such as time, numbers, 
and locations and so on. The questions in the test 
corpus of TREC and other organizations are also 
in short and simple form. Complex hierarchy in 
question types (Dragomir Radev et al 2001), 
question templates (Min-Yuh Day et al 2005), 
question parsing (Ulf Hermjakob, 2001) and 
various machine learning methods (Dell Zhang 
and Wee Sun Lee, 2003)are used for factoid 
question analysis, aiming to find what named 
entity is asked in the question. There are some 
questions which are very complicated or even 
need domain restricted knowledge and reasoning 
technique. Automatic Q&A system can not deal 
with such questions with current technique.    
In china, there is a new kind of web based Q&A 
system which is a special kind of discussion 
group. Unlike common discussion group, in the 
web based Q&A system one user posts a ques-
tion, other users can give answers to it. It is 
found that at least 50% percent questions 
(Valentin Jijkoun and Maarten de Rijke, 
2005)posted by users are non-factoid and surely 
more complicated both in question pattern and 
information need than those questions in the test 
set of TREC and other FAQ.  An example is as 
follows: 
 
This kind of Q&A system can complement the 
search engines effectively.  As the best search 
engines in china, Baidu open the Baidu Knowl-
edge2 Q&A system from 2003, and now it has 
more than 29 million question-answer pairs. 
There are also many other systems of this kind 
such as Google Groups, Yahoo Answers and 
Sina Knowledge3. This kind of system is a big 
question-answer pair database which can be 
treated as a FAQ database. How to search from 
the database and how to analyze the questions in 
the database needs new methods and techniques.   
More deeper and precise capture of the semantics 
in those complex questions is required. This phe-
nomenon has also been noticed by some re-
searchers and organizations. The spotlight gradu-
ally shifted to the processing and semantic un-
derstanding of complex questions. From 2006, 
TREC launched a new annually evaluation 
CIQ&A (complex, interactive Question Answer-
ing), aiming to promote the development of in-
teractive systems capable of addressing complex 
information needs. The targets of national pro-
grams AQUAINT and QUETAL are all at new 
interface and new enhancements to current state-
of-the-art Q&A systems to handle more complex 
inputs and situations. 
A few researchers and institutions serve as pio-
neers in complex questions study. Different tech-
nologies, such as definitions of different sets of 
question types, templates and sentence patterns 
(Noriko Tomuro, 2003) (Hyo-Jung Oh et al 
2005) machine learning methods (Radu Soricut 
and Eric Brill, 2004), language translation model 
(Jiwoon Jeon, W et al 2005), composition of 
information needs of the complex question 
(Sanda Harabagiu et al 2006) and so on, have 
been experimented on the processing of complex 
question, gearing the acquired information to the 
facility of other Q&A modules.  
Several major problems faced now by researcher 
of complex questions are stated as follow:  
First: Unlike factoid questions, it is very dif-
ficult to define a comprehensive type hierarchy 
for complex questions. Different domains under 
research may require definitions of different sets 
of question types, as shown in (Hyo-Jung Oh et 
al, 2005). Especially, the types of certain ques-
                                                 
2 http://zhidao.baidu.com/ 
3 http://iask.sina.com.cn/ 
2
tions are ambiguous and hard to identify. For 
example: 
 
This question type can be treated as definition, 
procedure or entity. 
Second: Lack of recognition of different seman-
tic chunks and the relations between them. 
FAQFinder (Radu Soricut and Eric Brill, 2004) 
also used semantic measure to credit the similar-
ity between different questions. Nevertheless, the 
question similarity is only a simple summation of 
the semantic similarity between words from the 
two question sentences. Question pattern are very 
useful and easy to implement, as justified by pre-
vious work. However, just like the problem with 
question types, question patterns have limitation 
on the coverage of all the variations of complex 
question formation. Currently, after the question 
processing step in most systems, the semantic 
meaning of large part of complex questions still 
remain vague. Besides, confining user?s input 
only within the selection of provided pattern may 
lead to unfriendly and unwelcome user interface. 
(Ingrid Zukerman and Eric Horvitz, 2001) used 
decision tree to model and recognize the infor-
mation need, question and answer coverage, 
topic, focus and restrictions of a question. Al-
though features employed in the experiments 
were described in detail, no selection process of 
those feature, or comparison between them was 
mentioned. 
This paper presents a general method for Chinese 
question analyzing. Our goal is to annotate the 
semantic chunks for the question automatically.  
2 Semantic Chunk Annotation 
Chinese language differs a lot from English in 
many aspects. Mature methodologies and fea-
tures well-justified in English Q&A systems are 
valuable sources of reference, but no direct copy 
is possible.  
The Ask-Answer system 4  is a Chinese online 
Q&A system where people can ask and answer 
questions like other web based Q&A system. The 
characteristic of this system is that it can give the 
answer automatically by searching from the 
asked question database when a new question is 
presented by people. The architecture of the 
automatically answer system is shown in figure 1.  
The system contains a list of question-answer 
pairs on particular subject. When users input a 
                                                 
 
 
 
4 http://haitianyuan.com/qa 
question from the web pages, the question is 
submitted to the system and then question-
answer pair is returned by searching from the 
questions asked before. The system includes four 
main parts: question pre-processing, question 
analyzing, searching and answer getting.  
The question pre-processing part will segment 
the input questions into words, label POS tags 
for every word.  Sometimes people ask two or 
more questions at one time, the questions should 
be made into simple forms by conjunctive struc-
ture detection. The question analyzing program 
will find out the question type, topic, focus and 
etc. The answer getting part will get the answer 
by computing the similarity between the input 
question and the questions asked before. The 
question analyzing part annotates the semantic 
chunks for the question. So that the question can 
be mapped into semantic space and the question 
similarity can be computed semantically. The 
Semantic chunk annotation is the most important 
part of the system. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Question Pre- processing 
 
Segmentation and  pos 
tagging 
Detect conjunctive structure  
Question Analyzing 
Semantic chunk annotationGet and extend key words
Question pattern and knowledge base 
Search reference question-answer pairs form database 
Answer getting 
Score the constituent 
answers 
Out put the top 
five answers 
 
Figure 1 the architecture of the automatically 
answer system 
Currently, no work has been reported yet on the 
question semantic chunk annotation in Chinese. 
The prosperity of major on-line discussion 
groups provides an abundant ready corpus for 
question answering research. Using questions 
collected from on-line discussion groups; we 
make a deep research on semantic meanings and 
build a question semantic chunk annotation 
model based on Conditional Random Field. 
Five types of semantic chunks were defined: 
Topic, Focus, Restriction, Rubbish information 
and Interrogative information. The topic of a 
3
question which is the topic or subject asked is the 
most important semantic chunk. The focus of a 
question is the asking point of the question. The 
restriction information can restrict the question?s 
information need and the answers. The rubbish 
information is those words in the question that 
has no semantic meanings for the question. Inter-
rogative information is a semantic tag set which 
corresponds to the question type. The interroga-
tive information includes interrogative words, 
some special verbs and nouns words and all these 
words together determine the question type. The 
semantic chunk information is shown in table 1.  
 
Semantic 
chunk   tag 
Abbreviation Meaning 
Topic T The question subject 
Focus F The additional information 
of topic 
Restrict 
 
Re Such as Time restriction and 
location restriction 
Rubbish 
information 
Ru Words no meaning for the 
question 
Other O other information without 
semantic meaning 
The following is interrogative information 
Quantity Wqua  
Description Wdes The answer need description
Yes/No Wyes The answer should be yes or 
no 
List Wlis The answer should be a list 
of entity 
Definition Wdef The answer is the definition 
of topic 
Location Wloc The answer is location 
Reason Wrea The answer can explain the 
question 
Contrast Wcon The answer is the compari-
son of the items proposed in 
the question 
People Wwho The answer is about the 
people?s information 
Choice Wcho The answer is one of the 
choice proposed in the ques-
tion 
Time Wtim The answer is the data or 
time length about the event 
in the question 
Entity Went The answer is the attribute 
of the topic. 
Table 1: Semantic chunks  
An annotation example question is as follows: 
 
This question can be annotated as follows: 
 
This kind of annotation is not convenient for CRF 
model, so the tags were transfer into the B I O 
form. (Shown as follows) 
 
Then the Semantic chunk annotation can be 
treated as a sequence tag problem.  
3 Semantic Chunk Annotation model 
3.1 Overview of the CRF model 
The conditional random field (CRF) is a dis-
criminative probabilistic model proposed by John 
Lafferty, et al(2001) to overcome the long-range 
dependencies problems associated with genera-
tive models. CRF was originally designed to la-
bel and segment sequences of observations, but 
can be used more generally. Let X, Y be random 
variables over observed data sequences and cor-
responding label sequences, respectively. For 
simplicity of descriptions, we assume that the 
random variable sequences X and Y have the 
same length, and use [ ]mxxxx ......, 21=   
and [ ]myyyy ......, 21=  to represent instances of 
X and Y, respectively. CRF defines the condi-
tional probability distribution P(Y |X) of label 
sequences given observation sequences as fol-
lows 
)),(exp(
)(
1
)|(
1
?
=
=
n
i
ii YXfXZ
XYP ?
?
?    (1) 
Where  is the normalizing factor that 
ensures equation 2. 
)(XZ?
 ? =y xyP 1)|(?                   (2) 
In equation 2 the i? is a model parameter and 
 is a feature function (often binary-
valued) that becomes positive (one for binary-
valued feature function) when X contains a cer-
tain feature in a certain position and Y takes a 
certain label, and becomes zero otherwise. 
Unlike Maximum Entropy model which use sin-
gle normalization constant to yield a joint distri-
bution, CRFs use the observation-dependent 
normalization  for conditional distribu-
tions. So CRFs can avoid the label biased prob-
lem. Given a set of training data 
),( YXfi
)(XZ?
}....2,1),,{( nkyxT kk ==  
 With an empirical distribution , CRF ),(
~
YXP
4
determines the model parameters }{ i?? =  by 
maximizing the log-likelihood of the training set 
)|(log),(
)|(log)(
,
~
1
xyPyxP
xyPP
yx
N
k
kk
?
??
?
?
?
=?
=                       (3) 
3.2 Features for the model 
The following features, which are used for train-
ing the CRF model, are selected according to the 
empirical observation and some semantic mean-
ings. These features are listed in the following 
table. 
 
Feature type in-
dex 
Feature type name 
1 Current word 
2 Current POS tag 
3 Pre-1 word POS tag 
4 Pre-2 word POS tag 
5 Post -1 word POS tag 
6 Post -2 word POS tag 
7 Question pattern 
8 Question type 
9 Is pattern key word 
10 Pattern tag 
Table 2: the Features for the model 
Current word: 
The current word should be considered when 
adding semantic tag for it. But there are too 
many words in Chinese language and only part 
of them will contribute to the performance, a set 
of words was selected. The word set includes 
segment note and some key words such as time 
key word and rubbish key word. When the cur-
rent word is in the word set the current word fea-
ture is the current word itself, and null on the 
other hand. 
Current POS tag: 
Current POS tag is the part of speech tag for the 
current word. 
Pre-1 word POS tag: 
Pre- 1 word POS tag is the POS tag of the first 
word before the labeling word in the sentence. If 
the Pre-1 word does not exit (the current is the 
first word in the sentence), the Pre- 1 word POS 
tag is set to null. 
Pre-2 word POS tag: 
Pre- 2 word POS tag is the POS tag of the second 
word before the labeling word in the sentence. If 
the Pre-2 word does not exit, the Pre- 2 word 
POS tag is set to null. 
Post -1 word POS tag: 
Post - 1 word POS tag is the POS tag of the first 
word after the labeling word in the sentence. If 
the Post -1 word does not exit (the current is the 
first word in the sentence), the Post - 1 word POS 
tag is set to null. 
Post -2 word POS tag: 
Post - 2 word POS tag is the POS tag of the sec-
ond word after the labeling word in the sentence. 
If the Post-2 word does not exit, the Pre- 2 word 
POS tag is set to null. 
Question pattern: 
Question pattern which is associated with ques-
tion type, can locate question topic, question fo-
cus by surface string matching. For example, 
(where is <topic>). The patterns are extracted 
from the training data automatically. When a pat-
tern is matched, it is treated as a feature. There 
are 1083 question patterns collected manually.  
Question type: 
Question type is an important feature for ques-
tion analyzing. The question patterns have the 
ability of deciding the question type. If there is 
no question pattern matching the question, the 
question type is defined by a decision tree algo-
rithm. 
Is pattern key word: 
For each question pattern, there are some key 
words. When the current word belongs to the 
pattern key word this feature is set to ?yes?, else 
it is set to ?no?. 
Pattern tag: 
When a pattern is matched, the topic, focus and 
restriction can be identified by the pattern. We 
can give out the tags for the question and the tags 
are treated as features. If there is no pattern is 
matched, the feature is set to null.   
4 Feature Selection experiment 
Feature selection is important in classifying sys-
tems such as neural networks (NNs), Maximum 
Entropy, Conditional Random Field and etc. The 
problem of feature selection has been tackled by 
many researchers. Principal component analysis 
(PCA) method and Rough Set Method are often 
used for feature selection. Recent years, mutual 
information has received more attention for fea-
ture selection problem.  
According to the information theory, the uncer-
tainty of a random variable X can be measured 
by its entropy . For a classifying problem, 
there are class label set represented by C and fea-
ture set represented by F. The conditional en-
tropy  measures the uncertainty about 
)(XH
)|( FCH
5
C when F is known, and the Mutual information 
I(C, F) is defined as:  
 F)|(C -(C));( HHFCI =                   (4) 
The feature set is known; so that the objective of 
training the model is to minimize the conditional 
entropy   equally maximize the mutual 
information . In the feature set F, some 
features are irrelevant or redundant. So that the 
goal of a feature selection problem is to find a 
feature S ( ), which achieve the higher 
values of . The set S is a subset of F and 
its size should be as small as possible. There are 
some algorithms for feature selection problem. 
The ideal greedy selection algorithm using mu-
tual information is realized as follows (Nojun 
Kwak and Chong-Ho Choi, 2002): 
)|( FCH
);( FCI
FS ?
);( FCI
 Input:   S- an empty set 
             F- The selected feature set 
Output:  a small reduced feature set S which is 
equivalent to F 
Step 1: calculate the MI with the Class 
set C , , compute  Ffi ?? );( ifCI
Step 2: select the feature that maximizes , 
set  
);( ifCI
}{},{\ ii fSfFF ??
Step 3: repeat until desired number of features 
are selected. 
1) Calculate the MI with the Class set C and S, 
Ffi ?? , compute  ),;( ifSCI
2) Select the feature that maximizes , 
set 
),;( ifSCI
}{},{\ ii fSfFF ??  
Step 4: Output the set S  that contains the se-
lected features 
To calculate MI the PDFs (Probability Distribu-
tion Functions) are required. When features and 
classing types are dispersing, the probability can 
be calculated statistically.  In our system, the 
PDFs are got from the training corpus statistically. 
The training corpus contains 14000 sentences. 
The training corpus was divided into 10 parts, 
with each part 1400 sentences.  And each part is 
divided into working set and checking set. The 
working set, which contains 90% percent data, 
was used to select feature by MI algorithm. The 
checking set, which contains 10% percent data, 
was used to test the performance of the selected 
feature sequence. When the feature sequence was 
selected by the MI algorithm, a sequence of CRF 
models was trained by adding one feature at each 
time. The checking data was used to test the per-
formance of these models.  
 The open test result 
Selected feature 
sequence 
1 2 3 4 5 6 7 8 9 10 
7, 10, 3, 1, 5, 2, 
4, 6, 8?9 
0.5104 0.8764 0.8864 0.8918 0.8925 0.8977 0.8992 0.9023 0.9025 0.9018 
7, 10, 1, 3, 5, 2, 
4?6?8?9 
0.5241 0.8775 0.8822 0.8911 0.8926 0.8956 0.8967 0.9010 0.9005 0.9007 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5090 0.8691 0.8748 0.8851 0.8852 0.8914 0.8929 0.8955 0.8955 0.8949 
7, 10, 1, 3, 5, 2, 
4, 6?9?8 
0.5157 0.8769 0.8823 0.8913 0.8925 0.8978 0.8985 0.9017 0.9018 0.9010 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5144 0.8821 0.8856 0.8921 0.8931 0.8972 0.8981 0.9010 0.9009 0.9007 
7, 10, 3, 1, 5, 2, 
4?6?8?9 
0.5086 0.8795 0.8876 0.8914 0.8919 0.8960 0.8967 0.9016 0.9013 0.9011 
7, 10, 1, 3, 5, 2, 
4, 6, 8, 9 
0.5202 0.8811 0.8850 0.8920 0.8931 0.8977 0.8980 0.9015 0.9013 0.9009 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5015 0.8858 0.8879 0.8948 0.8942 0.8998 0.8992 0.9033 0.9027 0.9023 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5179 0.8806 0.8805 0.8898 0.8908 0.8954 0.8958 0.8982 0.8982 0.8986 
7, 10, 1, 3, 5, 2, 
4, 6, 8?9 
0.5153 0.8921 0.8931 0.9006 0.9012 0.9041 0.9039 0.9071 0.9068 0.9067 
Table 3: the feature selection result and the test result 
In table 3, each row contains data corresponding 
to one part of the training corpus so there are ten 
rows with data in the table. The third row corre-
sponds to the first part and the last row corre-
sponds to the tenth part. There are eleven col-
umns in the table, the first columns is the fea-
tures sequence selected by the mutual informa-
tion algorithm for each part. The second column 
is the open test result with the first feature in the 
feature sequence. The third column is the open 
test result with the first two features in the fea-
ture sequence and so on. From the table, it is 
6
clear that the feature 7(Question pattern) and 
10(Pattern tag) are very important, while the fea-
ture 8(Question type) and 9(Is pattern key word) 
are not necessary. The explanation about this 
phenomenon is that the ?pattern key word? and 
?Question type? information can be covered by 
the Question patterns. So feature 8 and 9 are not 
used in the Conditional Random Field model. 
5 Semantic Chunk Annotation Experi-
ment 
The test and training data used in our system are 
collected from the website (Baidu knowledge 
and the Ask-Answer system), where people pro-
posed questions and answers. The training data 
consists of 14000 and the test data consists of 
4000 sentences. The data set consists of word 
tokens, POS and semantic chunk tags. The POS 
and semantic tags are assigned to each word to-
kens.  
The performance is measured with three rates: 
precision (Pre), recall (Rec) and F-score (F1). 
Pre = Match/Model                     (5) 
Rec=Match/Manual                    (6) 
F1=2*Pre*Rec/(Pre+Rec)              (7) 
Match is the count of the tags that was predicted 
right. Model is the count of the tags that was pre-
dicted by the model. Manual is the count of the 
tags that was labeled manually. 
Table 4 shows the performance of annotation of 
different semantic chunk types. The first column 
is the semantic chunk tag. The last three columns 
are precision, recall and F1 value of the semantic 
chunk performance, respectively.   
 
Label Manual Model Match Pre.() Rec.() F1 
B-T?I-T 17061?78462 16327?80488 14825?76461 90.80?95.00 86.89?97.45 88.80?96.21 
B-F?I-F 5072?13029 5079?13583 4657?12259 91.69?90.25 91.82?94.09 91.75?92.13 
B-Ru?I-Ru 775?30 11?0 2?0 18.18?0.00 0.26?0.00 0.51?0.00 
O 8354 8459 6676 78.92 79.91 79.41 
B-Wqua?I-Wqua 1363?934 1327?1028 1298?881 97.81?85.70 95.23?94.33 96.51?89.81 
B-Wyes?I-Wyes 5669?1162 5702?1098 5550?1083 97.33?98.63 97.90?93.20 97.62?95.84 
B-Wdes?I-Wdes 2907?278 2855?185 2779?184 97.34?99.46 95.60?66.19 96.46?79.48 
B-Wlis?I-Wlis 603?257 563?248 560?248 99.47?100 92.87?96.50 96.05?98.22 
B-Wdef?I-Wdef 1420?1813 1430?1878 1280?1695 89.51?90.26 90.14?93.49 89.82?91.85 
B-Wloc?I-Wloc 683?431 665?395 661?392 99.40?99.24 96.78?90.95 98.07?94.92 
B-Wrea?I-Wrea 902?159 873?83 843?82 96.56?98.80 93.46?51.57 94.99?67.77 
B-Wcon?I-Wcon 552?317 515?344 503?291 97.67?84.59 91.12?91.80 94.28?88.05 
B-Wwho?I-Wwho 420?364 357?350 348?336 97.48?96.00 82.86?92.31 89.58?94.12 
B-Wcho?I-Wcho 857?85 738?0 686?0 92.95?0.00 80.05?0.00 86.02?0.00 
B-Wtim?I-Wtim 408?427 401?419 355?380 88.53?90.69 87.01?88.99 87.76?89.83 
B-Went?I-Went 284?150 95?81 93?80 97.89?98.77 32.75?53.33 49.08?69.26 
Avg 145577 145577 135488 93.07 93.07 93.07 
Table 4: the performance of different semantic chunk
 
The semantic chunk type of ?Topic? and ?Focus? 
can be annotated well. Topic and focus semantic 
chunks have a large percentage in all the seman-
tic chunks and they are important for question 
analyzing. So the result is really good for the 
whole Q&A system. 
As for ?Rubbish? semantic chunk, it only has 
0.51 and 0.0 F1 measure for B-Ru and I-Ru. One 
reason is lacking enough training examples, for 
there are only 1031 occurrences in the training 
data. Another reason is sometimes restriction is 
complex. 
6 Conclusion and future work 
This paper present a new method for Chinese 
question analyzing based on CRF. The features 
are selected by using mutual information algo-
rithm. The selected features work effectively for 
the CRF model. The experiments on the test data 
set achieve 93.07% in F1 measure. In the future, 
new features should be discovered and new 
methods will be used.  
Acknowledgment  
This work is supported by Major Program of Na-
tional Natural Science Foundation of China 
(No.60435020 and No. 90612005) and the High 
Technology Research and Development Program 
of China (2006AA01Z197). 
 
References 
A.M. Turing. 1950. Computing Machinery and 
Intelligence. Mind, 236 (59): 433~460. 
Diego Moll?, Jose?Luis Vicedo. 2007. Question 
Answering in Restricted Domains: An Overview. 
Computational Linguistics, 33(1),  
7
Dragomir Radev, WeiGuo Fan, Leila Kosseim. 2001. 
The QUANTUM Question Answering System. 
TREC. 
Min-Yuh Day, Cheng-Wei Lee, Shih-Hung WU, 
Chormg-Shyong Ong,  Wen-Lian Hsu. 2005. An 
Integrated Knowledge-based and Machine 
Learning Approach for Chinese Question 
Classification. Proceedings of the IEEE 
International Conference on Natural Language 
Processing and Knowledge Engineering, Wuhan, 
China,:620~625. 
Ulf Hermjakob. 2001. Parsing and Question 
Classification for Question Answering.  
Proceedings of the ACL Workshop on Open-
Domain Question Answering, Toulouse,:19~25. 
Dell Zhang, Wee Sun Lee. 2003. Question 
classification using support vector machines. 
Proceedings of the 26th Annual International ACM 
Conference on Research and Development in 
Information Retrieval(SIGIR), Toronto, Canada,26 
~ 32. 
Valentin Jijkoun, Maarten de Rijke.2005. Retrieving 
Answers from Frequently Asked Questions Pages 
on the Web. CIKM?05, Bermen, Germany. 
Noriko Tomuro. 2003. Interrogative Reformulation 
Patterns and Acquisition of Question Paraphrases. 
Proceeding of the Second International Workshop 
on Paraphrasing, :33~40. 
Hyo-Jung Oh, Chung-Hee Lee, Hyeon-Jin Kim, 
Myung-Gil Jang. 2005. Descriptive Question 
Answering in Encyclopedia. Proceedings of the 
ACL Interactive Poster and Demonstration Sessions, 
pages 21?24, Ann Arbor. 
Radu Soricut, Eric Brill. 2004, Automatic Question 
Answering: Beyond the Factoid.  Proceedings of 
HLT-NAACL ,:57~64. 
Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005. 
Finding Similar Questions in Large Question and 
Answer Archives. CIKM?05, Bremen, Germany. 
Sanda Harabagiu, Finley Lacatusu and Andrew Hickl. 
2006 . Answering Complex Questions with Random 
Walk Models. SIGIR?06, Seattle, Washington, 
USA.pp220-227. 
Ingrid Zukerman, Eric Horvitz. 2001. Using Machine 
Learning Techniques to Interpret WH-questions. 
ACL. 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. Conditional Random Fields: probabilistic 
Models for Segmenting and Labeling Sequence 
Data. Proceedings of the Eighteenth International 
Conference on Machine Learning, p.282-289. 
Nojun Kwak and Chong-Ho Choi. 2002. Input 
feature selection for classification problems. 
IEEE Trans on Neural Networks,,13(1):143-
159 
 
8
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 218?222
Manchester, August 2008
Discriminative Learning of Syntactic and Semantic Dependencies
Lu Li
1
, Shixi Fan
2
, Xuan Wang
1
, Xiaolong Wang
1
Shenzhen Graduate School, Harbin Institute of Technology,
Xili, Shenzhen 518055, China
1
{lli,wangxuan,wangxl}@insun.hit.edu.cn
2
fanshixi@hit.edu.cn
Abstract
A Maximum Entropy Model based system
for discriminative learning of syntactic and
semantic dependencies submitted to the
CoNLL-2008 shared task (Surdeanu, et al,
2008) is presented in this paper. The sys-
tem converts the dependency learning task
to classification issues and reconstructs the
dependent relations based on classification
results. Finally F1 scores of 86.69, 69.95
and 78.35 are obtained for syntactic depen-
dencies, semantic dependencies and the
whole system respectively in closed chal-
lenge. For open challenge the correspond-
ing F1 scores are 86.69, 68.99 and 77.84.
1 Introduction
Given sentences and corresponding part-of-speech
of each word, the learning of syntactic and seman-
tic dependency contains two separable goals: (1)
building a dependency tree that defines the syn-
tactic dependency relationships between separated
words; (2) specifying predicates (no matter verbs
or nouns) of the sentences and labeling the seman-
tic dependents for each predicate.
In this paper a discriminative parser is pro-
posed to implement maximum entropy (ME) mod-
els (Berger, et al, 1996) to address the learning
task. The system is divided into two main subsys-
tems: syntactic dependency parsing and semantic
dependency labeling. The former is used to find a
well-formed syntactic dependency tree that occu-
pies all the words in the sentence. If edges are
added between any two words, a full-connected
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
graph is constructed and the dependency tree could
be found using a maximum spanning tree (MST)
algorithm (McDonald, et al, 2005). The latter fo-
cuses on separable predicates whose semantic de-
pendents could be determined using classification
tools, such as ME models
1
etc..
We participated in both closed and open chal-
lenge of the CoNLL-2008 shared task (Surdeanu,
et al, 2008). Results are reported on both develop-
ment and test sets in this paper.
2 System Description
2.1 Syntactic Parsing
The goal of syntactic parsing is to create a la-
beled syntactic dependency parse y for input sen-
tence x including words and their parts of speech
(POS). Inspired by the parsing model that imple-
ments maximum spanning tree (MST) algorithm
to induce the dependency parsing tree (McDonald,
et al, 2005), the system employs the same frame-
work. The incorporated features are defined over
parts of speech of words occurring between and
around a possible head-dependent relation.
Suppose G = (V, E) is a directed graph, where
V is the set of vertices denoting the words in sen-
tence x and E is the set of directed edges between
any two vertices with some scores. The MST al-
gorithm is to find the most probable subgraph of G
that satisfies tree constraints over all vertices. The
score function of the parsing tree y is defined as
s(y) =
?
(i,j)?y
s(i, j) (1)
where (i, j) ? y indicates an edge in y from word
i to word j and s(i, j) denotes its score. Suppose Y
1
http://homepages.inf.ed.ac.uk/s0450736/maxent.html
218
wi
w
j
p
i
p
j
(w
i
, p
i
) (w
j
, p
j
)
(w
i
, w
j
) (p
i
, p
j
)
(w
i
, p
j
) (w
j
, p
i
)
(w
i
, w
j
, p
i
) (w
i
, w
j
, p
j
)
(p
i
, p
j
, w
i
) (p
i
, p
j
, w
j
)
(w
i
, w
j
, p
i
, p
j
) (p
i
, p
k
, p
j
), i < k < j
(p
i
, p
i+1
, p
j?1
, p
j
) (p
i?1
, p
i
, p
j?1
, p
j
)
(p
i
, p
i+1
, p
j
, p
j+1
) (p
i?1
, p
i
, p
j
, p
j+1
)
Table 1: Features for syntactic parsing.
is the set of syntactic dependency labels, the score
function of edges is defined as
s(i, j) = max
l?Y
Pr(l|x, i, j) (2)
ME models are used to calculate the value of
Pr(l|x, i, j), where the features are extracted from
input sentence x. Given i and j as the subscripts
of words in the sentence and word i is the parent
of word j, the features can be illustrated in table
1. w
i
and p
i
are denoted as the ith word and the
ith part of speech respectively in the sentence. The
tuples define integrated features, such as (w
i
, p
i
)
indicates the feature combining the ith word and
ith part of speech. Besides these features, the dis-
tant between word i and word j in sentence x is
considered as a single feature. The distant is also
combined with features in table 1 to produce com-
plex features.
2.2 Semantic Dependency Labeling
Semantic dependencies are always concerning
with specific predicates. Unlike syntactic depen-
dencies, semantic dependency relationships usu-
ally can not be represented as a tree. Thus, the
method used for semantic dependency labeling
is somewhat different from syntactic dependency
parsing. The work of semantic labeling can be di-
vided into two stages: predicate tagging and de-
pendents recognizing.
2.2.1 Predicate Tagging
According to PropBank (Palmer, et al, 2005)
and NomBank (Meyers, et al, 2004), predicates
usually have several rolesets corresponding to dif-
ferent meanings. For example, the verb abandon
has three rolesets marked as ordinal numbers 01,
02 and 03 as described below.
w
i
p
i
p
i?1
p
i+1
(p
i?1
, p
i
) (p
i
, p
i+1
)
(p
i?2
, p
i
) (p
i
, p
i+2
)
(p
i?3
, p
i
) (p
i
, p
i+3
)
(p
i?1
, p
i
, p
i+1
) (w
i
, p
i
)
(w
i
, p
i?1
, p
i
) (w
i
, p
i
, p
i+1
)
(w
i
, p
i?2
, p
i
) (w
i
, p
i
, p
i+2
)
(w
i
, p
i?3
, p
i
) (w
i
, p
i
, p
i+3
)
(w
i
, p
i?1
, p
i
, p
i+1
)
Table 2: Features used for predicate tagging.
<frameset>
<predicate lemma=?abandon?>
<roleset id=?abandon.01? name=?leave
behind? vncls=?51.2?>
. . .
</roleset>
<roleset id=?abandon.02?
name=?exchange? vncls=?51.2?>
. . .
</roleset>
<roleset id=?abandon.03?
name=?surrender, give over? vncls=?-
?>
. . .
</roleset>
</predicate>
</frameset>
The goal of this part is to identify the predicates
in the sentences and to determine the roleset for
each of them. It should be cleared that the ordi-
nal numbers are only used to distinguish different
meanings of a predicate. However, if these num-
bers are treated as tags for predicates, some statisti-
cal properties will be obtained as illustrated in Fig-
ure 1. As can be seen, the distribution of the train
data would be quite informative for representing
the distribution of other three data sets. Based on
this idea, a classification framework is introduced
for predicate tagging.
Suppose the tag set is chosen to be T =
{01, 02, ..., 22} according to the horizontal axis of
Figure 1 and 00 is added to indicate that the ex-
amining word is not a predicate. Suppose t
i
is a
variable indicating the tag of word at position i in
sentences x. ME models are implemented to tag
the predicates.
t
i
= argmax
t? T
Pr(t|x, i) (3)
219
0 5 10 15 200
2
4
6
8
10
12
Ordinal Numbers of Predicates
Log
arith
mic
al N
umb
er o
f Oc
curr
enc
e
traindevelbrownws j
Figure 1: Distribution of the ordinal numbers of
predicates on different data sets. 01 - 21 are at-
tached with the predicates in the corpus and 22
stands for ?SU?.
The features for predicate tagging are listed in ta-
ble 2, where the symbols share the same mean-
ing as in table 1. Experiments show that this pure
statistic processing method is effective for predi-
cate tagging.
2.2.2 Dependents Recognizing
This subtask depends deeply on the results of
syntactic parsing and predicate tagging described
earlier in the system. Predicate tagging identifies
central words and syntactic parsing provides syn-
tactic features for its dependents identification and
classification.
Generally speaking, given a specific predicate in
a sentence, only a few of words are associated as its
semantic dependents. By statistical analysis a list
of part of speech tuples that are appearing to be se-
mantic dependency are collected. All other tuples
are filtered out to improve system performance.
Suppose (p, d) is a couple of predicate and one
of its possible dependents, T is the dependency
tree generated by syntactic parsing, L is the set of
semantic dependency labels. The dependents can
be recognized by using a classification model, ME
models are chosen as before.
l
(p,d)
= argmax
l?L
Pr(l|p, d, T ) (4)
Besides the semantic dependency labels, null is in-
cluded as a special tag to indicate that there is no
semantic dependency between p and d. As a result,
dependents identification (binary classification)
and dependents tagging (multi-classification) can
be solved together within one multi-classification
framework.
The selected features are listed below.
1. Predicate Features
? Lemma and POS of predicate, pred-
icate?s parent in syntactic dependency
tree.
? Voice active or passive.
? Syntactic dependency label of edge be-
tween predicate and its parent.
? POS framework POS list of predicate?s
siblings, POS list of predicate?s children.
? Syntactic dependency framework syn-
tactic dependency label list of the edges
between predicate?s parent and its sib-
lings.
? Parent framework syntactic depen-
dency label list of edges connecting to
predicate?s parent.
2. Dependent Features
? Lemma and POS of dependent, depen-
dent?s parent.
? POS framework POS list of depen-
dent?s siblings.
? Number of children of dependent?s par-
ent.
3. In Between Features
? Position of dependent according to
predicate: before or after.
? POS pair of predicate and dependent.
? Family relation between predicate and
dependent: ancestor or descendant.
? Path length between predicate and de-
pendent.
? Path POS POS list of all words appear-
ing on the path from predicate to depen-
dent.
? Path syntactic dependency label list of
dependency label of edges of path be-
tween predicate and dependent.
3 Experiment results
The classification models were trained using all the
training data. The detailed information are shown
in table 3. All experiments ran on 32-bit Intel(R)
Pentium(R) D CPU 3.00GHz processors with 2.0G
memory.
220
Feature Number Training Time
Syn. 7,488,533 30h
Prd. 1,484,398 8h
Sem. 3,588,514 12h
Table 3: Details of ME models. Syn. is for syntac-
tic parsing, Prd. is for predicate tagging and Sem.
is for semantic dependents recognizing.
Syntactic Semantic Overall
devel 85.29 69.60 77.49
brown 80.80 59.17 70.01
wsj 87.42 71.27 79.38
brown+wsj 86.69 69.95 78.35
(a) Closed Challenge
Syntactic Semantic Overall
devel 85.29 68.45 76.87
brown 80.80 58.22 69.51
wsj 87.42 70.32 78.87
brown+wsj 86.69 68.99 77.84
(b) Open Challenge
Table 4: Scores for joint learning of syntactic and
semantic dependencies.
3.1 Closed Challenge
The system for closed challenge is designed as a
two-stage parser: syntactic parsing and semantic
dependency labeling as described previously. Ta-
ble 4(a) shows the results on different corpus. As
shown in table 4(a), the scores of semantic depen-
dency labeling are quite low, that are influencing
the overall scores. The reason could be inferred
from the description in section 2.2.2 since seman-
tic dependent labeling inherits the errors from the
output of syntactic parsing and predicate tagging.
Following evaluates each part independently.
Besides the multiple classification model de-
scribed in table 3, a binary classification model
was built based on ME for predicate tagging. The
binary model can?t distinguish different rolesets of
predicate, but can identify which words are predi-
cates in sentences. The precision and recall for bi-
nary model are 90.80 and 88.87 respectively, while
for multiple model, the values are 84.60 and 85.60.
For semantic dependent labeling, experiments
were performed under conditions that the gold syn-
tactic dependency tree and predicates list were
given as input. The semantic scores became 80.09,
77.08 and 82.25 for devel, brown and wsj respec-
tively. This implies that the error of syntactic pars-
ing and predicate tagging could be probably aug-
mented in semantic dependent labeling. In order to
improve the performance of the whole system, the
deep dependence between the two stages should be
broken up in future research.
3.2 Open Challenge
In open challenge, the same models are used for
syntactic parsing and predicate tagging as in closed
challenge and two other models are trained for se-
mantic dependent labeling. Suppose M
mst
, M
malt
and M
chunk
are denoted as these three semantic
models, where M
mst
is the model used in closed
challenge, M
malt
is trained on the syntactic de-
pendency tree provided by the open corpus with
the same feature set as M
mst
, and M
chunk
is
trained using features extracted from name entity
and wordnet super senses results provided by the
open corpus.
Considering a possible dependent given a spe-
cific predicate, the feature set used for M
chunk
contains only six elements:
? Whether the dependent is in name entity
chunk: True or False.
? Name entity label of the dependent.
? Whether the dependent is in BBN name entity
chunk: True or False.
? BBN name entity label of the dependent.
? Whether the dependent is in wordnet super
sense chunk: True or False.
? Wordnet super sense label of the dependent.
After implementing these three models on se-
mantic dependents recognizing, the results were
merged to generate the scores described in table
4(b).
The merging strategy is quite simple. Given a
couple of predicate and dependent (p, d), the sys-
tem produces three semantic dependency labels
denoting as l
mst
, l
malt
and l
chunk
, the result la-
bel is chosen to be most frequent semantic label
among the three.
Comparing the scores of open challenge and
closed challenge, it can be found that the score of
the former is less than the latter, which is quite
strange since more resources were used in open
challenge. To examine the influences of differ-
ent semantic dependents recognizing models, each
221
Mmst
M
malt
M
chunk
devel 69.60 64.48 41.72
brown 59.17 56.52 34.04
wsj 71.27 66.40 41.83
Table 5: Semantic scores of different models.
model was implemented in the closed challenge
and the results are shown in table 5. Specially,
model M
chunk
generated too low scores and gave a
heavy negative influence on the final results. Find-
ing a good way to combine several results requires
further research.
4 Conclusions
This paper have presented a simple discriminative
system submitted to the CoNLL-2008 shared task
to address the learning task of syntactic and se-
mantic dependencies. The system was divided into
syntactic parsing and semantic dependents label-
ing. Maximum spanning tree was used to find
a syntactic dependency tree in the full-connected
graph constructed over the words of a sentence.
Maximum entropy models were implemented to
classify syntactic dependency edges, predicates
and their semantic dependents. A brief analysis
has also been given on the results of both closed
challenge and open challenge.
Acknowledgement
This research has been partially supported by the
National Natural Science Foundation of China
(No. 60435020 and No. 90612005), the Goal-
oriented Lessons from the National 863 Program
of China (No.2006AA01Z197) and Project of Mi-
crosoft Research Asia. We would like to thank
Zhixin Hao, Xiao Xin, Languang He and Tao Qian
for their wise suggestion and great help. Thanks
also to Muhammad Waqas Anwar for English im-
provement.
References
Adam Berger, Stephen Della Pietra, Vincent Della
Pietra 1996. A Maximum Entropy Approach to Nat-
ural Language Processing. Computational Linguis-
tics, 22(1):39-71.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young
and Ralph Grishman 2004. The NomBank Project:
An Interim Report HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation, 24-31.
Martha Palmer, Daniel Gildea, Paul Kingsbury 2005.
The Proposition Bank: An Annotated Corpus of Se-
mantic Roles Computational Linguistics, 31(1):71-
106.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez and Joakim Nivre 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008)
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c 2005. Non-projective Dependency Pars-
ing using Spanning Tree Algorithms. Proceedings of
HLT/EMNLP, 523-530.
222
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 13?17,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Cascade Method for Detecting Hedges and their Scope in Natural
Language Text
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan, Shixi Fan
Key Laboratory of Network Oriented Intelligent Computation
Harbin Institute of Technology Shenzhen Graduate School
Shenzhen, Guangdong, China
{tangbuzhou,yuanbo.hitsz}@gmail.com
{wangxl,wangxuan,fanshixi}@insun.hit.edu.cn
Abstract
Detecting hedges and their scope in nat-
ural language text is very important for
information inference. In this paper,
we present a system based on a cascade
method for the CoNLL-2010 shared task.
The system composes of two components:
one for detecting hedges and another one
for detecting their scope. For detecting
hedges, we build a cascade subsystem.
Firstly, a conditional random field (CRF)
model and a large margin-based model are
trained respectively. Then, we train an-
other CRF model using the result of the
first phase. For detecting the scope of
hedges, a CRF model is trained according
to the result of the first subtask. The ex-
periments show that our system achieves
86.36% F-measure on biological corpus
and 55.05% F-measure on Wikipedia cor-
pus for hedge detection, and 49.95% F-
measure on biological corpus for hedge
scope detection. Among them, 86.36%
is the best result on biological corpus for
hedge detection.
1 Introduction
Hedge cues are very common in natural language
text. Vincze et al (2008) report that 17.70% of
the sentences in the abstract section and 19.94% of
sentences in the full paper section contain hedges
on BioScope corpus. As Vincze et al (2008)
suggest that information that falls in the scope
of hedges can not be presented as factual in-
formation. Detecting hedges and their scope in
natural language text is very important for in-
formation inference. Recently, relative research
has received considerable interest in the biomed-
ical NLP community, including detecting hedges
and their in-sentence scope in biomedical texts
(Morante and Daelemans, 2009). The CoNLL-
2010 has launched a shared task for exploiting the
hedge scope annotated in the BioScope (Vincze et
al., 2008) and publicly available Wikipedia (Gan-
ter and Strube, 2009) weasel annotations. The
shared task contains two subtasks (Farkas et al,
2010): 1. learning to detect hedges in sentences on
BioScope and Wikipedia; 2. learning to detect the
in-sentence scope of these hedges on BioScope.
In this paper, we present a system based on a
cascade method for the CoNLL-2010 shared task.
The system composes of two components: one
for detecting hedges and another one for detect-
ing their scope. For detecting hedges, we build
a cascade subsystem. Firstly, conditional ran-
dom field (CRF) model and a large margin-based
model are trained respectively. Then, we train
another CRF model using the result of the first
phase. For detecting the scope of hedges, a CRF
model is trained according to the result of the first
subtask. The experiments show that our system
achieves 86.36% F-measure on biological corpus
and 55.05% F-measure on Wikipedia corpus for
hedge detection, and 49.95% F-measure on bio-
logical corpus for hedge scope detection. Among
them, 86.36% is the best result on biological cor-
pus for hedge detection.
2 System Description
As there are two subtasks, we present a system
based on a cascade supervised machine learning
methods for the CoNLL-2010 shared task. The ar-
chitecture of our system is shown in Figure 1.
The system composes of two subsystems for
two subtasks respectively, and the first subsystem
is a two-layer cascaded classifier.
2.1 Hedge Detection
The hedges are represented by indicating whether
a token is in a hedge and its position in the
CoNLL-2010 shared task. Three tags are used for
13
Figure 1: System architecture
this scheme, where O cue indicates a token out-
side of a hedge, B cue indicates a token at the
beginning of a hedge and I cue indicates a to-
ken inside of a hedge. In this subsystem, we do
preprocessing by GENIA Tagger (version 3.0.1)1
at first, which does lemma extraction, part-of-
speech (POS), chunking and named entity recog-
nition (NER) for feature extraction. For the out-
put of GENIA Tagger, we convert the first char
of a lemma into lower case and BIO chunk tag
into BIOS chunk tag, where S indicates a token
is a chunk, B indicates a token at the beginning
of a chunk, I indicates a token inside of a chunk,
and O indicates a token outside of a chunk. Then
a two-layer cascaded classifier is built for pre-
diction. There are a CRF classifier and a large
margin-based classifier in the first layer and a CRF
classifier in the second layer.
In the first layer, the following features are used
in our system:
? Word andWord Shape of the lemma: we used
the similar scheme as shown in (Tsai et al,
2005).
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
? Prefix and Suffix with length 3-5.
? Context of the lemma, POS and the chunk in
the window [-2,2].
? Combined features including L0C0, LiP0
and LiC0, where ?1 ? i ? 1 L denotes the
lemma of a word, P denotes a POS and C
denotes a chunk tag.
? The type of a chunk; the lemma and POS se-
quences of it.
? Whether a token is a part of the pairs ?neither
... nor? and ?either ... or? as both tokens of a
pair are always labeled with the same tag.
? Whether a token can possibly be classified
into B cue, I cue or O cue; its lemma, POS
and chunk tag for each possible case: these
features are extracted according to a dictio-
nary extracted from training corpus, which
lists all possible hedge tag for each word in
the training corpus.
In the second layer, we used some features
about the result of the last layer besides those men-
tioned above. They are listed as follow:
? The lemma and POS sequences of the hedge
predicted by each classifier.
? The times of a token classified into B cue,
I cue and O cue by the first two classifiers.
? Whether a token is the last token of the hedge
predicted by each classifier.
2.2 Hedge Scope Detection
We follow the way of Morante and Daelemans
(2009) to represent the scope of a hedge, where
F scope indicates a token at the beginning of a
scope sequence, L scope indicates a token at the
last of a scope sequence, and NONE indicates
others. In this phase, we do preprocessing by
GDep Tagger (version beta1)2 at first, which does
lemma extraction, part-of-speech (POS), chunk-
ing, named entity recognition (NER) and depen-
dency parse for feature extraction. For the out-
put of GDep Tagger, we deal with the lemma and
chunk tag using the same way mentioned in the
last section. Then, a CRF classifier is built for pre-
diction, which uses the following features:
2http://www.cs.cmu.edu/ sagae/parser/gdep
14
? Word.
? Context of the lemma, POS, the chunk, the
hedge and the dependency relation in the
window [-2,2].
? Combined features including L0C0,
L0H0, L0D0, LiP0, PiC0,PiH0, CiH0,
PiD0,CiD0, where ?1 ? i ? 1 L denotes
the lemma of a word, P denotes a POS, C
denotes a chunk tag, H denotes a hedge tag
and D denotes a dependency relation tag.
? The type of a chunk; the lemma and POS se-
quences of it.
? The type of a hedge; the lemma, POS and
chunk sequences of it.
? The lemma, POS, chunk, hedge and depen-
dency relation sequences of 1st and 2nd de-
pendency relation edges; the lemma, POS,
chunk, hedge and dependency relation se-
quences of the path from a token to the root.
? Whether there are hedges in the 1st, 2nd de-
pendency relation edges or path from a token
to the root.
? The location of a token relative to the nega-
tion signal: previous the first hedge, in the
first hedge, between two hedge cues, in the
last hedge, post the last hedge.
At last, we provided a postprocessing system
for the output of the classifier to build the com-
plete sequence of tokens that constitute the scope.
We applied the following postprocessing:
? If a hedge is bracketed by a F scope and a
L scope, its scope is formed by the tokens be-
tween them.
? If a hedge is only bracketed by a F scope, and
there is no L scope in the sentence, we search
the first possible word from the end of the
sentence according to a dictionary, which ex-
tracted from the training corpus, and assign it
as L scope. The scope of the hedge is formed
by the tokens between them.
? If a hedge is only bracketed by a F scope, and
there are at least one L scope in the sentence,
we think the last L scope is the L scope of the
hedge, and its scope is formed by the tokens
between them.
? If a hedge is only bracketed by a L scope,
and there is no F scope in the sentence, we
search the first possible word from the begin-
ning of the sentence to the hedge according to
the dictionary, and assign it as F scope. The
scope of the hedge is formed by the tokens
between them.
? If a hedge is only bracketed by a L scope,
and there are at least one F scope in the sen-
tence, we search the first possible word from
the hedge to the beginning of the sentence ac-
cording to the dictionary, and think it as the
F scope of the hedge. The scope of the hedge
is formed by the tokens between them.
? If a hedge is bracketed by neither of them, we
remove it.
3 Experiments and Results
Two annotated corpus: BioScope and Wikipedia
are supplied for the CoNLL-2010 shared task. The
BioScope corpus consists of two parts: biological
paper abstracts and biological full papers, and it
is used for two subtasks. The Wikipedia corpus is
only used for hedge detection. The detailed infor-
mation of these two corpora is shown in Table 1
and Table 2, respectively.
Abstracts Papers Test
#Documents 1273 9 15
#Sentences 11871 2670 5003
%Hedge sent. 17.70 19.44 15.75
#Hedges 2694 682 1043
#AvL. of sent. 30.43 27.95 31.30
#AvL. of scopes 17.27 14.17 17.51
Table 1: The detailed information of BioScope
corpus. ?AvL.? stands for average length.
Train Test
#Documents 2186 2737
#Sentences 11111 9634
%Hedge sentences 22.36 23.19
#Hedges 3133 3143
#AvL. of sentences 23.07 20.82
Table 2: The detail information of Wikipedia cor-
pus. ?AvL.? stands for average length.
In our experiments, CRF++-0.533 implemen-
3http://crfpp.sourceforge.net/
15
tation is employed to CRF, and svm hmm 3.104
implementation is employed to the large margin
method. All parameters are default except C
(the trade-off between training error and margin,
C=8000, for selecting C, the training corpus is par-
titioned into three parts, two of them are used for
training and the left one is used as a development
dataset) in svm hmm. Both of them are state-of-
the-art toolkits for the sequence labeling problem.
3.1 Hedge Detection
We first compare the performance of each single
classifier with the cascaded system on two corpora
in domain, respectively. Each model is trained by
whole corpus, and the performance of them was
evaluated by the official tool of the CoNLL-2010
shared task. There were two kinds of measure:
one for sentence-level performance and another
one for cue-match performance. Here, we only
focused on the first one, and the results shown in
Table 3.
Corpus System Prec. Recall F1
CRF 87.12 86.46 86.79
BioScope LM 85.24 87.72 86.46
CAS 85.03 87.72 86.36
CRF 86.10 35.77 50.54
Wikipedia LM 82.28 41.36 55.05
CAS 82.28 41.36 55.05
Table 3: In-sentence performance of the hedge
detection subsystem for in-domain test. ?Prec.?
stands for precision, ?LM? stands for large mar-
gin, and ?CAS? stands for cascaded system.
From Table 3, we can see that the cascaded sys-
tem is not better than other two single classifiers
and the single CRF classifier achieves the best per-
formance with F-measure 86.79%. The reason for
selecting this cascaded system for our final sub-
mission is that the cascaded system achieved the
best performance on the two training corpus when
we partition each one into three parts: two of them
are used for training and the left one is used for
testing.
For cross-domain test, we train a cascaded clas-
sifier using BioScope+Wikipedia cropus. Table 4
shows the results.
As shown in Table 5, the performance of cross-
domain test is worse than that of in-domain test.
4http://www.cs.cornell.edu/People/tj/svm light/svm-
hmm.html
Corpus Precision Recall F1
BioScope 89.91 73.29 80.75
Wikipedia 81.56 40.20 53.85
Table 4: Results of the hedge detection for cross-
domain test. ?LM? stands for large margin, and
?CAS? stands for cascaded system.
3.2 Hedge Scope Detection
For test the affect of postprocessing for hedge
scope detection, we test our system using two eval-
uation tools: one for scope tag and the other one
for sentence-level scope (the official tool). In or-
der to evaluate our system comprehensively, four
results are used for comparison. The ?gold? is the
performance using golden hedge tags for test, the
?CRF? is the performance using the hedge tags
prediction of single CRF for test, the ?LM? is the
performance using the hedge tag prediction of sin-
gle large margin for test, and ?CAS? is the per-
formance of using the hedge tag prediction of cas-
caded subsystem for test. The results of scope tag
and scope sentence-level are listed in Table 5 and
Table 6, respectively. Here, we should notice that
the result listed here is different with that submit-
ted to the CoNLL-2010 shared task because some
errors for feature extraction in the previous system
are revised here.
HD tag Precision Recall F1
F scope 92.06 78.83 84.94
gold L scope 80.56 68.67 74.14
NONE 99.68 99.86 99.77
F scope 78.83 66.89 72.37
CRF L scope 72.52 60.50 65.97
NONE 99.56 99.75 99.65
F scope 77.25 67.57 72.09
LM L scope 72.33 61.41 66.42
NONE 99.56 99.73 99.31
F scope 77.32 67.86 72.29
CAS L scope 72.00 61.29 66.22
NONE 99.57 99.73 99.65
Table 5: Results of the hedge scope tag. ?HD?
stands for hedge detection subsystem we used,
?LM? stands for large margin, and ?CAS? stands
for cascaded system.
As shown in Table 5, the performance of
L scope is much lower than that of F scope.
Therefore, the first problem we should solve is
16
HD subsystem Precision Recall F1
gold 57.92 55.95 56.92
CRF 52.36 48.40 50.30
LM 51.06 48.89 49.95
CAS 50.96 48.98 49.95
Table 6: Results of the hedge scope in-sentence.
?HD? stands for hedge detection subsystem we
used, ?LM? stands for large margin, and ?CAS?
stands for cascaded system.
how to improve the prediction performance of
L scope. Moreover, compared the performance
shown in Table 5 and 6, about 15% (F1 of L scope
in Table 5 - F1 in Table 6) scope labels are mis-
matched. An efficient postprocessing is needed to
do F-L scope pair match.
As ?CRF? hedge detection subsystem is bet-
ter than the other two subsystems, our system
achieves the best performance with F-measure
50.30% when using the ?CRF? subsystem.
4 Conclusions
This paper presents a cascaded system for the
CoNLL-2010 shared task, which contains two
subsystems: one for detecting hedges and an-
other one for detecting their scope. Although
the best performance of hedge detection subsys-
tem achieves F-measure 86.79%, the best per-
formance of the whole system only achieves F-
measure 50.30%. How to improve it, we think
some complex features such as context free gram-
mar may be effective for detecting hedge scope.
In addition, the postprocessing can be further im-
proved.
Acknowledgments
We wish to thank the organizers of the CoNLL-
2010 shared task for preparing the datasets
and organizing the challenge shared tasks.
We also wish to thank all authors supply-
ing the toolkits used in this paper. This
research has been partially supported by the
National Natural Science Foundation of China
(No.60435020 and No.90612005), National 863
Program of China (No.2007AA01Z194) and the
Goal-oriented Lessons from the National 863 Pro-
gram of China (No.2006AA01Z197).
References
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associ-
ation for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore,
August. Association for Computational Linguistics.
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the BioNLP 2009 Workshop, pages 28?
36, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Tzong-Han Tsai, Chia-Wei Wu, and Wen-Lian Hsu.
2005. Using Maximum Entropy to Extract Biomed-
ical Named Entities without Dictionaries. In Sec-
ond International Joint Conference on Natural Lan-
guage Processing, pages 268?273.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
17
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 182?188,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Instance Level Transfer Learning for Cross Lingual Opinion Analysis
Ruifeng Xu, Jun Xu and Xiaolong Wang
Key Laboratory of Network Oriented Intelligent Computation
Department of Computer Science and Technology
Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China
{xuruifeng,xujun}@hitsz.edu.cn, wangxl@insun.hit.edu.cn
Abstract
This paper presents two instance-level transfer
learning based algorithms for cross lingual
opinion analysis by transferring useful
translated opinion examples from other
languages as the supplementary training
data for improving the opinion classifier in
target language. Starting from the union of
small training data in target language and
large translated examples in other languages,
the Transfer AdaBoost algorithm is applied
to iteratively reduce the influence of low
quality translated examples. Alternatively,
starting only from the training data in target
language, the Transfer Self-training algorithm
is designed to iteratively select high quality
translated examples to enrich the training
data set. These two algorithms are applied to
sentence- and document-level cross lingual
opinion analysis tasks, respectively. The
evaluations show that these algorithms
effectively improve the opinion analysis by
exploiting small target language training data
and large cross lingual training data.
1 Introduction
In recent years, with the popularity of Web 2.0,
massive amount of personal opinions including
comments, reviews and recommendations in dif-
ferent languages have been shared on the Internet.
Accordingly, automated opinion analysis has
attracted growing attentions. Opinion analysis, also
known as sentiment analysis, sentiment classifica-
tion, and opinion mining, aims to identify opinions
in text and classify their sentiment polarity (Pang
and Lee, 2008).
Many sentiment resources such as sentiment
lexicons (e.g., SentiWordNet (Esuli and Sebastiani,
2006))and opinion corpora (e.g., MPQA (Blitzer
et al, 2007)) have been developed on different
languages in which most of them are for English.
The lack of reliably sentiment resources is one
of the core issues in opinion analysis for other
languages. Meanwhile, the manually annotation
is costly, thus the amount of available annotated
opinion corpora are still insufficient for supporting
supervised learning, even for English. These facts
motivate to ?borrow? the opinion resources in one
language (source language, SL) to another language
(target language, TL) for improving the opinion
analysis on the target language.
Cross lingual opinion analysis (CLOA) tech-
niques are investigated to improve opinion analysis
in TL through leveraging the opinion-related
resources, such as dictionaries and annotated
corpus in SL. Some CLOA works used bilingual
dictionaries (Mihalcea et al, 2007), or aligned
corpus (Kim and Hovy, 2006) to align the expres-
sions between source and target languages. These
works are puzzled by the limited aligned opinion
resources. Alternatively, some works used machine
translation system to do the opinion expression
alignment. Banea et al (2008) proposed several
approaches for cross lingual subjectivity analysis by
directly applying the translations of opinion corpus
in source language to train the opinion classifier
on target language. Wan (2009) combined the
annotated English reviews, unannotated Chinese
reviews and their translations to co-train two
separate classifiers for each language, respectively.
182
These works directly used all of the translation of
annotated corpus in source language as the training
data for target language without considering the
following two problems: (1) the machine translation
errors propagate to following CLOA procedure; (2)
The annotated corpora from different languages are
collected from different domains and different writ-
ing styles which lead the training and testing data
having different feature spaces and distributions.
Therefore, the performances of these supervised
learning algorithms are affected.
To address these problems, we propose two
instance level transfer learning based algorithms
to estimate the confidence of translated SL ex-
amples and to transfer the promising ones as
the supplementary TL training data. We firstly
apply Transfer AdaBoost (TrAdaBoost) (Dai et
al., 2007) to improve the overall performance with
the union of target and translated source language
training corpus. A boosting-like strategy is used
to down-weight the wrongly classified translated
examples during iterative training procedure. This
method aims to reduce the negative affection of low
quality translated examples. Secondly, we propose
a new Transfer Self-training algorithm (TrStr). This
algorithm trains the classifier by using only the
target language training data at the beginning. By
automatically labeling and selecting the translated
examples which is correct classified with higher
confidence, the classifier is iteratively trained by
appending new selected training examples. The
training procedure is terminated until no new
promising examples can be selected. Differen-
t from TrAdaBoost, TrStr aims to select high
quality translated examples for classifier training.
These algorithms are evaluated on sentence- and
document-level CLOA tasks, respectively. The
evaluations on simplified Chinese (SC) opinion
analysis by using small SC training data and large
traditional Chinese (TC) and English (EN) training
data, respectively, show that the proposed transfer
learning based algorithms effectively improve the
CLOA. Noted that, these algorithms are applicable
to different language pairs.
The rest of this paper is organized as follows.
Section 2 describes the transfer learning based
approaches for opinion analysis. Evaluations and
discussions are presented in Section 3. Finally,
Section 4 gives the conclusions and future work.
2 CLOA via Transfer Learning
Given a large translated SL opinion training data,
the objective of this study is to transfer more high
quality training examples for improving the TL
opinion analysis rather than use the whole translated
training data. Here, we propose to investigate the
instance level transfer learning based approaches.
In the case of transfer learning, the set of trans-
lated training SL examples is denoted by Ts =
{(xi, yi)}ni=1, and the TL training data is denoted
by Tt={(xi, yi)}n+mi=n+1, while the size of Tt is much
smaller than that of Ts, i.e., |m| ? |n|. The idea
of transfer learning is to use Tt as the indicator to
estimate the quality of translated examples. By
appending selected high quality translated examples
as supplement training data, the performance of
opinion analysis on TL is expected to be enhanced.
2.1 The TrAdaBoost Approach
TrAdaBoost is an extension of the AdaBoost
algorithm (Freund and Schapir, 1996). It uses
boosting technique to adjust the sample weight
automatically (Dai et al, 2007). TrAdaBoost joins
both the source and target language training data
during learning phase with different re-weighting
strategy. The base classifier is trained on the
union of the weighted source and target examples,
while the training error rate is measured on the
TL training data only. In each iteration, for a SL
training example, if it is wrongly classified by prior
base classifier, it tends to be a useless examples
or conflict with the TL training data. Thus, the
corresponding weight will be reduced to decrease
its negative impact. On the contrary, if a TL training
example is wrongly classified, the corresponding
weight will be increased to boost it. The ensemble
classifier is obtained after several iterations.
In this study, we apply TrAdaBoost algorithm
with small revision to fit the CLOA task, as de-
scribed in Algorithm 1. Noted that, our revised
algorithm can handle multi-category problem which
is different with original TrAdaBoost algorithm for
binary classification problem only. More details and
theoretical analysis of TrAdaBoost are given in Dai
et al?s work (Dai et al, 2007).
183
Algorithm 1 CLOA with TrAdaBoost.
Input: Ts, translated opinion training data in SL,
n= |Ts|; Tt, training data in TL , m= |Tt|; L,
base classifier; K, iteration number.
1: Initialize the distribution of training samples:
D1(i) = 1/(n+m).
2: for each k ? [1,K] do
3: Get a hypothesis hk by training L with the
combined training set Ts ? Tt using distribu-
tion Dk: hk = L(Ts ? Tt, Dk).
4: Calculate the training error of hk on Tt:
?t =
?n+m
i=n+1
Dk(i)?I[hk(xi) ?=yi]
?n+m
i=n+1 Dk(i)
.
5: if ?t = 0 or ?k ? 1/2 then
6: K = k ? 1, break.
7: end if
8: Set ?k = ?k/(1? ?k), ? = 1/(1 +
?
2 lnn
K ).
9: if hk(xi) ?= yi then
10: Update the distribution:
Dk+1(i) =
{ Dk(i)?
Zk
1 ? i ? n
Dk(i)/?k
Zk
n + 1 ? i ? n + m
, where
Zk is a normalization constant and
?n+m
i=1 Dk+1(i) = 1.
11: end if
12: end for
Output: argmaxy
?K
?K/2?I[hk(x) = y]log(1/?k)
/* I[?] is an indicator function, which equals 1 if the
inner expression is true and 0 otherwise.*/
2.2 The Transfer Self-training Approach
Different from TrAdaBoost which focuses on the
filtering of low quality translated examples, we
propose a new Transfer Self-training algorithm
(TrStr) to iteratively train the classifier through
transferring high quality translated SL training data
to enrich the TL training data. The flow of this
algorithm is given in Algorithm 2.
The algorithm starts from training a classifier
on Tt. This classifier is then applied to Ts, the
translated SL training data. For each category in
Ts (subjective/objective or positive/negative in our
different experiments), top p correctly classified
translated examples are selected. These translated
examples are regarded as high quality ones and thus
they are appended in the TL training data. Next, the
classifier is re-trained on the enriched training data.
The updated classifier is applied to SL examples
again to select more high quality examples. Such
Algorithm 2 CLOA with Transfer Self-training.
Input: Ts, translated opinion training data in SL,
n= |Ts|; Tt, training data in TL , m= |Tt|; L,
base classifier; K, iteration number.
1: T0 = Tt, k = 1.
2: Get a hypothesis hk by training a base classifier
L with the training set Tk?1.
3: for each instance (xi, yi) ? Ts do
4: Use hk to label (xi, yi) .
5: if ht(xi) = yi then
6: Add (xi, yi)to T ?
7: end if
8: end for
9: Choose p instances per class with top confi-
dence from T ? and denote the set as Tp.
10: Tk = Tk?1
?
Tp, Ts = Ts ? Tp.
11: k = k + 1.
12: Iterate K times over steps 2 to 11 or repeat until
Tp = ?.
Output: Final classifier by using the enriched train-
ing set Tk.
procedure terminates until the increments are less
than a specified threshold or the maximum number
of iterations is exceeded. The final classifier is
obtained by training on the union of target data and
selected high quality translated SL training data.
3 Evaluation and Discussion
The proposed approaches are evaluated on sentence-
and document-level opinion analysis tasks in the
bi-lingual case, respectively. In our experiments,
the TL is simplified Chinese (SC) and the SL for
the two experiments are English (EN)/traditional
Chinese (TC) and EN, respectively.
3.1 Experimental Setup
3.1.1 Datasets
In the sentence-level opinionated sentence recog-
nition experiment , the dataset is from the NTCIR-7
Multilingual Opinion Analysis Tasks (MOAT) (Se-
ki et al, 2008) corpora. The information of
this dataset is given in Table 1. Two experi-
ments are performed. The first one is denoted by
SenOR : TC ? SC, which uses TCs as source
language training dataset, while the second one
184
is SenOR : EN ? SC, which uses ENs1. SCs
is shrunk to different scale as the target language
training corpus by random. The opinion analysis
results are evaluated with simplified Chinese testing
dataset SCt under lenient and strict evaluation
standard 2, respectively, as described in (Seki et al,
2008).
Note Lang. Data Total subjective/objectiveLenient Strict
SCs SC Training 424 130/294 \SCt Test 4877 1869/3008 898/2022
TCs TC Training 1365 740/625 \
ENs EN Training 1694 648/1046 \
Table 1: The NTCIR-7 MOAT Corpora(unit:sentence).
In the document-level review polarity classifi-
cation experiment,, we used the dataset adopted
in (Wan, 2009). Its English subset is collected by
Blitzer et al (2007), which contains a collection of
8,000 product reviews about four types of products:
books, DVDs, electronics and kitchen appliances.
For each type of products, there are 1,000 positive
reviews and 1,000 negative ones, respectively. The
Chinese subset has 451 positive reviews and 435
negative reviews of electronics products such as
mp3 players, mobile phones etc. In our experiments,
the Chinese subset is further split into two parts
randomly: TL training dataset and test set. The
cross lingual review polarity classification task is
then denoted by DocSC: EN?SC.
In this study, Google Translate3 is choose for pro-
viding machine translation results.
3.1.2 Base Classifier and Baseline Methods
This study focus on the approaches improving the
opinion analysis by using cross lingual examples,
while the classifier improving on target language is
not our major target. Therefore, in the experiments,
a Support Vector Machines (SVM) with linear
kernel is used as the base classifier. We use the
1There are only 248 sentences in NTCIR-7 MOAT English
training data set. It is too small to use for CLOA. We s-
plit some samples from the test set to build a new English
dataset for training, which contains all sentences from topics:
N01,N02,T01,N02,N03,N04,N05,N06 and N07.
2All sentences are annotated by 3 assessors, strict standard
means all 3 assessors have the same annotation and lenient
means any 2 of them have the same annotation.
3http://translate.google.com/
open source SVM package ?LIBSVM(Chang and
Lin, 2001) with all default parameters. In the
opinionated sentence recognition experiment, we
use the presences of following linguistic features
to represent each sentence example including
opinion word, opinion operator, opinion indicator,
the unigram and bigram of Chinese words. It is
developed with the reference of (Xu et al, 2008).
In the review polarity classification experiment, we
use unigram, bigram of Chinese words as features
which is suggested by (Wan, 2009). Here, document
frequency is used for feature selection. Meanwhile,
term frequency weighting is chosen for document
representation.
In order to investigate the effectiveness of the
two proposed transfer learning approaches, they
are compared with following baseline methods: (1)
NoTr(T), which applies SVM with only TL training
data; (2) NoTr(S),which applies SVM classifier with
only the translated SL training data; (3) NoTr(S&T),
which applies SVM with the union of TL and SL
training data.
3.1.3 Evaluation Criteria
Accuracy (Acc), precision (P), recall (R) and F-
measure (F1) are used as evaluation metrics. All the
performances are the average of 10 experiments.
3.2 Experimental Results and Discussion
Here, the number of iterations in TrAdaBoost is set
to 10 in order to avoid over-discarding SL examples.
3.2.1 Sentence Level CLOA Results
The achieved performance of the opinionated
sentence recognition task under lenient and strict
evaluation are given in Table 2 respectively, in
which only 1/16 target train data is used as Tt.
It is shown that NoTr(T) achieves a acceptable
accuracy, but the recall and F1 for ?subjective?
category are obviously low. For the two sub-tasks,
i.e. SenOR : TC ?SC and SenOR :EN ?SC
tasks, the accuracies achieved by NoTr(S&T) are
always between that of NoTr(T) and NoTr(S).
The reason is that some translated examples from
source language may likely conflict with the target
language training data. It is shown that the direct
use of all of the translated training data is infeasible.
It is also shown that our approaches achieve better
185
Method Sub-task
Lenient Evaluation Strict Evaluation
Acc subjective objective Acc subjective objectiveP R F1 P R F1 P R F1 P R F1
NoTr(T) .6254 .534 .3468 .355 .6824 .7985 .7115 .6922 .5259 .3900 .3791 .7725 .8264 .7776
NoTr(S)
TC
?
SC
.6059 .4911 .7828 .6035 .7861 .4960 .6082 .6448 .4576 .8352 .5912 .8845 .5603 .6860
NoTr(S&T) .6101 .4943 .7495 .5957 .7711 .5236 .6235 .6531 .4632 .8051 .588 .8714 .5856 .7004
TrAdaBoost .6533 .5335 .7751 .6314 .8063 .5777 .6720 .7184 .5273 .8473 .6494 .9077 .6611 .7643
TrStr .6625 .5448 .7309 .6238 .7884 .6199 .6934 .7304 .5414 .8182 .6511 .896 .6914 .7801
NoTr(S)
EN
?
SC
.6590 .5707 .4446 .4998 .6966 .7922 .7413 .7390 .5872 .5100 .5459 .7944 .8408 .8169
NoTr(S&T) .6411 .5292 .5759 .5515 .7212 .6817 .7009 .7105 .5254 .608 .5637 .8129 .7560 .7834
TrAdaBoost .6723 .5988 .4371 .5018 .7019 .8184 .7549 .7630 .6485 .5019 .5614 .8002 .8789 .8371
TrStr .6686 .5691 .5746 .5678 .7360 .7271 .7292 .7484 .589 .6276 .6026 .8315 .8021 .8147
Table 2: The Performance of Opinionated Sentence Recognition Task.
performance on both tasks while few TL training
data is used. In which, TrStr performances the
best on SenOR:TC?SC task while TrAdaBoost
outperforms other methods on SenOR :EN?SC
task. The proposed transfer learning approaches
enhanced the accuracies achieved by NoTr(S&T)
for 4.2-8.6% under lenient evaluation and 5.3-11.8%
under strict evaluation, respectively.
3.2.2 Document Level CLOA Results
Method Acc positive negativeP R F1 P R F1
NoTr(T) .7542 .7447 .8272 .7747 .8001 .6799 .7235
NoTr(S) .7122 .6788 .8248 .7447 .7663 .5954 .6701
NoTr(S&T) .7531 .714 .8613 .7801 .8187 .6415 .7179
TrAdaBoost .7704 .8423 .6594 .7376 .7285 .8781 .7954
TrStr .7998 .8411 .7338 .7818 .7727 .8638 .8144
Table 3: The Results of Chinese Review Polarity Classi-
fication Task (Features:Unigrams; m=20).
Method Acc positive negativeP R F1 P R F1
NoTr(T) .7518 .7399 .8294 .7741 .7983 .6726 .7185
NoTr(S) .7415 .7143 .8204 .7637 .7799 .6598 .7148
NoTr(S&T) .7840 .7507 .8674 .8035 .8385 .6982 .7592
TrAdaBoost .7984 .8416 .7297 .7792 .7707 .8652 .8138
TrStr .8022 .8423 .7393 .7843 .7778 .8634 .8164
Table 4: The Results of Chinese Review Polarity Classi-
fication Task (Features:Unigrams+Bigrams; m=20).
Table 3 and Table 4 give the achieved results of
different methods on the task DocSC : EN?SC
by using 20 Chinese annotated reviews as Tt. It is
shown that transfer learning approaches outperform
other methods, in which TrStr performs better than
TrAdaBoost when unigram+bigram features are
used. Compared to NoTr(T&S), the accuracies
are increased about 1.8-6.2% relatively. Overall,
the transfer learning approaches are shown are
beneficial to TL polarity classification.
3.2.3 Influences of Target Training Corpus Size
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(a) SenOR : TC ? SC
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(b) SenOR : EN ? SC
Figure 1: Performances with Different Size of SCs on
Opinionated Sentence Recognition Task under Lenient E-
valuation
In order to estimate the influence of different size
of TL training data, we conduct a set of experiments
on both tasks. Fig 1 and Fig 2 show the influence
186
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  20  30  40  50  60  70  80  90  100
Ac
cu
ra
cy
Number of Target Training Instances
Transfer Self-training
TrAdaBoost
NoTr(S&T)
NoTr(T)
(a) Unigrams
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  20  30  40  50  60  70  80  90  100
Ac
cu
ra
cy
Number of Target Training Instances
Transfer Self-training
TrAdaBoost
NoTr(S&T)
NoTr(T)
(b) Unigrams+Bigrams
Figure 3: Performances with Different Number of TL Training Instances on Task of DocSC: EN?SC
 0.6
 0.65
 0.7
 0.75
 0.8
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(a) SenOR : TC ? SC
 0.6
 0.65
 0.7
 0.75
 0.8
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(b) SenOR : EN ? SC
Figure 2: Performances with Different Size of SCs on
Opinionated Sentence Recognition Task under Strict E-
valuation
on the opinionated sentence recognition task under
lenient and strict evaluation respectively. Fig 3
shows the influence on task DocSC : EN ?SC.
Fig 3(a) shows the results use unigram features
and Fig 3(b) uses both unigrams and bigrams. It is
observed that TrAdaBoost and TrStr achieve better
performances than the baseline NoTr(S&T) in most
cases. More specifically, TrStr performs the best
when few TL training data is used. When more TL
training data is used, the performance improvements
by transfer learning approaches become small. The
reason is that less target training data is helpful to
transfer useful knowledge in translated examples.
If too much TL training data is used, the weights
of SL instances may decrease exponentially after
several iterations, and thus more source training
data is not obviously helpful.
4 Conclusions and Future Work
To address the problems in CLOA caused by inac-
curate translations and different domain/category
distributions between training data in different
languages, two transfer learning based algorithms
are investigated to transfer promising translated SL
training data for improving the TL opinion analysis.
In this study, Transfer AdaBoost and Transfer
Self-Training algorithms are investigated to reduce
the influences of low quality translated examples
and to select high quality translated examples,
respectively. The evaluations on sentence- and
document-level opinion analysis tasks show that the
proposed algorithms improve opinion analysis by
using the union of few TL training data and selected
cross lingual training data.
One of our future directions is to develop other
transfer leaning algorithms for CLOA task. Another
future direction is to employ other moderate weight-
ing scheme on source training dataset to reduce the
over-discarding of training examples from source
language.
187
References
Bo Pang and Lillian Lee. 2008. Opinion mining
and sentiment analysis. Foundations and Trends
in Information Retrieval, 2(1?2):1?135.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource
for opinion mining. Proceedings of the 5th Inter-
national Conference on Language Resources and
Evaluation, 417?422.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and e-
motions in language. Language Resources and E-
valuation, 39(2?3):165?210.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. Proceedings of the
45th Annual Meeting of the Association of Com-
putational Linguistics, Prague, Czech Republic.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. Proceedings of
HLT/NAACL-2006, 200?207.
Carmen Banea, Rada Mihalcea, Janyce Wiebe and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, Honolulu, Hawaii,
127?135.
Xiaojun Wan. 2009. Co-training for cross-lingual
sentiment classification. Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP
of the AFNLP, Suntec, Singapore, 235?243.
Wenyuan Dai ,Qiang Yang, GuiRong Xue and Yong
Yu. 2007. Boosting for transfer learning. Pro-
ceedings of the 24th International Conference on
Machine Learning, 193?200.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: domain adaptation for sentiment classi-
fication. Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics,
440?447.
Xiaojun Wan. 2008. Using bilingual knowledge
and ensemble techniques for unsupervised chi-
nese sentiment analysis. Proceedings of EMNLP
2008,553?561.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. Proceedings
of the 13th International Conference on Machine
Learning, 148?156.
Yohei Seki, David K. Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, and Noriko Kand. 2008. Overview
of multilingual opinion analysis task at NTCIR-7.
Proceeding of NTCIR-7, NII, Tokyo, 185?203.
Ruifeng Xu, Kam-Fai Wong, Qin Lu, and Yunqing
Xia 2008. Learning Multilinguistic Knowledge
for Opinion Analysis. D. S. Huang et al, edi-
tors:Proceedings of ICIC 2008, volume 5226 of L-
NCS, 993?1000, Springer-Verlag.
Chih-Chung Chang and Chih-Jen Lin.
2001. LIBSVM: a library for support
vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
188
