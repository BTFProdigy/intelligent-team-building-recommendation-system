Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 481?488
Manchester, August 2008
Classifying What-type Questions by Head Noun Tagging 
Fangtao Li, Xian Zhang, Jinhui Yuan, Xiaoyan Zhu 
State Key Laboratory on Intelligent Technology and Systems 
Tsinghua National Laboratory for Information Science and Technology  
Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China 
zxy-dcs@tsinghua.edu.cn 
 
Abstract 
Classifying what-type questions into 
proper semantic categories is found more 
challenging than classifying other types 
in question answering systems.  In this 
paper, we propose to classify what-type 
questions by head noun tagging. The ap-
proach highlights the role of head nouns 
as the category discriminator of what-
type questions. To reduce the semantic 
ambiguities of head noun, we integrate 
local syntactic feature, semantic feature 
and category dependency among adjacent 
nouns with Conditional Random Fields 
(CRFs). Experiments on standard ques-
tion classification data set show that the 
approach achieves state-of-the-art per-
formances. 
1 Introduction 
Question classification is a crucial component of 
modern question answering system. It classifies 
questions into several semantic categories which 
indicate the expected semantic type of answers to 
the questions. The semantic category helps to 
filter out irrelevant answer candidates, and de-
termine the answer selection strategies.   1 
The widely used question category criteria is a 
two-layered taxonomy developed by Li and Roth 
(2002) from UIUC. The hierarchy contains 6 
coarse classes and 50 fine classes as shown in 
Table 1. In this paper, we focus on fine-category 
classification. Each fine category will be denoted 
as ?Coarse:fine?, such as ?HUM:individual?. 
A what-type question is defined as the one 
whose question word is ?what?, ?which?, 
?name? or ?list?. It is a dominant type in ques-
tion answering system. Li and Roth (2006) find  
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
Coarse Fine 
ABBR abbreviation, expression 
DESC definition, description, manner, reason 
ENTY animal, body, color, creation, currency, dis-
ease/medicine, event, food, instrument, language, 
letter, other, plant, product, religion, sport, sub-
stance, symbol, technique, term, vehicle, word 
HUM description, group, individual, title 
LOC city, country, mountain, other, state 
NUM code, count, date, distance, money, order, other, 
percent, period, speed, temperature, size, weight 
Table 1.  Question Ontology 
that the distribution of what-type questions over 
the semantic classes is quite diverse, and they are 
more difficult to be classified than other types.  
Table 2 shows the classification accuracies of 
each question word in UIUC data set using Sup-
port Vector Machine (SVM) with unigram fea-
tures. What-type questions account for more than 
70 percent in the data set, but the classification 
accuracy of this type only achieves 75.50%. In 
this experiment, 90.53% (86 over 95) of the er-
rors are generated by what-type questions. Due to 
its challenge, this paper focuses on what-type 
question classification. 
 
 Total Wrong Accuracy 
What-type 351 86 75.50% 
Where 26 2 92.31% 
When 26 0 100.0% 
Who 47 3 93.62% 
How 46 4 91.30% 
Why 4 0 100.0% 
Total 500 95 81.00% 
Table 2.  Classification performance for each 
question words with unigram 
Head noun has been presented to  play an im-
portant role in classifying what-type questions 
(Metzler and Croft, 2005).  It refers to the noun 
reflecting the focus of a question, such as 
?flower? in the question ?What is Hawaii's state 
481
flower??. These nouns can effectively reduce the 
noise generated by other words. If the head noun 
?length? is identified from the question ?What is 
the length of the coastline of the state of 
Alaska??, this question can be easily classified 
into ?NUM:distance?. However, the above SVM 
misclassified this question into ?LOC:-state?, as 
the words ?state? and ?Alaska? confused the 
classifier. Considering another two questions 
expressed in (Zhang and Lee, 2002), ?Which 
university did the president graduate from?? and 
?Which president is a graduate of the Harvard 
University?, although they contain similar words, 
it is not difficult to distinguish them with the 
head nouns ?university? and ?president? respec-
tively. 
Nevertheless, a head noun may correspond to 
several semantic categories. In this situation, we 
need to incorporate the head noun context for 
disambiguation. The potentially useful context 
features include local syntactic features, semantic 
features and neighbor?s semantic category. Take 
the noun ?money? as an example, it possibly cor-
responds to two categories: ?NUM:money? and 
?ENTY:currency?. If there is an adjacent word 
falling into ?Loc:country? category, the ?money? 
tends to belong to ?ENTY:currency?. Otherwise, 
if the ?ENTY:product? or ?HUM:individual? 
surrounds it, the word ?money? may refer to 
?NUM:money?. 
Based on the above notions, we propose a new 
strategy to classify what-type questions by word 
tagging, and the selected head noun determines 
question category.  The question classification 
task is formulated into word sequence tagging 
problem. All the question words are divided into 
semantic words and non-semantic words. The 
semantic word expresses certain semantic cate-
gory, such as ?dog? corresponding to category 
?ENTITY:animal?, while ?have? corresponding 
to no category. The label for semantic words is 
one of the question categories, and ?O? is for 
non-semantic word. Here, we just consider the 
nouns as semantic words, others as non-semantic 
words. Each word in a question will be tagged as 
a label using Conditional Random Fields model, 
and the head noun?s label is chosen as the ques-
tion category.  
In conclusion, the CRFs based approach has 
two main steps: the first step is to tag all the 
words in questions using CRFs, and the second 
step is choosing the head noun?s label as the 
question category. It can use the head noun to 
eliminate the noisy words, and take advantages 
of CRFs model to integrate not only the syntactic 
and semantic features, but also the adjacent cate-
gories to tag head noun. 
The rest of this paper is organized as follows: 
Section 2 discusses related work. Section 3 in-
troduces the Condition Random Fields(CRFs) 
and the defined Long-Dependency CRFs 
(LDCRFs). Section 4 describes the features used 
in the LDCRFs. The head noun extraction me-
thod is presented in Section 5. We evaluate the 
proposed approach in Section 6. Section 7 con-
cludes this paper and discusses future work. 
2 Related works 
Question Answering Track was first introduced 
in the Text REtrieval Conference (TREC) in 
1999. Since then, question classification has been 
a popular topic in the research community of text 
mining. Simple question classification approach-
es usually employ hand-crafted rules (such as 
Prager et. al, 1999), which are effective for spe-
cific question taxonomy. However, laborious 
human effort is required to create these rules. 
Some other systems employed machine learn-
ing approaches to classify questions.  Li and 
Roth (2002) presented a hierarchical classifier 
based on the Sparse Network of Winnows (Snow) 
architecture. Tow classifiers were involved in 
this work: the first one classified questions into 
the coarse categories; and the other classified 
questions into fine categories. Several syntactic 
and semantic features, including semi-
automatically constructed class-specific relation-
al features, were extracted and compared in their 
experiments. The results showed that the hierar-
chical classifier was effective for question classi-
fication task.  
Metzler and Croft (2005) used prior know-
ledge about correlations between question words 
and types to train word-specific question classifi-
ers. They identified the question words firstly, 
and trained separate classifier for each question 
word. WordNet was used as semantic features to 
boost the classification performance. In this pa-
per, according to question word, all the questions 
are classifie into two categories: what-type ones 
and non-what-type one. 
Recent question classification methods have 
paid more attention on the syntactic structure of 
sentence. They used a parser to get the syntactic 
tree, and then took advantage of the structure 
information. Zhang and Lee (2002) proposed a 
tree kernel Support Vector Machine classifier 
and experiment results showed that syntactic in-
formation and tree kernel could solve this prob-
482
lem. Nguyen et al (2007) proposed a subtree 
mining method for question classification. They 
formulated question classification as tree catego-
ry determination, and maximum entropy and 
boosting model with subtree features were used. 
The experiment results showed that the subtree 
mining method can achieve a higher accuracy in 
question classification task.  
In this paper, we formulate the what-type 
question classification as word sequence tagging 
problem. The tagged label is either one of the 
question categories for nouns s or ?O? for other 
words. Since head noun can be the discriminator 
for a question, its tag is extracted as the question 
category in our work. A long-dependency Condi-
tional Random Fields Classifier is defined to tag 
question words with the features which not only 
include the syntactic and semantic features, but 
also the semantic categories? transition features. 
3 Conditional Random Fields 
Conditional Random Fields (CRFs) are a type of 
discriminative probabilistic model proposed for 
labeling sequential data (Lafferty et al 2001). Its 
definition is as follows:      
Definition: Let ( )G V E= ,  be a graph such that 
( )v v VY ?=Y , so that Y  is indexed by the vertices of G . 
Then ( ),X Y  is a conditional random field in case, when 
conditioned on X , the random variables vY  obey the Mar-
kov property with respect to the 
graph: ( )v wp Y Y w v| , , ? =X ( )v wp Y Y w v| , ,X ? , where w v?  
means that w  and v  are neighbors in G . 
   
The joint distribution over the label sequence Y  
given X  has the form  
1
( ) exp ( ) ( )
( ) i i i ie E i v V i
p t e e s v v
Z
? ?
? , ? ,
| = , | , + , | , ,? ?? ?? ?? ?Y X Y X Y XX
where ( )Z X  is a normalization factor, is  is a 
state feature function and it  is a transition fea-
ture function, i?  and i?  are the corresponding 
weights. 
 Here we assume the features are given, then 
the parameter estimation problem is to determine 
the parameters 1 2 1 2( , )? ? ? ? ?= , , , ,? ?  from 
training data. The inference problem is to find 
the most probable label sequence y?  for input 
sequence x . 
  In the training set, we label all the noun 
words with semantic question categories, and 
other words will be labeled by ?O?. We suppose 
that only adjacent noun words connect with each 
other, and there is no edge between noun and 
non-noun words, i.e., noun word and non-noun 
words may share neighbor?s state features, but 
they are not connected by an edge. A labeled ex-
ample is shown as ?What/O was/O 
Queen/HUM:individual Victria/HUM:individual 
?s/O title/HUM:title regarding/O India/LOC:-
country?. In this labeled sentence, only three 
edges connect four noun words: Queen, Victria, 
title and India.  
 
Figure 1. Long-Dependency CRFs, the dotted 
lines summarize many outgoing edges 
 
  With this assumption, we define a Long-
Dependency Conditional Random Fields 
(LDCRFs) model (see Figure 1). The long de-
pendency means that the target words may have 
no edge with its neighbors, but connect with oth-
er words at a long distance. It can be considered 
as a type of linear- chain CRFs.  Its parameter 
estimation problem and inference problem can be 
solved by the algorithm for chain-structure CRFs 
(Sutton and McCallum, 2007). 
4 Feature Sets 
One of the most attractive advantages  of CRFs is 
that they can integrate rich features, including 
not only state features, but also transition fea-
tures. In this section, we will introduce the syn-
tactic, semantic and transition features used in 
our sequence tagging approach. 
4.1 Syntactic Features 
The questions, which have similar syntactic style, 
intend to belong to the same category. Besides 
words, part-of-speech, chunker, parser informa-
tion and question length are used as syntactic 
features. 
All the words are lemmatized to root forms, 
and a window size (here is 4) is set to utilize the 
surrounding words. 
The part-of-speech (POS) tagging is com-
pleted by SS Tagger (Tsuruoka and Tsujii, 2005), 
with our own improvement. 
The noun phrase chunking (NP chunking) 
module uses the basic NP chunker software from 
483
(Ramshaw and Marcus, 1995) to recognize the 
noun phrases in the question.  
The importance of question syntactic structure 
is reported in (Zhang and Lee, 2002; Nguyen et 
al. 2007). They used complex machine learning 
method to capture the tree architecture. The 
LDCRFs based approach just selects parent node, 
relation with parent and governor for each target 
word generated from Minipar(Lin, 1999). 
The length of question is another important 
syntactic feature. In our experiment, a threshold 
is set to denote the length as ?high? or ?low?. 
4.2 Semantic Features 
Semantic features concern what words mean and 
how these meanings combine in sentence to form 
sentence meanings. Named Entity is a predefined 
semantic category for noun word. WordNet 
(Fellbaum, 1998) is a public semantic lexicon for 
English language, and it is used to get hypernym 
for noun word and synset for head verb which is 
the first notional verb in the sentence. 
   Named Entity: Named entity recognizer as-
signs a semantic category to the noun phrase. It 
is widely used to provide semantic information in 
text mining. In this paper, Stanford Named Entity 
Recognizer (Finkel et al 2005) is used to classify 
noun phrases into four semantic categories: 
PERSON, LOCATION, ORGANIZARION and 
MISC. 
Noun Hypernym: Hypernyms can be consi-
dered as semantic abstractions. It helps to narrow 
the gap between training set and testing set. For 
example, ?What is Maryland's state bird??, if we 
recursively find the bird?s hypernym ?animal?, 
which appeared in training set, this question can 
be easily classified. 
In training set, we try to select appropriate 
hypernyms for each category. An correct Word-
Net sense is first assigned for each polysemous 
noun, and then all its hypernyms are recursively 
extracted. The sense determination step is 
processed with the algorithm in (Pedersen et al 
2005). They disambiguate word sense by assign-
ing a target word the sense, which is most related 
to the senses of its neighboring words.  
Since the word sense disambiguation method 
has low performance, with F1-measure below 
50% reported in (Pedersen et al 2005), a feature 
selection method is used to extract the most dis-
criminative hypernyms. The hypernyms selection 
method is processed as follows: we first remove 
the low frequency hypernyms, and select the 
hypernyms using a chi-square method. The chi-
square value measures the lack of independence 
between a hypernym h and category jc . It is de-
fined as: 
2
2 ( ) ( )( , )
( ) ( ) ( ) ( )j
A B C D AD CB
h c
A C B D A B C D
? + + + ? ?= + ? + ? + ? +  
where A is the number of hypernym h, which 
belongs to category jc ; B is the number of  h out 
of jc ; C is the number of other hypernyms in jc ; 
D is the number of other hypernyms out of jc . 
We set a threshold to select the most discri-
minative hypernym set. Extracted examples are 
shown in Figure 2.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Examples of extracted hypernym  
 
It can be seen that these hypernyms are appro-
priate to describe the semantic meaning of the 
category.  They are expected to work as the 
class-specific relational features which are semi-
constructed by (Li and Roth, 2002). In our ap-
proach, we just use the noun?s minimum upper 
hypernym, existing in training set, as the feature. 
Head Verb Synset:  To avoid losing question 
verb information, we extract head verb, which is 
the first notional verb in a question, and expand 
it using WordNet synset as feature. The head 
verb extraction is based on the following simple 
rules: 
If the first word is ?name? or ?list?, the head 
verb will be denoted as this word. If the first verb 
following question word is ?do? or other aux-
iliary verb, the next verb is extracted as head 
verb. Otherwise the head verb is extracted as the 
first verb after question word. 
4.3 Transition Features 
State transition feature captures the contextual 
constraints among labels. We define it as  
( 1 ) ( )y yt e i i e e y y??, ?=< , + >, | , = | =< , > .Y X Y
ENTITY:animal: 
animal, carnivore, chordate, equine, 
horse, living_thing, vertebrate, mammal, 
odd-toed_ungulate, organism, placental  
ENTITY:food: 
alcohol, beer, beverage, brew, cereal, 
condiment, crop, drink, drug_of_abuse, 
flavorer, food, foodstuff, helping, indefi-
nite_quantity, ingredient, liquid, output, 
produce, small_indefinite_quantity, pro-
duction, solid, substance, vegetable 
484
Where e represents the edge between adjacent 
nouns. It captures adjacent categories as features 
to tag the target noun. Note that, for simplicity, 
the value of above feature is independent of the 
observations X.  
5 Head Noun Extraction 
After tagging all the words in a question, we will 
extract head noun and assign its tagged label to 
the question as the final question classification 
result. 
The head noun extraction is a simple heuristic 
method inspired by  (Metzler and Croft, 2005). 
We first run a POS tagger on each question, and 
post-process them to make sure that each sen-
tence has at least one noun word. Next, the first 
NP chunk after the question word is extracted by 
shallow parsing. The head noun is determined by 
the following heuristic rules: 
1. If the NP chunker is before the first verb, 
or the NP chunk is after the first verb but 
there is no possessive case after the NP 
chunker, we mark the rightmost word in 
the chunker as head noun. 
2. Otherwise, extract the next NP chunker 
and recursively process the above rules. 
Although this method may depend on the per-
formance of POS tagger and shallow parser, it 
achieves the accuracy of over 95% on the UIUC 
data set in our implementation. 
6 Experiments 
6.1 Experiment Settings 
Data Set: 
We evaluate the proposed approach on the 
UIUC data set (Li and Roth, 2002). 5500 ques-
tions are selected for training, and 500 questions 
are selected for testing. The classification catego-
ries have been introduced as question ontology   
in section 1. This paper only focuses on 50 fine 
classes. 
To train the LDCRFs, we manually labeled all 
the noun words with one of 50 fine categories.  
Other words are labeled with ?O?. One of the 
labeled examples is ?What/O was/O 
Queen/HUM:individual Victria/HUM:individual 
?s/O title/HUM:title regarding/O In-
dia/LOC:country?. Ten people labeled 3407 
what-type questions as training set. Each ques-
tion was independently annotated by two people 
and reviewed by the third. For words which have 
more than one category, the annotators selected 
the most salient one according to the context. For 
testing set, 351 what-type questions were se-
lected for experiments evaluation. 
Evaluation metric: 
Accuracy performance is widely used to evaluate 
question classification methods [Li and Roth, 
2002; Zhang and Lee, 2003, Melter and Croft, 
2004; Nguyen et al 2007].   
6.2 Approach Performance Evaluation 
 # Wrong Accuracy 
SVM 86 75.50% 
LDCRFs-
based 
80 77.20% 
Table 3. LDCRFS-based Approach V.S. SVM 
Table 3 shows the compared results between 
the proposed LDCRFs based approach and SVM 
with unigram feature. The LDCRFs based ap-
proach achieves accuracy of 77.20%, compared 
with 75.50% of SVM. Observing the detailed 
classification results, we conclude two advantag-
es of LDCRFs over SVMs. First LDCRFs based 
approach focuses on head noun to reduce the 
noise generated by other words. The question 
?What is the length of the coastline of the state of 
Alaska?? is misclassified as ?LOC:state? by 
SVM, whereas it is correctly classified by our 
approach. Second, LDCRFs based approach can 
utilize rich features, including not only state fea-
tures, but also transition features. With the new 
features involved, LDCRFs is expected to im-
prove classification performance. This unigram 
result is used as our baseline. The following ex-
periments are conducted to test the new feature 
contribution. 
Syntactic Features: 
In addition to words, four types of features, in-
cluding part-of-speech (POS), chunker, parser 
information (Parser), and question length 
(Length), are extracted as syntactic features. 
 
 Accuracy 
Unigram (U) 77.20% 
U+POS 78.35% 
U+Chunker 77.20% 
U+Parser 79.20% 
U+Length 77.49% 
Total Syn 80.06% 
Table 4. Syntactic Feature Performance 
From the syntactic feature results in Table 4, 
we can draw the following conclusions?  
(a). Among four types of syntactic features, pars-
485
er information contributes mostly. (Metzler and 
Croft, 2005) once claimed that it didn?t make 
improvement by just incorporating these infor-
mation as explicit feature, and they should be 
used implicitly via a tree structure. Without using 
the complex tree mining and representing tech-
nique, our LDCRFs-based approach just incorpo-
rates the word parent, relation with parent and 
word governor from Minipar as features. The 
experiments show that the parser information 
feature is able to capture the syntactic structure 
information, and it makes much improvement in 
this sequence tagging approach. 
(b) Question length makes small improvement. 
However, the chunker features make no im-
provement, consistent with the observation re-
ported by (Li and Roth, 2006). 
? The best accuracy (80.06%) is achieved by 
integrating all the syntactic features.  
Semantic Features: 
 
 Accuracy 
Unigram(U) 77.20% 
U+NE 77.20% 
U+HVSyn 78.63% 
U+NHype 78.35% 
Total Sem 80.06% 
Table 5. Semantic Feature Performance 
The semantic features include Named Entity 
(NE), Noun Hypernym (NHype) and Head Verb 
Synset (HVSyn).  
From Table 5 we can draw the following conclu-
sions: 
(a) NE makes no improvement in classification 
task. The reason is that the named entity recog-
nizer contains only four semantic categories. It is 
too coarse to distinguish 50 fined-categories. 
 (b) The LDCRFs-based approach just considers 
the noun words as semantic words. The head 
verb synsets (HVSyn) are imported as one of 
semantic features. The experiment results show 
that it is effective to incorporate the head verb as 
features, which achieves the best individual accu-
racy among semantic features. 
(c) Noun hypernyms (NHype) are the most im-
portant semantic features. They narrow the se-
mantic gap between training set and testing set. 
From Section 4.2, we can see that the selected 
noun hypernyms are appropriate for each catego-
ry. While, the experiment with NHype features 
doesn?t make considerable improvement as we 
previously thought. The reason may come from 
the fact that the word sense disambiguation me-
thod has low performance. A hypernym selection 
method is used in training set, but we didn?t 
tackle the error in testing set. Once the word 
sense disambiguation is wrong, it will not make 
improvement, but generate noise (see the discus-
sion examples in next section).  
(d) It is an interesting result that using all the se-
mantic features can achieve the same accuracy as 
the syntactic features (80.06%).  
Feature Combination: 
In this section, we carry out experiments to ex-
amine whether the performance can be boosted 
by integrating syntactic features and semantic 
features. Several results are shown in Table 6. 
The experiments show that: 
(a)  Parser Information and Head Verb Synset are 
both the most contributive features for syntactic 
set and semantic feature set. While the perfor-
mance with these two features can?t beat the per-
formance by combining Parser Information and 
Noun Hypernyms.  
 
 Accuracy 
U+POS+NE+HVSyn 80.91% 
U+Parser+NHype 81.77% 
U+Parser+HVSyn 80.91% 
U+POS+Length+NHype 80.63% 
Total 82.05% 
Table 6. Combined Feature Performance 
(b) The best result for classifying what-type 
questions with our approach is achieved by inte-
grating all the features. The accuracy is 82.05%, 
which is 18.7 percent error reduction (from 
22.08% to 17.95%) over unigram feature set. It 
shows that the features we extract are effectively 
used in our CRFs based approach. 
Transition Feature:  
Transition feature can capture the information 
between adjacent categories. It offers another 
semantic feature for LDCRFs-based approach.  
 
 No transition 
features  
With transi-
tion features 
Syn 79.20% 80.06% 
Sem 79.49% 80.06% 
Total 81.48% 82.05% 
Table 7. Transition Feature Performance 
The performances of all these three experi-
ment decline without the transition features. It 
shows that the dependency between adjacent se-
486
mantic categories contributes to the classifier 
performance.  
6.3 System Performance Comparison and 
Discussion  
In this section, the what-type questions and non-
what-type questions are combined to show the 
final result. Non-what-type questions are classi-
fied using SVM with unigrams as reported in 
Section 1, and what-type questions are classified 
by the LDCRFs based approach. The combined 
results are used to compare with the current 
question classification methods. 
 
Classifier Accuracy
Li?s Hierarchical method 84.20% 
Nguyen?s tree method 83.60% 
Metzler?s U+ WordNet method 82.20% 
LDCRFs-based with U+Parser 83.60% 
LDCRFs-based with U+NHype 83.00% 
LDCRFs-based with total features 85.60% 
Table 8. Comparison with related work 
Table 8 shows the accuracies of the LDCRFs 
based question classification approach with dif-
ferent feature sets, in comparison with the tree 
method (Nguyen et al 2007), the WordNet Me-
thod (Metzler and Croft, 2005) and the hierarchical 
method (Li and Roth, 2002). We can see the 
LDCRFs-based approach is effective: 
(a) Without formulating the syntactic structure as 
a tree, the LDCRFs-based approach still achieves 
accuracy 83.60% with unigram and parser infor-
mation, which is the same as Nguyen?s tree clas-
sifier.  
(b) Although the LDCRFs-based approach with 
unigrams and Noun Hypernyms generates 
noise as described in Section 6.2, it still out-
performs the Metzler?s method using WordNet 
and unigram features (83.00% v.s. 82.20%).  
(c) The experiment with total features achieves 
the accuracy of 85.60%. It outperforms Li?s Hie-
rarchical classifier, even they use semi-automatic 
constructed features.  
 
6.3.1 Analysis and Discussion  
Even the sequence tagging model achieves high 
accuracy performance, there still exists many 
problems. We use the matrix defined in Li and 
Roth (2002) to show the performance errors. The 
metric is defined as follows: 
*2 /( )ij i j i jD Err N N= +   
Where i jErr  is the number of questions in class 
i that are misclassified as belong to class j, Ni 
and Nj are the numbers of questions in class i and 
j respectively.  
From the matrix in Figure 3, we can see two 
major mistake pairs are ?ENTY:substance? and 
?ENTY:other?, ?ENTY:currency? and 
?NUM:money?. They really have similar mean-
ings, which confuses even human beings.  
 
 
Figure 3. The gray-scale map of Matrix D[n,n]. The 
gray scale of the small box in position [i,j] denotes 
D[i,j]. The larger Dij is, the darker the color is. 
 
Several factors influence the performance: 
(a) Head noun extraction error: This error is 
mainly caused by errors of POS tagger and shal-
low parser. For the wrong POS example 
?what/WP hemisphere/EX is/VBZ the/DT Phil-
ippines/NNPS in/IN ?/.?, ?Philippines? is ex-
tracted as head word. The result is misclassified 
into ?LOC:country?. For the shallow parser error 
example ?what/WP/B-NP is/VBZ/B-VP the/D 
T/BNP speed/NN/I-NP humminbirds/NNS /I-NP 
fly/V- BP/B-VP ?/./O?, ?hummingbirds? is ex-
tract as head word, rather than ?speed?. The 
question is misclassified into ?ENTY:animal?. 
(b) WordNet sense disambiguation errors: In 
question ?What is the highest dam in the U.S. ?? 
The real sense for dam is dam#1: a barrier con-
structed to contain the flow of water or to keep 
out the sea; while the disambiguation method 
determine the second sense as dam#2: a metric 
unit of length equal to ten meters.  
(c) Lack of head nouns: the CRFs based ap-
proach is sensitive to the Head Noun. If the ques-
tion doesn?t contain the head noun, it is difficult 
to produce the correct result, such as the question 
?What is done with worn or outdated flags?? In 
the future work, we will focus on the head noun 
absence problem. 
487
7 Conclusion 
In this paper, we propose a novel approach with 
Conditional Random Fields to classify what-type 
questions. We first use the CRFs model to label 
all the words in a question, and then choose the 
label of head noun as the question category.  As 
far as we know, this is the first trial to formulate 
question classification into word sequence tag-
ging problem. We believe that the model has two 
distinguished advantages: 
1. Extracting head noun can eliminate the noise 
generated by the non-head words 
2. The Conditional Random Fields model can 
integrate rich features, including not only the 
syntactic and semantic features, but also the 
transition features between labels.  
Experiments show that the LDCRFs-based ap-
proach can achieve comparable performance to 
those of the state-of-the-art question answering 
systems. With the addition of more features, the 
performance of the LDCRFs based approach can 
be expected to be further improved. 
Acknowledgement 
This work is supported by National Natural 
Science Foundation of China (60572084, 
60621062), Hi-tech Research and Development 
Program of China (2006AA02Z321), National 
Basic Research Program of China 
(2007CB311003).  Thank Shuang Lin and Jiao 
Li for revising this paper. Thanks for the review-
ers? comments. 
 References 
Christiane Fellbaum. 1998. WordNet: an Electronic 
Lexical Database. MIT Press. 
Prager, J., D. Radev, E. Brown, A. Coden, and V. 
Samn. 1999. `The use of predictive annotation for 
question answering in TREC'. In: Proceedings of 
the 8th Text Retrieval Conference (TREC-8). 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Da-
ta. In Proceedings of ICML-2001. 
Li, X. and D. Roth. 2002. Learning question classifi-
ers. In Proceedings of the 19th International Confe-
rence on Compuatational Linguistics (COLING), 
pages 556?562. 
Zhang, D. and W. Lee. 2003. Question classification 
using support vector machines. In Proceedings of 
the 26th Annual International ACM SIGIR confe-
rence, pages 26?32 
Donald Metzler and W. Bruce Croft. 2004. Analysis 
of Statistical Question Classfication for Fact-based 
Questions. In Journal of Information Retrieval. 
Ted Pedersen, Satanjeev Banerjee, and Siddharth 
Patwardhan . 2005. Maximizing Semantic Related-
ness to Perform Word Sense Disambiguation. Uni-
versity of Minnesota Supercomputing Institute        
Research Report UMSI 2005/25, March. 
Xin Li, Dan Roth. 2006. Learning Question Classifi-
ers: The Role of Semantic Information. In Natural 
Language Engineering, 12(3):229-249 
Minh Le Nguyen, Thanh Tri Nguyen and Akira Shi-
mazu. 2007. Subtree Mining for Question Classifi-
cation Problem. In Proceedings of the 20th Interna-
tional Conference on Artificial Intelligence. Pages 
1695-1700. 
C. Sutton and A. McCallum. 2007. An introduction to 
conditional random fields for relational learning. 
In L. Getoor and B. Taskar (Eds.). Introduction to 
statistical relational learning. MIT Press. 
Y. Tsuruoka and J. Tsujii,. 2005. Bidirectional infe-
rence with the easiest-first strategy for tagging se-
quence data. In Proc. HLT/EMNLP?05, Vancouver, 
October, pp. 467-474. 
L. Ramshaw and M. Marcus. 1995. Text chunking 
using transformation-based learning, Proc. 3rd 
Workshop on Very Large Corpora, pp. 82?94. 
J.R. Finkel, T. Grenager and C. Manning. 2005. In-
corporating non-local information into information 
extraction systems by Gibbs sampling. Proc. 43rd 
Annual Meeting of ACL, pp. 363?370. 
D. Lin. 1999. MINIPAR: a minimalist parser. In Mar-
yland Linguistics Colloquium, University of Mary-
land, College Park. 
 
 
 
488
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 545?552,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Comparison and Semi-Quantitative Analysis of Words and  
Character-Bigrams as Features in Chinese Text Categorization 
 
 
Jingyang Li                       Maosong Sun                     Xian Zhang 
National Lab. of Intelligent Technology & Systems, Department of Computer Sci. & Tech. 
Tsinghua University, Beijing 100084, China 
lijingyang@gmail.com  sms@tsinghua.edu.cn  kevinn9@gmail.com 
 
  
 
Abstract 
Words and character-bigrams are both 
used as features in Chinese text process-
ing tasks, but no systematic comparison 
or analysis of their values as features for 
Chinese text categorization has been re-
ported heretofore. We carry out here a 
full performance comparison between 
them by experiments on various docu-
ment collections (including a manually 
word-segmented corpus as a golden stan-
dard), and a semi-quantitative analysis to 
elucidate the characteristics of their be-
havior; and try to provide some prelimi-
nary clue for feature term choice (in most 
cases, character-bigrams are better than 
words) and dimensionality setting in text 
categorization systems. 
1 Introduction1 
Because of the popularity of the Vector Space 
Model (VSM) in text information processing, 
document indexing (term extraction) acts as a 
pre-requisite step in most text information proc-
essing tasks such as Information Retrieval 
(Baeza-Yates and Ribeiro-Neto, 1999) and Text 
Categorization (Sebastiani, 2002). It is empiri-
cally known that the indexing scheme is a non-
trivial complication to system performance, es-
pecially for some Asian languages in which there 
are no explicit word margins and even no natural 
semantic unit. Concretely, in Chinese Text Cate-
gorization tasks, the two most important index-
                                                 
1 This research is supported by the National Natural Science 
Foundation of China under grant number 60573187 and  
60321002, and the Tsinghua-ALVIS Project co-sponsored 
by the National Natural Science Foundation of China under 
grant number 60520130299 and EU FP6. 
ing units (feature terms) are word and character-
bigram, so the problem is: which kind of terms2 
should be chosen as the feature terms, words or 
character-bigrams? 
To obtain an all-sided idea about feature 
choice beforehand,  we review here the possible 
feature variants (or, options). First, at the word 
level, we can do stemming, do stop-word prun-
ing, include POS (Part of Speech) information, 
etc. Second, term combinations (such as ?word-
bigram?, ?word + word-bigram?, ?character-
bigram + character-trigram?3, etc.) can also be 
used as features (Nie et al, 2000). But, for Chi-
nese Text Categorization, the ?word or bigram? 
question is fundamental. They have quite differ-
ent characteristics (e.g. bigrams overlap each 
other in text, but words do not) and influence the 
classification performance in different ways. 
In Information Retrieval, it is reported that bi-
gram indexing schemes outperforms word 
schemes to some or little extent (Luk and Kwok, 
1997; Leong and Zhou 1998; Nie et al, 2000). 
Few similar comparative studies have been re-
ported for Text Categorization (Li et al, 2003) so 
far in literature. 
Text categorization and Information Retrieval 
are tasks that sometimes share identical aspects 
(Sebastiani, 2002) apart from term extraction 
(document indexing), such as tfidf term weight-
ing and performance evaluation. Nevertheless, 
they are different tasks. One of the generally ac-
cepted connections between Information Re-
trieval and Text Categorization is that an infor-
mation retrieval task could be partially taken as a 
binary classification problem with the query as 
the only positive training document. From this 
                                                 
2 The terminology ?term? stands for both word and charac-
ter-bigram. Term or  combination of terms (in word-bigram 
or other forms) might be chosen as ?feature?. 
3 The terminology ?character? stands for Chinese character, 
and ?bigram? stands for character-bigram in this paper. 
545
viewpoint, an IR task and a general TC task have 
a large difference in granularity. To better illus-
trate this difference, an example is present here. 
The words ????(film producer)? and ???
?(dubbed film)? should be taken as different 
terms in an IR task because a document with one 
would not necessarily be a good match for a 
query with the other, so the bigram ???(film 
production)? is semantically not a shared part of 
these two words, i.e. not an appropriate feature 
term. But in a Text Categorization task, both 
words might have a similar meaning at the cate-
gory level (?film? category, generally), which 
enables us to regard the bigram ???? as a se-
mantically acceptable representative word snip-
pet for them, or for the category. 
There are also differences in some other as-
pects of IR and TC. So it is significant to make a 
detailed comparison and analysis here on the 
relative value of words and bigrams as features 
in Text Categorization. The organization of this 
paper is as follows: Section 2 shows some ex-
periments on different document collections to 
observe the common trends in the performance 
curves of the word-scheme and bigram-scheme; 
Section 3 qualitatively analyses these trends; 
Section 4 makes some statistical analysis to cor-
roborate the issues addressed in Section 3; Sec-
tion 5 summarizes the results and concludes. 
2 Performance Comparison 
Three document collections in Chinese language 
are used in this study. 
The electronic version of Chinese Encyclo-
pedia (?CE?): It has 55 subject categories and 
71674 single-labeled documents (entries). It is 
randomly split by a proportion of 9:1 into a train-
ing set with 64533 documents and a test set with 
7141 documents. Every document has the full-
text. This data collection does not have much of 
a sparseness problem. 
The training data from a national Chinese 
text categorization evaluation4 (?CTC?): It has 
36 subject categories and 3600 single-labeled5 
documents. It is randomly split by a proportion 
of 4:1 into a training set with 2800 documents 
and a test set with 720 documents. Documents in 
this data collection are from various sources in-
cluding news websites, and some documents 
                                                 
4 The Annual Evaluation of  Chinese Text Categorization 
2004, by 863 National Natural Science Foundation. 
5 In the original document collection, a document might 
have a secondary category label. In this study, only the pri-
mary category label is reserved. 
may be very short. This data collection has a 
moderate sparseness problem. 
A manually word-segmented corpus from 
the State Language Affairs Commission 
(?LC?): It has more than 100 categories and 
more than 20000 single-labeled documents6. In 
this study, we choose a subset of 12 categories 
with the most documents (totally 2022 docu-
ments). It is randomly split by a proportion of 2:1 
into a training set and a test set. Every document 
has the full-text and has been entirely word-
segmented7 by hand (which could be regarded as 
a golden standard of segmentation). 
All experiments in this study are carried out at 
various feature space dimensionalities to show 
the scalability. Classifiers used in this study are 
Rocchio and SVM. All experiments here are 
multi-class tasks and each document is assigned 
a single category label. 
The outline of this section is as follows: Sub-
section 2.1 shows experiments based on the Roc-
chio classifier, feature selection schemes besides 
Chi and term weighting schemes besides tfidf to 
compare the automatic segmented word features 
with bigram features on CE and CTC, and both 
document collections lead to similar behaviors; 
Subsection 2.2 shows experiments on CE by a 
SVM classifier,  in which, unlike with the Roc-
chio method, Chi feature selection scheme and 
tfidf term weighting scheme outperform other 
schemes; Subsection 2.3 shows experiments by a 
SVM classifier with Chi feature selection and 
tfidf term weighting on LC (manual word seg-
mentation) to compare the best word features 
with bigram features. 
2.1 The Rocchio Method and Various Set-
tings 
The Rocchio method is rooted in the IR tradition, 
and is very different from machine learning ones 
(such as SVM) (Joachims, 1997; Sebastiani, 
2002). Therefore, we choose it here as one of the 
representative classifiers to be examined. In the 
experiment, the control parameter of negative 
examples is set to 0, so this Rocchio based classi-
fier is in fact a centroid-based classifier. 
Chimax is a state-of-the-art feature selection 
criterion for dimensionality reduction (Yang and 
Peterson, 1997; Rogati and Yang, 2002). Chi-
max*CIG (Xue and Sun, 2003a) is reported to be 
better in Chinese text categorization by a cen-
                                                 
6 Not completed. 
7 And POS (part-of-speech) tagged as well. But POS tags 
are not used in this study. 
546
troid based classifier, so we choose it as another 
representative feature selection criterion besides 
Chimax. 
Likewise, as for term weighting schemes, in 
addition to tfidf, the state of the art (Baeza-Yates 
and Ribeiro-Neto, 1999), we also choose 
tfidf*CIG (Xue and Sun, 2003b). 
Two word segmentation schemes are used for 
the word-indexing of documents. One is the 
maximum match algorithm (?mmword? in the 
figures), which is a representative of simple and 
fast word segmentation algorithms.  The other is 
ICTCLAS8 (?lqword? in the figures). ICTCLAS 
is one of the best word segmentation systems 
(SIGHAN 2003) and reaches a segmentation 
precision of more than 97%, so we choose it as a 
representative of state-of-the-art schemes for 
automatic word-indexing of document). 
For evaluation of single-label classifications,  
F1-measure, precision, recall and accuracy 
(Baeza-Yates and Ribeiro-Neto, 1999; Sebastiani, 
2002) have the same value by microaveraging9, 
and are labeled with ?performance? in the fol-
lowing figures. 
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
p
e
rf
o
rm
a
n
ce
mmword
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
lqword
p
e
rf
o
rm
a
n
ce
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
bigram
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf      
chicig-tfidfcig
 
Figure 1. chi-tfidf and chicig-tfidfcig on CE 
Figure 1 shows the performance-
dimensionality curves of the chi-tfidf approach 
and the approach with CIG, by mmword, lqword 
and bigram document indexing, on the CE 
document collection. We can see that the original 
chi-tfidf approach is better at low dimensional-
ities (less than 10000 dimensions), while the CIG 
version is better at high dimensionalities and 
reaches a higher limit.10 
                                                 
8 http://www.nlp.org.cn/project/project.php?proj_id=6 
9 Microaveraging is more prefered in most cases than 
macroaveraging (Sebastiani 2002). 
10 In all figures in this paper, curves might be truncated due 
to the large scale of dimensionality, especially the curves of 
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
p
e
rf
o
rm
a
n
ce
mmword
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
lqword
p
e
rf
o
rm
a
n
ce
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
bigram
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf      
chicig-tfidfcig
 
Figure 2. chi-tfidf and chicig-tfidfcig on CTC 
Figure 2 shows the same group of curves for 
the CTC document collection. The curves fluctu-
ate more than the curves for the CE collection 
because of sparseness; The CE collection is more 
sensitive to the additions of terms that come with 
the increase of dimensionality. The CE curves in 
the following figures show similar fluctuations 
for the same reason. 
For a parallel comparison among mmword, 
lqword and bigram schemes, the curves in  Fig-
ure 1 and Figure 2 are regrouped and shown in 
Figure 3 and Figure 4. 
2 4 6 8
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf
mmword
lqword
bigram
2 4 6 8
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
dimensionality
chicig-tfidfcig
mmword
lqword
bigram
 
Figure 3. mmword, lqword and bigram on CE 
 
1 2 3 4 5
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf
mmword
lqword
bigram
1 2 3 4 5
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
dimensionality
chicig-tfidfcig
mmword
lqword
bigram
 
Figure 4. mmword, lqword and bigram on CTC 
                                                                          
bigram scheme. For these kinds of figures, at least one of 
the following is satisfied: (a) every curve has shown its 
zenith; (b) only one curve is not complete and has shown a 
higher zenith than other curves; (c) a margin line is shown 
to indicate the limit of the incomplete curve. 
547
We can see that the lqword scheme outper-
forms the mmword scheme at almost any dimen-
sionality, which means the more precise the word 
segmentation the better the classification per-
formance. At the same time, the bigram scheme 
outperforms both of the word schemes on a high 
dimensionality, wherea the word schemes might 
outperform the bigram scheme on a low dimen-
sionality. 
Till now, the experiments on CE and CTC 
show the same characteristics despite the per-
formance fluctuation on CTC caused by sparse-
ness. Hence in the next subsections CE is used 
instead of both of them because its curves are 
smoother. 
2.2 SVM on Words and Bigrams 
As stated in the previous subsection, the lqword 
scheme always outperforms the mmword scheme; 
we compare here only the lqword scheme with 
the bigram scheme.  
Support Vector Machine (SVM) is one of the 
best classifiers at present (Vapnik, 1995; 
Joachims, 1998), so we choose it as the main 
classifier in this study. The SVM implementation 
used here is LIBSVM (Chang, 2001); the type of 
SVM is set to ?C-SVC? and the kernel type is set 
to linear, which means a one-with-one scheme is 
used in the multi-class classification. 
Because the CIG?s effectiveness on a SVM 
classifier is not examined in Xue and Sun (2003a, 
2003b)?s report, we make here the four combina-
tions of schemes with and without CIG in feature 
selection and term weighting. The experiment 
results are shown in Figure 5. The collection 
used is CE. 
 
1 2 3 4 5 6 7
x 10
4
0.6
0.65
0.7
0.75
0.8
0.85
0.9
p
e
rf
o
rm
a
n
ce
dimensionality
lqword
chi-tfidf      
chi-tfidfcig   
chicig-tfidf   
chicig-tfidfcig
1 2 3 4 5 6 7
x 10
4
0.6
0.65
0.7
0.75
0.8
0.85
0.9
dimensionality
bigram
chi-tfidf      
chi-tfidfcig   
chicig-tfidf   
chicig-tfidfcig
 
Figure 5. chi-tfidf and cig-involved approaches 
on lqword and bigram 
Here we find that the chi-tfidf combination 
outperforms any approach with CIG, which is the 
opposite of the results with the Rocchio method. 
And the results with SVM are all better than the 
results with the Rocchio method. So we find that 
the feature selection scheme and the term 
weighting scheme are related to the classifier, 
which is worth noting. In other words, no feature 
selection scheme or term weighting scheme is 
absolutely the best for all classifiers. Therefore, a 
reasonable choice is to select the best performing 
combination of feature selection scheme, term 
weighting scheme and classifier, i.e. chi-tfidf and 
SVM. The curves for the lqword scheme and the 
bigram scheme are redrawn in Figure 6 to make 
them clearer. 
1 2 3 4 5 6 7
x 10
4
0.75
0.8
0.85
0.9
p
e
rf
o
rm
a
n
c
e
dimensionality
lqword
bigram
 
Figure 6. lqword and bigram on CE 
The curves shown in Figure 6 are similar to 
those in Figure 3. The differences are: (a) a lar-
ger dimensionality is needed for the bigram 
scheme to start outperforming the lqword scheme; 
(b) the two schemes have a smaller performance 
gap. 
The lqword scheme reaches its top perform-
ance at a dimensionality of around 40000, and 
the bigram scheme reaches its top performance 
at a dimensionality of around 60000 to 70000, 
after which both schemes? performances slowly 
decrease. The reason is that the low ranked terms 
in feature selection are in fact noise and do not 
help to classification, which is why the feature 
selection phase is necessary. 
2.3 Comparing Manually Segmented 
Words and Bigrams 
0 1 2 3 4 5 6 7 8 9 10
x 10
4
72
74
76
78
80
82
84
86
88
dimansionality
p
e
rf
o
rm
a
n
c
e
word        
bigram      
bigram limit
 
Figure 7. word and bigram on LC 
548
Up to now, bigram features seem to be better 
than word ones for fairly large dimensionalities. 
But it appears that word segmentation precision 
impacts classification performance. So we 
choose here a fully manually segmented docu-
ment collection to detect the best performance a 
word scheme could  reach and compare it with 
the bigram scheme. 
Figure 7 shows such an experiment result on 
the LC document collection (the circles indicate 
the maximums and the dash-dot lines indicate the 
superior limit and the asymptotic interior limit of 
the bigram scheme). The word scheme reaches a 
top performance around the dimensionality of 
20000, which is a little higher than the bigram 
scheme?s zenith around 70000. 
Besides this experiment on 12 categories of 
the LC document collection, some experiments 
on fewer (2 to 6) categories of this subset were 
also done, and showed similar behaviors. The 
word scheme shows a better performance than 
the bigram scheme and needs a much lower di-
mensionality. The simpler the classification task 
is, the more distinct this behavior is. 
3 Qualitative Analysis 
To analyze the performance of words and bi-
grams as feature terms in Chinese text categori-
zation, we need to investigate two aspects as fol-
lows. 
3.1 An Individual Feature Perspective 
The word is a natural semantic unit in Chinese 
language and expresses a complete meaning in 
text. The bigram is not a natural semantic unit 
and might not express a complete meaning in 
text, but there are also reasons for the bigram to 
be a good feature term. 
First, two-character words and three-character 
words account for most of all multi-character 
Chinese words (Liu and Liang, 1986). A two-
character word can be substituted by the same 
bigram. At the granularity of most categorization 
tasks, a three-character words can often be sub-
stituted by one of its sub-bigrams (namely the 
?intraword bigram? in the next section)  without 
a change of meaning. For instance, ???? is a 
sub-bigram of the word ????(tournament)? 
and could represent it without ambiguity. 
Second, a bigram may overlap on two succes-
sive words (namely the ?interword bigram? in 
the next section), and thus to some extent fills the 
role of a word-bigram. The word-bigram as a 
more definite (although more sparse) feature  
surely helps the classification. For instance, ??
?? is a bigram overlapping on the two succes-
sive words ? ? ? (weather)? and ? ? ?
(forecast)?, and could almost replace the word-
bigram (also a phrase) ?????(weather fore-
cast)?, which is more likely to be a representative 
feature of the category ????(meteorology)? 
than either word. 
Third, due to the first issue, bigram features 
have some capability of identifying OOV (out-
of-vocabulary) words 11 , and help improve the 
recall of classification. 
The above issues state the advantages of bi-
grams compared with words. But in the first and 
second issue, the equivalence between bigram 
and word or word-bigram is not perfect. For in-
stance, the word ???(literature)? is a also sub-
bigram of the word ????(astronomy)?, but 
their meanings are completely different. So the 
loss and distortion of semantic information is a 
disadvantage of bigram features over word fea-
tures.  
Furthermore, one-character words cover about 
7% of words and more than 30% of word occur-
rences in the Chinese language; they are effev-
tive in the word scheme and are not involved in 
the above issues. Note that the impact of effec-
tive one-character words on the classification is 
not as large as their total frequency, because the 
high frequency ones are often too common to 
have a good classification power, for instance, 
the word ?? (of, ?s)?. 
3.2 A Mass Feature Perspective 
Features are not independently acting in text 
classification. They are assembled together to 
constitute a feature space. Except for a few mod-
els such as Latent Semantic Indexing (LSI) 
(Deerwester et al, 1990), most models assume 
the feature space to be orthogonal. This assump-
tion might not affect the effectiveness of the 
models, but the semantic redundancy and com-
plementation among the feature terms do impact 
on the classification efficiency at a given dimen-
sionality. 
According to the first issue addressed in the 
previous subsection, a bigram might cover for 
more than one word. For instance, the bigram 
???? is a sub-bigram of the words ???
(fabric)?,???? (cotton fabric)?, ????
(knitted fabric)?, and also a good substitute of 
                                                 
11 The ?OOV words? in this paper stand for the words that 
occur in the test documents but not in the training document. 
549
them. So, to a certain extent, word features are 
redundant with regard to the bigram features as-
sociated to them. Similarly, according to the sec-
ond issue addressed, a bigram might cover for 
more than one word-bigram. For instance, the 
bigram ???? is a sub-bigram of the word-
bigrams (phrases) ?????(short story)?, ??
???(novelette)?, ?????(novel)? and also 
a good substitute for them. So, as an addition to 
the second issue stated in the previous subsection, 
a bigram feature might even cover for more than 
one word-bigram. 
On the other hand, bigrams features are also 
redundant with regard to word features associ-
ated with them. For instance, the ???? and ??
?? are both sub-bigrams of the previously men-
tioned word ?????. In some cases, more than 
one sub-bigram can be a good representative of a 
word. 
We make a word list and a bigram list sorted 
by the feature selection criterion in a descending 
order. We now try to find how the relative re-
dundancy degrees of the word list and the bigram 
list vary with the dimensionality. Following is-
sues are elicited by an observation on the two 
lists (not shown here due to space limitations). 
The relative redundancy rate in the word list 
keeps even while the dimensionality varies to a 
certain extent, because words that share a com-
mon sub-bigram might not have similar statistics 
and thus be scattered in the word feature list. 
Note that these words are possibly ranked lower 
in the list than the sub-bigram because feature 
selection criteria (such as Chi) often prefer 
higher frequency terms to lower frequency ones, 
and every word containing the bigram certainly 
has a lower frequency than the bigram itself. 
The relative redundancy in the bigram list 
might be not as even as in the word list. Good 
(representative) sub-bigrams of a word are quite 
likely to be ranked close to the word itself. For 
instance, ???? and ???? are sub-bigrams of 
the word ????(music composer)?, both the 
bigrams and the word are on the top of the lists. 
Theretofore, the bigram list has a relatively large 
redundancy rate at low dimensionalities. The 
redundancy rate should decrease along with the 
increas of dimensionality for: (a) the relative re-
dundancy in the word list counteracts the redun-
dancy in the bigram list, because the words that 
contain a same bigram are gradually included as 
the dimensionality increases; (b) the proportion 
of interword bigrams increases in the bigram list 
and there is generally no redundancy between 
interword bigrams and intraword bigrams. 
Last, there are more bigram features than word 
features because bigrams can overlap each other 
in the text but words can not. Thus the bigrams 
as a whole should theoretically contain more in-
formation than the words as a whole. 
From the above analysis and observations, bi-
gram features are expected to outperform word 
features at high dimensionalities. And word fea-
tures are expected to outperform bigram features 
at low dimensionalities.  
4 Semi-Quantitative Analysis 
In this section, a preliminary statistical analysis 
is presented to corroborate the statements in the 
above qualitative analysis and expected to be 
identical with the experiment results shown in 
Section 1. All statistics in this section are based 
on the CE document collection and the lqword 
segmentation scheme (because the CE document 
collection is large enough to provide good statis-
tical characteristics). 
4.1 Intraword Bigrams and Interword Bi-
grams 
In the previous section, only the intraword bi-
grams were discussed together with the words. 
But every bigram may have both intraword oc-
currences and interword occurrences. Therefore 
we need to distinguish these two kinds of bi-
grams at a statistical level. For every bigram, the 
number of intraword occurrences and the number 
of interword occurrences are counted and we can 
use 
 
1
log
1
interword#
intraword#
+? ?? ?+? ?  
as a metric to indicate its natual propensity to be 
a intraword bigram. The probability density of 
bigrams about on this metric is shown in Figure 
8. 
-12 -10 -8 -6 -4 -2 0 2 4 6 8 10
0
0.05
0.1
0.15
0.2
0.25
log(intraword#/interword#)
p
ro
b
a
b
ili
ty
 d
e
n
s
it
y
 
Figure 8. Bigram Probability Density on 
log(intraword#/interword#) 
550
The figure shows a mixture of two Gaussian 
distributions, the left one for ?natural interword 
bigrams? and the right one for ?natural intraword 
bigrams?. We can moderately distinguish these 
two kinds of bigrams by a division at -1.4. 
4.2 Overall Information Quantity of a Fea-
ture Space 
The performance limit of a classification is re-
lated to the quantity of information used. So a 
quantitative metric of the information a feature 
space can provide is need. Feature Quantity (Ai-
zawa, 2000) is suitable for this purpose because 
it comes from information theory and is additive; 
tfidf was also reported as an appropriate metric of 
feature quantity (defined as ?probability ? infor-
mation?). Because of the probability involved as 
a factor, the overall information provided by a 
feature space can be calculated on training data 
by summation. 
The redundancy and complementation men-
tioned in Subsection 3.2 must be taken into ac-
count in the calculation of overall information 
quantity. For bigrams, the redundancy with re-
gard to words associated with them between two 
intraword bigrams is given by 
 { }
1,2
1 2( ) min ( ), ( )
b w
tf w idf b idf b
?
??  
in which b1 and b2 stand for the two bigrams and 
w stands for any word containing both of them. 
The overall information quantity is obtained by 
subtracting the redundancy between each pair of 
bigrams from the sum of all features? feature 
quantity (tfidf). Redundancy among more than 
two bigrams is ignored. For words, there is only 
complementation among words but not redun-
dancy, the complementation with regard to bi-
grams associated with them is given by 
 
{ } if  exists;
if  does not exists.
( ) min ( ) ,
( ) ( ),
b w
b
b
tf w idf b
tf w idf w
?
????
???
 
in which b is an intraword bigram contained by 
w. The overall information is calculated by 
summing the complementations of all words. 
4.3 Statistics and Discussion 
Figure 9 shows the variation of these overall in-
formation metrics on the CE document collection. 
It corroborates the characteristics analyzed in 
Section 3 and corresponds with the performance 
curves in Section 2.  
Figure 10 shows the proportion of interword 
bigrams at different dimensionalities, which also 
corresponds with the analysis in Section 3. 
0 2 4 6 8 10 12 14 16
x 10
4
0
2
4
6
8
10
12
14
16
x 10
7
dimensionality
o
v
e
ra
ll 
in
fo
rm
a
ti
o
n
 q
u
a
n
ti
ty
word  
bigram
 
Figure 9. Overall Information Quantity on CE 
The curves do not cross at exactly the same 
dimensionality as in the figures in Section 1, be-
cause other complications impact on the classifi-
cation performance: (a) OOV word identifying 
capability, as stated in Subsection 3.1; (b) word 
segmentation precision; (c) granularity of the 
categories (words have more definite semantic 
meaning than bigrams and lead to a better per-
formance for small category granularities); (d) 
noise terms, introduced in the feature space dur-
ing the increase of dimensionality. With these 
factors, the actual curves would not keep increas-
ing as they do in Figure 9. 
0 2 4 6 8 10 12 14 16
x 10
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
dimensionality
in
te
rw
o
rd
 b
ig
ra
m
 p
ro
p
o
rt
io
n
 
Figure 10. Interword Bigram Proportion on CE 
5 Conclusion 
In this paper, we aimed to thoroughly compare 
the value of words and bigrams as feature terms 
in text categorization, and make the implicit 
mechanism explicit. 
Experimental comparison showed that the Chi 
feature selection scheme and the tfidf term 
weighting scheme are still the best choices for 
(Chinese) text categorization on a SVM classifier. 
In most cases, the bigram scheme outperforms 
the word scheme at high dimensionalities and 
usually reaches its top performance at a dimen-
551
sionality of around 70000. The word scheme of-
ten outperforms the bigram scheme at low di-
mensionalities and reaches its top performance at 
a dimensionality of less than 40000. 
Whether the best performance of the word 
scheme is higher than the best performance 
scheme depends considerably on the word seg-
mentation precision and the number of categories. 
The word scheme performs better with a higher 
word segmentation precision and fewer (<10) 
categories. 
A word scheme costs more document indexing 
time than a bigram scheme does; however a bi-
gram scheme costs more training time and classi-
fication time than a word scheme does at the 
same performance level due to its higher dimen-
sionality. Considering that the document index-
ing is needed in both the training phase and the 
classification phase, a high precision word 
scheme is more time consuming as a whole than 
a bigram scheme. 
As a concluding suggestion: a word scheme is 
more fit for small-scale tasks (with no more than 
10 categories and no strict classification speed 
requirements) and needs a high precision word 
segmentation system; a bigram scheme is more 
fit for large-scale tasks (with dozens of catego-
ries or even more) without too strict training 
speed requirements (because a high dimensional-
ity and a large number of categories lead to a 
long training time). 
Reference 
Akiko Aizawa. 2000. The Feature Quantity: An In-
formation Theoretic Perspective of Tfidf-like 
Measures, Proceedings of ACM SIGIR 2000, 104-
111. 
Ricardo Baeza-Yates, Berthier Ribeiro-Neto. 1999. 
Modern Information Retrieval, Addison-Wesley 
Chih-Chung Chang, Chih-Jen Lin. 2001. LIBSVM: A 
Library for Support Vector Machines, Software 
available at http://www.csie.ntu.edu.tw/~cjlin/ 
libsvm 
Steve Deerwester, Sue T. Dumais, George W. Furnas, 
Richard Harshman. 1990. Indexing by Latent Se-
mantic Analysis, Journal of the American Society 
for Information Science, 41:391-407. 
Thorsten Joachims. 1997. A Probabilistic Analysis of 
the Rocchio Algorithm with TFIDF for Text Cate-
gorization, Proceedings of 14th International Con-
ference on Machine Learning (Nashville, TN, 
1997), 143-151. 
Thorsten Joachims. 1998. Text Categorization with 
Support Vector Machine: Learning with Many 
Relevant Features, Proceedings of the 10th Euro-
pean Conference on Machine Learning, 137-142. 
Mun-Kew Leong, Hong Zhou. 1998. Preliminary 
Qualitative Analysis of Segmented vs. Bigram In-
dexing in Chinese, The 6th Text Retrieval Confer-
ence (TREC-6), NIST Special Publication 500-240, 
551-557. 
Baoli Li, Yuzhong Chen, Xiaojing Bai, Shiwen Yu. 
2003. Experimental Study on Representing Units in 
Chinese Text Categorization, Proceedings of the 
4th International Conference on  Computational 
Linguistics and Intelligent Text Processing (CI-
CLing 2003), 602-614. 
Yuan Liu, Nanyuan Liang. 1986. Basic Engineering 
for Chinese Processing ? Contemporary Chinese 
Words Frequency Count, Journal of Chinese In-
formation Processing, 1(1):17-25. 
Robert W.P. Luk, K.L. Kwok. 1997. Comparing rep-
resentations in Chinese information retrieval. Pro-
ceedings of ACM SIGIR 1997, 34-41. 
Jianyun Nie, Fuji Ren. 1999. Chinese Information 
Retrieval: Using Characters or Words? Informa-
tion Processing and Management, 35:443-462. 
Jianyun Nie, Jianfeng Gao, Jian Zhang, Ming Zhou. 
2000. On the Use of Words and N-grams for Chi-
nese Information Retrieval, Proceedings of 5th In-
ternational Workshop on Information Retrieval 
with Asian Languages 
Monica Rogati, Yiming Yang. 2002. High-performing 
Feature Selection for Text Classification, Proceed-
ings of ACM Conference on Information and 
Knowledge Management 2002, 659-661. 
Gerard Salton, Christopher Buckley. 1988. Term 
Weighting Approaches in Automatic Text Retrieval, 
Information Processing and Management, 
24(5):513-523. 
Fabrizio Sebastiani. 2002. Machine Learning in 
Automated Text Categorization, ACM Computing 
Surveys, 34(1):1-47 
Dejun Xue, Maosong Sun. 2003a. Select Strong In-
formation Features to Improve Text Categorization 
Effectiveness, Journal of Intelligent Systems, Spe-
cial Issue. 
Dejun Xue, Maosong Sun. 2003b. A Study on Feature 
Weighting in Chinese Text Categorization, Pro-
ceedings of the 4th International Conference on  
Computational Linguistics and Intelligent Text 
Processing (CICLing 2003), 594-604. 
Vladimir Vapnik. 1995. The Nature of Statistical 
Learning Theory, Springer. 
Yiming Yang, Jan O. Pederson. 1997.  A Comparative 
Study on Feature Selection in Text Categorization, 
Proceedings of ICML 1997, 412-420. 
552
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 42?49
Manchester, UK. August 2008
Answer Validation by Information Distance Calculation
Fangtao Li, Xian Zhang, Xiaoyan Zhu
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
zxy-dcs@mail.tsinghua.edu.cn
Abstract
In this paper,an information distance based
approach is proposed to perform answer
validation for question answering system.
To validate an answer candidate, the ap-
proach calculates the conditional informa-
tion distance between the question focus
and the candidate under certain condition
pattern set. Heuristic methods are de-
signed to extract question focus and gen-
erate proper condition patterns from ques-
tion. General search engines are employed
to estimate the Kolmogorov complexity,
hence the information distance. Experi-
mental results show that our approach is
stable and flexible, and outperforms tradi-
tional tfidf methods.
1 Introduction
Question answering(QA) system aims at finding
exact answers to a natural language question. In
order to correctly answer a question, several com-
ponents are implemented including question clas-
sification, passage retrieval, answer candidates
generation, answer validation etc. Answer Vali-
dation is to decide whether the candidate answers
are correct or not, or even to determine the accu-
rate confidence score to them. Most of QA systems
employ answer validation as the last step to iden-
tify the correct answer. If this component fails, it
is impossible to enable the question to be correctly
answered.
Automatic techniques for answer validation are
of great interest among question answering re-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
search. With automatic answer validation, the
system will carry out different refinements of its
searching criteria to check the relevance of new
candidate answers. In addition, since most of
QA systems rely on complex architectures and the
evaluation of their performances requires a huge
amount of work, the automatic assessment of can-
didates with respect to a given question will speed
up both algorithm refinement and testing.
Currently, answer validation is mainly viewed
as a classification problem or ranking problem.
Different models, such as Support Vector Ma-
chine (Shen and Klakow, 2006) and Maximum En-
tropy Model (Ittycheriah et al, 2001), are used to
integrate sophisticated linguistic features to deter-
mine the correctness of candidates. The answer
validation exercise (Penas et al , 2007) aims at
developing systems able to decide whether the an-
swer is correct or not. They formulate answer val-
idation as a text entailment problem. These ap-
proaches are dependent on sophisticated linguis-
tic analysis of syntactic and semantic relations be-
tween question and candidates. It is quite expen-
sive to use deep analysis for automatic answer val-
idation, especially in large scale data set. Thus it
is appropriate to find an alternative solution to this
problem. Here, we just consider the English an-
swer validation task.
This paper proposes a novel approach based on
information retrieval on the Web. The answer val-
idation problem is reformulated as distance calcu-
lation from an answer candidate to a question. The
hypothesis is that, among all candidates, the cor-
rect answer has the smallest distance from ques-
tion. We employ conditional normalized min dis-
tance, which is based on Kolmogorov Complexity
theory (Li and Vitanyi, 1997), for this task. The
distance measures the relevance between question
42
focus and candidates conditioned on a surface pat-
tern set. For distance calculation, we first ex-
tract the question focus, and then a hierarchical
pattern set is automatically constructed as condi-
tion. Since Kolmogrov Complexity can be approx-
imated through frequency counts. Two types of
search engine ?Google? and ?Altavista? are used
to approximate the distance.
The paper is organized as follows: Section 2
describes related work. The fundamental Kol-
mogorov Complexity theory is introduced in Sec-
tion 3. Section 4 presents our proposed answer val-
idation method based on information retrieval. In
Section 5, we describe the experiments and discus-
sions. The paper is concluded in Section 6.
2 Related Work
Answer Validation is an emerging topic in Ques-
tion Answering, where open domain systems are
often required to rank huge amounts of answer
candidates. This task can be viewed as a classi-
fication problem or re-ranking problem.
Early question answering systems focused on
employing surface text patterns (Subbotin and
Subbotin, 2001) for answer validation. Xu et
al. (2003) identified that pattern-based approaches
got bad performances due to poor system recall.
Some researchers exploited machine learning tech-
niques with rich syntactic or semantic features to
measure the similarity between question and an-
swer. Ittycheriah et al (2001) used Maximum En-
tropy model to combine rich features and automat-
ically learn feature weights. These features in-
cluded query expansion features, focus features,
named entity features, dependency relation fea-
tures, pattern features et al Shen and Klakow
(2006) presented three methods, including feature
vector, string kernel and tree kernel, to represent
surface text features and parse tree features in Sup-
port Vector Machines. Ko et al (2007) pro-
posed a probabilistic graphical model to estimate
the probability of correctness for all candidate an-
swers. Four types of features were employed,
including knowledge-based features, data-driven
features, string distance feature and synonym fea-
tures.
Started in 2006, the annual Answer Validation
Exercise (Penas et al , 2007) aims to develop sys-
tems to decide if the answer to a question is correct
or not. The English answer validation task is refor-
mulated as a Text Entailment problem. The triplet,
including question, answer and supporting text, is
given. The system determines if the supporting
text can entail the hypothesis, which is a reformu-
lation from the question and answer. All partici-
pants used lexical processing, including lemmati-
zation and part-of speech tagging. Some systems
used first order logic representations, performed
semantic analysis and took the validation decision
with a theorem proof.
The above approaches should process deep syn-
tactic and semantic analysis for either questions or
candidate answers. The annotated linguistic re-
source is hard to acquire for the supervised clas-
sification problem. Another alternative solution
for answer validation is to exploit the redundancy
of large scale data. Eric et al (2007) devel-
oped AskMSR question answering system. They
focus on the Web as a gigantic data repository
with tremendous redundancy that can be exploited
to extract the correct answer. Lin (2007) im-
plemented another Web-based question answering
system, named ARANEA, which is used approxi-
mate tfidf method for answer validation.
3 Preliminaries
3.1 Kolmogorov complexity
Kolmogorov complexity , or algorithm entropy ,
K(x) of a string x is the length of the shortest bi-
nary program to compute x. It defines randomness
of an individual string. Kolmogorov complexity
has been widely accepted as an information theory
for individual objects parallel to that of Shannon?s
information theory which is defined on an ensem-
ble of objects. It has also found many applications
in computer science such as average case analysis
of algorithms (Li and Vitanyi, 1997). For a uni-
versal Turing machine U , the Kolmogorov com-
plexity of a binary string x condition to another
binary string y, K
U
(x|y), is the length of the short-
est (prefix-free) program for U that outputs x with
input y. It has been proved that for different uni-
versal Turing machine U
?
, for all x, y
K
U
(x|y) = K
U
?
(x|y) + C,
where the constant C depends only on U
?
. Thus we
simply write K
U
(x|y) as K(x|y). Define K(x) =
K(x|?), where ? is the empty string. For for-
mal definitions and a comprehensive study of Kol-
mogorov complexity, see (Li and Vitanyi, 1997).
43
3.2 Information Distance
Based on the Kolmogovov complexity theory, in-
formation distance (Bennett et al, 1998) is a uni-
versal distance metric, which has been success-
fully applied to many applications. The informa-
tion distance D(x, y) is defined as the length of
a shortest binary program which can compute x
given y as well as compute y from x. It has been
proved that , up to an additive logarithmic term,
D(x, y) = max{K(x|y),K(y|x)}. The normal-
ized version of D(x, y), called the normalized in-
formation distance(NID), is defined as
d
max
(x, y) =
max{K(x|y),K(y|x)}
max{K(x),K(y)}
(1)
Parallel to this, the min distance is proposed in
(Zhang et al , 2007), defined as
D
min
(x, y) = min{K(x|y),K(y|x)}. (2)
And the normalized version is
d
min
(x, y) =
min{K(x|y),K(y|x)}
min{K(x),K(y)}
. (3)
3.3 Conditional Information Distance
Conditional information distance is defined as
d
max
(x, y|c) =
max{K(x|y, c),K(y|x, c)}
max{K(x|c),K(y|c)}
, (4)
d
min
(x, y|c) =
min{K(x|y, c),K(y|x, c)}
min{K(x|c),K(y|c)}
. (5)
where c is given in both x to y and y to x compu-
tation.
The information distance is proved to be uni-
versal (Zhang et al , 2007), that is, if x and y
are ?close? under any distance measure, they are
?close? under the measure of information distance.
However, it is not clear yet how to find out such
?closeness? in traditional information distance the-
ory. Now the conditional information distance pro-
vides a possible solution.Figure 1 gives a more in-
terpretable explanation: the condition c could map
the original concepts x and y into different x
c
and
y
c
, thus the variant ?closeness? could be reflected
by the distance between x
c
and y
c
, as shown in
Figure1.
Figure 1: Conditional information distances under different
conditions c?s
The Kolmogorov complexity is non-
computable, that is, to use the information
distance measures, we must estimate the K(x)
first. There are traditionally two ways to do
this: (1) by compression (Li et al , 2001),
and (2) by frequency counting based on coding
theorem (Cilibrasi and Vitanyi, 2007). The second
approach is implemented in this paper.
4 Answer Validation with Information
Distance
Given a question q and a candidate answer c, the
answer validation task can be considered as deter-
mining the degree of relevance of c with respect
to q. The intuition of our approach is that the dis-
tance between question and the correct answer is
smaller than other candidates. Take the question
?What is the capital of the USA?? as an example,
among all candidates, the correct answer ?Wash-
ington? is closest to the question under some dis-
tance measure. Thus the answer validation prob-
lem is to determine a proper distance measure.
Fortunately, it has been proved that the informa-
tion distance (Bennett et al, 1998) is universal so
that the similarity between the question and the an-
swer can surely be discovered using this measure.
Direct calculation of the unconditional distance
is difficult and non-flexible. We find it possible
and convenient to estimate the conditional infor-
mation distance between question focus and the
answers, under certain context as the condition. As
explained previously, different conditions lead to
different distance. With the most proper condition
and the nearest distance, the best answer can be
identified out of previously determined candidates.
The conditional normalized min distance is em-
ployed for distance calculation, which is defined
44
Figure 2: Sample of conditional information distance calculation.
as:
d
min
(x, y|c)
=
K
(
c(x,y)
)
?max{K
(
c(x,?)
)
,K
(
c(?,y)
)
}
min{K
(
c(x,?)
)
,K
(
c(?,y)
)
}?K
(
c(?,?)
)
where x represents the answer candidates, y is
the question focus, and c is condition pattern. The
function c(x, y) will be described in the Distance
Calculation section.
Figure 2 shows the procedure of distance cal-
culation. Given a question and a set of candidates,
we calculate the min information distance between
question focus and candidates conditioned on sur-
face patterns. Obviously, in order to calculate in-
formation distance, there are three issues to be ad-
dressed:
1. Question Focus Extraction: since the question
answer distance is reformulated as the mea-
sure between question focus and answer con-
ditioned on the surface pattern, it is important
to extract some words or phrases as question
focus.
2. Condition Pattern Generation: Obviously, the
generation of the condition is the key part.
We have built a well revised algorithm, in
which proper conditions can be generated
from question sentence according to some
heuristic rules.
3. Distance Calculation: after question focus
and condition patterns are obtained, the last
step is calculating the conditional distance to
estimate the relevance between question and
answer candidates.
4.1 Question Focus Extraction
Most factoid questions refer to specific objects. A
question is asked to learn some knowledge for this
object from certain perspective. In our approach,
we take the key named entity or noun phrase, usu-
ally as the subject or the main object of the ques-
tion sentence as the reference object. Take the
question ?What city is Lake Washington by? as ex-
ample, the specific object is ?Lake Washington?.
The question focus is identified using some heuris-
tic rules as follows:
1. The question is processed by shallow parsing.
All the noun phrases(NP) are extracted as NP set.
2. All the named entities(NE) in the question are
extracted as NE set.
3. If only one same element is identified in both
NE and NP set, this element is considered as ques-
tion focus.
4. If step 3 fails, but two elements from NE and
NP set have overlap words, then choose the ele-
ment with more words as question focus.
5. If step 3 and 4 fail, choose the candidate,
which is nearest with verb phrase in dependency
tree, as question focus.
4.2 Condition Pattern Generation
A set of hierarchical patterns is automatically con-
structed for conditional min distance calculation.
4.2.1 Condition Pattern Construction
Several operations are defined for patterns con-
struction from the original question sentence. We
describe pattern set construction with a sam-
ple question ?What year was President Kennedy
killed??:
1. With linguistic analysis, the question is
split into pieces of tokens. These tokens in-
45
clude wh-word phrases, preposition phrases, noun
phrases, verb phrases, key verb, etc. The exam-
ple question is split into ?What year?(wh-word
phrase), ?was?(key verb) ?President Kennedy?
(noun phrases), ?killed?(verb phrase).
2. Replace the wh-word phrases with the candi-
date placeholder ?c?. Then the words ?What year?
is replaced with placeholder ?c?.
3. Replace the question focus with the focus
placeholder ?f?, and add this pattern to the pat-
tern set. The example question focus is identified
as ?President Kennedy?. It is replaced with place-
holder ?f?. The first pattern ??c? was ?f? killed??
is generated.
4. Voice Transformation: with morphology
techniques, verbs are expanded with all their tense
forms ( i.e. present, past tense and past participle).
The tokens? order is adjusted to transform between
active voice and passive voice. Both patterns are
added to the patterns set. For sample question,
the passive pattern is translated into active pattern,
??c? kill ?f??.
5. Preposition addition: for time and location
questions, the preposition (i.e. in, on and at) is
added before the candidate ?c?; Then the pattern
??c? was ?f? killed? is reformulated as ?(in |on)
?c? was ?f? killed?.
6. Tokens shift: preposition phrase token could
be shifted to the begin or the end of pattern, and
?key verb? must be shift before the ?verb phrase?.
Then the pattern ?(in |on) ?c? was ?f? killed? can
be reformulated as ??f? was killed (in |on) ?c??.
7. Definitional patterns: several heuristic pat-
terns, as introduced at (Hildebrandt et al , 2004),
are added into our final pattern sets, such as ??c?,
?f??.
By such heuristic rules, the original pattern set is
obtained from question sentence. The patterns are
initially enclosed in quotation marks, which means
exact matching. However, by eliminating these
quotations, or reducing the scope that they cover,
the matching is relaxed as words co-occurrence.
The patterns are expanded into different strict-level
patterns by adding or removing quotation marks
for each tokens or adjacent tokens combination.
Several condition pattern samples are shown in Ta-
ble 1
Table 1: Sample condition patterns, ? ?? ? denotes exact
match in web query.
? ? <f>(was | were) killed (in | on) <c>?
? ? (in | on) <c>, <f>(was | were) killed?
? ? (in | on) <c>? & ?<f>(was | were) killed?
? ? (in | on) <c>? & ?<f>? & ?(was | were) killed?
? in | on <c><f>(was | were) killed
Each operation introduced above is given a pre-
defined confidence coefficient(cc). Then the con-
fidence coefficient of a pattern is defined as the
multiplication of cc for all performed operations
to generate this pattern.
4.2.2 Condition Pattern Ranking
From the previous step, a set of condition pat-
terns and corresponding confidence coefficient are
obtained. Let p
i
denotes the ith pattern in the pat-
tern set, and cc
i
is the confidence coefficient for the
ith pattern. The confidence coefficient estimation
in previous section contains much noise. And the
patterns with similar confidence coefficient make
little difference. Therefore, the exact confidence
coefficient value is not directly used. We cluster
the patterns into different priority groups. C
j
de-
notes the pattern cluster with jth priority. Here,
the smaller j means higher priority. The condi-
tion patterns are ranked mainly based on confi-
dence coefficient and the number of double quo-
tation marks. The following algorithm shows each
step in detail:
Table 2: patterns ranking algorithm
Input patterns set C = {(p
i
, cc
i
)}
Algorithm
(1) Initialize C
j
= ?, j = 0
(2) if C is empty, end this algorithm
(3) Select (p
max
, cc
max
), where cc
max
?
cc
i
, (p
i
, cc
i
) ? C
(4) if C
j
is empty, add cc
max
into C
j
, jump to
(2)
(5) select the minimum confidence coefficient
(p
min
, cc
min
) from C
j
, compare it with
(p
max
, cc
max
). if the number of double
quotes(??) in p
min
is equal to the number in
p
max
, add p
max
into C
j
. otherwise, j =
j + 1, C
j
= {p
max
}.
(6) jump to (2) and repeat
4.3 Distance Calculation
Conditional min distance d
min
is used to mea-
sure the relevance between question and candidate.
From section 3, d
min
is not computable, but ap-
proximated by frequency counts based on the cod-
ing theory:
46
dmin
(x, y|c)
=
K
(
c(x,y)
)
?max{K
(
c(x,?)
)
,K
(
c(?,y)
)
}
min{K
(
c(x,?)
)
,K
(
c(?,y)
)
}?K
(
c(?,?)
)
=
log f
(
c(x,y)
)
?min{log f
(
c(x,?)
)
,log f
(
c(?,y)
)
}
max{log f
(
c(x,?)
)
,log f
(
c(?,y)
)
}?log f
(
c(?,?)
)
The function c(x, ?) means substituting ?c? in c
by answer candidate x and removing placeholder
?f? if any. Similar definition applies to c(y, ?),
c(x, y). For example, given pattern ??f? was in-
vented in ?c??, question focus ?the telegraph? and
a candidate ?1867?. c(x, ?) is ?was invented in
1867?. c(y, ?) is ?the telegraph was invented?, and
c(x, y) is ?the telegraph was invented in 1867?.
The frequency counts f(x) are estimated as the
number of returned pages by certain search en-
gine with respect to x . f(c(?, ?)) denote the to-
tal pages indexed in search engine. Two types of
search engines ?Google? and ?Altavista? are em-
ployed.
The patterns are selected in priority order to cal-
culate the information distance for each candidate.
5 Experiment and Discussion
5.1 Experiment Setup
Data set: The standard QA test collection (Lin
and Katz, 2006) is employed in our experiments. It
consists of 109 factoid questions, covering several
domains including history, geography, physics, bi-
ology, economics, fashion knowledge, and etc.. 20
candidates are prepared for each questions. All an-
swer candidates are first extracted by the imple-
mented question answering system. Then we re-
view the candidate set for each question. If the cor-
rect answer is not in this set, it is manually added
into the set.
Performance Metric: The top 1 answer precision
and mean reciprocal rank (MRR) are used for per-
formance evaluation.The top 1 answer means the
correct answer ranks first with our distance calcu-
lation method, and MRR =
1
n
?
?
i
(
1
rank
i
), in
which the
1
rank
i
is 1 if the correct answer occurs in
the first position; 0.5 if it firstly occurs in the sec-
ond position; 0.33 for the third, 0.25 for the fourth,
0.2 for the fifth and 0 if none of the first five an-
swers is correct.
The open source factoid QA system ARANEA
(downloaded from Jimmy Lin?s website in 2005)
is used for comparison, which implements an ap-
proximate tfidf algorithm for candidate scoring.
Both ARANEA and our proposed approaches use
the internet directly. Google is used as the search
engine for ARENEA, and our conditional normal-
ized min distance is calculated with Google and
Altavista respectively.
5.2 Experiment Results
The performances of our proposed approach and
ARANEA are shown in Table 3. For top 1 an-
swer precision, our conditional min distance cal-
culation method through Google achieves 69.7%,
and Altavista is 66.1%, which make 56.6%
(69.7% v.s.42.2% ) and 50.0% (66.1% v.s 42.2%)
improvement compared with ARENEA?s tfidf
method. Our proposed methods achieve 0.756 and
0.772 compared with ARENEA?s 0.581 for MRR
measure.
Table 3: Performance comparison, where d
min
(G) denotes
the distance calculation through ?Google?, d
min
(A) through
?Altavista?
tfidf d
min
(G) d
min
(A)
# of Top 1 46 72 69
% of Top 1 42.2 69.7 66.1
MRR 0.581 0.772 0.756
Table 4 shows some correct answer validation
examples. the Google Condition(GC) and the Al-
tavista Condition(AC) columns are the employed
condition patterns for distance calculation. For
question 1400, the conditional normalized google
min distance calculates the distance between ques-
tion focus ?the telegragh? and all 20 answer can-
didates. The minimum distance score is achieved
between ?the telegraph? and ?1837? with the con-
dition pattern ??f? was invented in ?c??. There-
fore, the candidate ?1837? is validated as the cor-
rect answer. Meanwhile, the minimum value for
conditional normalized altavista min distance is
achieved on the same condition.
These results demonstrate that the distance cal-
culation method provides a feasible solution for
answer validation.
In discussion section, we will study three ques-
tions:
1. What is the role of search engine?
2. What is the role of condition pattern?
3. What is the role of question focus?
47
Table 4: Question Examples in conditional information calculation through Google and Altavista. GC:Google Condition;
AC:Altavista Condition
ID Question GC AC Answer Question focus
1400 When was the telegraph
invented?
??y was in-
vented in ?s?
??y was
invented in
?x?
1837 the telegraph
1401 What is the democratic
party symbol?
??y is ?x? ??y is ?x? the don-
key
the democratic
party symbol
1411 What Spanish explorer
discovered the Missis-
sippi River?
??x discov-
ered ?y?
??x? ?dis-
covered?
??y?
Hernando
de Soto
the Mississippi
River
1412 Who is the governor of
Colorado?
??y is ?x? ??y, ?x? Gov. Bill
Ritter
the governor of
Colorado
1484 What college did Allen
Iverson attend?
??y attended
?x?
??x? ?did
?y?
Georgetown
Univer-
sity
Allen Iverson at-
tend
5.3 Discussions
5.3.1 Role of Search Engine
The rise of world-wide-web has enticed millions
of users to create billions of web pages. The re-
dundancy of web information is an important re-
source for question answering. Our Kolmogorov
Complexity based information distance is approx-
imated with query frequency obtained by search
engine. Two types of search engines ?Google? and
?Altavista? are employed in this paper. The num-
ber of top 1 correct answer is 72 through ?Google?
and 69 through ?Altavista?. There is little differ-
ence between two numbers, which shows that the
information distance based on Kolmogorov Com-
plexity is independent of special search engine.
The performance didn?t vary much with the change
of search engine. Actually, if the local data is ac-
cumulated large enough, the information distance
can be approximated without the internet. The
quality and size of data set affect the experiment
performance.
5.3.2 Role of Condition Pattern
Pattern set offers convenient and flexible condi-
tion for information distance calculation. In the
experiment, there are 61 questions correctly an-
swered by both Google and Altavista. 46 ques-
tions of them employ different patterns. Consider-
ing Question 1412, the condition pattern in Google
is ??c? is ?f??, while in Altavista, it is ??f?, ?c??.
However, the correct answer ?Gov. Bill Ritter? is
identified by both methods. The information dis-
tance is stable over specific condition patterns.
5.3.3 Role of Question Focus
Question focus is considered as the discrimina-
tor for the question. The distance between a ques-
tion and a candidate is reformulated as the distance
between question focus and candidate conditioned
on a set of surface patterns. The proposed ap-
proach may not properly extract the question fo-
cus, but the answers can be correctly identified
when the condition pattern becomes loose enough.
Take the question 1484 ?What college did Allen
Iverson attend?? as example, the verb ?attend? is
tagged as ?noun?, then question focus is mistak-
enly extracted as ?Allen Iverson attend?, instead of
the correct ?Allen Iverson?. The two conditional
information distance method still identify the cor-
rect answer ?Georgetown University?. Because
they both employed the looser condition patterns
???c?? ??f??? and ???c?? did ??f???.Therefore, our
proposed distance answer validation methods are
robust to the question focus selection component.
From the discussion above, it can be seen that
our algorithm is stable and robust, not depending
on the specific search engine, condition pattern,
and question focus.
6 Conclusions
We have presented a novel approach for answer
validation based on information distance. The an-
swer validation task is reformulated as distance
calculation between question focus and candidate
conditioned on a set of surface patterns. The ex-
periments show that our proposed answer valida-
tion method makes a great improvement compared
48
with ARANEA?s tfidf method. Furthermore, The
experiments show that our approach is stable and
robust, not depending on the specific search en-
gine, condition pattern, and question focus. In fu-
ture work, we will try to calculate information dis-
tance in the local constructed data set, and expand
this distance measure into other application fields.
Acknowledgement
This work is supported by National Natural Sci-
ence Foundation of China (60572084, 60621062),
Hi-tech Research and Development Program of
China (2006AA02Z321), National Basic Research
Program of China (2007CB311003).
References
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, Ad-
wait Ratnaparkhi, and Richard J.Mammone. 2001.
Question answering using maximum entropy conm-
ponents. In Proceedings of the Second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language tecnologies.
Anselmo Penas, A. Rodrigo, F. Verdejo. 2007.
Overview of the Answer Validation Exercise 2007.
Working Notes for the CLEF 2007 Workshop.
C.H. Bennett, P. Gacs, M. Li, P. Vit?anyi, W. Zurek..
1998. Information Distance. IEEE Trans. Inform.
Theory, 44:4, 1407?1423.
Eric Brill and Susan Dumais and Michele Banko. 2002.
An analysis of the AskMSR question-answering sys-
tem. EMNLP ?02: the ACL-02 conference on Em-
pirical methods in natural language processing.
Hildebrandt W., Katz B., and Lin J. 2004. Answer-
ing Definition Questions Using Multiple Knowledge
Sources. Proceedings of Human Language Technol-
ogy Conference. Boston, USA.
Jeongwoo Ko, Luo Si, Eric Nyberg. 2007. A Proba-
bilistic Graphical Model for Joint Answer Ranking
in Question Answering. In Proceedings of the 30th
annual international ACM SIGIR conference on Re-
search and development in information retrieval.
Jimmy Lin and Boris Katz. 2006. Building a reusable
test collection for question answering. J. Am. Soc.
Inf. Sci. Technol..
Jimmy Lin. 2007. An Exploration of the Principles Un-
derlying Redundancy-Based Factoid Question An-
swering. ACM Transactions on Information Sys-
tems, 27(2):1-55.
Jinxi Xu, Ana Licuanan and Ralph Weischedel. 2003.
Trec 2003 qa at bbn: Answering definitional ques-
tions. In Proceedings of the 12th Text REtrieval
Conference, Gaithersburgh, MD, USA.
Ming Li and Paul MB Vitanyi. 1997. An Introduc-
tion to Kolmogorov Complexity and Its Applications.
Working Notes for the CLEF 2007 Workshop.
M. Li, J. Badger, X. Chen, S. Kwong, P. Kearney,
H. Zhang.. 2001. An information-based sequence
distance and its application to whole mitochondrial
genome phylogeny. Bioinformatics, 17:2.
M. Subbotin and S. Subbotin. 2001. Patterns of Po-
tential Answer Expressions as Clues to the Right An-
swers. In TREC-10 Notebook papers. Gaithesburg,
MD.
R. Cilibrasi, P.M.B. Vit?anyi. 2007. An Exploration of
the Principles Underlying Redundancy-Based Fac-
toid Question Answering. EEE Trans. Knowledge
and Data Engineering, 19:3, 370?383.
Shen, Dan and Dietrich Klakow . 2006. Exploring cor-
relation of dependency relation paths for answer ex-
traction. In Proceedings of COLING-ACL, Sydney,
Australia.
Xian Zhang, Yu Hao, Xiaoyan Zhu, and Ming Li. 2007.
Information Distance from a Question to an Answer.
In Proceedings of the 13th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining.
49
