Using Query Patterns to Learn the Duration of Events
Andrey Gusev Nathanael Chambers Pranav Khaitan Divye Khilnani
Steven Bethard Dan Jurafsky
Department of Computer Science, Stanford University
{agusev,nc,pranavkh,divyeraj,bethard,jurafsky}@cs.stanford.edu
Abstract
We present the first approach to learning the durations of events without annotated training data,
employing web query patterns to infer duration distributions. For example, we learn that ?war?
lasts years or decades, while ?look? lasts seconds or minutes. Learning aspectual information is an
important goal for computational semantics and duration information may help enable rich document
understanding. We first describe and improve a supervised baseline that relies on event duration
annotations. We then show how web queries for linguistic patterns can help learn the duration of
events without labeled data, producing fine-grained duration judgments that surpass the supervised
system. We evaluate on the TimeBank duration corpus, and also investigate how an event?s participants
(arguments) effect its duration using a corpus collected through Amazon?s Mechanical Turk. We make
available a new database of events and their duration distributions for use in research involving the
temporal and aspectual properties of events.
1 Introduction
Bridging the gap between lexical knowledge and world knowledge is crucial for achieving natural language
understanding. For example, knowing whether a nominal is a person or organization and whether a person
is male or female substantially improves coreference resolution, even when such knowledge is gathered
through noisy unsupervised approaches (Bergsma, 2005; Haghighi and Klein, 2009). However, existing
algorithms and resources for such semantic knowledge have focused primarily on static properties of
nominals (e.g. gender or entity type), not dynamic properties of verbs and events.
This paper shows how to learn one such property: the typical duration of events. Since an event?s
duration is highly dependent on context, our algorithm models this aspectual property as a distribution
over durations rather than a single mean duration. For example, a ?war? typically lasts years, sometimes
months, but almost never seconds, while ?look? typically lasts seconds or minutes, but rarely years or
decades. Our approach uses web queries to model an event?s typical distribution in the real world.
Learning such rich aspectual properties of events is an important area for computational semantics,
and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way
that gender has benefited nominal coreference systems. Event durations are also key to building event
timelines and other deeper temporal understandings of a text (Verhagen et al, 2007; Pustejovsky and
Verhagen, 2009).
The contributions of this work are:
? Demonstrating how to acquire event duration distributions by querying the web with patterns.
? Showing that a system that predicts event durations based only on our web count distributions can
outperform a supervised system that requires manually annotated training data.
? Making available an event duration lexicon with duration distributions for common English events.
We first review previous work and describe our re-implementation and augmentation of the latest
supervised system for predicting event durations. Next, we present our approach to learning event
distributions based on web counts. We then evaluate both of these models on an existing annotated corpus
of event durations and make comparisons to durations we collected using Amazon?s Mechanical Turk.
Finally, we present a generated database of event durations.
145
2 Previous Work
Early work on extracting event properties focused on linguistic aspect, for example, automatically
distinguishing culminated events that have an end point from non-culminated events that do not (Siegel
and McKeown, 2000). The more fine-grained task of predicting the duration of events was first proposed
by Pan et al (2006), who annotated each event in a small section of the TimeBank (Pustejovsky et al,
2003) with duration lower and upper bounds. They then trained support vector machines on their annotated
corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes,
hours, etc. Their models used features like bags of words, heads of syntactic subjects and objects, and
WordNet hypernyms of the events. This work provides a valuable resource in its annotated corpus and is
also a good baseline. We replicate their work and also add new features as described below.
Our approach to the duration problem is inspired by the standard use of web patterns for the acquisition
of relational lexical knowledge. Hearst (1998) first observed that a phrase like ?. . . algae, such as
Gelidium. . . ? indicates that ?Gelidium? is a type of ?algae?, and so hypernym-hyponym relations can
be identified by querying a text collection with patterns like ?such <noun> as <noun>? and ?<noun> ,
including <noun>?. A wide variety of pattern-based work followed, including the application of the idea
in VerbOcean to acquire aspects and temporal structure such as happens-before, using patterns like ?to
<verb> and then <verb>? (Chklovski and Pantel, 2004).
More recent work has learned nominal gender and animacy by matching patterns like ?<noun> *
himself? and ?<noun> and her? to a corpus of Web n-grams (Bergsma, 2005; Ji and Lin, 2009). Phrases like
?John Joseph?, which were observed often with masculine pronouns and never with feminine or neuter
pronouns, can thus have their gender identified as masculine. Ji and Lin found that such web-counts can
predict person names as well as a fully supervised named entity recognition system.
Our goal, then, is to integrate these two strands in the literature, applying pattern/web approaches to
the task of estimating event durations. One difference from previous work is the distributional nature of
the extracted knowledge. In the time domain, unlike in most previous relation-extraction domains, there is
rarely a single correct answer: ?war? may last months, years or decades, though years is the most likely.
Our goal is thus to produce a distribution over durations rather than a single mean duration.
3 Duration Prediction Tasks
In both our supervised and unsupervised models, we consider two types of event duration predictions: a
coarse-grained task in which we only want to know whether the event lasts more or less than a day, and a
fine-grained task in which we want to know whether the event lasts seconds, minutes, hours, days, weeks,
months or years. These two duration prediction tasks were originally suggested by Pan et al (2006), based
on their annotation of a subset of newspaper articles in the Timebank corpus (Pustejovsky et al, 2003).
Events were annotated with a minimum and maximum duration like the following:
? 5 minutes ? 1 hour: A Brooklyn woman who was watching her clothes dry in a laundromat.
? 1 week ? 3 months: Eileen Collins will be named commander of the Space Shuttle mission.
? 3 days ? 2 months: President Clinton says he is committed to a possible strike against Iraq. . .
Pan et al suggested the coarse-grained binary classification task because they found that the mean event
durations from their annotations were distributed bimodally across the corpus, roughly split into short
events (less than a day) and long events (more than a day). The fine-grained classification task provides
additional information beyond this simple two way distinction.
For both tasks, we must convert the minimum/maximum duration annotations into single labels. We
follow Pan et al (2006) and take the arithmetic mean of the minimum and maximum durations in seconds.
For example, in the first event above, 5 minutes would be converted into 300 seconds, 1 hour would be
converted into 3600 seconds, the resulting mean would be 1950 seconds, and therefore this event would
be labeled less-than-a-day for the coarse-grained task, and minutes for the fine-grained task. These labels
can then be used directly to train and evaluate our models.
146
4 Supervised Approach
Before describing our query-based approach, we describe our baseline, a replication and extension of the
supervised system from Pan et al (2006). We first briefly describe their features, which are shared across
the coarse and fine-grained tasks, and then suggest new features.
4.1 Pan et. al. Features
The Pan et al (2006) system included the following features which we also replicate:
Event Properties: The event token, lemma and part of speech (POS) tag.
Bag of Words: The n tokens to the left and right of the event word. However, because Pan et al
found that n = 0 performed best, we omit this feature.
Subject and Object: The head word of the syntactic subject and object of the event, along with their
lemmas and POS tags. Subjects and objects provide important context. For example, ?saw Europe? lasts
for weeks or months while ?saw the goal? lasts only seconds.
Hypernyms: WordNet hypernyms for the event, its subject and its object. Starting from the first
synset of each lemma, three hypernyms were extracted from the WordNet hierarchy. Hypernyms can help
cluster similar events together. For example, the event plan had three hypernym ancestors as features:
idea, content and cognition.
4.2 New Features
We present results for our implementation of the Pan et al (2006) system in Section 8. However, we also
implemented additional features.
Event Attributes: Timebank annotates individual events with four attributes: the event word?s tense
(past, present, future, none), aspect (e.g., progressive), modality (e.g., could, would, can, etc.), and event
class (occurrence, aspectual, state, etc.). We use each of these as a feature in our classifier. The aspect and
tense of the event, in particular, are well known indicators of the temporal shape of events (Vendler, 1976).
Named Entity Classes: Pan et al found the subject and object of the events to be useful features,
helping to identify the particular sense of the event. We used a named entity recognizer to add more
information about the subjects and objects, labeling them as persons, organizations, locations, or other.
Typed Dependencies: We coded aspects of the subcategorization frame of a predicate, such as
transitivity, or the presence of prepositional objects or adverbial modifiers, by adding a binary feature
for each typed dependency1 seen with a verb or noun. We experimented with including the head of the
argument itself, but results were best when only the dependency type was included.
Reporting Verbs: Many of the events in Timebank are reporting verbs (say, report, reply, etc.). We
used a list of reporting verbs to identify these events with a binary feature.
4.3 Classifier
Both the Pan et al feature set and our extended feature set were used to train supervised classifiers for the
two event duration prediction tasks. We experimented with naive bayes, logistic regression, maximum
entropy and support vector machine classifiers, but as discussed in Section 8, the maximum entropy model
performed best in cross-validations on the training data.
5 Unsupervised Approach
While supervised learning is effective for many NLP tasks, it is sensitive to the amount of available
training data. Unfortunately, the training data for event durations is very small, consisting of only 58 news
articles (Pan et al, 2006), and labeling further data is quite expensive. This motivates our desire to find an
1We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003).
147
approach that does not rely on labeled data, but instead utilizes the large amounts of text available on the
Web to search for duration-specific patterns. This section describes our web-based approach to learning
event durations.
5.1 Web Query Patterns
Temporal properties of events are often described explicitly in language-specific constructions which can
help us infer an event?s duration. Consider the following two sentences from our corpus:
? Many spend hours surfing the Internet.
? The answer is coming up in a few minutes.
These sentences explicitly describe the duration of the events. In the first, the dominating clause spend
hours tells us how long surfing the Internet lasts (hours, not seconds), and in the second, the preposition
attachment serves a similar role. These examples are very rare in the corpus, but as can be seen, are
extremely informative when present. We developed several such informative patterns, and searched the
Web to find instances of them being used with our target events.
For each pattern described below, we use Yahoo! to search for the patterns occurring with our events.
We collect the total hit counts and use them as indicators of duration. The Yahoo! search API returns two
numbers for a query: totalhits and deephits. The former excludes duplicate pages and limits the number
of documents per domain while the latter includes all duplicates. We take the sum of these two numbers
as our count (this worked better than either of the two individually on the training data and provides
a balance between the benefits of each estimate) and normalize the results as described in Section 5.2.
Queries are submitted as complete phrases with quotation marks, so the results only include exact phrase
matches. This greatly reduces the number of hits, but results in more precise distributions.
5.1.1 Coarse-Grained Patterns
The coarse grained task is a binary decision: less than a day or more than a day. We can model this
task directly by looking for constructions that can only be used with events that take less than a day.
The adverb yesterday fills this role nicely; an event modified by yesterday strongly implies that it took
place within a single day?s time. For example, ?shares closed at $18 yesterday? implies that the closing
happened in less than a day. We thus consider the following two query patterns:
? <eventpast> yesterday
? <eventpastp> yesterday
where <eventpast> is the past tense (preterite) form of the event (e.g., ran), and <eventpastp> is the past
progressive form of the event (e.g., was running).
5.1.2 Fine-Grained Patterns
For the fine-grained task, we need patterns that can identify when an event falls into any of the various
buckets: seconds, minutes, hours, etc. Thus, our fine-grained patterns are parameterized both by the event
and by the bucket of interest. We use the following patterns inspired in part by Dowty (1979):
1. <eventpast> for * <bucket>
2. <eventpastp> for * <bucket>
3. spent * <bucket> <eventger>
where <eventpast> and <eventpastp> are defined as above, <eventger> is the gerund form of the event (e.g.,
running), and the wildcard ?*? can match any single token2.
The following three patterns ultimately did not improve the system?s performance on the training data:
4. <eventpast> in * <bucket>
5. takes * <bucket> to <event>
6. <eventpast> last <bucket>
Pattern 4 returned a lot of hits, but had low precision as it picked up many non-durative expressions.
Pattern 5 was very precise but typically returned few hits, and pattern 6 worked for, e.g., last week, but did
not work for shorter durations. All reported systems use patterns 1-3 and do not include 4-6.
2We experimented with varying numbers of wildcards but found little difference in performance on the training data.
148
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(a) ?was saying for <bucket>?
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(b) ?for <bucket>?
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(c) (a) counts divided by (b) counts
Figure 1: Normalizing the distribution for the pattern ?was saying for <bucket>?.
We also tried adding subjects and/or objects to the patterns when they were present for an event.
However, we found that the benefit of the extra context was outweighed by the significantly fewer hits that
resulted. We implemented several backoff approaches that removed the subject and object from the query,
however, the counts from these backoff approaches were less reliable than just using the base event.
5.2 Predicting Durations from Patterns
To predict the duration of an event from the above patterns, we first insert the event into each pattern
template and query the web to see how often the filled template occurs. These counts form a distribution
over each of the bins of interest, e.g., in the fine-grained task we have counts for seconds, minutes, hours,
etc. We discard pattern distributions with very low total counts, and normalize the remaining pattern
distributions based on the frequency with which the pattern occurs in general. Finally, we uniformly
merge the distributions from all patterns, and use the resulting distribution to select a duration label for
the event. The following sections detail this process.
5.2.1 Coarse-Grained Prediction
For the coarse-grained task of less than a day vs. more than a day, we collect counts using the two
yesterday patterns described above. We then normalize these counts by the count of the event?s occurrence
in general. For example, given the event run, we query for ?ran yesterday? and divide by the count of
?ran?. This gives us the probability of seeing yesterday given that we saw ran. We average the probabilities
from the two yesterday patterns, and classify an event as lasting less than a day if its average probability
exceeds a threshold t. We optimized t to our training set (t = .002). This basically says that if an event
occurs with yesterday more than 0.2% of the time, we will assume that the event lasts less than a day.
5.2.2 Fine-Grained Prediction
As with the coarse-grained task, our fine-grained approach begins by collecting counts using the three
fine-grained patterns discussed above. Since each fine-grained pattern has both an <event> and a <bucket>
slot to be filled, for a single event and a single pattern, we end up making 8 queries to cover each of the 8
buckets: seconds, minutes, hours, days, weeks, months, years and decades. After these queries, we have a
pattern-specific distribution of counts over the various buckets, a coarse measure of the types of durations
that might be appropriate to this event. Figure 1(a) shows an example of such a distribution.
As can be seen in Figure 1(a), this initial distribution can be skewed in various ways ? in this case,
years is given far too much mass. This is because in addition to the single event interpretation of words
like ?saying?, there are iterative or habitual interpretations (Moens and Steedman, 1988; Frawley, 1992).
Iterative events occur repeatedly over a period of time, e.g., ?he?s been saying for years that. . . ? The two
interpretations are apparent in the raw distributions of smile and run in Figure 2. The large peak at years
for run shows that it is common to say someone ?was running for years.? Conversely, it is less common to
say someone ?was smiling for years,? so the distribution for smile is less biased towards years.
149
        

	
A
B
C
D 
	AB
CDBEA	FB
Figure 2: Two double peaked distributions.
Coverage of Fine-Grained Query Patterns
Number of Patterns Total Events Precision
At least one 1359 (81.7%) 57.3
At least two 1142 (68.6%) 58.6
All three 428 (25.7%) 65.7
Figure 3: The number of events that match n fine-
grained patterns and the pattern precision on these
events. The training set consists of 1664 events.
While the problem of distinguishing single events from iterative events is out of the scope of this paper
(though an interesting avenue for future research), we can partially address the problem by recognizing
that some buckets are simply more frequent in text than others. For example, Figure 1(b) shows that it is
by far more common to see ?for <bucket>? filled with years than with any other duration unit. Thus, for
each bucket, we divide the counts collected with the event patterns by the counts we get for the pattern
without the event3. Essentially, this gives us for each bucket the probability of the event given that bucket.
Figure 1(c) shows that the resulting normalized distribution fits our intution of how long ?saying? should
last much better than the raw counts: seconds and minutes have much more of the mass now.
After normalizing an event?s counts for each pattern, we combine the distributions from the three
different patterns if their hit counts pass certain confidence thresholds. The total hit count for each pattern
must exceed a minimum threshold tmin = 100 and not exceed a maximum threshold tmax = 100, 000
(both thresholds were optimized on the training data). The former avoids building distributions from a
sparse number of hits, and the latter avoids classifying generic and polysemous events like ?to make? that
return a large number of hits. We found such events to produce generic distributions that do not help in
classification. If all three patterns pass our confidence thresholds, we merge the pattern distributions by
summing them bucket-wise together and renormalizing the resulting distribution to sum to 1. Merging the
patterns mitigates the noise from any single pattern.
To predict the event?s duration, we then select the bucket with the highest smoothed score:
score(bi) = bi?1 + bi + bi+1
where bi is a duration bucket and 0 < i < 9. We define b0 = b9 = 0. In other words, the score of the
minute bucket is the sum of three buckets: second, minute and hour. This parallels the smoothing of the
evaluation metric introduced by (Pan et al, 2006) which we also adopt for evaluation in Section 7.
In the case that fewer than three of our patterns matched, we backoff to the majority class (months for
fine-grained, and more-than-a-day for coarse-grained). We experimented with only requiring one or two
patterns to match, but found the best results on training when requiring all three. Figure 3 shows the large
jump in precision when all three are required. The evaluation is discussed in Section 7.
5.2.3 Coarse-Grained Prediction via Fine-Grained Prediction
We can also use the distributions collected from the fine-grained task to predict coarse-grained labels. We
use the above approach and return less than a day if the selected fine-grained bucket was seconds, minutes
or hours, and more than a day otherwise. We also tried summing over the duration buckets: p(seconds) +
p(minutes) + p(hours) for less than day and p(days) + p(weeks) + p(months) + p(years) + p(decades) for
more than a day, but the simpler approach outperformed these summations in training.
3We also explored normalizing not by the global distribution on the Web, but by the average of the distributions of all the
events in our dataset. However, on the training data, using the global distribution performed better.
150
6 Datasets
6.1 Timebank Duration
As described in Section 3, Pan et al (2006) labeled 58 documents with event durations. We follow their
method of isolating the 10 WSJ articles as a separate test set which we call TestWSJ (147 events). For
the remaining 48 documents, they split the 2132 event instances into a Train and Test set with 1705 and
427 events respectively. Their split was conducted over the bag of events, so their train and test sets may
include events that came from the same document. Their particular split was unavailable.
We instead use a document-split that divides the two sets into bins of documents. Each document?s
entire set of events is assigned to either the training set or the test set, so we do not mix events across
sets. Since documents often repeat mentions of events, this split is more conservative by not mixing test
mentions with the training set. Train, Test, and TestWSJ contain 1664 events (714 unique verbs), 471 events
(274 unique), and 147 events (84 unique) respectively. For each base verb, we created queries as described
in Section 5.1.2. The train/test split is available at http://cs.stanford.edu/people/agusev/durations/.
6.2 Mechanical Turk Dataset
We also collected event durations from Amazon?s Mechanical Turk (MTurk), an online marketplace from
Amazon where requesters can find workers to solve Human Intelligence Tasks (HITs) for small amounts
of money. Prior work has shown that human judgments from MTurk can often be as reliable as trained
annotators (Snow et al, 2008) or subjects in controlled lab studies (Munro et al, 2010), particularly when
judgments are aggregated over many MTurk workers (?Turkers?). Our motivation for using Turkers is to
better analyze system errors. For example, if we give humans an event in isolation (no sentence context),
how well can they guess the durations assigned by the Pan et. al. annotators? This measures how big the
gap is between a system that looks only at the event, and a system that integrates all available context.
To collect event durations from MTurk, we presented Turkers with an event from the TimeBank (a
superset of the events annotated by Pan et al (2006)) and asked them to decide whether the event was most
likely to take seconds, minutes, hours, days, weeks, months, years or decades. We had events annotated
in two different contexts: in isolation, where only the event itself was given (e.g., ?allocated?), and in
subject-object context, where a minimal phrase including the event and its subject and object was given
(e.g., ?the mayor allocated funds?). In both types of tasks, we asked 10 Turkers to label each event,
and they were paid $0.0025 for each annotation ($0.05 for a block of 20 events). To filter out obvious
spammers, we added a test item randomly to each block, e.g., adding the event ?minutes? and rejecting
work from Turkers who labeled this anything other than the duration minutes.
The resulting annotations give duration distributions for each of our events. For example, when
presented the event ?remodeling?, 1 Turker responded with days, 6 with weeks, 2 with months and 1
with years. These annotations suggest that we generally expect ?remodeling? to take weeks, but it may
sometimes take more or less. To produce a single fine-grained label from these distributions, we take the
duration bin with the largest number of Turker annotations, e.g. for ?remodeling?, we would produce the
label weeks. To produce a single coarse-grained label, we use the label less-than-a-day if the fine-grained
label was seconds, minutes or hours and more-than-a-day otherwise.
7 Experiment Setup
As discussed in Section 3, we convert the minimum and maximum duration annotations into labels by
converting each to seconds using ISO standards and calculating the arithmetic mean. If the mean is
? 86400 seconds, it is considered less-than-a-day for the coarse-grained task. The fine-grained buckets
are similarly calculated, e.g., X is labeled days if 86400 < X ? 604800. The Pan et al (2006) evaluation
does not include a decades bucket, but our system still uses ?decades? in its queries.
We optimized all parameters of both the supervised and unsupervised systems on the training set, only
running on test after selecting our best performing model. We compare to the majority class as a baseline,
151
Coarse-Grained
Test TestWSJ
Supervised, Pan 73.3 73.5
Supervised, all 73.0 74.8
Fine-Grained
Test TestWSJ
Supervised, Pan 62.2 61.9
Supervised, all 62.4 66.0
Figure 4: Accuracies of the supervised maximum entropy classifiers with two different feature sets.
Coarse-Grained
Test TestWSJ
Majority class 62.4 57.1
Supervised, all 73.0* 74.8*
Web counts, yesterday 70.7* 74.8*
Web counts, buckets 72.4* 73.5*
Fine-Grained
Test TestWSJ
Majority class 59.2 52.4
Supervised, all 62.4 66.0?
Web counts, buckets 66.5* 68.7*
Figure 5: System accuracy compared against supervised and majority class. * indicates statistical
significance (McNemar?s Test, two-tailed) against majority class at the p < 0.01 level, ? at p < 0.05
tagging all events as more-than-a-day in the coarse-grained task and months in the fine-grained task.
To evaluate our models, we use simple accuracy on the coarse-grained task, and approximate agreement
matching as in Pan et al (2006) on the fine-grained task. In this approximate agreement, a guess is
considered correct if it chooses either the gold label or its immediate neighbor (e.g., hours is correct if
minutes, hours or days is the gold class). Pan et al use this approach since human labeling agreement is
low (44.4%) on the exact agreement fine-grained task.
8 Results
Figure 4 compares the performance of our two supervised models; the reimplementation of Pan et al
(2006) (Supervised, Pan), and our improved model with new features (Supervised, all). The new model
performs similarily to the Pan model on the in-domain Test set, but better on the out-of-domain financial
news articles in the TestWSJ test. On the latter, the new model improves over Pan et al by 1.3% absolute
on the coarse-grained task, and by 4.1% absolute on the fine-grained task. We report results from the
maximum entropy model as it slightly outperformed the naive bayes and support vector machine models4.
We compare these supervised results against our web-based unsupervised systems in Figure 5. For the
coarse-grained task, we have two web count systems described in Section 5: one based on the yesterday
patterns (Web counts, yesterday), and one based on first gathering the fine-grained bucket counts and
then converting those to coarse-grained labels (Web counts, buckets). Generally, these models perform
within 1-2% of the supervised model on the coarse-grained task, though the yesterday-based classifier
exactly matches the supervised system?s performance on the TestWSJ data. The supervised system?s
higher results are not statistically significant against our web-based systems.
For the fine-grained task, Figure 5 compares our web counts algorithm based on duration distributions
(Section 5) to the baseline and supervised systems. Our web counts approach outperforms the best
supervised system by 4.1% absolute on the Test set and by 2.7% absolute on the out-of-domain TestWSJ.
To get an idea of how much the subject/object context could help predict event duration if integrated
perfectly, we evaluated the Mechanical Turk annotations against the Pan et. al. annotated dataset using
approximate agreement as described in Section 7. Figure 6 gives the performance of the Turkers given
two types of context: just the event itself (Event only), and the event plus its subject and/or object (Event
and args). Turkers performed below the majority class baseline when given only the event, but generally
above the baseline when given the subject and object, improving up to 20% over the event-only condition.
Figure 7 shows examples of events with different learned durations.
4This differs from Pan et al who found support vector machines to be the best classifier.
152
Mechanical Turk Accuracy
Coarse Fine
Test WSJ Test WSJ
Majority class 62.4 57.1 59.2 52.4
Event only 52.0 49.4 42.1 43.8
Event and args 65.0 70.1 56.7 59.9
Figure 6: Accuracy of Mechanical Turkers
against Pan et. al. annotations.
Learned Examples
talk to tourism leaders minutes
driving hours
shut down the supply route days
travel weeks
the downturn across Asia months
build a museum years
Figure 7: Examples of web query durations.
9 Discussion
Our novel approach to learning event durations showed 4.1% and 2.7% absolute gains over a state-of-the-
art supervised classifier. Although the gain is not statistically significant, these results nonetheless suggest
that we are learning as much about event durations from the web counts as we are currently able to learn
with our improvements to Pan et al?s (2006) supervised system. This is encouraging because it indicates
that we may not need extensive manual annotations to acquire event durations. Further, our final query
system achieves these results with only the event word, and without considering the subject, object or
other types of context.
Despite the fact that we saw little gains in performance when including subjects and objects in our
query patterns, the Mechanical Turk evaluation suggests that more information may still be gleaned from
the additional context. Giving Turkers the subject and object improved their label accuracy by 10-20%
absolute. This suggests that finding a way to include subjects and objects in the web queries, for example
by using thesauri to generate related queries, is a valuable line of research for future work.
Finally, these MTurk experiments suggest that classifying events for duration out of context is a
difficult task. Pan et al (2006) reported 0.88 annotator agreement on the coarse-grained task when given
the entire document context. Out of context, given just the event word, our Turkers only achieved 52%
and 49% accuracy. Not surprisingly, the task is more difficult without the document. Our system, however,
was also only given the event word, but it was able achieve over 70% in accuracy. This suggests that rich
language understanding is often needed to correctly label an event for duration, but in the absence of such
understanding, modeling the duration by web counts appears to be a practical and useful alternative.
10 A Database of Event Durations
Given the strong performance of our model on duration classification, we are releasing a database of
events and their normalized duration distributions, as predicted by our bucket-based fine-grained model.
We extracted the 1000 most frequent verbs from a newspaper corpus (the NYT portion of Gigaword
Graff (2002)) with the 10 most frequent grammatical objects of each verb. These 10, 000 events and their
duration distributions are available at http://cs.stanford.edu/people/agusev/durations/.
Acknowledgements
Thanks to Chris Manning and the anonymous reviewers for insightful comments and feedback. This
research draws on data provided by Yahoo!, Inc., through its Yahoo! Search Services offering. We
gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-
C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those
of the author(s) and do not necessarily reflect the view of DARPA, AFRL, or the US government.
153
References
Bergsma, S. (2005). Automatic acquisition of gender information for anaphora resolution. In Advances
in Artificial Intelligence, Volume 3501 of Lecture Notes in Computer Science, pp. 342?353. Springer
Berlin / Heidelberg.
Chen, Z. and H. Ji (2009). Graph-based event coreference resolution. In Proceedings of the Workshop on
Graph-based Methods for Natural Language Processing (TextGraphs-4), Singapore, pp. 54?57. ACL.
Chklovski, T. and P. Pantel (2004). Verbocean: Mining the web for fine-grained semantic verb relations.
In D. Lin and D. Wu (Eds.), Proceedings of EMNLP 2004, Barcelona, Spain, pp. 33?40.
Dowty, D. R. (1979). Word Meaning and Montague Grammar. Kluwer Academic Publishers.
Frawley, W. (1992). Linguistic Semantics. Routledge.
Graff, D. (2002). English Gigaword. Linguistic Data Consortium.
Haghighi, A. and D. Klein (2009). Simple coreference resolution with rich syntactic and semantic features.
In Proceedings of EMNLP-2009, Singapore, pp. 1152?1161.
Hearst, M. A. (1998). Automated discovery of wordnet relations. In WordNet: An Electronic Lexical
Database. MIT Press.
Ji, H. and D. Lin (2009). Gender and animacy knowledge discovery from web-scale n-grams for
unsupervised person mention detection. In Proceedings of the Pacific Asia Conference on Language,
Information and Computation.
Klein, D. and C. D. Manning (2003). Accurate unlexicalized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics, Sapporo, Japan, pp. 423?430.
Moens, M. and M. Steedman (1988). Temporal ontology in natural language. Computational Linguis-
tics 2(14), 15?21.
Munro, R., S. Bethard, V. Kuperman, V. T. Lai, R. Melnick, C. Potts, T. Schnoebelen, and H. Tily (2010).
Crowdsourcing and language studies: the new generation of linguistic data. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk,
Los Angeles, pp. 122?130.
Pan, F., R. Mulkar, and J. Hobbs (2006). Learning event durations from event descriptions. In Proceedings
of COLING-ACL.
Pustejovsky, J., P. Hanks, R. Sauri, A. See, D. Day, L. Ferro, R. Gaizauskas, M. Lazo, A. Setzer, and
B. Sundheim (2003). The timebank corpus. Corpus Linguistics, 647?656.
Pustejovsky, J. and M. Verhagen (2009). Semeval-2010 task 13: Evaluating events, time expressions, and
temporal relations (tempeval-2). In Proceedings of the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009), Boulder, Colorado, pp. 112?116.
Siegel, E. V. and K. R. McKeown (2000). Learning methods to combine linguistic indicators: improving
aspectual classification and revealing linguistic insights. Computational Linguistics 26(4), 595?628.
Snow, R., B. O?Connor, D. Jurafsky, and A. Ng (2008). Cheap and fast ? but is it good? evaluating
non-expert annotations for natural language tasks. In Proceedings of EMNLP-2008, Hawaii.
Vendler, Z. (1976). Verbs and times. Linguistics in Philosophy, 97?121.
Verhagen, M., R. Gaizauskas, F. Schilder, M. Hepple, G. Katz, and J. Pustejovsky (2007). Semeval-2007
task 15: Tempeval temporal relation identification. In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, pp. 75?80.
154
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 2?10,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Customizing an Information Extraction System to a New Domain
Mihai Surdeanu, David McClosky, Mason R. Smith, Andrey Gusev,
and Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
{mihais,mcclosky,mrsmith,manning}@stanford.edu
agusev@cs.stanford.edu
Abstract
We introduce several ideas that improve the
performance of supervised information ex-
traction systems with a pipeline architecture,
when they are customized for new domains.
We show that: (a) a combination of a se-
quence tagger with a rule-based approach for
entity mention extraction yields better perfor-
mance for both entity and relation mention
extraction; (b) improving the identification of
syntactic heads of entity mentions helps rela-
tion extraction; and (c) a deterministic infer-
ence engine captures some of the joint domain
structure, even when introduced as a post-
processing step to a pipeline system. All in all,
our contributions yield a 20% relative increase
in F1 score in a domain significantly differ-
ent from the domains used during the devel-
opment of our information extraction system.
1 Introduction
Information extraction (IE) systems generally con-
sist of multiple interdependent components, e.g., en-
tity mentions predicted by an entity mention detec-
tion (EMD) model connected by relations via a re-
lation mention detection (RMD) component (Yao et
al., 2010; Roth and Yih, 2007; Surdeanu and Cia-
ramita, 2007). Figure 1 shows a sentence from a
sports domain where both entity and relation men-
tions are annotated. When training data exists, the
best performance in IE is generally obtained by su-
pervised machine learning approaches. In this sce-
nario, the typical approach for domain customiza-
tion is apparently straightforward: simply retrain
on data from the new domain (and potentially tune
model parameters). In this paper we argue that, even
when considerable training data is available, this is
not sufficient to maximize performance. We apply
several simple ideas that yield a significant perfor-
mance boost, and can be implemented with minimal
effort. In particular:
? We show that a combination of a conditional
random field model (Lafferty et al, 2001) with
a rule-based approach that is recall oriented
yields better performance for EMD and for
the downstream RMD component. The rule-
based approach includes gazetteers, which have
been shown to be important by Mikheev et al
(1999), among others.
? We improve the unification of the predicted se-
mantic annotations with the syntactic analy-
sis of the corresponding text, i.e., finding the
syntactic head of a given semantic constituent.
Since many features in an IE system depend on
syntactic analysis, this leads to more consistent
features and better extraction models.
? We add a simple inference engine that gener-
ates additional relation mentions based solely
on the relation mentions extracted by the RMD
model. This engine mitigates some of the limi-
tations of a text-based RMD model, which can-
not extract relations not explicitly stated in text.
We investigate these ideas using an IE system that
performs recognition of entity mentions followed by
extraction of binary relations between these men-
tions. We used as target a sports domain that is sig-
nificantly different from the corpora previously used
with this IE system. The target domain is also sig-
nificantly different from the dataset used to train the
2
Rookie	 ?Mike	 ?Anderson	 ?scored	 ?two	 ?second-??half	 ?touchdowns,	 ?	 ?
leading	 ?the	 ?Broncos	 ?to	 ?their	 ?sixth	 ?straight	 ?victory,	 ?31	 ?-??	 ?24	 ?	 ?
over	 ?the	 ?Sea?le	 ?Seahawks	 ?on	 ?Sunday.	 ?
ScoreType-??2	 ?
FinalScore	 ? FinalScore	 ?NFLGame	 ?NFLTeam	 ?
NFLTeam	 ? Date	 ?
teamInGame	 ?
gameWinner	 ?
touchdownPar?alCount	 ?
teamScoringAll	 ?
teamInGame	 ?
gameDate	 ?
gameLoser	 ?
teamScoringAll	 ?
teamFinalScore	 ?teamFinalScore	 ?
Figure 1: Sample sentence from the NFL domain. The domain contains entity mentions (underlined with entity types
in bold) and binary relations between entity mentions (indicated by arrows; relation types are italicized).
supporting natural language processing tools (e.g.,
syntactic parser). Our investigation shows that, de-
spite their simplicity, all our proposals help, yielding
a 20% relative improvement in RMD F1 score.
The paper is organized as follows: Section 2 sur-
veys related work. Section 3 describes the IE system
used. We cover the target domain that serves as use
case in this paper in Section 4. Section 5 introduces
our ideas and evaluates their impact in the target do-
main. Finally, Section 6 concludes the paper.
2 Related Work
Other recent works have analyzed the robustness of
information extraction systems. For example, Flo-
rian et al (2010) observed that EMD systems per-
form badly on noisy inputs, e.g., automatic speech
transcripts, and propose system combination (sim-
ilar to our first proposal) to increase robustness in
such scenarios. Ratinov and Roth (2009) also in-
vestigate design challenges for named entity recog-
nition, and showed that other design choices, such
as the representation of output labels and using fea-
tures built on external knowledge, are more impor-
tant than the learning model itself. These works are
conceptually similar to our paper, but we propose
several additional directions to improve robustness,
and we investigate their impact in a complete IE sys-
tem instead of just EMD.
Several of our lessons are drawn from the BioCre-
ative challenge1 and the BioNLP shared task (Kim
1http://biocreative.sourceforge.net/
et al, 2009). These tasks have shown the impor-
tance of high quality syntactic annotations and using
heuristic fixes to correct systematic errors (Schuman
and Bergler, 2006; Poon and Vanderwende, 2010,
among others). Systems in the latter task have also
shown the importance of high recall in the earlier
stages of pipeline system.
3 Description of the Generic IE System
We illustrate our proposed ideas using a simple IE
system that implements a pipeline architecture: en-
tity mention extraction followed by relation men-
tion extraction. Note however that the domain cus-
tomization discussion in Section 5 is independent of
the system architecture or classifiers used for EMD
and RMD, and we expect the proposed ideas to ap-
ply to other IE approaches as well.
We performed all pre-processing (tokenization,
part-of-speech (POS) tagging) with the Stanford
CoreNLP toolkit.2 For EMD we used the Stanford
named entity recognizer (Finkel et al, 2005). In all
our experiments we used a generic set of features
(?macro?) and the IO notation3 for entity mention la-
bels (e.g., the labels for the tokens ?over the Seattle
Seahawks on Sunday? (from Figure 1) are encoded
as ?O O NFLTEAM NFLTEAM O DATE?).
2http://nlp.stanford.edu/software/
corenlp.shtml
3The IO notation facilitates faster inference than the IOB
or IOB2 notations with minimal impact on performance, when
there are fewer adjacent mentions with the same type.
3
Argument
Features
? Head words of the two arguments
and their combination
? Entity mention labels of the two
arguments and their combination
Syntactic
Features
? Sequence of dependency labels
in the dependency path linking the
heads of the two arguments
? Lemmas of all words in the de-
pendency path
? Syntactic path in the constituent
parse tree between the largest con-
stituents headed by the same words
as the two arguments (similar
to Gildea and Jurafsky (2002))
Surface
Features
? Concatenation of POS tags be-
tween arguments
? Binary indicators set to true if
there is an entity mention with a
given type between the two argu-
ments
Table 1: Feature set used for RMD.
The RMD model was built from scratch as a
multi-class classifier that extracts binary relations
between entity mentions in the same sentence. Dur-
ing training, known relation mentions become pos-
itive examples for the corresponding label and all
other possible combinations between entity men-
tions in the same sentence become negative exam-
ples. We used a multiclass logistic regression classi-
fier with L2 regularization. Our feature set is taken
from (Yao et al, 2010; Mintz et al, 2009; Roth and
Yih, 2007; Surdeanu and Ciaramita, 2007) and mod-
els the relation arguments, the surface distance be-
tween the relation arguments, and the syntactic path
between the two arguments, using both constituency
and dependency representations. For syntactic in-
formation, we used the Stanford parser (Klein and
Manning, 2003) and the Stanford dependency repre-
sentation (de Marneffe et al, 2006).
For RMD, we implemented an additive feature se-
lection algorithm similar to the one in (Surdeanu
et al, 2008), which iteratively adds the feature
with the highest improvement in F1 score to the
current feature set, until no improvement is seen.
The algorithm was configured to select features
that yielded the best combined performance on the
dataset from Roth and Yih (2007) and the training
partition of ACE 2007.4 We used ten-fold cross val-
4LDC catalog numbers LDC2006E54 and LDC2007E11
Documents Words Entity Relation
Mentions Mentions
110 70,119 2,188 1,629
Table 2: Summary statistics of the NFL corpus, after our
conversion to binary relations.
idation on both datasets. We decided to use a stan-
dard F1 score to evaluate RMD performance rather
than the more complex ACE score because we be-
lieve that the former is more interpretable. We used
gold entity mentions for the feature selection pro-
cess. Table 1 summarizes the final set of features
selected.
Despite its simplicity, our approach achieves
comparable performance with other state-of-the-art
results reported on these datasets (Roth and Yih,
2007; Surdeanu and Ciaramita, 2007). For exam-
ple, Surdeanu and Ciaramita report a RMD F1 score
of 59.4 for ACE relation types (i.e., ignoring sub-
types) when gold entity mentions are used. Under
the same conditions, our RMD model obtains a F1
score of 59.2.
4 Description of the Target Domain
In this paper we report results on the ?Machine
Reading NFL Scoring? corpus.5 This corpus was
developed by LDC for the DARPA Machine Read-
ing project. The corpus contains 110 newswire arti-
cles on National Football League (NFL) games. The
annotations cover game information, such as partici-
pating teams, winners and losers, partial (e.g., a sin-
gle touchdown or three field goals) and final scores.
Most of the annotated relations in the original corpus
are binary (e.g. GAMEDATE(NFLGAME, DATE))
but some are n-ary relations or include other at-
tributes in addition of the relation type. We reduce
these to annotations compatible with our RMD ap-
proach as follows:
? We concatenate the cardinality of each scoring
event (i.e. how many scoring events are be-
ing talked about) to the corresponding SCORE-
TYPE entity label. Thus SCORETYPE-2 in-
dicates that there were two of a given type
of scoring event (touchdown, field goal, etc.).
This operation is necessary because the cardi-
nality of scoring events is originally annotated
as an additional attribute of the SCORETYPE
5LDC catalog number LDC2009E112
4
Entity Mentions Correct Predicted Actual P R F1
Date 141 190 174 74.2 81.0 77.5
FinalScore 299 328 347 91.2 86.2 88.6
NFLGame 71 109 147 65.1 48.3 55.5
NFLPlayoffGame 8 25 38 32.0 21.1 25.4
NFLTeam 651 836 818 77.9 79.6 78.7
ScoreType-1 329 479 525 68.7 62.7 65.5
ScoreType-2 49 68 79 72.1 62.0 66.7
ScoreType-3 17 26 36 65.4 47.2 54.8
ScoreType-4 6 11 14 54.5 42.9 48.0
Total 1571 2076 2188 75.7 71.8 73.7
Relation Mentions Correct Predicted Actual P R F1
fieldGoalPartialCount 33 41 101 80.5 32.7 46.5
gameDate 32 36 115 88.9 27.8 42.4
gameLoser 22 44 124 50.0 17.7 26.2
gameWinner 6 15 123 40.0 4.9 8.7
teamFinalScore 95 101 232 94.1 40.9 57.1
teamInGame 49 105 257 46.7 19.1 27.1
teamScoringAll 202 232 321 87.1 62.9 73.1
touchDownPartialCount 156 191 322 81.7 48.4 60.8
Total 595 766 1629 77.7 36.5 49.7
Table 3: Baseline results: stock system without any domain customization. Correct/Predicted/Actual indicate the num-
ber of mentions (entities or relations) that are correctly predicted/predicted/gold. P/R/F1 indicate precision/recall/F1
scores for the corresponding label.
entity and our EMD approach does not model
mention attributes.
? We split all n-ary relations into several new
binary relations. For example, the original
TEAMFINALSCORE(NFLTEAM, NFLGAME,
FINALSCORE) relation is split into three binary
relations: TEAMSCORINGALL(NFLTEAM,
FINALSCORE), TEAMINGAME(NFLGAME,
NFLTEAM), and TEAMFINALSCORE(NFL-
GAME, FINALSCORE).
Figure 1 shows an example annotated sentence af-
ter the above conversion and Table 2 lists the corpus
summary statistics for the new binary relations.
The purpose behind this corpus is to encourage
the development of systems that answer structured
queries that go beyond the functionality of informa-
tion retrieval engines, e.g.:
?For each NFL game, identify the win-
ning and losing teams and each team?s fi-
nal score in the game.?
?For each team losing to the Green Bay
Packers, tell us the losing team and the
number of points they scored.?6
6These queries would be written in a formal language but
5 Domain Customization
Table 3 lists the results of the generic IE system de-
scribed in Section 3 on the NFL domain. Through-
out this paper we will report results using ten-fold
cross-validation on all 110 documents in the cor-
pus.7 We consider an entity mention as correct if
both its boundaries and label match exactly the gold
mention. We consider a relation mention correct if
both its arguments and label match the gold relation
mention. For RMD, we report results using the ac-
tual mentions predicted by our EMD model (instead
of using gold entity mentions for RMD). For clar-
ity, we do not show in the tables some labels that are
highly uncommon in the data (e.g., SCORETYPE-5
appears only four times in the entire corpus); but the
?Total? results include all entity and relation men-
tions.
Table 3 shows that the stock IE system obtains an
are presented here in English for clarity.
7Generally, we do not condone reporting results using cross-
validation because it may be a recipe for over-fitting on the
corresponding corpus. However, all our domain customization
ideas were developed using outside world and domain knowl-
edge and were not tuned on this data, so we believe that there is
minimal over-fitting in this case.
5
Entity Mentions P R F1
Date 74.2 81.0 77.5
FinalScore 91.3 87.3 89.2
NFLGame 61.2 48.3 54.0
NFLPlayoffGame 33.3 21.1 25.8
NFLTeam 77.9 81.3 79.5
ScoreType-1 68.8 62.3 65.4
ScoreType-2 72.1 62.0 66.7
ScoreType-3 65.4 47.2 54.8
ScoreType-4 54.5 42.9 48.0
Total 75.6 72.5 74.0
Relation Mentions P R F1
fieldGoalPartialCount 78.0 31.7 45.1
gameDate 91.4 27.8 42.7
gameLoser 50.0 18.5 27.1
gameWinner 40.0 4.9 8.7
teamFinalScore 94.1 40.9 57.1
teamInGame 45.9 19.5 27.3
teamScoringAll 87.0 64.8 74.3
touchDownPartialCount 82.4 49.4 61.7
Total 77.6 37.1 50.2
Table 4: Performance after gazetteer-based features were
added to the EMD model.
EMD F1 score of 73.7 and a RMD F1 score of 49.7.
These are respectable results, in line with state-of-
the-art results in other domains.8 However, there
are some obvious areas for improvement. For exam-
ple, the score for a few relations (e.g., GAMELOSER
and GAMEWINNER) is quite low. This is caused by
the fact that these relations are often not explicitly
stated in text but rather implied (e.g., based on team
scores). Furthermore, the low recall of entity types
that are crucial for all relations (e.g., NFLTEAM and
NFLGAME) negatively impacts the overall recall of
RMD.
5.1 Combining a Rule-based Model with
Conditional Random Fields for EMD
A straightforward way to improve EMD perfor-
mance is to construct domain-specific gazetteers and
include gazetteer-based features in the model. We
constructed a NFL-specific gazetteer as follows: (a)
we included all 32 NFL team names; (b) we built a
lexicon for NFLGame nouns and verbs that included
game types (e.g., ?semi-final?, ?quarter-final?) and
8As a comparison, the best RMD system in ACE 2007 ob-
tained an ACE score of less than 35%, even though the ACE
score gives credit for approximate matches of entity mention
boundaries (Surdeanu and Ciaramita, 2007).
Entity Mentions P R F1
Date 74.2 81.0 77.5
FinalScore 91.3 87.3 89.2
NFLGame 61.2 48.3 54.0
NFLPlayoffGame 33.3 21.1 25.8
NFLTeam 71.4 96.9 82.3
ScoreType-1 68.8 62.3 65.4
ScoreType-2 72.1 62.0 66.7
ScoreType-3 65.4 47.2 54.8
ScoreType-4 54.5 42.9 48.0
Total 72.8 78.4 75.5
Relation Mentions P R F1
fieldGoalPartialCount 81.2 38.6 52.3
gameDate 93.9 27.0 41.9
gameLoser 51.1 19.4 28.1
gameWinner 38.9 5.7 9.9
teamFinalScore 94.1 40.9 57.1
teamInGame 47.4 24.5 32.3
teamScoringAll 87.0 68.8 76.9
touchDownPartialCount 81.6 56.5 66.8
Total 77.2 40.6 53.2
Table 5: Performance after gazetteer-based features were
added to the EMD model, and NFLTeam entity mentions
were extracted using the rule-based model rather than
classification.
typical game descriptors. The game descriptors
were manually bootstrapped from three seed words
(?victory?, ?loss?, ?game?) using Dekang Lin?s
dependency-based thesaurus.9 This process added
other relevant game descriptors such as ?triumph?,
?defeat?, etc. All in all, our gazetteer includes 32
team names and 50 game descriptors. The gazetteer
was built in less than four person hours.
We added features to our EMD model to indi-
cate if a sequence of words matches a gazetteer en-
try, allowing approximate matches (e.g., ?Cowboys?
matches ?Dallas Cowboys?). Table 4 lists the results
after this change. The improvements are modest: 0.3
for both EMD and RMD, caused by a 0.8 improve-
ment for NFLTEAM. The score for NFLGAME suf-
fers a loss of 1.5 F1 points, probably caused by the
fact that our NFLGAME gazetteer is incomplete.
These results are somewhat disappointing: even
though our gazetteer contains an exhaustive list of
NFL team names, the EMD recall for NFLTEAM
is still relatively low. This happens because city
9http://webdocs.cs.ualberta.ca/?lindek/
Downloads/sim.tgz
6
names that are not references to team names are rela-
tively common in this corpus, and the CRF model fa-
vors the generic city name interpretation. However,
since the goal is to answer structured queries over
the extracted relations, we would prefer a model
that favors recall for EMD, to avoid losing candi-
dates for RMD. While this can be achieved in dif-
ferent ways (Minkov et al, 2006), in this paper we
implement a very simple approach: we recognize
NFLTEAM mentions with a rule-based system that
extracts all token sequences that begin, end, or are
equal to a known team name. For example, ?Green
Bay? and ?Packers? are marked as team mentions,
but not ?Bay?. Note that this approach is prone to in-
troducing false positives, e.g., ?Green? in the above
example. For all other entity types we use the CRF
model with gazetteer-based features. Table 5 lists
the results for this model combination. The table
shows that the RMD performance is improved by 3
F1 points. The F1 score for NFLTEAM mentions is
also improved by 3 points, due to a significant in-
crease in recall (from 81% to 97%).
Of course, this simple idea works only for en-
tity types with low ambiguity. In fact, it does not
improve results if we apply it to NFLGAME or
SCORETYPE-*. However, low ambiguity entities
are common in many domains (e.g., medical). In
such domains, our approach offers a straightforward
way to address potential recall errors of a machine
learned model.
5.2 Improving Head Identification for Entity
Mentions
Table 1 indicates that most RMD features (e.g., lex-
ical information on arguments, dependency paths
between arguments) depend on the syntactic heads
of entity mentions. This observation applies to
other natural language processing (NLP) tasks as
well, e.g., semantic role labeling or coreference res-
olution (Gildea and Jurafsky, 2002; Haghighi and
Klein, 2009). It is thus crucial that syntactic heads
of mentions be correctly identified. Originally we
employed a common heuristic: we first try to find a
constituent with the exact same span as the given en-
tity mention in the parse tree of the entire sentence,
and extract its head. If no such constituent exists,
we parse only the text corresponding to the mention
and return the head of the generated tree (Haghighi
Entity Mentions P R F1
Date 69.5 75.9 72.5
FinalScore 90.9 88.8 89.8
NFLGame 60.5 51.0 55.4
NFLPlayoffGame 37.0 26.3 30.8
NFLTeam 72.4 98.3 83.4
ScoreType-1 69.7 62.1 65.7
ScoreType-2 76.9 63.3 69.4
ScoreType-3 64.3 50.0 56.3
ScoreType-4 72.7 57.1 64.0
Total 73.2 79.2 76.1
Relation Mentions P R F1
fieldGoalPartialCount 81.2 55.4 65.9
gameDate 93.9 27.0 41.9
gameLoser 51.2 17.7 26.3
gameWinner 50.0 8.9 15.2
teamFinalScore 96.5 47.4 63.6
teamInGame 48.3 33.5 39.5
teamScoringAll 86.7 72.9 79.2
touchDownPartialCount 89.1 61.2 72.6
Total 78.5 45.9 57.9
Table 6: Performance with the improved syntactic head
identification rules.
and Klein, 2009). Here we argue that the last step of
this heuristic is flawed: since most parsers are heav-
ily context dependent, they are likely to not parse
correctly arbitrarily short text fragments. For exam-
ple, the Stanford parser generates the incorrect parse
tree:
The syntactic head is ?5? for the mention ?a 5-yard
scoring pass? instead of ?pass.?10 This problem is
exacerbated out of domain, where the parse tree of
the entire sentence is likely to be incorrect, which
will often trigger the parsing of the isolated men-
tion text. For example, in the NFL domain, more
than 25% of entity mentions cannot be matched to
a constituent in the parse tree of the corresponding
sentence.
10We tokenize around dashes in this domain because scores
are often dash separated. However, this mention is incorrectly
parsed even when ?5-yard? is a single token.
7
teamFinalScore(G, S) :- teamInGame(T, G), teamScoringAll(T, S).
teamFinalScore(G, S) :- gameWinner(T, G), teamScoringAll(T, S).
teamFinalScore(G, S) :- gameLoser(T, G), teamScoringAll(T, S).
teamInGame(G, T) :- teamScoringAll(T, S), teamFinalScore(G, S).
gameWinner(G, T1) :- teamInGame(G, T1), teamInGame(G, T2),
teamFinalScore(G, S1), teamFinalScore(G, S2),
teamScoringAll(T1, S1), teamScoringAll(T2, S2),
greaterThan(S1, S2).
gameLoser(G, T1) :- teamInGame(G, T1), teamInGame(G, T2),
teamFinalScore(G, S1), teamFinalScore(G, S2),
teamScoringAll(T1, S1), teamScoringAll(T2, S2),
lessThan(S1, S2).
Table 7: Deterministic inference rules for the NFL domain as first-order Horn clauses. G, T, and S indicate game,
team, and score variables.
In this work, we propose several simple heuristics
that improve the parsing of isolated mention texts:
? We append ?It was ? to the beginning of the text
to be parsed. Since entity mentions are noun
phrases (NP), the new text is guaranteed to be
a coherent sentence. A similar heuristic was
used by Moldovan and Rus for the parsing of
WordNet glosses (2001).
? Because dashes are uncommon in the Penn
Treebank, we remove them from the text before
parsing.
? We guide the Stanford parser such that the final
tree contains a constituent with the same span
as the mention text.11
After implementing these heuristics, the Stanford
parser correctly parses the mention in the above ex-
ample as a NP headed by ?pass?. Table 6 lists
the overall extraction scores after deploying these
heuristics. The table shows that the RMD F1 score
is a considerable 4.7 points higher than before this
change (Table 5).
5.3 Deterministic Inference for RMD
Figure 1 underlines the fact that relations in the NFL
domain are highly inter-dependent. This is a com-
mon occurrence in many extraction tasks and do-
mains (Poon and Vanderwende, 2010; Carlson et
al., 2010). The typical way to address these situa-
tions is to jointly model these relations, e.g., using
Markov logic networks (MLN) (Poon and Vander-
wende, 2010). However, this implies a complete
redesign of the corresponding IE system, which
would essentially ignore all the effort behind exist-
ing pipeline systems.
11This is supported by the parser API.
Relation Mentions P R F1
fieldGoalPartialCount 81.2 55.4 65.9
gameDate 93.9 27.0 41.9
gameLoser 45.9 27.4 34.3
gameWinner 45.6 25.2 32.5
teamFinalScore 96.5 47.4 63.6
teamInGame 48.1 44.7 46.4
teamScoringAll 86.7 72.9 79.2
touchDownPartialCount 89.1 61.2 72.6
Total 74.2 49.6 59.5
Table 8: Performance after adding deterministic infer-
ence. The EMD scores are not affected by this change,
so they are not listed here.
In this work, we propose a simple method that
captures some of the joint domain structure indepen-
dently of the IE architecture and the EMD and RMD
models. We add a deterministic inference compo-
nent that generates new relation mentions based on
the data already extracted by the pipeline model. Ta-
ble 7 lists the rules of this inference component that
were developed for the NFL domain. These rules
are domain-dependent, but they are quite simple: the
first four rules implement transitive-closure rules for
relation mentions centered around the same NFL-
GAME mention; the last two add domain knowledge
that is not captured by the text extractors, e.g., the
game winner is the team with the higher score. Ta-
ble 8, which lists the RMD scores after inference, in-
dicates that the inference component is responsible
for an increase of approximately 2 F1 points, caused
by a recall boost of approximately 4%.
Table 9 lists the results of a post-hoc experiment,
where we removed several relation types from the
RMD classifier (the ones predicted with poor perfor-
mance) and let the deterministic inference compo-
nent generate them instead. This experiment shows
8
Without Inference With Inference
P R F1 P R F1
Skip gameWinner, gameLoser 78.6 45.6 57.7 75.1 48.4 58.8
Skip teamInGame 77.0 43.6 55.7 71.7 49.4 58.5
Skip teamInGame, teamFinalScore 74.5 37.1 49.6 70.9 47.6 56.9
Skip nothing 78.5 45.9 57.9 74.2 49.6 59.5
Table 9: Analysis of different combination strategies between the RMD classifier and inference: the RMD model skips
the relation types listed in the first column; the inference component generates all relation types. The other columns
show relation mention scores under the various configurations.
EMD RMD
F1 F1
Baseline 73.7 49.7
+ gazetteer features 74.0 50.2
+ rule-based model for NFLTeam 75.5 53.2
+ improved head identification 76.1 57.9
+ inference 76.1 59.5
Table 10: Summary of domain customization results.
that inference helps in all configurations, and, most
importantly, it is robust: even though the RMD score
without inference decreases by up to 8 F1 points
as relations are removed, the score after inference
varies by less than 3 F1 points (from 56.9 to 59.5
F1). This proves that deterministic inference is ca-
pable of generating relation mentions that are either
missed or cannot be modeled by the RMD classifier.
Finally, Table 10 summarizes the experiments
presented in this paper. It is clear that, despite their
simplicity, all our proposed ideas help. All in all,
our contributions yielded an improvement of 9.8 F1
points (approximately 20% relative) over the stock
IE system without these changes. Our best IE sys-
tem was used in a blind evaluation within the Ma-
chine Reading project. In this evaluation, systems
were required to answer 50 queries similar to the
examples in Section 4 and were evaluated on the
correctness of the individual facts extracted. Note
that this evaluation is more complex than the exper-
iments reported until now, because the correspond-
ing IE system requires additional components, e.g.,
the normalization of all DATE mentions and event
coreference (i.e., are two different game mentions
referring to the same real-world game?). For this
evaluation, we used an internal script for date nor-
malization and we did not implement event corefer-
ence. This system was evaluated at 46.7 F1 (53.7
precision and 41.2 recall), a performance that was
approximately 80% of the F1 score obtained by hu-
man annotators. This further highlights that strong
IE performance can be obtained with simple models.
6 Conclusions
This paper introduces a series of simple ideas that
improve the performance of IE systems when they
are customized to new domains. We evaluated our
contributions on a sports domain (NFL game sum-
maries) that is significantly different from the do-
mains used to develop our IE system or the language
processors used by our system.
Our analysis revealed several interesting and non-
obvious facts. First, we showed that accurate identi-
fication of syntactic heads of entity mentions, which
has received little attention in IE literature, is cru-
cial for good performance. Second, we showed that
a deterministic inference component captures some
of the joint domain structure, even when the under-
lying system follows a pipeline architecture. Lastly,
we introduced a simple way to tune precision and
recall by combining our entity mention extractor
with a rule-based system. Overall, our contributions
yielded a 20% improvement in the F1 score for rela-
tion mention extraction.
We believe that our contributions are model inde-
pendent and some, e.g., the better head identifica-
tion, even task independent. Some of our ideas re-
quire domain knowledge, but they are all very sim-
ple to implement. We thus expect them to impact
other problems as well, e.g., coreference resolution,
semantic role labeling.
Acknowledgments
We thank the reviewers for their detailed comments.
This material is based upon work supported by the Air
Force Research Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181. Any opinions, findings, and
conclusion or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect
the view of the Air Force Research Laboratory (AFRL).
9
References
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the Third ACM Interna-
tional Conference on Web Search and Data Mining
(WSDM).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2005).
Radu Florian, John Pitrelli, Salim Roukos, and Imed Zi-
touni. 2010. Improving mention detection robustness
to noisy input. In Proc. of Empirical Methods in Nat-
ural Language Processing (EMNLP).
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3).
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proc. of Empirical Methods in Natural Language
Processing (EMNLP).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the Workshop on BioNLP: Shared Task,
pages 1?9. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proc. of the International Conference on Machine
Learning (ICML).
Andrei Mikheev, Marc Moens, and Claire Grover. 1999.
Named entity recognition without gazetteers. In
EACL, pages 1?8.
Einat Minkov, Richard C. Wang, Anthony Tomasic, and
William W. Cohen. 2006. Ner systems that suit user?s
preferences: Adjusting the recall-precision trade-off
for entity extraction. In Proc. of HLT/NAACL.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Proc. of the Conference of the Association for
Computational Linguistics (ACL-IJCNLP).
Dan I. Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to ques-
tion answering. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics.
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics -
Human Language Technologies Conference (NAACL-
HLT).
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Introduction to Statistical Relational
Learning. MIT Press.
Jonathan Schuman and Sabine Bergler. 2006. Postnom-
inal prepositional phrase attachment in proteomics. In
Proceedings of the HLT-NAACL BioNLP Workshop on
Linking Natural Language and Biology, pages 82?89.
Association for Computational Linguistics, June.
Mihai Surdeanu and Massimiliano Ciaramita. 2007. Ro-
bust information extraction with perceptrons. In Pro-
ceedings of the NIST 2007 Automatic Content Extrac-
tion Workshop (ACE07).
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large
online qa collections. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2008).
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proc. of Empirical Methods
in Natural Language Processing (EMNLP).
10
