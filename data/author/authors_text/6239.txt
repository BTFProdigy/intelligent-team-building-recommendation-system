A reliable approach to automatic assessment 
of short answer free responses 
 
Lyle F Bachman, Nathan Carr, Greg Kamei, Mikyung Kim, Michael J Pan, Chris Salvador, Yasuyo Sawaki 
WebLAS, Applied Linguistics & TESL, UCLA  
Los Angeles, CA 90095 
{bachman, carr, kamei, kimmi, mjpan, chriss, ysawaki}@WebLAS.ucla.edu  
  
Abstract 
This paper discusses an innovative approach to 
the computer assisted scoring of student 
responses in WebLAS (web-based language 
assessment system)- a language assessment 
system delivered entirely over the web.  Expected 
student responses are limited production free 
response questions. 
The portions of WebLAS with which we are 
concerned are the task creation and scoring 
modules.  Within the task creation module, 
instructors and language experts do not only 
provide the task input and prompt. More 
importantly, they interactively inform the system 
how and how much to score student responses.  
This interaction consists of WebLAS? natural 
language processing (NLP) modules searching 
for alternatives of the provided ?gold standard? 
(Hirschman et al 2000) answer and asking for 
confirmation of score assignment.  WebLAS 
processes and stores all this information within 
its database, to be used in the task delivery and 
scoring phases. 
 
1 Introduction 
 
 Most assessment for placement, diagnosis, 
progress and achievement in our language 
programs are presently administered in paper and 
pencil (P&P) format.  This format carries a 
number of administrative costs and inefficiencies.  
It requires new hard copy forms of assessments 
for each course and class, incurring costs 
associated with copying, handling, distributing, 
and collecting test booklets and answer sheets to 
test takers.  Although some of the assessments 
can be scored by machine, teachers score those 
with free responses, such as open-ended 
questions and cloze (gap filling) tests. 
 WebLAS addresses the problems of a P&P 
format.  It provides an integrated approach to 
assessing language ability for the purpose of 
making decisions about placement, diagnosis, 
progress and achievement in the East Asian 
language programs, as the content specifications 
of the assessment system for these languages are 
based directly on the course content, as specified 
in scope and sequence charts, and utilize tasks that 
are similar to those used in classroom instruction.  
WebLAS is thus being designed with the 
following expected advantages as objectives: 
 
1. Greater administrative efficiency 
2. More authentic, interactive and valid 
assessments of language ability such as 
integration of course and assessment 
content and incorporation of cutting edge 
and multimedia technology for assessment 
 
 Nested within these objectives is the ability to 
automatically assess limited production free 
responses.  Existing systems such as e-Rater   
(Burstein et al focus on holistic essay scoring.  
Even so, systems such as PEG (Page 1966) 
disregard content and simply perform surface 
feature analysis, such as a tabulation of syntactical 
usage.  Others like LSA (Foltz et al1998) require 
a large corpora as basis for comparison.  Lately, 
there has been more interested in approaching the 
short answer scoring problem.  These few such as 
MITRE (Hirschman et al 2000) and ATM 
(Callear et al 2001) are extraordinarily 
programming intensive however, and 
incomprehensible to educators.  Additionally, they 
do not permit a partial credit scoring system, 
thereby introducing subjectivity into the scoring 
(Bachman 1990).  None are truly suited for short 
answer scoring in an educational context, since the 
scores produced are neither easily explanable nor 
justifiable to testtakers. 
WebLAS is developed in response to the 
needs of the language assessors.  Current 
methods for scoring P&P tests require the test 
creators to construct a scoring rubrid, by which 
human scorers reference as they score student 
responses.  Weblas imitates this process by 
prompting the test creator for the scoring rubrid.  
It tags and parses the model answer, extracts 
relevant elements from within the model answer 
and proposes possible alternatives interactively 
with the educator.   It also tags, parses, and 
extracts the same from the student responses.  
Elements are then pattern matched and scored. 
 
2 Using WebLAS  
 
Just as a scoring rubric for short answer 
scoring cannot be created in a vacuum, it would 
be difficult for us to discuss the scoring process 
without describing the task creation process.   
Task development consists of all the efforts 
that lead to the test administration.  The task 
development portion of WebLAS consists of 
three modules- task creation, task modification, 
and lexicon modification.  These are explained 
below. 
 
2.1 Using WebLAS 
 
WebLAS is written mostly in Perl.  Its 
capacity for regular expressions (regex) make it 
well suited for natural language processing (NLP) 
tasks, and its scripting abilities enable dynamic 
and interactive content deliverable over the web.  
There is also a complete repository of open 
source Perl modules available, eliminating the 
necessity to reinvent the wheel. 
One of the tools WebLAS incorporates is 
Wordnet, an English lexicon under development 
at Princeton with foundations in cognitive 
psychology (Fellbaum 1998).  A second tool 
WebLAS uses is the Link Grammar Parser, a 
research prototype available from Carnegie 
Mellon University (Grinberg et al1995).  Both 
Wordnet and Link Grammar are written in 
C/C++.  To interface with the systems, we make 
use of 2 Perl modules developed by Dan Brian1.  
Linguana::Wordnet converts to Berkeley DB 
                                                     
1  http://www.brians.org 
format2 for fast access, and allows for 
modifications to the lexicon.  
Linguana::LinkGrammar interfaces with the Link 
Grammar for parts of speech (POS) tagging and 
syntactic parsing.  For our web server we use the 
Apache Advanced Extranet web server.  To run 
perl scripts via the web, we use mod_perl, which 
enables us to run unmodified scripts.  Our 
database is MySQL server3. 
 
2.2 Task Development 
 
WebLAS is organized into four major 
components relative to the test event itself. These 
are test development, test delivery, response 
scoring, and test analysis.  Two of these are 
relevant to NLP- task development and test 
scoring. 
 
 
Figure 1.  Task Creation Flowchart 
 
                                                     
2  http://www.sleepycat.com 
3  http://www.mysql.org 
2.2.1 Task Creation 
 
The task creation module is somewhat of a 
misnomer.  At the time of using this module, the 
task has already been specified according to 
language assessment requirements.  The module 
actually facilitates the instructor with the process 
of storing into the database and preprocessing the 
task for automatic scoring, rather than creating 
the task itself.  This process is shown in the 
flowchart in Figure 1.  
 
a. The module requests from the instructor the 
task name, task type, input prompt, response 
prompt, and model answer for the task.  This 
information is stored within the database for 
retrieval. 
b. WebLAS sends Link Grammar the model 
answer, which returns the answer after 
tagging the POS and parsing it.  WebLAS 
then finds important elements of the model 
answer which are necessary to receive full 
credit from the parsed answer and confirms 
each one with the instructor.  Elements are 
generally phrases, such as ?the sushi 
restaurant? or ?next to the post office? but 
could be singletons as well, such as 
?Yamada-san? as well. 
c. After each element is confirmed, WebLAS 
searches Wordnet for possible alternatives of 
the elements and their individual words.  For 
example, it may deem ?near the post office? 
as a possible alternate to ?next to the post 
office.?  Once found, it asks for confirmation 
from the instructor again.  Additionally, the 
educator is prompted for other possibilities 
that were not found. 
d. The task creator dictates a ratings scale.  
Point values assigned to elements deriving 
directly from the model answer are assumed 
to be maximum values, i.e. full credit for the 
given element.  Alternatives to the model 
answer elements found can be assigned 
scoring less than or equal to the maximum 
value.  Thus an answer with numerous 
elements can be scored with a partial credit 
schema. 
e. WebLAS takes the input (model answer, 
elements, alternatives, score assignments) to 
create a scoring key.  The scoring key 
employs regular expressions for pattern 
matching.  For example, ?(next|near)? indicate 
that either ?next? or ?near? are acceptable 
answers.  Along with each regex is a point 
value, which is added to a test taker?s final 
score if the regex is matched with the student 
response. 
 
2.2.2 Task Modification 
 
The task modification module allows for 
instructors to go back and modify tasks they have 
created, as well as tasks others created.  The 
database tracks information relevant to the 
changes, including information on the modifier, 
date and time of the modification, evolving 
changes to the tasks, and any comments on the 
reasons for the change.  The database supports 
data synchronization, so that two instructors 
cannot and do not change tasks simultaneously. 
Should the model answer be changed, the 
scoring key creation of the task creation module is 
activated and the instructor is guided through the 
process again. 
 
2.2.3 Lexicon Modification 
 
The WebLAS lexicon is based on Wordnet.  
Wordnet is by no means complete, however, and it 
may be possible that instructors may find the need 
to add to its knowledge.  The lexicon is 
automatically updated given the input given during 
scoring key creation. 
One can also manually modify the lexicon 
through a guided process.  The system prompts for 
the word and its parts of speech and returns all 
possible word senses.  The user chooses a word 
sense, and is then given a choice of the relation 
type to modify (i.e. synonyms, antonyms, 
hyponyms, etc.).  The final step is the modification 
and confirmation of the change to the relation 
type. 
 
2.3      Test Scoring 
 
Once the task creation module creates the 
regexes, task scoring becomes trivial.  WebLAS 
simply needs to pattern match the regexes to score 
each element.  Additionally, WebLAS can be quite 
flexible in its scoring.  It is tolerant of a wide 
range of answers on the part of test takers, 
incorporating adapted soundex, edit distances, 
and word stemming algorithms, for phonetic, 
typographic, and morphological deviations from 
model answers. 
 
 
3 Lexicon Modification 
 
There are advantages to the WebLAS system.  
The first is a computational efficiency factor.  
The system is not a learning system (yet).  The 
automatic scoring section, if it did not use 
preprocessed regexes, would perform the same 
search for each student response.  This search 
becomes redundant and unnecessary.  By 
preprocessing the search, we reduce the linear 
time complexity- O(n), to a constant- O(1), with 
respect to the number of student responses. 
Second, partial scoring eliminates 
arbitrariness of scoring.  Rather than a simple 
credit/no credit schema, each element 
individually contributes to the final score 
tabulation. 
Reliability also increases.  Since the scores 
produced are repeatable, and do not change with 
each scoring, WebLAS has perfect intra-rater 
reliability.  Because the instructor confirms all 
scoring decisions beforehand, the scores are also 
explainable and justifiable, and can withstand 
criticism. 
 
4 Conclusion 
 
Our approach towards automatic computer 
scoring of open ended responses show promising 
potential for reasons of its reliability and 
robustness.  Future plans include making use of 
additional NLP algorithms such as inference and 
pronoun resolution, as well as inclusion of 
additional task types such as summarization, 
outline, and gap-fill tasks.  We should also like to 
bring the scoring online and provide the student 
with instantaneous feedback.  Pilot testing within 
the campus is scheduled for Winter and Spring 
2003 quarters, with full campus rollout in Fall 
2003. 
 
 
 
References  
 
Bachman, Lyle F. (1990) Fundamental 
Considerations in Language Testing.  Oxford 
University Press: Oxford. 
Bachman, Lyle F.; Palmer, Adrian S.  (1996)  
Language Testing in Practice.  Oxford 
University Press: Oxford. 
Brian, Daniel.  (2001)  Linguana: Perl as a 
language for conceptual representation in NLP 
systems.  Proceedings of the O?Reilly Perl 
Conference 2001. 24-31. 
Burstein, Jill; Leacock, Claudia; Swartz, Richard. 
(2001)  Automated evaluation of essays and 
short answers.  Proceedings of the 5th 
International Computer Assisted Assessment 
Conference (CAA 01). 
Callear, David; Jerrams-Smith, Jenny; Soh, David.  
(2001)  CAA of short non-MCQ answers.  
Proceedings of the 5th International Computer 
Assisted Assessment Conference (CAA 01). 
Chung, Gregory K.W.K; O?Neil, Harold F., Jr.  
(1997)  Methodological approaches to online 
scoring of essays.  University of California Los 
Angeles, Center for Research on Evaluation, 
Standards, and Student Testing technical report 
461. 
Grinberg, Dennis; Lafferty, John; Sleator, Daniel.  
(1995)  A robust parsing algorithm for link 
grammars, Carnegie Mellon University 
Computer Science technical report CMU-CS-95-
125, and Proceedings of the Fourth 
International Workshop on Parsing 
Technologies, Prague. 
Hirschman, Lynette; Breck, Eric; Light, Marc; 
Burger, John D.; Ferro, Lisa. (2000)  Automated 
grading of short answer tests.  IEEE Intelligent 
Systems. 15(5):31-35. 
Fellbaum, Christiane (editor).  (1998)  Wordnet: 
An electronic lexical database.  MIT Press, 
Cambridge, MA. 
Foltz P; Kintsch W; Landauer T.  (1998)  ?The 
measurement of textual coherence with latent 
semantic analysis.?  Discourse Processes.  
25(23):285-307. 
Page, E.B. (1966)  ?The imminence of grading 
essays by computer.?  Phi Delta Kappan.  
47:238-243. 
Proceedings of the 10th Conference on Parsing Technologies, pages 106?108,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Pomset mcfgs
Michael J Pan
University of California Los Angeles
mjpan@cs.ucla.edu
Abstract
This paper identifies two orthogonal dimen-
sions of context sensitivity, the first being
context sensitivity in concurrency and the
second being structural context sensitivity.
We present an example from natural lan-
guage which seems to require both types of
context sensitivity, and introduce partially
ordered multisets (pomsets) mcfgs as a for-
malism which succintly expresses both.
Introduction
Researchers in computer science and formal lan-
guage theory have separately investigated context
sensitivity of languages, addressing disjoint dimen-
sions of context sensitivity. Researchers in paral-
lel computing have explored the addition of con-
currency and free word order to context free lan-
guages, i.e. a concurrency context sensitivity (Gis-
cher, 1981; Warmuth and Haussler, 1984; Pratt,
1985; Pratt, 1986; Lodaya and Weil, 2000). Com-
putational linguistis have explored adding cross-
ing dependency and discontinuous constituency, i.e.
a structural context sensitivity (Seki et al, 1991;
Vijay-Shanker et al, 1987; Stabler, 1996).
Research considering the combination of two di-
mensions of expressing context sensitivity have been
sparse, e.g. (Becker et al, 1991), with research ded-
icated to this topic virtually nonexistent. Natural
languages are not well expressed by either form of
context sensitivity alone. For example, in Table 1,
sentences 1-8 are valid, but 9, 10 are invalid con-
structions of Norwegian. In addition to the cross-
ing dependency between the determiner and adverb
phrase, this example can be described by either
Derfor ga Jens Kari kyllingen tydeligvis ikke lenger kald
Therefore gave Jens Kari the chicken evidently not longer cold
Derfor ga Jens Kari tydeligvis kyllingen ikke lenger kald
Derfor ga Jens tydeligvis Kari kyllingen ikke lenger kald
Derfor ga Jens tydeligvis Kari ikke kyllingen lenger kald
Derfor ga Jens tydeligvis Kari ikke lenger kyllingen kald
Derfor ga Jens tydeligvis ikke lenger Kari kyllingen kald
Derfor ga tydeligvis Jens ikke lenger Kari kyllingen kald
Derfor ga tydeligvis ikke Jens lenger Kari kyllingen kald
* Derfor ga Jens ikke tydeligvis Kari lenger kyllingen kald
* Derfor ga Jens ikke tydeligvis kyllingen lenger Kari kald
Table 1: Bobaljik?s paradox/shape conservation example
Bobaljik?s paradox (Bobaljik, 1999), which asserts
that relative ordering of clausal constituents are not
unambiguously determined by the phrase structure,
or shape conservation (Mu?ller, 2000), i.e. that lin-
ear precedence is preserved despite movement op-
erations. In other words, the two structurally con-
text sensitive components (due to the crossing de-
pendency between them) can be shuffled arbitrarily,
leading to concurrent context sensitivity.
This paper proposes pomset mcfgs as a formal-
ism for perspicuously expressing both types of con-
text sensitivity. 1 The rest of the paper is organized
as follows. Section 1 introduces pomsets, pomset
operations, and pomset properties. Section 2 pro-
vides a definition of pomset mcfgs by extending the
standard definition of mcfgs, defined over tuples of
strings, to tuples of pomsets. Section 3 discusses
pomset mcfg parsing.
1Other pomset based formalisms (Lecomte and Retore,
1995; Basten, 1997; Nederhof et al, 2003) have been limited
to the use of pomsets in context free grammars only.
106
1 Pomsets
In this section, we define pomsets as a model for de-
scribing concurrency. A labelled partial order (LPO)
is a 4 tuple (V, ?, , ?) where V is a set of ver-
tices, ? is the alphabet,  is the partial order on the
vertices, and ? is the labelling function ?:V? ?.
A pomset is a LPO up to isomorphism. The con-
catenation of pomsets p and q is defined as ;(p,q)
= (Vp?Vq,?p ? ?q,p ? q ?Vp?Vq,?p ? ?q).
The concurrency of pomsets p and q is defined
as ?(p,q) = (Vp?Vq,?p ? ?q,p ? q,?p ? ?q).
Pomset isolation (?) is observed only in the con-
text of concurrency. The concurrence of an isolated
pomset with another pomset is defined as ?(?p,q) =
({vp}?Vq,p? ? ?q,q,{(p?,vp)}??q), where ?p is
the set of linearizations for p, and p? is a function
which returns an element of ?p. Let ?i be a pomset
concurrency operator restricted to an arity of i. Be-
cause concurrency is both associative and commu-
tative, without isolation, ?m?n = ?n?m = ?m+n, de-
feating any arity restrictions. Isolation allows us to
restrict the arity of the concurrency operator, guaran-
teeing that in all linearizations of the pomset, the lin-
earizations of the isolated subpomsets are contigu-
ous.2 A mildly concurrent operator ? ?n, i.e. an n-
concurrent operator, is a composite operator whose
concurrency is isolated and restricted to an arity of n,
such that it operates on at most n items concurrently.
2 Pomset mcfgs
There are many (structural) mildly context sensitive
grammar formalisms, e.g. mcfg, lcfrs, mg, and they
have been shown to be equivalent (Vijay-Shanker et
al., 1987). In this section we construct mcfgs over
pomsets (instead of strings) to define grammars with
both types of context sensitivity.
A pomset mcfg G is a 7-tuple (?,N,O,P,F,R,S)
such that ? is a finite non-empty set of atoms, i.e.
terminal symbols, N is a finite non-empty set of non-
terminal symbols, where N??=?, O is a set of valid
pomset operators, P is a set of i-tuples of pomsets
labelled by ??N, F is a finite set of pomset rewrit-
ing functions from tuples of elements of P into ele-
ments in P, F?{ g:Pn ?P | n>0 }, R is a finite set
2Pomset isolation is similar to proposals in for string iso-
lation in linear specification language (Goetz and Penn, 2000),
locking in idl-expressions (Nederhof and Satta, 2004), and in-
tegrity constraints in fo-tag (Becker et al, 1991).
of rewrite rules which pair n-ary elements of F with
n+1 nonterminals, and S?N is the start symbol, and
d(S) = 1.
This definition extends the standard mcfg defini-
tion (Seki et al, 1991), with two main differences.
First, strings have been generalized to pomsets, i.e.
P is a set of i-tuples of pomsets instead of i-tuples of
strings. It follows that F, the set of functions, oper-
ate on tuples of pomsets instead of tuples of strings,
and so forth. Second, pomset mcfgs explicitly spec-
ify O, the set of possible operators over the pomsets,
e.g. {;, ? ?2}; string mcfgs have an implied operator
set O={;} (i.e. just string concatenation).
Additionally, just as in mcfgs, where the arity of
string components are limited, we can limit the ar-
ity of the concurrency of pomsets. A n-concurrent
pomset mcfg is a pomset mcfg such that for all con-
currency operators ?i in the grammar, i?n. A pom-
set mcfg with no concurrency among its components
is a 1-concurrent pomset mcfg, just as a cfg is a 1-
mcfg.
3 Parsing
In this section we propose a strategy for parsing
pomset mcfgs, based on IDL parsing (Nederhof and
Satta, 2004). We define pomset graphs, which ex-
tend IDL graphs and pom-automata and are defined
over tuples of pomsets (or tuples of idl expressions),
rather than single pomsets or idl expressions. An in-
formal analysis of the computational complexity for
parsing pomset mcfgs follows.
Pomset graphs The construction is quite straight
forward, as pomsets themselves can already be con-
sidered as DAGs. However, in the pomset graph,
we add two vertices, the start and end vertices. We
then add precedence relations such that the start ver-
tex precedes all minimal vertices of the pomset, and
that the end vertex succeeds all maximal vertices of
the pomset. For any nonempty pomset, we define
Vmin ?V and Vmax ?V to be the minimal and
maximal, respectively, vertices of V. Informally, no
vertex in a pomset precede Vmin and none succeed
any in Vmax. Formally, ? v?V, v??V,v?6=v, Vmin =
{ v | (v?,v) 6? } and Vmax = { v | (v,v?) 6? }. The
start vertex is then labelled with the empty string, ,
and the end vertex is labelled with ??, a symbol not
in ?.
107
Given a pomset p= (Vp,?,,?p), a pomset
graph for p is a vertex labelled graph ?(p) =
(V? ,E,??) where V? and E are a finite set of ver-
tices and edges, where V?=Vp?{vs,ve} and E= 
?vs?Vmin?Vmax?ve, ??=??{,??}, where ?? is
a symbol not in ?, and ??=?p?{(vs,),(ve,??)} is
the vertex labelling function. Having defined the
pomset graph, we can apply the IDL parsing algo-
rithm to the graph.
Complexity While the complexity of the mem-
bership problem for pomset languages in general
is NP-complete (Feigenbaum et al, 1993), by re-
stricting the context sensitivity of the pomset gram-
mars, polynomial time complexity is achievable.
The complexity of the parsing of IDL graphs is
O(n3k) (Nederhof and Satta, 2004) where k is the
width of the graph, and the width is a measurement
of the number of paths being traversed in parallel,
i.e. the arity of the concurrent context sensitivity.
Our intuition is that the parameterization of the com-
plexity according to the number of parallel paths
applies even when structural context sensitivity is
added. Thus for a k-concurrent m-structural mcfg,
we conjecture that the complexity is O(n3km).
4 Conclusion
In this paper we identified two types of context sen-
sitivity, and provided a natural language example
which exhibits both types of context sensitivity. We
introduced pomset mcfgs as a formalism for describ-
ing grammars with both types of context sensitivity,
and outlined an informal proof of the its polynomial-
time parsing complexity.
References
Twan Basten. 1997. Parsing partially ordered multisets.
International Journal of Foundations of Computer Sci-
ence, 8(4):379?407.
Tilman Becker, Aravind K. Joshi, and Owen Rambow.
1991. Long distance scrambling and tree adjoining
grammars. In Proceedings of EACL-91, the 5th Con-
ference of the European Chapter of the Association for
Computational Linguistics.
Jonathan David Bobaljik. 1999. Adverbs: The hierarchy
paradox. Glot International, 4.
Joan Feigenbaum, Jeremy A. Kahn, and Carsten Lund.
1993. Complexity results for pomset languages. SIAM
Journal of Discrete Mathematics, 6(3):432?442.
Jay Gischer. 1981. Shuffle languages, Petri nets, and
context-sensitive grammars. Communications of the
ACM, 24(9):597?605, September.
Thilo Goetz and Gerald Penn. 2000. A proposed lin-
ear specification language. Technical Report 134, Ar-
beitspapiere des SFB 340.
A. Lecomte and C. Retore. 1995. Pomset logic as an
alternative categorial grammar. In Glyn Morrill and
Richard Oehrle, editors, Formal Grammar, pages 181?
196.
K. Lodaya and P. Weil. 2000. Series-parallel languages
and the bounded-width property. Theoretical Com-
puter Science, 237(1?2):347?380.
Gereon Mu?ller. 2000. Shape conservation and remnant
movement. In Proceedings of NELS 30.
Mark-Jan Nederhof and Giorgio Satta. 2004. IDL-
expressions: A formalism for representing and parsing
finite languages in natural language processing. Jour-
nal of Artificial Intelligence Research, 21:287?317.
Mark-Jan Nederhof, Giorgio Satta, and Stuart M.
Shieber. 2003. Partially ordered multiset context-free
grammars and ID/LP parsing. In Proceedings of the
Eighth International Workshop on Parsing Technolo-
gies, pages 171?182, Nancy, France, April.
Vaughan R. Pratt. 1985. The pomset model of paral-
lel processes : Unifying the temporal and the spatial.
Technical report, Stanford University, January.
Vaughan R. Pratt. 1986. Modelling concurrency with
partial orders. International Journal of Parallel Pro-
gramming, 15(1):33?71.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context free gram-
mars. Theoretical Computer Science, 88:191?229.
Edward P. Stabler. 1996. Derivational minimalism.
In Christian Retore?, editor, LACL, volume 1328 of
Lecture Notes in Computer Science, pages 68?95.
Springer.
K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In Proceedings of
the ACL, pages 104?111, Stanford, CA.
Manfred K. Warmuth and David Haussler. 1984. On the
complexity of iterated shuffle. J. Comput. Syst. Sci.,
28(3):345?358.
108
