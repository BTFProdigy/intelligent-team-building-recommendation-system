Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1020?1028,
Beijing, August 2010
Ukwabelana - An open-source morphological Zulu corpus
Sebastian Spiegler
Intelligent Systems Group
University of Bristol
spiegler@cs.bris.ac.uk
Andrew van der Spuy
Linguistics Department
University of the Witwatersrand
andrew.vanderspuy@wits.ac.za
Peter A. Flach
Intelligent Systems Group
University of Bristol
peter.flach@bris.ac.uk
Abstract
Zulu is an indigenous language of South
Africa, and one of the eleven official
languages of that country. It is spoken
by about 11 million speakers. Although
it is similar in size to some Western
languages, e.g. Swedish, it is consid-
erably under-resourced. This paper
presents a new open-source morphologi-
cal corpus for Zulu named Ukwabelana
corpus. We describe the agglutinating
morphology of Zulu with its multiple
prefixation and suffixation, and also
introduce our labeling scheme. Further,
the annotation process is described and
all single resources are explained. These
comprise a list of 10,000 labeled and
100,000 unlabeled word types, 3,000
part-of-speech (POS) tagged and 30,000
raw sentences as well as a morphological
Zulu grammar, and a parsing algorithm
which hypothesizes possible word roots
and enumerates parses that conform to the
Zulu grammar. We also provide a POS
tagger which assigns the grammatical
category to a morphologically analyzed
word type. As it is hoped that the corpus
and all resources will be of benefit to
any person doing research on Zulu or on
computer-aided analysis of languages,
they will be made available in the public
domain from http://www.cs.bris.
ac.uk/Research/MachineLearning/
Morphology/Resources/.
1 Introduction
Zulu (also known as isiZulu) is a Bantu language
of South Africa, classified as S.30 in Guthrie?s
classification scheme (Guthrie, 1971). Since
1994, it has been recognized as one of the eleven
official languages of South Africa. It has a written
history of about 150 years: the first grammar was
published by Grout (1859), and the first dictionary
by Colenso (1905). There are about 11 million
mother-tongue speakers, who constitute approxi-
mately 23% of South Africa?s population, making
Zulu the country?s largest language.
Zulu is highly mutually intelligible with the
Xhosa, Swati and Southern Ndebele languages,
and with Ndebele of Zimbabwe (Lanham, 1960),
to the extent that all of these can be consid-
ered dialects or varieties of a single language,
Nguni. Despite its size, Zulu is considerably
under-resourced, compared to Western languages
with similar numbers of speakers, e.g. Swedish.
There are only about four regular publications in
Zulu, there are few published books, and the lan-
guage is not used as a medium of instruction.
This of course is partly due to the short time-
span of its written history, but the main reason, of
course, is the apartheid history of South Africa:
for most of the twentieth century resources were
allocated to Afrikaans and English, the two former
official languages, and relatively few resources
to the indigenous Bantu languages. Since 1994,
Zulu has had a much larger presence in the media,
with several television programs being broadcast
in Zulu every day. Yet much needs to be done in
order to improve the resources available to Zulu
speakers and students of Zulu.
The aim of the project reported in this paper
was to establish a Zulu corpus, named the Uk-
wabelana corpus1, consisting of morphologically
labeled words (that is, word types) and part-of-
speech (POS) tagged sentences. Along with the
labeled corpus, unlabeled words and sentences, a
morphological grammar, a semi-automatic mor-
1Ukwabelana means ?to share? in Zulu where the ?k? is
pronounced voiced like a [g].
1020
phological analyzer and a POS tagger for morpho-
logically analyzed words will be provided.
The sources used for the corpus were limited to
fictional works and the Zulu Bible. This means
that there is not a wide variety of registers, and
perhaps even of vocabulary items. This defect will
have to be corrected in future work.
The Ukwabelana corpus can be used to de-
velop and train automatic morphological analyz-
ers, which in turn tag a large corpus of writ-
ten Zulu, similar to the Brown corpus or the
British National Corpus. Moreover, the list of
POS tagged sentences is an essential step towards
building an automatic syntactic tagger, which still
does not exist for Zulu, and a tagged corpus of
Zulu. Such a corpus would be beneficial to lan-
guage researchers as it provides them with ex-
amples of actual usage, as opposed to elicited
or invented examples, which may be artificial or
unlikely to occur in real discourse. This would
greatly improve the quality of Zulu dictionaries
and grammars, most of which rely heavily on
the work of Doke (1927) and Doke, Malcom and
Sikakana (1958), with little in the way of inno-
vation. Morphological tagging is also useful for
practical computational applications like predic-
tive text, spell-checking, grammar checking and
machine translation; in the case of Zulu, where
a large percentage of grammatical information is
conveyed by prefixes and suffixes rather than by
separate words, it is essential. For example, in
English, the negative is expressed by means of a
separate word ?not?, but in Zulu the negative is
constructed using a prefix-and-suffix combination
on the verb, and this combination differs accord-
ing to the mood of the verb (indicative, participial
or subjunctive). The practical computational ap-
plications mentioned could have a very great im-
pact on the use of Zulu as a written language, as
spell-checking and grammar checking would ben-
efit proofreaders, editors and writers. Machine
translation could aid in increasing the number of
texts available in Zulu, thus making it more of a
literary language, and allowing it to become es-
tablished as a language of education. The use
of Zulu in public life could also increase. Cur-
rently, the tendency is to use English, as this is
the language that reaches the widest audience. If
high-quality automatic translation becomes avail-
able, this would no longer be necessary. As it is
hoped that the Ukwabelana corpus will be of ben-
efit to any person doing research on Zulu or on
computer-aided analysis of languages, it will be
made available as the first morphologically anal-
ysed corpus of Zulu in the public domain.
2 Related work
In this section, we will give an overview of lin-
guistic research on Nguni languages, following
the discussions in van der Spuy (2001), and there-
after a summary of computational approaches to
the analysis of Zulu.
2.1 Linguistic research on Nguni languages
The five Nguni languages Zulu, Xhosa, South
African Ndebele, Swati, and Zimbabwean Nde-
bele are highly mutually intelligible, and for this
reason, works on any of the other Nguni languages
are directly relevant to an analysis of Zulu.
There have been numerous studies of Nguni
grammar, especially its morphology; in fact,
the Nguni languages probably rival Swahili and
Chewa for the title of most-studied Bantu lan-
guage. The generative approach to morphologi-
cal description (as developed by Aronoff (1976),
Selkirk (1982), Lieber (1980), Lieber (1992)) has
had very little influence on most of the work that
has been done on Nguni morphology.
Usually, the descriptions have been atheoreti-
cal or structuralist. Doke?s paradigmatic descrip-
tion of the morphology (Doke, 1927; Doke, 1935)
has remained the basis for linguistic work in the
Southern Bantu languages. Doke (1935) criticized
previous writers on Bantu grammars for basing
their classification, treatment and terminology on
their own mother tongue or Latin. His intention
was to create a grammatical structure for Bantu
which did not conform to European or classical
standards. Nevertheless, Doke himself could not
shake off the European mindset: he treated the
languages as if they had inflectional paradigms,
with characteristics like subjunctive or indicative
belonging to the whole word, rather than to identi-
fiable affixes; in fact, he claimed (1950) that Bantu
languages are ?inflectional with [just] a tendency
to agglutination?, and assumed that the morphol-
1021
ogy was linear not hierarchical. Most subsequent
linguistic studies and reference grammars of the
Southern Bantu languages have been directed at
refining or redefining Doke?s categories from a
paradigmatic perspective.
Important Nguni examples are Van Eeden
(1956), Van Wyk (1958), Beuchat (1966), Wilkes
(1971), Nkabinde (1975), Cope (1984), Davey
(1984), Louw (1984), Ziervogel et al (1985),
Gauton (1990), Gauton (1994), Khumalo (1992),
Poulos and Msimang (1998), Posthumus (1987),
Posthumus (1988), Posthumus (1988) and Posthu-
mus (2000). Among the very few generative
morphological descriptions of Nguni are Lanham
(1971), Mbadi (1988) and Du Plessis (1993). Lan-
ham (1971) gives a transformational analysis of
Zulu adjectival and relative forms. This analy-
sis can be viewed as diachronic rather than syn-
chronic. Mbadi (1988) applies Lieber (1980)
and Selkirk?s percolation theory (Selkirk, 1982)
to a few Xhosa morphological forms. Du Plessis
(1993) gives a hierarchical description of the mor-
phology of the verb, but he assumes that deriva-
tion is syntactical rather than lexical.
In short, there has been no thorough-going
generative analysis of the morphology which
has treated the Nguni languages as agglutinative
rather than inflectional.
2.2 Computational approaches to analyzing
Zulu
In the last decade, various computational ap-
proaches for Zulu have been reported. Based on
the Xerox finite-state toolbox by Beesley and Kart-
tunen (2003), Pretorius and Bosch (2003) devel-
oped a prototype of a computational morpholog-
ical analyzer for Zulu. Using a semi-automated
process, a morphological lexicon and a rule-base
were built incrementally. Later work (Pretorius
and Bosch, 2007) dealt with overgeneration of
the Zulu finite-state tool concerning locative for-
mation from nouns and verbal extensions to verb
roots. Pretorius and Bosch (2009) also used cross-
linguistic similarities and dissimilarities of Zulu
to bootstrap a morphological analyser for Xhosa.
Joubert et al (2004) followed a bootstrapping
approach to morphological analysis. A simple
framework uses morpheme lists, morphophono-
logical and morphosyntactic rules which are learnt
by consulting an oracle, in their case a linguis-
tic expert who corrects analyses. The frame-
work then revises its grammar so that the updated
morpheme lists and rules do not contradict previ-
ously found analyses. Botha and Barnard (2005)
compared two approaches for gathering Zulu text
corpora from the World Wide Web. They drew
the conclusion that using commercial search en-
gines for finding Zulu websites outperforms web-
crawlers even with a carefully selected starting
point. They saw the reason for that in the fact that
most documents on the internet are in one of the
world?s dominant languages. Bosch and Eiselen
(2005) presented a spell checker for Zulu based on
morphological analysis and regular expressions.
It was shown that after a certain threshold for
the lexicon size performance could only be im-
proved by incrementally extending morphological
rules. Experiments were performed for basic and
complex Zulu verbs and nouns, and large num-
bers of words still were not recognized. Spiegler
et al (2008) performed experiments where they
tested four machine learning algorithms for mor-
phological analysis with different degrees of su-
pervision. An unsupervised algorithm analyzed
a raw word list, two semi-supervised algorithms
were provided with word stems and subsequently
segmented prefix and suffix sequences, and the
supervised algorithm used a language model of
analysed words which was applied to new words.
They experimentally showed that there is a cer-
tain trade-off between the usage of labeled data
and performance. They also reckoned that com-
putational analysis improves if words of different
grammatical categories are analysed separately
since there exist homographic morphemes across
different word categories.
3 Zulu morphology
Zulu is an agglutinative language, with a complex
morphology. It presents an especial problem for
computational analysis, because words usually in-
corporate both prefixes and suffixes, and there can
be several of each. This makes it hard to identify
the root by mechanical means, as the root could
be the first, second, third, or even a later mor-
pheme in a word. The complexities involved are
1022
exacerbated by the fact that a considerable num-
ber of affixes, especially prefixes, have allomor-
phic forms. This is largely brought about by the
fact that Zulu has a prohibition against sequences
of vowels, so that a prefix whose canonical form is
nga- will have an allomorph ng- before roots that
begin with vowels. Given a sequence nga-, then, it
is possible that it constitutes an entire morpheme,
or the beginning of a morpheme like the verb root
ngabaz- ?to be uncertain?, or a morpheme ng- fol-
lowed by a vowel-commencing root like and- ?to
increase?. Furthermore, many morphemes are ho-
mographs, so that the prefix nga- could represent
either the potential mood morpheme or a form of
the negative that occurs in subordinate clauses;
and the sequence ng- could be the allomorph of ei-
ther of these, or of a number of homographic mor-
phemes ngi-, which represent the first person sin-
gular in various moods. Besides these phonologi-
cally conditioned allomorphs, there are also mor-
phologically conditioned ones, for example the
locative prefix e- has an allomorph o- that occurs
in certain morphological circumstances. Certain
morpheme sequences also exhibit syncretism, so
that while most nouns take a sequence of prefixes
known as the initial vowel and the noun prefix, as
in i-mi-zi ?villages?, nouns of certain classes, like
class 5, syncretise these two prefixes, as in i-gama
?name?, where the prefix i- represents both the ini-
tial vowel and the noun prefix.
Like all other Bantu languages, Zulu divides its
nouns into a number of classes. The class is often
identifiable from the noun prefix that is attached
to the noun, and it governs the agreement of all
words that modify the noun, as well as of predi-
cates of which the noun is a subject. Object agree-
ment may also be marked on the predicate. Two
examples of this agreement are given below.
Example 1.
Leso si-tshudeni e-si-hle e-ngi-si-fundis-ile si-phas-e kahle.
that student who-AGR-good who-I-him-teach-PAST AGR-
pass-PAST well.
?That good student whom I taught passed well.?
Example 2.
Lowo m-fundi o-mu-hle e-ngi-m-fundis-ile u-phas-e kahle.
that learner who-AGR-good who-I-him-teach-PAST AGR-
pass-PAST well.
?That good learner whom I taught passed well.?
The differences in agreement morphology in the
two sentences is brought about because the nouns
sitshudeni and mfundi belong to different classes.
Canonici (1996) argues that a noun should be as-
signed to a class by virtue of the agreement that it
takes. In terms of this criterion, there are twelve
noun classes in Zulu. These classes are numbered
1?7, 9, 10, 11, 14, 15. The numbering system
was devised by Meinhof (1906), and reflects the
historical affinities between Zulu and other Bantu
languages: Zulu lacks classes 8, 12 and 13, which
are found in other Bantu languages. In the labels
used on the database, morphemes that command
or show agreement have been labeled as <xn>,
where x is a letter or sequence of letters, and n is
a number: thus the morpheme m- in mfundi is la-
beled <n1>, as it marks the noun as belonging to
noun class 1. The morpheme si- in engisifundis-
ile is marked <o7>, as it shows object agreement
with a noun of class 7.
Zulu predicatives may be either verbal or non-
verbal ? the latter are referred to in the literature as
copulatives. Copulatives usually consist of a pred-
icative prefix and a base, which may be a noun,
an adjective, or a prepositional, locative or adver-
bial form. There may also be various tense, aspect
and polarity markers. They translate the English
verb ?be?, plus its complement ? Zulu has no di-
rect equivalent of ?be?; the verb -ba, which has
the closest meaning, is probably better translated
as ?become?. Examples of copulative forms are
ubenguthisha ?he was a teacher?, zimandla ?they
are strong?, basekhaya ?they are at home?. Pred-
icatives may occur in a variety of moods, tenses,
aspects and polarities; these are usually distin-
guished by the affixes attached to the base form.
Thus in engasesendlini ?(s)he no longer being in
the house?, the initial prefix e- indicates third per-
son singular, class 1, participial mood; the prefix
nga- denotes negative; the first prefix se- denotes
continuative aspect; the second prefix se- is the
locative prefix; n- shows that the noun belongs to
class 9; dl- is the noun root meaning ?house?, an
allomorph of the canonical form -dlu; and -ini is
the locative suffix. Thus in typical agglutinative
manner, each affix contributes a distinctive part of
1023
the meaning of the word as a whole. This charac-
teristic of the language was exploited in the label-
ing system used for the morphological corpus: la-
bels were designed so as to indicate the grammati-
cal function of the morpheme. A person searching
for past tense negative verbs, for example, could
simply search for the combination of <past >,
<neg> and <vr>. A complete list of morphemes,
allomorphs and their labels is provided along with
the corpus and other resources.
According to the Dokean grammatical tradition
(Doke, 1927), Zulu has a large number of parts
of speech. This is because what would be sepa-
rate words in other languages are often prefixes in
Zulu, and also because various subtypes of deter-
miner are given individual names. The parts of
speech recognised in the corpus are: noun, verb,
adjective, pronoun, adverb, conjunction, prepo-
sitional, possessive, locative, demonstrative, pre-
sentative, quantitative, copulative and relative.
Adjective includes the traditional Dokean ad-
jective (a closed class of roots which take noun
prefixes as their agreement prefixes) and the pred-
icative form of the Dokean relative, which is
seen as an open class of adjectives (cf. van der
Spuy (2006)). Pronouns are the personal pro-
nouns, which may also (sometimes in allomor-
phic form) be used as agreement morphemes in
quantifiers. Adverbs may be forms derived from
adjectives by prefixing ka- to the root, or mor-
phologically unanalysable forms like phansi ?in
front, forward?. Ideophones have been included
as adverbs. Prepositionals are words that incor-
porate the Dokean ?adverbials? na- ?with?, nga-
?by means of?, njenga- ?like?, kuna- ?more than?,
etc., which are better analysed as prepositions.
The presentative is Doke?s ?locative demonstra-
tive copulative? - the briefer name was suggested
by van der Spuy (2001). Copulatives are all
Doke?s copulatives, excluding the adjectives men-
tioned above. Relatives are all predicative forms
incorporating a relative prefix.
4 The labeling scheme
The labeling scheme has been based on the idea
that each morpheme in a word should be la-
beled, even when words belong to a very re-
stricted class. For example, the demonstratives
could have been labeled as composite forms, but
instead it is assumed that demonstratives con-
tain between one and three morphemes, e.g.
le<d>si<d7>ya<po3> ?a demonstrative of the
third position referring to class 7? - i.e.. ?that one
yonder, class 7?. It should be possible from this
detailed labeling to build up an amalgam of the
morphological structure of the word. The labels
have been chosen to be both as brief as possi-
ble and as transparent as possible, though trans-
parency was often sacrificed for brevity. Thus in-
dicative subject prefixes are labeled <i1-15>, rel-
ative prefixes are labeled <r>, and noun prefixes
are labeled <n1-15>; but negative subject pre-
fixes are labeled <g1-15> and possessive agree-
ment prefixes are labeled <z1-15>. Sometimes a
single label was used for several different forms,
when these are orthographically distinct, so for
example <asp> (aspect) is used as a label for
the following, among others: the continuative pre-
fix sa- and its allomorph se-, the exclusive pre-
fix se-, and the potential prefix nga- and its allo-
morph ng-. A person searching for forms contain-
ing the potential aspect would have to search for
?nga<asp> + ng<asp>?. However, there should
be no ambiguity, as the orthographic form would
eliminate this. The detailed description of the
scheme is provided by Spiegler et al (2010).
5 Annotation process
The goal of this project was to build a reason-
ably sized corpus of morphologically annotated
words of high quality which could be later used
for developing and training automatic morpholog-
ical analyzers. For this reason, we had gathered a
list of the commonest Zulu word types, defined
a partial grammar and parsed Zulu words with a
logic algorithm which proposes possible parses
based on the partial grammar. Compared to a
completely manual approach, this framework pro-
vided possible annotations to choose from or the
option to type in an annotation if none of the sug-
gestions was the correct one. This semi-automatic
process speeded up the labeling by an estimated
factor of 3-4, compared to a purely manual ap-
proach. In Figure 1 we illustrate the annotation
process and in the following subsections each step
is detailed.
1024
	 ?Hypothesis	 ?
generation	 ?
Hypothesis	 ?
evaluation	 ?
Best	 ?
hypothesis	 ?
Parsing	 ?
algorithm	 ?
Grammar	 ?update	 ?
Web	 ?interface	 ?
Annotation	 ?framework	 ?
Curation	 ? Ukwabelana	 ?
corpus	 ?Annotated	 ?word	 ?list	 ?
Unannotated	 ?
word	 ?list	 ?
Partial	 ?
grammar	 ?
Figure 1: Process view of the annotation.
5.1 Unannotated word list
A list of unannotated Zulu words has been com-
piled from fictional works and the Zulu Bible. The
original list comprises around 100,000 of the com-
monest Zulu word types. No information, mor-
phological or syntactic, was given along with the
words. We selected an initial subset of 10,000
words although our long-term goal is the complete
analysis of the entire word list.
5.2 Partial grammar
Our choice for representing the morphological
Zulu grammar was the formalism of Definite
Clause Grammars (DCGs) used in the logic pro-
gramming language Prolog. Although we de-
fined our grammar as a simple context-free gram-
mar, DCGs can also express context-sensitive
grammars by associating variables as arguments
to non-terminal symbols (Gazdar and Mellish,
1989). When defining our morphological gram-
mar, we assumed that a linguistic expert could
enumerate all or at least the most important mor-
phological rules and morphemes of ?closed? mor-
pheme categories, e.g. prefixes and suffixes of
nouns and verbs. Morphemes of ?open? categories
like noun and verb roots, however, would need to
be hypothesized during the semi-automatic anal-
ysis and confirmed by the linguistic expert. Our
final grammar comprised around 240 morpholog-
ical rules and almost 300 entries in the morpheme
dictionary. Since we did not only want to recog-
nize admissible Zulu words but also obtain their
morphological structure, we needed to extend our
DCG by adding parse construction arguments as
shown in the example below.
Example 3.
w((X)) --> n(X).
n((X,Y,Z)) --> iv(X),n2(Y),nr(Z).
iv(iv(a)) --> [a].
n2(n2(ba))--> [ba].
A possible parse for the word abantu ?people?
could be iv(a),n2(ba),*nr(ntu) where
?*? marks the hypothesized noun root.
With our partial grammar we could not directly
use the inbuilt Prolog parser since we had to ac-
count for missing dictionary entries: Zulu verb
and noun roots. We therefore implemented an
algorithm which would generate hypotheses for
possible parses according to our grammar. The al-
gorithm will be described in the next subsection.
5.3 Hypothesis generation
For the hypothesis generation we reverted to logic
programming and abductive reasoning. Abduc-
tion is a method of reasoning which is used with
incomplete information. It generates possible hy-
potheses (parses) for an observation (word) and a
given theory (grammar). Depending on the im-
plementation, abduction finds the best hypothe-
sis by evaluating all possible explanations. Our
abductive algorithm is an extension of the meta-
interpreter designed by Flach (1994) which only
enumerates possible parses based on the grammar.
A linguistic expert would then choose the best hy-
pothesis. The algorithm invokes rules top-down
starting with the most general until it reaches the
last level of syntactic variables. These variables
1025
are then matched against their dictionary entries
from the left to the right of the word. A possi-
ble parse is found if either all syntactic variables
can be matched to existing dictionary entries or
if an unmatched variable is listed as abducible.
Abducibles are predefined non-terminal symbols
whose dictionary entry can be hypothesized. In
our case, abducibles were noun and verb roots.
5.4 Evaluation and best hypothesis
Our annotation framework only enumerated al-
lowable parses for a given word, therefore a lin-
guistic expert needed to evaluate hypotheses. We
provided a web-interface to the annotation frame-
work, so that multiple users could participate in
the annotation process. They would choose either
a single or multiple correct parses. If none of the
hypotheses were correct, the user would provide
the correct analysis. Although our grammar was
incomplete it still generated a substantial number
of hypotheses per word. These were in no par-
ticular order and a result of the inherent ambi-
guity of Zulu morphology. We therefore experi-
mented with different ways of improving the pre-
sentation of parses. The most promising approach
was structural sorting. Parses were alphabetically
re-ordered according to their morphemes and la-
bels such that similar results were presented next
to each other.
5.5 Grammar update
The grammar was defined in an iterative process
and extended if the linguistic expert found mor-
phemes of closed categories which had not been
listed yet or certain patterns of incomplete or in-
correct parses caused by either missing or inaccu-
rate rules. The updated rules and dictionary were
considered for newly parsed words.
5.6 Annotated word list and curation process
Although there had been great effort in improv-
ing the hypothesis generation of the parsing al-
gorithm, a reasonable number of morphological
analyses still had to be provided manually. Dur-
ing the curation process, we therefore had to deal
with removing typos and standardizing morpheme
labels provided by different experts. In order to
guarantee a high quality of the morphological cor-
Category # Analyses #Word types
Verb 6965 4825
Noun 1437 1420
Relative 1042 988
Prepositional 969 951
Possessive 711 647
Copulative 558 545
Locative 380 379
Adverb 156 155
Modal 113 113
Demonstrative 63 61
Pronoun 38 31
Interjection 24 24
Presentative 15 15
Adjective 14 14
Conjunction 3 3
Total# 12488 10171
Table 1: Categories of labeled words.
pus, we also inspected single labels and analyses
for their correctness. This was done by examin-
ing frequencies of labels and label combinations
assuming that infrequent labels and combinations
were likely to be incorrect and needed to be man-
ually examined again. The finally curated corpus
has an estimated error of 0.4 ? 0.5 incorrect sin-
gle labels and 2.8? 2.1 incorrect complete analy-
ses per 100 parses. Along with each word?s anal-
ysis we wanted to provide part-of-speech (POS)
tags. This was done by using a set of rules which
determine the POS tag based on the morphologi-
cal structure. We developed a prototype of a POS
tagger which would assign the part-of-speech to a
given morphological analysis based on a set of 34
rules. A summary of morphological analyses and
words is given in Table 1. The rules are provided
in Spiegler et al (2010).
5.7 POS tagging of sentences
In addition to the list of morphologically labeled
words, we assigned parts-of-speech to a subset of
30,000 Zulu sentences. This task is straightfor-
ward if each word of a sentence only belongs to a
single grammatical category. This was the case for
2595 sentences. For 431 sentences, however, we
needed to disambiguate POS tags. We achieved
this by analysing the left and right context of a
word form and selecting the most probable part-
of-speech from a given list of possible tags.
The overall error is estimated at 3.1?0.3 incor-
rect POS tags per 100 words for the 3,000 sen-
1026
Dataset # Sentences #Word tokens #Word types #Words per sentence Word length
Raw 29,424 288,106 87,154 9.79?6.74 7.49?2.91
Tagged 3,026 21,416 7,858 7.08?3.75 6.81?2.68
Table 2: Statistics of raw and POS-tagged sentences.
tences we tagged. The summary statistics for raw
and tagged sentences are shown in Table 2.
6 The Ukwabelana corpus - a resource
description
The Ukwabelana corpus is three-fold:
1. It contains 10,000 morphologically labeled
words and 3,000 POS-tagged sentences.
2. The corpus also comprises around 100,000
common Zulu word types and 30,000 Zulu sen-
tences compiled from fictional works and the
Zulu Bible, from which the labeled words and
sentences have been sampled.
3. Furthermore, all software and additional data
used during the annotation process is provided:
the partial grammar in DCG format, the ab-
ductive algorithm for parsing with incomplete
information and a prototype for a POS tagger
which assigns word categories to morphologi-
cally analyzed words.
We are making these resources publicly available
from http://www.cs.bris.ac.uk/Research/
MachineLearning/Morphology/Resources/ so
that they will be of benefit to any person doing
research on Zulu or on computer-aided analysis
of languages.
7 Conclusions and future work
In this paper, we have given an overview of the
morphology of the language Zulu, which is spo-
ken by 23% and understood by more than half of
the South African population. As an indigenous
language with a written history of 150 years which
was only recognised as an official languages in
1994, it is considerably under-resourced. We have
spent considerable effort to compile the first open-
source corpus of labeled and unlabeled words as
well as POS-tagged and untagged sentences to
promote research on this Bantu language. We
have described the annotation process and the
tools for compiling this corpus. We see this work
as a first step in an ongoing effort to ultimately
label the entire word and sentence corpus.
Our future work includes further automation of
the annotation process by extending the described
abductive algorithm with a more sophisticated hy-
pothesis evaluation and by combining syntactical
and morphological information during the deci-
sion process. Our research interest also lies in the
field of automatic grammar induction which will
help to refine our partial grammar. Another aspect
is interactive labeling where a linguistic expert di-
rects the search of an online parsing algorithm by
providing additional information. Apart from the
benefits to language researchers, we foresee an ap-
plication of the corpus by machine learners which
can develop and train their algorithms for morpho-
logical analysis.
Acknowledgements
We would like to thank Etienne Barnard and the
Human Language Technologies Research Group
from the Meraka Institute for their support during
this project. Furthermore, we want to acknowl-
edge Johannes Magwaza, Bruno Gole?nia, Ksenia
Shalonova and Roger Tucker. The research work
was sponsored by EPSRC grant EP/E010857/1
Learning the morphology of complex synthetic
languages and a grant from the NRF (S. Africa).
References
Aronoff. 1976. Word Formation in Generative Grammar.
The MIT Press.
Beesley and Karttunen. 2003. Finite State Morphology.
University of Chicago Press.
Beuchat. 1966. The Verb in Zulu. African Studies, 22:137?
169.
Bosch and Eiselen. 2005. The Effectiveness of Morpho-
logical Rules for an isiZulu Spelling Checker. S. African
Journal of African Lang., 25:25?36.
Botha and Barnard. 2005. Two Approaches to Gathering
Text Corpora from the World Wide Web. 16th Ann. Symp.
of the Pattern Recog. Ass. of S. Africa.
1027
Canonici. 1996. Zulu Grammatical Structure. Zulu Lang.
and Literature, University of Natal, Durban.
Colenso. 1905. Zulu-English Dictionary. Natal, Vause,
Slatter & Co.
Cope. 1984. An Outline of Zulu Grammars. African Stud-
ies, 43(2):83?102.
Davey. 1984. Adjectives and Relatives in Zulu. S. African
Journal of African Lang., 4:125?138.
Doke. 1927. Text Book of Zulu Grammar. Witwatersrand
University Press.
Doke. 1935. Bantu Linguistic Terminology. Longman,
Green and Co, London.
Doke. 1954. Handbook of African Lang., chapter The S.ern
Bantu Lang. Oxford University Press.
Doke, Malcom and Sikakana. 1958. Zulu-English vocabu-
lary. Witwatersrand Uni. Press.
Du Plessis. 1993. Linguistica: Festschrift EB van Wyk,
chapter Inflection in Syntax, pp. 61?66. Van Schaik, Pre-
toria.
Flach. 1994. Simply Logical. John Wiley.
Gauton. 1990. Adjektiewe en Relatiewe in Zulu. Master?s
thesis, University of Pretoria.
Gauton. 1994. Towards the Recognition of a Word Class
?adjective? for Zulu. S. African Journal of African Lang.,
14:62?71.
Gazdar and Mellish. 1989. Natural Language Processing in
Prolog. Addison-Wesley.
Grout. 1859. The Isizulu: A Grammar Of The Zulu Lang.
Kessinger Publishing.
Guthrie. 1971. Comparative Bantu: An Introduction to
the Comparative Linguistics and Prehistory of the Bantu
Lang. Farnborough, Gregg International Publishers.
Joubert, Zimu, Davel, and Barnard. 2004. A Framework
for Bootstrapping Morphological Decomposition. Tech.
report, CSIR/University of Pretoria, S. Africa.
Khumalo. 1992. African Linguistic Contributions, chapter
The morphology of the direct relative in Zulu. Via Afrika.
Lanham. 1960. The Comparative Phonology of Nguni.
Ph.D. thesis, Witwatersrand Uni., Jo?burg, S. Africa.
Lanham. 1971. The Noun as Deep-Structure Source for
Nguni Adjectives and Relatives. African Studies, 30:294?
311.
Lieber. 1980. On the Organization of the Lexicon. Ph.D.
thesis, Massachusetts Institute of Technology.
Lieber. 1992. Deconstructing Morphology. The University
of Chicago Press.
Louw. 1984. Word Categories in Southern Bantu. African
Studies, 43(2):231?239.
Mbadi. 1988. Anthology of Articles on African Linguistics
and Literature, chapter The Percolation Theory in Xhosa
Morphology. Lexicon, Jo?burg.
Meinhof. 1906. Grundzu?ge einer Vergleichenden Gram-
matik der Bantusprachen. Reimer, Berlin.
Nkabinde. 1975. A Revision of the Word Categories in Zulu.
Ph.D. thesis, University of S. Africa.
Posthumus. 1987. Relevancy and Applicability of Terminol-
ogy Concerning the Essential Verb Categories in African
Lang. Logos, 7:185?212.
Posthumus. 1988. Identifying Copulatives in Zulu and S.ern
Sotho. S. African Journal of African Lang., 8:61?64.
Posthumus. 2000. The So-Called Adjective in Zulu. S.
African Journal of African Lang., 20:148?158.
Poulos and Msimang. 1998. A Linguistic Analysis of Zulu.
Via Afrika.
Pretorius and Bosch. 2003. Finite-State Computational
Morphology: An Analyzer Prototype For Zulu. Machine
Translation, 18:195?216.
Pretorius and Bosch. 2007. Containing Overgeneration in
Zulu Computational Morphology. Proceedings of 3rd
Lang. and Technology Conference, pp. 54 ? 58, Poznan.
Pretorius and Bosch. 2009. Exploiting Cross-Linguistic
Similarities in Zulu and Xhosa Computational Morphol-
ogy. Workshop on Lang. Technologies for African Lang.
(AfLaT), pp. 96?103.
Selkirk. 1982. The Syntax of Words. MIT Press.
Spiegler, Golenia, Shalonova, Flach, and Tucker. 2008.
Learning the Morphology of Zulu with Different Degrees
of Supervision. IEEE Workshop on Spoken Lang. Tech.
Spiegler, van der Spuy, Flach. 2010. Additional material for
the Ukwabelana Zulu corpus. Tech. report, University of
Bristol, U.K.
van der Spuy. 2001. Grammatical Structure and Zulu Mor-
phology. Ph.D. thesis, University of the Witwatersrand,
Jo?burg, S. Africa.
van der Spuy. 2006. Wordhood in Zulu. S.ern African Lin-
guistics and Applied Lang. Studies, 24(3):311?329.
Van Eeden. 1956. Zoeloe-Grammatika. Pro Ecclesia, Stel-
lenbosch.
Van Wyk. 1958. Woordverdeling in Noord-Sotho en Zulu:
?n bydrae tot die vraagstuk van word-identifikasie in die
Bantoetale. Ph.D. thesis, University of Pretoria.
Wilkes. 1971. Agtervoegsels van die werkwoord in Zulu.
Ph.D. thesis, Rand Afrikaans University.
Ziervogel, Louw, and Taljaard. 1985. A Handbook of the
Zulu Lang. Van Schaik, Pretoria.
1028
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 375?383,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Enhanced word decomposition by calibrating the decision threshold of
probabilistic models and using a model ensemble
Sebastian Spiegler
Intelligent Systems Laboratory,
University of Bristol, U.K.
spiegler@cs.bris.ac.uk
Peter A. Flach
Intelligent Systems Laboratory,
University of Bristol, U.K.
peter.flach@bristol.ac.uk
Abstract
This paper demonstrates that the use of
ensemble methods and carefully calibrat-
ing the decision threshold can signifi-
cantly improve the performance of ma-
chine learning methods for morphologi-
cal word decomposition. We employ two
algorithms which come from a family of
generative probabilistic models. The mod-
els consider segment boundaries as hidden
variables and include probabilities for let-
ter transitions within segments. The ad-
vantage of this model family is that it can
learn from small datasets and easily gen-
eralises to larger datasets. The first algo-
rithm PROMODES, which participated in
the Morpho Challenge 2009 (an interna-
tional competition for unsupervised mor-
phological analysis) employs a lower or-
der model whereas the second algorithm
PROMODES-H is a novel development of
the first using a higher order model. We
present the mathematical description for
both algorithms, conduct experiments on
the morphologically rich language Zulu
and compare characteristics of both algo-
rithms based on the experimental results.
1 Introduction
Words are often considered as the smallest unit
of a language when examining the grammatical
structure or the meaning of sentences, referred to
as syntax and semantics, however, words them-
selves possess an internal structure denominated
by the term word morphology. It is worthwhile
studying this internal structure since a language
description using its morphological formation is
more compact and complete than listing all pos-
sible words. This study is called morpholog-
ical analysis. According to Goldsmith (2009)
four tasks are assigned to morphological analy-
sis: word decomposition into morphemes, build-
ing morpheme dictionaries, defining morphosyn-
tactical rules which state how morphemes can
be combined to valid words and defining mor-
phophonological rules that specify phonological
changes morphemes undergo when they are com-
bined to words. Results of morphological analy-
sis are applied in speech synthesis (Sproat, 1996)
and recognition (Hirsimaki et al, 2006), machine
translation (Amtrup, 2003) and information re-
trieval (Kettunen, 2009).
1.1 Background
In the past years, there has been a lot of inter-
est and activity in the development of algorithms
for morphological analysis. All these approaches
have in common that they build a morphologi-
cal model which is then applied to analyse words.
Models are constructed using rule-based meth-
ods (Mooney and Califf, 1996; Muggleton and
Bain, 1999), connectionist methods (Rumelhart
and McClelland, 1986; Gasser, 1994) or statisti-
cal or probabilistic methods (Harris, 1955; Hafer
and Weiss, 1974). Another way of classifying ap-
proaches is based on the learning aspect during
the construction of the morphological model. If
the data for training the model has the same struc-
ture as the desired output of the morphological
analysis, in other words, if a morphological model
is learnt from labelled data, the algorithm is clas-
sified under supervised learning. An example for
a supervised algorithm is given by Oflazer et al
(2001). If the input data has no information to-
wards the desired output of the analysis, the algo-
rithm uses unsupervised learning. Unsupervised
algorithms for morphological analysis are Lin-
guistica (Goldsmith, 2001), Morfessor (Creutz,
2006) and Paramor (Monson, 2008). Minimally or
semi-supervised algorithms are provided with par-
tial information during the learning process. This
375
has been done, for instance, by Shalonova et al
(2009) who provided stems in addition to a word
list in order to find multiple pre- and suffixes. A
comparison of different levels of supervision for
morphology learning on Zulu has been carried out
by Spiegler et al (2008).
Our two algorithms, PROMODES and
PROMODES-H, perform word decomposi-
tion and are based on probabilistic methods
by incorporating a probabilistic generative
model.1 Their parameters can be estimated
from either labelled data, using maximum like-
lihood estimates, or from unlabelled data by
expectation maximization2 which makes them
either supervised or unsupervised algorithms.
The purpose of this paper is an analysis of the
underlying probabilistic models and the types of
errors committed by each one. Furthermore, it is
investigated how the decision threshold can be cal-
ibrated and a model ensemble is tested.
The remainder is structured as follows. In Sec-
tion 2 we introduce the probabilistic generative
process and show in Sections 2.1 and 2.2 how
we incorporate this process in PROMODES and
PROMODES-H. We start our experiments with ex-
amining the learning behaviour of the algorithms
in 3.1. Subsequently, we perform a position-wise
comparison of predictions in 3.2, show how we
find a better decision threshold for placing mor-
pheme boundaries in 3.3 and combine both algo-
rithms using a model ensemble to leverage indi-
vidual strengths in 3.4. In 3.5 we examine how
the single algorithms contribute to the result of the
ensemble. In Section 4 we will compare our ap-
proaches to related work and in Section 5 we will
draw our conclusions.
2 Probabilistic generative model
Intuitively, we could say that our models describe
the process of word generation from the left to the
right by alternately using two dice, the first for de-
ciding whether to place a morpheme boundary in
the current word position and the second to get a
corresponding letter transition. We are trying to
reverse this process in order to find the underlying
sequence of tosses which determine the morpheme
boundaries. We are applying the notion of a prob-
1PROMODES stands for PRObabilistic MOdel for different
DEgrees of Supervision. The H of PROMODES-H refers to
Higher order.
2In (Spiegler et al, 2009; Spiegler et al, 2010a) we have
presented an unsupervised version of PROMODES.
abilistic generative process consisting of words as
observed variables X and their hidden segmenta-
tion as latent variables Y . If a generative model is
fully parameterised it can be reversed to find the
underlying word decomposition by forming the
conditional probability distribution Pr(Y |X).
Let us first define the model-independent com-
ponents. A given word w j ?W with 1? j ? |W |
consists of n letters and has m = n?1 positions
for inserting boundaries. A word?s segmentation is
depicted as a boundary vector b j = (b j1, . . . ,b jm)
consisting of boundary values b ji ? {0,1} with
1? i? m which disclose whether or not a bound-
ary is placed in position i. A letter l j,i-1 precedes
the position i in w j and a letter l ji follows it. Both
letters l j,i-1 and l ji are part of an alphabet. Fur-
thermore, we introduce a letter transition t ji which
goes from l j,i-1 to l ji.
2.1 PROMODES
PROMODES is based on a zero-order model for
boundaries b ji and on a first-order model for letter
transitions t ji. It describes a word?s segmentation
by its morpheme boundaries and resulting letter
transitions within morphemes. A boundary vector
b j is found by evaluating each position i with
argmax
b ji
Pr(b ji|t ji) = (1)
argmax
b ji
Pr(b ji)Pr(t ji|b ji) .
The first component of the equation above is
the probability distribution over non-/boundaries
Pr(b ji). We assume that a boundary in i is in-
serted independently from other boundaries (zero-
order) and the graphemic representation of the
word, however, is conditioned on the length of
the word m j which means that the probability
distribution is in fact Pr(b ji|m j). We guarantee
?1r=0 Pr(b ji=r|m j) = 1. To simplify the notation
in later explanations, we will refer to Pr(b ji|m j)
as Pr(b ji).
The second component is the letter transition
probability distribution Pr(t ji|b ji). We suppose a
first-order Markov chain consisting of transitions
t ji from letter l j,i-1 ? AB to letter l ji ? A where A
is a regular letter alphabet and AB=A?{B} in-
cludes B as an abstract morpheme start symbol
which can occur in l j,i-1. For instance, the suf-
fix ?s? of the verb form gets, marking 3rd person
singular, would be modelled as B? s whereas a
morpheme internal transition could be g? e. We
376
guarantee ?l ji?A Pr(t ji|b ji)=1 with t ji being a tran-
sition from a certain l j,i?1 ? AB to l ji. The ad-
vantage of the model is that instead of evaluating
an exponential number of possible segmentations
(2m), the best segmentation b?j=(b
?
j1, . . . ,b
?
jm) is
found with 2m position-wise evaluations using
b?ji = argmax
b ji
Pr(b ji|t ji) (2)
=
?
?????
?????
1, if Pr(b ji=1)Pr(t ji|b ji=1)
> Pr(b ji=0)Pr(t ji|b ji=0)
0, otherwise .
The simplifying assumptions made, however,
reduce the expressive power of the model by not
allowing any dependencies on preceding bound-
aries or letters. This can lead to over-segmentation
and therefore influences the performance of PRO-
MODES. For this reason, we have extended the
model which led to PROMODES-H, a higher-order
probabilistic model.
2.2 PROMODES-H
In contrast to the original PROMODES model, we
also consider the boundary value b j,i-1 and mod-
ify our transition assumptions for PROMODES-
H in such a way that the new algorithm applies
a first-order boundary model and a second-order
transition model. A transition t ji is now defined
as a transition from an abstract symbol in l j,i-1 ?
{N ,B} to a letter in l ji ? A. The abstract sym-
bol is N or B depending on whether b ji is 0 or 1.
This holds equivalently for letter transitions t j,i-1.
The suffix of our previous example gets would be
modelled N ? t?B? s.
Our boundary vector b j is then constructed from
argmax
b ji
Pr(b ji|t ji, t j,i-1,b j,i-1) = (3)
argmax
b ji
Pr(b ji|b j,i-1)Pr(t ji|b ji, t j,i-1,b j,i-1) .
The first component, the probability distribution
over non-/boundaries Pr(b ji|b j,i-1), satisfies
?1r=0 Pr(b ji=r|b j,i-1)=1 with b j,i-1,b ji ? {0,1}.
As for PROMODES, Pr(b ji|b j,i-1) is short-
hand for Pr(b ji|b j,i-1,m j). The second
component, the letter transition proba-
bility distribution Pr(t ji|b ji,b j,i-1), fulfils
?l ji?A Pr(t ji|b ji, t j,i-1,b j,i-1)=1 with t ji being
a transition from a certain l j,i?1 ? AB to l ji. Once
again, we find the word?s best segmentation b?j in
2m evaluations with
b?ji = argmax
b ji
Pr(b ji|t ji, t j,i-1,b j,i-1) = (4)
?
??
??
1, if Pr(b ji=1|b j,i-1)Pr(t ji|b ji=1, t j,i-1,b j,i-1)
> Pr(b ji=0|b j,i-1)Pr(t ji|b ji=0, t j,i-1,b j,i-1)
0, otherwise .
We will show in the experimental results that in-
creasing the memory of the algorithm by looking
at b j,i?1 leads to a better performance.
3 Experiments and Results
In the Morpho Challenge 2009, PROMODES
achieved competitive results on Finnish, Turkish,
English and German ? and scored highest on non-
vowelized and vowelized Arabic compared to 9
other algorithms (Kurimo et al, 2009). For the
experiments described below, we chose the South
African language Zulu since our research work
mainly aims at creating morphological resources
for under-resourced indigenous languages. Zulu
is an agglutinative language with a complex mor-
phology where multiple prefixes and suffixes con-
tribute to a word?s meaning. Nevertheless, it
seems that segment boundaries are more likely in
certain word positions. The PROMODES family
harnesses this characteristic in combination with
describing morphemes by letter transitions. From
the Ukwabelana corpus (Spiegler et al, 2010b) we
sampled 2500 Zulu words with a single segmenta-
tion each.
3.1 Learning with increasing experience
In our first experiment we applied 10-fold cross-
validation on datasets ranging from 500 to 2500
words with the goal of measuring how the learning
improves with increasing experience in terms of
training set size. We want to remind the reader that
our two algorithms are aimed at small datasets.
We randomly split each dataset into 10 subsets
where each subset was a test set and the corre-
sponding 9 remaining sets were merged to a train-
ing set. We kept the labels of the training set
to determine model parameters through maximum
likelihood estimates and applied each model to
the test set from which we had removed the an-
swer keys. We compared results on the test set
against the ground truth by counting true positive
(TP), false positive (FP), true negative (TN) and
377
false negative (FN) morpheme boundary predic-
tions. Counts were summarised using precision3,
recall4 and f-measure5, as shown in Table 1.
Data Precision Recall F-measure
500 0.7127?0.0418 0.3500?0.0272 0.4687?0.0284
1000 0.7435?0.0556 0.3350?0.0197 0.4614?0.0250
1500 0.7460?0.0529 0.3160?0.0150 0.4435?0.0206
2000 0.7504?0.0235 0.3068?0.0141 0.4354?0.0168
2500 0.7557?0.0356 0.3045?0.0138 0.4337?0.0163
(a) PROMODES
Data Precision Recall F-measure
500 0.6983?0.0511 0.4938?0.0404 0.5776?0.0395
1000 0.6865?0.0298 0.5177?0.0177 0.5901?0.0205
1500 0.6952?0.0308 0.5376?0.0197 0.6058?0.0173
2000 0.7008?0.0140 0.5316?0.0146 0.6044?0.0110
2500 0.6941?0.0184 0.5396?0.0218 0.6068?0.0151
(b) PROMODES-H
Table 1: 10-fold cross-validation on Zulu.
For PROMODES we can see in Table 1a that
the precision increases slightly from 0.7127 to
0.7557 whereas the recall decreases from 0.3500
to 0.3045 going from dataset size 500 to 2500.
This suggests that to some extent fewer morpheme
boundaries are discovered but the ones which are
found are more likely to be correct. We believe
that this effect is caused by the limited memory
of the model which uses order zero for the occur-
rence of a boundary and order one for letter tran-
sitions. It seems that the model gets quickly sat-
urated in terms of incorporating new information
and therefore precision and recall do not drasti-
cally change for increasing dataset sizes. In Ta-
ble 1b we show results for PROMODES-H. Across
the datasets precision stays comparatively con-
stant around a mean of 0.6949 whereas the recall
increases from 0.4938 to 0.5396. Compared to
PROMODES we observe an increase in recall be-
tween 0.1438 and 0.2351 at a cost of a decrease in
precision between 0.0144 and 0.0616.
Since both algorithms show different behaviour
with increasing experience and PROMODES-H
yields a higher f-measure across all datasets, we
will investigate in the next experiments how these
differences manifest themselves at the boundary
level.
3 precision = T PT P+FP .
4recall = T PT P+FN .
5 f -measure = 2?precision?recallprecision+recall .
TNPH	 ?=	 ?0.8726	 ?
TNP	 ?	 ?	 ?=	 ?0.9472	 ?
	 ?
TPPH=	 ?0.5394	 ?
TPP	 ?	 ?	 ?=	 ?0.3045	 ?
	 ?
FPPH=	 ?0.1274	 ?
FPP	 ?	 ?	 ?=	 ?0.0528	 ?
	 ?
	 ?FNPH	 ?=	 ?0.4606	 ?
	 ?FNP	 ?	 ?	 ?	 ?=	 ?0.6955	 ?
	 ?
0.3109	 ? 0.7889	 ?
0.2111	 ?0.6891	 ?
+	 ?0.0819	 ?
(net)	 ?
+	 ?0.0486	 ?
(net)	 ?
0.5698	 ?0.8828	 ?
0.4302	 ?0.1172	 ?
	 ?
Figure 1: Contingency table for PROMODES [grey
with subscript P] and PROMODES-H [black with
subscript PH] results including gross and net
changes of PROMODES-H.
3.2 Position-wise comparison of algorithmic
predictions
In the second experiment, we investigated which
aspects of PROMODES-H in comparison to PRO-
MODES led to the above described differences in
performance. For this reason we broke down
the summary measures of precision and recall
into their original components: true/false positive
(TP/FP) and negative (TN/FN) counts presented in
the 2? 2 contingency table of Figure 1. For gen-
eral evidence, we averaged across all experiments
using relative frequencies. Note that the relative
frequencies of positives (TP + FN) and negatives
(TN + FP) each sum to one.
The goal was to find out how predictions
in each word position changed when applying
PROMODES-H instead of PROMODES. This
would show where the algorithms agree and
where they disagree. PROMODES classifies non-
boundaries in 0.9472 of the times correctly as TN
and in 0.0528 of the times falsely as boundaries
(FP). The algorithm correctly labels 0.3045 of the
positions as boundaries (TP) and 0.6955 falsely as
non-boundaries (FN). We can see that PROMODES
follows a rather conservative approach.
When applying PROMODES-H, the majority of
the FP?s are turned into non-boundaries, how-
ever, a slightly higher number of previously cor-
rectly labelled non-boundaries are turned into
false boundaries. The net change is a 0.0486 in-
crease in FP?s which is the reason for the decrease
in precision. On the other side, more false non-
378
boundaries (FN) are turned into boundaries than
in the opposite direction with a net increase of
0.0819 of correct boundaries which led to the in-
creased recall. Since the deduction of precision
is less than the increase of recall, a better over-all
performance of PROMODES-H is achieved.
In summary, PROMODES predicts more accu-
rately non-boundaries whereas PROMODES-H is
better at finding morpheme boundaries. So far we
have based our decision for placing a boundary in
a certain word position on Equation 2 and 4 as-
suming that P(b ji=1| . . .)> P(b ji=0| . . .)6 gives the
best result. However, if the underlying distribu-
tion for boundaries given the evidence is skewed,
it might be possible to improve results by introduc-
ing a certain decision threshold for inserting mor-
pheme boundaries. We will put this idea to the test
in the following section.
3.3 Calibration of the decision threshold
For the third experiment we slightly changed our
experimental setup. Instead of dividing datasets
during 10-fold cross-validation into training and
test subsets with the ratio of 9:1 we randomly split
the data into training, validation and test sets with
the ratio of 8:1:1. We then run our experiments
and measured contingency table counts.
Rather than placing a boundary if
P(b ji=1| . . .) > P(b ji=0| . . .) which corresponds
to P(b ji=1| . . .) > 0.50 we introduced a decision
threshold P(b ji=1| . . .) > h with 0? h? 1. This
is based on the assumption that the underlying
distribution P(b ji| . . .) might be skewed and an
optimal decision can be achieved at a different
threshold. The optimal threshold was sought on
the validation set and evaluated on the test set.
An overview over the validation and test results
is given in Table 2. We want to point out that the
threshold which yields the best f-measure result
on the validation set returns almost the same
result on the separate test set for both algorithms
which suggests the existence of a general optimal
threshold.
Since this experiment provided us with a set of
data points where the recall varied monotonically
with the threshold and the precision changed ac-
cordingly, we reverted to precision-recall curves
(PR curves) from machine learning. Following
Davis and Goadrich (2006) the algorithmic perfor-
6Based on Equation 2 and 4 we use the notation P(b ji| . . .)
if we do not want to specify the algorithm.
mance can be analysed more informatively using
these kinds of curves. The PR curve is plotted with
recall on the x-axis and precision on the y-axis for
increasing thresholds h. The PR curves for PRO-
MODES and PROMODES-H are shown in Figure
2 on the validation set from which we learnt our
optimal thresholds h?. Points were connected for
readability only ? points on the PR curve cannot
be interpolated linearly.
In addition to the PR curves, we plotted isomet-
rics for corresponding f-measure values which are
defined as precision= f -measure?recall2recall? f -measure and are hy-
perboles. For increasing f-measure values the iso-
metrics are moving further to the top-right corner
of the plot. For a threshold of h = 0.50 (marked
by ?3?) PROMODES-H has a better performance
than PROMODES. Nevertheless, across the entire
PR curve none of the algorithms dominates. One
curve would dominate another if all data points
of the dominated curve were beneath or equal
to the dominating one. PROMODES has its opti-
mal threshold at h? = 0.36 and PROMODES-H at
h? = 0.37 where PROMODES has a slightly higher
f-measure than PROMODES-H. The points of op-
timal f-measure performance are marked with ?4?
on the PR curve.
Prec. Recall F-meas.
PROMODES validation (h=0.50) 0.7522 0.3087 0.4378
PROMODES test (h=0.50) 0.7540 0.3084 0.4378
PROMODES validation (h?=0.36) 0.5857 0.7824 0.6699
PROMODES test (h?=0.36) 0.5869 0.7803 0.6699
PROMODES-H validation (h=0.50) 0.6983 0.5333 0.6047
PROMODES-H test (h=0.50) 0.6960 0.5319 0.6030
PROMODES-H validation (h?=0.37) 0.5848 0.7491 0.6568
PROMODES-H test (h?=0.37) 0.5857 0.7491 0.6574
Table 2: PROMODES and PROMODES-H on vali-
dation and test set.
Summarizing, we have shown that both algo-
rithms commit different errors at the word posi-
tion level whereas PROMODES is better in pre-
dicting non-boundaries and PROMODES-H gives
better results for morpheme boundaries at the de-
fault threshold of h = 0.50. In this section, we
demonstrated that across different decision thresh-
olds h for P(b ji=1| . . .) > h none of algorithms
dominates the other one, and at the optimal thresh-
old PROMODES achieves a slightly higher perfor-
mance than PROMODES-H. The question which
arises is whether we can combine PROMODES and
PROMODES-H in an ensemble that leverages indi-
vidual strengths of both.
379
0.4 0.5 0.6 0.7 0.8 0.9 1
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pre
cis
ion
 
 
Promodes
Promodes?H
Promodes?E
F?measure isometrics
Default result
Optimal result (h*)
Figure 2: Precision-recall curves for algorithms on validation set.
3.4 A model ensemble to leverage individual
strengths
A model ensemble is a set of individually trained
classifiers whose predictions are combined when
classifying new instances (Opitz and Maclin,
1999). The idea is that by combining PROMODES
and PROMODES-H, we would be able to avoid cer-
tain errors each model commits by consulting the
other model as well. We introduce PROMODES-E
as the ensemble of PROMODES and PROMODES-
H. PROMODES-E accesses the individual proba-
bilities Pr(b ji=1| . . .) and simply averages them:
Pr(b ji=1|t ji)+Pr(b ji=1|t ji,b j,i-1, t j,i-1)
2
> h .
As before, we used the default threshold
h = 0.50 and found the calibrated threshold
h? = 0.38, marked with ?3? and ?4? in Figure 2
and shown in Table 3. The calibrated threshold
improves the f-measure over both PROMODES and
PROMODES-H.
Prec. Recall F-meas.
PROMODES-E validation (h=0.50) 0.8445 0.4328 0.5723
PROMODES-E test (h=0.50) 0.8438 0.4352 0.5742
PROMODES-E validation (h?=0.38) 0.6354 0.7625 0.6931
PROMODES-E test (h?=0.38) 0.6350 0.7620 0.6927
Table 3: PROMODES-E on validation and test set.
The optimal solution applying h? = 0.38 is
more balanced between precision and recall and
boosted the original result by 0.1185 on the test
set. Compared to its components PROMODES and
PROMODES-H the f-measure increased by 0.0228
and 0.0353 on the test set.
In short, we have shown that by combining
PROMODES and PROMODES-H and finding the
optimal threshold, the ensemble PROMODES-E
gives better results than the individual models
themselves and therefore manages to leverage the
individual strengths of both to a certain extend.
However, can we pinpoint the exact contribution
of each individual algorithm to the improved re-
sult? We try to find an answer to this question in
the analysis of the subsequent section.
3.5 Analysis of calibrated algorithms and
their model ensemble
For the entire dataset of 2500 words, we have
examined boundary predictions dependent on the
relative word position. In Figure 3 and 4 we have
plotted the absolute counts of correct boundaries
(TP) and non-boundaries (TN) which PROMODES
predicted but not PROMODES-H, and vice versa,
as continuous lines. We furthermore provided the
number of individual predictions which were ulti-
mately adopted by PROMODES-E in the ensemble
as dashed lines.
In Figure 3a we can see for the default thresh-
old that PROMODES performs better in predicting
non-boundaries in the middle and the end of the
word in comparison to PROMODES-H. Figure 3b
380
shows the statistics for correctly predicted bound-
aries. Here, PROMODES-H outperforms PRO-
MODES in predicting correct boundaries across the
entire word length. After the calibration, shown
in Figure 4a, PROMODES-H improves the correct
prediction of non-boundaries at the beginning of
the word whereas PROMODES performs better at
the end. For the boundary prediction in Figure 4b
the signal disappears after calibration.
Concluding, it appears that our test language
Zulu has certain features which are modelled best
with either a lower or higher-order model. There-
fore, the ensemble leveraged strengths of both al-
gorithms which led to a better overall performance
with a calibrated threshold.
4 Related work
We have presented two probabilistic genera-
tive models for word decomposition, PROMODES
and PROMODES-H. Another generative model
for morphological analysis has been described
by Snover and Brent (2001) and Snover et al
(2002), however, they were interested in finding
paradigms as sets of mutual exclusive operations
on a word form whereas we are describing a gener-
ative process using morpheme boundaries and re-
sulting letter transitions.
Moreover, our probabilistic models seem to re-
semble Hidden Markov Models (HMMs) by hav-
ing certain states and transitions. The main differ-
ence is that we have dependencies between states
as well as between emissions whereas in HMMs
emissions only depend on the underlying state.
Combining different morphological analysers
has been performed, for example, by Atwell and
Roberts (2006) and Spiegler et al (2009). Their
approaches, though, used majority vote to decide
whether a morpheme boundary is inserted in a cer-
tain word position or not. The algorithms them-
selves were treated as black-boxes.
Monson et al (2009) described an indirect
approach to probabilistically combine ParaMor
(Monson, 2008) and Morfessor (Creutz, 2006).
They used a natural language tagger which was
trained on the output of ParaMor and Morfes-
sor. The goal was to mimic each algorithm since
ParaMor is rule-based and there is no access to
Morfessor?s internally used probabilities. The tag-
ger would then return a probability for starting a
new morpheme in a certain position based on the
original algorithm. These probabilities in com-
bination with a threshold, learnt on a different
dataset, were used to merge word analyses. In
contrast, our ensemble algorithm PROMODES-E
directly accesses the probabilistic framework of
each algorithm and combines them based on an
optimal threshold learnt on a validation set.
5 Conclusions
We have presented a method to learn a cali-
brated decision threshold from a validation set and
demonstrated that ensemble methods in connec-
tion with calibrated decision thresholds can give
better results than the individual models them-
selves. We introduced two algorithms for word de-
composition which are based on generative prob-
abilistic models. The models consider segment
boundaries as hidden variables and include prob-
abilities for letter transitions within segments.
PROMODES contains a lower order model whereas
PROMODES-H is a novel development of PRO-
MODES with a higher order model. For both
algorithms, we defined the mathematical model
and performed experiments on language data of
the morphologically complex language Zulu. We
compared the performance on increasing train-
ing set sizes and analysed for each word position
whether their boundary prediction agreed or dis-
agreed. We found out that PROMODES was bet-
ter in predicting non-boundaries and PROMODES-
H gave better results for morpheme boundaries at
a default decision threshold. At an optimal de-
cision threshold, however, both yielded a simi-
lar f-measure result. We then performed a fur-
ther analysis based on relative word positions and
found out that the calibrated PROMODES-H pre-
dicted non-boundaries better for initial word posi-
tions whereas the calibrated PROMODES for mid-
and final word positions. For boundaries, the cali-
brated algorithms had a similar behaviour. Subse-
quently, we showed that a model ensemble of both
algorithms in conjunction with finding an optimal
threshold exceeded the performance of the single
algorithms at their individually optimal threshold.
Acknowledgements
We would like to thank Narayanan Edakunni and
Bruno Gole?nia for discussions concerning this pa-
per as well as the anonymous reviewers for their
comments. The research described was sponsored
by EPSRC grant EP/E010857/1 Learning the mor-
phology of complex synthetic languages.
381
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
100
200
300
400
500
600
700
800
Relative word position
Abs
olut
e tru
e ne
gati
ves
 (TN
)
Performance on non?boundaries, default threshold
 
 Promodes (unique TN)Promodes?H (unique TN)Promodes and Promodes?E (unique TN)Promodes?H and Promodes?E (unique TN)
(a) True negatives, default
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
100
200
300
400
500
600
700
800
Relative word position
Abs
olut
e tru
e po
sitiv
es (
TP)
Performance on boundaries, default threshold
 
 Promodes (unique TP)Promodes?H (unique TP)Promodes and Promodes?E (unique TP)Promodes?H and Promodes?E (unique TP)
(b) True positives, default
Figure 3: Analysis of results using default threshold.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
100
200
300
400
500
600
700
800
Relative word position
Abs
olut
e tru
e ne
gati
ves
 (TN
)
Performance on non?boundaries, calibrated threshold
 
 Promodes (unique TN)Promodes?H (unique TN)Promodes and Promodes?E (unique TN)Promodes?H and Promodes?E (unique TN)
(a) True negatives, calibrated
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
100
200
300
400
500
600
700
800
Relative word position
Abs
olut
e tru
e po
sitiv
es (
TP)
Performance on boundaries, calibrated threshold
 
 Promodes (unique TP)Promodes?H (unique TP)Promodes and Promodes?E (unique TP)Promodes?H and Promodes?E (unique TP)
(b) True positives, calibrated
Figure 4: Analysis of results using calibrated threshold.
382
References
J. W. Amtrup. 2003. Morphology in machine trans-
lation systems: Efficient integration of finite state
transducers and feature structure descriptions. Ma-
chine Translation, 18(3):217?238.
E. Atwell and A. Roberts. 2006. Combinatory hy-
brid elementary analysis of text (CHEAT). Proceed-
ings of the PASCAL Challenges Workshop on Un-
supervised Segmentation of Words into Morphemes,
Venice, Italy.
M. Creutz. 2006. Induction of the Morphology of Nat-
ural Language: Unsupervised Morpheme Segmen-
tation with Application to Automatic Speech Recog-
nition. Ph.D. thesis, Helsinki University of Technol-
ogy, Espoo, Finland.
J. Davis and M. Goadrich. 2006. The relationship
between precision-recall and ROC curves. Interna-
tional Conference on Machine Learning, Pittsburgh,
PA, 233?240.
M. Gasser. 1994. Modularity in a connectionist
model of morphology acquisition. Proceedings of
the 15th conference on Computational linguistics,
1:214?220.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27:153?198.
J. Goldsmith. 2009. The Handbook of Computational
Linguistics, chapter Segmentation and morphology.
Blackwell.
M. A. Hafer and S. F. Weiss. 1974. Word segmenta-
tion by letter successor varieties. Information Stor-
age and Retrieval, 10:371?385.
Z. S. Harris. 1955. From phoneme to morpheme. Lan-
guage, 31(2):190?222.
T. Hirsimaki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-
pioja, and J. Pylkkonen. 2006. Unlimited vocabu-
lary speech recognition with morph language mod-
els applied to Finnish. Computer Speech And Lan-
guage, 20(4):515?541.
K. Kettunen. 2009. Reductive and generative ap-
proaches to management of morphological variation
of keywords in monolingual information retrieval:
An overview. Journal of Documentation, 65:267 ?
290.
M. Kurimo, S. Virpioja, and V. T. Turunen. 2009.
Overview and results of Morpho Challenge 2009.
Working notes for the CLEF 2009 Workshop, Corfu,
Greece.
C. Monson, K. Hollingshead, and B. Roark. 2009.
Probabilistic ParaMor. Working notes for the CLEF
2009 Workshop, Corfu, Greece.
C. Monson. 2008. ParaMor: From Paradigm
Structure To Natural Language Morphology Induc-
tion. Ph.D. thesis, Language Technologies Institute,
School of Computer Science, Carnegie Mellon Uni-
versity, Pittsburgh, PA, USA.
R. J. Mooney and M. E. Califf. 1996. Learning the
past tense of English verbs using inductive logic pro-
gramming. Symbolic, Connectionist, and Statistical
Approaches to Learning for Natural Language Pro-
cessing, 370?384.
S. Muggleton and M. Bain. 1999. Analogical predic-
tion. Inductive Logic Programming: 9th Interna-
tional Workshop, ILP-99, Bled, Slovenia, 234.
K. Oflazer, S. Nirenburg, and M. McShane. 2001.
Bootstrapping morphological analyzers by combin-
ing human elicitation and machine learning. Com-
putational. Linguistics, 27(1):59?85.
D. Opitz and R. Maclin. 1999. Popular ensemble
methods: An empirical study. Journal of Artificial
Intelligence Research, 11:169?198.
D. E. Rumelhart and J. L. McClelland. 1986. On
learning the past tenses of English verbs. MIT
Press, Cambridge, MA, USA.
K. Shalonova, B. Gole?nia, and P. A. Flach. 2009. To-
wards learning morphology for under-resourced fu-
sional and agglutinating languages. IEEE Transac-
tions on Audio, Speech, and Language Processing,
17(5):956965.
M. G. Snover and M. R. Brent. 2001. A Bayesian
model for morpheme and paradigm identification.
Proceedings of the 39th Annual Meeting on Asso-
ciation for Computational Linguistics, 490 ? 498.
M. G. Snover, G. E. Jarosz, and M. R. Brent. 2002.
Unsupervised learning of morphology using a novel
directed search algorithm: Taking the first step. Pro-
ceedings of the ACL-02 workshop on Morphological
and phonological learning, 6:11?20.
S. Spiegler, B. Gole?nia, K. Shalonova, P. A. Flach, and
R. Tucker. 2008. Learning the morphology of Zulu
with different degrees of supervision. IEEE Work-
shop on Spoken Language Technology.
S. Spiegler, B. Gole?nia, and P. A. Flach. 2009. Pro-
modes: A probabilistic generative model for word
decomposition. Working Notes for the CLEF 2009
Workshop, Corfu, Greece.
S. Spiegler, B. Gole?nia, and P. A. Flach. 2010a. Un-
supervised word decomposition with the Promodes
algorithm. In Multilingual Information Access Eval-
uation Vol. I, CLEF 2009, Corfu, Greece, Lecture
Notes in Computer Science, Springer.
S. Spiegler, A. v. d. Spuy, and P. A. Flach. 2010b. Uk-
wabelana - An open-source morphological Zulu cor-
pus. in review.
R. Sproat. 1996. Multilingual text analysis for text-to-
speech synthesis. Nat. Lang. Eng., 2(4):369?380.
383
