Automatic Extraction of Subcategorization Frames for Czech* 
Anoop Sarkar 
CIS Dept, Univ of Pennsylvania 
200 South 33rd Street, 
Philadelphia, PA 19104 USA 
anoop@linc, cis. upenn, edu 
Daniel Zeman 
0stay formflnf a aplikovand lingvistiky 
Univerzita Karlova 
Praha, Czechia 
zeman@ufa l  .mf f .  cun i .  cz  
Abstract 
We present some novel nmchine learning techniques 
for the identilication of subcategorization infornm- 
tion for verbs in Czech. We compare three different 
statistical techniques applied to this problem. We 
show how the learning algorithm can be used to dis- 
cover previously unknown subcategorization frames 
from the Czech Prague 1)ependency Treebank. The 
algorithm can then be used to label dependents of 
a verb in the Czech treebank as either arguments 
or adjuncts. Using our techniques, we are able to 
achieve 88% precision on unseen parsed text. 
1 Introduction 
Tl-te subcategorization f verbs is an essential is- 
sue in parsing, because it helps disambiguate the 
attachment of arguments and recover the correct 
predicate-argument relations by a parser. (CmToll 
and Minnen, 1998; CmToll and Rooth, 1998) give 
several reasons why subcategorization information 
is important for a natural anguage parser. Machine- 
readable dictionaries are not comprehensive enough 
to provide this lexical infornaation (Manning, 1993; 
Briscoe and Carroll, 1997). Furthermore, such dic- 
tionaries are available only for very few languages. 
We need some general method for the automatic ex- 
traction of subcategorization information from text 
corpora.  
Several techniques and results have been reported 
on learning subcategorization frames (SFs) from 
text corpora (Webster and Marcus, 1989; Brent, 
1991; Brent, 1993; Brent, 1994; Ushioda et al, 
1993; Manning, 1993; Ersan and Charniak, 1996; 
Briscoe and Carroll, 1997; Carroll and Minnen, 
1998; Carroll and Rooth, 1998). All of this work 
" Tiffs work was done during the second author's visit to tl~e 
University of Pennsylvania. We would like to thank Prof. Ar- 
avind Joshi, l)avid Chiang, Mark l)ras and the anonymous re- 
viewers for their comments. The first at,thor's work is partially 
supported by NS F Grant S BR 8920230. Many tools used in this 
work are the resuhs of project No. VS96151 of the Ministry of 
Education of the Czech Republic. The data (PDT) is thanks 
to grant No. 405/96/K214 of the Grant Agency of the Czech 
Republic. Both grants were given to the Institute of Fornml 
and Applied linguistics, Faculty of Mathenmtics and Physics, 
Charles University, Prague. 
deals with English. In this paper we report on 
techniques that automatically extract SFs for Czech, 
which is a flee word-order language, where verb 
complements have visible case marking.I 
Apart from the choice of target language, this 
work also differs from previous work in other ways. 
Unlike all other previous work in this area, we do 
not assume that the set of SFs is known to us in ad- 
vance. Also in contrast, we work with syntactically 
annotated ata (the Prague Dependency Treebank, 
PDT (HajiC 1998)) where the subcategorization in-
formation is not  given; although this might be con? 
sidered a simpler problem as compared to using raw 
text, we have discovered interesting problems that a 
user of a raw or tagged corpus is unlikely to face. 
We first give a detailed description of the task 
of uncovering SFs and also point out those prop- 
erties of Czech that have to be taken into account 
when searching lbr SFs. Then we discuss some dif-. 
ferences fl'Oln the other research efforts. We then 
present he three techniques that we use to learn SFs 
from the input data. 
In the input data, many observed ependents of 
the verb are adjuncts. To treat this problem effec- 
tively, we describe a novel addition to the hypoth- 
esis testing technique that uses subset of observed 
fl'ames to permit he learning algorithm to better dis- 
tinguish arguments fl-om adjtmcts. 
Using our techniques, we arc able to achieve 88% 
precision in distinguishing argunaents from adjuncts  
on unseen parsed text. 
2 lhsk Description 
In this section we describe precisely the proposed 
task. We also describe the input training material 
and the output produced by our algorithms. 
2.1 Identifying subeategorization frames 
Ill general, the problem of identifying subcatego- 
rization fi-ames is to distinguish between arguments 
and adjuncts among the constituents modifying a 
IOI/c of the ammymous rcviewcrs pointed out that (Basili 
and Vindigni. 1908) presents a corpus-driven acquisition of 
subcategorization frames for Italian. 
691 
f N4 R2(od) {2} 
N4 R2(od) R2(do) _ R2(od) R2(do) {0~j~/  
{~}~. .~ N4 R2(do){0} q~"--_//...~ 
N4 R6(v) R6(na) { 11 ~ N4 R6(v) l I} ~/ / f f _  
N4 R6(po) {1 } /-----------~" 
R2(od) {0} 
R2(do) {0} 
R6(v) {0} 
R6(na) {0} 
R6(po) {0 } 
N4 {2+1+1} 
empty 10} 
Figure 2: Computing the subsets of observed frames for tile verb absoh,ovat. The counts for each frame are 
given within braces {}. In this example, the frames N4 R2(od), N4 R6(v) and N4 R6(po) have been observed 
with other verbs in the corpus. Note that the counts in this figure do not correspond to the real counts for the 
verb absoh,ovat in the training corpus. 
where c(.) are counts in the training data. Using 
the values computed above: 
Pl -- 
7tl 
k2 
P2 --= - -  
77, 2 
k l  +k2  p -- 
7z 1 .-\]- 'It 2 
Taking these probabilities to be binomially dis- 
tributed, the log likelihood statistic (Dunning, 1993) 
is given by: 
- 2 log A = 
2\[log L(pt, k:l, rtl) @ log L(p2, k2, rl,2) -- 
log L(p, kl, n2) - log L(p, k2, n2)\] 
where, 
log L(p, n, k) = k logp + (,z -- k)log(1 - p) 
According to this statistic, tile greater the value of 
-2  log A for a particular pair of observed frame and 
verb, the more likely that frame is to be valid SF of 
the verb. 
3.2 T-scores 
Another statistic that has been used for hypothesis 
testing is the t-score. Using tile definitions from 
Section 3.1 we can compute t-scores using the equa- 
tion below and use its value to measure the associa- 
tion between a verb and a frame observed with it. 
T = Pl - P2 
where, 
p)  = , p(1 - 
In particular, the hypothesis being tested using 
the t-score is whether the distributions Pi and P2 
are not independent. If the value of T is greater 
than some threshold then the verb v should take the 
frame f as a SF. 
3.3 B inomia l  Mode ls  o f  M iscue  Probabi l i t ies  
Once again assuming that the data is binomially dis- 
tributed, we can look for fiames that co-occur with a 
verb by exploiting the miscue probability: the prob- 
ability of a frame co-occuring with a verb when it 
is not a valid SF. This is the method used by several 
earlier papers on SF extraction starting with (Brent, 
1991; Brent, 1993; Brent, 1994). 
Let us consider probability PU which is the prob- 
ability that a given verb is observed with a fiame but 
this frame is not a valid SF for this verb. p!f is the 
error probability oil identifying a SF for a verb. Let 
us consider a verb v which does not have as one of 
its valid SFs the frame f .  How likely is it that v will 
be seen 'm, or more times in the training data with 
fi'ame f?  If v has been seen a total of n times ill the 
data, then H*(p!f; m, 7z) gives us this likelihood. 
/ x 
H'(p,f~,n,)'L) = ~__,pif(1 t" )n - i (  ~" ) 
- " f  i 
i=rn. X / 
If H*(p; rn, n) is less than or equal to some small 
threshold value then it is extremely unlikely that the 
hypothesis is tree, and hence the frame f must be 
a SF of tile verb v. Setting the threshold value to 
0.0,5 gives us a 95% or better contidence value that 
the verb v has been observed often enough with a 
flame f for it to be a valid SE 
Initially, we consider only the observed fnnnes 
(OFs) from the treebank. There is a chance that 
some are subsets of some others but now we count 
only tile cases when the OFs were seen themselves. 
Let's assume the test statistic reiected the flame. 
Then it is not a real SF but there probably is a sub- 
set of it that is a real SE So we select exactly one of 
694 
tile subsets whose length is one member less: this 
is the successor of the rejected flame and inherits 
its frequency. Of course one frame may be suc- 
cessor of several onger frames and it can have its 
own count as OF. This is how frequencies accumu- 
late and frames become more likely to survive. The 
exalnple shown in Figure 2 illustrates how the sub- 
sets and successors are selected. 
An important point is the selection of the succes- 
sor. We have to select only one of the ~t possible 
successors of a flame of length 7z, otherwise we 
would break tile total frequency of the verb. Sup- 
pose there is m rejected flames of length 7z. "Ellis 
yields m * n possible modifications to consider be- 
fore selection of the successor. We implemented 
two methods for choosing a single successor flame: 
1. Choose the one that results in the strongest 
preference for some frame (that is, the succes- 
sor flmne results in the lowest entropy across 
the corpus). This measure is sensitive to the 
frequency of this flame in the rest of corpus. 
2? Random selection of the successor frame from 
the alternatives. 
Random selection resulted in better precision 
(88% instead of 86%). It is not clear wily a method 
that is sensitive to the frequency of each proposed 
successor frame does not perform better than ran- 
dom selection. 
The technique described here may sometimes re- 
sult in subset of a correct SF, discarding one or more 
of its members. Such frame can still hel ) parsers be- 
cause they can at least look for the dependents that 
have survived. 
4 Evaluation 
For the evalnation of the methods described above 
we used the Prague l)ependency Treebank (PI)T). 
We used 19,126 sentences of training data from tile 
PDT (about 300K words). In this training set, there 
were 33,641 verb tokens with 2,993 verb types. 
There were a total of 28,765 observed fiames (see 
Section 2.1 for exphmation of these terms). There 
were 914 verb types seen 5 or more times. 
Since there is no electronic valence dictionary for 
Czech, we evaluated our tiltering technique on a set 
of 500 test sentences which were unseen and sep- 
arate flom the training data. These test sentences 
were used as a gold standard by distinguishing the 
arguments and adjuncts manually. We then com- 
pared the accuracy of our output set of items marked 
as either arguments or adjuncts against this gold 
standard. 
First we describe the baseline methods. Base- 
line method 1: consider each dependent of a verb 
an adjunct. Baseline method 2: use just the longest 
known observed frame matching the test pattern. If 
no matching OF is known, lind the longest partial 
match in the OFs seen in the training data. We ex- 
ploit the functional and morphological tags while 
matching. No statistical filtering is applied in either 
baseline method. 
A comparison between all three methods that 
were proposed in this paper is shown in Table 1. 
The experiments howed that the method im- 
proved precision of this distinction flom 57% to 
88%. We were able to classify as many as 914 verbs 
which is a number outperlormed only by Manning, 
with 10x more data (note that our results arc for a 
different language). 
Also, our method discovered 137 subcategoriza- 
tion frames from the data. The known upper bound 
of frames that the algorithm could have found (the 
total number of the obsem, edframe types) was 450. 
5 Comparison with related work 
Preliminary work on SF extraction from coq~ora 
was done by (Brent, 1991; Brunt, 1993; Brent, 
1994) and (Webster and Marcus, 1989; Ushioda et 
al., 1993). Brent (Brent, 1993; Brent, 1994) uses the 
standard method of testing miscue probabilities for 
filtering frames observed with a verb. (Brent, 1994) 
presents a method lbr estimating 1)7. Brent applied 
his method to a small number of verbs and asso- 
ciated SF types. (Manning, 1993) applies Brent's 
method to parsed data and obtains a subcategoriza- 
tion dictionary for a larger set of verbs. (Briscoe 
and Carroll, 1997; Carroll and Minnen, 1998) dif- 
fers from earlier work in that a substantially larger 
set of SF types are considered; (Canoll and Rooth, 
1998) use an EM algorithm to learn subcategoriza- 
tion as a result of learning rule probabilities, and, in 
tnrn, to improve parsing accuracy by applying the 
verb SFs obtained. (Basili and Vindigni, 1998) use 
a conceptual clustering algorithm for acquiring sub- 
categorization fl'ames for Italian. They establish a 
partial order on partially overlapping OFs (similar 
to our Ot: subsets) which is then used to suggest a 
potential SF. A complete comparison of all the pre- 
vious approaches with tile current work is given in 
Table 2. 
While these approaches differ in size and quality 
of training data, number of SF types (e.g. intran- 
sitive verbs, transitive verbs) and number of verbs 
processed, there are properties that all have in con> 
mon. They all assume that they know tile set of pos- 
sible SF types in advance. Their task can be viewed 
as assigning one or more of the (known) SF types 
to a given verb. In addition, except for (Briscoe and 
Carroll, 1997; Carroll and Minnen, 1998), only a 
small number of SF types is considered. 
695 
Baseline Lik. Ratio q-scores Hyp. Testing 
Precision 55% 82% 82% 88% 
Recall: 55% 77% 77% 74% 
_h'f~: l 55% 79% 79% 80% 
% unknown 0% 6% 6% 16% 
Total verb nodes 
Total complements 
Nodes with known verbs 
Complements of known verbs 
Correct Suggestions 
True Arguments 
Suggested Arguments 
Incorrect arg suggestions 
Incorrect adj suggestions 
1027 
2144 
1027 
2144 
1187.5 
956.5 
0 
0 
956.5 
1 Baseline 2 
78% 
73% 
75% 
6% 
1027 
2144 
981 
2010 
1573.5 
910.5 
1122 
324 
112.5 
1027 
2144 
981 
2010 
1642.5 
910.5 
974 
215.5 
152 
1027 
2144 
981 
2010 
1652.9 
910.5 
1026 
236.3 
120.8 
1027 
2144 
907 
1812 
1596.5 
834.5 
674 
27.5 
188 
Table 1: Comparison between the baseline methods and the three methods proposed in this paper. Some of 
the values are not integers ince for some difficult cases in the test data, the value for each argument/adjunct 
decision was set to a value between \[0, 1\]. Recall is computed as the number of known verb complements 
divided by the total number of complements. Precision is computed as the number of correct suggestions 
divided by the number of known verb complements. Ffl=l = (2 x p x r)/(p + r). % unknown represents 
the percent of test data not considered by a particular method. 
Using a dependency treebank as input to our 
learning algorithm has both advantages and draw- 
backs. There are two main advantages of using a 
treebank: 
? Access to more accurate data. Data is less 
noisy when compared with tagged or parsed in- 
put data. We can expect correct identification 
of verbs and their dependents. 
? We can explore techniques (as we have done in 
this paper) that try and learn the set of SFs from 
the data itself, unlike other approaches where 
the set of SFs have to be set in advance. 
Also, by using a treebank we can use verbs in dif- 
ferent contexts which are problematic for previous 
approaches, e.g. we can use verbs that appear in 
relative clauses. However, there are two main draw- 
backs: 
Treebanks are expensive to build and so the 
techniques presented here have to work with 
less data. 
All the dependents of each verb are visible to 
the learning algorithm. This is contrasted with 
previous techniques that rely on linite-state x= 
traction rules which ignore many dependents 
of the verb. Thus our technique has to deal 
with a different kind of data as compared to 
previous approaches. 
We tackle the second problem by using the 
method of observed frame subsets described in Sec- 
tion 3.3. 
6 Conclus ion 
We arc currently incorporating the SF information 
produced by the methods described in this paper 
into a parser for Czech. We hope to duplicate the 
increase in performance shown by treebank-based 
parsers for English when they use SF information. 
Our methods can also be applied to improve the 
annotations in the original treebank that we use as 
training data. The automatic addition of subcate- 
gorization to the treebank can be exploited to add 
predicate-argument i formation to the treebank. 
Also, techniques for extracting SF information 
fiom data can be used along with other research 
which aims to discover elationships between dif- 
ferent SFs of a verb (Stevenson and Merlo, t999; 
Lapata and Brew, 1999; Lapata, 1999; Stevenson et 
al., 1999). 
The statistical models in this paper were based on 
the assumption that given a verb, different SFs oc- 
cur independently. This assumption is used to jus- 
tify the use of the binomial. Future work perhaps 
should look towards removing this assumption by 
modeling the dependence between different SFs for 
the same verb using a multinomial distribution. 
To summarize: we have presented techniques that 
can be used to learn subcategorization information 
for verbs. We exploit a dependency treebank to 
learn this information, and moreover we discover 
the final set of valid subcategorization frames from 
the training data. We achieve upto 88% precision on 
unseen data. 
We have also tried our methods on data which 
was automatically morphologically tagged which 
696 
Previous 
work 
(Ushioda et al, 1993) 
(Brent, 1993) 
(Mmming, 1993) 
(Brent, 1994) 
(Ersan and Charniak, 1996) 
(Briscoe and Carroll, 1997) 
(CatToll and Rooth, 1998) 
Data 
POS + 
FS ntles 
raw + 
FS rules 
POS + 
FS rules 
raw + 
heurist ics 
Full  
parsing 
Full  
parsing 
Unlabeled 
#SFs  
Current Work Ful ly  Learned 
Parsed 137 
6 33 
6 193 
19 3104 
12 126 
16 30 
160 14 
9+ 3 
914 
Method 
heuristics 
Hypothesis 
testing 
Miscue 
rate 
NA 
iterative 
estimation 
Corpus 
WNJ (300K) 
Brown ( 1. IM)  
Hypothesis  hand NYT  (4.1 M) 
testing 
Hypothesis  non-iter CHIL I )ES  (32K) 
testing est imation 
Hypothesis  hand WSJ  (36M) 
testing 
Hypothesis  Dict ionary various (7OK) 
testing est imation 
Inside- NA BNC (5-30M) 
outside 
Subsets+ Est imate PDT (300K) 
Hyp. testing 
Table 2: Comparison with previous work on automatic SF extraction from corpora 
al lowed us to use more data (82K sentences instead 
of  19K). The performance went up to 89% (a 1% 
improvement) .  
Re ferences  
Roberto Basili and Michele Vmdigni. 1998. Adapting a sub- 
categorization lexicon to a domain. In I'roceedings of 
the ECML'98 Workshop TANLPS: Towards adaptive NLP- 
d,iven systems: lingui'stic information, learning methods 
and applications, Chemnitz, Germany, Apr 24. 
Peter Bickel and Kjell l)oksum. 1977. Mathematical Statis- 
tics. Holden-Day Inc. 
Michael Brent. 1991. Automatic acquisition of subcategoriza- 
tion flames from untagged text. In Proceedings of the 29th 
Meeting of the AUL. pages 209-214, Berkeley, CA. 
Michael Brent. 1993. From grammar to lexicon: unsuper- 
vised learning of lexical syntax. ('Omlmtational Linguistics, 
19(3):243-262. 
Michael Brent. 1994. Acquisition of subcategorization frames 
using aggregated evidence fiom local syntactic ues. Lin- 
gmt, 92:433-470. Reprinted in Acqttisition of the Lexicon, 
L. Gleinnan and B. Landau (Eds.). MIT Press, Cambridge, 
MA. 
Ted Briscoe and John Carroll. 1997. Automatic extraction of 
subcategorization from corpora. In Proceedings of the 5th 
ANI, P Conference, pages 356-363. Washington. D.C. ACI,. 
John Carroll and Guido Minnen. 1998. Can subcategorisa- 
tion probabilities help a statistical parser. In Proceedings 
of the 6th AClJSIGDAT Workshop on Very lztrge ('orpora 
(WVLC-6), Montreal, Canada. 
Glenn Carroll and Mats Rooth. 1998. Valence induction with 
a head-lcxicalized PCFG. In Proceedings of the 3rd Confer- 
ence on Empirical Methods in Natural Language Processing 
(EMNLI' 3), Granada, Spain. 
Ted Dunning. 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational Ling,istics. 
19( 1):61-74, March. 
Murat Ersan and Eugene Chamiak. 1996. A statistical syn- 
tactic disambiguation program and what it learns. In 
S. Wcrmter, E. Riloff, and G. Scheler. editors, Comwc- 
tionist, Statistical and Symbolic Approaches in Learning 
.fi~r Natural xmguage I'rocessing, volume 1040 of Lecture 
Notes in ArtiJical Intelligence, pages 146-159. Springer- 
Verlag, Berlin. 
Jan ttaji,.? and Barbora ttladkfi. 1998. "Fagging inllective lan- 
guages: Prediction of morphological categories for a rich, 
structured tagset. In Proceedings of COLING-ACI, 98, Uni- 
versitd e Montrdal, Montreal, pages 483-490. 
Jan Itaji,L 1998. Building a syntactically annotated corpus: 
The prague dependency treebank. In Issues off Valency and 
Meaning, pages 106-132. Karolinum, Praha. 
Maria Lapata and Chris Brew. 1999. Using subcategorization 
to resolve verb class ambiguity. In Pascale Furtg and Joe 
Zhou, editors, Proceedings o1' WVL(TEMNI,I ~, pages 266-- 
274, 21-22 June. 
Maria Lapata. 1999. Acquiring lexical generalizations from 
corpora: A case study for diathesis alternations. In Proceed- 
ings q/37th Meeting olA( :L, pages 397-404. 
Christopher I). Manning. 1993. Automatic acquisition of a 
large subcategorization dictionary from corpora. In Pro- 
ceedil~gs of the 31st Meeting q/' the ACI,, pagcs 235-242, 
Columbus, Ohio. 
Suzanne Stevenson and Paola Merlo. 1999. Automatic verb 
classilication using distributions of grammatical features. In 
Proceedings of I'JACL '99, pages 45-52, Bergen, Norway, 
8-12 J une. 
Suzanne Stevenson, Paoht Merlo, Natalia Kariaeva, and Kamin 
Whitehouse. 1999. Supervised learning of lexical semantic 
classes using frequency distributions. In SIGLEX-99. 
Akira Ushioda, David A. Evans, Ted Gibson, and Alex Waibel. 
1993. The autonaatic acquisition of frequencies ofverb st, b- 
categorization frames from tagged corpora. In B. Boguraev 
and J. Pustejovsky, editors, Proceedings of the Workshop on 
Acquisition of Lexical Knowledge fi'om 7kvt, pages 95-106, 
Columbus, Otl, 21 June. 
Mort Webster and Mitchell Marcus. 1989. Automatic acquisi- 
tion of the lexical frames of verbs from sentence frames. In 
Proceedings of the 27th Meeting of the ACL, pages 177-184. 
697 
Learning Verb Argument Structure
from Minimally Annotated Corpora?
Anoop Sarkar and Woottiporn Tripasai
Dept. of Computer and Information Science
University of Pennsylvania
200 South 33rd Street,
Philadelphia, PA 19104-6389 USA
{anoop,tripasai}@linc.cis.upenn.edu
Abstract
In this paper we investigate the task of automatically
identifying the correct argument structure for a set
of verbs. The argument structure of a verb allows
us to predict the relationship between the syntac-
tic arguments of a verb and their role in the under-
lying lexical semantics of the verb. Following the
method described in (Merlo and Stevenson, 2001),
we exploit the distributions of some selected fea-
tures from the local context of a verb. These fea-
tures were extracted from a 23M word WSJ cor-
pus based on part-of-speech tags and phrasal chunks
alone. We constructed several decision tree classi-
fiers trained on this data. The best performing clas-
sifier achieved an error rate of 33.4%. This work
shows that a subcategorization frame (SF) learning
algorithm previously applied to Czech (Sarkar and
Zeman, 2000) is used to extract SFs in English. The
extracted SFs are evaluated by classifying verbs into
verb alternation classes.
1 Introduction
The classification of verbs based on their underlying
thematic structure involves distinguishing verbs that
take the same number and category of arguments
but assign different thematic roles to these argu-
ments. This is often termed as the classification of
verb diathesis roles or the lexical semantics of pred-
icates in natural language (see (Levin, 1993; Mc-
Carthy and Korhonen, 1998; Stevenson and Merlo,
1999; Stevenson et al, 1999; Lapata, 1999; Lapata
and Brew, 1999; Schulte im Walde, 2000)). Fol-
lowing the method described in (Merlo and Steven-
son, 2001; Stevenson and Merlo, 1999; Stevenson et
? This research was supported in part by NSF grant SBR-89-
20230. Thanks to Paola Merlo, Dan Gildea, David Chiang, Ar-
avind Joshi and the anonymous reviewers for their comments.
Also thanks to Virginie Nanta for an earlier collaboration with
the first author on an unsupervised version of this work.
al., 1999), we exploit the distributions of some se-
lected features from the local context of a verb but
we differ from these previous studies in the use of
minimally annotated data to construct our classifier.
The data we use is only passed through a part-of-
speech tagger and a chunker which is used to iden-
tify base phrasal categories such as noun-phrase and
verb-phrase chunks to identify potential arguments
of each verb.
Lexical knowledge acquisition plays an impor-
tant role in corpus-based NLP. Knowledge of verb
selectional preferences and verb subcategorization
frames (SFs) can be extracted from corpora for use
in various NLP tasks. However, knowledge of SFs
is often not fine-grained enough to distinguish vari-
ous verbs and the kinds of arguments that they can
select. We consider a difficult task in lexical knowl-
edge acquisition: that of finding the underlying ar-
gument structure which can be used to relate the ob-
served list of SFs of a particular verb. The task in-
volves identifying the roles assigned by the verb to
its arguments. Consider the following verbs, each
occuring with intransitive and transitive SFs1.
Unergative
(1) a. The horse raced past the barn.
b. The jockey raced the horse past the
barn.
Unaccusative
(2) a. The butter melted in the pan.
b. The cook melted the butter in the pan.
1The examples are taken from (Merlo and Stevenson,
2001). See (Levin, 1993) for more information. The partic-
ular categorization that we use here is motivated in (Stevenson
and Merlo, 1997)
Object-Drop
(3) a. The boy washed.
b. The boy washed the hall.
Each of the verbs above occurs with both the in-
transitive and transitive SFs. However, the verbs
differ in their underlying argument structure. Each
verb assigns a different role to their arguments in the
two subcategorization possibilities. For each verb
above, the following lists the roles assigned to each
of the noun phrase arguments in the SFs permitted
for the verb. This information can be used for ex-
tracting appropriate information about the relation-
ships between the verb and its arguments.
Unergative
INTRAN: NPagent raced
TRAN: NPcauser raced NPagent
Unaccusative
INTRAN: NPtheme melted
TRAN: NPcauser melted NPtheme
Object-Drop
INTRAN: NPagent washed
TRAN: NPagent washed NPtheme
Our task is to identify the transitive and intransi-
tive usage of a particular verb as being related via
this notion of argument structure. This is called the
argument structure classification of the verb. In the
remainder of this paper we will look at the problem
of placing verbs into such classes automatically.
Our results in this paper serve as a replication
and extension of the results in (Merlo and Steven-
son, 2001). Our main contribution in this paper is
to show that a subcategorization frame (SF) learn-
ing algorithm previously applied to Czech (Sarkar
and Zeman, 2000) can be applied to English and
evaluated by classifying verbs into verb alternation
classes. We perform this task using only tagged
and chunked data as input to our subcategorization
frame learning stage. Our result can be compared to
previous work (Merlo and Stevenson, 2001) which
did not use SF learning but used a 65M word WSJ
corpus which was tagged as well as automatically
parsed with a Treebank trained statistical parser.
It is important to note that (Merlo and Stevenson,
2001) extract some features using the tagged infor-
mation (in fact, those features that we use SF learn-
ing to extract) and other features using parse trees.
2 The Hypothesis
We create a probabilistic classifier that can automat-
ically classify a set of verbs into argument structure
classes with a reasonable error rate. We use the hy-
pothesis introduced by (Stevenson and Merlo, 1999)
that although a verb in a particular class can occur
in all of the syntactic contexts as verbs from other
classes the statistical distributions can be distin-
guished. In other words, verbs from certain classes
will be more likely to occur in some syntactic con-
texts than others. We identify features that pick
out the verb occurences in these contexts. By us-
ing these features, we will attempt to determine the
classification of those verbs. In the previous sec-
tion we saw that we sometimes have noun-phrase
arguments (NPcauser) as being a causer of the action
denoted by the verb. For example, (Stevenson and
Merlo, 1999) show that a classifier can exploit these
causativity facts to improve classifiction.
We use some new features in addition to the ones
proposed and used in (Merlo and Stevenson, 2001)
for this task. In addition, we include as a feature the
probabilistic classification of the verb as a transitive
or intransitive verb. Thus the classifier is simula-
neously placing each verb into the appropriate sub-
categorization frame as well as identifying the un-
derlying thematic roles of the verb arguments.
In our experiment, we will consider the follow-
ing set of classes (each of these were explained in
the previous section): unergative, unaccusative, and
object-drop. We test 76 verbs taken from (Levin,
1993) that are in one of these three classes. The par-
ticular verbs were chosen to include high frequency
as well as low frequency verb tokens in our partic-
ular corpus of 23M words of WSJ text.2 We used
all instances of these verbs from the WSJ corpus.
The data was annotated with the right classification
for each verb and the classifier was trained on 90%
of the verbs taken from the 23M word corpus and
tested on 10% of the data using 10-fold cross vali-
dation. We describe the experiment in greater detail
2The particular verbs selected were looked up in (Levin,
1993) and the class for each verb in the classification system
defined in (Stevenson and Merlo, 1997) was selected with some
discussion with linguists.
in Section 4.
3 Identifying subcategorization frames
An important part of identifying the argument struc-
ture of the verb is to find the verb?s subcategoriza-
tion frame (SF). For this paper, we are interested in
whether the verb takes an intransitive SF or a tran-
sitive SF.
In general, the problem of identifying subcatego-
rization frames is to distinguish between arguments
and adjuncts among the constituents modifying a
verb. For example, in ?John saw Mary yesterday at
the station?, only ?John? and ?Mary? are required
arguments while the other constituents are optional
(adjuncts).3
The problem of SF identification using statisti-
cal methods has had a rich discussion in the lit-
erature (Ushioda et al, 1993; Manning, 1993;
Briscoe and Carroll, 1997; Brent, 1994) (also see
the refences cited in (Sarkar and Zeman, 2000)). In
this paper, we use the method of hypothesis testing
to discover the SF for a given verb (Brent, 1994).
Along with the techniques given in these papers,
(Sarkar and Zeman, 2000; Korhonen et al, 2000)
also discuss other methods for hypothesis testing
such the use of the t-score statistic and the likeli-
hood ratio test. After experimenting with all three
of these methods we selected the likelihood ratio
test because it performed with higher accuracy on
a small set of hand-annotated instances. We use the
determination of the verb?s SF as an input to our ar-
gument structure classifier (see Section 4).
The method works as follows: for each verb, we
need to associate a score to the hypothesis that a par-
ticular set of dependents of the verb are arguments
of that verb. In other words, we need to assign a
value to the hypothesis that the observed frame un-
der consideration is the verb?s SF. Intuitively, we ei-
ther want to test for independence of the observed
frame and verb distributions in the data, or we want
to test how likely is a frame to be observed with a
particular verb without being a valid SF. We develop
these intuitions by using the method of hypothe-
sis testing using the likelihood ratio test. For fur-
3There is some controversy as to the correct subcategoriza-
tion of a given verb and linguists often disagree as to what is the
right set of SFs for a given verb. A machine learning approach
such as the one followed in this paper sidesteps this issue al-
together, since it is left to the algorithm to learn what is an
appropriate SF for a verb. The stance taken in this paper is that
the efficacy of SF learning is evaluated on some domain, as is
done here on learning verb alternations.
ther background on this method of hypothesis test-
ing the reader is referred to (Bickel and Doksum,
1977; Dunning, 1993).
3.1 Likelihood ratio test
Let us take the hypothesis that the distribution of an
observed frame f in the training data is independent
of the distribution of a verb v. We can phrase this
hypothesis as p( f | v) = p( f | !v) = p( f ), that
is distribution of a frame f given that a verb v is
present is the same as the distribution of f given
that v is not present (written as !v). We use the log
likelihood test statistic (Bickel and Doksum, 1977,
209) as a measure to discover particular frames and
verbs that are highly associated in the training data.
k1 = c( f , v)
n1 = c(v) = c( f , v) + c(! f , v)
k2 = c( f , !v)
n2 = c(!v) = c( f , !v) + c(! f , !v)
where c(?) are counts in the training data. Using the
values computed above:
p1 =
k1
n1
p2 =
k2
n2
p = k1 + k2
n1 + n2
Taking these probabilities to be binomially dis-
tributed, the log likelihood statistic (Dunning, 1993)
is given by:
?2 log ? =
2[log L(p1, k1, n1) + log L(p2, k2, n2) ?
log L(p, k1, n2) ? log L(p, k2, n2)]
where,
log L(p, n, k) = k log p + (n ? k) log(1 ? p)
According to this statistic, the greater the value of
?2 log ? for a particular pair of observed frame and
verb, the more likely that frame is to be valid SF of
the verb. If this value is above a certain threshold it
is taken to be a positive value for the binary feature
TRAN, else it is a positive feature for the binary fea-
ture INTRAN in the construction of the classifier.4
4 Steps in Constructing the Classifier
To construct the classifier, we will identify features
that can be used to accurately distinguish verbs into
different classes. The features are computed to be
the probability of observing a particular feature with
each verb to be classified. We use C5.0 (Quinlan,
1992) to generate the decision tree classifier. The
features are extracted from a 23M word corpus of
WSJ text (LDC WSJ 1988 collection). Note that the
training and test data constructed from this set are
produced by the classification of individual verbs
into their respective classes taken from (Merlo and
Stevenson, 2001).
We prepare the corpus by passing it through
Adwait Ratnaparkhi?s part-of-speech tagger (Rat-
naparkhi, 1996) (trained on the Penn Treebank
WSJ corpus) and then running Steve Abney?s chun-
ker (Abney, 1997) over the entire text. The output
of this stage and the input to our feature extractor is
shown below.
Pierre NNP nx 2
Vinken NNP
, ,
61 CD ax 3
years NNS
old JJ
, ,
will MD vx 2
join VB
the DT nx 2
board NN
as IN
a DT nx 3
nonexecutive JJ
director NN
Nov. NNP
29 CD
. .
We use the following features to construct the
classifier. The first four features were discussed and
motivated in (Stevenson and Merlo, 1999; Merlo
and Stevenson, 2001). In some cases, we have
modified the features to include information about
part-of-speech tags. The discussion below clarifies
4See (Sarkar and Zeman, 2000) for information on how the
threshold is selected.
the similarities and changes. The features we used
in addition are the last two in the following list,
the part-of-speech features and the subcategoriza-
tion frame features. 5
1. simple past (VBD), and past participle(VBN)
2. active (ACT) and passive (PASS)
3. causative (CAUS)
4. animacy (ANIM)
5. Part of Speech of the subject noun-phrase and
object noun-phrase
6. transitive (TRAN) and intransitive (INTRAN)
To calculate all the probability values of each fea-
tures, we perform the following steps.
4.1 Finding the main verb of the sentences
To find the main verb, we constructed a determin-
istic finite-state automaton that finds the main verb
within the verb phrase chunks. This DFA is used
in two steps. First, to select a set of main verbs
from which we select the final set of 76 verbs used
in our experiment. Secondly, the actual set of verbs
is incorporated into the DFA in the feature selection
step.
4.2 Obtaining the frequency distribution of the
features
The general form of the equation we use to find the
frequency distribution of each feature of the verb is
the following:
P(V j) =
C(V j)
?
1?x?N C(Vx)
where P(V j) is the distribution of feature j of the
verb, N is the total number of features of the partic-
ular type (e.g., the total number of CAUS features
or ANIM features as described below) and C(Vj)
is the number of times this feature of the verb was
observed in the corpus. The features computed us-
ing this formula are: ACT, PASS, TRAN, INTRAN,
VBD, and VBN.
5Note that while (Stevenson and Merlo, 1999; Merlo and
Stevenson, 2001) used a TRAN/INTRAN feature, in their case
it was estimated in a completely different way using tagged
data. Hence, while we use the same name for the feature here,
it is not the same kind of feature as the one used in the cited
work.
4.3 The causative feature: CAUS
To correctly obtain the causative values of the test-
ing verbs, we needed to know the meaning of
the sentences. In this paper, we approximate the
value by using the following approach. Also, the
causative value is not a probability but a weight
which is subsequently normalized.
We extract the subjects and objects of verbs and
put them into two sets. We use the last noun of the
subject noun phrase and object noun phrase (tagged
by NN, NNS, NNP, or NNPS), as the subject and
object of the sentences. Then the causative value is
CAUS = overlap
sum of all subject and objects in multiset
where the overlap is defined as the largest multiset
of elements belonging to both subjects and objects
multisets.
If subject is in the set {a, a, b, c} and object is in
set {a, d}, the intersection between both set will be
{a, a}, and the causative value will be 2(4+2) =
1
3 .
If subject is in the set {a, a, b, c} and object is
in the set {a, b, d}, the intersection between both
set will be {a, a, b}, and the causative value will be
(2+1)
(4+3) =
3
7 .
Note that using this measure, we expect to get
higher weights for tokens that occur frequently in
the object position and sometimes in the subject po-
sition. For example, CAUS({a, b}, {a, b}) = 24 while
CAUS({a, b}, {a, a, a}) = 35 . This difference in the
weight given by the CAUS feature is exploited in
the classifier.
4.4 The animate feature: ANIM
Similar to CAUS, we can only approximate the
value of animacy. We use the following formula to
find the value:
ANIM = number of occurrence of pronoun in
subject/number of occurrence of verbs
The set of pronouns used are I, we, you, she, he,
and they. In addition we use the set of part-of-
speech tags which are associated with animacy in
Penn Treebank tagset as part of set of features de-
scribed in the next section.
4.5 Part of Speech of object and subject
The part-of-speech feature picks up several subtle
cues about the differences in the types of arguments
selected by the verb in its subject or object position.
We count the occurrence of the head nouns of
the subject noun phrase and the object noun phrase.
Then, we find the frequency distribution by using
the same formula as before:
P(V j) =
C(V j)
?
1?x?N C(Vx)
where P(V j) is the distribution of part of speech j,
N is the total number of relevant POS features and
C(V j) is the number of occurrences of part of speech
j. Also, we limit the part of speech to only the fol-
lowing tags of speech: NNP, NNPS, EX, PRP, and
SUCH, where NNP is singular noun phrase, NNPS
is plural noun phrase, EX is ?there?, PRP is personal
pronoun, and SUCH is ?such?.
4.6 Transitive and intransitive SF of the verb
To find values for this feature we use the technique
described in Section 3. For each verb in our list
we extract all the subsequent NP and PP chunks
and their heads from the chunker output. We then
perform subcategorization frame learning with all
subsets of these extracted potential arguments. The
counts are appropriately assigned to these subsets to
provide a well-defined model. Using these counts
and the methods in Section 3 we categorize a verb
as either transitive or intransitive. For simplicity,
any number of arguments above zero is considered
to be a candidate for transitivity.
4.7 Constructing the Classifier
After we obtain all the probabilistic distributions
of the features of our testing verbs, we then use
C5.0 (Quinlan, 1992) to construct the classifier. The
data was annotated with the right classification for
each verb and the classifier was run on 10% of the
data using 10-fold cross validation.
5 Results
We tried all possible feature combinations (individ-
ual features and all possible conjunctions of those
features) to explore the contributions of each fea-
ture to the reduction of the error rate. The following
are the results of the best performing feature combi-
nations.
With our base features, ACT, PASS, VBD, VBN,
TRAN, and INTRAN we get the average error rate
of 49.4% for 10 fold cross validation. We can see
that when we add the CAUS feature, the average er-
ror decreases to 41.1%. The CAUS feature helps
in decreasing the error rate. Also, when we add
the ANIM feature, we get a much better perfor-
mance. Our average error rate decreases to 37.5%.
Features Average error rate SE Average error rate SE
from Decision Tree from Rule Set
TRAN, INTRAN, VBD, 49.4% 1.1% 67.7% 0.9%
VBN, PASS, ACT
TRAN, INTRAN, VBD, 41.1% 0.8% 40.8% 0.6%
VBN, PASS, ACT, CAUS
TRAN, INTRAN, VBD, 37.5% 0.8% 36.9% 1.0%
VBN, PASS, ACT, ANIM
TRAN, INTRAN, VBD, 39.2% 0.8% 38.1% 1.1%
VBN, PASS, ACT, PART
OF SPEECH
TRAN, INTRAN, VBD, 33.4% 0.7% 33.9% 0.8%
VBN, PASS, ACT, CAUS,
ANIM
TRAN, INTRAN, VBD, 39.0% 0.7% 37.1% 0.9%
VBN, PASS, ACT, CAUS,
PART OF SPEECH
TRAN, INTRAN, VBD, 35.8% 1.3% 35.9% 1.7%
VBN, PASS, ACT, ANIM,
PART OF SPEECH
TRAN, INTRAN, VBD, 39.5% 1.0% 38.3% 1.0%
VBN, PASS, ACT, CAUS,
ANIM, PART OF SPEECH
Figure 1: Results of the verb classification. Bold face results are for the best performing set of features in
the classifier.
This is the lowest error rate we can achieve by
adding one extra feature in addition to the base fea-
tures. The ANIM feature is an important feature
that we can use to construct the classifier. When we
add the PART OF SPEECH feature, the error rate
also decreases to 39.2%. Therefore, the PART OF
SPEECH also helps reduce the error rate as well.
When we put together the CAUS feature and ANIM
feature, we achieve the lowest error rate, which is
33.4%. When we put the PART OF SPEECH and
CAUS features together, the error rate does not re-
ally decrease (39.0%), comparing to the result with
only the PART OF SPEECH feature. The reason of
this result should be that there are some parts of the
PART OF SPEECH feature and CAUS feature that
overlap. When we add the ANIM and PART OF
SPEECH features together, the error rate does de-
crease to 35.8%. Although the result is not as good
as result of using ANIM and CAUS features, the
combination of the ANIM and PART OF SPEECH
features could be considered effective features that
we can use to construct the classifier. We then com-
bine all the features together. The result as expected
is not very good. The error rate is 39.5%. The rea-
son should be the same reason as the lower perfor-
mance when combining the CAUS and PART OF
SPEECH features.
Note that the features TRAN/INTRAN are
needed for computing a large subset of the features
used. Hence we did not conduct any experiments
without these features. These experiments show that
the use of SF learning can be useful to the perfor-
mance of the verb alternation classifier. The error
rate of the baseline classifier (picking the right ar-
gument structure at chance) was 65.5%. (Merlo and
Stevenson, 2001) calculate the expert-based upper
bound at this task to be an error rate of 13.5%.
Our best performing classifier achieves a 33.4%
error rate. In comparison, (Merlo and Stevenson,
2001) obtain an error rate of 30.2% using a tagged
and automatically parsed data set of 65M words of
WSJ text. Thus, while we obtain a slightly worse
error rate, this is obtained using a much smaller set
of training data.
6 Conclusion
In this paper, we discussed a technique which auto-
matically identified the correct argument structure
of a set of verbs. Our results in this paper serve as
a replication and extension of the results in (Merlo
and Stevenson, 2001). Our main contribution in
this paper is to show that with reasonable accuracy,
this task can be accomplished using only tagged and
chunked data. In addition, we incorporate some ad-
ditional features such as part-of-speech tags and the
use of subcategorization frame learning as part of
our classification algorithm.
We exploited the distributions of selected features
from the local context of the verb which was ex-
tracted from a 23M word WSJ corpus. We used
C5.0 to construct a decision tree classifier using the
values of those features. We were able to construct
a classifier that has an error rate of 33.4%. This
work shows that a subcategorization frame learning
algorithm (Sarkar and Zeman, 2000) can be applied
to the task of classifying verbs into verb alternation
classes.
In future work, we would like to classify verbs
into alternation classes on a per-token basis (as is
done in the approach taken by Gildea (2002)) rather
than the per-type we currently employ and also in-
corporate information about word senses in order
to feasibly include verb alternation information in
a statistical parser.
References
Steve Abney. 1997. Part of speech tagging and par-
tial parsing. In S. Young and G. Bloothooft, editors,
Corpus based methods in language and speech, pages
118?136. Dordrecht: Kluwer.
Peter Bickel and Kjell Doksum. 1977. Mathematical
Statistics. Holden-Day Inc.
Michael Brent. 1994. Acquisition of subcategorization
frames using aggregated evidence from local syntac-
tic cues. Lingua, 92:433?470. Reprinted in Acquisi-
tion of the Lexicon, L. Gleitman and B. Landau (Eds.).
MIT Press, Cambridge, MA.
Ted Briscoe and John Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In Proceed-
ings of the 5th ANLP Conference, pages 356?363,
Washington, D.C. ACL.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74, March.
Daniel Gildea. 2002. Probabilistic models of verb-
argument structure. In Proc. of COLING-2002.
A. Korhonen, G. Gorrell, and D. McCarthy. 2000. Sta-
tistical filtering and subcategorization frame acquisi-
tion. In Proceedings of EMNLP 2000.
Maria Lapata and Chris Brew. 1999. Using subcate-
gorization to resolve verb class ambiguity. In Pas-
cale Fung and Joe Zhou, editors, Proceedings of
WVLC/EMNLP, pages 266?274, 21-22 June.
Maria Lapata. 1999. Acquiring lexical generalizations
from corpora: A case study for diathesis alternations.
In Proceedings of 37th Meeting of ACL, pages 397?
404.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. Chicago University Press, Chicago, IL.
Christopher D. Manning. 1993. Automatic acquisition
of a large subcategorization dictionary from corpora.
In Proceedings of the 31st Meeting of the ACL, pages
235?242, Columbus, Ohio.
Diana McCarthy and Anna Korhonen. 1998. Detect-
ing verbal participation in diathesis alternations. In
Proceedings of COLING/ACL-1998. Student Session,
pages 1493?1495.
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic verb classification based on statistical distribu-
tion of argument structure. Computational Linguis-
tics, 27(3):373?408.
J. Ross Quinlan. 1992. C4.5: Programs for Machine
Learning. Series in Machine Learning. Morgan Kauf-
mann, San Mateo, CA.
A. Ratnaparkhi. 1996. A Maximum Entropy Part-Of-
Speech Tagger. In Proc. of the Empirical Methods in
Natural Language Processing Conference, University
of Pennsylvania.
Anoop Sarkar and Daniel Zeman. 2000. Automatic ex-
traction of subcategorization frames for czech. In Pro-
ceedings of COLING-2000.
Sabine Schulte im Walde. 2000. Clustering verbs se-
mantically according to their alternation behaviour. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING-2000), Saarbr-
cken, Germany, August.
Suzanne Stevenson and Paola Merlo. 1997. Lexical
structure and parsing complexity. Language and Cog-
nitive Processes, 12(2).
Suzanne Stevenson and Paola Merlo. 1999. Automatic
verb classification using distributions of grammatical
features. In Proceedings of EACL ?99, pages 45?52,
Bergen, Norway, 8?12 June.
Suzanne Stevenson, Paola Merlo, Natalia Kariaeva, and
Kamin Whitehouse. 1999. Supervised learning of
lexical semantic verb classes using frequency distri-
butions. In Proceedings of SIGLEX99: Standardizing
Lexical Resources, College Park, Maryland.
Akira Ushioda, David A. Evans, Ted Gibson, and Alex
Waibel. 1993. The automatic acquisition of frequen-
cies of verb subcategorization frames from tagged cor-
pora. In Proc. of the Workshop on Acquisition of Lex-
ical Knowledge from Text, Columbus, OH.
c? 2002 Association for Computational Linguistics
Squibs and Discussions
A Note on Typing Feature Structures
Shuly Wintner? Anoop Sarkar?
University of Haifa University of Pennsylvania
Feature structures are used to convey linguistic information in a variety of linguistic formalisms.
Various definitions of feature structures exist; one dimension of variation is typing: unlike untyped
feature structures, typed ones associate a type with every structure and impose appropriateness
constraints on the occurrences of features and on the values that they take. This work demon-
strates the benefits that typing can carry even for linguistic formalisms that use untyped feature
structures. We present a method for validating the consistency of (untyped) feature structure
specifications by imposing a type discipline. This method facilitates a great number of compile-
time checks: many possible errors can be detected before the grammar is used for parsing. We have
constructed a type signature for an existing broad-coverage grammar of English and implemented
a type inference algorithm that operates on the feature structure specifications in the grammar
and reports incompatibilities with the signature. We have detected a large number of errors in the
grammar, some of which are described in the article.
1. Introduction
Feature structures are used by a variety of linguistic formalisms as a means for rep-
resenting different levels of linguistic information. They are usually associated with
more elementary structures (such as phrase structure rules or trees) to provide an addi-
tional dimension for stating linguistic generalizations. A variant of feature structures,
typed feature structures, provide yet another dimension for such generalizations. It
is sometimes assumed that typed feature structures have linguistic advantages over
untyped ones and that they are, in general, more efficient to process. In this article
we show how typing can be useful also for systems that manipulate untyped feature
structures.
We present a method for validating the consistency of feature structure specifica-
tions by imposing a type discipline. This method facilitates a great number of compile-
time checks: many possible errors can be detected before the grammar is used for pars-
ing. Typed systems are used in one linguistic theory, Head-Driven Phrase Structure
Grammar (HPSG) (Pollard and Sag 1994), and we present here a different application
of them for theories that employ untyped feature structures. We constructed a type
signature for the XTAG English grammar (XTAG Research Group 2001), an existing
broad-coverage grammar of English. Then, we implemented a type inference algorithm
that operates on the feature structure specifications in the grammar. The algorithm re-
ports occurrences of incompatibility with the type signature. We have detected a large
number of errors in the grammar; four types of errors are described in the article.
The technique we propose was incorporated into the XTAG grammar development
system, which is based on the tree-adjoining grammar (TAG) formalism (Joshi, Levy,
? Department of Computer Science, University of Haifa, Mount Carmel, 31905 Haifa, Israel. E-mail:
shuly@cs.haifa.ac.il
? IRCS, University of Pennsylvania, 3401 Walnut Street, Philadelphia, PA-19104. E-mail: anoop@linc.
cis.upenn.edu
390
Computational Linguistics Volume 28, Number 3
and Takahashi 1975), lexicalized (Schabes, Abeille?, and Joshi 1988) and augmented
by unification-based feature structures (Vijay-Shanker and Joshi 1991). Tree-adjoining
languages fall into the class of mildly context-sensitive languages and as such are more
powerful than context-free languages. The TAG formalism in general, and lexicalized
TAGs in particular, are well-suited for linguistic applications. As first shown by Joshi
(1985) and Kroch and Joshi (1987), the properties of TAGs permit one to encapsulate
diverse syntactic phenomena in a very natural way.
The XTAG grammar development system makes limited use of feature structures
that can be attached to nodes in the trees that make up a grammar. Typically, feature
structures in XTAG are flat: nesting of structures is very limited. Furthermore, all fea-
ture structures in XTAG are finitely bounded: the maximum size of a feature structure
can be statically determined. During parsing, feature structures undergo unification as
the trees they are associated with are combined. But unification in XTAG is actually
highly limited: since all feature structures are bounded, unification can be viewed as
an atomic operation. Although the method we propose was tested on an XTAG gram-
mar, it is applicable in principle to any linguistic formalism that uses untyped feature
structures, in particular, to lexical-functional grammar (Kaplan and Bresnan 1982).
2. The Problem
XTAG is organized such that feature structures are specified in three different com-
ponents of the grammar: a Tree database defines feature structures attached to tree
families; a Syn database defines feature structures attached to lexically anchored trees;
and a Morph database defines feature structures attached to (possibly inflected) lexical
entries.
As an example, consider the verb seems. This verb can anchor several trees, among
which are trees of auxiliary verbs, such as the tree ?Vvx , depicted in Figure 1. This
tree, which is common to all auxiliary verbs, is associated with the feature structure
descriptions listed in Figure 1 (independently of the word that happens to anchor it).1
When the tree ?Vvx is anchored by seems, the lexicon specifies additional constraints
on the feature structures in this tree:
seem betaVvx VP.b:<mode> = inf/nom,
V.b:<mainv> = +
Finally, since ?seems? is an inflected form, the morphological database specifies more
constraints on the node that this word instantiates, as shown in Figure 2.
The actual feature structures that are associated with the lexicalized tree anchored
by ?seems? are the combination of the three sets of path equations. This organization
leaves room for several kinds of errors, inconsistencies, and typos in feature structure
manipulation. Nothing in the system can eliminate the following possible errors:
Undefined features: Every grammar makes use of a finite set of features in the
feature structure specification. As the features do not have to be declared,
however, certain bogus features can be introduced unintentionally, either
through typos or because of poor maintenance. In a grammar that has
an assign-case feature, the following statement is probably erroneous:
V.b:<asign-case> = acc.
1 We use ?handles? such as V.b or NP.t to refer to the feature structures being specified. Each node in a
tree is associated with two feature structures, ?top? (.t) and ?bottom? (.b) (Vijay-Shanker and Joshi
1991; XTAG Research Group 2001). Angular brackets delimit feature paths, and slashes denote
disjunctive (atomic) values.
391
Wintner and Sarkar A Note on Typing Feature Structures
V.t:<agr> = VP_r.b:<agr>
V.t:<assign-case> = VP_r.b:<assign-case>
V.t:<assign-comp> = VP_r.b:<assign-comp>
V.t:<displ-const set1> = VP_r.b:<displ-const set1>
V.t:<mainv> = VP_r.b:<mainv>
V.t:<mode> = VP_r.b:<mode>
V.t:<neg> = VP_r.b:<neg>
V.t:<tense> = VP_r.b:<tense>
VP.t:<assign-comp> = ecm
VP.t:<compar> = -
VP.t:<displ-const set1> = -
VP_r.b:<compar> = -
VP_r.b:<conditional> = VP.t:<conditional>
VP_r.b:<perfect> = VP.t:<perfect>
VP_r.b:<progressive> = VP.t:<progressive>
Figure 1
An example tree and its associated feature structure descriptions.
seems seem V <agr pers> = 3,
<agr num> = sing,
<agr 3rdsing> = +,
<mode> = ind,
<tense> = pres,
<assign-comp> = ind_nil/that/rel/if/whether,
<assign-case> = nom
Figure 2
The morphological database entry for seems.
Undefined values: The same problem can be manifested in values, rather than
features. In a grammar where nom is a valid value for the assign-case fea-
ture, the following statement is probably erroneous: V.b:<assign-case> =
non.
Incompatible feature equations: The grammar designer has a notion of what
paths can be equated, but this notion is not formally defined. Thus, it is
possible to find erroneous path equations such as VP.b:<assign-case> =
V.t:<tense>.
Such cases go undetected by XTAG and result in parsing errors. For example, the
statement V.b:<asign-case> = acc was presumably supposed to constrain the gram-
matical derivations to those in which the assign-case feature had the value acc. With
the typo, this statement never causes unification to fail (assuming that the feature
asign-case occurs nowhere else in the grammar); the result is overgeneration.
On the other hand, if the statement V.b:<assign-case> = non is part of the lexical
entry of some verb, and some derivations require that certain verbs have nom as their
value of assign-case, then that verb would never be a grammatical candidate for
those derivations. The result here is undergeneration.
392
Computational Linguistics Volume 28, Number 3
Note that nothing in the above description hinges on the particular linguistic
formalism or its implementation. The same problems are likely to occur in every
system that manipulates untyped feature structures.2
3. Introducing Typing
The problems discussed above are reminiscent of similar problems in programming
languages; in that domain, the solution lies in typing: a stricter type discipline provides
means for more compile-time checks to be performed, thus tracking potential errors
as soon as possible. Fortunately, such a solution is perfectly applicable to the case of
feature structures, as typed feature structures (TFSs) are well understood (Carpenter
1992). We briefly survey this concept below.
TFSs are defined over a signature consisting of a set of of types (Types) and a set
of features (Feats). Types are partially ordered by subsumption (denoted ??). The
least upper bound with respect to subsumption of t1 and t2 is denoted t1 unionsq t2. Each type
is associated with a set of appropriate features through a function Approp: Types ?
Feats ? Types. The appropriate values of a feature F in a type t have to be of
specified (appropriate) types. Features are inherited by subtypes: whenever F is ap-
propriate for a type t, it is also appropriate for all the types t? such that t  t ?. Each
feature F has to be introduced by some most general type Intro(F) (and be appropriate
for all its subtypes).
Figure 3 graphically depicts a type signature in which greater (more specific)
types are presented higher and the appropriateness specification is displayed above
the types. For example, for every feature structure of type verb, the feature assign-
case is appropriate, with values that are at least of type cases: Approp(verb, assign-
case) = cases .
A formal introduction to the theory of TFSs is given by Carpenter (1992). In-
formally, a TFS over a signature ?Types,, Feats, Approp? differs from an untyped
feature structure in two aspects: a TFS has a type; and the value of each feature is a
TFS?there is no need for atoms in a typed system. A TFS A whose type is t is well-
typed iff every feature F in A is such that Approp(t , F) is defined; every feature F in
A has value of type t ? such that Approp(t , F)  t ?; and all the substructures of A are
well-typed. It is totally well-typed if, in addition, every feature F such that Approp(t ,
F) is defined occurs in A. In other words, a TFS is totally well-typed if it has all and
only the features that are appropriate for its type, with appropriate values, and the
same holds for all its substructures.
Totally well-typed TFSs are informative and efficient to process. It might be prac-
tically difficult, however, for the writer of a grammar to specify the full information
such a structure encodes. To overcome this problem, type inference algorithms have
been devised that enable a system to infer a totally well-typed TFS automatically from
a partial description. Partial descriptions can specify
? the type of a TFS: V.t:verb
? a variable, referring to a TFS: VP.b:assign-case:X
? a path equation: VP.b:assign-case = NP.t:case
2 Some systems could have elaborate mechanisms implemented to deal with each kind of error
mentioned here. But typing provides a single mechanism that handles several different kinds of errors
simultaneously.
393
Wintner and Sarkar A Note on Typing Feature Structures
Figure 3
A simple type signature.
? a feature-value pair: NP.b:case:acc
? a conjunction of descriptions: V.t:(sign,assign-case:none)
The inferred feature structure is the most general TFS that is consistent with the
partial description. The inference fails iff the description is inconsistent (i.e., describes
no feature structure). See Figure 4 for some examples of partial descriptions and the
TFSs they induce, based on the signature of Figure 3.
4. Implementation
To validate feature structure specifications in XTAG we have implemented the type in-
ference algorithm suggested by Carpenter (1992, chapter 6). We manually constructed
a type signature suitable for the current use of feature structures in the XTAG gram-
mar of English (XTAG Research Group 2001). Then, we applied the type inference
algorithm to all the feature structure specifications of the grammar, such that each
feature structure was expanded with respect to the signature.
Type inference is applied off-line, before the grammar is used for parsing. As is the
case with other off-line applications, efficiency is not a critical issue. It is worth noting,
however, that for the grammar we checked (in which, admittedly, feature structures are
flat and relatively small), the validation procedure is highly efficient. As a benchmark,
we checked the consistency of 1,000 trees, each consisting of two to fourteen nodes.
The input file, whose size approached 1MB, contained over 33,000 path equations.
Validating the consistency of the benchmark trees took less than 33 seconds (more
than a thousand path equations per second).
4.1 The Signature
The signature for the XTAG grammar was constructed manually, by observing the
use of feature equations in the grammar and consulting its documentation. As noted
above, most feature structures used in the grammar are flat, but the number of features
in the top level is relatively high. The signature consists of 58 types and 56 features,
and its construction took a few hours. In principle, it should be possible to construct
signatures for untyped feature structures automatically, but such signatures will of
course be less readable than manually constructed ones.
4.2 Results
Applying the type inference algorithm to the XTAG English grammar, we have vali-
dated the consistency of all feature structures specified in the grammar. We have been
able to detect a great number of errors, which we discuss in this section. The errors
394
Computational Linguistics Volume 28, Number 3
Figure 4
Inferred TFSs.
can be classified into four different types: ambiguous names, typos, undocumented
features, and plain errors.
4.2.1 Ambiguous Names. Ambiguous names are an obvious error, but one that is not
easy to track without the typing mechanism that we discuss in this article. As the
XTAG grammar has been developed by as many as a dozen developers, over a period
of more than a decade, such errors are probably unavoidable. Specifically, a single
name is used for two different features or values, with completely different intentions
in mind.3 We have found several such errors in the grammar.
The feature gen was used for two purposes: in nouns, it referred to the gender,
and took values such as masc, fem, or neuter; in pronouns, it was a boolean feature
denoting genitive case. We even found a few cases in which the values of these in-
compatible features were equated. As another example, the value nom was used to
denote both nominative case, where it was an appropriate value for the case feature,
and to denote a nominal predicate, where it was the appropriate value of the mode
feature. Of course, these two features have nothing to do with each other and should
never be equated (hence, should never have the same value). Finally, values such as
nil or none were used abundantly for a variety of purposes.
3 Recall that by the feature introduction condition, each feature must be introduced by some most
general type (and be appropriate for all its subtypes).
395
Wintner and Sarkar A Note on Typing Feature Structures
4.2.2 Typos. Another type of error that is very difficult to track otherwise are plain
typos. The best example is probably a feature that occurred about 80% of the time as
relpron and the rest of the time as rel-pron:
S_r.t:<relpron> = NP_w.t:<rel-pron>
4.2.3 Undocumented Features. We have found a great number of features and values
that are not mentioned in the technical report documenting the grammar. Some of
them turned out to be remnants of old analyses that were obsolete; others indicated
a need for better documentation. Of course, the fewer features the grammar is using,
the more efficient unification (and, hence, parsing) becomes.
Other cases necessitated updates of the grammar documentation. For example, the
feature displ-const was documented as taking boolean values but turned out to be
a complex feature, with a substructure under the feature set1. The feature gen (in its
gender use) was defined at the top level of nouns, whereas it should have been under
the agr feature.
4.2.4 Other Errors. Finally, some errors are plain mistakes of the grammar designer.
For example, the specification S_r.t:<assign-case> = NP_w.t:<assign-case> im-
plies that assign-case is appropriate for nouns, which is of course wrong; the spec-
ification S_r.t:<case> = nom implies that sentences have cases; and the specifica-
tion V.t:<refl> = V_r.b:<refl> implies that verbs can be reflexive. Another ex-
ample is the specification D_r.b:<punct bal> = Punct_1.t:<punct>, which handles
the balancing of punctuation marks such as parentheses. This should have been either
D_r.b:<punct> = Punct_1.t:<punct> or D_r.b:<punct bal> = Punct_1.t:<punct
bal>.
4.3 Additional Advantages
Since the feature structure validation procedure practically expands path equations to
(most general) totally well-typed feature structures, we have implemented a mode in
which the system outputs the expanded TFSs. Users can thus have a better idea of
what feature structures are associated with tree nodes, both because all the features
are present, and because typing adds information that was unavailable in the untyped
specification. As an example, consider the following specification:
PP.b:<wh> = NP.b:<wh>
PP.b:<assign-case> = nom
PP.b:<assign-case> = N.t:<case>
NP.b:<agr> = N.t:<agr>
NP.b:<case> = N.t:<case>
N.t:<case> = nom/acc
When it is expanded by the system, the TFS that is output for PP.b is depicted in
Figure 5 (left). Note that the type of this TFS was set to p or v or comp, indicating
that there is not sufficient information for the type inference procedure to distinguish
among these three types. Many features that are not explicitly mentioned are added
by the inference procedure, with their ?default? (most general) values.
The node N.t is associated with a TFS, parts of which are depicted in Figure 5
(right). It is worth noting that the type of this TFS was correctly inferred to be noun,
and that the case feature is reentrant with the assign-case feature of the PP.b node
(through the reentrancy tag [304]), thus restricting it to nom, although the specification
listed a disjunctive value, nom/acc.
396
Computational Linguistics Volume 28, Number 3
PP.b
[52]p_or_v_or_comp(
wh:[184]bool,
assign-comp:[54]comps,
rel-pron:[55]rel-prons,
trace:[56]bot,
equiv:[57]bool,
compar:[58]bool,
super:[59]bool,
neg:[60]bool,
assign-case:[304]nom)
N.t
[289]noun(
wh:[290]bool,
agr:[298]agrs(
num:[118]nums,
pers:[119]persons),
conj:[299]conjs,
control:[300]bot,
displ-const:[302]constituents(
set1:[153]bool),
case:[304]nom,
definite:[305]bool,
const:[306]bool,
rel-clause:[307]bool,
pron:[308]bool,
quan:[309]bool,
gerund:[312]bool,
refl:[313]bool,
gen:[314]gens,
compl:[316]bool)
Figure 5
Expanded TFSs.
5. Further Research
We have described in this article a method for validating the consistency of feature
structure specifications in grammars that incorporate untyped feature structures. Al-
though the use of feature structures in XTAG is very limited, especially since all fea-
ture structures are finitely bounded, the method we describe is applicable to feature
structure?based grammatical formalisms in general; in particular, it will be interesting
to test it on broad-coverage grammars that are based on unbounded feature structures,
such as lexical functional grammars.
We have applied type inference only statically; feature structures that are created
at parse time are not validated. By modifying the unification algorithm currently used
in XTAG, however, it is possible to use TFSs in the grammar and apply type inference
at run time. This will enable detection of more errors at run time and provide for
better representation of feature structures and possibly for more efficient unifications.
In a new implementation of XTAG (Sarkar 2000), feature structure specifications are
not evaluated as structures are being constructed; rather, they are deferred to the final
stage of processing, when only valid trees remain. We plan to apply type inference to
the resulting feature structures in this implementation, so that run-time errors can be
detected as well.
Acknowledgments
This work was supported by an IRCS
fellowship and NSF grant SBR 8920230. The
work of the first author was supported by
the Israel Science Foundation (grant number
136/01-1).
References
Carpenter, Bob. 1992. The Logic of Typed
Feature Structures. Cambridge Tracts in
Theoretical Computer Science. Cambridge
University Press, Cambridge, England.
Joshi, Aravind K. 1985. ?Tree adjoining
grammars: How much context sensitivity
is required to provide a reasonable
structural description.? In D. Dowty,
I. Karttunen, and A. Zwicky, editors,
Natural Language Parsing. Cambridge
University Press, Cambridge, England,
pages 206?250.
397
Wintner and Sarkar A Note on Typing Feature Structures
Joshi, Aravind K., L. Levy, and
M. Takahashi. 1975. Tree adjunct
grammars. Journal of Computer and System
Sciences 10(1):136?163.
Kaplan, Ronald and Joan Bresnan. 1982.
?Lexical functional grammar: A formal
system for grammatical representation.?
In J. Bresnan, editor, The Mental
Representation of Grammatical Relations. MIT
Press, Cambridge, Massachusetts,
pages 173?281.
Kroch, Anthony S. and Aravind K. Joshi.
1987. ?Analyzing extraposition in a tree
adjoining grammar.? In G. Huck and
A. Ojeda, editors, Discontinuous
Constituents, Syntax and Semantics,
volume 20. Academic Press,
pages 107?149.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press and CSLI
Publications, Chicago, Illinois, and
Stanford, California.
Sarkar, Anoop. 2000. ?Practical experiments
in parsing using tree adjoining
grammars.? In Proceedings of the Fifth
Workshop on Tree Adjoining Grammars,
TAG+ 5, Paris, France, May 25?27.
Schabes, Yves, Anne Abeille?, and
Aravind K. Joshi. 1988. ?Parsing strategies
with ?lexicalized? grammars: Application
to tree adjoining grammars.? In
Proceedings of the 12th International
Conference on Computational Linguistics
(COLING?88), volume 2, pages 579?583,
Budapest, Hungary, August.
Vijay-Shanker, K. and Aravind K. Joshi.
1991. ?Unification based tree adjoining
grammars.? In J. Wedekind, editor,
Unification-Based Grammars. MIT Press,
Cambridge, Massachusetts.
XTAG Research Group. 2001. ?A lexicalized
tree adjoining grammar for English.?
Technical report IRCS-01-03, Institute for
Research in Cognitive Science, University
of Pennsylvania, Philadelphia.
Applying Co-Training methods to Statistical Parsing
Anoop Sarkar
Dept. of Computer and Information Science
University of Pennsylvania
200 South 33rd Street,
Philadelphia, PA 19104-6389 USA
anoop@linc.cis.upenn.edu
Abstract
We propose a novel Co-Training method for statistical
parsing. The algorithm takes as input a small corpus
(9695 sentences) annotated with parse trees, a dictionary
of possible lexicalized structures for each word in the
training set and a large pool of unlabeled text. The algo-
rithm iteratively labels the entire data set with parse trees.
Using empirical results based on parsing the Wall Street
Journal corpus we show that training a statistical parser
on the combined labeled and unlabeled data strongly out-
performs training only on the labeled data.
1 Introduction
The current crop of statistical parsers share a similar
training methodology. They train from the Penn Tree-
bank (Marcus et al, 1993); a collection of 40,000 sen-
tences that are labeled with corrected parse trees (ap-
proximately a million word tokens). In this paper, we
explore methods for statistical parsing that can be used
to combine small amounts of labeled data with unlimited
amounts of unlabeled data. In the experiment reported
here, we use 9695 sentences of bracketed data (234467
word tokens). Such methods are attractive for the follow-
ing reasons:
 Bracketing sentences is an expensive process. A
parser that can be trained on a small amount of la-
beled data will reduce this annotation cost.
 Creating statistical parsers for novel domains and
new languages will become easier.
 Combining labeled data with unlabeled data allows
exploration of unsupervised methods which can
now be tested using evaluations compatible with su-
pervised statistical parsing.
In this paper we introduce a new approach that com-
bines unlabeled data with a small amount of labeled
(bracketed) data to train a statistical parser. We use a Co-
Training method (Yarowsky, 1995; Blum and Mitchell,
 I would like to thank Aravind Joshi, Mitch Marcus, Mark Liberman,
B. Srinivas, David Chiang and the anonymous reviewers for helpful
comments on this work. This work was partially supported by NSF
Grant SBR8920230, ARO Grant DAAH0404-94-G-0426, and DARPA
Grant N66001-00-1-8915.
1998; Goldman and Zhou, 2000) that has been used pre-
viously to train classifiers in applications like word-sense
disambiguation (Yarowsky, 1995), document classifica-
tion (Blum and Mitchell, 1998) and named-entity recog-
nition (Collins and Singer, 1999) and apply this method
to the more complex domain of statistical parsing.
2 Unsupervised techniques in language
processing
While machine learning techniques that exploit anno-
tated data have been very successful in attacking prob-
lems in NLP, there are still some aspects which are con-
sidered to be open issues:
 Adapting to new domains: training on one domain,
testing (using) on another.
 Higher performance when using limited amounts of
annotated data.
 Separating structural (robust) aspects of the prob-
lem from lexical (sparse) ones to improve perfor-
mance on unseen data.
In the particular domain of statistical parsing there has
been limited success in moving towards unsupervised
machine learning techniques (see Section 7 for more dis-
cussion). A more promising approach is that of combin-
ing small amounts of seed labeled data with unlimited
amounts of unlabeled data to bootstrap statistical parsers.
In this paper, we use one such machine learning tech-
nique: Co-Training, which has been used successfully in
several classification tasks like web page classification,
word sense disambiguation and named-entity recogni-
tion.
Early work in combining labeled and unlabeled data
for NLP tasks was done in the area of unsupervised part
of speech (POS) tagging. (Cutting et al, 1992) reported
very high results (96% on the Brown corpus) for un-
supervised POS tagging using Hidden Markov Models
(HMMs) by exploiting hand-built tag dictionaries and
equivalence classes. Tag dictionaries are predefined as-
signments of all possible POS tags to words in the test
data. This impressive result triggered several follow-up
studies in which the effect of hand tuning the tag dictio-
nary was quantified as a combination of labeled and unla-
Pierre/NNP Vinken/NNP
NP
will/MD
join/VB
the/DT board/NN
NP
as/IN
a/DT non?executive/JJ director/NN
NP
PP
VP
VP
S
Figure 1: An example of the kind of output expected from a statistical parser.
beled data. The experiments in (Merialdo, 1994; Elwor-
thy, 1994) showed that only in very specific cases HMMs
were effective in combining labeled and unlabeled data.
However, (Brill, 1997) showed that aggressively us-
ing tag dictionaries extracted from labeled data could be
used to bootstrap an unsupervised POS tagger with high
accuracy (approx 95% on WSJ data). We exploit this ap-
proach of using tag dictionaries in our method as well
(see Section 3.2 for more details). It is important to point
out that, before attacking the problem of parsing using
similar machine learning techniques, we face a represen-
tational problem which makes it difficult to define the
notion of tag dictionary for a statistical parser.
The problem we face in parsing is more complex than
assigning a small fixed set of labels to examples. If the
parser is to be generally applicable, it has to produce
a fairly complex ?label? given an input sentence. For
example, given the sentence Pierre Vinken will join the
board as a non-executive director, the parser is expected
to produce an output as shown in Figure 1.
Since the entire parse cannot be reasonably considered
as a monolithic label, the usual method in parsing is to
decompose the structure assigned in the following way:
S(join) ! NP(Vinken) VP(join)
NP(Vinken) ! Pierre Vinken
VP(join) ! will VP(join)
VP(join) ! join NP(board) PP(as)
: : :
However, such a recursive decomposition of structure
does not allow a simple notion of a tag dictionary. We
solve this problem by decomposing the structure in an
approach that is different from that shown above which
uses context-free rules.
The approach uses the notion of tree rewriting as
defined in the Lexicalized Tree Adjoining Grammar
(LTAG) formalism (Joshi and Schabes, 1992)1 which re-
1This is a lexicalized version of Tree Adjoining Grammar (Joshi et
al., 1975; Joshi, 1985).
tains the notion of lexicalization that is crucial in the suc-
cess of a statistical parser while permitting a simple def-
inition of tag dictionary. For example, the parse in Fig-
ure 1 can be generated by assigning the structured labels
shown in Figure 2 to each word in the sentence (for sim-
plicity, we assume that the noun phrases are generated
here as a single word). We use a tool described in (Xia
et al, 2000) to convert the Penn Treebank into this rep-
resentation.
Pierre Vinken
NP
will VP
VP
NP
join NP
VP
S
the board
NP
VP
as NP
PP
VP
a non?executive director
NP
Figure 2: Parsing as tree classification and attachment.
Combining the trees together by rewriting nodes as
trees (explained in Section 2.1) gives us the parse tree
in Figure 1. A history of the bi-lexical dependencies that
define the probability model used to construct the parse
is shown in Figure 3. This history is called the derivation
tree.
In addition, as a byproduct of this kind of represen-
tation we obtain more than the phrase structure of each
sentence. We also produce a more embellished parse in
which phenomena such as predicate-argument structure,
subcategorization and movement are given a probabilis-
tic treatment.
Pierre_Vinken will the_board
a_nonexecutive_director
as
join
Figure 3: A derivation indicating all the attachments be-
tween trees that have occurred during the parse of the
sentence.
2.1 The Generative Model
A stochastic LTAG derivation proceeds as follows (Sch-
abes, 1992; Resnik, 1992). An initial tree is selected with
probability Pinit and other trees selected by words in the
sentence are combined using the operations of substitu-
tion and adjoining. These operations are explained below
with examples. Each of these operations is performed
with probability Pattach.
For each  that can be valid start of a derivation:
X

Pinit() = 1
Substitution is defined as rewriting a node in the fron-
tier of a tree with probability Pattach which is said to be
proper if:
X

0
Pattach(;  ! 
0
) = 1
where ;  !  0 indicates that tree  0 is substituting
into node  in tree  . An example of the operation of
substitution is shown in Figure 4.
Adjoining is defined as rewriting any internal node of a
tree by another tree. This is a recursive rule and each ad-
joining operation is performed with probability Pattach
which is proper if:
Pattach(;  ! NA) +
X

0
Pattach(;  ! 
0
) = 1
Pattach here is the probability that 
0 rewrites an in-
ternal node  in tree  or that no adjoining (NA) occurs
at node  in  . The additional factor that accounts for no
adjoining at a node is required for the probability to be
well-formed. An example of the operation of adjoining
is shown in Figure 5.
Each LTAG derivationD which was built starting from
tree  with n subsequent attachments has the probability:
Pr(D) = Pinit()
Y
1in
Pattach(;  ! 
0
i
)
Pierre Vinken
NP
join NP
VP
S
NP
join NP
VP
S
Pierre Vinken
NP
Figure 4: Example substitution of the tree for
Pierre Vinken into the tree for join: (join);NP !

0
(Pierre Vinken).
will VP
VP
NP
join NP
VP
S
NP
will
join NP
VP
VP
S
Figure 5: Example adjoining of the tree for will into the
tree for join: (join);VP !  0(will).
Note that assuming each tree is lexicalized by one
word the derivation D corresponds to a sentence of n+1
words.
In the next section we show how to exploit this notion
of tag dictionary to the problem of statistical parsing.
3 Co-Training methods for parsing
Many supervised methods of learning from a Treebank
have been studied. The question we want to pursue in
this paper is whether unlabeled data can be used to im-
prove the performance of a statistical parser and at the
same time reduce the amount of labeled training data
necessary for good performance. We will assume the
data that is input to our method will have the following
characteristics:
1. A small set of sentences labeled with corrected
parse trees and large set of unlabeled data.
2. A pair of probabilistic models that form parts of a
statistical parser. This pair of models must be able
to mutually constrain each other.
3. A tag dictionary (used within a backoff smoothing
strategy) for labels are not covered in the labeled
set.
The pair of probabilistic models can be exploited to
bootstrap new information from unlabeled data. Since
both of these steps ultimately have to agree with each
other, we can utilize an iterative method called Co-
Training that attempts to increase agreement between a
pair of statistical models by exploiting mutual constraints
between their output.
Co-Training has been used before in applications like
word-sense disambiguation (Yarowsky, 1995), web-page
classification (Blum and Mitchell, 1998) and named-
entity identification (Collins and Singer, 1999). In all
of these cases, using unlabeled data has resulted in per-
formance that rivals training solely from labeled data.
However, these previous approaches were on tasks that
involved identifying the right label from a small set of
labels (typically 2?3), and in a relatively small parame-
ter space. Compared to these earlier models, a statistical
parser has a very large parameter space and the labels
that are expected as output are parse trees which have
to be built up recursively. We discuss previous work in
combining labeled and unlabeled data in more detail in
Section 7.
Co-training (Blum and Mitchell, 1998; Yarowsky,
1995) can be informally described in the following man-
ner:
 Pick two (or more) ?views? of a classification prob-
lem.
 Build separate models for each of these ?views? and
train each model on a small set of labeled data.
 Sample an unlabeled data set and to find examples
that each model independently labels with high con-
fidence. (Nigam and Ghani, 2000)
 Confidently labeled examples can be picked in var-
ious ways. (Collins and Singer, 1999; Goldman and
Zhou, 2000)
 Take these examples as being valuable as training
examples and iterate this procedure until the unla-
beled data is exhausted.
Effectively, by picking confidently labeled data from
each model to add to the training data, one model is la-
beling data for the other model.
3.1 Lexicalized Grammars and Mutual Constraints
In the representation we use, parsing using a lexicalized
grammar is done in two steps:
1. Assigning a set of lexicalized structures to each
word in the input sentence (as shown in Figure 2).
2. Finding the correct attachments between these
structures to get the best parse (as shown in Fig-
ure 1).
Each of these two steps involves ambiguity which can
be resolved using a statistical model. By explicitly rep-
resenting these two steps independently, we can pursue
independent statistical models for each step:
1. Each word in the sentence can take many different
lexicalized structures. We can introduce a statistical
model that disambiguates the lexicalized structure
assigned to a word depending on the local context.
2. After each word is assigned a certain set of lexical-
ized structures, finding the right parse tree involves
computing the correct attachments between these
lexicalized structures. Disambiguating attachments
correctly using an appropriate statistical model is
essential to finding the right parse tree.
These two models have to agree with each other on
the trees assigned to each word in the sentence. Not only
do the right trees have to be assigned as predicted by the
first model, but they also have to fit together to cover the
entire sentence as predicted by the second model2. This
represents the mutual constraint that each model places
on the other.
3.2 Tag Dictionaries
For the words that appear in the (unlabeled) training data,
we collect a list of part-of-speech labels and trees that
each word is known to select in the training data. This
information is stored in a POS tag dictionary and a tree
dictionary. It is important to note that no frequency or
any other distributional information is stored. The only
information stored in the dictionary is which tags or trees
can be selected by each word in the training data.
We use a count cutoff for trees in the labeled data and
combine observed counts into an unobserved tree count.
This is similar to the usual technique of assigning the
token unknown to infrequent word tokens. In this way,
trees unseen in the labeled data but in the tag dictionary
are assigned a probability in the parser.
The problem of lexical coverage is a severe one for
unsupervised approaches. The use of tag dictionaries is
a way around this problem. Such an approach has al-
ready been used for unsupervised part-of-speech tagging
in (Brill, 1997) where seed data of which POS tags can
be selected by each word is given as input to the unsu-
pervised tagger.
2See x7 for a discussion of the relation of this approach to that of
SuperTagging (Srinivas, 1997)
In future work, it would be interesting to extend mod-
els for unknown-word handling or other machine learn-
ing techniques in clustering or the learning of subcatego-
rization frames to the creation of such tag dictionaries.
4 Models
As described before, we treat parsing as a two-step pro-
cess. The two models that we use are:
1. H1: selects trees based on previous context (tagging
probability model)
2. H2: computes attachments between trees and re-
turns best parse (parsing probability model)
4.1 H1: Tagging probability model
We select the most likely trees for each word by examin-
ing the local context. The statistical model we use to de-
cide this is the trigram model that was used by B. Srinivas
in his SuperTagging model (Srinivas, 1997). The model
assigns an n-best lattice of tree assignments associated
with the input sentence with each path corresponding to
an assignment of an elementary tree for each word in the
sentence. (for further details, see (Srinivas, 1997)).
P (TjW)
= P (T
0
: : : T
n
jW
0
: : :W
n
) (1)
=
P (T
0
: : : T
n
) P (W
0
: : :W
n
jT
0
: : : T
n
)
P (W
0
: : :W
n
)
(2)
 P (T
i
jT
i 2
T
i 1
) P (W
i
jT
i
) (3)
where T
0
: : : T
n
is a sequence of elementary trees as-
signed to the sentence W
0
: : :W
n
.
We get (2) by using Bayes theorem and we obtain (3)
from (2) by ignore the denominator and by applying the
usual Markov assumptions.
The output of this model is a probabilistic ranking of
trees for the input sentence which is sensitive to a small
local context window.
4.2 H2: Parsing probability model
Once the words in a sentence have selected a set of el-
ementary trees, parsing is the process of attaching these
trees together to give us a consistent bracketing of the
sentences. Notation: Let  stand for an elementary tree
which is lexicalized by a word: w and a part of speech
tag: p.
Let Pinit (introduced earlier in 2.1) stand for the prob-
ability of being root of a derivation tree defined as fol-
lows:
X

Pinit() = 1
including lexical information, this is written as:
Pr(; w; pjtop = 1) =
Pr( jtop = 1) (4)
Pr(pj; top = 1) (5)
Pr(wj; p; top = 1); (6)
where the variable top indicates that  is the tree that
begins the current derivation. There is a useful approxi-
mation for Pinit:
Pr(; w; pjtop = 1)  Pr(labeljtop = 1)
where label is the label of the root node of  .
^
Pr(labeljtop = 1) =
Count(top = 1; label) + 
Count(top = 1) + N
(7)
where N is the number of bracketing labels and  is a
constant used to smooth zero counts.
Let Pattach (introduced earlier in 2.1) stand for the
probability of attachment of  0 into another  :
Pattach(;  ! NA) +
X

0
Pattach(;  ! 
0
) = 1
including lexical information, this is written as:
Pr(
0
; p
0
; w
0
jNode; ; w; p) (8)
Pr(NAjNode; ; w; p) (9)
We decompose (8) into the following components:
Pr(
0
; p
0
; w
0
jNode; ; w; p) =
Pr(
0
jNode; ; w; p) (10)
Pr(p
0
j
0
; Node; ; w; p) (11)
Pr(w
0
jp
0
; 
0
; Node; ; w; p); (12)
We do a similar decomposition for (9).
For each of the equations above, we use a backoff
model which is used to handle sparse data problems. We
compute a backoff model as follows:
Let e
1
stand for the original lexicalized model and e
2
be the backoff level which only uses part of speech infor-
mation:
e
1
: Node; ; w; p
e
2
: Node; ; p
For both Pinit and Pattach, let c = Count(e1). Then
the backoff model is computed as follows:
(c)e
1
+ (1  (c))e
2
where (c) = c
(c+D)
and D is the diversity of e
1
(i.e.
the number of distinct counts for e
1
).
For Pattach we further smooth probabilities (10), (11)
and (12). We use (10) as an example, the other two are
handled in the same way.
^
Pr(
0
jNode; ; w; p) =
(Count(Node; ; w; p; 
0
) + )
(Count(Node; ; w; p) + k)
(13)
Count(Node; ; w; p) =
X
y2T
0
Count(Node; ; w; p; y) (14)
where k is the diversity of adjunction, that is: the num-
ber of different trees that can attach at that node. T 0 is
the set of all trees  0 that can possibly attach at Node in
tree  .
For our experiments, the value of  is set to 1
100;000
.
5 Co-Training algorithm
We are now in the position to describe the Co-Training
algorithm, which combines the models described in Sec-
tion 4.1 and in Section 4.2 in order to iteratively label a
large pool of unlabeled data.
We use the following datasets in the algorithm:
labeled a set of sentences bracketed with the correct
parse trees.
cache a small pool of sentences which is the focus of
each iteration of the Co-Training algorithm.
unlabeled a large set of unlabeled sentences. The only
information we collect from this set of sentences is
a tree-dictionary: tree-dict and part-of-speech dic-
tionary: pos-dict. Construction of these dictionaries
is covered in Section 3.2.
In addition to the above datasets, we also use the usual
development test set (termed dev in this paper), and a test
set (called test) which is used to evaluate the bracketing
accuracy of the parser.
The Co-Training algorithm consists of the following
steps which are repeated iteratively until all the sentences
in the set unlabeled are exhausted.
1. Input: labeled and unlabeled
2. Update cache
 Randomly select sentences from unlabeled and
refill cache
 If cache is empty; exit
3. Train models H1 and H2 using labeled
4. Apply H1 and H2 to cache.
5. Pick most probablen from H1 (run through H2) and
add to labeled.
6. Pick most probable n from H2 and add to labeled
7. n = n + k; Go to Step 2
For the experiment reported here, n = 10, and k was
set to be n in each iteration. We ran the algorithm for 12
iterations (covering 20480 of the sentences in unlabeled)
and then added the best parses for all the remaining sen-
tences.
6 Experiment
6.1 Setup
The experiments we report were done on the Penn Tree-
bank WSJ Corpus (Marcus et al, 1993). The various
settings for the Co-Training algorithm (from Section 5)
are as follows:
 labeled was set to Sections 02-06 of the Penn Tree-
bank WSJ (9625 sentences)
 unlabeled was 30137 sentences (Section 07-21 of
the Treebank stripped of all annotations).
 A tag dictionary of all lexicalized trees from labeled
and unlabeled.
 Novel trees were treated as unknown tree tokens.
 The cache size was 3000 sentences.
While it might seem expensive to run the parser over
the cache multiple times, we use the pruning capabilities
of the parser to good use here. During the iterations we
set the beam size to a value which is likely to prune out
all derivations for a large portion of the cache except the
most likely ones. This allows the parser to run faster,
hence avoiding the usual problem with running an iter-
ative algorithm over thousands of sentences. In the ini-
tial runs we also limit the length of the sentences entered
into the cache because shorter sentences are more likely
to beat out the longer sentences in any case. The beam
size is reset when running the parser on the test data to
allow the parser a better chance at finding the most likely
parse.
6.2 Results
We scored the output of the parser on Section 23 of the
Wall Street Journal Penn Treebank. The following are
some aspects of the scoring that might be useful for com-
parision with other results: No punctuations are scored,
including sentence final punctuation. Empty elements
are not scored. We used EVALB (written by Satoshi
Sekine and Michael Collins) which scores based on PAR-
SEVAL (Black et al, 1991); with the standard parame-
ter file (as per standard practice, part of speech brackets
were not part of the evaluation). Also, we used Adwait
Ratnaparkhi?s part-of-speech tagger (Ratnaparkhi, 1996)
to tag unknown words in the test data.
We obtained 80.02% and 79.64% labeled bracketing
precision and recall respectively (as defined in (Black
et al, 1991)). The baseline model which was only
trained on the 9695 sentences of labeled data performed
at 72.23% and 69.12% precision and recall. These re-
sults show that training a statistical parser using our Co-
training method to combine labeled and unlabeled data
strongly outperforms training only on the labeled data.
It is important to note that unlike previous studies, our
method of moving towards unsupervised parsing are di-
rectly compared to the output of supervised parsers.
Certain differences in the applicability of the usual
methods of smoothing to our parser cause the lower ac-
curacy as compared to other state of the art statistical
parsers. However, we have consistently seen increase in
performance when using the Co-Training method over
the baseline across several trials. It should be empha-
sised that this is a result based on less than 20% of data
that is usually used by other parsers. We are experiment-
ing with the use of an even smaller set of labeled data to
investigate the learning curve.
7 Previous Work: Combining Labeled and
Unlabeled Data
The two-step procedure used in our Co-Training method
for statistical parsing was incipient in the SuperTag-
ger (Srinivas, 1997) which is a statistical model for tag-
ging sentences with elementary lexicalized structures.
This was particularly so in the Lightweight Dependency
Analyzer (LDA), which used shortest attachment heuris-
tics after an initial SuperTagging stage to find syntactic
dependencies between words in a sentence. However,
there was no statistical model for attachments and the
notion of mutual constraints between these two steps was
not exploited in this work.
Previous studies in unsupervised methods for parsing
have concentrated on the use of inside-outside algorithm
(Lari and Young, 1990; Carroll and Rooth, 1998). How-
ever, there are several limitations of the inside-outside al-
gorithm for unsupervised parsing, see (Marcken, 1995)
for some experiments that draw out the mismatch be-
tween minimizing error rate and iteratively increasing the
likelihood of the corpus. Other approaches have tried to
move away from phrase structural representations into
dependency style parsing (Lafferty et al, 1992; Fong and
Wu, 1996). However, there are still inherent computa-
tional limitations due to the vast search space (see (Pietra
et al, 1994) for discussion). None of these approaches
can even be realistically compared to supervised parsers
that are trained and tested on the kind of representations
and the complexity of sentences that are found in the
Penn Treebank.
(Chelba and Jelinek, 1998) combine unlabeled and
labeled data for parsing with a view towards language
modeling applications. The goal in their work is not to
get the right bracketing or dependencies but to reduce the
word error rate in a speech recognizer.
Our approach is closely related to previous Co-
Training methods (Yarowsky, 1995; Blum and Mitchell,
1998; Goldman and Zhou, 2000; Collins and Singer,
1999). (Yarowsky, 1995) first introduced an iterative
method for increasing a small set of seed data used to
disambiguate dual word senses by exploiting the con-
straint that in a segment of discourse only one sense of
a word is used. This use of unlabeled data improved
performance of the disambiguator above that of purely
supervised methods. (Blum and Mitchell, 1998) further
embellish this approach and gave it the name of Co-
Training. Their definition of Co-Training includes the
notion (exploited in this paper) that different models can
constrain each other by exploiting different ?views? of the
data. They also prove some PAC results on learnability.
They also discuss an application of classifying web pages
by using their method of mutually constrained models.
(Collins and Singer, 1999) further extend the use of clas-
sifiers that have mutual constraints by adding terms to
AdaBoost which force the classifiers to agree (called Co-
Boosting). (Goldman and Zhou, 2000) provide a variant
of Co-Training which is suited to the learning of deci-
sion trees where the data is split up into different equiv-
alence classes for each of the models and they use hy-
pothesis testing to determine the agreement between the
models. In future work we would like to experiment
whether some of these ideas could be incorporated into
our model.
In future work we would like to explore use of the en-
tire 1M words of the WSJ Penn Treebank as our labeled
data and to use a larger set of unbracketed WSJ data as
input to the Co-Training algorithm. In addition, we plan
to explore the following points that bear on understand-
ing the nature of the Co-Training learning algorithm:
 The contribution of the dictionary of trees extracted
from the unlabeled set is an issue that we would like
to explore in future experiments. Ideally, we wish to
design a co-training method where no such informa-
tion is used from the unlabeled set.
 The relationship between co-training and EM bears
investigation. (Nigam and Ghani, 2000) is a study
which tries to separate two factors: (1) The gradi-
ent descent aspect of EM vs. the iterative nature
of co-training and (2) The generative model used in
EM vs. the conditional independence between the
features used by the two models that is exploited in
co-training. Also, EM has been used successfully
in text classification in combination of labeled and
unlabeled data (see (Nigam et al, 1999)).
 In our experiments, unlike (Blum and Mitchell,
1998) we do not balance the label priors when pick-
ing new labeled examples for addition to the train-
ing data. One way to incorporate this into our algo-
rithm would be to incorporate some form of sample
selection (or active learning) into the selection of
examples that are considered as labeled with high
confidence (Hwa, 2000).
8 Conclusion
In this paper, we proposed a new approach for training
a statistical parser that combines labeled with unlabeled
data. It uses a Co-Training method where a pair of mod-
els attempt to increase their agreement on labeling the
data. The algorithm takes as input a small corpus of
9695 sentences (234467 word tokens) of bracketed data,
a large pool of unlabeled text and a tag dictionary of lexi-
calized structures for each word in this training set (based
on the LTAG formalism). The algorithm presented itera-
tively labels the unlabeled data set with parse trees. We
then train a statistical parser on the combined set of la-
beled and unlabeled data.
We obtained 80.02% and 79.64% labeled bracketing
precision and recall respectively. The baseline model
which was only trained on the 9695 sentences of labeled
data performed at 72.23% and 69.12% precision and re-
call. These results show that training a statistical parser
using our Co-training method to combine labeled and un-
labeled data strongly outperforms training only on the la-
beled data.
It is important to note that unlike previous studies, our
method of moving towards unsupervised parsing can be
directly compared to the output of supervised parsers.
Unlike previous approaches to unsupervised parsing our
method can be trained and tested on the kind of represen-
tations and the complexity of sentences that are found in
the Penn Treebank.
In addition, as a byproduct of our representation we
obtain more than the phrase structure of each sentence.
We also produce a more embellished parse in which phe-
nomena such as predicate-argument structure, subcate-
gorization and movement are given a probabilistic treat-
ment.
References
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Grishman, P. Har-
rison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman,
M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991.
A procedure for quantitatively comparing the syntactic coverage of
english grammars. In Proc. DARPA Speech and Natural Language
Workshop, pages 306?311. Morgan Kaufmann.
A. Blum and T. Mitchell. 1998. Combining Labeled and Unlabeled
Data with Co-Training. In Proc. of 11th Annual Conf. on Comp.
Learning Theory (COLT), pages 92?100.
E. Brill. 1997. Unsupervised learning of disambiguation rules for part
of speech tagging. In Natural Language Processing Using Very
Large Corpora. Kluwer Academic Press.
G. Carroll and M. Rooth. 1998. Valence In-
duction with a Head-Lexicalized PCFG.
http://xxx.lanl.gov/abs/cmp-lg/9805001, May.
C. Chelba and F. Jelinek. 1998. Exploiting syntactic structure for lan-
guage modeling. In Proc. of COLING-ACL ?98, pages 225?231,
Montreal.
M. Collins and Y. Singer. 1999. Unsupervised Models for Named En-
tity Classification. In Proc. of WVLC/EMNLP-99, pages 100?110.
D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun. 1992. A practical
part-of-speech tagger. In Proc. of 3rd ANLP Conf., Trento, Italy.
ACL.
D. Elworthy. 1994. Does baum-welch re-estimation help taggers? In
Proc. of 4th ANLP Conf., pages 53?58, Stuttgart, October 13-15.
E. W. Fong and D. Wu. 1996. Learning restricted probabilistic link
grammars. In S. Wermter, E. Riloff, and G. Scheler, editors, Con-
nectionist, Statistical and Symbolic Approaches to Learning for Nat-
ural Language Processing, pages 173?187. Springer-Verlag.
S. Goldman and Y. Zhou. 2000. Enhancing supervised learning with
unlabeled data. In Proc. of ICML?2000, Stanford University, June
29?July 2.
Rebecca Hwa. 2000. Sample selection for statistical grammar induc-
tion. In Proceedings of EMNLP/VLC-2000, pages 45?52.
A. K. Joshi and Y. Schabes. 1992. Tree-adjoining grammar and lex-
icalized grammars. In M. Nivat and A. Podelski, editors, Tree au-
tomata and languages, pages 409?431. Elsevier Science.
A. K. Joshi, L. Levy, and M. Takahashi. 1975. Tree Adjunct Gram-
mars. Journal of Computer and System Sciences.
A. K. Joshi. 1985. Tree Adjoining Grammars: How much context Sen-
sitivity is required to provide a reasonable structural description. In
D. Dowty, I. Karttunen, and A. Zwicky, editors, Natural Language
Parsing, pages 206?250. Cambridge University Press, Cambridge,
U.K.
J. Lafferty, D. Sleator, and D. Temperley. 1992. Grammatical trigrams:
A probabilistic model of link grammar. In Proc. of the AAAI Conf.
on Probabilistic Approaches to Natural Language.
K. Lari and S. J. Young. 1990. The estimation of stochastic context-
free grammars using the Inside-Outside algorithm. Computer
Speech and Language, 4:35?56.
C. de Marcken. 1995. Lexical heads, phrase structure and the induc-
tion of grammar. In D. Yarowsky and K. Church, editors, Proc. of
3rd WVLC, pages 14?26, MIT, Cambridge, MA.
M. Marcus, B. Santorini, and M. Marcinkiewiecz. 1993. Building
a large annotated corpus of english. Computational Linguistics,
19(2):313?330.
B. Merialdo. 1994. Tagging english text with a probabilistic model.
Computational Linguistics, 20(2):155?172.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and
applicability of co-training. In Proc. of Ninth International Confer-
ence on Information and Knowledge (CIKM-2000).
Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom
Mitchell. 1999. Text Classification from Labeled and Unlabeled
Documents using EM. Machine Learning, 1(34).
S. Della Pietra, V. Della Pietra, J. Gillett, J. Lafferty, H. Printz, and
L. Ures?. 1994. Inference and estimation of a long-range trigram
model. In R. Carrasco and J. Oncina, editors, Proc. of ICGI-94.
Springer-Verlag.
A. Ratnaparkhi. 1996. A Maximum Entropy Part-Of-Speech Tagger.
In Proc. of EMNLP-96, University of Pennsylvania.
P. Resnik. 1992. Probabilistic tree-adjoining grammars as a framework
for statistical natural language processing. In Proc. of COLING ?92,
volume 2, pages 418?424, Nantes, France.
Y. Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In
Proc. of COLING ?92, volume 2, pages 426?432, Nantes, France.
B. Srinivas. 1997. Complexity of Lexical Descriptions and its Rele-
vance to Partial Parsing. Ph.D. thesis, Department of Computer
and Information Sciences, University of Pennsylvania.
F. Xia, M. Palmer, and A. Joshi. 2000. A Uniform Method of Grammar
Extraction and its Applications. In Proc. of EMNLP/VLC-2000.
D. Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rival-
ing Supervised Methods. In Proc. 33rd Meeting of the ACL, pages
189?196, Cambridge, MA.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 305?312
Manchester, August 2008
Homotopy-based Semi-Supervised Hidden Markov Models
for Sequence Labeling?
Gholamreza Haffari and Anoop Sarkar
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
{ghaffar1,anoop}@cs.sfu.ca
Abstract
This paper explores the use of the homo-
topy method for training a semi-supervised
Hidden Markov Model (HMM) used for
sequence labeling. We provide a novel
polynomial-time algorithm to trace the lo-
cal maximum of the likelihood function
for HMMs from full weight on the la-
beled data to full weight on the unla-
beled data. We present an experimental
analysis of different techniques for choos-
ing the best balance between labeled and
unlabeled data based on the characteris-
tics observed along this path. Further-
more, experimental results on the field seg-
mentation task in information extraction
show that the Homotopy-based method
significantly outperforms EM-based semi-
supervised learning, and provides a more
accurate alternative to the use of held-out
data to pick the best balance for combin-
ing labeled and unlabeled data.
1 Introduction
In semi-supervised learning, given a sample con-
taining both labeled data L and unlabeled data
U , the maximum likelihood estimator ?mle maxi-
mizes:
L(?) :=
?
(x,y)?L
log P (x,y|?)+
?
x?U
log P (x|?)
(1)
where y is a structured output label, e.g. a se-
quence of tags in the part-of-speech tagging task,
or parse trees in the statistical parsing task. When
the number of labeled instances is very small com-
pared to the unlabeled instances, i.e. |L| ? |U |,
? We would like to thank Shihao Ji and the anonymous
reviewers for their comments. This research was supported in
part by NSERC, Canada.
? c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the likelihood of labeled data is dominated by that
of unlabeled data, and the valuable information in
the labeled data is almost completely ignored.
Several studies in the natural language process-
ing (NLP) literature have shown that as the size of
unlabeled data increases, the performance of the
model with ?mle may deteriorate, most notably
in (Merialdo, 1993; Nigam et al, 2000). One strat-
egy commonly used to alleviate this problem is to
explicitly weigh the contribution of labeled and un-
labeled data in (1) by ? ? [0, 1]. This new parame-
ter controls the influence of unlabeled data but is
estimated either by (a) an ad-hoc setting, where
labeled data is given more weight than unlabeled
data, or (b) by using the EM algorithm or (c) by
using a held-out set. But each of these alternatives
is problematic: the ad-hoc strategy does not work
well in general; the EM algorithm ignores the la-
beled data almost entirely; and using held-out data
involves finding a good step size for the search,
but small changes in ? may cause drastic changes
in the estimated parameters and the performance
of the resulting model. Moreover, if labeled data is
scarce, which is usually the case, using a held-out
set wastes a valuable resource1 .
In this paper, we use continuation techniques
(Corduneanu and Jaakkola, 2002) for determining
? for structured prediction tasks involving HMMs,
and more broadly, the product of multinomials
(PoM) model. We provide a polynomial-time al-
gorithm for HMMs to trace the local maxima of
the likelihood function from full weight on the la-
beled data to full weight on the unlabeled data. In
doing so, we introduce dynamic programming al-
gorithms for HMMs that enable the efficient com-
putation over unlabeled data of the covariance be-
tween pairs of state transition counts and pairs
of state-state and state-observation counts. We
present a detailed experimental analysis of differ-
ent techniques for choosing the best balance be-
1Apart from these reasons, we also provide an experimen-
tal comparision between the homotopy based approach, the
EM algorithm, and the use of a held out set.
305
tween labeled and unlabeled data based on the
characteristics observed along this path. Further-
more, experimental results on the field segmen-
tation task in information extraction show that
the Homotopy-based method significantly outper-
forms EM-based semi-supervised learning, and
provides a more accurate alternative to the use of
held-out data to pick the best balance for combin-
ing labeled and unlabeled data. We argue this ap-
proach is a best bet method which is robust to dif-
ferent settings and types of labeled and unlabeled
data combinations.
2 Homotopy Continuation
A continuation method embeds a given hard root
finding problem G(?) = 0 into a family of prob-
lems H(?(?), ?) = 0 parameterized by ? such
that H(?(1), 1) = 0 is the original given problem,
and H(?(0), 0) = 0 is an easy problem F (?) = 0
(Richter and DeCarlo, 1983). We start from a solu-
tion ?
0
for F (?) = 0, and deform it to a solution
?
1
for G(?) = 0 while keeping track of the so-
lutions of the intermediate problems2. A simple
deformation or homotopy function is:
H(?, ?) = (1? ?)F (?) + ?G(?) (2)
There are many ways to define a homotopy map,
but it is not trivial to always guarantee the exis-
tence of a path of solutions for the intermediate
problems. Fortunately for the homotopy map we
will consider in this paper, the path of solutions
which starts from ? = 0 to ? = 1 exists and is
unique.
In order to find the path numerically, we seek
a curve ?(?) which satisfies H(?(?), ?) = 0.
This is found by differentiating with respect to ?
and solving the resulting differential equation. To
handle singularities along the path and to be able
to follow the path beyond them, we introduce a
new variable s (which in our case is the unit path
length) and solve the following differential equa-
tion for (?(s), ?(s)):
?H(?, ?)
??
d?
ds
+
?H(?, ?)
??
d?
ds
= 0 (3)
subject to ||(d?
ds
,
d?
ds
)||
2
= 1 and the initial con-
dition (?(0), ?(0)) = (?
0
, 0). We use the Euler
2This deformation gives us a solution path (?(?), ?)
in Rd+1 for ? ? [0, 1], where each component of the d-
dimensional solution vector ?(?) = (?
1
(?), .., ?
d
(?)) is a
function of ?.
method (see Algorithm 1) to solve (3) but higher
order methods such as Runge-Kutta of order 2 or 3
can also be used.
3 Homotopy-based Parameter Estimation
One way to control the contribution of the labeled
and unlabeled data is to parameterize the log like-
lihood function as L
?
(?) defined by
1? ?
|L|
?
(x,y)?L
log P (x, y|?) +
?
|U |
?
x?U
log P (x|?)
How do we choose the best ?? An operator called
EM
?
is used with the property that its fixed points
(locally) maximize L
?
(?). Starting from a fixed
point of EM
?
when ? is zero3, the path of fixed
point of this operator is followed for ? > 0 by
continuation techniques. Finally the best value for
? is chosen based on the characteristics observed
along the path. One option is to choose an allo-
cation value where the first critical4 point occurs
had we followed the path based on ?, i.e. without
introducing s (see Sec. 2). Beyond the first criti-
cal point, the fixed points may not have their roots
in the starting point which has all the informa-
tion from labeled data (Corduneanu and Jaakkola,
2002). Alternatively, an allocation may be cho-
sen which corresponds to the model that gives the
maximum entropy for label distributions of unla-
beled instances (Ji et al, 2007). In our experi-
ments, we compare all of these methods for de-
termining the choice of ?.
3.1 Product of Multinomials Model
Product of Multinomials (PoM) model is an im-
portant class of probabilistic models especially for
NLP which includes HMMs and PCFGs among
others (Collins, 2005). In the PoM model, the
probability of a pair (x,y) is
P (x,y|?) =
M
?
m=1
?
???
m
?
m
(?)
Count(x,y,?) (4)
where Count(x,y, ?) shows how many times an
outcome ? ? ?
m
has been seen in the input-output
pair (x,y), and M is the total number of multino-
mials. A multinomial distribution parameterized
3In general, EM
0
can have multiple local maxima, but in
our case, EM
0
has only one global maximum, found analyti-
cally using relative frequency estimation.
4A critical point is where a discontinuity or bifurcation oc-
curs. In our setting, almost all of the critical points correspond
to discontinuities (Corduneanu, 2002).
306
by ?
m
is put on each discrete space ?
m
where the
probability of an outcome ? is denoted by ?
m
(?).
So for each space ?
m
, we have
?
???
m
?
m
(?) =
1.
Consider an HMM with K states. There are
three types of parameters: (i) initial state probabili-
ties P (s) which is a multinomial over states ?
0
(s),
(ii) state transition probabilities P (s?|s) which are
K multinomials over states ?
s
(s
?
) , and (iii) emis-
sion probabilities P (a|s) which are K multinomi-
als over observation alphabet ?
s+K
(a). To com-
pute the probability of a pair (x,y), normally we
go through the sequence and multiply the proba-
bility of the seen state-state and state-observation
events:
P (x,y|?) = ?
0
(y
0
)?
y
1
+K
(x
1
)
|y|
Y
t=2
?
y
t?1
(y
t
)?
y
t
+K
(x
t
)
which is in the form of (4) if it is written in terms
of the multinomials involved.
3.2 EM
?
Operator for the PoM Model
Usually EM is used to maximize L(?) and esti-
mate the model parameters in the situation where
some parts of the training data are hidden. EM has
an intuitive description for the PoM model: start-
ing from an arbitrary value for parameters, itera-
tively update the probability mass of each event
proportional to its count in labeled data plus its ex-
pected count in the unlabeled data, until conver-
gence.
By changing the EM?s update rule, we get an
algorithm for maximizing L
?
(?):
?
?
m
(?) =
1? ?
|L|
?
(x,y)?L
Count(x,y, ?) +
?
|U |
?
x?U
?
y?Y
x
Count(x,y, ?)P (y|x,?
old
) (5)
where ??
m
is the unnormalized parameter vector,
i.e. ?
m
(?) =
?
?
m
(?)
P
???
m
?
?
m
(?)
. The expected counts
can be computed efficiently based on the forward-
backward recurrence for HMMs (Rabiner, 1989)
and inside-outside recurrence for PCFGs (Lari and
Young, 1990). The right hand side of (5) is an op-
erator we call EM
?
which transforms the old pa-
rameter values to their new (unnormalized) values.
EM
0
and EM
1
correspond respectively to purely
supervised and unsupervised parameter estimation
settings, and:
EM
?
(?) = (1? ?)EM
0
(?) + ?EM
1
(?) (6)
3.3 Homotopy for the PoM Model
The iterative maximization algorithm, described in
the previous section, proceeds until it reaches a
fixed point EM
?
(?) =
?
?, where based on (6):
(1? ?) (
?
?? EM
0
(?))
| {z }
F (?)
+? (
?
?? EM
1
(?))
| {z }
G(?)
= 0 (7)
The above condition governs the (local) maxima
of EM
?
. Comparing to (2) we can see that (7) can
be viewed as a homotopy map.
We can generalize (7) by replacing (1? ?) with
a function g
1
(?) and ? with g
2
(?)
5
. This corre-
sponds to other ways of balancing labeled and un-
labeled data log-likelihoods in (1). Moreover, we
may partition the parameter set and use the homo-
topy method to just estimate the parameters in one
partition while keeping the rest of parameters fixed
(to inject some domain knowledge to the estima-
tion procedure), or repeat it through partitions. We
will see this in Sec. 5.2 where the transition matrix
of an HMM is frozen and the emission probabili-
ties are learned with the continuation method.
Algorithm 1 describes how to use continuation
techniques used for homotopy maps in order to
trace the path of fixed points for the EM
?
oper-
ator. The algorithm uses the Euler method to solve
the following differential equation governing the
fixed points of EM
?
:
[
??
?
?
EM
1
(?)? I EM
1
(?)? EM
0
]
[
d
?
?
d?
]
= 0
For PoM models?
?
?
EM
1
(?) can be written com-
pactly as follows6:
1
|U |
?
x?U
COV
P (y|x,?)
[
Count(x,y)
]
?H (8)
where COV
P (y|x,?)
[Count(x,y)] is the con-
ditional covariance matrix of all features
Count(x,y, ?) given an unlabeled instance
x. We denote the entry corresponding to events ?
1
and ?
2
of this matrix by COV
P (y|x,?)
(?
1
, ?
2
); H
is a block diagonal matrix built from H
?
i
where
H
?
i
= (
?
?
i
(?
1
), ..,
?
?
i
(?
|?
i
|
)) ? I?
1
|?
i
|?|?
i
|
?
???
i
?
?
i
(?)
5However the following two conditions must be satisfied:
(i) the deformation map is reduced to ( ???EM
0
(?)) at ? =
0 and ( ???EM
1
(?)) at ? = 1, and (ii) the path of solutions
exists for Eqn. (2).
6A full derivation is provided in (Haffari and Sarkar, 2008)
307
Algorithm 1 Homotopy Continuation for EM
?
1: Input: Labeled data set L
2: Input: Unlabeled data set U
3: Input: Step size ?
4: Initialize [ ?? ?] = [EM
0
0] based on L
5: ?old ? [0 1]
6: repeat
7: Compute ?
?
?
EM
1
(?) and EM
1
(?) based
on unlabeled data U
8: Compute ? = [d?? d?] as the kernel of
[??
?
?
EM
1
(?)? I EM
1
(?)? EM
0
]
9: if ? ? ?old < 0 then
10: ? ? ??
11: end if
12: [ ?? ?]? [ ?? ?] + ? ?
||?||
2
13: ?old ? ?
14: until ? ? 1
Computing the covariance matrix in (8) is a
challenging problem because it consists of sum-
ming quantities over all possible structures Y
x
as-
sociated with each unlabeled instance x, which is
exponential in the size of the input for HMMs.
4 Efficient Computation of the Covari-
ance Matrix
The entry COV
P (y|x,?)
(?
1
, ?
2
) of the features co-
variance matrix is
E[Count(x,y, ?
1
)Count(x,y, ?
2
)]?
E[Count(x,y, ?
1
)]E[Count(x,y, ?
2
)]
where the expectations are taken under P (y|x,?).
To efficiently calculate the covariance, we need
to be able to efficiently compute the expectations.
The linear count expectations can be computed ef-
ficiently by the forward-backward recurrence for
HMMs. However, we have to design new algo-
rithms for quadratic count expectations which will
be done in the rest of this section.
We add a special begin symbol to the se-
quences and replace the initial probabilities with
P (s|begin). Based on the terminology used in (4),
the outcomes belong to two categories: ? = (s, s?)
where state s? follows state s, and ? = (s, a)
where symbol a is emitted from state s. De-
fine the feature function f
?
(x,y, t) to be 1 if the
outcome ? happens at time step t, and 0 other-
wise. Based on the fact that Count(x,y, ?) =
?
|x|
t=1
f
?
(x,y, t), we have
E[Count(x,y, ?
1
)Count(x,y, ?
2
)] =
?
t
1
?
t
2
?
y?Y
x
f
?
1
(x,y, t
1
)f
?
2
(x,y, t
2
)P (y|x,?)
which is the summation of |x|2 different expecta-
tions. Fixing two positions t
1
and t
2
, each expec-
tation is the probability (over all possible labels) of
observing ?
1
and ?
2
at these two positions respec-
tively, which can be efficiently computed using the
following data structure. Prepare an auxiliary table
Z
x containing P (x
[i+1,j]
, s
i
, s
j
), for every pair of
states s
i
and s
j
for all positions i, j (i ? j):
Z
x
i,j
(s
i
, s
j
) =
X
s
i+1
,..,s
j?1
j?1
Y
k=i
P (s
k+1
|s
k
)P (x
k+1
|s
k+1
)
Let matrix Mx
k
= [M
x
k
(s, s
?
)] where Mx
k
(s, s
?
) =
P (s
?
|s)P (x
k
|s
?
); then Zx
i,j
=
?
j?1
k=i
M
x
k
. Forward
and backward probabilities can also be computed
from Zx, so building this table helps to compute
both linear and quadratic count expectations.
With this table, computing the quadratic counts
is straightforward. When both events are of type
state-observation, i.e. ? = (s, a) and ?? = (s?, a?),
their expected quadratic count can be computed as
?
t
1
?
t
2
?
x
t
1
,a
?
x
t
2
,a
?
[
?
k
P (k|begin)Zx
1,t
1
(k, s).
Z
x
t
1
,t
2
(s, s
?
).
?
k
Z
x
t
2
,n
(s
?
, k)P (end|k)
]
where ?
x
t
,a
is 1 if x
t
is equal to a and 0 otherwise.
Likewise we can compute the expected quadratic
counts for other combination of events: (i) both
are of type state-state, (ii) one is of type state-state
and the other state-observation.
There are L(L+1)
2
tables needed for a sequence
of length L, and the time complexity of building
each of them is O(K3) where K is the number of
states in the HMM. When computing the covari-
ance matrix, the observations are fixed and there
is no need to consider all possible combinations
of observations and states. The most expensive
part of the computation is the situation where the
two events are of type state-state which amounts
to O(L2K4) matrix updates. Noting that a single
entry needs O(K) for its updating, the time com-
plexity of computing expected quadratic counts for
a single sequence is O(L2K5). The space needed
to store the auxiliary tables is O(L2K2) and the
space needed for covariance matrix is O((K2 +
NK)
2
) where N is the alphabet size.
5 Experimental Results
In the field segmentation task, a document is con-
sidered to be a sequence of fields. The goal is
308
[EDITOR A. Elmagarmid, editor.] [TITLE Transaction Models for Advanced Database Applications] [PUBLISHER Morgan-
Kaufmann,] [DATE 1992.]
Figure 1: A field segmentation example for Citations dataset.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.12
0.13
0.14
0.15
0.16
0.17
0.18
?
Er
ro
r (
pe
r p
os
itio
n)
EM?2
freez
 Error on Citation Test (300L5000U)
 
 
Viterbi Decoding
SMS Decoding
?MLE
(a)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.12
0.13
0.14
0.15
0.16
0.17
0.18
0.19
0.2
0.21
?
Er
ro
r (
pe
r p
os
itio
n)
EM?
freez
 Error on Citation Test (300L5000U)
 
 
Viterbi Decoding
SMS Decoding
?MLE
(b)
Figure 2: EM
?
error rates while increasing the allocation from 0 to 1 by the step size 0.025.
to segment the document into fields, and to label
each field. In our experiments we use the bibli-
ographic citation dataset described in (Peng and
McCallum, 2004) (see Fig. 1 for an example of
the input and expected label output for this task).
This dataset has 500 annotated citations with 13
fields; 5000 unannotated citations were added to
it later by (Grenager et al, 2005). The annotated
data is split into a 300-document training set, a
100-document development (dev) set, and a 100-
document test set7.
We use a first order HMM with the size of hid-
den states equal to the number of fields (equal to
13). We freeze the transition probabilities to what
has been observed in the labeled data and only
learn the emission probabilities. The transition
probabilities are kept frozen due to the nature of
this task in which the transition information can
be learned with very little labeled data, e.g. first
start with ?author? then move to ?title? and so on.
However, the challenging aspect of this dataset is
to find the segment spans for each field, which de-
pends on learning the emission probabilities, based
on the fixed transition probabilities.
At test time, we use both Viterbi (most probable
sequence of states) decoding and sequence of most
probable states decoding methods, and abbreviate
them by Viterbi and SMS respectively. We report
results in terms of precision, recall and F-measure
for finding the citation fields, as well as accuracy
calculated per position, i.e. the ratio of the words
labeled correctly for sequences to all of the words.
The segment-based precision and recall scores are,
7From http://www.stanford.edu/grenager/data/unsupie.tgz
of course, lower than the accuracy computed on
the per-token basis. However, both these numbers
need to be taken into account in order to under-
stand performance in the field segmentation task.
Each input word sequence in this task is very long
(with an average length of 36.7) but the number of
fields to be recovered is a small number compar-
atively (on average there are 5.4 field segments in
a sentence where the average length of a segment
is 6.8). Even a few one-word mistakes in finding
the full segment span leads to a drastic fall in pre-
cision and recall. The situation is quite different
from part-of-speech tagging, or even noun-phrase
chunking using sequence learning methods. Thus,
for this task both the per-token accuracy as well as
the segment precision and recall are equally impor-
tant in gauging performance.
Smoothing to remove zero components in the
starting point is crucial otherwise these features do
not generalize well and yet we know that they have
been observed in the unlabeled data. We use a sim-
ple add-? smoothing, where ? is .2 for transition
table entries and .05 for the emission table entries.
In all experiments, we deal with unknown words in
test data by replacing words seen less than 5 times
in training by the unknown word token.
5.1 Problems with MLE
MLE chooses to set ? = |U |
|L|+|U |
which almost
ignores labeled data information and puts all the
weight on the unlabeled data8. To see this empir-
ically, we show the per position error rates at dif-
8One anonymous reviewer suggests using ? = |L|
|L|+|U|
but the ?best bet? for different tasks that we mention in the
Introduction may not necessarily be a small ? value.
309
ferent source allocation for HMMs trained on 300
labeled and 5000 unlabeled sequences for the Ci-
tation dataset in Fig. 2(a). For each allocation we
have run EM
?
algorithm, initialized to smoothed
counts from labeled data, until convergence. As
the plots show, initially the error decreases as ? in-
creases; however, it starts to increase after ? passes
a certain value. MLE has higher error rates com-
pared to complete data estimate, and its perfor-
mance is far from the best way of combining la-
beled and unlabeled data.
In Fig. 2(b), we have done similar experiment
with the difference that for each value of ?, the
starting point of the EM
?
is the final solution
found in the previous value of ?. As seen in the
plot, the intermediate local optima have better per-
formance compared to the previous experiment,
but still the imbalance between labeled and unla-
beled data negatively affects the quality of the so-
lutions compared to the purely supervised solution.
The likelihood surface is non-convex and has
many local maxima. Here EM performs hill climb-
ing on the likelihood surface, and arguably the re-
sulting (locally optimal) model may not reflect the
quality of the globally optimal MLE. But we con-
jecture that even the MLE model(s) which globally
maximize the likelihood may suffer from the prob-
lem of the size imbalance between labeled and un-
labeled data, since what matters is the influence
of unlabeled data on the likelihood. (Chang et.
al., 2007) also report on using hard-EM on these
datasets9 in which the performance degrades com-
pared to the purely supervised model.
5.2 Choosing ? in Homotopy-based HMM
We analyze different criteria in picking the best
value of ? based on inspection of the continuation
path. The following criteria are considered:
? monotone: The first iteration in which the
monotonicity of the path is changed, or equiva-
lently the first iteration in which the determinant
of ??
?
?
EM
1
(?)?I in Algorithm 1 becomes zero
(Corduneanu and Jaakkola, 2002).
? minEig: Instead of looking into the determinant
of the above matrix, consider its minimum eigen-
value. Across all iterations, choose the one for
which this minimum eigenvalue is the lowest.
?maxEnt: Choose the iteration whose model puts
the maximum entropy on the labeling distribution
for unlabeled data (Ji et al, 2007).
9In Hard-EM, the probability mass is fully assigned to the
most probable label, instead of all possible labels.
The second criterion is new, and experimentally
has shown a good performance; it indicates the
amount of singularity of a matrix.
100 150 200 250 300 350 400 450 500
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
# Unlabeled Sequences
?
Best Selected Allocations
 
 
monton
maxEnt
minEig
EM
Figure 3: ? values picked by different methods.
The size of the labeled data is fixed to 100, and
results are averaged over 4 runs. The ? values
picked by MaxEnt method for 500 unlabeled ex-
amples was .008.
We fix 100 labeled sequences and vary the num-
ber of unlabeled sequences from 100 to 500 by a
step of 50. All of the experiments are repeated
four times with different randomly chosen unla-
beled datasets, and the results are the average over
four runs. The chosen allocations based on the de-
scribed criteria are plotted in Figure 3, and their
associated performance measures can be seen in
Figure 4.
Figure 3 shows that as the unlabeled data set
grows, the reliance of ?minEig? and ?monotone?
methods on unlabeled data decreases whereas in
EM it increases. The ?minEig? method is more
conservative than ?monotone? in that it usually
chooses smaller ? values. The plots in Fig-
ure 4 show that homotopy-based HMM always
outperforms EM-based HMM. Moreover, ?max-
Ent? method outperforms other ways of picking
?. However, as the size of the unlabeled data in-
creases, the three methods tend to have similar per-
formances.
5.3 Homotopy v.s. other methods
In the second set of experiments, we compare
the performance of the homotopy based method
against the competitive methods for picking the
value of ?.
We use all of the labeled sequences (size is 300)
and vary the number of unlabeled sequences from
300 to 1000 by the step size of 100. For the first
competitive method, 100 labeled sequences are put
in a held out set and used to select the best value
310
100 200 300 400 500
0.2
0.25
F?measure
# Unlabeled Sequences
100 200 300 400 500
0.8
0.82
0.84
Total accuracy
# Unlabeled Sequences
100 200 300 400 500
0.1
0.15
0.2
0.25
0.3
F?measure
100 200 300 400 500
0.8
0.82
0.84
Total accuracy
 
 
monotone maxEnt minEig EM
Figure 4: The comparison of different techniques
for choosing the best allocation based on datasets
with 100 labeled sequences and varying number of
unlabeled sequences. Each figure shows the av-
erage over 4 runs. F-measure is calculated based
on the segments, and total accuracy is calculated
based on tokens in individual positions. The two
plots in the top represent Viterbi decoding, and the
two plots in the bottom represent SMS decoding.
of ? based on brute-force search using a fixed step
size; afterwards, this value is used to train HMM
(based on 200 remaining labeled sequences and
unlabeled data). The second competitive method,
which we call ?Oracle?, is similar to the previous
method except we use the test set as the held out set
and all of the 300 labeled sequences as the train-
ing set. In a sense, the resulting model is the best
we can expect from cross validation based on the
knowledge of true labels for the test set. Despite
the name ?Oracle?, in this setting the ? value is se-
lected based on the log-likelihood criterion, so it is
possible that the ?Oracle? method is outperformed
by another method in terms of precision/recall/f-
score. Finally, EM is considered as the third base-
line.
The results are summarized in Table 1. When
decoding based on SMS, the homotopy-based
HMM outperforms the ?Held-out? method for all
of performance measures, and generally behaves
better than the ?Oracle? method. When decoding
based on Viterbi, the accuracy of the homotopy-
based HMM is better than ?Held-out? and is in
the same range as the ?Oracle?; the three meth-
ods have roughly the same f-score. The ? value
found by Homotopy gives a small weight to unla-
beled data, and so it might seem that it is ignoring
the unlabeled data. This is not the case, even with
a small weight the unlabeled data has an impact,
as can be seen in the comparison with the purely
Supervised baseline in Table 1 where the Homo-
topy method outperforms the Supervised baseline
by more than 3.5 points of f-score with SMS-
decoding. Homotopy-based HMM with SMS-
decoding outperforms all of the other methods.
We noticed that accuracy was better for 700 un-
labeled examples in this dataset, and so we include
those results as well in Table 1. We observed some
noise in unlabeled sequences; so as the size of the
unlabeled data set grows, this noise increases as
well. In addition to finding the right balance be-
tween labeled and unlabeled data, this is another
factor in semi-supervised learning. For each par-
ticular unlabeled dataset size (we experimented us-
ing 300 to 1000 unlabeled data with a step size of
100) the Homotopy method outperforms the other
alternatives.
6 Related Previous Work
Homotopy based parameter estimation was orig-
inally proposed in (Corduneanu and Jaakkola,
2002) for Na??ve Bayes models and mixture of
Gaussians, and (Ji et al, 2007) used it for HMM-
based sequence classification which means that an
input sequence x is classified into a class label
y ? {1, . . . , k} (the class label is not structured,
i.e. not a sequence of tags). The classification is
done using a collection of k HMMs by computing
Pr(x, y | ?
y
) which sums over all states in each
HMM ?y for input x. The algorithms in (Ji et al,
2007) could be adapted to the task of sequence la-
beling, but we argue that our algorithms provide a
straightforward and direct solution.
There have been some studies using the Cita-
tion dataset, but it is not easy to directly compare
their results due to differences in preprocessing,
the amount of the previous knowledge and rich
features used by the models, and the training data
which were used. (Chang et. al., 2007) used a first
order HMM in order to investigate injecting prior
domain knowledge to self-training style bootstrap-
ping by encoding human knowledge into declara-
tive constraints. (Grenager et al, 2005) used a first
order HMM which has a diagonal transition matrix
and a specialized boundary model. In both works,
the number of randomly selected labeled and un-
labeled training data is varied, which makes a di-
311
size of ? Viterbi decoding SMS decoding
unlab data p, r, f-score accuracy p, r, f-score accuracy
Homotopy 700 .004 .292, .290, .290 87.1% .321, .332, .326 89%1000 .004 .292, .291, .291 87.9% .296, .298, .296 88.6%
Held-out 700 .220 .311, .291, .297 87.1% .295, .288, .289 87.2%1000 .320 .300, .276, .283 86.9% .308, .281, .287 87.2%
Oracle 700 .150 .284, .293, .287 87.8% .295, .313, .303 88%1000 .200 .285, .294, .289 87.9% .277, .292, .284 88.7%
EM 700 .700 .213, .211, .211 84.8% .213, .220, .216 85.2%1000 .770 .199, .198, .198 83.7% .187, .198, .192 83.6%
Supervised 0 0 .281, .278, .279 87% .298, .280, .288 88.4%
Table 1: Results using entire labeled data with segment precision/recall/f-score and token based accuracy.
rect numerical comparison impossible. (Peng and
McCallum, 2004) used only labeled data to train
conditional random fields and HMMs with second
order state transitions where they allow observa-
tion in each position to depend on the current state
as well as observation of the previous position.
7 Conclusion
In many NLP tasks, the addition of unlabeled data
to labeled data can decrease the performance on
that task. This is often because the unlabeled data
can overwhelm the information obtained from the
labeled data. In this paper, we have described a
methodology and provided efficient algorithms for
an approach that attempts to ensure that unlabeled
data does not hurt performance. The experimen-
tal results show that homotopy-based training per-
forms better than other commonly used compet-
itive methods. We plan to explore faster ways
for computing the (approximate) covariance ma-
trix, e.g., label sequences can be sampled from
P (y|x,?) and an approximation of the covari-
ance matrix can be computed based on these sam-
ples. Also, it is possible to compute the covariance
matrix in polynomial-time for labels which have
richer interdependencies such as those generated
by a context free grammars (Haffari and Sarkar,
2008). Finally, in Algorithm 1 we used a fixed
step size; the number of iterations in the homo-
topy path following can be reduced greatly with
adaptive step size methods (Allgower and Georg,
1993).
References
E. L. Allgower, K. Georg 1993. Continuation and Path
Following, Acta Numerica, 2:1-64.
M. Chang and L. Ratinov and D. Roth. 2007. Guiding
Semi-Supervision with Constraint-Driven Learning,
ACL 2007.
M. Collins 2005. Notes on the EM Algorithm, NLP
course notes, MIT.
A. Corduneanu. 2002. Stable Mixing of Complete and
Incomplete Information, Masters Thesis, MIT.
A. Corduneanu and T. Jaakkola. 2002. Continuation
Methods for Mixing Heterogeneous Sources, UAI
2002.
T. Grenager, D. Klein, and C. Manning. 2005. Unsu-
pervised Learning of Field Segmentation Models for
Information Extraction, ACL 2005.
G. Haffari and A. Sarkar. 2008. A Continuation
Method for Semi-supervised Learning in Product
of Multinomials Models, Technical Report. Simon
Fraser University. School of Computing Science.
K. Lari, and S. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm, Computer Speech and Language
(4).
S. Ji, L. Watson and L. Carin. 2007. Semi-Supervised
Learning of Hidden Markov Models via a Homotopy
Method, manuscript.
B. Merialdo. 1993. Tagging English text with a proba-
bilistic model, Computational Linguistics
K. Nigam, A. McCallum, S. Thrun and T. Mitchell.
2000. Text Classification from Labeled and Unla-
beled Documents using EM, Machine Learning, 39.
p. 103-134.
F. Peng and A. McCallum. 2004. Accurate Information
Extraction from Research Papers using Conditional
Random Fields, HLT-NAACL 2004.
L. Rabiner. 1989. A Tutorial on Hidden Markov Mod-
els and Selected Applications in Speech Recogni-
tion, Proc. of the IEEE, 77(2).
S. Richter, and R. DeCarlo. 1983. Continuation meth-
ods: Theory and applications, IEEE Trans. on Auto-
matic Control, Vol 26, issue 6.
312
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 590?599, Prague, June 2007. c?2007 Association for Computational Linguistics
Experimental Evaluation of LTAG-based Features
for Semantic Role Labeling
Yudong Liu and Anoop Sarkar
Simon Fraser University
Burnaby, BC, Canada
{yudongl,anoop}@cs.sfu.ca
Abstract
This paper proposes the use of Lexical-
ized Tree-Adjoining Grammar (LTAG) for-
malism as an important additional source
of features for the Semantic Role Labeling
(SRL) task. Using a set of one-vs-all Sup-
port Vector Machines (SVMs), we evalu-
ate these LTAG-based features. Our exper-
iments show that LTAG-based features can
improve SRL accuracy significantly. When
compared with the best known set of fea-
tures that are used in state of the art SRL sys-
tems we obtain an improvement in F-score
from 82.34% to 85.25%.
1 Introduction
Semantic Role Labeling (SRL) aims to identify and
label all the arguments for each predicate occurring
in a sentence. It involves identifying constituents in
the sentence that represent the predicate?s arguments
and assigning pre-specified semantic roles to them.
[A0seller Ports of Call Inc.] reached agreements to
[Vverb sell] [A1thing its remaining seven aircraft]
[A2buyer to buyers that weren?t disclosed] .
is an example of SRL annotation from the PropBank
corpus (Palmer et al, 2005), where the subscripted
information maps the semantic roles A0, A1, A2
to arguments for the predicate sell as defined in the
PropBank Frame Scheme. For SRL, high accuracy
has been achieved by:
(i) proposing new types of features (see Table 1 in
Section 3 for previously proposed features),
(ii) modeling the predicate frameset by capturing de-
pendencies between arguments (Gildea and Juraf-
sky, 2002; Pradhan et al, 2004; Toutanova et al,
2005; Punyakanok et al, 2005a),
(iii) dealing with incorrect parser output by using
more than one parser (Pradhan et al, 2005b).
Our work in this paper falls into category (i). We
propose several novel features based on Lexicalized
Tree Adjoining Grammar (LTAG) derivation trees
in order to improve SRL performance. To show
the usefulness of these features, we provide an ex-
perimental study comparing LTAG-based features
with the standard set of features and kernel meth-
ods used in state-of-the-art SRL systems. The LTAG
formalism provides an extended domain of locality
in which to specify predicate-argument relationships
and also provides the notion of a derivation tree.
These two properties of LTAG make it well suited
to address the SRL task.
SRL feature extraction has relied on various syn-
tactic representations of input sentences, such as
syntactic chunks (Hacioglu et al, 2004) and full
syntactic parses (Gildea and Jurafsky, 2002). In
contrast with features from shallow parsing, previ-
ous work (Gildea and Palmer, 2002; Punyakanok et
al., 2005b) has shown the necessity of full syntactic
parsing for SRL. In order to generalize the path fea-
ture (see Table 1 in Section 3) which is probably the
most salient (while being the most data sparse) fea-
ture for SRL, previous work has extracted features
from other syntactic representations, such as CCG
derivations (Gildea and Hockenmaier, 2003) and de-
pendency trees (Hacioglu, 2004) or integrated fea-
tures from different parsers (Pradhan et al, 2005b).
To avoid explicit feature engineering on trees, (Mos-
chitti, 2004) used convolution kernels on selective
portions of syntactic trees. In this paper, we also
compare our work with tree kernel based methods.
Most SRL systems exploit syntactic trees as the
main source of features. We would like to take this
one step further and show that using LTAG deriva-
590
SNP VP
MD(will) VP
V(join) PP
S
VP
V(join)
VP
MD(will) VP?
S
VP
MD(will)
VP
V(join)?1(join)
NP ?1(will) PP
?2(will)
NP ?3(join)
PP
?1: ?1:
?2: ?3:
?1: ?2:
Figure 1: A parse tree schematic, and two plausible
LTAG derivation trees for it: derivation tree ?1 uses
elementary trees ?1 and ?1 while ?2 uses ?2 and ?3.
tion trees as an additional source of features can im-
prove both argument identification and classification
accuracy in SRL.
2 Using LTAG-based Features in SRL
We assume some familiarity with Lexicalized Tree-
Adjoining Grammar (LTAG); (Joshi and Schabes,
1997) is a good introduction to this formalism. A
LTAG is defined to be a set of lexicalized elementary
trees (etree for short), of which there are two types,
initial trees and auxiliary trees. Typically etrees
can be composed through two operations into parse
trees, substitution and adjunction. We use sister ad-
junction which is commonly used in LTAG statisti-
cal parsers to deal with the relatively flat Penn Tree-
bank trees (Chiang, 2000). The tree produced by
composing the etrees is the derived/parse tree and
the tree that records the history of composition is the
derivation tree.
A reasonable way to define SRL features is to pro-
vide a strictly local dependency (i.e. within a sin-
gle etree) between predicate and argument. There
have been many different proposals on how to main-
tain syntactic locality (Xia, 1999; Chen and Vijay-
Shanker, 2000) and SRL locality (Chen and Ram-
bow, 2003; Shen and Joshi, 2005) when extract-
ing LTAG etrees from a Treebank. These proposed
methods are exemplified by the derivation tree ?1 in
Fig. 1. However, in most cases they can only provide
a local dependency between predicate and argument
for 87% of the argument constituents (Chen and
Rambow, 2003), which is too low to provide high
SRL accuracy. In LTAG-based statistical parsers,
high accuracy is obtained by using the Magerman-
Collins head-percolation rules in order to provide
the etrees (Chiang, 2000). This method is exem-
plified by the derivation tree ?2 in Fig. 1. Compar-
ing ?1 with ?2 in Fig. 1 and assuming that join is
the predicate and the NP is the potential argument,
the path feature as defined over the LTAG deriva-
tion tree ?2 is more useful for the SRL task as it dis-
tinguishes between main clause and non-finite em-
bedded clause predicates. This alternative derivation
tree also exploits the so-called extended domain of
locality (Joshi and Schabes, 1997) (the examples in
Section 2.1 show this clearly). In this paper, we cru-
cially rely on features defined on LTAG derivation
trees of the latter kind. We use polynomial kernels
to create combinations of features defined on LTAG
derivation trees.
2.1 LTAG-based Feature Extraction
In order to create training data for the LTAG-based
features, we convert the Penn Treebank phrase struc-
ture trees into LTAG derivations. First, we prune the
Treebank parse tree using certain constraints. Then
we decompose the pruned parse trees into a set of
LTAG elementary trees and obtain a derivation tree.
For each constituent in question, we extract features
from the LTAG derivation tree. We combine these
features with the standard features used for SRL
and train an SVM classifier on the combined LTAG
derivation plus SRL annotations from the PropBank
corpus.
For the test data, we report on results using the
gold-standard Treebank data, and in addition we also
report results on automatically parsed data using the
Charniak parser (Charniak, 2000) as provided by the
CoNLL 2005 shared task. We did this for three rea-
sons: (i) our results are directly comparable to those
who have used the Charniak parses distributed with
the CoNLL 2005 data-set; (ii) we avoid the possi-
bility of a better parser identifying a larger number
of argument constituents and thus leading to bet-
ter results, which is orthogonal to the discrimina-
tive power of our proposed LTAG-based features;
and (iii) the quality of LTAG derivation trees de-
pends indirectly on the quality of head dependen-
cies recovered by the parser and it is a well-known
folklore result (see Table 3 in (McDonald et al,
591
2005)) that applying the head-percolation heuristics
on parser output produces better dependencies when
compared to dependencies directly recovered by the
parser (whether the parser is an LTAG parser or a
lexicalized PCFG parser).
2.1.1 Pruning Parse Trees
Given a parse tree, the pruning component iden-
tifies the predicate in the tree and then only admits
those nodes that are sisters to the path from the pred-
icate to the root. It is commonly used in the SRL
community (cf. (Xue and Palmer, 2004)) and our ex-
periments show that 91% of the SRL targets can be
recovered despite this aggressive pruning. We make
two enhancements to the pruned Propbank tree: we
enrich the sister nodes with head information, a part-
of-speech tag and word pair: ?t, w? and PP nodes are
expanded to include the NP complement of the PP
(including head information). The target SRL node
is still the PP. Figure 2 is a pruned parse tree for a
sentence from the PropBank.
2.1.2 Decompositions of Parse Trees
After pruning, the pruned tree is decom-
posed around the predicate using standard head-
percolation based heuristic rules1 to convert a Tree-
bank tree into an LTAG derivation tree. Figure 3
shows the resulting etrees after decomposition. Fig-
ure 4 is the derivation tree for the entire pruned tree.
Each node in this derivation tree represents an etree
in Figure 3. In our model we make an independence
assumption that each SRL is assigned to each con-
stituent independently, conditional only on the path
from the predicate etree to the argument etree in the
derivation tree. Different etree siblings in the LTAG
derivation tree do not influence each other in our cur-
rent models.
2.1.3 LTAG-based Features
We defined 5 LTAG feature categories: predicate
etree-related features (P for short), argument etree-
related features (A), subcategorization-related fea-
tures (S), topological relation-related features (R),
intermediate etree-related features (I). Since we
consider up to 6 intermediate etrees between the
predicate and the argument etree, we use I-1 to I-6
to represent these 6 intermediate trees respectively.
1using http://www.isi.edu/?chiang/software/treep/treep.html
S
NP
NNP-H
Inc.
VP-H
VBD-H
reached
NP
NNS-H
agreements
S
VP-H
TO-H
to
VP
VB-H
sell
NP
NN-H
aircraft
PP
TO-H
to
NP
NNS-H
buyers
Figure 2: The pruned tree for the sentence ?Ports of
Call Inc. reached agreements to sell its remaining
seven aircraft to buyers that weren?t disclosed.?
VP
VB
sell
NP
NN
aircraft
PP
TO
to
S
VP
TO
to
e0: e1: e2: e3:
NP
NNS
agreements
S
VP
VBD
reached
NP
NNP
Inc.
e4: e5: e6:
Figure 3: Elementary trees after decomposition of
the pruned tree.
Category P: Predicate etree & its variants Pred-
icate etree is an etree with predicate, such as e0 in
Figure 3. This new feature complements the pred-
icate feature in the standard SRL feature set. One
variant is to remove the predicate lemma. Another
variant is a combination of predicate tree w/o predi-
cate lemma&POS and voice. In addition, this variant
combined with predicate lemma comprises another
new feature. In the example, these three variants are
(VP(VB)) and (VP) active and (VP) active sell re-
spectively.
Category A: Argument etree & its variants Anal-
ogous to the predicate etree, the argument etree is an
etree with the target constituent and its head. Similar
592
e5(reached)
e6(Inc.) e4(agreements)
e3(to)
e0(sell)
e1(aircraft) e2(to)
Figure 4: LTAG derivation tree for Figure 2.
to predicate etree related features, argument etree,
argument etree with removal of head word, combi-
nation of argument etree w/o head POS&head word
and head Named Entity (NE) label (if any) are con-
sidered. For example, in Figure 3, these 3 features
for e6 are e6, (NP(NNP)) and (NP) LOC with head
word ?Inc.? having NE label ?LOC?.
Category S: Index of current argument etree in
subcat frame of predicate etree Sub-categorization
is a standard feature that denotes the immediate ex-
pansion of the predicate?s parent. For example, it
is V NP PP for predicate sell in the given sentence.
For argument etree e1 in Figure 3, the index feature
value is 1 since it is the very first element in the (or-
dered) subcat sequence.
Category R:
Relation type between argument etree & predi-
cate etree This feature is a combination of position
and modifying relation. Position is a binary valued
standard feature to describe if the argument is before
or after the predicate in a parse tree. For each argu-
ment etree and intermediate etree, we consider three
types of modifying relations they may have with the
predicate etree: modifying (value 1), modified (value
2) and neither (value 3). From Figure 4, we can see
e1 modifies e0 (predicate tree). So their modifying
relation type value is 1; Combining this value with
the position value, this feature for e1 is ?1 after?.
Attachment point of argument etree This fea-
ture describes where the argument etree is sister-
adjoined/adjoined to the predicate etree, or the other
way around. For e1 in the example, VP in the predi-
cate tree is the attachment point.
Distance This feature is the number of intermediate
etrees between argument etree and predicate etree in
the derivation tree. In Figure 4, the distance from e4
to the predicate etree is 1 since only one intermediate
etree e3 is between them.
Category I:
Intermediate etree related features Intermediate
etrees are those etrees that are located between the
predicate etree and argument etrees. The set of fea-
tures we propose for each intermediate etree is quite
similar to those for argument etrees except we do
not consider the named-entity label for head words
in this case.
Relation type of intermediate etree & predicate
etree.
Attachment point of intermediate etree.
Distance between intermediate etree and predicate
etree.
Up to 6 intermediate etrees are considered and the
category I features are extracted for each of them (if
they exist).
Each etree represents a linguistically meaningful
fragment. The features of relation type, attachment
point as well as the distance characterize the topo-
logical relations among the relevant etrees. In par-
ticular, the attachment point and distance features
can explicitly capture important information hidden
in the standard path feature. The intermediate tree
related features can give richer contextual informa-
tion between predicate tree and argument trees. We
added the subcat index feature to be complemen-
tary to the sub-cat and syntactic frame features in
the standard feature set.
3 Standard Feature Set
Our standard feature set is a combination of features
proposed by (Gildea and Jurafsky, 2002), (Surdeanu
et al, 2003; Pradhan et al, 2004; Pradhan et al,
2005b) and (Xue and Palmer, 2004). All features
listed in Table 1 are used for argument classifica-
tion in our baseline system; and features with aster-
isk are not used for argument identification2. We
compare this baseline SRL system with a system
that includes a combination of these features with
the LTAG-based features. Our baseline uses all fea-
tures that have been used in the state-of-the-art SRL
systems and as our experimental results show, these
standard features do indeed obtain state-of-the-art
2This is a standard idea in the SRL literature: removing fea-
tures more useful for classification, e.g. named entity features,
makes the classifier for identification more accurate.
593
Table 1: Standard features adopted by a typical SRL
system. Features with asterisk ? are not used for ar-
gument identification.
Basic features from (Gildea and Jurafsky, 2002)
? predicate lemma and voice
? phrase type and head word
? path from phrase to predicate 1
? position: phrase relative to predicate: before or after
? sub-cat records the immediate structure that expands from
predicate?s parent
2
Additional features proposed by (Surdeanu et al 2003;
Pradhan et al, 2004, 2005)
? predicate POS
? head word POS
? first/last word/POS
? POS of word immediately before/after phrase
? path length 1
? LCA(Lowest Common Ancestor) path from phrase to its
lowest common ancestor with predicate
? punctuation immediately before/after phrase?
? path trigrams?: up to 9 are considered
? head word named entity label such as ?PER, ORG,
LOC??
? content word named entity label for PP parent node?
Additional features proposed by (Xue and Palmer, 2004)
? predicate phrase type
? predicate head word
? voice position
? syntactic frame?
1 In Fig. 2 NNS?NP?S?VP?VB is the path from the con-
stituent NNS(agreements) to the predicate VB(sell) and the
path length is 4.
2 This feature is different from the frame feature which usu-
ally refers to all the semantic participants for the particular
predicate.
accuracy on the SRL task. We will show that adding
LTAG-based features can improve the accuracy over
this very strong baseline.
4 Experiments
4.1 Experimental Settings
Training data (PropBank Sections 2-21) and test
data (PropBank Section 23) are taken from CoNLL-
2005 shared task3 All the necessary annotation in-
formation such as predicates, parse trees as well as
Named Entity labels is part of the data. The ar-
3http://www.lsi.upc.es/?srlconll/.
gument set we consider is {A0, A1, A2, A3, A4,
AM} where AM is a generalized annotation of all
adjuncts such as AM-TMP, AM-LOC, etc., where
PropBank function tags like TMP or LOC in AM-
TMP, AM-LOC are ignored (a common setting for
SRL, see (Xue and Palmer, 2004; Moschitti, 2004)).
We chose these labels for our experiments because
they have sufficient training/test data for the per-
formance comparison and provide sufficient counts
for accurate significance testing. However, we also
provide the evaluation result on the test set for full
CoNLL-2005 task (all argument types).
We use SVM-light4 (Joachims, 1999) with a poly-
nomial kernel (degree=3) as our binary classifier for
argument classification. We applied a linear kernel
to argument identification because the training cost
of this phase is extremely computationally expen-
sive. We use 30% of the training samples to fine tune
the regularization parameter c and the loss-function
cost parameter j for both stages of argument identifi-
cation and classification. With parameter validation
experiments, we set c = 0.258 and j = 1 for the ar-
gument identification learner and c = 0.1 and j = 4
for the argument classification learner.
The classification performance is evaluated using
Precision/Recall/F-score (p/r/f) measures. We ex-
tracted all the gold labels of A0-A4 and AM with
the argument constituent index from the original test
data as the ?gold output?. When we evaluate, we
contrast the output of our system with the gold out-
put and calculate the p/r/f for each argument type.
Our evaluation criteria which is based on predict-
ing the SRL for constituents in the parse tree is based
on the evaluation used in (Toutanova et al, 2005).
However, we also predict and evaluate those Prop-
Bank arguments which do not have a corresponding
constituent in the gold parse tree or the automatic
parse tree: the missing constituent case. We also
evaluate discontinuous PropBank arguments using
the notation used in the CoNLL-2005 data-set but
we do not predict them. This is contrast with some
previous studies where the problematic cases have
been usually discarded or the largest constituents in
the parse tree that almost capture the missing con-
stituent cases are picked as being the correct answer.
Note that, in addition to the constituent based evalu-
4http://svmlight.joachims.org/
594
Gold Standard Charniak Parser
std std+ltag std std+ltag
p(%) 95.66 96.79 87.71 89.11
r(%) 94.36 94.59 84.86 85.51
f(%) 95.00 95.68 86.26 87.27?
Table 2: Argument identification results on test data
ation, in Section 4.4 we also provide the evaluation
of our model on the CoNLL-2005 data-set.
Because the main focus of this work is to evaluate
the impact of the LTAG-based features, we did not
consider the frameset or a distribution over the en-
tire argument set or apply any inference/constraints
as a post-processing stage as most current SRL sys-
tems do. We focus our experiments on showing the
value added by introducing LTAG-based features to
the SRL task over and above what is currently used
in SRL research.
4.2 Argument Identification
Table 2 shows results on argument identification (a
binary classification of constituents into argument or
non-argument). To fully evaluate the influence of the
LTAG-based features, we report the identification re-
sults on both Gold Standard parses and on Charniak
parser output (Charniak, 2000)5.
As we can see, after combing the LTAG-based
features with the standard features, F-score in-
creased from 95.00% to 95.68% with Gold-standard
parses; and from 86.26% to 87.27% with the Char-
niak parses (a larger increase). We can see LTAG-
based features help in argument identification for
both cases. This result is better than (Xue and
Palmer, 2004), and better on gold parses com-
pared to (Toutanova et al, 2005; Punyakanok et al,
2005b).
4.3 Argument Classification
Based on the identification results, argument clas-
sification will assign the semantic roles to the ar-
gument candidates. For each argument of A0-A4
and AM, a ?one-vs-all? SVM classifier is trained on
both the standard feature set (std) and the augmented
feature set (std+ltag). Table 3 shows the classifi-
cation results on the Gold-standard parses with the
5We use the parses supplied with the CoNLL-2005 shared
task for reasons of comparison.
gold argument identification; Table 4 and 5 show the
classification results on the Charniak parser with the
gold argument identification and the automatic ar-
gument identification respectively. Scores for multi-
class SRL are calculated based on the total number
of correctly predicted labels, total number of gold
labels and the number of labels in our prediction for
this argument set.
class std(p/r/f)% std+ltag(p/r/f)%
A0
96.69 96.71 96.71 96.77
96.70 96.74
A1
93.82 93.30 97.30 94.87
93.56 96.07
A2
87.05 79.98 92.43 81.42
83.37 86.58
A3
94.44 68.79 97.69 73.41
79.60 83.33
A4
96.55 82.35 94.11 78.43
88.89 85.56
AM
98.41 96.61 98.67 97.88
97.50 98.27
multi- 95.35 93.62 97.15 94.70
class 94.48 95.91
Table 3: Argument classification results on Gold-
standard parses with gold argument boundaries
4.4 Discussion
From the results shown in the tables, we can see that
by adding the LTAG-based features, the overall per-
formance of the systems is improved both for argu-
ment identification and for argument classification.
Table 3 and 4 show that with the gold argu-
ment identification, the classification for each class
in {A0, A1, A2, A3, AM} consistently benefit from
LTAG-based features. Especially for A3, LTAG-
based features lead to more than 3 percent improve-
ment. But for A4 arguments, the performance drops
3 percent in both cases. As we noticed in Table
5, which presents the argument classification results
on Charniak parser output with the automatic ar-
gument identification, the prediction accuracy for
classes A0, A1, A3, A4 and AM is improved, but
drops a little for A2.
In addition, we also evaluated our feature set
on the full CoNLL 2005 shared task. The over-
595
class std(p/r/f)% std+ltag(p/r/f)%
A0
96.04 92.92 96.07 92.92
94.46 94.47
A1
90.64 85.71 94.64 86.67
88.11 90.48
A2
84.46 75.72 89.26 75.22
79.85 81.64
A3
87.50 62.02 87.10 68.35
72.59 76.60
A4
90.00 79.12 90.54 73.62
84.21 81.21
AM
95.14 85.54 96.60 86.51
90.09 91.27
multi- 93.25 86.45 94.71 87.15
class 89.72 90.77
Table 4: Argument classification results on Charniak
parser output with gold argument boundaries
all performance using LTAG features increased from
74.41% to 75.31% in terms of F-score on the full ar-
gument set. Our accuracy is most closely compara-
ble to the 78.63% accuracy achieved on the full task
by (Pradhan et al, 2005a). However, (Pradhan et
al., 2005a) uses some additional information since it
deals with incorrect parser output by using multiple
parsers. The 79.44% accuracy obtained by the top
system in CoNLL 2005 (Punyakanok et al, 2005a)
is not directly comparable since their system used
the more accurate n-best parser output of (Charniak
and Johnson, 2005). In addition their system also
used global inference. Our focus in this paper was
to propose new LTAG features and to evaluate im-
pact of these features on the SRL task.
We also compared our proposed feature set
against predicate/argument features (PAF) proposed
by (Moschitti, 2004). We conducted an experiment
using SVM-light-TK-1.2 toolkit6. The PAF tree ker-
nel is combined with the standard feature vectors by
a linear operator. With settings of Table 5, its multi-
class performance (p/r/f)% is 83.09/80.18/81.61
with linear kernel and 85.36/81.79/83.53 with poly-
nomial kernel (degree=3) over std feature vectors.
6http://ai-nlp.info.uniroma2.it/moschitti/TK1.2-
software/Tree-Kernel.htm
class std(p/r/f)% std+ltag(p/r/f)%
A0
86.50 86.18 88.17 87.70
86.34 87.93?
A1
78.73 83.82 88.78 85.22
81.19 86.97?
A2
85.40 73.93 83.11 75.42
79.25 79.08
A3
85.71 60.76 85.71 68.35
71.11 76.06?
A4
84.52 78.02 89.47 74.72
81.15 81.43
AM
80.47 82.11 83.87 81.54
81.29 82.69?
multi- 81.79 82.90 86.04 84.47
class 82.34 85.25?
Table 5: Argument classification results on Charniak
parser output with automatic argument boundaries
4.5 Significance Testing
To assess the statistical significance of the im-
provements in accuracy we did a two-tailed sig-
nificance test on the results of both Table 2 and
5 where Charniak?s parser outputs were used.
We chose SIGF7, which is an implementation
of a computer-intensive, stratified approximate-
randomization test (Yeh, 2000). The statistical dif-
ference is assessed on SRL identification, classifica-
tion for each class (A0-A4, AM) and the full SRL
task (overall performance). In Table 2 and 5, we la-
beled numbers under std+ltag that are statistically
significantly better from those under std with aster-
isk. The significance tests show that for identifica-
tion and full SRL task, the improvements are statis-
tically significant with p value of 0.013 and 0.0001
at a confidence level of 95%. The significance test
on each class shows that the improvement by adding
LTAG-based features is statistically significant for
class A0, A1, A3 and AM. Even though in Table 5
the performance of A2 appears to be worse it is not
significantly so, and A4 is not significantly better. In
comparison, the performance of PAF did not show
significantly better than std with p value of 0.593 at
the same confidence level of 95%.
7http://www.coli.uni-saarland.de/?pado/sigf/index.html
596
full full?P full?R full?S full?A full?I std
id 90.5 90.6 90.0 90.5 90.5 90.1 89.6
A0 84.5 84.3 84.6 84.5 84.3 83.5 84.2
A1 89.8 90.1 89.4 89.3 89.6 89.3 88.9
A2 84.2 84.2 84.0 83.7 83.6 83.6 84.9
A3 76.7 80.7 75.1 76.0 75.6 76.7 78.6
A4 80.0 83.3 80.0 79.6 80.0 80.0 79.2
AM 82.8 83.3 82.9 82.8 82.6 83.1 82.4
Table 6: Impact of each LTAG feature category (P, R, S, A, I defined in Section 2.1.3) on argument classi-
fication and identification on CoNLL-2005 development set (WSJ Section 24). full denotes the full feature
set, and we use ?? to denote removal of a feature category of type ?. For example, full?P is the feature set
obtained by removing all P category features. std denotes the standard feature set.
5 Analysis of the LTAG-based features
We analyzed the drop in performance when a partic-
ular type of LTAG feature category is removed from
the full set of LTAG features (we use the broad cat-
egories P, R, S, A, I as defined in Section 2.1.3).
Table 6 shows how much performance is lost (or
gained) when a particular type of LTAG feature is
dropped from the full set.
These experiments were done on the development
set from CoNLL-2005 shared task, using the pro-
vided Charniak parses. All the SVM models were
trained using a polynomial kernel with degree 3. It
is clear that the S, A, I category features help in most
cases and P category features hurt in most cases,
including argument identification. It is also worth
noting that the R and I category features help most
for identification. This vindicates the use of LTAG
derivations as a way to generalize long paths in the
parse tree between the predicate and argument. Al-
though it seems LTAG features have negative impact
on prediction of A3 arguments on this development
set, dropping the P category features can actually
improve performance over the standard feature set.
In contrast, for the prediction of A2 arguments, none
of the LTAG feature categories seem to help.
Note that since we use a polynomial kernel in the
full set, we cannot rule out the possibility that a fea-
ture that improves performance when dropped may
still be helpful when combined in a non-linear ker-
nel with features from other categories. However,
this analysis on the development set does indicate
that overall performance may be improved by drop-
ping the P feature category. We plan to examine this
effect in future work.
6 Related Work
There has been some previous work in SRL that uses
LTAG-based decomposition of the parse tree. (Chen
and Rambow, 2003) use LTAG-based decomposi-
tion of parse trees (as is typically done for statis-
tical LTAG parsing) for SRL. Instead of extracting
a typical ?standard? path feature from the derived
tree, (Chen and Rambow, 2003) uses the path within
the elementary tree from the predicate to the con-
stituent argument. Under this frame, they only re-
cover semantic roles for those constituents that are
localized within a single etree for the predicate, ig-
noring cases that occur outside the etree. As stated
in their paper, ?as a consequence, adjunct seman-
tic roles (ARGM?s) are basically absent from our
test corpus?; and around 13% complement seman-
tic roles cannot be found in etrees in the gold parses.
In contrast, we recover all SRLs by exploiting more
general paths in the LTAG derivation tree. A simi-
lar drawback can be found in (Gildea and Hocken-
maier, 2003) where a parse tree pathwas defined in
terms of Combinatory Categorial Grammar (CCG)
types using grammatical relations between predicate
and arguments. The two relations they defined can
only capture 77% arguments in Propbank and they
had to use a standard path feature as a replacement
when the defined relations cannot be found in CCG
derivation trees. In our framework, we use interme-
diate sub-structures from LTAG derivations to cap-
ture these relations instead of bypassing this issue.
597
Compared to (Liu and Sarkar, 2006), we have
used a more sophisticated learning algorithm and a
richer set of syntactic LTAG-based features in this
task. In particular, in this paper we built a strong
baseline system using a standard set of features and
did a thorough comparison between this strong base-
line and our proposed system with LTAG-based fea-
tures. The experiments in (Liu and Sarkar, 2006)
were conducted on gold parses and it failed to show
any improvements after adding LTAG-based fea-
tures. Our experimental results show that LTAG-
based features can help improve the performance of
SRL systems. While (Liu and Sarkar, 2006) propose
some new features for SRL based on LTAG deriva-
tions, we propose several novel features and in ad-
dition they do not show that their features are useful
for SRL.
Our approach shares similar motivations with the
approach in (Shen and Joshi, 2005) which uses Prop-
Bank information to recover an LTAG treebank as if
it were hidden data underlying the Penn Treebank.
However their goal was to extract an LTAG grammar
using PropBank information from the Treebank, and
not the SRL task.
Features extracted from LTAG derivations are dif-
ferent and provide distinct information when com-
pared to predicate-argument features (PAF) or sub-
categorization features (SCF) used in (Moschitti,
2004) or even the later use of argument spanning
trees (AST) in the same framework. The adjunc-
tion operation of LTAG and the extended domain of
locality is not captured by those features as we have
explained in detail in Section 2.
7 Conclusion and Future Work
In this paper we show that LTAG-based features
improve on the best known set of features used in
current SRL prediction systems: the F-score for
argument identification increased from 86.26% to
87.27% and from 82.34% to 85.25% for the SRL
task. The analysis of the impact of each LTAG fea-
ture category shows that the intermediate etrees are
important for the improvement. In future work we
plan to explore the impact that different types of
LTAG derivation trees have on this SRL task, and ex-
plore the use of tree kernels defined over the LTAG
derivation tree. LTAG derivation tree kernels were
previously used for parse re-ranking by (Shen et al,
2003). Our work also provides motivation to do SRL
and LTAG parsing simultaneously.
Acknowledgements
This research was partially supported by NSERC,
Canada (RGPIN: 264905). We would like to thank
Aravind Joshi, Libin Shen, and the anonymous re-
viewers for their comments.
References
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
ACL-2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL-2000.
J. Chen and O. Rambow. 2003. Use of deep linguistic
features for the recognition and labeling of semantic
arguments. In EMNLP-2003.
J. Chen and K. Vijay-Shanker. 2000. Automated Extrac-
tion of TAGs from the Penn Treebank. In Proc. of the
6th International Workshop on Parsing Technologies
(IWPT-2000), Italy.
D. Chiang. 2000. Statistical parsing with an automati-
cally extracted tree adjoining grammars. In ACL-2000.
D. Gildea and J. Hockenmaier. 2003. Identifying se-
mantic roles using combinatory categorial grammar.
In EMNLP-2003.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
58(3):245?288.
D. Gildea and M. Palmer. 2002. The necessity of parsing
for predicate argument recognition. In ACL-2002.
K. Hacioglu, S. Pradhan, W. Ward, J. Martin, and D. Ju-
rafsky. 2004. Semantic role labeling by tagging syn-
tactic chunks. In CoNLL-2004 Shared Task.
K. Hacioglu. 2004. Semantic role labeling using depen-
dency trees. In COLING-2004.
T. Joachims. 1999. Making large-scale svm learning
practical. Advances in Kernel Methods - Support Vec-
tor Machines.
A. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. Handbook of Formal Languages, 3.
Y. Liu and A. Sarkar. 2006. Using LTAG-Based Features
for Semantic Role Labeling. In TAG+8-2006.
598
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line Large-Margin Training of Dependency Parsers. In
ACL-2005.
A. Moschitti. 2004. A study on convolution kernels for
shallow semantic parsing. In ACL-2004.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
S. Pradhan, W. Ward, K. Hacioglu, , J. H. Martin, and
D. Jurafsky. 2004. Shallow Semantic Parsing Using
Support Vector Machines. In HLT-NAACL-2004.
S. Pradhan, K. Hacioglu, W. Ward, J. Martin, and D. Ju-
rafsky. 2005a. Semantic role chunking combin-
ing complementary syntactic views. In CoNLL-2005
Shared Task.
S. Pradhan, W. Ward, K. Hacioglu, , J. H. Martin, and
D. Jurafsky. 2005b. Semantic role labeling using dif-
ferent syntactic views. In ACL-2005.
V. Punyakanok, D. Roth, and W. Yih. 2005a. Gener-
alized inference with multiple semantic role labeling
systems (shared task paper). In CoNLL-2005.
V. Punyakanok, D. Roth, and W. Yih. 2005b. The neces-
sity of syntactic parsing for semantic role labeling. In
IJCAI-2005.
L. Shen and A. Joshi. 2005. Building an LTAG Tree-
bank. Technical Report Technical Report MS-CIS-05-
15,5, CIS Department, University of Pennsylvania.
L. Shen, A. Sarkar, and A. Joshi. 2003. Using LTAG
based features in parse reranking. In EMNLP-2003.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In ACL-2003.
K. Toutanova, A. Haghighi, and C. D. Manning. 2005.
Joint learning improves semantic role labeling. In
ACL-2005.
F. Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of 5th Natural Lan-
guage Processing Pacific Rim Symposium (NLPRS-
99), Beijing, China.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In EMNLP-2004.
A. Yeh. 2000. More accurate tests for the statistical sig-
nificance of result differences. In COLING-2000.
599
331
332
333
334
335
336
337
338
A Clustering Approach for the Nearly Unsupervised Recognition of
Nonliteral Language?
Julia Birke and Anoop Sarkar
School of Computing Science, Simon Fraser University
Burnaby, BC, V5A 1S6, Canada
jbirke@alumni.sfu.ca, anoop@cs.sfu.ca
Abstract
In this paper we present TroFi (Trope
Finder), a system for automatically classi-
fying literal and nonliteral usages of verbs
through nearly unsupervised word-sense
disambiguation and clustering techniques.
TroFi uses sentential context instead of
selectional constraint violations or paths
in semantic hierarchies. It also uses lit-
eral and nonliteral seed sets acquired and
cleaned without human supervision in or-
der to bootstrap learning. We adapt a
word-sense disambiguation algorithm to
our task and augment it with multiple seed
set learners, a voting schema, and addi-
tional features like SuperTags and extra-
sentential context. Detailed experiments
on hand-annotated data show that our en-
hanced algorithm outperforms the base-
line by 24.4%. Using the TroFi algo-
rithm, we also build the TroFi Example
Base, an extensible resource of annotated
literal/nonliteral examples which is freely
available to the NLP research community.
1 Introduction
In this paper, we propose TroFi (Trope Finder),
a nearly unsupervised clustering method for sep-
arating literal and nonliteral usages of verbs. For
example, given the target verb ?pour?, we would
expect TroFi to cluster the sentence ?Custom
demands that cognac be poured from a freshly
opened bottle? as literal, and the sentence ?Salsa
and rap music pour out of the windows? as nonlit-
eral, which, indeed, it does. We call our method
nearly unsupervised. See Section 3.1 for why we
use this terminology.
We reduce the problem of nonliteral language
recognition to one of word-sense disambiguation
?This research was partially supported by NSERC,
Canada (RGPIN: 264905). We would like to thank Bill
Dolan, Fred Popowich, Dan Fass, Katja Markert, Yudong
Liu, and the anonymous reviewers for their comments.
by redefining literal and nonliteral as two differ-
ent senses of the same word, and we adapt an ex-
isting similarity-based word-sense disambiguation
method to the task of separating usages of verbs
into literal and nonliteral clusters. This paper fo-
cuses on the algorithmic enhancements necessary
to facilitate this transformation from word-sense
disambiguation to nonliteral language recognition.
The output of TroFi is an expandable example base
of literal/nonliteral clusters which is freely avail-
able to the research community.
Many systems that use NLP methods ? such as
dialogue systems, paraphrasing and summariza-
tion, language generation, information extraction,
machine translation, etc. ? would benefit from be-
ing able to recognize nonliteral language. Con-
sider an example based on a similar example from
an automated medical claims processing system.
We must determine that the sentence ?she hit the
ceiling? is meant literally before it can be marked
up as an ACCIDENT claim. Note that the typical
use of ?hit the ceiling? stored in a list of idioms
cannot help us. Only using the context, ?She broke
her thumb while she was cheering for the Patriots
and, in her excitement, she hit the ceiling,? can we
decide.
We further motivate the usefulness of the abil-
ity to recognize literal vs. nonliteral usages using
an example from the Recognizing Textual Entail-
ment (RTE-1) challenge of 2005. (This is just an
example; we do not compute entailments.) In the
challenge data, Pair 1959 was: Kerry hit Bush hard
on his conduct on the war in Iraq. ? Kerry shot
Bush. The objective was to report FALSE since
the second statement in this case is not entailed
from the first one. In order to do this, it is cru-
cial to know that ?hit? is being used nonliterally in
the first sentence. Ideally, we would like to look
at TroFi as a first step towards an unsupervised,
scalable, widely applicable approach to nonliteral
language processing that works on real-world data
from any domain in any language.
329
2 Previous Work
The foundations of TroFi lie in a rich collec-
tion of metaphor and metonymy processing sys-
tems: everything from hand-coded rule-based sys-
tems to statistical systems trained on large cor-
pora. Rule-based systems ? some using a type
of interlingua (Russell, 1976); others using com-
plicated networks and hierarchies often referred
to as metaphor maps (e.g. (Fass, 1997; Martin,
1990; Martin, 1992) ? must be largely hand-coded
and generally work well on an enumerable set
of metaphors or in limited domains. Dictionary-
based systems use existing machine-readable dic-
tionaries and path lengths between words as one
of their primary sources for metaphor processing
information (e.g. (Dolan, 1995)). Corpus-based
systems primarily extract or learn the necessary
metaphor-processing information from large cor-
pora, thus avoiding the need for manual annota-
tion or metaphor-map construction. Examples of
such systems can be found in (Murata et. al., 2000;
Nissim &Markert, 2003; Mason, 2004). The work
on supervised metonymy resolution by Nissim &
Markert and the work on conceptual metaphors by
Mason come closest to what we are trying to do
with TroFi.
Nissim & Markert (2003) approach metonymy
resolution with machine learning methods, ?which
[exploit] the similarity between examples of con-
ventional metonymy? ((Nissim & Markert, 2003),
p. 56). They see metonymy resolution as a classi-
fication problem between the literal use of a word
and a number of pre-defined metonymy types.
They use similarities between possibly metonymic
words (PMWs) and known metonymies as well as
context similarities to classify the PMWs. The
main difference between the Nissim & Markert al-
gorithm and the TroFi algorithm ? besides the fact
that Nissim & Markert deal with specific types
of metonymy and not a generalized category of
nonliteral language ? is that Nissim & Markert
use a supervised machine learning algorithm, as
opposed to the primarily unsupervised algorithm
used by TroFi.
Mason (2004) presents CorMet, ?a corpus-
based system for discovering metaphorical map-
pings between concepts? ((Mason, 2004), p. 23).
His system finds the selectional restrictions of
given verbs in particular domains by statistical
means. It then finds metaphorical mappings be-
tween domains based on these selectional prefer-
ences. By finding semantic differences between
the selectional preferences, it can ?articulate the
higher-order structure of conceptual metaphors?
((Mason, 2004), p. 24), finding mappings like
LIQUID?MONEY. Like CorMet, TroFi uses
contextual evidence taken from a large corpus and
also uses WordNet as a primary knowledge source,
but unlike CorMet, TroFi does not use selectional
preferences.
Metaphor processing has even been ap-
proached with connectionist systems storing
world-knowledge as probabilistic dependencies
(Narayanan, 1999).
3 TroFi
TroFi is not a metaphor processing system. It does
not claim to interpret metonymy and it will not tell
you what a given idiom means. Rather, TroFi at-
tempts to separate literal usages of verbs from non-
literal ones.
For the purposes of this paper we will take the
simplified view that literal is anything that falls
within accepted selectional restrictions (?he was
forced to eat his spinach? vs. ?he was forced to eat
his words?) or our knowledge of the world (?the
sponge absorbed the water? vs. ?the company
absorbed the loss?). Nonliteral is then anything
that is ?not literal?, including most tropes, such as
metaphors, idioms, as well phrasal verbs and other
anomalous expressions that cannot really be seen
as literal. In terms of metonymy, TroFi may clus-
ter a verb used in a metonymic expression such as
?I read Keats? as nonliteral, but we make no strong
claims about this.
3.1 The Data
The TroFi algorithm requires a target set (called
original set in (Karov & Edelman, 1998)) ? the
set of sentences containing the verbs to be classi-
fied into literal or nonliteral ? and the seed sets:
the literal feedback set and the nonliteral feed-
back set. These sets contain feature lists consist-
ing of the stemmed nouns and verbs in a sentence,
with target or seed words and frequent words re-
moved. The frequent word list (374 words) con-
sists of the 332 most frequent words in the British
National Corpus plus contractions, single letters,
and numbers from 0-10. The target set is built us-
ing the ?88-?89 Wall Street Journal Corpus (WSJ)
tagged using the (Ratnaparkhi, 1996) tagger and
the (Bangalore & Joshi, 1999) SuperTagger; the
feedback sets are built using WSJ sentences con-
330
Algorithm 1 KE-train: (Karov & Edelman, 1998) algorithm adapted to literal/nonliteral classification
Require: S: the set of sentences containing the target word
Require: L: the set of literal seed sentences
Require: N : the set of nonliteral seed sentences
Require: W: the set of words/features, w ? s means w is in sentence s, s 3 w means s contains w
Require: : threshold that determines the stopping condition
1: w-sim0(wx, wy) := 1 if wx = wy, 0 otherwise
2: s-simI0(sx, sy) := 1, for all sx, sy ? S ? S where sx = sy, 0 otherwise
3: i := 0
4: while (true) do
5: s-simLi+1(sx, sy) :=
?
wx?sx p(wx, sx)maxwy?sy w-simi(wx, wy), for all sx, sy ? S ? L
6: s-simNi+1(sx, sy) :=
?
wx?sx p(wx, sx)maxwy?sy w-simi(wx, wy), for all sx, sy ? S ?N
7: for wx, wy ? W ?W do
8: w-simi+1(wx, wy) :=
{
i = 0 ?sx3wx p(wx, sx)maxsy3wy s-simIi (sx, sy)
else
?
sx3wx p(wx, sx)maxsy3wy{s-simLi (sx, sy), s-simNi (sx, sy)}
9: end for
10: if ?wx,maxwy{w-simi+1(wx, wy) ? w-simi(wx, wy)} ?  then
11: break # algorithm converges in 1 steps.
12: end if
13: i := i + 1
14: end while
taining seed words extracted from WordNet and
the databases of known metaphors, idioms, and
expressions (DoKMIE), namely Wayne Magnu-
son English Idioms Sayings & Slang and George
Lakoff?s Conceptual Metaphor List, as well as ex-
ample sentences from these sources. (See Section
4 for the sizes of the target and feedback sets.) One
may ask why we need TroFi if we have databases
like the DoKMIE. The reason is that the DoKMIE
are unlikely to list all possible instances of non-
literal language and because knowing that an ex-
pression can be used nonliterally does not mean
that you can tell when it is being used nonliter-
ally. The target verbs may not, and typically do
not, appear in the feedback sets. In addition, the
feedback sets are noisy and not annotated by any
human, which is why we call TroFi unsupervised.
When we useWordNet as a source of example sen-
tences, or of seed words for pulling sentences out
of the WSJ, for building the literal feedback set,
we cannot tell if the WordNet synsets, or the col-
lected feature sets, are actually literal. We provide
some automatic methods in Section 3.3 to ensure
that the feedback set feature sets that will harm us
in the clustering phase are removed. As a side-
effect, we may fill out sparse nonliteral sets.
In the next section we look at the Core TroFi
algorithm and its use of the above data sources.
3.2 Core Algorithm
Since we are attempting to reduce the problem of
literal/nonliteral recognition to one of word-sense
disambiguation, TroFi makes use of an existing
similarity-based word-sense disambiguation algo-
rithm developed by (Karov & Edelman, 1998),
henceforth KE.
The KE algorithm is based on the principle of
attraction: similarities are calculated between sen-
tences containing the word we wish to disam-
biguate (the target word) and collections of seed
sentences (feedback sets) (see also Section 3.1).
A target set sentence is considered to be at-
tracted to the feedback set containing the sentence
to which it shows the highest similarity. Two sen-
tences are similar if they contain similar words and
two words are similar if they are contained in sim-
ilar sentences. The resulting transitive similarity
allows us to defeat the knowledge acquisition bot-
tleneck ? i.e. the low likelihood of finding all pos-
sible usages of a word in a single corpus. Note
that the KE algorithm concentrates on similarities
in the way sentences use the target literal or non-
literal word, not on similarities in the meanings of
the sentences themselves.
Algorithms 1 and 2 summarize the basic TroFi
version of the KE algorithm. Note that p(w, s) is
the unigram probability of word w in sentence s,
331
Algorithm 2 KE-test: classifying literal/nonliteral
1: For any sentence sx ? S
2: if maxsy s-sim
L(sx, sy) > maxsy s-sim
N (sx, sy)
then
3: tag sx as literal
4: else
5: tag sx as nonliteral
6: end if
normalized by the total number of words in s.
In practice, initializing s-simI0 in line (2) of
Algorithm 1 to 0 and then updating it from
w-sim0 means that each target sentence is still
maximally similar to itself, but we also dis-
cover additional similarities between target sen-
tences. We further enhance the algorithm
by using Sum of Similarities. To implement
this, in Algorithm 2 we change line (2) into:
?
sy s-sim
L(sx, sy) >
?
sy s-sim
N (sx, sy)
Although it is appropriate for fine-grained tasks
like word-sense disambiguation to use the single
highest similarity score in order to minimize noise,
summing across all the similarities of a target set
sentence to the feedback set sentences is more
appropriate for literal/nonliteral clustering, where
the usages could be spread across numerous sen-
tences in the feedback sets. We make another
modification to Algorithm 2 by checking that the
maximum sentence similarity in line (2) is above a
certain threshold for classification. If the similar-
ity is above this threshold, we label a target-word
sentence as literal or nonliteral.
Before continuing, let us look at an example.
The features are shown in bold.
Target Set
1 The girl and her brother grasped their mother?s hand.
2 He thinks he has grasped the essentials of the institute?s
finance philosophies.
3 The president failed to grasp ACTech?s finance quandary.
Literal Feedback Set
L1 The man?s aging mother gripped her husband?s
shoulders tightly.
L2 The child gripped her sister?s hand to cross the road.
L3 The president just doesn?t get the picture, does he?
Nonliteral Feedback Set
N1 After much thought, he finally grasped the idea.
N2 This idea is risky, but it looks like the director of the
institute has comprehended the basic principles behind it.
N3 Mrs. Fipps is having trouble comprehending the legal
straits of the institute.
N4 She had a hand in his fully comprehending the quandary.
The target set consists of sentences from the
corpus containing the target word. The feedback
sets contain sentences from the corpus containing
synonyms of the target word found in WordNet
(literal feedback set) and the DoKMIE (nonliteral
feedback set). The feedback sets also contain ex-
ample sentences provided in the target-word en-
tries of these datasets. TroFi attempts to cluster the
target set sentences into literal and nonliteral by
attracting them to the corresponding feature sets
using Algorithms 1 & 2. Using the basic KE algo-
rithm, target sentence 2 is correctly attracted to the
nonliteral set, and sentences 1 and 3 are equally
attracted to both sets. When we apply our sum of
similarities enhancement, sentence 1 is correctly
attracted to the literal set, but sentence 3 is now in-
correctly attracted to the literal set too. In the fol-
lowing sections we describe some enhancements ?
Learners & Voting, SuperTags, and Context ? that
try to solve the problem of incorrect attractions.
3.3 Cleaning the Feedback Sets
In this section we describe how we clean up the
feedback sets to improve the performance of the
Core algorithm. We also introduce the notion of
Learners & Voting.
Recall that neither the raw data nor the collected
feedback sets are manually annotated for training
purposes. Since, in addition, the feedback sets are
collected automatically, they are very noisy. For
instance, in the example in Section 3.2, the lit-
eral feedback set sentence L3 contains an idiom
which was provided as an example sentence in
WordNet as a synonym for ?grasp?. In N4, we
have the side-effect feature ?hand?, which unfor-
tunately overlaps with the feature ?hand? that we
might hope to find in the literal set (e.g. ?grasp his
hand?). In order to remove sources of false attrac-
tion like these, we introduce the notion of scrub-
bing. Scrubbing is founded on a few basic prin-
ciples. The first is that the contents of the DoK-
MIE come from (third-party) human annotations
and are thus trusted. Consequently we take them
as primary and use them to scrub the WordNet
synsets. The second is that phrasal and expres-
sion verbs, for example ?throw away?, are often
indicative of nonliteral uses of verbs ? i.e. they are
not the sum of their parts ? so they can be used
for scrubbing. The third is that content words ap-
pearing in both feedback sets ? for example ?the
wind is blowing? vs. ?the winds of war are blow-
ing? for the target word ?blow? ? will lead to im-
pure feedback sets, a situation we want to avoid.
The fourth is that our scrubbing action can take a
number of different forms: we can choose to scrub
332
just a word, a whole synset, or even an entire fea-
ture set. In addition, we can either move the of-
fending item to the opposite feedback set or re-
move it altogether. Moving synsets or feature sets
can add valuable content to one feedback set while
removing noise from the other. However, it can
also cause unforeseen contamination. We experi-
mented with a number of these options to produce
a whole complement of feedback set learners for
classifying the target sentences. Ideally this will
allow the different learners to correct each other.
For Learner A, we use phrasal/expression verbs
and overlap as indicators to select whole Word-
Net synsets for moving over to the nonliteral feed-
back set. In our example, this causes L1-L3 to
be moved to the nonliteral set. For Learner B,
we use phrasal/expression verbs and overlap as
indicators to remove problematic synsets. Thus
we avoid accidentally contaminating the nonliteral
set. However, we do end up throwing away infor-
mation that could have been used to pad out sparse
nonliteral sets. In our example, this causes L1-L3
to be dropped. For Learner C, we remove feature
sets from the final literal and nonliteral feedback
sets based on overlapping words. In our exam-
ple, this causes L2 and N4 to be dropped. Learner
D is the baseline ? no scrubbing. We simply use
the basic algorithm. Each learner has benefits and
shortcomings. In order to maximize the former
and minimize the latter, instead of choosing the
single most successful learner, we introduce a vot-
ing system. We use a simple majority-rules algo-
rithm, with the strongest learners weighted more
heavily. In our experiments we double the weights
of Learners A and D. In our example, this results
in sentence 3 now being correctly attracted to the
nonliteral set.
3.4 Additional Features
Even before voting, we attempt to improve the cor-
rectness of initial attractions through the use of
SuperTags, which allows us to add internal struc-
ture information to the bag-of-words feature lists.
SuperTags (Bangalore & Joshi, 1999) encode a
great deal of syntactic information in a single tag
(each tag is an elementary tree from the XTAG
English Tree Adjoining Grammar). In addition
to a word?s part of speech, they also encode in-
formation about its location in a syntactic tree ?
i.e. we learn something about the surrounding
words as well. We devised a SuperTag trigram
composed of the SuperTag of the target word and
the following two words and their SuperTags if
they contain nouns, prepositions, particles, or ad-
verbs. This is helpful in cases where the same
set of features can be used as part of both literal
and nonliteral expressions. For example, turning
?It?s hard to kick a habit like drinking? into ?habit
drink kick/B nx0Vpls1 habit/A NXN,? results in
a higher attraction to sentences about ?kicking
habits? than to sentences like ?She has a habit of
kicking me when she?s been drinking.?
Note that the creation of Learners A and B
changes if SuperTags are used. In the origi-
nal version, we only move or remove synsets
based on phrasal/expression verbs and overlapping
words. If SuperTags are used, we also move or
remove feature sets whose SuperTag trigram indi-
cates phrasal verbs (verb-particle expressions).
A final enhancement involves extending the
context to help with disambiguation. Sometimes
critical disambiguation features are contained not
in the sentence with the target word, but in an
adjacent sentence. To add context, we simply
group the sentence containing the target word with
a specified number of surrounding sentences and
turn the whole group into a single feature set.
4 Results
TroFi was evaluated on the 25 target words listed
in Table 1. The target sets contain from 1 to 115
manually annotated sentences for each verb. The
first round of annotations was done by the first an-
notator. The second annotator was given no in-
structions besides a few examples of literal and
nonliteral usage (not covering all target verbs).
The authors of this paper were the annotators. Our
inter-annotator agreement on the annotations used
as test data in the experiments in this paper is quite
high. ? (Cohen) and ? (S&C) on a random sam-
ple of 200 annotated examples annotated by two
different annotators was found to be 0.77. As per
((Di Eugenio & Glass, 2004), cf. refs therein), the
standard assessment for ? values is that tentative
conclusions on agreement exists when .67 ? ? <
.8, and a definite conclusion on agreement exists
when ? ? .8.
In the case of a larger scale annotation effort,
having the person leading the effort provide one
or two examples of literal and nonliteral usages
for each target verb to each annotator would al-
most certainly improve inter-annotator agreement.
Table 1 lists the total number of target sentences,
plus the manually evaluated literal and nonliteral
333
counts, for each target word. It also provides the
feedback set sizes for each target word. The to-
tals across all words are given at the bottom of the
table.
absorb assault die drag drown
Lit Target 4 3 24 12 4
Nonlit Target 62 0 11 41 1
Target 66 3 35 53 5
Lit FB 286 119 315 118 25
Nonlit FB 1 0 7 241 21
escape examine fill fix flow
Lit Target 24 49 47 39 10
Nonlit Target 39 37 40 16 31
Target 63 86 87 55 41
Lit FB 124 371 244 953 74
Nonlit FB 2 2 66 279 2
grab grasp kick knock lend
Lit Target 5 1 10 11 77
Nonlit Target 13 4 26 29 15
Target 18 5 36 40 92
Lit FB 76 36 19 60 641
Nonlit FB 58 2 172 720 1
miss pass rest ride roll
Lit Target 58 0 8 22 25
Nonlit Target 40 1 20 26 46
Target 98 1 28 48 71
Lit FB 236 1443 42 221 132
Nonlit FB 13 156 6 8 74
smooth step stick strike touch
Lit Target 0 12 8 51 13
Nonlit Target 11 94 73 64 41
Target 11 106 81 115 54
Lit FB 28 5 132 693 904
Nonlit FB 75 517 546 351 406
Totals: Target=1298; Lit FB=7297; Nonlit FB=3726
Table 1: Target and Feedback Set Sizes.
The algorithms were evaluated based on how
accurately they clustered the hand-annotated sen-
tences. Sentences that were attracted to neither
cluster or were equally attracted to both were put
in the opposite set from their label, making a fail-
ure to cluster a sentence an incorrect clustering.
Evaluation results were recorded as recall, pre-
cision, and f-score values. Literal recall is defined
as (correct literals in literal cluster / total correct
literals). Literal precision is defined as (correct
literals in literal cluster / size of literal cluster).
If there are no literals, literal recall is 100%; lit-
eral precision is 100% if there are no nonliterals in
the literal cluster and 0% otherwise. The f-score
is defined as (2 ? precision ? recall) / (precision
+ recall). Nonliteral precision and recall are de-
fined similarly. Average precision is the average
of literal and nonliteral precision; similarly for av-
erage recall. For overall performance, we take the
f-score of average precision and average recall.
We calculated two baselines for each word. The
first was a simple majority-rules baseline. Due to
the imbalance of literal and nonliteral examples,
this baseline ranges from 60.9% to 66.7% with an
average of 63.6%. Keep in mind though that us-
ing this baseline, the f-score for the nonliteral set
will always be 0%. We come back to this point
at the end of this section. We calculated a sec-
ond baseline using a simple attraction algorithm.
Each target set sentence is attracted to the feed-
back set containing the sentence with which it has
the most words in common. This corresponds well
to the basic highest similarity TroFi algorithm.
Sentences attracted to neither, or equally to both,
sets are put in the opposite cluster to where they
belong. Since this baseline actually attempts to
distinguish between literal and nonliteral and uses
all the data used by the TroFi algorithm, it is the
one we will refer to in our discussion below.
Experiments were conducted to first find the
results of the core algorithm and then determine
the effects of each enhancement. The results are
shown in Figure 1. The last column in the graph
shows the average across all the target verbs.
On average, the basic TroFi algorithm (KE)
gives a 7.6% improvement over the baseline, with
some words, like ?lend? and ?touch?, having
higher results due to transitivity of similarity. For
our sum of similarities enhancement, all the in-
dividual target word results except for ?examine?
sit above the baseline. The dip is due to the fact
that while TroFi can generate some beneficial sim-
ilarities between words related by context, it can
also generate some detrimental ones. When we
use sum of similarities, it is possible for the tran-
sitively discovered indirect similarities between a
target nonliteral sentence and all the sentences in a
feedback set to add up to more than a single direct
similarity between the target sentence and a single
feedback set sentence. This is not possible with
highest similarity because a single sentence would
have to show a higher similarity to the target sen-
tence than that produced by sharing an identical
word, which is unlikely since transitively discov-
ered similarities generally do not add up to 1. So,
although highest similarity occasionally produces
better results than using sum of similarities, on av-
erage we can expect to get better results with the
latter. In this experiment alone, we get an average
f-score of 46.3% for the sum of similarities results
? a 9.4% improvement over the high similarity re-
sults (36.9%) and a 16.9% improvement over the
baseline (29.4%).
334
Figure 1: TroFi Evaluation Results.
In comparing the individual results of all our
learners, we found that the results for Learners A
and D (46.7% and 46.3%) eclipsed Learners B and
C by just over 2.5%. Using majority-rules voting
with Learners A and D doubled, we were able to
obtain an average f-score of 48.4%, showing that
voting does to an extent balance out the learners?
varying results on different words.
The addition of SuperTags caused improve-
ments in some words like ?drag? and ?stick?. The
overall gain was only 0.5%, likely due to an over-
generation of similarities. Future work may iden-
tify ways to use SuperTags more effectively.
The use of additional context was responsible
for our second largest leap in performance after
sum of similarities. We gained 4.9%, bringing
us to an average f-score of 53.8%. Worth noting
is that the target words exhibiting the most sig-
nificant improvement, ?drown? and ?grasp?, had
some of the smallest target and feedback set fea-
ture sets, supporting the theory that adding cogent
features may improve performance.
With an average of 53.8%, all words but one
lie well above our simple-attraction baseline, and
some even achieve much higher results than the
majority-rules baseline. Note also that, using this
latter baseline, TroFi boosts the nonliteral f-score
from 0% to 42.3%.
5 The TroFi Example Base
In this section we discuss the TroFi Example Base.
First, we examine iterative augmentation. Then
we discuss the structure and contents of the exam-
ple base and the potential for expansion.
After an initial run for a particular target word,
we have the cluster results plus a record of the
feedback sets augmented with the newly clustered
sentences. Each feedback set sentence is saved
with a classifier weight, with newly clustered sen-
tences receiving a weight of 1.0. Subsequent runs
may be done to augment the initial clusters. For
these runs, we use the classifiers from our initial
run as feedback sets. New sentences for clustering
are treated like a regular target set. Running TroFi
produces new clusters and re-weighted classifiers
augmented with newly clustered sentences. There
can be as many runs as desired; hence iterative
augmentation.
We used the iterative augmentation process to
build a small example base consisting of the target
words from Table 1, as well as another 25 words
drawn from the examples of scholars whose work
335
***pour***
*nonliteral cluster*
wsj04:7878 N As manufacturers get bigger , they are likely to
pour more money into the battle for shelf space , raising the
ante for new players ./.
wsj25:3283 N Salsa and rap music pour out of the windows ./.
wsj06:300 U Investors hungering for safety and high yields
are pouring record sums into single-premium , interest-earning
annuities ./.
*literal cluster*
wsj59:3286 L Custom demands that cognac be poured from a
freshly opened bottle ./.
Figure 2: TroFi Example Base Excerpt.
was reviewed in Section 2. It is important to note
that in building the example base, we used TroFi
with an Active Learning component (see (Birke,
2005)) which improved our average f-score from
53.8% to 64.9% on the original 25 target words.
An excerpt from the example base is shown
in Figure 2. Each entry includes an ID num-
ber and a Nonliteral, Literal, or Unannotated
tag. Annotations are from testing or from
active learning during example-base construc-
tion. The TroFi Example Base is available at
http://www.cs.sfu.ca/?anoop/students/jbirke/. Fur-
ther unsupervised expansion of the existing clus-
ters as well as the production of additional clusters
is a possibility.
6 Conclusion
In this paper we presented TroFi, a system for
separating literal and nonliteral usages of verbs
through statistical word-sense disambiguation and
clustering techniques. We suggest that TroFi is
applicable to all sorts of nonliteral language, and
that, although it is currently focused on English
verbs, it could be adapted to other parts of speech
and other languages.
We adapted an existing word-sense disam-
biguation algorithm to literal/nonliteral clustering
through the redefinition of literal and nonliteral as
word senses, the alteration of the similarity scores
used, and the addition of learners and voting, Su-
perTags, and additional context.
For all our models and algorithms, we carried
out detailed experiments on hand-annotated data,
both to fully evaluate the system and to arrive at
an optimal configuration. Through our enhance-
ments we were able to produce results that are, on
average, 16.9% higher than the core algorithm and
24.4% higher than the baseline.
Finally, we used our optimal configuration of
TroFi, together with active learning and iterative
augmentation, to build the TroFi Example Base,
a publicly available, expandable resource of lit-
eral/nonliteral usage clusters that we hope will be
useful not only for future research in the field of
nonliteral language processing, but also as train-
ing data for other statistical NLP tasks.
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Supertag-
ging: an approach to almost parsing. Comput. Linguist.
25, 2 (Jun. 1999), 237-265.
Julia Birke. 2005. A Clustering Approach for the Unsuper-
vised Recognition of Nonliteral Language. M.Sc. Thesis.
School of Computing Science, Simon Fraser University.
Barbara Di Eugenio and Michael Glass. 2004. The kappa
statistic: a second look. Comput. Linguist. 30, 1 (Mar.
2004), 95-101.
William B. Dolan. 1995. Metaphor as an emergent property
of machine-readable dictionaries. In Proceedings of Rep-
resentation and Acquisition of Lexical Knowledge: Poly-
semy, Ambiguity, and Generativity (March 1995, Stanford
University, CA). AAAI 1995 Spring Symposium Series,
27-29.
Dan Fass. 1997. Processing metonymy and metaphor.
Greenwich, CT: Ablex Publishing Corporation.
Yael Karov and Shimon Edelman. 1998. Similarity-based
word sense disambiguation. Comput. Linguist. 24, 1 (Mar.
1998), 41-59.
James H. Martin. 1990. A computational model of metaphor
interpretation. Toronto, ON: Academic Press, Inc.
James H. Martin. 1992. Computer understanding of con-
ventional metaphoric language. Cognitive Science 16, 2
(1992), 233-270.
Zachary J. Mason. 2004. CorMet: a computational, corpus-
based conventional metaphor extraction system. Comput.
Linguist. 30, 1 (Mar. 2004), 23-44.
Masaki Murata, Qing Ma, Atsumu Yamamoto, and Hitoshi
Isahara. 2000. Metonymy interpretation using x no y ex-
amples. In Proceedings of SNLP2000 (Chiang Mai, Thai-
land, 10 May 2000).
Srini Narayanan. 1999. Moving right along: a computational
model of metaphoric reasoning about events. In Proceed-
ings of the 16th National Conference on Artificial Intelli-
gence and the 11th IAAI Conference (Orlando, US, 1999).
121-127.
Malvina Nissim and Katja Markert. 2003. Syntactic features
and word similarity for supervised metonymy resolution.
In Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-03) (Sapporo,
Japan, 2003). 56-63.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Empirical Methods
in Natural Language Processing Conference (University
of Pennsylvania, May 17-18 1996).
Sylvia W. Russell. 1976. Computer understanding of
metaphorically used verbs. American Journal of Compu-
tational Linguistics, Microfiche 44.
336
Training a Perceptron with Global and Local Features
for Chinese Word Segmentation
Dong Song and Anoop Sarkar
School of Computing Science, Simon Fraser University
Burnaby, BC, Canada V5A1S6
{dsong,anoop}@cs.sfu.ca
Abstract
This paper proposes the use of global fea-
tures for Chinese word segmentation. These
global features are combined with local fea-
tures using the averaged perceptron algo-
rithm over N-best candidate word segmenta-
tions. The N-best candidates are produced
using a conditional random field (CRF)
character-based tagger for word segmenta-
tion. Our experiments show that by adding
global features, performance is significantly
improved compared to the character-based
CRF tagger. Performance is also improved
compared to using only local features. Our
system obtains an F-score of 0.9355 on the
CityU corpus, 0.9263 on the CKIP corpus,
0.9512 on the SXU corpus, 0.9296 on the
NCC corpus and 0.9501 on the CTB cor-
pus. All results are for the closed track in
the fourth SIGHAN Chinese Word Segmen-
tation Bakeoff.
1 Introduction
Most natural language processing tasks require that
the input be tokenized into individual words. For
some languages, including Chinese, this is challeng-
ing since the sentence is typically written as a string
of characters without spaces between words. Word
segmentation is the task of recovering the most plau-
sible grouping of characters into words. In this pa-
per, we describe the system we developed for the
fourth SIGHAN Chinese Word Segmentation Bake-
off1. We test our system in the closed track2 for all
five corpora: Academia Sinica (CKIP), City Uni-
versity of Hong Kong (CityU), National Chinese
1Further details at: www.china-language.gov.cn/bakeoff08/
bakeoff-08 basic.html
2We do not use any extra annotation, especially for punctu-
ation, dates, numbers or English letters.
Corpus (NCC), University of Colorado (CTB), and
Shanxi University (SXU).
2 System Description
The architecture of our system is shown in Figure 1.
For each of the training corpora in the bakeoff, we
produce a 10-fold split: in each fold, 90% of the cor-
pus is used for training and 10% is used to produce
an N-best list of candidates. The N-best list is pro-
duced using a character-based conditional random
field (CRF) (Lafferty et al, 2001; Kudo et al, 2004)
tagger. The true segmentation can now be compared
with the N-best list in order to train an averaged per-
ceptron algorithm (Collins, 2002a). This system is
then used to predict the best word segmentation from
an N-best list for each sentence in the test data.
Training Corpus
Weight Vector
N?best Candidates
Training With
Decoding With
Conditional Random
N?best Candidates
Field
Local Features Global Features
Average Perceptron Input Sentence
Average Perceptron
Output
Conditional RandomField
(10?Fold Split)
Figure 1: Outline of the segmentation process
2.1 Learning Algorithm
Given an unsegmented sentence x, the word seg-
mentation problem can be defined as finding the
143
Sixth SIGHAN Workshop on Chinese Language Processing
most probable segmentation F (x) from a set of pos-
sible segmentations of x.
F (x) = argmax
y?GEN(x)
?(x, y) ? w (1)
The set of possible segmentations is given by
GEN(x) and the na??ve method is to first generate all
possible segmented candidates. For a long sentence,
generating those candidates and picking the one with
the highest score is time consuming.
In our approach, N-best candidates for each train-
ing example are produced with the CRF++ soft-
ware (Kudo et al, 2004). The CRF is used as a tag-
ger that tags each character with the following tags:
for each multi-character word, its first character is
given a B (Beginning) tag , its last character is as-
signed an E (End) tag, while each of its remaining
characters is provided an M (Middle) tag. In addi-
tion, for a single-character word, S (Single) is used
as its tag3. Let c0 be the current character, c?1, c?2
are the two preceding characters, and c1, c2 are the
two characters to the right . Using this notation, the
features used in our CRF models are: c0, c?1, c1,
c?2, c2, c?1c0, c0c1, c?1c1, c?2c?1 and c0c2.
We use the now standard method for producing N-
best candidates in order to train our re-ranker which
uses global and local features: 10-folds of training
data are used to train the tagger on 90% of the data
and then produce N-best lists for the remaining 10%.
This process gives us an N-best candidate list for
each sentence and the candidate that is most similar
to the true segmentation, called yb. We map a seg-
mentation y to features associated with the segmen-
tation using the mapping ?(?). The score of a seg-
mentation y is provided by the dot-product ?(y) ?w.
The perceptron algorithm (Fig. 2) finds the weight
parameter vector w using online updates. The pre-
dicted segmentation y
?
i based on the current weight
vector is compared to the the best candidate yb, and
whenever there is a mismatch, the algorithm updates
the parameter vector by incrementing the parame-
ter value for features in yb, and by decrementing the
value for features in y
?
i.
The voted perceptron (Freund and Schapire,
1999) has considerable advantages over the standard
3Note that performance of the CRF tagger could be im-
proved with the use of other tagsets. However, this does not
affect our comparative experiments in this paper.
Inputs: Training Data ?(x1, y1), . . . , (xm, ym)?
Initialization: Set w = 0
Algorithm:
for t = 1, . . . , T do
for i = 1, . . . ,m do
Calculate y
?
i, where
y
?
i = argmax
y?N-best Candidates
?(y) ? w
if y
?
i 6= y
b then
w = w +?(yb) ? ?(y
?
i)
end if
end for
end for
Figure 2: Training using a perceptron algorithm over
N-best candidates.
perceptron. However, due to the computational is-
sues with the voted perceptron, the averaged per-
ceptron algorithm (Collins, 2002a) is used instead.
Rather than using w, we use the averaged weight
parameter ? over the m training examples for future
predictions on unseen data:
? =
1
mT
?
i=1..m,t=1..T
wi,t
In calculating ?, an accumulating parameter vec-
tor ?i,t is maintained and updated using w for each
training example; therefore, ?i,t =
?
wi,t. After
the last iteration, ?i,t/mT produces the final para-
meter vector ?.
When the number of features is large, it is time
consuming to calculate the total parameter ?i,t for
each training example. To reduce the time complex-
ity, we adapted the lazy update proposed in (Collins,
2002b), which was also used in (Zhang and Clark,
2007). After processing each training sentence, not
all dimensions of ?i,t are updated. Instead, an up-
date vector ? is used to store the exact location (i, t)
where each dimension of the averaged parameter
vector was last updated, and only those dimensions
corresponding to features appearing in the current
sentence are updated. While for the last example in
the last iteration, each dimension of ? is updated, no
matter whether the candidate output is correct.
2.2 Feature Templates
The feature templates used in our system include
both local features and global features. For local fea-
tures, we consider twomajor categories: word-based
144
Sixth SIGHAN Workshop on Chinese Language Processing
features and character-based features. Five specific
types of features from (Zhang and Clark, 2007) that
are shown in Table 1 were used in our system. In our
initial experiments, the other features used in (Zhang
and Clark, 2007) did not improve performance and
so we do not include them in our system.
1 word w
2 word bigram w1w2
3 single character word w
4 space-separated characters c1 and c2
5 character bi-gram c1c2 in any word
Table 1: local feature templates. Rows 1, 2 and 3
are word-based and rows 4 and 5 are character-based
features
In our system, we also used two types of global
features per sentence (see Table 2). By global, we
mean features over the entire segmented sentence.4
6 sentence confidence score
7 sentence language model score
Table 2: global feature template
The sentence confidence score is calculated by
CRF++ during the production of the N-best candi-
date list, and it measures how confident each candi-
date is close to the true segmentation.
The sentence language model score for each seg-
mentation candidate is produced using the SRILM
toolkit (Stolcke, 2002) normalized using the formula
P 1/L, where P is the probability-based language
model score and L is the length of the sentence in
words (not in characters). For global features, the
feature weights are not learned using the perceptron
algorithm but are determined using a development
set.
3 Experiments and Analysis
Our system is tested on all five corpora provided in
the fourth SIGHAN Bakeoff, in the closed track.
3.1 Parameter Pruning
First, the value of the parameter N, which is the
maximum number of N-best candidates, was deter-
mined. An oracle procedure proceeds as follows:
80% of the training corpus is used to train the CRF
4It is important to distinguish this kind of global feature
from another type of ?global? feature that either enforces con-
sistency or examines the use of a feature in the entire training
or testing corpus.
model, and produce N-best outputs for each sen-
tence on the remaining 20% of the data. Then these
N candidates are compared with the true segmen-
tation, and for each training sentence, the candidate
closest to the truth is chosen as the final output. Test-
ing on different values of N, we chose N to be 20
in all our experiments since that provided the best
tradeoff between accuracy and speed.
Next, the weight for sentence confidence score
Scrf and that for language model score Slm are de-
termined. To simplify the process, we assume that
the weights for both Scrf and Slm are equal. In this
step, each training corpus is separated into a train-
ing set (80% of the whole corpus) and a held-out
set (20% of the corpus). Then, the perceptron algo-
rithm is applied on the training set with different Scrf
and Slm values, and for various number of iterations.
The weight values we test include 2, 4, 6, 8, 10, 20,
30, 40, 50, 100 and 200. From the experiments, the
weights are chosen to be 100 for CKIP corpus, 10
for CityU corpus, 30 for NCC corpus, 20 for CTB
corpus, and 10 for SXU corpus.
While determining the weights for global fea-
tures, the number of training iterations can be deter-
mined as well. Experiments show that, as the num-
ber of iterations increases, the accuracy stabilizes in
most cases, reflecting the convergence of the learn-
ing algorithm. Analyzing the learning curves, we fix
the number of training iterations to be 5 for CKIP
corpus, 9 for NCC corpus, and 8 for the CityU, CTB
and SXU corpora.
3.2 Results on the Fourth SIGHAN Bakeoff
In each experiment, F-score (F ) is used to evalu-
ate the segmentation accuracy. Table 3 shows the
F-score on the fourth SIGHAN Bakeoff corpora. In
this table, we record the performance of our system,
the score from the character-based CRF method and
the score from the averaged perceptron using only
local features.
Our system outperforms the baseline character-
based CRF tagger. In addition, the use of global
features in the re-ranker produces better results than
only using local features.
The only data set on which the performance of
our system is lower than the character-based CRF
method is CKIP corpus. For this data set during the
parameter pruning step, the weight for Scrf and Slm
145
Sixth SIGHAN Workshop on Chinese Language Processing
CKIP NCC CityU CTB SXU
Character-based CRF method 0.9332 0.9248 0.9320 0.9468 0.9473
Averaged Perceptron with only
local features
0.9180 0.9125 0.9273 0.9450 0.9387
Our System 0.9263 0.9296 0.9355 0.9501 0.9512
Our System (With modified
weight for global features)
0.9354 ? ? ? ?
Significance (p-value) ? 1.19e-12 ? 4.43e-69 ? 3.55e-88 ? 2.17e-18 ? 2.18e-38
Table 3: F-scores on the Fourth SIGHAN Bakeoff Corpora
was too large. By lowering the weight from 100 to
4, we obtains an F-score of 0.9354, which is signifi-
cantly better than the baseline CRF tagger.
The significance values in Table 3 were produced
using the McNemar?s Test (Gillick, 1989)5. All our
results are significantly better.
4 Related Work
Re-ranking over N-best lists has been applied to so
many tasks in natural language that it is not possi-
ble to list them all here. Closest to our approach
is the work in (Kazama and Torisawa, 2007). They
proposed a margin perceptron approach for named
entity recognition with non-local features on an N-
best list. In contrast to their approach, in our sys-
tem, global features examine the entire sentence in-
stead of partial phrases. For word segmentation,
(Wang and Shi, 2006) implemented a re-ranking
method with POS tagging features. In their ap-
proach, character-based CRF model produces the N-
best list for each test sentence. The Penn Chinese
TreeBank is used to train a POS tagger, which is
used in re-ranking. However the POS tags are used
as local and not global features. Note that we would
not use POS tags in the closed track.
5 Conclusion
We have participated in the closed track of the fourth
SIGHAN Chinese word segmentation bakeoff, and
we provide results on all five corpora. We have
shown that by combining global and local features,
we can improve accuracy over simply using local
features, and we also show improved accuracy over
the baseline CRF character-based tagger for word
segmentation.
5www.fon.hum.uva.nl/Service/Statistics/McNemars test.html
References
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proc. of the Empirical
Methods in Natural Language Processing (EMNLP).
MACL, 2002, 1?8, 2000.
M. Collins. 2002. Ranking Algorithms for Named-
Entity Extractions: Boosting and the Voted Percep-
tron. In Proc. of ACL 2002.
Y. Freund and R. Schapire. 1999. Large Margin Classi-
fication using the Perceptron Algorithm. In Machine
Learning, 37(3): 277?296.
L. Gillick and S. Cox. 1989. Some Statistical Issues
in the Comparison of Speech Recognition Algorithms.
In Proc. of IEEE Conf. on Acoustics, Speech and Sig.
Proc., Glasgow, 1989, 532?535.
J. Kazama and K. Torisawa. 2007. A New Perceptron
Algorithm for Sequence Labeling with Non-local Fea-
tures. In Proc. of EMNLP-CoNLL 2007 , pages 315?
324.
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Ap-
pliying Conditional Random Fields to Japanese Mor-
phological Analysis. In Proc. of EMNLP 2004.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML-2001, pages 591?598.
A. Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Proc. of EMNLP 1996.
A. Stolcke. 2002. SRILM - An Extensible Language
Modeling Toolkit. In Proc. Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado, September 2002.
M. Wang and Y. Shi. 2006. A Maximum Entropy Model
for Part-of-Speech Tagging. In Proc. of the Fifth
SIGHAN Workshop on Chinese Language Processing,
Sydney, 2006, pages 205?208.
Y. Zhang and S. Clark. 2007. Chinese Segmentation with
a Word-based Perceptron Algorithm. In Proc. of ACL
2007.
146
Sixth SIGHAN Workshop on Chinese Language Processing
Example Selection for Bootstrapping Statistical Parsers
Mark Steedman?, Rebecca Hwa?, Stephen Clark?, Miles Osborne?, Anoop Sarkar?
Julia Hockenmaier?, Paul Ruhlen? Steven Baker?, Jeremiah Crim?
?School of Informatics, University of Edinburgh
{steedman,stephenc,julia,osborne}@cogsci.ed.ac.uk
?Institute for Advanced Computer Studies, University of Maryland
hwa@umiacs.umd.edu
?School of Computing Science, Simon Fraser University
anoop@cs.sfu.ca
?Center for Language and Speech Processing, Johns Hopkins University
jcrim@jhu.edu,ruhlen@cs.jhu.edu
?Department of Computer Science, Cornell University
sdb22@cornell.edu
Abstract
This paper investigates bootstrapping for statis-
tical parsers to reduce their reliance on manu-
ally annotated training data. We consider both
a mostly-unsupervised approach, co-training,
in which two parsers are iteratively re-trained
on each other?s output; and a semi-supervised
approach, corrected co-training, in which a
human corrects each parser?s output before
adding it to the training data. The selection of
labeled training examples is an integral part of
both frameworks. We propose several selection
methods based on the criteria of minimizing er-
rors in the data and maximizing training util-
ity. We show that incorporating the utility cri-
terion into the selection method results in better
parsers for both frameworks.
1 Introduction
Current state-of-the-art statistical parsers (Collins, 1999;
Charniak, 2000) are trained on large annotated corpora
such as the Penn Treebank (Marcus et al, 1993). How-
ever, the production of such corpora is expensive and
labor-intensive. Given this bottleneck, there is consider-
able interest in (partially) automating the annotation pro-
cess.
To overcome this bottleneck, two approaches from ma-
chine learning have been applied to training parsers. One
is sample selection (Thompson et al, 1999; Hwa, 2000;
Tang et al, 2002), a variant of active learning (Cohn et al,
1994), which tries to identify a small set of unlabeled sen-
tences with high training utility for the human to label1.
Sentences with high training utility are those most likely
to improve the parser. The other approach, and the fo-
cus of this paper, is co-training (Sarkar, 2001), a mostly-
unsupervised algorithm that replaces the human by hav-
ing two (or more) parsers label training examples for each
other. The goal is for both parsers to improve by boot-
strapping off each other?s strengths. Because the parsers
may label examples incorrectly, only a subset of their out-
put, chosen by some selection mechanism, is used in or-
der to minimize errors. The choice of selection method
significantly affects the quality of the resulting parsers.
We investigate a novel approach of selecting training
examples for co-training parsers by incorporating the idea
of maximizing training utility from sample selection. The
selection mechanism is integral to both sample selection
and co-training; however, because co-training and sam-
ple selection have different goals, their selection methods
focus on different criteria: co-training typically favors se-
lecting accurately labeled examples, while sample selec-
tion typically favors selecting examples with high train-
ing utility, which often are not sentences that the parsers
already label accurately. In this work, we investigate se-
lection methods for co-training that explore the trade-off
between maximizing training utility and minimizing er-
rors.
Empirical studies were conducted to compare selection
methods under both co-training and a semi-supervised
framework called corrected co-training (Pierce and
Cardie, 2001), in which the selected examples are man-
ually checked and corrected before being added to the
1In the context of training parsers, a labeled example is a
sentence with its parse tree. Throughout this paper, we use the
term ?label? and ?parse? interchangeably.
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 157-164
                                                         Proceedings of HLT-NAACL 2003
training data. For co-training, we show that the benefit of
selecting examples with high training utility can offset the
additional errors they contain. For corrected co-training,
we show that selecting examples with high training util-
ity reduces the number of sentences the human annotator
has to check. For both frameworks, we show that selec-
tion methods that maximize training utility find labeled
examples that result in better trained parsers than those
that only minimize error.
2 Co-training
Blum and Mitchell (1998) introduced co-training to
bootstrap two classifiers with different views of the data.
The two classifiers are initially trained on a small amount
of annotated seed data; then they label unannotated data
for each other in an iterative training process. Blum and
Mitchell prove that, when the two views are conditionally
independent given the label, and each view is sufficient
for learning the task, co-training can boost an initial
weak learner using unlabeled data.
The theory underlying co-training has been extended
by Dasgupta et al (2002) to prove that, by maximizing
their agreement over the unlabeled data, the two learn-
ers make few generalization errors (under the same in-
dependence assumption adopted by Blum and Mitchell).
Abney (2002) argues that this assumption is extremely
strong and typically violated in the data, and he proposes
a weaker independence assumption.
Goldman and Zhou (2000) show that, through care-
ful selection of newly labeled examples, co-training can
work even when the classifiers? views do not satisfy
the independence assumption. In this paper we investi-
gate methods for selecting labeled examples produced by
two statistical parsers. We do not explicitly maximize
agreement (along the lines of Abney?s algorithm (2002))
because it is too computationally intensive for training
parsers.
The pseudocode for our co-training framework is given
in Figure 1. It consists of two different parsers and a cen-
tral control that interfaces between the two parsers and
the data. At each co-training iteration, a small set of sen-
tences is drawn from a large pool of unlabeled sentences
and stored in a cache. Both parsers then attempt to label
every sentence in the cache. Next, a subset of the newly
labeled sentences is selected to be added to the train-
ing data. The examples added to the training set of one
parser (referred to as the student) are only those produced
by the other parser (referred to as the teacher), although
the methods we use generalize to the case in which the
parsers share a single training set. During selection, one
parser first acts as the teacher and the other as the student,
and then the roles are reversed.
A and B are two different parsers.
M iA and M iB are the models of A and B at step i.
U is a large pool of unlabeled sentences.
U i is a small cache holding a subset of U at step i.
L is the manually labeled seed data.
LiA and LiB are the labeled training examples for A and B
at step i.
Initialize:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
Loop:
U i ? Add unlabeled sentences from U .
M iA and M iB parse the sentences in U i and
assign scores to them according to their scoring
functions fA and fB .
Select new parses {PA} and {PB} according to some
selection method S, which uses the scores
from fA and fB .
Li+1A is L
i
A augmented with {PB}
Li+1B is L
i
B augmented with {PA}
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
Figure 1: The pseudo-code for the co-training algorithm
3 Selecting Training Examples
In each iteration, selection is performed in two steps.
First, each parser uses some scoring function, f , to assess
the parses it generated for the sentences in the cache.2
Second, the central control uses some selection method,
S, to choose a subset of these labeled sentences (based on
the scores assigned by f ) to add to the parsers? training
data. The focus of this paper is on the selection phase, but
to more fully investigate the effect of different selection
methods we also consider two possible scoring functions.
3.1 Scoring functions
The scoring function attempts to quantify the correctness
of the parses produced by each parser. An ideal scor-
ing function would give the true accuracy rates (e.g., F-
score, the combined labeled precision and recall rates).
In practice, accuracy is approximated by some notion
of confidence. For example, one easy-to-compute scor-
ing function measures the conditional probability of the
(most likely) parse. If a high probability is assigned, the
parser is said to be confident in the label it produced.
In our experimental studies, we considered the selec-
tion methods? interaction with two scoring functions: an
oracle scoring function fF-score that returns the F-score
of the parse as measured against a gold standard, and a
2In our experiments, both parsers use the same scoring func-
tion.
practical scoring function fprob that returns the condi-
tional probability of the parse.3
3.2 Selection methods
Based on the scores assigned by the scoring function,
the selection method chooses a subset of the parser la-
beled sentences that best satisfy some selection criteria.
One such criterion is the accuracy of the labeled exam-
ples, which may be estimated by the teacher parser?s con-
fidence in its labels. However, the examples that the
teacher correctly labeled may not be those that the stu-
dent needs. We hypothesize that the training utility of
the examples for the student parser is another important
criterion.
Training utility measures the improvement a parser
would make if that sentence were correctly labeled and
added to the training set. Like accuracy, the utility of
an unlabeled sentence is difficult to quantify; therefore,
we approximate it with values that can be computed from
features of the sentence. For example, sentences contain-
ing many unknown words may have high training util-
ity; so might sentences that a parser has trouble parsing.
Under the co-training framework, we estimate the train-
ing utility of a sentence for the student by comparing the
score the student assigned to its parse (according to its
scoring function) against the score the teacher assigned
to its own parse.
To investigate how the selection criteria of utility and
accuracy affect the co-training process, we considered a
number of selection methods that satisfy the requirements
of accuracy and training utility to varying degrees. The
different selection methods are shown below. For each
method, a sentence (as labeled by the teacher parser) is
selected if:
? above-n (Sabove-n): the score of the teacher?s parse
(using its scoring function) ? n.
? difference (Sdiff-n): the score of the teacher?s parse
is greater than the score of the student?s parse by
some threshold n.
? intersection (Sint-n): the score of the teacher?s parse
is in the set of the teacher?s n percent highest-
scoring labeled sentences, and the score of the stu-
dent?s parse for the same sentence is in the set of
the student?s n percent lowest-scoring labeled sen-
tences.
Each selection method has a control parameter, n, that
determines the number of labeled sentences to add at each
co-training iteration. It also serves as an indirect control
3A nice property of using conditional probability,
Pr(parse|sentence), as the scoring function is that it
normalizes for sentence length.
of the number of errors added to the training set. For ex-
ample, the Sabove-n method would allow more sentences
to be selected if n was set to a low value (with respect to
the scoring function); however, this is likely to reduce the
accuracy rate of the training set.
The above-n method attempts to maximize the accu-
racy of the data (assuming that parses with higher scores
are more accurate). The difference method attempts to
maximize training utility: as long as the teacher?s label-
ing is more accurate than that of the student, it is cho-
sen, even if its absolute accuracy rate is low. The inter-
section method attempts to maximize both: the selected
sentences are accurately labeled by the teacher and incor-
rectly labeled by the student.
4 Experiments
Experiments were performed to compare the effect of
the selection methods on co-training and corrected co-
training. We consider a selection method, S1, superior
to another, S2, if, when a large unlabeled pool of sen-
tences has been exhausted, the examples selected by S1
(as labeled by the machine, and possibly corrected by the
human) improve the parser more than those selected by
S2. All experiments shared the same general setup, as
described below.
4.1 Experimental Setup
For two parsers to co-train, they should generate com-
parable output but use independent statistical models.
In our experiments, we used a lexicalized context free
grammar parser developed by Collins (1999), and a lex-
icalized Tree Adjoining Grammar parser developed by
Sarkar (2002). Both parsers were initialized with some
seed data. Since the goal is to minimize human annotated
data, the size of the seed data should be small. In this pa-
per we used a seed set size of 1, 000 sentences, taken from
section 2 of the Wall Street Journal (WSJ) Penn Tree-
bank. The total pool of unlabeled sentences was the re-
mainder of sections 2-21 (stripped of their annotations),
consisting of about 38,000 sentences. The cache size is
set at 500 sentences. We have explored using different
settings for the seed set size (Steedman et al, 2003).
The parsers were evaluated on unseen test sentences
(section 23 of the WSJ corpus). Section 0 was used as
a development set for determining parameters. The eval-
uation metric is the Parseval F-score over labeled con-
stituents: F-score = 2?LR?LPLR+LP , where LP and LR
are labeled precision and recall rate, respectively. Both
parsers were evaluated, but for brevity, all results reported
here are for the Collins parser, which received higher Par-
seval scores.
80
80.5
81
81.5
82
82.5
83
83.5
84
0 2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-70%diff-10%int-60%No selection(Human annotated)
80
80.5
81
81.5
82
82.5
83
83.5
84
0 2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-90%diff-10%int-30%No selection(Human annotated)
(a) (b)
Figure 2: A comparison of selection methods using the oracle scoring function, fF-score, controlling for the label
quality of the training data. (a) The average accuracy rates are about 85%. (b) The average accuracy rates (except for
those selected by Sdiff-10%) are about 95%.
4.2 Experiment 1: Selection Methods and
Co-Training
We first examine the effect of the three selection meth-
ods on co-training without correction (i.e., the chosen
machine-labeled training examples may contain errors).
Because the selection decisions are based on the scores
that the parsers assign to their outputs, the reliability of
the scoring function has a significant impact on the per-
formance of the selection methods. We evaluate the ef-
fectiveness of the selection methods using two scoring
functions. In Section 4.2.1, each parser assesses its out-
put with an oracle scoring function that returns the Par-
seval F-score of the output (as compared to the human
annotated gold-standard). This is an idealized condition
that gives us direct control over the error rate of the la-
beled training data. By keeping the error rates constant,
our goal is to determine which selection method is more
successful in finding sentences with high training utility.
In Section 4.2.2 we replace the oracle scoring function
with fprob, which returns the conditional probability of
the best parse as the score. We compare how the selection
methods? performances degrade under the realistic con-
dition of basing selection decisions on unreliable parser
output assessment scores.
4.2.1 Using the oracle scoring function, fF-score
The goal of this experiment is to evaluate the selection
methods using a reliable scoring function. We therefore
use an oracle scoring function, fF-score, which guaran-
tees a perfect assessment of the parser?s output. This,
however, may be too powerful. In practice, we expect
even a reliable scoring function to sometimes assign high
scores to inaccurate parses. We account for this effect by
adjusting the selection method?s control parameter to af-
fect two factors: the accuracy rate of the newly labeled
training data, and the number of labeled sentences added
at each training iteration. A relaxed parameter setting
adds more parses to the training data, but also reduces
the accuracy of the training data.
Figure 2 compares the effect of the three selection
methods on co-training for the relaxed (left graph) and
the strict (right graph) parameter settings. Each curve in
the two graphs charts the improvement in the parser?s ac-
curacy in parsing the test sentences (y-axis) as it is trained
on more data chosen by its selection method (x-axis).
The curves have different endpoints because the selection
methods chose a different number of sentences from the
same 38K unlabeled pool. For reference, we also plotted
the improvement of a fully-supervised parser (i.e., trained
on human-annotated data, with no selection).
For the more relaxed setting, the parameters are chosen
so that the newly labeled training data have an average
accuracy rate of about 85%:
? Sabove-70% requires the labels to have an F-score ?
70%. It adds about 330 labeled sentences (out of the
500 sentence cache) with an average accuracy rate
of 85% to the training data per iteration.
? Sdiff-10% requires the score difference between the
teacher?s labeling and the student?s labeling to be at
least 10%. It adds about 50 labeled sentences with
an average accuracy rate of 80%.
? Sint-60% requires the teacher?s parse to be in the
top 60% of its output and the student?s parse for the
same sentence to be in its bottom 60%. It adds about
150 labeled sentences with an average accuracy rate
of 85%.
Although none rivals the parser trained on human an-
notated data, the selection method that improves the
parser the most is Sdiff-10%. One interpretation is that
the training utility of the examples chosen by Sdiff-10%
outweighs the cost of errors introduced into the training
data. Another interpretation is that the other two selection
methods let in too many sentences containing errors. In
the right graph, we compare the same Sdiff-10% with the
other two selection methods using stricter control, such
that the average accuracy rate for these methods is now
about 95%:
? Sabove-90% now requires the parses to be at least
90% correct. It adds about 150 labeled sentences
per iteration.
? Sint-30% now requires the teacher?s parse to be in
the top 30% of its output and the student?s parse for
the same sentence in its bottom 30%. It adds about
15 labeled sentences.
The stricter control on Sabove-90% improved the
parser?s performance, but not enough to overtake
Sdiff-10% after all the sentences in the unlabeled pool
had been considered, even though the training data of
Sdiff-10% contained many more errors. Sint-30% has a
faster initial improvement4, closely tracking the progress
of the fully-supervised parser. However, the stringent re-
quirement exhausted the unlabeled data pool before train-
ing the parser to convergence. Sint-30% might continue
to help the parser to improve if it had access to more un-
labeled data, which is easier to acquire than annotated
data5.
Comparing the three selection methods under both
strict and relaxed control settings, the results suggest that
training utility is an important criterion in selecting train-
ing examples, even at the cost of reduced accuracy.
4.2.2 Using the fprob scoring function
To determine the effect of unreliable scores on the se-
lection methods, we replace the oracle scoring function,
fF-score, with fprob, which approximates the accuracy
of a parse with its conditional probability. Although this
is a poor estimate of accuracy (especially when computed
from a partially trained parser), it is very easy to compute.
The unreliable scores also reduce the correlation between
the selection control parameters and the level of errors in
the training data. In this experiment, we set the parame-
ters for all three selection methods so that approximately
4A fast improvement rate is not a central concern here, but
it will be more relevant for corrected co-training.
5This oracle experiment is bounded by the size of the anno-
tated portion of the WSJ corpus.
79.8
80
80.2
80.4
80.6
80.8
81
81.2
1000 1500 2000 2500 3000 3500 4000 4500 5000
Par
sin
g A
ccu
rac
y o
n T
est
 Da
ta (F
scor
e)
Number of Training Sentences
above-70%diff-30%int-30%
Figure 3: A comparison of selection methods using the
conditional probability scoring function, fprob.
30-50 sentences were added to the training data per iter-
ation. The average accuracy rate of the training data for
Sabove-70% was about 85%, and the rate for Sdiff-30%
and Sint-30% was about 75%.
As expected, the parser performances of all three selec-
tion methods using fprob (shown in Figure 3) are lower
than using fF-score (see Figure 2). However, Sdiff-30%
and Sint-30% helped the co-training parsers to improve
with a 5% error reduction (1% absolute difference) over
the parser trained only on the initial seed data. In con-
trast, despite an initial improvement, using Sabove-70%
did not help to improve the parser. In their experiments on
NP identifiers, Pierce and Cardie (2001) observed a sim-
ilar effect. They hypothesize that co-training does not
scale well for natural language learning tasks that require
a huge amount of training data because too many errors
are accrued over time. Our experimental results suggest
that the use of training utility in the selection process can
make co-training parsers more tolerant to these accumu-
lated errors.
4.3 Experiment 2: Selection Methods and
Corrected Co-training
To address the problem of the training data accumulating
too many errors over time, Pierce and Cardie proposed
a semi-supervised variant of co-training called corrected
co-training, which allows a human annotator to review
and correct the output of the parsers before adding it to
the training data. The main selection criterion in their
co-training system is accuracy (approximated by confi-
dence). They argue that selecting examples with nearly
correct labels would require few manual interventions
from the annotator.
We hypothesize that it may be beneficial to consider
the training utility criterion in this framework as well.
We perform experiments to determine whether select-
ing fewer (and possibly less accurately labeled) exam-
80
81
82
83
84
85
86
87
2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-90%diff-10%int-30%No selection
80
81
82
83
84
85
86
87
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Constituents to Correct in the Training Data
above-90%diff-10%int-30%No selection
(a) (b)
Figure 4: A comparison of selection methods for corrected co-training using fF-score (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
ples with higher training utility would require less effort
from the annotator. In our experiments, we simulated
the interactive sample selection process by revealing the
gold standard. As before, we compare the three selection
methods using both fF-score and fprob as scoring func-
tions.6
4.3.1 Using the oracle scoring function, fF-score
Figure 4 shows the effect of the three selection meth-
ods (using the strict parameter setting) on corrected co-
training. As a point of reference, we plot the improve-
ment rate for a fully supervised parser (same as the one
in Figure 2). In addition to charting the parser?s perfor-
mance in terms of the number of labeled training sen-
tences (left graph), we also chart the parser?s performance
in terms of the the number of constituents the machine
mislabeled (right graph). The pair of graphs indicates the
amount of human effort required: the left graph shows
the number of sentences the human has to check, and the
right graph shows the number of constituents the human
has to correct.
Comparing Sabove-90% and Sdiff-10%, we see that
Sdiff-10% trains a better parser than Sabove-90% when all
the unlabeled sentences have been considered. It also im-
proves the parser using a smaller set of training exam-
ples. Thus, for the same parsing performance, it requires
the human to check fewer sentences than Sabove-90% and
the reference case of no selection (Figure 4(a)). On the
other hand, because the labeled sentences selected by
Sdiff-10% contain more mistakes than those selected by
Sabove-90%, Sdiff-10% requires slightly more corrections
6The selection control parameters are the same as the previ-
ous set of experiments, using the strict setting (i.e., Figure 2(b))
for fF-score.
than Sabove-90% for the same level of parsing perfor-
mance; though both require fewer corrections than the
reference case of no selection (Figure 4(b)). Because
the amount of effort spent by the annotator depends on
the number of sentences checked as well as the amount
of corrections made, whether Sdiff-10% or Sabove-90% is
more effort reducing may be a matter of the annotator?s
preference.
The selection method that improves the parser at the
fastest rate is Sint-30%. For the same parser performance
level, it selects the fewest number of sentences for a hu-
man to check and requires the human to make the least
number of corrections. However, as we have seen in the
earlier experiment, very few sentences in the unlabeled
pool satisfy its stringent criteria, so it ran out of data be-
fore the parser was trained to convergence. At this point
we cannot determine whether Sint-30% might continue to
improve the parser if we used a larger set of unlabeled
data.
4.3.2 Using the fprob scoring function
We also consider the effect of unreliable scores in the
corrected co-training framework. A comparison between
the selection methods using fprob is reported in Figure
5. The left graph charts parser performance in terms of
the number of sentences the human must check; the right
charts parser performance in terms of the number of con-
stituents the human must correct. As expected, the unreli-
able scoring function degrades the effectiveness of the se-
lection methods; however, compared to its unsupervised
counterpart (Figure 3), the degradation is not as severe.
In fact, Sdiff-30% and Sint-30% still require fewer train-
ing data than the reference parser. Moreover, consistent
with the other experiments, the selection methods that at-
tempt to maximize training utility achieve better parsing
performance than Sabove-70%. Finally, in terms of reduc-
ing human effort, the three selection methods require the
human to correct comparable amount of parser errors for
the same level of parsing performance, but for Sdiff-30%
and Sint-30%, fewer sentences need to be checked.
4.3.3 Discussion
Corrected co-training can be seen as a form of active
learning, whose goal is to identify the smallest set of un-
labeled data with high training utility for the human to
label. Active learning can be applied to a single learner
(Lewis and Catlett, 1994) and to multiple learners (Fre-
und et al, 1997; Engelson and Dagan, 1996; Ngai and
Yarowsky, 2000). In the context of parsing, all previ-
ous work (Thompson et al, 1999; Hwa, 2000; Tang et
al., 2002) has focussed on single learners. Corrected co-
training is the first application of active learning for mul-
tiple parsers. We are currently investigating comparisons
to the single learner approaches.
Our approach is similar to co-testing (Muslea et al,
2002), an active learning technique that uses two classi-
fiers to find contentious examples (i.e., data for which the
classifiers? labels disagree) for a human to label. There is
a subtle but significant difference, however, in that their
goal is to reduce the total number of labeled training ex-
amples whereas we also wish to reduce the number of
corrections made by the human. Therefore, our selection
methods must take into account the quality of the parse
produced by the teacher in addition to how different its
parse is from the one produced by the student. The inter-
section method precisely aims at selecting sentences that
satisfy both requirements. Exploring different selection
methods is part of our on-going research effort.
5 Conclusion
We have considered three selection methods that have dif-
ferent priorities in balancing the two (often competing)
criteria of accuracy and training utility. We have em-
pirically compared their effect on co-training, in which
two parsers label data for each other, as well as corrected
co-training, in which a human corrects the parser labeled
data before adding it to the training set. Our results sug-
gest that training utility is an important selection criterion
to consider, even at the cost of potentially reducing the ac-
curacy of the training data. In our empirical studies, the
selection method that aims to maximize training utility,
Sdiff-n, consistently finds better examples than the one
that aims to maximize accuracy, Sabove-n. Our results
also suggest that the selection method that aims to maxi-
mize both accuracy and utility, Sint-n, shows promise in
improving co-training parsers and in reducing human ef-
fort for corrected co-training; however, a much larger un-
labeled data set is needed to verify the benefit of Sint-n.
The results of this study indicate the need for scor-
ing functions that are better estimates of the accuracy of
the parser?s output than conditional probabilities. Our
oracle experiments show that, by using effective selec-
tion methods, the co-training process can improve parser
peformance even when the newly labeled parses are
not completely accurate. This suggests that co-training
may still be beneficial when using a practical scoring
function that might only coarsely distinguish accurate
parses from inaccurate parses. Further avenues to ex-
plore include the development of selection methods to
efficiently approximate maximizing the objective func-
tion of parser agreement on unlabeled data, following the
work of Dasgupta et al (2002) and Abney (2002). Also,
co-training might be made more effective if partial parses
were used as training data. Finally, we are conducting ex-
periments to compare corrected co-training with other ac-
tive learning methods. We hope these studies will reveal
ways to combine the strengths of co-training and active
learning to make better use of unlabeled data.
Acknowledgments
This work has been supported, in part, by NSF/DARPA
funded 2002 Human Language Engineering Workshop
at JHU, EPSRC grant GR/M96889, the Department of
Defense contract RD-02-5700, and ONR MURI Con-
tract FCPO.810548265. We would like to thank Chris
Callison-Burch, Michael Collins, John Henderson, Lil-
lian Lee, Andrew McCallum, and Fernando Pereira for
helpful discussions; to Ric Crabbe, Adam Lopez, the par-
ticipants of CS775 at Cornell University, and the review-
ers for their comments on this paper.
References
Steven Abney. 2002. Bootstrapping. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics, pages 360?367, Philadelphia, PA.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings of the
11th Annual Conference on Computational Learning Theory,
pages 92?100, Madison, WI.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Annual Meeting of the NAACL.
David Cohn, Les Atlas, and Richard Ladner. 1994. Improv-
ing generalization with active learning. Machine Learning,
15(2):201?221.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
Sanjoy Dasgupta, Michael Littman, and David McAllester.
2002. PAC generalization bounds for co-training. In T. G.
Dietterich, S. Becker, and Z. Ghahramani, editors, Advances
80
81
82
83
84
85
86
87
2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-70%diff-30%int-30%No selection
80
81
82
83
84
85
86
87
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Constituents to Correct in the Training Data
above-70%diff-30%int-30%No selection
(a) (b)
Figure 5: A comparison of selection methods for corrected co-training using fprob (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
in Neural Information Processing Systems 14, Cambridge,
MA. MIT Press.
Sean P. Engelson and Ido Dagan. 1996. Minimizing manual
annotation cost in supervised training from copora. In Pro-
ceedings of the 34th Annual Meeting of the ACL, pages 319?
326.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query by com-
mittee algorithm. Machine Learning, 28(2-3):133?168.
Sally Goldman and Yan Zhou. 2000. Enhancing supervised
learning with unlabeled data. In Proceedings of the 17th In-
ternational Conference on Machine Learning, Stanford, CA.
Rebecca Hwa. 2000. Sample selection for statistical grammar
induction. In Proceedings of the 2000 Joint SIGDAT Confer-
ence on EMNLP and VLC, pages 45?52, Hong Kong, China,
October.
David D. Lewis and Jason Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Proceedings
of the Eleventh International Conference on Machine Learn-
ing, pages 148?156.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ion Muslea, Steve Minton, and Craig Knoblock. 2002. Selec-
tive sampling with redundant views. In Proceedings of the
Seventeenth National Conference on Artificial Intelligence,
pages 621?626.
Grace Ngai and David Yarowsky. 2000. Rule writing or an-
notation: Cost-efficient resource usage for base noun phrase
chunking. In Proceedings of the 38th Annual Meeting of the
ACL, pages 117?125, Hong Kong, China, October.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets. In
Proceedings of the Empirical Methods in NLP Conference,
Pittsburgh, PA.
Anoop Sarkar. 2001. Applying co-training methods to statisti-
cal parsing. In Proceedings of the 2nd Annual Meeting of the
NAACL, pages 95?102, Pittsburgh, PA.
Anoop Sarkar. 2002. Statistical Parsing Algorithms for Lexi-
calized Tree Adjoining Grammars. Ph.D. thesis, University
of Pennsylvania.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark,
Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven
Baker, and Jeremiah Crim. 2003. Bootstrapping statistical
parsers from small datasets. In The Proceedings of the An-
nual Meeting of the European Chapter of the ACL. To ap-
pear.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active
learning for statistical natural language parsing. In Proceed-
ings of the 40th Annual Meeting of the ACL, pages 120?127,
July.
Cynthia A. Thompson, Mary Elaine Califf, and Raymond J.
Mooney. 1999. Active learning for natural language pars-
ing and information extraction. In Proceedings of ICML-99,
pages 406?414, Bled, Slovenia.
A Smorgasbord of Features for Statistical Machine Translation
Franz Josef Och
USC/ISI
Daniel Gildea
U. of Rochester
Sanjeev Khudanpur
Johns Hopkins U.
Anoop Sarkar
Simon Fraser U.
Kenji Yamada
Xerox/XRCE
Alex Fraser
USC/ISI
Shankar Kumar
Johns Hopkins U.
Libin Shen
U. of Pennsylvania
David Smith
Johns Hopkins U.
Katherine Eng
Stanford U.
Viren Jain
U. of Pennsylvania
Zhen Jin
Mt. Holyoke
Dragomir Radev
U. of Michigan
Abstract
We describe a methodology for rapid exper-
imentation in statistical machine translation
which we use to add a large number of features
to a baseline system exploiting features from a
wide range of levels of syntactic representation.
Feature values were combined in a log-linear
model to select the highest scoring candidate
translation from an n-best list. Feature weights
were optimized directly against the BLEU eval-
uation metric on held-out data. We present re-
sults for a small selection of features at each
level of syntactic representation.
1 Introduction
Despite the enormous progress in machine translation
(MT) due to the use of statistical techniques in recent
years, state-of-the-art statistical systems often produce
translations with obvious errors. Grammatical errors in-
clude lack of a main verb, wrong word order, and wrong
choice of function words. Frequent problems of a less
grammatical nature include missing content words and
incorrect punctuation.
In this paper, we attempt to address these problems by
exploring a variety of new features for scoring candidate
translations. A high-quality statistical translation system
is our baseline, and we add new features to the exist-
ing set, which are then combined in a log-linear model.
To allow an easy integration of new features, the base-
line system provides an n-best list of candidate transla-
tions which is then reranked using the new features. This
framework allows us to incorporate different types of fea-
tures, including features based on syntactic analyses of
the source and target sentences, which we hope will ad-
dress the grammaticality of the translations, as well as
lower-level features. As we work on n-best lists, we can
easily use global sentence-level features.
We begin by describing our baseline system and the
n-best rescoring framework within which we conducted
our experiments. We then present a selection of new fea-
tures, progressing from word-level features to those based
to part-of-speech tags and syntactic chunks, and then to
features based on Treebank-based syntactic parses of the
source and target sentences.
2 Log-linear Models for Statistical MT
The goal is the translation of a text given in some source
language into a target language. We are given a source
(?Chinese?) sentence f = fJ1 = f1, . . . , fj , . . . , fJ ,
which is to be translated into a target (?English?) sentence
e = eI1 = e1, . . . , ei, . . . , eI Among all possible target
sentences, we will choose the sentence with the highest
probability:
e?I1 = argmax
eI1
{Pr(eI1|f
J
1 )} (1)
As an alternative to the often used source-channel ap-
proach (Brown et al, 1993), we directly model the pos-
terior probability Pr(eI1|fJ1 ) (Och and Ney, 2002) us-
ing a log-linear combination of feature functions. In
this framework, we have a set of M feature functions
hm(eI1, f
J
1 ),m = 1, . . . ,M . For each feature function,
there exists a model parameter ?m,m = 1, . . . ,M . The
direct translation probability is given by:
Pr(eI1|f
J
1 ) =
exp[
?M
m=1 ?mhm(e
I
1, f
J
1 )]
?
e?I1
exp[
?M
m=1 ?mhm(e
?I
1, f
J
1 )]
(2)
We obtain the following decision rule:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(e
I
1, f
J
1 )
}
(3)
The standard criterion for training such a log-linear
model is to maximize the probability of the parallel train-
ing corpus consisting of S sentence pairs {(fs, es) : s =
1, . . . , S}. However, this does not guarantee optimal per-
formance on the metric of translation quality by which
our system will ultimately be evaluated. For this reason,
we optimize the parameters directly against the BLEU
metric on held-out data. This is a more difficult optimiza-
tion problem, as the search space is no longer convex.
Figure 1: Example segmentation of Chinese sentence and
its English translation into alignment templates.
However, certain properties of the BLEU metric can be
exploited to speed up search, as described in detail by
Och (2003). We use this method of optimizing feature
weights throughout this paper.
2.1 Baseline MT System: Alignment Templates
Our baseline MT system is the alignment template system
described in detail by Och, Tillmann, and Ney (1999)
and Och and Ney (2004). In the following, we give a
short description of this baseline model.
The probability model of the alignment template sys-
tem for translating a sentence can be thought of in distinct
stages. First, the source sentence words fJ1 are grouped to
phrases f?K1 . For each phrase f? an alignment template z is
chosen and the sequence of chosen alignment templates
is reordered (according to piK1 ). Then, every phrase f?
produces its translation e? (using the corresponding align-
ment template z). Finally, the sequence of phrases e?K1
constitutes the sequence of words eI1.
Our baseline system incorporated the following feature
functions:
Alignment Template Selection Each alignment
template is chosen with probability p(z|f?), estimated by
relative frequency. The corresponding feature function in
our log-linear model is the log probability of the product
of p(z|f?) for all used alignment templates used.
Word Selection This feature is based on the lexical
translation probabilities p(e|f), estimated using relative
frequencies according to the highest-probability word-
level alignment for each training sentence. A translation
probability conditioned on the source and target position
within the alignment template p(e|f, i, j) is interpolated
with the position-independent probability p(e|f).
Phrase Alignment This feature favors monotonic
alignment at the phrase level. It measures the ?amount
of non-monotonicity? by summing over the distance (in
the source language) of alignment templates which are
consecutive in the target language.
Language Model Features As a language model
feature, we use a standard backing off word-based tri-
gram language model (Ney, Generet, and Wessel, 1995).
The baseline system actually includes four different lan-
guage model features trained on four different corpora:
the news part of the bilingual training data, a large Xin-
hua news corpus, a large AFP news corpus, and a set of
Chinese news texts downloaded from the web.
Word/Phrase Penalty This word penalty feature
counts the length in words of the target sentence. Without
this feature, the sentences produced tend to be too short.
The phrase penalty feature counts the number of phrases
produced, and can allow the model to prefer either short
or long phrases.
Phrases from Conventional Lexicon The baseline
alignment template system makes use of the Chinese-
English lexicon provided by LDC. Each lexicon entry is
a potential phrase translation pair in the alignment tem-
plate system. To score the use of these lexicon entries
(which have no normal translation probability), this fea-
ture function counts the number of times such a lexicon
entry is used.
Additional Features A major advantage of the log-
linear modeling approach is that it is easy to add new
features. In this paper, we explore a variety of features
based on successively deeper syntactic representations of
the source and target sentences, and their alignment. For
each of the new features discussed below, we added the
feature value to the set of baseline features, re-estimated
feature weights on development data, and obtained re-
sults on test data.
3 Experimental Framework
We worked with the Chinese-English data from the recent
evaluations, as both large amounts of sentence-aligned
training corpora and multiple gold standard reference
translations are available. This is a standard data set,
making it possible to compare results with other systems.
In addition, working on Chinese allows us to use the ex-
isting Chinese syntactic treebank and parsers based on it.
For the baseline MT system, we distinguish the fol-
lowing three different sentence- or chunk-aligned parallel
training corpora:
? training corpus (train): This is the basic training
corpus used to train the alignment template transla-
tion model (word lexicon and phrase lexicon). This
corpus consists of about 170M English words. Large
parts of this corpus are aligned on a sub-sentence
level to avoid the existence of very long sentences
which would be filtered out in the training process
to allow a manageable word alignment training.
? development corpus (dev): This is the training cor-
pus used in discriminative training of the model-
parameters of the log-linear translation model. In
most experiments described in this report this cor-
pus consists of 993 sentences (about 25K words) in
both languages.
? test corpus (test): This is the test corpus used to
assess the quality of the newly developed feature
functions. It consists of 878 sentences (about 25K
words).
For development and test data, we have four English (ref-
erence) translations for each Chinese sentence.
3.1 Reranking, n-best lists, and oracles
For each sentence in the development, test, and the blind
test corpus a set of 16,384 different alternative transla-
tions has been produced using the baseline system. For
extracting the n-best candidate translations, an A* search
is used. These n-best candidate translations are the basis
for discriminative training of the model parameters and
for re-ranking.
We used n-best reranking rather than implementing
new search algorithms. The development of efficient
search algorithms for long-range dependencies is very
complicated and a research topic in itself. The rerank-
ing strategy enabled us to quickly try out a lot of new
dependencies, which would not have been be possible if
the search algorithm had to be changed for each new de-
pendency.
On the other hand, the use of n-best list rescoring lim-
its the possibility of improvements to what is available
in the n-best list. Hence, it is important to analyze the
quality of the n-best lists by determining how much of an
improvement would be possible given a perfect reranking
algorithm. We computed the oracle translations, that is,
the set of translations from our n-best list that yields the
best BLEU score.1
We use the following two methods to compute the
BLEU score of an oracle translation:
1. optimal oracle (opt): We select the oracle sentences
which give the highest BLEU score compared to the
set of 4 reference translations. Then, we compute
BLEU score of oracle sentences using the same set
of reference translations.
2. round-robin oracle (rr): We select four differ-
ent sets of oracle sentences which give the highest
BLEU score compared to each of the 4 references
translations. Then, we compute for each set of or-
acle sentences a BLEU score using always those
three references to score that have not been cho-
sen to select the oracle. Then, these 4 3-reference
BLEU scores are averaged.
1Note that due to the corpus-level holistic nature of the
BLEU score it is not trivial to compute the optimal set of oracle
translations. We use a greedy search algorithm for the oracle
translations that might find only a local optimum. Empirically,
we do not observe a dependence on the starting point, hence we
believe that this does not pose a significant problem.
Table 1: Oracle BLEU scores for different sizes of the
n-best list. The avBLEUr3 scores are computed with
respect to three reference translations averaged over the
four different choices of holding out one reference.
avBLEUr3[%] BLEUr4
n rr opt opt
human 35.8 -
1 28.3 28.3 31.6
4 29.1 30.8 34.5
16 29.9 33.2 37.3
64 30.6 35.6 38.7
256 31.3 37.8 42.8
1024 31.7 40.0 45.3
4096 32.0 41.8 47.3
The first method provides the theoretical upper bound of
what BLEU score can be obtained by rescoring a given n-
best list. Using this method with a 1000-best list, we ob-
tain oracle translations that outperform the BLEU score
of the human translations. The oracle translations achieve
113% against the human BLEU score on the test data
(Table 1), while the first best translations obtain 79.2%
against the human BLEU score. The second method uses
a different references for selection and scoring. Here, us-
ing an 1000-best list, we obtain oracle translations with a
relative human BLEU score of 88.5%.
Based on the results of the oracle experiment, and
in order to make rescoring computationally feasible for
features requiring significant computation for each hy-
pothesis, we used the top 1000 translation candidates for
our experiments. The baseline system?s BLEU score is
31.6% on the test set (equivalent to the 1-best oracle in
Table 1). This is the benchmark against which the contri-
butions of the additional features described in the remain-
der of this paper are to be judged.
3.2 Preprocessing
As a precursor to developing the various syntactic fea-
tures described in this report, the syntactic represen-
tations on which they are based needed to be com-
puted. This involved part-of-speech tagging, chunking,
and parsing both the Chinese and English side of our
training, development, and test sets.
Applying the part-of-speech tagger to the often un-
grammatical MT output from our n-best lists sometimes
led to unexpected results. Often the tagger tries to ?fix
up? ungrammatical sentences, for example by looking for
a verb when none is present:
China NNP 14 CD open JJ border NN
cities NNS achievements VBZ remarkable JJ
Here, although achievements has never been seen as a
verb in the tagger?s training data, the prior for a verb
in this position is high enough to cause a present tense
verb tag to be produced. In addition to the inaccura-
cies of the MT system, the difference in genre from the
tagger?s training text can cause problems. For example,
while our MT data include news article headlines with no
verb, headlines are not included in the Wall Street Journal
text on which the tagger is trained. Similarly, the tagger
is trained on full sentences with normalized punctuation,
leading it to expect punctuation at the end of every sen-
tence, and produce a punctuation tag even when the evi-
dence does not support it:
China NNP ?s POS economic JJ
development NN and CC opening VBG
up RP 14 CD border NN cities NNS
remarkable JJ achievements .
The same issues affect the parser. For example the
parser can create verb phrases where none exist, as in the
following example in which the tagger correctly did not
identify a verb in the sentence:
These effects have serious implications for designing
syntactic feature functions. Features such ?is there a verb
phrase? may not do what you expect. One solution would
be features that involve the probability of a parse subtree
or tag sequence, allowing us to ask ?how good a verb
phrase is it?? Another solution is more detailed features
examining more of the structure, such as ?is there a verb
phrase with a verb??
4 Word-Level Feature Functions
These features, directly based on the source and target
strings of words, are intended to address such problems as
translation choice, missing content words, and incorrect
punctuation.
4.1 Model 1 Score
We used IBM Model 1 (Brown et al, 1993) as one of the
feature functions. Since Model 1 is a bag-of-word trans-
lation model and it gives the sum of all possible alignment
probabilities, a lexical co-occurrence effect, or triggering
effect, is expected. This captures a sort of topic or seman-
tic coherence in translations.
As defined by Brown et al (1993), Model 1 gives a
probability of any given translation pair, which is
p(f |e; M1) =

(l + 1)m
m?
j=1
l?
i=0
t(fj |ei).
We used GIZA++ to train the model. The training data is
a subset (30 million words on the English side) of the en-
tire corpus that was used to train the baseline MT system.
For a missing translation word pair or unknown words,
where t(fj |ei) = 0 according to the model, a constant
t(fj |ei) = 10?40 was used as a smoothing value.
The average %BLEU score (average of the best four
among different 20 search initial points) is 32.5. We also
tried p(e|f ; M1) as feature function, but did not obtain
improvements which might be due to an overlap with the
word selection feature in the baseline system.
The Model 1 score is one of the best performing fea-
tures. It seems to ?fix? the tendency of our baseline sys-
tem to delete content words and it improves word selec-
tion coherence by the triggering effect. It is also possible
that the triggering effect might work on selecting a proper
verb-noun combination, or a verb-preposition combina-
tion.
4.2 Lexical Re-ordering of Alignment Templates
As shown in Figure 1 the alignment templates (ATs)
used in the baseline system can appear in various con-
figurations which we will call left/right-monotone and
left/right-continuous. We built 2 out of these 4 models to
distinguish two types of lexicalized re-ordering of these
ATs:
The left-monotone model computes the total proba-
bility of all ATs being left monotone: where the lower
left corner of the AT touches the upper right corner of the
previous AT. Note that the first word in the current AT
may or may not immediately follow the last word in the
previous AT. The total probability is the product over all
alignment templates i, either P (ATi is left-monotone) or
1 ? P (ATi is left-monotone).
The right-continuous model computes the total prob-
ability of all ATs being right continuous: where the
lower left corner of the AT touches the upper right cor-
ner of the previous AT and the first word in the cur-
rent AT immediately follows the last word in the pre-
vious AT. The total probability is the product over all
alignment templates i, either P (ATi is right-continuous)
or 1 ? P (ATi is right-continuous).
In both models, the probabilities P have been esti-
mated from the full training data (train).
5 Shallow Syntactic Feature Functions
By shallow syntax, we mean the output of the part-of-
speech tagger and chunkers. We hope that such features
can combine the strengths of tag- and chunk-based trans-
lation systems (Schafer and Yarowsky, 2003) with our
baseline system.
5.1 Projected POS Language Model
This feature uses Chinese POS tag sequences as surro-
gates for Chinese words to model movement. Chinese
words are too sparse to model movement, but an attempt
to model movement using Chinese POS may be more
successful. We hope that this feature will compensate for
a weak model of word movement in the baseline system.
Chinese POS sequences are projected to English us-
ing the word alignment. Relative positions are indicated
for each Chinese tag. The feature function was also tried
without the relative positions:
CD +0 M +1 NN +3 NN -1 NN +2 NN +3
14 (measure) open border cities
The table shows an example tagging of an English hy-
pothesis showing how it was generated from the Chinese
sentence. The feature function is the log probability out-
put by a trigram language model over this sequence. This
is similar to the HMM Alignment model (Vogel, Ney, and
Tillmann, 1996) but in this case movement is calculated
on the basis of parts of speech.
The Projected POS feature function was one of the
strongest performing shallow syntactic feature functions,
with a %BLEU score of 31.8. This feature function can
be thought of as a trade-off between purely word-based
models, and full generative models based upon shallow
syntax.
6 Tree-Based Feature Functions
Syntax-based MT has shown promise in the
work of, among others, Wu and Wong (1998) and
Alshawi, Bangalore, and Douglas (2000). We hope that
adding features based on Treebank-based syntactic
analyses of the source and target sentences will address
grammatical errors in the output of the baseline system.
6.1 Parse Tree Probability
The most straightforward way to integrate a statistical
parser in the system would be the use of the (log of the)
parser probability as a feature function. Unfortunately,
this feature function did not help to obtain better results
(it actually seems to significantly hurt performance).
To analyze the reason for this, we performed an ex-
periment to test if the used statistical parser assigns a
higher probability to presumably grammatical sentences.
The following table shows the average log probability as-
signed by the Collins parser to the 1-best (produced), or-
acle and the reference translations:
Hypothesis 1-best Oracle Reference
log(parseProb) -147.2 -148.5 -154.9
We observe that the average parser log-probability of
the 1-best translation is higher than the average parse
log probability of the oracle or the reference translations.
Hence, it turns out that the parser is actually assigning
higher probabilities to the ungrammatical MT output than
to the presumably grammatical human translations. One
reason for that is that the MT output uses fewer unseen
words and typically more frequent words which lead to
a higher language model probability. We also performed
experiments to balance this effect by dividing the parser
probability by the word unigram probability and using
this ?normalized parser probability? as a feature function,
but also this did not yield improvements.
6.2 Tree-to-String Alignment
A tree-to-string model is one of several syntax-
based translation models used. The model is a
conditional probability p(f |T (e)). Here, we used
a model defined by Yamada and Knight (2001) and
Yamada and Knight (2002).
Internally, the model performs three types of opera-
tions on each node of a parse tree. First, it reorders the
child nodes, such as changing VP ? VB NP PP into
VP ? NP PP VB. Second, it inserts an optional word at
each node. Third, it translates the leaf English words into
Chinese words. These operations are stochastic and their
probabilities are assumed to depend only on the node, and
are independent of other operations on the node, or other
nodes. The probability of each operation is automatically
obtained by a training algorithm, using about 780,000 En-
glish parse tree-Chinese sentence pairs. The probability
of these operations ?(eki,j) is assumed to depend on the
edge of the tree being modified, eki,j , but independent of
everything else, giving the following equation,
p(f |T (e)) =
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j) (4)
where ? varies over the possible alignments between the
f and e and ?(eki,j) is the particular operations (in ?) for
the edge eki,j .
The model is further extended to incorporate phrasal
translations performed at each node of the input parse
tree (Yamada and Knight, 2002). An English phrase cov-
ered by a node can be directly translated into a Chinese
phrase without regular reorderings, insertions, and leaf-
word translations.
The model was trained using about 780,000 English
parse tree-Chinese sentence pairs. There are about 3 mil-
lion words on the English side, and they were parsed by
Collins? parser.
Since the model is computationally expensive, we
added some limitations on the model operations. As the
base MT system does not produce a translation with a
big word jump, we restrict the model not to reorder child
nodes when the node covers more than seven words. For
a node that has more than four children, the reordering
probability is set to be uniform. We also introduced prun-
ing, which discards partial (subtree-substring) alignments
if the probability is lower than a threshold.
The model gives a sum of all possible alignment prob-
abilities for a pair of a Chinese sentence and an English
parse tree. We also calculate the probability of the best
alignment according to the model. Thus, we have the fol-
lowing two feature functions:
hTreeToStringSum(e, f) = log(
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
hTreeToStringViterbi(e, f) = log(max
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
As the model is computationally expensive, we sorted the
n-best list by the sentence length, and processed them
from the shorter ones to the longer ones. We used 10
CPUs for about five days, and 273/997 development sen-
tences and 237/878 test sentences were processed.
The average %BLEU score (average of the best four
among different 20 search initial points) was 31.7 for
both hTreeToStringSum and hTreeToStringViterbi. Among the pro-
cessed development sentences, the model preferred the
oracle sentences over the produced sentence in 61% of
the cases.
The biggest problem of this model is that it is compu-
tationally very expensive. It processed less than 30% of
the n-best lists in long CPU hours. In addition, we pro-
cessed short sentences only. For long sentences, it is not
practical to use this model as it is.
6.3 Tree-to-Tree Alignment
A tree-to-tree translation model makes use of syntac-
tic tree for both the source and target language. As in
the tree-to-string model, a set of operations apply, each
with some probability, to transform one tree into another.
However, when training the model, trees for both the
source and target languages are provided, in our case
from the Chinese and English parsers.
We began with the tree-to-tree alignment model pre-
sented by Gildea (2003). The model was extended to han-
dle dependency trees, and to make use of the word-level
alignments produced by the baseline MT system. The
probability assigned by the tree-to-tree alignment model,
given the word-level alignment with which the candidate
translation was generated, was used as a feature in our
rescoring system.
We trained the parameters of the tree transformation
operations on 42,000 sentence pairs of parallel Chinese-
English data from the Foreign Broadcast Information Ser-
vice (FBIS) corpus. The lexical translation probabili-
ties Pt were trained using IBM Model 1 on the 30 mil-
lion word training corpus. This was done to overcome
the sparseness of the lexical translation probabilities es-
timated while training the tree-to-tree model, which was
not able to make use of as much training data.
As a test of the tree-to-tree model?s discrimination, we
performed an oracle experiment, comparing the model
scores on the first sentence in the n-best list with candi-
date giving highest BLEU score. On the 1000-best list for
the 993-sentence development set, restricting ourselves
to sentences with no more than 60 words and a branching
factor of no more than five in either the Chinese or En-
glish tree, we achieved results for 480, or 48% of the 993
sentences. Of these 480, the model preferred the pro-
duced over the oracle 52% of the time, indicating that
it does not in fact seem likely to significantly improve
BLEU scores when used for reranking. Using the prob-
ability of the source Chinese dependency parse aligning
with the n-best hypothesis dependency parse as a feature
function, making use of the word-level alignments, yields
a 31.6 %BLEU score ? identical to our baseline.
6.4 Markov Assumption for Tree Alignments
The tree-based feature functions described so far have the
following limitations: full parse tree models are expen-
sive to compute for long sentences and for trees with flat
constituents and there is limited reordering observed in
the n-best lists that form the basis of our experiments. In
addition to this, higher levels of parse tree are rarely ob-
served to be reordered between source and target parse
trees.
In this section we attack these problems using a simple
Markov model for tree-based alignments. It guarantees
tractability: compared to a coverage of approximately
30% of the n-best list by the unconstrained tree-based
models, using the Markov model approach provides 98%
coverage of the n-best list. In addition, this approach is
robust to inaccurate parse trees.
The algorithm works as follows: we start with word
alignments and two parameters: n for maximum number
of words in tree fragment and k for maximum height of
tree fragment. We proceed from left to right in the Chi-
nese sentence and incrementally grow a pair of subtrees,
one subtree in Chinese and the other in English, such that
each word in the Chinese subtree is aligned to a word in
the English subtree. We grow this pair of subtrees un-
til we can no longer grow either subtree without violat-
ing the two parameter values n and k. Note that these
aligned subtree pairs have properties similar to alignment
templates. They can rearrange in complex ways between
source and target. Figure 2 shows how subtree-pairs for
parameters n = 3 and k = 3 can be drawn for this
sentence pair. In our experiments, we use substantially
bigger tree fragments with parameters set to n = 8 and
k = 9.
Once these subtree-pairs have been obtained, we can
easily assert a Markov assumption for the tree-to-tree and
tree-to-string translation models that exploits these pair-
ings. Let consider a sentence pair in which we have dis-
covered n subtree-pairs which we can call Frag0, . . .,
Fragn. We can then compute a feature function for the
sentence pair using the tree-to-string translation model as
follows:
hMarkovTreeToString =
logPtree-to-string(Frag0) + . . . + logPtree-to-string(Fragn)
Using this Markov assumption on tree alignments with
Figure 2: Markov assumption on tree alignments.
the Tree to String model described in Section 6.2 we ob-
tain a coverage improvement to 98% coverage from the
original 30%. The accuracy of the tree to string model
also improved with a %BLEU score of 32.0 which is the
best performing single syntactic feature.
6.5 Using TAG elementary trees for scoring word
alignments
In this section, we consider another method for carving
up the full parse tree. However, in this method, instead of
subtree-pairs we consider a decomposition of parse trees
that provides each word with a fragment of the original
parse tree as shown in Figure 3. The formalism of Tree-
Adjoining Grammar (TAG) provides the definition what
each tree fragment should be and in addition how to de-
compose the original parse trees to provide the fragments.
Each fragment is a TAG elementary tree and the compo-
sition of these TAG elementary trees in a TAG deriva-
tion tree provides the decomposition of the parse trees.
The decomposition into TAG elementary trees is done by
augmenting the parse tree for source and target sentence
with head-word and argument (or complement) informa-
tion using heuristics that are common to most contempo-
rary statistical parsers and easily available for both En-
glish and Chinese. Note that we do not use the word
alignment information for the decomposition into TAG
elementary trees.
Once we have a TAG elementary tree per word,
we can create several models that score word align-
ments by exploiting the alignments between TAG ele-
mentary trees between source and target. Let tfi and
tei be the TAG elementary trees associated with the
aligned words fi and ei respectively. We experimented
with two models over alignments: unigram model over
alignments:
?
i P (fi, tfi , ei, tei) and conditional model:?
i P (ei, tei | fi, tfi) ? P (fi+1, tfi+1 | fi, tfi)
We trained both of these models using the SRI Lan-
guage Modeling Toolkit using 60K aligned parse trees.
We extracted 1300 TAG elementary trees each for Chi-
Figure 3: Word alignments with TAG elementary trees.
nese and for English. The unigram model gets a %BLEU
score of 31.7 and the conditional model gets a %BLEU
score of 31.9.
%BLEU
Baseline 31.6
IBM Model 1 p(f |e) 32.5
Tree-to-String Markov fragments 32.0
Right-continuous alignment template 32.0
TAG conditional bigrams 31.9
Left-monotone alignment template 31.9
Projected POS LM 31.8
Tree-to-String 31.7
TAG unigram 31.7
Tree-to-Tree 31.6
combination 32.9
Table 2: Results for the baseline features, each new fea-
ture added to the baseline features on its own, and a com-
bination of new features.
7 Conclusions
The use of discriminative reranking of an n-best list pro-
duced with a state-of-the-art statistical MT system al-
lowed us to rapidly evaluate the benefits of off-the-shelf
parsers, chunkers, and POS taggers for improving syntac-
tic well-formedness of the MT output. Results are sum-
marized in Table 2; the best single new feature improved
the %BLEU score from 31.6 to 32.5. The 95% confi-
dence intervals computed with the bootstrap resampling
method are about 0.8%. In addition to experiments with
single features we also integrated multiple features using
a greedy approach where we integrated at each step the
feature that most improves the BLEU score. This feature
integration produced a statistically significant improve-
ment of absolute 1.3% to 32.9 %BLEU score.
Our single best feature, and in fact the only single fea-
ture to produce a truly significant improvement, was the
IBM Model 1 score. We attribute its success that it ad-
dresses the weakness of the baseline system to omit con-
tent words and that it improves word selection by em-
ploying a triggering effect. We hypothesize that this al-
lows for better use of context in, for example, choosing
among senses of the source language word.
A major goal of this work was to find out if we can ex-
ploit annotated data such as treebanks for Chinese and
English and make use of state-of-the-art deep or shal-
low parsers to improve MT quality. Unfortunately, none
of the implemented syntactic features achieved a statisti-
cally significant improvement in the BLEU score. Poten-
tial reasons for this might be:
? As described in Section 3.2, the use of off-the-shelf
taggers and parsers has various problems due to vari-
ous mismatches between the parser training data and
our application domain. This might explain that the
use of the parser probability as feature function was
not successful. A potential improvement might be to
adapt the parser by retraining it on the full training
data that has been used by the baseline system.
? The use of a 1000-best list limits the potential im-
provements. It is possible that more improvements
could be obtained using a larger n-best list or a word
graph representation of the candidates.
? The BLEU score is possibly not sufficiently sensi-
tive to the grammaticality of MT output. This could
not only make it difficult to see an improvement in
the system?s output, but also potentially mislead the
BLEU-based optimization of the feature weights. A
significantly larger corpus for discriminative train-
ing and for evaluation would yield much smaller
confidence intervals.
? Our discriminative training technique, which di-
rectly optimizes the BLEU score on a development
corpus, seems to have overfitting problems with
large number of features. One could use a larger de-
velopment corpus for discriminative training or in-
vestigate alternative discriminative training criteria.
? The amount of annotated data that has been used
to train the taggers and parsers is two orders of
magnitude smaller than the parallel training data
that has been used to train the baseline system (or
the word-based features). Possibly, a comparable
amount of annotated data (e.g. a treebank with 100
million words) is needed to obtain significant im-
provements.
This is the first large scale integration of syntactic analy-
sis operating on many different levels with a state-of-the-
art phrase-based MT system. The methodology of using
a log-linear feature combination approach, discriminative
reranking of n-best lists computed with a state-of-the-art
baseline system allowed members of a large team to si-
multaneously experiment with hundreds of syntactic fea-
ture functions on a common platform.
Acknowledgments
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 0121285.
References
Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 2000.
Learning dependency translation models as collections of
finite state head transducers. Computational Linguistics,
26(1):45?60.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Gildea, Daniel. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41st Annual Meeting of the
Association for Computational Linguistics (ACL), Sapporo,
Japan.
Ney, Hermann, M. Generet, and Frank Wessel. 1995. Ex-
tensions of absolute discounting for language modeling. In
Proc. of the Fourth European Conf. on Speech Communica-
tion and Technology, Madrid, Spain.
Och, Franz Josef. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL), Sap-
poro, Japan.
Och, Franz Josef and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL), Philadel-
phia, PA.
Och, Franz Josef and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics. Accepted for Publication.
Och, Franz Josef, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical machine
translation. In Proc. of the Joint SIGDAT Conf. on Empiri-
cal Methods in Natural Language Processing and Very Large
Corpora, College Park, MD.
Schafer, Charles and David Yarowsky. 2003. Statistical ma-
chine translation using coercive two-level syntactic transduc-
tion. In Proc. of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Philadel-
phia, PA.
Vogel, Stephan, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational Lin-
guistics, Copenhagen, Denmark.
Wu, Dekai and H. Wong. 1998. Machine translation with a
stochastic grammatical channel. In COLING-ACL ?98: 36th
Annual Meeting of the Association for Computational Lin-
guistics and 17th Int. Conf. on Computational Linguistics,
Montreal, Canada.
Yamada, Kenji and Kevin Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of the 39th Annual Meet-
ing of the Association for Computational Linguistics (ACL),
Toulouse, France.
Yamada, Kenji and Kevin Knight. 2002. A decoder for syntax-
based MT. In Proc. of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Philadelphia,
PA.
Discriminative Reranking for Machine Translation
Libin Shen
Dept. of Comp. & Info. Science
Univ. of Pennsylvania
Philadelphia, PA 19104
libin@seas.upenn.edu
Anoop Sarkar
School of Comp. Science
Simon Fraser Univ.
Burnaby, BC V5A 1S6
anoop@cs.sfu.ca
Franz Josef Och
Info. Science Institute
Univ. of Southern California
Marina del Rey, CA 90292
och@isi.edu
Abstract
This paper describes the application of discrim-
inative reranking techniques to the problem of
machine translation. For each sentence in the
source language, we obtain from a baseline sta-
tistical machine translation system, a ranked   -
best list of candidate translations in the target
language. We introduce two novel perceptron-
inspired reranking algorithms that improve on
the quality of machine translation over the
baseline system based on evaluation using the
BLEU metric. We provide experimental results
on the NIST 2003 Chinese-English large data
track evaluation. We also provide theoretical
analysis of our algorithms and experiments that
verify that our algorithms provide state-of-the-
art performance in machine translation.
1 Introduction
The noisy-channel model (Brown et al, 1990) has been
the foundation for statistical machine translation (SMT)
for over ten years. Recently so-called reranking tech-
niques, such as maximum entropy models (Och and Ney,
2002) and gradient methods (Och, 2003), have been ap-
plied to machine translation (MT), and have provided
significant improvements. In this paper, we introduce
two novel machine learning algorithms specialized for
the MT task.
Discriminative reranking algorithms have also con-
tributed to improvements in natural language parsing
and tagging performance. Discriminative reranking al-
gorithms used for these applications include Perceptron,
Boosting and Support Vector Machines (SVMs). In the
machine learning community, some novel discriminative
ranking (also called ordinal regression) algorithms have
been proposed in recent years. Based on this work, in
this paper, we will present some novel discriminative
reranking techniques applied to machine translation. The
reranking problem for natural language is neither a clas-
sification problem nor a regression problem, and under
certain conditions MT reranking turns out to be quite dif-
ferent from parse reranking.
In this paper, we consider the special issues of apply-
ing reranking techniques to the MT task and introduce
two perceptron-like reranking algorithms for MT rerank-
ing. We provide experimental results that show that the
proposed algorithms achieve start-of-the-art results on the
NIST 2003 Chinese-English large data track evaluation.
1.1 Generative Models for MT
The seminal IBM models (Brown et al, 1990) were
the first to introduce generative models to the MT task.
The IBM models applied the sequence learning paradigm
well-known from Hidden Markov Models in speech
recognition to the problem of MT. The source and tar-
get sentences were treated as the observations, but the
alignments were treated as hidden information learned
from parallel texts using the EM algorithm. This source-
channel model treated the task of finding the probability
	

, where  is the translation in the target (English)
language for a given source (foreign) sentence  , as two
generative probability models: the language model 

which is a generative probability over candidate transla-
tions and the translation model 
 which is a gener-
ative conditional probability of the source sentence given
a candidate translation  .
The lexicon of the single-word based IBM models does
not take word context into account. This means unlikely
alignments are being considered while training the model
and this also results in additional decoding complexity.
Several MT models were proposed as extensions of the
IBM models which used this intuition to add additional
linguistic constraints to decrease the decoding perplexity
and increase the translation quality.
Wang and Waibel (1998) proposed an SMT model
based on phrase-based alignments. Since their transla-
tion model reordered phrases directly, it achieved higher
accuracy for translation between languages with differ-
ent word orders. In (Och and Weber, 1998; Och et al,
1999), a two-level alignment model was employed to uti-
lize shallow phrase structures: alignment between tem-
plates was used to handle phrase reordering, and word
alignments within a template were used to handle phrase
to phrase translation.
However, phrase level alignment cannot handle long
distance reordering effectively. Parse trees have also
been used in alignment models. Wu (1997) introduced
constraints on alignments using a probabilistic syn-
chronous context-free grammar restricted to Chomsky-
normal form. (Wu, 1997) was an implicit or self-
organizing syntax model as it did not use a Treebank. Ya-
mada and Knight (2001) used a statistical parser trained
using a Treebank in the source language to produce parse
trees and proposed a tree to string model for alignment.
Gildea (2003) proposed a tree to tree alignment model us-
ing output from a statistical parser in both source and tar-
get languages. The translation model involved tree align-
ments in which subtree cloning was used to handle cases
of reordering that were not possible in earlier tree-based
alignment models.
1.2 Discriminative Models for MT
Och and Ney (2002) proposed a framework for MT based
on direct translation, using the conditional model 

estimated using a maximum entropy model. A small
number of feature functions defined on the source and
target sentence were used to rerank the translations gen-
erated by a baseline MT system. While the total num-
ber of feature functions was small, each feature function
was a complex statistical model by itself, as for exam-
ple, the alignment template feature functions used in this
approach.
Och (2003) described the use of minimum error train-
ing directly optimizing the error rate on automatic MT
evaluation metrics such as BLEU. The experiments
showed that this approach obtains significantly better re-
sults than using the maximum mutual information cri-
terion on parameter estimation. This approach used the
same set of features as the alignment template approach
in (Och and Ney, 2002).
SMT Team (2003) also used minimum error training
as in Och (2003), but used a large number of feature func-
tions. More than 450 different feature functions were
used in order to improve the syntactic well-formedness
of MT output. By reranking a 1000-best list generated by
the baseline MT system from Och (2003), the BLEU (Pa-
pineni et al, 2001) score on the test dataset was improved
from 31.6% to 32.9%.
2 Ranking and Reranking
2.1 Reranking for NLP tasks
Like machine translation, parsing is another field of natu-
ral language processing in which generative models have
been widely used. In recent years, reranking techniques,
especially discriminative reranking, have resulted in sig-
nificant improvements in parsing. Various machine learn-
ing algorithms have been employed in parse reranking,
such as Boosting (Collins, 2000), Perceptron (Collins and
Duffy, 2002) and Support Vector Machines (Shen and
Joshi, 2003). The reranking techniques have resulted in a
13.5% error reduction in labeled recall/precision over the
previous best generative parsing models. Discriminative
reranking methods for parsing typically use the notion of
a margin as the distance between the best candidate parse
and the rest of the parses. The reranking problem is re-
duced to a classification problem by using pairwise sam-
ples.
In (Shen and Joshi, 2004), we have introduced a new
perceptron-like ordinal regression algorithm for parse
reranking. In that algorithm, pairwise samples are used
for training and margins are defined as the distance be-
tween parses of different ranks. In addition, the uneven
margin technique has been used for the purpose of adapt-
ing ordinal regression to reranking tasks. In this paper,
we apply this algorithm to MT reranking, and we also
introduce a new perceptron-like reranking algorithm for
MT.
2.2 Ranking and Ordinal Regression
In the field of machine learning, a class of tasks (called
ranking or ordinal regression) are similar to the rerank-
ing tasks in NLP. One of the motivations of this paper
is to apply ranking or ordinal regression algorithms to
MT reranking. In the previous works on ranking or or-
dinal regression, the margin is defined as the distance
between two consecutive ranks. Two large margin ap-
proaches have been used. One is the PRank algorithm,
a variant of the perceptron algorithm, that uses multi-
ple biases to represent the boundaries between every two
consecutive ranks (Crammer and Singer, 2001; Harring-
ton, 2003). However, as we will show in section 3.7, the
PRank algorithm does not work on the reranking tasks
due to the introduction of global ranks. The other ap-
proach is to reduce the ranking problem to a classification
problem by using the method of pairwise samples (Her-
brich et al, 2000). The underlying assumption is that the
samples of consecutive ranks are separable. This may
become a problem in the case that ranks are unreliable
when ranking does not strongly distinguish between can-
didates. This is just what happens in reranking for ma-
chine translation.
3 Discriminative Reranking for MT
The reranking approach for MT is defined as follows:
First, a baseline system generates   -best candidates. Fea-
tures that can potentially discriminate between good vs.
bad translations are extracted from these   -best candi-
dates. These features are then used to determine a new
ranking for the   -best list. The new top ranked candidate
in this   -best list is our new best candidate translation.
3.1 Advantages of Discriminative Reranking
Discriminative reranking allows us to use global features
which are unavailable for the baseline system. Second,
we can use features of various kinds and need not worry
about fine-grained smoothing issues. Finally, the statis-
tical machine learning approach has been shown to be
effective in many NLP tasks. Reranking enables rapid
experimentation with complex feature functions, because
the complex decoding steps in SMT are done once to gen-
erate the N-best list of translations.
3.2 Problems applying reranking to MT
First, we consider how to apply discriminative reranking
to machine translation. We may directly use those algo-
rithms that have been successfully used in parse rerank-
ing. However, we immediately find that those algorithms
are not as appropriate for machine translation. Let 
be the candidate ranked at the  th position for the source
sentence, where ranking is defined on the quality of the
candidates. In parse reranking, we look for parallel hy-
perplanes successfully separating  and  Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 307?308,
New York City, June 2006. c?2006 Association for Computational Linguistics
5. Inductive Semi-supervised Learning Methods for Natural Language Processing
Anoop Sarkar and Gholamreza Haffari, Simon Fraser University
Supervised machine learning methods which learn from labelled (or annotated) data are now widely
used in many different areas of Computational Linguistics and Natural Language Processing. There are
widespread data annotation endeavours but they face problems: there are a large number of languages and
annotation is expensive, while at the same time raw text data is plentiful. Semi-supervised learning methods
aim to close this gap.
The last 6-7 years have seen a surge of interest in semi-supervised methods in the machine learning
and NLP communities focused on the one hand on analysing the situations in which unlabelled data can be
useful, and on the other hand, providing feasible learning algorithms.
This recent research has resulted in a wide variety of interesting methods which are different with respect
to the assumptions they make about the learning task. In this tutorial, we survey recent semi-supervised
learning methods, discuss assumptions behind various approaches, and show how some of these methods
have been applied to NLP tasks.
5.1 Tutorial Outline
1. Introduction
? Spectrum of fully supervised to unsupervised learning, clustering vs. classifiers or model-
based learning
? Inductive vs. Transductive learning
? Generative vs. Discriminative learning
2. Mixtures of Generative Models
? Analysis
? Stable Mixing of Labelled and Unlabelled data
? Text Classification by EM
3. Multiple view Learning
? Co-training algorithm
? Yarowsky algorithm
? Co-EM algorithm
? Co-Boost algorithm
? Agreement Boost algorithm
? Multi-task Learning
4. Semi-supervised Learning for Structured Labels (Discriminative models)
? Simple case: Random Walk
? Potential extension to Structured SVM
5. NLP tasks and semi-supervised learning
? Using EM-based methods to combine labelled and unlabelled data
? When does it work? Some negative examples of semi-supervised learning in NLP
? Examples of various NLP tasks amenable to semi-supervised learning: chunking, parsing,
word-sense disambiguation, etc.
? Semi-supervised methods proposed within NLP and their relation to machine learning
methods covered in this tutorial
? Semi-supervised learning for structured models relevant for NLP such as sequence learning
and parsing
? Semi-supervised learning for domain adaptation in NLP
307
5.2 Target Audience
The target audience is expected to be researchers in computational linguistics and natural language process-
ing who wish to explore methods that will possibly allow learning from smaller size labelled datasets by
exploiting unlabelled data. In particular those who are interested in NLP research into new languages or
domains for which resources do not currently exist, or in novel NLP tasks that do not have existing large
amounts of annotated data. We assume some familiarity with commonly used supervised learning methods
in NLP.
Anoop Sarkar is an Assistant Professor in the School of Computing Science at Simon Fraser University. His
research has been focused on machine learning algorithms applied to the study of natural language. He is
especially interested in algorithms that combine labeled and unlabeled data and learn new information with
weak supervision. Anoop received his PhD from the Department of Computer and Information Science at
the University of Pennsylvania, with Prof. Aravind Joshi was his advisor. His PhD dissertation was entitled
Combining Labeled and Unlabeled Data in Statistical Natural Language Parsing. A full list of papers is
available at http://www.cs.sfu.ca/ anoop. His email address is anoop@cs.sfu.ca
Gholamreza Haffari is a second year PhD student in the School of Computing Science at Simon Fraser
University. He is working under the supervision of Prof. Sarkar towards a thesis on semi-supervised learning
for structured models in NLP. His home page is http://www.cs.sfu.ca/ ghaffar1, and his email address is
ghaffar1@cs.sfu.ca
308
Proceedings of NAACL HLT 2007, Companion Volume, pages 97?100,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Exploiting Rich Syntactic Information for Relation
Extraction from Biomedical Articles?
Yudong Liu and Zhongmin Shi and Anoop Sarkar
School of Computing Science
Simon Fraser University
{yudongl,zshi1,anoop}@cs.sfu.ca
Abstract
This paper proposes a ternary relation
extraction method primarily based on
rich syntactic information. We identify
PROTEIN-ORGANISM-LOCATION re-
lations in the text of biomedical articles.
Different kernel functions are used with
an SVM learner to integrate two sources
of information from syntactic parse trees:
(i) a large number of syntactic features
that have been shown useful for Seman-
tic Role Labeling (SRL) and applied here
to the relation extraction task, and (ii) fea-
tures from the entire parse tree using a
tree kernel. Our experiments show that the
use of rich syntactic features significantly
outperforms shallow word-based features.
The best accuracy is obtained by combin-
ing SRL features with tree kernels.
1 Introduction
Biomedical functional relations (relations for short)
state interactions among biomedical substances. For
instance, the PROTEIN-ORGANISM-LOCATION
(POL) relation that we study in this paper provides
information about where a PROTEIN is located in
an ORGANISM, giving a valuable clue to the bi-
ological function of the PROTEIN and helping to
identify suitable drug, vaccine and diagnostic tar-
gets. Fig. 1 illustrates possible locations of proteins
in Gram+ and Gram? bacteria. Previous work in
biomedical relation extraction task (Sekimizu et al,
1998; Blaschke et al, 1999; Feldman et al, 2002)
suggested the use of predicate-argument structure by
taking verbs as the center of the relation ? in con-
trast, in this paper we directly link protein named en-
tities (NEs) to their locations; in other related work,
(Claudio et al, 2006) proposed an approach that
?This research was partially supported by NSERC, Canada.
cytoplasm cytoplasm 
Gram+ Gram- 
cytoplasmic 
membrane 
cell wall 
periplasm 
outer 
membrane secreted inner membrane 
Figure 1: Illustration of bacterial locations
solely considers the shallow semantic features ex-
tracted from sentences.
For relation extraction in the newswire domain,
syntactic features have been used in a generative
model (Miller et al, 2000) and in a discriminative
log-linear model (Kambhatla, 2004). In comparison,
we use a much larger set of syntactic features ex-
tracted from parse trees, many of which have been
shown useful in SRL task. Kernel-based methods
have also been used for relation extraction (Zelenko
et al, 2003; Culotta and Sorensen, 2004; Bunescu
and Mooney, 2005) on various syntactic represen-
tations, such as dependency trees or constituency-
based parse trees. In contrast, we explore a much
wider variety of syntactic features in this work. To
benefit from both views, a composite kernel (Zhang
et al, 2006) integrates the flat features from enti-
ties and structured features from parse trees. In our
work, we also combine a linear kernel with a tree
kernel for improved performance.
2 SRL Features for Information Extraction
Fig. 2 shows one example illustrating the ternary re-
lation we are identifying. In this example, ?Exoen-
zyme S? is a PROTEIN name, ?extracellular? a LO-
CATION name and ?Pseudomonas aeruginosa? an
ORGANISM name. Our task is to identify if there
exists a ?PROTEIN-ORGANISM-LOCATION? re-
lation among these three NEs.
To simplify the problem, we first reduce the POL
97
SPROTEIN/NP
PROTEIN/NNP
Exoenzyme
PROTEIN/NNP-H
S
VP-H
VBZ-H
is
NP
NP-H
DT
an
LOCATION/JJ
extracellular
NN-H
product
PP
IN-H
of
ORGANISM/NP
ORGANISM/FW
Pseudomonas
ORGANISM/FW-H
aeruginosa
Figure 2: An example of POL ternary relation in a parse tree
ternary relation extraction problem into two binary
relation extraction problems. Specifically, we split
the POL ternary relation into binary relations as: (1)
PO: PROTEIN and ORGANISM, and (2) PL: PRO-
TEIN and LOCATION.
The ORGANISM-LOCATION relation is ignored
because it does not consider the PROTEIN and is
less meaningful than the PO and PL relations. Based
on this simplification, and following the idea of
SRL, we take the PROTEIN name in the role of the
predicate (verb) and the ORGANISM/LOCATION
name as its argument candidates in question. Then
the problem of identifying the binary relations of PO
and PL has been reduced to the problem of argu-
ment classification problem given the predicate and
the argument candidates. The reason we pick PRO-
TEIN names as predicates is that we assume PRO-
TEIN names play a more central role in linking the
binary relations to the final ternary relations.
Compared to a corpus for the standard SRL task,
there are some differences in this task: first is the
relative position of PROTEIN names and ORGAN-
ISM/LOCATION names. Unlike the case in SRL,
where arguments locate either before or after the
predicate, in this application it is possible that one
NE is embedded in another. A second difference is
that a predicate in SRL scenario typically consists of
only one word; here a PROTEIN name can contain
up to 8 words.
We do not use PropBank data in our model at all.
All of our training data and test data is annotated by
domain expert biologists and parsed by Charniak-
Johnson?s parser (released in 2006). When there is
a misalignment between the NE and the constituent
in the parse tree, we insert a new NP parent node for
the NE.
3 System Description
Figure 3: High-level system architecture
Fig. 3 shows the system overview. The input to
our system consists of titles and abstracts that are
extracted from MEDLINE records. These extracted
sentences have been annotated with the NE infor-
mation (PROTEIN, ORGANISM and LOCATION).
The Syntactic Annotator parses the sentences and in-
serts the head information to the parse trees by using
the Magerman/Collins head percolation rules. The
main component of the system is our SRL-based
relation extraction module, where we first manu-
ally extract features along the path from the PRO-
TEIN name to the ORGANISM/LOCATION name
and then train a binary SVM classifier for the binary
relation extraction. Finally, we fuse the extracted
binary relations into a ternary relation. In contrast
with our discriminative model, a statistical parsing
based generative model (Shi et al, 2007) has been
proposed for a related task on this data set where the
NEs and their relations are extracted together and
used to identify which NEs are relevant in a particu-
lar sentence. Since our final goal is to facilitate the
biologists to generate the annotated corpus, in future
98
? each word and its Part-of-Speech (POS) tag of PRO name
? head word (hw) and its POS of PRO name
? subcategorization that records the immediate structure that
expands from PRO name. Non-PRO daughters will be elim-
inated
? POS of parent node of PRO name
? hw and its POS of the parent node of PRO name
? each word and its POS of ORG name (in the case of ?PO ?
relation extraction).
? hw and its POS of ORG name
? POS of parent node of ORG name
? hw and its POS of the parent node of ORG name
? POS of the word immediately before/after ORG name
? punctuation immediately before/after ORG name
? feature combinations: hw of PRO name hw of ORG name,
hw of PRO name POS of hw of ORG name, POS of hw of
PRO name POS of hw of ORG name
? path from PRO name to ORG name and the length of the
path
? trigrams of the path. We consider up to 9 trigrams
? lowest common ancestor node of PRO name and ORG
name along the path
? LCA (Lowest Common Ancestor) path that is from ORG
name to its lowest common ancestor with PRO name
? relative position of PRO name and ORG name. In parse
trees, we consider 4 types of positions that ORGs are relative
to PROs: before, after, inside, other
Table 1: Features adopted from the SRL task. PRO:
PROTEIN; ORG: ORGANISM
work we plan to take the relevant labeled NEs from
the generative model as our input.
Table 1 and Table 2 list the features that are used
in the system.
4 Experiments and Evaluation
4.1 Data set
Our experimental data set is derived from a small
expert-curated corpus, where the POL relations and
relevant PROTEIN, ORGANISM and LOCATION
NEs are labeled. It contains ?150k words, 565 rela-
tion instances for POL, 371 for PO and 431 for PL.
4.2 Systems and Experimental Results
We built several models to compare the relative util-
ity of various types of rich syntactic features that
we can exploit for this task. For various represen-
tations, such as feature vectors, trees and their com-
binations, we applied different kernels in a Support
Vector Machine (SVM) learner. We use Joachims?
? subcategorization that records the immediate structure that
expands from ORG name. Non-ORG daughters will be elim-
inated
? if there is an VP node along the path as ancestor of ORG
name
? if there is an VP node as sibling of ORG name
? path from PRO name to LCA and the path length (L1)
? path from ORG name to LCA and the path length (L2)
? combination of L1 and L2
? sibling relation of PRO and ORG
? distance between PRO name and ORG name in the sen-
tence. ( 3 valued: 0 if nw (number of words) = 0; 1 if 0 <
nw <= 5; 2 if nw > 5)
? combination of distance and sibling relation
Table 2: New features used in the SRL-based rela-
tion extraction system.
SVM light1 with default linear kernel to feature vec-
tors and Moschetti?s SVM-light-TK-1.22 with the
default tree kernel. The models are:
Baseline1 is a purely word-based system, where
the features consist of the unigrams and bigrams
between the PROTEIN name and the ORGAN-
ISM/LOCATION names inclusively, where the stop-
words are selectively eliminated.
Baseline2 is a naive approach that assumes that any
example containing PROTEIN, LOCATION names
has the PL relation. The same assumption is made
for PO and POL relations.
PAK system uses predicate-argument structure ker-
nel (PAK) based method. PAKwas defined in (Mos-
chitti, 2004) and only considers the path from the
predicate to the target argument, which in our set-
ting is the path from the PROTEIN to the ORGAN-
ISM or LOCATION names.
SRL is an SRL system which is adapted to use our
new feature set. A default linear kernel is applied
with SVM learning.
TRK system is similar to PAK system except that
the input is an entire parse tree instead of a PAK
path.
TRK+SRL combines full parse trees and manually
extracted features and uses the kernel combination.
1http://svmlight.joachims.org/
2http://ai-nlp.info.uniroma2.it/moschitti/TK1.2-
software/Tree-Kernel.htm
99
Method PL PO POL
Measure Prec Rec F Acc Prec Rec F Acc Prec Rec F Acc
Baseline1 98.1 61.0 75.3 60.6 88.4 59.7 71.3 58.5 57.1 90.9 70.1 56.3
Baseline2 61.9 100.0 76.5 61.9 48.8 100.0 65.6 48.9 59.8 100.0 74.8 59.8
PAK 71.0 71.0 71.0 64.6 69.0 66.7 67.8 61.8 66.0 69.9 67.9 62.6
SRL 72.9 77.1 74.9 70.3 66.0 71.0 68.4 64.5 70.6 67.5 69.0 65.8
TRK 69.8 81.6 75.3 72.0 64.2 84.1 72.8 72.0 79.6 66.2 72.3 71.3
TRK+SRL 74.9 79.4 77.1 72.8 73.9 78.1 75.9 72.6 75.3 74.5 74.9 71.8
Table 3: Percent scores of Precision/Recall/F-score/Accuracy for identifying PL, PO and POL relations.
4.3 Fusion of Binary relations
We predict the POL ternary relation by fusing PL
and PO binary relations if they belong to the same
sentence and have the same PROTEIN NE. The pre-
diction is made by the sum of confidence scores
(produced by the SVM) of the PL and PO relations.
This is similar to the postprocessing step in SRL task
in which the semantic roles assigned to the argu-
ments have to realize a legal final semantic frame
for the given predicate.
4.4 Discussion
Table 3 shows the results using 5-fold cross valida-
tion. We report figures on ternary relation extraction
and extraction of the two binary relations. Compari-
son between the PAK model and SRL model shows
that manually specified features are more discrimi-
native for binary relation extraction; they boost pre-
cision and accuracy for ternary relation extraction.
In contrast to the SRL model for binary relation ex-
traction, the TRK model obtains lower recall but
higher precision. The combination of SRL with the
TRK system gives best overall accuracy of 71.8%
outperforming shallow word based features.
5 Conclusion
In this paper we explored the use of rich syntac-
tic features for the relation extraction task. In con-
trast with the previously used set of syntactic fea-
tures for this task, we use a large number of fea-
tures originally proposed for the Semantic Role La-
beling task. We provide comprehensive experiments
using many different models that use features from
parse trees. Using rich syntactic features by com-
bining SRL features with tree kernels over the en-
tire tree obtains 71.8% accuracy which significantly
outperforms shallow word-based features which ob-
tains 56.3% accuracy.
References
C. Blaschke, M. Andrade, C. Ouzounis, and A. Valencia. 1999.
Automatic extraction of biological information from scien-
tific text: Protein-protein interactions. In AAAI-ISMB 1999.
R. C. Bunescu and R. J. Mooney. 2005. A shortest path depen-
dency kernel for relation extraction. In Proc. HLT/EMNLP-
2005.
G. Claudio, A. Lavelli, and L. Romano. 2006. Exploiting
Shallow Linguistic Information for Relation Extraction from
Biomedical Literature. In Proc. EACL 2006.
A. Culotta and J. Sorensen. 2004. Dependency tree kernels for
relation extraction. In Proc. ACL-2004.
R. Feldman, Y. Regev, M. Finkelstein-Landau, E. Hurvitz, and
B. Kogan. 2002. Mining biomedical literature using infor-
mation extraction. Current Drug Discovery.
N. Kambhatla. 2004. Combining lexical, syntactic, and seman-
tic features with maximum entropy models for information
extraction. In Proc. ACL-2004 (poster session).
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 2000. A
novel use of statistical parsing to extract information from
text. Proc. NAACL-2000.
A. Moschitti. 2004. A study on convolution kernels for shallow
semantic parsing. In Proc. ACL-2004.
T. Sekimizu, H.S. Park, and J. Tsujii. 1998. Identifying the
interaction between genes and gene products based on fre-
quently seen verbs in medline abstracts. In Genome Infor-
matics. 62-71.
Z. Shi, A. Sarkar, and F. Popowich. 2007. Simultaneous Iden-
tification of Biomedical Named-Entity and Functional Re-
lation UsingStatistical Parsing Techniques. In NAACL-HLT
2007 (short paper).
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel meth-
ods for relation extraction. Journal of Machine Learning
Research.
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A Composite
Kernel to Extract Relations between Entities with Both Flat
and Structured Features. In Proc. ACL-2006.
100
Proceedings of NAACL HLT 2007, Companion Volume, pages 161?164,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Simultaneous Identification of Biomedical Named-Entity and
Functional Relations Using Statistical Parsing Techniques ?
Zhongmin Shi and Anoop Sarkar and Fred Popowich
School of Computing Science
Simon Fraser University
{zshi1,anoop,popowich}@cs.sfu.ca
Abstract
In this paper we propose a statistical pars-
ing technique that simultaneously iden-
tifies biomedical named-entities (NEs)
and extracts subcellular localization re-
lations for bacterial proteins from the
text in MEDLINE articles. We build
a parser that derives both syntactic and
domain-dependent semantic information
and achieves an F-score of 48.4% for the
relation extraction task. We then propose
a semi-supervised approach that incor-
porates noisy automatically labeled data
to improve the F-score of our parser to
83.2%. Our key contributions are: learn-
ing from noisy data, and building an an-
notated corpus that can benefit relation ex-
traction research.
1 Introduction
Relation extraction from text is a step beyond
Named-Entity Recognition (NER) and generally de-
mands adequate domain knowledge to build rela-
tions among domain-specific concepts. A Biomedi-
cal Functional Relation (relation for short) states in-
teractions among biomedical substances. In this pa-
per we focus on one such relation: Bacterial Protein
Localization (BPL), and introduce our approach for
identifying BPLs from MEDLINE1 articles.
BPL is a key functional characteristic of pro-
teins. It is essential to the understanding of the func-
tion of different proteins and the discovery of suit-
able drugs, vaccines and diagnostic targets. We are
collaborating with researchers in molecular biology
with the goal of automatically extracting BPLs from
?This research was partially supported by NSERC, Canada.
1MEDLINE is a bibliographic database of biomedical
scientific articles at National Library of Medcine (NLM,
http://www.nlm.nih.gov/).
text with BioNLP techniques, to expand their pro-
tein localization database, namely PSORTdb2(Rey
et al, 2005). Specifically, the task is to produce as
output the relation tuple BPL(BACTERIUM, PRO-
TEIN, LOCATION) along with source sentence and
document references. The task is new to BioNLP
in terms of the specific biomedical relation being
sought. Therefore, we have to build annotated cor-
pus from scratch and we are unable to use existing
BioNLP shared task resources in our experiments.
In this paper we extract from the text of biomedical
articles a relation among: a LOCATION (one of the
possible locations shown in Figure 1 for Gram+ and
Gram- bacteria); a particular BACTERIUM, e.g. E.
Coli, and a PROTEIN name, e.g. OprF.
(Nair and Rost, 2002) used the text taken from
Swiss-Prot annotations of proteins, and trained a
subcellular classifier on this data. (Hoglund et al,
2006) predicted subcellular localizations using an
SVM trained on both text and protein sequence data,
by assigning each protein name a vector based on
terms co-occurring with the localization name for
each organism. (Lu and Hunter, 2005) applied a hi-
erarchical architecture of SVMs to predict subcel-
lular localization by incorporating a semantic hier-
archy of localization classes modeled with biolog-
ical processing pathways. These approaches either
ignore the actual location information in their pre-
dicted localization relations, or only focus on a small
portion of eukaryotic proteins. The performance of
these approaches are not comparable due to different
tasks and datasets.
2 System Outline
During our system?s preprocessing phase, sentences
are automatically annotated with both syntactic in-
formation and domain-specific semantic informa-
tion. Syntactic annotations are provided by a statis-
tical parser (Charniak and Johnson, 2005). Domain-
2http://db.psort.org.
161
cytoplasm cytoplasm 
Gram+ Gram- 
cytoplasmic 
membrane 
cell wall 
periplasm 
outer 
membrane secreted 
inner 
membrane 
Figure 1: Illustration of possible locations of pro-
teins with respect to the bacterial cell structure.
specific semantic information includes annotations
on PROTEIN, BACTERIUM and LOCATION NEs
by dictionary lookups from UMLS3, NCBI Taxon-
omy4 and SwissProt5, and two automatic Bio-NE
recognizers: MMTx6 and Lingpipe7.
We propose the use of a parser that simultane-
ously identifies NEs and extracts the BPL relations
from each sentence. We define NEs to be Relevant
to each other only if they are arguments of a BPL re-
lation, otherwise they are defined to be Irrelevant.
A sentence may contain multiple PROTEIN (LO-
CATION or ORGANISM) NEs, e.g., there are two
PROTEIN NEs in the sentence below but only one,
OmpA, is relevant. Our system aims to identify the
correct BPL relation among all possible BPL tuples
(candidate relations) in the sentence by only recog-
nizing relevant NEs. Each input sentence is assumed
to have at least one BPL relation.
Nine of 10 monoclonal antibodies mapped within the carboxy-
terminal region of [PROTEIN OprF] that is homologous to
the [ORGANISM Escherichia coli] [LOCATION outer membrane]
protein [PROTEIN OmpA].
3 Statistical Syntactic and Semantic Parser
Similar to the approach in (Miller et al, 2000) and
(Kulick et al, 2004), our parser integrates both syn-
tactic and semantic annotations into a single annota-
tion as shown in Figure 2. A lexicalized statistical
parser (Bikel, 2004) is applied to the parsing task.
The parse tree is decorated with two types of seman-
3http://www.nlm.nih.gov/research/umls/
4http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=Taxonomy
5http://www.ebi.ac.uk/swissprot/
6MetaMap Transfer, http://mmtx.nlm.nih.gov/
7http://www.alias-i.com/
PO_LNK/NP 
PO_PTR/PP PO_PTR/PP 
DT PO_PTR/NP PRN NN 
The PROTEIN_R/NP 
PROTEIN_R/JJ 
phospholipase 
PROTEIN_R/NNP 
C 
-LRB- 
-LRB- 
NP 
NNP 
PLC 
gene 
IN 
of 
PO_PTR/NP 
ORGANISM_R/NP 
ORGANISM_R/NNP ORGANISM_R/NNP 
Pseudomonas aeruginosa 
-RRB- 
-RRB- 
Figure 2: An example of parsing results
tic annotations:
1) Annotations on relevant PROTEIN, BAC-
TERIUM and LOCATION NEs. Tags are PRO-
TEIN R, BACTERIUM R and LOCATION R respec-
tively.
2) Annotations on paths between relevant NEs. The
lower-most node that spans both NEs is tagged as
LNK and all nodes along the path to the NEs are
tagged as PTR.
Binary relations are apparently much easier to
represent on the parse tree, therefore we split the
BPL ternary relation into two binary relations: BP
(BACTERIUM and PROTEIN) and PL (PROTEIN
and LOCATION). After capturing BP and PL rela-
tions, we will predict BPL as a fusion of BP and PL,
see ?4.1. In contrast to the global inference done us-
ing our generative model, heavily pipelined discrim-
inative approaches usually have problems with error
propagation. A more serious problem in a pipelined
system when using syntactic parses for relation ex-
traction is the alignment between the named enti-
ties produced by a separate system and the syntac-
tic parses produced by the statistical parser. This
alignment issue is non-trivial and we could not pro-
duce a pipelined system that dealt with this issue
satisfactorily for our dataset. As a result, we did
not directly compare our generative approach to a
pipelined strategy.
4 Experiment Settings and Evaluations
The training and test sets are derived from a small
expert-curated corpus. Table 1 lists numbers of sen-
tences and relevant NEs in each BP/PL/BPL set.
Since the parsing results include both NE and path
tags (note that we do not use any external NER sys-
tem), there are two metrics to produce and evalu-
ate PL or BP relations: Name-only and Name-path
metrics. The name-only metric only measures Rel-
162
PL BP BPL
Training set 289 / 605 258 / 595 352 / 852
Test set 44 / 134 28 / 127 62 / 182
Table 1: Sizes of training and test sets (number of
sentences / number of relevant NEs)
evant PROTEIN, BACTERIUM and LOCATION
NEs (see Section 2). It does not take path annota-
tions into account. The name-only metric is mea-
sured in terms of Precision, Recall and F-score, in
which True Positive (TP ) is the number of correctly
identified NEs, False Positive (FP ) is the number of
incorrectly identified NEs and False Negative (FN )
is the number of correct NEs that are not identified.
The name-path measures nodes being annotated
as LNK, PTR or R along the path between NEs
on the parse tree, therefore it represents confidence
of NEs being arguments of the relation. The name-
path metric is a macro-average measure, which is
the average performance of all sentences in data set.
In measurement of the name-path metric, TP is the
number of correctly annotated nodes on the path be-
tween relevant NEs. FP is the number of incor-
rectly annotated nodes on the path and FN is the
number of correct nodes that are not identified.
4.1 Fusion of BP and PL
The BPL relation can be predicted by a fusion of
BP and PL once they are extracted. Specifically, a
BP and a PL that are extracted from the same sen-
tence are merged into a BPL. The predicted BPL
relations are then evaluated by the same name-only
and name-path metrics as for binary relations. In the
name-path metric, nodes on both PL and BP paths
are counted. Note that we do not need a common
protein NER to merge the BP and PL relations. E.g.,
for name-only evaluation, assume true BPL(B1, P1,
L1): if we predict BP(B1, ) and PL(P1, L2), then
TP=2 due to B1, P1; FP=1 due to L2; and FN=1
due to P1.
5 NER and BPL Extraction
Baseline: An intuitive method for relation extrac-
tion would assume that any sentence containing
PROTEIN, ORGANISM and LOCATION NEs has
the relation. We employ this method as a baseline
system, in which NEs are identified by the auto-
matic NE recognizers and dictionary lookups as in-
troduced in ?2. The system is evaluated against the
test set in Table 1. Results in Table 2 show low pre-
cision for PROTEIN NER and the name-path metric.
Extraction using Supervised Parsing: We first ex-
periment a fully supervised approach by training the
parser on the BP/PL training set and evaluate on the
test set (see Table 1). The name-only and name-path
evaluation results in Table 2 show poor syntactic
parsing annotation quality and low recall on PRO-
TEIN NER. The major reason of these problems is
the lack of training data.
Extraction using Semi-supervised Parsing: Ex-
periments with purely supervised learning show that
our generative model requires a large curated set
to minimize the sparse data problem, but domain-
specific annotated corpora are always rare and ex-
pensive. However, there is a huge source of unla-
beled MEDLINE articles available that may meet
our needs, by assuming that any sentence contain-
ing BACTERIUM, PROTEIN and LOCATION NEs
has the BPL relation. We then choose such sentences
from a subset of the MEDLINE database as the
training data. These sentences, after being parsed
and BPL relations inserted, are in fact the very noisy
data when used to train the parser, since the assumed
relations do not necessarily exist. The reason this
noisy data works at all is probably because we can
learn a preference for structural relations between
entities that are close to each other in the sentence,
and thus distinguish between competing relations in
the same sentence. In future work, we hope to ex-
plore explicit bootstrapping from the labeled data to
improve the quality of the noisy data.
Two experiments were carried out corresponding
to choices of the training set: 1) noisy data only, 2)
noisy data and curated training data. Evaluation re-
sults given in Table 2.
Evaluation results on the name-only metric show
that, compared to supervised parsing, our semi-
supervised method dramatically improves recall for
NER. For instance, recall for PROTEIN NER in-
creases from 25.0% to 81.3%; recall on BAC-
TERIUM and LOCATION NERs increases about
30%. As for the name-path metric, the over-
all F-score is much higher than our fully super-
vised method increasing from 39.9% to 74.5%. It
shows that the inclusion of curated data in the semi-
163
Name-only Evaluation (%) Name-Path Evaluation (%)
Method Measure PL BP BPL PL BP BPL
PROT LOC PROT BACT
P 42.3 78.6 41.9 81.3 40.7 27.1 38.9 31.0
Baseline R 92.5 97.3 87.8 97.4 90.9 56.5 69.0 60.7
F 58.0 87.0 56.7 88.6 56.2 36.6 49.8 41.0
Supervised P 66.7 87.5 66.7 72.7 76.9 45.9 41.2 43.9
(training data R 25.0 56.0 10.5 47.1 35.3 36.7 36.3 36.5
only) F 36.4 68.3 18.2 57.1 48.4 40.8 38.6 39.9
Semi-supervised P 66.7 95.5 70.6 94.1 80.8 76.2 83.5 79.3
(noisy data R 84.2 80.8 80.0 84.2 81.8 67.8 72.4 67.0
only) F 74.4 87.5 75.0 88.9 81.3 71.7 77.5 74.2
Semi-supervised P 73.9 95.5 76.5 94.1 84.8 77.0 81.1 78.7
(noisy data + R 81.0 80.8 81.3 84.2 81.7 68.5 73.7 70.7
training data) F 77.3 87.5 78.8 88.9 83.2 72.5 77.2 74.5
Table 2: Name-only and name-path evaluation results. PROTEIN, LOCATION and BACTERIUM are
PROT, LOC and BACT for short. The training data is the subset of curated data in Table 1.
supervised method does not improve performance
much. Precision of PROTEIN NER increases 6.5%
on average, while F-score of overall BPL extraction
increases only slightly. We experimented with train-
ing the semi-supervised method using noisy data
alone, and testing on the entire curated set, i.e., 333
and 286 sentences for BP and PL extractions respec-
tively. Note that we do not directly train from the
training set in this method, so it is still ?unseen? data
for this model. The F-scores of path-only and path-
name metrics are 75.5% and 67.1% respectively.
6 Discussion and Future Work
In this paper we introduced a statistical parsing-
based method to extract biomedical relations from
MEDLINE articles. We made use of a large un-
labeled data set to train our relation extraction
model. Experiments show that the semi-supervised
method significantly outperforms the fully super-
vised method with F-score increasing from 48.4%
to 83.2%. We have implemented a discriminative
model (Liu et al, 2007) which takes as input the ex-
amples with gold named entities and identifies BPL
relations on them. In future work, we plan to let the
discriminative model take the output of our parser
and refine our current results further. We also plan
to train a graphical model based on all extracted BP,
PL and BPL relations to infer relations from multi-
ple sentences and documents.
References
D. Bikel. 2004. A distributional analysis of a lexicalized statis-
tical parsing model. In Proc. of EMNLP ?04, pages 182?189.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and maxent discriminative reranking. In Proc. of ACL
?05, pages 173?180.
A. Hoglund, T. Blum, S. Brady, P. Donnes, J. Miguel,
M. Rocheford, O. Kohlbacher, and H. Shatkay. 2006. Sig-
nificantly improved prediction of subcellular localization by
integrating text and protein sequence data. In Proc. of PSB
?06, volume 11, pages 16?27.
S. Kulick, A. Bies, M. Libeman, M. Mandel, R. McDonald,
M. Palmer, A. Schein, and L. Ungar. 2004. Integrated an-
notation for biomedical information extraction. In Proc. of
HLT/NAACL ?04, pages 61?68, Boston, May.
Y. Liu, Z. Shi, and A. Sarkar. 2007. Exploiting rich syntactic
information for relation extraction from biomedical articles.
In NAACL-HLT ?07, poster track, Rochester, NY, April.
Z. Lu and L. Hunter. 2005. Go molecular function terms are
predictive of subcellular localization. In Proc. of PSB ?05,
volume 10, pages 151?161.
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 2000. A
novel use of statistical parsing to extract information from
text. In Proc. of NAACL ?06, pages 226?233.
R. Nair and B. Rost. 2002. Inferring subcellular localization
through automated lexical analysis. In Bioinformatics, vol-
ume 18, pages 78?86.
S. Rey, M. Acab, J. Gardy, M. Laird, K. deFays, C. Lam-
bert, and F. Brinkman. 2005. Psortdb: A database of sub-
cellular localizations for bacteria. Nucleic Acids Research,
33(D):164?168.
164
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 415?423,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Active Learning for Statistical Phrase-based Machine Translation?
Gholamreza Haffari and Maxim Roy and Anoop Sarkar
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
{ghaffar1,maximr,anoop}@cs.sfu.ca
Abstract
Statistical machine translation (SMT) mod-
els need large bilingual corpora for train-
ing, which are unavailable for some language
pairs. This paper provides the first serious ex-
perimental study of active learning for SMT.
We use active learning to improve the qual-
ity of a phrase-based SMT system, and show
significant improvements in translation com-
pared to a random sentence selection baseline,
when test and training data are taken from the
same or different domains. Experimental re-
sults are shown in a simulated setting using
three language pairs, and in a realistic situa-
tion for Bangla-English, a language pair with
limited translation resources.
1 Introduction
Statistical machine translation (SMT) systems have
made great strides in translation quality. However,
high quality translation output is dependent on the
availability of massive amounts of parallel text in
the source and target language. However, there are a
large number of languages that are considered ?low-
density?, either because the population speaking the
language is not very large, or even if millions of peo-
ple speak the language, insufficient amounts of par-
allel text are available in that language.
A statistical translation system can be improved
or adapted by incorporating new training data in the
form of parallel text. In this paper, we propose sev-
eral novel active learning (AL) strategies for statis-
tical machine translation in order to attack this prob-
lem. Conventional techniques for AL of classifiers
are problematic in the SMT setting. Selective sam-
pling of sentences for AL may lead to a parallel cor-
pus where each sentence does not share any phrase
?We would like to thank Chris Callison-Burch for fruitful
discussions. This research was partially supported by NSERC,
Canada (RGPIN: 264905) and by an IBM Faculty Award to the
third author.
pairs with the others. Thus, new sentences cannot
be translated since we lack evidence for how phrase
pairs combine to form novel translations. In this pa-
per, we take the approach of exploration vs. exploita-
tion: where in some cases we pick sentences that
are not entirely novel to improve translation statis-
tics, while also injecting novel translation pairs to
improve coverage.
There may be evidence to show that AL is use-
ful even when we have massive amounts of parallel
training data. (Turchi et al, 2008) presents a com-
prehensive learning curve analysis of a phrase-based
SMT system, and one of the conclusions they draw
is, ?The first obvious approach is an effort to iden-
tify or produce data sets on demand (active learning,
where the learning system can request translations of
specific sentences, to satisfy its information needs).?
Despite the promise of active learning for SMT
there has been very little experimental work pub-
lished on this issue (see Sec. 5). In this paper, we
make several novel contributions to the area of ac-
tive learning for SMT:
? We use a novel framework for AL, which to our
knowledge has not been used in AL experiments be-
fore. We assume a small amount of parallel text and
a large amount of monolingual source language text.
Using these resources, we create a large noisy par-
allel text which we then iteratively improve using
small injections of human translations.
? We provide many useful and novel features use-
ful for AL in SMT. In translation, we can leverage a
whole new set of features that were out of reach for
classification systems: we devise features that look
at the source language, but also devise features that
make an estimate of the potential utility of transla-
tions from the source, e.g. phrase pairs that could be
extracted.
? We show that AL can be useful in domain adapta-
tion. We provide the first experimental evidence in
SMT that active learning can be used to inject care-
415
fully selected translations in order to improve SMT
output in a new domain.
? We compare our proposed features to a random se-
lection baseline in a simulated setting for three lan-
guage pairs. We also use a realistic setting: using hu-
man expert annotations in our AL system we create
an improved SMT system to translate from Bangla
to English, a language pair with very few resources.
2 An Active Learning Framework for SMT
Starting from an SMT model trained initially on
bilingual data, the problem is to minimize the hu-
man effort in translating new sentences which will
be added to the training data to make the retrained
SMT model achieves a certain level of performance.
Thus, given a bitext L := {(fi, ei)} and a mono-
lingual source text U := {fj}, the goal is to select
a subset of highly informative sentences from U to
present to a human expert for translation. Highly in-
formative sentences are those which, together with
their translations, help the retrained SMT system
quickly reach a certain level of translation quality.
This learning scenario is known as active learning
with Selective Sampling (Cohn et al, 1994).
Algorithm 1 describes the experimental setup we
propose for active learning. We train our initial MT
system on the bilingual corpus L, and use it to trans-
late all monolingual sentences in U . We denote sen-
tences in U together with their translations as U+
(line 4 of Algorithm 1). Then we retrain the SMT
system on L?U+ and use the resulting model to de-
code the test set. Afterwards, we select and remove
a subset of highly informative sentences from U ,
and add those sentences together with their human-
provided translations to L. This process is continued
iteratively until a certain level of translation quality,
which in our case is measured by the BLEU score, is
met. In the baseline, against which we compare our
sentence selection methods, the sentences are cho-
sen randomly.
When (re-)training the model, two phrase tables
are learned: one from L and the other one from
U+. The phrase table obtained from U+ is added
as a new feature function in the log-linear trans-
lation model. The alternative is to ignore U+ as
in a conventional AL setting, however, in our ex-
periments we have found that using more bilingual
data, even noisy data, results in better translations.
Algorithm 1 AL-SMT
1: Given bilingual corpus L, and monolingual cor-
pus U .
2: MF?E = train(L, ?)
3: for t = 1, 2, ... do
4: U+ = translate(U,MF?E)
5: Select k sentence pairs from U+, and ask a
human for their true translations.
6: Remove the k sentences from U , and add the
k sentence pairs (translated by human) to L
7: MF?E = train(L,U+)
8: Monitor the performance on the test set T
9: end for
Phrase tables from U+ will get a 0 score in mini-
mum error rate training if they are not useful, so our
method is more general. Also, this method has been
shown empirically to be more effective (Ueffing et
al., 2007b) than (1) using the weighted combination
of the two phrase tables from L and U+, or (2) com-
bining the two sets of data and training from the bi-
text L ? U+.
The setup in Algorithm 1 helps us to investigate
how to maximally take advantage of human effort
(for sentence translation) when learning an SMT
model from the available data, that includes bilin-
gual and monolingual text.
3 Sentence Selection Strategies
Our sentence selection strategies can be divided into
two categories: (1) those which are independent of
the target language and just look into the source lan-
guage, and (2) those which also take into account the
target language. From the description of the meth-
ods, it will be clear to which category they belong to.
We will see in Sec. 4 that the most promising sen-
tence selection strategies belong to the second cate-
gory.
3.1 The Utility of Translation Units
Phrases are basic units of translation in phrase-based
SMT models. The phrases potentially extracted
from a sentence indicate its informativeness. The
more new phrases a sentence can offer, the more
informative it is. Additionally phrase translation
probabilities need to be estimated accurately, which
means sentences that contain rare phrases are also
informative. When selecting new sentences for hu-
416
man translation, we need to pay attention to this
tradeoff between exploration and exploitation, i.e.
selecting sentences to discover new phrases vs es-
timating accurately the phrase translation probabil-
ities. A similar argument can be made that empha-
sizes the importance of words rather than phrases for
any SMT model. Also we should take into account
that smoothing is a means for accurate estimation of
translation probabilities when events are rare. In our
work, we focus on methods that effectively expand
the lexicon or set of phrases of the model.
3.1.1 Phrases (Geom-Phrase, Arith-Phrase)1
The more frequent a phrase is in the unlabeled
data, the more important it is to know its translation;
since it is more likely to occur in the test data (es-
pecially when the test data is in-domain with respect
to unlabeled data). The more frequent a phrase is in
the labeled data, the more unimportant it is; since
probably we have observed most of its translations.
Based on the above observations, we measure the
importance score of a sentence as:
?pg(s) :=
[ ?
x?Xps
P (x|U)
P (x|L)
] 1
|Xps | (1)
where Xps is the set of possible phrases that sentence
s can offer, and P (x|D) is the probability of observ-
ing x in the data D: P (x|D) = Count(x)+?P
x?XpD
Count(x)+? .
The score (1) is the averaged probability ratio of
the set of candidate phrases, i.e. the probability of
the candidate phrases under a probabilistic phrase
model based on U divided by that based on L. In ad-
dition to the geometric average in (1), we may also
consider the arithmetic average score:
?pa(s) := 1|Xps |
?
x?Xps
P (x|U)
P (x|L) (2)
Note that (1) can be re-written as
1
|Xps |
?
x?Xps log P (x|U)P (x|L) in the logarithm space,
which is similar to (2) with the difference of
additional log.
In parallel data L, phrases are the ones which are
extracted by the usual phrase extraction algorithm;
but what are the candidate phrases in the unlabeled
1The names in the parentheses are short names used to iden-
tify the method in the experimental results.
data? Considering the k-best list of translations can
tell us the possible phrases the input sentence may
offer. For each translation, we have access to the
phrases used by the decoder to produce that output.
However, there may be islands of out-of-vocabulary
(OOV) words that were not in the phrase table and
not translated by the decoder as a phrase. We group
together such groups of OOV words to form an OOV
phrase. The set of possible phrases we extract from
the decoder output contain those coming from the
phrase table (from labeled data L) and those coming
from OOVs. OOV phrases are also used in our com-
putation, where P (x | L) for an OOV phrase x is
the uniform probability over all OOV phrases.
3.1.2 n-grams (Geom n-gram, Arith n-gram)
As an alternative to phrases, we consider n-grams
as basic units of generalization. The resulting score
is the weighted combination of the n-gram based
scores:
?Ng (s) :=
N?
n=1
wn
|Xns |
?
x?Xns
log P (x|U, n)P (x|L, n) (3)
where Xns denotes n-grams in the sentence s, and
P (x|D, n) is the probability of x in the set of n-
grams in D. The weights wn adjust the importance
of the scores of n-grams with different lengths. In
addition to taking geometric average, we also con-
sider the arithmetic average:
?Na (s) :=
N?
n=1
wn
|Xns |
?
x?Xns
P (x|U, n)
P (x|L, n) (4)
As a special case when N = 1, the score motivates
selecting sentences which increase the number of
unique words with new words appearing with higher
frequency in U than L.
3.2 Similarity to the Bilingual Training Data
(Similarity)
The simplest way to expand the lexicon set is to
choose sentences from U which are as dissimilar
as possible to L. We measure the similarity using
weighted n-gram coverage (Ueffing et al, 2007b).
3.3 Confidence of Translations (Confidence)
The decoder produces an output translation e using
the probability p(e | f). This probability can be
417
treated as a confidence score for the translation. To
make the confidence score for sentences with dif-
ferent lengths comparable, we normalize using the
sentence length (Ueffing et al, 2007b).
3.4 Feature Combination (Combined)
The idea is to take into account the information from
several simpler methods, e.g. those mentioned in
Sec. 3.1?3.3, when producing the final ranking of
sentences. We can either merge the output rankings
of those simpler models2, or use the scores gener-
ated by them as input features for a higher level
ranking model. We use a linear model:
F (s) = ?
k
?k?k(s) (5)
where ?k are the model parameters, and ?k(.) are
the feature functions from Sections 3.1?3.3, e.g.
confidence score, similarity to L, and score for the
utility of translation units. Using 20K of Spanish
unlabeled text we compared the r2 correlation co-
efficient between each of these scores which, apart
from the arithmetic and geometric versions of the
same score, showed low correlation. And so the in-
formation they provide should be complementary to
each other.
We train the parameters in (5) using two bilingual
development sets dev1 and dev2, the sentences in
dev1 can be ranked with respect to the amount by
which each particular sentence improves the BLEU
score of the retrained3 SMT model on dev2. Having
this ranking, we look for the weight vector which
produces the same ordering of sentences. As an al-
ternative to this method (or its computationally de-
manding generalization in which instead of a single
sentence, several sets of sentences of size k are se-
lected and ranked) we use a hill climbing search on
the surface of dev2?s BLEU score. For a fixed value
of the weight vector, dev1 sentences are ranked and
then the top-k output is selected and the amount
of improvement the retrained SMT system gives on
dev2?s BLEU score is measured. Starting from a
random initial value for ?k?s, we improve one di-
mension at a time and traverse the discrete grid
2To see how different rankings can be combined, see (Re-
ichart et al, 2008) which proposes this for multi-task AL.
3Here the retrained SMT model is the one learned by adding
a particular sentence from dev1 into L.
placed on the values of the weight vector. Starting
with a coarse grid, we make it finer when we get
stuck in local optima during hill climbing.
3.5 Hierarchical Adaptive Sampling (HAS)
(Dasgupta and Hsu, 2008) propose a technique for
sample selection that, under certain settings, is guar-
anteed to be no worse than random sampling. Their
method exploits the cluster structure (if there is any)
in the unlabeled data. Ideally, querying the label
of only one of the data points in a cluster would
be enough to determine the label of the other data
points in that cluster. Their method requires that the
data set is provided in the form of a tree represent-
ing a hierarchical clustering of the data. In AL for
SMT, such a unique clustering of the unlabeled data
would be inappropriate or ad-hoc. For this reason,
we present a new algorithm inspired by the ratio-
nale provided in (Dasgupta and Hsu, 2008) that can
be used in our setting, where we construct a tree-
based partition of the data dynamically4 . This dy-
namic tree construction allows us to extend the HAS
algorithm from classifiers to the SMT task.
The algorithm adaptively samples sentences from
U while building a hierarchical clustering of the sen-
tences in U (see Fig. 1 and Algorithm 2). At any it-
eration, first we retrain the SMT model and translate
all monolingual sentences. At this point one mono-
lingual set of sentences represented by one of the
tree leaves is chosen for further partitioning: a leaf
H is chosen which has the lowest average decoder
confidence score for its sentence translations. We
then rank all sentences in H based on their similar-
ity to L and put the top ?|H| sentences in H1 and
the rest in H2. To select K sentences, we randomly
sample ?K sentences from H1 and (1 ? ?)K sen-
tences from H2 and ask a human for their transla-
tions.
3.6 Reverse Model (Reverse)
While a translation system MF?E is built from lan-
guage F to language E, we also build a translation
system in the reverse direction ME?F . To mea-
sure how informative a monolingual sentence f is,
we translate it to English by MF?E and then project
4The dynamic nature of the hierarchy comes from two fac-
tors: (1) selecting a leaf node for splitting, and (2) splitting a
leaf node based on its similarity to the growing L.
418
Algorithm 2 Hierarchical-Adaptive-Sampling
1: MF?E = train(L, ?)
2: Initialize the tree T by setting its root to U
3: v := root(T )
4: for t = 1, 2, ... do
5: // rank and split sentence in v
X1, X2 := Partition(L, v, ?)
6: // randomly sample and remove sents from Xi
Y1, Y2 := Sampling(X1, X2, ?)
7: // make Xi children of node v in the tree T
T := UpdateTree(X1, X2, v, T )
8: // Y +i has sents in Yi together with human trans
L := L ? Y +1 ? Y +2
9: MF?E = train(L,U)
10: for all leaves l ? T do
11: Z[l] := Average normalized confidence scores
of sentence translations in l
12: end for
13: v := BestLeaf(T, Z)
14: Monitor the performance on the test set
15: end for
H1H2
H22 H21
H := U
Figure 1: Adaptively sampling the sentences while con-
structing a hierarchical clustering of U .
the translation back to French using ME?F . Denote
this reconstructed version of the original French
sentence by f? . Comparing f with f? using BLEU (or
other measures) can tell us how much information
has been lost due to our direct and/or reverse transla-
tion systems. The sentences with higher information
loss are selected for translation by a human.
4 Experiments
The SMT system we applied in our experiments is
PORTAGE (Ueffing et al, 2007a). The models (or
features) which are employed by the decoder are:
(a) one or several phrase table(s), which model the
translation direction p(f | e), (b) one or several
n-gram language model(s) trained with the SRILM
toolkit (Stolcke, 2002); in the experiments reported
here, we used 4-gram models on the NIST data,
and a trigram model on EuroParl, (c) a distortion
corpus language use sentences
EuroParl Fr,Ge,Sp
in-dom L 5K
in-dom U 20K
in-dom dev 2K
in-dom test 2K
See Sec. 4.2 Bangla
in-dom L 11K
in-dom U 20K
in-dom dev 450
in-dom test 1K
Hansards Fr out-dom L 5K
Table 1: Specification of different data sets we will use in
experiments. The target language is English in the bilin-
gual sets, and the source languages are either French (Fr),
German (Ge), Spanish (Sp), or Bangla.
model which assigns a penalty based on the number
of source words which are skipped when generating
a new target phrase, and (d) a word penalty. These
different models are combined log-linearly. Their
weights are optimized w.r.t. BLEU score using the
algorithm described in (Och, 2003). This is done on
a development corpus which we will call dev1 in this
paper.
The weight vectors in n-gram and similarity
methods are set to (.15, .2, .3, .35) to emphasize
longer n-grams. We set ? = ? = .35 for HAS,
and use the 100-best list of translations when identi-
fying candidate phrases while setting the maximum
phrase length to 10. We set ? = .5 to smooth proba-
bilities when computing scores based on translation
units.
4.1 Simulated Low Density Language Pairs
We use three language pairs (French-English,
German-English, Spanish-English) to compare all of
the proposed sentence selection strategies in a simu-
lated AL setting. The training data comes from Eu-
roParl corpus as distributed for the shared task in
the NAACL 2006 workshop on statistical machine
translation (WSMT06). For each language pair, the
first 5K sentences from its bilingual corpus consti-
tute L, and the next 20K sentences serve as U where
the target side translation is ignored. The size of L
was taken to be 5K in order to be close to a real-
istic setting in SMT. We use the first 2K sentences
from the test sets provided for WSMT06, which are
in-domain, as our test sets. The corpus statistics are
summarized in Table 1. The results are shown in
Fig. 2. After building the initial MT systems, we se-
419
0 5 10 15 20 25
0.19
0.195
0.2
0.205
0.21
0.215
0.22
0.225
Added Sentences (multiple of 200)
French to English
 
 
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
0 5 10 15 20 25
0.145
0.15
0.155
0.16
0.165
0.17
0.175
Added Sentences (multiple of 200)
German to English
 
 
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
0 5 10 15 20 25
0.2
0.205
0.21
0.215
0.22
0.225
0.23
Added Sentences (multiple of 200)
Spanish to English
 
 
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
0 5 10 15 20 25
0.19
0.195
0.2
0.205
0.21
0.215
0.22
0.225
Added Sentences (multiple of 200)
French to English
 
 
Geom 4?gram
Geom 1?gram
Similarity
Combined
Random
0 5 10 15 20 25
0.145
0.15
0.155
0.16
0.165
0.17
0.175
Added Sentences (multiple of 200)
German to English
 
 
Geom 4?gram
Geom 1?gram
Similarity
Combined
Random
0 5 10 15 20 25
0.2
0.205
0.21
0.215
0.22
0.225
0.23
Added Sentences (multiple of 200)
Spanish to English
 
 
Geom 4?gram
Geom 1?gram
Similarity
Combined
Random
Figure 2: BLEU scores for different sentence selection strategies per iteration of the AL algorithm. Plots at the top
show the performance of sentence selection methods which depend on the target language in addition to the source
language (hierarchical adaptive sampling, reverse model, decoder confidence, average and geometric phrase-based
score), and plots at the bottom show methods which are independent of the target language (geometric 4-gram and
1-gram, similarity to L, and random sentence selection baseline).
lect and remove 200 sentence from U in each itera-
tion and add them together with translations to L for
25 iterations. Each experiment which involves ran-
domness, such as random sentence selection base-
line and HAS, is averaged over three independent
runs. Selecting sentences based on the phrase-based
utility score outperforms the strong random sentence
selection baseline and other methods (Table 2). De-
coder confidence performs poorly as a criterion for
sentence selection in this setting, and HAS which
is built on top of confidence and similarity scores
outperforms both of them. Although choosing sen-
tences based on their n-gram score ignores the re-
lationship between source and target languages, this
methods outperforms random sentence selection.
4.2 Realistic Low Density Language Pair
We apply active learning to the Bangla-English ma-
chine translation task. Bangla is the official lan-
guage of Bangladesh and second most spoken lan-
guage in India. It has more than 200 million speak-
ers around the world. However, Bangla has few
available language resources, and lacks resources
for machine translation. In our experiments, we use
training data provided by the Linguistic Data Con-
sortium5 containing ?11k sentences. It contains
newswire text from the BBC Asian Network and
some other South Asian news websites. A bilingual
Bangla-English dictionary collected from different
websites was also used as part of the training set
which contains around 85k words. Our monolingual
corpus6 is built by collecting text from the Prothom
Alo newspaper, and contains all the news available
for the year of 2005 ? including magazines and pe-
riodicals. The corpus has 18,067,470 word tokens
and 386,639 word types. For our language model we
used data from the English section of EuroParl. The
5LDC Catalog No.: LDC2008E29.
6Provided by the Center for Research on Bangla Language
Processing, BRAC University, Bangladesh.
420
development set used to optimize the model weights
in the decoder, and test set used for evaluation was
taken from the same LDC corpus mentioned above.
We applied our active learning framework to the
problem of creating a larger Bangla-English parallel
text resource. The second author is a native speaker
of Bangla and participated in the active learning
loop, translating 100 sentences in each iteration. We
compared a smaller number of alternative methods
to keep the annotation cost down. The results are
shown in Fig. 3. Unlike the simulated setting, in this
realistic setting for AL, adding more human transla-
tion does not always result in better translation per-
formance7. Geom 4-gram and Geom phrase are the
features that prove most useful in extracting useful
sentences for the human expert to translate.
4.3 Domain Adaptation
In this section, we investigate the behavior of the
proposed methods when unlabeled data U and test
data T are in-domain and parallel training text L is
out-of-domain.
We report experiments for French to English
translation task where T and development sets are
the same as those in section 4.1 but the bilingual
training data come from Hansards8 corpus. The do-
main is similar to EuroParl, but the vocabulary is
very different. The results are shown in Fig. 4, and
summarized in Table 3. As expected, unigram based
sentence selection performs well in this scenario
since it quickly expands the lexicon set of the bilin-
gual data in an effective manner (Fig 5). By ignor-
7This is likely due to the fact that the translator in the AL
loop was not the same as the original translator for the labeled
data.
8The transcription of official records of the Cana-
dian Parliament as distributed at http://www.isi.edu/natural-
language/download/hansard
Lang. Geom Phrase Random (baseline)
Pair bleu% per% wer% bleu% per% wer%
Fr-En 22.49 27.99 38.45 21.97 28.31 38.80
Gr-En 17.54 31.51 44.28 17.25 31.63 44.41
Sp-En 23.03 28.86 39.17 23.00 28.97 39.21
Table 2: Phrase-based utility selection is compared
with random sentence selection baseline with respect to
BLEU, wer (word error rate), and per (position indepen-
dent word error rate) across three language pairs.
method bleu% per% wer%
Geom 1-gram 14.92 34.83 46.06
Confidence 14.74 35.02 46.11
Random (baseline) 14.11 35.28 46.47
Table 3: Comparison of methods in domain adaptation
scenario. The bold numbers show statistically significant
improvement with respect to the baseline.
ing sentences for which the translations are already
known based on L, it does not waste resources. On
the other hand, it raises the importance of high fre-
quency words in U . Interestingly, decoder confi-
dence is also a good criterion for sentence selection
in this particular case.
5 Related Work
Despite the promise of active learning for SMT
for domain adaptation and low-density/low-resource
languages, there has been very little work published
on this issue. A Ph.D. proposal by Chris Callison-
Burch (Callison-burch, 2003) lays out the promise
of AL for SMT and proposes some algorithms.
However, the lack of experimental results means that
performance and feasibility of those methods can-
not be compared to ours. (Mohit and Hwa, 2007)
provide a technique to classify phrases as difficult
to translate (DTP), and incorporate human transla-
tions for these phrases. Their approach is differ-
ent from AL: they use human translations for DTPs
in order to improve translation output in the de-
coder. There is work on sampling sentence pairs for
SMT (Kauchak, 2006; Eck et al, 2005) but the goal
0 1 2 3 4 5
0.05
0.051
0.052
0.053
0.054
0.055
0.056
0.057
Added Sentences (multiple of 100)
BL
EU
 s
co
re
Bangla to English
 
 
Geom Phrase
HAS
Geom 4?gram
Random
Figure 3: Improving Bangla to English translation perfor-
mance using active learning.
421
0 5 10 15 20 25
0.125
0.13
0.135
0.14
0.145
0.15
Added Sentences (multiple of 200)
BL
EU
 s
co
re
French to English
 
 
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
0 5 10 15 20 25
0.125
0.13
0.135
0.14
0.145
0.15
Added Sentences (multiple of 200)
BL
EU
 s
co
re
French to English
 
 
Geom 4?gram
Geom 1?gram
Combined
Random
Figure 4: Performance of different sentence selection
methods for domain adaptation scenario.
has been to limit the amount of training data in order
to reduce the memory footprint of the SMT decoder.
To compute this score, (Eck et al, 2005) use n-gram
features very different from the n-gram features pro-
posed in this paper. (Kato and Barnard, 2007) imple-
ment an AL system for SMT for language pairs with
limited resources (En-Xhosa, En-Zulu, En-Setswana
and En-Afrikaans), but the experiments are on a very
small simulated data set. The only feature used is
the confidence score of the SMT system, which we
showed in our experiments is not a reliable feature.
6 Conclusions
We provided a novel active learning framework for
SMT which utilizes both labeled and unlabeled data.
Several sentence selection strategies were proposed
and comprehensively compared across three simu-
lated language pairs and a realistic setting of Bangla-
English translation with scarce resources. Based
on our experiments, we conclude that paying atten-
tion to units of translations, i.e. words and candi-
date phrases in particular, is essential to sentence se-
Fr2En Ge2En Sp2En Ha2En
Avg # of trans
1.30 1.26 1.27 1.30
1.24 1.25 1.20 1.26
1.22 1.23 1.19 1.24
1.22 1.24 1.19 1.24
Avg phrase len
2.85 2.56 2.85 2.85
3.47 2.74 3.54 3.17
3.95 3.34 3.94 3.48
3.58 2.94 3.63 3.36
# of phrases
27,566 29,297 30,750 27,566
78,026 64,694 93,593 108,787
79,343 63,191 93,276 115,177
77,394 65,198 94,597 115,671
# unique events
31,824 33,141 34,937 31,824
103,124 84,512 125,094 117,214
86,210 69,357 100,176 127,314
84,787 72,280 101,636 128,912
Table 4: Average number of english phrases per source
language phrase, average length of the source language
phrases, number of source language phrases, and number
of phrase pairs which has been seen once in the phrase ta-
bles across three language pairs (French text taken from
Hansard is abbreviated by ?Ha?). From top to bottom
in each row, the numbers belong to: before starting AL,
and after finishing AL based on ?Geom Phrase?, ?Confi-
dence?, and ?Random?.
lection in AL-SMT. Increasing the coverage of the
bilingual training data is important but is not the
only factor (see Table 4 and Fig. 5). For exam-
ple, decoder confidence for sentence selection has
low coverage (in terms of new words), but performs
well in the domain adaptation scenario and performs
poorly otherwise. In future work, we plan to ex-
plore selection methods based on potential phrases,
adaptive sampling using features other than decoder
confidence and the use of features from confidence
estimation in MT (Ueffing and Ney, 2007).
0 5 10 15 20 25
2000
4000
6000
8000
10000
12000
14000
16000
18000
Added Sentences (multiple of 200)
N
um
be
r o
f N
ew
 W
or
ds
                 
French to English
 
 
Geom 4?gram
HAS
Reverse
Confidence
Similarity
Random
Geom 1?gram
Geom Phrase
Figure 5: Number of words in domain adaptation sce-
nario.
422
References
Chris Callison-burch. 2003. Active learning for statisti-
cal machine translation. In PhD Proposal, Edinburgh
University.
David Cohn, Les Atlas, and Richard Ladner. 1994. Im-
proving generalization with active learning. In Ma-
chine Learning Journal.
Sanjoy Dasgupta and Daniel Hsu. 2008. Hierarchical
sampling for active learning. In proceedings of Inter-
national Conference on Machine Learning.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine translation
based in n-gram frequency and tf-idf. In proceedings
of International Workshop on Spoken Language Trans-
lation (IWSLT).
R.S.M. Kato and E. Barnard. 2007. Statistical transla-
tion with scarce resources: a south african case study.
SAIEE Africa Research Journal, 98(4):136?140, De-
cember.
David Kauchak. 2006. Contribution to research on ma-
chine translation. In PhD Thesis, University of Cali-
fornia at San Diego.
Behrang Mohit and Rebecca Hwa. 2007. Localization
of difficult-to-translate phrases. In proceedings of the
2nd ACL Workshop on Statistical Machine Transla-
tions.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In proceedings of
Annual Meeting of the Association for Computational
Linguistics (ACL).
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-
poport. 2008. Multi-task active learning for linguistic
annotations. In proceedings of Annual Meeting of the
Association for Computational Linguistics (ACL).
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In proceedings of Interna-
tional Conference on Spoken Language Processing
(ICSLP).
Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008.
Learning performance of a machine translation sys-
tem: a statistical and computational analysis. In pro-
ceedings of the Third Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics (ACL).
Nicola Ueffing and Hermann Ney. 2007. Word-level
confidence estimation for machine translation. Com-
putational Linguistics, 33(1):9?40.
N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson.
2007a. NRC?s Portage system for WMT 2007. In
Proc. ACL Workshop on SMT.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007b. Transductive learning for statistical machine
translation. In proceedings of Annual Meeting of the
Association for Computational Linguistics (ACL).
423
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 25?32,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Transductive learning for statistical machine translation
Nicola Ueffing
National Research Council Canada
Gatineau, QC, Canada
nicola.ueffing@nrc.gc.ca
Gholamreza Haffari and Anoop Sarkar
Simon Fraser University
Burnaby, BC, Canada
{ghaffar1,anoop}@cs.sfu.ca
Abstract
Statistical machine translation systems are
usually trained on large amounts of bilin-
gual text and monolingual text in the tar-
get language. In this paper we explore the
use of transductive semi-supervised meth-
ods for the effective use of monolingual data
from the source language in order to im-
prove translation quality. We propose sev-
eral algorithms with this aim, and present the
strengths and weaknesses of each one. We
present detailed experimental evaluations on
the French?English EuroParl data set and on
data from the NIST Chinese?English large-
data track. We show a significant improve-
ment in translation quality on both tasks.
1 Introduction
In statistical machine translation (SMT), translation
is modeled as a decision process. The goal is to find
the translation t of source sentence s which maxi-
mizes the posterior probability:
argmax
t
p(t | s) = argmax
t
p(s | t) ? p(t) (1)
This decomposition of the probability yields two dif-
ferent statistical models which can be trained in-
dependently of each other: the translation model
p(s | t) and the target language model p(t).
State-of-the-art SMT systems are trained on large
collections of text which consist of bilingual corpora
(to learn the parameters of p(s | t)), and of monolin-
gual target language corpora (for p(t)). It has been
shown that adding large amounts of target language
text improves translation quality considerably. How-
ever, the availability of monolingual corpora in the
source language does not help improve the system?s
performance. We will show how such corpora can
be used to achieve higher translation quality.
Even if large amounts of bilingual text are given,
the training of the statistical models usually suffers
from sparse data. The number of possible events,
i.e. phrase pairs or pairs of subtrees in the two lan-
guages, is too big to reliably estimate a probabil-
ity distribution over such pairs. Another problem is
that for many language pairs the amount of available
bilingual text is very limited. In this work, we will
address this problem and propose a general frame-
work to solve it. Our hypothesis is that adding infor-
mation from source language text can also provide
improvements. Unlike adding target language text,
this hypothesis is a natural semi-supervised learn-
ing problem. To tackle this problem, we propose
algorithms for transductive semi-supervised learn-
ing. By transductive, we mean that we repeatedly
translate sentences from the development set or test
set and use the generated translations to improve the
performance of the SMT system. Note that the eval-
uation step is still done just once at the end of our
learning process. In this paper, we show that such
an approach can lead to better translations despite
the fact that the development and test data are typi-
cally much smaller in size than typical training data
for SMT systems.
Transductive learning can be seen as a means to
adapt the SMT system to a new type of text. Say a
system trained on newswire is used to translate we-
blog texts. The proposed method adapts the trained
models to the style and domain of the new input.
2 Baseline MT System
The SMT system we applied in our experiments is
PORTAGE. This is a state-of-the-art phrase-based
translation system which has been made available
25
to Canadian universities for research and education
purposes. We provide a basic description here; for a
detailed description see (Ueffing et al, 2007).
The models (or features) which are employed by
the decoder are: (a) one or several phrase table(s),
which model the translation direction p(s | t), (b) one
or several n-gram language model(s) trained with
the SRILM toolkit (Stolcke, 2002); in the experi-
ments reported here, we used 4-gram models on the
NIST data, and a trigram model on EuroParl, (c)
a distortion model which assigns a penalty based
on the number of source words which are skipped
when generating a new target phrase, and (d) a word
penalty. These different models are combined log-
linearly. Their weights are optimized w.r.t. BLEU
score using the algorithm described in (Och, 2003).
This is done on a development corpus which we will
call dev1 in this paper. The search algorithm imple-
mented in the decoder is a dynamic-programming
beam-search algorithm.
After the main decoding step, rescoring with ad-
ditional models is performed. The system generates
a 5,000-best list of alternative translations for each
source sentence. These lists are rescored with the
following models: (a) the different models used in
the decoder which are described above, (b) two dif-
ferent features based on IBM Model 1 (Brown et al,
1993), (c) posterior probabilities for words, phrases,
n-grams, and sentence length (Zens and Ney, 2006;
Ueffing and Ney, 2007), all calculated over the N -
best list and using the sentence probabilities which
the baseline system assigns to the translation hy-
potheses. The weights of these additional models
and of the decoder models are again optimized to
maximize BLEU score. This is performed on a sec-
ond development corpus, dev2.
3 The Framework
3.1 The Algorithm
Our transductive learning algorithm, Algorithm 1,
is inspired by the Yarowsky algorithm (Yarowsky,
1995; Abney, 2004). The algorithm works as fol-
lows: First, the translation model is estimated based
on the sentence pairs in the bilingual training data L.
Then, a set of source language sentences, U , is trans-
lated based on the current model. A subset of good
translations and their sources, Ti, is selected in each
iteration and added to the training data. These se-
lected sentence pairs are replaced in each iteration,
and only the original bilingual training data, L, is
kept fixed throughout the algorithm. The process
of generating sentence pairs, selecting a subset of
good sentence pairs, and updating the model is con-
tinued until a stopping condition is met. Note that
we run this algorithm in a transductive setting which
means that the set of sentences U is drawn either
from a development set or the test set that will be
used eventually to evaluate the SMT system or from
additional data which is relevant to the development
or test set. In Algorithm 1, changing the definition
of Estimate, Score and Select will give us the dif-
ferent semi-supervised learning algorithms we will
discuss in this paper.
Given the probability model p(t | s), consider the
distribution over all possible valid translations t for
a particular input sentence s. We can initialize
this probability distribution to the uniform distribu-
tion for each sentence s in the unlabeled data U .
Thus, this distribution over translations of sentences
from U will have the maximum entropy. Under
certain precise conditions, as described in (Abney,
2004), we can analyze Algorithm 1 as minimizing
the entropy of the distribution over translations of U .
However, this is true only when the functions Esti-
mate, Score and Select have very prescribed defini-
tions. In this paper, rather than analyze the conver-
gence of Algorithm 1 we run it for a fixed number
of iterations and instead focus on finding useful def-
initions for Estimate, Score and Select that can be
experimentally shown to improve MT performance.
3.2 The Estimate Function
We consider the following different definitions for
Estimate in Algorithm 1:
Full Re-training (of all translation models): If
Estimate(L, T ) estimates the model parameters
based on L ? T , then we have a semi-supervised al-
gorithm that re-trains a model on the original train-
ing data L plus the sentences decoded in the last it-
eration. The size of L can be controlled by filtering
the training data (see Section 3.5).
Additional Phrase Table: If, on the other hand, a
new phrase translation table is learned on T only
and then added as a new component in the log-linear
model, we have an alternative to the full re-training
26
Algorithm 1 Transductive learning algorithm for statistical machine translation
1: Input: training set L of parallel sentence pairs. // Bilingual training data.
2: Input: unlabeled set U of source text. // Monolingual source language data.
3: Input: number of iterations R, and size of n-best list N .
4: T?1 := {}. // Additional bilingual training data.
5: i := 0. // Iteration counter.
6: repeat
7: Training step: pi(i) := Estimate(L, Ti?1).
8: Xi := {}. // The set of generated translations for this iteration.
9: for sentence s ? U do
10: Labeling step: Decode s using pi(i) to obtain N best sentence pairs with their scores
11: Xi := Xi ? {(tn, s, pi(i)(tn | s))Nn=1}
12: end for
13: Scoring step: Si := Score(Xi) // Assign a score to sentence pairs (t, s) from X .
14: Selection step: Ti := Select(Xi, Si) // Choose a subset of good sentence pairs (t, s) from X .
15: i := i+ 1.
16: until i > R
of the model on labeled and unlabeled data which
can be very expensive if L is very large (as on the
Chinese?English data set). This additional phrase
table is small and specific to the development or
test set it is trained on. It overlaps with the origi-
nal phrase tables, but also contains many new phrase
pairs (Ueffing, 2006).
Mixture Model: Another alternative for Estimate
is to create a mixture model of the phrase table prob-
abilities with new phrase table probabilities
p(s | t) = ? ? Lp(s | t) + (1? ?) ? Tp(s | t) (2)
where Lp and Tp are phrase table probabilities esti-
mated on L and T , respectively. In cases where new
phrase pairs are learned from T , they get added into
the merged phrase table.
3.3 The Scoring Function
In Algorithm 1, the Score function assigns a score to
each translation hypothesis t. We used the following
scoring functions in our experiments:
Length-normalized Score: Each translated sen-
tence pair (t, s) is scored according to the model
probability p(t | s) normalized by the length |t| of the
target sentence:
Score(t, s) = p(t | s) 1|t| (3)
Confidence Estimation: The confidence estimation
which we implemented follows the approaches sug-
gested in (Blatz et al, 2003; Ueffing and Ney, 2007):
The confidence score of a target sentence t is cal-
culated as a log-linear combination of phrase pos-
terior probabilities, Levenshtein-based word poste-
rior probabilities, and a target language model score.
The weights of the different scores are optimized
w.r.t. classification error rate (CER).
The phrase posterior probabilities are determined
by summing the sentence probabilities of all trans-
lation hypotheses in the N -best list which contain
this phrase pair. The segmentation of the sentence
into phrases is provided by the decoder. This sum
is then normalized by the total probability mass of
the N -best list. To obtain a score for the whole tar-
get sentence, the posterior probabilities of all target
phrases are multiplied. The word posterior proba-
bilities are calculated on basis of the Levenshtein
alignment between the hypothesis under consider-
ation and all other translations contained in the N -
best list. For details, see (Ueffing and Ney, 2007).
Again, the single values are multiplied to obtain a
score for the whole sentence. For NIST, the lan-
guage model score is determined using a 5-gram
model trained on the English Gigaword corpus, and
on French?English, we use the trigram model which
was provided for the NAACL 2006 shared task.
3.4 The Selection Function
The Select function in Algorithm 1 is used to create
the additional training data Ti which will be used in
27
the next iteration i + 1 by Estimate to augment the
original bilingual training data. We use the follow-
ing selection functions:
Importance Sampling: For each sentence s in the
set of unlabeled sentences U , the Labeling step in
Algorithm 1 generates an N -best list of translations,
and the subsequent Scoring step assigns a score for
each translation t in this list. The set of generated
translations for all sentences in U is the event space
and the scores are used to put a probability distri-
bution over this space, simply by renormalizing the
scores described in Section 3.3. We use importance
sampling to select K translations from this distri-
bution. Sampling is done with replacement which
means that the same translation may be chosen sev-
eral times. These K sampled translations and their
associated source sentences make up the additional
training data Ti.
Selection using a Threshold: This method com-
pares the score of each single-best translation to a
threshold. The translation is considered reliable and
added to the set Ti if its score exceeds the thresh-
old. Else it is discarded and not used in the addi-
tional training data. The threshold is optimized on
the development beforehand. Since the scores of the
translations change in each iteration, the size of Ti
also changes.
Keep All: This method does not perform any fil-
tering at all. It is simply assumed that all transla-
tions in the set Xi are reliable, and none of them are
discarded. Thus, in each iteration, the result of the
selection step will be Ti = Xi. This method was
implemented mainly for comparison with other se-
lection methods.
3.5 Filtering the Training Data
In general, having more training data improves the
quality of the trained models. However, when it
comes to the translation of a particular test set, the
question is whether all of the available training data
are relevant to the translation task or not. Moreover,
working with large amounts of training data requires
more computational power. So if we can identify a
subset of training data which are relevant to the cur-
rent task and use only this to re-train the models, we
can reduce computational complexity significantly.
We propose to Filter the training data, either
bilingual or monolingual text, to identify the parts
corpus use sentences
EuroParl phrase table+LM 688K
train100k phrase table 100K
train150k phrase table 150K
dev06 dev1 2,000
test06 test 3,064
Table 1: French?English corpora
corpus use sentences
non-UN phrase table+LM 3.2M
UN phrase table+LM 5.0M
English Gigaword LM 11.7M
multi-p3 dev1 935
multi-p4 dev2 919
eval-04 test 1,788
eval-06 test 3,940
Table 2: NIST Chinese?English corpora
which are relevant w.r.t. the test set. This filtering
is based on n-gram coverage. For a source sentence
s in the training data, its n-gram coverage over the
sentences in the test set is computed. The average
over several n-gram lengths is used as a measure
of relevance of this training sentence w.r.t. the test
corpus. Based on this, we select the top K source
sentences or sentence pairs.
4 Experimental Results
4.1 Setting
We ran experiments on two different corpora: one
is the French?English translation task from the Eu-
roParl corpus, and the other one is Chinese?English
translation as performed in the NIST MT evaluation
(www.nist.gov/speech/tests/mt).
For the French?English translation task, we used
the EuroParl corpus as distributed for the shared task
in the NAACL 2006 workshop on statistical ma-
chine translation. The corpus statistics are shown
in Table 1. Furthermore we filtered the EuroParl
corpus, as explained in Section 3.5, to create two
smaller bilingual corpora (train100k and train150k
in Table 1). The development set is used to optimize
the model weights in the decoder, and the evaluation
is done on the test set provided for the NAACL 2006
shared task.
For the Chinese?English translation task, we used
the corpora distributed for the large-data track in the
28
setting EuroParl NIST
full re-training w/ filtering ? ??
full re-training ?? ?
mixture model ? ?
new phrase table ff:
keep all ?? ?
imp. sampling norm. ?? ?
conf. ?? ?
threshold norm. ?? ?
conf. ?? ?
Table 3: Feasibility of settings for Algorithm 1
2006 NIST evaluation (see Table 2). We used the
LDC segmenter for Chinese. The multiple transla-
tion corpora multi-p3 and multi-p4 were used as de-
velopment corpora. Evaluation was performed on
the 2004 and 2006 test sets. Note that the train-
ing data consists mainly of written text, whereas the
test sets comprise three and four different genres:
editorials, newswire and political speeches in the
2004 test set, and broadcast conversations, broad-
cast news, newsgroups and newswire in the 2006
test set. Most of these domains have characteristics
which are different from those of the training data,
e.g., broadcast conversations have characteristics of
spontaneous speech, and the newsgroup data is com-
paratively unstructured.
Given the particular data sets described above, Ta-
ble 3 shows the various options for the Estimate,
Score and Select functions (see Section 3). The ta-
ble provides a quick guide to the experiments we
present in this paper vs. those we did not attempt due
to computational infeasibility. We ran experiments
corresponding to all entries marked with ? (see Sec-
tion 4.2). For those marked ?? the experiments pro-
duced only minimal improvement over the baseline
and so we do not discuss them in this paper. The en-
tries marked as ? were not attempted because they
are not feasible (e.g. full re-training on the NIST
data). However, these were run on the smaller Eu-
roParl corpus.
Evaluation Metrics
We evaluated the generated translations using
three different evaluation metrics: BLEU score (Pa-
pineni et al, 2002), mWER (multi-reference word
error rate), and mPER (multi-reference position-
independent word error rate) (Nie?en et al, 2000).
Note that BLEU score measures quality, whereas
mWER and mPER measure translation errors. We
will present 95%-confidence intervals for the base-
line system which are calculated using bootstrap re-
sampling. The metrics are calculated w.r.t. one and
four English references: the EuroParl data comes
with one reference, the NIST 2004 evaluation set
and the NIST section of the 2006 evaluation set
are provided with four references each, whereas the
GALE section of the 2006 evaluation set comes
with one reference only. This results in much lower
BLEU scores and higher error rates for the transla-
tions of the GALE set (see Section 4.2). Note that
these values do not indicate lower translation qual-
ity, but are simply a result of using only one refer-
ence.
4.2 Results
EuroParl
We ran our initial experiments on EuroParl to ex-
plore the behavior of the transductive learning algo-
rithm. In all experiments reported in this subsec-
tion, the test set was used as unlabeled data. The
selection and scoring was carried out using impor-
tance sampling with normalized scores. In one set
of experiments, we used the 100K and 150K train-
ing sentences filtered according to n-gram coverage
over the test set. We fully re-trained the phrase ta-
bles on these data and 8,000 test sentence pairs sam-
pled from 20-best lists in each iteration. The results
on the test set can be seen in Figure 1. The BLEU
score increases, although with slight variation, over
the iterations. In total, it increases from 24.1 to 24.4
for the 100K filtered corpus, and from 24.5 to 24.8
for 150K, respectively. Moreover, we see that the
BLEU score of the system using 100K training sen-
tence pairs and transductive learning is the same as
that of the one trained on 150K sentence pairs. So
the information extracted from untranslated test sen-
tences is equivalent to having an additional 50K sen-
tence pairs.
In a second set of experiments, we used the whole
EuroParl corpus and the sampled sentences for fully
re-training the phrase tables in each iteration. We
ran the algorithm for three iterations and the BLEU
score increased from 25.3 to 25.6. Even though this
29
0 2 4 6 8 10 12 14 16 1824.05
24.1
24.15
24.2
24.25
24.3
24.35
24.4
24.45
Iteration
Bleu
 sco
re
0 2 4 6 8 10 12 14 1624.45
24.5
24.55
24.6
24.65
24.7
24.75
24.8
24.85
Iteration
Bleu
 sco
re
Figure 1: Translation quality for importance sampling with full re-training on train100k (left) and train150k
(right). EuroParl French?English task.
is a small increase, it shows that the unlabeled data
contains some information which can be explored in
transductive learning.
In a third experiment, we applied the mixture
model idea as explained in Section 3.2. The initially
learned phrase table was merged with the learned
phrase table in each iteration with a weight of ? =
0.1. This value for ? was found based on cross val-
idation on a development set. We ran the algorithm
for 20 iterations and BLEU score increased from
25.3 to 25.7. Since this is very similar to the re-
sult obtained with the previous method, but with an
additional parameter ? to optimize, we did not use
mixture models on NIST.
Note that the single improvements achieved here
are slightly below the 95%-significance level. How-
ever, we observe them consistently in all settings.
NIST
Table 4 presents translation results on NIST with
different versions of the scoring and selection meth-
ods introduced in Section 3. In these experiments,
the unlabeled data U for Algorithm 1 is the develop-
ment or test corpus. For this corpus U , 5,000-best
lists were generated using the baseline SMT system.
Since re-training the full phrase tables is not feasi-
ble here, a (small) additional phrase table, specific to
U , was trained and plugged into the SMT system as
an additional model. The decoder weights thus had
to be optimized again to determine the appropriate
weight for this new phrase table. This was done on
the dev1 corpus, using the phrase table specific to
dev1. Every time a new corpus is to be translated,
an adapted phrase table is created using transductive
learning and used with the weight which has been
learned on dev1. In the first experiment presented
in Table 4, all of the generated 1-best translations
were kept and used for training the adapted phrase
tables. This method yields slightly higher transla-
tion quality than the baseline system. The second
approach we studied is the use of importance sam-
pling (IS) over 20-best lists, based either on length-
normalized sentence scores (norm.) or confidence
scores (conf.). As the results in Table 4 show, both
variants outperform the first method, with a consis-
tent improvement over the baseline across all test
corpora and evaluation metrics. The third method
uses a threshold-based selection method. Combined
with confidence estimation as scoring method, this
yields the best results. All improvements over the
baseline are significant at the 95%-level.
Table 5 shows the translation quality achieved on
the NIST test sets when additional source language
data from the Chinese Gigaword corpus compris-
ing newswire text is used for transductive learning.
These Chinese sentences were sorted according to
their n-gram overlap (see Section 3.5) with the de-
velopment corpus, and the top 5,000 Chinese sen-
tences were used. The selection and scoring in Al-
gorithm 1 were performed using confidence estima-
tion with a threshold. Again, a new phrase table was
trained on these data. As can be seen in Table 5, this
30
select score BLEU[%] mWER[%] mPER[%]
eval-04 (4 refs.)
baseline 31.8?0.7 66.8?0.7 41.5?0.5
keep all 33.1 66.0 41.3
IS norm. 33.5 65.8 40.9
conf. 33.2 65.6 40.4
thr norm. 33.5 65.9 40.8
conf. 33.5 65.3 40.8
eval-06 GALE (1 ref.)
baseline 12.7?0.5 75.8?0.6 54.6?0.6
keep all 12.9 75.7 55.0
IS norm. 13.2 74.7 54.1
conf. 12.9 74.4 53.5
thr norm. 12.7 75.2 54.2
conf. 13.6 73.4 53.2
eval-06 NIST (4 refs.)
baseline 27.9?0.7 67.2?0.6 44.0?0.5
keep all 28.1 66.5 44.2
IS norm. 28.7 66.1 43.6
conf. 28.4 65.8 43.2
thr norm. 28.3 66.1 43.5
conf. 29.3 65.6 43.2
Table 4: Translation quality using an additional
adapted phrase table trained on the dev/test sets.
Different selection and scoring methods. NIST
Chinese?English, best results printed in boldface.
system outperforms the baseline system on all test
corpora. The error rates are significantly reduced in
all three settings, and BLEU score increases in all
cases. A comparison with Table 4 shows that trans-
ductive learning on the development set and test cor-
pora, adapting the system to their domain and style,
is more effective in improving the SMT system than
the use of additional source language data.
In all experiments on NIST, Algorithm 1 was run
for one iteration. We also investigated the use of an
iterative procedure here, but this did not yield any
improvement in translation quality.
5 Previous Work
Semi-supervised learning has been previously ap-
plied to improve word alignments. In (Callison-
Burch et al, 2004), a generative model for word
alignment is trained using unsupervised learning on
parallel text. In addition, another model is trained on
a small amount of hand-annotated word alignment
data. A mixture model provides a probability for
system BLEU[%] mWER[%] mPER[%]
eval-04 (4 refs.)
baseline 31.8?0.7 66.8?0.7 41.5?0.5
add Chin. data 32.8 65.7 40.9
eval-06 GALE (1 ref.)
baseline 12.7?0.5 75.8?0.6 54.6?0.6
add Chin. data 13.1 73.9 53.5
eval-06 NIST (4 refs.)
baseline 27.9?0.7 67.2?0.6 44.0?0.5
add Chin. data 28.1 65.8 43.2
Table 5: Translation quality using an additional
phrase table trained on monolingual Chinese news
data. Selection step using threshold on confidence
scores. NIST Chinese?English.
word alignment. Experiments showed that putting a
large weight on the model trained on labeled data
performs best. Along similar lines, (Fraser and
Marcu, 2006) combine a generative model of word
alignment with a log-linear discriminative model
trained on a small set of hand aligned sentences. The
word alignments are used to train a standard phrase-
based SMT system, resulting in increased translation
quality .
In (Callison-Burch, 2002) co-training is applied
to MT. This approach requires several source lan-
guages which are sentence-aligned with each other
and all translate into the same target language. One
language pair creates data for another language pair
and can be naturally used in a (Blum and Mitchell,
1998)-style co-training algorithm. Experiments on
the EuroParl corpus show a decrease in WER. How-
ever, the selection algorithm applied there is actually
supervised because it takes the reference translation
into account. Moreover, when the algorithm is run
long enough, large amounts of co-trained data in-
jected too much noise and performance degraded.
Self-training for SMT was proposed in (Ueffing,
2006). An existing SMT system is used to translate
the development or test corpus. Among the gener-
ated machine translations, the reliable ones are au-
tomatically identified using thresholding on confi-
dence scores. The work which we presented here
differs from (Ueffing, 2006) as follows:
? We investigated different ways of scoring and
selecting the reliable translations and compared
our method to this work. In addition to the con-
31
fidence estimation used there, we applied im-
portance sampling and combined it with confi-
dence estimation for transductive learning.
? We studied additional ways of exploring the
newly created bilingual data, namely re-
training the full phrase translation model or cre-
ating a mixture model.
? We proposed an iterative procedure which
translates the monolingual source language
data anew in each iteration and then re-trains
the phrase translation model.
? We showed how additional monolingual
source-language data can be used in transduc-
tive learning to improve the SMT system.
6 Discussion
It is not intuitively clear why the SMT system can
learn something from its own output and is improved
through semi-supervised learning. There are two
main reasons for this improvement: Firstly, the se-
lection step provides important feedback for the sys-
tem. The confidence estimation, for example, dis-
cards translations with low language model scores or
posterior probabilities. The selection step discards
bad machine translations and reinforces phrases of
high quality. As a result, the probabilities of low-
quality phrase pairs, such as noise in the table or
overly confident singletons, degrade. Our experi-
ments comparing the various settings for transduc-
tive learning shows that selection clearly outper-
forms the method which keeps all generated transla-
tions as additional training data. The selection meth-
ods investigated here have been shown to be well-
suited to boost the performance of semi-supervised
learning for SMT.
Secondly, our algorithm constitutes a way of
adapting the SMT system to a new domain or style
without requiring bilingual training or development
data. Those phrases in the existing phrase tables
which are relevant for translating the new data are
reinforced. The probability distribution over the
phrase pairs thus gets more focused on the (reliable)
parts which are relevant for the test data. For an anal-
ysis of the self-trained phrase tables, examples of
translated sentences, and the phrases used in trans-
lation, see (Ueffing, 2006).
References
S. Abney. 2004. Understanding the Yarowsky Algo-
rithm. Comput. Ling., 30(3).
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2003. Confidence estimation for machine transla-
tion. Final report, JHU/CLSP Summer Workshop.
www.clsp.jhu.edu/ws2003/groups/estimate/.
A. Blum and T. Mitchell. 1998. Combining Labeled and
Unlabeled Data with Co-Training. In Proc. Computa-
tional Learning Theory.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Compu-
tational Linguistics, 19(2).
C. Callison-Burch, D. Talbot, and M. Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In Proc. ACL.
C. Callison-Burch. 2002. Co-training for statistical ma-
chine translation. Master?s thesis, School of Informat-
ics, University of Edinburgh.
A. Fraser and D. Marcu. 2006. Semi-supervised training
for statistical word alignment. In Proc. ACL.
S. Nie?en, F. J. Och, G. Leusch, and H. Ney. 2000. An
evaluation tool for machine translation: Fast evalua-
tion for MT research. In Proc. LREC.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. ACL.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. ICSLP.
N. Ueffing and H. Ney. 2007. Word-level confidence es-
timation for machine translation. Computational Lin-
guistics, 33(1):9?40.
N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson.
2007. NRC?s Portage system for WMT 2007. In
Proc. ACL Workshop on SMT.
N. Ueffing. 2006. Using monolingual source-language
data to improve MT performance. In Proc. IWSLT.
D. Yarowsky. 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods. In
Proc. ACL.
R. Zens and H. Ney. 2006. N-gram posterior
probabilities for statistical machine translation. In
Proc. HLT/NAACL Workshop on SMT.
32
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 181?189,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Active Learning for Multilingual Statistical Machine Translation?
Gholamreza Haffari and Anoop Sarkar
School of Computing Science, Simon Fraser University
British Columbia, Canada
{ghaffar1,anoop}@cs.sfu.ca
Abstract
Statistical machine translation (SMT)
models require bilingual corpora for train-
ing, and these corpora are often multi-
lingual with parallel text in multiple lan-
guages simultaneously. We introduce an
active learning task of adding a new lan-
guage to an existing multilingual set of
parallel text and constructing high quality
MT systems, from each language in the
collection into this new target language.
We show that adding a new language using
active learning to the EuroParl corpus pro-
vides a significant improvement compared
to a random sentence selection baseline.
We also provide new highly effective sen-
tence selection methods that improve AL
for phrase-based SMT in the multilingual
and single language pair setting.
1 Introduction
The main source of training data for statistical
machine translation (SMT) models is a parallel
corpus. In many cases, the same information is
available in multiple languages simultaneously as
a multilingual parallel corpus, e.g., European Par-
liament (EuroParl) and U.N. proceedings. In this
paper, we consider how to use active learning (AL)
in order to add a new language to such a multilin-
gual parallel corpus and at the same time we con-
struct an MT system from each language in the
original corpus into this new target language. We
introduce a novel combined measure of translation
quality for multiple target language outputs (the
same content from multiple source languages).
The multilingual setting provides new opportu-
nities for AL over and above a single language
pair. This setting is similar to the multi-task AL
scenario (Reichart et al, 2008). In our case, the
multiple tasks are individual machine translation
tasks for several language pairs. The nature of the
translation processes vary from any of the source
?Thanks to James Peltier for systems support for our ex-
periments. This research was partially supported by NSERC,
Canada (RGPIN: 264905) and an IBM Faculty Award.
languages to the new language depending on the
characteristics of each source-target language pair,
hence these tasks are competing for annotating the
same resource. However it may be that in a single
language pair, AL would pick a particular sentence
for annotation, but in a multilingual setting, a dif-
ferent source language might be able to provide a
good translation, thus saving annotation effort. In
this paper, we explore how multiple MT systems
can be used to effectively pick instances that are
more likely to improve training quality.
Active learning is framed as an iterative learn-
ing process. In each iteration new human labeled
instances (manual translations) are added to the
training data based on their expected training qual-
ity. However, if we start with only a small amount
of initial parallel data for the new target language,
then translation quality is very poor and requires
a very large injection of human labeled data to
be effective. To deal with this, we use a novel
framework for active learning: we assume we are
given a small amount of parallel text and a large
amount of monolingual source language text; us-
ing these resources, we create a large noisy par-
allel text which we then iteratively improve using
small injections of human translations. When we
build multiple MT systems from multiple source
languages to the new target language, each MT
system can be seen as a different ?view? on the de-
sired output translation. Thus, we can train our
multiple MT systems using either self-training or
co-training (Blum and Mitchell, 1998). In self-
training each MT system is re-trained using human
labeled data plus its own noisy translation output
on the unlabeled data. In co-training each MT sys-
tem is re-trained using human labeled data plus
noisy translation output from the other MT sys-
tems in the ensemble. We use consensus transla-
tions (He et al, 2008; Rosti et al, 2007; Matusov
et al, 2006) as an effective method for co-training
between multiple MT systems.
This paper makes the following contributions:
? We provide a new framework for multilingual
MT, in which we build multiple MT systems
and add a new language to an existing multi-
lingual parallel corpus. The multilingual set-
181
ting allows new features for active learning
which we exploit to improve translation qual-
ity while reducing annotation effort.
? We introduce new highly effective sentence
selection methods that improve phrase-based
SMT in the multilingual and single language
pair setting.
? We describe a novel co-training based active
learning framework that exploits consensus
translations to effectively select only those
sentences that are difficult to translate for all
MT systems, thus sharing annotation cost.
? We show that using active learning to add
a new language to the EuroParl corpus pro-
vides a significant improvement compared to
the strong random sentence selection base-
line.
2 AL-SMT: Multilingual Setting
Consider a multilingual parallel corpus, such as
EuroParl, which contains parallel sentences for
several languages. Our goal is to add a new lan-
guage to this corpus, and at the same time to con-
struct high quality MT systems from the existing
languages (in the multilingual corpus) to the new
language. This goal is formalized by the following
objective function:
O =
D?
d=1
?d ? TQ(MF d?E) (1)
where F d?s are the source languages in the mul-
tilingual corpus (D is the total number of lan-
guages), and E is the new language. The transla-
tion quality is measured by TQ for individual sys-
temsMF d?E ; it can be BLEU score or WER/PER
(Word error rate and position independent WER)
which induces a maximization or minimization
problem, respectively. The non-negative weights
?d reflect the importance of the different transla-
tion tasks and
?
d ?d = 1. AL-SMT formulation
for single language pair is a special case of this
formulation where only one of the ?d?s in the ob-
jective function (1) is one and the rest are zero.
Moreover the algorithmic framework that we in-
troduce in Sec. 2.1 for AL in the multilingual set-
ting includes the single language pair setting as a
special case (Haffari et al, 2009).
We denote the large unlabeled multilingual cor-
pus by U := {(f1j , .., f
D
j )}, and the small labeled
multilingual corpus by L := {(f1i , .., f
D
i , ei)}. We
overload the term entry to denote a tuple in L or
in U (it should be clear from the context). For a
single language pair we use U and L.
2.1 The Algorithmic Framework
Algorithm 1 represents our AL approach for the
multilingual setting. We train our initial MT sys-
tems {MF d?E}
D
d=1 on the multilingual corpus L,
and use them to translate all monolingual sen-
tences in U. We denote sentences in U together
with their multiple translations by U+ (line 4 of
Algorithm 1). Then we retrain the SMT sys-
tems on L ? U+ and use the resulting model to
decode the test set. Afterwards, we select and
remove a subset of highly informative sentences
from U, and add those sentences together with
their human-provided translations to L. This pro-
cess is continued iteratively until a certain level of
translation quality is met (we use the BLEU score,
WER and PER) (Papineni et al, 2002). In the
baseline, against which we compare our sentence
selection methods, the sentences are chosen ran-
domly.
When (re-)training the models, two phrase ta-
bles are learned for each SMT model: one from
the labeled data L and the other one from pseudo-
labeled data U+ (which we call the main and aux-
iliary phrase tables respectively). (Ueffing et al,
2007; Haffari et al, 2009) show that treating U+
as a source for a new feature function in a log-
linear model for SMT (Och and Ney, 2004) allows
us to maximally take advantage of unlabeled data
by finding a weight for this feature using minimum
error-rate training (MERT) (Och, 2003).
Since each entry in U+ has multiple transla-
tions, there are two options when building the aux-
iliary table for a particular language pair (F d, E):
(i) to use the corresponding translation ed of the
source language in a self-training setting, or (ii) to
use the consensus translation among all the trans-
lation candidates (e1, .., eD) in a co-training set-
ting (sharing information between multiple SMT
models).
A whole range of methods exist in the literature
for combining the output translations of multiple
MT systems for a single language pair, operating
either at the sentence, phrase, or word level (He et
al., 2008; Rosti et al, 2007; Matusov et al, 2006).
The method that we use in this work operates at
the sentence level, and picks a single high qual-
ity translation from the union of the n-best lists
generated by multiple SMT models. Sec. 5 gives
182
Algorithm 1 AL-SMT-Multiple
1: Given multilingual corpora L and U
2: {MF d?E}
D
d=1 = multrain(L, ?)
3: for t = 1, 2, ... do
4: U+ = multranslate(U, {MF d?E}
D
d=1)
5: Select k sentences from U+, and ask a hu-
man for their true translations.
6: Remove the k sentences from U, and add
the k sentence pairs (translated by human)
to L
7: {MF d?E}
D
d=1 = multrain(L,U
+
)
8: Monitor the performance on the test set
9: end for
more details about features which are used in our
consensus finding method, and how it is trained.
Now let us address the important question of se-
lecting highly informative sentences (step 5 in the
Algorithm 1) in the following section.
3 Sentence Selection: Multiple Language
Pairs
The goal is to optimize the objective function
(1) with minimum human effort in providing the
translations. This motivates selecting sentences
which are maximally beneficial for all the MT sys-
tems. In this section, we present several protocols
for sentence selection based on the combined in-
formation from multiple language pairs.
3.1 Alternating Selection
The simplest selection protocol is to choose k sen-
tences (entries) in the first iteration of AL which
improve maximally the first modelMF 1?E , while
ignoring other models. In the second iteration, the
sentences are selected with respect to the second
model, and so on (Reichart et al, 2008).
3.2 Combined Ranking
Pick any AL-SMT scoring method for a single lan-
guage pair (see Sec. 4). Using this method, we
rank the entries in unlabeled data U for each trans-
lation task defined by language pair (F d, E). This
results in several ranking lists, each of which rep-
resents the importance of entries with respect to
a particular translation task. We combine these
rankings using a combined score:
Score
(
(f1, .., fD)
)
=
D?
d=1
?dRankd(f
d
)
Rankd(.) is the ranking of a sentence in the list for
the dth translation task (Reichart et al, 2008).
3.3 Disagreement Among the Translations
Disagreement among the candidate translations of
a particular entry is evidence for the difficulty of
that entry for different translation models. The
reason is that disagreement increases the possibil-
ity that most of the translations are not correct.
Therefore it would be beneficial to ask human for
the translation of these hard entries.
Now the question is how to quantify the no-
tion of disagreement among the candidate trans-
lations (e1, .., eD). We propose two measures of
disagreement which are related to the portion of
shared n-grams (n ? 4) among the translations:
? Let ec be the consensus among all the can-
didate translations, then define the disagree-
ment as
?
d ?d
(
1? BLEU(ec, ed)
)
.
? Based on the disagreement of every pair
of candidate translations:
?
d ?d
?
d?
(
1 ?
BLEU(ed
?
, ed)
)
.
For the single language pair setting, (Haffari et
al., 2009) presents and compares several sentence
selection methods for statistical phrase-based ma-
chine translation. We introduce novel techniques
which outperform those methods in the next sec-
tion.
4 Sentence Selection: Single Language
Pair
Phrases are basic units of translation in phrase-
based SMT models. The phrases which may po-
tentially be extracted from a sentence indicate its
informativeness. The more new phrases a sen-
tence can offer, the more informative it is; since it
boosts the generalization of the model. Addition-
ally phrase translation probabilities need to be es-
timated accurately, which means sentences that of-
fer phrases whose occurrences in the corpus were
rare are informative. When selecting new sen-
tences for human translation, we need to pay atten-
tion to this tradeoff between exploration and ex-
ploitation, i.e. selecting sentences to discover new
phrases v.s. estimating accurately the phrase trans-
lation probabilities. Smoothing techniques partly
handle accurate estimation of translation probabil-
ities when the events occur rarely (indeed it is the
main reason for smoothing). So we mainly focus
on how to expand effectively the lexicon or set of
phrases of the model.
The more frequent a phrase (not a phrase pair)
is in the unlabeled data, the more important it is to
183
know its translation; since it is more likely to see
it in test data (specially when the test data is in-
domain with respect to unlabeled data). The more
frequent a phrase is in the labeled data, the more
unimportant it is; since probably we have observed
most of its translations.
In the labeled dataL, phrases are the ones which
are extracted by the SMT models; but what are
the candidate phrases in the unlabeled data U?
We use the currently trained SMT models to an-
swer this question. Each translation in the n-best
list of translations (generated by the SMT mod-
els) corresponds to a particular segmentation of
a sentence, which breaks that sentence into sev-
eral fragments (see Fig. 1). Some of these frag-
ments are the source language part of a phrase pair
available in the phrase table, which we call regular
phrases and denote their set byXregs for a sentence
s. However, there are some fragments in the sen-
tence which are not covered by the phrase table ?
possibly because of the OOVs (out-of-vocabulary
words) or the constraints imposed by the phrase
extraction algorithm ? called Xoovs for a sentence
s. Each member of Xoovs offers a set of potential
phrases (also referred to as OOV phrases) which
are not observed due to the latent segmentation of
this fragment. We present two generative models
for the phrases and show how to estimate and use
them for sentence selection.
4.1 Model 1
In the first model, the generative story is to gen-
erate phrases for each sentence based on indepen-
dent draws from a multinomial. The sample space
of the multinomial consists of both regular and
OOV phrases.
We build two models, i.e. two multinomials,
one for labeled data and the other one for unla-
beled data. Each model is trained by maximizing
the log-likelihood of its corresponding data:
LD :=
?
s?D
?P (s)
?
x?Xs
logP (x|?D) (2)
where D is either L or U , ?P (s) is the empiri-
cal distribution of the sentences1, and ?D is the
parameter vector of the corresponding probability
1P? (s) is the number of times that the sentence s is seen
in D divided by the number of all sentences in D.
distribution. When x ? Xoovs , we will have
P (x|?U ) =
?
h?Hx
P (x, h|?U )
=
?
h?Hx
P (h)P (x|h,?U )
=
1
|Hx|
?
h?Hx
?
y?Y hx
?U (y) (3)
where Hx is the space of all possible segmenta-
tions for the OOV fragment x, Y hx is the result-
ing phrases from x based on the segmentation h,
and ?U (y) is the probability of the OOV phrase
y in the multinomial associated with U . We let
Hx to be all possible segmentations of the frag-
ment x for which the resulting phrase lengths are
not greater than the maximum length constraint for
phrase extraction in the underlying SMT model.
Since we do not know anything about the segmen-
tations a priori, we have put a uniform distribution
over such segmentations.
Maximizing (2) to find the maximum likelihood
parameters for this model is an extremely diffi-
cult problem2. Therefore, we maximize the fol-
lowing lower-bound on the log-likelihood which
is derived using Jensen?s inequality:
LD ?
?
s?D
?P (s)
[ ?
x?Xregs
log ?D(x)
+
?
x?Xoovs
?
h?Hx
1
|Hx|
?
y?Y hx
log ?D(y)
]
(4)
Maximizing (4) amounts to set the probability of
each regular / potential phrase proportional to its
count / expected count in the data D.
Let ?k(xi:j) be the number of possible segmen-
tations from position i to position j of an OOV
fragment x, and k is the maximum phrase length;
?k(x1:|x|) =
?
??
??
0, if |x| = 0
1, if |x| = 1
?k
i=1 ?k(xi+1:|x|), otherwise
which gives us a dynamic programming algorithm
to compute the number of segmentation |Hx| =
?k(x1:|x|) of the OOV fragment x. The expected
count of a potential phrase y based on an OOV
segment x is (see Fig. 1.c):
E[y|x] =
?
i?j ?[y=xi:j ]?k(x1:i?1)?k(xj+1:|x|)
?k(x)
2Setting partial derivatives of the Lagrangian to zero
amounts to finding the roots of a system of multivariate poly-
nomials (a major topic in Algebraic Geometry).
184
i will go to school on friday
Regular Phrases
OOV segment
go
to
school
go to
to school
2/3
2/3
1/3
1/3
1/3
i will
in friday
XXX
XXX
.01
.004
.
.
.
.
.
.
.
.
.
(a)
potential phr.
source
target prob
count
(b)
(c)
Figure 1: The given sentence in (b) is segmented, based on the source side phrases extracted from the phrase table in (a), to
yield regular phrases and OOV segment. The table in (c) shows the potential phrases extracted from the OOV segment ?go to
school? and their expected counts (denoted by count) where the maximum length for the potential phrases is set to 2. In the
example, ?go to school? has 3 segmentations with maximum phrase length 2: (go)(to school), (go to)(school), (go)(to)(school).
where ?[C] is 1 if the condition C is true, and zero
otherwise. We have used the fact that the num-
ber of occurrences of a phrase spanning the indices
[i, j] is the product of the number of segmentations
of the left and the right sub-fragments, which are
?k(x1:i?1) and ?k(xj+1:|x|) respectively.
4.2 Model 2
In the second model, we consider a mixture model
of two multinomials responsible for generating
phrases in each of the labeled and unlabeled data
sets. To generate a phrase, we first toss a coin and
depending on the outcome we either generate the
phrase from the multinomial associated with regu-
lar phrases ?regU or potential phrases ?
oov
U :
P (x|?U ) := ?U?
reg
U (x) + (1? ?U )?
oov
U (x)
where ?U includes the mixing weight ? and the
parameter vectors of the two multinomials. The
mixture model associated with L is written simi-
larly. The parameter estimation is based on maxi-
mizing a lower-bound on the log-likelihood which
is similar to what was done for the Model 1.
4.3 Sentence Scoring
The sentence score is a linear combination of two
terms: one coming from regular phrases and the
other from OOV phrases:
?1(s) :=
?
|Xregs |
?
x?Xregs
log
P (x|?U )
P (x|?L)
+
1? ?
|Xoovs |
?
x?Xoovs
?
h?Hx
1
|Hx|
log
?
y?Y hx
P (y|?U )
P (y|?L)
where we use either Model 1 or Model 2 for
P (.|?D). The first term is the log probability ra-
tio of regular phrases under phrase models corre-
sponding to unlabeled and labeled data, and the
second term is the expected log probability ratio
(ELPR) under the two models. Another option for
the contribution of OOV phrases is to take log of
expected probability ratio (LEPR):
?2(s) :=
?
|Xregs |
?
x?Xregs
log
P (x|?U )
P (x|?L)
+
1? ?
|Xoovs |
?
x?Xoovs
log
?
h?Hx
1
|Hx|
?
y?Y hx
P (y|?U )
P (y|?L)
It is not difficult to prove that there is no difference
between Model 1 and Model 2 when ELPR scor-
ing is used for sentence selection. However, the
situation is different for LEPR scoring: the two
models produce different sentence rankings in this
case.
5 Experiments
Corpora. We pre-processed the EuroParl corpus
(http://www.statmt.org/europarl) (Koehn, 2005)
and built a multilingual parallel corpus with
653,513 sentences, excluding the Q4/2000 por-
tion of the data (2000-10 to 2000-12) which is
reserved as the test set. We subsampled 5,000
sentences as the labeled data L and 20,000 sen-
tences as U for the pool of untranslated sentences
(while hiding the English part). The test set con-
sists of 2,000 multi-language sentences and comes
from the multilingual parallel corpus built from
Q4/2000 portion of the data.
Consensus Finding. Let T be the union of the n-
best lists of translations for a particular sentence.
The consensus translation tc is
argmax
t?T
w1
LM(t)
|t|
+w2
Qd(t)
|t|
+w3Rd(t)+w4,d
where LM(t) is the score from a 3-gram language
model, Qd(t) is the translation score generated by
the decoder for MF d?E if t is produced by the
dth SMT model, Rd(t) is the rank of the transla-
tion in the n-best list produced by the dth model,
w4,d is a bias term for each translation model to
make their scores comparable, and |t| is the length
185
1000 2000 3000 4000 500022.6
22.7
22.8
22.9
23
23.1
23.2
23.3
23.4
23.5
23.6
Added Sentences
BLE
U Sc
ore
French to English
 
 
Model 2 ? LEPRModel 1 ? ELPRGeom PhraseRandom
1000 2000 3000 4000 500023.2
23.4
23.6
23.8
24
24.2
24.4
24.6
24.8
25
Added Sentences
BLE
U Sc
ore
Spanish to English
 
 
Model 2 ? LEPRModel 1 ? ELPRGeom PhraseRandom
1000 2000 3000 4000 500016.2
16.4
16.6
16.8
17
17.2
17.4
17.6
17.8
Added Sentences
BLE
U Sc
ore
German to English
 
 
Model 2 ? LEPRModel 1 ? ELPRGeom PhraseRandom
Figure 2: The performance of different sentence selection strategies as the iteration of AL loop goes on for three translation
tasks. Plots show the performance of sentence selection methods for single language pair in Sec. 4 compared to the GeomPhrase
(Haffari et al, 2009) and random sentence selection baseline.
of the translation sentence. The number of weights
wi is 3 plus the number of source languages, and
they are trained using minimum error-rate training
(MERT) to maximize the BLEU score (Och, 2003)
on a development set.
Parameters. We use add- smoothing where  =
.5 to smooth the probabilities in Sec. 4; moreover
? = .4 for ELPR and LEPR sentence scoring and
maximum phrase length k is set to 4. For the mul-
tilingual experiments (which involve four source
languages) we set ?d = .25 to make the impor-
tance of individual translation tasks equal.
0 1000 2000 3000 4000 500018
18.5
19
19.5
20
20.5
Added Sentences
Avg 
BLEU
 Sco
re
Mulilingual da?de?nl?sv to en
 
 Self?TrainingCo?Training
Figure 3: Random sentence selection baseline using self-
training and co-training (Germanic languages to English).
5.1 Results
First we evaluate the proposed sentence selection
methods in Sec. 4 for the single language pair.
Then the best method from the single language
pair setting is used to evaluate sentence selection
methods for AL in multilingual setting. After
building the initial MT system for each experi-
ment, we select and remove 500 sentences from
U and add them together with translations to L for
10 total iterations. The random sentence selection
baselines are averaged over 3 independent runs.
mode self-train co-train
Method wer per wer per
Combined Rank 40.2 30.0 40.0 29.6
Alternate 41.0 30.2 40.1 30.1
Disagree-Pairwise 41.9 32.0 40.5 30.9
Disagree-Center 41.8 31.8 40.6 30.7
Random Baseline 41.6 31.0 40.5 30.7
Germanic languages to English
mode self-train co-train
Method wer per wer per
Combined Rank 37.7 27.3 37.3 27.0
Alternate 37.7 27.3 37.3 27.0
Random Baseline 38.6 28.1 38.1 27.6
Romance languages to English
Table 1: Comparison of multilingual selection methods with
WER (word error rate), PER (position independent WER).
95% confidence interval for WER numbers is 0.7 and for PER
numbers is 0.5. Bold: best result, italic: significantly better.
We use three language pairs in our single lan-
guage pair experiments: French-English, German-
English, and Spanish- English. In addition to ran-
dom sentence selection baseline, we also compare
the methods proposed in this paper to the best
method reported in (Haffari et al, 2009) denoted
by GeomPhrase, which differs from our models
since it considers each individual OOV segment as
a single OOV phrase and does not consider subse-
quences. The results are presented in Fig. 2. Se-
lecting sentences based on our proposed methods
outperform the random sentence selection baseline
and GeomPhrase. We suspect for the situations
where L is out-of-domain and the average phrase
length is relatively small, our method will outper-
form GeomPhrase even more.
For the multilingual experiments, we use Ger-
manic (German, Dutch, Danish, Swedish) and Ro-
mance (French, Spanish, Italian, Portuguese3) lan-
3A reviewer pointed out that EuroParl English-Portuguese
186
0 1000 2000 3000 4000 5000
18.2
18.4
18.6
18.8
19
19.2
19.4
19.6
19.8
20
Added Sentences
Avg 
BLE
U Sc
ore
Self?Train Mulilingual da?de?nl?sv to en
 
 
AlternateCombineRankDisagree?PairwiseDisagree?CenterRandom
1000 1500 2000 2500 3000 3500 4000 4500 500019.3
19.4
19.5
19.6
19.7
19.8
19.9
20
20.1
20.2
20.3
Added Sentences
Avg 
BLE
U Sc
ore
Co?Train Mulilingual da?de?nl?sv to en
 
 
AlternateCombineRankDisagree?PairwiseDisagree?CenterRandom
0 1000 2000 3000 4000 500021.6
21.8
22
22.2
22.4
22.6
22.8
23
23.2
23.4
23.6
Added Sentences
Avg 
BLE
U Sc
ore
Self?Train Mulilingual fr?es?it?pt to en
 
 
AlternateCombineRankRandom
1000 1500 2000 2500 3000 3500 4000 4500 500022.6
22.8
23
23.2
23.4
23.6
23.8
Added Sentences
Avg 
BLE
U Sc
ore
Co?Train Mulilingual fr?es?it?pt to en
 
 
AlternateCombineRankRandom
Figure 4: The left/right plot show the performance of our AL methods for multilingual setting combined with self-training/co-
training. The sentence selection methods from Sec. 3 are compared with random sentence selection baseline. The top plots cor-
respond to Danish-German-Dutch-Swedish to English, and the bottom plots correspond to French-Spanish-Italian-Portuguese
to English.
guages as the source and English as the target lan-
guage as two sets of experiments.4 Fig. 3 shows
the performance of random sentence selection for
AL combined with self-training/co-training for the
multi-source translation from the four Germanic
languages to English. It shows that the co-training
mode outperforms the self-training mode by al-
most 1 BLEU point. The results of selection
strategies in the multilingual setting are presented
in Fig. 4 and Tbl. 1. Having noticed that Model
1 with ELPR performs well in the single language
pair setting, we use it to rank entries for individual
translation tasks. Then these rankings are used by
?Alternate? and ?Combined Rank? selection strate-
gies in the multilingual case. The ?Combined
Rank? method outperforms all the other methods
including the strong random selection baseline in
both self-training and co-training modes. The
disagreement-based selection methods underper-
form the baseline for translation of Germanic lan-
guages to English, so we omitted them for the Ro-
mance language experiments.
5.2 Analysis
The basis for our proposed methods has been the
popularity of regular/OOV phrases in U and their
data is very noisy and future work should omit this pair.
4Choice of Germanic and Romance for our experimental
setting is inspired by results in (Cohn and Lapata, 2007)
unpopularity in L, which is measured by P (x|?U )P (x|?L) .
We need P (x|?U ), the estimated distribution of
phrases in U , to be as similar as possible to P ?(x),
the true distribution of phrases in U . We investi-
gate this issue for regular/OOV phrases as follows:
? Using the output of the initially trained MT sys-
tem on L, we extract the regular/OOV phrases as
described in ?4. The smoothed relative frequen-
cies give us the regular/OOV phrasal distributions.
? Using the true English translation of the sen-
tences in U , we extract the true phrases. Separat-
ing the phrases into two sets of regular and OOV
phrases defined by the previous step, we use the
smoothed relative frequencies and form the true
OOV/regular phrasal distributions.
We use the KL-divergence to see how dissim-
ilar are a pair of given probability distributions.
As Tbl. 2 shows, the KL-divergence between the
true and estimated distributions are less than that
De2En Fr2En Es2En
KL(P ?reg ? Preg) 4.37 4.17 4.38
KL(P ?reg ? unif ) 5.37 5.21 5.80
KL(P ?oov ? Poov) 3.04 4.58 4.73
KL(P ?oov ? unif ) 3.41 4.75 4.99
Table 2: For regular/OOV phrases, the KL-divergence be-
tween the true distribution (P ?) and the estimated (P ) or uni-
form (unif ) distributions are shown, where:
KL(P ? ? P ) :=
P
x P
?(x) log P
?(x)
P (x) .
187
100 101 102 103 104 10510
?6
10?5
10?4
10?3
10?2
10?1
100
Rank
Prob
abili
ty
Regular Phrases in U
 
 Estimated DistributionTrue Distribution
100 101 102 103 104 10510
?6
10?5
10?4
10?3
10?2
10?1
100
Rank
Prob
abili
ty
OOV Phrases in U
 
 Estimated DistributionTrue Distribution
Figure 5: The log-log Zipf plots representing the true and
estimated probabilities of a (source) phrase vs the rank of
that phrase in the German to English translation task. The
plots for the Spanish to English and French to English tasks
are also similar to the above plots, and confirm a power law
behavior in the true phrasal distributions.
between the true and uniform distributions, in all
three language pairs. Since uniform distribution
conveys no information, this is evidence that there
is some information encoded in the estimated dis-
tribution about the true distribution. However
we noticed that the true distributions of regu-
lar/OOV phrases exhibit Zipfian (power law) be-
havior5 which is not well captured by the esti-
mated distributions (see Fig. 5). Enhancing the es-
timated distributions to capture this power law be-
havior would improve the quality of the proposed
sentence selection methods.
6 Related Work
(Haffari et al, 2009) provides results for active
learning for MT using a single language pair. Our
work generalizes to the use of multilingual corpora
using new methods that are not possible with a sin-
gle language pair. In this paper, we also introduce
new selection methods that outperform the meth-
ods in (Haffari et al, 2009) even for MT with a
single language pair. In addition in this paper by
considering multilingual parallel corpora we were
able to introduce co-training for AL, while (Haf-
fari et al, 2009) only use self-training since they
are using a single language pair.
5This observation is at the phrase level and not at the word
(Zipf, 1932) or even n-gram level (Ha et al, 2002).
(Reichart et al, 2008) introduces multi-task ac-
tive learning where unlabeled data require annota-
tions for multiple tasks, e.g. they consider named-
entities and parse trees, and showed that multi-
ple tasks helps selection compared to individual
tasks. Our setting is different in that the target lan-
guage is the same across multiple MT tasks, which
we exploit to use consensus translations and co-
training to improve active learning performance.
(Callison-Burch and Osborne, 2003b; Callison-
Burch and Osborne, 2003a) provide a co-training
approach to MT, where one language pair creates
data for another language pair. In contrast, our
co-training approach uses consensus translations
and our setting for active learning is very differ-
ent from their semi-supervised setting. A Ph.D.
proposal by Chris Callison-Burch (Callison-burch,
2003) lays out the promise of AL for SMT and
proposes some algorithms. However, the lack of
experimental results means that performance and
feasibility of those methods cannot be compared
to ours.
While we use consensus translations (He et al,
2008; Rosti et al, 2007; Matusov et al, 2006)
as an effective method for co-training in this pa-
per, unlike consensus for system combination, the
source languages for each of our MT systems are
different, which rules out a set of popular methods
for obtaining consensus translations which assume
translation for a single language pair. Finally, we
briefly note that triangulation (see (Cohn and Lap-
ata, 2007)) is orthogonal to the use of co-training
in our work, since it only enhances each MT sys-
tem in our ensemble by exploiting the multilingual
data. In future work, we plan to incorporate trian-
gulation into our active learning approach.
7 Conclusion
This paper introduced the novel active learning
task of adding a new language to an existing multi-
lingual set of parallel text. We construct SMT sys-
tems from each language in the collection into the
new target language. We show that we can take ad-
vantage of multilingual corpora to decrease anno-
tation effort thanks to the highly effective sentence
selection methods we devised for active learning
in the single language-pair setting which we then
applied to the multilingual sentence selection pro-
tocols. In the multilingual setting, a novel co-
training method for active learning in SMT is pro-
posed using consensus translations which outper-
forms AL-SMT with self-training.
188
References
Avrim Blum and Tom Mitchell. 1998. Combin-
ing Labeled and Unlabeled Data with Co-Training.
In Proceedings of the Eleventh Annual Conference
on Computational Learning Theory (COLT 1998),
Madison, Wisconsin, USA, July 24-26. ACM.
Chris Callison-Burch and Miles Osborne. 2003a.
Bootstrapping parallel corpora. In NAACL work-
shop: Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond.
Chris Callison-Burch and Miles Osborne. 2003b. Co-
training for statistical machine translation. In Pro-
ceedings of the 6th Annual CLUK Research Collo-
quium.
Chris Callison-burch. 2003. Active learning for statis-
tical machine translation. In PhD Proposal, Edin-
burgh University.
Trevor Cohn and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use of
multi-parallel corpora. In ACL.
Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F.J.
Smith. 2002. Extension of zipf?s law to words and
phrases. In Proceedings of the 19th international
conference on Computational linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In NAACL.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In EACL.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In ACL ?02: Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for lin-
guistic annotations. In ACL.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard M. Schwartz, and Bon-
nie Jean Dorr. 2007. Combining outputs from mul-
tiple machine translation systems. In NAACL.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In ACL.
George Zipf. 1932. Selective Studies and the Principle
of Relative Frequency in Language. Harvard Uni-
versity Press.
189
Using LTAG Based Features in Parse Reranking?
Libin Shen
Dept. of Computer & Info. Sci.
University of Pennsylvania
libin@cis.upenn.edu
Anoop Sarkar
School of Computing Science
Simon Fraser University
anoop@cs.sfu.ca
Aravind K. Joshi
Dept. of Computer & Info. Sci.
University of Pennsylvania
joshi@cis.upenn.edu
Abstract
We propose the use of Lexicalized Tree
Adjoining Grammar (LTAG) as a source
of features that are useful for reranking
the output of a statistical parser. In this
paper, we extend the notion of a tree ker-
nel over arbitrary sub-trees of the parse to
the derivation trees and derived trees pro-
vided by the LTAG formalism, and in ad-
dition, we extend the original definition
of the tree kernel, making it more lexi-
calized and more compact. We use LTAG
based features for the parse reranking task
and obtain labeled recall and precision of
89.7%/90.0% on WSJ section 23 of Penn
Treebank for sentences of length ? 100
words. Our results show that the use
of LTAG based tree kernel gives rise to
a 17% relative difference in f -score im-
provement over the use of a linear kernel
without LTAG based features.
1 Introduction
Recent work in statistical parsing has explored al-
ternatives to the use of (smoothed) maximum likeli-
hood estimation for parameters of the model. These
alternatives are distribution-free (Collins, 2001),
providing a discriminative method for resolving
parse ambiguity. Discriminative methods provide a
ranking between multiple choices for the most plau-
sible parse tree for a sentence, without assuming that
a particular distribution or stochastic process gener-
ated the alternative parses.
?We would like to thank Michael Collins for providing the
original n-best parsed data on which we ran our experiments
and the anonymous reviewers for their comments. The sec-
ond author is partially supported by NSERC, Canada (RGPIN:
264905).
Discriminative methods permit the use of feature
functions that can be used to condition on arbitrary
aspects of the input. This flexibility makes it possi-
ble to incorporate features of various of kinds. Fea-
tures can be defined on characters, words, part of
speech (POS) tags and context-free grammar (CFG)
rules, depending on the application to which the
model is applied.
Features defined on n-grams from the input are
the most commonly used for NLP applications.
Such n-grams can either be defined explicitly us-
ing some linguistic insight into the problem, or the
model can be used to search the entire space of n-
gram features using a kernel representation. One
example is the use of a polynomial kernel over se-
quences. However, to use all possible n-gram fea-
tures typically introduces too many noisy features,
which can result in lower accuracy. One way to
solve this problem is to use a kernel function that is
tailored for particular NLP applications, such as the
tree kernel (Collins and Duffy, 2001) for statistical
parsing.
In addition to n-gram features, more complex
high-level features are often exploited to obtain
higher accuracy, especially when discriminative
models are used for statistical parsing. For ex-
ample, all possible sub-trees can be used as fea-
tures (Collins and Duffy, 2002; Bod, 2003). How-
ever, most of the sub-trees are linguistically mean-
ingless, and are a source of noisy features thus limit-
ing efficiency and accuracy. An alternative to the use
of arbitrary sets of sub-trees is to use the set of ele-
mentary trees as defined in Lexicalized Tree Adjoin-
ing Grammar (LTAG) (Joshi and Schabes, 1997).
LTAG based features not only allow a more limited
and a linguistically more valid set of features over
sub-trees, they also provide the use of features that
use discontinuous sub-trees which are outside the
scope of previous tree kernel definitions using arbi-
trary sub-trees. In this paper, we use the LTAG based
features in the parse reranking problem (Collins,
2000; Collins and Duffy, 2002). We use the Sup-
port Vector Machine (SVM) (Vapnik, 1999) based
algorithm proposed in (Shen and Joshi, 2003) as the
reranker in this paper. We apply the tree kernel to
derivation trees of LTAG, and extract features from
derivation trees. Both the tree kernel and the linear
kernel on the richer feature set are used. Our exper-
iments show that the use of tree kernel on derivation
trees makes the notion of a tree kernel more power-
ful and more applicable.
2 Lexicalized Tree Adjoining Grammar
In this section, we give a brief introduction to the
Lexicalized Tree Adjoining Grammar (more details
can be found in (Joshi and Schabes, 1997)). In
LTAG, each word is associated with a set of elemen-
tary trees. Each elementary tree represents a possi-
ble tree structure for the word. There are two kinds
of elementary trees, initial trees and auxiliary trees.
Elementary trees can be combined through two op-
erations, substitution and adjunction. Substitution is
used to attach an initial tree, and adjunction is used
to attach an auxiliary tree. In addition to adjunction,
we also use sister adjunction as defined in the LTAG
statistical parser described in (Chiang, 2000).1 The
tree resulting from the combination of elementary
trees is is called a derived tree. The tree that records
the history of how a derived tree is built from the
elementary trees is called a derivation tree.2
We illustrate the LTAG formalism using an exam-
ple.
Example 1: Pierre Vinken will join the board as a
non-executive director.
The derived tree for Example 1 is shown in Fig. 1
(we omit the POS tags associated with each word to
save space), and Fig. 2 shows the elementary trees
for each word in the sentence. Fig. 3 is the deriva-
tion tree (the history of tree combinations). One of
1Adjunction is used in the case where both the root node and
the foot node appear in the Treebank tree. Sister adjunction is
used in generating modifier sub-trees as sisters to the head, e.g
in basal NPs.
2Each node ??n? in the derivation tree is an elementary tree
name ? along with the location n in the parent elementary tree
where ? is inserted. The location n is the Gorn tree address (see
Fig. 4).
S
NP
Pierre Vinken
VP
will VP
VP
join NP
the board
PP
as NP
a non-executive director
Figure 1: Derived tree (parse tree) for Example 1.
NP
Pierre
NP
Vinken
VP
will VP?
S
NP? VP
join NP?
NP
the
NP
board
VP
VP? PP
as NP?
NP
a
NP
non-executive
NP
director
?1: ?2: ?2: ?1:
?3: ?3: ?4: ?6: ?5: ?4:
Figure 2: Elementary trees for Example 1.
the properties of LTAG is that it factors recursion in
clause structure from the statement of linguistic con-
straints, thus making these constraints strictly local.
For example, in the derivation tree of Examples 1,
?1(join) and ?2(V inken) are directly connected
whether there is an auxiliary tree ?2(will) or not.
We will show how this property affects our redefined
tree kernel later in this paper. In our experiments in
this paper, we only use LTAG grammars where each
elementary tree is lexicalized by exactly one word
(terminal symbol) on the frontier.
3 Parse Reranking
In recent years, reranking techniques have been suc-
cessfully used in statistical parsers to rerank the out-
put of history-based models (Black et al, 1993). In
this paper, we will use the LTAG based features to
improve the performance of reranking. Our motiva-
tions for using LTAG based features for reranking
are the following:
? Unlike the generative model, it is trivial to in-
corporate features of various kinds in a rerank-
ing setting. Furthermore the nature of rerank-
ing makes it possible to use global features,
?1(join)??
?2(Vinken)?00?
?1(Pierre)?0?
?2(will)?01? ?3(board)?011?
?3(the)?0?
?4(as)?01?
?4(director)?011?
?5(non-executive)?0?
?6(a)?0?
Figure 3: Derivation tree: shows how the elementary
trees shown in Fig. 2 can be combined to provide an
analysis for the sentence in Example 1.
which allow us to combine features that are de-
fined on arbitrary sub-trees in the parse tree and
features defined on a derivation tree.
? Several hand-crafted and arbitrary features
have been exploited in the statistical parsing
task, especially when parsing the WSJ Penn
Treebank dataset where performance has been
finely tuned over the years. Showing a positive
contribution in this task will be a convincing
test for the use of LTAG based features.
? The parse reranking dataset is well established.
We use the dataset defined in (Collins, 2000).
In (Collins, 2000), two reranking algorithms were
proposed. One was based on Markov Random
Fields, and the other was based on the Boosting al-
gorithm. In both these models, the loss functions
were computed directly on the feature space. Fur-
thermore, a rich feature set was introduced that was
specifically selected by hand to target the limitations
of generative models in statistical parsing.
In (Collins and Duffy, 2002), the Voted Percep-
tron algorithm was used for parse reranking. The
S0
NP00? VP01
join010 NP011?
Figure 4: Example of how each node in an elemen-
tary tree has a unique node address using the Gorn
notation. 0 is the root with daughters 00, 01, and so
on recursively, e.g. first daughter 01 is 010.
VP
will VP
.
.
. PP
.
.
. NP
a
Figure 5: A sub-tree which is linguistically mean-
ingless.
tree kernel was used to compute the number of com-
mon sub-trees of two parse trees. The features used
by this tree kernel contains all the hand selected fea-
tures of (Collins, 2000). It is worth mentioning that
the f -scores reported in (Collins and Duffy, 2002)
are about 1% less than the results in (Collins, 2000).
In (Shen and Joshi, 2003), a SVM based rerank-
ing algorithm was proposed. In that paper, the no-
tion of preference kernels was introduced to solve
the reranking problem. Two distinct kernels, the tree
kernel and the linear kernel were used with prefer-
ence kernels.
4 Using LTAG Based Features
4.1 Motivation
While the tree kernel is an easy way to compute sim-
ilarity between two parse trees, it takes too many lin-
guistically meaningless sub-trees into consideration.
Let us consider the example sentence in Example
1. The parse tree, or derived tree, for this sentence
is shown in Fig. 1. Fig. 5 shows one of the lin-
guistically meaningless sub-trees. The number of
meaningless sub-trees is a misleading measure for
discriminating good parse trees from bad. Further-
more, the number of meaningless sub-trees is far
greater than the number of useful sub-trees. This
limits both efficiency and accuracy on the test data.
The use of unwanted sub-trees greatly increases the
hypothesis space of a learning machine, and thus de-
creases the expected accuracy on test data. In this
work, we consider the hypothesis that linguistically
meaningful sub-trees reveal correlations of interest
and therefore are useful in stochastic models.
We notice that each sub-tree of a derivation tree
is linguistically meaningful because it represents a
valid sub-derivation. We claim that derivation trees
provide a more accurate measure of similarity be-
tween two parses. This is one of the motivations
for applying tree kernels to derivation trees. Note
that the use of features on derivation trees is differ-
ent from the use of features on dependency graphs,
derivation trees include many complex patterns of
tree names and attachment sites and can represent
word to word dependencies that are not possible in
traditional dependency graphs.
For example, the derivation tree for Example 1
with and without optional modifiers such as ?4(as)
are minimally different. In contrast, in derived
(parse) trees, there is an extra VP node which
changes quite drastically the set of sub-trees with
and without the PP modifier. In addition, using only
sub-trees from the derived tree, we cannot repre-
sent a common sub-tree that contains only the words
Vinken and join since this would lead to a discontin-
uous sub-tree. However, LTAG based features can
represent such cases trivially.
The comparison between (Collins, 2000) and
(Collins and Duffy, 2002) in ?3 shows that it is hard
to add new features to improve performance. Our
hypothesis is that the LTAG based features provide
a novel set of abstract features that complement the
hand selected features from (Collins, 2000) and the
LTAG based features will help improve performance
in parse reranking.
4.2 Extracting Derivation Trees
Before we can use LTAG based features we need
to obtain an LTAG derivation tree for each parse
tree under consideration by the reranker. Our solu-
tion is to extract elementary trees and the derivation
tree simultaneously from the parse trees produced
by an n-best statistical parser. Our training and
test data consists of n-best output from the Collins
parser (see (Collins, 2000) for details on the dataset).
Since the Collins parser uses a lexicalized context-
free grammar as a basis for its statistical model, we
obtain parse trees that are of the type shown in Fig.
6. From this tree we extract elementary trees and
derivation trees by recursively traversing the spine
of the parse tree. The spine is the path from a non-
terminal lexicalized by a word to the terminal sym-
bol on the frontier equal to that word. Every sub-tree
rooted at a non-terminal lexicalized by a different
word is excised from the parse tree and recorded into
S(join)
NP-A(Vinken)
Pierre Vinken
VP(join)
will VP(join)
VP(join)
join NP-A(board)
the board
PP(as)
as NP-A(director)
a non-executive director
Figure 6: Sample output parse from the Collins
parser. Each non-terminal is lexicalized by the pars-
ing model. -A marks arguments recovered by the
parser.
the derivation tree as a substitution. Repeated non-
terminals on the spine (e.g. VP(join) . . . VP(join) in
Fig. 6) are excised along with the sub-trees hang-
ing off of it and recorded into the derivation tree as
an adjunction. The only other case is those sub-
trees rooted at non-terminals that are attached to
the spine. These sub-trees are excised and recorded
into the derivation tree as cases of sister adjunction.
Each sub-tree excised is recursively analyzed with
this method, split up into elementary trees and then
recorded into the derivation tree. The output of our
algorithm for the input parse tree in Fig. 6 is shown
in Fig. 2 and Fig. 3. Our algorithm is similar to
the derivation tree extraction explained in (Chiang,
2000), except we extract our LTAG from n-best sets
of parse trees, while in (Chiang, 2000) the LTAG is
extracted from the Penn Treebank.3 For other tech-
niques for LTAG grammar extraction see (Xia, 2001;
Chen and Vijay-Shanker, 2000).
4.3 Using Derivation Trees
In this paper, we have described two models to em-
ploy derivation trees. Model 1 uses tree kernels on
derivation trees. In order to make the tree kernel
more lexicalized, we extend the original definition
of the tree kernel, which we will describe below.
Model 2 abstracts features from derivation trees and
uses them with a linear kernel.
In Model 1, we combine the SVM results of the
tree kernel on derivation trees with the SVM results
given by a linear kernel based on features on the de-
rived trees.
3Also note that the path from the root node to the foot node
in auxiliary trees can be greater than one (for trees with S roots).
In Model 2, the vector space of the linear kernel
consists of both LTAG based features defined on the
derived trees and features defined on the derivation
tree. The following LTAG features have been used
in Model 2.
? Elementary tree. Each node in the derivation
tree is used as a feature.
? Bigram of parent and its child. Each pair
of parent elementary tree and child elementary
tree, as well as the type of operation (substi-
tution, adjunction or sister adjunction) and the
Gorn address on parent (see Fig. 4) is used as a
feature.
? Lexicalized elementary tree. Each elemen-
tary tree associated with its lexical item is used
as a feature.
? Lexicalized bigram. In Bigram of parent and
its child, each elementary tree is lexicalized
(we use closed class words, e.g. adj, adv, prep,
etc. but not noun or verb).
4.4 Lexicalized Tree Kernel
In (Collins and Duffy, 2001), the notion of a tree ker-
nel is introduced to compute the number of common
sub-trees of two parse trees. For two parse trees, p1
and p2, the tree kernel Tree(p1, p2) is defined as:
Tree(p1, p2) =
?
n1 in p1
n2 in p2
T (n1, n2) (1)
The recursive function T is defined as follows: If n1
and n2 have the same bracketing tag (e.g. S, NP, VP,
. . .) and the same number of children,
T (n1, n2) = ?
?
i
(1 + T (n1i, n2i)), (2)
where, nki is the ith child of the node nk, ? is a
weight coefficient used to control the importance of
large sub-trees and 0 < ? ? 1.
If n1 and n2 have the same bracketing tag but dif-
ferent number of children, T (n1, n2) = ?. If they
don?t have the same bracketing tag, T (n1, n2) = 0.
In (Collins and Duffy, 2002), lexical items are all
located at the leaf nodes of parse trees. Therefore
VP(join)
VP(join)
V(join) NP(board)
PP(as)
P(as) NP(director)
tree with root node n:
VP
VP
V NP
PP
P NP
ptn(n):
lex(n): (join, join, as)
Figure 7: A lexicalized sub-tree rooted at n and
its decomposition into a pattern, ptn(n) and corre-
sponding vector of lexical information, lex(n).
sub-trees that do not contain any leaf node are not
lexicalized. Furthermore, due to the introduction of
parameter ?, lexical information is almost ignored
for sub-trees whose root node is not close to the leaf
nodes, i.e. sub-trees rooted at S node.
In order to make the tree kernel more lexicalized,
we associate each node with a lexical item. For ex-
ample, Fig. 7 shows a lexicalized sub-tree and its
decomposition into features. As shown in Fig. 7 the
lexical information lex(t) extracted from the lexical-
ized tree consists of words from the root and its im-
mediate children. This is because we wish to ig-
nore irrelevant lexicalizations such as NP(board) in
Fig. 7.
A lexicalized sub-tree rooted on node n is split
into two parts. One is the pattern tree of n, ptn(n).
The other is the vector of lexical information of n,
lex(n), which contains the lexical items of the root
node and the children of the root.
For two tree nodes n1 and n2, the recursive func-
tion LT (n1, n2) used to compute the lexicalized tree
kernel is defined as follows.
LT (n1, n2) = (1 + Cnt(lex(n1), lex(n2)))
? T ?(ptn(n1), ptn(n2)), (3)
where T ? is the same as the original recursive func-
tion T defined in (2), except that T is defined on
parse tree nodes, while T ? is defined on patterns of
parse tree nodes. Cnt(x, y) counts the number of
common elements in vector x and y. For example,
Cnt((join, join, as), (join, join, in)) = 2, since
2 elements of the two vectors are the same.
It can be shown that the lexicalized tree kernel
counts the number of common sub-trees that meet
the following constraints.
? None or one node in the sub-tree is lexicalized
? The lexicalized node is the root node or a child
of the root, if applicable.
Therefore our new tree kernel is more lexicalized.
On the other hand, it immediately follows that the
lexicalized tree kernel is well-defined. It means that
we can embed the lexicalized tree kernel into a high
dimensional space. The proof is similar to the proof
for the tree kernel in (Collins and Duffy, 2001).
Another important advantage of the lexicalized
tree kernel is that it is more compressible. It is noted
in (Collins and Duffy, 2001) that training trees can
be combined by sharing sub-trees to speed up the
test. As far as the lexicalized tree kernel is con-
cerned, the pattern trees are more compressible be-
cause there is no lexical item at the leaf nodes of
pattern trees. Lexical information can be attached
to the nodes of the result pattern forest. In our ex-
periment, we select five parses from each sentence
in Collins? training data and represent these parses
with shared structure. The number of the nodes in
the pattern forest is only 1/7 of the total number of
the nodes the selected parse trees.
4.5 Tree Kernel for Derivation Trees
In order to apply the (lexicalized) tree kernel to
derivation trees, we need to make some modifica-
tions to the original recursive definition of the tree
kernel.
For derivation trees, the recursive function is trig-
gered if the two root nodes have the same non-
lexicalized elementary tree (sometimes called su-
pertag). Note that these two nodes will have the
same number of children which are initial trees (aux-
iliary trees are not counted). In comparison, the re-
cursive function in (2), T (n1, n2) is computed if and
only if n1 and n2 have the same bracketing tag and
they have the same number of children.
For each node, its children are attached with one
of the two distinct operations, substitution or adjunc-
tion. For substituted children, the computation of the
tree kernel is almost the same as that for CFG parse
tree. However, there is a problem with the adjoined
children. Let us first have a look at a sentence in
Penn Treebank.
Example 2: COMMERCIAL PAPER placed di-
rectly by General Motors Acceptance Corp.: 8.55%
30 to 44 days; 8.25% 45 to 59 days; 8.45% 60 to 89
days; 8% 90 to 119 days; 7.90% 120 to 149 days;
7.80% 150 to 179 days; 7.55% 180 to 270 days.
In this example, seven sub-trees of the same type
are sister adjoined to the same place of an initial tree.
So the number of common sub-trees increases dra-
matically if the tree kernel is applied on two similar
parses of this sentence. Experimental evidence indi-
cates that this is harmful to accuracy. Therefore, for
derivation trees, we are only interested in sub-trees
that contain at most 2 adjunction branches for each
node. The number of constrained common sub-trees
for the derivation tree kernel can be computed by
the recursive function DT over derivation tree nodes
n1, n2:
DT (n1, n2) = (1 + A1(n1, n2) + A2(n1, n2))
? T?(sub(n1), sub(n2)) (4)
where sub(nk) is the sub-tree of nk in which chil-
dren adjoined to the root of nk are pruned. T? is
similar to the original recursive function T defined
in (2), but it is defined on derivation tree nodes re-
cursively. A1 and A2 are used to count the number
of common sub-trees whose root nodes only contain
one or two adjunction children respectively.
A1(n1, n2) =
?
i,j
DT (a1i, a2j),
where, a1i is the ith adjunct of n1, and a2j is the jth
adjunct of n2. Similarly, we have:
A2(n1, n2) =
?
i<k,j<l
DT (a1i, a2j) ? DT (a1k, a2l)
The tree kernel for derivation trees is a well-defined
kernel function because we can easily define an em-
bedding space according to the definition of the new
tree kernel. By substituting DT for T ? in (3), we ob-
tain the lexicalized tree kernel for LTAG derivation
trees (using LT in (1)).
5 Experiments
As described above, we use the SVM based voting
algorithm (Shen and Joshi, 2003) in our reranking
experiments. We use preference kernels and pair-
wise parse trees in our reranking models.
We use the same data set as described in (Collins,
2000). Section 2-21 of the Penn WSJ Treebank are
used as training data, and section 23 is used for fi-
nal test. The training data contains around 40,000
sentences, each of which has 27 distinct parses on
average. Of the 40,000 training sentences, the first
36,000 are used to train SVMs. The remaining 4,000
sentences are used as development data.
Due to the computational complexity of SVM, we
have to divide training data into slices to speed up
training. Each slice contain two pairs of parses from
every sentence. Specifically, slice i contains pos-
itive samples ((p?k, pki),+1) and negative samples
((pki, p?k),?1), where p?k is the best parse for sen-
tence k, pki is the parse with the ith highest log-
likelihood in all the parses for sentence k and it is
not the best parse (Shen and Joshi, 2003). There are
about 60000 samples in each slice in average.
For the tree kernel SVMs of Model 1, we take
3 slices as a chunk, and train an SVM for each
chunk. Due to the limitation of computing resource,
we have only trained on 3 chunks. The results of
tree kernel SVMs are combined with simple com-
bination. Then the outcome is combined with the
result of the linear kernel SVMs trained on features
extracted from the derived trees which are reported
in (Shen and Joshi, 2003). For each parse, the num-
ber of the brackets in it and the log-likelihood given
by Collins? parser Model 2 are also used in the com-
putation of the score of a parse. For each parse p, its
score Sco(p) is defined as follows:
Sco(p) = MT (p) + ? ? ML(p) + ? ? l(p) + ? ? b(p),
where MT (p) is the output of the tree kernel SVMs,
ML(p) is the output of linear kernel SVMs, l(p) is
the log-likelihood of parse p, and b(p) is the num-
ber of brackets in parse p. We noticed that the SVM
systems prefers to give higher scores to the parses
with less brackets. As a result, the system has a high
precision but a low recall. Therefore, we take the
number of brackets, b(p), as a feature to make the
recall and precision balanced. The three weight pa-
rameters are tuned on the development data.
The results are shown in Table 1. With Model
1, we achieve LR/LP of 89.7%/90.0% on sentences
?40 Words (2245 sentences)
Model LR LP CBs 0 CBs 2 CBs
CO99 88.5% 88.7% 0.92 66.7% 87.1%
CO00 90.1% 90.4% 0.73 70.7% 89.6%
CD02 89.1% 89.4% 0.85 69.3% 88.2%
SJ 03 89.9% 90.3% 0.75 71.7% 89.4%
M1 90.2% 90.5% 0.72 72.3% 90.0%
M2 89.8% 90.3% 0.76 71.6% 89.6%
?100 Words (2416 sentences)
Model LR LP CBs 0 CBs 2 CBs
CO99 88.1% 88.3% 1.06 64.0% 85.1%
CO00 89.6% 89.9% 0.87 68.3% 87.7%
CD02 88.6% 88.9% 0.99 66.5% 86.3%
SJ 03 89.4% 89.8% 0.89 69.2% 87.6%
M1 89.7% 90.0% 0.86 70.0% 88.2%
M2 89.3% 89.8% 0.89 69.1% 87.7%
Table 1: Results on section 23 of the WSJ Tree-
bank. LR/LP = labeled recall/precision. CBs = av-
erage number of crossing brackets per sentence. 0
CBs, 2 CBs are the percentage of sentences with
0 or ? 2 crossing brackets respectively. CO99 =
(Collins, 1999) Model 2. CO00 = (Collins, 2000).
CD02 = (Collins and Duffy, 2002). SJ03 = linear
kernel of (Shen and Joshi, 2003). M1=Model 1.
M2=Model 2.
with ? 100 words. Our results show a 17% rel-
ative difference in f -score improvement over the
use of a linear kernel without LTAG based features
(Shen and Joshi, 2003). In addition, we also get
non-trivial improvement on the number of crossing
brackets. These results verify the benefit of using
LTAG based features and confirm the hypothesis that
LTAG based features provide a novel set of abstract
features that complement the hand selected features
from (Collins, 2000). Our results on Model 1 show
a 1% error reduction on the previous best reranking
result using the dataset reported in (Collins, 2000).
Also, Model 1 provides a 10% reduction in error
over (Collins and Duffy, 2002) where the features
from tree kernel were over arbitrary sub-trees.
For Model 2, we first train 22 SVMs on 22 dis-
tinct slices. Then we combine the results of individ-
ual SVMs with simple combination. However, the
overall performance does not improve. But we no-
tice that the use of LTAG based features gives rise to
0.874
0.875
0.876
0.877
0.878
0.879
0.88
0.881
0 5 10 15 20
ID of slices
without LTAG
with LTAG
Figure 8: Comparison of performance of individual
SVMs in Model 2: with and without LTAG based
features. X-axis stands for the ID of the slices on
which the SVMs are trained.Y-axis stands for the f -
score.
improvement on most of the single SVMs, as shown
in Fig. 8.
We think there are several reasons to account for
why our Model 2 doesn?t work as well for the full
task when compared with Model 1. Firstly, the train-
ing slice is not large enough. Local optimization on
each slice does not result in global optimization (as
seen in Fig. 8). Secondly, the LTAG based features
that we have used in the linear kernel in Model 2 are
not as useful as the tree kernel in Model 1.4 The last
reason is that we do not set the importance of LTAG
based features. One shortcoming of kernel methods
is that the coefficient of each feature must be set be-
fore the training (Herbrich, 2002). In our case, we
do not tune the coefficients for the LTAG based fea-
tures in Model 2.
6 Conclusions and Future Work
In this paper, we have proposed methods for using
LTAG based features in the parse reranking task.
The experimental results show that the use of LTAG
based features gives rise to improvement over al-
ready finely tuned results. We used LTAG based fea-
tures for the parse reranking task and obtain labeled
recall and precision of 89.7%/90.0% on WSJ sec-
tion 23 of Penn Treebank for sentences of length ?
100 words. Our results show that the use of LTAG
4In Model 1, we implicitly take every sub-tree of the deriva-
tion trees as a feature, but in Model 2, we only consider a small
set of sub-trees in a linear kernel.
based tree kernel gives rise to a 17% relative differ-
ence in f -score improvement over the use of a linear
kernel without LTAG based features. In future work,
we will use some light-weight machine learning al-
gorithms for which training is faster, such as vari-
ants of the Perceptron algorithm. This will allow us
to use larger training data chunks and take advan-
tage of global optimization in the search for relevant
features.
References
E. Black, F. Jelinek, J. Lafferty, Magerman D. M., R. Mercer,
and S. Roukos. 1993. Towards history-based grammars:
Using richer models for probabilistic parsing. In Proc. of the
ACL 1993.
R. Bod. 2003. An Efficient Implementation of a New DOP
Model. In Proc. of EACL 2003, Budapest.
J. Chen and K. Vijay-Shanker. 2000. Automated Extraction of
TAGs from the Penn Treebank. In Proc. of the 6th IWPT.
D. Chiang. 2000. Statistical Parsing with an Automatically-
Extracted Tree Adjoining Grammar. In Proc. of ACL-2000.
M. Collins and N. Duffy. 2001. Convolution kernels for natural
language. In Proc. of the 14th NIPS.
M. Collins and N. Duffy. 2002. New ranking algorithms for
parsing and tagging: Kernels over discrete structures, and
the voted perceptron. In Proc. of ACL 2002.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Collins. 2000. Discriminative reranking for natural lan-
guage parsing. In Proc. of 7th ICML.
M. Collins. 2001. Parameter estimation for statistical parsing
models: Theory and practice of distribution-free methods.
In Proc. of IWPT 2001. Invited Talk at IWPT 2001.
R. Herbrich. 2002. Learning Kernel Classifiers: Theory and
Algorithms. MIT Press.
A. K. Joshi and Y. Schabes. 1997. Tree-adjoining grammars.
In G. Rozenberg and A. Salomaa, editors, Handbook of For-
mal Languages, volume 3, pages 69 ? 124. Springer.
L. Shen and A. K. Joshi. 2003. An SVM based voting algo-
rithm with application to parse reranking. In Proc. of CoNLL
2003.
V. N. Vapnik. 1999. The Nature of Statistical Learning Theory.
Springer, 2nd edition.
F. Xia. 2001. Investigating the Relationship between Gram-
mars and Treebanks for Natural Languages. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 126?129,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Voting between Dictionary-based and Subword Tagging Models for
Chinese Word Segmentation
Dong Song and Anoop Sarkar
School of Computing Science, Simon Fraser University
Burnaby, BC, Canada V5A1S6
{dsong,anoop}@cs.sfu.ca
Abstract
This paper describes a Chinese word seg-
mentation system that is based on ma-
jority voting among three models: a for-
ward maximum matching model, a con-
ditional random field (CRF) model us-
ing maximum subword-based tagging, and
a CRF model using minimum subword-
based tagging. In addition, it contains a
post-processing component to deal with
inconsistencies. Testing on the closed
track of CityU, MSRA and UPUC corpora
in the third SIGHAN Chinese Word Seg-
mentation Bakeoff, the system achieves a
F-score of 0.961, 0.953 and 0.919, respec-
tively.
1 Introduction
Tokenizing input text into words is the first step of
any text analysis task. In Chinese, a sentence is
written as a string of characters, to which we shall
refer by their traditional name of hanzi, without
separations between words. As a result, before any
text analysis on Chinese, word segmentation task
has to be completed so that each word is ?isolated?
by the word-boundary information.
Participating in the third SIGHAN Chinese
Word Segmentation Bakeoff in 2006, our system
is tested on the closed track of CityU, MSRA and
UPUC corpora. The sections below provide a de-
tailed description of the system and our experi-
mental results.
2 System Description
In our segmentation system, a hybrid strategy
is applied (Figure 1): First, forward maximum
matching (Chen and Liu, 1992), which is a
dictionary-based method, is used to generate a
segmentation result. Also, the CRF model us-
ing maximum subword-based tagging (Zhang et
al., 2006) and the CRF model using minimum
subword-based tagging, both of which are statis-
tical methods, are used individually to solve the
problem. In the next step, the solutions from
these three methods are combined via the hanzi-
level majority voting algorithm. Then, a post-
processing procedure is applied in order to to get
the final output. This procedure merges adjoin-
ing words to match the dictionary entries and then
splits words which are inconsistent with entries in
the training corpus.
Forward
Post?processing
Majority Voting
Result
Input Sentence
Tagging
Subword?based
Minimum
CRF with
Tagging
Subword?based
Maximum
CRF with
Matching
Maximum
Figure 1: Outline of the segmentation process
2.1 Forward Maximum Matching
The maximum matching algorithm is a greedy
segmentation approach. It proceeds through the
sentence, mapping the longest word at each point
with an entry in the dictionary. In our system,
the well-known forward maximum matching algo-
rithm (Chen and Liu, 1992) is implemented.
The maximum matching approach is simple and
efficient, and it results in high in-vocabulary ac-
curacy; However, the small size of the dictionary,
which is obtained only from the training data, is
a major bottleneck for this approach to be applied
by itself.
126
2.2 CRF Model with Maximum
Subword-based Tagging
Conditional random fields (CRF), a statistical se-
quence modeling approach (Lafferty et al, 2001),
has been widely applied in various sequence
learning tasks including Chinese word segmen-
tation. In this approach, most existing methods
use the character-based IOB tagging. For ex-
ample, ??(all) ??(extremely important)?
is labeled as ??(all)/O ?(until)/B (close)/I
?(heavy)/I(demand)/I?.
Recently (Zhang et al, 2006) proposed a maxi-
mum subword-based IOB tagger for Chinese word
segmentation, and our system applies their ap-
proach which obtains a very high accuracy on the
shared task data from previous SIGHAN compe-
titions. In this method, all single-hanzi words and
the top frequently occurring multi-hanzi words are
extracted from the training corpus to form the lexi-
con subset. Then, each word in the training corpus
is segmented for IOB tagging, with the forward
maximum matching algorithm, using the formed
lexicon subset as the dictionary. In the above
example, the tagging labels become ??(all)/O
?(until)/B (close)/I ?(important)/I?, as-
suming that ??(important)? is the longest sub-
word in this word, and it is one of the top fre-
quently occurring words in the training corpus.
After tagging the training corpus, we use the
package CRF++1 to train the CRF model. Sup-
pose w0 represents the current word, w?1 is the
first word to the left, w?2 is the second word to
the left, w1 is the first word to the right, and w2
is the second word to the right, then in our experi-
ments, the types of unigram features used include
w0, w?1, w1, w?2, w2, w0w?1, w0w1, w?1w1,
w?2w?1, and w2w0. In addition, only combina-
tions of previous observation and current observa-
tion are exploited as bigram features.
2.3 CRF Model with Minimum
Subword-based Tagging
In our third model, we applies a similar approach
as in the previous section. However, instead of
finding the maximum subwords, we explore the
minimum subwords. At the beginning, we build
the dictionary using the whole training corpus.
Then, for each word in the training data, a forward
shortest matching is used to get the sequence of
minimum-length subwords, and this sequence is
1available from http://www/chasen.org/?taku/software
tagged in the same IOB format as before. Suppose
?a?, ?ac?, ?de? and ?acde? are the only entries in
the dictionary. Then, for the word ?acde?, the se-
quence of subwords is ?a?, ?c? and ?de?, and the
tags assigned to ?acde? are ?a/B c/I de/I?.
After tagging the training data set, CRF++
package is executed again to train this type of
model, using the identical unigram and bigram
feature sets that are used in the previous model.
Meanwhile, the unsegmented test data is seg-
mented by the forward shortest matching algo-
rithm. After this initial segmentation process, the
result is fed into the trained CRF model for re-
segmentation by assigning IOB tags.
2.4 Majority Voting
Having the segmentation results from the above
three models in hand, in this next step, we adopt
the hanzi-level majority voting algorithm. First,
for each hanzi in a segmented sentence, we tag it
either as ?B? if it is the first hanzi of a word or
a single-hanzi word, or as ?I? otherwise. Then,
for a given hanzi in the results from those three
models, if at least two of the models provide the
identical tag, it will be assigned that tag. For in-
stance, suppose ?a c de? is the segmentation result
via forward maximum matching, and it is also the
result from CRF model with maximum subword-
based tagging, and ?ac d e? is the result from the
third model. Then, for ?a?, since all of them as-
sign ?B? to it, ?a? is given the ?B? tag; for ?c?,
because two of segmentations tag it as ?B?, ?c? is
given the ?B? tag as well. Similarly, the tag for
each remaining hanzi is determined by this major-
ity voting process, and we get ?a c de? as the result
for this example.
To test the performance of each of the three
models and that of the majority voting, we di-
vide the MSRA corpus into training set and held-
out set. Throughout all the experiments we con-
ducted, we discover that those two CRF models
perform much better than the pure hanzi-based
CRF method, and that the voting process improves
the performance further.
2.5 Post-processing
While analyzing errors with the segmentation re-
sult from the held-out set, we find two incon-
sistency problems: First, the inconsistency be-
tween the dictionary and the result: that is, certain
words that appear in the dictionary are separated
into consecutive words in the test result; Second,
127
the inconsistency among words in the dictionary;
For instance, both ?)????(scientific research)
and ?)?(science)??(research)? appear in the
training corpus.
To deal with the first phenomena, for the seg-
mented result, we try to merge adjoining words to
match the dictionary entries. Suppose ?a b c de?
are the original voting result, and ?ab?, ?abc? and
?cd? form the dictionary. Then, we merge ?a?, ?b?
and ?c? together to get the longest match with the
dictionary. Therefore, the output is ?abc de?.
For the second problem, we introduce the split
procedure. In our system, we only consider two
consecutive words. First, all bigrams are extracted
from the training corpus, and their frequencies are
counted. After that, for example, if ?a b? appears
more often than ?ab?, then whenever in the test
result we encounter ?ab?, we split it into ?a b?.
The post-processing steps detailed above at-
tempt to maximize the value of known words in
the training data as well as attempting to deal with
the word segmentation inconsistencies in the train-
ing data.
3 Experiments and Analysis
The third International Chinese Language
Processing Bakeoff includes four different cor-
pora, Academia Sinica (CKIP), City University
of Hong Kong (CityU), Microsoft Research
(MSRA), and University of Pennsylvania and
University of Colorado, Boulder (UPUC), for the
word segmentation task.
In this bakeoff, we test our system in CityU,
MSRA and UPUC corpora, and follow the closed
track. That is, we only use training material from
the training data for the particular corpus we are
testing on. No other material or any type of ex-
ternal knowledge is used, including part-of-speech
information, externally generated word-frequency
counts, Arabic and Chinese numbers, feature char-
acters for place names and common Chinese sur-
names.
3.1 Results on SIGHAN Bakeoff 2006
To observe the result of majority voting and the
contribution of the post-processing step, the ex-
periment is ran for each corpus by first producing
the outcome of majority voting and then producing
the output from the post-processing. In each ex-
periment, the precision (P ), recall (R), F-measure
(F ), Out-of-Vocabulary rate (OOV ), OOV recall
rate (ROOV ), and In-Vocabulary rate (RIV ) are
recorded. Table 1,2,3 show the scores for the
CityU corpus, for the MSRA corpus, and for the
UPUC corpus, respectively.
Majority Voting Post-processing
P 0.956 0.958
R 0.962 0.963
F 0.959 0.961
OOV 0.04 0.04
ROOV 0.689 0.689
RIV 0.974 0.974
Table 1: Scores for CityU corpus
Majority Voting Post-processing
P 0.952 0.954
R 0.952 0.952
F 0.952 0.953
OOV 0.034 0.034
ROOV 0.604 0.604
RIV 0.964 0.964
Table 2: Scores for MSRA corpus
Majority Voting Post-processing
P 0.908 0.909
R 0.927 0.929
F 0.918 0.919
OOV 0.088 0.088
ROOV 0.628 0.628
RIV 0.956 0.958
Table 3: Scores for UPUC corpus
From those tables, we can see that a simple ma-
jority voting algorithm produces accuracy that is
higher than each individual system and reason-
ably high F-scores overall. In addition, the post-
processing step indeed helps to improve the per-
formance.
3.2 Error analysis
The errors that occur in our system are mainly due
to the following three factors:
First, there is inconsistency between the gold
segmentation and the training corpus. Although
the inconsistency problem within the training cor-
pus is intended to be tackled in the post-processing
step, we cannot conclude that the segmentation
128
for certain words in the gold test set alays fol-
lows the convention in the training data set. For
example, in the MSRA training corpus, ??)
u?(Chinese government) is usually considered
as a single word; while in the gold test set, it is
separated as two words ??)?(Chinese) and ?u
?(government). This inconsistency issue lowers
the system performance. This problem, of course,
affects all competing systems.
Second, we don?t have specific steps to deal
with words with postfixes such as ?V?(person).
Compared to our system, (Zhang, 2005) proposed
a segmentation system that contains morpholog-
ically derived word recognition post-processing
component to solve this problem. Lacking of such
a step prevents us from identifying certain types
of words such as ???V?(worker) to be a single
word.
In addition, the unknown words are still trou-
blesome because of the limited size of the training
corpora. In the class of unknown words, we en-
counter person names, numbers, dates, organiza-
tion names and words translated from languages
other than Chinese. For example, in the produced
CityU test result, the translated person name ??
-b???(Mihajlovic) is incorrectly separated
as ??-b? and ????. Moreover, in cer-
tain cases, person names can also create ambigu-
ity. Take the name ?B?0?(Qiu, Beifang) in
UPUC test set for example, without understand-
ing the meaning of the whole sentence, it is dif-
ficult even for human to determine whether it is
a person name or it represents ?B?(autumn), ??
0?(north), with the meaning of ?the autumn in the
north?.
4 Alternative to Majority Voting
In designing the voting procedure, we also attempt
to develop and use a segmentation lattice, which
proceeds using a similar underlying principle as
the one applied in (Xu et al, 2005).
In our approach, for an input sentence, the seg-
mentation result using each of our three models is
transformed into an individual lattice. Also, each
edge in the lattice is assigned a particular weight,
according to certain features such as whether or
not the output word from that edge is in the dictio-
nary. After building the three lattices, one for each
model, we merge them together. Then, the shortest
path, referring to the path that has the minimum
weight, is extracted from the merged lattice, and
therefore, the segmentation result is determined by
this shortest path.
However, in the time we had to run our experi-
ments on the test data, we were unable to optimize
the edge weights to obtain high accuracy on some
held-out set from the training corpora. So instead,
we tried a simple method for finding edge weights
by uniformly distributing the weight for each fea-
ture; Nevertheless, by testing on the shared task
data from the 2005 SIGHAN bakeoff, the perfor-
mance is not competitive, compared to our simple
majority voting method described above. As a re-
sult, we decide to abandon this approach for this
year?s SIGHAN bakeoff.
5 Conclusion
Our Chinese word segmentation system is based
on majority voting among the initial outputs from
forward maximum matching, from a CRF model
with maximum subword-based tagging, and from
a CRF model with minimum subword-based tag-
ging. In addition, we experimented with various
steps in post-processing which effectively boosted
the overall performance.
In future research, we shall explore more so-
phisticated ways of voting, including the contin-
uing investigation on the segmentation lattice ap-
proach. Also, more powerful methods on how
to accurately deal with unknown words, including
person and place names, without external knowl-
edge, will be studied as well.
References
Keh-jiann Chen, and Shing-Huan Liu. 1992. Word Identi-
fication for Mandarin Chinese Sentences. In Fifth Inter-
national Conference on Computational Linguistics, pages
101?107.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Models
for Segmenting and Labeling Sequence Data. In Proc. of
ICML-2001, pages 591?598.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney.
2005. Integrated Chinese Word Segmentation in Statisti-
cal Machine Translation. In Proc. of IWSLT-2005.
Huipeng Zhang, Ting Liu, Jinshan Ma, and Xiantao Liu.
2005. Chinese Word Segmentation with Multiple Post-
processors in HIT-IRLab. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Processing,
pages 172?175.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based Tagging by Conditional Random
Fields for Chinese Word Segmentation. In Proc. of HLT-
NAACL 2006.
129
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 127?132,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using LTAG-Based Features for Semantic Role Labeling
Yudong Liu and Anoop Sarkar
Computing Science Department
Simon Fraser University
British Columbia, Canada, V5A 1S6
yudongl,anoop@cs.sfu.ca
Abstract
Semantic role labeling (SRL) methods
typically use features from syntactic parse
trees. We propose a novel method that
uses Lexicalized Tree-Adjoining Gram-
mar (LTAG) based features for this task.
We convert parse trees into LTAG deriva-
tion trees where the semantic roles are
treated as hidden information learned by
supervised learning on annotated data de-
rived from PropBank. We extracted var-
ious features from the LTAG derivation
trees and trained a discriminative decision
list model to predict semantic roles. We
present our results on the full CoNLL 2005
SRL task.
1 Introduction
Semantic role labeling (SRL) is a natural exten-
sion of the syntactic parsing task. In SRL, par-
ticular syntactic constituents in a parse tree for a
sentence are identified with semantic roles. The
labels assigned to various types of arguments and
adjuncts differ in different annotation schemes.
In this paper, we use the PropBank corpus of
predicate-argument structures (Palmer, Gildea and
Kingsbury, 2005). We assume we are given a syn-
tactic parse tree and a particular predicate in the
sentence for which we then identify the arguments
and adjuncts and their labels. In this paper we
compare two models for the identification of se-
mantic role labels in a parse tree: A model that
uses a path in the parse tree (or the derived tree in
TAG terminology) and various associated features
related to this, and we compare this model with a
model that converts the syntactic parse tree into
a Lexicalized Tree-Adjoining Grammar (LTAG)
derivation tree and uses features extracted from the
elementary trees and the LTAG derivation tree.
In each model the features of that model are
used in a discriminative model for semantic role
labeling. The model is a simple decision list
learner that uses tree patterns extracted from the
LTAG derivation trees in order to classify con-
stituents into their semantic roles. We present re-
sults on the full CoNLL 2005 SRL task (Carreras
and Ma`rquez, 2005) a dataset built by combining
the Treebank and parser data with the PropBank
annotations.
2 Background about SRL
A semantic role is defined to be the relationship
that a syntactic constituent has with the predicate.
For example, the following sentence, taken from
the PropBank corpus, shows the annotation of se-
mantic roles:
[A0 Late buying] [V gave] [A2 the Paris
Bourse] [A1 a parachute] [AM-TMP after its
free fall early in the day].
Here, the arguments for the predicate gave are
defined in the PropBank Frame Scheme (Palmer,
Gildea and Kingsbury, 2005) as:
V: verb A2: beneficiary
A0: giver AM-TMP: temporal
A1: thing given
Recognizing and labeling semantic argu-
ments is a key task for answering ?Who?,
?When?,?What?, ?Where?, ?Why?, etc. questions
in Information Extraction, Question Answering,
Summarization (Melli et al 2005), and, in general,
in all NLP tasks in which some kind of semantic
interpretation is needed.
Most previous research treats the semantic role
labeling task as a classification problem, and di-
vides it into two phases: argument identification
and argument classification. Argument identifi-
cation involves classifying each syntactic element
in a sentence into either an argument or a non-
argument. Argument classification involves clas-
sifying each argument identified into a specific se-
mantic role. A variety of machine learning meth-
ods have been applied to this task. One of the most
important steps in building an accurate classifier is
feature selection. Different from the widely used
127
feature functions that are based on the syntactic
parse tree (Gildea and Jurafsky, 2002), we explore
the use of LTAG-based features in a simple dis-
criminative decision-list learner.
3 LTAG Based Feature Extraction
In this section, we introduce the main components
of our system. First, we do a pruning on the given
parse trees with certain constraints. Then we de-
compose the pruned parse trees into a set of LTAG
elementary trees. For each constituent in question,
we extract features from its corresponding deriva-
tion tree. We train using these features in a deci-
sion list model.
3.1 Pruning the Parse Trees
Given a parse tree, the pruning component identi-
fies the predicate in the tree and then only admits
those nodes that are sisters to the path from the
predicate to the root. It is commonly used in the
SRL community (cf. (Xue and Palmer, 2004)) and
our experiments show that 91% of the SRL targets
can be recovered despite this aggressive pruning.
There are two advantages to this pruning: the ma-
chine learning method used for prediction of SRLs
is not overwhelmed with a large number of non-
SRL nodes; and the process is far more efficient
as 80% of the target nodes in a full parse tree are
pruned away in this step. We make two enhance-
ments to the pruned Propbank tree: we enrich the
sister nodes with their head information, which is
a part-of-speech tag and word pair: ?t, w? and PP
nodes are expanded to include the NP complement
of the PP (including the head information). Note
that the target SRL node is still the PP. Figure 1
shows the pruned parse tree for a sentence from
PropBank section 24.
3.2 LTAG-based Decomposition
As next step, we decompose the pruned tree
around the predicate using standard head-
percolation based heuristic rules1 to convert a
Treebank tree into a LTAG derivation tree. We
do not use any sophistical adjunct/argument or
other extraction heuristics using empty elements
(as we don?t have access to them in the CoNLL
2005 data). Also, we do not use any substitution
nodes in our elementary trees: instead we exclu-
sively use adjunction or sister adjunction for the
attachment of sub-derivations. As a result the
1using http://www.isi.edu/?chiang/software/treep/treep.html
root node in an LTAG derivation tree is a spinal
elementary tree and the derivation tree provides
the path from the predicate to the constituent in
question. Figure 2 shows the resulting elementary
tree after decomposition of the pruned tree. For
each of the elementary trees we consider their
labeling in the derivation tree to be their semantic
role labels from the training data. Figure 3 is the
derivation tree for the entire pruned tree.
Note that the LTAG-based decomposition of the
parse tree allows us to use features that are distinct
from the usual parse tree path features used for
SRL. For example, the typical parse tree feature
from Figure 2 used to identify constituent (NP (NN
terminal)) as A0 would be the parse tree fragment:
NP ? NP ? SBAR ? S ? V P ? S ? V P ?
V BG cover (the arrows signify the path through
the parse tree). Using the LTAG-based decompo-
sition means that our SRL model can use any fea-
tures from the derivation tree such as in Figure 2,
including the elementary tree shapes.
3.3 Decision List Model for SRL
Before we train or test our model, we convert
the training, development and test data into LTAG
derivation trees as described in the previous sec-
tion. In our model we make an independence as-
sumption that each semantic role is assigned to
each constituent independently, conditional only
on the path from the predicate elementary tree
to the constituent elementary tree in the deriva-
tion tree. Different elementary tree siblings in the
LTAG derivation tree do not influence each other
in our current models. Figure 4 shows the differ-
ent derivation trees for the target constituent (NP
(NN terminal)): each providing a distinct semantic
role labeling for a particular constituent. We use
a decision list learner for identifying SRLs based
on LTAG-based features. In this model, LTAG el-
ementary trees are combined with some distance
information as features to do the semantic role la-
beling. The rationale for using a simple DL learner
is given in (Gildea and Jurafsky, 2002) where es-
sentially it based on their experience with the set-
ting of backoff weights for smoothing, it is stated
that the most specific single feature matching the
training data is enough to predict the SRL on test
data. For simplicity, we only consider one inter-
mediate elementary tree (if any) at one time in-
stead of multiple intermediate trees along the path
from the predicate to the argument.
128
SNP
PRP-H
He
VP-H
VBZ-H
backflips
PP
IN-H
into
NP
NP-H
NN-H
terminal
,
,
SBAR
WHNP-H
WDT-H
which
S
VP-H
VBZ-H
explodes
,
,
S
VP-H
VBG-H
covering
NP
NN-H
face
PP
IN-H
with
NP
NNS-H
microchips
Figure 1: The pruned tree for the sentence ?He backflips into a desktop computer terminal, which ex-
plodes, covering Huntz Hall ?s face with microchips.?
S
VP-H
VBG-H
cover
NP
NN-H
face
PP
IN-H
with
NP
NNS-H
microchips
S
VP-H
VBZ-H
explodes
,
,
SBAR
WHNP-H
WDT-H
which
predicate: A1: A2: NULL: NULL: R-A0:
NP
NP-H
NN-H
terminal
PP
IN-H
into
S
VP-H
VBZ-H
backflips
NP
PRP-H
He
A0: NULL: NULL: NULL:
Figure 2: The resulting elementary trees after decomposition of the pruned tree.
129
S(backflips)
NP(he) PP(into)
NP(terminal)
,(,) SBAR(which)
S(explodes)
,(,) S(cover)
NP(face) PP(with)
Figure 3: The LTAG derivation tree (with no semantic role labels) corresponding to the pruned tree.
A0: NP-NP(NN,terminal)
R-A0: SBAR-WHNP(WDT,which)
NULL: S-VP(VBZ,explodes)
predicate: S-VPH(VBG,cover)
A1: NP-NP(NN,terminal)
R-A0: SBAR-WHNP(WDT,which)
NULL: S-VP(VBZ,explodes)
predicate: S-VPH(VBG,cover)
A0: NP-NP(NN,terminal)
R-A0: SBAR-WHNP(WDT,which)
AM-ADV: S-VP(VBZ,explodes)
predicate: S-VPH(VBG,cover)
Figure 4: Different LTAG derivation trees corresponding to different assignments of semantic roles to
constituents. The constituent in question is (NP (NN terminal)).
NP
NP
NN
terminal
SBAR
S
VP
S
VP
VBG
cover
VP
VBG
cover
PP
IN
with
NP
NNS
microchips
SBAR
WHNP
WDT
which
S
VP
S
VP
VBG
cover
VP
VBZ
explodes
S
VP
VBG
cover
Figure 5: Tree patterns in tree pattern matching
130
The input to the learning algorithm is labeled
examples of the form (xi, yi). yi is the label (either
NULL for no SRL, or the SRL) of the ith example.
xi is a feature vector ?P,A,Dist, Position,R-
type, ti ? tI , Distti?, where P is the predicate
elementary tree, A is the tree for the constituent
being labeled with a SRL, tI is a set of interme-
diate elementary trees between the predicate tree
and the argument tree. Each P,A, I tree consists
of the elementary tree template plus the tag, word
pair: ?t, w?.
All possible combinations of fully-
lexicalized/postag/un-lexicalized elementary
trees are used for each example. Dist and Distti
denote the distance to the predicate from the
argument tree and the intermediate elementary
tree respectively. Position is interpreted as the
position that the target is relative to the predicate.
R-type denotes the relation type of the predicate
and the target constituent. 3 types are defined: if
the predicate dominates (directly or undirectly)
the argument in the derivation tree, we have the
relation of type-1; if the other way around, the
argument dominates (directly or undirectly) the
predicate then we have the relation of type-2; and
finally type-3 means that neither the predicate
or the argument dominate each other in the
derivation tree and instead are dominated (again,
directly or indirectly) by another elementary tree.
The output of the learning algorithm is a func-
tion h(x, y) which is an estimate of the conditional
probability p(y | x) of seeing SRL y given pat-
tern x. h is interpreted as a decision list of rules
x ? y ranked by the score h(x, y). In testing,
we simply pick the first rule that matches the par-
ticular test example x. We trained different mod-
els using the same learning algorithm. In addition
to the LTAG-based method, we also implemented
a pattern matching based method on the derived
(parse) tree using the same model. In this method,
instead of considering each intermediate elemen-
tary tree between the predicate and the argument,
we extract the whole path from the predicate to the
argument. So the input is more like a tree than a
discrete feature vector. Figure 5 shows the patterns
that are extracted from the same pruned tree.
4 Experiments and Results
We use the PropBank corpus of predicate-
argument structures (Palmer, Gildea and Kings-
bury, 2005) as our source of annotated data for the
dev = Sec24 p(%) r(%) f(%)
test = Sec23
M1: dev 78.42 77.03 77.72
M1: test 80.52 79.40 79.96
M2: dev 81.11 79.39 80.24
M2: test 83.47 81.82 82.64
M3: dev 80.98 79.56 80.26
M3: test 81.86 83.34 82.60
Table 1: Results on the CoNLL 2005 shared task
using gold standard parse trees. M1 is the LTAG-
based model, M2 is the derived tree pattern match-
ing Model, M3 is a hybrid model
SRL task. However, there are many different ways
to evaluate performance on the PropBank, leading
to incomparable results. To avoid such a situation,
in this paper we use the CoNLL 2005 shared SRL
task data (Carreras and Ma`rquez, 2005) which
provides a standard train/test split, a standard
method for training and testing on various prob-
lematic cases involving coordination. However, in
some cases, the CoNLL 2005 data is not ideal for
the use of LTAG-based features as some ?deep? in-
formation cannot be recovered due to the fact that
trace information and other empty categories like
PRO are removed entirely from the training data.
As a result some of the features that undo long-
distance movement via trace information in the
TreeBank as used in (Chen and Rambow, 2003)
cannot be exploited in our model. Our results are
shown in Table 1. Note that we test on the gold
standard parse trees because we want to compare
a model using features from the derived parse trees
to the model using the LTAG derivation trees.
5 Related Work
In the community of SRL researchers (cf. (Gildea
and Jurafsky, 2002; Punyakanok, Roth and Yih,
2005; Pradhan et al 2005; Toutanova et al,
2005)), the focus has been on two different aspects
of the SRL task: (a) finding appropriate features,
and (b) resolving the parsing accuracy problem by
combining multiple parsers/predictions. Systems
that use parse trees as a source of feature func-
tions for their models have typically outperformed
shallow parsing models on the SRL task. Typi-
cal features extracted from a parse tree is the path
from the predicate to the constituent and various
generalizations based on this path (such as phrase
type, position, etc.). Notably the voice (passive or
131
active) of the verb is often used and recovered us-
ing a heuristic rule. We also use the passive/active
voice by labeling this information into the parse
tree. However, in contrast with other work, in this
paper we do not focus on the problem of parse ac-
curacy: where the parser output may not contain
the constituent that is required for recovering all
SRLs.
There has been some previous work in SRL
that uses LTAG-based decomposition of the parse
tree and we compare our work to this more
closely. (Chen and Rambow, 2003) discuss a
model for SRL that uses LTAG-based decompo-
sition of parse trees (as is typically done for sta-
tistical LTAG parsing). Instead of using the typi-
cal parse tree features used in typical SRL models,
(Chen and Rambow, 2003) uses the path within
the elementary tree from the predicate to the con-
stituent argument. They only recover seman-
tic roles for those constituents that are localized
within a single elementary tree for the predicate,
ignoring cases that occur outside the elementary
tree. In contrast, we recover all SRLs regardless
of locality within the elementary tree. As a result,
if we do not compare the machine learning meth-
ods involved in the two approaches, but rather the
features used in learning, our features are a natural
generalization of (Chen and Rambow, 2003).
Our approach is also very akin to the approach
in (Shen and Joshi, 2005) which uses PropBank
information to recover an LTAG treebank as if it
were hidden data underlying the Penn Treebank.
This is similar to our approach of having several
possible LTAG derivations representing recovery
of SRLs. However, (Shen and Joshi, 2005) do
not focus on the SRL task, and in both of these
instances of previous work using LTAG for SRL,
we cannot directly compare our performance with
theirs due to differing assumptions about the task.
6 Conclusion and Future Work
In this paper, we proposed a novel model for
SRL using features extracted from LTAG deriva-
tion trees. A simple decision list learner is applied
to train on the tree patterns containing new fea-
tures. This simple learning method enables us to
quickly explore new features for this task. How-
ever, this work is still preliminary: a lot of addi-
tional work is required to be competitive with the
state-of-the-art SRL systems. In particular, we do
not deal with automatically parsed data yet, which
leads to a drop in our performance. We also do not
incorporate various other features commonly used
for SRL, as our goal in this paper was to make a
direct comparison between simple pattern match-
ing features on the derived tree and compare them
to features from LTAG derivation trees.
References
X. Carreras and L. Ma`rquez 2005. Introduction to
the CoNLL-2005 Shared Task. In Proc. of CoNLL
2005.
J. Chen and O. Rambow. 2003. Use of Deep Linguis-
tic Features for the Recognition and Labeling of Se-
mantic Arguments. In Proceedings of the 2003 Con-
ference on Empirical Methods in Natural Language
Processing, Sapporo, Japan, 2003.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
58(3):245?288
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Seman-
tic Roles. Computational Linguistics, 31(1).
G. Melli and Y. Wang and Y. Liu and M. Kashani and Z.
Shi and B. Gu and A. Sarkar and F. Popowich 2005.
Description of SQUASH, the SFU Question An-
swering Summary Handler for the DUC-2005 Sum-
marization Task. In Proceeding of Document Un-
derstanding Conference (DUC-2005)
S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and
D. Jurafsky. 2005. Semantic Role Chunking Com-
bining Complementary Syntactic Views, In Pro-
ceedings of the 9th Conference on Natural Language
Learning (CoNLL 2005), Ann Arbor, MI, 2005.
V. Punyakanok, D. Roth, and W Yih. 2005. Gener-
alized Inference with Multiple Semantic Role La-
beling Systems (shared task paper). Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL) pp. 181-184
Ruppenhofer, Josef, Collin F. Baker and Charles J. Fill-
more. 2002. The FrameNet Database and Soft-
ware Tools. In Braasch, Anna and Claus Povlsen
(eds.), Proceedings of the Tenth Euralex Interna-
tional Congress. Copenhagen, Denmark. Vol. I: 371-
375.
L. Shen and A. Joshi. 2005. Building an LTAG Tree-
bank. Technical Report MS-CIS-05-15, CIS Depart-
ment, University of Pennsylvania.
K. Toutanova, A. Haghighi, and C. D. Manning. 2005.
Joint learning improves semantic role labeling. ACL
2005
N. Xue and M. Palmer. 2004. Calibrating Features
for Semantic Role Labeling, In Proceedings of
EMNLP-2004. Barcelona, Spain.
132
Proceedings of the Workshop on Computational Approaches to Figurative Language, pages 21?28,
Rochester, NY, April 26, 2007. c?2007 Association for Computational Linguistics
Active Learning for the Identification of Nonliteral Language ?
Julia Birke and Anoop Sarkar
School of Computing Science, Simon Fraser University
Burnaby, BC, V5A 1S6, Canada
jbirke@alumni.sfu.ca, anoop@cs.sfu.ca
Abstract
In this paper we present an active learn-
ing approach used to create an annotated
corpus of literal and nonliteral usages
of verbs. The model uses nearly unsu-
pervised word-sense disambiguation and
clustering techniques. We report on exper-
iments in which a human expert is asked
to correct system predictions in different
stages of learning: (i) after the last iter-
ation when the clustering step has con-
verged, or (ii) during each iteration of the
clustering algorithm. The model obtains
an f-score of 53.8% on a dataset in which
literal/nonliteral usages of 25 verbs were
annotated by human experts. In compari-
son, the same model augmented with ac-
tive learning obtains 64.91%. We also
measure the number of examples required
when model confidence is used to select
examples for human correction as com-
pared to random selection. The results of
this active learning system have been com-
piled into a freely available annotated cor-
pus of literal/nonliteral usage of verbs in
context.
1 Introduction
In this paper, we propose a largely automated
method for creating an annotated corpus of literal vs.
nonliteral usages of verbs. For example, given the
verb ?pour?, we would expect our method to iden-
tify the sentence ?Custom demands that cognac be
poured from a freshly opened bottle? as literal, and
the sentence ?Salsa and rap music pour out of the
windows? as nonliteral, which, indeed, it does.
?This research was partially supported by NSERC, Canada
(RGPIN: 264905). We would like to thank Bill Dolan, Fred
Popowich, Dan Fass, Katja Markert, Yudong Liu, and the
anonymous reviewers for their comments.
We reduce the problem of nonliteral language
recognition to one of word-sense disambiguation
(WSD) by redefining literal and nonliteral as two
different senses of the same word, and we adapt
an existing similarity-based word-sense disambigua-
tion method to the task of separating usages of verbs
into literal and nonliteral clusters. Note that treat-
ing this task as similar to WSD only means that
we use features from the local context around the
verb to identify it as either literal or non-literal. It
does not mean that we can use a classifier trained on
WSD annotated corpora to solve this issue, or use
any existing WSD classification technique that re-
lies on supervised learning. We do not have any an-
notated data to train such a classifier, and indeed our
work is focused on building such a dataset. Indeed
our work aims to first discover reliable seed data
and then bootstrap a literal/nonliteral identification
model. Also, we cannot use any semi-supervised
learning algorithm for WSD which relies on reliably
annotated seed data since we do not possess any reli-
ably labeled data (except for our test data set). How-
ever we do exploit a noisy source of seed data in
a nearly unsupervised approach augmented with ac-
tive learning. Noisy data containing example sen-
tences of literal and nonliteral usage of verbs is used
in our model to cluster a particular instance of a verb
into one class or the other. This paper focuses on the
use of active learning using this model. We suggest
that this approach produces a large saving of effort
compared to creating such an annotated corpus man-
ually.
An active learning approach to machine learn-
ing is one in which the learner has the ability to
influence the selection of at least a portion of its
training data. In our approach, a clustering algo-
rithm for literal/nonliteral recognition tries to anno-
tate the examples that it can, while in each iteration
it sends a small set of examples to a human expert
to annotate, which in turn provides additional ben-
efit to the bootstrapping process. Our active learn-
21
ing method is similar to the Uncertainty Sampling
algorithm of (Lewis & Gale, 1994) but in our case
interacts with iterative clustering. As we shall see,
some of the crucial criticisms leveled against un-
certainty sampling and in favor of Committee-based
sampling (Engelson & Dagan, 1996) do not apply in
our case, although the latter may still be more accu-
rate in our task.
2 Literal vs. Nonliteral Identification
For the purposes of this paper we will take the sim-
plified view that literal is anything that falls within
accepted selectional restrictions (?he was forced to
eat his spinach? vs. ?he was forced to eat his words?)
or our knowledge of the world (?the sponge ab-
sorbed the water? vs. ?the company absorbed the
loss?). Nonliteral is then anything that is ?not lit-
eral?, including most tropes, such as metaphors, id-
ioms, as well as phrasal verbs and other anomalous
expressions that cannot really be seen as literal. We
aim to automatically discover the contrast between
the standard set of selectional restrictions for the lit-
eral usage of verbs and the non-standard set which
we assume will identify the nonliteral usage.
Our identification model for literal vs. nonliteral
usage of verbs is described in detail in a previous
publication (Birke & Sarkar, 2006). Here we pro-
vide a brief description of the model so that the use
of this model in our proposed active learning ap-
proach can be explained.
Since we are attempting to reduce the problem
of literal/nonliteral recognition to one of word-sense
disambiguation, we use an existing similarity-based
word-sense disambiguation algorithm developed by
(Karov & Edelman, 1998), henceforth KE. The KE
algorithm is based on the principle of attraction:
similarities are calculated between sentences con-
taining the word we wish to disambiguate (the target
word) and collections of seed sentences (feedback
sets). It requires a target set ? the set of sentences
containing the verbs to be classified into literal or
nonliteral ? and the seed sets: the literal feedback
set and the nonliteral feedback set. A target set sen-
tence is considered to be attracted to the feedback set
containing the sentence to which it shows the highest
similarity. Two sentences are similar if they contain
similar words and two words are similar if they are
contained in similar sentences. The resulting transi-
tive similarity allows us to defeat the knowledge ac-
quisition bottleneck ? i.e. the low likelihood of find-
ing all possible usages of a word in a single corpus.
Note that the KE algorithm concentrates on similari-
ties in the way sentences use the target literal or non-
literal word, not on similarities in the meanings of
the sentences themselves.
Algorithms 1 and 2 summarize our approach.
Note that p(w, s) is the unigram probability of word
w in sentence s, normalized by the total number of
words in s. We omit some details about the algo-
rithm here which do not affect our discussion about
active learning. These details are provided in a pre-
vious publication (Birke & Sarkar, 2006).
As explained before, our model requires a target
set and two seed sets: the literal feedback set and
the nonliteral feedback set. We do not explain the
details of how these feedback sets were constructed
in this paper, however, it is important to note that the
feedback sets themselves are noisy and not carefully
vetted by human experts. The literal feedback set
was built from WSJ newswire text, and for the non-
literal feedback set, we use expressions from vari-
ous datasets such as the Wayne Magnuson English
Idioms Sayings & Slang and George Lakoff?s Con-
ceptual Metaphor List, as well as example sentences
from these sources. These datasets provide lists of
verbs that may be used in a nonliteral usage, but we
cannot explicitly provide only those sentences that
contain nonliteral use of that verb in the nonliteral
feedback set. In particular, knowing that an expres-
sion can be used nonliterally does not mean that you
can tell when it is being used nonliterally. In fact
even the literal feedback set has noise from nonlit-
eral uses of verbs in the news articles. To deal with
this issue (Birke & Sarkar, 2006) provides automatic
methods to clean up the feedback sets during the
clustering algorithm. Note that the feedback sets are
not cleaned up by human experts, however the test
data is carefully annotated by human experts (details
about inter-annotator agreement on the test set are
provided below). The test set is not large enough to
be split up into a training and test set that can support
learning using a supervised learning method.
The sentences in the target set and feedback sets
were augmented with some shallow syntactic in-
formation such as part of speech tags provided
22
Algorithm 1 KE-train: (Karov & Edelman, 1998) algorithm adapted to literal/nonliteral identification
Require: S: the set of sentences containing the target word (each sentence is classified as literal/nonliteral)
Require: L: the set of literal seed sentences
Require: N : the set of nonliteral seed sentences
Require: W: the set of words/features, w ? s means w is in sentence s, s 3 w means s contains w
Require: ?: threshold that determines the stopping condition
1: w-sim0(wx, wy) := 1 if wx = wy, 0 otherwise
2: s-simI0(sx, sy) := 1, for all sx, sy ? S ? S where sx = sy, 0 otherwise
3: i := 0
4: while (true) do
5: s-simLi+1(sx, sy) :=
?
wx?sx p(wx, sx)maxwy?sy w-simi(wx, wy), for all sx, sy ? S ? L
6: s-simNi+1(sx, sy) :=
?
wx?sx p(wx, sx)maxwy?sy w-simi(wx, wy), for all sx, sy ? S ?N
7: for wx, wy ? W ?W do
8: w-simi+1(wx, wy) :=
{
i = 0
?
sx3wx p(wx, sx)maxsy3wy s-sim
I
i (sx, sy)
else
?
sx3wx p(wx, sx)maxsy3wy{s-sim
L
i (sx, sy), s-simNi (sx, sy)}
9: end for
10: if ?wx,maxwy{w-simi+1(wx, wy)? w-simi(wx, wy)} ? ? then
11: break # algorithm converges in 1? steps.
12: end if
13: i := i + 1
14: end while
by a statistical tagger (Ratnaparkhi, 1996) and Su-
perTags (Bangalore & Joshi, 1999).
This model was evaluated on 25 target verbs:
absorb, assault, die, drag, drown, escape,
examine, fill, fix, flow, grab, grasp, kick,
knock, lend, miss, pass, rest, ride, roll,
smooth, step, stick, strike, touch
The verbs were carefully chosen to have vary-
ing token frequencies (we do not simply learn on
frequently occurring verbs). As a result, the tar-
get sets contain from 1 to 115 manually annotated
sentences for each verb to enable us to measure ac-
curacy. The annotations were not provided to the
learning algorithm: they were only used to evaluate
the test data performance. The first round of anno-
tations was done by the first annotator. The second
annotator was given no instructions besides a few
examples of literal and nonliteral usage (not cov-
ering all target verbs). The authors of this paper
were the annotators. Our inter-annotator agreement
on the annotations used as test data in the experi-
ments in this paper is quite high. ? (Cohen) and ?
(S&C) on a random sample of 200 annotated exam-
ples annotated by two different annotators was found
to be 0.77. As per ((Di Eugenio & Glass, 2004), cf.
refs therein), the standard assessment for ? values is
that tentative conclusions on agreement exists when
.67 ? ? < .8, and a definite conclusion on agree-
ment exists when ? ? .8.
In the case of a larger scale annotation effort, hav-
ing the person leading the effort provide one or two
examples of literal and nonliteral usages for each tar-
get verb to each annotator would almost certainly
improve inter-annotator agreement.
The algorithms were evaluated based on how
accurately they clustered the hand-annotated sen-
tences. Sentences that were attracted to neither clus-
ter or were equally attracted to both were put in the
opposite set from their label, making a failure to
cluster a sentence an incorrect clustering.
Evaluation results were recorded as recall, preci-
sion, and f-score values. Literal recall is defined as
(correct literals in literal cluster / total correct liter-
als). Literal precision is defined as (correct literals
in literal cluster / size of literal cluster). If there are
no literals, literal recall is 100%; literal precision is
100% if there are no nonliterals in the literal clus-
ter and 0% otherwise. The f-score is defined as (2 ?
23
Algorithm 2 KE-test: classifying literal/nonliteral
1: For any sentence sx ? S
2: if maxsy s-sim
L(sx, sy) >
max
sy
s-simN (sx, sy)
then
3: tag sx as literal
4: else
5: tag sx as nonliteral
6: end if
precision ? recall) / (precision + recall). Nonliteral
precision and recall are defined similarly. Average
precision is the average of literal and nonliteral pre-
cision; similarly for average recall. For overall per-
formance, we take the f-score of average precision
and average recall.
We calculated two baselines for each word. The
first was a simple majority-rules baseline (assign
each word to the sense which is dominant which
is always literal in our dataset). Due to the imbal-
ance of literal and nonliteral examples, this baseline
ranges from 60.9% to 66.7% for different verbs with
an average of 63.6%. Keep in mind though that us-
ing this baseline, the f-score for the nonliteral set
will always be 0% ? which is the problem we are
trying to solve in this work. We calculated a second
baseline using a simple attraction algorithm. Each
sentence in the target set is attracted to the feedback
set with which it has the most words in common.
For the baseline and for our own model, sentences
attracted to neither, or equally to both sets are put
in the opposite cluster to which they belong. This
second baseline obtains a f-score of 29.36% while
the weakly supervised model without active learn-
ing obtains an f-score of 53.8%. Results for each
verb are shown in Figure 1.
3 Active Learning
The model described thus far is weakly supervised.
The main proposal in this paper is to push the re-
sults further by adding in an active learning compo-
nent, which puts the model described in Section 2 in
the position of helping a human expert with the lit-
eral/nonliteral clustering task. The two main points
to consider are: what to send to the human annotator,
and when to send it.
We always send sentences from the undecided
cluster ? i.e. those sentences where attraction to
either feedback set, or the absolute difference of
the two attractions, falls below a given threshold.
The number of sentences falling under this threshold
varies considerably from word to word, so we ad-
ditionally impose a predetermined cap on the num-
ber of sentences that can ultimately be sent to the
human. Based on an experiment on a held-out set
separate from our target set of sentences, sending
a maximum of 30% of the original set was deter-
mined to be optimal in terms of eventual accuracy
obtained. We impose an order on the candidate sen-
tences using similarity values. This allows the origi-
nal sentences with the least similarity to either feed-
back set to be sent to the human first. Further, we
alternate positive similarity (or absolute difference)
values and values of zero. Note that sending ex-
amples that score zero to the human may not help
attract new sentences to either of the feedback sets
(since scoring zero means that the sentence was not
attracted to any of the sentences). However, human
help may be the only chance these sentences have to
be clustered at all.
After the human provides an identification for a
particular example we move the sentence not only
into the correct cluster, but also into the correspond-
ing feedback set so that other sentences might be
attracted to this certifiably correctly classified sen-
tence.
The second question is when to send the sentences
to the human. We can send all the examples after
the first iteration, after some intermediate iteration,
distributed across iterations, or at the end. Sending
everything after the first iteration is best for coun-
teracting false attractions before they become en-
trenched and for allowing future iterations to learn
from the human decisions. Risks include sending
sentences to the human before our model has had
a chance to make potentially correct decision about
them, counteracting any saving of effort. (Karov &
Edelman, 1998) state that the results are not likely
to change much after the third iteration and we have
confirmed this independently: similarity values con-
tinue to change until convergence, but cluster al-
legiance tends not to. Sending everything to the
human after the third iteration could therefore en-
tail some of the damage control of sending every-
thing after the first iteration while giving the model
24
a chance to do its best. Another possibility is to
send the sentences in small doses in order to gain
some bootstrapping benefit at each iteration i.e. the
certainty measures will improve with each bit of hu-
man input, so at each iteration more appropriate sen-
tences will be sent to the human. Ideally, this would
produce a compounding of benefits. On the other
hand, it could produce a compounding of risks. A fi-
nal possibility is to wait until the last iteration in the
hope that our model has correctly clustered every-
thing else and those correctly labeled examples do
not need to be examined by the human. This imme-
diately destroys any bootstrapping possibilities for
the current run, although it still provides benefits for
iterative augmentation runs (see Section 4).
A summary of our results in shown in Figure 1.
The last column in the graph shows the average
across all the target verbs. We now discuss the vari-
ous active learning experiments we performed using
our model and a human expert annotator.
3.1 Experiment 1
Experiments were performed to determine the best
time to send up to 30% of the sentences to the human
annotator. Sending everything after the first iteration
produced an average accuracy of 66.8%; sending ev-
erything after the third iteration, 65.2%; sending a
small amount at each iteration, 60.8%; sending ev-
erything after the last iteration, 64.9%. Going just by
the average accuracy, the first iteration option seems
optimal. However, several of the individual word re-
sults fell catastrophically below the baseline, mainly
due to original sentences having been moved into a
feedback set too early, causing false attraction. This
risk was compounded in the distributed case, as pre-
dicted. The third iteration option gave slightly bet-
ter results (0.3%) than the last iteration option, but
since the difference was minor, we opted for the sta-
bility of sending everything after the last iteration.
These results show an improvement of 11.1% over
the model from Section 2. Individual results for each
verb are given in Figure 1.
3.2 Experiment 2
In a second experiment, rather than letting our model
select the sentences to send to the human, we se-
lected them randomly. We found no significant dif-
ference in the results. For the random model to out-
perform the non-random one it would have to select
only sentences that our model would have clustered
incorrectly; to do worse it would have to select only
sentences that our model could have handled on its
own. The likelihood of the random choices coming
exclusively from these two sets is low.
3.3 Experiment 3
Our third experiment considers the effort-savings of
using our literal/nonliteral identification model. The
main question must be whether the 11.1% accuracy
gain of active learning is worth the effort the hu-
man must contribute. In our experiments, the hu-
man annotator is given at most 30% of the sentences
to classify manually. It is expected that the human
will classify these correctly and any additional ac-
curacy gain is contributed by the model. Without
semi-supervised learning, we might expect that if
the human were to manually classify 30% of the sen-
tences chosen at random, he would have 30% of the
sentences classified correctly. However, in order to
be able to compare the human-only scenario to the
active learning scenario, we must find what the av-
erage f-score of the manual process is. The f-score
depends on the distribution of literal and nonliteral
sentences in the original set. For example, in a set
of 100 sentences, if there are exactly 50 of each, and
of the 30 chosen for manual annotation, half come
from the literal set and half come from the nonlit-
eral set, the f-score will be exactly 30%. We could
compare our performance to this, but that would be
unfair to the manual process since the sets on which
we did our evaluation were by no means balanced.
We base a hypothetical scenario on the heavy imbal-
ance often seen in our evaluation sets, and suggest
a situation where 96 of our 100 sentences are literal
and only 4 are nonliteral. If it were to happen that all
4 of the nonliteral sentences were sent to the human,
we would get a very high f-score, due to a perfect
recall score for the nonliteral cluster and a perfect
precision score for the literal cluster. If none of the
four nonliteral sentences were sent to the human, the
scores for the nonliteral cluster would be disastrous.
This situation is purely hypothetical, but should ac-
count for the fact that 30 out of 100 sentences an-
notated by a human will not necessarily result in an
average f-score of 30%: in fact, averaging the re-
sults of the three sitatuations described above results
25
Figure 1: Active Learning evaluation results. Baseline refers to the second baseline from Section 2. Semi-
supervised: Trust Seed Data refers to the standard KE model that trusts the seed data. Optimal Semi-
supervised refers to the augmented KE model described in (Birke & Sarkar, 2006). Active Learning refers
to the model proposed in this paper.
in an avarage f-score of nearly 36.9%. This is 23%
higher than the 30% of the balanced case, which is
1.23 times higher. For this reason, we give the hu-
man scores a boost by assuming that whatever the
human annotates in the manual scenario will result
in an f-score that is 1.23 times higher. For our ex-
periment, we take the number of sentences that our
active learning method sent to the human for each
word ? note that this is not always 30% of the to-
tal number of sentences ? and multiply that by 1.23
? to give the human the benefit of the doubt, so to
speak. Still we find that using active learning gives
us an avarage accuracy across all words of 64.9%,
while we get only 21.7% with the manual process.
This means that for the same human effort, using
the weakly supervised classifier produced a three-
fold improvement in accuracy. Looking at this con-
versely, this means that in order to obtain an ac-
curacy of 64.9%, by a purely manual process, the
human would have to classify nearly 53.6% of the
sentences, as opposed to the 17.7% he needs to do
using active learning. This is an effort-savings of
about 35%. To conclude, we claim that our model
combined with active learning is a helpful tool for
a literal/nonliteral clustering project. It can save the
human significant effort while still producing rea-
sonable results.
4 Annotated corpus built using active
learning
In this section we discuss the development of an an-
notated corpus of literal/nonliteral usages of verbs
in context. First, we examine iterative augmenta-
tion. Then we discuss the structure and contents of
the annotated corpus and the potential for expansion.
After an initial run for a particular target word, we
have the cluster results plus a record of the feedback
sets augmented with the newly clustered sentences.
26
***pour***
*nonliteral cluster*
wsj04:7878 N As manufacturers get bigger , they are likely to
pour more money into the battle for shelf space , raising the
ante for new players ./.
wsj25:3283 N Salsa and rap music pour out of the windows ./.
wsj06:300 U Investors hungering for safety and high yields
are pouring record sums into single-premium , interest-earning
annuities ./.
*literal cluster*
wsj59:3286 L Custom demands that cognac be poured from a
freshly opened bottle ./.
Figure 2: Excerpt from our annotated corpus of lit-
eral/nonliteral usages of verbs in context.
Each feedback set sentence is saved with a weight,
with newly clustered sentences receiving a weight
of 1.0. Subsequent runs may be done to augment
the initial clusters. For these runs, we use the the
output identification over the examples from our ini-
tial run as feedback sets. New sentences for cluster-
ing are treated like a regular target set. Running the
algorithm in this way produces new clusters and a
re-weighted model augmented with newly clustered
sentences. There can be as many runs as desired;
hence iterative augmentation.
We used the iterative augmentation process to
build a small annotated corpus consisting of the tar-
get words from Table 1, as well as another 25 words
drawn from the examples of previously published
work (see Section 5). It is important to note that
in building the annotated corpus, we used the Ac-
tive Learning component as described in this paper,
which improved our average f-score from 53.8% to
64.9% on the original 25 target words, and we ex-
pect also improved performance on the remainder of
the words in the annotated corpus.
An excerpt from the annotated corpus is shown in
Figure 2. Each entry includes an ID number and a
Nonliteral, Literal, or Unannotated tag. Annotations
are from testing or from active learning during anno-
tated corpus construction. The corpus is available at
http://www.cs.sfu.ca/?anoop/students/jbirke/. Fur-
ther unsupervised expansion of the existing clusters
as well as the production of additional clusters is a
possibility.
5 Previous Work
To our knowledge there has not been any previous
work done on taking a model for literal/nonliteral
language and augmenting it with an active learning
approach which allows human expert knowledge to
become part of the learning process.
Our approach to active learning is similar to the
Uncertainty Sampling approach of (Lewis & Gale,
1994) and (Fujii et. al., 1998) in that we pick those
examples that we could not classify due to low con-
fidence in the labeling at a particular point. We
employ a resource-limited version in which only
a small fixed sample is ever annotated by a hu-
man. Some of the criticisms leveled against un-
certainty sampling and in favor of Committee-based
sampling (Engelson & Dagan, 1996) (and see refs
therein) do not apply in our case.
Our similarity measure is based on two views of
sentence- and word-level similarity and hence we
get an estimate of appropriate identification rather
than just correct classification. As a result, by em-
bedding an Uncertainty Sampling active learning
model within a two-view clustering algorithm, we
gain the same advantages as other uncertainty sam-
pling methods obtain when used in bootstrapping
methods (e.g. (Fujii et. al., 1998)). Other machine
learning approaches that derive from optimal exper-
iment design are not appropriate in our case because
we do not yet have a strong predictive (or generative)
model of the literal/nonliteral distinction.
Our machine learning model only does identifi-
cation of verb usage as literal or nonliteral but it
can be seen as a first step towards the use of ma-
chine learning for more sophisticated metaphor and
metonymy processing tasks on larger text corpora.
Rule-based systems ? some using a type of interlin-
gua (Russell, 1976); others using complicated net-
works and hierarchies often referred to as metaphor
maps (e.g. (Fass, 1997; Martin, 1990; Martin, 1992)
? must be largely hand-coded and generally work
well on an enumerable set of metaphors or in lim-
ited domains. Dictionary-based systems use exist-
ing machine-readable dictionaries and path lengths
between words as one of their primary sources
for metaphor processing information (e.g. (Dolan,
1995)). Corpus-based systems primarily extract or
learn the necessary metaphor-processing informa-
tion from large corpora, thus avoiding the need for
manual annotation or metaphor-map construction.
Examples of such systems are (Murata et. al., 2000;
Nissim & Markert, 2003; Mason, 2004).
27
Nissim & Markert (2003) approach metonymy
resolution with machine learning methods, ?which
[exploit] the similarity between examples of con-
ventional metonymy? ((Nissim & Markert, 2003),
p. 56). They see metonymy resolution as a classi-
fication problem between the literal use of a word
and a number of pre-defined metonymy types. They
use similarities between possibly metonymic words
(PMWs) and known metonymies as well as context
similarities to classify the PMWs.
Mason (2004) presents CorMet, ?a corpus-based
system for discovering metaphorical mappings be-
tween concepts? ((Mason, 2004), p. 23). His system
finds the selectional restrictions of given verbs in
particular domains by statistical means. It then finds
metaphorical mappings between domains based on
these selectional preferences. By finding seman-
tic differences between the selectional preferences,
it can ?articulate the higher-order structure of con-
ceptual metaphors? ((Mason, 2004), p. 24), finding
mappings like LIQUID?MONEY.
Metaphor processing has even been ap-
proached with connectionist systems storing
world-knowledge as probabilistic dependencies
(Narayanan, 1999).
6 Conclusion
In this paper we presented a system for separating
literal and nonliteral usages of verbs through statis-
tical word-sense disambiguation and clustering tech-
niques. We used active learning to combine the pre-
dictions of this system with a human expert anno-
tator in order to boost the overall accuracy of the
system by 11.1%. We used the model together with
active learning and iterative augmentation, to build
an annotated corpus which is publicly available, and
is a resource of literal/nonliteral usage clusters that
we hope will be useful not only for future research in
the field of nonliteral language processing, but also
as training data for other statistical NLP tasks.
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging:
an approach to almost parsing. Comput. Linguist. 25, 2 (Jun.
1999), 237-265.
Julia Birke and Anoop Sarkar. 2006. In Proceedings of the 11th
Conference of the European Chapter of the Association for
Computational Linguistics, EACL-2006. Trento, Italy. April
3-7.
Barbara Di Eugenio and Michael Glass. 2004. The kappa
statistic: a second look. Comput. Linguist. 30, 1 (Mar. 2004),
95-101.
William B. Dolan. 1995. Metaphor as an emergent property
of machine-readable dictionaries. In Proceedings of Repre-
sentation and Acquisition of Lexical Knowledge: Polysemy,
Ambiguity, and Generativity (March 1995, Stanford Univer-
sity, CA). AAAI 1995 Spring Symposium Series, 27-29.
Sean P. Engelson and Ido Dagan. 1996. In Proc. of 34th Meet-
ing of the ACL. 319?326.
Dan Fass. 1997. Processing metonymy and metaphor. Green-
wich, CT: Ablex Publishing Corporation.
Atsushi Fujii, Takenobu Tokunaga, Kentaro Inui and Hozumi
Tanaka. 1998. Selective sampling for example-based word
sense disambiguation. Comput. Linguist. 24, 4 (Dec. 1998),
573?597.
Yael Karov and Shimon Edelman. 1998. Similarity-based word
sense disambiguation. Comput. Linguist. 24, 1 (Mar. 1998),
41-59.
David D. Lewis and William A. Gale. 1994. A sequential algo-
rithm for training text classifiers. In Proc. of SIGIR-94.
James H. Martin. 1990. A computational model of metaphor
interpretation. Toronto, ON: Academic Press, Inc.
James H. Martin. 1992. Computer understanding of conven-
tional metaphoric language. Cognitive Science 16, 2 (1992),
233-270.
Zachary J. Mason. 2004. CorMet: a computational, corpus-
based conventional metaphor extraction system. Comput.
Linguist. 30, 1 (Mar. 2004), 23-44.
Masaki Murata, Qing Ma, Atsumu Yamamoto, and Hitoshi Isa-
hara. 2000. Metonymy interpretation using x no y exam-
ples. In Proceedings of SNLP2000 (Chiang Mai, Thailand,
10 May 2000).
Srini Narayanan. 1999. Moving right along: a computational
model of metaphoric reasoning about events. In Proceed-
ings of the 16th National Conference on Artificial Intelli-
gence and the 11th IAAI Conference (Orlando, US, 1999).
121-127.
Malvina Nissim and Katja Markert. 2003. Syntactic features
and word similarity for supervised metonymy resolution. In
Proceedings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL-03) (Sapporo, Japan,
2003). 56-63.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Empirical Methods
in Natural Language Processing Conference (University of
Pennsylvania, May 17-18 1996).
Sylvia W. Russell. 1976. Computer understanding of
metaphorically used verbs. American Journal of Computa-
tional Linguistics, Microfiche 44.
28
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 1?9,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Exploration of the LTAG-Spinal Formalism and Treebank
for Semantic Role Labeling
Yudong Liu and Anoop Sarkar
School of Computing Science
Simon Fraser University
{yudongl,anoop}@cs.sfu.ca
Abstract
LTAG-spinal is a novel variant of tradi-
tional Lexicalized Tree Adjoining Gram-
mar (LTAG) introduced by (Shen, 2006).
The LTAG-spinal Treebank (Shen et al,
2008) combines elementary trees ex-
tracted from the Penn Treebank with Prop-
bank annotation. In this paper, we present
a semantic role labeling (SRL) system
based on this new resource and provide an
experimental comparison with CCGBank
and a state-of-the-art SRL system based
on Treebank phrase-structure trees. Deep
linguistic information such as predicate-
argument relationships that are either im-
plicit or absent from the original Penn
Treebank are made explicit and accessible
in the LTAG-spinal Treebank, which we
show to be a useful resource for semantic
role labeling.
1 Introduction
Semantic Role Labeling (SRL) aims to identify
and label all the arguments for each predicate in
a sentence. Specifically, it involves identifying
portions of the sentence that represent the pred-
icate?s arguments and assigning pre-specified se-
mantic roles to them.
[A0seller Ports of Call Inc.] reached agreements to
[Vverb sell] [A1thing its remaining seven aircraft]
[A2buyer to buyers that weren?t disclosed] .
is an example of SRL annotation from the Prop-
Bank corpus (Palmer et al, 2005), where the sub-
scripted information maps the semantic roles A0,
A1 and A2 to arguments for the predicate sell as
defined in the PropBank Frame Scheme.
The availability of annotated corpora like Prop-
Bank and FrameNet (Fillmore et al, 2001) have
provided rapid development of research into
SRL (Gildea and Jurafsky, 2002; Gildea and
Palmer, 2002; Surdeanu et al, 2003; Chen and
Rambow, 2003; Gildea and Hockenmaier, 2003;
Xue and Palmer, 2004; Pradhan et al, 2004; Prad-
han et al, 2005). The shared tasks in CoNLL-
2004 (Carreras and Ma`rquez, 2004), CoNLL-
2005 (Carreras and Ma`rquez, 2005) and CoNLL-
2008 (Surdeanu et al, 2008) were all focused on
SRL.
SRL systems (Gildea and Jurafsky, 2002;
Gildea and Palmer, 2002) have extensively used
features defined over Penn Treebank phrase-
structure trees. Other syntactic representations
such as CCG derivations (Gildea and Hocken-
maier, 2003) and dependency trees (Hacioglu,
2004; Surdeanu et al, 2008) have also been ex-
plored. It has been previously noted that LTAG,
which has the useful property of extended domain
of locality (EDL), is well-suited to address the
SRL task, c.f. (Chen and Rambow, 2003; Liu and
Sarkar, 2007). However, LTAG elementary trees
were extracted from the derived parse trees by
using Magerman-Collins style head-percolation
based heuristic rules (Liu and Sarkar, 2007). The
LTAG-spinal Treebank (Shen et al, 2008) pro-
vided a corpus of derivation trees where elemen-
tary trees were extracted from the Penn Tree-
bank in combination with the Propbank predicate-
argument annotation. The LTAG-spinal Treebank
can be used to overcome some of the limitations of
the previous work on SRL using LTAG: (Liu and
Sarkar, 2007) uses LTAG-based features extracted
from phrase-structure trees as an additional source
of features and combined them with features from
a phrase-structure based SRL framework; (Chen
and Rambow, 2003) only considers those comple-
ment/adjunct semantic roles that can be localized
in LTAG elementary trees, which leads to a loss
of over 17% instances of semantic roles even from
gold-standard trees.
The LTAG-spinal formalism was initially pro-
posed for automatic treebank extraction and sta-
tistical parsing (Shen and Joshi, 2005). However,
its Propbank-guided treebank extraction process
further strengthens the connection between the
LTAG-spinal and semantic role labeling. In this
paper, we present an SRL system that was built to
1
explore the utility of this new formalism, its Tree-
bank and the output of its statistical parser. Ex-
periments show that our LTAG-spinal based SRL
system achieves very high precision on both gold-
standard and automatic parses, and significantly
outperforms the one using CCGbank. More im-
portantly, it shows that LTAG-spinal is an useful
resource for semantic role labeling, with the po-
tential for further improvement.
2 LTAG-spinal, its Treebank and Parsers
This section gives a brief introduction of LTAG-
spinal formalism, its Treebank that is extracted
with the help of Propbank annotation, and its two
statistical parsers that are trained on the Tree-
bank. Predicate-argument relations encoded in the
LTAG-spinal treebank will also be discussed to il-
lustrate its compatibility with Propbank and their
potential utility for the SRL task.
2.1 LTAG-spinal
The LTAG-spinal formalism (Shen et al, 2008)
is a variant of Lexicalized Tree Adjoining Gram-
mar (LTAG) (Abeille? and Rambow, 2001). Com-
pared to traditional LTAG, the two types of ele-
mentary trees (e-tree for short), initial and auxil-
iary trees, are in spinal form with no substitution
nodes for arguments appearing in the predicate e-
tree: a spinal initial tree is composed of a lexi-
cal spine from the root to the anchor, and noth-
ing else; a spinal auxiliary tree is composed of a
lexical spine and a recursive spine from the root
to the foot node. For example, in Figure 1 (from
(Shen et al, 2008)), the lexical spine for the auxil-
iary tree is B1, .., Bi, .., Bn, the recursive spine is
B1, .., Bi, .., B?1 . Two operations attachment and
adjunction are defined in LTAG-spinal where ad-
junction is the same as adjunction in the traditional
LTAG; attachment stems from sister adjunction as
defined in Tree Insertion Grammar (TIG) (Schabes
and Shieber, 1994), which corresponds to the case
where the root of an initial tree is taken as a child
of another spinal e-tree. The two operations are
applied to LTAG-spinal e-tree pairs resulting in an
LTAG derivation tree which is similar to a depen-
dency tree (see Figure 2). In Figure 2, e-tree an-
chored with continue is the only auxiliary tree; all
other e-trees are initial trees. The arrow is directed
from parent to child, with the type of operation
labeled on the arc. The operation types are: att
denotes attachment operation; adj denotes adjunc-
tion operation. The sibling nodes may have differ-
An
B1A1
Bn
initial: auxiliary:
B1*
Bi
Figure 1: Spinal elementary trees
ent landing site along the parent spine. For ex-
ample, among the child nodes of stabilize e-tree,
to e-tree has VP as landing site; while even has S
as landing site. Such information, on some level,
turns out to be helpful to differentiate the semantic
role played by the different child nodes.
So far, we can see that in contrast with tradi-
tional LTAG where arguments refer to obligatory
constituents only, subcategorization frames and
argument-adjunct distinction are underspecified
in LTAG-spinal. Since argument-adjunct disam-
biguation is one of the major challenges faced by
LTAG treebank construction, LTAG-spinal works
around this issue by leaving the disambiguation
task for further deep processing, such as seman-
tic role labeling.
LTAG-spinal is weakly equivalent to traditional
LTAG with adjunction constraints1 (Shen, 2006).
The Propbank (Palmer et al, 2005) is an an-
notated corpus of verb subcategorization and al-
ternations which was created by adding a layer
of predicate-argument annotation over the phrase
structure trees in the Penn Treebank. The LTAG-
spinal Treebank is extracted from the Penn Tree-
bank by exploiting Propbank annotation. Specif-
ically, as described in (Shen et al, 2008), a Penn
Treebank syntax tree is taken as an LTAG-spinal
derived tree; then information from the Penn Tree-
bank and Propbank is merged using tree transfor-
mations. For instance, LTAG predicate coordina-
tion and instances of adjunction are recognized
using Propbank annotation. LTAG elementary
trees are then extracted from the transformed Penn
Treebank trees recursively, using the Propbank an-
notation and a Magerman-Collins style head per-
colation table.
This guided extraction process allows syntax
and semantic role information to be combined in
LTAG-spinal derivation trees. For example, the
1null adjunction (NA), obligatory adjunction (OA) and se-
lective adjunction (SA)
2
Figure 2: An example of LTAG-spinal sub-derivation tree, from LTAG-spinal Treebank Section 22
Figure 3: Three examples of LTAG-spinal derivation trees where predicates and their Propbank style
argument labels are given. These examples are from LTAG-spinal Treebank Section 22.
Penn Treebank does not differentiate raising verbs
and control verbs, however, based on the Propbank
information, LTAG-spinal makes this distinction
explicit. Thus, the error of taking a subject ar-
gument which is not semantically an argument of
the raising verb can be avoided. Another prop-
erty of LTAG-spinal Treebank extraction lies in the
flexibility and simplicity of the treatment of pred-
icate coordination (see (Shen et al, 2008)). Fig-
ure 3 shows three examples of Propbank annota-
tion as decorations over the LTAG-spinal deriva-
tion trees. In each derivation tree, each node is
associated with LTAG-spinal e-trees. Each argu-
ment (A0, A1, etc.) is referred to as A and the
predicate is called P . In most cases, the argument
is found locally in the derivation tree due to the
extended domain of locality in e-trees. Thus, most
arguments are identified by the pattern P ? A or
P ? A. The next section contains a discussion of
such patterns in more detail.
Two statistical parsers have been developed
by Libin Shen specifically for training on the
LTAG-spinal treebank: a left-to-right incremental
parser (Shen and Joshi, 2005) and a bidirectional
incremental parser (Shen and Joshi, 2008). If one
compares the output of these two parsers, the left-
to-right parser produces full LTAG-spinal deriva-
tion trees (including all the information about
specific elementary trees used in the derivation
and the attachment information within the e-trees)
while the bidirectional parser produces derivation
trees without information about elementary trees
or attachment points (similar to output from a de-
pendency parser). In this paper, we use the left-
to-right incremental parser for its richer output
because our SRL system uses feature functions
that use information about the elementary trees in
the derivation tree and the attachment points be-
tween e-trees. The landing site of child node along
the parent spine is useful for identifying different
types of arguments in SRL. For example, assume
the parent spine is ?S-VP-VB-anchor? (the root la-
bel is S, and ?anchor? is where the lexical item is
inserted). Along with direction information, the
landing site label ?S? is likely to be a good indi-
cator for argument A0 (subject) while the landing
site label ?VP? could be a good indicator for ?A1?
(object). In this sense, the incremental left-to-
right parser is preferable for semantic role label-
ing. However, having been developed earlier than
the bidirectional parser, the incremental parser ob-
tains 1.2% less in dependency accuracy compared
to the bidirectional parser (Shen and Joshi, 2008).
2.2 Predicate-argument relations in the
LTAG-spinal Treebank
The Propbank-guided extraction process for
LTAG-spinal treebank naturally creates a close
connection between these two resources. To ex-
amine the compatibility of the LTAG-spinal Tree-
bank with Propbank, (Shen et al, 2008) provides
the frequency for specific types of paths from
the predicate to the argument in the LTAG-spinal
derivation trees from the LTAG-spinal Treebank.
The 8 most frequent patterns account for 95.5%
of the total predicate-argument pairs of the LTAG-
spinal Treebank, of which 88.4% are directly con-
nected pairs. These statistics not only provide em-
3
Path Pattern Number Percent
1 P?A 8294 81.3
2 P?A, V?A 720 7.1
3 P?Px?A 437 4.3
4 P?Coord?Px?A 216 2.1
5 P?Ax?Py?A 84 0.82
6 P?Coord?Px?A 40 0.39
7 P?Px?Py?A 13 0.13
total recovered w/ patterns 9804 96.1
total 10206 100.0
Table 1: Distribution of the 7 most frequent
predicate-argument pair patterns in LTAG-spinal
Treebank Section 22. P : predicate, A: argument,
V : modifying verb, Coord: predicate coordina-
tion.
pirical justification for the notion of the extended
domain of locality (EDL) in LTAG-spinal (Shen et
al., 2008), they also provide motivation to explore
this Treebank for the SRL task.
We collected similar statistics from Treebank
Section 22 for the SRL task, shown in Table 1,
where 7 instead of 8 patterns suffice in our setting.
Each pattern describes one type of P(redicate)-
A(rgument) pair with respect to their dependency
relation and distance in the LTAG-spinal deriva-
tion tree. The reason that we combine the two pat-
terns P?A and V?A into one is that from SRL
perspective, they are equivalent in terms of the de-
pendency relation and distance between the pred-
icate. Each token present in the patterns, such as
P, Px, Py, V, A, Ax and Coord, denotes a spinal
e-tree in the LTAG-spinal derivation tree.
To explain the patterns more specifically, take
the LTAG-spinal sub-derivation tree in Figure 2
as an example, Assume P(redicate) in question is
stabilize then (stabilize ? even), (stabilize ?
if), (stabilize ? Street), (stabilize ? continue),
(stabilize ? to) all belong to pattern 1; but only
(stabilize ? Street) is actual predicate-argument
pair. Similarly, when take continue as P, the
predicate-argument pair (continue ? stabilize)
belongs to pattern 2, where stabilize corresponds
to A(rgument) in the pattern; (continue, Street) in
(Street ? stabilize ? continue) is an example of
pattern 3, where stabilize corresponds to Px and
Street corresponds to A in the pattern 3 schema.
Pattern 4 denotes the case where argument (A) is
shared between coordinated predicates (P and Px);
The main difference of pattern 5-7 exists where
the sibling node of A(rgument) is categorized into:
predicate (Px) in pattern 7, predicate coordination
node (Coord) in pattern 6 and others (Ax) in pat-
tern 5. We will retain this difference instead of
merging it since the semantic relation between P
and A varies based on these differences. Example
sentences for other (rarer) patterns can be found
in (Shen et al, 2008).
3 LTAG-spinal based SRL System De-
scription
In this section, we describe our LTAG-spinal based
SRL system. So far, we have studied LTAG-spinal
formalism, its treebank and parsers. In particular,
the frequency distribution of the seven most seen
predicate-argument pair patterns in LTAG-spinal
Treebank tells us that predicate-argument relation-
ships typical to semantic role labeling are often lo-
cal in LTAG-spinal derivation trees.
Pruning, argument identification and argument
classification ? the 3-stage architecture now stan-
dard in SRL systems is also used in this paper.
Specifically, for the sake of efficiency, nodes with
high probability of being NULL (non-argument)
should be filtered at the beginning; usually filter-
ing is done based on some heuristic rules; after the
pruning stage, argument identification takes place
with the goal of classifying the pruning-survival
nodes into argument and non-argument; for those
nodes that have been classified as arguments, ar-
gument classification component will further label
them with different argument types, such as A0,
A1, etc. Argument identification and classifica-
tion are highly ambiguous tasks and are usually
accomplished using a machine learning method.
For our LTAG-spinal based SRL system, we
first collect the argument candidates for each pred-
icate from the LTAG-spinal derivation tree. For
each candidate, features are extracted to capture
the predicate-argument relations. Binary classi-
fiers for identification and classification are trained
using SVMs and combined in a one-vs-all model.
The results are evaluated using precision/recall/f-
score.
3.1 Candidate Locations for Arguments
In SRL systems that perform role labeling of con-
stituents in a phrase-structure tree, statistics show
that after pruning, ?98% of the SRL argument
nodes are retained in the gold-standard trees in
the Penn Treebank, which provides a high upper-
bound for the recall of the SRL system. Pruning
away unnecessary nodes using a heuristic makes
4
learning easier as well, as many of the false posi-
tives are pruned away leading to a more balanced
binary classification problem during the seman-
tic role identification and classification steps. We
need a similar heuristic over LTAG-spinal nodes
that will have high coverage with respect to SRL
arguments and provide a high upper-bound for re-
call.
As previously shown that the seven most fre-
quent predicate-argument pair patterns that are
used to describe the specific types of paths from
the predicate to the argument account for?96% of
the total number of predicate-argument pairs in the
LTAG-spinal Treebank. These patterns provide a
natural candidate selection strategy for our SRL.
Table 2 shows a similar oracle test applied to the
output of the LTAG-spinal parser on Section 22.
The total drop in oracle predicate-argument iden-
tifiation drops 10.5% compared to gold-standard
trees. 9.8% is lost from patterns 1 and 2. If ex-
clude those pairs that belong to pattern i in tree-
bank but belong to pattern j (i 6= j) in automatic
parses (so the pattern exists but is the wrong one
for that constituent), the number drops to 81.6%
from 85.6%. This indicates that in terms of the
impact of the syntactic parser errors for SRL, the
LTAG-spinal parser will suffer even more than the
phase structure parser. An alternative is to exhaus-
tively search for predicate-argument pairs without
considering patterns, which we found introduces
too much noise in the learner to be feasible. Thus,
the predicate-argument pairs selected through this
phase are considered as argument candidates for
our SRL system.
3.2 Features
Based on the patterns, features are defined on
predicate-argument pairs from LTAG derivation
Path Pattern Number Percent
1 P?A 7441 72.9
2 P?A, V?A 583 5.7
3 P?Px?A 384 3.8
4 P?Coord?Px?A 180 1.76
5 P?Ax?Py?A 75 0.73
6 P?Coord?Px?A 48 0.47
7 P?Px?Py?A 22 0.21
total recovered w/ patterns 8733 85.6
total 10206 100.0
Table 2: Distribution of the 7 patterns in LTAG-
spinal parser output for Section 22.
tree, mainly including predicate e-trees, argument
e-trees, intermediate e-trees and their ?topological
relationships? such as operation, spine node, rel-
ative position and distance. The following are the
specific features used in our classifiers:
Features from predicate e-tree and its variants
predicate lemma, POS tag of predicate, predicate
voice, spine of the predicate e-tree, 2 variants of
predicate e-tree: replacing anchor in the spine
with predicate lemma, replacing anchor POS in
the spine with voice. In Figure 2, if take stabi-
lize as predicate, these two variants are S-VP-VB-
stabilize and S-VP-VB-active respectively.
Features from argument e-tree and its variants
argument lemma, POS tag of argument, Named
Entity (NE) label of the argument, spine of the ar-
gument e-tree, 2 variants of argument e-tree: re-
placing anchor in the spine with argument lemma,
replacing anchor POS with NE label if any, label
of root node of the argument spine. In Figure 2,
if take stabilize as predicate, and Street as argu-
ment, the two variants are XP-NNP-street and XP-
ORGANIZATION2 respectively.
PP content word of argument e-tree if the root
label of the argument e-tree is PP, anchor of the
last daughter node. NE variant of this feature: re-
place its POS with the NE label if any.
Features from the spine node (SP1) spine node is
the landing site between predicate e-tree and argu-
ment e-tree. Features include the index along the
host spine3, label of the node, operation involved
(att or adj).
Relative position of predicate and argument in the
sentence: before/after.
Order of current child node among its siblings.
In pattern 1, predicate e-tree is parent, and argu-
ment e-tree is child. This feature refers to the order
of argument e-tree among its siblings nodes (with
predicate e-tree as parent).
Distance of predicate e-tree and argument tree in
the LTAG derivation tree: For example, for pattern
1 and 2, the distance has value 0; for pattern 3, the
distance has value 1.
Pattern ID valued 1-7. (see Table 1 and Table 2)
Combination of position and pattern ID, combi-
nation of distance and pattern ID, combination of
2XP-NNP is a normalized e-tree form used in (Shen et
al., 2008) for efficiency and to avoid the problem of sparse
data over too many e-trees.
3it can either be predicate e-tree or argument e-tree. For
example, for pattern P?A, the A(rgument) e-tree is the host
spine.
5
position and order.
Features from intermediate predicate e-tree
same features as predicate e-tree features.
Features from spine node of intermediate pred-
icate e-tree and argument e-tree (SP2) for
predicate-argument pairs of pattern 3-7. These
features are similar to the SP1 features but instead
between intermediate predicate e-tree and argu-
ment e-tree.
Relative position between predicate e-tree and in-
termediate e-tree.
Combination relative positions of argument e-tree
and intermediate predicate e-tree + relative posi-
tion of argument e-tree and predicate e-tree.
The features listed above are used to represent
each candidate constituent (or node) in the LTAG-
spinal derivation tree in training and test data. In
both cases, we identify SRLs for nodes for each
predicate. In training each node comes with the
appropriate semantic role label, or NULL if it does
not have any (for the predicate). In test data,
we first identify nodes as arguments using these
features (ARG v.s. NULL classification) and then
classify a node identified as an argument with the
particular SRL using one-vs-all binary classifica-
tion.
4 Experiments
4.1 Data Set
Following the usual convention for parsing and
SRL experiments, LTAG-spinal Treebank Section
2-21 is used for training and Section 23 for test-
ing. Propbank argument set is used which includes
numbered arguments A0 to A5 and 13 adjunct-like
arguments. 454 sentences in the Penn Treebank
are skipped from the LTAG-spinal Treebank (Shen
et al, 2008)4, which results in 115 predicate-
argument pairs ignored in the test set.
We applied SVM-light (Joachims, 1999) with
default linear kernel to feature vectors. 30% of
the training samples are used to fine tune the reg-
ularization parameter c and the loss-function cost
parameter j for both argument identification and
classification. With parameter validation experi-
ments, we set c = 0.1 and j = 1 for {A0, AM-
4Based on (Shen et al, 2008), the skipped 454 sentences
amount to less than 1% of the total sentences. 314 of these
454 sentences have gapping structures. Since PTB does not
annotate the trace of deleted predicates, additional manual
annotation is required to handle these sentences. For the rest
of the 146 sentences, abnormal structures are generated due
to tagging errors.
NEG}, c = 0.1, j = 2 for {A1, A2, A4, AM-
EXT} and c = 0.1 and j = 4 for the rest.
For comparison, we also built up a standard 3-
stage phrase-structure based SRL system, where
exactly the same data set5 is used from 2004
February release of the Propbank. SVM-light with
linear kernel is used to train on a standard fea-
ture set (Xue and Palmer, 2004). The Charniak
and Johnson parser (2006) is used to produce the
automatic parses. Note that this phrase-structure
based SRL system is state-of-the-art and we have
included all the features proposed in the litera-
ture that use phrase-structure trees. This system
obtains a higher SRL accuracy which can be im-
proved only by using global inference and other
ways (such as using multiple parsers) to improve
the accuracy on automatic parses.
4.2 Results
We compared our LTAG-spinal based SRL system
with phrase-structure based one (see the descrip-
tion in earlier sections), for argument identifica-
tion and classification. In order to analyze the im-
pact of errors in syntactic parsers, results are pre-
sented on both gold-standard trees and automatic
parses. Based on the fact that nearly 97% e-trees
that correspond to the core arguments6 belong to
pattern 1 and 2, which accounts for the largest por-
tion of argument loss in automatic parses, the clas-
sification results are also given for these core argu-
ments. We also compare with the CCG-based SRL
presented in (Gildea and Hockenmaier, 2003)7,
which has a similar motivation as this paper, ex-
cept they use the Combinatory Categorial Gram-
mar formalism and the CCGBank syntactic Tree-
bank which was converted from the Penn Tree-
bank.
Scoring strategy To have a fair evaluation of argu-
ments between the LTAG-spinal dependency parse
and the Penn Treebank phrase structure, we report
the root/head-word based scoring strategy for per-
formance comparison, where a case is counted as
positive as long as the root of the argument e-tree
is correctly identified in LTAG-spinal and the head
word of the argument constituent is correctly iden-
tified in phrase structure. In contrast, boundary-
5The same 454 sentences are ignored.
6A0, A1, A2, A3, A4, A5
7Their data includes the 454 sentences. However, the
missing 115 predicate-argument pairs account for less than
1% of the total number of predicate-argument pairs in the test
data, so even if we award these cases to the CCGBank system
the system performance gap still remains.
6
based scoring is more strict in that the string span
of the argument must be correctly identified in
identification and classification.
Results from using gold standard trees Ta-
ble 3 shows the results when gold standard trees
are used. We can see that with gold-standard
derivations, LTAG-spinal obtains the highest pre-
cision on identification and classification; it also
achieves a competitive f-score (highest f-score for
identification) with the recall upper-bound lower
by 2-3% than phrase-structure based SRL. How-
ever, the recall gap between the two SRL systems
gets larger for classification compared to identifi-
cation8, which is due to the low recall that is ob-
served with our LTAG-spinal based SRL based on
our current set of features. If compare the differ-
ence between the root/head-word based score and
the boundary based score in the 3 scenarios, we
notice that the difference reflects the discrepancy
between the argument boundaries. It is not sur-
prising to see that phrase-structure based one has
the best match. However, CCGBank appears to
have a large degree of mismatch. In this sense,
root/head word based scoring provides fair com-
parison between LTAG-spinal SRL system and the
CCGBank SRL system.
Recent work (Boxwell and White, 2008)
changes some structures in the CCGBank to cor-
respond more closely with the Probbank annota-
tions. They also resolve split arguments that occur
in Propbank and add these annotations into a re-
vised version of the CCGBank. As a result they
show that the oracle f-score improves by over 2
points over the (Gildea and Hockenmaier, 2003)
oracle results for the numbered arguments only
(A0, . . ., A5). It remains an open question whether
a full SRL system based on a CCG parser trained
on this new version of the CCGBank will be com-
petitive against the LTAG-spinal based and phrase-
structure based SRL systems.
Results from using automatic parses Table 4
shows the results when automatic parses are used.
With automatic parses, the advantage of LTAG-
spinal in the precision scores still exists: giving
a higher score in both identification and core argu-
ment classification; only 0.5% lower for full argu-
ment classification. However, with over 6% dif-
ference in upper-bound of recall (?85.6% from
LTAG-spinal; ?91.7% from Charniak?s parser),
8no NULL examples are involved when training for argu-
ment classification.
the gap in recall becomes larger: increased to
?10% in automatic parses from ?6% in gold-
standard trees.
The identification result is not available for
CCG-based SRL. In terms of argument classifica-
tion, it is significantly outperformed by the LTAG-
spinal based SRL. In particular, it can be seen that
the LTAG-spinal parser performs much better on
argument boundaries than CCG-based one.
One thing worth mentioning is that since neither
the LTAG-spinal parser nor Charniak?s parser pro-
vides trace (empty category) information in their
output, no trace information is used for LTAG-
spinal based SRL or the phrase-structure based
SRL even though it is available in their gold-
standard trees.
5 Conclusion and Future Work
With a small feature set, the LTAG-spinal based
SRL system described in this paper provides the
highest precision in almost all the scenarios, which
indicates that the shallow semantic relations, e.g.,
the predicate-argument relations that are encoded
in the LTAG-spinal Treebank are useful for SRL,
especially when compared to the phrase structure
Penn Treebank. (Shen et al, 2008) achieves an f-
score of 91.6% for non-trace SRL identification on
the entire Treebank by employing a simple rule-
based system, which also suggested this conclu-
sion. In other words, there is a tighter connection
between the syntax and semantic role labels in the
LTAG-spinal representation.
However, in contrast to the high precision, the
recall performance of LTAG-spinal based SRL
needs a further improvement, especially for the ar-
gument classification task. From SRL perspective,
on one hand, this may be due to the pattern-based
candidate selection, which upper-bounds the num-
ber of predicate-argument pairs that can be re-
covered for SRL; on the other hand, it suggests
that the features for argument classification need
to be looked at more carefully, compared to the
feature selection for argument identification, es-
pecially for A2 and A3 (as indicated by our error
analysis on the results on the development set). A
possible solution is to customize a different fea-
ture set for each argument type during classifica-
tion, especially for contextual information.
Experiments show that when following the
pipelined architecture, the performance of LTAG-
based SRL is more severely degraded by the syn-
tactic parser, compared to the SRL using phrase
7
Identification gold-standard trees (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 96.0/92.1/94.0 93.0/94.0/93.5 n/a
classification (core) gold-standard trees (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 90.6/83.4/86.9 87.2/88.4/87.8 82.4/78.6/80.4
classification (full) gold-standard trees (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 88.2/81.7/84.8 86.1/87.1/86.6 76.3/67.8/71.8
Boundary 87.4/81.0/84.1 86.0/87.0/86.5 67.5/60.0/63.5
Table 3: Using gold standard trees: comparison of the three SRL systems for argument identification,
core and full argument classification
Identification automatic parses (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 85.8/80.0/82.8 85.8/87.7/86.7 n/a
classification (core) automatic parses (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 81.0/71.5/76.0 80.1/82.8/81.4 76.1/73.5/74.8
classification (full) automatic parses (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 78.0/70.0/73.7 78.5/80.3/79.4 71.0/63.1/66.8
Boundary 72.3/65.0/68.5 73.8/75.5/74.7 55.7/49.5/52.4
Table 4: Using automatic parses: comparison of the three SRL systems for argument identification, core
and full argument classification
structure and CCG formalism. Even though the
left-to-right statistical parser that was trained and
evaluated on the LTAG-spinal Treebank achieves
an f-score of 89.3% for dependencies on Section
23 of this treebank (Shen and Joshi, 2005), the
SRL that used this output is worse than expected.
An oracle test shows that via the same 7 patterns,
only 81.6% predicate-argument pairs can be re-
covered from the automatic parses, which is a big
drop from 96.1% when we use the LTAG-spinal
Treebank trees. Parser accuracy is high overall,
but needs to be more accurate in recovering the
dependencies between predicate and argument.
Based on the observation that the low recall
occurs not only to the SRL when the automatic
parses are used but also when the gold trees are
used, we would expect that a thorough error analy-
sis and feature calibrating can give us a better idea
in terms of how to increase the recall in both cases.
In on-going work, we also plan to improve
the dependency accuracy for predicate and argu-
ment dependencies by using the SRL predictions
as feedback for the syntactic parser. Our hypoth-
esis is that this approach combined with features
that would improve the recall numbers would lead
to a highly accurate SRL system.
As a final note, we believe that our effort on us-
ing LTAG-spinal for SRL is a valuable exploration
of the LTAG-spinal formalism and its Treebank re-
source. We hope our work will provide useful in-
formation on how to better utilize this formalism
and the Treebank resource for semantic role label-
ing.
Acknowledgements
We would like to thank Aravind Joshi and Lu-
cas Champollion for their useful comments and
for providing us access to the LTAG-spinal Tree-
bank. We would especially like to thank Libin
Shen for providing us with the LTAG-spinal sta-
tistical parser for our experiments and for many
helpful comments.
8
References
A. Abeille? and O. Rambow, editors. 2001. Tree Ad-
joining Grammars: Formalisms, Linguistic Analysis
and Processing. Center for the Study of Language
and Information.
Stephen A. Boxwell and Michael White. 2008. Pro-
jecting propbank roles onto the ccgbank. In LREC-
2008.
X. Carreras and L. Ma`rquez. 2004. Introduction to the
CoNLL-2004 Shared Task. In CoNLL-2004.
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task. In CoNLL-2005.
J. Chen and O. Rambow. 2003. Use of deep linguistic
features for the recognition and labeling of semantic
arguments. In EMNLP-2003.
C.J. Fillmore, C. Wooters, and C.F. Baker. 2001.
Building a large lexical databank which provides
deep semantics. In PACLIC15-2001.
D. Gildea and J. Hockenmaier. 2003. Identifying se-
mantic roles using combinatory categorial grammar.
In EMNLP-2003.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
58(3):245?288.
D. Gildea and M. Palmer. 2002. The necessity of
parsing for predicate argument recognition. In ACL-
2002.
K. Hacioglu. 2004. Semantic role labeling using de-
pendency trees. In COLING-2004.
T. Joachims. 1999. Making large-scale svm learning
practical. Advances in Kernel Methods - Support
Vector Machines.
Y. Liu and A. Sarkar. 2007. Experimental evaluation
of LTAG-based features for semantic role labeling.
In EMNLP-2007.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
S. Pradhan, W. Ward, K. Hacioglu, , J. H. Martin, and
D. Jurafsky. 2004. Shallow Semantic Parsing Using
Support Vector Machines. In HLT-NAACL-2004.
S. Pradhan, W. Ward, K. Hacioglu, , J. H. Martin, and
D. Jurafsky. 2005. Semantic role labeling using dif-
ferent syntactic views. In ACL-2005.
Yves Schabes and Stuart M. Shieber. 1994. An
alternative conception of tree-adjoining derivation.
Computational Linguistics, 20(1):91?124.
L. Shen and Aravind Joshi. 2005. Incremental ltag
parsing. In HLT-EMNLP-2005.
L. Shen and A. Joshi. 2008. Ltag dependency pars-
ing with bidirectional incremental construction. In
EMNLP-2008.
L. Shen, L. Champollion, and A. Joshi. 2008. Ltag-
spinal and the treebank: A new resource for in-
cremental, dependency and semantic parsing. Lan-
guage Resources and Evaluation, 42(1):1?19.
L. Shen. 2006. Statistical LTAG Parsing. Ph.D. thesis,
University of Pennsylvania.
M. Surdeanu, S. Harabagiu, J. Williams, and
P. Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In ACL-2003.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez,
and J. Nivre. 2008. The conll 2008 shared task
on joint parsing of syntactic and semantic dependen-
cies. In CoNLL-2008.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In EMNLP-2004.
9
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1089?1099,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Efficient Left-to-Right Hierarchical Phrase-based Translation with
Improved Reordering
Maryam Siahbani, Baskaran Sankaran, Anoop Sarkar
Simon Fraser University
Burnaby BC. CANADA
{msiahban,baskaran,anoop}@cs.sfu.ca
Abstract
Left-to-right (LR) decoding (Watanabe et al,
2006b) is a promising decoding algorithm for
hierarchical phrase-based translation (Hiero).
It generates the target sentence by extending
the hypotheses only on the right edge. LR de-
coding has complexity O(n2b) for input of n
words and beam size b, compared toO(n3) for
the CKY algorithm. It requires a single lan-
guage model (LM) history for each target hy-
pothesis rather than two LM histories per hy-
pothesis as in CKY. In this paper we present an
augmented LR decoding algorithm that builds
on the original algorithm in (Watanabe et al,
2006b). Unlike that algorithm, using experi-
ments over multiple language pairs we show
two new results: our LR decoding algorithm
provides demonstrably more efficient decod-
ing than CKY Hiero, four times faster; and by
introducing new distortion and reordering fea-
tures for LR decoding, it maintains the same
translation quality (as in BLEU scores) ob-
tained phrase-based and CKY Hiero with the
same translation model.
1 Introduction
Hiero (Chiang, 2007) models translation using a lex-
icalized synchronous context-free grammar (SCFG)
extracted from word aligned bitexts. Typically,
CKY-style decoding is used for Hiero with time
complexity O(n3) for source input with n words.
Scoring the target language output using a lan-
guage model within CKY-style decoding requires
two histories per hypothesis, one on the left edge
of each span and one on the right, due to the fact
that the target side is not generated in left to right
order, but rather built bottom-up from sub-spans.
This leads to complex problems in efficient lan-
guage model integration and requires state reduc-
tion techniques (Heafield et al, 2011; Heafield et
al., 2013). The size of a Hiero SCFG grammar is
typically larger than phrase-based models extracted
from the same data creating challenges in rule ex-
traction and decoding time especially for larger
datasets (Sankaran et al, 2012).
In contrast, the LR-decoding algorithm could
avoid these shortcomings such as faster time com-
plexity, reduction in the grammar size and the sim-
plified left-to-right language model scoring. It
means LR decoding has the potential to replace
CKY decoding for Hiero. Despite these attractive
properties, we show that the original LR-Hiero de-
coding proposed by (Watanabe et al, 2006b) does
not perform to the same level of the standard CKY
Hiero with cube pruning (see Table 3). In addition,
the current LR decoding algorithm does not obtain
BLEU scores comparable to phrase-based or CKY-
based Hiero models for different language pairs (see
Table 4). In this paper we propose modifications to
the LR decoding algorithm that addresses these limi-
tations and provides, for the first time, a true alterna-
tive to the standard CKY Hiero algorithm that uses
left-to-right decoding.
We introduce a new extended version of the LR
decoding algorithm presented in (Watanabe et al,
2006b) which is demonstrably more efficient than
the CKY Hiero algorithm. We measure the effi-
ciency of the LR Hiero decoder in a way that is
independent of the choice of system and program-
ming language by measuring the number of lan-
guage model queries. Although more efficient, the
new LR decoding algorithm suffered from lower
BLEU scores compared to CKY Hiero. Our anal-
ysis of left to right decoding showed that it has more
potential for search errors due to early pruning of
good hypotheses. This is unlike bottom-up decoding
(CKY) which keeps best hypotheses for each span.
To address this issue, we introduce two novel fea-
tures into the Hiero SMT model that deal with re-
ordering and distortion. Our experiments show that
LR decoding with these features using prefix lexi-
1089
calized target side rules equals the scores obtained
by CKY decoding with prefix lexicalized target side
rules and phrase-based translation system. It per-
forms four times fewer language model queries on
average, compare to CKY Hiero decoding with un-
restricted Hiero rules: 6466.7 LM queries for CKY
Hiero (with cube pruning) compared to 1500.45 LM
queries in LR Hiero (with cube pruning). While
translation quality suffers by only about 0.67 in
BLEU score on average, across two different lan-
guage pairs.
2 Left-to-Right Decoding for Hiero
Hierarchical phrase-based SMT (Chiang, 2005; Chi-
ang, 2007) uses a synchronous context free gram-
mar (SCFG), where the rules are of the form X ?
??, ??, where X is a non-terminal, ? and ? are
strings of terminals and non-terminals.
Chiang (2007) places certain constraints on the
extracted rules in order to simplify decoding. This
includes limiting the maximum number of non-
terminals (rule arity) to two and disallowing any rule
with consecutive non-terminals on the foreign lan-
guage side. It further limits the length of the initial
phrase-pair as well as the number of terminals and
non-terminals in the rule. For translating sentences
longer than the maximum phrase-pair length, the de-
coder relies on additional glue rules S ? ?X,X?
and S ? ?SX,SX? that allows monotone combi-
nation of phrases. The glue rules are used when no
rules could match or the span length is larger than
the maximum phrase-pair length.
2.1 Rule Extraction for LR Decoding
Left-to-right Hiero (Watanabe et al, 2006b) gener-
ates the target hypotheses left to right, but for syn-
chronous context-free grammar (SCFG) as used in
Hiero. The target-side rules are constrained to be
prefix lexicalized. These constrained SCFG rules
are defined as:
X ? ??,
?
b ?? (1)
where ? is a mixed string of terminals and non-
terminals. ?b is a terminal sequence prefixed to the
possibly empty non-terminal sequence ?. For the
sake of simplicity, We refer to these type of rules as
their work
students
X1
X2
X6
X4 X5
X3have
not yet
done
.
schuler ihre noch nicht gemacht haben .arbeit
students have done their workyet .not
(b)
(a)
gemacht
schuler
X1
X2
X6
X5 X4
X3 haben
noch nicht
ihre arbeit
.
1
2
3 6
45
Figure 1: (a): A word-aligned German-English sentence
pair. The bars above the source words indicate phrase-
pairs having at least two words. (b): its corresponding
left-to-right target derivation tree. Superscripts on the
source non-terminals show the indices of the rules (see
Fig 2) used in derivation.
GNF rules1 in this paper.
Rule extraction is similar to Hiero, except any
rules violating GNF form on the target side are
excluded. Rule extraction considers each smaller
source-target phrase pair within a larger phrase pair
and replaces the spans with non-terminal X , yield-
ing hierarchical rules. Figure 1(a) shows a word-
aligned German-English sentence with a phrase
pair ?ihre arbeit noch nicht gemacht haben,
have not yet done their work? that will lead to a
SCFG rule. Given other smaller phrases (marked by
bars above the source side), we extract a GNF rule2:
X ?
?X
1
noch nicht X
2
haben, have not yet X
2
X
1
?
(2)
In order to avoid data sparsity and for better gen-
eralization, Watanabe et al (2006b) adds four glue
rules for each lexical rule ? ?f, e?? which are analo-
gous to the glue rules defined in (Chiang, 2007) (see
above) except that these glue rules for LR decoding
1Griebach Normal Form (GNF), although the synchronous
grammar is not in this normal form, rather only the target side
is prefix lexicalized as if it were in GNF form.
2 LR-Hiero rule extraction excludes non-GNF rules such as
X ? ?X1 noch nicht gemacht X2, X2 not yet done X1?.
1090
allow reordering as well.
X ? ?
?
fX1, e?X1? X ? ?X1
?
fX2, e?X1X2?
X ? ?X1
?
f, e?X1? X ? ?X1
?
fX2, e?X2X1?
(3)
It might appear that the restriction that target-side
rules be GNF is a severe restriction on the cover-
age of possible hypotheses compared to the full set
of rules permitted by the Hiero extraction heuris-
tic. However there is some evidence in the liter-
ature that discontinuous spans on the source side
in translation rules is a lot more useful than dis-
continuous spans in the target side (which is disal-
lowed in the GNF). For instance, (Galley and Man-
ning, 2010) do an extensive study of discontinuous
spans on source and target side and show that source
side discontinuous spans are very useful but remov-
ing discontinuous spans on the target side only low-
ers the BLEU score by 0.2 points (using the Joshua
SMT system on Chinese-English). Removing dis-
continuous spans means that the target side rules
have the form: uX,Xu,XuX,XXu, or uXX of
which we disallow Xu,XuX,XXu. Zhang and
Zong (2012) also conduct a study on discontinuous
spans on source and target side of Hiero rules and
conclude that source discontinuous spans are always
more useful than discontinuities on the target side
with experiments on four language pairs (zh-en, fr-
en, de-en and es-en). As we shall also see in our
experimental results (see Table 4) we can get close
to the BLEU scores obtained using the full set of Hi-
ero rules by using only target lexicalized rules in our
LR decoder.
2.2 LR-Hiero Decoding
LR-Hiero decoding uses a top-down depth-first
search, which strictly grows the hypotheses in target
surface ordering. Search on the source side follows
an Earley-style search (Earley, 1970), the dot jumps
around on the source side of the rules based on the
order of nonterminals on the target side. This search
is integrated with beam search or cube pruning to
efficiently find the k-best translations.
Several important details about the algorithm of
LR-Hiero decoding are implicit and unexplained
in (Watanabe et al, 2006b). In this section we de-
scribe the LR-Hiero decoding algorithm in more de-
tail than the original description in (Watanabe et al,
Algorithm 1: LR-Hiero Decoding
1: Input sentence: f = f
0
f
1
. . . fn
2: F = FutureCost(f) (Precompute future cost for
spans)
3: for i = 0, . . . , n do
4: Si = {} (Create empty stacks)
5: h
0
= (?s?, [[0, n]], ?,F
[0,n]) (Initial hypothesis
4-tuple)
6: Add h
0
to S
0
(Push initial hyp into first Stack)
7: for i = 0, . . . , n? 1 do
8: for each h in Si do
9: [u, v] = pop(hs) (Pop first uncovered span
from list)
10: R = GetSpanRules([u, v]) (Extract rules
matching the entire span [u, v])
11: for r ? R do
12: h? = GrowHypothesis(h, r, [u, v],F) (New
hypothesis)
13: Add h? to Sl, where l = |h?cov| (Add new
hyp to stack)
14: return arg max(Sn)
15: GrowHypothesis(h, r, [u, v],F)
16: h? = (h?t = ?, h
?
s = hs, h
?
cov = ?, h
?
c = 0)
17: rX = {Xj , Xk, . . . |j C k C . . .} (Get NTs in
surface order)
18: for each X in reverse(rX) do
19: push(h?s, span(X)) (Push uncovered spans to
LIFO list)
20: h?t = Concatenate(ht, rt)
21: h?cov = UpdateCoverage(hcov, rs)
22: h?c = ComputeCost(g(h
?
),F?h?cov )
23: return h?
2006b). We explain our own modified algorithm for
LR decoding with cube pruning in Section 2.3.
Algorithm 1 shows the pseudocode for LR de-
coding. Decoding the example in Figure 1(b)
is explained using a walk-through shown in Fig-
ure 2. Each partial hypothesis h is a 4-tuple
(ht, hs, hcov, hc): consisting of a translation prefix
ht, a (LIFO-ordered) list hs of uncovered spans,
source words coverage set hcov and the hypothesis
cost hc. The initial hypothesis is a null string with
just a sentence-initial marker ?s? and the list hs con-
taining a span of the whole sentence, [0, n]. The hy-
potheses are stored in stacks S0, . . . , Sn, where each
stack corresponds to a coverage vector of same size,
covering same number of source words (Koehn et
al., 2003).
At the beginning of beam search the initial hy-
1091
? X ? schuler ihre arbeit nochnicht gemacht haben .?schuler ? X11?ihrearbeit nochnicht gemacht haben .?schuler ? X12?ihre arbeit nochnicht gemacht ? haben X 22?.?schuler X 13?ihrearbeit ? nochnicht ? X23?gemacht? haben X 22?.?schuler ? X13 ?ihre arbeit? nochnicht gemacht haben X 22?.?schuler ihre arbeit nochnicht gemacht haben ?X 22?.?schuler ihre arbeit nochnicht gemacht haben .
1) X??schuler X1/ students X1?2) X??X1heban X 2/have X 1X 2?3 )X??X 1nochnicht X2/not yet X 2X 1?4 ) X??gemacht /done ?5 )X?? ihre arbeit / their work ?6 )X?? ./ . ?
[0,8]students [1,8 ]students have [1,6 ][7,8]students have not yet [5,6] [1,3 ][7,8]students have not yet done [1,3 ][7,8]students have not yet done their work [7,8]students have not yet done their work .
rules source side coverage hypothesis
GG <s><s><s><s><s><s>
<s>
</s>
Figure 2: Illustration of the LR-Hiero decoding process in Figure 1. (a) Rules pane show the rules used in the derivation
(glue rules are marked byG) (b) Decoder state using Earley dot notation (superscripts show rule#) (c) Hypotheses pane
showing translation prefix and ordered list of yet-to-be-covered spans.
pothesis h0 is added to the decoder stack S0 (line 6
in Algoorithm 1). Hypotheses in each decoder stack
are expanded iteratively, generating new hypotheses,
which are added to the latter stacks corresponding to
the number of source words covered. In each step it
pops from the LIFO list hs, the span [u, v] of the
next hypothesis h to be processed.
All rules that match the entire span [u, v] are then
obtained efficiently via pattern matching (Lopez,
2007). GetSpanRules addresses possible ambigui-
ties in matched rules to the given span [u, v]. For
example, given a rule r, with source side rs :
?X1 the X2? and source phrase p : ?ok, the more
the better?. There is ambiguity in matching r to
p. GetSpanRules returns a distinct matched rule for
each possible matching.
The GrowHypothesis routine creates a new can-
didate by expanding given hypothesis h using rule
r and computes the complete hypothesis score in-
cluding language model score. Since the target-side
rules are in GNF, the translation prefix of the new
hypothesis is obtained by simply concatenating the
terminal prefixes of h and r in same order (line 20).
UpdateCoverage updates source word coverage set
using the source side of r. The hs list is built by
pushing the non-terminal spans of rule r in a reverse
order (lines 17 and 18). The reverse ordering main-
tains the left-to-right generation of the target side.
In the walk-through in Figure 2, the derivation
process starts by expanding the initial hypothesis h0
(first item in the right pane of Fig 2) with the rule
(rule #1 in left pane) to generate a new partial candi-
date having a terminal prefix of ?s? students (second
item in right pane). The second item in the middle
pane shows the current position of the parser em-
ploying Earley?s dot notation, indicating that the first
word has already been translated. Now the decoder
considers the second hypothesis and pops the span
[1, 8]. It then matches the rule (#2) and pushes the
spans [1, 6] and [7, 8] into the list hs in the reverse
order of their appearance in the target-side rule. At
each step the new hypothesis is added to the decoder
stack Sl depending on the number of covered words
in the new hypothesis (line 13 in Algorithm 1).
For pruning we use an estimate of the future cost3
of the spans uncovered by current hypothesis to-
gether with the hypothesis cost. The future cost is
precomputed (line 2 Algorithm 1) in a way simi-
lar to the phrase-based models (Koehn et al, 2007)
using only the terminal rules of the grammar. The
ComputeCost method (line 22 in Algorithm 1) uses
the usual log-linear model and scores a hypothesis
based on its different feature scores g(h?) and the
future cost of the yet to be covered spans (F?h?cov ).
Time complexity of left to right Hiero decoding with
beam search is O(n2b) in practice where n is the
length of source sentence and b is the size of beam
(Huang and Mi, 2010).
2.3 LR-Hiero Decoding with Cube Pruning
The Algorithm 1 presented earlier does an ex-
haustive search as it generates all possible partial
translations for a given stack that are reachable from
the hypotheses in previous stacks. However only a
few of these hypotheses are retained, while majority
of them are pruned away. The cube pruning tech-
nique (Chiang, 2007) avoids the wasteful generation
of poor hypotheses that are likely to be pruned away
by efficiently restricting the generation to only high
scoring partial translations.
We modify the cube pruning for LR-decoding
that takes into account the next uncovered span to
3 Watanabe et al (2006b) also use a similar future cost, even
though it is not discussed in the paper (p.c.).
1092
Algorithm 2: LR-Hiero Decoding with Cube Pruning
1: Input sentence: f = f
0
f
1
. . . fn
2: F = FutureCost(f) (Precompute future cost for
spans)
3: S
0
= {} (Create empty initial stack)
4: h
0
= (?s?, [[0, n]], ?,F
[0,n]) (Initial hypothesis
4-tuple)
5: Add h
0
to S
0
(Push initial hyp into first Stack)
6: for i = 1, . . . , n do
7: cubeList = {} (MRL is max rule length)
8: for p = max(i? MRL, 0), . . . , i? 1 do
9: {G} = Grouped(Sp) (Group based on the first
uncovered span)
10: for g ? {G} do
11: [u, v] = gspan
12: R = GetSpanRules([u, v])
13: for Rs ? R do
14: cube = [ghyps, Rs]
15: Add cube to cubeList
16: Si = Merge(cubeList,F) (Create stack Si and
add new hypotheses to it, see Figure 3)
17: return arg max(Sn)
18: Merge(CubeList,F)
19: heapQ = {}
20: for each (H,R) in cubeList do
21: [u, v] = span of rule R
22: h? = GrowHypothesis(h
1
, r
1
, [u, v],F) (from
Algorithm 1)
23: push(heapQ, (h?c, h
?
, [H,R])
24: hypList = {}
25: while |heapQ| > 0 and |hypList| < K do
26: (h?c, h
?
, [H,R]) = pop(heapQ)
27: push(heapQ,GetNeighbours([H,R])
28: Add h? to hypList
29: return hypList
be translated indicated by the Earley?s dot nota-
tion. The Algorithm 2 shows the pseudocode for
LR-decoding using cube pruning. The structure of
stacks and hypotheses and computing the future cost
is similar to Algorithm 1 (lines 1-5). To fill stack
Si, it iterates over previous stacks (line 8 in Algo-
rithm 2) 4. All hypotheses in each stack Sp (cov-
ering p words on the source-side) are first parti-
tioned into a set of groups, {G}, based on their
first uncovered span (line 9) 5. Each group g is a
4As the length of rules are limited (at most MRL), we can
ignore stacks with index less than i? MRL
5The beam search decoder in Phrase-based system (Huang
and Chiang, 2007; Koehn et al, 2007; Sankaran et al, 2010)
2-tuple (gspan, ghyps), where ghyps is a list of hy-
potheses which share the same first uncovered span
gspan. Rules matching the span gspan are obtained
from routine GetSpanRules, which are then grouped
based on unique source side rules (i.e. each Rs con-
tains rules that share the same source side s but have
different target sides). Each ghyps and possible Rs6
create a cube which is added to cubeList.
In LR-Hiero, each hypothesis is developed with
only one uncovered span, therefore each cube al-
ways has just two dimensions: (1) hypotheses with
the same number of covered words and similar first
uncovered span, (2) rules sharing the same source
side. In Figure 3(a), each group of hypotheses,
ghyps, is shown in a green box (in stacks), and each
rectangle on the top is a cube. Figure 3 is using the
example in Figure 2.
The Merge routine is the core function of cube
pruning which generates the best hypotheses from
all cubes (Chiang, 2007). For each possible cube,
(H,R), the best hypothesis is generated by calling
GrowHypothesis(h1, r1, span,F) where h1 and
r1 are the best hypothesis and rule in H and R re-
spectively (line 22). Figure 3 (b) shows a more de-
tailed view of a cube (shaded cube in Figure 3(a)).
Rows are hypotheses and columns are rules which
are sorted based on their scores.
The first best hypotheses, h?, along with their
score, h?c and corresponding cube, (H,R) are
placed in a priority queue, heapQ (triangle in Fig-
ure 3). Iteratively the best hypothesis is popped
from the queue (line 26) and its neighbours in
the cube are added to the priority queue (using
GetNeighbours([H,Q])). It continues to generate
all K best hypotheses. Using cube pruning tech-
nique, each stack is filled with K best hypotheses
without generating all possible hypotheses in each
cube.
groups the hypotheses in a given stack based on their coverage
vector. But this idea does not work in LRHiero decoding in
which the expansion of each hypothesis is restricted to its first
uncovered span. We have also tried another way of grouping
hypotheses: group by all uncovered spans, hs. Our experiments
did not show any significant difference between the final results
(BLEU score), therefore we decided to stick to the simpler idea:
using first uncovered span for grouping.
6Note that, just rules whose number of terminals in their
source side is equal to i? p can be used.
1093
...
1 2 3 4 5
[1,8][1,8][0,3][0,3][5,8]
[1,6][1,6][1,6][0,3][0,3]
[5,6][5,6][1,4][6,8][6,8]
[5,6][5,6][5,6][1,3][7,8]
[1,3]
thei
trer
the thew
theo tier
tiek
toeitreo
rehtetseudnX126nd453a246vn4y2n4.ocwl4tseh
dnX126n453d46vn43gm231y4.ocwl4thek
bXb(gd453a246vn4y2n4.ocwl4tt o
)312 1v62 1v
trer
the thew
theo tier
tiek
toeitreo
thei
(a) (b)
Figure 3: Example of generating hypotheses in cube pruning using Figure 2: (a) Hypotheses in previous stacks are
grouped based on their first uncovered span, and build cubes (grids on top). Cubes are in different sizes because
of different number of rules and group sizes. Cubes are fed to a priority queue (triangle) and new hypotheses are
iteratively popped from the queue and added to the current stack, S
5
. (b) Generating hypotheses from a cube. The top
side of the grid denotes the target side of rules sharing the same source side (Rs) along with their scores. Left side of
the grid shows the hypotheses in a same group, their first uncovered span and their scores. Hypothesis generated from
row 1 and column 1 is added to the queue at first. Once it is popped from the queue, its neighbours (in the grid) are
subsequently added to the queue.
Figure 3 (b) shows the derivation of the two best
hypotheses from the cube. The best hypothesis of
this cube which is likely created from the best hy-
pothesis and rule (left top most entry) is popped
at first step. Then, GetNeighbours calls GrowHy-
pothesis to generate next potential best hypotheses
of this cube (neighbours of the popped entry which
are shaded in Figure 3(b)). These hypotheses are
added to the priority queue. In the next iteration, the
best hypothesis is popped from all candidates in the
queue and algorithm continues.
3 Features
We use the following standard SMT features for the
log-linear model of LR-Hiero: relative-frequency
translation probabilities p(f |e) and p(e|f), lexical
translation probabilities pl(f |e) and pl(e|f), a lan-
guage model probability, word count and phrase
count. In addition we also use the glue rule count
and the two reordering penalty features employed
by Watanabe et al (2006b; 2006a). These features
compute the height and width (span size of the en-
tire subtree) of all subtrees which are backtraced in
the derivation of a hypothesis. A non-terminal Xi
is pushed into the LIFO list of a partial hypothesis;
it?s backtrace refers to the set of NTs that must be
popped before Xi.
In Figure 1(b), X2 has two subtrees X3 and X6,
where X3 should be processed before X6. The sub-
tree rooted atX3 in Figure 1(b) has a height of 2 and
span [1, 6] having a width of 5. Similarly, X4 should
be backtraced beforeX5 and has height and width of
1. Backtracing applies only for rules having at least
two non-terminals. Thus the total height and width
penalty for this derivation are 3 and 6 respectively.
However, the height and width features do not
distinguish between a rule that reorders the non-
terminals in source and target from one that pre-
serves the ordering. Rules #2 and #3 in Figure 2
are treated equally although they have different or-
derings. The decoder is thus agnostic to this dif-
ference and would not be able to exploit this ef-
fectively to control reordering and instead would
rely on the partial LM score. This issue is exac-
erbated for glue rules, where the decoder has to
choose from different possibilities without any way
to favour one over the others. Instead of the rule
#2, the decoder could use its reordered version
?X1 haben X2, have X2 X1? leading to a poor
translation.
1094
The features we introduce can be used to learn
if the model should favour monotone translations at
the cost of re-orderings or vice versa and hence can
easily adapt to different language pairs. Further, our
experiments (see Section 4) suggest that the features
h andw are not sufficient by themselves to model re-
ordering for language pairs exhibiting very different
syntactic structure.
3.1 Distortion Features
Our distortion features are inspired by their name-
sake in phrase-based system, with some modifica-
tions to adapt the idea for the discontiguous phrases
in LR-Hiero grammar.
r : hf
1
X
1
f
2
X
2
f
3
, tX
2
X
1
i I = [`, f
1
, f
2
, f
3
, X
2
, X
1
,a]
f2 f3 X1 f1 X2 (a)
r : ? X1noch nicht X 2/not yet X2 X1?
I=[(1,1) ,(3,5) ,(5,6) ,(1,3) ,(6,6)]
.1ihre2arbeit3noch4nicht5gemacht 6 (b)
Figure 4: (a) Distortion feature computation using a rule
r. (b) Example of distortion computation for applying r
3
on phrase ?ihre arbeit noch nicht gemacht haben?. sub-
scripts between words show the indices which are used to
build I . Distortion would be: d = 2 + 0 + 5 + 3.
Consider a rule r = ??,?b ??, with the source
term ? being a mixed string of terminals and non-
terminals. Representing the non-terminal spans and
each sequence of terminals in ? as distinct items, our
distortion feature counts the total length of jumps be-
tween the items during Earley parsing.
Figure 4 (a) explains the computation of our dis-
tortion feature for an example rule r. Let I =
[I0, . . . , Ik] be the items denoting the terminal se-
quences and non-terminal spans with I0 and Ik be-
ing dummy items (` and a in Fig) marking the left
and right indices of the rule r in input sentence f .
Other items are arranged by their realization order
on the target-side with the terminal sequences pre-
ceding non-terminal spans. The items for the exam-
ple rule are shown in Figure 4 (a). The distortion
feature is computed as follows:
d(r) =
k?
j=1
|I
L
j ? I
R
j?1| (4)
where superscripts refer to position of left (L) and
right (R) edge of each item in the source sentence
f . These are then aggregated across the rules of a
derivation D as: d =
?
r?D d(r). For each item
Ij , we count the jump from the end of previous item
to the beginning of the current. In Figure 4 (a) the
jumps are indicated by the arrows above the rule.
Figure 4 (b) shows an example of distortion com-
putation for r3 and phrase ?ihre arbeit noch nicht
gemacht haben? from Figure 2.
Since the glue rules are likely to be used in the top
levels (possibly with large distortion) of the deriva-
tion, we would want the decoder to learn the distor-
tion for regular and glue rules separately. We thus
use two distortion features for the two rule types and
we call them dp and dg.
These features do not directly model the source-
target reordering, but only capture the source-side
jumps. Furthermore they apply for both monotone
and reordering rules. We now introduce a new fea-
ture for exclusively modelling the reordering.
3.2 Reordering Feature
This feature simply counts the number of reordering
rules, where the non-terminals in source and target
sides are reordered. Thus r?? = rule(D, ??), where
rule(D, ??) is the number of reordering rules in D.
Similar to width and height, this feature is applied
for rule having at least two non-terminals. This fea-
ture is applied to regular and glue rules.
4 Experiments
We conduct different types of experiments to evalu-
ate LR-Hiero decoding developed by cube pruning
and integrating new features into LR-Hiero system
for two language pairs: German-English (de-en) and
Czech-English (cs-en).Table 1 shows the dataset de-
tails.
4.1 System Setup
In our experiments we use four baselines as well
as our implementation of LR-Hiero (written in
Python):
1095
Corpus Train/Dev/Test
cs-en Europarl(v7), CzEng(v0.9);
News commentary
7.95M/3000/3003
de-en Europarl(v7); News
commentary
1.5M/2000/2000
Table 1: Corpus statistics in number of sentences
Model cs-en de-en
Phrase-based 233.0 77.2
Hiero 1,961.6 858.5
LR-Hiero 230.5 101.3
Table 2: Model sizes (millions of rules). We do not count
glue rules for LR-Hiero which are created at runtime as
needed.
? Hiero: we used Kriya, our open-source im-
plementation of Hiero in Python, which per-
forms comparably to other open-source Hiero
systems (Sankaran et al, 2012). Kriya can
obtain statistically significantly equal BLEU
scores when compared with Moses (Koehn et
al., 2007) for several language pairs (Razmara
et al, 2012; Callison-Burch et al, 2012).
? Hiero-GNF: where we use Hiero decoder with
the restricted LR-Hiero grammar (GNF rules).
? LR-Hiero: our implementation of LR-Hiero
(Watanabe et al, 2006b) in Python.
? phrase-based: Moses (Koehn et al, 2007)
? LR-Hiero+CP: LR-Hiero decoding with cube
pruning.
We use a 5-gram LM trained on the Gigaword cor-
pus and use KenLM (Heafield, 2011) for LM scor-
ing during decoding. We tune weights by minimiz-
ing BLEU loss on the dev set through MERT (Och,
2003) and report BLEU scores on the test set. We
use comparable pop limits in each of the decoders:
1000 for Moses and LR-Hiero and 500 with cube
pruning for CKY Hiero and LR-Hiero+CP. Other
extraction and decoder settings such as maximum
phrase length, etc. were identical across settings so
that the results are comparable.
Table 2 shows how the LR-Hiero grammar is
much smaller than CKY-based Hiero.
Model cs-en de-en
#queries / time(ms) #queries / time(ms)
Hiero 5,679.7 / 16.12 7,231.62 / 20.33
Hiero-GNF 4,952.5 / 14.71 5,858.74 / 18.23
LR-Hiero (1000) 46,333.21 / 163.6 83,518.63 / 328.11
LR-Hiero (500) 24,141.03 / 97.61 42,783.12 / 192.23
LR-Hiero+CP 1,303.2 / 4.2 1,697.7 / 5.67
Table 3: Comparing average number and time of lan-
guage model queries.
4.2 Time Efficiency Comparison
To evaluate the performance of LR-Hiero decod-
ing with cube pruning (LR-Hiero+CP), we compare
it with three baselines: (i) CKY Hiero, (ii) CKY
Hiero-GNF, and (iii) LR-Hiero (without cube prun-
ing) with two different beam size 500 and 1000.
When it comes to instrument timing results, there are
lots of system level details that we wish to abstract
away from, and focus only on the number of ?edges?
processed by the decoder. In comparison of parsing
algorithms, the common practice is to measure the
number of edges processed by different algorithms
for the same reason (Moore and Dowding, 1991).
By analogy to parsing algorithm comparisons, we
compare the different decoding algorithms with re-
spect to the number of calls made to the language
model (LM) since that directly corresponds to the
number of hypotheses considered by the decoder.
A decoder is more time efficient if it can consider
fewer translation hypotheses while maintaining the
same BLEU score. All of the baselines use the same
wrapper to query the language model, and we have
instrumented the wrapper to count the statistics we
need and thus we can say this is a fair comparison.
For this experiment we use a sample set of 50 sen-
tences taken from the test sets.
Table 3 shows the results in terms of average num-
ber of language model queries and times in millisec-
onds.
4.3 Reordering Features
To evaluate the new reordering features proposed
to LR-Hiero (Section 3.2), LR-Hiero+CP with new
features is compared to all baselines. Table 4 shows
the BLEU scores of different models in two lan-
guage pairs. The baseline (Watanabe et al, 2006b)
model uses all the features mentioned therein but is
1096
Model cs-en de-en
Phrase-based 20.32 24.71
CKY Hiero 20.64 25.52
CKY Hiero-GNF 20.04 24.84
LR-Hiero 18.30 23.47
LR-Hiero + reordering feats 20.20 24.90
LR-Hiero + CP + reordering feats 20.15 24.83
CKY Hiero-GNF + reordering feats 20.52 25.09
CKY Hiero + reordering feats 20.77 25.72
Table 4: BLEU scores. The rows are grouped such that
each group use the same model. The last row in part 2 of
table shows LR-Hiero+CP using our new features in ad-
dition to the baseline Watanabe features (line LR-Hiero
baseline). The last part shows CKY Hiero using new re-
ordering features. The reordering features used are dp, dg
and r??. LR-Hiero+CP has a beam size of 500 while LR-
Hiero has a beam size of 1000, c.f. with the LM calls
shown in Table 3.
worse than both phrase-based and CKY-Hiero base-
lines by up to 2.3 BLEU points.
All the reported results are obtained from a single
optimizer run. However we observed insignificant
changes in different tuning runs in our experiments.
We find a gain of about 1 BLEU point when we add
a single distortion feature d and a further gain of
0.3 BLEU (not shown due to lack of space) when
we split the distortion feature for the two rule types
(dp and dg). The last line in part two of Table 4
shows a consistent gain of 1.6 BLEU over the LR-
Hiero baseline for both language pairs. It shows that
LR-Hiero maintains the BLEU scores obtained by
?phrase-based? and ?CKY Hiero-GNF?.
We performed statistical significance tests us-
ing two different tools: Moses bootstrap resam-
pling and MultEval (Clark et al, 2011). The dif-
ference between ?LR-Hiero+CP+reordering feat?
and three baselines: ?phrase-based?, ?CKY Hiero-
GNF?, ?LR-Hiero+reordering feat? are not statis-
tically significant even for p-value of 0.1 for both
tools.
To investigate the impact of proposed reordering
features with other decoder or models. We add these
features to both Hiero and Hiero-GNF7. The last
part of Table 4 shows the performance CKY decoder
7Feature r?? is defined for SCFG rules and cannot be
adopted to phrase-based translation systems; and Moses uses
distortion feature therefore we omit Moses from this experi-
ment.
with different models (full Hiero and GNF) with the
new reordering features in terms of BLEU score.
The results show that these features are helpful in
both models. Although, they do not make a big dif-
ference in Hiero with full model, they can alleviate
the lack of non-GNF rules in Hiero-GNF.
Nguyen and Vogel (2013) integrate traditional
phrase-based features: distortion and lexicalized re-
ordering into Hiero as well. They show that such
features can be useful to boost the translation quality
of CKY Hiero with the full rule set. Nguyen and Vo-
gel (2013) compute the distortion feature in a differ-
ent way, only applicable to CKY. The distortion for
each cell is computed after the translation for non-
terminal sub-spans is complete. In LR-decoding,
we compute distortion for rules even though we are
yet to translate some of the sub-spans. Thus our ap-
proach computes the distortion incrementally for the
untranslated sub-spans which are later added. Un-
like (Nguyen and Vogel, 2013), our distortion fea-
ture can be applied to both LR and CKY-decoding
(Table 4). We have also introduced another reorder-
ing feature (Section 3.2) not proposed previously.
5 Conclusion and Future Work
We provided a detailed description of left-to-right
Hiero decoding, many details of which were only
implicit in (Watanabe et al, 2006b). We presented
an augmented LR decoding algorithm that builds on
the original algorithm in (Watanabe et al, 2006b)
but unlike that algorithm, using experiments over
multiple language pairs we showed two new results:
(i) Our LR decoding algorithm provides demonstra-
bly more efficient decoding than CKY Hiero and the
original LR decoding algorithm in (Watanabe et al,
2006b). And, (ii) by introducing new distortion and
reordering features for LR decoding we show that
it maintains the BLEU scores obtained by phrase-
based and CKY Hiero-GNF.
CKY Hiero uses standard Hiero-style translation
rules capturing better reordering model than prefix
lexicalized target-side translation rules used in LR-
Hiero. Our LR-decoding algorithm is 4 times faster
in terms of LM calls while translation quality suffers
by about 0.67 in BLEU score on average.
Unlike Watanabe et al (2006b), our new features
can easily adapt to the reordering requirements of
different language pairs. We also introduce the use
1097
of future cost in decoding algorithm which is an es-
sential part in decoding. We have shown in this pa-
per that left-to-right (LR) decoding can be consid-
ered as a potential faster alternative to CKY decod-
ing for Hiero-style machine translation systems.
In future work, we plan to apply lexicalized re-
ordering models to LR-Hiero. It has been shown to
be useful for Hiero in some languages therefore it
is promising to improve translation quality in LR-
Hiero which suffers from lack of modeling power
of non-GNF target side rules. We also plan to ex-
tend the glue rules in LR-Hiero to provide a bet-
ter reordering model. We believe such an exten-
sion would be very effective in reducing search er-
rors and capturing better reordering models in lan-
guage pairs involving complex reordering require-
ments like Chinese-English.
Acknowledgments
This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Fac-
ulty Award to the third author. The authors wish
to thank Taro Watanabe and Marzieh Razavi for
their valuable discussions and suggestions, and the
anonymous reviewers for their helpful comments.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In In ACL, pages
263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume
2, HLT ?11, pages 176?181, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Commun. ACM, 13(2):94?102, February.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 966?
974, Los Angeles, California, June. Association for
Computational Linguistics.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left language
model state for syntactic machine translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation, pages 183?190, San Francisco,
California, USA, 12.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2013.
Grouping language model boundary words to speed K-
Best extraction from hypergraphs. In Proceedings of
the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, Atlanta, Georgia, USA,
6.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In In Proc. of the Sixth Work-
shop on Statistical Machine Translation.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
In ACL 07.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 273?283, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL, pages
976?985.
Robert C. Moore and John Dowding. 1991. Efficient
bottom-up parsing. In HLT. Morgan Kaufmann.
Thuylinh Nguyen and Stephan Vogel. 2013. Integrat-
ing phrase-based reordering features into chart-based
decoder for machine translation. In Proc. of ACL.
1098
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Majid Razmara, Baskaran Sankaran, Ann Clifton, and
Anoop Sarkar. 2012. Kriya - the sfu system for trans-
lation task at wmt-12. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, WMT
?12, pages 356?361, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.
2010. Incremental decoding for phrase-based statis-
tical machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 216?223, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya - an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics (PBML), (97):83?98, apr.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2006a. NTT statistical machine translation
for iwslt 2006. In Proceedings of IWSLT 2006, pages
95?102.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006b. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of ACL.
Jiajun Zhang and Chenqqing Zong. 2012. A Compar-
ative Study on Discontinuous Phrase Translation. In
NLPCC 2012, pages 164?175.
1099
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 221?226,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Two Improvements to Left-to-Right Decoding for Hierarchical
Phrase-based Machine Translation
Maryam Siahbani and Anoop Sarkar
School of Computing Science
Simon Fraser University
Burnaby BC. Canada
msiahban,anoop@cs.sfu.ca
Abstract
Left-to-right (LR) decoding (Watanabe et
al., 2006) is promising decoding algorithm
for hierarchical phrase-based translation
(Hiero) that visits input spans in arbitrary
order producing the output translation in
left to right order. This leads to far fewer
language model calls, but while LR decod-
ing is more efficient than CKY decoding,
it is unable to capture some hierarchical
phrase alignments reachable using CKY
decoding and suffers from lower transla-
tion quality as a result. This paper in-
troduces two improvements to LR decod-
ing that make it comparable in translation
quality to CKY-based Hiero.
1 Introduction
Hierarchical phrase-based translation (Hi-
ero) (Chiang, 2007) uses a lexicalized syn-
chronous context-free grammar (SCFG) extracted
from word and phrase alignments of a bitext. De-
coding for Hiero is typically done with CKY-style
decoding with time complexity O(n
3
) for source
input with n words. Computing the language
model score for each hypothesis within CKY de-
coding requires two histories, the left and the right
edge of each span, due to the fact that the target
side is built inside-out from sub-spans (Heafield
et al., 2011; Heafield et al., 2013).
LR-decoding algorithms exist for phrase-
based (Koehn, 2004; Galley and Manning, 2010)
and syntax-based (Huang and Mi, 2010; Feng et
al., 2012) models and also for hierarchical phrase-
based models (Watanabe et al., 2006; Siahbani et
al., 2013), which is our focus in this paper.
Watanabe et al. (2006) first proposed left-to-
right (LR) decoding for Hiero (LR-Hiero hence-
forth) which uses beam search and runs in O(n
2
b)
in practice where n is the length of source sentence
and b is the size of beam (Huang and Mi, 2010).
To simplify target generation, SCFG rules are con-
strained to be prefix-lexicalized on target side aka
Griebach Normal Form (GNF). Throughout this
paper we abuse the notation for simplicity and use
the term GNF grammars for such SCFGs. This
constraint drastically reduces the size of gram-
mar for LR-Hiero in comparison to Hiero gram-
mar (Siahbani et al., 2013). However, the orig-
inal LR-Hiero decoding algorithm does not per-
form well in comparison to current state-of-the-art
Hiero and phrase-based translation systems. Siah-
bani et al. (2013) propose an augmented version
of LR decoding to address some limitations in the
original LR-Hiero algorithm in terms of transla-
tion quality and time efficiency.
Although, LR-Hiero performs much faster than
Hiero in decoding and obtains BLEU scores com-
parable to phrase-based translation system on
some language pairs, there is still a notable gap be-
tween CKY-Hiero and LR-Hiero (Siahbani et al.,
2013). We show in this paper using instructive ex-
amples that CKY-Hiero can capture some complex
phrasal re-orderings that are observed in language
pairs such as Chinese-English that LR-Hiero can-
not (c.f. Sec.3).
We introduce two improvements to LR decod-
ing of GNF grammars: (1) We add queue diversity
to the cube pruning algorithm for LR-Hiero, and
(2) We extend the LR-Hiero decoder to capture all
the hierarchical phrasal alignments that are reach-
able in CKY-Hiero (restricted to using GNF gram-
mars). We evaluate our modifications on three
language pairs and show that LR-Hiero can reach
the translation scores comparable to CKY-Hiero in
two language pairs, and reduce the gap between
Hiero and LR-Hiero on the third one.
2 LR Decoding with Queue Diversity
LR-Hiero uses a constrained lexicalized SCFG
which we call a GNF grammar: X ? ??,
?
b ??
where ? is a string of non-terminal and terminal
symbols,
?
b is a string of terminal symbols and ? is
a possibly empty sequence of non-terminals. This
ensures that as each rule is used in a derivation,
221
Algorithm 1: LR-Hiero Decoding
1: Input sentence: f = f
0
f
1
. . . f
n
2: F = FutureCost(f) (Precompute future cost
1
for spans)
3: S
0
= {} (Create empty initial stack)
4: h
0
= (?s?, [[0, n]], ?,F
[0,n]
) (Initial hypothesis 4-tuple)
5: Add h
0
to S
0
(Push initial hyp into first Stack)
6: for i = 1, . . . , n do
7: cubeList = {} (MRL is max rule length)
8: for p = max(i? MRL, 0), . . . , i? 1 do
9: {G} = Grouped(S
p
) (based on the first uncovered
span)
10: for g ? {G} do
11: [u, v] = g
span
12: R = GetSpanRules([u, v])
13: for R
s
? R do
14: cube = [g
hyps
, R
s
]
15: Add cube to cubeList
16: S
i
= Merge(cubeList,F) (Create stack S
i
and add
new hypotheses to it, see Figure 1)
17: return argmax(S
n
)
18: Merge(CubeList,F)
19: heapQ = {}
20: for each (H,R) in cubeList do
21: hypList = getBestHypotheses((H,R),F , d) (d
best hypotheses of each cube)
22: for each h
?
in hypList do
23: push(heapQ, (h
?
c
, h
?
, [H,R]) (Push new hyp
in queue)
24: hypList = {}
25: while |heapQ| > 0 and |hypList| < K do
26: (h
?
c
, h
?
, [H,R]) = pop(heapQ) (pop the best
hypothesis)
27: push(heapQ,GetNeighbours([H,R]) (Push
neighbours to queue)
28: Add h
?
to hypList
29: return hypList
the target string is generated from left to right.
The rules are obtained from a word and phrase
aligned bitext using the rule extraction algorithm
in (Watanabe et al., 2006).
LR-Hiero decoding uses a top-down depth-first
search, which strictly grows the hypotheses in tar-
get surface ordering. Search on the source side
follows an Earley-style search (Earley, 1970), the
dot jumps around on the source side of the rules
based on the order of nonterminals on the target
side. This search is integrated with beam search
or cube pruning to find the k-best translations.
Algorithm 1 shows the pseudocode for LR-
Hiero decoding with cube pruning (Chiang, 2007)
(CP). LR-Hiero with CP was introduced in (Siah-
bani et al., 2013). In this pseudocode, we have in-
troduced the notion of queue diversity (explained
below). However to understand our change we
need to understand the algorithm in more detail.
1
The future cost is precomputed in a way similar to the
phrase-based models (Koehn et al., 2007) using only the ter-
minal rules of the grammar.
9.18.2
8.3 8.58.05
8.1 8.48.68.88.9
3.21.30.9 6.66.76.9
8.9
7.1 8.58.7
9.39.08.17.2
1.51.31.26.76.86.9
...
S i
Figure 1: Cubes (grids) are fed to a priority queue (trian-
gle) and generated hypotheses are iteratively popped from the
queue and added to stack S
i
. Lower scores are better. Scores
of rules and hypotheses appear on the top and left side of the
grids respectively. Shaded entries are hypotheses in the queue
and black ones are popped from the queue and added to S
i
.
Each source side non-terminal is instantiated with
the legal spans given the input source string, e.g.
if there is a Hiero rule ?aX
1
, a
?
X
1
? and if a only
occurs at position 3 in the input then this rule can
be applied to span [3, i] for all i, 4 < i ? n for in-
put of length n and source side X
1
is instantiated
to span [4, i]. A worked out example of how the
decoder works is shown in Figure 2. Each partial
hypothesis h is a 4-tuple (h
t
, h
s
, h
cov
, h
c
): con-
sisting of a translation prefix h
t
, a (LIFO-ordered)
list h
s
of uncovered spans, source words coverage
set h
cov
and the hypothesis cost h
c
. The initial hy-
pothesis is a null string with just a sentence-initial
marker ?s? and the list h
s
containing a span of the
whole sentence, [0, n]. The hypotheses are stored
in stacks S
0
, . . . , S
n
, where S
p
contains hypothe-
ses covering p source words just like in stack de-
coding for phrase-based SMT (Koehn et al., 2003).
To fill stack S
i
we consider hypotheses in each
stack S
p
2
, which are first partitioned into a set of
groups {G}, based on their first uncovered span
(line 9). Each group g is a 2-tuple (g
span
, g
hyps
),
where g
hyps
is a list of hypotheses which share the
same first uncovered span g
span
. Rules matching
the span g
span
are obtained from routine GetSpan-
Rules. Each g
hyps
and possible R
s
create a cube
which is added to cubeList.
The Merge routine gets the best hypotheses
from all cubes (see Fig.1). Hypotheses (rows) and
columns (rules) are sorted based on their scores.
GetBestHypotheses((H,R),F , d) uses current
hypothesis H and rule R to produce new hypothe-
ses. The first best hypothesis, h
?
along with its
score h
?
c
and corresponding cube (H,R) is placed
in a priority queue heapQ (triangle in Figure 1
and line 23 in Algorithm 1). Iteratively the K best
2
As the length of rules are limited (at most MRL), we can
ignore stacks with index less than i? MRL
222
rules
hypotheses
?s?[0, 15]
G 1)?Taiguo shi X
1
/Thailand X
1
? ?s? Thailand [2,15]
G 2)?yao X
1
/wants X
1
?
G 3)?liyong X
1
/to utilize X
1
?
4)?zhe bi qian X
1
/this money X
1
?
5)?X
1
zhuru geng duo X
2
/to inject more X
2
X
1
?
6)?liudong X
1
/circulating X
1
?
G 7)?zijin X
1
/capital X
1
?
8)?./.?
9)?xiang jingji/to the economy?
?s?Thailand wants [3,15]
?s?Thailand wants to utilize [4,15]
?s?Thailand wants to utilize this money [7,15]
?s?Thailand wants to utilize this money to inject more [12,15][7,9]
?s?Thailand wants to utilize this money to inject more circulating [13,15][7,9]
?s?Thailand wants to utilize this money to inject more circulating capital [14,15][7,9]
?s?Thailand wants to utilize this money to inject more circulating capital . [7,9]
?s?Thailand wants to utilize this money to inject more circulating capital . to the economy?/s?
Figure 2: The process of translating the Chinese sentence in Figure 3(b) in LR-Hiero. Left side shows the rules used in the
derivation (G indicates glue rules as defined in (Watanabe et al., 2006)). The hypotheses column shows the translation prefix
and the ordered list of yet-to-be-covered spans.
T? b ch ng shu  ,? ? ? li?nh? zh?ngf? , b?ngqi? y u n?ngl?? gu?nch? .m?qi?n
He added that the coalition government carrying out the economic reform plancapable ofand
j?ngj? g ig?  j?hu??
is now in stable .
X1
condition
zhu?ngku?ng w?nd?ng 0      1                   2            3   4               5                    6                      7                                  8                         9   10                11        12              13                     14             15             16            17      18
(a)
T?igu? sh?  y?o zh? b? qi?n xi?ng j?ngj? zh?r? g?ng du? .l?y?ng
Thailand wants to circulating capital to the economyinject morethis money to
li?d?ng z?j?n
utilize .
S 1 S 20               1          2              3                  4            5     6            7                 8             9               10           11         12                    13           14      15
(b)
Figure 3: Two Chinese-English sentence pairs from devset data in experiments. (a) Correct rule cannot be matched to [6,18],
our modifications match the rule to the first subspan [6,9] (b) LR-Hiero detects a wrong span for X
2
[12,15], we modify the
rule matching match X
2
to all subspans [12,13], [12,14] and [12,15], corresponding to 3 hypotheses.
hypotheses in the queue are popped (line 26) and
for each hypothesis its neighbours in the cube are
added to the priority queue (line 27). Decoding
finishes when stack S
n
has been filled.
The language model (LM) score violates the
hypotheses generation assumption of CP and can
cause search errors. In Figure 1, the topmost
and leftmost entry of the right cube has a score
worse than many hypotheses in the left cube due
to the LM score. This means the right cube
has hypotheses that are ignored. This type of
search error hurts LR-Hiero more than CKY-
Hiero, due to the fact that hypotheses scores in
LR-Hiero rely on a future cost, while CKY-Hiero
uses the inside score for each hypothesis. To
solve this issue for LR-Hiero we introduce the no-
tion of queue diversity which is the parameter d
in GetBestHypotheses((H,R),F , d). This pa-
rameter guarantees that each cube will produce at
least d candidate hypotheses for the priority queue.
d=1 in standard cube pruning for LR-Hiero (Siah-
bani et al., 2013). We apply the idea of diver-
sity at queue level, before generating K best hy-
pothesis, such that the GetBestHypotheses rou-
tine generates d best hypotheses from each cube
and all these hypotheses are pushed to the prior-
ity queue (line 22-23). We fill each stack differ-
ently from CKY-Hiero and so queue diversity is
different from lazy cube pruning (Pust and Knight,
2009) or cube growing (Huang and Chiang, 2007;
Vilar and Ney, 2009; Xu and Koehn, 2012).
3 Capturing Missing Alignments
Figure 3(a) and Figure 3(b) show two examples of
a common problem in LR-Hiero decoding. The
decoder steps for Figure 3(b) are shown in Fig-
ure 2. The problem occurs in Step 5 of Figure 2
where rule #5 is matched to span [7, 15]. Dur-
ing decoding LR-Hiero maintains a stack (last-
in-first-out) of yet-to-be-covered spans and tries
to translate the first uncovered span (span [7, 15]
in Step 5). LR-Hiero should match rule #5 to
span [7, 15], therefore X
2
is forced to match span
[12, 15] which leads to the translation of span [7, 9]
(corresponding to X
1
) being reordered around it
223
Corpus Train/Dev/Test
Cs-En Europarl(v7) + CzEng(v0.9); News
commentary(nc) 2008&2009; nc 2011
7.95M/3000/3003
De-En Europarl(v7); WMT2006; WMT2006 1.5M/2000/2000
Zh-En HK + GALE phase-1; MTC part 1&3;
MTC part 4
2.3M/1928/919
Table 1: Corpus statistics in number of sentences. Tuning and test sets for Chinese-English has 4 references.
Model Cs-En De-En Zh-En
Hiero 20.77 25.72 27.65
LR-Hiero (Watanabe et al., 2006) 20.72 25.10 25.99
LR-Hiero+CP (Siahbani et al., 2013) 20.15 24.83 -
LR-Hiero+CP (QD=1) 20.68 25.14 24.44
LR-Hiero+CP (QD=15) - - 26.10
LR-Hiero+CP+(ab) 20.88 25.22 26.55
LR-Hiero+CP+(abc) 20.89 25.22 26.52
(a) BLEU scores for different baselines and modifications of this paper.
QD=15 for Zh-En in last three rows. (b) Average number of language model queries.
Table 2: (a) BLEU (b) LM calls
causing the incorrect translation in Step 9. If we
use the same set of rules for translation in Hi-
ero (CKY-based decoder), the decoder is able to
generate the correct translation for span [7, 14] (it
works bottom-up and generate best translation for
each source span). Then it combines translation of
[7, 14] with translation of spans [0, 7] and [14, 15]
using glue rules (monotonic combination).
In Figure 3(a) monotonic translations after span
[6, 9] are out of reach of the LR-Hiero decoder
which has to use the non-terminals to support
the reordering within span [6, 9]. In this exam-
ple the first few phrases are translated monoton-
ically, then for span [6, 18] we have to apply rule
?muqian X
1
wending, is now in stable X
1
? to ob-
tain the correct translation. But this rule cannot
be matched to span [6, 18] and the decoder fails
to generate the correct translation. While CKY-
Hiero can apply this rule to span [6, 9], generate
correct translation for this span and monotonically
combine it with translation of other spans ([0, 6],
[9, 18]).
In both these cases, CKY-Hiero has no diffi-
culty in reaching the target sentence with the same
GNF rules. The fact that we have to process spans
as they appear in the stack in LR-Hiero means
that we cannot combine arbitrary adjacent spans
to deal with such cases. So purely bottom-up de-
coders such as CKY-Hiero can capture the align-
ments in Figure 3 but LR-Hiero cannot.
We extend the LR-Hiero decoder to handle such
cases by making the GNF grammar more expres-
sive. Rules are partitioned to three types based on
the right boundary in the source and target side.
The rhs after the? shows the new rules we create
within the decoder using a new non-terminal X
r
to match the right boundary.
(a) ??a?,
?
b?? ? ??a?X
r
,
?
b?X
r
?
(b) ??X
n
,
?
b?X
n
? ? ??X
n
X
r
,
?
b?X
n
X
r
?
(c) ??X
n
,
?
b?X
m
? ? ??X
n
X
r
,
?
b?X
m
X
r
?
(1)
where ? is a string of terminals and non-terminals,
a? and
?
b are terminal sequences of source and tar-
get respectively, ? is a possibly empty sequence
of non-terminals and X
n
and X
m
are different
non-terminals distinct from X
r
3
. The extra non-
terminal X
r
lets us add a new yet-to-be-covered
span to the bottom of the stack at each rule appli-
cation which lets us match any two adjacent spans
just as in CKY-Hiero. This captures the missing
alignments that could not be previously captured
in the LR-Hiero decoder
4
.
In Table 4 we translated devset sentences using
forced decoding to show that our modifications to
LR-Hiero in this section improves the alignment
coverage when compared to CKY-Hiero.
4 Experiments
We evaluate our modifications to LR-Hiero de-
coder on three language pairs (Table 1): German-
English (De-En), Czech-English (Cs-En) and
Chinese-English (Zh-En).
3
In rule type (c) X
n
will be in ? and X
m
will be in ?.
4
For the sake of simplicity, in rule type (b) we can merge
X
n
and X
r
as they are in the same order on both source and
target side.
224
We use a 5-gram LM trained on the Gigaword
corpus and use KenLM (Heafield, 2011). We
tune weights by minimizing BLEU loss on the dev
set through MERT (Och, 2003) and report BLEU
scores on the test set. Pop limit for Hiero and LR-
Hiero+CP is 500 and beam size LR-Hiero is 500.
Other extraction and decoder settings such as max-
imum phrase length, etc. were identical across set-
tings. To make the results comparable we use the
same feature set for all baselines, Hiero as well
(including new features proposed by (Siahbani et
al., 2013)).
We use 3 baselines: (i) our implementation of
(Watanabe et al., 2006): LR-Hiero with beam
search (LR-Hiero) and (ii) LR-Hiero with cube
pruning (Siahbani et al., 2013): (LR-Hiero+CP);
and (iii) Kriya, an open-source implementation of
Hiero in Python, which performs comparably to
other open-source Hiero systems (Sankaran et al.,
2012).
Table 3 shows model sizes for LR-Hiero (GNF)
and Hiero (SCFG). Typical Hiero rule extraction
excludes phrase-pairs with unaligned words on
boundaries (loose phrases). We use similar rule
extraction as Hiero, except that exclude non-GNF
rules and include loose phrase-pairs as terminal
rules.
Table 2a shows the translation quality of dif-
ferent systems in terms of BLEU score. Row
3 is from (Siahbani et al., 2013)
5
. As we dis-
cussed in Section 2, LR-Hiero+CP suffers from
severe search errors on Zh-En (1.5 BLEU) but us-
ing queue diversity (QD=15) we fill this gap. We
use the same QD(=15) in next rows for Zh-en.
For Cs-En and De-En we use regular cube prun-
ing (QD=1), as it works as well as beam search
(compare rows 4 and 2).
We measure the benefit of the new modified
rules from Section 3: (ab): adding modifications
for rules type (a) and (b); (abc): modification
of all rules. We can see that for all language
pairs (ab) constantly improves performance of LR-
Hiero, significantly better than LR-Hiero+CP and
LR-Hiero (p-value<0.05) on Cs-En and Zh-En,
evaluated by MultEval (Clark et al., 2011). But
modifying rule type (c) does not show any im-
provement due to spurious ambiguity created by
5
We report results on Cs-En and De-En in (Siahbani et
al., 2013). Row 4 is the same translation system as row 3
(LR-Hiero+CP). We achieve better results than our previous
work (Siahbani et al., 2013) (row 4 vs. row 3) due to bug
corrections and adding loose phrases as terminal rules.
Model Cs-En De-En Zh-En
Hiero 1,961.6 858.5 471.9
LR-Hiero 266.5 116.0 100.9
Table 3: Model sizes (millions of rules).
Model Cs-En De-En Zh-En
Hiero 318 351 187
LR-Hiero 278 300 132
LR-Hiero+(abc) 338 361 174
Table 4: No. of sentence covered in forced decoding of a sam-
ple of sentences from the devset. We improve the coverage
by 31% for Chinese-English and more than 20% for the other
two language pairs.
type (c) rules.
Figure 2b shows the results in terms of average
number of language model queries on a sample set
of 50 sentences from test sets. All of the base-
lines use the same wrapper to KenLM (Heafield,
2011) to query the language model, and we have
instrumented the wrapper to count the statistics.
In (Siahbani et al., 2013) we discuss that LR-Hiero
with beam search (Watanabe et al., 2006) does not
perform at the same level of state-of-the-art Hi-
ero (more LM calls and less translation quality).
As we can see in this figure, adding new mod-
ified rules slightly increases the number of lan-
guage model queries on Cs-En and De-En so that
LR-Hiero+CP still works 2 to 3 times faster than
Hiero. On Zh-En, LR-Hiero+CP applies queue
diversity (QD=15) which reduces search errors
and improves translation quality but increases the
number of hypothesis generation as well. LR-
Hiero+CP with our modifications works substan-
tially faster than LR-Hiero while obtain signifi-
cantly better translation quality on Zh-En.
Comparing Table 2a with Figure 2b we can see
that overall our modifications to LR-Hiero decoder
significantly improves the BLEU scores compared
to previous LR decoders for Hiero. We obtain
comparable results to CKY-Hiero for Cs-En and
De-En and remarkably improve results on Zh-En,
while at the same time making 2 to 3 times less
LM calls on Cs-En and De-En compared to CKY-
Hiero.
Acknowledgments
This research was partially supported by NSERC,
Canada RGPIN: 262313 and RGPAS: 446348
grants to the second author. The authors wish to
thank Baskaran Sankaran for his valuable discus-
sions and the anonymous reviewers for their help-
ful comments.
225
References
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 176?181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Commun. ACM, 13(2):94?102, February.
Yang Feng, Yang Liu, Qun Liu, and Trevor Cohn.
2012. Left-to-right tree-to-string decoding with pre-
diction. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, EMNLP-CoNLL ?12, pages 1191?1200,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Michel Galley and Christopher D. Manning. 2010.
Accurate non-hierarchical phrase-based translation.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 966?974, Los Angeles, California, June. As-
sociation for Computational Linguistics.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tet-
suo Kiso, and Marcello Federico. 2011. Left lan-
guage model state for syntactic machine translation.
In Proceedings of the International Workshop on
Spoken Language Translation, pages 183?190, San
Francisco, California, USA, 12.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping language model boundary words
to speed K-Best extraction from hypergraphs. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, Georgia, USA, 6.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In In Proc. of the Sixth
Workshop on Statistical Machine Translation.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In In ACL 07.
Liang Huang and Haitao Mi. 2010. Efficient incre-
mental decoding for tree-to-string translation. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
273?283, Cambridge, MA, October. Association for
Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: A beam search de-
coder for phrase-based statistical machine transla-
tion models. In AMTA, pages 115?124.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Michael Pust and Kevin Knight. 2009. Faster mt
decoding through pervasive laziness. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 141?144,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Baskaran Sankaran, Majid Razmara, and Anoop
Sarkar. 2012. Kriya - an end-to-end hierarchi-
cal phrase-based mt system. The Prague Bulletin
of Mathematical Linguistics (PBML), 97(97):83?98,
apr.
Maryam Siahbani, Baskaran Sankaran, and Anoop
Sarkar. 2013. Efficient left-to-right hierarchical
phrase-based translation with improved reordering.
In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, Seattle,
USA, October. Association for Computational Lin-
guistics.
David Vilar and Hermann Ney. 2009. On lm heuris-
tics for the cube growing algorithm. In Annual Con-
ference of the European Association for Machine
Translation, pages 242?249, Barcelona, Spain, may.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of ACL.
Wenduan Xu and Philipp Koehn. 2012. Extending hi-
ero decoding in moses with cube growing. Prague
Bull. Math. Linguistics, 98:133?.
226
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 533?537,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Improved Reordering for Shallow-n Grammar based Hierarchical
Phrase-based Translation
Baskaran Sankaran and Anoop Sarkar
School of Computing Science
Simon Fraser University
Burnaby BC. Canada
{baskaran, anoop}@cs.sfu.ca
Abstract
Shallow-n grammars (de Gispert et al, 2010)
were introduced to reduce over-generation in
the Hiero translation model (Chiang, 2005) re-
sulting in much faster decoding and restricting
reordering to a desired level for specific lan-
guage pairs. However, Shallow-n grammars
require parameters which cannot be directly
optimized using minimum error-rate tuning
by the decoder. This paper introduces some
novel improvements to the translation model
for Shallow-n grammars. We introduce two
rules: a BITG-style reordering glue rule and a
simpler monotonic concatenation rule. We use
separate features for the new rules in our log-
linear model allowing the decoder to directly
optimize the feature weights. We show this
formulation of Shallow-n hierarchical phrase-
based translation is comparable in translation
quality to full Hiero-style decoding (without
shallow rules) while at the same time being
considerably faster.
1 Introduction
Hierarchical phrase-based translation (Chiang,
2005; Chiang, 2007) extends the highly lexicalized
models from phrase-based translation systems in
order to model lexicalized reordering and discon-
tiguous phrases. However, a major drawback in this
approach, when compared to phrase-based systems,
is the total number of rules that are learnt are several
orders of magnitude larger than standard phrase
tables, which leads to over-generation and search
errors and contribute to much longer decoding
times. Several approaches have been proposed to
address these issues: from filtering the extracted
synchronous grammar (Zollmann et al, 2008; He
et al, 2009; Iglesias et al, 2009) to alternative
Bayesian approaches for learning minimal gram-
mars (Blunsom et al, 2008; Blunsom et al, 2009;
Sankaran et al, 2011). The idea of Shallow-n gram-
mars (de Gispert et al, 2010) takes an orthogonal
direction for controlling the over-generation and
search space in Hiero decoder by restricting the
degree of nesting allowed for Hierarchical rules.
We propose an novel statistical model for
Shallow-n grammars which does not require addi-
tional non-terminals for monotonic re-ordering and
also eliminates hand-tuned parameters and instead
introduces an automatically tunable alternative. We
introduce a BITG-style (Saers et al, 2009) reorder-
ing glue rule (? 3) and a monotonic X-glue rule
(? 4). Our experiments show the resulting Shallow-n
decoding is comparable in translation quality to full
Hiero-style decoding while at the same time being
considerably faster.
All the experiments in this paper were done using
Kriya (Sankaran et al, 2012) hierarchical phrase-
based system which also supports decoding with
Shallow-n grammars. We extended Kriya to addi-
tionally support reordering glue rules as well.
2 Shallow-n Grammars
Formally a Shallow-n grammar G is defined as a 5-
tuple: G = (N,T,R,Rg, S), such that T is a set of
finite terminals and N a set of finite non-terminals
{X0, . . . , XN}. Rg refers to the glue rules that
rewrite the start symbol S:
S ? <X, X> (1)
S ? <SX, SX> (2)
R is the set of finite production rules in G and has
two types, viz. hierarchical (3) and terminal (4). The
hierarchical rules at each level n are additionally
conditioned to have at least one Xn?1 non-terminal
533
in them. ? represents the indices for aligning non-
terminals where co-indexed non-terminal pairs are
rewritten synchronously.
Xn ? <?, ?, ? >, ?, ? ? {{Xn?1} ? T+} (3)
X0 ? <?, ?>, ?, ? ? T+ (4)
de Gispert et al (2010) also proposed additional
non-terminals Mk to enable reordering over longer
spans by concatenating the hierarchical rules within
the span. It also uses additional parameters such
as monotonicity level (K1 and K2), maximum and
minimum rule spans allowed for the non-terminals
(?3.1 and 3.2 in de Gispert et al (2010)). The mono-
tonicity level parameters determine the number of
non-terminals that are combined in monotonic or-
der at the N ? 1 level and can be adapted to the
reordering requirements of specific language pairs.
The maximum and minimum rule spans further con-
trol the usage of hierarchical rule in a derivation by
stipulating the underlying span to be within a range
of values. Intuitively, this avoids hierarchical rules
being used for a source phrase that is either too short
or too long. While these parameters offer flexibility
for adapting the translation system to specific lan-
guage pairs, they have to be manually tuned which
is tedious and error-prone.
We propose an elegant and automatically tun-
able alternative for the Shallow-n grammars setting.
Specifically, we introduce a BITG-style reordering
glue rule (? 3) and a monotonic X-glue rule (? 4).
Our experiments show the resulting Shallow-n de-
coding to perform to the same level as full-Hiero de-
coding at the same time being faster.
In addition, our implementation of Shallow-n
grammar differs from (de Gispert et al, 2010) in
at least two other aspects. First, their formula-
tion constrains the X in the glue rules to be at the
top-level and specifically they define them to be:
S ? <SXN , SXN> and S ? <XN , XN>,
where XN is the non-terminal corresponding to the
top-most level. Interestingly, this resulted in poor
BLEU scores and we found the more generic glue
rules (as in (1) and (2)) to perform significantly bet-
ter, as we show later.
Secondly, they also employ pattern-based filter-
ing (Iglesias et al, 2009) in order to reducing redun-
dancies in the Hiero grammar by filtering it based on
certain rule patterns. However in our limited experi-
ments, we observed the filtered grammar to perform
worse than the full grammar, as also noted by (Zoll-
mann et al, 2008). Hence, we do not employ any
grammar filtering in our experiments.
3 Reordering Glue Rule
In this paper, we propose an additional BITG-style
glue rule (called R-glue) as in (5) for reordering the
phrases along the left-branch of the derivation.
S ? <SX, XS> (5)
In order to use this rule sparsely in the derivation,
we use a separate feature for this rule and apply a
penalty of 1. Similar to the case of regular glue
rules, we experimented with a variant of the reorder-
ing glue rule, where X is restricted to the top-level:
S ? <SXN , XNS> and S ? <XN , XN>.
3.1 Language Model Integration
The traditional phrase-based decoders using beam
search generate the target hypotheses in the left-to-
right order. In contrast, Hiero-style systems typ-
ically use CKY chart-parsing decoders which can
freely combine target hypotheses generated in inter-
mediate cells with hierarchical rules in the higher
cells. Thus the generation of the target hypotheses
are fragmented and out of order compared to the left
to right order preferred by n-gram language models.
This leads to challenges in the estimation of lan-
guage model scores for partial target hypothesis,
which is being addressed in different ways in the
existing Hiero-style systems. Some systems add a
sentence initial marker (<s>) to the beginning of
each path and some other systems have this implic-
itly in the derivation through the translation mod-
els. Thus the language model scores for the hypoth-
esis in the intermediate cell are approximated, with
the true language model score (taking into account
sentence boundaries) being computed in the last cell
that spans the entire source sentence.
We introduce a novel improvement in computing
the language model scores: for each of the target
hypothesis fragment, our approach finds the best po-
sition for the fragment in the final sentence and uses
the corresponding score. We compute three different
scores corresponding to the three positions where
the fragment can end up in the final sentence, viz.
534
sentence initial, middle and final: and choose the
best score. As an example for fragment tf consist-
ing of a sequence of target tokens, we compute LM
scores for i) <s> tf , ii) tf and iii) tf </s> and use
the best score for pruning alone1.
This improvement significantly reduces the
search errors while performing cube pruning (Chi-
ang, 2007) at the cost of additional language model
queries. While this approach works well for the
usual glue rules, it is particularly effective in the case
of reordering glue rules. For example, a partial can-
didate covering a non-final source span might trans-
late to the final position in the target sentence. If we
just compute the LM score for the target fragment
as is done normally, this might get pruned early on
before being reordered by the new glue rule. Our ap-
proach instead computes the three LM scores and it
would correctly use the last LM score which is likely
to be the best, for pruning.
4 Monotonic Concatenation Glue rule
The reordering glue rule facilitates reordering at the
top-level. However, this is still not sufficient to allow
long-distance reordering as the shallow-decoding re-
stricts the depth of the derivation. Consider the Chi-
nese example in Table 1, in which translation of the
Chinese word corresponding to the English phrase
the delegates involves a long distance reordering to
the beginning of the sentence. Note that, three of the
four human references prefer this long distance re-
ordering, while the fourth one avoids the movement
by using a complex construction with relative clause
and a sentence initial prepositional phrase.
Such long distance reordering is very difficult in
conventional Hiero decoding and more so with the
Shallow-n grammars. While the R-glue rule per-
mit such long distance movements, it also requires
a long phrase generated by a series of rules to be
moved as a block. We address this issue, by adding
a monotonic concatenation (called X-glue) rule that
concatenates a series of hierarchical rules. In order
to control overgeneration, we apply this rule only at
the N ? 1 level similar to de Gispert et al (2010).
XN?1 ? <XN?1XN?1, XN?1XN?1> (6)
1This ensures the the LM score estimates are never underes-
timated for pruning. We retain the LM score for fragment (case
ii) for estimating the score for the full candidate sentence later.
However unlike their approach, we use this rule as
a feature in the log-linear model so that its weight
can be optimized in the tuning step. Also, our ap-
proach removes the need for additional parameters
K1 and K2 for controlling monotonicity, which was
being tuned manually in their work. For the Chinese
example above, shallow-1 decoding using R and X-
glue rules achieve the complex movement resulting
in a significantly better translation than full-Hiero
decoding as shown in the last two lines in Table 1.
5 Experiments
We present results for Chinese-English translation
as it often requires heavy reordering. We use the
HK parallel text and GALE phase-1 corpus consist-
ing of?2.3M sentence pairs for training. For tuning
and testing, we use the MTC parts 1 and 3 (1928
sentences) and MTC part 4 (919 sentences) respec-
tively. We used the usual pre-processing pipeline
and an additional segmentation step for the Chinese
side of the bitext using the LDC segmenter2.
Our log-linear model uses the standard features
conditional (p(e|f) and p(f |e)) and lexical (pl(e|f)
and pl(f |e)) probabilities, phrase (pp) and word
(wp) penalties, language model and regular glue
penalty (mg) apart from two additional features for
R?glue (rg) and X?glue (xg).
Table 2 shows the BLEU scores and decoding
time for the MTC test-set. We provide the IBM
BLEU (Papineni et al, 2002) scores for the Shallow-
n grammars for order: n = 1, 2, 3 and compare it to
the full-Hiero baseline. Finally, we experiment with
two variants of the S glue rules, i) a restricted ver-
sion where the glue rules combine only X at level
N , (column ?Glue: XN ? in table), ii) more free vari-
ant where they are allowed to use any X freely (col-
umn ?Glue: X? in table).
As it can be seen, the unrestricted glue rules vari-
ant (column ?Glue: X?) consistently outperforms
the glue rules restricted to the top-level non-terminal
XN , achieving a maximum BLEU score of 26.24,
which is about 1.4 BLEU points higher than the lat-
ter and is also marginally higher than full Hiero. The
decoding speeds for free-Glue and restricted-Glue
variants were mostly identical and so we only pro-
vide the decoding time for the latter. Shallow-2 and
2We slightly modified the LDC segmenter, in order to cor-
rectly handle non-Chinese characters in ASCII and UTF8.
535
Source ???????????????????????????????????
Gloss in argentine capital beunos aires participate united nations global climate conference delegates continue
to work.
Ref 0 delegates attending the un conference on world climate continue their work in the argentine capital of
buenos aires.
Ref 1 the delegates to the un global climate conference held in Buenos aires, capital city of argentina, go on with
their work.
Ref 2 the delegates continue their works at the united nations global climate talks in buenos aires, capital of
argentina
Ref 3 in buenos aires, the capital of argentina, the representatives attending un global climate meeting continued
their work.
Full-Hiero:
Baseline
in the argentine capital of buenos aires to attend the un conference on global climate of representatives
continue to work.
Sh-1 Hiero: R-
glue & X-glue
the representatives were in the argentine capital of beunos aires to attend the un conference on global climate
continues to work.
Table 1: An example for the level of reordering in Chinese-English translation
Grammar Glue: XN Glue: X Time
Full Hiero 25.96 0.71
Shallow-1 23.54 24.04 0.24
+ R-Glue 23.41 24.15 0.25
+ X-Glue 23.75 24.74 0.72
Shallow-2 24.54 25.12 0.55
+ R-Glue 24.75 25.60 0.57
+ X-Glue 24.33 25.43 0.69
Shallow-3 24.88 25.89 0.62
+ R-Glue 24.77 26.24 0.63
+ X-Glue 24.75 25.83 0.69
Table 2: Results for Chinese-English. The decoding time
is in secs/word on the Test set for column ?Glue: X?.
Bold font indicate best BLEU for each shallow-order.
shallow-3 free glue variants achieve BLEU scores
comparable to full-Hiero and at the same time being
12? 20% faster.
R-glue (rg) appears to contribute more than
the X-glue (xg) as can be seen in shallow-2 and
shallow-3 cases. Interestingly, xg is more helpful for
the shallow-1 case specifically when the glue rules
are restricted. As the glue rules are restricted, the
X-glue rules concatenates other lower-order rules
before being folded into the glue rules. Both rg and
xg improve the BLEU scores by 0.58 over the plain
shallow case for shallow orders 1 and 2 and performs
comparably for shallow-3 case. We have also con-
ducted experiments for Arabic-English (Table 3) and
we notice that X-glue is more effective and that R-
glue is helpful for higher shallow orders.
Grammar Glue: X Time
Full Hiero 37.54 0.67
Shallow-1 36.90 0.40
+ R-Glue 36.98 0.43
+ X-Glue 37.21 0.57
Shallow-2 36.97 0.57
+ R-Glue 36.80 0.58
+ X-Glue 37.36 0.61
Shallow-3 36.88 0.61
+ R-Glue 37.18 0.63
+ X-Glue 37.31 0.64
Table 3: Results for Arabic-English. The decoding time
is in secs/word on the Test set.
5.1 Effect of our novel LM integration
Here we analyze the effect of our novel LM integra-
tion approach in terms of BLEU score and search er-
rors comparing it to the naive method used in typical
Hiero systems. In shallow setting, our method im-
proved the BLEU scores by 0.4 for both Ar-En and
Cn-En. In order to quantify the change in the search
errors, we compare the model scores of the (corre-
sponding) candidates in the N-best lists obtained by
the two methods and compute the % of high scor-
ing candidates in each. Our approach was clearly
superior with 94.6% and 77.3% of candidates hav-
ing better scores respectively for Cn-En and Ar-En.
In full decoding setting the margin of improvements
were reduced slightly- BLEU improved by 0.3 and
about 57?69% of target candidates had better model
scores for the two language pairs.
536
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Pro-
ceedings of Neural Information Processing Systems.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of Association of
Computational Linguistics, pages 782?790.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
Association of Computational Linguistics, pages 263?
270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite-
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36.
Zhongjun He, Yao Meng, and Hao Yu. 2009. Discarding
monotone composed rule for hierarchical phrase-based
statistical machine translation. In Proceedings of the
3rd International Universal Communication Sympo-
sium, pages 25?29.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
the 12th Conference of the European Chapter of the
ACL, pages 380?388.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of Association of Computational Lin-
guistics, pages 311?318.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proceedings of the 11th International Conference on
Parsing Technologies, pages 29?32. Association for
Computational Linguistics.
Baskaran Sankaran, Gholamreza Haffari, and Anoop
Sarkar. 2011. Bayesian extraction of minimal scfg
rules for hierarchical phrase-based translation. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation, pages 533?541.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya ? an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics, (97):83?98, April.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics, pages 1145?1152.
537
Proceedings of NAACL-HLT 2013, pages 947?957,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Multi-Metric Optimization Using Ensemble Tuning
Baskaran Sankaran, Anoop Sarkar
Simon Fraser University
Burnaby BC. CANADA
{baskaran,anoop}@cs.sfu.ca
Kevin Duh
Nara Institute of Science & Technology
Ikoma, Nara. JAPAN
kevinduh@is.naist.jp
Abstract
This paper examines tuning for statistical ma-
chine translation (SMT) with respect to mul-
tiple evaluation metrics. We propose several
novel methods for tuning towards multiple ob-
jectives, including some based on ensemble
decoding methods. Pareto-optimality is a nat-
ural way to think about multi-metric optimiza-
tion (MMO) and our methods can effectively
combine several Pareto-optimal solutions, ob-
viating the need to choose one. Our best
performing ensemble tuning method is a new
algorithm for multi-metric optimization that
searches for Pareto-optimal ensemble models.
We study the effectiveness of our methods
through experiments on multiple as well as
single reference(s) datasets. Our experiments
show simultaneous gains across several met-
rics (BLEU, RIBES), without any significant
reduction in other metrics. This contrasts the
traditional tuning where gains are usually lim-
ited to a single metric. Our human evaluation
results confirm that in order to produce better
MT output, optimizing multiple metrics is bet-
ter than optimizing only one.
1 Introduction
Tuning algorithms are used to find the weights for a
statistical machine translation (MT) model by min-
imizing error with respect to a single MT evalua-
tion metric. The tuning process improves the per-
formance of an SMT system as measured by this
metric; with BLEU (Papineni et al, 2002) being
the most popular choice. Minimum error-rate train-
ing (MERT) (Och, 2003) was the first approach in
MT to directly optimize an evaluation metric. Sev-
eral alternatives now exist: MIRA (Watanabe et al,
2007; Chiang et al, 2008), PRO (Hopkins and May,
2011), linear regression (Bazrafshan et al, 2012)
and ORO (Watanabe, 2012) among others.
However these approaches optimize towards the
best score as reported by a single evaluation met-
ric. MT system developers typically use BLEU and
ignore all the other metrics. This is done despite
the fact that other metrics model wide-ranging as-
pects of translation: from measuring the translation
edit rate (TER) in matching a translation output to a
human reference (Snover et al, 2006), to capturing
lexical choices in translation as in METEOR (Lavie
and Denkowski, 2009) to modelling semantic simi-
larity through textual entailment (Pado? et al, 2009)
to RIBES, an evaluation metric that pays attention
to long-distance reordering (Isozaki et al, 2010).
While some of these metrics such as TER, ME-
TEOR are gaining prominence, BLEU enjoys the
status of being the de facto standard tuning metric
as it is often claimed and sometimes observed that
optimizing with BLEU produces better translations
than other metrics (Callison-Burch et al, 2011).
The gains obtained by the MT system tuned on
a particular metric do not improve performance as
measured under other metrics (Cer et al, 2010), sug-
gesting that over-fitting to a specific metric might
happen without improvements in translation quality.
In this paper we propose a new tuning framework
for jointly optimizing multiple evaluation metrics.
Pareto-optimality is a natural way to think about
multi-metric optimization and multi-metric opti-
mization (MMO) was recently explored using the
notion of Pareto optimality in the Pareto-based
Multi-objective Optimization (PMO) approach (Duh
et al, 2012). PMO provides several equivalent solu-
tions (parameter weights) having different trade-offs
between the different MT metrics. In (Duh et al,
2012) the choice of which option to use rests with
the MT system developer and in that sense their ap-
proach is an a posteriori method to specify the pref-
erence (Marler and Arora, 2004).
In contrast to this, our tuning framework pro-
vides a principled way of using the Pareto optimal
options using ensemble decoding (Razmara et al,
2012). We also introduce a novel method of ensem-
ble tuning for jointly tuning multiple MT evaluation
metrics and further combine this with the PMO ap-
947
proach (Duh et al, 2012). We also introduce three
other approaches for multi-metric tuning and com-
pare their performance to the ensemble tuning. Our
experiments yield the highest metric scores across
many different metrics (that are being optimized),
something that has not been possible until now.
Our ensemble tuning method over multiple met-
rics produced superior translations than single met-
ric tuning as measured by a post-editing task.
HTER (Snover et al, 2006) scores in our human
evaluation confirm that multi-metric optimization
can lead to better MT output.
2 Related Work
In grammar induction and parsing (Spitkovsky et al,
2011; Hall et al, 2011; Auli and Lopez, 2011) have
proposed multi-objective methods based on round-
robin iteration of single objective optimizations.
Research in SMT parameter tuning has seen a
surge of interest recently, including online/batch
learning (Watanabe, 2012; Cherry and Foster, 2012),
large-scale training (Simianer et al, 2012; He
and Deng, 2012), and new discriminative objec-
tives (Gimpel and Smith, 2012; Zheng et al, 2012;
Bazrafshan et al, 2012). However, few works
have investigated the multi-metric tuning problem in
depth. Linear combination of BLEU and TER is re-
ported in (Zaidan, 2009; Dyer et al, 2009; Servan
and Schwenk, 2011); an alternative is to optimize on
BLEU with MERT while enforcing that TER does
not degrade per iteration (He and Way, 2009). Stud-
ies on metric tunability (Liu et al, 2011; Callison-
Burch et al, 2011; Chen et al, 2012) have found
that the metric used for evaluation may not be the
best metric used for tuning. For instance, (Mauser et
al., 2008; Cer et al, 2010) report that tuning on lin-
ear combinations of BLEU-TER is more robust than
a single metric like WER.
The approach in (Devlin and Matsoukas, 2012)
modifies the optimization function to include traits
such as output length so that the hypotheses pro-
duced by the decoder have maximal score according
to one metric (BLEU) but are subject to an output
length constraint, e.g. that the output is 5% shorter.
This is done by rescoring an N-best list (forest) for
the metric combined with each trait condition and
then the different trait hypothesis are combined us-
ing a system combination step. The traits are in-
dependent of the reference (while tuning). In con-
trast, our method is able to combine multiple metrics
(each of which compares to the reference) during the
tuning step and we do not depend on N-best list (or
forest) rescoring or system combination.
Duh et. al. (2012) proposed a Pareto-based ap-
proach to SMT multi-metric tuning, where the lin-
ear combination weights do not need to be known in
advance. This is advantageous because the optimal
weighting may not be known in advance. However,
the notion of Pareto optimality implies that multiple
?best? solutions may exist, so the MT system devel-
oper may be forced to make a choice after tuning.
These approaches require the MT system devel-
oper to make a choice either before tuning (e.g. in
terms of linear combination weights) or afterwards
(e.g. the Pareto approach). Our method here is dif-
ferent in that we do not require any choice. We
use ensemble decoding (Razmara et al, 2012) (see
sec 3) to combine the different solutions resulting
from the multi-metric optimization, providing an el-
egant solution for deployment. We extend this idea
further and introduce ensemble tuning, where the
metrics have separate set of weights. The tuning
process alternates between ensemble decoding and
the update step where the weights for each metric
are optimized separately followed by joint update of
metric (meta) weights.
3 Ensemble Decoding
We now briefly review ensemble decoding (Razmara
et al, 2012) which is used as a component in the al-
gorithms we present. The prevalent model of statis-
tical MT is a log-linear framework using a vector of
feature functions ?:
p(e|f) ? exp
(
w ? ?
)
(1)
The idea of ensemble decoding is to combine sev-
eral models dynamically at decode time. Given mul-
tiple models, the scores are combined for each par-
tial hypothesis across the different models during
decoding using a user-defined mixture operation ?.
p(e|f) ? exp
(
w1 ? ?1 ? w2 ? ?2 ? . . .
)
(2)
(Razmara et al, 2012) propose several mixture
operations, such as log-wsum (simple linear mix-
ture), wsum (log-linear mixture) and max (choose lo-
948
cally best model) among others. The different mix-
ture operations allows the user to encode the be-
liefs about the relative strengths of the models. It
has been applied successfully for domain adaptation
setting and shown to perform better approaches that
pre-compute linear mixtures of different models.
4 Multi-Metric Optimization
In statistical MT, the multi-metric optimization
problem can be expressed as:
w? = argmax
w
g
(
[M1(H), . . . ,Mk(H)]
)
(3)
where H = N (f ;w)
where N (f ;w) is the decoding function generating
a set of candidate hypotheses H based on the model
parameters w, for the source sentences f . For each
source sentence fi ? f there is a set of candidate
hypotheses {hi} ? H . The goal of the optimiza-
tion is to find the weights that maximize the func-
tion g(.) parameterized by different evaluation met-
rics M1, . . . ,Mk.
For the Pareto-optimal based approach such as
PMO (Duh et al, 2012), we can replace g(?) above
with gPMO(?) which returns the points in the Pareto
frontier. Alternately a weighted averaging function
gwavg(?) would result in a linear combination of the
metrics being considered, where the tuning method
would maximize the joint metric. This is similar to
the (TER-BLEU)/2 optimization (Cer et al, 2010;
Servan and Schwenk, 2011).
We introduce four methods based on the above
formulation and each method uses a different type
of g(?) function for combining different metrics and
we compare experimentally with existing methods.
4.1 PMO Ensemble
PMO (Duh et al, 2012) seeks to maximize the num-
ber of points in the Pareto frontier of the metrics con-
sidered. The inner routine of the PMO-PRO tuning
is described in Algorithm 1. This routine is con-
tained within an outer loop that iterates for a fixed
number iterations of decoding the tuning set and op-
timizing the weights.
The tuning process with PMO-PRO is inde-
pendently repeated with different set of weights
for metrics1 yielding a set of equivalent solutions
1For example Duh et al (2012) use five different weight
Algorithm 1 PMO-PRO (Inner routine for tuning)
1: Input: Hypotheses H = N (f ;w); Weights w
2: Initialize T = {}
3: for each f in tuning set f do
4: {h} = H(f)
5: {M({h})} = ComputeMetricScore({h}, e?)
6: {F} = FindParetoFrontier({M({h})})
7: for each h in {h} do
8: if h ? F then add (1, h) to T
9: else add (`, h) to T (see footnote 1)
10: wp ? PRO(T ) (optimize using PRO)
11: Output: Pareto-optimal weights wp
{ps1 , . . . , psn} which are points on the Pareto fron-
tier. The user then chooses one solution by making a
trade-off between the performance gains across dif-
ferent metrics. However, as noted earlier this a pos-
teriori choice ignores other solutions that are indis-
tinguishable from the chosen one.
We alleviate this by complementing PMO with
ensemble decoding, which we call PMO ensemble,
in which each point in the Pareto solution is a dis-
tinct component in the ensemble decoder. This idea
can also be used in other MMO approaches such as
linear combination of metrics (gwavg(.)) mentioned
above. In this view, PMO ensemble is a special case
of ensemble combination, where the decoding is per-
formed by an ensemble of optimal solutions.
The ensemble combination model introduces new
hyperparameters ? that are the weights of the en-
semble components (meta weights). These ensem-
ble weights could set to be uniform in a na??ve
implementation. Or the user can encode her be-
liefs or expectations about the individual solutions
{ps1 , . . . , psn} to set the ensemble weights (based
on the relative importance of the components). Fi-
nally, one could also include a meta-level tuning step
to set the weights ?.
The PMO ensemble approach is graphically il-
lustrated in Figure 1; we will also refer to this fig-
ure while discussing other methods.2 The orig-
settings for metrics (M1,M2), viz. (0.0, 1.0), (0.3, 0.7),
(0.5, 0.5), (0.7, 0.3) and (1.0, 0.0). They combine the met-
ric weights qi with the sentence-level metric scores Mi as
` =
(?
k qkMk
)
/k where ` is the target value for negative
examples (the else line in Alg 1) in the optimization step.
2The illustration is based on two metrics, metric-1 and
metric-2, but could be applied to any number of metrics. With-
out loss of generality we assume accuracy metrics, i.e. higher
949
Me
tric
?2
Metric?1
Figure 1: Illustration of different MMO approaches involving
two metrics. Solid (red) arrows indicate optimizing two met-
rics independently and the dashed (green) arrow optimize them
jointly. The Pareto frontier is indicated by the curve.
inal PMO-PRO seeks to maximize the points on
the Pareto frontier (blue curve in the figure) lead-
ing to Pareto-optimal solutions. On the other hand,
the PMO ensemble combines the different Pareto-
optimal solutions and potentially moving in the di-
rection of dashed (green) arrows to some point that
has higher score in either or both dimensions.
4.2 Lateen MMO
Lateen EM has been proposed as a way of jointly
optimizing multiple objectives in the context of de-
pendency parsing (Spitkovsky et al, 2011). It uses
a secondary hard EM objective to move away, when
the primary soft EM objective gets stuck in a local
optima. The course correction could be performed
under different conditions leading to variations that
are based on when and how often to shift from one
objective function to another during optimization.
The lateen technique can be applied to the multi-
metric optimization in SMT by treating the differ-
ent metrics as different objective functions. While
the several lateen variants are also applicable for our
task, our objective here is to improve performance
across the different metrics (being optimized). Thus,
we restrict ourselves to the style where the search
alternates between the metrics (in round-robin fash-
ion) at each iteration. Since the notion of conver-
gence is unclear in lateen setting, we stop after a
fixed number of iterations optimizing the tuning set.
In terms of Figure 1, lateen MMO corresponds to al-
ternately maximizing the metrics along two dimen-
sions as depicted by the solid arrows.
By the very nature of lateen-alternation, the
metric score is better.
weights obtained at each iteration are likely to be
best for the metric that was optimized in that itera-
tion. Thus, one could use weights from the last k
iterations (for lateen-tuning with as many metrics)
and then decode the test set with an ensemble of
these weights as in PMO ensemble. However in
practice we find the weights to converge and we sim-
ply use the weights from the final iteration to decode
the test set in our lateen experiments.
4.3 Union of Metrics
At each iteration lateen MMO excludes all but one
metric for optimization. An alternative would be to
consider all the metrics at each iteration so that the
optimizer could try to optimize them jointly. This
has been the general motivation for considering the
linear combination of metrics (Cer et al, 2010; Ser-
van and Schwenk, 2011) resulting in a joint metric,
which is then optimized.
However due to the scaling differences between
the scores of different metrics, the linear combi-
nation might completely suppress the metric hav-
ing scores in the lower-range. As an example, the
RIBES scores that are typically in the high 0.7-0.8
range, dominate the BLEU scores that is typically
around 0.3. While the weighted linear combination
tries to address this imbalance, they introduce ad-
ditional parameters that are manually fixed and not
separately tuned.
We avoid this linear combination pitfall by taking
the union of the metrics under which we consider
the union of training examples from all metrics and
optimize them jointly. Mathematically,
w? = argmax
w
g(M1(H)) ? . . . ? g(Mk(H)) (4)
Most of the optimization approaches involve two
phases: i) select positive and negative examples and
ii) optimize parameters to favour positive examples
while penalizing negative ones. In the union ap-
proach, we independently generate positive and neg-
ative sets of examples for all the metrics and take
their union. The optimizer now seeks to move to-
wards positive examples from all metrics, while pe-
nalizing others.
This is similar to the PMO-PRO approach except
that here the optimizer tries to simultaneously max-
imize the number of high scoring points across all
950
metrics. Thus, instead of the entire Pareto frontier
curve in Figure 1, the union approach optimizes the
two dimensions simultaneously in each iteration.
5 Ensemble Tuning
These methods, even though novel, under utilize the
power of ensembles as they combine the solution
only at the end of the tuning process. We would
prefer to tightly integrate the idea of ensembles into
the tuning. We thus extend the ensemble decoding
to ensemble tuning. The feature weights are repli-
cated separately for each evaluation metric, which
are treated as components in the ensemble decoding
and tuned independently in the optimization step.
Initially the ensemble decoder decodes a devset us-
ing a weighted ensemble to produce a single N-best
list. For the optimization, we employ a two-step ap-
proach of optimizing the feature weights (of each
ensemble component) followed by a step for tun-
ing the meta (component) weights. The optimized
weights are then used for decoding the devset in the
next iteration and the process is repeated for a fixed
number of iterations.
Modifying the MMO representation in Equa-
tion 3, we formulate ensemble tuning as:
Hens = Nens
(
f ; {wM};?;?
)
(5)
w? =
{
argmax
wMi
Hens | 1?i?k
}
(6)
? = argmax
?
g ({Mi(Hens)|1?i?k} ;w?) (7)
Here the ensemble decoder function Nens(.)
is parameterized by an ensemble of weights
wM1 , . . . , wMk (denoted as {wM} in Eq 5) for each
metric and a mixture operation (?). ? represents the
weights of the ensemble components.
Pseudo-code for ensemble tuning is shown in Al-
gorithm 2. In the beginning of each iteration (line 2),
the tuning process ensemble decodes (line 4) the
tuning set using the weights obtained from the pre-
vious iteration. Equation 5 gives the detailed expres-
sion for the ensemble decoding, whereHens denotes
the N-best list generated by the ensemble decoder.
The method now uses a dual tuning strategy in-
volving two phases to optimize the weights. In the
first step it optimizes each of the k metrics indepen-
dently (lines 6-7) along its respective dimension in
Algorithm 2 Ensemble Tuning Algorithm
1: Input: Tuning set f ,
Metrics M1, . . . ,Mk (ensemble components)
Initial weights {wM} ? wM1 , . . . wMk and
Component (meta) weights ?
2: for j = 1, . . . do
3: {w(j)M } ? {wM}
4: Ensemble decode the tuning set
Hens = Nens(f ; {w
(j)
M };?;?)
5: {wM} = {}
6: for each metric Mi ? {M} do
7: w?Mi ? PRO(Hens, wMi) (use PRO)
8: Add w?Mi to {wM}
9: ?? PMO-PRO(Hens, {wM}) (Alg 1)
10: Output: Optimal weights {wM} and ?
the multi-metric space (as shown by the solid arrows
along the two axes in Figure 1). This yields a new
set of weights w? for the features in each metric.
The second tuning step (line 9) then optimizes
the meta weights (?) so as to maximize the multi-
metric objective along the joint k-dimensional space
as shown in Equation 7. This is illustrated by the
dashed arrows in the Figure 1. While g(.) could be
any function that combines multiple metrics, we use
the PMO-PRO algorithm (Alg. 1) for this step.
The main difference between ensemble tuning and
PMO ensemble is that the former is an ensemble
model over metrics and the latter is an ensemble
model over Pareto solutions. Additionally, PMO en-
semble uses the notion of ensembles only for the fi-
nal decoding after tuning has completed.
5.1 Implementation Notes
All the proposed methods fit naturally within the
usual SMT tuning framework. However, some
changes are required in the decoder to support en-
semble decoding and in the tuning scripts for op-
timizing with multiple metrics. For ensemble de-
coding, the decoder should be able to use multiple
weight vectors and dynamically combine them ac-
cording to some desired mixture operation. Note
that, unlike Razmara et al (2012), our approach uses
just one model but has different weight vectors for
each metric and the required decoder modifications
are simpler than full ensemble decoding.
While any of the mixture operations proposed
by Razmara et al (2012) could be used, in this pa-
951
per we use log-wsum ? the linear combination of the
ensemble components and log-wmax ? the combina-
tion that prefers the locally best component. These
are simpler to implement and also performed com-
petitively in their domain adaptation experiments.
Unless explicitly noted otherwise, the results pre-
sented in Section 6 are based on linear mixture oper-
ation log-wsum, which empirically performed better
than the log-wmax for ensemble tuning.
6 Experiments
We evaluate the different methods on Arabic-
English translation in single as well as multiple ref-
erences scenario. Corpus statistics are shown in
Table 1. For all the experiments in this paper,
we use Kriya, our in-house Hierarchical phrase-
based (Chiang, 2007) (Hiero) system, and inte-
grated the required changes for ensemble decoding.
Kriya performs comparably to the state of the art in
phrase-based and hierarchical phrase-based transla-
tion over a wide variety of language pairs and data
sets (Sankaran et al, 2012).
We use PRO (Hopkins and May, 2011) for op-
timizing the feature weights and PMO-PRO (Duh
et al, 2012) for optimizing meta weights, wher-
ever applicable. In both cases, we use SVM-
Rank (Joachims, 2006) as the optimizer.
We used the default parameter settings for dif-
ferent MT tuning metrics. For METEOR, we tried
both METEOR-tune and METEOR-hter settings
and found the latter to perform better in BLEU and
TER scores, even though the former was marginally
better in METEOR3 and RIBES scores. We ob-
served the margin of loss in BLEU and TER to out-
weigh the gains in METEOR and RIBES and we
chose METEOR-hter setting for both optimization
and evaluation of all our experiments.
6.1 Evaluation on Tuning Set
Unlike conventional tuning methods, PMO (Duh et
al., 2012) was originally evaluated on the tuning set
to avoid potential mismatch with the test set. In
order to ensure robustness of evaluation, they re-
decode the devset using the optimal weights from
the last tuning iteration and report the scores on 1-
3This behaviour was also noted by Denkowski and Lavie
(2011) in their analysis of Urdu-English system for tunable met-
rics task in WMT11.
best candidates.
Corpus Training size Tuning/ test set
ISI corpus 1.1 M
1664/ 1313 (MTA)
1982/ 987 (ISI)
Table 1: Corpus Statistics (# of sentences) for Arabic-English.
MTA (4-refs) and ISI (1-ref).
We follow the same strategy and compare our
PMO-ensemble approach with PMO-PRO (denoted
P) and a linear combination4 (denoted L) base-
line. Similar to Duh et al (2012), we use
five different BLEU:RIBES weight settings, viz.
(0.0, 1.0), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3) and
(1.0, 0.0), marked L1 through L5 or P1 through P5.
The Pareto frontier is then computed from 80 points
(5 runs and 15 iterations per run) on the devset.
Figure 2(a) shows the Pareto frontier of L and P
baselines using BLEU and RIBES as two metrics.
The frontier of the P dominates that of L for most
part showing that the PMO approach benefits from
picking Pareto points during the optimization.
We use the PMO-ensemble approach to combine
the optimized weights from the 5 tuning runs and
re-decode the devset employing ensemble decoding.
This yields the points LEns and PEns in the plot,
which obtain better scores than most of the indi-
vidual runs of L and P. This ensemble approach of
combining the final weights also generalizes to the
unseen test set as we show later.
Figure 2(b) plots the change in BLEU during tun-
ing in the multiple references and the single refer-
ence scenarios. We show for each baseline method L
and P, plots for two different weight settings that ob-
tain high BLEU and RIBES scores. In both datasets,
our ensemble tuning approach dominates the curves
of the (L and P) baselines. In summary, these results
confirm that the ensemble approach achieves results
that are competitive with previous MMO methods
on the devset Pareto curve. We now provide a more
comprehensive evaluation on the test set.
6.2 Evaluation on Test Set
This section contains multi-metric optimization re-
sults on the unseen test sets, one test set has multi-
ple references and the other has a single-reference.
4Linear combination is a generalized version of the com-
bined (TER-BLEU)/2 metric and its variants.
952
 0.765
 0.766
 0.767
 0.768
 0.769
 0.77
 0.771
 0.772
 0.773
 0.33  0.335  0.34  0.345  0.35
R
I
B
E
S
BLEU
 L1
 L2
L3  L4  
 L5
P1   P2 P3
 P4 P5
LEns  
 PEns
PMO-PROLin-Comb
(a) Pareto frontier and BLEU-RIBES scores: MTA 4-refs devset
 0.26
 0.28
 0.3
 0.32
 0.34
 0.36
 0.38
 2  4  6  8  10  12  14
B
L
E
U
Iterations
L1-mtaL5-mtaP1-mtaP5-mtaEns-Tune-mtaL1-isiL3-isiP1-isiP3-isiEns-Tune-isi
(b) Tuning BLEU scores: MTA 4-refs and ISI 1-ref devsets
Figure 2: Devset (redecode): Comparison of Lin-comb (L) and PMO-PRO (P) with Ensemble decoding (Lens and PEns) and
Ensemble tuning (Ens-Tune)
We plot BLEU scores against other metrics (RIBES,
METEOR and TER) and this allows us to compare
the performance of each metric relative to the de-
facto standard BLEU metric.
Baseline points are identified by single letters B
for BLEU, T for TER, etc. and the baseline (single-
metric optimized) score for each metric is indicated
by a dashed line on the corresponding axis. MMO
points use a series of single letters referring to the
metrics used, e.g. BT for BLEU-TER. The union of
metrics method is identified with the suffix ?J? and
lateen method with suffix ?L? (thus BT-L refers to the
lateen tuning with BLEU-TER). MMO points with-
out any suffix use the ensemble tuning approach.
Figures 3 and 4(a) plot the scores for the MTA test
set with 4-references. We see noticeable and some
statistically significant improvements in BLEU and
RIBES (see Table 2 for BLEU improvements).
All our MMO approaches, except for the union
method, show gains on both BLEU and RIBES axes.
Figures 3(b) and 4(a) show that none of the proposed
methods managed to improve the baseline scores for
METEOR and TER. However, several of our en-
semble tuning combinations work well for both ME-
TEOR (BR, BMRTB3, etc.) and TER (BMRT and
BRT) in that they improved or were close to the
baseline scores in either dimension. We again see in
these figures that the MMO approaches can improve
the BLEU-only tuning by 0.3 BLEU points, without
much drop in other metrics. This is in tune with the
finding that BLEU could be tuned easily (Callison-
Burch et al, 2011) and also explains why it remains
Approach and Tuning Metric(s)
BLEU
MTA ISI
Single Objective Baselines
BLEU 36.06 37.20
METEOR 35.05 36.91
RIBES 33.35 36.60
TER 33.92 35.85
Ensemble Tuning: 2 Metrics
B-M 36.02 37.26
B-R 36.15 37.37
B-T 35.72 36.31
Ensemble Tuning: 3 Metrics
B-M-R 36.36 37.37
B-M-T 36.22 36.89
B-R-T 35.97 36.72
Ensemble Tuning: > 3 Metrics
B-M-R-T 35.94 36.84
B-M-R-T-B3 36.16 37.12
B-M-R-T-B3-B2-B1 36.08 37.24
Table 2: BLEU Scores on MTA (4 refs) and ISI (1 ref) test sets
using the standard mteval script. Boldface scores indicate scores
that are comparable to or better than the baseline BLEU-only
tuning. Italicized scores indicate statistically significant differ-
ences at p-value 0.05 computed with bootstrap significance test.
a popular choice for optimizing SMT systems.
Among the different MMO methods the ensem-
ble tuning performs better than lateen or union ap-
proaches. In terms of the number of metrics being
optimized jointly, we see substantial gains when us-
ing a small number (typically 2 or 3) of metrics. Re-
sults seem to suffer beyond this number; probably
because there might not be a space that contain so-
lution(s) optimal for all the metrics that are jointly
optimized.
We hypothesize that each metric correlates well
953
 0.808
 0.81
 0.812
 0.814
 0.816
 0.818
 0.82
 0.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37
R
I
B
E
S
BLEU
 B
 M
 R
 T  PEns
 LEns
BM 
BR  
 BT
MR  
 BMR
 BMT
 BRT
BMRT  
 BMRTB3
 BMRTB3B2B1
 BM-J
 BR-J
 BT-J
BMT-J  
 BM-L
BR-L  
BMR-L  
(a) BLEU-RIBES scores
 0.495
 0.5
 0.505
 0.51
 0.515
 0.52
 0.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37
M
E
T
E
O
R
BLEU
B
 M
 R
 T
 PEns(BR)
LEns(BR)  BM  BR
 BT
MR  
 BMR
 BMT
 BRT
 BMRTB3BMRTB3B2B1  
 BM-J
 BR-J
 BT-J
BMT-J  
 BM-L
BR-L  
BMR-L  
(b) BLEU-METEOR scores
Figure 3: MTA 4-refs testset: Comparison of different MMO approaches. The dashed lines correspond to baseline scores tuned on
the respective metrics in the axes. The union of metrics method is identified with the suffix J and lateen with suffix L.
 0.4
 0.405
 0.41
 0.415
 0.42
 0.425
 0.43
 0.435
 0.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37
n
T
E
R
BLEU
B  
 M
 R
 T
 PEns(BR)
LEns(BR)  
BM   BR
 BT
MR   BMR
 BMTBRT  
 BMRT
 BMRTB3
 BM-J
BR-J  
 BT-J BMT-J  
BM-L  
BR-L  
BMR-L  
(a) MTA (4-refs)
 0.44
 0.445
 0.45
 0.455
 0.46
 0.465
 0.355  0.36  0.365  0.37  0.375
n
T
E
R
BLEU
B  
M  
 R
 T
PEns(BR)   LEns(BR)
  BM
 BR
 BMR
 B3MT
BRT  
BMRT  
 BMRTB3
BMRTB3B2B1  
 BR-J
BR-L  
(b) ISI (1-ref)
Figure 4: BLEU-TER scores: Comparison of different MMO approaches. We plot nTER (1-TER) scores for easy reading of the
plots. The dashed lines correspond to baseline scores tuned on the respective metrics in the axes.
(in a looser sense) with few others, but not all. For
example, union optimizations BR-J and BMT-J per-
form close to or better than RIBES and TER base-
lines, but get very poor score in METEOR. On the
other hand BM-J is close to the METEOR baseline,
while doing poorly on the RIBES and TER. This be-
haviour is also evident from the single-metric base-
lines, where R and T-only settings are clearly distin-
guished from the M-only system. It is not clear if
such distinct classes of metrics could be bridged by
some optimal solution and the metric dichotomy re-
quires further study as this is key to practical multi-
metric tuning in SMT.
The lateen and union approaches appear to be
very sensitive to the number of metrics and they
generally perform well for two metrics case and
show degradation for more metrics. Unlike other
approaches, the union approach failed to improve
over the baseline BLEU and this could be attributed
to the conflict of interest among the metrics, while
choosing example points for the optimization step.
The positive example preferred by a particular met-
ric could be a negative example for the other metric.
This would only confuse the optimizer resulting in
poor solutions. Our future line of work would be to
study the effect of avoiding such of conflicting ex-
amples in the union approach.
For the single-reference (ISI) dataset, we only
plot the BLEU-TER case in Figure 4(b) due to lack
of space. The results are similar to the multiple
references set indicating that MMO approaches are
equally effective for single references5. Table 2
5One could argue that MMO methods require multiple ref-
erences since each metric might be picking out a different ref-
954
Metric
Single-metric Tuning Ensemble Tuning
B-only M-only B-M-R
BLEU 37.89 37.18 39.01
HBLEU 51.93 53.59 53.14
METEOR 61.31 61.56 61.68
HMETEOR 72.35 72.39 72.74
TER 0.520 0.532 0.516
HTER 0.361 0.370 0.346
Table 3: Post-editing Human Evaluation: Regular (untargeted)
and human-targeted scores. Human targeted scores are com-
puted against the post-edited reference and regular scores are
computed with the original references. Best scores are in bold-
face and statistically significant ones (at p = 0.05) are italicized.
shows the BLEU scores for our ensemble tuning
method (for various combinations) and we again see
improvements over the baseline BLEU-only tuning.
6.3 Human Evaluation
So far we have shown that multi-metric optimiza-
tion can improve over single-metric tuning on a sin-
gle metric like BLEU and we have shown that our
methods find a tuned model that performs well with
respect to multiple metrics. Is the output that scores
higher on multiple metrics actually a better trans-
lation? To verify this, we conducted a post-editing
human evaluation experiment. We compared our en-
semble tuning approach involving BLEU, METEOR
and RIBES (B-M-R) with systems optimized for
BLEU (B-only) and METEOR (M-only).
We selected 100 random sentences (that are at
least 15 words long) from the Arabic-English MTA
(4 references) test set and translated them using the
three systems (two single metric systems and BMR
ensemble tuning). We shuffled the resulting trans-
lations and split them into 3 sets such that each set
has equal number of the translations from three sys-
tems. The translations were edited by three human
annotators in a post-editing setup, where the goal
was to edit the translations to make them as close
to the references as possible, using the Post-Editing
Tool: PET (Aziz et al, 2012). The annotators were
not Arabic-literate and relied only on the reference
translations during post-editing. The identifiers that
link each translation to the system that generated it
are removed to avoid annotator bias.
In the end we collated post-edited translations for
each system and then computed the system-level
erence sentence. Our experiment shows that even with a single
reference MMO methods can work.
human-targeted (HBLEU, HMETEOR, HTER)
scores, by using respective post-edited translations
as the reference. First comparing the HTER (Snover
et al, 2006) scores shown in Table 3, we see
that the single-metric system optimized for ME-
TEOR performs slightly worse than the one op-
timized for BLEU, despite using METEOR-hter
version (Denkowski and Lavie, 2011). Ensemble
tuning-based system optimized for three metrics (B-
M-R) improves HTER by 4% and 6.3% over BLEU
and METEOR optimized systems respectively.
The single-metric system tuned with M-only set-
ting scores high on HBLEU, closely followed by the
ensemble system. We believe this to be caused by
chance rather than any systematic gains by the M-
only tuning; the ensemble system scores high on
HMETEOR compared to the M-only system. While
HTER captures the edit distance to the targeted ref-
erence, HMETEOR and HBLEU metrics capture
missing content words or synonyms by exploiting
n-grams and paraphrase matching.
We also computed the regular variants (BLEU,
METEOR and TER), which are scored against orig-
inal references. The ensemble system outperformed
the single-metric systems in all the three metrics.
The improvements were also statistically significant
at p-value of 0.05 for BLEU and TER.
7 Conclusion
We propose and present a comprehensive study of
several multi-metric optimization (MMO) methods
in SMT. First, by exploiting the idea of ensemble de-
coding (Razmara et al, 2012), we propose an effec-
tive way to combine multiple Pareto-optimal model
weights from previous MMO methods (e.g. Duh et
al. (2012)), obviating the need for manually trading
off among metrics. We also proposed two new vari-
ants: lateen-style MMO and union of metrics.
We also extended ensemble decoding to a new
tuning algorithm called ensemble tuning. This
method demonstrates statistically significant gains
for BLEU and RIBES with modest reduction in ME-
TEOR and TER. Further, in our human evaluation,
ensemble tuning obtains the best HTER among com-
peting baselines, confirming that optimizing on mul-
tiple metrics produces human-preferred translations
compared to the conventional optimization approach
involving a single metric.
955
References
Michael Auli and Adam Lopez. 2011. Training a log-
linear parser with loss functions via softmax-margin.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 333?
343, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Wilker Aziz, Sheila Castilho Monteiro de Sousa, and
Lucia Specia. 2012. PET: a tool for post-editing
and assessing machine translation. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources Associa-
tion (ELRA).
Marzieh Bazrafshan, Tagyoung Chung, and Daniel
Gildea. 2012. Tuning as linear regression. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 543?547, Montre?al, Canada. ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. ACL.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical mt system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the ACL, pages 555?
563. ACL.
Boxing Chen, Roland Kuhn, and Samuel Larkin. 2012.
Port: a precision-order-recall mt evaluation metric for
tuning. In Proceedings of the 50th Annual Meeting
of the ACL (Volume 1: Long Papers), pages 930?939,
Jeju Island, Korea. ACL.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 427?436, Montre?al, Canada. ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 224?233. ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization and
Evaluation of Machine Translation Systems. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation, Edinburgh, Scotland, July. ACL.
Jacob Devlin and Spyros Matsoukas. 2012. Trait-based
hypothesis selection for machine translation. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 528?532. ACL.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2012. Learning to
translate with multiple objectives. In Proceedings of
the 50th Annual Meeting of the ACL, Jeju Island, Ko-
rea. ACL.
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The university of maryland statistical
machine translation system for the fourth workshop on
machine translation. In Proc. of the Fourth Workshop
on Machine Translation.
Kevin Gimpel and Noah A. Smith. 2012. Struc-
tured ramp loss minimization for machine transla-
tion. In Proceedings of the 2012 Conference of
the North American Chapter of the ACL: Human
Language Technologies, pages 221?231, Montre?al,
Canada. ACL.
Keith Hall, Ryan T. McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives. In
Proceedings of the Empirical Methods in Natural Lan-
guage Processing, pages 1489?1499.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 292?301, Jeju Island, Ko-
rea. ACL.
Yifan He and Andy Way. 2009. Improving the objec-
tive function in minimum error rate training. In MT
Summit.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, Edin-
burgh, Scotland. ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic evalu-
ation of translation quality for distant language pairs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 944?
952, Cambridge, MA. ACL.
Thorsten Joachims. 2006. Training linear svms in linear
time. In Proceedings of the 12th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, pages 217?226.
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23(2-3):105?115.
956
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
R. T. Marler and J. S. Arora. 2004. Survey of
multi-objective optimization methods for engineer-
ing. Structural and Multidisciplinary Optimization,
26(6):369?395, April.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2008.
Automatic evaluation measures for statistical machine
translation system optimization. In International Con-
ference on Language Resources and Evaluation, Mar-
rakech, Morocco.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160?167. ACL.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2-3):181?193.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of Asso-
ciation of Computational Linguistics, pages 311?318.
ACL.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th Annual Meeting of the ACL, Jeju, Re-
public of Korea. ACL.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics (PBML), 97(97):83?98.
Christophe Servan and Holger Schwenk. 2011. Optimis-
ing multiple metrics with mert. Prague Bull. Math.
Linguistics, 96:109?118.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 11?21, Jeju Island, Korea. ACL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas, pages 223?231.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen EM: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the Empirical Methods in
Natural Language Processing, pages 1269?1280. As-
sociation of Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773. ACL.
Taro Watanabe. 2012. Optimized online rank learn-
ing for machine translation. In Proceedings of the
2012 Conference of the North American Chapter of the
ACL: Human Language Technologies, pages 253?262,
Montre?al, Canada, June. ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Daqi Zheng, Yifan He, Yang Liu, and Qun Liu. 2012.
Maximum rank correlation training for statistical ma-
chine translation. In MT Summit XIII.
957
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 32?42,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Combining Morpheme-based Machine Translation with
Post-processing Morpheme Prediction
Ann Clifton and Anoop Sarkar
Simon Fraser University
Burnaby, British Columbia, Canada
{ann clifton,anoop}@sfu.ca
Abstract
This paper extends the training and tun-
ing regime for phrase-based statistical ma-
chine translation to obtain fluent trans-
lations into morphologically complex lan-
guages (we build an English to Finnish
translation system). Our methods use
unsupervised morphology induction. Un-
like previous work we focus on morpho-
logically productive phrase pairs ? our
decoder can combine morphemes across
phrase boundaries. Morphemes in the tar-
get language may not have a corresponding
morpheme or word in the source language.
Therefore, we propose a novel combina-
tion of post-processing morphology pre-
diction with morpheme-based translation.
We show, using both automatic evaluation
scores and linguistically motivated analy-
ses of the output, that our methods out-
perform previously proposed ones and pro-
vide the best known results on the English-
Finnish Europarl translation task. Our
methods are mostly language independent,
so they should improve translation into
other target languages with complex mor-
phology.
1 Translation and Morphology
Languages with rich morphological systems
present significant hurdles for statistical ma-
chine translation (SMT), most notably data
sparsity, source-target asymmetry, and prob-
lems with automatic evaluation.
In this work, we propose to address the prob-
lem of morphological complexity in an English-
to-Finnish MT task within a phrase-based trans-
lation framework. We focus on unsupervised
segmentation methods to derive the morpholog-
ical information supplied to the MT model in
order to provide coverage on very large data-
sets and for languages with few hand-annotated
resources. In fact, in our experiments, unsuper-
vised morphology always outperforms the use
of a hand-built morphological analyzer. Rather
than focusing on a few linguistically motivated
aspects of Finnish morphological behaviour, we
develop techniques for handling morphological
complexity in general. We chose Finnish as our
target language for this work, because it ex-
emplifies many of the problems morphologically
complex languages present for SMT. Among all
the languages in the Europarl data-set, Finnish
is the most difficult language to translate from
and into, as was demonstrated in the MT Sum-
mit shared task (Koehn, 2005). Another reason
is the current lack of knowledge about how to ap-
ply SMT successfully to agglutinative languages
like Turkish or Finnish.
Our main contributions are: 1) the intro-
duction of the notion of segmented translation
where we explicitly allow phrase pairs that can
end with a dangling morpheme, which can con-
nect with other morphemes as part of the trans-
lation process, and 2) the use of a fully seg-
mented translation model in combination with
a post-processing morpheme prediction system,
using unsupervised morphology induction. Both
of these approaches beat the state of the art
on the English-Finnish translation task. Mor-
phology can express both content and function
categories, and our experiments show that it is
important to use morphology both within the
translation model (for morphology with content)
and outside it (for morphology contributing to
fluency).
Automatic evaluation measures for MT,
BLEU (Papineni et al, 2002), WER (Word
Error Rate) and PER (Position Independent
Word Error Rate) use the word as the basic
unit rather than morphemes. In a word com-
32
prised of multiple morphemes, getting even a
single morpheme wrong means the entire word is
wrong. In addition to standard MT evaluation
measures, we perform a detailed linguistic anal-
ysis of the output. Our proposed approaches
are significantly better than the state of the art,
achieving the highest reported BLEU scores on
the English-Finnish Europarl version 3 data-set.
Our linguistic analysis shows that our models
have fewer morpho-syntactic errors compared to
the word-based baseline.
2 Models
2.1 Baseline Models
We set up three baseline models for compari-
son in this work. The first is a basic word-
based model (called Baseline in the results);
we trained this on the original unsegmented
version of the text. Our second baseline is a
factored translation model (Koehn and Hoang,
2007) (called Factored), which used as factors
the word, ?stem?1 and suffix. These are de-
rived from the same unsupervised segmenta-
tion model used in other experiments. The re-
sults (Table 3) show that a factored model was
unable to match the scores of a simple word-
based baseline. We hypothesize that this may
be an inherently difficult representational form
for a language with the degree of morphologi-
cal complexity found in Finnish. Because the
morphology generation must be precomputed,
for languages with a high degree of morpho-
logical complexity, the combinatorial explosion
makes it unmanageable to capture the full range
of morphological productivity. In addition, be-
cause the morphological variants are generated
on a per-word basis within a given phrase, it
excludes productive morphological combination
across phrase boundaries and makes it impossi-
ble for the model to take into account any long-
distance dependencies between morphemes. We
conclude from this result that it may be more
useful for an agglutinative language to use mor-
phology beyond the confines of the phrasal unit,
and condition its generation on more than just
the local target stem. In order to compare the
1see Section 2.2.
performance of unsupervised segmentation for
translation, our third baseline is a segmented
translation model based on a supervised segmen-
tation model (called Sup), using the hand-built
Omorfi morphological analyzer (Pirinen and Lis-
tenmaa, 2007), which provided slightly higher
BLEU scores than the word-based baseline.
2.2 Segmented Translation
For segmented translation models, it cannot be
taken for granted that greater linguistic accu-
racy in segmentation yields improved transla-
tion (Chang et al, 2008). Rather, the goal in
segmentation for translation is instead to maxi-
mize the amount of lexical content-carrying mor-
phology, while generalizing over the information
not helpful for improving the translation model.
We therefore trained several different segmenta-
tion models, considering factors of granularity,
coverage, and source-target symmetry.
We performed unsupervised segmentation of
the target data, using Morfessor (Creutz and
Lagus, 2005) and Paramor (Monson, 2008), two
top systems from the Morpho Challenge 2008
(their combined output was the Morpho Chal-
lenge winner). However, translation models
based upon either Paramor alone or the com-
bined systems output could not match the word-
based baseline, so we concentrated on Morfes-
sor. Morfessor uses minimum description length
criteria to train a HMM-based segmentation
model. When tested against a human-annotated
gold standard of linguistic morpheme segmen-
tations for Finnish, this algorithm outperforms
competing unsupervised methods, achieving an
F-score of 67.0% on a 3 million sentence cor-
pus (Creutz and Lagus, 2006). Varying the per-
plexity threshold in Morfessor does not segment
more word types, but rather over-segments the
same word types. In order to get robust, com-
mon segmentations, we trained the segmenter
on the 5000 most frequent words2; we then used
this to segment the entire data set. In order
to improve coverage, we then further segmented
2For the factored model baseline we also used the
same setting perplexity = 30, 5,000 most frequent words,
but with all but the last suffix collapsed and called the
?stem?.
33
Training Set Test Set
Total 64,106,047 21,938
Morph 30,837,615 5,191
Hanging Morph 10,906,406 296
Table 1: Morpheme occurences in the phrase table
and in translation.
any word type that contained a match from the
most frequent suffix set, looking for the longest
matching suffix character string. We call this
method Unsup L-match.
After the segmentation, word-internal mor-
pheme boundary markers were inserted into
the segmented text to be used to reconstruct
the surface forms in the MT output. We
then trained the Moses phrase-based system
(Koehn et al, 2007) on the segmented and
marked text. After decoding, it was a sim-
ple matter to join together all adjacent mor-
phemes with word-internal boundary markers
to reconstruct the surface forms. Figure 1(a)
gives the full model overview for all the vari-
ants of the segmented translation model (super-
vised/unsupervised; with and without the Un-
sup L-match procedure).
Table 1 shows how morphemes are being used
in the MT system. Of the phrases that included
segmentations (?Morph? in Table 1), roughly a
third were ?productive?, i.e. had a hanging mor-
pheme (with a form such as stem+) that could
be joined to a suffix (?Hanging Morph? in Ta-
ble 1). However, in phrases used while decoding
the development and test data, roughly a quar-
ter of the phrases that generated the translated
output included segmentations, but of these,
only a small fraction (6%) had a hanging mor-
pheme; and while there are many possible rea-
sons to account for this we were unable to find
a single convincing cause.
2.3 Morphology Generation
Morphology generation as a post-processing step
allows major vocabulary reduction in the trans-
lation model, and allows the use of morpholog-
ically targeted features for modeling inflection.
A possible disadvantage of this approach is that
in this model there is no opportunity to con-
sider the morphology in translation since it is
removed prior to training the translation model.
Morphology generation models can use a vari-
ety of bilingual and contextual information to
capture dependencies between morphemes, of-
ten more long-distance than what is possible us-
ing n-gram language models over morphemes in
the segmented model.
Similar to previous work (Minkov et al, 2007;
Toutanova et al, 2008), we model morphology
generation as a sequence learning problem. Un-
like previous work, we use unsupervised mor-
phology induction and use automatically gener-
ated suffix classes as tags. The first phase of our
morphology prediction model is to train a MT
system that produces morphologically simplified
word forms in the target language. The output
word forms are complex stems (a stem and some
suffixes) but still missing some important suffix
morphemes. In the second phase, the output of
the MT decoder is then tagged with a sequence
of abstract suffix tags. In particular, the out-
put of the MT decoder is a sequence of complex
stems denoted by x and the output is a sequence
of suffix class tags denoted by y. We use a list
of parts from (x,y) and map to a d -dimensional
feature vector ?(x,y), with each dimension be-
ing a real number. We infer the best sequence
of tags using:
F (x) = argmax
y
p(y | x,w)
where F (x ) returns the highest scoring output
y?. A conditional random field (CRF) (Lafferty
et al, 2001) defines the conditional probability
as a linear score for each candidate y and a global
normalization term:
log p(y | x,w) = ?(x,y) ?w ? logZ
where Z =
?
y??GEN(x) exp(?(x,y
?) ? w). We
use stochastic gradient descent (using crfsgd3)
to train the weight vector w. So far, this is
all off-the-shelf sequence learning. However, the
output y? from the CRF decoder is still only a
sequence of abstract suffix tags. The third and
final phase in our morphology prediction model
3http://leon.bottou.org/projects/sgd
34
Morphological Pre-Processing
English Training Data Finnish Training Datawords
stem+ +morph
words
Post-Process:Morph Re-Stitching
stem+ +morph
Evaluation against original reference
Fully inflected surface form
MT SystemAlignment:word          word          word
stem+      +morph        stem
(a) Segmented Translation Model
Morphlgica Pe-ci-gsnETDrrrrrrrrrrnETDrrrrrrrrrrnETD
lgictrrrrrtcETFwdtrrrrrlgic
METFwE EePm+ r:TiR:TEmillP-erdEnglish?Training?Data Finnish?Training?DatanETDl
lgictrtcETFwdt
nETDl
:ElgR:TEmillrdsMETFwrSiRpgPgmwP-e
lgictrtcETFwdt
:ElgR:TEmillrvsruSfMETFwE Eehryi-iT+gPE-
mEcF iArlgicsrlgictcETFwdt ?+-e?+eirMEDi l?T?+mir?ETcrc+FFP-elgictcETFwdtrtcETFwv
?? ?+gPE-r+e+P-lgrETPeP-+ rTi?iTi-mi
f?  hrP-? imgiDrl?T?+mir?ETc
METFwE EePm+ r:TiR:TEmillP-ervlgictrtcETFwdtrtcETFwv
(b) Post-Processing Model Translation & Generation
Figure 1: Training and testing pipelines for the SMT models.
is to take the abstract suffix tag sequence y? and
then map it into fully inflected word forms, and
rank those outputs using a morphemic language
model. The abstract suffix tags are extracted
from the unsupervised morpheme learning pro-
cess, and are carefully designed to enable CRF
training and decoding. We call this model CRF-
LM for short. Figure 1(b) shows the full pipeline
and Figure 2 shows a worked example of all the
steps involved.
We use the morphologically segmented train-
ing data (obtained using the segmented corpus
described in Section 2.24) and remove selected
suffixes to create a morphologically simplified
version of the training data. The MT model is
trained on the morphologically simplified train-
ing data. The output from the MT system is
then used as input to the CRF model. The
CRF model was trained on a ?210,000 Finnish
sentences, consisting of ?1.5 million tokens; the
2,000 sentence Europarl test set consisted of
41,434 stem tokens. The labels in the output
sequence y were obtained by selecting the most
productive 150 stems, and then collapsing cer-
tain vowels into equivalence classes correspond-
ing to Finnish vowel harmony patterns. Thus
4Note that unlike Section 2.2 we do not use Unsup
L-match because when evaluating the CRF model on the
suffix prediction task it obtained 95.61% without using
Unsup L-match and 82.99% when using Unsup L-match.
variants -ko? and -ko become vowel-generic en-
clitic particle -kO, and variants -ssa? and -ssa
become the vowel-generic inessive case marker
-ssA, etc. This is the only language-specific com-
ponent of our translation model. However, we
expect this approach to work for other agglu-
tinative languages as well. For fusional lan-
guages like Spanish, another mapping from suf-
fix to abstract tags might be needed. These suf-
fix transformations to their equivalence classes
prevent morphophonemic variants of the same
morpheme from competing against each other in
the prediction model. This resulted in 44 possi-
ble label outputs per stem which was a reason-
able sized tag-set for CRF training. The CRF
was trained on monolingual features of the seg-
mented text for suffix prediction, where t is the
current token:
Word Stem st?n, .., st, .., st+n(n = 4)
Morph Prediction yt?2, yt?1, yt
With this simple feature set, we were able to
use features over longer distances, resulting in
a total of 1,110,075 model features. After CRF
based recovery of the suffix tag sequence, we use
a bigram language model trained on a full seg-
mented version on the training data to recover
the original vowels. We used bigrams only, be-
cause the suffix vowel harmony alternation de-
pends only upon the preceding phonemes in the
word from which it was segmented.
35
original training data:
koskevaa mietinto?a? ka?sitella?a?n
segmentation:
koske+ +va+ +a mietinto?+ +a? ka?si+ +te+ +lla?+ +a?+ +n
(train bigram language model with mapping A = { a, a? })
map final suffix to abstract tag-set:
koske+ +va+ +A mietinto?+ +A ka?si+ +te+ +lla?+ +a?+ +n
(train CRF model to predict the final suffix)
peeling of final suffix:
koske+ +va+ mietinto?+ ka?si+ +te+ +lla?+ +a?+
(train SMT model on this transformation of training data)
(a) Training
decoder output:
koske+ +va+ mietinto?+ ka?si+ +te+ +lla?+ +a?+
decoder output stitched up:
koskeva+ mietinto?+ ka?sitella?a?+
CRF model prediction:
x = ?koskeva+ mietinto?+ ka?sitella?a?+?, y = ?+A +A +n?
koskeva+ +A mietinto?+ +A ka?sitella?a?+ +n
unstitch morphemes:
koske+ +va+ +A mietinto?+ +A ka?si+ +te+ +lla?+ +a?+ +n
language model disambiguation:
koske+ +va+ +a mietinto?+ +a? ka?si+ +te+ +lla?+ +a?+ +n
final stitching:
koskevaa mietinto?a? ka?sitella?a?n
(the output is then compared to the reference translation)
(b) Decoding
Figure 2: Worked example of all steps in the post-processing morphology prediction model.
3 Experimental Results
For all of the models built in this paper, we used
the Europarl version 3 corpus (Koehn, 2005)
English-Finnish training data set, as well as the
standard development and test data sets. Our
parallel training data consists of ?1 million sen-
tences of 40 words or less, while the develop-
ment and test sets were each 2,000 sentences
long. In all the experiments conducted in this
paper, we used the Moses5 phrase-based trans-
lation system (Koehn et al, 2007), 2008 version.
We trained all of the Moses systems herein using
the standard features: language model, reorder-
ing model, translation model, and word penalty;
in addition to these, the factored experiments
called for additional translation and generation
features for the added factors as noted above.
We used in all experiments the following set-
tings: a hypothesis stack size 100, distortion
limit 6, phrase translations limit 20, and maxi-
mum phrase length 20. For the language models,
we used SRILM 5-gram language models (Stol-
cke, 2002) for all factors. For our word-based
Baseline system, we trained a word-based model
using the same Moses system with identical set-
tings. For evaluation against segmented trans-
lation systems in segmented forms before word
reconstruction, we also segmented the baseline
system?s word-based output. All the BLEU
scores reported are for lowercase evaluation.
We did an initial evaluation of the segmented
output translation for each system using the no-
5http://www.statmt.org/moses/
Segmentation m-BLEU No Uni
Baseline 14.84?0.69 9.89
Sup 18.41?0.69 13.49
Unsup L-match 20.74?0.68 15.89
Table 2: Segmented Model Scores. Sup refers to the
supervised segmentation baseline model. m-BLEU
indicates that the segmented output was evaluated
against a segmented version of the reference (this
measure does not have the same correlation with hu-
man judgement as BLEU). No Uni indicates the seg-
mented BLEU score without unigrams.
tion of m-BLEU score (Luong et al, 2010) where
the BLEU score is computed by comparing the
segmented output with a segmented reference
translation. Table 2 shows the m-BLEU scores
for various systems. We also show the m-BLEU
score without unigrams, since over-segmentation
could lead to artificially high m-BLEU scores.
In fact, if we compare the relative improvement
of our m-BLEU scores for the Unsup L-match
system we see a relative improvement of 39.75%
over the baseline. Luong et. al. (2010) report
an m-BLEU score of 55.64% but obtain a rel-
ative improvement of 0.6% over their baseline
m-BLEU score. We find that when using a
good segmentation model, segmentation of the
morphologically complex target language im-
proves model performance over an unsegmented
baseline (the confidence scores come from boot-
strap resampling). Table 3 shows the evalua-
tion scores for all the baselines and the methods
introduced in this paper using standard word-
based lowercase BLEU, WER and PER. We do
36
Model BLEU WER TER
Baseline 14.68 74.96 72.42
Factored 14.22 76.68 74.15
(Luong et.al, 2010) 14.82 - -
Sup 14.90 74.56 71.84
Unsup L-match 15.09? 74.46 71.78
CRF-LM 14.87 73.71 71.15
Table 3: Test Scores: lowercase BLEU, WER and
TER. The ? indicates a statistically significant im-
provement of BLEU score over the Baseline model.
The boldface scores are the best performing scores
per evaluation measure.
better than (Luong et al, 2010), the previous
best score for this task. We also show a bet-
ter relative improvement over our baseline when
compared to (Luong et al, 2010): a relative im-
provement of 4.86% for Unsup L-match com-
pared to our baseline word-based model, com-
pared to their 1.65% improvement over their
baseline word-based model. Our best perform-
ing method used unsupervised morphology with
L-match (see Section 2.2) and the improvement
is significant: bootstrap resampling provides a
confidence margin of ?0.77 and a t-test (Collins
et al, 2005) showed significance with p = 0.001.
3.1 Morphological Fluency Analysis
To see how well the models were doing at get-
ting morphology right, we examined several pat-
terns of morphological behavior. While we wish
to explore minimally supervised morphological
MT models, and use as little language spe-
cific information as possible, we do want to
use linguistic analysis on the output of our sys-
tem to see how well the models capture essen-
tial morphological information in the target lan-
guage. So, we ran the word-based baseline sys-
tem, the segmented model (Unsup L-match),
and the prediction model (CRF-LM) outputs,
along with the reference translation through the
supervised morphological analyzer Omorfi (Piri-
nen and Listenmaa, 2007). Using this analy-
sis, we looked at a variety of linguistic construc-
tions that might reveal patterns in morphologi-
cal behavior. These were: (a) explicitly marked
noun forms, (b) noun-adjective case agreement,
(c) subject-verb person/number agreement, (d)
transitive object case marking, (e) postposi-
tions, and (f) possession. In each of these cat-
egories, we looked for construction matches on
a per-sentence level between the models? output
and the reference translation.
Table 4 shows the models? performance on the
constructions we examined. In all of the cat-
egories, the CRF-LM model achieves the best
precision score, as we explain below, while the
Unsup L-match model most frequently gets the
highest recall score.
A general pattern in the most prevalent of
these constructions is that the baseline tends
to prefer the least marked form for noun cases
(corresponding to the nominative) more than
the reference or the CRF-LM model. The base-
line leaves nouns in the (unmarked) nominative
far more than the reference, while the CRF-LM
model comes much closer, so it seems to fare
better at explicitly marking forms, rather than
defaulting to the more frequent unmarked form.
Finnish adjectives must be marked with the
same case as their head noun, while verbs must
agree in person and number with their subject.
We saw that in both these categories, the CRF-
LM model outperforms for precision, while the
segmented model gets the best recall.
In addition, Finnish generally marks di-
rect objects of verbs with the accusative
or the partitive case; we observed more
accusative/partitive-marked nouns following
verbs in the CRF-LM output than in the base-
line, as illustrated by example (1) in Fig. 3.
While neither translation picks the same verb as
in the reference for the input ?clarify,? the CRF-
LM-output paraphrases it by using a grammat-
ical construction of the transitive verb followed
by a noun phrase inflected with the accusative
case, correctly capturing the transitive construc-
tion. The baseline translation instead follows
?give? with a direct object in the nominative
case.
To help clarify the constructions in question,
we have used Google Translate6 to provide back-
6http://translate.google.com/
37
Construction Freq. Baseline Unsup L-match CRF-LM
P R F P R F P R F
Noun Marking 5.5145 51.74 78.48 62.37 53.11 83.63 64.96 54.99 80.21 65.25
Trans Obj 1.0022 32.35 27.50 29.73 33.47 29.64 31.44 35.83 30.71 33.07
Noun-Adj Agr 0.6508 72.75 67.16 69.84 69.62 71.00 70.30 73.29 62.58 67.51
Subj-Verb Agr 0.4250 56.61 40.67 47.33 55.90 48.17 51.48 57.79 40.17 47.40
Postpositions 0.1138 43.31 29.89 35.37 39.31 36.96 38.10 47.16 31.52 37.79
Possession 0.0287 66.67 70.00 68.29 75.68 70.00 72.73 78.79 60.00 68.12
Table 4: Model Accuracy: Morphological Constructions. Freq. refers to the construction?s average number
of occurrences per sentence, also averaged over the various translations. P, R and F stand for precision,
recall and F-score. The constructions are listed in descending order of their frequency in the texts. The
highlighted value in each column is the most accurate with respect to the reference value.
translations of our MT output into English; to
contextualize these back-translations, we have
provided Google?s back-translation of the refer-
ence.
The use of postpositions shows another dif-
ference between the models. Finnish postposi-
tions require the preceding noun to be in the
genitive or sometimes partitive case, which oc-
curs correctly more frequently in the CRF-LM
than the baseline. In example (2) in Fig. 3,
all three translations correspond to the English
text, ?with the basque nationalists.? However,
the CRF-LM output is more grammatical than
the baseline, because not only do the adjective
and noun agree for case, but the noun ?bask-
ien? to which the postposition ?kanssa? belongs is
marked with the correct genitive case. However,
this well-formedness is not rewarded by BLEU,
because ?baskien? does not match the reference.
In addition, while Finnish may express pos-
session using case marking alone, it has another
construction for possession; this can disam-
biguate an otherwise ambiguous clause. This al-
ternate construction uses a pronoun in the geni-
tive case followed by a possessive-marked noun;
we see that the CRF-LM model correctly marks
this construction more frequently than the base-
line. As example (3) in Fig. 3 shows, while nei-
ther model correctly translates ?matkan? (?trip?),
the baseline?s output attributes the inessive
?yhteydess? (?connection?) as belonging to ?tu-
lokset? (?results?), and misses marking the pos-
session linking it to ?Commissioner Fischler?.
Our manual evaluation shows that the CRF-
LM model is producing output translations that
are more morphologically fluent than the word-
based baseline and the segmented translation
Unsup L-match system, even though the word
choices lead to a lower BLEU score overall when
compared to Unsup L-match.
4 Related Work
The work on morphology in MT can be grouped
into three categories, factored models, seg-
mented translation, and morphology generation.
Factored models (Koehn and Hoang, 2007)
factor the phrase translation probabilities over
additional information annotated to each word,
allowing for text to be represented on multi-
ple levels of analysis. We discussed the draw-
backs of factored models for our task in Sec-
tion 2.1. While (Koehn and Hoang, 2007; Yang
and Kirchhoff, 2006; Avramidis and Koehn,
2008) obtain improvements using factored mod-
els for translation into English, German, Span-
ish, and Czech, these models may be less useful
for capturing long-distance dependencies in lan-
guages with much more complex morphological
systems such as Finnish. In our experiments
factored models did worse than the baseline.
Segmented translation performs morphologi-
cal analysis on the morphologically complex text
for use in the translation model (Brown et al,
1993; Goldwater and McClosky, 2005; de Gis-
pert and Marin?o, 2008). This method unpacks
complex forms into simpler, more frequently oc-
curring components, and may also increase the
symmetry of the lexically realized content be-
38
(1) Input: ?the charter we are to approve today both strengthens and gives visible shape to the common fundamental rights
and values our community is to be based upon.?
a. Reference: perusoikeuskirja , jonka ta?na?a?n aiomme hyva?ksya? , seka? vahvistaa etta? selventa?a? (sel-
venta?a?/VERB/ACT/INF/SG/LAT-clarify) niita? (ne/PRONOUN/PL/PAR-them) yhteisia? perusoikeuksia ja -
arvoja , joiden on oltava yhteiso?mme perusta.
Back-translation: ?Charter of Fundamental Rights, which today we are going to accept that clarify and strengthen
the common fundamental rights and values, which must be community based.?
b. Baseline: perusoikeuskirja me hyva?ksymme ta?na?a?n molemmat vahvistaa ja antaa (antaa/VERB/INF/SG/LAT-
give) na?kyva? (na?kya?/VERB/ACT/PCP/SG/NOM-visible) muokata yhteista? perusoikeuksia ja arvoja on perustut-
tava.
Back-translation: ?Charter today, we accept both confirm and modify to make a visible and common values, funda-
mental rights must be based.?
c. CRF-LM: perusoikeuskirja on hyva?ksytty ta?na?a?n , seka? vahvistaa ja antaa (antaa/VERB/ACT/INF/SG/LAT-give)
konkreettisen (konkreettinen/ADJECTIVE/SG/GEN,ACC-concrete) muodon (muoto/NOUN/SG/GEN,ACC-
shape) yhteisia? perusoikeuksia ja perusarvoja , yhteiso?n on perustuttava.
Back-translation: ?Charter has been approved today, and to strengthen and give concrete shape to the common
basic rights and fundamental values, the Community must be based.?
(2) Input: ?with the basque nationalists?
a. Reference: baskimaan kansallismielisten kanssa
basque-SG/NOM+land-SG/GEN,ACC nationalists-PL/GEN with-POST
b. Baseline: baskimaan kansallismieliset kanssa
basque-SG/NOM-+land-SG/GEN,ACC kansallismielinen-PL/NOM,ACC-nationalists POST-with
c. CRF-LM: kansallismielisten baskien kanssa
nationalists-PL/GEN basques-PL/GEN with-POST
(3) Input: ?and in this respect we should value the latest measures from commissioner fischler , the results of his trip to
morocco on the 26th of last month and the high level meetings that took place, including the one with the king
himself?
a. Reference: ja ta?ssa? mielessa? osaamme myo?s arvostaa komission ja?sen fischlerin viimeisimpia? toimia , jotka ovat
ha?nen (ha?nen/GEN-his) marokkoon 26 lokakuuta tekemns (tekema?nsa?/POSS-his) matkan (matkan/GEN-
tour) ja korkean tason kokousten jopa itsensa? kuninkaan kanssa tulosta
Back-translation: ?and in this sense we can also appreciate the Commissioner Fischler?s latest actions, which are his
to Morocco 26 October trip to high-level meetings and even the king himself with the result
b. Baseline: ja ta?ssa? yhteydessa? olisi arvoa viimeisin toimia komission ja?sen fischler , tulokset monitulkintaisia marokon
yhteydessa? (yhteydess/INE-connection) , ja viime kuussa pidettiin korkean tason kokouksissa , mukaan luettuna
kuninkaan kanssa
Back-translation: ?and in this context would be the value of the last act, Commissioner Fischler, the results of the
Moroccan context, ambiguous, and last month held high level meetings, including with the king?
c. CRF-LM: ja ta?ssa? yhteydessa? meida?n olisi lisa?arvoa viimeista? toimenpiteita? kuin komission ja?sen fischler , etta? ha?nen
(ha?nen/GEN-his) kokemuksensa (kokemuksensa/POSS-experience) marokolle (marokolle-Moroccan) viime kuun
26 ja korkean tason tapaamiset ja?rjestettiin, kuninkaan kanssa
Back-translation: ?and in this context, we should value the last measures as the Commissioner Fischler, that his
experience in Morocco has on the 26th and high-level meetings took place, including with the king.?
Figure 3: Morphological fluency analysis (see Section 3.1).
tween source and target. In a somewhat or-
thogonal approach to ours, (Ma et al, 2007) use
alignment of a parallel text to pack together ad-
jacent segments in the alignment output, which
are then fed back to the word aligner to boot-
strap an improved alignment, which is then used
in the translation model. We compared our re-
sults against (Luong et al, 2010) in Table 3
since their results are directly comparable to
ours. They use a segmented phrase table and
language model along with the word-based ver-
sions in the decoder and in tuning a Finnish tar-
get. Their approach requires segmented phrases
to match word boundaries, eliminating morpho-
logically productive phrases. In their work a seg-
mented language model can score a translation,
but cannot insert morphology that does not
show source-side reflexes. In order to perform
a similar experiment that still allowed for mor-
phologically productive phrases, we tried train-
ing a segmented translation model, the output
of which we stitched up in tuning so as to tune
to a word-based reference. The goal of this ex-
periment was to control the segmented model?s
tendency to overfit by rewarding it for using
correct whole-word forms. However, we found
39
that this approach was less successful than us-
ing the segmented reference in tuning, and could
not meet the baseline (13.97% BLEU best tun-
ing score, versus 14.93% BLEU for the base-
line best tuning score). Previous work in seg-
mented translation has often used linguistically
motivated morphological analysis selectively ap-
plied based on a language-specific heuristic. A
typical approach is to select a highly inflecting
class of words and segment them for particular
morphology (de Gispert and Marin?o, 2008; Ra-
manathan et al, 2009). Popovic? and Ney (2004)
perform segmentation to reduce morphological
complexity of the source to translate into an iso-
lating target, reducing the translation error rate
for the English target. For Czech-to-English,
Goldwater and McClosky (2005) lemmatized the
source text and inserted a set of ?pseudowords?
expected to have lexical reflexes in English.
Minkov et. al. (2007) and Toutanova et. al.
(2008) use a Maximum Entropy Markov Model
for morphology generation. The main draw-
back to this approach is that it removes morpho-
logical information from the translation model
(which only uses stems); this can be a prob-
lem for languages in which morphology ex-
presses lexical content. de Gispert (2008) uses
a language-specific targeted morphological clas-
sifier for Spanish verbs to avoid this issue. Tal-
bot and Osborne (2006) use clustering to group
morphological variants of words for word align-
ments and for smoothing phrase translation ta-
bles. Habash (2007) provides various methods
to incorporate morphological variants of words
in the phrase table in order to help recognize out
of vocabulary words in the source language.
5 Conclusion and Future Work
We found that using a segmented translation
model based on unsupervised morphology in-
duction and a model that combined morpheme
segments in the translation model with a post-
processing morphology prediction model gave us
better BLEU scores than a word-based baseline.
Using our proposed approach we obtain better
scores than the state of the art on the English-
Finnish translation task (Luong et al, 2010):
from 14.82% BLEU to 15.09%, while using a
simpler model. We show that using morpho-
logical segmentation in the translation model
can improve output translation scores. We
also demonstrate that for Finnish (and possi-
bly other agglutinative languages), phrase-based
MT benefits from allowing the translation model
access to morphological segmentation yielding
productive morphological phrases. Taking ad-
vantage of linguistic analysis of the output we
show that using a post-processing morphology
generation model can improve translation flu-
ency on a sub-word level, in a manner that is
not captured by the BLEU word-based evalua-
tion measure.
In order to help with replication of the results
in this paper, we have run the various morpho-
logical analysis steps and created the necessary
training, tuning and test data files needed in or-
der to train, tune and test any phrase-based ma-
chine translation system with our data. The files
can be downloaded from natlang.cs.sfu.ca.
In future work we hope to explore the utility of
phrases with productive morpheme boundaries
and explore why they are not used more per-
vasively in the decoder. Evaluation measures
for morphologically complex languages and tun-
ing to those measures are also important future
work directions. Also, we would like to explore
a non-pipelined approach to morphological pre-
and post-processing so that a globally trained
model could be used to remove the target side
morphemes that would improve the translation
model and then predict those morphemes in the
target language.
Acknowledgements
This research was partially supported by
NSERC, Canada (RGPIN: 264905) and a
Google Faculty Award. We would like to thank
Christian Monson, Franz Och, Fred Popowich,
Howard Johnson, Majid Razmara, Baskaran
Sankaran and the anonymous reviewers for
their valuable comments on this work. We
would particularly like to thank the developers
of the open-source Moses machine translation
toolkit and the Omorfi morphological analyzer
for Finnish which we used for our experiments.
40
References
Eleftherios Avramidis and Philipp Koehn. 2008. En-
riching morphologically poor languages for statis-
tical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, page 763?770, Columbus, Ohio, USA.
Association for Computational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and R. L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. Computational Linguis-
tics, 19(2):263?311.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word seg-
mentation for machine translation performance.
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 224?232, Colum-
bus, Ohio, June. Association for Computational
Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL05). Association for Computational Lin-
guistics.
Mathias Creutz and Krista Lagus. 2005. Inducing
the morphological lexicon of a natural language
from unannotated text. In Proceedings of the In-
ternational and Interdisciplinary Conference on
Adaptive Knowledge Representation and Reason-
ing (AKRR?05), pages 106?113, Espoo, Finland.
Mathias Creutz and Krista Lagus. 2006. Morfes-
sor in the morpho challenge. In Proceedings of
the PASCAL Challenge Workshop on Unsuper-
vised Segmentation of Words into Morphemes.
Adria? de Gispert and Jose? Marin?o. 2008. On the
impact of morphology in English to Spanish sta-
tistical MT. Speech Communication, 50(11-12).
Sharon Goldwater and David McClosky. 2005.
Improving statistical MT through morphological
analysis. In Proceedings of the Human Language
Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing,
pages 676?683, Vancouver, B.C., Canada. Associ-
ation for Computational Linguistics.
Philipp Koehn and Hieu Hoang. 2007. Factored
translation models. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 868?876, Prague,
Czech Republic. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In ACL ?07: Proceedings of
the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages
177?108, Prague, Czech Republic. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit X, pages 79?86,
Phuket, Thailand. Association for Computational
Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning, pages
282?289, San Francisco, California, USA. Associ-
ation for Computing Machinery.
Minh-Thang Luong, Preslav Nakov, and Min-Yen
Kan. 2010. A hybrid morpheme-word repre-
sentation for machine translation of morphologi-
cally rich languages. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 148?157, Cam-
bridge, Massachusetts. Association for Computa-
tional Linguistics.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages
304?311, Prague, Czech Republic. Association for
Computational Linguistics.
Einat Minkov, Kristina Toutanova, and Hisami
Suzuki. 2007. Generating complex morphology
for machine translation. In In Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics (ACL07), pages 128?135,
Prague, Czech Republic. Association for Compu-
tational Linguistics.
Christian Monson. 2008. Paramor and morpho chal-
lenge 2008. In Lecture Notes in Computer Science:
Workshop of the Cross-Language Evaluation Fo-
rum (CLEF 2008), Revised Selected Papers.
Habash Nizar. 2007. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of
the 46th Annual Meeting of the Association of
Computational Linguistics, Columbus, Ohio. As-
sociation for Computational Linguistics.
41
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei jing Zhu. 2002. BLEU: A method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Associ-
ation for Computational Linguistics ACL, pages
311?318, Philadelphia, Pennsylvania, USA. Asso-
ciation for Computational Linguistics.
Tommi Pirinen and Inari Listenmaa.
2007. Omorfi morphological analzer.
http://gna.org/projects/omorfi.
Maja Popovic? and Hermann Ney. 2004. Towards
the use of word stems and suffixes for statisti-
cal machine translation. In Proceedings of the 4th
International Conference on Language Resources
and Evaluation (LREC), pages 1585?1588, Lis-
bon, Portugal. European Language Resources As-
sociation (ELRA).
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya.
2009. Case markers and morphology: Address-
ing the crux of the fluency problem in English-
Hindi SMT. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Associa-
tion for Computational Linguistics and the 4th In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, pages 800?808, Suntec, Singa-
pore. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. 7th International Confer-
ence on Spoken Language Processing, 3:901?904.
David Talbot and Miles Osborne. 2006. Modelling
lexical redundancy for machine translation. In
Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics, pages 969?976, Sydney, Australia, July.
Association for Computational Linguistics.
Kristina Toutanova, Hisami Suzuki, and Achim
Ruopp. 2008. Applying morphology generation
models to machine translation. In Proceedings
of the 46th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 514?522, Columbus, Ohio,
USA. Association for Computational Linguistics.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly
inflected languages. In Proceedings of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics, pages 41?48, Trento, Italy. As-
sociation for Computational Linguistics.
42
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710?714,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Ensemble Model that Combines Syntactic and Semantic Clustering
for Discriminative Dependency Parsing
Gholamreza Haffari
Faculty of Information Technology
Monash University
Melbourne, Australia
reza@monash.edu
Marzieh Razavi and Anoop Sarkar
School of Computing Science
Simon Fraser University
Vancouver, Canada
{mrazavi,anoop}@cs.sfu.ca
Abstract
We combine multiple word representations
based on semantic clusters extracted from the
(Brown et al, 1992) algorithm and syntac-
tic clusters obtained from the Berkeley parser
(Petrov et al, 2006) in order to improve dis-
criminative dependency parsing in the MST-
Parser framework (McDonald et al, 2005).
We also provide an ensemble method for com-
bining diverse cluster-based models. The two
contributions together significantly improves
unlabeled dependency accuracy from 90.82%
to 92.13%.
1 Introduction
A simple method for using unlabeled data in
discriminative dependency parsing was provided
in (Koo et al, 2008) which involved clustering the
labeled and unlabeled data and then each word in the
dependency treebank was assigned a cluster identi-
fier. These identifiers were used to augment the fea-
ture representation of the edge-factored or second-
order features, and this extended feature set was
used to discriminatively train a dependency parser.
The use of clusters leads to the question of
how to integrate various types of clusters (possibly
from different clustering algorithms) in discrimina-
tive dependency parsing. Clusters obtained from the
(Brown et al, 1992) clustering algorithm are typi-
cally viewed as ?semantic?, e.g. one cluster might
contain plan, letter, request, memo, . . . while an-
other may contain people, customers, employees,
students, . . .. Another clustering view that is more
?syntactic? in nature comes from the use of state-
splitting in PCFGs. For instance, we could ex-
tract a syntactic cluster loss, time, profit, earnings,
performance, rating, . . .: all head words of noun
phrases corresponding to cluster of direct objects of
verbs like improve. In this paper, we obtain syn-
tactic clusters from the Berkeley parser (Petrov et
al., 2006). This paper makes two contributions: 1)
We combine together multiple word representations
based on semantic and syntactic clusters in order to
improve discriminative dependency parsing in the
MSTParser framework (McDonald et al, 2005), and
2) We provide an ensemble method for combining
diverse clustering algorithms that is the discrimina-
tive parsing analog to the generative product of ex-
perts model for parsing described in (Petrov, 2010).
These two contributions combined significantly im-
proves unlabeled dependency accuracy: 90.82% to
92.13% on Sec. 23 of the Penn Treebank, and we
see consistent improvements across all our test sets.
2 Dependency Parsing
A dependency tree represents the syntactic structure
of a sentence with a directed graph (Figure 1), where
nodes correspond to the words, and arcs indicate
head-modifier pairs (Mel?c?uk, 1987). Graph-based
dependency parsing searches for the highest-scoring
tree according to a part-factored scoring function. In
the first-order parsing models, the parts are individ-
ual head-modifier arcs in the dependency tree (Mc-
Donald et al, 2005). In the higher-order models, the
parts consist of arcs together with some context, e.g.
the parent or the sister arcs (McDonald and Pereira,
2006; Carreras, 2007; Koo and Collins, 2010). With
a linear scoring function, the parse for a sentence s
is:
PARSE(s) = arg max
t?T (s)
?
r?t
w ? f(s, r) (1)
where T (s) is the space of dependency trees for s,
and f(s, r) is the feature vector for the part r which
is linearly combined using the model parameter w
to give the part score. The above argmax search
for non-projective dependency parsing is accom-
710
root ForIN-1
PP-2
0111
Japan
NNP-19
NP-10
0110
,
,-0
,-0
0010
the
DT-15
DT-15
1101
trend
NN-23
NP-18
1010
improves
VBZ-1
S-14
0101
access
NN-13
NP-24
0011
to
TO-0
TO-0
0011
American
JJ-31
JJ-31
0110
markets
NNS-25
NP-9
1011
Figure 1: Dependency tree with cluster identifiers obtained from the split non-terminals from the Berkeley parser output. The first
row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is
the first 4 bits (to save space in this figure) of the (Brown et al, 1992) clusters.
plished using minimum spanning tree algorithms
(West, 2001) or approximate inference algorithms
(Smith and Eisner, 2008; Koo et al, 2010). The
(Eisner, 1996) algorithm is typically used for pro-
jective parsing. The model parameters are trained
using a discriminative learning algorithm, e.g. av-
eraged perceptron (Collins, 2002) or MIRA (Cram-
mer and Singer, 2003). In this paper, we work with
both first-order and second-order models, we train
the models using MIRA, and we use the (Eisner,
1996) algorithm for inference.
The baseline features capture information about
the lexical items and their part of speech (POS) tags
(as defined in (McDonald et al, 2005)). In this work,
following (Koo et al, 2008), we use word cluster
identifiers as the source of an additional set of fea-
tures. The reader is directed to (Koo et al, 2008)
for the list of cluster-based feature templates. The
clusters inject long distance syntactic or semantic in-
formation into the model (in contrast with the use
of POS tags in the baseline) and help alleviate the
sparse data problem for complex features that in-
clude n-grams.
3 The Ensemble Model
A word can have different syntactic or semantic
cluster representations, each of which may lead to a
different parsing model. We use ensemble learning
(Dietterich, 2002) in order to combine a collection
of diverse and accurate models into a more powerful
model. In this paper, we construct the base models
based on different syntactic/semantic clusters used
in the features in each model. Our ensemble parsing
model is a linear combination of the base models:
PARSE(s) = arg max
t?T (s)
?
k
?k
?
r?t
wk ? fk(s, r) (2)
where ?k is the weight of the kth base model, and
each base model has its own feature mapping fk(.)
based on its cluster annotation. Each expert pars-
ing model in the ensemble contains all of the base-
line and the cluster-based feature templates; there-
fore, the experts have in common (at least) the base-
line features. The only difference between individ-
ual parsing models is the assigned cluster labels, and
hence some of the cluster-based features. In a fu-
ture work, we plan to take the union of all of the
feature sets and train a joint discriminative parsing
model. The ensemble approach seems more scal-
able though, since we can incrementally add a large
number of clustering algorithms into the ensemble.
4 Syntactic and Semantic Clustering
In our ensemble model we use three different clus-
tering methods to obtain three types of word rep-
resentations that can help alleviate sparse data in a
dependency parser. Our first word representation is
exactly the same as the one used in (Koo et al, 2008)
where words are clustered using the Brown algo-
rithm (Brown et al, 1992). Our two other clusterings
are extracted from the split non-terminals obtained
from the PCFG-based Berkeley parser (Petrov et al,
2006). Split non-terminals from the Berkeley parser
output are converted into cluster identifiers in two
different ways: 1) the split POS tags for each word
are used as an alternate word representation. We
call this representation Syn-Low, and 2) head per-
colation rules are used to label each non-terminal in
the parse such that each non-terminal has a unique
daughter labeled as head. Each word is assigned a
cluster identifier which is defined as the parent split
non-terminal of that word if it is not marked as head,
else if the parent is marked as head we recursively
check its parent until we reach the unique split non-
terminal that is not marked as head. This recursion
terminates at the start symbol TOP. We call this rep-
resentation Syn-High. We only use cluster identi-
fiers from the Berkeley parser, rather than dependen-
cies, or any other information.
711
First order features
Sec Baseline BrownSyn-LowSyn-High Ensemble
00 89.61 90.39 90.01 89.97 90.82
34.68 36.97 34.42 34.94 37.96
01 90.44 91.48 90.89 90.76 91.84
36.36 38.62 35.66 36.56 39.67
23 90.02 91.13 90.46 90.35 91.30
34.13 39.64 36.95 35.00 39.43
24 88.84 90.06 89.44 89.40 90.33
30.85 34.49 32.49 31.22 34.05
Second order features
Sec Baseline BrownSyn-LowSyn-High Ensemble
00 90.34 90.98 90.89 90.59 91.41
38.02 41.04 38.80 39.16 40.93
01 91.48 92.13 91.95 91.72 92.51
41.48 43.84 42.24 41.28 45.05
23 90.82 91.84 91.31 91.21 92.13
39.18 43.66 40.84 39.97 44.28
24 89.87 90.61 90.28 90.31 91.18
35.53 37.99 37.32 35.61 39.55
Table 1: For each test section and model, the number in the
first/second row is the unlabeled-accuracy/unlabeled-complete-
correct. See the text for more explanation.
(TOP
(S-14
(PP-2 (IN-1 For)
(NP-10 (NNP-19 Japan)))
(,-0 ,)
(NP-18 (DT-15 the) (NN-23 trend))
(VP-6 (VBZ-1 improves)
(NP-24 (NN-13 access))
(PP-14 (TO-0 to)
(NP-9 (JJ-31 American)
(NNS-25 markets))))))
For the Berkeley parser output shown above, the
resulting word representations and dependency tree
is shown in Fig. 1. If we group all the head-words in
the training data that project up to split non-terminal
NP-24 then we get a cluster: loss, time, profit, earn-
ings, performance, rating, . . . which are head words
of the noun phrases that appear as direct object of
verbs like improve.
5 Experimental Results
The experiments were done on the English Penn
Treebank, using standard head-percolation rules
(Yamada and Matsumoto, 2003) to convert the
phrase structure into dependency trees. We split the
Treebank into a training set (Sections 2-21), a devel-
Verb Noun Pronoun Adverb Adjective Adpos. Conjunc.
0.0
4
0.0
6
0.0
8
0.1
0
0.1
2
0.1
4
BaselineBrownSyn?LowSyn?HighEnsemble
(a)
1 3 5 7 9 11 13 +15
0.80
0.85
0.90
0.95
Dependency length
Fsc
ore
!
!
!
!
!
!
!
!
!
!
! ! !
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
!
! ! ! !
!
!
!
!
!
!
!
BaselineBrownSyn?LowSyn?HighEnsemble
(b)
Figure 2: (a) Error rate of the head attachment for different
types of modifier categories. (b) F-score for each dependency
length.
opment set (Section 22), and test sets (Sections 0,
1, 23, and 24). All our experimental settings match
previous work (Yamada and Matsumoto, 2003; Mc-
Donald et al, 2005; Koo et al, 2008). POS tags for
the development and test data were assigned by MX-
POST (Ratnaparkhi, 1996), where the tagger was
trained on the entire training corpus. To generate
part of speech tags for the training data, we used 20-
way jackknifing, i.e. we tagged each fold with the
tagger trained on the other 19 folds. We set model
weights ?k in Eqn (2) to one for all experiments.
Syntactic State-Splitting The sentence-specific
word clusters are derived from the parse trees using
712
Berkeley parser1, which generates phrase-structure
parse trees with split syntactic categories. To gen-
erate parse trees for development and test data, the
parser is trained on the entire training data to learn
a PCFG with latent annotations using split-merge
operations for 5 iterations. To generate parse trees
for the training data, we used 20-way jackknifing as
with the tagger.
Word Clusterings from Brown Algorithm The
word clusters were derived using Percy Liang?s im-
plementation of the (Brown et al, 1992) algorithm
on the BLLIP corpus (Charniak et al, 2000) which
contains ?43M words of Wall Street Journal text.2
This produces a hierarchical clustering over the
words which is then sliced at a certain height to ob-
tain the clusters. In our experiments we use the clus-
ters obtained in (Koo et al, 2008)3, but were unable
to match the accuracy reported there, perhaps due to
additional features used in their implementation not
described in the paper.4
Results Table 1 presents our results for each
model on each test set. In this table, the baseline
(first column) does not use any cluster-based fea-
tures, the next three models use cluster-based fea-
tures using different clustering algorithms, and the
last column is our ensemble model which is the lin-
ear combination of the three cluster-based models.
As Table 1 shows, the ensemble model has out-
performed the baseline and individual models in al-
most all cases. Among the individual models, the
model with Brown semantic clusters clearly outper-
forms the baseline, but the two models with syntac-
tic clusters perform almost the same as the baseline.
The ensemble model outperforms all of the individ-
ual models and does so very consistently across both
first-order and second-order dependency models.
Error Analysis To better understand the contri-
bution of each model to the ensemble, we take a
closer look at the parsing errors for each model and
the ensemble. For each dependent to head depen-
1code.google.com/p/berkeleyparser
2Sentences of the Penn Treebank were excluded from the
text used for the clustering.
3people.csail.mit.edu/maestro/papers/bllip-clusters.gz
4Terry Koo was kind enough to share the source code for the
(Koo et al, 2008) paper with us, and we plan to incorporate all
the features in our future work.
dency, Fig. 2(a) shows the error rate for each depen-
dent grouped by a coarse POS tag (c.f. (McDonald
and Nivre, 2007)). For most POS categories, the
Brown cluster model is the best individual model,
but for Adjectives it is Syn-High, and for Pronouns
it is Syn-Low that is the best. But the ensemble al-
ways does the best in every grammatical category.
Fig. 2(b) shows the F-score of the different models
for various dependency lengths, where the length of
a dependency from word wi to word wj is equal to
|i ? j|. We see that different models are experts on
different lengths (Syn-Low on 8, Syn-High on 9),
while the ensemble model can always combine their
expertise and do better at each length.
6 Comparison to Related Work
Several ensemble models have been proposed for
dependency parsing (Sagae and Lavie, 2006; Hall et
al., 2007; Nivre and McDonald, 2008; Attardi and
Dell?Orletta, 2009; Surdeanu and Manning, 2010).
Essentially, all of these approaches combine dif-
ferent dependency parsing systems, i.e. transition-
based and graph-based. Although graph-based mod-
els are globally trained and can use exact inference
algorithms, their features are defined over a lim-
ited history of parsing decisions. Since transition-
based parsing models have the opposite character-
istics, the idea is to combine these two types of
models to exploit their complementary strengths.
The base parsing models are either independently
trained (Sagae and Lavie, 2006; Hall et al, 2007;
Attardi and Dell?Orletta, 2009; Surdeanu and Man-
ning, 2010), or their training is integrated, e.g. using
stacking (Nivre and McDonald, 2008; Attardi and
Dell?Orletta, 2009; Surdeanu and Manning, 2010).
Our work is distinguished from the aforemen-
tioned works in two dimensions. Firstly, we com-
bine various graph-based models, constructed using
different syntactic/semantic clusters. Secondly, we
do exact inference on the shared hypothesis space of
the base models. This is in contrast to previous work
which combine the best parse trees suggested by the
individual base-models to generate a final parse tree,
i.e. a two-phase inference scheme.
7 Conclusion
We presented an ensemble of different dependency
parsing models, each model corresponding to a dif-
713
ferent syntactic/semantic word clustering annota-
tion. The ensemble obtains consistent improve-
ments in unlabeled dependency parsing, e.g. from
90.82% to 92.13% for Sec. 23 of the Penn Tree-
bank. Our error analysis has revealed that each syn-
tactic/semantic parsing model is an expert in cap-
turing different dependency lengths, and the ensem-
ble model can always combine their expertise and
do better at each dependency length. We can in-
crementally add a large number models using dif-
ferent clustering algorithms, and our preliminary re-
sults show increased improvement in accuracy when
more models are added into the ensemble.
Acknowledgements
This research was partially supported by NSERC,
Canada (RGPIN: 264905). We would like to thank
Terry Koo for his help with the cluster-based fea-
tures for dependency parsing and Ryan McDonald
for the MSTParser source code which we modified
and used for the experiments in this paper.
References
G. Attardi and F. Dell?Orletta. 2009. Reverse revision
and linear tree combination for dependency parsing.
In Proc. of NAACL-HLT.
P. F. Brown, P. V. deSouza, R. L. Mercer, T. J. Watson,
V. J. Della Pietra, and J. C. Lai. 1992. Class-based
n-gram models of natural language. Computational
Linguistics, 18(4).
X. Carreras. 2007. Experiments with a higher-order pro-
jective dependency parser. In Proc. of EMNLP-CoNLL
Shared Task.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.
2000. BLLIP 1987-89 WSJ Corpus Release 1, LDC
No. LDC2000T43, Linguistic Data Consortium.
M. Collins. 2002. Discriminative training methods for
hidden markov models: theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. J. Mach.
Learn. Res., 3:951?991.
T. Dietterich. 2002. Ensemble learning. In The Hand-
book of Brain Theory and Neural Networks, Second
Edition.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In COLING.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? a study in multilingual parser optimization.
In Proc. of CoNLL Shared Task.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. of ACL/HLT.
T. Koo, A. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of EMNLP-CONLL.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
I. Mel?c?uk. 1987. Dependency syntax: theory and prac-
tice. State University of New York Press.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proc. of ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. COLING-ACL.
S. Petrov. 2010. Products of random latent variable
grammars. In Proc. of NAACL-HLT.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of NAACL-HLT.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
M. Surdeanu and C. Manning. 2010. Ensemble models
for dependency parsing: Cheap and good? In Proc. of
NAACL.
D. West. 2001. Introduction to Graph Theory. Prentice
Hall, 2nd editoin.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of IWPT.
714
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 620?628,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Bootstrapping via Graph Propagation
Max Whitney and Anoop Sarkar?
Simon Fraser University, School of Computing Science
Burnaby, BC V5A 1S6, Canada
{mwhitney,anoop}@sfu.ca
Abstract
Bootstrapping a classifier from a small set
of seed rules can be viewed as the propaga-
tion of labels between examples via features
shared between them. This paper introduces a
novel variant of the Yarowsky algorithm based
on this view. It is a bootstrapping learning
method which uses a graph propagation algo-
rithm with a well defined objective function.
The experimental results show that our pro-
posed bootstrapping algorithm achieves state
of the art performance or better on several dif-
ferent natural language data sets.
1 Introduction
In this paper, we are concerned with a case of semi-
supervised learning that is close to unsupervised
learning, in that the labelled and unlabelled data
points are from the same domain and only a small
set of seed rules is used to derive the labelled points.
We refer to this setting as bootstrapping. In contrast,
typical semi-supervised learning deals with a large
number of labelled points, and a domain adaptation
task with unlabelled points from the new domain.
The two dominant discriminative learning meth-
ods for bootstrapping are self-training (Scud-
der, 1965) and co-training (Blum and Mitchell,
1998). In this paper we focus on a self-training
style bootstrapping algorithm, the Yarowsky algo-
rithm (Yarowsky, 1995). Variants of this algorithm
have been formalized as optimizing an objective
function in previous work by Abney (2004) and Haf-
fari and Sarkar (2007), but it is not clear that any
perform as well as the Yarowsky algorithm itself.
We take advantage of this formalization and in-
troduce a novel algorithm called Yarowsky-prop
which builds on the algorithms of Yarowsky (1995)
and Subramanya et al (2010). It is theoretically
?This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant. We would like to thank Gho-
lamreza Haffari and the anonymous reviewers for their com-
ments. We particularly thank Michael Collins, Jason Eisner, and
Damianos Karakos for the data we used in our experiments.
x denotes an example
f , g denote features
i, k denote labels
X set of training examples
Fx set of features for example x
Y current labelling of X
Yx current label for example x
? value of Yx for unlabelled examples
L number of labels (not including ?)
? set of currently labelled examples
V set of currently unlabelled examples
Xf set of examples with feature f
?f set of currently labelled examples with f
Vf set of currently unlabelled examples with f
?j set of examples currently labelled with j
?fj set of examples with f currently labelled with j
Table 1: Notation of Abney (2004).
well-understood as minimizing an objective func-
tion at each iteration, and it obtains state of the art
performance on several different NLP data sets. To
our knowledge, this is the first theoretically mo-
tivated self-training bootstrapping algorithm which
performs as well as the Yarowsky algorithm.
2 Bootstrapping
Abney (2004) defines useful notation for semi-
supervised learning, shown in table 1. Note that ?,
V , etc. are relative to the current labelling Y . We
additionally define F to be the set of all features,
and use U to denote the uniform distribution. In the
bootstrapping setting the learner is given an initial
partial labelling Y (0) where only a few examples are
labelled (i.e. Y (0)x = ? for most x).
Abney (2004) defines three probability distribu-
tions in his analysis of bootstrapping: ?fj is the pa-
rameter for feature f with label j, taken to be nor-
malized so that ?f is a distribution over labels. ?x is
the labelling distribution representing the current Y ;
it is a point distribution for labelled examples and
uniform for unlabelled examples. pix is the predic-
tion distribution over labels for example x.
The approach of Haghighi and Klein (2006b) and
Haghighi and Klein (2006a) also uses a small set of
620
Algorithm 1: The basic Yarowsky algorithm.
Require: training data X and a seed DL ?(0)
1: apply ?(0) to X produce a labelling Y (0)
2: for iteration t to maximum or convergence do
3: train a new DL ? on Y (t)
4: apply ? to X , to produce Y (t+1)
5: end for
seed rules but uses them to inject features into a joint
model p(x, j) which they train using expectation-
maximization for Markov random fields. We focus
on discriminative training which does not require
complex partition functions for normalization. Blum
and Chawla (2001) introduce an early use of trans-
ductive learning using graph propagation. X. Zhu
and Z. Ghahramani and J. Lafferty (2003)?s method
of graph propagation is predominantly transductive,
and the non-transductive version is closely related to
Abney (2004) c.f. Haffari and Sarkar (2007).1
3 Existing algorithms
3.1 Yarowsky
A decision list (DL) is a (ordered) list of feature-
label pairs (rules) which is produced by assigning
a score to each rule and sorting on this score. It
chooses a label for an example from the first rule
whose feature is a feature of the example. For a
DL the prediction distribution is defined by pix(j) ?
maxf?Fx ?fj . The basic Yarowsky algorithm is
shown in algorithm 1. Note that at any point some
training examples may be left unlabelled by Y (t).
We use Collins and Singer (1999) for our exact
specification of Yarowsky.2 It uses DL rule scores
?fj ?
|?fj |+ 
|?f |+ L
(1)
where  is a smoothing constant. When constructing
a DL it keeps only the rules with (pre-normalized)
score over a threshold ?. In our implementation we
add the seed rules to each subsequent DL.3
1Large-scale information extraction, e.g. (Hearst, 1992),
Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and
Shepherd, 1997), and Junto (Talukdar, 2010) among others, also
have similarities to our approach. We focus on the formal anal-
ysis of the Yarowsky algorithm by Abney (2004).
2It is similar to that of Yarowsky (1995) but is better spec-
ified and omits word sense disambiguation optimizations. The
general algorithm in Yarowsky (1995) is self-training with any
kind of underlying supervised classifier, but we follow the con-
vention of using Yarowsky to refer to the DL algorithm.
3This is not clearly specified in Collins and Singer (1999),
3.2 Yarowsky-cautious
Collins and Singer (1999) also introduce a variant
algorithm Yarowsky-cautious. Here the DL training
step keeps only the top n rules (f, j) over the thresh-
old for each label j, ordered by |?f |. Additionally
the threshold ? is checked against |?fj |/|?f | instead
of the smoothed score. n begins at n0 and is incre-
mented by ?n at each iteration. We add the seed DL
to the new DL after applying the cautious pruning.
Cautiousness limits not only the size of the DL but
also the number of labelled examples, prioritizing
decisions which are believed to be of high accuracy.
At the final iteration Yarowsky-cautious uses the
current labelling to train a DL without a threshold
or cautiousness, and this DL is used for testing. We
call this the retraining step.4
3.3 DL-CoTrain
Collins and Singer (1999) also introduce the co-
training algorithm DL-CoTrain. This algorithm al-
ternates between two DLs using disjoint views of
the features in the data. At each step it trains a DL
and then produces a new labelling for the other DL.
Each DL uses thresholding and cautiousness as we
describe for Yarowsky-cautious. At the end the DLs
are combined, the result is used to label the data, and
a retraining step is done from this single labelling.
3.4 Y-1/DL-1-VS
One of the variant algorithms of Abney (2004) is
Y-1/DL-1-VS (referred to by Haffari and Sarkar
(2007) as simply DL-1). Besides various changes
in the specifics of how the labelling is produced,
this algorithm has two differences versus Yarowsky.
Firstly, the smoothing constant  in (1) is replaced
by 1/|Vf |. Secondly, pi is redefined as pix(j) =
1
|Fx|
?
f?Fx ?fj , which we refer to as the sum def-
inition of pi. This definition does not match a literal
DL but is easier to analyze.
We are not concerned here with the details of
Y-1/DL-1-VS, but we note that Haffari and Sarkar
but is used for DL-CoTrain in the same paper.
4The details of Yarowsky-cautious are not clearly specified
in Collins and Singer (1999). Based on similar parts of DL-
CoTrain we assume the that the top n selection is per label
rather in total, that the thresholding value is unsmoothed, and
that there is a retraining step. We also assume their notation
Count?(x) to be equivalent to |?f |.
621
(2007) provide an objective function for this al-
gorithm using a generalized definition of cross-
entropy in terms of Bregman distance, which mo-
tivates our objective in section 4. The Breg-
man distance between two discrete probability dis-
tributions p and q is defined as B?(p, q) =?
i [?(pi)? ?(qi)? ?
?(qi)(pi ? qi)]. As a specific
case we have Bt2(p, q) =
?
i(pi? qi)
2 = ||p? q||2.
Then Bregman distance-based entropy is Ht2(p) =
?
?
i p
2
i , KL-Divergence is Bt2 , and cross-entropy
follows the standard definition in terms of Ht2 and
Bt2 . The objective minimized by Y-1/DL-1-VS is:
?
x?X
f?Fx
Ht2(?x||?f ) =
?
x?X
f?Fx
[
Bt2(?x||?f )?
?
y
?2x
]
.
(2)
3.5 Yarowsky-sum
As a baseline for the sum definition of pi, we intro-
duce the Yarowsky-sum algorithm. It is the same
as Yarowsky except that we use the sum definition
when labelling: for example x we choose the label j
with the highest (sum) pix(j), but set Yx = ? if the
sum is zero. Note that this is a linear model similar
to a conditional random field (CRF) (Lafferty et al,
2001) for unstructured multiclass problems.
3.6 Bipartite graph algorithms
Haffari and Sarkar (2007) suggest a bipartite
graph framework for semi-supervised learning
based on their analysis of Y-1/DL-1-VS and objec-
tive (2). The graph has vertices X ? F and edges
{(x, f) : x ? X, f ? Fx}, as in the graph shown
in figure 1(a). Each vertex represents a distribution
over labels, and in this view Yarowsky can be seen as
alternately updating the example distributions based
on the feature distributions and visa versa.
Based on this they give algorithm 2, which
we call HS-bipartite. It is parametrized by two
functions which are called features-to-example and
examples-to-feature here. Each can be one of
two choices: average(S) is the normalized aver-
age of the distributions of S, while majority(S)
is a uniform distribution if all labels are supported
by equal numbers of distributions of S, and other-
wise a point distribution with mass on the best sup-
ported label. The average-majority form is similar
Algorithm 2: HS-bipartite.
1: apply ?(0) to X produce a labelling Y (0)
2: for iteration t to maximum or convergence do
3: for f ? F do
4: let p = examples-to-feature({?x : x ? Xf})
5: if p 6= U then let ?f = p
6: end for
7: for x ? X do
8: let p = features-to-example({?f : f ? Fx})
9: if p 6= U then let ?x = p
10: end for
11: end for
to Y-1/DL-1-VS, and the majority-majority form
minimizes a different objective similar to (2).
In our implementation we label training data (for
the convergence check) with the ? distributions from
the graph. We label test data by constructing new
?x = examples-to-feature(Fx) for the unseen x.
3.7 Semi-supervised learning algorithm of Sub-
ramanya et al (2010)
Subramanya et al (2010) give a semi-supervised al-
gorithm for part of speech tagging. Unlike the algo-
rithms described above, it is for domain adaptation
with large amounts of labelled data rather than boot-
strapping with a small number of seeds.
This algorithm is structurally similar to Yarowsky
in that it begins from an initial partial labelling and
repeatedly trains a classifier on the labelling and
then relabels the data. It uses a CRF (Lafferty et al,
2001) as the underlying supervised learner. It dif-
fers significantly from Yarowsky in two other ways:
First, instead of only training a CRF it also uses a
step of graph propagation between distributions over
the n-grams in the data. Second, it does the propa-
gation on distributions over n-gram types rather than
over n-gram tokens (instances in the data).
They argue that using propagation over types al-
lows the algorithm to enforce constraints and find
similarities that self-training cannot. We are not con-
cerned here with the details of this algorithm, but
it motivates our work firstly in providing the graph
propagation which we will describe in more detail in
section 4, and secondly in providing an algorithmic
structure that we use for our algorithm in section 5.
3.8 Collins and Singer (1999)?s EM
We implemented the EM algorithm of Collins and
Singer (1999) as a baseline for the other algorithms.
622
Method V N (u) qu
?-? X ? F Nx = Fx, Nf = Xf qx = ?x, qf = ?f
pi-? X ? F Nx = Fx, Nf = Xf qx = pix, qf = ?f
?-only F Nf =
?
x?Xf
Fx \ f qf = ?f
?T-only F Nf =
?
x?Xf
Fx \ f qf = ?Tf
Table 2: Graph structures for propagation.
They do not specify tuning details, but to get com-
parable accuracy we found it was necessary to do
smoothing and to include weights ?1 and ?2 on the
expected counts of seed-labelled and initially unla-
belled examples respectively (Nigam et al, 2000).
4 Graph propagation
The graph propagation of Subramanya et al (2010)
is a method for smoothing distributions attached to
vertices of a graph. Here we present it with an alter-
nate notation using Bregman distances as described
in section 3.4.5 The objective is
?
?
u?V
v?N (i)
wuvBt2(qu, qv) + ?
?
u?V
Bt2(qu, U) (3)
where V is a set of vertices, N (v) is the neighbour-
hood of vertex v, and qv is an initial distribution for
each vertex v to be smoothed. They give an iterative
update to minimize (3). Note that (3) is independent
of their specific graph structure, distributions, and
semi-supervised learning algorithm.
We propose four methods for using this propaga-
tion with Yarowsky. These methods all use con-
stant edge weights (wuv = 1). The distributions
and graph structures are shown in table 2. Figure 1
shows example graphs for ?-? and ?-only. pi-? and
?T-only are similar, and are described below.
The graph structure of ?-? is the bipartite graph
of Haffari and Sarkar (2007). In fact, ?-? the propa-
gation objective (3) and Haffari and Sarkar (2007)?s
Y-1/DL-1-VS objective (2) are identical up to con-
stant coefficients and an extra constant term.6 ?-?
5We omit the option to hold some of the distributions at fixed
values, which would add an extra term to the objective.
6The differences are specifically: First, (3) adds the con-
stant coefficients ? and ?. Second, (3) sums over each edge
twice (once in each direction), while (2) sums over each only
once. Since wuv = wvu and Bt2(qu, qv) = Bt2(qv, qu), this
can be folded into the constant ?. Third, after expanding (2)
there is a term |Fx| inside the sum for Ht2(?x) which is not
present in (3). This does not effect the direction of minimiza-
tion. Fourth, Bt2(qu, U) in (3) expands to Ht2(qu) plus a con-
stant, adding an extra constant term to the total.
?f|F |
?f4
?f3
?f2
?f1 ?x1
?x2
?x3
?x4
?x|X|
... ...
(a) ?-? method
?f1
?f|F |
?f2
?f4
?f3
...
(b) ?-only method
Figure 1: Example graphs for ?-? and ?-only propagation.
therefore gives us a direct way to optimize (2).
The other three methods do not correspond to the
objective of Haffari and Sarkar (2007). The pi-?
method is like ?-? except for using pi as the distribu-
tion for example vertices.
The bipartite graph of the first two methods dif-
fers from the structure used by Subramanya et al
(2010) in that it does propagation between two dif-
ferent kinds of distributions instead of only one kind.
We also adopt a more comparable approach with a
graph over only features. Here we define adjacency
by co-occurrence in the same example. The ?-only
method uses this graph and ? as the distribution.
Finally, we noted in section 3.7 that the algo-
rithm of Subramanya et al (2010) does one addi-
tional step in converting from token level distribu-
tions to type level distributions. The ?T-only method
therefore uses the feature-only graph but for the dis-
tribution uses a type level version of ? defined by
?Tfj =
1
|Xf |
?
x?Xf
pix(j).
5 Novel Yarowsky-prop algorithm
We call our graph propagation based algorithm
Yarowsky-prop. It is shown with ?T-only propaga-
tion in algorithm 3. It is based on the Yarowsky al-
gorithm, with the following changes: an added step
to calculate ?T (line 4), an added step to calculate ?P
(line 5), the use of ?P rather than the DL to update
the labelling (line 6), and the use of the sum defini-
tion of pi. Line 7 does DL training as we describe in
sections 3.1 and 3.2. Propagation is done with the
iterative update of Subramanya et al (2010).
This algorithm is adapted to the other propagation
methods described in section 4 by changing the type
of propagation on line 5. In ?-only, propagation is
623
Algorithm 3: Yarowsky-prop.
1: let ?fj be the scores of the seed rules // crf train
2: for iteration t to maximum or convergence do
3: let pix(j) = 1|Fx|
?
f?Fx
?fj // post. decode
4: let ?Tfj =
P
x?Xf
pix(j)
|Xf |
// token to type
5: propagate ?T to get ?P // graph propagate
6: label the data with ?P // viterbi decode
7: train a new DL ?fj // crf train
8: end for
done on ?, using the graph of figure 1(b). In ?-? and
pi-? propagation is done on the respective bipartite
graph (figure 1(a) or the equivalent with pi). Line
4 is skipped for these methods, and ? is as defined
in section 2. For the bipartite graph methods ?-?
and pi-? only the propagated ? values on the feature
nodes are used for ?P (the distributions on the exam-
ple nodes are ignored after the propagation itself).
The algorithm uses ?fj values rather than an ex-
plicit DL for labelling. The (pre-normalized) score
for any (f, j) not in the DL is taken to be zero. Be-
sides using the sum definition of pi when calculating
?T, we also use a sum in labelling. When labelling
an example x (at line 6 and also on testing data) we
use arg maxj
?
f?Fx: ?Pf 6=U
?Pfj , but set Yx = ? if
the sum is zero. Ignoring uniform ?Pf values is in-
tended to provide an equivalent to the DL behaviour
of using evidence only from rules that are in the list.
We include the cautiousness of Yarowsky-
cautious (section 3.2) in the DL training on line 7. At
the labelling step on line 6 we label only examples
which the pre-propagated ? would also assign a label
(using the same rules described above for ?P). This
choice is intended to provide an equivalent to the
Yarowsky-cautious behaviour of limiting the num-
ber of labelled examples; most ?Pf are non-uniform,
so without it most examples become labelled early.
We observe further similarity between the
Yarowsky algorithm and the general approach of
Subramanya et al (2010) by comparing algorithm
3 here with their algorithm 1. The comments in al-
gorithm 3 give the corresponding parts of their algo-
rithm. Note that each line has a similar purpose.
6 Evaluation
6.1 Tasks and data
For evaluation we use the tasks of Collins and Singer
(1999) and Eisner and Karakos (2005), with data
Rank Score Feature Label
1 0.999900 New-York loc.
2 0.999900 California loc.
3 0.999900 U.S. loc.
4 0.999900 Microsoft org.
5 0.999900 I.B.M. org.
6 0.999900 Incorporated org.
7 0.999900 Mr. per.
8 0.999976 U.S. loc.
9 0.999957 New-York-Stock-Exchange loc.
10 0.999952 California loc.
11 0.999947 New-York loc.
12 0.999946 court-in loc.
13 0.975154 Company-of loc.
...
Figure 2: A DL from iteration 5 of Yarowsky on the named en-
tity task. Scores are pre-normalized values from the expression
on the left side of (1), not ?fj values. Context features are indi-
cated by italics; all others are spelling features. Specific feature
types are omitted. Seed rules are indicated by bold ranks.
kindly provided by the respective authors.
The task of Collins and Singer (1999) is named
entity classification on data from New York Times
text.7 The data set was pre-processed by a statisti-
cal parser (Collins, 1997) and all noun phrases that
are potential named entities were extracted from the
parse tree. Each noun phrase is to be labelled as
a person, organization, or location. The parse tree
provides the surrounding context as context features
such as the words in prepositional phrase and rela-
tive clause modifiers, etc., and the actual words in
the noun phrase provide the spelling features. The
test data additionally contains some noise examples
which are not in the three named entity categories.
We use the seed rules the authors provide, which are
the first seven items in figure 2. For DL-CoTrain,
we use their two views: one view is the spelling fea-
tures, and the other is the context features. Figure 2
shows a DL from Yarowsky training on this task.
The tasks of Eisner and Karakos (2005) are word
sense disambiguation on several English words
which have two senses corresponding to two dif-
ferent words in French. Data was extracted from
the Canadian Hansards, using the English side to
produce training and test data and the French side
to produce the gold labelling. Features are the
original and lemmatized words immediately adja-
7We removed weekday and month examples from the test set
as they describe. They note 88962 examples in their training set,
but the file has 89305. We did not find any filtering criteria that
produced the expected size, and therefore used all examples.
624
cent to the word to be disambiguated, and origi-
nal and lemmatized context words in the same sen-
tence. Their seeds are pairs of adjacent word fea-
tures, with one feature for each label (sense). We
use the ?drug?, ?land?, and ?sentence? tasks, and
the seed rules from their best seed selection: ?alco-
hol?/?medical?, ?acres?/?court?, and ?reads?/?served?
respectively (they do not provide seeds for their
other three tasks). For DL-CoTrain we use adjacent
words for one view and context words for the other.
6.2 Experimental set up
Where applicable we use smoothing  = 0.1, a
threshold ? = 0.95, and cautiousness parameters
n0 = ?n = 5 as in Collins and Singer (1999)
and propagation parameters ? = 0.6, ? = 0.01 as
in Subramanya et al (2010). Initial experiments
with different propagation parameters suggested that
as long as ? was set at this value changing ? had
relatively little effect on the accuracy. We did not
find any propagation parameter settings that outper-
formed this choice. For the Yarowsky-prop algo-
rithms we perform a single iteration of the propa-
gation update for each iteration of the algorithm.
For EM we use weights ?1 = 0.98, and ?2 = 0.02
(see section 3.8), which were found in initial experi-
ments to be the best values, and results are averaged
over 10 random initializations.
The named entity test set contains some examples
that are neither person, organization, nor location.
Collins and Singer (1999) define noise accuracy as
accuracy that includes such instances, and clean ac-
curacy as accuracy calculated across only the exam-
ples which are one of the known labels. We report
only clean accuracy in this paper; noise accuracy
tracks clean accuracy but is a little lower. There is
no difference on the word sense data sets. We also
report (clean) non-seeded accuracy, which we define
to be clean accuracy over only examples which are
not assigned a label by the seed rules. This is in-
tended to evaluate what the algorithm has learned,
rather than what it can achieve by using the input
information directly (Daume, 2011).
We test Yarowsky, Yarowsky-cautious,
Yarowsky-sum, DL-CoTrain, HS-bipartite in
all four forms, and Yarowsky-prop cautious and
non-cautious and in all four forms. For each algo-
rithm except EM we perform a final retraining step
Gold Spelling features Context features
loc. Waukegan maker, LEFT
loc. Mexico, president, of president-of, RIGHT
loc. La-Jolla, La Jolla company, LEFT
Figure 3: Named entity test set examples where Yarowsky-prop
?-only is correct and no other tested algorithms are correct. The
specific feature types are omitted.
as described for Yarowsky-cautious (section 3.2).
Our programs and experiment scripts have been
made available.8
6.3 Accuracy
Table 3 shows the final test set accuracies for the
all the algorithms. The seed DL accuracy is also
included for reference.
The best performing form of our novel algo-
rithm is Yarowsky-prop-cautious ?-only. It numer-
ically outperforms DL-CoTrain on the named entity
task, is not (statistically) significantly worse on the
drug and land tasks, and is significantly better on
the sentence task. It also numerically outperforms
Yarowsky-cautious on the named entity task and is
significantly better on the drug task. Is significantly
worse on the land task, where most algorithms con-
verge at labelling all examples with the first sense. It
is significantly worse on the sentence task, although
it is the second best performing algorithm and sev-
eral percent above DL-CoTrain on that task.
Figure 3 shows (all) three examples from the
named entity test set where Yarowsky-prop-cautious
?-only is correct but none of the other Yarowsky
variants are. Note that it succeeds despite mis-
leading features; ?maker? and ?company? might be
taken to indicate a company and ?president-of? an
organization, but all three examples are locations.
Yarowsky-prop-cautious ?-? and pi-? also per-
form respectably, although not as well. Yarowsky-
prop-cautious ?T-only and the non-cautious versions
are significantly worse. Although ?T-only was in-
tended to incorporate Subramanya et al (2010)?s
idea of type level distributions, it in fact performs
worse than ?-only. We believe that Collins and
Singer (1999)?s definition (1) of ? incorporates suf-
ficient type level information that the creation of a
separate distribution is unnecessary in this case.
Figure 4 shows the test set non-seeded accuracies
as a function of the iteration for many of the algo-
8The software is included with the paper submission and
will be maintained at https://github.com/sfu-natlang/yarowsky.
625
Algorithm
Task
named entity drug land sentence
EM
81.05 78.64 55.96 54.85 32.86 31.07 67.88 65.42
?0.31 ?0.34 ?0.41 ?0.43 ?0.00 ?0.00 ?3.35 ?3.57
Seed DL 11.29 0.00 5.18 0.00 2.89 0.00 7.18 0.00
DL-CoTrain (cautious) 91.56 90.49 59.59 58.17 78.36 77.72 68.16 65.69
Yarowsky 81.19 78.79 55.70 54.02 79.03 78.41 62.91 60.04
Yarowsky-cautious 91.11 89.97 54.40 52.63 79.10 78.48 78.64 76.99
Yarowsky-cautious sum 91.56 90.49 54.40 52.63 78.36 77.72 78.64 76.99
HS-bipartite avg-avg 45.84 45.89 52.33 50.42 78.36 77.72 54.56 51.05
HS-bipartite avg-maj 81.98 79.69 52.07 50.14 78.36 77.72 55.15 51.67
HS-bipartite maj-avg 73.55 70.18 52.07 50.14 78.36 77.72 55.15 51.67
HS-bipartite maj-maj 73.66 70.31 52.07 50.14 78.36 77.72 55.15 51.67
Yarowsky-prop ?-? 80.39 77.89 53.63 51.80 78.36 77.72 55.34 51.88
Yarowsky-prop pi-? 78.34 75.58 54.15 52.35 78.36 77.72 54.56 51.05
Yarowsky-prop ?-only 78.56 75.84 54.66 52.91 78.36 77.72 54.56 51.05
Yarowsky-prop ?T-only 77.88 75.06 52.07 50.14 78.36 77.72 54.56 51.05
Yarowsky-prop-cautious ?-? 90.19 88.95 56.99 55.40 78.36 77.72 74.17 72.18
Yarowsky-prop-cautious pi-? 89.40 88.05 58.55 57.06 78.36 77.72 70.10 67.78
Yarowsky-prop-cautious ?-only 92.47 91.52 58.55 57.06 78.36 77.72 75.15 73.22
Yarowsky-prop-cautious ?T-only 78.45 75.71 58.29 56.79 78.36 77.72 54.56 51.05
Num. train/test examples 89305 / 962 134 / 386 1604 / 1488 303 / 515
Table 3: Test set percent accuracy and non-seeded test set percent accuracy (respectively) for the algorithms on all tasks. Bold
items are a maximum in their column. Italic items have a statistically significant difference versus DL-CoTrain (p < 0.05 with a
McNemar test). For EM, ? indicates one standard deviation but statistical significance was not measured.
rithms on the named entity task. The Yarowsky-prop
non-cautious algorithms quickly converge to the fi-
nal accuracy and are not shown. While the other
algorithms (figure 4(a)) make a large accuracy im-
provement in the final retraining step, the Yarowsky-
prop (figure 4(b)) algorithms reach comparable ac-
curacies earlier and gain much less from retraining.
We did not implement Collins and Singer (1999)?s
CoBoost; however, in their results it performs com-
parably to DL-CoTrain and Yarowsky-cautious. As
with DL-CoTrain, CoBoost requires two views.
6.4 Cautiousness
Cautiousness appears to be important in the perfor-
mance of the algorithms we tested. In table 3, only
the cautious algorithms are able to reach the 90%
accuracy range.
To evaluate the effects of cautiousness we ex-
amine the Yarowsky-prop ?-only algorithm on the
named entity task in more detail. This algorithm has
two classifiers which are trained in conjunction: the
DL and the propagated ?P. Figure 5 shows the train-
ing set coverage (of the labelling on line 6 of algo-
rithm 3) and the test set accuracy of both classifiers,
for the cautious and non-cautious versions.
The non-cautious version immediately learns a
DL over all feature-label pairs, and therefore has full
coverage after the first iteration. The DL and ?P con-
verge to similar accuracies within a few more itera-
tions, and the retraining step increases accuracy by
less than one percent. On the other hand, the cau-
tious version gradually increases the coverage over
the iterations. The DL accuracy follows the cover-
age closely (similar to the behaviour of Yarowsky-
cautious, not shown here), while the propagated
classifier accuracy jumps quickly to near 90% and
then increases only gradually.
Although the DL prior to retraining achieves a
roughly similar accuracy in both versions, only the
cautious version is able to reach the 90% accuracy
range in the propagated classifier and retraining.
Presumably the non-cautious version makes an early
mistake, reaching a local minimum which it cannot
escape. The cautious version avoids this by making
only safe rule selection and labelling choices.
Figure 5(b) also helps to clarify the difference in
retraining that we noted in section 6.3. Like the
non-propagated DL algorithms, the DL component
of Yarowsky-prop has much lower accuracy than the
propagated classifier prior to the retraining step. But
after retraining, the DL and ?P reach very similar ac-
curacies.
626
 
0.5
 
0.55 0.6
 
0.65 0.7
 
0.75 0.8
 
0.85 0.9
 
0.95  0
 
100
 
200
 
300
 
400
 
500
 
600
Non-seeded test accuracy
Iteratio
n
DL-Co
Train (c
autious) Yarows
ky
Yarow
sky-ca
utious
Yarow
sky-ca
utious
 sum
(a) Collins & Singer algorithms (plus sum form)
 
0.5
 
0.55 0.6
 
0.65 0.7
 
0.75 0.8
 
0.85 0.9
 
0.95  0
 
100
 
200
 
300
 
400
 
500
 
600
Non-seeded test accuracy
Iteratio
nYaro
wsky-p
rop-ca
utious
 phi-th
eta
Yarow
sky-pr
op-cau
tious p
i-theta
Yarow
sky-pr
op-cau
tious t
heta-o
nly
Yarow
sky-pr
op-cau
tious t
hetaty
pe-onl
y
(b) Yarowsky propagation cautious
Figure 4: Non-seeded test accuracy versus iteration for various
algorithms on named entity. The results for the Yarowsky-prop
algorithms are for the propagated classifier ?P , except for the
final DL retraining iteration.
6.5 Objective function
The propagation method ?-? was motivated by opti-
mizing the equivalent objectives (2) and (3) at each
iteration. Figure 6 shows the graph propagation ob-
jective (3) along with accuracy for Yarowsky-prop
?-? without cautiousness. The objective value de-
creases as expected, and converges along with accu-
racy. Conversely, the cautious version (not shown
here) does not clearly minimize the objective, since
cautiousness limits the effect of the propagation.
7 Conclusions
Our novel algorithm achieves accuracy compara-
ble to Yarowsky-cautious, but is better theoretically
motivated by combining ideas from Haffari and
Sarkar (2007) and Subramanya et al (2010). It also
achieves accuracy comparable to DL-CoTrain, but
does not require the features to be split into two in-
dependent views.
As future work, we would like to apply our al-
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9 1  0
 
100
 
200
 
300
 
400
 
500
 
600
Non-seeded test accuracy | Coverage
Iteratio
n
main dl
cove
rage
(a) Non-cautious
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9 1  0
 
100
 
200
 
300
 
400
 
500
 
600
Non-seeded test accuracy | Coverage
Iteratio
n
main dl
cove
rage
(b) Cautious
Figure 5: Internal train set coverage and non-seeded test accu-
racy (same scale) for Yarowsky-prop ?-only on named entity.
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9 1
 
10
 
100
 
1000 55
000
 
60000
 
65000
 
70000
 
75000
 
80000
 
85000
Non-seeded test accuracy | Coverage
Propagation objective value
Iteratio
n
main
cove
rage
objectiv
e
Figure 6: Non-seeded test accuracy (left axis), coverage (left
axis, same scale), and objective value (right axis) for Yarowsky-
prop ?-?. Iterations are shown on a log scale. We omit the first
iteration (where the DL contains only the seed rules) and start
the plot at iteration 2 where there is a complete DL.
gorithm to a structured task such as part of speech
tagging. We also believe that our method for adapt-
ing Collins and Singer (1999)?s cautiousness to
Yarowsky-prop can be applied to similar algorithms
with other underlying classifiers, even to structured
output models such as conditional random fields.
627
References
S. Abney. 2004. Understanding the Yarowsky algorithm.
Computational Linguistics, 30(3).
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM International Con-
ference on Digital Libraries, DL ?00.
A. Blum and S. Chawla. 2001. Learning from labeled
and unlabeled data using graph mincuts. In Proc.
19th International Conference on Machine Learning
(ICML-2001).
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of Computational Learning Theory.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In In EMNLP
1999: Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 100?110.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain, July.
Association for Computational Linguistics.
Hal Daume. 2011. Seeding, transduction, out-of-
sample error and the Microsoft approach... Blog
post at http://nlpers.blogspot.com/2011/04/seeding-
transduction-out-of-sample.html, April 6.
Jason Eisner and Damianos Karakos. 2005. Bootstrap-
ping without the boot. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 395?402, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
Gholamreza Haffari and Anoop Sarkar. 2007. Analysis
of semi-supervised learning with the Yarowsky algo-
rithm. In UAI 2007, Proceedings of the Twenty-Third
Conference on Uncertainty in Artificial Intelligence,
Vancouver, BC, Canada, pages 159?166.
Aria Haghighi and Dan Klein. 2006a. Prototype-driven
grammar induction. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 881?888, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2006b. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320?327, New York
City, USA, June. Association for Computational Lin-
guistics.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics - Vol-
ume 2, COLING ?92, pages 539?545, Stroudsburg,
PA, USA. Association for Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using EM. Machine Learning, 30(3).
Ellen Riloff and Jessica Shepherd. 1997. A corpus-
based approach for building semantic lexicons. In In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, pages 117?
124.
H. J. Scudder. 1965. Probability of error of some adap-
tive pattern-recognition machines. IEEE Transactions
on Information Theory, 11:363?371.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-supervised
learning of structured tagging models. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 167?176, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Partha Pratim Talukdar. 2010. Graph-based weakly-
supervised methods for information extraction & in-
tegration. Ph.D. thesis, University of Pennsylvania.
Software: https://github.com/parthatalukdar/junto.
X. Zhu and Z. Ghahramani and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In Proceedings of International Con-
ference on Machine Learning.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 189?196,
Cambridge, Massachusetts, USA, June. Association
for Computational Linguistics.
628
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940?949,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Mixing Multiple Translation Models in Statistical Machine Translation
Majid Razmara1 George Foster2 Baskaran Sankaran1 Anoop Sarkar1
1 Simon Fraser University, 8888 University Dr., Burnaby, BC, Canada
{razmara,baskaran,anoop}@sfu.ca
2 National Research Council Canada, 283 Alexandre-Tache? Blvd, Gatineau, QC, Canada
george.foster@nrc.gc.ca
Abstract
Statistical machine translation is often faced
with the problem of combining training data
from many diverse sources into a single trans-
lation model which then has to translate sen-
tences in a new domain. We propose a novel
approach, ensemble decoding, which com-
bines a number of translation systems dynam-
ically at the decoding step. In this paper,
we evaluate performance on a domain adap-
tation setting where we translate sentences
from the medical domain. Our experimental
results show that ensemble decoding outper-
forms various strong baselines including mix-
ture models, the current state-of-the-art for do-
main adaptation in machine translation.
1 Introduction
Statistical machine translation (SMT) systems re-
quire large parallel corpora in order to be able to
obtain a reasonable translation quality. In statisti-
cal learning theory, it is assumed that the training
and test datasets are drawn from the same distribu-
tion, or in other words, they are from the same do-
main. However, bilingual corpora are only available
in very limited domains and building bilingual re-
sources in a new domain is usually very expensive.
It is an interesting question whether a model that is
trained on an existing large bilingual corpus in a spe-
cific domain can be adapted to another domain for
which little parallel data is present. Domain adap-
tation techniques aim at finding ways to adjust an
out-of-domain (OUT) model to represent a target do-
main (in-domain or IN).
Common techniques for model adaptation adapt
two main components of contemporary state-of-the-
art SMT systems: the language model and the trans-
lation model. However, language model adapta-
tion is a more straight-forward problem compared to
translation model adaptation, because various mea-
sures such as perplexity of adapted language models
can be easily computed on data in the target domain.
As a result, language model adaptation has been well
studied in various work (Clarkson and Robinson,
1997; Seymore and Rosenfeld, 1997; Bacchiani and
Roark, 2003; Eck et al, 2004) both for speech recog-
nition and for machine translation. It is also easier to
obtain monolingual data in the target domain, com-
pared to bilingual data which is required for transla-
tion model adaptation. In this paper, we focused on
adapting only the translation model by fixing a lan-
guage model for all the experiments. We expect do-
main adaptation for machine translation can be im-
proved further by combining orthogonal techniques
for translation model adaptation combined with lan-
guage model adaptation.
In this paper, a new approach for adapting the
translation model is proposed. We use a novel sys-
tem combination approach called ensemble decod-
ing in order to combine two or more translation
models with the goal of constructing a system that
outperforms all the component models. The strength
of this system combination method is that the sys-
tems are combined in the decoder. This enables
the decoder to pick the best hypotheses for each
span of the input. The main applications of en-
semble models are domain adaptation, domain mix-
ing and system combination. We have modified
Kriya (Sankaran et al, 2012), an in-house imple-
mentation of hierarchical phrase-based translation
system (Chiang, 2005), to implement ensemble de-
coding using multiple translation models.
We compare the results of ensemble decoding
with a number of baselines for domain adaptation.
In addition to the basic approach of concatenation of
in-domain and out-of-domain data, we also trained
a log-linear mixture model (Foster and Kuhn, 2007)
940
as well as the linear mixture model of (Foster et al,
2010) for conditional phrase-pair probabilities over
IN and OUT. Furthermore, within the framework of
ensemble decoding, we study and evaluate various
methods for combining translation tables.
2 Baselines
The natural baseline for model adaption is to con-
catenate the IN and OUT data into a single paral-
lel corpus and train a model on it. In addition to
this baseline, we have experimented with two more
sophisticated baselines which are based on mixture
techniques.
2.1 Log-Linear Mixture
Log-linear translation model (TM) mixtures are of
the form:
p(e?|f?) ? exp
( M?
m
?m log pm(e?|f?)
)
where m ranges over IN and OUT, pm(e?|f?) is an
estimate from a component phrase table, and each
?m is a weight in the top-level log-linear model, set
so as to maximize dev-set BLEU using minimum
error rate training (Och, 2003). We learn separate
weights for relative-frequency and lexical estimates
for both pm(e?|f?) and pm(f? |e?). Thus, for 2 compo-
nent models (from IN and OUT training corpora),
there are 4 ? 2 = 8 TM weights to tune. Whenever
a phrase pair does not appear in a component phrase
table, we set the corresponding pm(e?|f?) to a small
epsilon value.
2.2 Linear Mixture
Linear TM mixtures are of the form:
p(e?|f?) =
M?
m
?mpm(e?|f?)
Our technique for setting ?m is similar to that
outlined in Foster et al (2010). We first extract a
joint phrase-pair distribution p?(e?, f?) from the de-
velopment set using standard techniques (HMM
word alignment with grow-diag-and symmeteriza-
tion (Koehn et al, 2003)). We then find the set
of weights ?? that minimize the cross-entropy of the
mixture p(e?|f?) with respect to p?(e?, f?):
?? = argmax
?
?
e?,f?
p?(e?, f?) log
M?
m
?mpm(e?|f?)
For efficiency and stability, we use the EM algo-
rithm to find ??, rather than L-BFGS as in (Foster et
al., 2010). Whenever a phrase pair does not appear
in a component phrase table, we set the correspond-
ing pm(e?|f?) to 0; pairs in p?(e?, f?) that do not appear
in at least one component table are discarded. We
learn separate linear mixtures for relative-frequency
and lexical estimates for both p(e?|f?) and p(f? |e?).
These four features then appear in the top-level
model as usual ? there is no runtime cost for the lin-
ear mixture.
3 Ensemble Decoding
Ensemble decoding is a way to combine the exper-
tise of different models in one single model. The
current implementation is able to combine hierar-
chical phrase-based systems (Chiang, 2005) as well
as phrase-based translation systems (Koehn et al,
2003). However, the method can be easily extended
to support combining a number of heterogeneous
translation systems e.g. phrase-based, hierarchical
phrase-based, and/or syntax-based systems. This
section explains how such models can be combined
during the decoding.
Given a number of translation models which are
already trained and tuned, the ensemble decoder
uses hypotheses constructed from all of the models
in order to translate a sentence. We use the bottom-
up CKY parsing algorithm for decoding. For each
sentence, a CKY chart is constructed. The cells of
the CKY chart are populated with appropriate rules
from all the phrase tables of different components.
As in the Hiero SMT system (Chiang, 2005), the
cells which span up to a certain length (i.e. the max-
imum span length) are populated from the phrase-
tables and the rest of the chart uses glue rules as de-
fined in (Chiang, 2005).
The rules suggested from the component models
are combined in a single set. Some of the rules may
be unique and others may be common with other
component model rule sets, though with different
scores. Therefore, we need to combine the scores
of such common rules and assign a single score to
941
them. Depending on the mixture operation used for
combining the scores, we would get different mix-
ture scores. The choice of mixture operation will be
discussed in Section 3.1.
Figure 1 illustrates how the CKY chart is filled
with the rules. Each cell, covering a span, is popu-
lated with rules from all component models as well
as from cells covering a sub-span of it.
In the typical log-linear model SMT, the posterior
probability for each phrase pair (e?, f?) is given by:
p(e? | f?) ? exp
(
?
i
wi?i(e?, f?)
? ?? ?
w??
)
Ensemble decoding uses the same framework for
each individual system. Therefore, the score of a
phrase-pair (e?, f?) in the ensemble model is:
p(e? | f?) ? exp
(
w1 ? ?1? ?? ?
1st model
? w2 ? ?2? ?? ?
2nd model
? ? ? ?
)
where? denotes the mixture operation between two
or more model scores.
3.1 Mixture Operations
Mixture operations receive two or more scores
(probabilities) and return the mixture score (prob-
ability). In this section, we explore different options
for mixture operation and discuss some of the char-
acteristics of these mixture operations.
? Weighted Sum (wsum): in wsum the ensemble
probability is proportional to the weighted sum
of all individual model probabilities (i.e. linear
mixture).
p(e? | f?) ?
M?
m
?m exp
(
wm ? ?m
)
where m denotes the index of component mod-
els, M is the total number of them and ?i is the
weight for component i.
? Weighted Max (wmax): where the ensemble
score is the weighted max of all model scores.
p(e? | f?) ? max
m
(
?m exp
(
wm ? ?m
))
? Model Switching (Switch): in model switch-
ing, each cell in the CKY chart gets populated
only by rules from one of the models and the
other models? rules are discarded. This is based
on the hypothesis that each component model
is an expert on certain parts of sentence. In this
method, we need to define a binary indicator
function ?(f? ,m) for each span and component
model to specify rules of which model to retain
for each span.
?(f? ,m) =
?
?
?
1, m = argmax
n?M
?(f? , n)
0, otherwise
The criteria for choosing a model for each cell,
?(f? , n), could be based on:
? Max: for each cell, the model that has the
highest weighted best-rule score wins:
?(f? , n) = ?n max
e
(wn ? ?n(e?, f?))
? Sum: Instead of comparing only the
scores of the best rules, the model with
the highest weighted sum of the probabil-
ities of the rules wins. This sum has to
take into account the translation table limit
(ttl), on the number of rules suggested by
each model for each cell:
?(f? , n) = ?n
?
e?
exp
(
wn ? ?n(e?, f?)
)
The probability of each phrase-pair (e?, f?) is
computed as:
p(e? | f?) =
M?
m
?(f? ,m) pm(e? | f?)
? Product (prod): in Product models or Prod-
uct of Experts (Hinton, 1999), the probability
of the ensemble model or a rule is computed as
the product of the probabilities of all compo-
nents (or equally the sum of log-probabilities,
i.e. log-linear mixture). Product models can
also make use of weights to control the contri-
bution of each component. These models are
942
Figure 1: The cells in the CKY chart are populated using rules from all component models and sub-span cells.
generally known as Logarithmic Opinion Pools
(LOPs) where:
p(e? | f?) ? exp
(
M?
m
?m (wm ? ?m)
)
Product models have been used in combining
LMs and TMs in SMT as well as some other
NLP tasks such as ensemble parsing (Petrov,
2010).
Each of these mixture operations has a specific
property that makes it work in specific domain adap-
tation or system combination scenarios. For in-
stance, LOPs may not be optimal for domain adapta-
tion in the setting where there are two or more mod-
els trained on heterogeneous corpora. As discussed
in (Smith et al, 2005), LOPs work best when all the
models accuracies are high and close to each other
with some degree of diversity. LOPs give veto power
to any of the component models and this perfectly
works for settings such as the one in (Petrov, 2010)
where a number of parsers are trained by changing
the randomization seeds but having the same base
parser and using the same training set. They no-
ticed that parsers trained using different randomiza-
tion seeds have high accuracies but there are some
diversities among them and they used product mod-
els for their advantage to get an even better parser.
We assume that each of the models is expert in some
parts and so they do not necessarily agree on cor-
rect hypotheses. In other words, product models (or
LOPs) tend to have intersection-style effects while
we are more interested in union-style effects.
In Section 4.2, we compare the BLEU scores of
different mixture operations on a French-English ex-
perimental setup.
3.2 Normalization
Since in log-linear models, the model scores are
not normalized to form probability distributions, the
scores that different models assign to each phrase-
pair may not be in the same scale. Therefore, mixing
their scores might wash out the information in one
(or some) of the models. We experimented with two
different ways to deal with this normalization issue.
A practical but inexact heuristic is to normalize the
scores over a shorter list. So the list of rules coming
from each model for a cell in CKY chart is normal-
ized before getting mixed with other phrase-table
rules. However, experiments showed changing the
scores with the normalized scores hurts the BLEU
score radically. So we use the normalized scores
only for pruning and the actual scores are intact.
We could also globally normalize the scores to ob-
tain posterior probabilities using the inside-outside
algorithm. However, we did not try it as the BLEU
scores we got using the normalization heuristic was
not promissing and it would impose a cost in de-
coding as well. More investigation on this issue has
been left for future work.
A more principled way is to systematically find
the most appropriate model weights that can avoid
this problem by scaling the scores properly. We
used a publicly available toolkit, CONDOR (Van-
den Berghen and Bersini, 2005), a direct optimizer
based on Powell?s algorithm, that does not require
943
explicit gradient information for the objective func-
tion. Component weights for each mixture operation
are optimized on the dev-set using CONDOR.
4 Experiments & Results
4.1 Experimental Setup
We carried out translation experiments using the Eu-
ropean Medicines Agency (EMEA) corpus (Tiede-
mann, 2009) as IN, and the Europarl (EP) corpus1 as
OUT, for French to English translation. The dev and
test sets were randomly chosen from the EMEA cor-
pus.2 The details of datasets used are summarized in
Table 1.
Dataset Sents
Words
French English
EMEA 11770 168K 144K
Europarl 1.3M 40M 37M
Dev 1533 29K 25K
Test 1522 29K 25K
Table 1: Training, dev and test sets for EMEA.
For the mixture baselines, we used a standard
one-pass phrase-based system (Koehn et al, 2003),
Portage (Sadat et al, 2005), with the following 7
features: relative-frequency and lexical translation
model (TM) probabilities in both directions; word-
displacement distortion model; language model
(LM) and word count. The corpus was word-aligned
using both HMM and IBM2 models, and the phrase
table was the union of phrases extracted from these
separate alignments, with a length limit of 7. It
was filtered to retain the top 20 translations for each
source phrase using the TM part of the current log-
linear model.
For ensemble decoding, we modified an in-house
implementation of hierarchical phrase-based sys-
tem, Kriya (Sankaran et al, 2012) which uses the
same features mentioned in (Chiang, 2005): for-
ward and backward relative-frequency and lexical
TM probabilities; LM; word, phrase and glue-rules
penalty. GIZA++(Och and Ney, 2000) has been used
for word alignment with phrase length limit of 7.
In both systems, feature weights were optimized
using MERT (Och, 2003) and with a 5-gram lan-
1www.statmt.org/europarl
2Please contact the authors to access the data-sets.
guage model and Kneser-Ney smoothing was used
in all the experiments. We used SRILM (Stolcke,
2002) as the langugage model toolkit. Fixing the
language model allows us to compare various trans-
lation model combination techniques.
4.2 Results
Table 2 shows the results of the baselines. The first
group are the baseline results on the phrase-based
system discussed in Section 2 and the second group
are those of our hierarchical MT system. Since the
Hiero baselines results were substantially better than
those of the phrase-based model, we also imple-
mented the best-performing baseline, linear mixture,
in our Hiero-style MT system and in fact it achieves
the hights BLEU score among all the baselines as
shown in Table 2. This baseline is run three times
the score is averaged over the BLEU scores with
standard deviation of 0.34.
Baseline PBS Hiero
IN 31.84 33.69
OUT 24.08 25.32
IN + OUT 31.75 33.76
LOGLIN 32.21 ?
LINMIX 33.81 35.57
Table 2: The results of various baselines implemented in
a phrase-based (PBS) and a Hiero SMT on EMEA.
Table 3 shows the results of ensemble decoding
with different mixture operations and model weight
settings. Each mixture operation has been evalu-
ated on the test-set by setting the component weights
uniformly (denoted by uniform) and by tuning the
weights using CONDOR (denoted by tuned) on a
held-out set. The tuned scores (3rd column in Ta-
ble 3) are averages of three runs with different initial
points as in Clark et al (2011). We also reported the
BLEU scores when we applied the span-wise nor-
malization heuristic. All of these mixture operations
were able to significantly improve over the concate-
nation baseline. In particular, Switching:Max could
gain up to 2.2 BLEU points over the concatenation
baseline and 0.39 BLEU points over the best per-
forming baseline (i.e. linear mixture model imple-
mented in Hiero) which is statistically significant
based on Clark et al (2011) (p = 0.02).
Prod when using with uniform weights gets the
944
Mixture Operation Uniform Tuned Norm.
WMAX 35.39 35.47 (s=0.03) 35.47
WSUM 35.35 35.53 (s=0.04) 35.45
SWITCHING:MAX 35.93 35.96 (s=0.01) 32.62
SWITCHING:SUM 34.90 34.72 (s=0.23) 34.90
PROD 33.93 35.24 (s=0.05) 35.02
Table 3: The results of ensemble decoding on EMEA for Fr2En when using uniform weights, tuned weights and
normalization heuristic. The tuned BLEU scores are averaged over three runs with multiple initial points, as in (Clark
et al, 2011), with the standard deviations in brackets .
lowest score among the mixture operations, how-
ever after tuning, it learns to bias the weights to-
wards one of the models and hence improves by
1.31 BLEU points. Although Switching:Sum outper-
forms the concatenation baseline, it is substantially
worse than other mixture operations. One explana-
tion that Switching:Max is the best performing op-
eration and Switching:Sum is the worst one, despite
their similarities, is that Switching:Max prefers more
peaked distributions while Switching:Sum favours a
model that has fewer hypotheses for each span.
An interesting observation based on the results in
Table 3 is that uniform weights are doing reasonably
well given that the component weights are not opti-
mized and therefore model scores may not be in the
same scope (refer to discussion in ?3.2). We suspect
this is because a single LM is shared between both
models. This shared component controls the vari-
ance of the weights in the two models when com-
bined with the standard L-1 normalization of each
model?s weights and hence prohibits models to have
too varied scores for the same input. Though, it may
not be the case when multiple LMs are used which
are not shared.
Two sample sentences from the EMEA test-set
along with their translations by the IN, OUT and En-
semble models are shown in Figure 2. The boxes
show how the Ensemble model is able to use n-
grams from the IN and OUT models to construct
a better translation than both of them. In the first
example, there are two OOVs one for each of the
IN and OUT models. Our approach is able to re-
solve the OOV issues by taking advantage of the
other model?s presence. Similarly, the second exam-
ple shows how ensemble decoding improves lexical
choices as well as word re-orderings.
5 Related Work
5.1 Domain Adaptation
Early approaches to domain adaptation involved in-
formation retrieval techniques where sentence pairs
related to the target domain were retrieved from the
training corpus using IR methods (Eck et al, 2004;
Hildebrand et al, 2005). Foster et al (2010), how-
ever, uses a different approach to select related sen-
tences from OUT. They use language model per-
plexities from IN to select relavant sentences from
OUT. These sentences are used to enrich the IN
training set.
Other domain adaptation methods involve tech-
niques that distinguish between general and domain-
specific examples (Daume? and Marcu, 2006). Jiang
and Zhai (2007) introduce a general instance weight-
ing framework for model adaptation. This approach
tries to penalize misleading training instances from
OUT and assign more weight to IN-like instances
than OUT instances. Foster et al (2010) propose a
similar method for machine translation that uses fea-
tures to capture degrees of generality. Particularly,
they include the output from an SVM classifier that
uses the intersection between IN and OUT as pos-
itive examples. Unlike previous work on instance
weighting in machine translation, they use phrase-
level instances instead of sentences.
A large body of work uses interpolation tech-
niques to create a single TM/LM from interpolating
a number of LMs/TMs. Two famous examples of
such methods are linear mixtures and log-linear mix-
tures (Koehn and Schroeder, 2007; Civera and Juan,
2007; Foster and Kuhn, 2007) which were used as
baselines and discussed in Section 2. Other meth-
ods include using self-training techniques to exploit
monolingual in-domain data (Ueffing et al, 2007;
945
SOURCE ame?norrhe?e , menstruations irre?gulie`res
REF amenorrhoea , irregular menstruation
IN amenorrhoea , menstruations irre?gulie`res
OUT ame?norrhe?e , irregular menstruation
ENSEMBLE amenorrhoea , irregular menstruation
SOURCE le traitement par naglazyme doit e?tre supervise? par un me?decin ayant l? expe?rience de
la prise en charge des patients atteints de mps vi ou d? une autre maladie me?tabolique
he?re?ditaire .
REF naglazyme treatment should be supervised by a physician experienced in the manage-
ment of patients with mps vi or other inherited metabolic diseases .
IN naglazyme treatment should be supervise? by a doctor the with
in the management of patients with mps vi or other hereditary metabolic disease .
OUT naglazyme ?s treatment must be supervised by a doctor with the experience of the care
of patients with mps vi. or another disease hereditary metabolic .
ENSEMBLE naglazyme treatment should be supervised by a physician experienced
in the management of patients with mps vi or other hereditary metabolic disease .
Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems.
Bertoldi and Federico, 2009). In this approach, a
system is trained on the parallel OUT and IN data
and it is used to translate the monolingual IN data
set. Iteratively, most confident sentence pairs are se-
lected and added to the training corpus on which a
new system is trained.
5.2 System Combination
Tackling the model adaptation problem using sys-
tem combination approaches has been experimented
in various work (Koehn and Schroeder, 2007; Hilde-
brand and Vogel, 2009). Among these approaches
are sentence-based, phrase-based and word-based
output combination methods. In a similar approach,
Koehn and Schroeder (2007) use a feature of the fac-
tored translation model framework in Moses SMT
system (Koehn and Schroeder, 2007) to use multiple
alternative decoding paths. Two decoding paths, one
for each translation table (IN and OUT), were used
during decoding. The weights are set with minimum
error rate training (Och, 2003).
Our work is closely related to Koehn and
Schroeder (2007) but uses a different approach to
deal with multiple translation tables. The Moses
SMT system implements (Koehn and Schroeder,
2007) and can treat multiple translation tables in
two different ways: intersection and union. In in-
tersection, for each span only the hypotheses would
be used that are present in all phrase tables. For
each set of hypothesis with the same source and
target phrases, a new hypothesis is created whose
feature-set is the union of feature sets of all corre-
sponding hypotheses. Union, on the other hand, uses
hypotheses from all the phrase tables. The feature
set of these hypotheses are expanded to include one
feature set for each table. However, for the corre-
sponding feature values of those phrase-tables that
did not have a particular phrase-pair, a default log
probability value of 0 is assumed (Bertoldi and Fed-
erico, 2009) which is counter-intuitive as it boosts
the score of hypotheses with phrase-pairs that do not
belong to all of the translation tables.
Our approach is different from Koehn and
Schroeder (2007) in a number of ways. Firstly, un-
like the multi-table support of Moses which only
supports phrase-based translation table combination,
our approach supports ensembles of both hierarchi-
cal and phrase-based systems. With little modifica-
tion, it can also support ensemble of syntax-based
systems with the other two state-of-the-art SMT sys-
946
tems. Secondly, our combining method uses the
union option, but instead of preserving the features
of all phrase-tables, it only combines their scores
using various mixture operations. This enables us
to experiment with a number of different opera-
tions as opposed to sticking to only one combination
method. Finally, by avoiding increasing the number
of features we can add as many translation models
as we need without serious performance drop. In
addition, MERT would not be an appropriate opti-
mizer when the number of features increases a cer-
tain amount (Chiang et al, 2008).
Our approach differs from the model combina-
tion approach of DeNero et al (2010), a generaliza-
tion of consensus or minimum Bayes risk decoding
where the search space consists of those of multi-
ple systems, in that model combination uses forest
of derivations of all component models to do the
combination. In other words, it requires all compo-
nent models to fully decode each sentence, compute
n-gram expectations from each component model
and calculate posterior probabilities over transla-
tion derivations. While, in our approach we only
use partial hypotheses from component models and
the derivation forest is constructed by the ensemble
model. A major difference is that in the model com-
bination approach the component search spaces are
conjoined and they are not intermingled as opposed
to our approach where these search spaces are inter-
mixed on spans. This enables us to generate new
sentences that cannot be generated by component
models. Furthermore, various combination methods
can be explored in our approach. Finally, main tech-
niques used in this work are orthogonal to our ap-
proach such as Minimum Bayes Risk decoding, us-
ing n-gram features and tuning using MERT.
Finally, our work is most similar to that of
Liu et al (2009) where max-derivation and max-
translation decoding have been used. Max-
derivation finds a derivation with highest score and
max-translation finds the highest scoring translation
by summing the score of all derivations with the
same yield. The combination can be done in two
levels: translation-level and derivation-level. Their
derivation-level max-translation decoding is similar
to our ensemble decoding with wsum as the mixture
operation. We did not restrict ourself to this par-
ticular mixture operation and experimented with a
number of different mixing techniques and as Ta-
ble 3 shows we could improve over wsum in our
experimental setup. Liu et al (2009) used a mod-
ified version of MERT to tune max-translation de-
coding weights, while we use a two-step approach
using MERT for tuning each component model sep-
arately and then using CONDOR to tune component
weights on top of them.
6 Conclusion & Future Work
In this paper, we presented a new approach for do-
main adaptation using ensemble decoding. In this
approach a number of MT systems are combined at
decoding time in order to form an ensemble model.
The model combination can be done using various
mixture operations. We showed that this approach
can gain up to 2.2 BLEU points over its concatena-
tion baseline and 0.39 BLEU points over a powerful
mixture model.
Future work includes extending this approach to
use multiple translation models with multiple lan-
guage models in ensemble decoding. Different
mixture operations can be investigated and the be-
haviour of each operation can be studied in more
details. We will also add capability of support-
ing syntax-based ensemble decoding and experi-
ment how a phrase-based system can benefit from
syntax information present in a syntax-aware MT
system. Furthermore, ensemble decoding can be ap-
plied on domain mixing settings in which develop-
ment sets and test sets include sentences from dif-
ferent domains and genres, and this is a very suit-
able setting for an ensemble model which can adapt
to new domains at test time. In addition, we can
extend our approach by applying some of the tech-
niques used in other system combination approaches
such as consensus decoding, using n-gram features,
tuning using forest-based MERT, among other pos-
sible extensions.
Acknowledgments
This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Fac-
ulty Award to the last author. We would like to
thank Philipp Koehn and the anonymous reviewers
for their valuable comments. We also thank the de-
velopers of GIZA++ and Condor which we used for
our experiments.
947
References
M. Bacchiani and B. Roark. 2003. Unsupervised lan-
guage model adaptation. In Acoustics, Speech, and
Signal Processing, 2003. Proceedings. (ICASSP ?03).
2003 IEEE International Conference on, volume 1,
pages I?224 ? I?227 vol.1, april.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, StatMT
?09, pages 182?189, Stroudsburg, PA, USA. ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. ACL.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270, Mor-
ristown, NJ, USA. ACL.
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Second Workshop
on Statistical Machine Translation, StatMT ?07, pages
177?180, Stroudsburg, PA, USA. ACL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume 2,
HLT ?11, pages 176?181. ACL.
P. Clarkson and A. Robinson. 1997. Language model
adaptation using mixtures and an exponentially decay-
ing cache. In Proceedings of the 1997 IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP ?97)-Volume 2 - Volume 2,
ICASSP ?97, pages 799?, Washington, DC, USA.
IEEE Computer Society.
Hal Daume?, III and Daniel Marcu. 2006. Domain
adaptation for statistical classifiers. J. Artif. Int. Res.,
26:101?126, May.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine transla-
tion. In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 975?983, Stroudsburg, PA, USA. ACL.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language model adaptation for statistical machine
translation based on information retrieval. In In Pro-
ceedings of LREC.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for smt. In Proceedings of the Second
Workshop on Statistical Machine Translation, StatMT
?07, pages 128?135, Stroudsburg, PA, USA. ACL.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 451?
459, Stroudsburg, PA, USA. ACL.
Almut Silja Hildebrand and Stephan Vogel. 2009. CMU
system combination for WMT?09. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 47?50, Stroudsburg, PA, USA.
ACL.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th EAMT
2005, Budapest, Hungary, May.
Geoffrey E. Hinton. 1999. Products of experts. In Artifi-
cial Neural Networks, 1999. ICANN 99. Ninth Interna-
tional Conference on (Conf. Publ. No. 470), volume 1,
pages 1?6.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 264?271, Prague, Czech
Republic, June. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT ?07, pages 224?
227, Stroudsburg, PA, USA. ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL, pages 127?133, Edmonton, May.
NAACL.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, ACL ?09, pages
576?584, Stroudsburg, PA, USA. ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual Meet-
ing of the ACL, pages 440?447, Hongkong, China, Oc-
tober.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Meeting of the ACL, Sapporo, July. ACL.
948
Slav Petrov. 2010. Products of random latent variable
grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 19?27, Stroudsburg, PA, USA. ACL.
Fatiha Sadat, Howard Johnson, Akakpo Agbago, George
Foster, Joel Martin, and Aaron Tikuisis. 2005.
Portage: A phrase-based machine translation system.
In In Proceedings of the ACL Worskhop on Building
and Using Parallel Texts, Ann Arbor. ACL.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics, 97(97), April.
Kristie Seymore and Ronald Rosenfeld. 1997. Us-
ing story topics for language model adaptation. In
George Kokkinakis, Nikos Fakotakis, and Evangelos
Dermatas, editors, EUROSPEECH. ISCA.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 18?25, Stroudsburg, PA, USA. ACL.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings International Con-
ference on Spoken Language Processing, pages 257?
286.
Jorg Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 25?32, Prague, Czech Republic, June. ACL.
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of pow-
ell?s UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157?175,
September.
949
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1105?1115,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Graph Propagation for Paraphrasing Out-of-Vocabulary Words in
Statistical Machine Translation?
Majid Razmara1 Maryam Siahbani1 Gholamreza Haffari2 Anoop Sarkar1
1 Simon Fraser University, Burnaby, BC, Canada
{razmara,msiahban,anoop}@sfu.ca
2 Monash University, Clayton, VIC, Australia
reza@monash.edu
Abstract
Out-of-vocabulary (oov) words or phrases
still remain a challenge in statistical machine
translation especially when a limited amount of
parallel text is available for training or when
there is a domain shift from training data to
test data. In this paper, we propose a novel
approach to finding translations for oov words.
We induce a lexicon by constructing a graph on
source language monolingual text and employ
a graph propagation technique in order to find
translations for all the source language phrases.
Our method differs from previous approaches
by adopting a graph propagation approach that
takes into account not only one-step (from oov
directly to a source language phrase that has a
translation) but multi-step paraphrases from oov
source language words to other source language
phrases and eventually to target language transla-
tions. Experimental results show that our graph
propagation method significantly improves per-
formance over two strong baselines under intrin-
sic and extrinsic evaluation metrics.
1 Introduction
Out-of-vocabulary (oov) words or phrases still re-
main a challenge in statistical machine translation.
SMT systems usually copy unknown words verba-
tim to the target language output. Although this is
helpful in translating a small fraction of oovs such
as named entities for languages with same writ-
ing systems, it harms the translation in other types
of oovs and distant language pairs. In general,
copied-over oovs are a hindrance to fluent, high
quality translation, and we can see evidence of this
in automatic measures such as BLEU (Papineni
et al, 2002) and also in human evaluation scores
such as HTER. The problem becomes more se-
vere when only a limited amount of parallel text is
available for training or when the training and test
data are from different domains. Even noisy trans-
lation of oovs can aid the language model to better
?This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant. The third author was sup-
ported by an early career research award from Monash Uni-
versity to visit Simon Fraser University.
re-order the words in the target language (Zhang
et al, 2012).
Increasing the size of the parallel data can re-
duce the number of oovs. However, there will al-
ways be some words or phrases that are new to the
system and finding ways to translate such words
or phrases will be beneficial to the system. Re-
searchers have applied a number of approaches to
tackle this problem. Some approaches use pivot
languages (Callison-Burch et al, 2006) while oth-
ers use lexicon-induction-based approaches from
source language monolingual corpora (Koehn and
Knight, 2002; Garera et al, 2009; Marton et al,
2009).
Pivot language techniques tackle this problem
by taking advantage of available parallel data be-
tween the source language and a third language.
Using a pivot language, oovs are translated into a
third language and back into the source language
and thereby paraphrases to those oov words are
extracted (Callison-Burch et al, 2006). For each
oov, the system can be augmented by aggregating
the translations of all its paraphrases and assign
them to the oov. However, these methods require
parallel corpora between the source language and
one or multiple pivot languages.
Another line of work exploits spelling and mor-
phological variants of oov words. Habash (2008)
presents techniques for online handling of oov
words for Arabic to English such as spelling ex-
pansion and morphological expansion. Huang et
al. (2011) proposes a method to combine sub-
lexical/constituent translations of an oov word or
phrase to generate its translations.
Several researchers have applied lexicon-
induction methods to create a bilingual lexicon
for those oovs. Marton et al (2009) use a mono-
lingual text on the source side to find paraphrases
to oov words for which the translations are avail-
able. The translations for these paraphrases are
1105
then used as the translations of the oov word.
These methods are based on the distributional hy-
pothesis which states that words appearing in the
same contexts tend to have similar meaning (Har-
ris, 1954). Marton et al (2009) showed that this
method improves over the baseline system where
oovs are untranslated.
We propose a graph propagation-based exten-
sion to the approach of Marton et al (2009) in
which a graph is constructed from source language
monolingual text1 and the source-side of the avail-
able parallel data. Nodes that have related mean-
ings are connected together and nodes for which
we have translations in the phrase-table are an-
notated with target-side translations and their fea-
ture values. A graph propagation algorithm is then
used to propagate translations from labeled nodes
to unlabeled nodes (phrases appearing only in the
monolingual text and oovs). This provides a gen-
eral purpose approach to handle several types of
oovs, including morphological variants, spelling
variants and synonyms2.
Constructing such a huge graph and propagat-
ing messages through it pose severe computational
challenges. Throughout the paper, we will see how
these challenges are dealt with using scalable algo-
rithms.
2 Collocational Lexicon Induction
Rapp (1995) introduced the notion of a distribu-
tional profile in bilingual lexicon induction from
monolingual data. A distributional profile (DP) of
a word or phrase type is a co-occurrence vector
created by combining all co-occurrence vectors of
the tokens of that phrase type. Each distributional
profile can be seen as a point in a |V |-dimensional
space where V is the vocabulary where each word
type represents a unique axis. Points (i.e. phrase
types) that are close to one another in this high-
dimensional space can represent paraphrases. This
approach has also been used in machine trans-
lation to find in-vocabulary paraphrases for oov
words on the source side and find a way to trans-
late them.
2.1 Baseline System
Marton et al (2009) was the first to successfully
integrate a collocational approach to finding trans-
1Here on by monolingual data we always mean monolin-
gual data on the source language
2Named entity oovs may be handled properly by copying
or transliteration.
lations for oov words into an end-to-end SMT sys-
tem. We explain their method in detail as we will
compare against this approach. The method re-
lies on monolingual distributional profiles (DPs)
which are numerical vectors representing the con-
text around each word. The goal is to find words or
phrases that appear in similar contexts as the oovs.
For each oov a distributional profile is created by
collecting all words appearing in a fixed distance
from all occurrences of the oov word in the mono-
lingual text. These co-occurrence counts are con-
verted to an association measure (Section 2.2) that
encodes the relatedness of each pair of words or
phrases.
Then, the most similar phrases to each oov are
found by measuring the similarity of their DPs to
that of the oov word. Marton et al (2009) uses
a heuristic to prune the search space for finding
candidate paraphrases by keeping the surrounding
context (e.g. L R) of each occurrences of the
oov word. All phrases that appear in any of such
contexts are collected as candidate paraphrases.
For each of these paraphrases, a DP is constructed
and compared to that of the oov word using a sim-
ilarity measure (Section 2.2).
The top-k paraphrases that have translations in
the phrase-table are used to assign translations and
scores to each oov word by marginalizing transla-
tions over paraphrases:
p(t|o) =
?
s
p(t|s)p(s|o)
where t is a phrase on the target side, o is the oov
word or phrase, and s is a paraphrase of o. p(s|o)
is estimated using a similarity measure over DPs
and p(t|s) is coming from the phrase-table.
We reimplemented this collocational approach
for finding translations for oovs and used it as a
baseline system.
Alternative ways of modeling and comparing
distributional profiles have been proposed (Rapp,
1999; Fung and Yee, 1998; Terra and Clarke,
2003; Garera et al, 2009; Marton et al, 2009).
We review some of them here and compare their
performance in Section 4.3.
2.2 Association Measures
Given a word u, its distributional profile DP (u)
is constructed by counting surrounding words (in
a fixed window size) in a monolingual corpus.
DP (u) = {?A(u,wi)? | wi ? V }
1106
The counts can be collected in positional3 (Rapp,
1999) or non-positional way (count all the word
occurrences within the sliding window). A(?, ?)
is an association measure and can simply be de-
fined as co-occurrence counts within sliding win-
dows. Stronger association measures can also be
used such as:
Conditional probability: the probability for the
occurrence of each word in DP given the occur-
rence of u: CP(u,wi) = P (wi|u) (Schu?tze and
Pedersen, 1997)
Pointwise Mutual Information: this measure is
a transformation of the independence assumption
into a ratio. Positive values indicate that words
co-occur more than what we expect under the in-
dependence assumption (Lin, 1998):
PMI(u,wi) = log2 P (u,wi)P (u)P (wi)
Likelihood ratio: (Dunning, 1993) uses the like-
lihood ratio for word similarity:
?(u,wi) =
L(P (wi|u); p) ? L(P (wi|?u); p)
L(P (wi|u); p1) ? L(P (wi|?u); p2)
where L is likelihood function under the assump-
tion that word counts in text have binomial distri-
butions. The numerator represents the likelihood
of the hypothesis that u and wi are independent
(P (wi|u) = P (wi|?u) = p) and the denomina-
tor represents the likelihood of the hypothesis that
u and wi are dependent (P (wi|u) 6= P (wi|?u) ,
P (wi|u) = p1, P (wi|?u) = p2 )4.
Chi-square test: is a statistical hypothesis testing
method to evaluate independence of two categori-
cal random variables, e.g. whether the occurrence
of u and wi (denoted by x and y respectively) are
independent. The test statistics ?2(u,wi) is the
deviation of the observed counts fx,y from their
expected values Ex,y:
?2(u,wi) :=
?
x?{wi,?wi}
?
y?{u,?u}
(fx,y ? Ex,y)2
Ex,y
2.3 Similarity Measures
Various functions have been used to estimate
the similarity between distributional profiles.
3e.g., position 1 is the word immediately after, position -1
is the word immediately before etc.
4Binomial distribution B(k;n, ?) gives the probability of
observing k heads in n tosses of a coin where the coin pa-
rameter is ?. In our context, p, p1 and p2 are parameters of
Binomial distributions estimated using maximum likelihood.
Given two distributional profiles DP (u) and
DP (v), some similarity functions can be defined
as follows. Note that A(?, ?) stands for the various
association measures defined in Sec. 2.2.
Cosine coefficient is the cosine the angle between
two vectors DP (u) and DP (v):
cos(DP (u), DP (v)) =?
wi?V A(u,wi)A(v, wi)??
wi?V A(u,wi)2
??
wi?V A(v, wi)2
L1-Norm computes the accumulated distance
between entries of two distributional profiles
(L1(?, ?)). It has been used as word similarity mea-
sure in language modeling (Dagan et al, 1999).
L1(DP (u), DP (v)) =
?
wi?V
|A(u,wi)?A(v, wi)|
Jensen-Shannon Divergence is a symmetric ver-
sion of contextual average mutual information
(KL) which is used by (Dagan et al, 1999) as
word similarity measure.
JSD(DP (u), DP (v)) =KL(DP (u), AV GDP (u, v))+
KL(DP (v), AV GDP (u, v))
AV GDP (u, v) =
{
A(u,wi) +A(v, wi)
2 | wi ? V
}
KL(DP (u), DP (v)) =
?
wi?V
A(u,wi)log
A(u,wi)
A(v, wi)
3 Graph-based Lexicon Induction
We propose a novel approach to alleviate the oov
problem. Given a (possibly small amount of) par-
allel data between the source and target languages,
and a large monolingual data in the source lan-
guage, we construct a graph over all phrase types
in the monolingual text and the source side of the
parallel corpus and connect phrases that have sim-
ilar meanings (i.e. appear in similar context) to one
another. To do so, the distributional profiles of
all source phrase types are created. Each phrase
type represents a vertex in the graph and is con-
nected to other vertices with a weight defined by a
similarity measure between the two profiles (Sec-
tion 2.3). There are three types of vertices in the
graph: i) labeled nodes which appear in the par-
allel corpus and for which we have the target-side
1107
translations5; ii) oov nodes from the dev/test set
for which we seek labels (translations); and iii) un-
labeled nodes (words or phrases) from the mono-
lingual data which appear usually between oov
nodes and labeled nodes. When a relatively small
parallel data is used, unlabeled nodes outnumber
labeled ones and many of them lie on the paths
between an oov node to labeled ones.
Marton et al (2009)?s approach ignores these
bridging nodes and connects each oov node to the
k-nearest labeled nodes. One may argue that these
unlabeled nodes do not play a major role in the
graph and the labels will eventually get to the oov
nodes from the labeled nodes by directly connect-
ing them. However based on the definition of the
similarity measures using context, it is quite possi-
ble that an oov node and a labeled node which are
connected to the same unlabeled node do not share
any context words and hence are not directly con-
nected. For instance, consider three nodes, u (un-
labeled), o (oov) and l (labeled) where u has the
same left context words with o but share the right
context with l. o and l are not connected since they
do not share any context word.
Once a graph is constructed based on simi-
larities of phrases, graph propagation is used to
propagate the labels from labeled nodes to unla-
beled and oov nodes. The approach is based on
the smoothness assumption (Chapelle et al, 2006)
which states if two nodes are similar according to
the graph, then their output labels should also be
similar.
The baseline approach (Marton et al, 2009) can
be formulated as a bipartite graph with two types
of nodes: labeled nodes (L) and oov nodes (O).
Each oov node is connected to a number of labeled
nodes, and vice versa and there is no edge between
nodes of the same type. In such a graph, the sim-
ilarity of each pair of nodes is computed using
one of the similarity measures discussed above.
The labels are translations and their probabilities
(more specifically p(e|f)) from the phrase-table
extracted from the parallel corpus. Translations
get propagated to oov nodes using a label prop-
agation technique. However beside the difference
in the oov label assignment, there is a major differ-
ence between our bipartite graph and the baseline
(Marton et al, 2009): we do not use a heuristic to
5It is possible that a phrase appears in the parallel corpus,
but not in the phrase-table. This happens when the word-
alignment module is not able to align the phrase to a target
side word or words.
reduce the number of neighbor candidates and we
consider all possible candidates that share at least
one context word. This makes a significant differ-
ence in practice as shown in Section 4.3.1.
We also take advantage of unlabeled nodes to
help connect oov nodes to labeled ones. The dis-
cussed bipartite graph can easily be expanded to a
tripartite graph by adding unlabeled nodes. Fig-
ure 1 illustrate a tripartite graph in which unla-
beled nodes are connected to both labeled and oov
nodes. Again, there is no edge between nodes
of the same type. We also created the full graph
where all nodes can be freely connected to nodes
of any type including the same type. However,
constructing such graph and doing graph propa-
gation on it is computationally very expensive for
large n-grams.
3.1 Label Propagation
Let G = (V,E,W ) be a graph where V is the set
of vertices,E is the set of edges, andW is the edge
weight matrix. The vertex set V consists of la-
beled VL and unlabeled VU nodes, and the goal of
the labeling propagation algorithm is to compute
soft labels for unlabeled vertices from the labeled
vertices. Intuitively, the edge weight W (u, v) en-
codes the degree of our belief about the similarity
of the soft labeling for nodes u and v. A soft label
Y?v ? ?m+1 is a probability vector in (m + 1)-
dimensional simplex, where m is the number of
possible labels and the additional dimension ac-
counts for the undefined ? label6.
In this paper, we make use of the modified Ad-
sorption (MAD) algorithm (Talukdar and Cram-
mer, 2009) which finds soft label vectors Y?v to
solve the following unconstrained optimization
problem:
min
Y?
?1
?
v?VL
p1,v||Yv ? Y?v||22 + (1)
?2
?
v,u
p2,vWv,u||Y?v ? Y?u||22 + (2)
?3
?
v
p3,v||Y?v ?Rv||22 (3)
where ?i and pi,v are hyper-parameters (?v :?
i pi,v = 1)7, and Rv ? ?m+1 encodes our prior
belief about the labeling of a node v. The first
6Capturing those cases where the given data is not enough
to reliably compute a soft labeling using the initial m real
labels.
7The values of these hyper-parameters are set to their de-
faults in the Junto toolkit (Talukdar and Crammer, 2009).
1108
o1
o2
o3
l1
l2
l3
u1 u2 u3 u4 u5
t11 : p11
t12 : p12
t13 : p13
t21 : p21
t22 : p22
t23 : p23
t31 : p31
t32 : p32
t33 : p33
O : oov nodes L : labeled nodes
U : unlabeled nodes
sim(o1, l1)
Figure 1: A tripartite graph between oov, labeled and unlabeled nodes. Translations propagate either directly from labeled
nodes to oov nodes or indirectly via unlabeled nodes.
term (1) enforces the labeling of the algorithm to
match the seed labeling Yv with different extent
for different labeled nodes. The second term (2)
enforces the smoothness of the labeling according
to the graph structure and edge weights. The last
term (3) regularizes the soft labeling for a vertex
v to match a priori label Rv, e.g. for high-degree
unlabeled nodes (hubs in the graph) we may be-
lieve that the neighbors are not going to produce
reliable label and hence the probability of unde-
fined label ? should be higher. The optimiza-
tion problem can be solved with an efficient iter-
ative algorithm which is parallelized in a MapRe-
duce framework (Talukdar et al, 2008; Rao and
Yarowsky, 2009). We used the Junto label prop-
agation toolkit (Talukdar and Crammer, 2009) for
label propagation.
3.2 Efficient Graph Construction
Graph-based approaches can easily become com-
putationally very expensive as the number of
nodes grow. In our case, we use phrases in the
monolingual text as graph vertices. These phrases
are n-grams up to a certain value, which can re-
sult in millions of nodes. For each node a distribu-
tional profile (DP) needs to be created. The num-
ber of possible edges can easily explode in size
as there can be as many as O(n2) edges where n
is the number of nodes. A common practice to
control the number of edges is to connect each
node to at most k other nodes (k-nearest neigh-
bor). However, finding the top-k nearest nodes to
each node requires considering its similarity to all
the other nodes which requires O(n2) computa-
tions and since n is usually very large, doing such
is practically intractable. Therefore, researchers
usually resort to an approximate k-NN algorithms
such as locality-sensitive hashing (?; Goyal et al,
2012).
Fortunately, since we use context words as cues
for relating their meaning and since the similar-
ity measures are defined based on these cues, the
number of neighbors we need to consider for each
node is reduced by several orders of magnitude.
We incorporate an inverted-index-style data struc-
ture which indicates what nodes are neighbors
based on each context word. Therefore, the set
of neighbors of a node consists of union of all the
neighbors bridged by each context word in the DP
of the node. However, the number of neighbors to
be considered for each node even after this dras-
tic reduction is still large (in order of a few thou-
sands).
In order to deal with the computational chal-
lenges of such a large graph, we take advantage of
the Hadoop?s MapReduce functionality to do both
graph construction and label propagation steps.
4 Experiments & Results
4.1 Experimental Setup
We experimented with two different domains for
the bilingual data: Europarl corpus (v7) (Koehn,
1109
Dataset Domain Sents TokensFr En
Bitext Europarl 10K 298K 268KEMEA 1M 16M 14M
Monotext Europarl 2M 60M ?
Dev-set WMT05 2K 67K 58K
Test-set WMT05 2K 66K 58K
Table 1: Statistics of training sets in different domains.
2005), and European Medicines Agency docu-
ments (EMEA) (Tiedemann, 2009) from French
to English. For the monolingual data, we used
French side of the Europarl corpus and we used
ACL/WMT 20058 data for dev/test sets. Table 1
summarizes statistics of the datasets used.
From the dev and test sets, we extract all source
words that do not appear in the phrase-table con-
structed from the parallel data. From the oovs, we
exclude numbers as well as named entities. We
apply a simple heuristic to detect named entities:
basically words that are capitalized in the original
dev/test set that do not appear at the beginning of
a sentence are named entities. Table 2 shows the
number of oov types and tokens for Europarl and
EMEA systems in both dev and test sets.
Dataset Dev Testtypes tokens types tokens
Europarl 1893 2229 1830 2163
EMEA 2325 4317 2294 4190
Table 2: number of oovs in dev and test sets for Europarl and
EMEA systems.
For the end-to-end MT pipeline, we used
Moses (Koehn et al, 2007) with these stan-
dard features: relative-frequency and lexical trans-
lation model (TM) probabilities in both direc-
tions; distortion model; language model (LM)
and word count. Word alignment is done using
GIZA++ (Och and Ney, 2003). We used distortion
limit of 6 and max-phrase-length of 10 in all the
experiments. For the language model, we used the
KenLM toolkit (Heafield, 2011) to create a 5-gram
language model on the target side of the Europarl
corpus (v7) with approximately 54M tokens with
Kneser-Ney smoothing.
4.1.1 Phrase-table Integration
Once the translations and their probabilities for
each oov are extracted, they are added to the
8http://www.statmt.org/wpt05/mt-shared-task/
phrase-table that is induced from the parallel text.
The probability for new entries are added as a
new feature in the log-linear framework to be
tuned along with other features. The value of
this newly introduced feature for original entries
in the phrase-table is set to 1. Similarly, the value
of original four probability features in the phrase-
table for the new entries are set to 1. The entire
training pipeline is as follows: (i) a phrase table is
constructed using parallel data as usual, (ii) oovs
for dev and test sets are extracted, (iii) oovs are
translated using graph propagation, (iv) oovs and
translations are added to the phrase table, intro-
ducing a new feature type, (v) the new phrase table
is tuned (with a LM) using MERT (Och, 2003) on
the dev set.
4.2 Evaluation
If we have a list of possible translations for oovs
with their probabilities, we become able to eval-
uate different methods we discussed. We word-
aligned the dev/test sets by concatenating them to
a large parallel corpus and running GIZA++ on
the whole set. The resulting word alignments are
used to extract the translations for each oov. The
correctness of this gold standard is limited to the
size of the parallel data used as well as the quality
of the word alignment software toolkit, and is not
100% precise. However, it gives a good estimate
of how each oov should be translated without the
need for human judgments.
For evaluating our baseline as well as graph-
based approaches, we use both intrinsic and
extrinsic evaluations. Two intrinsic evaluation
metrics that we use to evaluate the possible
translations for oovs are Mean Reciprocal Rank
(MRR) (Voorhees, 1999) and Recall. Intrinsic
evaluation metrics are faster to apply and are used
to optimize different hyper-parameters of the ap-
proach (e.g. window size, phrase length, etc.).
Once we come up with the optimized values for
the hyper-parameters, we extrinsically evaluate
different approaches by adding the new transla-
tions to the phrase-table and run it through the MT
pipeline.
4.2.1 MRR
MRR is an Information Retrieval metric used to
evaluate any process that produces a ranked list of
possible candidates. The reciprocal rank of a list
is the inverse of the rank of the correct answer in
the list. Such score is averaged over a set, oov set
1110
in our case, to get the mean-reciprocal-rank score.
MRR = 1|O|
|O|?
i=1
1
ranki
O = {oov}
In a few cases, there are multiple translations for
an oov word (i.e. appearing more than once in the
parallel corpus and being assigned to multiple dif-
ferent phrases), we take the average of reciprocal
ranks for each of them.
4.2.2 Recall
MRR takes the probabilities of oov translations
into account in sorting the list of candidate trans-
lations. However, in an MT pipeline, the language
model is supposed to rerank the hypotheses and
move more appropriate translations (in terms of
fluency) to the top of the list. Hence, we also
evaluate our candidate translation regardless of the
ranks. Since Moses uses a certain number of trans-
lations per source phrase (called the translation ta-
ble limit or ttl which we set to 20 in our experi-
ments) , we use the recall measure to evaluate the
top ttl translations in the list. Recall is another In-
formation Retrieval measure that is the fraction of
correct answers that are retrieved. For example, it
assigns score of 1 if the correct translation of the
oov word is in the top-k list and 0 otherwise. The
scores are averaged over all oovs to compute re-
call.
Recall = |{gold standard} ? {candidate list}||{gold standard}|
4.3 Intrinsic Results
In Section 2.2 and 2.3, different types of associa-
tion measures and similarity measures have been
explained to build and compare distributional pro-
files. Table 3 shows the results on Europarl when
using different similarity combinations. The mea-
sures are evaluated by fixing the window size to
4 and maximum candidate paraphrase length to 2
(e.g. bigram). First column shows the association
measures used to build DPs. As the results show,
the combination of PMI as association measure
and cosine as DP similarity measure outperforms
the other possible combinations. We use these two
measures throughout the rest of the experiments.
Figure 2 illustrates the effects of different win-
dow sizes and paraphrase lengths on MRR. As the
figure shows, the best MRR is reached when using
window size of 4 and trigram nodes. Going from
trigram to 4-gram results in a drop in MRR. One
Assoc cosine(%) L1norm(%) JSD(%)MRR RCL MRR RCL MRR RCL
CP 1.66 4.16 2.18 5.55 2.33 6.32
LLR 1.79 4.26 0.13 0.37 0.5 1.00
PMI 3.91 7.75 0.50 1.17 0.59 1.21
Chi 1.66 4.16 0.26 0.55 0.03 0.05
Table 3: Results of intrinsic evaluations (MRR and Recall)
on Europarl, window size 4 and paraphrase length 2
3.5	 ?
3.7	 ?
3.9	 ?
4.1	 ?
4.3	 ?
2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ?
MR
R	 ?(%
)	 ?
Window	 ?Size	 ?
unigram	 ? bigram	 ? trigram	 ? quadgram	 ?
Figure 2: Effects of different window sizes and paraphrase
length on the MRR of the dev set.
reason would be that distributional profiles for 4-
grams are very sparse and that negatively affects
the stability of similarity measures.
Figure 3 illustrates the effect of increasing the
size of monolingual text on both MRR and recall.
1? refers to the case of using 125k sentences for
the monolingual text and the 16? indicates using
the whole Europarl text on the source side (? 2M
sentences). As shown, there is a linear correla-
tion between the logarithm of the data size and
the MRR and recall ratios. Interestingly, MRR is
growing faster than recall by increasing the mono-
lingual text size, which means that the scoring
function gets better when more data is available.
The figure also indicates that a much bigger mono-
lingual text data can be used to further improve the
quality of the translations, however, at the expense
of more computational resources.
MRR	 ?Ra?o	 ?
Recall	 ?Ra?o	 ?
0 
1 
2 
3 
4 
5 
0 1x 2x 4x 8x 16x 
Mono-text Size Ratio 
Figure 3: Effect of increasing the monolingual text size on
MRR and Recall.
1111
Graph Neighbor MRR % RCL %
Bipartite 20 5.2 12.5
Tripartite 15+5 5.9 12.6
Full 20 5.1 10.9
Baseline 20 3.7 7.2
Table 4: Intrinsic results of different types of graphs when
using unigram nodes on Europarl.
Type Node MRR % RCL %
Bipartite unigram 5.2 12.5bigram 6.8 15.7
Tripartite unigram 5.9 12.6bigram 6.9 15.9
Baseline bigram 3.9 7.7
Table 5: Results on using unigram or bigram nodes.
4.3.1 Graph-based Results
Table 4 shows the intrinsic results on the Eu-
roparl corpus when using unigram nodes in each
of the graphs. The results are evaluated on the
dev-set based on the gold alignment created us-
ing GIZA++. Each node is connected to at most
20 other nodes (same as the max-paraphrase-limit
in the baseline). For the tripartite graph, each
node is connected to 15 labeled nodes and 5 un-
labeled ones. The tripartite graph gets a slight im-
provement over the bipartite one, however, the full
graph failed to have the same increase. One rea-
son is that allowing paths longer than 2 between
oov and labeled nodes causes more noise to prop-
agate into the graph. In other words, a paraphrase
of a paraphrase of a paraphrase is not necessarily
a useful paraphrase for an oov as the translation
may no longer be a valid one.
Table 5 also shows the effect of using bigrams
instead of unigrams as graph nodes. There is an
improvement by going from unigrams to bigrams
in both bipartite and tripartite graphs. We did not
use trigrams or larger n-grams in our experiments.
4.4 Extrinsic Results
The generated candidate translations for the oovs
can be added to the phrase-table created using
the parallel corpus to increase the coverage of the
phrase-table. This aggregated phrase-table is to be
tuned along with the language model on the dev
set, and run on the test set. BLEU (Papineni et
al., 2002) is still the de facto evaluation metric for
machine translation and we use that to measure
the quality of our proposed approaches for MT.
In these experiments, we do not use alignment in-
formation on dev or test sets unlike the previous
section.
Table 6 reports the Bleu scores for different do-
mains when the oov translations from the graph
propagation is added to the phrase-table and com-
pares them with the baseline system (i.e. Moses).
Results for our approach is based on unigram tri-
partite graphs and show that we improve over the
baseline in both the same-domain (Europarl) and
domain adaptation (EMEA) settings.
Table 7 shows some translations found by our
system for oov words.
oov gold standard candiate list
spe?cialement
undone
particularly
especially
special
particular
particularly
specific
only
particular
should
and
especially
assentiment approval
support
agreement
approval
accession
will approve
endorses
Table 7: Two examples of oov translations found by our
method.
5 Related work
There has been a long line of research on learning
translation pairs from non-parallel corpora (Rapp,
1995; Koehn and Knight, 2002; Haghighi et al,
2008; Garera et al, 2009; Marton et al, 2009;
Laws et al, 2010). Most have focused on ex-
tracting a translation lexicon by mining monolin-
gual resources of data to find clues, using prob-
abilistic methods to map words, or by exploit-
ing the cross-language evidence of closely related
languages. Most of them evaluated only high-
frequency words of specific types (nouns or con-
tent words) (Rapp, 1995; Koehn and Knight, 2002;
Haghighi et al, 2008; Garera et al, 2009; Laws et
al., 2010) In contrast, we do not consider any con-
straint on our test data and our data includes many
low frequency words. It has been shown that trans-
lation of high-frequency words is easier than low
frequency words (Tamura et al, 2012).
Some methods have used a third language(s)
as pivot or bridge to find translation pairs (Mann
and Yarowsky, 2001; Schafer and Yarowsky, 2002;
Callison-Burch et al, 2006).
1112
Corpus System MRR Recall Dev Bleu Test Bleu
Europarl Baseline ? ? 28.53 28.97Our approach 5.9 12.6 28.76 29.40*
EMEA Baseline ? ? 20.05 20.34Our approach 3.6 7.4 20.54 20.80*
* Statistically significant with p < 0.02 using the bootstrap resampling significance test (in Moses).
Table 6: Bleu scores for different domains with or without using oov translations.
Context similarity has been used effectively in
bilingual lexicon induction (Rapp, 1995; Koehn
and Knight, 2002; Haghighi et al, 2008; Gar-
era et al, 2009; Marton et al, 2009; Laws et al,
2010). It has been modeled in different ways: in
terms of adjacent words (Rapp, 1999; Fung and
Yee, 1998), or dependency relations (Garera et al,
2009). Laws et al (2010) used linguistic analy-
sis in the form of graph-based models instead of a
vector space. But all of these researches used an
available seed lexicon as the basic source of simi-
larity between source and target languages unlike
our method which just needs a monolingual cor-
pus of source language which is freely available
for many languages and a small bilingual corpora.
Some methods tried to alleviate the lack of seed
lexicon by using orthographic similarity to extract
a seed lexicon (Koehn and Knight, 2002; Fiser and
Ljubesic, 2011). But it is not a practical solution
in case of unrelated languages.
Haghighi et al (2008) and Daume? and Jagarla-
mudi (2011) proposed generative models based on
canonical correlation analysis to extract transla-
tion lexicons for non-parallel corpora by learning a
matching between source and target lexicons. Us-
ing monolingual features to represent words, fea-
ture vectors are projected from source and target
words into a canonical space to find the appropri-
ate matching between them. Their method relies
on context features which need a seed lexicon and
orthographic features which only works for phylo-
genetically related languages.
Graph-based semi-supervised methods have
been shown to be useful for domain adaptation in
MT as well. Alexandrescu and Kirchhoff (2009)
applied a graph-based method to determine simi-
larities between sentences and use these similari-
ties to promote similar translations for similar sen-
tences. They used a graph-based semi-supervised
model to re-rank the n-best translation hypothe-
sis. Liu et al (2012) extended Alexandrescu?s
model to use translation consensus among simi-
lar sentences in bilingual training data by devel-
oping a new structured label propagation method.
They derived some features to use during decoding
process that has been shown useful in improving
translation quality. Our graph propagation method
connects monolingual source phrases with oovs to
obtain translation and so is a very different use of
graph propagation from these previous works.
Recently label propagation has been used for
lexicon induction (Tamura et al, 2012). They used
a graph based on context similarity as well as co-
occurrence graph in propagation process. Similar
to our approach they used unlabeled nodes in la-
bel propagation process. However, they use a seed
lexicon to define labels and comparable corpora to
construct graphs unlike our approach.
6 Conclusion
We presented a novel approach for inducing oov
translations from a monolingual corpus on the
source side and a parallel data using graph prop-
agation. Our results showed improvement over
the baselines both in intrinsic evaluations and on
BLEU. Future work includes studying the effect
of size of parallel corpus on the induced oov trans-
lations. Increasing the size of parallel corpus on
one hand reduces the number of oovs. But, on
the other hand, there will be more labeled para-
phrases that increases the chance of finding the
correct translation for oovs in the test set.
Currently, we find paraphrases for oov words.
However, oovs can be considered as n-grams
(phrases) instead of unigrams. In this scenario,
we also can look for paraphrases and translations
for phrases containing oovs and add them to the
phrase-table as new translations along with the
translations for unigram oovs.
We also plan to explore different graph propa-
gation objective functions. Regularizing these ob-
jective functions appropriately might let us scale
to much larger data sets with an order of magni-
tude more nodes in the graph.
1113
References
Andrei Alexandrescu and Katrin Kirchhoff. 2009.
Graph-based learning for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, NAACL ?09, pages 119?127,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 17?24. Association for
Computational Linguistics.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Mach. Learn., 34(1-3):43?69,
February.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 407?412, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Comput. Linguist.,
19(1):61?74, March.
Darja Fiser and Nikola Ljubesic. 2011. Bilingual lexi-
con extraction from comparable corpora for closely
related languages. In RANLP, pages 125?131.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 1, ACL ?98, pages 414?
420. Association for Computational Linguistics.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning, CoNLL ?09,
pages 129?137, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Amit Goyal, Hal Daume III, and Raul Guerra. 2012.
Fast Large-Scale Approximate Graph Construction
for NLP. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?12.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 57?60. Association for
Computational Linguistics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771?779.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197.
Chung-Chi Huang, Ho-Ching Yen, Ping-Che Yang,
Shih-Ting Huang, and Jason S Chang. 2011. Us-
ing sublexical translations to handle the oov prob-
lem in machine translation. ACM Transactions on
Asian Language Information Processing (TALIP),
10(3):16.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 workshop on Unsuper-
vised lexical acquisition - Volume 9, ULA ?02, pages
9?16, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Florian Laws, Lukas Michelbacher, Beate Dorow,
Christian Scheible, Ulrich Heid, and Hinrich
Schu?tze. 2010. A linguistically grounded graph
model for bilingual lexicon extraction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics: Posters, COLING ?10, pages
614?622, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98, pages
768?774, Stroudsburg, PA, USA. Association for
Computational Linguistics.
1114
Shujie Liu, Chi-Ho Li, Mu Li, and Ming Zhou. 2012.
Learning translation consensus with structured la-
bel propagation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers - Volume 1, ACL ?12, pages
302?310, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Gideon S. Mann and David Yarowsky. 2001. Mul-
tipath translation lexicon induction via bridge lan-
guages. In Proceedings of the second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language technolo-
gies, NAACL ?01, pages 1?8, Stroudsburg, PA,
USA.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, pages 381?390, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings of
the 41th Annual Meeting of the ACL, Sapporo, July.
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Delip Rao and David Yarowsky. 2009. Ranking
and semi-supervised classification on large scale
graphs using map-reduce. In Proceedings of the
2009 Workshop on Graph-based Methods for Nat-
ural Language Processing, TextGraphs-4. Associa-
tion for Computational Linguistics.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, ACL ?95, pages 320?322. Association for
Computational Linguistics.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, ACL ?99, pages 519?
526. Association for Computational Linguistics.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity mea-
sures and bridge languages. In proceedings of the
6th conference on Natural language learning - Vol-
ume 20, COLING-02, pages 1?7, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hinrich Schu?tze and Jan O. Pedersen. 1997. A
cooccurrence-based thesaurus and two applications
to information retrieval. Inf. Process. Manage.,
33(3):307?318, May.
Partha Pratim Talukdar and Koby Crammer. 2009.
New Regularized Algorithms for Transductive
Learning. In European Conference on Machine
Learning (ECML-PKDD).
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pas?ca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In EMNLP-
CoNLL, pages 24?36.
Egidio L. Terra and Charles L. A. Clarke. 2003. Fre-
quency estimates for statistical word similarity mea-
sures. In HLT-NAACL.
Jorg Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natu-
ral Language Processing, volume V, pages 237?248.
John Benjamins, Amsterdam/Philadelphia.
Ellen M. Voorhees. 1999. TREC-8 Question Answer-
ing Track Report. In Proceedings of the 8th Text
Retrieval Conference, pages 77?82.
Jiajun Zhang, Feifei Zhai, and Chengqing Zong. 2012.
Handling unknown words in statistical machine
translation from a new perspective. In Natural Lan-
guage Processing and Chinese Computing, pages
176?187. Springer.
1115
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 334?339,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Stacking for Statistical Machine Translation?
Majid Razmara and Anoop Sarkar
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
{razmara,anoop}@sfu.ca
Abstract
We propose the use of stacking, an ensem-
ble learning technique, to the statistical machine
translation (SMT) models. A diverse ensem-
ble of weak learners is created using the same
SMT engine (a hierarchical phrase-based sys-
tem) by manipulating the training data and a
strong model is created by combining the weak
models on-the-fly. Experimental results on two
language pairs and three different sizes of train-
ing data show significant improvements of up
to 4 BLEU points over a conventionally trained
SMT model.
1 Introduction
Ensemble-based methods have been widely used
in machine learning with the aim of reduc-
ing the instability of classifiers and regressors
and/or increase their bias. The idea behind
ensemble learning is to combine multiple mod-
els, weak learners, in an attempt to produce a
strong model with less error. It has also been
successfully applied to a wide variety of tasks in
NLP (Tomeh et al, 2010; Surdeanu and Man-
ning, 2010; F. T. Martins et al, 2008; Sang, 2002)
and recently has attracted attention in the statis-
tical machine translation community in various
work (Xiao et al, 2013; Song et al, 2011; Xiao
et al, 2010; Lagarda and Casacuberta, 2008).
In this paper, we propose a method to adopt
stacking (Wolpert, 1992), an ensemble learning
technique, to SMT. We manipulate the full set of
training data, creating k disjoint sets of held-out
and held-in data sets as in k-fold cross-validation
and build a model on each partition. This creates
a diverse ensemble of statistical machine transla-
tion models where each member of the ensemble
has different feature function values for the SMT
log-linear model (Koehn, 2010). The weights of
model are then tuned using minimum error rate
training (Och, 2003) on the held-out fold to pro-
vide k weak models. We then create a strong
?This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Faculty Award
to the second author.
model by stacking another meta-learner on top of
weak models to combine them into a single model.
The particular second-tier model we use is a model
combination approach called ensemble decoding
which combines hypotheses from the weak mod-
els on-the-fly in the decoder.
Using this approach, we take advantage of the
diversity created by manipulating the training data
and obtain a significant and consistent improve-
ment over a conventionally trained SMT model
with a fixed training and tuning set.
2 Ensemble Learning Methods
Two well-known instances of general framework
of ensemble learning are bagging and boosting.
Bagging (Breiman, 1996a) (bootstrap aggregat-
ing) takes a number of samples with replacement
from a training set. The generated sample set
may have 0, 1 or more instances of each origi-
nal training instance. This procedure is repeated
a number of times and the base learner is ap-
plied to each sample to produce a weak learner.
These models are aggregated by doing a uniform
voting for classification or averaging the predic-
tions for regression. Bagging reduces the vari-
ance of the base model while leaving the bias rela-
tively unchanged and is most useful when a small
change in the training data affects the prediction
of the model (i.e. the model is unstable) (Breiman,
1996a). Bagging has been recently applied to
SMT (Xiao et al, 2013; Song et al, 2011)
Boosting (Schapire, 1990) constructs a strong
learner by repeatedly choosing a weak learner
and applying it on a re-weighted training set. In
each iteration, a weak model is learned on the
training data, whose instance weights are modi-
fied from the previous iteration to concentrate on
examples on which the model predictions were
poor. By putting more weight on the wrongly
predicted examples, a diverse ensemble of weak
learners is created. Boosting has also been used in
SMT (Xiao et al, 2013; Xiao et al, 2010; Lagarda
334
Algorithm 1: Stacking for SMT
Input: D = {?fj , ej?}Nj=1 . A parallel corpusInput: k . # of folds (i.e. weak learners)
Output: STRONGMODEL s
1: D1, . . . ,Dk ? SPLIT(D, k)
2: for i = 1? k do
3: T i ? D ?Di . Use all but current partition as
training set.
4: ?i? TRAIN(T i) . Train feature functions.
5: Mi? TUNE(?i, Di) . Tune the model on thecurrent partition.
6: end for
7: s? COMBINEMODELS(M1 , . . .,Mk) . Combine all
the base models to produce a strong stacked model.
and Casacuberta, 2008).
Stacking (or stacked generalization) (Wolpert,
1992) is another ensemble learning algorithm that
uses a second-level learning algorithm on top of
the base learners to reduce the bias. The first
level consists of predictors g1, . . . , gk where gi :
Rd ? R, receiving input x ? Rd and produc-
ing a prediction gi(x). The next level consists
of a single function h : Rd+k ? R that takes
?x, g1(x), . . . , gk(x)? as input and produces an en-
semble prediction y? = h(x, g1(x), . . . , gk(x)).
Two categories of ensemble learning are ho-
mogeneous learning and heterogeneous learning.
In homogeneous learning, a single base learner
is used, and diversity is generated by data sam-
pling, feature sampling, randomization and pa-
rameter settings, among other strategies. In het-
erogeneous learning different learning algorithms
are applied to the same training data to create a
pool of diverse models. In this paper, we focus on
homogeneous ensemble learning by manipulating
the training data.
In the primary form of stacking (Wolpert,
1992), the training data is split into multiple dis-
joint sets of held-out and held-in data sets using
k-fold cross-validation and k models are trained
on the held-in partitions and run on held-out par-
titions. Then a meta-learner uses the predictions
of all models on their held-out sets and the actual
labels to learn a final model. The details of the
first-layer and second-layer predictors are consid-
ered to be a ?black art? (Wolpert, 1992).
Breiman (1996b) linearly combines the weak
learners in the stacking framework. The weights
of the base learners are learned using ridge regres-
sion: s(x) = ?k ?kmk(x), where mk is a base
model trained on the k-th partition of the data and
s is the resulting strong model created by linearly
interpolating the weak learners.
Stacking (aka blending) has been used in the
system that won the Netflix Prize1, which used a
multi-level stacking algorithm.
Stacking has been actively used in statistical
parsing: Nivre and McDonald (2008) integrated
two models for dependency parsing by letting one
model learn from features generated by the other;
F. T. Martins et al (2008) further formalized the
stacking algorithm and improved on Nivre and
McDonald (2008); Surdeanu and Manning (2010)
includes a detailed analysis of ensemble models
for statistical parsing: i) the diversity of base
parsers is more important than the complexity of
the models; ii) unweighted voting performs as well
as weighted voting; and iii) ensemble models that
combine at decoding time significantly outperform
models that combine multiple models at training
time.
3 Our Approach
In this paper, we propose a method to apply stack-
ing to statistical machine translation (SMT) and
our method is the first to successfully exploit
stacking for statistical machine translation. We
use a standard statistical machine translation en-
gine and produce multiple diverse models by par-
titioning the training set using the k-fold cross-
validation technique. A diverse ensemble of weak
systems is created by learning a model on each
k?1 fold and tuning the statistical machine trans-
lation log-linear weights on the remaining fold.
However, instead of learning a model on the output
of base models as in (Wolpert, 1992), we combine
hypotheses from the base models in the decoder
with uniform weights. For the base learner, we
use Kriya (Sankaran et al, 2012), an in-house hier-
archical phrase-based machine translation system,
to produce multiple weak models. These mod-
els are combined together using Ensemble Decod-
ing (Razmara et al, 2012) to produce a strong
model in the decoder. This method is briefly ex-
plained in next section.
3.1 Ensemble Decoding
SMT Log-linear models (Koehn, 2010) find the
most likely target language output e given the
source language input f using a vector of feature
functions ?:
p(e|f) ? exp
(
w ? ?
)
1http://www.netflixprize.com/
335
Ensemble decoding combines several models
dynamically at decoding time. The scores are
combined for each partial hypothesis using a
user-defined mixture operation ? over component
models.
p(e|f) ? exp
(
w1 ? ?1 ?w2 ? ?2 ? . . .
)
We previously successfully applied ensemble
decoding to domain adaptation in SMT and
showed that it performed better than approaches
that pre-compute linear mixtures of different mod-
els (Razmara et al, 2012). Several mixture oper-
ations were proposed, allowing the user to encode
belief about the relative strengths of the compo-
nent models. These mixture operations receive
two or more probabilities and return the mixture
probability p(e? | f?) for each rule e?, f? used in the
decoder. Different options for these operations
are:
? Weighted Sum (wsum) is defined as:
p(e? | f?) ?
M?
m
?m exp
(
wm ? ?m
)
where m denotes the index of component
models, M is the total number of them and
?m is the weight for component m.
? Weighted Max (wmax) is defined as:
p(e? | f?) ? max
m
(
?m exp
(
wm ? ?m
))
? Prod or log-wsum is defined as:
p(e? | f?) ? exp
( M?
m
?m (wm ? ?m)
)
? Model Switching (Switch): Each cell in the
CKY chart is populated only by rules from
one of the models and the other models? rules
are discarded. Each component model is con-
sidered as an expert on different spans of the
source. A binary indicator function ?(f? ,m)
picks a component model for each span:
?(f? ,m) =
?
?
?
1, m = argmax
n?M
?(f? , n)
0, otherwise
The criteria for choosing a model for each
cell, ?(f? , n), could be based on max
Train size Src tokens Tgt tokens
Fr - En
0+dev 67K 58K
10k+dev 365K 327K
100k+dev 3M 2.8M
Es - En
0+dev 60K 58K
10k+dev 341K 326K
100k+dev 2.9M 2.8M
Table 1: Statistics of the training set for different systems and
different language pairs.
(SW:MAX), i.e. for each cell, the model that
has the highest weighted score wins:
?(f? , n) = ?n maxe (wn ? ?n(e?, f?))
Alternatively, we can pick the model with
highest weighted sum of the probabilities of
the rules (SW:SUM). This sum has to take into
account the translation table limit (ttl), on the
number of rules suggested by each model for
each cell:
?(f? , n) = ?n
?
e?
exp
(
wn ? ?n(e?, f?)
)
The probability of each phrase-pair (e?, f?) is
then:
p(e? | f?) =
M?
m
?(f? ,m) pm(e? | f?)
4 Experiments & Results
We experimented with two language pairs: French
to English and Spanish to English on the Europarl
corpus (v7) (Koehn, 2005) and used ACL/WMT
2005 2 data for dev and test sets.
For the base models, we used an in-house
implementation of hierarchical phrase-based sys-
tems, Kriya (Sankaran et al, 2012), which uses
the same features mentioned in (Chiang, 2005):
forward and backward relative-frequency and lex-
ical TM probabilities; LM; word, phrase and glue-
rules penalty. GIZA++ (Och and Ney, 2003) has
been used for word alignment with phrase length
limit of 10. Feature weights were optimized using
MERT (Och, 2003). We built a 5-gram language
model on the English side of Europarl and used the
Kneser-Ney smoothing method and SRILM (Stol-
cke, 2002) as the language model toolkit.
2http://www.statmt.org/wpt05/mt-shared-task/
336
Direction k-fold Resub Mean WSUM WMAX PROD SW:MAX SW:SUM
Fr - En
2 18.08 19.67 22.32 22.48 22.06 21.70 21.81
4 18.08 21.80 23.14 23.48 23.55 22.83 22.95
8 18.08 22.47 23.76 23.75 23.78 23.02 23.47
Es - En
2 18.61 19.23 21.62 21.33 21.49 21.48 21.51
4 18.61 21.52 23.42 22.81 22.91 22.81 22.92
8 18.61 22.20 23.69 23.89 23.51 22.92 23.26
Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
Direction Corpus k-fold Baseline BMA WSUM WMAX PROD SW:MAX SW:SUM
Fr - En 10k+dev 6 28.75 29.49 29.87 29.78 29.21 29.69 29.59100k+dev 11 / 51 29.53 29.75 34.00 34.07 33.11 34.05 33.96
Es - En 10k+dev 6 28.21 28.76 29.59 29.51 29.15 29.10 29.21100k+dev 11 / 51 33.25 33.44 34.21 34.00 33.17 34.19 34.22
Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
4.1 Training on devset
We first consider the scenario in which there is
no parallel data between a language pair except
a small bi-text used as a devset. We use no spe-
cific training data and construct a SMT system
completely on the devset by using our approach
and compare to two different baselines. A natu-
ral baseline when having a limited parallel text is
to do re-substitution validation where the model
is trained on the whole devset and is tuned on the
same set. This validation process suffers seriously
from over-fitting. The second baseline is the mean
of BLEU scores of all base models.
Table 2 summarizes the BLEU scores on the
testset when using stacking only on the devset on
two different language pairs. As the table shows,
increasing the number of folds results in higher
BLEU scores. However, doing such will generally
lead to higher variance among base learners.
Figure 1 shows the BLEU score of each of the
base models resulted from a 20-fold partitioning
of the devset alng with the strong models? BLEU
scores. As the figure shows, the strong models are
generally superior to the base models whose mean
is represented as a horizontal line.
4.2 Training on train+dev
When we have some training data, we can use
the cross-validation-style partitioning to create k
splits. We then train a system on k ? 1 folds and
tune on the devset. However, each system eventu-
ally wastes a fold of the training data. In order to
take advantage of that remaining fold, we concate-
nate the devset to the training set and partition the
whole union. In this way, we use all data available
to us. We experimented with two sizes of train-
ing data: 10k sentence pairs and 100k, that with
the addition of the devset, we have 12k and 102k
sentence-pair corpora.
Table 1 summarizes statistics of the data sets
used in this scenario. Table 3 reports the BLEU
scores when using stacking on these two corpus
sizes. The baselines are the conventional systems
which are built on the training-set only and tuned
on the devset as well as Bayesian Model Averaging
(BMA, see ?5). For the 100k+dev corpus, we sam-
pled 11 partitions from all 51 possible partitions
by taking every fifth partition as training data. The
results in Table 3 show that stacking can improve
over the baseline BLEU scores by up to 4 points.
Examining the performance of the different
mixture operations, we can see that WSUM and
WMAX typically outperform other mixture oper-
ations. Different mixture operations can be domi-
nant in different language pairs and different sizes
of training sets.
5 Related Work
Xiao et al (2013) have applied both boosting
and bagging on three different statistical machine
translation engines: phrase-based (Koehn et al,
2003), hierarchical phrase-based (Chiang, 2005)
and syntax-based (Galley et al, 2006) and showed
SMT can benefit from these methods as well.
Duan et al (2009) creates an ensemble of mod-
els by using feature subspace method in the ma-
chine learning literature (Ho, 1998). Each mem-
ber of the ensemble is built by removing one non-
LM feature in the log-linear framework or varying
the order of language model. Finally they use a
sentence-level system combination on the outputs
of the base models to pick the best system for each
337
ws
um
wm
ax
sw
:m
ax
sw
:su
m
prod

testset BLEU
Mo
dels
 
Mea
n
Bas
e M
ode
ls
Stro
ng M
ode
ls
Figure 1: BLEU scores for all the base models and stacked models on the Fr-En devset with 20-fold cross validation. The
horizontal line shows the mean of base models? scores.
sentence. Though, they do not combine the hy-
potheses search spaces of individual base models.
Our work is most similar to that of Duan et
al. (2010) which uses Bayesian model averaging
(BMA) (Hoeting et al, 1999) for SMT. They used
sampling without replacement to create a num-
ber of base models whose phrase-tables are com-
bined with that of the baseline (trained on the full
training-set) using linear mixture models (Foster
and Kuhn, 2007).
Our approach differs from this approach in a
number of ways: i) we use cross-validation-style
partitioning for creating training subsets while
they do sampling without replacement (80% of the
training set); ii) in our approach a number of base
models are trained and tuned and they are com-
bined on-the-fly in the decoder using ensemble de-
coding which has been shown to be more effective
than offline combination of phrase-table-only fea-
tures; iii) in Duan et al (2010)?s method, each sys-
tem gives up 20% of the training data in exchange
for more diversity, but in contrast, our method not
only uses all available data for training, but pro-
motes diversity through allowing each model to
tune on a different data set; iv) our approach takes
advantage of held out data (the tuning set) in the
training of base models which is beneficial espe-
cially when little parallel data is available or tun-
ing/test sets and training sets are from different do-
mains.
Empirical results (Table 3) also show that our
approach outperforms the Bayesian model averag-
ing approach (BMA).
6 Conclusion & Future Work
In this paper, we proposed a novel method on ap-
plying stacking to the statistical machine transla-
tion task. The results when using no, 10k and 100k
sentence-pair training sets (along with a develop-
ment set for tuning) show that stacking can yield
an improvement of up to 4 BLEU points over con-
ventionally trained SMT models which use a fixed
training and tuning set.
Future work includes experimenting with larger
training sets to investigate how useful this ap-
proach can be when having different sizes of train-
ing data.
References
Leo Breiman. 1996a. Bagging predictors. Machine
Learning, 24(2):123?140, August.
Leo Breiman. 1996b. Stacked regressions. Machine
Learning, 24(1):49?64, July.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
?05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 263?
270, Morristown, NJ, USA. ACL.
Nan Duan, Mu Li, Tong Xiao, and Ming Zhou. 2009.
The feature subspace method for smt system combi-
nation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ?09, pages 1096?
1104, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Nan Duan, Hong Sun, and Ming Zhou. 2010. Transla-
tion model generalization using probability averag-
ing for machine translation. In Proceedings of the
338
23rd International Conference on Computational
Linguistics, COLING ?10, pages 304?312, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
157?166, Honolulu, Hawaii, October. Association
for Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ?07, pages 128?135, Stroudsburg, PA, USA.
ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 961?968, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tin Kam Ho. 1998. The random subspace method for
constructing decision forests. IEEE Trans. Pattern
Anal. Mach. Intell., 20(8):832?844, August.
Jennifer A. Hoeting, David Madigan, Adrian E.
Raftery, and Chris T. Volinsky. 1999. Bayesian
Model Averaging: A Tutorial. Statistical Science,
14(4):382?401.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, pages 127?133, Edmonton,
May. NAACL.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Antonio Lagarda and Francisco Casacuberta. 2008.
Applying boosting to statistical machine translation.
In Annual Meeting of European Association for Ma-
chine Translation (EAMT), pages 88?96.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings of
the 41th Annual Meeting of the ACL, Sapporo, July.
ACL.
Majid Razmara, George Foster, Baskaran Sankaran,
and Anoop Sarkar. 2012. Mixing multiple transla-
tion models in statistical machine translation. In The
50th Annual Meeting of the Association for Compu-
tational Linguistics, Proceedings of the Conference,
July 8-14, 2012, Jeju Island, Korea - Volume 1: Long
Papers, pages 940?949. The Association for Com-
puter Linguistics.
Erik F. Tjong Kim Sang. 2002. Memory-based shal-
low parsing. J. Mach. Learn. Res., 2:559?594,
March.
Baskaran Sankaran, Majid Razmara, and Anoop
Sarkar. 2012. Kriya an end-to-end hierarchical
phrase-based mt system. The Prague Bulletin of
Mathematical Linguistics, 97(97), April.
Robert E. Schapire. 1990. The strength of weak learn-
ability. Mach. Learn., 5(2):197?227, July.
Linfeng Song, Haitao Mi, Yajuan Lu?, and Qun Liu.
2011. Bagging-based system combination for do-
main adaption. In Proceedings of the 13th Machine
Translation Summit (MT Summit XIII), pages 293?
299. International Association for Machine Transla-
tion, September.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing,
pages 257?286.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 649?652, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nadi Tomeh, Alexandre Allauzen, Guillaume Wis-
niewski, and Franc?ois Yvon. 2010. Refining word
alignment with discriminative training. In Proceed-
ings of The Ninth Conference of the Association for
Machine Translation in the Americas (AMTA 2010).
David H. Wolpert. 1992. Stacked generalization. Neu-
ral Networks, 5:241?259.
Tong Xiao, Jingbo Zhu, Muhua Zhu, and Huizhen
Wang. 2010. Boosting-based system combina-
tion for machine translation. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 739?748,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Tong Xiao, Jingbo Zhu, and Tongran Liu. 2013. Bag-
ging and boosting statistical machine translation sys-
tems. Artificial Intelligence, 195:496?527, Febru-
ary.
339
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 216?223,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Incremental Decoding for Phrase-based Statistical Machine Translation
Baskaran Sankaran, Ajeet Grewal and Anoop Sarkar
School of Computing Science
Simon Fraser University
8888 University Drive
Burnaby BC. V5A 2Y1. Canada
{baskaran, asg10, anoop}@cs.sfu.ca
Abstract
In this paper we focus on the incremental
decoding for a statistical phrase-based ma-
chine translation system. In incremental
decoding, translations are generated incre-
mentally for every word typed by a user,
instead of waiting for the entire sentence
as input. We introduce a novel modifi-
cation to the beam-search decoding algo-
rithm for phrase-based MT to address this
issue, aimed at efficient computation of fu-
ture costs and avoiding search errors. Our
objective is to do a faster translation dur-
ing incremental decoding without signifi-
cant reduction in the translation quality.
1 Introduction
Statistical Machine Translation has matured sig-
nificantly in the past decade and half, resulting in
the proliferation of several web-based and com-
mercial translation services. Most of these ser-
vices work on sentence or document level, where
a user enters a sentence or chooses a document
for translation, which are then translated by the
servers. Translation in such typical scenarios is
still offline in the sense that the user input and
translation happen sequentially without any inter-
action between the two phases.
In this paper we study decoding for SMT with
the constraint that translations are to be gener-
ated incrementally for every word typed in by the
user. Such a translation service can be used for
language learning, where the user is fluent in the
target language and experiments with many differ-
ent source language sentences interactively, or in
real-time translation environments such as speech-
speech translation or translation during interactive
chats.
We use a phrase-based decoder similar to
Moses (Koehn et al, 2007) and propose novel
modifications in the decoding algorithm to tackle
incremental decoding. Our system maintains a
partial decoder state at every stage and uses it
while decoding for each newly added word. As
the decoder has access only to the partial sentence
at every stage, the future costs change with ev-
ery additional word and this has to be taken into
account while continuing from an existing partial
decoder state. Another major issue is that as incre-
mental decoding is provided new input one word
at at time, some of the entries that were pruned out
at an earlier decoder state might later turn out to
better candidates resulting in search errors com-
pared to decoding the entire sentence at once. It
is to be noted that, the search error problem is re-
lated to the inability to compute full future cost
in incremental decoding. Our proposed modifica-
tions address these twin challenges and allow for
efficient incremental decoding.
2 Incremental Decoding
2.1 Beam Search for Phrase-based SMT
In this section we review the usual beam search de-
coder for phrase-based MT because we present our
modifications for incremental decoding using the
same notation. Beam search decoding for phrase-
based SMT (Koehn, 2004) begins by collecting
the translation options from the phrase table for all
possible phrases of a given input sentence and pre-
computes the future cost for all possible contigu-
ous sequences in the sentence. The pseudo-code
for the usual beam-search decoding algorithm is
illustrated in Algorithm 1.
The decoder creates n bins for storing hypothe-
ses grouped by the number of source words cov-
ered. Starting from a null hypothesis in bin 0, the
decoder iterates through bins 1 though n filling
them with new hypotheses by extending the en-
tries in the earlier bins.
A hypothesis contains the target words gener-
ated (e), the source positions translated so far (f )
commonly known as coverage set and the score
of the current translation (p) computed by the
weighted log-linear combination of different fea-
ture functions. It also contains a back-pointer to
216
Algorithm 1 Phrase-based Decoder pseudocode
(Koehn, 2004)
1: Given: sentence Sn: s1s2...sn of length n
2: Pre-compute future costs for all contiguous
sequences
3: Initialize bins bi where i = 1 . . . n
4: Create initial hypothesis: {e : (), f : (), p :
1.0}
5: for i = 1 to n do
6: for hyp ? bi do
7: for newHyp that extends hyp do
8: nf := num src words covered by
newHyp
9: Add newHyp to bin bnf
10: Prune bin bnf using future costs
11: Find best hypothesis in bn
12: Output best path that leads to best hypothesis
its parent hypothesis in the previous state and other
information used for pruning and computing cost
in later iterations.
As a new hypothesis is generated by extending
an existing hypothesis with a new phrase pair, de-
coder updates the associated information such as
coverage set, the target words generated, future
cost (for translating rest of the source words) and
its translation score. For example, consider Span-
ish to English translation: for the source sentence
Maria no daba una bofetada, the hypothesis {e :
(Mary), f : (1), p : 0.534} which is the hypoth-
esis that covers Maria can be extended to a new
hypothesis {e : (Mary, slap), f : (1, 3, 4, 5), p :
0.043} by choosing a new phrase pair (daba una
bofetada, slap) covering the source phrases Maria
and daba una bofetada. The probability score is
obtained by weighted log-linear sum of the fea-
tures of the phrases contained in the derivation so
far.
An important aspect of beam search decoding
is the pruning away of low-scoring hypotheses in
each bin to reduce the search space and thus mak-
ing the decoding faster. To do this effectively,
beam search decoding uses the future cost of a hy-
pothesis together with its current cost. The future
cost is an estimate of the translation cost of the
input words that are yet to be translated, and is
typically pre-computed for all possible contiguous
sequences in the input sentence before the decod-
ing step. The future cost prevents the any hypothe-
ses that are low-scoring, but potentially promising,
from being pruned.
2.2 Incremental Decoder - Challenges
Our goal for the incremental decoder (ID) is to
generate output translations incrementally for par-
tial phrases as the source sentence is being input
by the user. We assume white-space to be the word
delimiter and the partial sentence is decoded for
every encounter of the space character. We further
assume the return key to mark end-of-sentence
(EOS) and use it to compute language model score
for the entire sentence.
As we noted above, future costs cannot be pre-
computed as in regular decoding because the com-
plete input sentence is not known while decod-
ing incrementally. Thus the incremental decoder
can only use a partial future cost until the EOS
is reached. The partial future cost could result
in some of the potentially better candidates being
pruned away in earlier stages. This leads to search
errors and result in lower translation quality.
2.3 Approach
We use a modified beam search for incremental
decoding (ID) and the two key modifications are
aimed at addressing the issues of future cost and
search errors. Beam search for ID begins with
a single bin for the first word and more bins are
added as the sentence is completed by the user.
Our approach requires that the decoder states for
the partial source sentence can be stored in a way
that allows efficient retrieval. It also maintains a
current decoder state, which includes all the bins
and the hypotheses contained in them, all pertain-
ing to the present sentence.
At each step ID goes through a pre-process
phase, where it recomputes the partial future costs
for all the spans accounting for the new word and
updates the current decoder state with new partial
future costs. It then generates new hypotheses into
all the earlier bins and in the newly created us-
ing any new phrases (resulting from the new word
added by the user) not used earlier.
Algorithm 2 shows the pseudocode of our incre-
mental decoder. Given a partial sentence Si1 ID
starts with the pre-process phase illustrated sepa-
rately in algorithm 3. We use Ptype(l) to denote
phrases of length l words and Htype to denote the
set of hypotheses; in both cases type correspond to
either old or new, indicating if it was not known in
the previous decoding state or not.
1we use Si and si to denote a i word partial sentence and
ith word in a (partial) sentence respectively
217
Algorithm 2 Incremental Decoder pseudocode
1: Input: (partial) sentence Sp: s1s2...si?1si
with ls words where si is the new word
2: PreProcess(Sp) (Algorithm 3)
3: for every bin bj in (1 . . . i) do
4: Update future cost and cover set ? Hold
5: Add any new phrase of length bj (subject to
d)
6: for bin bk in (bj?MaxPhrLen . . . bj?1) do
7: Generate Hnew for bj by extending:
8: every Hold with every other Pnew(bj ?
bk)
9: every Hnew with every other Pany(bj ?
bk)
10: Prune bin bj
Algorithm 3 PreProcess subroutine
1: Input: partial sentence Sp of length ls
2: Retrieve partial decoder object for Sp?1
3: Identify possible Pnew (subject to Max-
PhrLen)
4: Recompute fc for all spans in 1...ls
5: for every Pnew in local phrase table do
6: Load translation options to table
7: for every Pold in local phrase table do
8: Update fc with the recomputed cost
Given Si, the pre-process phase extracts the new
set of phrases (Pnew) for the ith word and adds
them to the existing phrases (Pold). It then recom-
putes the future-cost (fc) for all the contiguous se-
quences in the partial input and updates existing
entries in the local copy of phrase table with new
fc.
In decoding phase, ID generates new hypothe-
ses in two ways: i) by extending the existing hy-
potheses Hold in the previous decoder state Si?1
with new phrases Pnew and ii) by generating new
hypotheses Hnew that are unknown in the previous
state.
The main difference between incremental de-
coding and regular beam-search decoding is inside
the two ?for? loops corresponding to lines 3? 9 in
algorithm 2. In the outer loop each of the existing
hypotheses are updated to reflect the recomputed
fc and coverage set. Any new phrases belonging
to the current bin are also added to it2.
2Based on our implementation of lazier cube pruning they
are added to a priority queue, the contents of which are
flushed into the bin at the end of inner for-loop and before
the pruning step
Hypothesis surfaces
P. Queue
Hypstack
hyp 2hyp 1
A single surface
     
     
     
     
     





Figure 1: Illustration of Lazier Cube Pruning
The inner for-loop corresponds to the extension
of hypotheses sets (grouped by same coverage set)
to generate new hypotheses. Here a distinction is
made between hypotheses Hold corresponding to
previous decoder state Sp?1 and hypotheses Hnew
resulting from the addition of word si. Hold is ex-
tended only using the newly found phrases Pnew,
whereas the newer hypotheses are processed as in
regular beam-search.
2.4 Lazier Cube Pruning
We have adapted the pervasive lazy algorithm
(or ?lazier cube pruning?) proposed originally for
Hiero-style systems by (Pust and Knight, 2009)
for our phrase-based system. This step corre-
sponds to the lines 5?9 of algorithm 2 and allows
us to only generate as many hypotheses as speci-
fied by the configurable parameters, beam size and
beam threshold. Figure 1 illustrates the process of
lazier cube pruning for a single bin.
At the highest level it uses a priority queue,
which is populated by the different hyper-edges
or surfaces3, each corresponding to a pair of hy-
potheses that are being merged to create a new
hypothesis. New hypotheses are generated iter-
atively, such that the hypothesis with the highest
score is chosen in each iteration from among dif-
ferent hyper-edges bundles.
However, this will lead to search errors as have
been observed earlier. Any hyper-edge that has
been discarded due to poor score in an early stage
might later become a better candidate. The prob-
lem worsens further when using smaller beam
sizes (for interactive decoding in real-time set-
tings, we even consider a beam size of 3). In
3Unlike Hiero-style systems, only two hypotheses are
merged in a phrase-based system and hence the term surface
218
the next section, we introduce the idea of delayed
pruning to reduce search errors.
3 Delayed Pruning
Delayed pruning (DP) in our decoder was inspired
by the well known fable about the race between
a tortoise and a hare. If the decoding is consid-
ered to be a race between competing candidate hy-
potheses with the winner being the best hypothe-
sis for Viterbi decoding or among the top-n candi-
dates for n-best decoding.4
In this analogy, a hypothesis having a poor
score, might just be a tortoise having a slow start
(due to a bad estimate of the true future cost for
what the user intends to type in the future) as op-
posed to a high scoring hare in the same state.
Pruning such hypotheses early on is not risk-free
and might result in search errors. We hypothe-
size that, given enough chance it might improve its
score and move ahead of a hare in terms of trans-
lation score.
We implement DP by relaxing the lazier cube
pruning step to generate a small, fixed number
of hypotheses for coverage sets that are not rep-
resented in the priority queue and place them in
the bin. These hypotheses are distinct from the
usual top-k derivations. Thus, the resulting bin
will have entries from all possible hyper-edge bun-
dles. Though this reduces the search error prob-
lem, it leads to increasing number of possibilities
to be explored at later stages with vast majority
of them being worse hypotheses that should be
pruned away.
We use a two level strategy of delay and then
prune, to avoid such exponentially increasing
search space and at the same time to reduce search
error. At the delay level, the idea is to delay the
pruning for few promising tortoises, instead of re-
taining a fixed number of hypotheses from all un-
represented hyper-edges. We use the normalized
language model scores of the top-hypotheses in
each hyper-edge that is not represented in cube
pruning and based on a threshold (which is ob-
tained using a development test set), we selec-
tively choose few hyper-edge bundles and gen-
erate a small number (typically 1-3) of hypothe-
ses from each of them and flag them as tortoises.
4The analogy is used to compare two or more hypotheses
in terms of their translation scores and not speed. Though our
objective is faster incremental decoding, we use the analogy
here to compare the scores.
These tortoises are extended minimally at each it-
eration subject to their normalized LM score.
While this significantly reduces the total num-
ber of hypotheses at initial bins, many of these
tortoises might not show improvement even after
several bins. Thus at the prune level, we prune out
tortoises that does not improve beyond a threshold
number of bins called race course limit. The race
course limit signifies the number of steps a tortoise
has in order to get into the decoder beam.
When a tortoise improves in score and breaks
into the beam during cube pruning, it is de-
flagged as a tortoise and enters the regular decod-
ing stream. We found DP to be effective in reduc-
ing the search error for incremental decoder in our
experiments.
4 Evaluation and Discussion
The evaluation was performed using our own im-
plementation of the beam-search decoding algo-
rithms. The architecture of our system is similar
to Moses, which we also use for training and for
minimum error rate training (MERT) of the log-
linear model for translation (Och, 2003; Koehn et
al., 2007). Our features include 7 standard phrase-
based features: 4 translation model features, i.e.
p(f |e), p(e|f), plex(f |e) and plex(e|f), where e
and f are target and source phrases respectively;
features for phrase penalty, word penalty and lan-
guage model, and we do not include the reorder-
ing feature. We used Giza++ and Moses respec-
tively for aligning the sentences and training the
system. The decoder was written in Java and in-
cludes cube pruning (Huang and Chiang, 2007)
and lazier cube pruning (Pust and Knight, 2009)
functionalities as part of the decoder. Our de-
coder supports both regular beam search (similar
to Moses) and incremental decoding.
In our experiments we experimented various ap-
proaches for storing partial decoder states includ-
ing memcache and transactional persistence using
JDBM but found that the serialization and deseri-
alization of decoder objects directly into and from
the memory to work better in terms of speed and
memory requirements. The partial object is re-
trieved and deserialized from the memory when
required by the incremental decoder.
We evaluated the incremental decoder for trans-
lations between French and English (in both direc-
tions). We used the Workshop on Machine Trans-
lation shared task (WMT07) dataset for training,
219
optimizing and testing. The system was trained us-
ing Moses and the feature weights were optimized
using MERT. To benchmark our Java decoder, we
compare it with Moses by running it in regular
beam search mode. The Moses systems were also
optimized separately on the WMT07 devsets.
Apart from comparing our decoder with Moses
in regular beam search, we also compared the in-
cremental decoding with regular regular beam us-
ing our decoder. To make it comparable with
incremental decoding, we used the regular beam
search to re-decode the sentence fragments for ev-
ery additional word in the input sentence. We
measured the following parameters in our empir-
ical analysis: translation quality (as measured by
BLEU (Papineni et al, 2002) and TER (Snover et
al., 2006)), search errors and translation speed. Fi-
nally, we also measured the effect of different race
course limits on BLEU and decoding speed for in-
cremental decoding.
4.1 Benchmarking our decoder
In this section we compare our decoder with
Moses for regular beam search decoding. Table 1
gives the BLEU and TER for the two language
pairs. Our decoder implementation compares
favourably with Moses for Fr-En: the slightly bet-
ter BLEU and TER for our decoder in Fr-En is
possibly due to the minor differences in the con-
figuration settings. For En-Fr translation, Moses
performs better in both metrics. There are differ-
ences in the beam size between the two decoders,
in our system the beam size is set to 100 compared
to the default value of 1000 (the cube pruning pop
limit) in Moses; we are planning to explore this
and remove any other differences between them.
However based on our understanding of the Moses
implementation and our experiments, we believe
our decoder to be comparable in accuracy with the
Moses implementation. The numbers in the bold-
face are statistically significant at 95% confidence
interval.
4.2 Re-decoding v.s. Incremental decoding
We test our hypothesis that incremental decod-
ing can benefit by using partial decoder states for
decoding every additional word in the input sen-
tence. In order to do this, we run our incremen-
tal decoder in both regular beam search mode and
in incremental decoding mode. In regular beam
search mode, we forced the beam search decoder
to re-decode the sentence fragments for every ad-
ditional word and in incremental decoding mode,
we used the partial decoding states to incremen-
tally decode lastly added word. We then compare
the BLEU and TER scores between them to vali-
date our hypothesis.
We further test effectiveness of delayed prun-
ing (DP) in incremental decoding by comparing
it to the case where we turn off the DP. For in-
cremental decoding, we set the beam size and the
race course limit (for DP) to be 3. Additionally,
we used a threshold of?2.0 (in log-scale) for nor-
malized LM in the delay phase of DP, which was
obtained by testing on a separate development test
set.
We would like to highlight two observations
from the results in Table 2. First the regular beam
search indicate possible search errors due to the
small beam size (cube pruning pop limit) and the
BLEU scores has decreased by 0.56 for Fr-En
and by over 2.5 for En-Fr, than the scores cor-
responding to a beam size of 100 shown in Ta-
ble 1. Secondly, we find the incremental decoding
to perform better for the same beam size. How-
ever, incremental decoding without delay pruning
still seems to incur search errors when compared
with the regular decoding with a larger beam. De-
layed pruning alleviates this issue and improves
the BLEU and TER significantly. This we believe,
is mainly because the strategy to delay the pruning
retains the potentially better partial hypotheses for
every coverage set. It should be noted that results
in Table 2 pertain only to our decoder implemen-
tation and not with Moses.
We now give a comparative note between our
approach and the pruning strategy in regular beam
search. Delaying the hypothesis pruning is the im-
portant aspect in our approach to incremental de-
coding. In the case of regular beam search, the
hypotheses are pruned when they fall out of the
beam and the idea is to have a larger beam size
to avoid the early pruning of potentially good can-
didates. With the advent of cube pruning (Huang
and Chiang, 2007), the ?cube pruning pop limit?
(in Moses) determines the number of hypotheses
retained in each stack. In both the cases, it is pos-
sible that some of the coverage sets go unrepre-
sented in the stack due to poor candidate scores.
This is not desirable in the incremental decoding
setting as this might lead to search errors while
decoding a partial sentence.
Additionally, Moses offers an option (cube
220
Decoder
Fr-En En-Fr
BLEU TER BLEU TER
Moses 26.98 0.551 27.24 0.610
Our decoder 27.53 0.541 26.96 0.657
Table 1: Regular beam search: Moses v.s. Our decoder
Decoder
Fr-En En-Fr
BLEU TER BLEU TER
Re-decode w/ beam search 26.96 0.548 24.33 0.635
ID w/o delay pruning 27.01 0.547 25.00 0.618
ID w/ delay pruning 27.62 0.545 25.45 0.616
Table 2: BLEU and TER: Re-decoding v.s. Incremental Decoding (ID)
pruning diversity) to control the number of hy-
potheses generated for each coverage set (though
set to ?0? by default). It might be possible to use
this in conjunction with cube pruning pop limit as
an alternative to our delayed pruning in the incre-
mental decoding setting (with the risk of combina-
torial explosion in the search space).
In contrast, the delayed pruning not only avoids
search errors but also provides a dynamically man-
ageable search space (refer section 4.2.2) by re-
taining the best of the potential candidates. In a
practical scenario like real-time translation of in-
ternet chat, translation speed is an important con-
sideration. Furthermore, it is better to avoid large
number of candidates and generate only few best
ones, as only the top few translations will be used
by the system. Thus we believe our delayed prun-
ing approach to be a principled pruning strategy
that combines the different factors in an elegant
framework.
4.2.1 Search Errors
As BLEU only indirectly indicates the number
of search errors made by algorithm, we used a
more direct way of quantifying the search errors
incurred by the ID in comparison to regular beam
search. We define the search error to be the differ-
ence between the translation scores of the best hy-
potheses produced by the ID and the regular beam
search and then compute the mean squared error
(MSE) for the entire test set. We use this method
to compare ID in the two settings of delayed prun-
ing being turned off (using a smaller beam size
of 3 to simulate the requirements of near instanta-
neous translations in real-time environments) and
delayed pruning turned on. We compare the model
score in these cases with the model score for the
best result obtained from the regular beam search
decoder (using a larger beam of size 100).
Direction
Beam search against
Incremental Decoding
w/o DP w/ DP
Fr-En 0.3823 0.3235
En-Fr 1.1559 0.6755
Table 3: Search Errors in Incremental Decoding
The results are shown in Table 3 and as can be
clearly seen, ID shows much lesser mean square
error with the DP turned on than when it is turned
off. Together the BLEU and TER numbers and
the mean square search error show that delayed
pruning is useful in the incremental decoding set-
ting. Comparing the En-Fr and Fr-En results show
that the two language pairs show slightly different
characteristics but the experiments in both direc-
tions support our overall conclusions.
4.2.2 Speed
In this experiment, we set out to evaluate the
ID against the regular beam-search in which sen-
tence fragments are incrementally decoded for ad-
ditional words. In order compare with the in-
cremental decoder, we modified the regular de-
coder to decode the partial phrases, so that it re-
decodes the partial phrase from the scratch instead
of reusing the earlier state.
We ran the timing experiments on a Dell ma-
chine with an Intel Core i7 processor and 12 GB
memory, clocking 2.67 GHz and running Linux
(CentOS 5.3). We measured the time taken for de-
coding the fragment with every word added and
221
averaged it first over the sentence and then the en-
tire test set. The average time (in msecs) includes
the future cost computation for both. We also mea-
sured the average number of hypotheses for every
bin at the end of decoding a complete sentence,
which was also averaged over the test set.
The results in Table 4 show that the incremen-
tal decoder was significantly faster than the beam
search in re-decoding mode almost by a factor of
9 in the best case (for Fr-En). The speedup is pri-
marily due to two factors, i) computing the future
cost for the new phrases as opposed to computing
it for all the phrases and ii) using partial decoder
states without having to re-generate hypotheses
through the cube pruning step and the latencies
associated with computing LM scores for them.
The addition of delayed pruning slowed down the
speed at most by 7 msecs (for En-Fr). In addition,
delayed pruning can be seen generating far more
hypotheses than the other two cases. Clearly, this
is because of the delay in pruning the tortoises un-
til the race course limit. Even with such signifi-
cantly large number of hypotheses being retained
for every bin, DP results in improved speed (over
re-decoding from scratch) and better performance
by avoiding search errors (compared to the incre-
mental decoder that does not use DP).
4.3 Effect of Race course limit
Table 5 shows the effect of different race course
limits on translation quality measured using
BLEU. We generally expect the race course limit
to behave similar to the beam size as they both al-
low more hypotheses in the bin thereby reducing
search error although at the expense of increasing
decoding time.
However, in our experiments for Fr-En, we did
not find significant variations in BLEU for differ-
ent race course limits. This could be due to the
absence of long distance re-orderings between En-
glish and French and that the smallest race course
limit of 3 is sufficient for capturing all cases of lo-
cal re-ordering. As expected, we find the decoding
speed to slightly decrease and the average number
of hypotheses per bin to increase with the increas-
ing race course limit.
5 Related Work
Google5 does seem to perform incremental decod-
ing, but the underlying algorithms are not public
5translate.google.com
knowledge. They may be simply re-translating the
input each time using a fast decoder or re-using
prior decoder states as we do here.
Intereactive translation using text prediction
strategies have been studied well (Foster et al,
1997; Foster et al, 2002; Och et al, 2003). They
all attempt to interactively help the human user in
the postediting process, by suggesting completion
of the word/phrase based on the user accepted pre-
fix and the source sentece. Incremental feedback
is part of Caitra (Koehn, 2009) an interactive tool
for human-aided MT and works on a similar set-
ting to interactive MT. In Caitra, the source text
is pre-translated first and during the interactions it
dynamically generates user suggestions.
Our incremental decoder work differs from
these text prediction based approaches, in the
sense that the input text is not available to the de-
coder beforehand and the decoding is being done
dynamically for every source word as opposed to
generating suggestions dynamically for complet-
ing target sentece.
6 Conclusion and Future Work
We presented a modified beam search algorithm
for an efficient incremental decoder (ID), which
will allow translations to be generated incremen-
tally for every word typed by a user, instead of
waiting for the entire sentence as input by reusing
the partial decoder state. Our proposed modifica-
tions help us to efficiently compute partial future
costs in the incremental setting. We introduced the
notion of delayed pruning (DP) to avoid search
errors in incremental decoding. We showed that
reusing the partial decoder states is faster than re-
decoding the input from the scratch every time a
new word is typed by the user. Our exhaustive ex-
periments further demonstrated DP to be highly
effective in avoiding search errors under the in-
cremental decoding setting. In our experiments in
this paper we used a very tight beam size; in fu-
ture work, we would like to explore the tradeoff
between speed, accuracy and the utility of delayed
pruning by varying the beam size in our experi-
ments.
References
George Foster, Pierre Isabelle, and Pierre Plamondon.
1997. Target-text mediated interactive machine
translation. Machine Translation, 12(1/2):175?194.
222
Decoder
Fr-En En-Fr
Avg time Avg Hyp/ bin Avg time Avg Hyp/ bin
Re-decode 724.46 2.21 130.29 2.32
ID w/o DP 84.85 2.89 27.58 2.89
ID w/ DP 87.01 85.11 34.35 60.46
Table 4: Speed: Re-decoding v.s. Incremental Decoding (ID)
Race Fr-En En-Fr
Course
BLEU Avg time Avg Hyp/ bin BLEU Avg time Avg Hyp/ bin
Limit
3 26.75 87.83 85.11 25.39 36.15 75.03
4 26.77 91.14 86.35 25.37 36.21 77.69
5 26.77 90.81 86.52 25.37 36.25 78.47
6 26.77 95.91 86.56 25.37 37.34 78.71
7 26.77 91.67 86.57 25.37 36.26 78.81
Table 5: Effect of different race course limits
George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-friendly text prediction for translators.
In EMNLP ?02: Proceedings of the ACL-02 con-
ference on Empirical methods in natural language
processing, pages 148?155, Morristown, NJ, USA.
Association for Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: A beam search
decoder for phrase-based statistical machine trans-
lation models. In Robert E. Frederking and
Kathryn Taylor, editors, AMTA, volume 3265 of Lec-
ture Notes in Computer Science, pages 115?124.
Springer.
Philipp Koehn. 2009. A web-based interactive com-
puter aided translation tool. In In Proceedings of
ACL-IJCNLP 2009: Software Demonstrations, Sun-
tec, Singapore, August.
Franz Josef Och, Richard Zens, and Hermann Ney.
2003. Efficient search for interactive statistical ma-
chine translation. In EACL ?03: Proceedings of the
tenth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 387?
393, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL.
Michael Pust and Kevin Knight. 2009. Faster mt
decoding through pervasive laziness. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 141?144,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas: AMTA 2006.
223
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 533?541,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Bayesian Extraction of Minimal SCFG Rules for
Hierarchical Phrase-based Translation
Baskaran Sankaran
Simon Fraser University
Burnaby BC, Canada
baskaran@cs.sfu.ca
Gholamreza Haffari
Monash University
Melbourne, Australia
reza@monash.edu
Anoop Sarkar
Simon Fraser University
Burnaby BC, Canada
anoop@cs.sfu.ca
Abstract
We present a novel approach for extracting
a minimal synchronous context-free grammar
(SCFG) for Hiero-style statistical machine
translation using a non-parametric Bayesian
framework. Our approach is designed to ex-
tract rules that are licensed by the word align-
ments and heuristically extracted phrase pairs.
Our Bayesian model limits the number of
SCFG rules extracted, by sampling from the
space of all possible hierarchical rules; addi-
tionally our informed prior based on the lex-
ical alignment probabilities biases the gram-
mar to extract high quality rules leading to im-
proved generalization and the automatic iden-
tification of commonly re-used rules. We
show that our Bayesian model is able to ex-
tract minimal set of hierarchical phrase rules
without impacting the translation quality as
measured by the BLEU score.
1 Introduction
Hierarchical phrase-based (Hiero) machine transla-
tion (Chiang, 2007) has attracted significant interest
within the Machine Translation community. It ex-
tends phrase-based translation by automatically in-
ferring a synchronous grammar from an aligned bi-
text. The synchronous context-free grammar links
non-terminals in source and target languages. De-
coding in such systems employ a modified CKY-
parser that is integrated with a language model.
The primary advantage of Hiero-style systems lie
in their unsupervised model of syntax for transla-
tion: allowing long-distance reordering and cap-
turing certain syntactic constructions, particularly
those that involve discontiguous phrases. It has
been demonstrated to be a successful framework
with comparable performance with other statisti-
cal frameworks and suitable for large-scale cor-
pora (Zollmann et al, 2008). However, one of the
major difficulties in Hiero-style systems has been on
learning a concise and general synchronous gram-
mar from the bitext.
While most of the research in Hiero-style sys-
tems is focused on the improving the decoder, and
in particular the link to the language model, compar-
atively few papers have considered the inference of
the probabilistic SCFG from the word alignments.
A majority of the systems employ the classic rule-
extraction algorithm (Chiang, 2007) which extracts
rules by replacing possible sub-spans (permitted by
the word alignments) with a non-terminal and then
using relative frequencies to estimate the probabilis-
tic synchronous context-free grammar. One of the
issues in building Hiero-style systems is in manag-
ing the size of the synchronous grammar. The origi-
nal approach extracts a larger number of rules when
compared to a phrase-based system on the same data
leading to practical issues in terms of memory re-
quirements and decoding speed.
Extremely large Hiero phrase tables may also lead
to statistical issues, where the probability mass has
to be shared by more rules: the probability p(e|f)
has to be shared by all the rules having the same
source side string f , leading to fragmentation and
resulting in many rules having very poor probability.
Approaches to improve the inference (the induc-
tion of the SCFG rules from the bitext) typically
follows two streams. One focusses on filtering the
extracted hierarchical rules either by removing re-
dundancy (He et al, 2009) or by filtering rules
based on certain patterns (Iglesias et al, 2009),
while the other stream is concerned about alterna-
tive approaches for learning the synchronous gram-
mar (Blunsom et al, 2008; Blunsom et al, 2009; de
Gispert et al, 2010). This paper falls under the lat-
ter category and we use a non-parametric Bayesian
approach for rule extraction for Hiero-style systems.
Our objective in this paper is to provide a principled
533
rule extraction method using a Bayesian framework
that can extract the minimal SCFG rules without re-
ducing the BLEU score.
2 Motivation and Related Work
The large number of rules in Hiero-style systems
leads to slow decoding and increased memory re-
quirements. The heuristic rule extraction algo-
rithm (Chiang, 2007) introduces redundant mono-
tone composed rules (He et al, 2009) in the SCFG
grammar. The research on Hiero rule extraction falls
into two broad categories: i) rule reduction by elim-
inating a subset of rules extracted by the heuristic
approach and ii) alternate approaches for rule extrac-
tion.
There have been approaches to reduce the size of
Hiero phrase table, without significantly affecting
the translation quality. He et. al. (2009) proposed the
idea of discarding monotone composed rules from
the phrase table that can instead be obtained dynami-
cally by combining the minimal rules in the same or-
der. They achieve up to 70% reduction in the phrase
table by discarding these redundant rules, without
appreciable reduction in the performance as mea-
sured by BLEU. Empirically analyzing the effective-
ness of specific rule patterns, (Iglesias et al, 2009)
show that some patterns having over 95% of the to-
tal SCFG rules can be safely eliminated without any
reduction in the BLEU score.
Along a different track, some prior works have
employed alternate rule extraction approaches using
a Bayesian framework (DeNero et al, 2008; Blun-
som et al, 2008; Blunsom et al, 2009). (DeNero
et al, 2008) use a Maximum likelihood model of
learning phrase pairs (Marcu and Wong, 2002), but
use sampling to compute the expected counts of the
phrase pairs for the E-step. Other recent approaches
use Gibbs sampler for learning the SCFG by explor-
ing a fixed grammar having pre-defined rule tem-
plates (Blunsom et al, 2008) or by reasoning over
the space of derivations (Blunsom et al, 2009).
We differ from earlier Bayesian approaches in that
our model is guided by the word alignments to rea-
son over the space of the SCFG rules and this re-
stricts the search space of our model. We believe
the word alignments to encode information, useful
for identifying the good phrase-pairs. For example,
several attempts have been made to learn a phrasal
translation model directly from the bitext without
the word alignments (Marcu and Wong, 2002; DeN-
ero et al, 2008; Blunsom et al, 2008), but without
any clear breakthrough that can scale to larger cor-
pora.
Our model exploits the word alignment informa-
tion in the form of lexical alignment probability in
order to construct an informative prior over SCFG
rules and it moves away from a heuristic framework,
instead using a Bayesian non-parametric model to
infer a minimal, high-quality grammar from the
data.
3 Model
Our model is based on similar assumptions as the
original Hiero system. We assume that the bitext has
been word aligned, and that we can use that word
alignment to extract phrase pairs.
Given the word alignments and the heuristically
extracted phrase pairs Rp, our goal is to extract the
minimal set of hierarchical rules Rg that would best
explain Rp. This is achieved by inferring a distribu-
tion over the derivations for each phrase pair, where
the set of derivations collectively specify the gram-
mar. In the following, we denote the sequence of
derivations for the set of phrase pairs by r, which is
composed of grammar rules r. We will essentially
read off our learned grammar from the sequence of
derivations r.
Our non-parametric model reasons over the space
of the (hierarchical and terminal) rules and sam-
ples a set of rules by employing a prior based on
the alignment probability of the words in the phrase
pairs. We hypothesize that the resulting grammar
will be compact and also will explain the phrase
pairs better (the SCFG rules will maximize the like-
lihood of producing the entire set of observed phrase
pairs).
Using Bayes? rule, the posterior over the deriva-
tions r given the phrase pairs Rp can be written as:
P (r|Rp) ? P (Rp|r)P (r) (1)
where P (Rp|r) is equal to one when the sequence
of rules r and phrase-pairs Rp are consistent, i.e. r
can be partitioned into derivations to compose the
set of phrase-pairs such that the derivations respect
534
the given word alignments; otherwise P (Rp|r) is
zero. The overall structure of the model is analo-
gous to the Bayesian model for inducing Tree Sub-
stitution Grammars proposed by Cohn et al (2009).
Note that, our model extracts hierarchical rules for
the word-aligned phrase pairs and not for the sen-
tences.
Similar to the other Hiero-style systems, we use
two types of rules: terminal and hierarchical rules.
For each phrase-pair, our model either generates a
terminal rule by not segmenting the phrase-pair, or
decides to segment the phrase-pair and extract some
rules.
Though it is possible to segment phrase-pairs by
two (or more) non-overlapping spans, we propose
a simpler model in this paper and restrict the hierar-
chical rules to contain only one non-terminal (unlike
the case of classic Hiero-style grammars containing
two non-terminals). This simpler model, samples
the space of derivations and identifies a sub-span
for introducing the non-terminal, which can be ex-
pressed as terminal rules (it is not decomposed fur-
ther). Figure 1 shows an example phrase-pair with
the Viterbi-best word alignment and Figure 2 shows
two possible derivations for the same phrase-pair
with the non-terminals introduced at different sub-
spans. It can be seen that the sub-phrase correspond-
ing to the non-terminal spanX1 is directly written as
a terminal rule and is not decomposed further.
While the resulting model is slightly weaker than
the original Hiero grammar, it should be noted our
simpler model does allow reordering and discontigu-
ous alignments. For example our model includes
rules such as, X ? (?X1?, ????X1), which can
capture phrases like (not X1, ne X1 pas) in the case
of English-French translation. In terms of the re-
ordering, our model lies in between the hierarchi-
cal phrase-based and phrase-based models. To sum-
marize, the segmentation of each phrase-pair in our
model results in two rules: a hierarchical rule with
one nonterminal as well as a terminal rule.
More specifically, the generative process for gen-
erating a phrase pair x from the grammar rules
may have two steps as follows. In the first step,
the model decides on the type of the rule tx ?
{TERMINAL,HIERARCHICAL} used to generate the
phrase-pair based on a Bernoulli distribution, having
a prior ? coming from a Beta distribution:
tx ? Bernoulli(?)
? ? Beta(lx, 0.5)
The lexical alignment probability lx controls the
tendency for extracting hierarchical rules from the
phrase-pair x. For a given phrase-pair, lx is com-
puted by taking the (geometric or arithmetic) aver-
age of the reverse and forward alignment probabil-
ities, which we explain later in this section. Inte-
grating out ? gives us the conditional probabilities
of choosing the rule type tx as:
p(tterm|x) ? n
x
term + lx (2)
p(thier|x) ? n
x
hier + 0.5 (3)
where nxterm and n
x
hier denote the number of termi-
nal or hierarchical rules, among the rules extracted
so far from the phrase-pair x during the sampling.
In the second step, if the rule type tx =
HIERARCHICAL, the model generates the phrase-
pair by sampling from the hierarchical and terminal
rules. We use a Dirichlet Process (DP) to model the
generation of hierarchical rules r:
G ? DP (?h, P0(r))
r ? G
Integrating out the grammar G, the predictive dis-
tribution of a hierarchical rule rx for generating the
current phrase-pair (conditioned on the rules from
the rest of the phrase-pairs) is:
p(rx|r
?x, ?h, P0) ? n
?x
rx + ?hP0(rx) (4)
where n?xrx is the count of the rule rx in the rest of
the phrase-pairs that is represented by r?x, P0 is the
base measure, and ?h is the concentration parameter
controlling the model?s preference towards using an
existing hierarchical rule from the cache or to create
a new rule sanctioned by the base distribution. We
use the lexical alignment probabilities of the compo-
nent rules as our base measure P0:
P0(r) =
[( ?
(k,l)?a
p(el|fk)
) 1
|a|
( ?
(k,l)?a
p(fk|el)
) 1
|a|
] 1
2
(5)
535
octavo y noveno Fondos Europeos de Desarrollo para el ejercicio
Eighth and Ninth European Development Funds for the financial year
Figure 1: An example phrase-pair with Viterbi alignments
X ? (Eighth and Ninth X1 for the financial year, octavo y noveno X1 para el ejercicio)
X ? (European Development Funds, Fondos Europeos de Desarrollo)
X ? (Eighth and Ninth X1, octavo y noveno X1)
X ? (European Development Funds for the financial year,
Fondos Europeos de Desarrollo para el ejercicio)
Figure 2: Two possible derivations of the phrase-pair in Figure 1
where a is the set of alignments in the given sub-
span; if the sub-span has multiple Viterbi alignments
from different phrase-pairs, we consider the union of
all such alignments. DeNero et al (2008) use a sim-
ilar prior- geometric mean of the forward and reverse
IBM-1 alignments. However, we use the product of
geometric means of the forward and reverse align-
ment scores. We also experimented with the arith-
metic mean of the lexical alignment probabilities.
The lexical prior lx in the first step can be defined
similarly. We found the particular combination of,
?arithmetic mean? for the lexical prior lx (in the first
step) and ?geometric mean? for the base distribution
P0 (in the second step) to work better, as we discuss
later in Section 5.
Assuming the heuristically extracted phrase pairs
to be the input to our inference algorithm, our
approach samples the space of rules to find the
best possible segmentation for the sentences as de-
fined by the cache and base distribution. We ex-
plore a subset of the space of rules being consid-
ered by (Blunsom et al, 2009) ? i.e., only those
rules satisfying the word alignments and heuristi-
cally grown phrase alignments.
4 Inference
We train our model by using a Gibbs sampler ? a
Markov Chain Monte Carlo (MCMC) method for
sampling one variable in the model, conditional to
the other variables. The sampling procedure is re-
peated for what is called a long Gibbs chain span-
ning several iterations, while the counts are collected
at fixed thin intervals in the chain. As is common in
the MCMC procedures, we ignore samples from a
fixed number of initial burn-in iterations, allowing
the model to move away from the initial bias. The
rules in the final sampler state at the end of the Gibbs
chain along with their counts averaged by the num-
ber of thin iterations become our translation model.
In our model, a sample for a given phrase pair
corresponds either to its terminal derivation or two
rules in a hierarchical derivation. The model sam-
ples a derivation from the space of derivations that
are consistent with the word alignments. In order
to achieve this, we need an efficient way to enumer-
ate the derivations for a phrase pair such that they
are consistent with the alignments. We use the lin-
ear time algorithm to maximally decompose a word-
aligned phrase pair, so as to encode it as a compact
alignment tree (Zhang et al, 2008).
f0 f1 f2 f3 f4
e0 e1 e2 e3 e4 e5
Figure 3: Example phrase pair with alignments.
536
For a phrase-pair with a given alignment as shown
in Figure 3, Zhang et al (2008) generalize theO(n+
K) time algorithm for computing all K common in-
tervals of two different permutations of length n.
The contiguous blocks of the alignment are cap-
tured as the nodes in the alignment tree and the tree
structure for the example phrase pair in Figure 3 is
shown in Figure 4. The italicized nodes form a left-
branching chain in the alignment tree and the sub-
spans of this chain also lead to alignment nodes that
are not explicitly captured in the tree (Please refer
to Zhang et al (2008) for details). In our work, each
node in the tree (and also each sub-span in the left-
branching chain) corresponds to an aligned source-
target sub-span within the phrase-pair, and is a po-
tential site for introducing the non-terminal X to
generate hierarchical rules.
Given this alignment tree for a phrase pair, a
derivation can be obtained by introducing a non-
terminal at some node nd in the tree and re-writing
the span rooted at nd as a separate rule. As men-
tioned earlier, we compute the derivation probability
as a product of the probabilities of the component
rules, which are computed using the Equation 4.
We initialize the sampler by using our lexical
alignment prior and sampling from the distribution
of derivations as suggested by the priors. We found
this to perform better in practice, than a naive sam-
pler without an initializer.
At each iteration, the Gibbs sampler processes the
phrase pairs in random order. For each phrase pair
Rp, it visits the nodes in the corresponding align-
ment tree and computes the posterior probability of
the derivations and samples from this posterior dis-
tribution. To speedup the sampling, we store the
pre-computed alignment tree for the phrase pairs and
just recompute the derivation probabilities based on
the sampler state at every iteration. While the sam-
pler state is updated with the counts at each iteration,
we accumulate the counts only at fixed intervals in
the Gibbs chain. In applying the model for decoding,
we use the grammar from the final sampler state.
Since our model includes only one hyperparam-
eter ?h, we tune its value manually by empirically
experimenting on a small set of initial phrase pairs.
We keep for future work the task of automatically
tuning for hyper-parameter values by sampling.
([0,5],[0,4])
([0,2],[0,2])
([0,1],[0,1])
([0,0],[0,0]) ([1,1],[1,1])
([2,2],[2,2])
([4,5],[3,4])
Figure 4: Decomposed alignment tree for the example
alignment in Fig. 3.
5 Experiments
We use the English-Spanish data from WMT-10
shared task for the experiments to evaluate the effec-
tiveness of our Bayesian rule extraction approach.
We used the entire shared task training set except
the UN data for training translation model and the
language model was trained with the same set and
an additional 2 million sentences from the UN data,
using SRILM toolkit with Knesser-Ney discounting.
We tuned the feature weights on the WMT-10 dev-
set using MERT (Och, 2003) and evaluate on the
test set by computing lower-cased BLEU score (Pa-
pineni et al, 2002) using the WMT-10 standard eval-
uation script.
We use Kriya ? an in-house implementation of hi-
erarchical phrase-based translation written predom-
inantly in Python. Kriya supports the entire transla-
tion pipeline of SCFG rule extraction and decoding
with cube pruning (Huang and Chiang, 2007) and
LM integration (Chiang, 2007). We use the 7 fea-
tures (4 translation model features, extracted rules
penalty, word penalty and language model) as is typ-
ical in Hiero-style systems. For tuning the feature
weights, we have adapted the MERT implementa-
tion in Moses1 for use with Kriya as the decoder.
We started by training and evaluating the two
baseline systems using i) two non-terminals and
ii) one non-terminal, which were trained using the
conventional heuristic extraction approach. For the
baseline with one non-terminal, we modified the
heuristic rule extraction algorithm appropriately2.
1www.statmt.org/moses/
2Given an initial phrase pair, the algorithm would introduce
a non-terminal for each sub-span consistent with the alignments
and extract rules corresponding to each sub-span. The con-
537
Experiment
# of rules filtered
for devset
(in millions)
BLEU
Baseline (w/ 2 non-terminals) 52.36 27.45
Baseline (w/ 1 non-terminal) 22.09 26.71
Pattern-based filtering? 18.78 24.61
1 non-terminal; monotone & non-monotone 10.36 24.17
1 non-terminal; non-monotone 3.62 23.99
Table 1: Kriya: Baseline and Filtering experiments. ?: This is the initial rule set used in Iglesias et al (2009) obtained
by greedy filtering. Rows 4 and 5 represents the filtering that uses single non-terminal rules with row 4 allowing
monotone rules in addition to the non-monotone (reordering) rules.
As part of the baseline methods to be applied to min-
imize the number of SCFG rules, We also wanted to
assess the effect of a simpler rule filtering, where
the idea is to filter the heuristically extracted rules
based on certain patterns. Our first baseline filtering
strategy uses the heuristic methods in Iglesias et al
(2009) in order to minimize the number of rules3.
For the other baseline filtering experiments, we re-
tained only one non-terminal rules and then further
limited it by retaining only non-monotone one non-
terminal rules; in both cases the terminal rules were
retained.
Table 1 shows the results for baseline and the rule
filtering experiments. Restricting rule extraction to
just one non-terminal doesn?t affect the BLEU score
significantly and this justifies the simpler model
used in this paper. Secondly, we find significant re-
duction in the BLEU for the pattern-based filtering
strategy and this is because we only use the initial
rule set obtained by greedy filtering without aug-
menting it with other specific patterns. The other
two filtering methods reduced the BLEU further but
not significantly. The second column in the table
gives the number of SCFG rules filtered for the dev-
set, which is typically much less than the full set of
rules. We later use this to put in perspective the
effective reduction in the model size achieved by
our Bayesian model. We can ideally compare our
Bayesian rule extraction using Gibbs sampling with
straints relating to two non-terminals (such as, no adjacent non-
terminals in source side) does not apply for the one non-terminal
case.
3It should be noted that we didn?t use the augmentations to
the initial rule set (Iglesias et al, 2009) and our objective is to
find the impact of the filtering approaches.
the baselines and the filtering approaches. However,
running our Gibbs sampler on the full set of phrase
pairs demand sampling to be distributed, possibly
with approximation (?; ?), which we reserve for our
future work.
In this work, we focus on evaluating our Gibbs
sampler on reasonable sized set of phrase pairs with
corresponding baselines. We filter the initial phrase
pairs based on their frequency using three different
thresholds, viz. 20, 10 and 3- resulting in smaller
sets of initial phrase pairs because we throw out in-
frequent phrase pairs (the threshold-20 case is the
smallest initial set of phrase pairs). This allows us
to run our sampler as a stand-alone instance for the
three sets, obviating the need for distributed sam-
pling.
Table 2 shows the number of unique phrase pairs
in each set. While, the filtering reduces the number
of phrase pairs to a small fraction of the total phrase
pairs, it also increases the unknown words (OOV)
in the test set by a factor between 1.8 and 3. In or-
der to address this issue due to the OOV words, we
additionally added non-decomposable phrase pairs
having just one word at either source or target side,
Phrase-pairs set
# of Unique
phrase-pairs
Testset
OOV
All phrase-pairs 110782174 1136
Threshold-20 292336 3735
Threshold-10 606590 3056
Threshold-3 2689855 2067
Table 2: Phrase-pair statistics for different frequency
threshold
538
Experiment Threshold-20 Threshold-10 Threshold-3
Baseline (w/ 2 non-terminals) 24.30 25.96 26.34
Baseline (w/ 1 non-terminal) 24.00 25.90 26.83
Bayesian rule extraction 23.39 24.30 25.22
Table 3: BLEU scores: Heuristic vs Bayesian rule extraction
Experiment Rules Extracted (in millions) Reduction
Heuristic (1 nt) Bayesian
Threshold-20 1.93 (0.117) 1.86 (0.07) 3.57 (38.34)
Threshold-10 2.91 (1.09) 2.10 (0.28) 27.7 (73.95)
Threshold-3 7.46 (5.64) 2.45 (0.71) 67.17 (87.28)
Table 4: Model compression: Heuristic vs Bayesian rule extraction
Priors ?h BLEU
Arith + Arith means 0.5 22.46
Arith + Geom means 0.5 23.39
Geom + Arith means 0.5 22.96
Arith + Geom means 0.5 22.83
Arith + Geom means 0.1 22.88
Arith + Geom means 0.2 22.97
Arith + Geom means 0.3 22.98
Arith + Geom means 0.4 22.69
Arith + Geom means 0.5 23.39
Arith + Geom means 0.6 22.89
Arith + Geom means 0.7 22.82
Arith + Geom means 0.8 22.82
Arith + Geom means 0.9 22.67
Table 5: Effect of different priors and ?h on Threshold-
20 set. The two priors correspond to the lexical prior lx
in the first step and the base distribution P0 in the second
step.
as coverage rules. The coverage rules (about 1.8
million) were added separately to the SCFG rules
induced by both heuristic algorithm and Gibbs sam-
pler. This is justified because we only add the rules
that can not be decomposed further by both rule ex-
traction approaches. However, note that both ap-
proaches can independently induce rules that over-
lap with the coverage rules set and in such cases we
simply add the original corpus count to the counts
returned by the respective rule extraction method.
The Gibbs sampler considers the phrase pairs in
random order at each iteration and induces SCFG
rules by sampling a derivation for each phrase pair.
Given a phrase pair x with raw corpus frequency fx,
we simply scale the count for its sampled deriva-
tion r by its frequency fx. Alternately, we also ex-
perimented with independently sampling for each
instance of the phrase pair and found their perfor-
mances to be comparable. Sampling phrase pairs
once and then scaling the sampled derivation, help
us to speed up the sampling process. In our experi-
ments, we ran the Gibbs sampler for 2000 iterations
with a burn-in period of 200, collecting counts every
50 iterations. We set the concentration parameter ?h
to be 0.5 based on our experiments detailed later in
this section.
The BLEU scores for the SCFG learned from the
Gibbs sampler are shown in Table 3. We first note
that, the threshold-20 set has lower baseline BLEU
than threshold-10 and threshold-3 sets, as can be ex-
pected because threshold-20 set uses a much smaller
subset of the full set of phrase pairs to extract hier-
archical rules. The Bayesian approach results in a
maximum BLEU score reduction of 1.6 for the sets
using thresholds 10 and 3, compared to the one non-
terminal baseline. The two non-terminal baseline is
also provided to place our results in perspective.
Table 4 shows the model size, including the cov-
erage rules for the two rule extraction approaches.
The number of extracted rules, excluding the cov-
erage rules are shown within the parenthesis. The
last column shows the reduction in the model size
for both with and without the coverage rules; yield-
ing a maximum absolute reduction of 67.17% for the
539
threshold-3 phrase pairs set. It can be seen that the
number of rules are far fewer than the rules extracted
using the baseline heuristic methods for filtering de-
tailed in Table 1. Interestingly, we obtain a smaller
model size, even as we decrease the threshold to in-
clude more initial phrase pairs used as input to the
inference procedure, e.g. a 67.17% reduction over
the rules extracted from the threshold-3 phrase pairs
v.s. a 27.7% reduction for threshold-10.
These results show that our model is capable of
extracting high-value Hiero-style SCFG rules, albeit
with a reduction in the BLEU score. However, our
current approach offers scope for improvement in
several avenues, for example we can use annealing
to perturb the initial sampling iterations to encour-
age the Gibbs sampler to explore several derivations
for each phrase pair. Though this might result in
slightly large models than the current ones, we still
expect substantial reduction than the original Hiero
rule extraction. In future, we also plan to sample the
hyperparameter ?h, instead of using a fixed value.
Table 5 shows the effect of different values of
the concentration parameter ?h and the priors used
in the model. The order of priors in each setting
correspond to the prior used in deciding the rule-
type and identifying the non-terminal span for sam-
pling a derivation. We found the geometric mean to
work better in both cases. We further found that the
concentration parameter ?h value 0.5 gives the best
BLEU score.
6 Conclusion and Future Work
We proposed a novel method for extracting mini-
mal set of hierarchical rules using non-parametric
Bayesian framework. We demonstrated substantial
reduction in the size of extracted grammar with the
best case reduction of 67.17%, as compared to the
heuristic approach, albeit with a slight reduction in
the BLEU scores.
We plan to extend our model to handle two non-
terminals to allow for better reordering. We also
plan to run our sampler on the full set of phrase
pairs using distributed sampling and our prelimi-
nary results in this direction are encouraging. Fi-
nally, we would like to directly sample from the
Viterbi aligned sentence pairs instead of relying on
the heuristically extracted phrase pairs. This can
be accomplished by using a model that is closer
to the Tree Substitution Grammar induction model
in (Cohn et al, 2009) but in our case the model
would infer a Hiero-style SCFG from word-aligned
sentence pairs.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Pro-
ceedings of Neural Information Processing Systems-
08.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of Association of
Computational Linguistics-09, pages 782?790. Asso-
ciation for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proceedings of Human Language Tech-
nologies: North American Chapter of the Association
for Computational Linguistics-09, pages 548?556. As-
sociation for Computational Linguistics.
Adria` de Gispert, Juan Pino, and William Byrne. 2010.
Hierarchical phrase-based translation grammars ex-
tracted from alignment posterior probabilities. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 545?554.
Association for Computational Linguistics.
John DeNero, Alexandre Bouchard-Cote, and Klein Dan.
2008. Sampling alignment structure under a bayesian
translation model. In In Proceedings of Empirical
Methods in Natural Language Processing-08, pages
314?323. Association for Computational Linguistics.
Zhongjun He, Yao Meng, and Hao Yu. 2009. Discarding
monotone composed rule for hierarchical phrase-based
statistical machine translation. In Proceedings of the
3rd International Universal Communication Sympo-
sium, pages 25?29. ACM.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151.
Association for Computational Linguistics.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern for
efficient hierarchical translation. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), pages 380?388. Association for Com-
putational Linguistics.
540
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In In Proceedings of Empirical Methods in Natu-
ral Language Processing-02, pages 133?139. Associ-
ation for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In In Proceedings of
Association of Computational Linguistics, pages 311?
318. Association for Computational Linguistics.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING) - Volume 1, pages 1081?1088. As-
sociation for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING) - Vol-
ume 1, pages 1145?1152. Association for Computa-
tional Linguistics.
541
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 356?361,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Kriya - The SFU System for Translation Task at WMT-12
Majid Razmara and Baskaran Sankaran and Ann Clifton and Anoop Sarkar
School of Computing Science
Simon Fraser University
8888 University Drive
Burnaby BC. V5A 1S6. Canada
{razmara, baskaran, aca69, anoop}@cs.sfu.ca
Abstract
This paper describes our submissions for the
WMT-12 translation task using Kriya - our hi-
erarchical phrase-based system. We submitted
systems in French-English and English-Czech
language pairs. In addition to the baseline sys-
tem following the standard MT pipeline, we
tried ensemble decoding for French-English.
The ensemble decoding method improved the
BLEU score by 0.4 points over the baseline
in newstest-2011. For English-Czech, we seg-
mented the Czech side of the corpora and
trained two different segmented models in ad-
dition to our baseline system.
1 Baseline Systems
Our shared task submissions are trained in the hier-
archical phrase-based model (Chiang, 2007) frame-
work. Specifically, we use Kriya (Sankaran et al,
2012) - our in-house Hiero-style system for training
and decoding. We now briefly explain the baseline
systems in French-English and English-Czech lan-
guage pairs.
We use GIZA++ for word alignments and the
Moses (Koehn et al, 2007) phrase-extractor for ex-
tracting the initial phrases. The translation models
are trained using the rule extraction module in Kriya.
In both cases, we pre-processed the training data by
running it through the usual pre-processing pipeline
of tokenization and lowercasing.
For French-English baseline system, we trained
a simplified hierarchical phrase-based model where
the right-hand side can have at most one non-
terminal (denoted as 1NT) instead of the usual two
non-terminal (2NT) model. In our earlier experi-
ments we found the 1NT model to perform com-
parably to the 2NT model for close language pairs
such as French-English (Sankaran et al, 2012) at the
same time resulting in a smaller model. We used the
shared-task training data consisting of Europarl (v7),
News commentary and UN documents for training
the translation models having a total of 15 M sen-
tence pairs (we did not use the Fr-En Giga paral-
lel corpus for the training). We trained a 5-gram
language model for English using the English Gi-
gaword (v4).
For English-Czech, we trained a standard Hiero
model that has up to two non-terminals on the right-
hand side. We used the Europarl (v7), news com-
mentary and CzEng (v0.9) corpora having 7.95M
sentence pairs for training translation models. We
trained a 5-gram language model using the Czech
side of the parallel corpora and did not use the Czech
monolingual corpus.
The baseline systems use the following 8 stan-
dard Hiero features: rule probabilities p(e|f) and
p(f |e); lexical weights pl(e|f) and pl(f |e); word
penalty, phrase penalty, language model and glue
rule penalty.
1.1 LM Integration in Kriya
The kriya decoder is based on a modified CYK al-
gorithm similar to that of Chiang (2007). We use
a novel approach in computing the language model
(LM) scores in Kriya, which deserves a mention
here.
The CKY decoder in Hiero-style systems can
freely combine target hypotheses generated in inter-
356
mediate cells with hierarchical rules in the higher
cells. Thus the generation of the target hypotheses
are fragmented and out of order in Hiero, compared
to the left to right order preferred by n-gram lan-
guage models.
This leads to challenges in estimating LM scores
for partial target hypotheses and this is typically ad-
dressed by adding a sentence initial marker (<s>)
to the beginning of each derivation path.1 Thus the
language model scores for the hypothesis in the in-
termediate cell are approximated, with the true lan-
guage model score (taking into account sentence
boundaries) being computed in the last cell that
spans the entire source sentence.
Kriya uses a novel idea for computing LM scores:
for each of the target hypothesis fragment, it finds
the best position for the fragment in the final sen-
tence and uses the corresponding score. Specifi-
cally, we compute three different scores correspond-
ing to the three states where the fragment can end
up in the final sentence, viz. sentence initial, middle
and final and choose the best score. Thus given a
fragment tf consisting of a sequence of target to-
kens, we compute LM scores for (i) <s> tf , (ii)
tf and (iii) tf </s> and use the best score (only)
for pruning.2 While this increases the number of
LM queries, we exploit the language model state in-
formation in KenLM (Heafield, 2011) to optimize
the queries by saving the scores for the unchanged
states. Our earlier experiments showed significant
reduction in search errors due to this approach, in
addition to a small but consistent increase in BLEU
score (Sankaran et al, 2012).
2 French-English System
In addition to the baseline system, we also trained
separate systems for News and Non-News genres
for applying ensemble decoding (Razmara et al,
2012). The news genre system was trained only us-
ing the news-commentary corpus (about 137K sen-
1Alternately systems add sentence boundary markers (<s>
and </s>) to the training data so that they are explicitly present
in the translation and language models. While this can speed
up the decoding as the cube pruning is more aggressive, it also
limits the applicability of rules having the boundary contexts.
2This ensures the the LM score estimates are never underes-
timated for pruning. We retain the LM score for fragment (case
ii) for estimating the score for the full candidate sentence later.
tence pairs) and the non-news genre system was
trained on the Europarl and UN documents data
(14.8M sentence pairs). The ensemble decoding
framework combines the models of these two sys-
tems dynamically when decoding the testset. The
idea is to effectively use the small amount of news
genre data in order to maximize the performance on
the news-based testsets. In the following sections,
we explain in broader detail how this system combi-
nation technique works as well as the details of this
experiment and the evaluation results.
2.1 Ensemble Decoding
In the ensemble decoding framework we view trans-
lation task as a domain mixing problem involving
news and non-news genres. The official training
data is from two major sources: news-commentary
data and Europarl/UN data and we hope to exploit
the distinctive nature of the two genres. Given that
the news data is smaller comparing to parliamen-
tary proceedings data, we could tune the ensemble
decoding to appropriately boost the weight for the
news genre mode during decoding. The ensemble
decoding approach (Razmara et al, 2012) takes ad-
vantage of multiple translation models with the goal
of constructing a system that outperforms all the
component models. The key strength of this system
combination method is that the systems are com-
bined dynamically at decode time. This enables the
decoder to pick the best hypotheses for each span of
the input.
In ensemble decoding, given a number of transla-
tion systems which are already trained and tuned, all
of the hypotheses from component models are used
in order to translate a sentence. The scores of such
rules are combined in the decoder (i.e. CKY) using
various mixture operations to assign a single score to
them. Depending on the mixture operation used for
combining the scores, we would get different mix-
ture scores.
Ensemble decoding extends the log-linear frame-
work which is found in state-of-the-art machine
translation systems. Specifically, the probability of
a phrase-pair (e?, f?) in the ensemble model is:
p(e? | f?) ? exp
(
w1 ? ?1? ?? ?
1st model
? w2 ? ?2? ?? ?
2nd model
? ? ? ?
)
357
where? denotes the mixture operation between two
or more model scores.
Mixture operations receive two or more scores
(probabilities) and return the mixture score (prob-
ability). In this section, we explore different options
for this mixture operation.
Weighted Sum (wsum): in wsum the ensemble
probability is proportional to the weighted sum
of all individual model probabilities.
p(e? | f?) ?
M?
m
?m exp
(
wm ? ?m
)
where m denotes the index of component mod-
els, M is the total number of them and ?i is the
weight for component i.
Weighted Max (wmax): where the ensemble score
is the weighted max of all model scores.
p(e? | f?) ? max
m
(
?m exp
(
wm ? ?m
))
Product (prod): in prod, the probability of the en-
semble model or a rule is computed as the prod-
uct of the probabilities of all components (or
equally the sum of log-probabilities). When
using this mixture operation, ensemble de-
coding would be a generalization of the log-
linear framework over multiple models. Prod-
uct models can also make use of weights to
control the contribution of each component.
These models are generally known as Logarith-
mic Opinion Pools (LOPs) where:
p(e? | f?) ? exp
(
M?
m
?m wm ? ?m
)
Model Switching: in model switching, each cell in
the CKY chart gets populated only by rules
from one of the models and the other mod-
els? rules are discarded. This is based on the
hypothesis that each component model is an
expert on different parts of sentence. In this
method, we need to define a binary indicator
function ?(f? ,m) for each span and component
model.
?(f? ,m) =
?
?
?
1, m = argmax
n?M
?(f? , n)
0, otherwise
The criteria for choosing a model for each cell,
?(f? , n), could be based on:
Max: for each cell, the model that has the
highest weighted top-rule score wins:
?(f? , n) = ?n max
e
(wn ? ?n(e?, f?))
Sum: Instead of comparing only the score of
the top rules, the model with the high-
est weighted sum of the probability of
the rules wins (taking into account the
ttl(translation table limit) limit on the
number of rules suggested by each model
for each cell):
?(f? , n) = ?n
?
e?
exp
(
wn ? ?n(e?, f?)
)
The probability of each phrase-pair (e?, f?) is
computed as:
p(e? | f?) =
?
m
?(f? ,m) pm(e? | f?)
Since log-linear models usually look for the best
derivation, they do not need to normalize the scores
to form probabilities. Therefore, the scores that dif-
ferent models assign to each phrase-pair may not be
in the same scale. Therefore, mixing their scores
might wash out the information in one (or some)
of the models. We applied a heuristic to deal with
this problem where the scores are normalized over
a shorter list. So the list of rules coming from each
model for a certain cell in the CKY chart is normal-
ized before getting mixed with other phrase-table
rules. However, experiments showed using normal-
ized scores hurts the BLEU score radically. So we
use the normalized scores only for pruning and for
mixing the actual scores are used.
As a more principled way, we used a toolkit,
CONDOR (Vanden Berghen and Bersini, 2005), to
optimize the weights of our component models on
a dev-set. CONDOR, which is publicly available, is
a direct optimizer based on Powell?s algorithm that
does not require explicit gradient information for the
objective function.
2.2 Experiments and Results
As mentioned earlier all the experiments reported
for French-English use a simpler Hiero translation
358
Method Devset Test-11 Test-12
Baseline Hiero 26.03 27.63 28.15
News data 24.02 26.47 26.27
Non-news data 26.09 27.87 28.15
Ensemble PROD 25.66 28.25 28.09
Table 1: French-English BLEU scores. Best performing
setting is shown in Boldface.
model having at most one non-terminal (1NT) on the
right-hand side. We use 7567 sentence pairs from
news-tests 2008 through 2010 for tuning and use
news-test 2011 for testing in addition to the 2012
test data. The feature weights were tuned using
MERT (Och, 2003) and we report the devset (IBM)
BLEU scores and the testset BLEU scores computed
using the official evaluation script (mteval-v11b.pl).
The results for the French-English experiments
are reported in Table 1. We note that both baseline
Hiero model and the model trained from the non-
news genre get comparable BLEU scores. The news
genre model however gets a lesser BLEU score and
this is to be expected due to the very small training
data available for this genre.
Table 2 shows the results of applying various mix-
ture operations on the devset and testset, both in nor-
malized (denoted by Norm.) and un-normalized set-
tings (denoted by Base). We present results for these
mixture operations using uniform weights (i.e. un-
tuned weights) and for PROD we also present the
results using the weights optimized by CONDOR.
Most of the mixture operations outperform the Test-
11 BLEU of the baseline models (shown in Table 1)
even with uniform (untuned) weights. We took the
best performing operation (i.e. PROD) and tuned its
component weights using our optimizer which lead
to 0.26 points improvement over its uniform-weight
version.
The last row in Table 1 reports the BLEU score
for this mixture operation with the tuned weights
on the Test-12 dataset and it is marginally less than
the baseline model. While this is disappointing, this
also runs counter to our empirical results from other
datasets. We are currently investigating this aspect
as we hope to improve the robustness and applicabil-
ity of our ensemble approach for different datasets
and language pairs.
Mix. Operation Weights Base Norm.
WMAX uniform 27.67 27.94
WSUM uniform 27.72 27.95
SWITCHMAX uniform 27.96 26.21
SWITCHSUM uniform 27.98 27.98
PROD uniform 27.99 28.09
PROD optimized 28.25 28.11
Table 2: Applying ensemble decoding with different mix-
ture operations on the Test-11 dataset. Best performing
setting is shown in Boldface.
3 English-Czech System
3.1 Morpheme Segmented Model
For English-Czech, we additionally experimented
using morphologically segmented versions of the
Czech side of the parallel data, since previous
work (Clifton and Sarkar, 2011) has shown that seg-
mentation of morphologically rich languages can
aid translation. To derive the segmentation, we
built an unsupervised morphological segmentation
model using the Morfessor toolkit (Creutz and La-
gus, 2007).
Morfessor uses minimum description length cri-
teria to train a HMM-based segmentation model.
Varying the perplexity threshold in Morfessor does
not segment more word types, but rather over-
segments the same word types. We hand tuned the
model parameters over training data size and per-
plexity; these control the granularity and coverage of
the segmentations. Specifically, we trained different
segmenter models on varying sets of most frequent
words and different perplexities and identified two
sets that performed best based on a separate held-
out set. These two sets correspond to 500k most fre-
quent words and a perplexity of 50 (denoted SM1)
and 10k most frequent words and a perplexity of 20
(denoted SM2). We then used these two models to
segment the entire data set and generate two differ-
ent segmented training sets. These models had the
best combination of segmentation coverage of the
training data and largest segments, since we found
empirically that smaller segments were less mean-
ingful in the translation model. The SM2 segmenta-
tion segmented more words than SM1, but more fre-
quently segmented words into single-character units.
359
For example, the Czech word ?dlaebn??? is broken
into the useful components ?dlaeb + n??? by SM1, but
is oversegmented into ?dl + a + e + b + n??? by SM2.
However, SM1 fails to find a segmentation at all for
the related word ?dlaebn??mi?, while SM2 breaks it
up similiarly with an additional suffix: ?dl + a + e +
b + n?? + mi?.
With these segmentation models, we segmented
the target side of the training and dev data before
training the translation model. Similarly, we also
train segmented language models corresponding to
the two sets SM1 and SM2. The MERT tuning step
uses the segmented dev-set reference to evaluate the
segmented hypotheses generated by the decoder for
optimizing the weights for the BLEU score. How-
ever for evaluating the test-set, we stitched the seg-
ments in the decoder output back into unsegmented
forms in a post-processing step, before performing
evaluation against the original unsegmented refer-
ences. The hypotheses generated by the decoder
can have incomplete dangling segments where one
or more prefixes and/or suffixes are missing. While
these dangling segments could be handled in a dif-
ferent way, we use a simple heuristic of ignoring the
segment marker ?+? by just removing the segment
marker. In next section, we report the results of us-
ing the unsegmented model as well as its segmented
counterparts.
3.2 Experiments and Results
In the English-Czech experiments, we used the same
datasets for the dev and test sets as in French-
English experiments (dev: news-tests 2008, 2009,
2010 with 7567 sentence pairs and test: news-
test2011 with 3003 sentence pairs). Similarly,
MERT (Och, 2003) has been used to tune the feature
weights and we report the BLEU scores of two test-
sets computed using the official evaluation script
(mteval-v11b.pl).
Table 3.2 shows the results of different segmenta-
tion schemes on the WMT-11 and WMT-12 test-sets.
SM1 slightly outperformed the other two models in
Test-11, however the unsegmented model performed
best in Test-12, though marginally. We are currently
investigating this and are also considering the pos-
sibility employing the idea of morpheme prediction
in the post-decoding step in combination with this
morpheme-based translation as suggested by Clifton
Segmentation Test-11 Test-12
Baseline Hiero 14.65 12.40
SM1 : 500k-ppl50 14.75 12.34
SM2 : 10k-ppl20 14.57 12.34
Table 3: The English-Czech results for different segmen-
tation settings. Best performing setting is shown in Bold-
face.
and Sarkar (2011).
4 Conclusion
We submitted systems in two language pairs French-
English and English-Czech for WMT-12 shared
task. In French-English, we experimented the en-
semble decoding framework that effectively utilizes
the small amount of news genre data to improve the
performance in the testset belonging to the same
genre. We obtained a moderate gain of 0.4 BLEU
points with the ensemble decoding over the baseline
system in newstest-2011. For newstest-2012, it per-
forms comparably to that of the baseline and we are
presently investigating the lack of improvement in
newstest-2012. For Cz-En, We found that the BLEU
scores do not substantially differ from each other
and also the minor differences are not consistent for
Test-11 and Test-12.
References
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, pages 32?42.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1):3:1?3:34, February.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
360
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177?
180. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of Association of Computational Lin-
guistics, pages 160?167.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics, Jeju, Republic of Korea,
July. Association for Computational Linguistics. To
appear.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics, 97(97):83?98, April.
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of pow-
ell?s UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157?175,
September.
361
