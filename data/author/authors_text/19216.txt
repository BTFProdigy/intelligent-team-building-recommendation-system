Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1783?1792, Dublin, Ireland, August 23-29 2014.
Adapting taggers to Twitter with not-so-distant supervision
Barbara Plank
1
, Dirk Hovy
1
, Ryan McDonald
2
and Anders S?gaard
1
Center for Language Technology, University of Copenhagen
1
Google Inc.
2
{bplank,dirkh}@cst.dk,ryanmcd@google.com,soegaard@hum.ku.dk
Abstract
We experiment with using different sources of distant supervision to guide unsupervised and
semi-supervised adaptation of part-of-speech (POS) and named entity taggers (NER) to Twitter.
We show that a particularly good source of not-so-distant supervision is linked websites. Specif-
ically, with this source of supervision we are able to improve over the state-of-the-art for Twitter
POS tagging (89.76% accuracy, 8% error reduction) and NER (F1=79.4%, 10% error reduction).
1 Introduction
Twitter contains a vast amount of information, including first stories and breaking news (Petrovic et al.,
2010), fingerprints of public opinions (Jiang et al., 2011) and recommendations of relevance to poten-
tially very small target groups (Benson et al., 2011). In order to automatically extract this information,
we need to be able to analyze tweets, e.g., determine the part-of-speech (POS) of words and recognize
named entities. Tweets, however, are notoriously hard to analyze (Foster et al., 2011; Eisenstein, 2013;
Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions
for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, frag-
mented or mixed language, etc.
Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample
Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts
of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski
et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective
samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing
even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due
to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems
perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and
suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by learning
from additional unlabeled tweets. This is the hypothesis we explore in this paper.
We present a semi-supervised learning method that does not require additional labeled in-domain data
to correct sample bias, but rather leverages pools of unlabeled Twitter data. However, since taggers
trained on newswire perform poorly on Twitter data, we need additional guidance when utilizing the
unlabeled data. This paper proposes distant supervision to help our models learn from unlabeled data.
Distant supervision is a weakly supervised learning paradigm, where a knowledge resource is exploited
to gather (possible noisy) training instances (Mintz et al., 2009). Our basic idea is to can use linguistic
analysis of linked websites as a novel kind of distant supervision for learning how to analyze tweets. We
explore standard sources of distant supervision, such as Wiktionary for POS tagging, but we also propose
to use the linked websites of tweets with URLs as supervision. The intuition is that we can use websites
to provide a richer linguistic context for our tagging decisions. We exploit the fact that tweets with URLs
provide a one-to-one map between an unlabeled instance and the source of supervision, making this
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1783
1: X = {?x
i
, y
i
?}
N
i=1
labeled tweets
2: U = {?x
i
, w
i
?}
M
i=1
unlabeled tweet-website pairs
3: I iterations
4: k = 1000 pool size
5: v=train(X) base model
6: for i ? I do
7: for ?x, w? ? pool
k
(U) do
8: y?=predict(?x, w?;v)
9: X ? X ? {?y?,x?}
10: end for
11: v=train(X)
12: end for
13: return v
Figure 1: Semi-supervised learning with not-so-distant supervision, i.e. tweet-website pairs {?x
i
, w
i
?}.
SELF-TRAINING, WEB, DICT, DICT?WEB and WEB?DICT differ only in how predict() (line 8) is
implemented (cf. Section 2).
less distant supervision. Note that we use linked websites only for semi-supervised learning, but do not
require them at test time.
Our semi-supervised learning method enables us to learn POS tagging and NER models that perform
more robustly across different samples of tweets than existing approaches. We consider both the scenario
where a small sample of labeled Twitter data is available, and the scenario where only newswire data is
available. Training on a mixture of out-of-domain (WSJ) and in-domain (Twitter) data as well as unla-
beled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our
tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd
2 Tagging with not-so-distant supervision
We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), popula-
tion drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use
unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training
sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper
presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and
not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging deci-
sions in tweets on a richer linguistic context than what is available in the tweets. This semi-supervised
approach gives state-of-the-art performance across available Twitter POS and NER data sets.
The overall semi-supervised learning algorithm is presented in Figure 1. The aim is to correct model
bias by predicting tag sequences on small pools of unlabeled tweets, and re-training the model across
several iterations to gradually correct model bias. Since information from hyperlinks will be important,
the unlabeled data U is a corpus of tweets containing URLs. We present a baseline and four system
proposals that only differ in their treatment of the predict() function.
In the SELF-TRAINING baseline, predict() corresponds to standard Viterbi inference on the unlabeled
Twitter data. This means, the current model v is applied to the tweets by disregarding the websites in
the tweet-website pairs, i.e., tagging x without considering w. Then the automatically tagged tweets are
added to the current pool of labeled data and the procedure is iterated (line 7-11 in Figure 1).
In the WEB method, we additionally use the information from the websites. The current model v
is used to predict tags for the pooled tweets and the website they linked to. For all the words that
occur both in a tweet and on the corresponding website, we then project the tag most frequently
assigned to those words on the website to their occurrences in the tweet. This enables us to basically
condition the tag decision for each such word on its accumulated context on the website. The assumption
of course being that the word in the tweet has the part-of-speech it most often has on the website linked to.
1784
Example Here is an example of a tweet that contains a URL:
(1) #Localization #job: Supplier / Project Manager - Localisation Vendor - NY, NY, United States
http://bit.ly/16KigBg #nlppeople
The words in the tweet are all common words, but they occur without linguistic context that could
help a tagging model to infer whether these words are nouns, verbs, named entities, etc. However, on the
website that the tweet refers to, all of these words occur in context:
(2) The Supplier/Project Manager performs the selection and maintenance . . .
For illustration, the Urbana-Champaign POS tagger
1
incorrectly tags Supplier in (1) as an adjective.
In (2), however, it gets the same word right and tags it as a noun. The tagging of (2) could potentially
help us infer that Supplier is also a noun in (1).
Obviously, the superimposition of tags in the WEB method may change the tag of a tweet word such
that it results in an unlikely tag sequence, as we will discuss later. Therefore we also implemented
type-constrained decoding (T?ackstr?om et al., 2013), i.e., prune the lattice such that the tweet words ob-
served on the website have one of the tags they were labeled with on the website (soft constraints), or,
alternatively, were forced during decoding to have the most frequent tags they were labeled with (hard
constraint decoding), thereby focusing on licensed sequences. However, none of these approaches per-
formed significantly better than the simple WEB approach on held-out data. This suggests that sequential
dependencies are less important for tagging Twitter data, which is of rather fragmented nature. Also, the
WEB approach allows us to override transitional probabilities that are biased by the observations we
made about the distribution of tags in our out-of-domain data.
Furthermore, we combine the not-so-distant supervision from linked websites (WEB) with supervision
from dictionaries (DICT). The idea here is to exploit the fact that many word types in a dictionary are
actually unambiguous, i.e., contain only a single tag. In particular, 93% of the word types in Wiktionary
2
are unambiguous. Wiktionary is a crowdsourced tag dictionary that has previously been used for mini-
mally supervised POS tagging (Li et al., 2012; T?ackstr?om et al., 2013). In the case of NER, we use a
gazetteer that combines information on PER, LOC and ORG from the KnownLists of the Illinois tagger.
3
For this gazetteer, 79% of the word types contained only a single named entity tag.
We experiment with a model that uses the dictionary only (DICT) and two ways to combine the two
sources. In the former setup, the current model is first applied to tag the tweets, then any token that
appears in the dictionary and is unambiguous is projected back to the tweet. The next two methods are
combinations of WEB and DICT: either first project the predicted tags from the website and then, in case
of conflicts, overrule predictions by the dictionary (WEB?DICT), or the other way around (DICT?WEB).
The intuition behind the idea of using linked websites as not-so-distant supervision is that while tweets
are hard to analyze (even for humans) because of the limited context available in 140 character messages,
tweets relate to real-world events, and Twitter users often use hyperlinks to websites to indicate what
real-world events their comments address. In fact, we observed that about 20% of tweets contain URLs.
The websites they link to are often newswire sites that provide more context and are written in a more
canonical language, and are therefore easier to process. Our analysis of the websites can then potentially
inform our analysis of the tweets. The tweets with the improved analyses can then be used to bootstrap
our tagging models using a self-training mechanism. Note that our method does not require tweets to
contain URLs at test time, but rather uses unlabeled tweets with URLs during training to build better
tagging models for tweets in general. At test time, these models can be applied to any tweet.
1
http://cogcomp.cs.illinois.edu/demo/pos/
2
http://en.wiktionary.org/ - We used the Wiktionary version derived by Li et al. (2012).
3
http://cogcomp.cs.illinois.edu/page/software_view/NETagger
1785
3 Experiments
3.1 Model
In our experiments we use a publicly available implementation of conditional random fields (CRF) (Laf-
ferty et al., 2001).
4
We use the features proposed by Gimpel et al. (2011), in particular features for word
tokens, a set of features that check for the presence of hyphens, digits, single quotes, upper/lowercase,
3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2
i
for
i ? 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is pub-
licly available.
5
We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000}
showing similar performance. The number of iterations i is set on the development data.
For NER on websites, we use the Stanford NER system (Finkel et al., 2005)
6
with POS tags from the
LAPOS tagger (Tsuruoka et al., 2011).
7
For POS we found it to be superior to use the current POS model
for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line
NER tagging rather than retagging the websites in every iteration.
3.2 Data
In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semi-
supervised domain adaptation (DA), respectively (Daum?e et al., 2010; Plank, 2011). In unsupervised
DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from both
domains, besides unlabeled target data, but the amount of labeled target data is much smaller than the
labeled source data. Most annotated corpora for English are newswire corpora. Some annotated Twitter
data sets have been made available recently, described next.
POS NER
train
WSJ (700k) REUTER-CONLL (Tjong Kim Sang and De Meulder, 2003) (200k)
GIMPEL-TRAIN (Owoputi et al., 2013) (14k) FININ-TRAIN (Finin et al., 2010) (170k)
dev
FOSTER-DEV (Foster et al., 2011) (3k) n/a
RITTER-DEV (Ritter et al., 2011) (2k) n/a
test
FOSTER-TEST (Foster et al., 2011) (2.8k) RITTER-TEST (Ritter et al., 2011) (46k)
GIMPEL-TEST (Gimpel et al., 2011) (7k) FININ-TEST (Finin et al., 2010) (51k)
HOVY-TEST (Hovy et al., 2014) FROMREIDE-TEST (Fromreide et al., 2014) (20k)
Table 1: Overview of data sets. Number in parenthesis: size in number of tokens.
Training data. An overview of the different data sets is given in Table 3.2. In our experiments, we
use the SANCL shared task
8
splits of the OntoNotes 4.0 distribution of the WSJ newswire annotations
as newswire training data for POS tagging.
9
For NER, we use the CoNLL 2003 data sets of annotated
newswire from the Reuters corpus.
10
The in-domain training POS data comes from Gimpel et al. (2011),
and the in-domain NER data comes from Finin et al. (2010) (FININ-TRAIN). These data sets are added
to the newswire sets when doing semi-supervised DA. Note that for NER, we thus do not rely on expert-
annotated Twitter data, but rely on crowdsourced annotations. We use MACE
11
(Hovy et al., 2013) to
resolve inter-annotator conflicts between turkers (50 iterations, 10 restarts, no confidence threshold). We
believe relying on crowdsourced annotations makes our set-up more robust across different samples of
Twitter data.
Development and test data. We use several evaluation sets for both tasks to prevent overfitting to a
specific sample. We use the (out-of-sample) development data sets from Ritter et al. (2011) and Foster
4
http://www.chokkan.org/software/crfsuite/
5
http://www.ark.cs.cmu.edu/TweetNLP/
6
http://http://nlp.stanford.edu/software/CRF-NER.shtml
7
http://www.logos.ic.i.u-tokyo.ac.jp/
?
tsuruoka/lapos/
8
https://sites.google.com/site/sancl2012/home/shared-task
9
LDC2011T03.
10
http://www.clips.ua.ac.be/conll2003/ner/
11
http://www.isi.edu/publications/licensed-sw/mace/
1786
et al. (2011). For NER, we simply use the parameters from our POS tagging experiments and thus do
not assume to have access to further development data. For both POS tagging and NER, we have three
test sets. For POS tagging, the ones used in Foster et al. (2011) (FOSTER-TEST) and Ritter et al. (2011)
(RITTER-TEST),
12
as well as the one presented in Hovy et al. (2014) (HOVY-TEST). For NER, we use
the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets.
One is a manual correction of a held-out portion of FININ-TRAIN, named FININ-TEST; the other one
is referred to as FROMREIDE-TEST. Since the different POS corpora use different tag sets, we map all
of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a
few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire
tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong.
Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a
few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The
major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We
follow Finin et al. (2010) in doing so.
Unlabeled data We downloaded 200k tweet-website pairs from the Twitter search API over a period
of one week in August 2013 by searching for tweets that contain the string http and downloading the
content of the websites they linked to. We filter out duplicate tweets and restrict ourselves to websites
that contain more than one sentence (after removing boilerplate text, scripts, HTML, etc).
13
We also
require website and tweet to have at least one matching word that is not a stopword (as defined by the
NLTK stopword list).
14
Finally we restrict ourselves to pairs where the website is a subsite, because
website head pages tend to contain mixed content that is constantly updated. The resulting files are all
tokenized using the Twokenize tool.
15
Tweets were treated as one sentence, similar to the approaches in
Gimpel et al. (2011) and Owoputi et al. (2013); websites were processed by applying the Moses sentence
splitter.
16
The out-of-vocabulary (OOV) rates in Figure 2 show that in-domain training data reduces the number
of unseen words considerably, especially in the NER data sets. They also suggest that some evaluation
data sets share more vocabulary with our training data than others. In particular, we would expect better
performance on FOSTER-TEST than on RITTER-TEST and HOVY-TEST in POS tagging, as well as better
performance on FININ-TEST than on the other two NER test sets. In POS tagging, we actually do see
better results with FOSTER-TEST across the board, but in NER, FININ-TEST actually turns out to be the
hardest data set.
4 Results
4.1 POS results
Baselines We use three supervised CRF models as baselines (cf. the first part of Table 2). The first
supervised model is trained only on WSJ. This model does very well on FOSTER-DEV and FOSTER-
TEST, presumably because of the low OOV rates (Figure 2). The second supervised model is trained
only on GIMPEL-TRAIN; the third on the concatenation of WSJ and GIMPEL-TRAIN. While the second
baseline performs well on held-out data from its own sample (90.3% on GIMPEL-DEV), it performs
poorly across our out-of-sample test and development sets. Thus, it seems to overfit the sample of
tweets described in Gimpel et al. (2011). The third model trained on the concatenation of WSJ and
GIMPEL-TRAIN achieves the overall best baseline performance (88.4% macro-average accuracy). We
note that this is around one percentage point better than the best available off-the-shelf system for Twitter
(Owoputi et al., 2013) with an average accuracy of 87.5%.
12
Actually (Ritter et al., 2011) do cross-validation over this data, but we use the splits of Derczynski et al. (2013) for POS.
13
Using https://github.com/miso-belica/jusText
14
ftp://ftp.cs.cornell.edu/pub/smart/english.stop
15
https://github.com/brendano/ark-tweet-nlp
16
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ems/support/
split-sentences.perl
1787
Figure 2: Test set (type-level) OOV rates for POS (left) and NER (right).
l l
l
l l l
l l l l l l l
l l l l l l l l l l l l l l l l l l l l l l
0 5 10 15 20 25 30
88.5
89.0
89.5
90.0
DEV?avg wsj
iteration
accu
racy
l self?trainingWebDictWeb<DictDict<Web
l l
l l l
l l l l l l l
l l l l l l l l l l l l
l l
l l l l
0 5 10 15 20 25 30
88.8
89.0
89.2
89.4
89.6
89.8
90.0
90.2
DEV?avg wsj+gimpel
iteration
accu
racy
Figure 3: Learning curves on DEV-avg for systems trained on WSJ (left) and WSJ+GIMPEL (right) used
to set the hyperparameter i.
Learning with URLs The results of our approaches are presented in Table 2. The hyperparameter i
was set on the development data (cf. Figure 3). Note, again, that they do not require the test data to
contain URLs. First of all, naive self-training does not work: accuracy declines or is just around baseline
performance (Table 2 and Figure 3). In contrast, our augmented self-training methods with WEB or
DICT reach large improvements. In case we assume no target training data (train on WSJ only, i.e.
unsupervised DA), we obtain improvements of up to 9.1% error reduction. Overall the system improves
from 88.42% to 89.07%. This also holds for the second scenario, i.e. training on WSJ+GIMPEL-TRAIN
(semi-supervised DA, i.e., the case where we have some labeled target data, besides the pool of unlabeled
tweets) where we reach error reductions of up to 10%. Our technique, in other words, improves the
robustness of taggers, leading to much better performance on new samples of tweets.
4.2 NER results
For our NER results, cf. Table 3, we used the same feature models and parameter settings as those used for
POS tagging, except conditioning also on POS information. It is conceivable that other parameter settings
would have led to better results, but we did not want to assume the existence of in-domain development
data for this task. Our baselines are again supervised systems, as well as off-the-shelf systems. Our in-
1788
DEV-avg TEST TEST-avg
FOSTER HOVY RITTER
Baselines trained on
WSJ 88.82 91.87 87.01 86.38 88.42
GIMPEL-TRAIN 83.32 84.86 86.03 81.67 84.19
WSJ+GIMPEL-TRAIN 89.07 91.59 87.50 87.39 88.83
Systems trained on WSJ
SELF-TRAINING i = 25 85.52 91.80 86.72 85.90 88.14
DICT i = 25 85.61 92.08 87.63 85.68 88.46
WEB i = 25 85.27 92.47 87.30 86.60 88.79
DICT?WEB i = 25 86.11 92.61 87.70 86.69 89.00
WEB?DICT i = 25 86.15 92.57 88.12 86.51 89.07
max err.red 4.7% 9.1% 8.6% 2.3% 4.2%
Systems trained on WSJ+GIMPEL-TRAIN
SELF-TRAINING i = 27 89.12 91.83 86.88 87.43 88.71
DICT i = 27 89.43 92.22 88.38 87.69 89.43
WEB i = 27 89.82 92.43 87.43 88.21 89.36
DICT?WEB i = 27 90.04 92.43 88.38 88.48 89.76
WEB?DICT i = 27 90.04 92.40 87.99 88.39 89.59
max err.red 8.9% 10% 7.1% 8.6% 8.4%
Table 2: POS results.
house supervised baselines perform better than the available off-the-shelf systems, including the system
provided by Ritter et al. (2011) (TEST-avg of 54.2%). We report micro-average F
1
-scores over entity
types, computed using the publicly available evaluation script.
17
Our approaches again lead to substantial
error reductions of 8?13% across our NER evaluation data sets.
TEST TEST-avg
RITTER FROMREIDE FININ
Baseline trained on
CONLL+FININ-TRAIN 77.44 82.13 74.02 77.86
Systems trained on CONLL+FININ-TRAIN
SELF-TRAINING i = 27 78.63 82.88 74.89 78.80
DICT i = 27 65.24 69.1 65.45 66.60
WEB i = 27 78.29 83.82 74.99 79.03
DICT?WEB i = 27 78.53 83.91 75.83 79.42
WEB?DICT i = 27 65.97 69.92 65.86 67.25
err.red 9.1% 13.3% 8.0% 9.8%
Table 3: NER results.
5 Error analysis
The majority of cases where our taggers improve on the ARK tagger (Owoputi et al., 2013) seem to
relate to richer linguistic context. The ARK tagger incorrectly tags the sequence Man Utd as PRT-
NOUN, whereas our taggers correctly predict NOUN-NOUN. In a similar vein, our taggers correctly
predict the tag sequence NOUN-NOUN for Radio Edit, while the ARK tagger predicts NOUN-VERB.
However, some differences seem arbitrary. For example, the ARK tagger tags the sequence Nokia
17
http://www.cnts.ua.ac.be/conll2000/chunking/
1789
D5000 in FOSTER-TEST as NOUN-NUM. Our systems correctly predict NOUN-NOUN, but it is not
clear which analysis is better in linguistic terms. Our systems predict a sequence such as Love his version
to be VERB-PRON-NOUN, whereas the ARK tagger predicts VERB-DET-NOUN. Both choices seem
linguistically motivated.
Finally, some errors are made by all systems. For example, the word please in please, do that, for
example, is tagged as VERB by all systems. In FOSTER-TEST, this is annotated as X (which in the PTB
style was tagged as interjection UH). Obviously, please often acts as a verb, and while its part-of-speech
in this case may be debatable, we see please annotated as a verb in similar contexts in the PTB, e.g.:
(3) Please/VERB make/VERB me/PRON . . .
It is interesting to look at the tags that are projected from the websites to the tweets. Several of the
observed projections support the intuition that coupling tweets and the websites they link to enables us
to condition our tagging decisions on a richer linguistic context. Consider, for example Salmon-Safe,
initially predicted to be a NOUN, but after projection correctly analyzed as an ADJ:
Word Context Initial tag Projected tag
Salmon-Safe . . . parks NOUN ADJ
Snohomish . . . Bakery ADJ NOUN
toxic ppl r . . . NOUN ADJ
One of the most frequent projections is analyzing you?re, correctly, as a VERB rather than an ADV (if
the string is not split by tokenization).
One obvious limitation of the WEB-based models is that the projections apply to all occurrences of a
word. In rare cases, some words occur with different parts of speech in a single tweet, e.g., wish in:
(4) If I gave you one wish that will become true . What?s your wish ?... ? i wish i?ll get <num> wishes
from you :p <url>
In this case, our models enforce all occurrences of wish to, incorrectly, be verbs.
6 Related work
Previous work on tagging tweets has assumed labeled training data (Ritter et al., 2011; Gimpel et al.,
2011; Owoputi et al., 2013; Derczynski et al., 2013). Strictly supervised approaches to analyzing Twitter
has the weakness that labeled data quickly becomes unrepresentative of what people write on Twitter.
This paper presents results using no in-domain labeled data that are significantly better than several off-
the-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data
to reach new highs across several data sets.
Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings
(Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T?ackstr?om et al., 2013).
Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea
of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et
al. (2012) for search query tagging.
Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and
the associated snippets provided by the search engine, projecting tags from the snippets to the queries,
guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more
advanced matching of snippets and search queries, giving priority to n-gram matches with larger n.
Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less
spelling variation than tweets.
In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and
Torisawa, 2007; Cucerzan, 2007). R?ud et al. (2011) consider using search engines for distant supervision
of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use
click-through data. They use the search engine snippets to generate feature representations rather than
projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts,
1790
applying their model to Twitter data, but their results are considerably below the state of the art. Also,
their source of supervision is not linked to the individual tweets in the way mentioned websites are.
In sum, our method is the first successful application of distant supervision to POS tagging and NER
for Twitter. Moreover, it is, to the best of our knowledge, the first paper that addresses both problems
using the same technique. Finally, our results are significantly better than state-of-the-art results in both
POS tagging and NER.
7 Conclusion
We presented a semi-supervised approach to POS tagging and NER for Twitter data that uses dictionaries
and linked websites as a source of not-so-distant (or linked) supervision to guide the bootstrapping. Our
approach outperforms off-the-shelf taggers when evaluated across various data sets, achieving average
error reductions across data sets of 5% on POS tagging and 10% on NER over state-of-the-art baselines.
References
Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. 2013. How noisy social media text,
how diffrnt social media sources? In IJCNLP.
Edward Benson, Aria Haghighi, and Regina Barzilay. 2011. Event discovery in social media feeds. In ACL.
Silvia Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In EMNLP-CoNLL.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In ACL.
Hal Daum?e, Abhishek Kumar, and Avishek Saha. 2010. Frustratingly easy semi-supervised domain adaptation.
In ACL Workshop on Domain Adaptation for NLP.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-speech tagging for all:
overcoming sparse and noisy data. In RANLP.
Jacob Eisenstein. 2013. What to do about bad language on the internet. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Anno-
tating named entities in Twitter data with crowdsourcing. In NAACL-HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Jenny Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into infor-
mation extraction systems by Gibbs sampling. In ACL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van
Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In
IJCNLP.
Hege Fromreide, Dirk Hovy, and Anders S?gaard. 2014. Crowdsourcing and annotating ner for twitter #drift. In
LREC.
Kuzman Ganchev, Keith Hall, Ryan McDonald, and Slav Petrov. 2012. Using search-logs to improve query
tagging. In ACL.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-Speech Tagging for Twitter:
Annotation, Features, and Experiments. In ACL.
David Hand. 2006. Classifier technology and illusion of progress. Statistical Science, 21(1):1?15.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with
MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014. When pos datasets don?t add up: Combatting sample bias.
In LREC.
1791
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2009. Self-training with products of latent variable grammars.
In EMNLP.
Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL.
Long Jiang, Mo Yo, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent Twitter sentiment
classification. In ACL.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploiting wikipedia as external knowledge for named entity
recognition. In EMNLP-CoNLL.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models
for segmenting and labeling sequence data. In ICML.
Shen Li, Jo?ao Grac?a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In EMNLP.
David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In HLT-NAACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In ACL.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013.
Improved part-of-speech tagging for online conversational text with word clusters. In NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In LREC.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to
Twitter. In NAACL.
Barbara Plank. 2011. Domain Adaptation for Parsing. Ph.D. thesis, University of Groningen.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In
EMNLP.
Stefan R?ud, Massimiliano Ciaramita, Jens M?uller, and Hinrich Sch?utze. 2011. Piggyback: Using search engines
for robust cross-domain named entity recognition. In ACL.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type con-
straints for cross-lingual part-of-speech tagging. TACL, 1:1?12.
Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In In CoNLL.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Kazama. 2011. Learning with lookahead: can history-based
models rival globally optimized models? In CoNLL.
Chun-Kai Wang, Bo-June Hsu, Ming-Wei Chang, and Emre Kiciman. 2013. Simple and knowledge-intensive
generative model for named entity recognition. Technical report, Microsoft Research.
1792
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts,
pages 11?13, Dublin, Ireland, August 23-29 2014.
Selection Bias, Label Bias, and Bias in Ground Truth
Anders S?gaard, Barbara Plank, and Dirk Hovy
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk, {bplank|dhovy}@cst.dk
Introduction
Language technology is biased toward English newswire. In POS tagging, we get 97?98 words right out
of a 100 in English newswire, but results drop to about 8 out of 10 when running the same technology
on Twitter data. In dependency parsing, we are able to identify the syntactic head of 9 out of 10 words
in English newswire, but only 6?7 out of 10 in tweets. Replace references to Twitter with references to a
low-resource language of your choice, and the above sentence is still likely to hold true.
The reason for this bias is obviously that mainstream language technology is data-driven, based on
supervised statistical learning techniques, and annotated data resources are widely available for English
newswire. The situation that arises when applying off-the-shelf language technology, induced from
annotated newswire corpora, to something like Twitter, is a bit like when trying to predict elections from
Xbox surveys (Wang et al., 2013). Our induced models suffer from a data selection bias.
This is actually not the only way our data is biased. The available resources for English newswire
are the result of human annotators following specific guidelines. Humans err, leading to label bias, but
more importantly, annotation guidelines typically make debatable linguistic choices. Linguistics is not
an exact science, and we call the influence of annotation guidelines bias in ground truth.
In the tutorial, we present various case studies for each kind of bias, and show several methods that
can be used to deal with bias. This results in improved performance of NLP systems.
Selection Bias
The situation that arises when applying off-the-shelf language technology, induced from annotated
newswire corpora, to something like Twitter, is, as mentioned, a bit like when trying to predict elec-
tions from Xbox surveys. In the case of elections, however, we can correct most of the selection bias by
post-stratification or instance weighting (Wang et al., 2013). In language technology, the bias correction
problem is harder.
In the case of elections, you have a single output variable and various demographic observed variables.
All values taken by discrete variables at test time can be assumed to have been observed, and all values
observed at training time can be assumed to be seen at test time. In language technology, we typically
have several features only seen in training data and several features only seen in test data.
The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al.,
2006; Turian et al., 2010), while the former has led to the development of learning algorithms that
prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associ-
ated with less predictive, correlated features from being updated. Note, however, that post-stratification
(Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Roy-
all, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization
of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster
et al., 2011), but most work on domain adaptation in language technology has focused on predictive
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
11
approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Mc-
Closky et al., 2010; Chen et al., 2011).
Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algo-
rithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive
bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny,
2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training
sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the
distribution observed in the population. As mentioned, this will never solve the problem with unseen
features, since you cannot up-weigh a null feature.
Semi-supervised learning can correct modest selection bias, but if the domain gap is too wide, our
initial predictions in the target domain will be poor, and semi-supervised learning is likely to increase
bias rather than decrease it. However, recent work has shown that semi-supervised learning can be
combined with distant supervision and correct bias in cases where semi-supervised learning algorithms
typically fail (Plank et al., 2014).
In the tutorial we illustrate these different approaches to selection bias correction, with discriminative
learning of POS taggers for English Twitter as our running example.
Label Bias
In most annotation projects, there is an initial stage, where the project managers compare annotators?
performance, compute agreement scores, select reliable annotators, adjudicate, and elaborate on anno-
tation guidelines, if necessary. Such procedures are considered necessary to correct for the individual
biases of the annotators (label bias). However, this is typically only for the first batches of data, and it
is well-known that even some of the most widely used annotated corpora (such as the Penn Treebank)
contain many errors (Dickinson and Meurers, 2003) in the form of inconsistent annotations of the same
n-grams.
Obviously, using non-expert annotators, e.g., through crowd-sourcing platforms, increase the label
bias considerably. One way to reduce this bias involves collecting several annotations for each datapoint
and averaging over them, which is often feasible because of the low cost of non-expert annotation. This
is called majority voting and is analogous to using ensembles of models to obtain more robust systems.
In the tutorial we discuss alternatives to averaging over annotators, incl., using EM to estimate anno-
tator confidence (Hovy et al., 2013), and joint learning of annotator competence and model parameters
(Raykar and Yu, 2012).
Bias in Ground Truth
In annotation projects, we use inter-annotator agreement measures and annotation guidelines to ensure
consistent annotations. However, annotation guidelines often make linguistically debatable and even
somewhat arbitrary decisions, and inter-annotator agreement is often less than perfect. Some annotators,
for example, may annotate socialin social media as a noun, others may annotate it as an adjective. In
this part of the tutorial, we discuss how to correct for the bias introduced by annotation guidelines. For
both label bias and bias in ground truth, we, again, use POS tagging for English Twitter as our running
example.
Evaluation
Once we accept our data is biased in different ways, we need to reconsider model evaluation. If our data
was selected in a biased way, say from a few editions of the Wall Street Journal, does significance over
data points make much sense? If our annotators have individual biases, can we no longer evaluate our
models on the data of one or two annotators? If the annotation guidelines introduce biases in ground truth,
can we somehow correct for that? In practice we typically do not have hundreds of datasets annotated by
different annotators using different annotation guidelines, but in the tutorial we present various ways of,
nevertheless, correcting for some of these biases.
12
Acknowledgements
This research is funded by the ERC Starting Grant LOWLANDS No. 313695.
References
John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence
learning. In EMNLP.
Minmin Chen, Killiang Weinberger, and John Blitzer. 2011. Co-training for domain adaptation. In NIPS.
Markus Dickinson and Detmar Meurers. 2003. Detecting errors in part-of-speech annotation. In EACL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van
Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In
IJCNLP.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with
MACE. In NAACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL.
David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In
NAACL-HLT.
Barbara Plank, Dirk Hovy, Ryan McDonald, and Anders S?gaard. 2014. Adapting taggers to Twitter with not-so-
distant supervision. In COLING.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminating Spammers and Ranking Annotators for Crowdsourced
Labeling Tasks. Journal of Machine Learning Research, 13:491?518.
Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers
trained on small datasets. In ACL.
R Royall. 1988. The prediction approach to sampling theory. In Rao Krishnaiah, editor, Handbook of Statistics.
North-Holland.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser
ensembles. In EMNLP-CoNLL.
Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of Statistical Planning and Inference, 90:227?244.
T Smith. 1988. Post-stratification. The Statistician, 40.
Charles Sutton, Michael Sindelar, and Andrew McCallum. 2006. Reducing weight undertraining in structured
discriminative learning. In NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In ACL.
Wei Wang, David Rotschild, Sharad Goel, and Andrew Gelman. 2013. Forecasting elections with non-
representative polls. Forthcoming in International Journal of Forecasting.
Bianca Zadrozny. 2004. Learning and evaluating classifiers under sample selection bias. In ICML.
13
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 968?973,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Importance weighting and unsupervised domain adaptation
of POS taggers: a negative result
Barbara Plank, Anders Johannsen and Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140, DK-2300 Copenhagen S
bplank@cst.dk,ajohannsen@hum.ku.dk,soegaard@hum.ku.dk
Abstract
Importance weighting is a generalization
of various statistical bias correction tech-
niques. While our labeled data in NLP is
heavily biased, importance weighting has
seen only few applications in NLP, most of
them relying on a small amount of labeled
target data. The publication bias toward
reporting positive results makes it hard to
say whether researchers have tried. This
paper presents a negative result on unsu-
pervised domain adaptation for POS tag-
ging. In this setup, we only have unlabeled
data and thus only indirect access to the
bias in emission and transition probabili-
ties. Moreover, most errors in POS tag-
ging are due to unseen words, and there,
importance weighting cannot help. We
present experiments with a wide variety of
weight functions, quantilizations, as well
as with randomly generated weights, to
support these claims.
1 Introduction
Many NLP tasks rely on the availability of anno-
tated data. The majority of annotated data, how-
ever, is sampled from newswire corpora. The
performance of NLP systems, e.g., part-of-speech
(POS) tagger, parsers, relation extraction sys-
tems, etc., drops significantly when they are ap-
plied to data that departs from newswire conven-
tions. So while we can extract information, trans-
late and summarize newswire in major languages
with some success, we are much less successful
processing microblogs, chat, weblogs, answers,
emails or literature in a robust way. The main rea-
sons for the drops in accuracy have been attributed
to factors such as previously unseen words and bi-
grams, missing punctuation and capitalization, as
well as differences in the marginal distribution of
data (Blitzer et al., 2006; McClosky et al., 2008;
S?gaard and Haulrich, 2011).
The move from one domain to another (from a
source to a new target domain), say from newspa-
per articles to weblogs, results in a sample selec-
tion bias. Our training data is now biased, since
it is sampled from a related, but nevertheless dif-
ferent distribution. The problem of automatically
adjusting the model induced from source to a dif-
ferent target is referred to as domain adaptation.
Some researchers have studied domain adap-
tation scenarios, where small samples of labeled
data have been assumed to be available for the
target domains. This is usually an unrealistic as-
sumption, since even for major languages, small
samples are only available from a limited number
of domains, and in this work we focus on unsuper-
vised domain adaptation, assuming only unlabeled
target data is available.
Jiang and Zhai (2007), Foster et al. (2010; Plank
and Moschitti (2013) and S?gaard and Haulrich
(2011) have previously tried to use importance
weighting to correct sample bias in NLP. Im-
portance weighting means assigning a weight
to each training instance, reflecting its impor-
tance for modeling the target distribution. Im-
portance weighting is a generalization over post-
stratification (Smith, 1991) and importance sam-
pling (Smith et al., 1997) and can be used to cor-
rect bias in the labeled data.
Out of the four papers mentioned, only S?gaard
and Haulrich (2011) and Plank and Moschitti
(2013) considered an unsupervised domain adap-
tation scenario, obtaining mixed results. These
two papers assume covariate shift (Shimodaira,
2000), i.e., that there is only a bias in the marginal
distribution of the training data. Under this as-
sumption, we can correct the bias by applying a
weight function
P
t
(x)
P
s
(x)
to our training data points
(labeled sentences) and learn from the weighted
data. Of course this weight function cannot be
968
computed in general, but we can approximate it
in different ways.
In POS tagging, we typically factorize se-
quences into emission and transition probabilities.
Importance weighting can change emission prob-
abilities and transition probabilities by assigning
weights to sentences. For instance, if our corpus
consisted of three sequences: 1) a/A b/A, 2) a/A
b/B, and 3) a/A b/B, then P (B|A) = 2/3. If se-
quences two and three were down-weighted to 0.5,
then P (B|A) = 1/2.
However, this paper argues that importance
weighting cannot help adapting POS taggers to
new domains using only unlabeled target data. We
present three sources of evidence: (a) negative
results with the most obvious weight functions
across various English datasets, (b) negative re-
sults with randomly sampled weights, as well as
(c) an analysis of annotated data indicating that
there is little variation in emission and transition
probabilities across the various domains.
2 Related work
Most prior work on importance weighting use a
domain classifier, i.e., train a classifier to discrimi-
nate between source and target instances (S?gaard
and Haulrich, 2011; Plank and Moschitti, 2013)
(y ? {s, t}). For instance, S?gaard and Haulrich
(2011) train a n-gram text classifier and Plank
and Moschitti (2013) a tree-kernel based clas-
sifier on relation extraction instances. In these
studies,
?
P (t|x) is used as an approximation of
P
t
(x)
P
s
(x)
, following Zadrozny (2004). In ?3, we fol-
low the approach of S?gaard and Haulrich (2011),
but consider a wider range of weight functions.
Others have proposed to use kernel mean match-
ing (Huang et al., 2007) or minimizing KL-
divergence (Sugiyama et al., 2007).
Jiang and Zhai (2007) use importance weight-
ing to select a subsample of the source data by
subsequently setting the weight of all selected data
points to 1, and 0 otherwise. However, they do
so by relying on a sequential model trained on
labeled target data. Our results indicate that the
covariate shift assumption fails to hold for cross-
domain POS tagging. While the marginal distri-
butions obviously do differ (since we can tell do-
mains apart without POS analysis), this is most
likely not the only difference. This might explain
the positive results obtained by Jiang and Zhai
(2007). We will come back to this in ?4.
Cortes et al. (2010) show that importance
weighting potentially leads to over-fitting, but pro-
pose to use quantiles to obtain more robust weight
functions. The idea is to rank all weights and ob-
tain q quantiles. If a data point x is weighted by
w, and w lies in the ith quantile of the ranking
(i ? q), x is weighted by the average weight of
data points in the ith quantile.
The weighted structured perceptron (?3) used in
the experiments below was recently used for a dif-
ferent problem, namely for correcting for bias in
annotations (Plank et al., 2014).
l l l l
l l l l l l l l l l l l l l l l
0 5 10 15 20
92
93
94
95
96
97
98
99 l wsjanswersreviews
emailsweblogsnewsgroups
Figure 1: Training epochs vs tagging accuracy for
the baseline model on the dev data.
3 Experiments
3.1 Data
We use the data made available in the SANCL
2012 Shared Task (Petrov and McDonald, 2012).
The training data is the OntoNotes 4.0 release
of the Wall Street Journal section of the Penn
Treebank, while the target domain evaluation data
comes from various sources, incl. Yahoo Answers,
user reviews, emails, weblogs and newsgroups.
For each target domain, we have both development
and test data.
3.2 Model
In the weighted perceptron (Cavallanti et al.,
2006), we make the learning rate dependent on the
current instance x
n
, using the following update:
w
i+1
? w
i
+ ?
n
?(y
n
? sign(w
i
? x
n
))x
n
(1)
where ?
n
is the weight associated with x
n
. See
Huang et al. (2007) for similar notation.
We extend this idea straightforwardly to the
structured perceptron (Collins, 2002), for which
969
System Answers Newsgroups Reviews Avg Emails Weblogs WSJ
Our system 91.08 91.57 91.59 91.41 87.97 92.19 97.32
SANCL12-2nd 90.99 92.32 90.65 91.32 ? ? 97.76
SANCL12-best 91.79 93.81 93.11 92.90 ? ? 97.29
SANCL12-last 88.24 89.70 88.15 88.70 ? ? 95.14
FLORS basic 91.17 92.41 92.25 88.67 91.37 97.11 91.94
Table 1: Tagging accuracies and comparison to prior work on the SANCL test sets (fine-grained POS).
we use an in-house implementation. We use
commonly used features, i.e., w,w
?1
, w
?2
,
w
+1
, w
+2
, digit, hyphen, capitalization, pre-
/suffix features, and Brown word clusters. The
model seems robust with respect to number
of training epochs, cf. Figure 1. Therefore
we fix the number of epochs to five and use
this setting in all our experiments. Our code
is available at: https://bitbucket.org/
bplank/importance-weighting-exp.
3.3 Importance weighting
In our first set of experiments, we follow S?gaard
and Haulrich (2011) in using document classifiers
to obtain weights for the source instances. We
train a text classifier that discriminates the two
domains (source and target). For each sentence
in the source and target domain (the unlabeled
text that comes with the SANCL data), we mark
whether it comes from the source or target do-
main and train a binary classifier (logistic regres-
sion) to discriminate between the two. For ev-
ery sentence in the source we obtain its probabil-
ity for the target domain by doing 5-fold cross-
validation. While S?gaard and Haulrich (2011)
use only token-based features (word n-grams ?
3), we here exploit a variety of features: word
token n-grams, and two generalizations: using
Brown clusters (estimated from the union of the
5 target domains), and Wiktionary tags (if a word
has multiple tags, we assign it the union of tags as
single tag; OOV words are marked as such).
The distributions of weights can be seen in the
upper half of Figure 2.
3.3.1 Results
Table 1 shows that our baseline model achieves
state-of-the-art performance compared to
SANCL (Petrov and McDonald, 2012)
1
and
FLORS (Schnabel and Sch?utze, 2014). Our
results align well with the second best POS
tagger in the SANCL 2012 Shared Task. Note
1
https://sites.google.com/site/sancl2012/home/
shared-task/results
Figure 2: Histogram of different weight functions.
that the best tagger in the shared task explicitly
used normalization and various other heuristics
to achieve better performance. In the rest of the
paper, we use the universal tag set part of the
SANCL data (Petrov et al., 2012).
Figure 3 presents our results on development
data for different importance weighting setups.
None of the above weight functions lead to signifi-
cant improvements on any of the datasets. We also
tried scaling and binning the weights, as suggested
by Cortes et al. (2010), but results kept fluctuating
around baseline performance, with no significant
improvements.
3.4 Random weighting
Obviously, weight functions based on document
classifiers may simply not characterize the rele-
vant properties of the instances and hence lead to
bad re-weighting of the data. We consider three
random sampling strategies, namely sampling ran-
dom uniforms, random exponentials, and random
970
Figure 3: Results on development data for different weight functions, i.e., document classifiers trained
on a) raw tokens; b) tokens replaced by Wiktionary tags; c) tokens replaced by Brown cluster ids. The
weight was the raw p
t
(y|x) value, no scaling, no quantiles. Replacing only open-class tokens for b) and
c) gave similar or lower performance.
Zipfians and ran 500 samples for each. For these
experiments, we estimate significance cut-off lev-
els of tagging accuracies using the approximate
randomization test. To find the cut-off levels,
we randomly replace labels with gold labels until
the achieved accuracy significantly improves over
the baseline for more than 50% of the samples.
For each accuracy level, 50 random samples were
taken.
llll
l
l
llll
lll
ll
llll
l
ll
l
l
llll
l
l
ll
lll
llllll
l
llll
llllll
llllll
l
lll
lllllll
l
ll
l
llllll
l
l
l
lll
llllll
l
l
lllll
ll
l
l
lllllll
llllll
l
l
l
llll
llll
l
lll
l
ll
ll
ll
ll
ll
l
llll
ll
l
ll
ll
ll
ll
l
ll
lllllll
l
l
l
lll
llllll
l
l
ll
ll
ll
ll
l
l
lllll
ll
l
llll
lllllll
llllll
l
ll
ll
ll
l
l
lll
lll
ll
lll
l
l
l
l
lllll
l
lll
l
l
lllll
l
ll
0 200 400
93.2
93.6
94.0
answers
Index
l randomexpzipf
ll
ll
l
lllll
ll
l
l
ll
l
l
l
l
l
l
ll
l
ll
ll
ll
ll
ll
llll
ll
l
ll
l
l
ll
ll
l
ll
ll
llll
lllll
l
ll
l
l
l
l
ll
l
ll
l
lllll
l
l
ll
l
ll
lll
ll
l
l
lll
lll
l
ll
ll
l
llll
ll
l
l
l
l
l
l
l
ll
l
l
l
l
lllll
lllllll
l
ll
ll
l
l
l
llll
ll
l
l
ll
ll
ll
l
ll
ll
l
l
l
lll
l
l
l
ll
l
ll
lll
lll
ll
lll
l
ll
ll
llll
ll
l
ll
l
l
llllll
lll
l
l
lll
llll
l
lll
ll
l
l
l
ll
l
l
lll
l
l
l
l
l
llll
lllll
0 200 4009
4.2
94.4
94.6
94.8
reviews
Index
TA
l
lll
l
llll
l
ll
llll
ll
llll
ll
llllll
ll
ll
llll
l
l
l
ll
llllll
lll
ll
lll
l
ll
l
llll
l
ll
l
ll
lll
ll
l
l
l
lll
ll
l
lll
l
l
l
ll
ll
ll
lll
llll
l
ll
lllll
l
llll
lll
l
l
lll
ll
l
l
l
llll
l
llll
l
l
l
lll
ll
ll
l
llllll
ll
l
llll
ll
lllll
lll
l
l
lllll
l
l
ll
l
l
l
l
lllll
ll
ll
l
lll
lll
ll
l
l
l
l
ll
lllll
lll
llll
l
l
llllllll
l
ll
llll
lll
l
l
l
l
lll
l
l
l
l
l
ll
ll
lll
l
lll
ll
l
ll
l
l
l
ll
ll
llll
llll
0 200 400
93.4
93.8
emails
Index
TA
l
ll
lll
ll
l
lll
ll
ll
l
l
ll
ll
llllllll
ll
lllll
l
l
l
ll
l
l
ll
l
lll
lll
llll
l
lll
l
l
lllll
ll
lll
lllllll
llll
ll
l
l
ll
ll
l
l
lllll
lllll
llllll
lllll
lllllll
l
ll
lllll
l
ll
l
l
ll
l
l
llllll
l
l
l
l
llll
llllll
ll
ll
lllll
llllllll
l
lll
l
ll
l
l
lllllll
l
ll
l
l
lllll
lll
ll
ll
llll
llllll
l
llll
ll
l
l
l
ll
lll
l
llll
l
lllllll
llll
l
l
lll
l
l
ll
l
l
l
lll
l
l
l
0 200 400
94.4
94.8
95.2
weblogs
Index
TA
lll
lll
lll
ll
ll
lll
lll
l
ll
ll
l
lllllllll
lllll
l
l
lll
l
ll
ll
ll
llll
l
llll
llll
l
l
lll
l
lllll
lllll
llllllll
llll
llll
l
ll
l
lll
ll
ll
lllll
lllllllllll
llll
llll
l
ll
lll
ll
lllllll
lllll
l
llllll
l
l
lll
ll
l
lllll
l
ll
ll
ll
lll
ll
llll
lll
l
llll
llllll
lll
lll
llll
lllll
ll
ll
l
llllllll
ll
ll
llll
lll
l
l
llll
l
ll
l
llll
llll
l
ll
l
lll
ll
l
l
lll
llllll
ll
l
0 200 400
94.2
94.6
95.0
newsgroups
Index
TA
0 200 400
93.0
93.4
93.8
answers
Index
0 200 400
94.2
94.6
reviews
Index
TA
0 200 400
93.2
93.6
94.0
emails
Index
TA
0 200 400
94.2
94.6
95.0
95.4
weblogs
Index
TA
0 200 400
94.0
94.4
94.8
newsgroups
Index
TA
0 50 100 150
92.5
93.0
93.5
94.0
answers
0 50 100 150
93.5
94.0
94.5
reviews
TA
0 50 100 150
92.5
93.0
93.5
94.0
emails
TA
0 50 100 150
94.0
94.5
95.0
weblogs
TA
0 50 100 1509
3.5
94.0
94.5
95.0
newsgroups
TA
Figure 4: Random weight functions (500 runs
each) on test sets. Solid line is the baseline per-
formance, while the dashed line is the p-value cut-
off. From top: random, exponential and Zipfian
weighting. All runs fall below the cut-off.
3.4.1 Results
The dashed lines in Figure 4 show the p-value cut-
offs for positive results. We see that most random
weightings of data lead to slight drops in perfor-
mance or are around baseline performance, and no
weightings lead to significant improvements. Ran-
dom uniforms seem slightly better than exponen-
tials and Zipfians.
domain (tokens) avg tag ambiguity OOV KL ?
type token
wsj (train/test: 731k/39k) 1.09 1.41 11.5 0.0006 0.99
answers (28k) 1.09 1.22 27.7 0.048 0.77
reviews (28k) 1.07 1.19 29.5 0.040 0.82
emails (28k) 1.07 1.19 29.9 0.027 0.92
weblogs (20k) 1.05 1.11 22.1 0.010 0.96
newsgroups (20k) 1.05 1.14 23.1 0.011 0.96
Table 2: Relevant statistics for our analysis (?4)
on the test sets: average tag ambiguity, out-of-
vocabulary rate, and KL-divergence and Pearson
correlation coefficient (?) on POS bigrams.
4 Analysis
Some differences between the gold-annotated
source domain data and the gold-annotated tar-
get data used for evaluation are presented in Ta-
ble 2. One important observation is the low ambi-
guity of word forms in the data. This makes the
room for improvement with importance weight-
ing smaller. Moreover, the KL divergencies over
POS bigrams are also very low. This tells us that
transition probabilities are also relatively constant
across domains, again suggesting limited room for
improvement for importance weighting.
Compared to this, we see much bigger differ-
ences in OOV rates. OOV rates do seem to explain
most of the performance drop across domains.
In order to verify this, we implemented a ver-
sion of our structured perceptron tagger with type-
constrained inference (T?ackstr?om et al., 2013).
This technique only improves performance on un-
seen words, but nevertheless we saw significant
improvements across all five domains (cf. Ta-
ble 3). This suggests that unseen words are a
more important problem than the marginal distri-
bution of data for unsupervised domain adaptation
of POS taggers.
971
ans rev email webl newsg
base 93.41 94.44 93.54 94.81 94.55
+type constr. 94.09? 94.85? 94.31? 95.99? 95.97?
p-val cut-off 93.90 94.85 94.10 95.3 95.10
Table 3: Results on the test sets by adding Wik-
tionary type constraints. ?=p-value < 0.001.
We also tried Jiang and Zhai?s subset selection
technique (?3.1 in Jiang and Zhai (2007)), which
assumes labeled training material for the target
domain. However, we did not see any improve-
ments. A possible explanation for these different
findings might be the following. Jiang and Zhai
(2007) use labeled target data to learn their weight-
ing model, i.e., in a supervised domain adaptation
scenario. This potentially leads to very different
weight functions. For example, let the source do-
main be 100 instances of a/A b/B and 100 in-
stances of b/B b/B, and the target domain be 100
instances of a/B a/B. Note that a domain classi-
fier would favor the first 100 sentences, but in an
HMM model induced from the labeled target data,
things look very different. If we apply Laplace
smoothing, the probability of a/A b/B accord-
ing to the target domain HMM model would be
? 8.9e
?7
, and the probability of b/B b/B would
be ? 9e
?5
. Note also that this set-up does not as-
sume covariate shift.
5 Conclusions and Future Work
Importance weighting, a generalization of various
statistical bias correction techniques, can poten-
tially correct bias in our labeled training data, but
this paper presented a negative result about impor-
tance weighting for unsupervised domain adapta-
tion of POS taggers. We first presented exper-
iments with a wide variety of weight functions,
quantilizations, as well as with randomly gener-
ated weights, none of which lead to significant im-
provements. Our analysis indicates that most er-
rors in POS tagging are due to unseen words, and
what remains seem to not be captured adequately
by unsupervised weight functions.
For future work we plan to extend this work to
further weight functions, data sets and NLP tasks.
Acknowledgements
This research is funded by the ERC Starting Grant
LOWLANDS No. 313695.
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Clau-
dio Gentile. 2006. Tracking the best hyperplane
with a simple budget perceptron. In COLT.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri.
2010. Learning bounds for importance weighting.
In NIPS.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP.
Jiayuan Huang, Alexander Smola, Arthur Gretton,
Karsten Borgwardt, and Bernhard Sch?olkopf. 2007.
Correcting sample bias by unlabeled data. In NIPS.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2008. When is self-training effective for pars-
ing? In COLING.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In ACL.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In EACL.
Tobias Schnabel and Hinrich Sch?utze. 2014. Flors:
Fast and simple domain adaptation for part-of-
speech tagging. TACL, 2:15?16.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227?244.
Peter Smith, Mansoor Shafi, and Hongsheng Gao.
1997. Quick simulation: A review of importance
sampling techniques in communications systems.
IEEE Journal on Selected Areas in Communica-
tions, 15(4):597?613.
T.M.F. Smith. 1991. Post-stratification. The Statisti-
cian, 40:315?323.
972
Anders S?gaard and Martin Haulrich. 2011.
Sentence-level instance-weighting for graph-based
and transition-based dependency parsing. In IWPT.
Masashi Sugiyama, Shinichi Nakajima, Hisashi
Kashima, Paul von B?unau, and Motoaki Kawanabe.
2007. Direct importance estimation with model se-
lection and its application to covariate shift adapta-
tion. In NIPS.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, 1:1?12.
Bianca Zadrozny. 2004. Learning and evaluating clas-
sifiers under sample selection bias. In ICML.
973
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742?751,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Learning part-of-speech taggers with inter-annotator agreement loss
Barbara Plank, Dirk Hovy, Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140, DK-2300 Copenhagen S
bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk
Abstract
In natural language processing (NLP) an-
notation projects, we use inter-annotator
agreement measures and annotation guide-
lines to ensure consistent annotations.
However, annotation guidelines often
make linguistically debatable and even
somewhat arbitrary decisions, and inter-
annotator agreement is often less than
perfect. While annotation projects usu-
ally specify how to deal with linguisti-
cally debatable phenomena, annotator dis-
agreements typically still stem from these
?hard? cases. This indicates that some er-
rors are more debatable than others. In this
paper, we use small samples of doubly-
annotated part-of-speech (POS) data for
Twitter to estimate annotation reliability
and show how those metrics of likely inter-
annotator agreement can be implemented
in the loss functions of POS taggers. We
find that these cost-sensitive algorithms
perform better across annotation projects
and, more surprisingly, even on data an-
notated according to the same guidelines.
Finally, we show that POS tagging mod-
els sensitive to inter-annotator agreement
perform better on the downstream task of
chunking.
1 Introduction
POS-annotated corpora and treebanks are collec-
tions of sentences analyzed by linguists accord-
ing to some linguistic theory. The specific choice
of linguistic theory has dramatic effects on down-
stream performance in NLP tasks that rely on syn-
tactic features (Elming et al., 2013). Variation
across annotated corpora in linguistic theory also
poses challenges to intrinsic evaluation (Schwartz
et al., 2011; Tsarfaty et al., 2012), as well as
for languages where available resources are mu-
tually inconsistent (Johansson, 2013). Unfortu-
nately, there is no grand unifying linguistic the-
ory of how to analyze the structure of sentences.
While linguists agree on certain things, there is
still a wide range of unresolved questions. Con-
sider the following sentence:
(1) @GaryMurphyDCU of @DemMattersIRL
will take part in a panel discussion on Octo-
ber 10th re the aftermath of #seanref . . .
While linguists will agree that in is a preposi-
tion, and panel discussion a compound noun, they
are likely to disagree whether will is heading the
main verb take or vice versa. Even at a more basic
level of analysis, it is not completely clear how to
assign POS tags to each word in this sentence: is
part a particle or a noun; is 10th a numeral or a
noun?
Some linguistic controversies may be resolved
by changing the vocabulary of linguistic theory,
e.g., by leaving out numerals or introducing ad
hoc parts of speech, e.g. for English to (Marcus
et al., 1993) or words ending in -ing (Manning,
2011). However, standardized label sets have
practical advantages in NLP (Zeman and Resnik,
2008; Zeman, 2010; Das and Petrov, 2011; Petrov
et al., 2012; McDonald et al., 2013).
For these and other reasons, our annotators
(even when they are trained linguists) often dis-
agree on how to analyze sentences. The strategy
in most previous work in NLP has been to monitor
and later resolve disagreements, so that the final
labels are assumed to be reliable when used as in-
put to machine learning models.
Our approach
Instead of glossing over those annotation disagree-
ments, we consider what happens if we embrace
the uncertainty exhibited by human annotators
742
when learning predictive models from the anno-
tated data.
To achieve this, we incorporate the uncertainty
exhibited by annotators in the training of our
model. We measure inter-annotator agreement on
small samples of data, then incorporate this in the
loss function of a structured learner to reflect the
confidence we can put in the annotations. This
provides us with cost-sensitive online learning al-
gorithms for inducing models from annotated data
that take inter-annotator agreement into consider-
ation.
Specifically, we use online structured percep-
tron with drop-out, which has previously been ap-
plied to POS tagging and is known to be robust
across samples and domains (S?gaard, 2013a). We
incorporate the inter-annotator agreement in the
loss function either as inter-annotator F1-scores
or as the confusion probability between annota-
tors (see Section 3 below for a more detailed de-
scription). We use a small amounts of doubly-
annotated Twitter data to estimate F1-scores and
confusion probabilities, and incorporate them dur-
ing training via a modified loss function. Specif-
ically, we use POS annotations made by two an-
notators on a set of 500 newly sampled tweets
to estimate our agreement scores, and train mod-
els on existing Twitter data sets (described be-
low). We evaluate the effect of our modified
training by measuring intrinsic as well as down-
stream performance of the resulting models on two
tasks, namely named entity recognition (NER) and
chunking, which both use POS tags as input fea-
tures.
2 POS-annotated Twitter data sets
The vast majority of POS-annotated resources
across languages contain mostly newswire text.
Some annotated Twitter data sets do exist for En-
glish. Ritter et al. (2011) present a manually an-
notated data set of 16 thousand tokens. They
do not report inter-annotator agreement. Gimpel
et al. (2011) annotated about 26 thousand tokens
and report a raw agreement of 92%. Foster et
al. (2011) annotated smaller portions of data for
cross-domain evaluation purposes. We refer to the
data as RITTER, GIMPEL and FOSTER below.
In our experiments, we use the RITTER splits
provided by Derczynski et al. (2013), and the
October splits of the GIMPEL data set, version
0.3. We train our models on the concatenation of
RITTER-TRAIN and GIMPEL-TRAIN and evaluate
them on the remaining data, the dev and test set
provided by Foster et al. (2011) as well as an in-
house annotated data set of 3k tokens (see below).
The three annotation efforts (Ritter et al., 2011;
Gimpel et al., 2011; Foster et al., 2011) all used
different tagsets, however, and they also differ in
tokenization, as well as a wide range of linguistic
decisions. We mapped all the three corpora to the
universal tagset provided by Petrov et al. (2012)
and used the same dummy symbols for numbers,
URLs, etc., in all the data sets. Following (Fos-
ter et al., 2011), we consider URLs, usernames
and hashtags as NOUN. We did not change the tok-
enization.
The data sets differ in how they analyze many of
the linguistically hard cases. Consider, for exam-
ple, the analysis of will you come out to in GIM-
PEL and RITTER (Figure 1, top). While Gimpel
et al. (2011) tag out and to as adpositions, Ritter
et al. (2011) consider them particles. What is the
right analysis depends on the compositionality of
the construction and the linguistic theory one sub-
scribes to.
Other differences include the analysis of abbre-
viations (PRT in GIMPEL; X in RITTER and FOS-
TER), colon (X in GIMPEL; punctuation in RIT-
TER and FOSTER), and emoticons, which can take
multiple parts of speech in GIMPEL, but are al-
ways X in RITTER, while they are absent in FOS-
TER. GIMPEL-TRAIN and RITTER-TRAIN are
also internally inconsistent. See the bottom of Fig-
ure 1 for examples and Hovy et al. (2014) for a
more detailed discussion on differences between
the data sets.
Since the mapping to universal tags could
potentially introduce errors, we also annotated
a data set directly using universal tags. We
randomly selected 200 tweets collected over the
span of one day, and had three annotators tag
this set. We split the data in such a way that
each annotator had 100 tweets: two annotators
had disjoint sets, the third overlapped 50 items
with each of the two others. In this way, we
obtained an initial set of 100 doubly-annotated
tweets. The annotators were not provided with
annotation guidelines. After the first round of
annotations, we achieved a raw agreement of
0.9, a Cohen?s ? of 0.87, and a Krippendorff?s
? of 0.87. We did one pass over the data to
adjudicate the cases where annotators disagreed,
743
. . .
will you come out to the
. . .GIMPEL VERB PRON VERB ADP ADP DET
RITTER VERB PRON VERB PRT PRT DET
RITTER
. . .
you/PRON come/VERB out/PRT to/PRT
. . .
it/PRON comes/VERB out/ADP nov/NOUN
GIMPEL
. . .
Advances/NOUN and/CONJ Social/NOUN Media/NOUN .../X
. . .
Journalists/NOUN and/CONJ Social/ADJ Media/NOUN experts/NOUN
Figure 1: Annotation differences between (top) and within (bottom) two available Twitter POS data sets.
or where they had flagged their choice as debat-
able. The final data set (lowlands.test),
referred below to as INHOUSE, contained 3,064
tokens (200 tweets) and is publicly available
at http://bitbucket.org/lowlands/
costsensitive-data/, along with the data
used to compute inter-annotator agreement scores
for learning cost-sensitive taggers, described in
the next section.
3 Computing agreement scores
Gimpel et al. (2011) used 72 doubly-annotated
tweets to estimate inter-annotator agreement, and
we also use doubly-annotated data to compute
agreement scores. We randomly sampled 500
tweets for this purpose. Each tweet was anno-
tated by two annotators, again using the univer-
sal tag set (Petrov et al., 2012). All annotators
were encouraged to use their own best judgment
rather than following guidelines or discussing dif-
ficult cases with each other. This is in contrast to
Gimpel et al. (2011), who used annotation guide-
lines. The average inter-annotator agreement was
0.88 for raw agreement, and 0.84 for Cohen?s ?.
Gimpel et al. (2011) report a raw agreement of
0.92.
We use two metrics to provide a more detailed
picture of inter-annotator agreement, namely
F1-scores between annotators on individual parts
of speech, and tag confusion probabilities, which
we derive from confusion matrices.
The F1-score relates to precision and recall
in the usual way, i.e, as the harmonic mean
between those two measure. In more detail, given
two annotators A
1
and A
2
, we say the precision
Figure 2: Inter-annotator F1-scores estimated
from 500 tweets.
of A
1
relative to A
2
with respect to POS tag T in
some data setX , denoted Prec
T
(A
1
(X), A
2
(X)),
is the number of tokens both A
1
and A
2
predict to
be T over the number of times A
1
predicts a token
to be T . Similarly, we define the recall with re-
spect to some tag T , i.e., Rec
T
(A
1
(X), A
2
(X)),
as the number of tokens both A
1
and A
2
predict
to be T over the number of times A
2
predicts
a token to be T . The only difference with
respect to standard precision and recall is that
the gold standard is replaced by a second anno-
tator, A
2
. Note that Prec
T
(A
1
(X), A
2
(X)) =
Rec
T
(A
2
(X), A
1
(X)). It follows from all of
the above that the F1-score is symmetrical, i.e.,
F1
T
(A
1
(X), A
2
(X)) = F1
T
(A
2
(X), A
1
(X)).
The inter-annotator F1-scores over the 12
POS tags in the universal tagset are presented in
Figure 2. It shows that there is a high agreement
for nouns, verbs and punctuation, while the agree-
744
Figure 3: Confusion matrix of POS tags obtained
from 500 doubly-annotated tweets.
ment is low, for instance, for particles, numerals
and the X tag.
We compute tag confusion probabilities
from a confusion matrix over POS tags like
the one in Figure 3. From such a matrix,
we compute the probability of confusing
two tags t
1
and t
2
for some data point x,
i.e. P ({A
1
(x), A
2
(x)} = {t
1
, t
2
}) as the
mean of P (A
1
(x) = t
1
, A
2
(x) = t
2
) and
P (A
1
(x) = t
2
, A
2
(x) = t
1
), e.g., the confusion
probability of two tags is the mean of the prob-
ability that annotator A
1
assigns one tag and A
2
another, and vice versa.
We experiment with both agreement scores (F1
and confusion matrix probabilities) to augment the
loss function in our learner. The next section de-
scribes this modification in detail.
4 Inter-annotator agreement loss
We briefly introduce the cost-sensitive perceptron
classifier. Consider the weighted perceptron loss
on our ith example ?x
i
, y
i
? (with learning rate ? =
1), L
w
(?x
i
, y
i
?):
?(sign(w ? x
i
), y
i
) max(0,?y
i
w ? x
i
)
In a non-cost-sensitive classifier, the weight
function ?(y
j
, y
i
) = 1 for 1 ? i ? N . The
1: X = {?x
i
, y
i
?}
N
i=1
with x
i
= ?x
1
i
, . . . , x
m
i
?
2: I iterations
3: w = ?0?
m
4: for iter ? I do
5: for 1 ? i ? N do
6: y? = arg max
y?Y
w ? ?(x
i
, y)
7: w? w+ ?(y?, y
i
)[?(x
i
, y
i
)??(x
i
, y?)]
8: w
?
+ = w
9: end for
10: end for
11: return w
?
/ = (N ? I)
Figure 4: Cost-sensitive structured perceptron (see
Section 3 for weight functions ?).
two cost-sensitive systems proposed only differ in
how we formulate ?(?, ?). In one model, the loss is
weighted by the inter-annotator F1 of the gold tag
in question. This boils down to
?(y
j
, y
i
) = F1
y
i
(A
1
(X), A
2
(X))
where X is the small sample of held-out data used
to estimate inter-annotator agreement. Note that
in this formulation, the predicted label is not taken
into consideration.
The second model is slightly more expressive
and takes both the gold and predicted tags into ac-
count. It basically weights the loss by how likely
the gold and predicted tag are to be mistaken for
each other, i.e., (the inverse of) their confusion
probability:
?(y
j
, y
i
)) = 1?P ({A
1
(X), A
2
(X)} = {y
j
, y
i
})
In both loss functions, a lower gamma value
means that the tags are more likely to be confused
by a pair of annotators. In this case, the update is
smaller. In contrast, the learner incurs greater loss
when easy tags are confused.
It is straight-forward to extend these cost-
sensitive loss functions to the structured percep-
tron (Collins, 2002). In Figure 4, we provide the
pseudocode for the cost-sensitive structured online
learning algorithm. We refer to the cost-sensitive
structured learners as F1- and CM-weighted be-
low.
5 Experiments
In our main experiments, we use structured per-
ceptron (Collins, 2002) with random corruptions
745
using a drop-out rate of 0.1 for regularization, fol-
lowing S?gaard (2013a). We use the LXMLS
toolkit implementation
1
with default parameters.
We present learning curves across iterations, and
only set parameters using held-out data for our
downstream experiments.
2
5.1 Results
Our results are presented in Figure 5. The top left
graph plots accuracy on the training data per iter-
ation. We see that CM-weighting does not hurt
training data accuracy. The reason may be that
the cost-sensitive learner does not try (as hard) to
optimize performance on inconsistent annotations.
The next two plots (upper mid and upper right)
show accuracy over epochs on in-sample evalua-
tion data, i.e., GIMPEL-DEV and RITTER-TEST.
Again, the CM-weighted learner performs better
than our baseline model, while the F1-weighted
learner performs much worse.
The interesting results are the evaluations on
out-of-sample evaluation data sets (FOSTER and
IN-HOUSE) - lower part of Figure 5. Here, both
our learners are competitive, but overall it is clear
that the CM-weighted learner performs best. It
consistently improves over the baseline and F1-
weighting. The former is much more expressive
as it takes confusion probabilities into account and
does not only update based on gold-label uncer-
tainty, as is the case with the F1-weighted learner.
5.2 Robustness across regularizers
Discriminative learning typically benefits from
regularization to prevent overfitting. The simplest
is the averaged perceptron, but various other meth-
ods have been suggested in the literature.
We use structured perceptron with drop-out, but
results are relatively robust across other regular-
ization methods. Drop-out works by randomly
dropping a fraction of the active features in each
iteration, thus preventing overfitting. Table 1
shows the results for using different regularizers,
in particular, Zipfian corruptions (S?gaard, 2013b)
and averaging. While there are minor differences
across data sets and regularizers, we observe that
the corresponding cell using the loss function sug-
gested in this paper (CM) always performs better
than the baseline method.
1
https://github.com/gracaninja/
lxmls-toolkit/
2
In this case, we use FOSTER-DEV as our development
data to avoid in-sample bias.
6 Downstream evaluation
We have seen that our POS tagging model im-
proves over the baseline model on three out-of-
sample test sets. The question remains whether
training a POS tagger that takes inter-annotator
agreement scores into consideration is also effec-
tive on downstream tasks. Therefore, we eval-
uate our best model, the CM-weighted learner,
in two downstream tasks: shallow parsing?also
known as chunking?and named entity recogni-
tion (NER).
For the downstream evaluation, we used the
baseline and CM models trained over 13 epochs,
as they performed best on FOSTER-DEV (cf. Fig-
ure 5). Thus, parameters were optimized only on
POS tagging data, not on the downstream evalu-
ation tasks. We use a publicly available imple-
mentation of conditional random fields (Lafferty
et al., 2001)
3
for the chunking and NER exper-
iments, and provide the POS tags from our CM
learner as features.
6.1 Chunking
The set of features for chunking include informa-
tion from tokens and POS tags, following Sha and
Pereira (2003).
We train the chunker on Twitter data (Ritter et
al., 2011), more specifically, the 70/30 train/test
split provided by Derczynski et al. (2013) for POS
tagging, as the original authors performed cross
validation. We train on the 70% Twitter data (11k
tokens) and evaluate on the remaining 30%, as
well as on the test data from Foster et al. (2011).
The FOSTER data was originally annotated for
POS and constituency tree information. We con-
verted it to chunks using publicly available conver-
sion software.
4
Part-of-speech tags are the ones
assigned by our cost-sensitive (CM) POS model
trained on Twitter data, the concatenation of Gim-
pel and 70% Ritter training data. We did not in-
clude the CoNLL 2000 training data (newswire
text), since adding it did not substantially improve
chunking performance on tweets, as also shown
in (Ritter et al., 2011).
The results for chunking are given in Ta-
ble 2. They show that using the POS tagging
model (CM) trained to be more sensitive to inter-
annotator agreement improves performance over
3
http://crfpp.googlecode.com
4
http://ilk.uvt.nl/team/sabine/
homepage/software.html
746
5 10 15 20 25
Epochs
74
75
76
77
78
79
80
81
82
Ac
cu
rac
y(
%)
TRAINING
BASELINE
F1
CM
5 10 15 20 25
Epochs
77.5
78.0
78.5
79.0
79.5
80.0
80.5
Ac
cu
rac
y(
%)
GIMPEL-DEV
BASELINE
F1
CM
5 10 15 20 25
Epochs
83.5
84.0
84.5
85.0
85.5
86.0
86.5
87.0
Ac
cu
rac
y(
%)
RITTER-TEST
BASELINE
F1
CM
5 10 15 20 25
Epochs
81.0
81.5
82.0
82.5
83.0
83.5
84.0
Ac
cu
rac
y(
%)
FOSTER-DEV
BASELINE
F1
CM
5 10 15 20 25
Epochs
82.5
83.0
83.5
84.0
84.5
85.0
Ac
cu
rac
y(
%)
FOSTER-TEST
BASELINE
F1
CM
5 10 15 20 25
Epochs
82.2
82.4
82.6
82.8
83.0
83.2
83.4
83.6
83.8
84.0
Ac
cu
rac
y(
%)
IN-HOUSE
BASELINE
F1
CM
Figure 5: POS accuracy for the three models: baseline, confusion matrix loss (CM) and F1-weighted
(F1) loss for increased number of training epochs. Top row: in-sample accuracy on training (left) and
in-sample evaluation datasets (center, right). Bottom row: out-of-sample accuracy on various data sets.
CM is robust on both in-sample and out-of-sample data.
RITTER-TEST
F1: All NP VP PP
BL 76.20 78.61 74.25 86.79
CM 76.42 79.07 74.98 86.19
FOSTER-TEST
F1: All NP VP PP
BL 68.49 70.73 60.56 86.50
CM 68.97 71.25 61.97 87.24
Table 2: Downstream results on chunking. Overall
F1 score (All) as well as F1 for NP, VP and PP.
the baseline (BL) for the downstream task of
chunking. Overall chunking F1 score improves.
More importantly, we report on individual scores
for NP, VP and PP chunks, where we see consis-
tent improvements for NPs and VPs (since both
nouns and verbs have high inter-annotator agree-
ment), while results on PP are mixed. This is to
be expected, since PP phrases involve adposition-
als (ADP) that are often confused with particles
(PRT), cf. Figure 3. Our tagger has been trained
to deliberately abstract away from such uncertain
cases. The results show that taking uncertainty in
POS annotations into consideration during train-
ing has a positive effect in downstream results. It
is thus better if we do not try to urge our models
to make a firm decision on phenomena that neither
747
BASELINE CM
Regularizer FOSTER-DEV FOSTER-TEST IN-HOUSE FOSTER-DEV FOSTER-TEST IN-HOUSE
Averaging 0.827 0.837 0.830 0.831 0.844 0.833
Drop-out 0.827 0.838 0.827 0.836 0.843 0.833
Zipfian 0.821 0.835 0.833 0.825 0.838 0.836
Table 1: Results across regularizers (after 13 epochs).
linguistic theories nor annotators do agree upon.
6.2 NER
In the previous section, we saw positive effects of
cost-sensitive POS tagging for chunking, and here
we evaluate it on another downstream task, NER.
For the named entity recognition setup, we use
commonly used features, in particular features
for word tokens, orthographic features like the
presence of hyphens, digits, single quotes, up-
per/lowercase, 3 character prefix and suffix infor-
mation. Moreover, we add Brown word cluster
features that use 2,4,6,8,..,16 bitstring prefixes es-
timated from a large Twitter corpus (Owoputi et
al., 2013).
5
For NER, we do not have access to carefully
annotated Twitter data for training, but rely on
the crowdsourced annotations described in Finin
et al. (2010). We use the concatenation of the
CoNLL 2003 training split of annotated data from
the Reuters corpus and the Finin data for training,
as in this case training on the union resulted in a
model that is substantially better than training on
any of the individual data sets. For evaluation, we
have three Twitter data set. We use the recently
published data set from the MSM 2013 challenge
(29k tokens)
6
, the data set of Ritter et al. (2011)
used also by Fromheide et al. (2014) (46k tokens),
as well as an in-house annotated data set (20k to-
kens) (Fromheide et al., 2014).
F1: RITTER MSM IN-HOUSE
BL 78.20 82.25 82.58
CM 78.30 82.00 82.77
Table 3: Downstream results for named entity
recognition (F1 scores).
Table 3 shows the result of using our POS mod-
els in downstream NER evaluation. Here we ob-
serve mixed results. The cost-sensitive model is
5
http://www.ark.cs.cmu.edu/TweetNLP/
6
http://oak.dcs.shef.ac.uk/msm2013/ie_
challenge/
able to improve performance on two out of the
three test sets, while being slightly below baseline
performance on the MSM challenge data. Note
that in contrast to chunking, POS tags are just one
of the many features used for NER (albeit an im-
portant one), which might be part of the reason
why the picture looks slightly different from what
we observed above on chunking.
7 Related work
Cost-sensitive learning takes costs, such as mis-
classification cost, into consideration. That is,
each instance that is not classified correctly during
the learning process may contribute differently to
the overall error. Geibel and Wysotzki (2003) in-
troduce instance-dependent cost values for the per-
ceptron algorithm and apply it to a set of binary
classification problems. We focus here on struc-
tured problems and propose cost-sensitive learn-
ing for POS tagging using the structured percep-
tron algorithm. In a similar spirit, Higashiyama
et al. (2013) applied cost-sensitive learning to the
structured perceptron for an entity recognition task
in the medical domain. They consider the dis-
tance between the predicted and true label se-
quence smoothed by a parameter that they esti-
mate on a development set. This means that the
entire sequence is scored at once, while we update
on a per-label basis.
The work most related to ours is the recent study
of Song et al. (2012). They suggest that some er-
rors made by a POS tagger are more serious than
others, especially for downstream tasks. They de-
vise a hierarchy of POS tags for the Penn tree-
bank tag set (e.g. the class NOUN contains NN,
NNS, NNP, NNPS and CD) and use that in an
SVM learner. They modify the Hinge loss that
can take on three values: 0, ?, 1. If an error oc-
curred and the predicted tag is in the same class as
the gold tag, a loss ? occurred, otherwise it counts
as full cost. In contrast to our approach, they let
the learner focus on the more difficult cases by oc-
curring a bigger loss when the predicted POS tag
748
is in a different category. Their approach is thus
suitable for a fine-grained tagging scheme and re-
quires tuning of the cost parameter ?. We tackle
the problem from a different angle by letting the
learner abstract away from difficult, inconsistent
cases as estimated from inter-annotator scores.
Our approach is also related to the literature
on regularization, since our cost-sensitive loss
functions are aimed at preventing over-fitting to
low-confidence annotations. S?gaard (2013b;
2013a) presented two theories of linguistic varia-
tion and perceptron learning algorithms that reg-
ularize models to minimize loss under expected
variation. Our work is related, but models varia-
tions in annotation rather than variations in input.
There is a large literature related to the issue of
learning from annotator bias. Reidsma and op den
Akker (2008) show that differences between anno-
tators are not random slips of attention but rather
different biases annotators might have, i.e. differ-
ent mental conceptions. They show that a classi-
fier trained on data from one annotator performed
much better on in-sample (same annotator) data
than on data of any other annotator. They propose
two ways to address this problem: i) to identify
subsets of the data that show higher inter-annotator
agreement and use only that for training (e.g. for
speaker address identification they restrict the data
to instances where at least one person is in the
focus of attention); ii) if available, to train sepa-
rate models on data annotated by different anno-
tators and combine them through voting. The lat-
ter comes at the cost of recall, because they de-
liberately chose the classifier to abstain in non-
consensus cases.
In a similar vein, Klebanov and Beigman (2009)
divide the instance space into easy and hard cases,
i.e. easy cases are reliably annotated, whereas
items that are hard show confusion and disagree-
ment. Hard cases are assumed to be annotated
by individual annotator?s coin-flips, and thus can-
not be assumed to be uniformly distributed (Kle-
banov and Beigman, 2009). They show that learn-
ing with annotator noise can have deteriorating ef-
fect at test time, and thus propose to remove hard
cases, both at test time (Klebanov and Beigman,
2009) and training time (Beigman and Klebanov,
2009).
In general, it is important to analyze the data
and check for label biases, as a machine learner is
greatly affected by annotator noise that is not ran-
dom but systematic (Reidsma and Carletta, 2008).
However, rather than training on subsets of data or
training separate models ? which all implicitly as-
sume that there is a large amount of training data
available ? we propose to integrate inter-annotator
biases directly into the loss function.
Regarding measurements for agreements, sev-
eral scores have been suggested in the literature.
Apart from the simple agreement measure, which
records how often annotators choose the same
value for an item, there are several statistics that
qualify this measure by adjusting for other fac-
tors, such as Cohen?s ? (Cohen and others, 1960),
the G-index score (Holley and Guilford, 1964), or
Krippendorff?s ? (Krippendorf, 2004). However,
most of these scores are sensitive to the label dis-
tribution, missing values, and other circumstances.
The measure used in this paper is less affected by
these factors, but manages to give us a good un-
derstanding of the agreement.
8 Conclusion
In NLP, we use a variety of measures to assess
and control annotator disagreement to produce ho-
mogenous final annotations. This masks the fact
that some annotations are more reliable than oth-
ers, and which is thus not reflected in learned pre-
dictors. We incorporate the annotator uncertainty
on certain labels by measuring annotator agree-
ment and use it in the modified loss function of
a structured perceptron. We show that this ap-
proach works well independent of regularization,
both on in-sample and out-of-sample data. More-
over, when evaluating the models trained with our
loss function on downstream tasks, we observe im-
provements on two different tasks. Our results
suggest that we need to pay more attention to an-
notator confidence when training predictors.
Acknowledgements
We would like to thank the anonymous review-
ers and Nathan Schneider for valuable comments
and feedback. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
References
Eyal Beigman and Beata Klebanov. 2009. Learning
with annotation noise. In ACL.
Jacob Cohen et al. 1960. A coefficient of agreement
749
for nominal scales. Educational and psychological
measurement, 20(1):37?46.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In ACL.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez, and Anders
S?gaard. 2013. Down-stream effects of tree-to-
dependency conversions. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with
crowdsourcing. In NAACL-HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Hege Fromheide, Dirk Hovy, and Anders S?gaard.
2014. Crowdsourcing and annotating NER for Twit-
ter #drift. In Proceedings of LREC 2014.
Peter Geibel and Fritz Wysotzki. 2003. Perceptron
based learning with example dependent and noisy
costs. In ICML.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In ACL.
Shohei Higashiyama, Kazuhiro Seki, and Kuniaki Ue-
hara. 2013. Clinical entity recognition using
cost-sensitive structured perceptron for NTCIR-10
MedNLP. In NTCIR.
Jasper Wilson Holley and Joy Paul Guilford. 1964.
A Note on the G-Index of Agreement. Educational
and Psychological Measurement, 24(4):749.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014.
When POS datasets don?t add up: Combatting sam-
ple bias. In Proceedings of LREC 2014.
Richard Johansson. 2013. Training parsers on incom-
patible treebanks. In NAACL.
Beata Klebanov and Eyal Beigman. 2009. From an-
notator agreement to noise models. Computational
Linguistics, 35(4):495?503.
Klaus Krippendorf, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Christopher D Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? In Computational Linguistics and Intelligent
Text Processing, pages 171?189. Springer.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In ACL.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Dennis Reidsma and Jean Carletta. 2008. Reliabil-
ity measurement without limits. Computational Lin-
guistics, 34(3):319?326.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting ?subjective? annotations. In Workshop on
Human Judgements in Computational Linguistics,
COLING.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In EMNLP.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In ACL.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL.
Anders S?gaard. 2013a. Part-of-speech tagging with
antagonistic adversaries. In ACL.
Anders S?gaard. 2013b. Zipfian corruptions for robust
pos tagging. In NAACL.
750
Hyun-Je Song, Jeong-Woo Son, Tae-Gil Noh, Seong-
Bae Park, and Sang-Jo Lee. 2012. A cost sensitive
part-of-speech tagging: differentiating serious errors
from minor errors. In ACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In EACL.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In IJCNLP.
Daniel Zeman. 2010. Hard problems of tagset con-
version. In Proceedings of the Second International
Conference on Global Interoperability for Language
Resources.
751
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377?382,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Experiments with crowdsourced re-annotation of a POS tagging data set
Dirk Hovy, Barbara Plank, and Anders S?gaard
Center for Language Technology
University of Copenhagen
Njalsgade 140, 2300 Copenhagen
{dirk|bplank}@cst.dk, soegaard@hum.ku.dk
Abstract
Crowdsourcing lets us collect multiple an-
notations for an item from several annota-
tors. Typically, these are annotations for
non-sequential classification tasks. While
there has been some work on crowdsourc-
ing named entity annotations, researchers
have largely assumed that syntactic tasks
such as part-of-speech (POS) tagging can-
not be crowdsourced. This paper shows
that workers can actually annotate sequen-
tial data almost as well as experts. Fur-
ther, we show that the models learned from
crowdsourced annotations fare as well as
the models learned from expert annota-
tions in downstream tasks.
1 Introduction
Training good predictive NLP models typically re-
quires annotated data, but getting professional an-
notators to build useful data sets is often time-
consuming and expensive. Snow et al (2008)
showed, however, that crowdsourced annotations
can produce similar results to annotations made
by experts. Crowdsourcing services such as Ama-
zon?s Mechanical Turk has since been successfully
used for various annotation tasks in NLP (Jha et
al., 2010; Callison-Burch and Dredze, 2010).
However, most applications of crowdsourcing
in NLP have been concerned with classification
problems, such as document classification and
constructing lexica (Callison-Burch and Dredze,
2010). A large part of NLP problems, however, are
structured prediction tasks. Typically, sequence
labeling tasks employ a larger set of labels than
classification problems, as well as complex inter-
actions between the annotations. Disagreement
among annotators is therefore potentially higher,
and the task of annotating structured data thus
harder.
Only a few recent studies have investi-
gated crowdsourcing sequential tasks; specifically,
named entity recognition (Finin et al, 2010; Ro-
drigues et al, 2013). Results for this are good.
However, named entities typically use only few la-
bels (LOC, ORG, and PER), and the data contains
mostly non-entities, so the complexity is manage-
able. The question of whether a more linguisti-
cally involved structured task like part-of-speech
(POS) tagging can be crowdsourced has remained
largely unaddressed.
1
In this paper, we investigate how well lay anno-
tators can produce POS labels for Twitter data. In
our setup, we present annotators with one word at
a time, with a minimal surrounding context (two
words to each side). Our choice of annotating
Twitter data is not coincidental: with the short-
lived nature of Twitter messages, models quickly
lose predictive power (Eisenstein, 2013), and re-
training models on new samples of more represen-
tative data becomes necessary. Expensive profes-
sional annotation may be prohibitive for keeping
NLP models up-to-date with linguistic and topical
changes on Twitter. We use a minimum of instruc-
tions and require few qualifications.
Obviously, lay annotation is generally less re-
liable than professional annotation. It is there-
fore common to aggregate over multiple annota-
tions for the same item to get more robust anno-
tations. In this paper we compare two aggrega-
tion schemes, namely majority voting (MV) and
MACE (Hovy et al, 2013). We also show how we
can use Wiktionary, a crowdsourced lexicon, to fil-
ter crowdsourced annotations. We evaluate the an-
notations in several ways: (a) by testing their ac-
curacy with respect to a gold standard, (b) by eval-
uating the performance of POS models trained on
1
One of the reviewers alerted us to an unpublished mas-
ters thesis, which uses pre-annotation to reduce tagging to
fewer multiple-choice questions. See Related Work section
for details.
377
the annotations across several existing data sets,
as well as (c) by applying our models in down-
stream tasks. We show that with minimal context
and annotation effort, we can produce structured
annotations of near-expert quality. We also show
that these annotations lead to better POS tagging
models than previous models learned from crowd-
sourced lexicons (Li et al, 2012). Finally, we
show that models learned from these annotations
are competitive with models learned from expert
annotations on various downstream tasks.
2 Our Approach
We crowdsource the training section of the data
from Gimpel et al (2011)
2
with POS tags. We use
Crowdflower,
3
to collect five annotations for each
word, and then find the most likely label for each
word among the possible annotations. See Figure
1 for an example. If the correct label is not among
the annotations, we are unable to recover the cor-
rect answer. This was the case for 1497 instances
in our data (cf. the token ?:? in the example).
We thus report on oracle score, i.e., the best label
sequence that could possibly be found, which is
correct except for the missing tokens. Note that
while we report agreement between the crowd-
sourced annotations and the crowdsourced anno-
tations, our main evaluations are based on models
learned from expert vs. crowdsourced annotations
and downstream applications thereof (chunking
and NER). We take care in evaluating our models
across different data sets to avoid biasing our
evaluations to particular annotations. All the data
sets used in our experiments are publicly available
at http://lowlands.ku.dk/results/.
x Z y
@USER NOUN,NOUN,X,NOUN,-,NOUN NOUN
: .,.,-,.,.,. X
I PRON,NOUN,PRON,NOUN,PRON,- PRON
owe VERB,VERB,-,VERB,VERB,VERB VERB
U PRON,X,-,NOUN,NOUN,PRON PRON
? = 0.9, 0.4, 0.2, 0.8, 0.8, 0.9
Figure 1: Five annotations per token, supplied by 6
different annotators (- = missing annotation), gold
label y. ? = competence values for each annotator.
2
http://www.ark.cs.cmu.edu/TweetNLP/
3
http://crowdflower.com
3 Crowdsourcing Sequential Annotation
In order to use the annotations to train models that
can be applied across various data sets, i.e., mak-
ing out-of-sample evaluation possible (see Section
5), we follow Hovy et al (2014) in using the uni-
versal tag set (Petrov et al, 2012) with 12 labels.
Figure 2: Screen shot of the annotation interface
on Crowdflower
Annotators were given a bold-faced word with
two words on either side and asked to select the
most appropriate tag from a drop down menu. For
each tag, we spell out the name of the syntactic
category, and provide a few example words.
See Figure 2 for a screenshot of the interface.
Annotators were also told that words can belong
to several classes, depending on the context. No
additional guidelines were given.
Only trusted annotators (in Crowdflower:
Bronze skills) that had answered correctly on 4
gold tokens (randomly chosen from a set of 20
gold tokens provided by the authors) were allowed
to submit annotations. In total, 177 individual
annotators supplied answers. We paid annotators
a reward of $0.05 for 10 tokens. The full data set
contains 14,619 tokens. Completion of the task
took slightly less than 10 days. Contributors were
very satisfied with the task (4.5 on a scale from 1
to 5). In particular, they felt instructions were clear
(4.4/5), and that the pay was reasonable (4.1/5).
4 Label Aggregation
After collecting the annotations, we need to aggre-
gate the annotations to derive a single answer for
each token. In the simplest scheme, we choose the
majority label, i.e., the label picked by most an-
notators. In case of ties, we select the final label
at random. Since this is a stochastic process, we
average results over 100 runs. We refer to this as
MAJORITY VOTING (MV). Note that in MV we
trust all annotators to the same degree. However,
crowdsourcing attracts people with different mo-
378
tives, and not all of them are equally reliable?
even the ones with Bronze level. Ideally, we would
like to factor this into our decision process.
We use MACE
4
(Hovy et al, 2013) as our sec-
ond scheme to learn both the most likely answer
and a competence estimate for each of the annota-
tors. MACE treats annotator competence and the
correct answer as hidden variables and estimates
their parameters via EM (Dempster et al, 1977).
We use MACE with default parameter settings to
give us the weighted average for each annotated
example.
Finally, we also tried applying the joint learn-
ing scheme in Rodrigues et al (2013), but their
scheme requires that entire sequences are anno-
tated by the same annotators, which we don?t have,
and it expects BIO sequences, rather than POS
tags.
Dictionaries Decoding tasks profit from the use
of dictionaries (Merialdo, 1994; Johnson, 2007;
Ravi and Knight, 2009) by restricting the number
of tags that need to be considered for each word,
also known as type constraints (T?ackstr?om et al,
2013). We follow Li et al (2012) in including
Wiktionary information as type constraints into
our decoding: if a word is found in Wiktionary,
we disregard all annotations that are not licensed
by the dictionary entry. If the word is not found in
Wiktionary, or if none of its annotations is licensed
by Wiktionary, we keep the original annotations.
Since we aggregate annotations independently
(unlike Viterbi decoding), we basically use Wik-
tionary as a pre-filtering step, such that MV and
MACE only operate on the reduced annotations.
5 Experiments
Each of the two aggregation schemes above pro-
duces a final label sequence y? for our training cor-
pus. We evaluate the resulting annotated data in
three ways.
1. We compare y? to the available expert annota-
tion on the training data. This tells us how similar
lay annotation is to professional annotation.
2. Ultimately, we want to use structured anno-
tations for supervised training, where annotation
quality influences model performance on held-out
test data. To test this, we train a CRF model
(Lafferty et al, 2001) with simple orthographic
features and word clusters (Owoputi et al, 2013)
4
http://www.isi.edu/publications/
licensed-sw/mace/
on the annotated Twitter data described in Gim-
pel et al (2011). Leaving out the dedicated test
set to avoid in-sample bias, we evaluate our mod-
els across three data sets: RITTER (the 10% test
split of the data in Ritter et al (2011) used in Der-
czynski et al (2013)), the test set from Foster et
al. (2011), and the data set described in Hovy et
al. (2014).
We will make the preprocessed data sets avail-
able to the public to facilitate comparison. In ad-
dition to a supervised model trained on expert an-
notations, we compare our tagging accuracy with
that of a weakly supervised system (Li et al, 2012)
re-trained on 400,000 unlabeled tweets to adapt to
Twitter, but using a crowdsourced lexicon, namely
Wiktionary, to constrain inference. We use param-
eter settings from Li et al (2012), as well as their
Wikipedia dump, available from their project web-
site.
5
3. POS tagging is often the first step for further
analysis, such as chunking, parsing, etc. We
test the downstream performance of the POS
models from the previous step on chunking and
NER. We use the models to annotate the training
data portion of each task with POS tags, and
use them as features in a chunking and NER
model. For both tasks, we train a CRF model
on the respective (POS-augmented) training set,
and evaluate it on several held-out test sets. For
chunking, we use the test sets from Foster et al
(2011) and Ritter et al (2011) (with the splits
from Derczynski et al (2013)). For NER, we use
data from Finin et al (2010) and again Ritter et al
(2011). For chunking, we follow Sha and Pereira
(2003) for the set of features, including token
and POS information. For NER, we use standard
features, including POS tags (from the previous
experiments), indicators for hyphens, digits,
single quotes, upper/lowercase, 3-character prefix
and suffix information, and Brown word cluster
features
6
with 2,4,8,16 bitstring prefixes estimated
from a large Twitter corpus (Owoputi et al, 2013).
We report macro-averages over all these data sets.
6 Results
Agreement with expert annotators Table 1
shows the accuracy of each aggregation compared
to the gold labels. The crowdsourced annotations
5
https://code.google.com/p/
wikily-supervised-pos-tagger/
6
http://www.ark.cs.cmu.edu/TweetNLP/
379
majority 79.54
MACE-EM 79.89
majority+Wiktionary 80.58
MACE-EM+Wiktionary 80.75
oracle 89.63
Table 1: Accuracy (%) of different annotations wrt
gold data
aggregated using MV agree with the expert anno-
tations in 79.54% of the cases. If we pre-filter the
data using Wiktionary, the agreement becomes
80.58%. MACE leads to higher agreement with
expert annotations under both conditions (79.89
and 80.75). The small difference indicates that
annotators are consistent and largely reliable,
thus confirming the Bronze-level qualification
we required. Both schemes cannot recover the
correct answer for the 1497 cases where none of
the crowdsourced labels matched the gold label,
i.e. y /? Z
i
. The best possible result either of them
could achieve (the oracle) would be matching all
but the missing labels, an agreement of 89.63%.
Most of the cases where the correct label was
not among the annotations belong to a small set
of confusions. The most frequent was mislabeling
?:? and ?. . .?, both mapped to X. Annotators
mostly decided to label these tokens as punctu-
ation (.). They also predominantly labeled your,
my and this as PRON (for the former two), and a
variety of labels for the latter, when the gold label
is DET.
RITTER FOSTER HOVY
Li et al (2012) 73.8 77.4 79.7
MV 80.5 81.6 83.7
MACE 80.4 81.7 82.6
MV+Wik 80.4 82.1 83.7
MACE+Wik 80.5 81.9 83.7
Upper bounds
oracle 82.4 83.7 85.1
gold 82.6 84.7 86.8
Table 2: POS tagging accuracies (%).
Effect on POS Tagging Accuracy Usually, we
don?t want to match a gold standard, but we
rather want to create new annotated training
data. Crowdsourcing matches our gold standard
to about 80%, but the question remains how useful
this data is when training models on it. After all,
inter-annotator agreement among professional an-
notators on this task is only around 90% (Gimpel
et al, 2011; Hovy et al, 2014). In order to evalu-
ate how much each aggregation scheme influences
tagging performance of the resulting model, we
train separate models on each scheme?s annota-
tions and test on the same four data sets. Table
2 shows the results. Note that the differences be-
tween the four schemes are insignificant. More
importantly, however, POS tagging accuracy us-
ing crowdsourced annotations are on average only
2.6% worse than gold using professional annota-
tions. On the other hand, performance is much
better than the weakly supervised approach by Li
et al (2012), which only relies on a crowdsourced
POS lexicon.
POS model from CHUNKING NER
MV 74.80 75.74
MACE 75.04 75.83
MV+Wik 75.86 76.08
MACE+Wik 75.86 76.15
Upper bounds
oracle 76.22 75.85
gold 79.97 75.81
Table 3: Downstream accuracy for chunking (l)
and NER (r) of models using POS.
Downstream Performance Table 3 shows the
accuracy when using the POS models trained
in the previous evaluation step. Note that we
present the average over the two data sets used
for each task. Note also how the Wiktionary con-
straints lead to improvements in downstream per-
formance. In chunking, we see that using the
crowdsourced annotations leads to worse perfor-
mance than using the professional annotations.
For NER, however, we find that some of the POS
taggers trained on aggregated data produce bet-
ter NER performance than POS taggers trained on
expert-annotated gold data. Since the only dif-
ference between models are the respective POS
features, the results suggest that at least for some
tasks, POS taggers learned from crowdsourced an-
notations may be as good as those learned from
expert annotations.
7 Related Work
There is considerable work in the literature on
modeling answer correctness and annotator com-
petence as latent variables (Dawid and Skene,
380
1979; Smyth et al, 1995; Carpenter, 2008; White-
hill et al, 2009; Welinder et al, 2010; Yan et al,
2010; Raykar and Yu, 2012). Rodrigues et al
(2013) recently presented a sequential model for
this. They estimate annotator competence as la-
tent variables in a CRF model using EM. They
evaluate their approach on synthetic and NER data
annotated on Mechanical Turk, showing improve-
ments over the MV baselines and the multi-label
model by Dredze et al (2009). The latter do not
model annotator reliability but rather model label
priors by integrating them into the CRF objective,
and re-estimating them during learning. Both re-
quire annotators to supply a full sentence, while
we use minimal context, which requires less anno-
tator commitment and makes the task more flexi-
ble. Unfortunately, we could not run those mod-
els on our data due to label incompatibility and
the fact that we typically do not have complete se-
quences annotated by the same annotators.
Mainzer (2011) actually presents an earlier pa-
per on crowdsourcing POS tagging. However, it
differs from our approach in several ways. It uses
the Penn Treebank tag set to annotate Wikipedia
data (which is much more canonical than Twitter)
via a Java applet. The applet automatically labels
certain categories, and only presents the users with
a series of multiple choice questions for the re-
mainder. This is highly effective, as it eliminates
some sources of possible disagreement. In con-
trast, we do not pre-label any tokens, but always
present the annotators with all labels.
8 Conclusion
We use crowdsourcing to collect POS annotations
with minimal context (five-word windows). While
the performance of POS models learned from
this data is still slightly below that of models
trained on expert annotations, models learned
from aggregations approach oracle performance
for POS tagging. In general, we find that the
use of a dictionary tends to make aggregations
more useful, irrespective of aggregation method.
For some downstream tasks, models using the
aggregated POS tags perform even better than
models using expert-annotated tags.
Acknowledgments
We would like to thank the anonymous review-
ers for valuable comments and feedback. This re-
search is funded by the ERC Starting Grant LOW-
LANDS No. 313695.
References
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing Speech and Language Data With Amazon?s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Technical report, Ling-
Pipe.
A. Philip Dawid and Allan M. Skene. 1979. Max-
imum likelihood estimation of observer error-rates
using the EM algorithm. Applied Statistics, pages
20?28.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39(1):1?
38.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Mark Dredze, Partha Pratim Talukdar, and Koby Cram-
mer. 2009. Sequence learning from data with multi-
ple labels. In ECML/PKDD Workshop on Learning
from Multi-Label Data.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with
crowdsourcing. In NAACL-HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-Speech Tagging
for Twitter: Annotation, Features, and Experiments.
In ACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014.
When pos datasets don t add up: Combatting sample
bias. In LREC.
381
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara
Rosenthal, and Kathleen McKeown. 2010. Corpus
creation for new genres: A crowdsourced approach
to pp attachment. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk. Association
for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Shen Li, Jo?ao Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In EMNLP.
Jacob Emil Mainzer. 2011. Labeling parts of
speech using untrained annotators on mechanical
turk. Master?s thesis, The Ohio State University.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational linguistics,
20(2):155?171.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Sujith Ravi and Kevin Knight. 2009. Minimized Mod-
els for Unsupervised Part-of-Speech Tagging. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP. Association for Computational Lin-
guistics.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminat-
ing Spammers and Ranking Annotators for Crowd-
sourced Labeling Tasks. Journal of Machine Learn-
ing Research, 13:491?518.
Alan Ritter, Sam Clark, Oren Etzioni, et al 2011.
Named entity recognition in tweets: an experimental
study. In EMNLP.
Filipe Rodrigues, Francisco Pereira, and Bernardete
Ribeiro. 2013. Sequence labeling with multiple an-
notators. Machine Learning, pages 1?17.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. Ad-
vances in neural information processing systems,
pages 1085?1092.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, Mar(1):1?12.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In NIPS.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. Advances in
Neural Information Processing Systems, 22:2035?
2043.
Yan Yan, R?omer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
Jennifer Dy. 2010. Modeling annotator exper-
tise: Learning when everybody knows a bit of some-
thing. In International Conference on Artificial In-
telligence and Statistics.
382
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 507?511,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Linguistically debatable or just plain wrong?
Barbara Plank, Dirk Hovy and Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140, DK-2300 Copenhagen S
bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk
Abstract
In linguistic annotation projects, we typ-
ically develop annotation guidelines to
minimize disagreement. However, in this
position paper we question whether we
should actually limit the disagreements
between annotators, rather than embracing
them. We present an empirical analysis
of part-of-speech annotated data sets that
suggests that disagreements are systematic
across domains and to a certain extend also
across languages. This points to an un-
derlying ambiguity rather than random er-
rors. Moreover, a quantitative analysis of
tag confusions reveals that the majority of
disagreements are due to linguistically de-
batable cases rather than annotation errors.
Specifically, we show that even in the ab-
sence of annotation guidelines only 2% of
annotator choices are linguistically unmo-
tivated.
1 Introduction
In NLP, we often model annotation as if it re-
flected a single ground truth that was guided by
an underlying linguistic theory. If this was true,
the specific theory should be learnable from the
annotated data. However, it is well known that
there are linguistically hard cases (Zeman, 2010),
where no theory provides a clear answer, so an-
notation schemes commit to more or less arbi-
trary decisions. For example, in parsing auxil-
iary verbs may head main verbs, or vice versa,
and in part-of-speech (POS) tagging, possessive
pronouns may belong to the category of deter-
miners or the category of pronouns. This posi-
tion paper argues that annotation projects should
embrace these hard cases rather than pretend they
can be unambiguously resolved. Instead of using
overly specific annotation guidelines, designed to
minimize inter-annotator disagreement (Duffield
et al, 2007), and adjudicating between annotators
of different opinions, we should embrace system-
atic inter-annotator disagreements. To motivate
this, we present an empirical analysis showing
1. that certain inter-annotator disagreements are
systematic, and
2. that actual errors are in fact so infrequent as
to be negligible, even when linguists annotate
without guidelines.
The empirical analysis presented below relies
on text corpora annotated with syntactic cate-
gories or parts-of-speech (POS). POS is part of
most linguistic theories, but nevertheless, there
are still many linguistic constructions ? even very
frequent ones ? whose POS analysis is widely
debated. The following sentences exemplify some
of these hard cases that annotators frequently
disagree on. Note that we do not claim that both
analyses in each of these cases (1?3) are equally
good, but that there is some linguistic motivation
for either analysis in each case.
(1) Noam goes out tonight
NOUN VERB ADP/PRT ADV/NOUN
(2) Noam likes social media
NOUN VERB ADJ/NOUN NOUN
(3) Noam likes his car
NOUN VERB DET/PRON NOUN
To substantiate our claims, we first compare
the distribution of inter-annotator disagreements
across domains and languages, showing that most
disagreements are systematic (Section 2). This
suggests that most annotation differences derive
from hard cases, rather than random errors.
We then collect a corpus of such disagreements
and have experts mark which ones are due to ac-
tual annotation errors, and which ones reflect lin-
guistically hard cases (Section 3). The results
show that the majority of disagreements are due
507
to hard cases, and only about 20% of conflict-
ing annotations are actual errors. This suggests
that inter-annotator agreement scores often hide
the fact that the vast majority of annotations are
actually linguistically motivated. In our case, less
than 2% of the overall annotations are linguisti-
cally unmotivated.
Finally, in Section 4, we present an experiment
trying to learn a model to distinguish between hard
cases and annotation errors.
2 Annotator disagreements across
domains and languages
In this study, we had between 2-10 individual an-
notators with degrees in linguistics annotate dif-
ferent kinds of English text with POS tags, e.g.,
newswire text (PTB WSJ Section 00), transcripts
of spoken language (from a database containing
transcripts of conversations, Talkbank
1
), as well
as Twitter posts. Annotators were specifically not
presented with guidelines that would help them re-
solve hard cases. Moreover, we compare profes-
sional annotation to that of lay people. We in-
structed annotators to use the 12 universal POS
tags of Petrov et al (2012). We did so in or-
der to make comparison between existing data
sets possible. Moreover, this allows us to fo-
cus on really hard cases, as any debatable case in
the coarse-grained tag set is necessarily also part
of the finer-grained tag set.
2
For each domain,
we collected exactly 500 doubly-annotated sen-
tences/tweets. Besides these English data sets, we
also obtained doubly-annotated POS data from the
French Social Media Bank project (Seddah et al,
2012).
3
All data sets, except the French one, are
publicly available at http://lowlands.ku.
dk/.
We present disagreements as Hinton diagrams
in Figure 1a?c. Note that the spoken language data
does not include punctuation. The correlations
between the disagreements are highly significant,
with Spearman coefficients ranging from 0.644
1
http://talkbank.org/
2
Experiments with variation n-grams on WSJ (Dickinson
and Meurers, 2003) and the French data lead us to estimate
that the fine-to-coarse mapping of POS tags disregards about
20% of observed tag-pair confusion types, most of which re-
late to fine-grained verb and noun distinctions, e.g. past par-
ticiple versus past in ?[..] criminal lawyers speculated/VBD
vs. VBN that [..]?.
3
We mapped POS tags into the universal POS tags using
the mappings available here: https://code.google.
com/p/universal-pos-tags/
(spoken and WSJ) to 0.869 (spoken and Twit-
ter). Kendall?s ? ranges from 0.498 (Twitter and
WSJ) to 0.659 (spoken and Twitter). All diagrams
have a vaguely ?dagger?-like shape, with the blade
going down the diagonal from top left to bot-
tom right, and a slightly curved ?hilt? across the
counter-diagonal, ending in the more pronounced
ADP/PRT confusion cells.
Disagreements are very similar across all three
domains. In particular, adpositions (ADP) are con-
fused with particles (PRT) (as in the case of ?get
out?); adjectives (ADJ) are confused with nouns
(as in ?stone lion?); pronouns (PRON) are con-
fused with determiners (DET) (?my house?); nu-
merals are confused with adjectives, determiners,
and nouns (?2nd time?); and adjectives are con-
fused with adverbs (ADV) (?see you later?). In
Twitter, the X category is often confused with
punctuations, e.g., when annotating punctuation
acting as discourse continuation marker.
Our analyses show that a) experts disagree on
the known hard cases when freely annotating text,
and b) that these disagreements are the same
across text types. More surprisingly, though, we
also find that, as discussed next, c) roughly the
same disagreements are also observed when com-
paring the linguistic intuitions of lay people.
More specifically, we had lay annotators on the
crowdsourcing platform Crowdflower re-annotate
the training section of Gimpel et al (2011). They
collected five annotations per word. Only annota-
tors that had answered correctly on 4 gold items
(randomly chosen from a set of 20 gold items
provided by the authors) were allowed to submit
annotations. In total, 177 individual annotators
supplied answers. We paid annotators a reward
of $0.05 for 10 items. The full data set con-
tains 14,619 items and is described in further de-
tail in Hovy et al (2014). Annotators were satis-
fied with the task (4.5 on a scale from 1 to 5) and
felt that instructions were clear (4.4/5), and the pay
reasonable (4.1/5). The crowdsourced annotations
aggregated using majority voting agree with the
expert annotations in 79.54% of the cases. If we
pre-filter the data via Wiktionary and use an item-
response model (Hovy et al, 2013) rather than ma-
jority voting, the agreement rises to 80.58%.
Figure 2 presents the Hinton diagram of the dis-
agreements of lay people. Disagreements are very
similar to the disagreements between expert an-
notators, especially on Twitter data (Figure 1b).
508
a) b) c)
Figure 1: Hinton diagrams of inter-annotator disagreement on (a) excerpt from WSJ (Marcus et al,
1993), (b) random Twitter sample, and (c) pre-transcribed spoken language excerpts from talkbank.org
One difference is that lay people do not confuse
numerals very often, probably because they rely
more on orthographic cues than on distributional
evidence. The disagreements are still strongly cor-
related with the ones observed with expert anno-
tators, but at a slightly lower coefficient (with a
Spearman?s ? of 0.493 and Kendall?s ? of 0.366
for WSJ).
Figure 2: Disagreement between lay annotators
Lastly, we compare the disagreements of anno-
tators on a French social media data set (Seddah et
al., 2012), which we mapped to the universal POS
tag set. Again, we see the familiar dagger shape.
The Spearman coefficient with English Twitter is
0.288; Kendall?s ? is 0.204. While the correlation
is weaker across languages than across domains, it
remains statistically significant (p < 0.001).
3 Hard cases and annotation errors
In the previous section, we demonstrated that
some disagreements are consistent across domains
and languages. We noted earlier, though, that dis-
agreements can arise both from hard cases and
from annotation errors. This can explain some
Figure 3: Disagreement on French social media
of the variation. In this section, we investigate
what happens if we weed out obvious errors by
detecting annotation inconsistencies across a cor-
pus. The disagreements that remain are the truly
hard cases.
We use a modified version of the a priori algo-
rithm introduced in Dickinson and Meurers (2003)
to identify annotation inconsistencies. It works
by collecting ?variation n-grams?, i.e. the longest
sequence of words (n-gram) in a corpus that has
been observed with a token being tagged differ-
ently in another occurence of the same n-gram in
the same corpus. The algorithm starts off by look-
ing for unigrams and expands them until no longer
n-grams are found.
For each variation n-gram that we found in
WSJ-00, i.e, a word in various contexts and the
possible tags associated with it, we present anno-
tators with the cross product of contexts and tags.
Essentially, we ask for a binary decision: Is the tag
plausible for the given context?
We used 3 annotators with PhD degrees in lin-
guistics. In total, our data set contains 880 items,
509
i.e. 440 annotated confusion tag pairs. The raw
agreement was 86%. Figure 4 shows how truly
hard cases are distributed over tag pairs (dark gray
bars), as well as the proportion of confusions with
respect to a given tag pair that are truly hard cases
(light gray bars). The figure shows, for instance,
that the variation n-gram regarding ADP-ADV is
the second most frequent one (dark gray), and
approximately 70% of ADP-ADV disagreements
are linguistically hard cases (light gray). NOUN-
PRON disagreements are always linguistically de-
batable cases, while they are less frequent.
Figure 4: Relative frequency of hard cases
A survey of hard cases. To further test the idea
of there being truly hard cases that probably can-
not be resolved by linguistic theory, we presented
nine linguistics faculty members with 10 of the
above examples and asked them to pick their fa-
vorite analyses. In 8/10 cases, the faculty mem-
bers disagreed on the right analysis.
4 Learning to detect annotation errors
In this section, we examine whether we can learn
a classifier to distinguish between hard cases and
annotation errors. In order to do so, we train a clas-
sifier on the annotated data set containing 440 tag-
confusion pairs by relying only on surface form
features. If we balance the data set and perform 3-
fold cross-validation, a L2-regularized logistic re-
gression (L2-LR) model achieves an f
1
-score for
detecting errors at 70% (cf. Table 1), which is
above average, but not very impressive.
The two classes are apparently not easily sepa-
rable using surface form features, as illustrated in
f
1
HARD CASES ERRORS
L2-LR 73%(71-77) 70%(65-75)
NN 76%(76-77) 71%(68-72)
Table 1: Classification results
Figure 5: Hard cases and errors in 2d-PCA
the two-dimensional plot in Figure 5, obtained us-
ing PCA. The logistic regression decision bound-
ary is plotted as a solid, black line. This is prob-
ably also why the nearest neighbor (NN) classi-
fier does slightly better, but again, performance is
rather low. While other features may reveal that
the problem is in fact learnable, our initial experi-
ments lead us to conclude that, given the low ratio
of errors over truly hard cases, learning to detect
errors is often not worthwhile.
5 Related work
Juergens (2014) presents work on detecting lin-
guistically hard cases in the context of word
sense annotations, e.g., cases where expert an-
notators will disagree, as well as differentiat-
ing between underspecified, overspecified and
metaphoric cases. This work is similar to ours in
spirit, but considers a very different task. While
we also quantify the proportion of hard cases and
present an analysis of these cases, we also show
that disagreements are systematic.
Our work also relates to work on automatically
correcting expert annotations for inconsistencies
(Dickinson and Meurers, 2003). This work is
very different in spirit from our work, but shares
an interest in reconsidering expert annotations,
and we made use of their mining algorithm here.
There has also been recent work on adjudicat-
510
ing noisy crowdsourced annotations (Dawid and
Skene, 1979; Smyth et al, 1995; Carpenter, 2008;
Whitehill et al, 2009; Welinder et al, 2010; Yan
et al, 2010; Raykar and Yu, 2012; Hovy et al,
2013). Again, their objective is orthogonal to
ours, namely to collapse multiple annotations into
a gold standard rather than embracing disagree-
ments.
Finally, Plank et al (2014) use small samples of
doubly-annotated POS data to estimate annotator
reliability and show how those metrics can be im-
plemented in the loss function when inducing POS
taggers to reflect confidence we can put in annota-
tions. They show that not biasing the theory to-
wards a single annotator but using a cost-sensitive
learning scheme makes POS taggers more robust
and more applicable for downstream tasks.
6 Conclusion
In this paper, we show that disagreements between
professional or lay annotators are systematic and
consistent across domains and some of them are
systematic also across languages. In addition, we
present an empirical analysis of POS annotations
showing that the vast majority of inter-annotator
disagreements are competing, but valid, linguis-
tic interpretations. We propose to embrace such
disagreements rather than using annotation guide-
lines to optimize inter-annotator agreement, which
would bias our models in favor of some linguistic
theory.
Acknowledgements
We would like to thank the anonymous reviewers
for their feedback, as well as Djam?e Seddah for the
French data. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
References
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Technical report, Ling-
Pipe.
A. Philip Dawid and Allan M. Skene. 1979. Max-
imum likelihood estimation of observer error-rates
using the EM algorithm. Applied Statistics, pages
20?28.
Markus Dickinson and Detmar Meurers. 2003. Detect-
ing errors in part-of-speech annotation. In EACL.
Cecily Duffield, Jena Hwang, Susan Brown, Dmitriy
Dligach, Sarah Vieweg, Jenny Davis, and Martha
Palmer. 2007. Criteria for the manual grouping of
verb senses. In LAW.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-Speech Tagging
for Twitter: Annotation, Features, and Experiments.
In ACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders S?gaard. 2014.
Experiments with crowdsourced re-annotation of a
POS tagging data set. In ACL.
David Juergens. 2014. An analysis of ambiguity in
word sense annotations. In LREC.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In EACL.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminat-
ing Spammers and Ranking Annotators for Crowd-
sourced Labeling Tasks. Journal of Machine Learn-
ing Research, 13:491?518.
Djam?e Seddah, Benoit Sagot, Marie Candito, Virginie
Mouilleron, and Vanessa Combet. 2012. The
French Social Media Bank: a treebank of noisy user
generated content. In COLING.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. In NIPS.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In NIPS.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. In NIPS.
Yan Yan, R?omer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
Jennifer Dy. 2010. Modeling annotator expertise:
Learning when everybody knows a bit of something.
In AIStats.
Daniel Zeman. 2010. Hard problems of tagset con-
version. In Proceedings of the Second International
Conference on Global Interoperability for Language
Resources.
511
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 1?11,
Dublin, Ireland, August 23-24 2014.
More or less supervised supersense tagging of Twitter
Anders Johannsen, Dirk Hovy, H
?
ector Mart??nez Alonso, Barbara Plank, Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140
ajohannsen@hum.ku.dk, dirk@cst.dk, alonso@hum.ku.dk
plank@cst.dk, soegaard@hum.ku.dk
Abstract
We present two Twitter datasets annotated
with coarse-grained word senses (super-
senses), as well as a series of experiments
with three learning scenarios for super-
sense tagging: weakly supervised learn-
ing, as well as unsupervised and super-
vised domain adaptation. We show that
(a) off-the-shelf tools perform poorly on
Twitter, (b) models augmented with em-
beddings learned from Twitter data per-
form much better, and (c) errors can be
reduced using type-constrained inference
with distant supervision from WordNet.
1 Introduction
Supersense tagging (SST, Ciaramita and Altun,
2006) is the task of assigning high-level ontolog-
ical classes to open-class words (here, nouns and
verbs). It is thus a coarse-grained word sense dis-
ambiguation task. The labels are based on the lexi-
cographer file names for Princeton WordNet (Fell-
baum, 1998). They include 15 senses for verbs
and 26 for nouns (see Table 1). While WordNet
also provides catch-all supersenses for adjectives
and adverbs, these are grammatically, not seman-
tically motivated, and do not provide any higher-
level abstraction (recently, however, Tsvetkov et
al. (2014) proposed a semantic taxonomy for ad-
jectives). They will not be considered in this paper.
Coarse-grained categories such as supersenses
are useful for downstream tasks such as question-
answering (QA) and open relation extraction (RE).
SST is different from NER in that it has a larger set
of labels and in the absence of strong orthographic
cues (capitalization, quotation marks, etc.). More-
over, supersenses can be applied to any of the lex-
ical parts of speech and not only proper names.
Also, while high-coverage gazetteers can be found
for named entity recognition, the lexical resources
available for SST are very limited in coverage.
Twitter is a popular micro-blogging service,
which, among other things, is used for knowledge
sharing among friends and peers. Twitter posts
(tweets) announce local events, say talks or con-
certs, present facts about pop stars or program-
ming languages, or simply express the opinions of
the author on some subject matter.
Supersense tagging is relevant for Twitter, be-
cause it can aid e.g. QA and open RE. If someone
posts a message saying that some LaTeX module
now supports ?drawing trees?, it is important to
know whether the post is about drawing natural
objects such as oaks or pines, or about drawing
tree-shaped data representations.
This paper is, to the best of our knowledge, the
first work to address the problem of SST for Twit-
ter. While there exist corpora of newswire and
literary texts that are annotated with supersenses,
e.g., SEMCOR (Miller et al., 1994), no data is
available for microblogs or related domains. This
paper introduces two new data sets.
Furthermore, most, if not all, of previous work
on SST has relied on gold standard part-of-speech
(POS) tags as input. However, in a domain such
as Twitter, which has proven to be challenging
for POS tagging (Foster et al., 2011; Ritter et
al., 2011), results obtained under the assumption
of available perfect POS information are almost
meaningless for any real-life application.
In this paper, we instead use predicted POS tags
and investigate experimental settings in which one
or more of the following resources are available to
us:
? a large corpus of unlabeled Twitter data;
? Princeton WordNet (Fellbaum, 1998);
? SEMCOR (Miller et al., 1994); and
? a small corpus of Twitter data annotated with
supersenses.
We approach SST of Twitter using various de-
grees of supervision for both learning and domain
adaptation (here, from newswire to Twitter). In
1
weakly supervised learning, only unlabeled data
and the lexical resource WordNet are available to
us. While the quality of lexical resources varies,
this is the scenario for most languages. We present
an approach to weakly supervised SST based on
type-constrained EM-trained second-order HMMs
(HMM2s) with continuous word representations.
In contrast, when using supervised learning, we
can distinguish between two degrees of supervi-
sion for domain adaptation. For some languages,
e.g., Basque, English, Swedish, sense-annotated
resources exist, but these corpora are all limited
to newswire or similar domains. In such lan-
guages, unsupervised domain adaptation (DA)
techniques can be used to exploit these resources.
The setting does not presume labeled data from
the target domain. We use discriminative mod-
els for unsupervised domain adaptation, training
on SEMCOR and testing on Twitter.
Finally, we annotated data sets for Twitter, mak-
ing supervised domain adaptation (SU) exper-
iments possible. For supervised domain adapta-
tion, we use the annotated training data sets from
both the newswire and the Twitter domain, as well
as WordNet.
For both unsupervised domain adaptation and
supervised domain adaptation, we use structured
perceptron (Collins, 2002), i.e., a discriminative
HMM model, and search-based structured predic-
tion (SEARN) (Daume et al., 2009). We aug-
ment both the EM-trained HMM2, discrimina-
tive HMMs and SEARN with type constraints and
continuous word representations. We also exper-
imented with conditional random fields (Lafferty
et al., 2001), but obtained worse or similar results
than with the other models.
Contributions In this paper, we present two
Twitter data sets with manually annotated su-
persenses, as well as a series of experiments
with these data sets. These experiments cover
existing approaches to related tasks, as well as
some new methods. In particular, we present
type-constrained extensions of discriminative
HMMs and SEARN sequence models with con-
tinuous word representations that perform well.
We show that when no in-domain labeled data
is available, type constraints improve model
performance considerably. Our best models
achieve a weighted average F1 score of 57.1 over
nouns and verbs on our main evaluation data
set, i.e., a 20% error reduction over the most
frequent sense baseline. The two annotated Twit-
ter data sets are publicly released for download
at https://github.com/coastalcph/
supersense-data-twitter.
n.Tops n.object v.cognition
n.act n.person v.communication
n.animal n.phenomenon v.competition
n.artifact n.plant v.consumption
n.attribute n.possession v.contact
n.body n.process v.creation
n.cognition n.quantity v.emotion
n.communication n.relation v.motion
n.event n.shape v.perception
n.feeling n.state v.possession
n.food n.substance v.social
n.group n.time v.stative
n.location v.body v.weather
n.motive v.change
Table 1: The 41 noun and verb supersenses in
WordNet
2 More or less supervised models
This sections covers the varying degree of super-
vision of our systems as well as the usage of type
constraints as distant supervision.
2.1 Distant supervision
Distant supervision in these experiments was im-
plemented by only allowing a system to predict
a certain supersense for a given word if that su-
persense had either been observed in the training
data, or, for unobserved words, if the sense was
the most frequent sense in WordNet. If the word
did not appear in the training data nor in WordNet,
no filtering was applied. We refer to the distant-
supervision strategy as type constraints.
Distant supervision was implemented differ-
ently in SEARN and the HMM model. SEARN
decomposes sequential labelling into a series of
binary classifications. To constrain the labels we
simply pick the top-scoring sense for each token
from the allowed set. Structured perceptron uses
Viterbi decoding. Here we set the emission prob-
abilities for disallowed senses to negative infinity
and decode as usual.
2.2 Weakly supervised HMMs
The HMM2 model is a second-order hidden
Markov model (Mari et al., 1997; Thede and
Harper, 1999) using logistic regression to estimate
emission probabilities. In addition we constrain
2
w1
t
1
t
2
P(t
2
|t
1
)
P(w
1
|t
1
)
t
3
w
2
w
3
Figure 1: HMM2 with continuous word represen-
tations
the inference space of the HMM2 tagger using
type-level tag constraints derived from WordNet,
leading to roughly the model proposed by Li et
al. (2012), who used Wiktionary as a (part-of-
speech) tag dictionary. The basic feature model
of Li et al. (2012) is augmented with continuous
word representation features as shown in Figure 1,
and our logistic regression model thus works over
a combination of discrete and continuous variables
when estimating emission probabilities. We do 50
passes over the data as in Li et al. (2012).
We introduce two simplifications for the HMM2
model. First, we only use the most frequent senses
(k = 1) in WordNet as type constraints. The
most frequent senses seem to better direct the EM
search for a local optimum, and we see dramatic
drops in performance on held-out data when we
include more senses for the words covered by
WordNet. Second, motivated by computational
concerns, we only train and test on sequences of
(predicted) nouns and verbs, leaving out all other
word classes. Our supervised models performed
slightly worse on shortened sequences, and it is an
open question whether the HMM2 models would
perform better if we could train them on full sen-
tences.
2.3 Structured perceptron and SEARN
We use two approaches to supervised sequen-
tial labeling, structured perceptron (Collins, 2002)
and search-based structured prediction (SEARN)
(Daume et al., 2009). The structured perceptron
is a in-house reimplementation of Ciaramita and
Altun (2006).
1
SEARN performed slightly better
than structured perceptron, so we use it as our in-
house baseline in the experiments below. In this
section, we briefly explain the two approaches.
1
https://github.com/coastalcph/
rungsted
2.3.1 Structured perceptron (HMM)
Structured perceptron learning was introduced in
Collins (2002) and is an extension of the online
perceptron learning algorithm (Rosenblatt, 1958)
with averaging (Freund and Schapire, 1999) to
structured learning problems such as sequence la-
beling.
In structured perceptron for sequential labeling,
where we learn a function from sequences of data
points x
1
. . . x
n
to sequences of labels y
1
. . . y
n
,
we begin with a random weight vector w
0
initial-
ized to all zeros. This weight vector is used to
assign weights to transitions between labels, i.e.,
the discriminative counterpart of P (y
i+1
| y
i
), and
emissions of tokens given labels, i.e., the counter-
part of P (x
i
| y
i
). We use Viterbi decoding to de-
rive a best path
?
y through the correspondingm?n
lattice (with m the number of labels). Let the fea-
ture mapping ?(x,y) be a function from a pair
of sequences ?x,y? to all the features that fired
to make y the best path through the lattice for x.
Now the structured update for a sequence of data
points is simply ?(?(x,y)??(x,
?
y)), i.e., a fixed
positive update of features that fired to produce the
correct sequence of labels, and a fixed negative up-
date of features that fired to produce the best path
under the model. Note that if y =
?
y, no features
are updated.
2.3.2 SEARN
SEARN is a way of decomposing structured pre-
diction problems into search and history-based
classification. In sequential labeling, we decom-
pose the sequence of m tokens into m classifica-
tion problems, conditioning our labeling of the ith
token on the history of i ? 1 previous decisions.
The cost of a mislabeling at training time is de-
fined by a cost function over output structures. We
use Hamming loss rather than F
1
as our cost func-
tion, and we then use stochastic gradient descent
with quantile loss as a our cost-sensitive learning
algorithm. We use a publicly available implemen-
tation.
2
3 Experiments
We experiment with weakly supervised learning,
unsupervised domain adaptation, as well as su-
pervised domain adaptation, i.e., where our mod-
els are induced from hand-annotated newswire
and Twitter data. Note that in all our experiments,
2
http://hunch.net/
?
vw/
3
we use predicted POS tags as input to the system,
in order to produce a realistic estimate of SST per-
formance.
3.1 Data
Our experiments rely on combinations of available
resources and newly annotated Twitter data sets
made publicly available with this paper.
3.1.1 Available resources
Princeton WordNet (Fellbaum, 1998) is the main
resource for SST. The lexicographer file names
provide the label alphabet of the task, and the tax-
onomy defined therein is used not only in the base-
lines, but also as a feature in the discriminative
models. We use the WordNet 3.0 distribution.
SEMCOR (Miller et al., 1994) is a sense-
annotated corpus composed of 80% newswire and
20% literary text, using the sense inventory from
WordNet. SEMCOR comprises 23k distinct lem-
mas in 234k instances. We use the texts which
have full annotations, leaving aside the verb-only
texts (see Section 6).
We use a distributional semantic model in order
to incorporate distributional information as fea-
tures in our system. In particular, we use the
neural-network based models from (Mikolov et
al., 2013), also referred as word embeddings. This
model makes use of skip-grams (n-grams that do
not need to be consecutive) within a word window
to calculate continuous-valued vector representa-
tions from a recurrent neural network. These dis-
tributional models have been able to outperform
state of the art in the SemEval-2012 Task 2 (Mea-
suring degrees of relational similarity). We calcu-
late the embeddings from an in-house corpus of
57m English tweets using a window size 5 and
yielding vectors of 100 dimensions.
We also use the first 20k tweets of the 57m
tweets to train our HMM2 models.
3.1.2 Annotation
While an annotated newswire corpus and a high-
quality lexical resource already enable us to train,
we also need at least a small sample of anno-
tated tweets data to evaluate SST for Twitter. Fur-
thermore, if we want to experiment with super-
vised SST, we also need sufficient annotated Twit-
ter data to learn the distribution of sense tags.
This paper presents two data sets: (a) super-
sense annotations for the POS+NER-annotated
data set described in Ritter et al. (2011), which we
use for training, development and evaluation, us-
ing the splits proposed in Derczynski et al. (2013),
and (b) supersense annotations for a sample of 200
tweets, which we use for additional, out-of-sample
evaluation. We call these data sets RITTER-
{TRAIN,DEV,EVAL} and IN-HOUSE-EVAL, re-
spectively. The IN-HOUSE-EVAL dataset was
downloaded in 2013 and is a sample of tweets that
contain links to external homepages but are other-
wise unbiased. It was previously used (with part-
of-speech annotation) in (Plank et al., 2014). Both
data sets are made publicly available with this pa-
per.
Supersenses are annotated with in spans defined
by the BIO (Begin-Inside-Other) notation. To ob-
tain the Twitter data sets, we carried out an an-
notation task. We first pre-annotated all data sets
with WordNet?s most frequent senses. If the word
was not in WordNet and a noun, we assigned it the
sense n.person. All other words were labeled O.
Chains of nouns were altered to give every ele-
ment the sense of the head noun, and the BI tags
adjusted, i.e.:
Empire/B-n.loc State/B-n.loc Building/B-n.artifact
was changed to
Empire/B-n.artifact State/I-n.artifact Building/I-
n.artifact
For the RITTER data, three paid student an-
notators worked on different subsets of the pre-
annotated data. They were asked to correct mis-
takes in both the BIO notation and the assigned
supersenses. They were free to chose from the full
label set, regardless of the pre-annotation. While
the three annotators worked on separate parts, they
overlapped on a small part of RITTER-TRAIN (841
tokens). On this subset, we computed agreement
scores and annotation difficulties. The average
raw agreement was 0.86 and Cohen?s ? 0.77. The
majority of tokens received the O label by all an-
notators; this happended in 515 out of 841 cases.
Excluding these instances to evaluate the perfor-
mance on the more difficult content words, raw
agreement dropped to 0.69 and Cohen?s ? to 0.69.
The IN-HOUSE-EVAL data set was annotated
by two different annotators, namely two of the au-
thors of this article. Again, for efficiency reasons
they worked on different subsets of the data, with
an overlapping portion. Their average raw agree-
ment was 0.65 and their Cohen?s ? 0.62. For this
data set, we also compute F
1
, defined as usual as
the harmonic mean of recall and precision. To
4
compute this, we set one of the annotators as gold
data and the other as predicted data. However,
since F
1
is symmetrical, the order does not mat-
ter. The annotation F
1
gives us another estimate
of annotation difficulty. We present the figures in
Table 3.
3.2 Baselines
For most word sense disambiguation studies, pre-
dicting the most frequent sense (MFS) of a word
has been proven to be a strong baseline. Follow-
ing this, our MFS baseline simply predicts the su-
persense of the most frequent WordNet sense for
a tuple of a word and a part of speech. We use
the part of speech predicted by the LAPOS tagger
(Tsuruoka et al., 2011). Any word not in Word-
Net is labeled as noun.person, which is the most
frequent sense overall in the training data. After
tagging, we run a script to correct the BI tag pre-
fixes, as described above for the annotation ask.
We also compare to the performance of exist-
ing SST systems. In particular we use Sense-
Learner (Mihalcea and Csomai, 2005) as a base-
line, which produces estimates of the WordNet
sense for each word. For these predictions, we
retrieve the corresponding supersense. Finally,
we use a publicly available reimplementation of
Ciaramita and Altun (2006) by Michael Heilman,
which reaches comparable performance on gold-
tagged SEMCOR.
3
3.3 Model parameters
We use the feature model of Paa? and Reichartz
(2009) in all our models, except the weakly su-
pervised models. For the structured perceptron we
set the number of passes over the training data on
the held-out development data. The weakly super-
vised models use the default setting proposed in
Li et al. (2012). We have used the standard online
setup for SEARN, which only takes one pass over
the data.
The type of embedding is the same in all our
experiments. For a given word the embedding fea-
ture is a 100 dimensional vector, which combines
the embedding of the word with the embedding of
adjacent words. The feature combination f
e
for a
word w
t
is calculated as:
f
e
(w
t
) =
1
2
(e(w
t?1
) + e(w
t+1
))? 2e(w
t
),
3
http://www.ark.cs.cmu.edu/mheilman/
questions/SupersenseTagger-10-01-12.tar.
gz
where the factor of two is chosen heurestically to
give more weight to the current word.
We also set a parameter k on development data
for using the k-most frequent senses inWordNet
as type constraints. Our supervised models are
trained on SEMCOR+RITTER-TRAIN or simply
RITTER-TRAIN, depending on what gave us the
best performance on the held-out data.
4 Results
The results are presented in Table 2. We dis-
tinguish between three settings with various de-
grees of supervision: weakly supervised, which
uses no domain annotated information, but solely
relies on embeddings trained on unlabeled Twit-
ter data; unsupervised domain adaptation (DA),
which uses SemCor for supervised training; and
supervised domain adaptation (SU), which uses
annotated Twitter data in addition to the SemCor
data for training.
In each of the two domain adaptation settings,
SEARN and HMM are evaluated with type con-
straints as distant supervision, and without for
comparison. SEARN without embeddings or dis-
tant supervision serves as an in-house baseline.
In Table 3 we present the WordNet token cov-
erage of predicted nouns and verbs in the devel-
opment and evaluation data, as well as the inter-
annotator agreement F
1
scores.
All the results presented in Table 2 are
(weighted averaged) F
1
measures obtained on pre-
dicted POS tags. Note that these results are con-
siderably lower than results on supersense tagging
newswire (up to 80 F
1
) that assume gold standard
POS tags (Ciaramita and Altun, 2006; Paa? and
Reichartz, 2009).
The re-implementation of the state-of-the-art
system improves slightly upon the most frequent
sense baseline. SenseLearner does not seem to
capture the relevant information and does not
reach baseline performance. In other words, there
is no off-the-shelf tool for supersense tagging of
Twitter that does much better than assigning the
most frequent sense to predicted nouns and verbs.
Our weakly supervised model performs worse
than the most frequent sense baseline. This is a
negative result. It is, however, well-known from
the word sense disambiguation literature that the
MFS is a very strong baseline. Moreover, the EM
learning problem is hard because of the large la-
bel set and weak distributional evidence for super-
5
RITTER IN-HOUSE
DEV EVAL EVAL
Wordnet noun-verb
token coverage 83.72 70.22 41.18
Inter-annotator
agreement (F1) 81.01 69.15 61.57
Table 3: Properties of dataset.
senses.
The unsupervised domain adaptation and fully
supervised systems perform considerably better
than this baseline across the board. In the unsuper-
vised domain adaptation setup, we see huge im-
provements from using type constraints as distant
supervision. In the supervised setup, we only see
significant improvements adding type constraints
for the structured perceptron (HMM), but not for
search-based structured prediction (SEARN).
For all the data sets, there is still a gap between
model performance and human inter-annotator
agreement levels (see Table 3), leaving some room
for improvements. We hope that the release of the
data sets will help further research into this.
4.1 Coarse-grained evaluation
We also experimented with the more coarse-
grained classes proposed by Yuret and Yatbaz
(2010). Here our best model obtained an F
1
score
for mental concepts (nouns) of 72.3%, and 62.6%
for physical concepts, on RITTER-DEV. The over-
all F
1
score for verbs is 85.6%. The overall F
1
is
75.5%. Note that this result is not directly com-
parable to the figure (72.9%) reported in Yuret
and Yatbaz (2010), since they use different data
sets, exclude verbs and make different assump-
tions, e.g., relying on gold POS tags.
5 Error analysis
We have seen that inter-annotator agreements on
supersense annotation are reliable at above .60
but far from perfect. The Hinton diagram in Ta-
ble 2 presents the confusion matrix between our
annotators on IN-HOUSE-EVAL.
Errors in the prediction primarily stem from
two sources: out-of-vocabulary words and incor-
rect POS tags. Figure 3 shows the distribution of
senses over the words that were not contained in
either the training data, WordNet, or the Twitter
data used to learn the embeddings. The distribu-
tion follows a power law, with the most frequent
sense being noun.person, followed by noun.group,
and noun.artifact. The first two are related to NER
categories, namely PER and ORG, and can be ex-
pected, since Twitter users frequently talk about
new actors, musicians, and bands. Nouns of com-
munication are largely related to films, but also in-
clude Twitter, Facebook, and other forms of social
media. Note that verbs occur only towards the tail
end of the distribution, i.e., there are very few un-
known verbs, even in Twitter.
Overall, our models perform best on labels with
low lexical variability, such as quantities, states
and times for nouns, as well as consumption, pos-
session and stative for verbs. This is unsurprising,
since these classes have lower out-of-vocabulary
rates.
With regards to the differences between source
(SEMCOR) and target (Twitter) domains, we ob-
serve that the distribution of supersenses is al-
ways headed by the same noun categories like
noun.person or noun.group, but the frequency of
out-of-vocabulary stative verbs plummets in the
target domain, as some semantic types are more
closed class than others. There are for instance
fewer possibilities for creating new time units
(noun.time) or stative verbs like be than people or
company names (noun.person or noun.group, re-
spectively).
The weakly supervised model HMM2 has
higher precision (57% on RITTER-DEV) than re-
call (48.7%), which means that it often predicts
words to not belong to a semantic class. This
suggests an alternative strategy, which is to train
a model on sequences of purely non-O instances.
This would force the model to only predict O on
words that do not appear in the reduced sequences.
One important source of error seems to be un-
reliable part-of-speech tagging. In particular we
predict the wrong POS for 20-35% of the verbs
across the data sets, and for 4-6.5% of the nouns.
In the SEMCOR data, for comparability, we have
wrongly predicted tags for 6-8% of the anno-
tated tokens. Nevertheless, the error propaga-
tion of wrongly predicted nouns and verbs is par-
tially compensated by our systems, since they are
trained on imperfect input, and thus it becomes
possible for the systems to predict a noun super-
sense for a verb and viceversa. In our data we have
found e.g. that the noun Thanksgiving was incor-
rectly tagged as a verb, but its supersense was cor-
rectly predicted to be noun.time, and that the verb
guess had been mistagged as noun but the system
6
Resources Results
Token-level Type-level RITTER IN-HOUSE
SemCor Twitter Embeddings Type constraints DEV EVAL EVAL
General baselines
MFS - - - + 47.54 44.98 38.65
SENSELEARNER + - - - 14.61 26.24 22.81
HEILMAN + - - - 48.96 45.03 39.65
Weakly supervised systems
HMM2 - - - + 47.09 42.12 26.99
Unsupervised domain adaptation systems (DA)
SEARN (Baseline) + - - - 48.31 42.34 34.30
SEARN + - + - 52.45 48.30 40.22
SEARN + - + + 56.59 50.89 40.50
HMM + - + - 52.40 47.90 40.51
HMM + - + + 57.14 50.98 41.84
Supervised domain adaptation systems (SU)
SEARN (Baseline) + + - - 58.30 52.12 36.86
SEARN + + + - 63.05 57.09 42.37
SEARN + + + + 62.72 57.14 42.42
HMM + + + - 57.20 49.26 39.88
HMM + + + + 60.66 51.40 41.60
Table 2: Weighted F1 average over 41 supersenses.
7
Figure 2: Inter-annotator confusion matrix on TWITTER-EVAL.
0
0.1
0.2
0.3
0.4
noun.
person noun.
group
noun.
artifac
t
noun.
comm
unicat
ion
noun.
event
noun.
locatio
n
noun.
time noun.
act
noun.
food
noun.
attribu
te
noun.
relatio
n
verb.c
ogniti
on
verb.c
reatio
n
verb.e
motio
n
verb.m
otion
verb.p
ercept
ion
verb.s
tative
Figure 3: Sense distribution of OOV words.
8
still predicted the correct verb.cognition as super-
sense.
6 Related Work
There has been relatively little previous work on
supersense tagging, and to the best of our knowl-
edge, all of it has been limited to English newswire
and literature (SEMCOR and SENSEVAL).
The task of supersense tagging was first intro-
duced by Ciaramita and Altun (2006), who used
a structured perceptron trained and evaluated on
SEMCOR via 5-fold cross validation. Their eval-
uation included a held-out development set on
each fold that was used to estimate the number of
epochs. They used additional training data con-
taining only verbs. More importantly, they relied
on gold standard POS tags. Their overall F
1
score
on SEMCOR was 77.1. Reichartz and Paa? (Re-
ichartz and Paa?, 2008; Paa? and Reichartz, 2009)
extended this work, using a CRF model as well
as LDA topic features. They report an F
1
score
of 80.2, again relying on gold standard POS fea-
tures. Our implementation follows their setup and
feature model, but we rely on predicted POS fea-
tures, not gold standard features.
Supersenses provide information similar to
higher-level distributional clusters, but more in-
terpretable, and have thus been used as high-
level features in various tasks, such as preposi-
tion sense disambiguation, noun compound inter-
pretation, and metaphor detection (Ye and Bald-
win, 2007; Tratz and Hovy, 2010; Tsvetkov et al.,
2013). Princeton WordNet only provides a fully
developed taxonomy of supersenses for verbs and
nouns, but Tsvetkov et al. (2014) have recently
proposed an extension of the taxonomy to cover
adjectives. Outside of English, supersenses have
been annotated for Arabic Wikipedia articles by
Schneider et al. (2012).
In addition, a few researchers have tried to
solve coarse-grained word sense disambiguation
problems that are very similar to supersense tag-
ging. Kohomban and Lee (2005) and Kohom-
ban and Lee (2007) also propose to use lexicogra-
pher file identifers from Princeton WordNet senses
(supersenses) and, in addition, discuss how to re-
trieve fine-grained senses from those predictions.
They evaluate their model on all-words data from
SENSEEVAL-2 and SENSEEVAL-3. They use a
classification approach rather than structured pre-
diction.
Yuret and Yatbaz (2010) present a weakly unsu-
pervised approach to this problem, still evaluating
on SENSEVAL-2 and SENSEVAL-3. They focus
only on nouns, relying on gold part-of-speech, but
also experiment with a coarse-grained mapping,
using only three high level classes.
For Twitter, we are aware of little previous work
on word sense disambiguation. Gella et al. (2014)
present lexical sample word sense disambiguation
annotation of 20 target nouns on Twitter, but no
experimental results with this data. There has also
been related work on disambiguation to Wikipedia
for Twitter (Cassidy et al., 2012).
In sum, existing work on supersense tagging
and coarse-grained word sense disambiguation for
English has to the best of our knowledge all fo-
cused on newswire and literature. Moreover, they
all rely on gold standard POS information, making
previous performance estimates rather optimistic.
7 Conclusion
In this paper, we present two Twitter data sets with
manually annotated supersenses, as well as a se-
ries of experiments with these data sets. The data
is publicly available for download.
In this article we have provided, to the best
of our knowledge, the first supersense tagger for
Twitter. We have shown that off-the-shelf tools
perform poorly on Twitter, and we offer two
strategies?namely distant supervision and the us-
age of embeddings as features?that can be com-
bined to improve SST for Twitter.
We propose that distant supervision imple-
mented as type constraints during decoding is a
viable method to limit the mispredictions of su-
persenses by our systems, thereby enforcing pre-
dicted senses that a word has in WordNet. This ap-
proach compensates for the size limitations of the
training data and mitigates the out-of-vocabulary
effect, but is still subject to the coverage of Word-
Net; which is far from perfect for words coming
from high-variability sources such as Twitter.
Using distributional semantics as features in
form of word embeddings also improves the pre-
diction of supersenses, because it provides seman-
tic information for words, regardless of whether
they have been observed the training data. This
method does not require a hand-created knowl-
edge base like WordNet, and is a promising tech-
nique for domain adaptation of supersense tag-
ging.
9
References
Taylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz Zu-
biaga, and Hongzhao Huang. 2012. Analysis and
enhancement of wikification for microblogs with
context expansion. In COLING, volume 12, pages
441?456.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proc. of EMNLP, pages 594?602, Sydney, Australia,
July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Hal Daume, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, pages 297?325.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press USA.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Yoav Freund and Robert Schapire. 1999. Large margin
classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Spandana Gella, Paul Cook, and Timothy Baldwin.
2014. One sense per tweeter and other lexical se-
mantic tales of Twitter. In EACL.
Upali Kohomban and Wee Lee. 2005. Learning se-
mantic classes for word sense disambiguation. In
ACL.
Upali Kohomban and Wee Lee. 2007. Optimizing
classifier performance in word sense disambiguation
by redefining word sense classes. In IJCAI.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Shen Li, Jo?ao Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In EMNLP.
Jean-Francois Mari, Jean-Paul Haton, and Abdelaziz
Kriouile. 1997. Automatic word recognition based
on second-order hidden Markov models. IEEE
Transactions on Speech and Audio Processing,
5(1):22?25.
Rada Mihalcea and Andras Csomai. 2005. Sense-
learner: Word sense disambiguation for all words in
unrestricted text. In Proceedings of the ACL 2005
on Interactive poster and demonstration sessions,
pages 53?56. Association for Computational Lin-
guistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS.
George A. Miller, Martin Chodorow, Shari Landes,
Claudia Leacock, and Robert G. Thomas. 1994.
Using a semantic concordance for sense identifica-
tion. In Proceedings of the workshop on Human
Language Technology, pages 240?243. Association
for Computational Linguistics.
Gerhard Paa? and Frank Reichartz. 2009. Exploit-
ing semantic constraints for estimating supersenses
with CRFs. In Proc. of the Ninth SIAM Interna-
tional Conference on Data Mining, pages 485?496,
Sparks, Nevada, May.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of EACL.
Frank Reichartz and Gerhard Paa?. 2008. Estimating
Supersenses with Conditional Random Fields. In
Proceedings of ECMLPKDD.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In EMNLP.
Frank Rosenblatt. 1958. The perceptron: a probabilis-
tic model for information storage and organization
in the brain. Psychological Review, 65(6):386?408.
Nathan Schneider, Behrang Mohit, Kemal Oflazer, and
Noah A Smith. 2012. Coarse lexical semantic an-
notation with supersenses: an arabic case study. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 253?
258. Association for Computational Linguistics.
Scott Thede and Mary Harper. 1999. A second-order
hidden Markov model for part-of-speech tagging. In
ACL.
Stephen Tratz and Eduard Hovy. 2010. Isi: automatic
classification of relations between nominals using a
maximum entropy classifier. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 222?225. Association for Computational Lin-
guistics.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: can
history-based models rival globally optimized mod-
els? In CoNLL.
10
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-
man. 2013. Cross-lingual metaphor detection us-
ing common semantic features. Meta4NLP 2013,
page 45.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014.
Augmenting english adjective senses with super-
senses. In Proc. of LREC.
Patrick Ye and Timothy Baldwin. 2007. Melb-yb:
Preposition sense disambiguation using rich seman-
tic features. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 241?244.
Association for Computational Linguistics.
Deniz Yuret and Mehmet Yatbaz. 2010. The noisy
channel model for unsupervised word sense disam-
biguation. Computational Linguistics, 36:111?127.
11
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 213?217,
Dublin, Ireland, August 23-24, 2014.
Copenhagen-Malm
?
o: Tree Approximations of Semantic Parsing Problems
Natalie Schluter
?
, Jakob Elming, Sigrid Klerke, H
?
ector Mart??nez Alonso, Dirk Hovy
Barbara Plank, Anders Johannsen, and Anders S?gaard
?
Dpt. of Computer Science Center for Language Technology
Malm?o University University of Copenhagen
natalie.schluter@mah.se {zmk867,skl,alonso}@hum.ku.dk
{dirk,bplank}@cst.dk,
{ajohannsen,soegaard}@hum.ku.dk
Abstract
In this shared task paper for SemEval-
2014 Task 8, we show that most se-
mantic structures can be approximated by
trees through a series of almost bijective
graph transformations. We transform in-
put graphs, apply off-the-shelf methods
from syntactic parsing on the resulting
trees, and retrieve output graphs. Us-
ing tree approximations, we obtain good
results across three semantic formalisms,
with a 15.9% error reduction over a state-
of-the-art semantic role labeling system on
development data. Our system came in 3/6
in the shared task closed track.
1 Introduction
Semantic analyses often go beyond tree-
structured representations, assigning multiple se-
mantic heads to nodes, some semantic formalisms
even tolerating directed cycles.
1
At the same
time, syntactic parsing is a mature field with effi-
cient, highly optimised decoding and learning al-
gorithms for tree-structured representations. We
present tree approximation algorithms that in com-
bination with a state-of-the-art syntactic parser
achieve competitive performance in semantic di-
graph parsing.
We investigate two kinds of tree approximation
algorithms that we will refer to as pruning algo-
rithms and packing algorithms. Our pruning al-
gorithms simply remove and reverse edges until
the graph is a tree; edge reversals are then undone
as a postprocessing step. Our packing algorithms,
on the other hand, carry out two bijective graph
This work is licenced under a Creative Commons Attribu-
tion 4.0 International License. Page numbers and proceed-
ings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
1
For example, HPSG predicate-argument structures (Pol-
lard and Sag, 1994).
transformations to pack structural information into
new edge labels, making it possible to reconstruct
most of the structural complexity as a postprocess-
ing step. Specifically, we present a packing al-
gorithm that consists of two fully bijective graph
transformations, in addition to a further transfor-
mation that incurs only a small information loss.
We carry out experiments across three seman-
tic annotations of the Wall Street Journal section
of the Penn Treebank (Marcus et al., 1993), cor-
responding to simplified versions of the semantic
formalisms minimal recursion semantics (MRS)
(Copestake et al., 2005), Enju-style predicate-
argument structures (Miyao and Tsujii, 2003), and
Prague-style tectogrammar semantics (B?ohmov?a
et al., 2003). We show that pruning and pack-
ing algorithms lead to state-of-the-art performance
across these semantic formalisms using an off-the-
shelf syntactic dependency parser.
2 Related work
Sagae and Tsujii (2008) present a pruning algo-
rithm in their paper on transition-based parsing of
directed acyclic graphs (DAGs), which discards
the edges of longest span entering nodes. They
apply the dependency parser described in Sagae
and Tsujii (2007) to the tree representations. We
note that this algorithm is not sufficient to produce
trees in our case, where the input graphs are not
necessarily acyclic. It does correspond roughly to
our LONGEST-EDGE baseline, which removes the
longest edge in cycles, in addition to flow reversal.
Sagae and Tsujii (2008) also present a shift-
reduce automaton approach to parsing DAGs. In
their paper, they report a labeled F1-score of
88.7% on the PAS dataset (see Section 3), while
we obtain 89.1%, however the results are thus not
directly comparable due to different data splits.
2
2
We obtained code to run this as a baseline, but were un-
able to, due to memory leaks, caused by subsets of our data,
and on the subsets of data that actually parsed, recall was very
213
The shared task organizers of the Broad-
coverage Semantic Dependency Parsing task at
SemEval-2014
3
also presented a pruning-based
baseline system. They eliminate re-entrancies in
the graph by removing dependencies to nodes with
multiple incoming edges. Of these edges, they
again keep the shortest. They incorporate all sin-
gleton nodes by attaching nodes to the immedi-
ately following node or to a virtual root - in case
the singleton is sentence-final. Finally, they inte-
grate fragments by subordinating remaining nodes
with in-degree 0 to the root node. They apply the
parser described in Bohnet (2010), also used be-
low, to the resulting trees. This system obtained
a labeled F1-score of 54.7% on the PAS dataset.
The performance of their pruning algorithm was
also considerably lower than our algorithms on the
other datasets considered below.
3 Tree approximations
This section describes two approaches to approxi-
mating graphs by trees, namely pruning and pack-
ing. Pruning optimizes the number of ?good?
edges in trees (Section 3.1), while packing trans-
forms graphs into trees by means of a pipeline of
operations which are 99.6% reversible (see Fig-
ure 1); that is, almost no information from the
original graphs is lost in the trees (Section 3.2).
Under both approaches, we first introduce arti-
ficial root nodes to the graphs and append them
to the word list. Graphs may initially be discon-
nected. We connect all weakly connected com-
ponents as follows. We first identify a most im-
portant node in each weakly connected compo-
nent, which we will refer to as the root. This root
is taken to be the first node with the ?top? fea-
ture from the data, if one exists. If none exists,
then the node with highest degree is chosen as the
?root?. (Note that the ?root? of each non-singleton
connected component is marked as a ?top? node
in the inverse transformation.) The root of each
non-singleton weakly connected component is at-
tached as a dependent of the artificial root node
with a special new label for the corresponding
edge. Also, each disconnected node is attached
as a dependent of the node to the right of it, with
a distinct special new label. It is these connected
graphs that we take to be the input in the following
low, suggesting that maybe the decoding algorithm was tuned
to a specific planarization of the complex graphs.
3
http://alt.qcri.org/semeval2014/task8/
two subsections describing our graph pruning and
packing algorithms.
3.1 Graph pruning
Our PRUNING algorithm removes a small number
of edges in the semantic graphs to be able to repre-
sent them as trees. The average edge counts from
the training data (see Section 4.1) indicate that the
potential edge loss in pruning is relatively small
(5.7% in the worst case). In this approach, two
transformations on the connected semantic graphs
are carried out: pruning and flow reversal.
Pruning. The input digraph may contain under-
lying undirected cycles. We break these cycles
by iteratively removing the longest edge from the
node with the fewest predecessors (lowest depth)
in the digraph. The resulting underlying undi-
rected graph is a tree.
Depth-first flow reversal. We then carry out
depth-first traversal of the resulting underlying
undirected tree, reversing the direction of edges
from the leaves upwards, as needed, until reach-
ing the root. Any reversed edge?s label is given a
special prefix, so that this reversal can be undone
in a post-processing step.
Following the above two transformations, we train
our parsers on the transformed semantic annota-
tions and output graphs such as the one in Fig-
ure 1a.
3.2 Graph packing
Our PACKING algorithm consists of a pipeline of
four graph transformations. The two major trans-
formations are for coordination and generalised
long-distance dependencies, being both parallel
path inducing constructions. The transformations
are both linguistically and topologically inspired
by the f-structure annotated c-structures in Lex-
ical Functional Grammar and f-structure parsing
via off-the-shelf dependency parsers (Schluter and
Van Genabith, 2009). We further ensure the defin-
ing tree property that every node is connected by a
unique path from the root, by carrying out flow re-
versal when necessary. Finally remaining parallel
paths are broken according to an heuristic on path
locality.
Coordination. In some semantic representa-
tions of coordination, individual conjunct nodes
may all dominate a same argument, or be domi-
nated by a same head. In both these cases, paral-
lel paths are generated. The same structures may
214
a)
b)
c)
Figure 1: Example of pruned (top), packed (middle), and original (bottom) semantic graph. (Sentence
22002004 from the PAS dataset.)
be represented if the head or arguments are ?fac-
tored out?. To do this, we remove all edges from
conjuncts towards a same argument (resp. from
a shared head to each conjunct), and introduce a
new edge from the root of the coordination sub-
tree towards this argument (resp. from a shared
head to the root of the coordination subtree). The
new edges receive a special prefix to facilitate ap-
plying the inverse transformation.
Breadth-first flow reversal. Unlike our pruning
algorithm, there is not yet any clear distinct path
from the root to the all nodes (as there are not
leaves yet). After carrying out the coordination
transformation, we carry out a breadth-first search
on the graph to direct flow away from the root, and
again, reversed edges? labels are given a special
prefix. As we do this, we test resulting nodes to
see if there are any parallel paths leading to them.
If so, these paths may be transformed immediately
according to the following transformation.
Generalized long-distance dependencies.
Long-distance dependencies are represented
in f-structure annotated c-structures by path
equations. This gives a tree representation of
parallel paths, at least one of which is exactly
one edge long. Given two parallel paths p
1
and
p
2
in the graph, where p
1
= (v
1
, l, v
n
) and p
2
=
(v
1
, l
1
, v
2
), (v
2
, l
2
, v
3
), . . . , (v
n?1
, l
n?1
, v
n
), we
remove the last edge of p
2
and augment p
1
?s label
with the representation l
1
: l
2
: ? ? ? : l
n?1
of p
2
. p
1
becomes (v
1
, l and l
1
: l
2
: ? ? ? : l
n?1
, v
n
), indi-
cating that v
n
is also the child (with dependency
label l
n?1
) of the node found by travelling (from
v
1
) down an l
1
labelled edge, followed by an l
2
labelled edge, and so on until the child of the l
n?2
labelled edge is found.
Maximum average locality heuristic. Follow-
ing these transformations, there may still be paral-
lel paths in the graph: those not parallel to a single
edge. We remove ?worst? re-entrant edges using
the simple heuristic that the path with the lowest
average edge span should be conserved entirely.
These removed edges clearly cannot be recovered
after transformation.
Our parsers are trained on the output graphs of
these four transformations such as the one in Fig-
ure 1b. We observe the main difference between
PRUNING and PACKING: coordination and long-
distance dependencies. For example, PACKING
keeps the edge between the conjunction and the
first conjunct, which is pruned away in PRUNING.
Such a difference provides a partial explanation
for the lower recall of PRUNING vis-`a-vis PACK-
ING (see Section 4.5).
4 Experiments
4.1 Data
The three datasets are semantic annotations of the
WSJ section of the Penn Treebank of English. The
average sentence length, which is also the aver-
age number of dependency edges in the tree ap-
proximations that we use to induce our semantic
parsers, is 22.93. The three semantic formalisms
are slightly richer, and the average number of
edges in the PAS-annotated treebank is 24.32. For
DM, the average number of edges is 23.77, and
for DM it is 23.33. While the pruning-based ap-
proaches thus suffers from a modest information
loss, throwing out 5.7% of the edges in the worst
215
case, this is not the case for packing. The re-
versibility of the packed representations is given
by the score upper bound in the last row in Ta-
ble 1. We use the dataset splits of the SemEval
2014 shared task.
4.2 Model
For both our pruning and packing models, we use
the Mate parser (Bohnet, 2010)
4
with default pa-
rameters to learn our parsing models. The Mate
parser is trained on the output of the transforma-
tion pipeline on Sections 00-19 of the three se-
mantically annotated WSJ datasets. Some models
use Brown clusters generated from Sections 00-
19 only. This does not solve OOV problems, but
allows of slightly better generalisation across dis-
tributionally similar words in the training data.
4.3 Baselines
We use the SemEval 2014 shared task baseline
(SIMPLE-PRUNE; see Section 2), as well as the
LONGEST-EDGE baseline, also mentioned above.
The latter is our strongest baseline system. It is
very similar to PRUNING, in doing both edge prun-
ing and flow reversal, but the pruning step only
removes the longest edge rather than considering
node depth. Our third baseline is the Mate seman-
tic role labeling learner (SRL-DEP) (Bj?orkelund
et al., 2009), which uses predicted syntactic parses
as input; for this, we use the syntactic parses made
available in the SemEval 2014 shared task for
replicability.
Approach Cl DM PAS PCEDT Av
Systems
PRUNING
NO 86.6 88.8 72.7 82.7
YES 86.9 89.1 72.5 82.8
PACKING
NO 85.8 88.7 71.8 82.1
YES 86.1 88.7 72.9 82.6
Baselines
SIMPLE-PRUNE 54.7 50.9 67.8 57.8
LONGEST-EDGE 83.8 88.9 66.1 79.6
SRL-DEP 79.5 82.4 70.1 77.4
Upper bound
PACKING 99.9 99.5 99.5 99.6
Table 1: Labelled F1-score results on development
data, with and without use of Brown clusters (Cl).
4.4 Results
The results are presented in Tables 1 through 3,
where the system evaluations for the SemEval task
are marked with asterisks in Table 2. We note that
all our approaches do considerably better than our
4
https://code.google.com/p/mate-tools/
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 84.8 87.7 71.2 81.2
(W/ TOP) REC 84.0 88.4 68.6 80.3
F1 84.4 88.0 69.9 80.8
?
PREC 85.4 87.9 70.8 81.4
(W/O TOP) REC 84.6 88.6 68.8 80.7
F1 85.0 88.3 69.9 81.1
PRUNING PREC 87.2 91.3 72.8 83.8
(W/ TOP) REC 80.2 81.3 62.8 74.8
F1 83.6 86.0 67.4 79.0
?
PREC 87.2 91.3 72.8 83.8
(W/O TOP) REC 85.1 85.1 68.0 79.4
F1 86.2 88.1 70.3 81.5
Table 2: Labelled results on test data, with and
without evaluation of top nodes. The scores with
asterisks correspond to the output evaluated in the
SemEval task.
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 86.8 89.1 84.8 86.9
(W/ TOP) REC 86.0 89.8 81.8 85.9
F1 86.4 89.4 83.2 86.3
PREC 87.5 89.4 85.4 87.4
(W/O TOP) REC 86.7 90.1 83.0 86.6
F1 87.1 89.7 84.2 87.0
PRUNING PREC 89.2 92.6 88.2 90.0
(W/ TOP) REC 82.0 82.5 76.1 80.2
F1 85.4 87.3 81.7 84.8
PREC 89.2 92.6 88.2 90.0
(W/O TOP) REC 87.1 86.3 82.4 85.3
F1 88.1 89.3 85.2 87.5
Table 3: Unlabelled results on test data, with and
without evaluation of top nodes.
three baselines. The error reduction of our best
system over the SRL system across all three for-
malisms is 24.2%, and the error reduction over
the more competitive pruning baseline LONGEST-
EDGE is 15.9%. As mentioned in Section 2, these
results seem to promise better performance than
current DAG parsing models. Note from the re-
sults in Table 2 that, as expected, PRUNING leads
to higher precision than PACKING at the expense
of recall.
4.5 Error Analysis
We observe that pruning leads to high precision,
while our packing algorithm gives us much bet-
ter recall. This is not surprising, since our packed
representations introduce new labels, making it
harder to generalize at training time. On the other
hand, pruning approaches suffer in recall, simply
because edges are thrown away in preprocessing
the data.
216
5 Conclusions
In this paper, we experimented with using tree ap-
proximation algorithms to reduce semantic struc-
tures to trees and use off-the-shelf structured pre-
diction techniques to train semantic parsers. Our
approximation algorithms include both pruning
and packing algorithms, i.e., algorithms that try
to reduce graphs to trees optimally, as well as al-
gorithms that pack information about graphs into
trees from which we later recover the richer struc-
tures. Using these tree approximation algorithms,
we obtain 15.9% error reductions over a state-of-
the-art SRL system.
References
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Proc.
of CoNLL: Shared Task, pages 43?48, Boulder, CO,
USA.
Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and Barbora
Hladk?a. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeill?e,
editor, Treebanks: Building and Using Syntacti-
cally Annotated Corpora, pages 103?127. Kluwer,
Netherlands.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proc. of
COLING, pages 89?97, Beijing, China.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics. Research
on Language and Computation, 3:281?332.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2003. Probabilis-
tic modeling of argument structures including non-
local dependencies. In Proc. of RANLP, pages 79?
85, Borovets, Bulgaria.
Carl Pollard and Ivan Sag. 1994. Head-driven phrase
structure grammar. University of Chicago Press.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models
and parser ensembles. In Proc. of CoNLL Shared
task session of EMNLP-CoNLL, pages 1044?1050,
Prague, Czech Republic.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proc. of COLING,
pages 753?760, Manchester, UK.
Natalie Schluter and Josef Van Genabith. 2009. De-
pendency parsing resources for French. In Proc. of
NODALIDA, pages 166?173, Odense, Denmark.
217
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1?10,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
What?s in a p-value in NLP?
Anders S?gaard, Anders Johannsen, Barbara Plank, Dirk Hovy and Hector Martinez
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
In NLP, we need to document that our pro-
posed methods perform significantly bet-
ter with respect to standard metrics than
previous approaches, typically by re-
porting p-values obtained by rank- or
randomization-based tests. We show that
significance results following current re-
search standards are unreliable and, in ad-
dition, very sensitive to sample size, co-
variates such as sentence length, as well as
to the existence of multiple metrics. We
estimate that under the assumption of per-
fect metrics and unbiased data, we need a
significance cut-off at ?0.0025 to reduce
the risk of false positive results to <5%.
Since in practice we often have consider-
able selection bias and poor metrics, this,
however, will not do alone.
1 Introduction
In NLP, we try to improve upon state of the art
language technologies, guided by experience and
intuition, as well as error analysis from previous
experiments, and research findings often consist in
system comparisons showing that System A is bet-
ter than System B.
Effect size, i.e., one system?s improvements
over another, can be seen as a random variable.
If the random variable follows a known distribu-
tion, e.g., a normal distribution, we can use para-
metric tests to estimate whether System A is bet-
ter than System B. If it follows a normal dis-
tribution, we can use Student?s t-test, for exam-
ple. Effect sizes in NLP are generally not nor-
mally distributed or follow any of the other well-
studied distributions (Yeh, 2000; S?gaard, 2013).
The standard significance testing methods in NLP
are therefore rank- or randomization-based non-
parametric tests (Yeh, 2000; Riezler and Maxwell,
2005; Berg-Kirkpatrick et al., 2012). Specifi-
cally, most system comparisons across words, sen-
tences or documents use bootstrap tests (Efron and
Tibshirani, 1993) or approximate randomization
(Noreen, 1989), while studies that compare perfor-
mance across data sets use rank-based tests such as
Wilcoxon?s test.
The question we wish to address here is: how
likely is a research finding in NLP to be false?
Naively, we would expect all reported findings to
be true, but significance tests have their weak-
nesses, and sometimes researchers are forced
to violate test assumptions and basic statistical
methodology, e.g., when there is no one estab-
lished metric, when we can?t run our models on
full-length sentences, or when data is biased. For
example, one such well-known bias from the tag-
ging and parsing literature is what we may refer to
as the WSJ FALLACY. This is the false belief that
performance on the test section of the Wall Street
Journal (WSJ) part of the English Penn treebank
is representative for performance on other texts in
English. In other words, it is the belief that our
samples are always representative. However, (the
unawareness of) selection bias is not the only rea-
son research findings in NLP may be false.
In this paper, we critically examine significance
results in NLP by simulations, as well as running
a series of experiments comparing state-of-the-art
POS taggers, dependency parsers, and NER sys-
tems, focusing on the sensitivity of p-values to var-
ious factors.
Specifically, we address three important factors:
Sample size. When system A is reported to be
better than system B, this may not hold across do-
mains (cf. WSJ FALLACY). More importantly,
though, it may not even hold on a sub-sample of
the test data, or if we added more data points to
the test set. Below, we show that in 6/10 of our
POS tagger evaluations, significant effects become
insignificant by (randomly) adding more test data.
1
Covariates. Sometimes we may bin our results
by variables that are actually predictive of the out-
come (covariates) (Simmons et al., 2011). In some
subfields of NLP, such as machine translation or
(unsupervised) syntactic parsing, for example, it
is common to report results that only hold for sen-
tences up to some length. If a system A is reported
to be better than a system B on sentences up to
some length, A need not be better than B, neither
for a different length nor in general, since sentence
length may actually be predictive of A being better
than B.
Multiple metrics. In several subfields of NLP,
we have various evaluation metrics. However, if
a system A is reported to be better than a system
B with respect to some metric M
1
, it need not be
better with respect to some other metric M
2
. We
show that even in POS tagging it is sometimes the
case that results are significant with respect to one
metric, but not with respect to others.
While these caveats should ideally be avoided
by reporting significance over varying sample
sizes and multiple metrics, some of these effects
also stem from the p-value cut-off chosen in the
NLP literature. In some fields, p-values are re-
quired to be much smaller, e.g., in physics, where
the 5   criterion is used, and maybe we should also
be more conservative in NLP?
We address this question by a simulation of the
interaction of type 1 and type 2 error in NLP and
arrive at an estimate that more than half of research
findings in NLP with p < 0.05 are likely to be
false, even with a valid metric and in the absence
of selection bias. From the same simulations, we
propose a new cut-off level at 0.0025 or smaller
for cases where the metric can be assumed to be
valid, and where there is no selection bias.
1
We
briefly discuss what to do in case of selection bias
or imperfect metrics.
Note that we do not discuss false discovery rate
control or family wise error rate procedures here.
While testing with different sample sizes could
be be considered multiple hypothesis testing, as
pointed out by one of our anonymous reviewers,
NLP results should be robust across sample sizes.
Note that the p < 0.0025 cut-off level corresponds
1
In many fields, including NLP, it has become good prac-
tice to report actual p-values, but we still need to understand
how significance levels relate to the probability that research
findings are false, to interpret such values. The fact that we
propose a new cut-off level for the ideal case with perfect
metrics and no bias does not mean that we do not recommend
reporting actual p-values.
to a Bonferroni correction for a family of m = 20
hypotheses.
Our contributions
Several authors have discussed significance test-
ing in NLP before us (Yeh, 2000; Riezler and
Maxwell, 2005; Berg-Kirkpatrick et al., 2012), but
while our discussion touches on many of the same
topics, this paper is to the best of our knowledge
the first to:
a) show experimentally how sensitive p-values
are to sample size, i.e., that in standard NLP
experiments, significant effects may actually
disappear by adding more data.
b) show experimentally that multiple metrics
and the use of covariates in evaluation in-
crease the probability of positive test results.
c) show that even under the assumption of per-
fect metrics and unbiased data, as well as our
estimates of type 1 and 2 error in NLP, you
need at least p < 0.0025 to reduce the prob-
ability of a research finding being false to be
< 5%.
2 Significance testing in NLP
Most NLP metric for comparing system outputs
can be shown to be non-normally distributed
(S?gaard, 2013) and hence, we generally cannot
use statistical tests that rely on such an assump-
tion, e.g., Student?s t-test. One alternative to such
tests are non-parametric rank-based tests such as
Wilcoxon?s test. Rank-based tests are sometimes
used in NLP, and especially when the number of
observations is low, e.g., when evaluating perfor-
mance across data sets, such tests seem to be the
right choice (Demsar, 2006; S?gaard, 2013). The
draw-back of rank-based tests is their relatively
weak statistical power. When we reduce scores to
ranks, we throw away information, and rank-based
tests are therefore relatively conservative, poten-
tially leading to high type 2 error rate ( , i.e., the
number of false negatives over trials). An alterna-
tive, however, are randomization-based tests such
as the bootstrap test (Efron and Tibshirani, 1993)
and approximate randomization (Noreen, 1989),
which are the de facto standards in NLP. In this
paper, we follow Berg-Kirkpatrick et al. (2012) in
focusing on the bootstrap test. The bootstrap test is
non-parametric and stronger than rank-based test-
ing, i.e., introduces fewer type 2 errors. For small
samples, however, it does so at the expense of a
2
higher type 1 error (?, i.e., the number of false
positives). The reason for this is that for the boot-
strap test to work, the original sample has to cap-
ture most of the variation in the population. If the
sample is very small, though, this is likely not the
case. Consequently, with small sample sizes, there
is a risk that the calculated p-value will be arti-
ficially low?simply because the bootstrap sam-
ples are too similar. In our experiments below, we
make sure only to use bootstrap when sample size
is > 200, unless otherwise stated. In our experi-
ments, we average across 3 runs for POS and NER
and 10 runs for dependency parsing.
DOMAIN #WORDS TASKS
POS Dep. NER
CONLL 2007
Bio 4k ?
Chem 5k ?
SWITCHBOARD 4
Spoken 162k ?
ENGLISH WEB TREEBANK
Answers 29k ? ?
Emails 28k ? ?
Newsgrs 21k ? ?
Reviews 28k ? ?
Weblogs 20k ? ?
WSJ 40k ? ?
FOSTER
Twitter 3k ?
CONLL 2003
News 50k ?
Table 1: Evaluation data.
3 Experiments
Throughout the rest of the paper, we use four run-
ning examples: a synthetic toy example and three
standard experimental NLP tasks, namely POS
tagging, dependency parsing and NER. The toy
example is supposed to illustrate the logic behind
our reasoning and is not specific to NLP. It shows
how likely we are to obtain a low p-value for the
difference in means when sampling from exactly
the same (Gaussian) distributions. For the NLP
setups (2-4), we use off-the-shelf models or avail-
able runs, as described next.
3.1 Models and data
We use pre-trained models for POS tagging and
dependency parsing. For NER, we use the output
of the best performing systems from the CoNLL
2003 shared task. In all three NLP setups, we
compare the outcome of pairs of systems. The
data sets we use for each of the NLP tasks are
listed in Table 1 (Nivre et al., 2007a; Foster et
Figure 1: Accuracies of LAPOS VS. STANFORD
across 10 data sets.
al., 2011; Tjong Kim Sang and De Meulder, 2003,
LDC99T42; LDC2012T13).
POS tagging. We compare the performance
of two state-of-the-art newswire taggers across 10
evaluation data sets (see Table 1), namely the LA-
POS tagger (Tsuruoka et al., 2011) and the STAN-
FORD tagger (Toutanova et al., 2003), both trained
on WSJ00?18. We use the publicly available pre-
trained models from the associated websites.
2
Dependency parsing. Here we compare the
pre-trained linear SVM MaltParser model for En-
glish (Nivre et al., 2007b) to the compositional
vector grammar model for the Stanford parser
(Socher et al., 2013). For this task, we use the sub-
set of the POS data sets that comes with Stanford-
style syntactic dependencies (cf. Table 1), exclud-
ing the Twitter data set which we found too small
to produce reliable results.
NER. We use the publicly available runs of
the two best systems from the CoNLL 2003
shared task, namely FLORIAN (Florian et al.,
2003) and CHIEU-NG (Chieu and Ng, 2003).
3
3.2 Standard comparisons
POS tagging. Figure 1 shows that the LAPOS
tagger is marginally better than STANFORD on
macro-average, but it is also significantly better? If
we use the bootstrap test over tagging accuracies,
the difference between the two taggers is only sig-
nificant (p < 0.05) in 3/10 cases (see Table 2),
namely SPOKEN, ANSWERS and REVIEWS. In
two of these cases, LAPOS is significantly better
2
http://www.logos.ic.i.u-tokyo.ac.jp/
?
tsuruoka/lapos/ and http://nlp.stanford.
edu/software/tagger.shtml
3
http://www.cnts.ua.ac.be/conll2003/
ner/
3
TA (b) UA (b) SA (b) SA(w)
Bio 0.3445 0.0430 0.3788 0.9270
Chem 0.3569 0.2566 0.4515 0.9941
Spoken <0.001 <0.001 <0.001 <0.001
Answers <0.001 0.0143 <0.001 <0.001
Emails 0.2020 <0.001 0.1622 0.0324
Newsgrs 0.3965 0.0210 0.1238 0.6602
Reviews 0.0020 0.0543 0.0585 0.0562
Weblogs 0.2480 0.0024 0.2435 0.9390
WSJ 0.4497 0.0024 0.2435 0.9390
Twitter 0.4497 0.0924 0.1111 0.7853
Table 2: POS tagging p-values across tagging ac-
curacy (TA), accuracy for unseen words (UA) and
sentence-level accuracy (SA) with bootstrap (b)
and Wilcoxon (w) (p < 0.05 gray-shaded).
LAS UAS
Answers 0.020 <0.001
Emails 0.083 <0.001
Newsgroups 0.049 <0.001
Reviews <0.001 <0.001
Weblogs <0.001 <0.001
WSJ <0.001 <0.001
Table 3: Parsing p-values (MALT-LIN
VS. STANFORD-RNN) across LAS and UAS
(p < 0.05 gray-shaded).
than STANFORD, but in one case it is the other way
around. If we do a Wilcoxon test over the results
on the 10 data sets, following the methodology
in Demsar (2006) and S?gaard (2013), the differ-
ence, which is ?0.12% on macro-average, is not
significant (p ? 0.1394). LAPOS is thus not sig-
nificantly better than STANFORD across data sets,
but as we have already seen, it is significantly bet-
ter on some data sets. So if we allow ourselves
to cherry-pick our data sets and report significance
over word-level tagging accuracies, we can at least
report significant improvements across a few data
sets.
Dependency parsing. Using the bootstrap test
over sentences, we get the p-values in Table 3.
We see that differences are always significant
wrt. UAS, and in most cases wrt. LAS.
NER. Here we use the macro-f
1
as our stan-
dard metric. FLORIAN is not significantly bet-
ter than CHIEU-NG with p < 0.05 as our cut-
off (p ? 0.15). The two systems were also re-
ported to have overlapping confidence intervals in
the shared task.
3.3 p-values across metrics
In several NLP subfields, multiple metrics are in
use. This happens in dependency parsing where
multiple metrics (Schwartz et al., 2011; Tsarfaty
et al., 2012) have been proposed in addition to un-
labeled and labeled attachment scores, as well as
exact matches. Perhaps more famously, in ma-
chine translation and summarization it is com-
mon practice to use multiple metrics, and there
exists a considerable literature on that topic (Pa-
pineni et al., 2002; Lin, 2004; Banerjee and Lavie,
2005; Clark et al., 2011; Rankel et al., 2011).
Even in POS tagging, some report tagging ac-
curacies, tagging accuracies over unseen words,
macro-averages over sentence-level accuracies, or
number of exact matches.
The existence of several metrics is not in it-
self a problem, but if researchers can cherry-pick
their favorite metric when reporting results, this
increases the a priori chance of establishing sig-
nificance. In POS tagging, most papers report sig-
nificant improvements over tagging accuracy, but
some report significant improvements over tag-
ging accuracy of unknown words, e.g., Denis and
Sagot (2009) and Umansky-Pesin et al. (2010).
This corresponds to the situation in psychology
where researchers cherry-pick between several de-
pendent variables (Simmons et al., 2011), which
also increases the chance of finding a significant
correlation.
Toy example. We draw two times 100 val-
ues from identical (0, 1)-Gaussians 1000 times
and calculate a t-test for two independent sam-
ples. This corresponds to testing the effect size
between two systems on a 1000 randomly cho-
sen test sets with N = 100. Since we are sam-
pling from the same distribution, the chance of
p < ? should be smaller than ?. In our simula-
tion, the empirical chance of obtaining p < 0.01
is .8%, and the chance of obtaining p < 0.05 is
4.8%, as expected. If we simulate a free choice
between two metrics by introducing choice be-
tween a pair of samples and a distorted copy of
that pair (inducing random noise at 10%), simu-
lating the scenario where we have a perfect metric
and a suboptimal metric, the chance of obtaining
p < 0.05 is 10.0%. We see a significant correla-
tion (p < 0.0001) between Pearson?s ? between
the two metrics, and the p-value. The less the two
metrics are correlated, the more likely we are to
obtain p < 0.05. If we allow for a choice between
two metrics, the chance of finding a significant dif-
ference increases considerably. If the two metrics
are identical, but independent (introducing a free
choice between two pairs of samples), we have
4
P (A_B) = P (A) + P (B)  P (A)P (B), hence
the chance of obtaining p < 0.01 is 1.9%, and the
chance of obtaining p < 0.05 is 9.75%.
POS tagging. In our POS-tagging experiments,
we saw a significant improvement in 3/10 cases
following the standard evaluation methodology
(see Table 2). If we allow for a choice between
tagging accuracy and sentence-level accuracy, we
see a significant improvement in 4/10 cases, i.e.,
for 4/10 data sets the effect is significance wrt. at
least one metric. If we allow for a free choice be-
tween all three metrics (TA, UA, and SA), we ob-
serve significance in 9/10 cases. This way the ex-
istence of multiple metrics almost guarantees sig-
nificant differences. Note that there are only two
data sets (Answers and Spoken), where all metric
differences appear significant.
Dependency parsing. While there are multi-
ple metrics in dependency parsing (Schwartz et
al., 2011; Tsarfaty et al., 2012), we focus on
the two standard metrics: labeled (LAS) and un-
labeled attachment score (UAS) (Buchholz and
Marsi, 2006). If we just consider the results in
Table 3, i.e., only the comparison of MALT-LIN
VS. STANFORD-RNN, we observe significant im-
provements in all cases, if we allow for a free
choice between metrics. Bod (2000) provides a
good example of a parsing paper evaluating mod-
els using different metrics on different test sets.
Chen et al. (2008), similarly, only report UAS.
NER. While macro-f
1
is fairly standard in
NER, we do have several available multiple met-
rics, including the unlabeled f
1
score (collapsing
all entity types), as well as the f
1
scores for each
of the individual entity types (see Derczynski and
Bontcheva (2014) for an example of only report-
ing f
1
for one entity type). With macro-f
1
and
f
1
for the individual entity types, we observe that,
while the average p-value for bootstrap tests over
five runs is around 0.15, the average p-value with a
free choice of metrics is 0.02. Hence, if we allow
for a free choice of metrics, FLORIAN comes out
significantly better than CHIEU-NG.
3.4 p-values across sample size
We now show that p-values are sensitive to sam-
ple size. While it is well-known that studies with
low statistical power have a reduced chance of
detecting true effects, studies with low statistical
power are also more likely to introduce false pos-
itives (Button et al., 2013). This, combined with
the fact that free choice between different sample
Figure 2: The distribution of p-values with (above)
and without (below) multiple metrics.
Figure 3: POS tagging p-values varying sample
sizes (p < 0.05 shaded).
sizes also increases the chance of false positives
(Simmons et al., 2011), is a potential source of er-
ror in NLP.
Toy example. The plot in Figure 2 shows the
distribution of p-values across 1000 bootstrap tests
(above), compared to the distribution of p-values
with a free choice of four sample sizes. It is clear
that the existence of multiple metrics makes the
probability of a positive result much higher.
POS tagging. The same holds for POS tag-
ging. We plot the p-values across various sample
sizes in Figure 3. Note that even when we ignore
the smallest sample size (500 words), where re-
sults may be rather unreliable, it still holds that for
Twitter, Answers, Newsgrs, Reviews, Weblogs and
WSJ, i.e., more than half of the data sets, a sig-
nificant result (p < 0.05) becomes insignificant
by increasing the sample size. This shows how
unreliable significance results in NLP with cut-off
p < 0.05 are.
5
Figure 4: Parsing p-values varying sample sizes
(p < 0.05 shaded)
Figure 5: NER p-values varying sample sizes (p <
0.05 shaded)
Dependency parsing. We performed simi-
lar experiments with dependency parsers, seeing
much the same picture. Our plots are presented in
Figure 4. We see that while effect sizes are al-
ways significant wrt. UAS, LAS differences be-
come significant when adding more data in 4/6
cases. An alternative experiment is to see how
often a bootstrap test at a particular sample size
comes out significant. The idea is to sample, say,
10% of the test data 100 times and report the ra-
tio of positive results. We only present the results
for MALT-LIN VS. STANFORD-RNN in Table 4,
but the full set of results (including comparisons of
more MaltParser and Stanford parser models) are
made available at http://lowlands.ku.dk.
For MALT-LIN VS. STANFORD-RNN differ-
ences on the full Emails data set are consistently
insignificant, but on small sample sizes we do get
significant test results in more than 1/10 cases. We
see the same picture with Newsgrs and Reviews.
On Weblogs and WSJ, the differences on the full
data sets are consistently significant, but here we
see that the test is underpowered at small sam-
ple sizes. Note that we use bootstrap tests over
sentences, so results with small samples may be
somewhat unreliable. In sum, these experiments
show how small sample sizes not only increase the
chance of false negatives, but also the chance of
false positives (Button et al., 2013).
NER. Our plots for NER are presented in Fig-
ure 5. Here, we see significance at small sam-
ple sizes, but the effect disappears with more data.
This is an example of how underpowered studies
may introduce false positives (Button et al., 2013).
3.5 p-values across covariates
Toy example. If we allow for a choice between
two subsamples, using a covariate to single out a
subset of the data, the chance of finding a signifi-
cant difference increases. Even if we let the subset
be a random 50-50 split, the chance of obtaining
p < 0.01 becomes 2.7%, and the chance of obtain-
ing p < 0.05 is 9.5%. If we allow for both a choice
of dependent variables and a random covariate, the
chance of obtaining p < 0.01 is 3.7%, and the
chance of obtaining p < 0.05 is 16.2%. So iden-
tical Gaussian variables will appear significantly
different in 1/6 cases, if our sample size is 100,
and if we are allowed a choice between two iden-
tical, but independent dependent variables, and a
choice between two subsamples provided by a ran-
dom covariate.
POS We see from Figure 6 that p-values are
also very sensitive to sentence length cut-offs. For
instance, LAPOS is significantly (p < 0.05) bet-
ter than STANFORD on sentences shorter than 16
words in EMAILS, but not on sentences shorter
than 14 words. On the other hand, when longer
sentences are included, e.g., up to 22 words, the
effect no longer appears significant. On full sen-
tence length, four differences seem significant, but
if we allow ourselves to cherry-pick a maximum
sentence length, we can observe significant differ-
ences in 8/10 cases.
Figure 6: POS tagging p-values varying sentence
length (p < 0.05 shaded)
We observe similar results in Dependency
parsing and NER when varying sentence length,
but do not include them here for space rea-
sons. The results are available at http://
lowlands.ku.dk. We also found that other
covariates are used in evaluations of dependency
parsers and NER systems. In dependency pars-
ing, for example, parsers can either be evaluated
6
N Emails Newsgrs Reviews Weblogs WSJ
LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
10% 14 % 100 % 9 % 100 % 33% 100 % 42 % 99 % 28 % 75 %
25% 15 % 100 % 23 % 100 % 52% 100 % 68 % 100 % 27 % 98 %
50% 19 % 100 % 25 % 100 % 78% 100 % 100 % 100 % 60 % 100 %
75% 22 % 100 % 41 % 100 % 97% 100 % 100 % 100 % 80 % 100 %
100% 0 % 100 % 36 % 100 % 100% 100 % 100 % 100 % 100 % 100 %
Table 4: Ratio of positive results (p < 0.05) for MALT-LIN VS. STANFORD-RNN at sample sizes (N )
.
on naturally occurring text such as in our experi-
ments or at tailored test suites, typically focusing
on hard phenomena (Rimell et al., 2009). While
such test suites are valuable resources, cf. Man-
ning (2011), they do introduce free choices for re-
searchers, increasing the a priori chance of posi-
tive results. In NER, it is not uncommon to leave
out sentences without any entity types from eval-
uation data. This biases evaluation toward high
recall systems, and the choice between including
them or not increases chances of positive results.
4 How likely are NLP findings to be
false?
The previous sections have demonstrated how
many factors can contribute to reporting an erro-
neously significant result. Given those risks, it is
natural to wonder how likely we are as a field to
report false positives. This can be quantified by
the positive predictive value (PPV), or probability
that a research finding is true. PPV is defined as
(1  )R
R  R+?
(1)
The PPV depends on the type 1 and 2 error rates
(? and  ) and the ratio of true relations over null
relations in the field (R) (Ioannidis, 2005).
R. The likelihood that a research finding is true
depends on the ratio of true relations over null re-
lations in the field, usually denoted R (Ioannidis,
2005). Out of the systems that researchers in the
field would test out (not rejecting them a priori),
how many of them are better than the current state
of the art? The a priori likelihood of a relation be-
ing true, i.e., a new system being better than state
of the art, is R/(R+1). Note that while the space
of reasonably motivated methods may seem big to
researchers in the field, there is often more than
one method that is better than the current state of
the art. Obviously, as the state of the art improves,
R drops. On the other hand, if R becomes very
low, researchers are likely to move on to new ap-
plications where R is higher.
The type 1 error rate (?) is also known as the
false positive rate, or the likelihood to accept a
non-significant result. Since our experiments are
fully automated and deterministic, and precision
usually high, the type 1 error rate is low in NLP.
What is not always appreciated in the field is that
this should lead us to expect true effects to be
highly significant with very low p-values, much
like in physics. The type 2 error rate ( ) is the
false negative rate, i.e., the likelihood that a true
relation is never found. This factors into the recall
of our experimental set-ups.
So what values should we use to estimate PPV?
Our estimate for R (how often reasonable hy-
potheses lead to improvements over state of the
art) is around 0.1. This is based on a sociolog-
ical rather than an ontological argument. With
? = 0.05 and R = 0.1, researchers get positive
results inR+(1 R)? cases, i.e.,? 1/7 cases. If
researchers needed to test more than 7 approaches
to ?hit the nail?, they would never get to write pa-
pers. With ? = 0.05, and   set to 0.5, we find that
the probability of a research finding being true ?
given there is no selection bias and with perfectly
valid metrics ? is just 50%:
PPV =
(1  )R
R  R+?
=
0.5?0.1
0.1 0.05+0.05
=
0.05
0.1
= 0.5
(2)
In other words, if researchers do a perfect experi-
ment and report p < 0.05, the chance of that find-
ing being true is the chance of seeing tail when
flipping a coin. With p < 0.01, the chance is 5/6,
i.e., the chance of not getting a 3 when rolling a
die. Of course these parameters are somewhat ar-
bitrary. Figure 7 shows PPV for various values of
?.
In the experiments in Section 3, we consistently
used the standard p-value cut-off of 0.05. How-
ever, our experiments have shown that significance
results at this threshold are unreliable and very
sensitive to the choice of sample size, covariates,
or metrics. Based on the curves in Figure 7, we
7
Figure 7: PPV for different ? (horizontal line is PPV for p = 0.05, vertical line is ? for PPV=0.95).
could propose a p-value cut-off at p < 0.0025.
This is the cut-off that ? in the absence of bias and
with perfect metrics ? gives us the level of con-
fidence we expect as a research community, i.e.,
PPV = 0.95. Significance results would thus be
more reliable and reduce type 1 error.
5 Discussion
Incidentally, the p < 0.0025 cut-off also leads to
a 95% chance of seeing the same effect on held-
out test data in Berg-Kirkpatrick et al. (2012) (see
their Table 1, first row). The caveat is that this
holds only in the absence of bias and with perfect
metrics. In reality, though, our data sets are of-
ten severely biased (Berg-Kirkpatrick et al., 2012;
S?gaard, 2013), and our metrics are far from per-
fect (Papineni et al., 2002; Lin, 2004; Banerjee
and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et
al., 2012). Here, we discuss how to address these
challenges.
Selection bias. The WSJ FALLACY (Section
1) has been widely discussed in the NLP litera-
ture (Blitzer et al., 2006; Daume III, 2007; Jiang
and Zhai, 2007; Plank and van Noord, 2011). But
if our test data is biased, how do we test whether
System A performs better than System B in gen-
eral? S?gaard (2013) suggests to predict signif-
icance across data sets. This only assumes that
data sets are randomly chosen, e.g., not all from
newswire corpora. This is also standard practice in
the machine learning community (Demsar, 2006).
Poor metrics. For tasks such as POS tagging
and dependency parsing, our metrics are subopti-
mal (Manning, 2011; Schwartz et al., 2011; Tsar-
faty et al., 2012). System A and System B may
perform equally well as measured by some met-
ric, but contribute very differently to downstream
tasks. Elming et al. (2013) show how parsers
trained on different annotation schemes lead to
very different downstream results. This suggests
that being wrong with respect to a gold standard,
e.g., choosing NP analysis over a ?correct? DP
analysis, may in some cases lead to better down-
stream performance. See the discussion in Man-
ning (2011) for POS tagging. One simple ap-
proach to this problem is to report results across
available metrics. If System A improves over Sys-
tem B wrt. most metrics, we obtain significance
against the odds. POS taggers and dependency
parsers should also be evaluated by their impact
on downstream performance, but of course down-
stream tasks may also introduce multiple metrics.
6 Conclusion
In sum, we have shown that significance results
with current research standards are unreliable, and
we have provided a more adequate p-value cut-off
under the assumption of perfect metrics and unbi-
8
ased data. In the cases where these assumptions
cannot be met, we suggest reporting significance
results across datasets wrt. all available metrics.
Acknowledgements
We would like to thank the anonymous review-
ers, as well as Jakob Elming, Matthias Gondan,
and Natalie Schluter for invaluable comments and
feedback. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
References
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: an automatic metric for MT evaluation with
improved correlation with human judgments. In
ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for MT and/or Summarization.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statis-
tical significance in nlp. In EMNLP.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Rens Bod. 2000. Parsing with the shortest derivation.
In COLING.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In CoNLL.
Katherine Button, John Ioannidis, Claire Mokrysz,
Brian Nosek, Jonathan Flint, Emma Robinson, and
Marcus Munafo. 2013. Power failure: why small
sample size undermines the reliability of neuro-
science. Nature Reviews Neuroscience, 14:365?376.
Wenliang Chen, Youzheng Wu, and Hitoshi Isahara.
2008. Learning Reliable Information for Depen-
dency Parsing Adaptation. In COLING.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In CoNLL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In ACL.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Janez Demsar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1?30.
Pascal Denis and Beno??t Sagot. 2009. Coupling an
annotated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort.
In PACLIC.
Leon Derczynski and Kalina Bontcheva. 2014.
Passive-aggressive sequence labeling with discrim-
inative post-editing for recognising person entities
in tweets. In EACL.
Bradley Efron and Robert Tibshirani. 1993. An intro-
duction to the bootstrap. Chapman & Hall, Boca
Raton, FL.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez Alonso, and
Anders S?gaard. 2013. Down-stream effects of
tree-to-dependency conversions. In NAACL.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In CoNLL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
John Ioannidis. 2005. Why most published research
findings are false. PLoS Medicine, 2(8):696?701.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In WAS.
Chris Manning. 2011. Part-of-speech tagging from
97% to 100%: Is it time for some linguistics? In
CICLing.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 Shared Task on
Dependency Parsing. In EMNLP-CoNLL.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007b. MaltParser:
a language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Eric Noreen. 1989. Computer intensive methods for
testing hypotheses. Wiley.
Kishore Papineni, Salim Roukus, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL, pages
311?318, Philadelphia, Pennsylvania.
Barbara Plank and Gertjan van Noord. 2011. Effective
measures of domain similarity for parsing. In ACL.
Peter Rankel, John Conroy, Eric Slud, and Dianne
O?Leary. 2011. Ranking human and machine sum-
marization systems. In EMNLP.
9
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance test-
ing for MT. In ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization.
Laura Rimell, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In EMNLP.
Roy Schwartz, and Omri Abend, Roi Reichart, and
Ari Rappoport. 2011. Neutralizing linguisti-
cally problematic annotations in unsupervised de-
pendency parsing evaluation. In ACL.
Joseph Simmons, Leif Nelson, and Uri Simonsohn.
2011. False-positive psychology: undisclosed flexi-
bility in data collection and analysis allows present-
ing anything as significant. Psychological Science,
22(11):1359?1366.
Richard Socher, John Bauer, Chris Manning, and An-
drew Ng. 2013. Parsing with compositional vector
grammars. In ACL.
Anders S?gaard. 2013. Estimating effect size across
datasets. In NAACL.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
In CoNLL.
Kristina Toutanova, Dan Klein, Chris Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In
NAACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In EACL.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: can
history-based models rival globally optimized mod-
els? In CoNLL.
Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-
poport. 2010. A multi-domain web-based algorithm
for POS tagging of unknown words. In COLING.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In ACL.
10
