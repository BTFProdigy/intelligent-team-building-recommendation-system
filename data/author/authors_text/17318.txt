Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 356?363, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
WBI-NER: The impact of domain-specific features on the performance of
identifying and classifying mentions of drugs
Tim Rockta?schel Torsten Huber Michael Weidlich Ulf Leser
Humboldt-Universita?t zu Berlin
Knowledge Management in Bioinformatics
Unter den Linden 6
Berlin, 10099, Germany
{trocktae,thuber,weidlich,leser}@informatik.hu-berlin.de
Abstract
Named entity recognition (NER) systems
are often based on machine learning tech-
niques to reduce the labor-intensive devel-
opment of hand-crafted extraction rules and
domain-dependent dictionaries. Nevertheless,
time-consuming feature engineering is often
needed to achieve state-of-the-art performance.
In this study, we investigate the impact of such
domain-specific features on the performance
of recognizing and classifying mentions of
pharmacological substances. We compare the
performance of a system based on general fea-
tures, which have been successfully applied
to a wide range of NER tasks, with a system
that additionally uses features generated from
the output of an existing chemical NER tool
and a collection of domain-specific resources.
We demonstrate that acceptable results can be
achieved with the former system. Still, our ex-
periments show that using domain-specific fea-
tures outperforms this general approach. Our
system ranked first in the SemEval-2013 Task
9.1: Recognition and classification of pharma-
cological substances.
1 Introduction
The accurate identification of drug mentions in text
is an important prerequisite for many applications, in-
cluding the retrieval of information about substances
in drug development (e.g. Roberts and Hayes (2008)),
the identification of adverse drug effects (e.g. Lea-
man et al (2010)) and the recognition of drug-drug
interactions (e.g. Thomas et al (2011)). Given that
most of the information related to drug research is
covered by medical reports and pharmacological pub-
lications, computational methods for information ex-
traction should be used to support this task.
The SemEval-2013 Task 9.1 competition1 (Segura-
Bedmar et al, 2013) aims at a fair assessment on the
state-of-the-art of tools that recognize and classify
mentions of pharmacological substances in natural
language texts ? a task referred to as drug named
entity recognition (NER). The goal of participating
teams is to recreate the gold annotation on a held-
out part of an annotated corpus. Four classes of en-
tities have to be identified: Drug, DrugN, Group
and Brand. Entities of class Drug denote any kind
of drug that is approved for use in humans, whereas
DrugN denotes substances that are not approved.
Group are terms describing a group of drugs and
Brand stands for drug names introduced by a phar-
maceutical company.
The aim of this study is to examine whether it is
worthwhile to implement domain-specific features
for supporting drug NER. The question we attempt
to answer is whether such features really help in iden-
tifying and classifying mentions of drugs or whether
a mostly domain-independent feature set, which can
be applied to many other tasks, achieves a compara-
ble performance.
2 Related work
Various NER systems for identifying different
classes of chemical entities, including mentions of
drugs, trivial names and IUPAC terms, have been pro-
posed.
1http://www.cs.york.ac.uk/semeval-2013/task9/
(accessed 2013-04-29)
356
Klinger et al (2008) trained a conditional random
field (CRF) (Lafferty et al, 2001) for extracting men-
tions of IUPAC and IUPAC-like entities. They report
an F1 measure of 85.6% on a hand-annotated corpus
consisting of MEDLINE abstracts.
Segura-Bedmar et al (2008) introduced Drug-
NER, which is based on UMLS MetaMap Trans-
fer (MMTx) and nomenclature rules by the World
Health Organization International Nonproprietary
Names (INNs). Their system extracts and classifies
mentions of drugs and achieves a precision of 99.1%
and a recall of 99.8% on a silver-standard corpus.
OSCAR (Open-Source Chemistry Analysis Rou-
tines) (Corbett and Murray-Rust, 2006; Jessop et al,
2011) extracts mentions of a wide range of chemi-
cals using a maximum entropy Markov model (Mc-
Callum et al, 2000). It achieves an F1 of 83.2% on
a corpus consisting of PubMed abstracts and 80.7%
on a corpus consisting of chemistry papers (Corbett
and Copestake, 2008).
Hettne et al (2009) compiled Jochem (the joint
chemical dictionary) from ChemIDplus, ChEBI,
DrugBank, PubChem, HMDB, KEGG, MeSH and
CAS Registry IDs. Jochem was used with Peregrine
(Schuemie et al, 2007), a dictionary-based NER tool,
achieving an F1 of 50% on the SCAI corpus (Kola?rik
et al, 2008).
We developed ChemSpot (Rockta?schel et al,
2012), a system for extracting mentions of various
kinds of chemicals from text. We applied a CRF
for extracting mentions of IUPAC entities based on
the work of Klinger et al (2008) and used Jochem
(Hettne et al, 2009) with an adapted matching-
mechanism for identifying trivial names, drugs and
brands. ChemSpot v1.0 achieved an overall F1 of
68.1% on the SCAI corpus. In the meantime, we have
worked on several enhancements (see Section 3.1).
The SemEval-2013 Task 9.1 poses new challenges
on NER tools. Instead of targeting all kinds of chem-
icals, it focuses on drugs, i.e., pharmacological sub-
stances that affect humans and are used for adminis-
tration. Moreover, entities need to be classified into
the four categories mentioned above.
3 Methods
Our approach is based on a linear-chain CRF with
mostly domain-independent features commonly ap-
plied to NER tasks. In addition, we employ vari-
ous domain-specific features derived from the out-
put of ChemSpot?s components, as well as Jochem,
the PHARE ontology (Coulet et al, 2011) and the
ChEBI ontology (De Matos et al, 2010). In the
following, we first explain extensions to ChemSpot.
Subsequently, we give a brief introduction to linear-
chain CRFs before describing the general and
domain-specific features used by our system. Finally,
we explain the experimental setup and discuss our
results.
3.1 Improvements of ChemSpot
To improve ChemSpot?s chemical NER performance,
we extend it by two components and modify its
match-expansion mechanism.
The first addition is a pattern-based tagger
for chemical formulae. In its basic form it ex-
tracts mentions matching the regular expression
(S N?(\+|-)?)+ where S denotes a chemical
symbol and N a natural number greater one.2 This
pattern is augmented by filters to comply with other
naming conventions, such as correct grouping of
compounds with parentheses.
The second extension targets ambiguous ab-
breviations. For example, the abbreviation ?DAG?
could denote ?diacylglycerol? or ?directed acyclic
graph?. We use ABBREV, an algorithm proposed
by Schwartz and Hearst (2003), for extracting such
abbreviations and their definitions (e.g. ?diacylglyc-
erol (DAG)?). Note that the position of the long form
(LF) and short form (SF) is interchangeable. To dis-
ambiguate between chemical and non-chemical ab-
breviations, we apply the following two rules to the
mentions extracted by ChemSpot: (1) For a given
pair of LF and SF, we check whether the LF was
found to be a chemical but the SF was not. In this
case we add a new annotation for every occurrence
of the SF in the document. (2) Contrary to that, if
only the SF was tagged as a chemical but the LF was
not, we assume that the abbreviation does not refer
to a chemical and remove all annotations of the SF
in the document.
ChemSpot?s match-expansion often leads to the
extraction of non-chemical suffixes corresponding to
verbs, e.g., ?-induced?, ?-enriched? or ?-mediated?.
2By convention, 1 is ommited (e.g. CO2 instead of C1O2).
357
yiyi?1yi?2 yi+1 yi+2
?? ? ? ?
x
Figure 1: A factor graph for a 1st-order linear-chain
CRF (2nd-order with dashed edges). Note that for
each feature function fj in the factor ?, the same
weight ?j is used at all other positions (gray) in the
sequence (parameter tying).
To tackle this issue we stop the expansion at tokens
whose part-of-speech tag refers to a verb form. Fur-
thermore, we integrated OPSIN (Lowe et al, 2011)
to normalize entity mentions to InChI strings.
The current v1.5 release of ChemSpot achieves an
overall F1 of 74.2% on the SCAI corpus, improving
the performance by 6.1 percentage points (pp) F1
compared to ChemSpot v1.0.
3.2 Linear-chain conditional random fields
Contrary to the hybrid strategy used in ChemSpot,
we follow a purely machine learning based approach
for drug NER in this work. NER can be formulated
as a sequence labeling task where the goal is to find
a sequence of labels y = {y1, . . . , yn} given a se-
quence x = {x1, . . . , xn} of observed input tokens.
Labels commonly follow the IOB format, where B
denotes a token at the beginning of an entity men-
tion, I denotes the continuation of a mention and O
corresponds to tokens that are not part of a mention.
Extracting entity mentions from a tokenized text x
then amounts to finding y? = arg maxy p(y |x).
Linear-chain CRFs are well-known discriminative
undirected graphical models that encode the condi-
tional probability p(y |x) of a set of input variables
x and a sequence of output variables y (see Wallach
(2004) or Klinger and Tomanek (2007) for an intro-
duction). In the case of NER, x is a sequence of n
tokens and y a sequence of n corresponding labels.
Linear-chain CRFs of order k factorize p(y |x) into
a product of factors ?, globally normalized by an
input-dependent partition function Z(x):
p(y |x) =
1
Z(x)
n?
i=1
?(yi?k, . . . , yi?1, yi,x, i).
A factor ? is commonly defined as the exponen-
tial function of a sum of weighted feature functions
{f1, . . . , fm}:
1
Z(x)
n?
i=1
exp
?
?
m?
j=1
?jfj(yi?k, . . . , yi?1, yi,x, i)
?
? .
The feature function weights {?j} can be learned
from training data and are shared across all positions
i (parameter tying).
The factorization of a CRF can be illustrated by
a factor graph (Kschischang et al, 2001). A factor
graph is a bipartite graph, where one set of nodes
corresponds to random variables and the other to fac-
tors. Each factor is connected to the variables of its
domain, making the factorization of the model ap-
parent. Figure 1 shows a factor graph for a segment
of a linear-chain CRF of order one and two respec-
tively. In a linear-chain CRF of order k, the label of
a token at position i is connected via feature func-
tions of a factor to the input sequence x as well as
the previous k labels. For example, in a first-order
linear-chain CRF, one of the feature functions could
be f[O?B,capital](yi?1, yi,x, i), which evaluates to 1 if
yi?1 = O, yi = B and xi starts with a capital letter
(otherwise it yields 0). Multiplication with the weight
?[O?B,capital] yields an unnormalized local score that
indicates how favorable a transition from O to B is
provided that the token at position i starts with a capi-
tal letter. Note that the terms feature function and fea-
ture are often used synonymously and the case differ-
entiation for different labels is implicit. In this work
a feature fcapital denotes its corresponding set of first-
order feature functions {f[s?t,capital](yi?1, yi,x, i) |
(s, t) ? {(B,B), (B,I), (B,O), (I,B), (I,I), (I,O),
(O,B), (O,O)}}3 (similarly for second-order).
We employ MALLET (McCallum, 2002) as under-
lying CRF implementation and use a second-order
linear-chain CRF with offset conjunctions of order
two. Offset conjunctions of order k adds features
around a window of k to the features at a partic-
ular position, providing more contextual informa-
tion. Akin to Klinger et al (2008), we perform fine-
grained tokenization, splitting at special characters
and transitions from alphanumeric characters to dig-
its. An exemplary tagging sequence is shown in Ta-
ble 1.
3Transitions from O to I are invalid.
358
i 0 1 2 3 4 5 6 7 8 9 10
y O B-DrugN O B-DrugN I-DrugN I-DrugN O B-Group O O O
x Both ibogaine and 18 - MC ameliorate opioid withdrawal signs .
Table 1: Example label sequence for the tokenized sentence MedLine.d110.s4 of the training corpus.
Feature Class Description
FC
fCHEMSPOT part of a prediction by ChemSpot
fIUPAC part of an IUPAC entity
fFORMULA part of a chemical formula
fDICTIONARY part of a dictionary match
fABBREV part of a chemical abbreviation
FJ
fJOCHEM dictionaries in Jochem
fPREFIX frequent chemical prefix
fSUFFIX frequent chemical suffix
FO
fPHARE PHARE ontology
fCHEBDESCS #descendants in ChEBI ontology
fCHEBDEPTH average depth in ChEBI ontology
FG
fKLINGER see Klinger et al (2008)
fBANNER see Leaman and Gonzalez (2008)
fABNER see Settles (2005)
FF
fUPPERCASESENT. part of an upper-case sentence
fPREVWINDOW text of preceding four tokens
fNEXTWINDOW text of succeeding four tokens
Table 2: Overview of features used for identifying
and classifying mentions of pharmacological sub-
stances.
Note that the sequence-labeling approach in the
described form cannot cope with discontinuous en-
tity mentions. Since only a tiny fraction (? 0.3%) of
entities in the training corpus are discontinuous, we
simply neglect these for training and tagging.
3.3 Feature sets
An overview of the features used by our system is
shown in Table 2. Our first two submissions for the
SemEval-2013 Task 9.1 differ only in that they use
different subsets of these features. Run 1 employs a
feature set assembled from common general features
used for biomedical NER (FG ? FF ), whereas Run
2 additionally uses features tailored for extracting
mentions of chemicals (FC ? FJ ? FO).
3.3.1 Run 1: general features FG and FF
We employ a union of common, rather domain-
independent features published by Klinger et al
(2008), Settles (2005) and Leaman and Gonzalez
(2008). Note that these feature sets have been suc-
cessfully applied to a wide range of different biomed-
ical NER tasks, e.g., identifying mentions of DNA
sequences, genes, diseases, mutations, IUPAC terms,
cell lines and cell types. They encompass morpho-
logical, syntactic and orthographic features, such as
the text of the token itself, token character n-grams
of length 2 and 3, prefixes and suffixes of length 2, 3,
and 4, characters left and right to a token and part-of-
speech tags. Furthermore, they contain various regu-
lar expressions that capture, for instance, whether a
token starts with a capital letter or contains digits.
In addition, we employ features based on NER
examples in FACTORIE (McCallum et al, 2009).
Specifically, we use the text of the four preceding
and succeeding tokens and whether a token is part of
a sentence that contains only upper-case characters.
The latter is commonly the case for headlines, which
likely contain an entity mention.
3.3.2 Run 2: domain-specific features
In addition to the features of Run 1, we use pre-
dictions of our improved version of ChemSpot, as
well as features derived from Jochem, PHARE and
ChEBI.
ChemSpot-based features FC: When a token is
part of a mention extracted by one of ChemSpot?s
components (i.e. IUPAC entity, chemical formula,
dictionary match or chemical abbreviation), we use
the name of the respective component as feature. In
addition, we determine whether a token is part of an
entity predicted by ChemSpot after match-expansion,
boundary-correction and resolution of overlapping
entities. Using the output of ChemSpot as features
for our system could be framed as stacking (see
Wolpert (1992)).
Jochem-based features FJ : For every dictionary
contained in Jochem, we check whether a token is
part of an entity in that dictionary and use the name of
the dictionary as feature. Furthermore, we compile
359
a list of frequent chemical suffixes and prefixes of
length three from Jochem.
Ontology-based features FO: It is often hard to
determine whether a mention refers to a specific
chemical entity or rather an abstract term denoting
a group of chemicals. To distinguish between these
two cases, we calculate the average depth and the
number of descendants of a term in the ChEBI on-
tology and use the binned count as feature. The idea
behind these features is that the specificity of an en-
tity correlates positively with its depth in the ontol-
ogy (e.g. leaf nodes are likely specific chemicals)
and negatively with the number of descendants (i.e.
having few descendants indicates a specific entity).
Further ontology-based features are derived from
PHARE, which consists of 200 curated relations. If
possible, we map a token to a term in that ontology
and use its label as feature.
3.4 Experiments
We perform document-level 10-fold cross-validation
(CV) on the training corpus to measure the impact
of domain-specific features. To ensure comparability
between Run 1 and Run 2, we use the same splits for
evaluation. Furthermore, we train models on the com-
plete training corpus and evaluate on the test corpus
of DDI Task 9.1 for each run respectively. In addi-
tion, we train a third model based on the best feature
set determined with CV and use the entity mentions
of the Task 9.2 test corpus, which also contains anno-
tations of drug-drug interactions, as additional train-
ing data (Run 3). Following the SemEval-2013 Task
9.1 metrics, we evaluate exact matching performance
(correct entity boundaries) and strict matching per-
formance (correct boundaries and correct type).
4 Results
Table 3 shows micro-average CV results for identi-
fying and classifying mentions of pharmacological
substances in the training corpus. The performance
varies drastically between different entity classes re-
gardless of the feature set, e.g., Run 1 achieves an F1
of 91.0% for Drug, but only 15.9% F1 for DrugN.
Run 2 outperforms Run 1 for entities of class
Drug (+1.2 pp F1) and DrugN (+4.9 pp F1), but
yields a lower performance for Brand (?0.9 pp
F1) and no change for Group entities. Overall, the
Run 1 Run 2
P R F1 P R F1 ?F1
Drug 92.1 89.9 91.0 92.0 92.3 92.2 +1.2
DrugN 54.7 9.3 15.9 62.4 12.5 20.8 +4.9
Group 87.2 82.5 84.8 87.3 82.3 84.8 0.0
Brand 87.8 70.8 78.4 87.1 69.8 77.5 ?0.9
Exact 93.9 86.9 90.3 94.5 89.0 91.7 +1.4
Strict 90.3 83.6 86.8 90.3 85.1 87.6 +0.8
Table 3: Document-level 10-fold cross-validation
micro-average results on the training corpus.
micro-average F1 measure increases by 0.8 pp for
strict matching and 1.4 pp F1 for exact matching.
The performance on the test corpus (see Table 4)
is drastically lower compared to CV results (e.g. 17.6
pp F1 for strict evaluation of Run 1). Except for enti-
ties of class Group, using domain-specific features
leads to a superior performance for identifying and
classifying mentions of pharmacological substances.
Run 2 outperforms Run 1 by 1.6 pp F1 for strict eval-
uation and 5.9 pp F1 for exact evaluation. Using en-
tity mentions of the Task 9.2 test corpus as additional
training data (Run 3) further boosts the performance
by 0.7 pp F1 for strict evaluation.
5 Discussion
Our results show a clear performance advantage
when using domain-specific features tailored for
identifying mentions of chemicals. CV results and
results on the test corpus show an increase in preci-
sion and recall for exact matching and an increase
in recall for strict matching. The considerably higher
recall for exact matching can be attributed to a higher
coverage of chemical entities by features that exploit
domain-knowledge.
It is striking that the performance for DrugN enti-
ties is extremely low compared to the other classes.
We believe that this might be due to two reasons.
First, entities of this class are underrepresented in
the training corpus (? 3%). Since machine learning
based methods tend to favor the majority class, it is
likely that many DrugN entities were classified as
mentions of one of the much larger classes Drug (?
64%) or Group (? 23%). This can be confirmed by
the large differences between strict and exact match-
ing results shown in Table 3 and Table 4.
360
Run 1 Run 2 Run 3
# P R F1 P R F1 ?F1 P R F1 ?F1
Drug 351 74.2 79.5 76.8 72.9 85.2 78.6 +1.8 73.6 85.2 79.0 +0.4
DrugN 121 25.0 4.1 7.1 35.7 8.3 13.4 +6.3 31.4 9.1 14.1 +0.7
Group 155 77.3 74.8 76.1 78.1 73.5 75.7 ?0.4 79.2 76.1 77.6 +1.9
Brand 59 76.2 81.4 78.7 77.8 83.1 80.3 +1.6 81.0 86.4 83.6 +3.3
Exact 686 82.1 72.9 77.2 85.6 80.8 83.1 +5.9 85.5 81.3 83.3 +0.2
Strict 686 73.6 65.3 69.2 73.0 68.8 70.8 +1.6 73.4 69.8 71.5 +0.7
DrugBank (Strict) 304 86.9 85.2 86.0 87.3 86.2 86.8 +0.8 88.1 87.5 87.8 +1.0
MEDLINE (Strict) 382 60.8 49.5 54.5 60.5 55.0 57.6 +3.1 60.7 55.8 58.1 +0.5
Table 4: Results on the test corpus. ?F1 denotes the F1 pp difference to the preceding Run and # the number
of annotated mentions
Second, DrugN denotes substances that have an ef-
fect on humans, but are not approved for medical use
? a property that is rarely stated along with the entity
mention and can thus often only be determined with
domain-knowledge.
We think it is also important to point to the large
difference between results obtained by 10-fold CV
on the training corpus and test results (e.g. up to 17.6
pp F1 for Run 1). One reason might be the large frac-
tion (? 83%) of entity mentions that appear more
than once in the training corpus compared to pre-
sumably many unseen entities in the test corpus. For
10-fold CV this means that an entity in the evaluation
fold has already been seen with a high probability in
one of the nine training folds, yielding results that
overestimate the generalization performance. More-
over, our results indicate that identifying and clas-
sifying pharmacological substances is much harder
for MEDLINE documents than for DrugBank docu-
ments with a difference of up to 31.5 pp F1 (cf. the
last two rows of Table 4). Hence, another apparent
reason for the performance differences is the sub-
stantial skew in the ratio of DrugBank to MEDLINE
documents in the training corpus (roughly 4:1) com-
pared to the test corpus (roughly 1:1). Since both sets
of documents stem from different resources, this can
be referred to as domain-adaptation problem.
In additional experiments we found that the
general-purpose chemical NER tool ChemSpot
achieves an F1 of 65.5% for exact matching on the
test corpus. This is 17.8 pp F1 below our best results
obtained with a machine learning based system (cf.
Run 3) that is able to exploit properties of the task-
specific annotations of the corpora.
6 Conclusion
We described our contribution to the SemEval-2013
Task 9.1: Recognition and classification of pharma-
cological substances. We found that a system based
on rather general features commonly used for a wide
range of biomedical NER tasks yields competetive
results. Implementing this system needed no domain-
adaptation and its performance could be sufficient for
applications building upon drug NER. Nevertheless,
adding domain-specific features boosts the perfor-
mance considerably. Further improvements can be
achieved by using entity annotations of the Task 9.2
test corpus as additional training data.
We identified two limitations of our approach.
First, we found that entities of the minority class
(DrugN) are very hard to classify correctly. Second,
differences between DrugBank and MEDLINE docu-
ments probably cause a domain-adaptation problem.
For future work, one could investigate whether the
latter can be addressed by domain-adaptation tech-
niques (e.g. Satpal and Sarawagi (2007)). To cope
with DrugN entities, one could implement features
derived from those resources that were used by the
annotators for deciding whether a substance is ap-
proved for use in humans, e.g., Drugs@FDA4 and
the WHO ATC5 classification system.
4http://www.accessdata.fda.gov/scripts/cder/
drugsatfda/ (accessed 2013-04-29)
5http://www.whocc.no/atc_ddd_index/ (accessed
2013-04-29)
361
Acknowledgements
We thank Philippe Thomas for preparing a simplified for-
mat of the corpora. We thank him and Roman Klinger for
fruitful discussions.
Funding: Tim Rockta?schel is funded by the German Fed-
eral Ministry of Economics and Technology (BMWi)
[KF2205209MS2], Torsten Huber and Michael Weidlich
are funded by the German Federal Ministry of Education
and Research (BMBF) [0315746].
References
Peter Corbett and Ann Copestake. 2008. Cascaded classi-
fiers for confidence-based chemical named entity recog-
nition. BMC Bioinf., 9(Suppl 11):S4.
Peter Corbett and Peter Murray-Rust. 2006. High-
throughput identification of chemistry in life science
texts. In Proc. of CompLife 2006, pages 107?118.
Adrien Coulet, Yael Garten, Michel Dumontier, Russ B.
Altman, Mark A. Musen, and Nigam H. Shah. 2011. In-
tegration and publication of heterogeneous text-mined
relationships on the Semantic Web. J. Biomed. Seman-
tics, 2(Suppl 2):S10.
Paula De Matos, Rafael Alca?ntara, Adriano Dekker, Mar-
cus Ennis, Janna Hastings, Kenneth Haug, Inmaculada
Spiteri, Steve Turner, and Christoph Steinbeck. 2010.
Chemical Entities of Biological Interest: an update. Nu-
cleic Acids Res., 38:D249?D254.
Kristina M. Hettne, Rob H. Stierum, Martijn J. Schuemie,
Peter J.M. Hendriksen, Bob J.A. Schijvenaars, Erik
M. Van Mulligen, Jos Kleinjans, and Jan A. Kors. 2009.
A dictionary to identify small molecules and drugs in
free text. Bioinformatics, 25(22):2983?2991.
David M. Jessop, Sam E. Adams, Egon L. Willighagen,
Lezan Hawizy, and Peter Murray-Rust. 2011. OS-
CAR4: a flexible architecture for chemical text-mining.
J. Cheminf., 3(1):41.
Roman Klinger, Corinna Kola?rik, Juliane Fluck, Martin
Hofmann-Apitius, and Christoph M. Friedrich. 2008.
Detection of IUPAC and IUPAC-like chemical names.
In Proc. of ISMB. Bioinformatics, volume 24, pages
i268?i276.
Roman Klinger and Katrin Tomanek. 2007. Classical
Probabilistic Models and Conditional Random Fields.
Algorithm Engineering Report TR07-2-013. Depart-
ment of Computer Science, Dortmund University of
Technology. ISSN 1864-4503.
Corinna Kola?rik, Roman Klinger, Christoph M. Friedrich,
Martin Hofmann-Apitius, and Juliane Fluck. 2008.
Chemical names: terminological resources and corpora
annotation. In Proc. of the Workshop on Building and
evaluating resources for biomedical text mining, pages
51?58.
Frank R. Kschischang, Brendan J. Frey, and Hans-Andrea
Loeliger. 2001. Factor graphs and the sum-product
algorithm. IEEE Trans. Inf. Theory, 47(2):498?519.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields : Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proc. of ICML-2001, pages 282?289.
Robert Leaman and Graciela Gonzalez. 2008. BANNER:
an executable survey of advances in biomedical named
entity recognition. In Proc. of Pac Symp Biocomput,
pages 652?663.
Robert Leaman, Laura Wojtulewicz, Ryan Sullivan, An-
nie Skariah, Jian Yang, and Graciela Gonzalez. 2010.
Towards internet-age pharmacovigilance: extracting ad-
verse drug reactions from user posts to health-related
social networks. In Proc. of Workshop BioNLP, pages
117?125. ACL.
Daniel M. Lowe, Peter T. Corbett, Peter Murray-Rust, and
Robert C. Glen. 2011. Chemical name to structure:
Opsin, an open source solution. J. Chem. Inf. Model.,
51(3):739?753.
Andrew K. McCallum. 2002. MALLET: A
Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Andrew K. McCallum, Dayne Freitag, and Fernando.
Pereira. 2000. Maximum entropy Markov models for
information extraction and segmentation. In Proc. of
ICML-2000, pages 591?598.
Andrew K. McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via im-
peratively defined factor graphs. In Proc. of Neural
Information Processing Systems (NIPS).
Phoebe M. Roberts and William S. Hayes. 2008. Infor-
mation needs and the role of text mining in drug de-
velopment. In Proc. of Pac Symp Biocomput, pages
592?603.
Tim Rockta?schel, Michael Weidlich, and Ulf Leser. 2012.
ChemSpot: A Hybrid System for Chemical Named En-
tity Recognition. Bioinformatics, 28(12):1633?1640.
Sandeepkumar Satpal and Sunita Sarawagi. 2007. Do-
main adaptation of conditional probability models
via feature subsetting. In Knowledge Discovery in
Databases: PKDD 2007, pages 224?235. Springer.
Martijn J. Schuemie, Rob Jelier, and Jan A. Kors. 2007.
Peregrine: Lightweight gene name normalization by
dictionary lookup. In Proc. of the Second BioCreative
Challenge, volume 2, pages 131?133.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Proc. of Pac Symp Biocomput,
volume 8, pages 451?462.
Isabel Segura-Bedmar, Paloma Mart??nez, and Mar??a
Herrero-Zazo. 2013. Semeval-2013 task 9: Extraction
of drug-drug interactions from biomedical texts. In
362
Proc. of the 7th International Workshop on Semantic
Evaluation (SemEval 2013).
Isabel Segura-Bedmar, Paloma Mart??nez, and Mar??a
Segura-Bedmar. 2008. Drug name recognition and clas-
sification in biomedical texts. A case study outlining
approaches underpinning automated systems. Drug
Discovery Today, 13(17-18):816?823.
Burr Settles. 2005. ABNER: an open source tool for
automatically tagging genes, proteins and other entity
names in text. Bioinformatics, 21(14):3191?3192.
Philippe Thomas, Mariana Neves, Ille?s Solt, Domonkos
Tikk, and Ulf Leser. 2011. Relation extraction for drug-
drug interactions using ensemble learning. In Proc.
of the 1st Challenge Task on Drug-Drug Interaction
Extraction 2011, pages 11?18.
Hanna M. Wallach. 2004. Conditional Random Fields: An
Introduction. Technical Report MS-CIS-04-21. Depart-
ment of Computer and Information Science, University
of Pennsylvania.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5(2):241?259.
363
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 628?635, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
WBI-DDI: Drug-Drug Interaction Extraction using Majority Voting
Philippe Thomas Mariana Neves Tim Rockta?schel Ulf Leser
Humboldt-Universita?t zu Berlin
Knowledge Management in Bioinformatics
Unter den Linden 6
Berlin, 10099, Germany
{thomas,neves,trocktae,leser}@informatik.hu-berlin.de
Abstract
This work describes the participation of the
WBI-DDI team on the SemEval 2013 ? Task
9.2 DDI extraction challenge. The task con-
sisted of extracting interactions between pairs
of drugs from two collections of documents
(DrugBank and MEDLINE) and their clas-
sification into four subtypes: advise, effect,
mechanism, and int. We developed a two-step
approach in which pairs are initially extracted
using ensembles of up to five different clas-
sifiers and then relabeled to one of the four
categories. Our approach achieved the sec-
ond rank in the DDI competition. For interac-
tion detection we achieved F1 measures rang-
ing from 73 % to almost 76 % depending on
the run. These results are on par or even higher
than the performance estimation on the train-
ing dataset. When considering the four inter-
action subtypes we achieved an F1 measure of
60.9 %.
1 Introduction
A drug-drug interaction (DDI) can be described as
interplay between drugs taken during joint adminis-
tration. DDIs usually lead to an increase or decrease
in drug effects when compared to isolated treatment.
For instance, sildenafil (Viagra) in combination with
nitrates can cause a potentially live-threatening de-
crease in blood pressure (Cheitlin et al, 1999). It is
therefore crucial to consider potential DDI effects
when co-administering drugs to patients. As the
level of medication generally is raising all over the
world, the potential risk of unwanted side effects,
such as DDIs, is constantly increasing (Haider et al,
2007).
Only a fraction of knowledge about DDIs is
contained in specialized databases such as Drug-
Bank (Knox et al, 2011). These structured knowl-
edge bases are often the primary resource of infor-
mation for researchers. However, the majority of
new DDI findings are still initially reported in scien-
tific publications, which results in the situation that
structured knowledge bases lag behind recently pub-
lished research results. Thus, there is an urgent need
for researchers and database curators to cope with
the fast growth of biomedical literature (Hunter and
Cohen, 2006).
The SemEval 2013 ? Task 9.2 (Extraction of
Drug-Drug Interactions from BioMedical Texts)
is a competitive evaluation of methods for ex-
tracting mentions of drug-drug interactions from
texts (Segura-Bedmar et al, 2013). For training,
the organizers provide a corpus annotated with drug-
names and interactions between them. This corpus
is composed of 572 articles collected from Drug-
Bank and 142 PubMed abstracts. Interactions are
binary (always between two drugs) and undirected,
as target and agent roles are not annotated. Fur-
thermore, the two interacting drugs are always men-
tioned within the same sentence. In contrast to
the previous DDI-challenge 2011 (Segura-Bedmar
et al, 2011), four different DDI-subtypes (advise,
effect, mechanism, and int) have been introduced.
Details about the four subclasses can be found in the
task?s annotation guideline.
628
Figure 1: Workflow developed for the SemEval 2013
Task 9.2 challenge.
2 Methods
Binary relationship extraction is often tackled as a
pair-wise classification problem, where all
(n
2
)
co-
occurring entities in a sentence are classified as in-
teracting or not. To account for the four different
subtypes of DDIs, the problem definition could be
translated into a multiclass classification problem
between all co-occurring entities.
Contrary to that, we propose a two step strat-
egy: First, we detect general drug-drug interac-
tions regardless of subtype using a multitude of dif-
ferent machine-learning methods. The output of
these methods is aggregated using a majority vot-
ing approach. Second, detected interactions are re-
classified into one of the four possible DDI cate-
gories. The latter is referred to as DDI relabeling
throughout this paper. A detailed view on the pro-
posed workflow is depicted in Figure 1.
2.1 Preprocessing
Sentences have been parsed using Charniak-Johnson
PCFG reranking-parser (Charniak and Johnson,
2005) with a self-trained re-ranking model aug-
mented for biomedical texts (McClosky, 2010). Re-
sulting constituent parse trees have been converted
into dependency graphs using the Stanford con-
verter (De Marneffe et al, 2006). In the last step, we
created an augmented XML using the open source
Corpus Sentences
Pairs
Positive Negative Total
DrugBank 5,675 3,788 22,217 26,005
MEDLINE 1,301 232 1,555 1,787
Table 1: Basic statistics of the DDI training corpus shown
for DrugBank and MEDLINE separately.
framework from Tikk et al (2010). This XML file
encompasses tokens with respective part-of-speech
tags, constituent parse tree, and dependency parse
tree information. This format has been subsequently
transformed into a related XML format1 used by two
of the utilized classifiers. Properties of the training
corpus are shown for DrugBank and MEDLINE in
Table 1.
2.2 Machine Learning Methods
Tikk et al (2010) systematically analyzed nine dif-
ferent machine learning approaches for the extrac-
tion of undirected binary protein-protein interac-
tions. This framework has been successfully applied
to other domains, such as the I2B2 relation extrac-
tion challenge (Solt et al, 2010), the previous DDI
extraction challenge (Thomas et al, 2011), and to
the extraction of neuroanatomical connectivity state-
ments (French et al, 2012).
Drug entities are blinded by replacing the entity
name with a generic string to ensure the generality
of the approach. Without entity blinding drug names
are incorporated as features, which clearly affects
generalization capabilities of a classifier on unseen
entity mentions (Pyysalo et al, 2008).
We decided to use the following methods
provided by the framework: All-paths graph
(APG) (Airola et al, 2008), shallow lin-
guistic (SL) (Giuliano et al, 2006), subtree
(ST) (Vishwanathan and Smola, 2002), subset tree
(SST) (Collins and Duffy, 2001), and spectrum tree
(SpT) (Kuboyama et al, 2007) method. The SL
method uses only shallow linguistic features, i.e.,
token, stem, part-of-speech tag and morphologic
properties of the surrounding words. APG builds
a classifier using surface features and a weighting
1https://github.com/jbjorne/TEES/wiki/
Interaction-XML
629
scheme for dependency parse tree features. The
remaining three classifier (ST, SST, and SpT) build
kernel functions based on different subtree repre-
sentations on the constituent parse tree. To calculate
the constituent?tree kernels ST and SST we used
the SVM-LIGHT-TK toolkit (Moschitti, 2006).
Before applying these methods, constituent parse
trees have been reduced to the shortest-enclosed
parse following the recommendations from Zhang
et al (2006). For a more detailed description
of the different methods we refer to the original
publications.
In addition to the PPI framework, we also
employed the general purpose relationship ex-
traction tool ?Turku Event Extraction System?
(TEES) (Bjo?rne et al, 2011), a customized version
of the case-based reasoning system Moara (Neves
et al, 2009), and a self-developed feature based
classifier which is referred to as SLW. Regarding
TEES, we have used the edge extraction function-
ality for performing relationship extraction. TEES
considers features related to the tokens (e.g., part-of-
speech tags), dependency chains, dependency path
N-grams, entities (e.g., entity types) and external re-
sources, such as hypernyms in WordNet.
Moara is a case-based reasoning system for the
extraction of relationships and events. During train-
ing, interaction pairs are converted into cases and
saved into a HyperSQL database which are re-
trieved through case similarity during the classifica-
tion. Cases are composed by the following features:
the type of the entities (e.g. Brand and Group),
the part-of-speech tag of the tokens between the two
drugs (inclusive), the tags of the shortest depen-
dency path between the two drugs, and the lemma
of the non-entity tokens of the shortest dependency
path using BioLemmatizer (Liu et al, 2012). We
also consider the PHARE ontology (Coulet et al,
2011) in the lemma feature: When a lemma matches
any of the synonyms contained in this ontology, the
category of the respective term is considered instead.
Case similarity is calculated by exact feature match-
ing, except for the part-of-speech tags whose com-
parison is based on global alignment using insertion,
deletion, and substitution costs as proposed by Spa-
sic et al (2005).
SLW is inspired by SL (Giuliano et al, 2006;
Bunescu and Mooney, 2006) and uses the Breeze2
library. We generate n-grams over sequences of
arbitrary features (e.g. POS-tags, morphological
and syntactical features) to describe the global con-
text of an entity pair. Furthermore, we calculate
features from the local context of entities, but in
addition to SL, we include domain-specific fea-
tures used for identifying and classifying pharma-
cological substances (see our paper for DDI Task
9.1 (Rockta?schel et al, 2013)). In addition, we take
the name of the classes of a pair?s two entities as
feature to capture that entities of some class (e.g.
Brand and Group) are more likely to interact than
others (e.g. Brand and Brand).
2.3 Ensemble learning
Several community competitions previously noted
that combinations of predictions from different tools
help to achieve better results than one method
alone (Kim et al, 2009; Leitner et al, 2010). More
importantly, it is well known that ensembles increase
robustness by decreasing the risk of selecting a bad
classifier (Polikar, 2006). In this work we combined
the output of several classifiers by using majority
voting. The ensemble is used to predict DDIs re-
gardless of the four different subtypes. This com-
plies with the partial match evaluation criterion de-
fined by the competition organizers.
2.4 Relabeling
To account for DDI subtypes, we compared two ap-
proaches: (a) using the subtype prediction of TEES;
(b) training a multi-class classifier (SLW) on the
available training data for DDI subtypes. We de-
cided on using TEES, as it generated superior results
over SLW (data not shown). Thus, previously identi-
fied DDIs are relabeled into one of the four possible
subtypes using the most likely interaction subtype
from TEES.
3 Results
3.1 Cross validation
In order to compare the different approaches, we
performed document-wise 10-fold cross validation
(CV) on the training set. It has been shown that such
2http://www.scalanlp.org/
630
Type Pairs Precision Recall F1
total 3,119 78.6 78.6 78.6
effect 1,633 79.8 79.1 79.4
mechanism 1,319 79.8 79.2 79.4
advise 826 77.3 76.4 76.9
int 188 68.5 80.9 74.1
Table 4: Performance estimation for relabeling DDIs.
Pairs denotes the number of instances of this type in the
training corpus.
a setting provides more realistic performance esti-
mates than instance-wise CV (S?tre et al, 2008).
All approaches have been tested using the same
splits to ensure comparability. For APG, ST, SST,
and SpT we followed the parameter optimization
strategy defined by Tikk et al (2010). For TEES
and Moara, we used the cost parameter C (50000)
and best performing features, respectively, based on
the CV results. For SL and SLW, we used the default
parameters.
We performed several different CV experiments:
First, we performed CV on the two corpora (Drug-
Bank and MEDLINE) separately. Second, data
from the other corpus has been additionally used
during the training phase. This allows us to esti-
mate the impact of additional, but potentially differ-
ent text. CV results for DrugBank and MEDLINE
are shown in Table 2 and 3 respectively.
3.2 Relabeling
Performance of relabeling is evaluated by perform-
ing 10-fold CV on the training set using the same
splits as in previous analysis. Note that this experi-
ment is solely performed on positive DDI instances
to estimate separability of the four different DDI-
subtypes. Results for relabeling are shown in Ta-
ble 4.
3.3 Test dataset
For the test set we submitted results using the fol-
lowing three majority voting ensembles. For Run 1
we used Moara+SL+TEES, for Run 2 we used
APG+Moara+SL+SLW+TEES and for Run 3 we
used SL+SLW+TEES. Due to time constraints we
did not use different ensembles for the two corpora.
We rather decided to use ensembles which achieved
generally good results for both training corpora. All
classifiers, except APG, have been retrained on the
combination of MEDLINE and DrugBank using
the parameter setting yielding the highest F1 in the
training phase. For APG, we trained two different
models: One model is trained on MEDLINE and
DrugBank and one model is trained on DrugBank
solely. The first model is applied on the MEDLINE
test set and the latter on the DrugBank test set. Esti-
mated results on the training corpus and official re-
sults on the test corpus are shown in Table 5.
4 Discussion
4.1 Training dataset
Document-wise CV results for the DrugBank corpus
show no clear effect when using MEDLINE as ad-
ditional training data. By using MEDLINE during
the training phase we observe an average decrease of
0.3 percentage points (pp) in F1 and an average in-
crease of 0.7 pp in area under the receiver operating
characteristic curve (AUC). The strongest impact
can be observed for APG with a decrease of 2.3 pp
in F1. We therefore decided to train APG mod-
els for DrugBank without additional MEDLINE
data. For almost all ensembles (with the excep-
tion of APG+SpT+SL) we observe superior results
when using only DrugBank as training data. Inter-
estingly, this effect can mostly be attributed to an
average increase of 3.3 pp in recall, whereas preci-
sion remains fairly stable between ensembles using
DrugBank solely and those with additional training
data.
In contrast for MEDLINE, all methods largely
benefit from additional training data with an aver-
age increase of 9.8 pp and 3.6 pp for F1 and AUC re-
spectively. For the ensemble based approaches, we
observe an average increase of 13.8 pp for F1when
using DrugBank data in addition.
When ranking the different methods by F1 and
calculating correlation between the two differ-
ent corpora, we observe only a weak correlation
(Kendall?s ? = 0.286, p< 1). In other words, ma-
chine learning methods show varying performance-
ranks between the two corpora. This difference is
most pronounced for SL and SpT, with four ranks
difference between DrugBank and MEDLINE. It
is noteworthy that the two corpora are not directly
631
Regular CV Combined CV
Method P R F1 AUC P R F1 AUC
SL 61.5 79.0 69.1 92.8 62.1 78.4 69.2 93.0
APG 77.2 62.6 69.0 91.5 75.9 59.8 66.7 91.6
TEES 77.2 62.0 68.6 87.3 75.5 60.9 67.3 86.9
SLW 73.7 60.0 65.9 91.3 73.4 61.2 66.6 91.3
Moara 72.1 55.2 62.5 ? 72.0 54.7 62.1 ?
SpT 51.4 73.4 60.3 87.3 52.7 71.4 60.6 87.7
SST 51.9 61.2 56.0 85.4 55.1 57.1 56.0 86.1
ST 47.3 64.2 54.2 82.3 48.3 64.3 54.9 82.7
SL+SLW+TEES 76.1 69.9 72.7 ? 75.9 65.3 70.1 ?
APG+SL+TEES 79.3 69.9 74.2 ? 79.2 65.4 71.5 ?
Moara+SL+TEES 79.9 69.6 74.2 ? 79.6 65.1 71.6 ?
Moara+SL+APG 81.4 70.6 75.5 ? 81.3 70.3 75.3 ?
APG+Moara+SL+SLW+TEES 84.0 68.1 75.1 ? 83.7 64.2 72.6 ?
APG+SpT+TEES 76.8 68.0 72.1 ? 77.1 63.4 69.6 ?
APG+SpT+SL 68.7 74.8 71.5 ? 69.7 73.8 71.6 ?
Table 2: Cross validation results on DrugBank corpus. Regular CV is training and evaluation on DrugBank only.
Combined CV is training on DrugBank and MEDLINE and testing on DrugBank. Higher F1 between these two
settings are indicated in boldface for each method. Single methods are ranked by F1.
Regular CV Combined CV
Method P R F1 AUC P R F1 AUC
TEES 70.7 36.0 44.5 82.2 59.6 46.5 51.4 84.9
SpT 37.8 38.6 34.6 78.6 42.3 55.3 47.1 80.4
APG 46.5 44.3 42.4 82.3 38.1 62.2 46.4 82.8
SST 31.3 37.7 31.8 74.1 36.7 61.7 44.9 79.5
SL 43.7 40.1 38.7 78.9 34.7 67.1 44.7 81.1
SLW 58.0 14.3 20.4 73.4 50.1 38.0 42.0 82.4
Moara 49.8 31.9 37.6 ? 45.6 43.2 41.9 ?
ST 25.2 43.8 30.1 70.5 36.1 48.3 39.8 74.2
SL+SLW+TEES 73.6 29.0 37.6 ? 55.2 52.7 53.1 ?
APG+SL+TEES 60.7 37.9 43.4 ? 49.9 62.4 54.3 ?
Moara+SL+TEES 68.0 33.0 42.2 ? 62.1 55.5 57.4 ?
Moara+SL+APG 57.7 36.7 42.4 ? 48.3 60.9 52.8 ?
APG+Moara+SL+SLW+TEES 73.3 28.3 36.8 ? 60.6 54.4 56.5 ?
APG+SpT+TEES 58.5 37.4 41.7 ? 57.5 59.2 57.1 ?
APG+SpT+SL 48.3 39.9 40.0 ? 43.6 64.3 51.0 ?
Table 3: Cross validation results on MEDLINE corpus. Regular CV is training and evaluation on MEDLINE only.
Combined CV is training on DrugBank and MEDLINE and testing on MEDLINE. Higher F1 between these two
settings are indicated in boldface for each method. Single methods are ranked by F1.
632
Evaluation
Training Test
Run 1 Run 2 Run 3 Run 1 Run 2 Run 3
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
Partial 78.7 67.3 72.6 82.9 66.4 73.7 75.2 67.6 71.2 84.1 65.4 73.6 86.1 65.7 74.5 80.1 72.2 75.9
Strict 65.7 56.1 60.5 70.0 56.0 62.2 63.0 56.7 59.7 68.5 53.2 59.9 69.5 53.0 60.1 64.2 57.9 60.9
-mechanism 61.8 49.7 55.1 68.1 50.0 57.7 59.2 50.3 54.4 72.2 51.7 60.2 74.9 52.3 61.6 65.3 58.6 61.8
-effect 68.8 57.9 62.9 71.8 57.6 63.9 66.1 57.4 61.5 63.7 57.5 60.4 63.6 55.8 59.5 60.7 61.4 61.0
-advise 64.6 60.5 62.5 68.2 59.7 63.6 61.1 61.5 61.3 73.3 53.4 61.8 74.5 55.7 63.7 69.0 58.4 63.2
-int 68.6 50.0 57.8 75.4 52.1 61.6 70.9 56.9 63.1 67.8 41.7 51.6 67.3 38.5 49.0 67.8 41.7 51.6
Table 5: Relation extraction results on the training and test set. Run 1 builds a majority voting on Moara+SL+TEES,
Run 2 on APG+Moara+SL+SLW+TEES, and Run 3 on SL+SLW+TEES. Partial characterizes only DDI detection
without classification of subtypes, whereas strict requires correct identification of subtypes as well.
comparable, as DrugBank is one order of magnitude
larger in terms of instances than the MEDLINE cor-
pus. Additionally, documents come from different
sources and it is tempting to speculate that there
might be a certain amount of domain specificity be-
tween DrugBank and MEDLINE sentences.
We tested for domain specificity by performing
cross-corpus experiments, i.e., we trained a classi-
fier on DrugBank, applied it on MEDLINE and vice
versa. When training on MEDLINE and testing
on DrugBank, we observe an average decrease of
about 15 pp in F1 in comparison to DrugBank in-
domain CV results. For the other setting, we observe
a lower decrease of approximately 5 pp in compari-
son to MEDLINE in-domain CV results.
From the current results, it seems that the doc-
uments from DrugBank and MEDLINE have dif-
ferent syntactic properties. However, this requires a
more detailed analysis of different aspects like dis-
tribution of sentence length, negations, or passives
between the two corpora (Cohen et al, 2010; Tikk
et al, 2013). We assume that transfer learning tech-
niques could improve results on both corpora (Pan
and Yang, 2010).
The DDI-relabeling capability of TEES is very
balanced with F1 measures ranging from 74.1 % to
79.4 % for all four DDI subclasses. This is unex-
pected since classes like ?effect? occur almost ten
times more often than classes like ?int? and classi-
fiers often have problems with predicting minority
classes.
4.2 Test dataset
On the test set, our best run achieves an F1 of 76 %
using the partial evaluation schema. This is slightly
better than the performance for DrugBank training
data shown in Table 2 and substantially better than
estimations for MEDLINE (see Table 3). With
F1 measures ranging between 74 % to 76 % only
minor performance differences can be observed be-
tween the three different ensembles.
When switching from partial to strict evaluation
scheme an average decrease of 15 pp in F1 can be ob-
served. As estimated on the training data, relabeling
performance is indeed very similar for the different
DDI-subtypes. Only for the class with the least in-
stances (int), a larger decrease in comparison to the
other three classes can be observed for the test set.
In general, results for test set are on par or higher
than results for the training set.
5 Conclusion
In this paper we presented our approach for the
SemEval 2013 ? Task 9.2 DDI extraction challenge.
Our strategy builds on a cascaded (coarse to fine
grained) classification strategy, where a majority
voting ensemble of different methods is initially
used to find generic DDIs. Predicted interactions
are subsequently relabeled into four different sub-
types. DDI extraction seems to be a more difficult
task for MEDLINE abstracts than for DrugBank ar-
ticles. In our opinion, this cannot be fully attributed
to the slightly higher ratio of positive instances in
DrugBank and points towards structural differences
between the two corpora.
Acknowledgments
This work was supported by the German Research
Foundation (DFG) [LE 1428/3-1] and the Federal
633
Ministry of Economics and Technology (BMWi)
[KF 2205209MS2].
References
A. Airola, S. Pyysalo, J. Bjo?rne, T. Pahikkala,
F. Ginter, and T. Salakoski. 2008. All-paths graph
kernel for protein-protein interaction extraction
with evaluation of cross-corpus learning. BMC
Bioinformatics, 9 Suppl 11:S2.
J. Bjo?rne, J. Heimonen, F. Ginter, A. Airola,
T. Pahikkala, and T. Salakoski. 2011. Extracting
Contextualized Complex Biological Events with
Rich Graph-Based Features Sets. Computational
Intelligence, 27(4):541?557.
R. C. Bunescu and R. J. Mooney. 2006. Sub-
sequence Kernels for Relation Extraction. Ad-
vances in Neural Information Processing Sys-
tems, 18:171.
E. Charniak and M. Johnson. 2005. Coarse-to-
Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proc. of ACL?05, pages 173?180.
M. D. Cheitlin, A. M. Hutter, R. G. Brindis, P. Ganz,
S. Kaul, R. O. Russell, and R. M. Zusman. 1999.
Use of sildenafil (viagra) in patients with cardio-
vascular disease. J Am Coll Cardiol, 33(1):273?
282.
K. Cohen, Helen L Johnson, Karin Verspoor,
Christophe Roeder, and Lawrence E Hunter.
2010. The structural and content aspects of ab-
stracts versus bodies of full text journal articles
are different. BMC Bioinformatics, 11:492.
M. Collins and N. Duffy. 2001. Convolution Kernels
for Natural Language. In Proc. of NIPS?01, pages
625?632.
A. Coulet, Y. Garten, M. Dumontier, R. Altman,
M. Musen, and N. Shah. 2011. Integration and
publication of heterogeneous text-mined relation-
ships on the semantic web. Journal of Biomedical
Semantics, 2(Suppl 2):S10.
M.C. De Marneffe, B. MacCartney, and C.D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In Proc. of LREC
2006, pages 449?454.
L. French, S. Lane, L. Xu, C. Siu, C. Kwok, Y. Chen,
C. Krebs, and P. Pavlidis. 2012. Application and
evaluation of automated methods to extract neu-
roanatomical connectivity statements from free
text. Bioinformatics, 28(22):2963?2970.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Ex-
ploiting Shallow Linguistic Information for Re-
lation Extraction from Biomedical Literature. In
Proc. of EACL?06, pages 401?408.
S. I. Haider, K. Johnell, M. Thorslund, and J. Fast-
bom. 2007. Trends in polypharmacy and potential
drug-drug interactions across educational groups
in elderly patients in Sweden for the period 1992
- 2002. Int J Clin Pharmacol Ther, 45(12):643?
653.
L. Hunter and K. Cohen. 2006. Biomedical language
processing: what?s beyond PubMed? Mol Cell,
21(5):589?594.
J.D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-
jii. 2009. Overview of BioNLP?09 shared task on
event extraction. In Proc. of BioNLP?09, pages
1?9.
C. Knox, V. Law, T. Jewison, P. Liu, S. Ly, A. Frol-
kis, A. Pon, K. Banco, C. Mak, V. Neveu,
Y. Djoumbou, R. Eisner, A. Chi Guo, and D. S
Wishart. 2011. Drugbank 3.0: a comprehensive
resource for ?omics? research on drugs. Nucleic
Acids Res, 39(Database issue):D1035?D1041.
T. Kuboyama, K. Hirata, H. Kashima, K. F. Aoki-
Kinoshita, and H. Yasuda. 2007. A Spectrum
Tree Kernel. Information and Media Technolo-
gies, 2(1):292?299.
F. Leitner, S.A. Mardis, M. Krallinger, G. Ce-
sareni, L.A. Hirschman, and A. Valencia. 2010.
An overview of BioCreative II. 5. IEEE
IEEE/ACM Transactions on Computational Biol-
ogy and Bioinformatics, pages 385?399.
H. Liu, T. Christiansen, W. Baumgartner, and
K. Verspoor. 2012. Biolemmatizer: a lemmatiza-
tion tool for morphological processing of biomed-
ical text. Journal of Biomedical Semantics, 3(1):3.
D. McClosky. 2010. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Brown University.
A. Moschitti. 2006. Efficient Convolution Kernels
for Dependency and Constituent Syntactic Trees.
In Proc. of ECML?06, pages 318?329.
M. Neves, J.-M. Carazo, and A. Pascual-Montano.
2009. Extraction of biomedical events using case-
based reasoning. In Proc. of BioNLP?09, pages
68?76.
S. J. Pan and Q. Yang. 2010. A Survey on Transfer
634
Learning. IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345?1359.
R. Polikar. 2006. Ensemble Based Systems in Deci-
sion Making. IEEE Circuits and Systems Maga-
zine, 6(3):21?45.
S. Pyysalo, R. S?tre, J. Tsujii, and T. Salakoski.
2008. Why Biomedical Relation Extraction Re-
sults are Incomparable and What to do about it.
In Proc. of SMBM?08, pages 149?152.
T. Rockta?schel, T. Huber, M. Weidlich, and U. Leser.
2013. WBI-NER: The impact of domain-specific
features on the performance of identifying and
classifying mentions of drugs. In Proceedings of
the 7th International Workshop on Semantic Eval-
uation (SemEval 2013).
R. S?tre, K. Sagae, and J. Tsujii. 2008. Syntactic
features for protein-protein interaction extraction.
In Proc. of LBM?07.
I. Segura-Bedmar, P. Mart??nez, and M. Herrero-
Zazo. 2013. Semeval-2013 task 9: Extraction of
drug-drug interactions from biomedical texts. In
Proc. of the 7th International Workshop on Se-
mantic Evaluation (SemEval 2013).
I. Segura-Bedmar, P. Mart??nez, and D. Sanchez-
Cisneros. 2011. The 1st ddiextraction-2011 chal-
lenge task: Extraction of drug-drug interactions
from biomedical text. In Proc. of the 1st Chal-
lenge Task on Drug-Drug Interaction Extraction
2011, pages 1?9.
I. Solt, F. P. Szidarovszky, and D. Tikk. 2010. Con-
cept, Assertion and Relation Extraction at the
2010 i2b2 Relation Extraction Challenge using
parsing information and dictionaries. In Proc. of
i2b2/VA Shared-Task.
I. Spasic, S. Ananiadou, and J. Tsujii. 2005. MaS-
TerClass: a case-based reasoning system for the
classification of biomedical terms. Bioinformat-
ics, 21(11):2748?2758.
P. Thomas, M. Neves, I. Solt, D. Tikk, and U. Leser.
2011. Relation extraction for drug-drug interac-
tions using ensemble learning. In Proc. of the
1st Challenge Task on Drug-Drug Interaction Ex-
traction 2011, pages 11?18.
D. Tikk, I. Solt, P. Thomas, and U. Leser. 2013.
A detailed error analysis of 13 kernel methods
for protein-protein interaction extraction. BMC
Bioinformatics, 14(1):12.
D. Tikk, P. Thomas, P. Palaga, J. Hakenberg, and
U. Leser. 2010. A comprehensive benchmark of
kernel methods to extract protein-protein interac-
tions from literature. PLoS Comput Biol, 6.
S. V. N. Vishwanathan and A. J. Smola. 2002. Fast
Kernels for String and Tree Matching. In Proc. of
NIPS?02, pages 569?576.
M. Zhang, J. Zhang, J. Su, and G. Zhou. 2006. A
Composite Kernel to Extract Relations between
Entities with Both Flat and Structured Features.
In Proc. of ICML?06, pages 825?832.
635
