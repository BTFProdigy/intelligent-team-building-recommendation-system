CRFs-Based Named Entity Recognition Incorporated with Heuristic 
Entity List Searching 
Fan Yang 
National Laboratory of 
Pattern Recognition 
Institute of Automation, 
Chinese Academy of 
Sciences  
fyang@nlpr.ia.ac.cn 
Jun Zhao 
National Laboratory of 
Pattern Recognition 
Institute of Automation, 
Chinese Academy of 
Sciences  
jzhao@nlpr.ia.ac.cn
Bo Zou 
National Laboratory of 
Pattern Recognition 
Institute of Automation, 
Chinese Academy of 
Sciences  
bzou@nlpr.ia.ac.cn
 
 
Abstract 
Chinese Named entity recognition is one of 
the most important tasks in NLP. Two 
kinds of Challenges we confront are how to 
improve the performance in one corpus and 
keep its performance in another different 
corpus. We use a combination of statistical 
models, i.e. a language model to recognize 
person names and two CRFs models to 
recognize Location names and 
Organization names respectively. We also 
incorporate an efficient heuristic named 
entity list searching process into the 
framework of statistical model in order to 
improve both the performance and the 
adaptability of the statistical NER system. 
We participate in the NER tests on open 
tracks of MSRA. The testing results show 
that our system can performs well. 
1 Introduction 
Named Entity Recognition (NER) is one of the 
most important tasks in NLP, and acts as a critical 
role in some language processing applications, 
such as Information Extraction and Integration, 
Text Classification etc. Many efforts have been 
paid to improve the performance of NER. 
NER task in Chinese has some differences from 
in English as follows. 1) There is no space between 
Chinese characters, which make boundary 
recognition more difficult. 2) In English, a 
capitalized letter at the beginning position of a 
word implies that the word is a part of a named 
entity. However, this kind of characteristic does 
not exist in Chinese. 
In the paper, we will focus on two kinds of 
problems. 1) How to improve the performance of 
Chinese NER in one corpus, which contains 
boosting precision rate, recall rate and F-measure 
rate. 2) How to enhance the  adaptability of a 
Chinese NER system, which means that a system 
can get a good performance on a testing set which 
has many differences from the training set. To 
solve the first problem, we should select a good 
model and adjust parameters carefully. But there is 
no framework that can solve the second problem 
completely.  
Our goal is to find a way to solve these two 
problems. We select a language model to recognize 
Person names, and two CRFs models are used to 
recognize Location and Organization separately.  
We also try to incorporate a large-scale named 
entity list into the statistical model, where a 
heuristic searching method is developed to match 
the entities in the list quickly and efficiently. 
2 Framework of NER System 
The Input of the system is a raw text. We will 
apply some pre-processing such as code 
transformation. Then the heuristic searching will 
be executed to find the appearance of the entities in 
the named entity list. After that, two CRFs that 
have been trained before will be used to recognize 
Location and Organization based on the result of 
word segmentation, and a language model will be 
used to find Person names. All the results will be 
integrated at last. 
171
Sixth SIGHAN Workshop on Chinese Language Processing
 
Figure 1.  System Frameworks 
3 System Details 
3.1 Using heuristic method to search entity 
list 
NER task meets many difficulties which come 
from the complexity of the construction of named 
entities. Named entities have flexible internal 
styles and external environments. Building a good 
model to describe the condition precisely will have 
many troubles. The statistical models we common-
ly used have some shortcomings, especially when 
they are adapted to a corpus of new domains or 
styles. We try to use an improved searching 
method to make up the relatively poor adaptability 
of the statistical models. The heuristic searching 
method is more flexible especially in the following 
two aspects. 1) Abbreviation can be matched. 2) 
Suffix in location and organization is universal, but 
it should not be taken in count when we search 
named entities. 
The framework of the algorithm can be briefly 
described as follows: 
 
1) Building an inverse index using Chinese characters 
as key term; 
2) Using the text as a query to search for entities; 
3) When comes terminal condition, a heuristic 
function is invoked to determine whether the 
character sequence is an entity; 
4) When comes creation condition, a heuristic 
function is invoked to judge whether a new entity is 
created; 
5) The labeled sequence is output. 
Table 1. Heuristic Searching Method 
One advantage of heuristic searching method is 
that the heuristic function can be set to fit a special 
corpus. The heuristic searching method we used in 
Bakeoff-4 is as follows:  
Un-segmenting test 
Person recognition 
Heuristic searching 
Location 
recognition 
Organization 
recognition 
Word segmentation 
Output results 
z Ignoring the suffix key word in Location and 
Organization names. For example 
??? ??/Tongfang /Corporation? and 
???/Tongfang? will get same score under 
this heuristic rule. 
z Ignoring the Location name as a prefix in an 
Organization name. For example, 
???/Ameri ??can /General Motors ? and 
???/General Motors? will get same score 
under this heuristic rule 
z Taking Abbreviation rules in consideration. 
For example??? ??/Peking /University? 
can be abbreviated as ?? ?/Bei /Da? rather 
than ???/Peking? or ???/University? 
Heuristic searching method also has such advan-
tages as follows: 
It is easy to be expanded to a corpus of new 
domain or style. We only need to add the entities 
in the new domain into list 
Searching method will improve the recall 
performance remarkably 
But the precision will be reduced for the ambi-
guities, i.e. whether a sequence that matches an 
entity in the list really constructs an entity in the 
text. We will disambiguate it using statistical 
models. 
3.2 Conditional Random Fields Model 
Conditional Random Fields (CRFs) is an 
undirected graphical model that encodes a 
conditional probability distribution using a given 
set of features. Currently it is widely used as a 
discriminate model for sequence labeling: 
1 2
1
1
( | ) exp( ( , , ))
k
i i c c
c C i
P Y X f y y X
Z
?
? =
= ??     (1) 
CRFs is considered to be a very effective model 
to resolve the issue of sequence labeling for the 
following characteristics: 
Because it uses a non-greedy whole sentence 
joint labeling method, high accuracy rate can be 
guaranteed and bias labeling can be avoided. 
Any types of features can be integrated in the 
model flexibly. 
Over-fitting can be avoided to some extent by 
integrating a priori with training data. 
172
Sixth SIGHAN Workshop on Chinese Language Processing
As a discriminate model, CRFs inherits the 
advantages of both Hidden Markov Model (HMM) 
and Maximum Entropy Markov Model (MEMM) 
as well. 
3.3 Person names recognize 
We use language model to recognize Personal 
names. We use character-based rather than word-
based model to avoid the word segmentation errors. 
We construct a context model and an entity model 
to respectively describe external and internal 
features of Personal names. The details of the 
model are as follows: 
We use a tri-gram model as the context model: 
( ) ( )? --?
m
1i
1i2ii wcwc|wcPWCP
=
               (2) 
Entity model: 
( )
( ) ( )( ) ( )( )
1 1
1 1 1
2
1 2 1
1
1
2
| |
| | , | ,
i ik i ik
i il iki l i k
k
wc wc i wc wc k k
k
wc wc l wc wc k wc
l
P w w wc P w w B M M E
P w B P w M w P w E w
? ?
?
?
?
=
? ?? ?= ??
? ? ??

" " " ?? (3) 
Where B means the beginning of the entity, M 
means middle, E means end. 
Some expert knowledge is employed to assist 
the recognition process of language model. 
z A Chinese family name list (476) and a 
Japanese family name list (9189) are used to 
restrict and select the generated candidates. 
z A list of commonly used character in Russian 
and European name. 
z Constrain of name length: A Chinese name 
cannot contain more than 8 characters. 
3.4 Location names recognition 
Location names have some composition characters. 
1) There may be some key words as suffix, such as: 
??/Shi, ?/Zheng, ?/Hu, ?/Shan? etc. 2) Other 
parts of Location names are always OOV words, 
such as ????/Dagang Village, ???/Fuzi 
Temple?.  So the right boundary of Location can 
be determined easily. The mainly problem in 
Location recognition is on abbreviation, such as 
?????/JinJiLuYu? is the combination of four 
location abbreviations. In our system, CRFs model 
can be supported by the heuristic searching method 
because it can match the abbreviation of entity in 
list. Using the searching method can boost the 
recall rate of location recognition significantly. We 
construct the recognition model based on the word-
segmented texts. 
To construct a CRFs model, we select the 
following features: 
z A list of key word suffix is used to trigger the 
recognition processing. 
z Using a list of indication words to restrict the 
boundary. 
z Heuristic searching method is used to assist 
Location recognition. 
The features we used in CRFs model is followed: 
 
W0 Current Word 
W-1?W-2 , W1?W2 Two words before and 
behind 
W-1W0?W0W1 Bi-gram Features 
POS0 POS tag 
PRE-1?PRE0?PRE1 Pre-Position reference 
words 
SRE-1?SRE0?SRE1 Suf-Position reference 
words 
Key Has Key suffix 
DIC In Dictionary 
Table 2. Features used in Location recognition 
Statement: 
The indication words used in Location 
recognition include ?for-indicate? and ?back-
indicate? words, where ?for-indicate? denotes the 
indicating words that occur as the left neighbor of 
the candidate Location named entity, while ?back-
indicate? denotes the indicating words that occur 
as the right neighbor. ?for-indicate? and ?back-
indicate? words are got from the training corpus. 
We calculate the mutual information between 
neighbor words and location entity, and get the top 
N words as indication words. 
( , )
( , ) ( , ) log
( ) ( )
p x y
MI x y p x y
p x p y
=     (4) 
We select 1216 for-indicate words and 1227 
back-indicate words. We also get 607 key words as 
location name suffix. 
3.5 Organization names recognition 
Organization name recognition is the most difficult 
part in NER task. The difficulties are as follows. 1) 
The composition of Organization name is very 
complex. For example: ??? ??/Dalian /Shide 
??/Group?, the first words in the entity is a 
location name. The second is a phonetic name 
173
Sixth SIGHAN Workshop on Chinese Language Processing
which is also an OOV word and the last one is a 
key word as suffix.2) The boundary of organizat-
ion name is hard to be classified, and the length of 
organization names is dynamitic. 3) Organization 
names are easily confused as Location names. We 
must use contextual information to determine its 
type. 4) To recognize the abbreviation of an 
organization is also a difficult task. So we choose 
the following features to solve the above problems. 
z A list of key word suffix is used to trigger the 
recognition processing. 
z Using indication words to define the boundary 
of organization. 
z Heuristic searching method is used to assist 
Location recognition. 
The features we used in the CRFs model are the 
same as used in Location model. We use the 
mutual information to select 513 for-indicate 
words and 1195 back-indicate words from training 
corpus. The number of key suffix words is 3129. 
4 Experiments 
We participate in the SigHAN Microsoft Research 
Asia (MSRA) corpus in open track. The table 3 is 
the official result of NER by our system. 
 
 R P F 
Person 0.9657 0.9574 0.9615 
Location 0.9593 0.9769 0.968 
Organization 0.8778 0.9338 0.9049 
overall 0.9377 0.9603 0.9489 
Table 3.  SigHAN MSRA corpus test results 
The training corpora we used comes from 1) 
1998 People?s Daily corpus; 2) the training corpus 
supplied by MSRA for SigHAN bakeoff 4. These 
two corpora have many difference and we focus on 
how to get a good performance both on training 
corpus and testing corpus. We select some general 
features and get assistance from the heuristic 
searching method. A good list is very important, 
which has been proved by the experimental data. 
We collect nearly 1 million personal names, 40 
thousand location names and more than 300,000 
organization names. 
5 Conclusion 
In the paper, we give a presentation to our Chinese 
Named Entity Recognition System. It uses a 
language mode to recognize personal names, and 
two CRFs models to find Location and 
organization separately. We also have a flexible 
heuristic searching method to match entity in 
named entity list with text characters sequence. 
Our system achieves a good result in the open 
NER track of MSRA corpus.  
Acknowledgement 
The work is supported by the National High 
Technology Development 863 Program of China 
under Grants no. 2006AA01Z144, the National 
Natural Science Foundation of China under Grants 
No. 60673042, the Natural Science Foundation of 
Beijing under Grants no. 4052027, 4073043. 
References 
J. Lafferty, A. McCallum, and F. Pereira. 2001. 
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data, In Proc. 
ICML 2001.  
F?Sha, F. Pereira. Shallow parsing with conditional 
random  fields. In Proc. NAACL 2003 
HP Zhang, HK Yu, DY Xiong, Q Liu and Liu Qun. 
HHMM-based Chinese Lexical Analyzer ICTCLAS. 
In Proc. Second of SIGHAN Workshop on Chinese 
Language Processing 2003. 
Youzheng Wu, Jun Zhao, Bo Xu. Chinese Named Entity 
Recognition Model Based on Multiple Features. In 
Proceedings of HLT/EMNLP 2005 
Youzheng Wu, Jun Zhao, Bo Xu. Chinese Named Entity 
Recognition Combining a Statistical Model with 
Human Knowledge. In Proceedings of ACL2003 
Workshop on Multilingual and Mixed-language 
Named Entity Recognition 
 
174
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of ACL-08: HLT, pages 541?549,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
 
Chinese-English Backward Transliteration Assisted with Mining Mono-lingual Web Pages    Fan Yang, Jun Zhao, Bo Zou, Kang Liu, Feifan Liu  National Laboratory of Pattern Recognition   Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China {fyang,jzhao,bzou,kliu,ffliu}@nlpr.ia.ac.cn     
Abstract 
In this paper, we present a novel backward transliteration approach which can further as-sist the existing statistical model by mining monolingual web resources. Firstly, we em-ploy the syllable-based search to revise the transliteration candidates from the statistical model. By mapping all of them into existing words, we can filter or correct some pseudo candidates and improve the overall recall. Secondly, an AdaBoost model is used to re-rank the revised candidates based on the in-formation extracted from monolingual web pages. To get a better precision during the re-ranking process, a variety of web-based in-formation is exploited to adjust the ranking score, so that some candidates which are less possible to be transliteration names will be as-signed with lower ranks. The experimental re-sults show that the proposed framework can significantly outperform the baseline translit-eration system in both precision and recall. 1 Introduction* The task of Name Entity (NE) translation is to translate a name entity from source language to target language, which plays an important role in machine translation and cross-language informa-tion retrieval (CLIR). Transliteration is a subtask in NE translation, which translates NEs based on the phonetic similarity. In NE translation, most person names are transliterated, and some parts of location names or organization names also need to be trans-literated. Transliteration has two directions: for-ward transliteration which transforms an original name into target language, and backward translit-eration which recovers a name back to its original expression. For instance, the original English per-                                                           *Contact: Jun ZHAO, jzhao@nlpr.ia.ac.cn.  
son name ?Clinton? can be forward transliterated to its Chinese expression ??/ke ?/lin?/dun? and the backward transliteration is the inverse process-ing. In this paper, we focus on backward translit-eration from Chinese to English. Many previous researches have tried to build a transliteration model using statistical approach [Knight and Graehl, 1998; Lin and Chen, 2002; Virga and Khudanpur, 2003; Gao, 2004]. There are two main challenges in statistical backward trans-literation: First, statistical transliteration approach selects the most probable translations based on the knowledge learned from the training data. This approach, however, does not work well when there are multiple standards [Gao, 2004]. Second, back-ward transliteration is more challenging than for-ward transliteration as it is required to disambiguate the noises introduced in the forward transliteration and estimate the original name as close as possible [Lin and Chen, 2002]. One of the most important causes in introducing noises is that: some silent syllables in original names have been missing when they are transliterated to target lan-guage. For example, when ?Campbell? is translit-erated into ??/kan?/bei?/er?, the ?p? is missing.  In order to make up the disadvantages of statisti-cal approach, some researchers have been seeking for the assistance of web resource. [Wang et al, 2004; Cheng et al, 2004; Nagata et al, 2001; Zhang et al 2005] used bilingual web pages to ex-tract translation pairs. Other efforts have been made to combine a statistical transliteration model with web mining [Al-Onaizan and Knight, 2002; Long Jiang et al 2007]. Most of these methods need bilingual resources. However, those kinds of resources are not readily available in many cases. Moreover, to search for bilingual pages, we have to depend on the performance of search engines. We can?t get Chinese-English bilingual pages when the input is a Chinese query. Therefore, the existing 
541
 
assistance approaches using web-mining to assist transliteration are not suitable for Chinese to Eng-lish backward transliteration. Thus in this paper, we mainly focus on the fol-lowing two problems to be solved in transliteration. Problem I: Some silent syllables are missing in English-Chinese forward transliteration. How to recover them effectively and efficiently in back-ward transliteration is still an open problem. Problem II: Statistical transliteration always chooses the translations based on probabilities. However, in some cases, the correct translation may have lower probability. Therefore, more stud-ies are needed on combination with other tech-niques as supplements. Aiming at these two problems, we propose a method which mines monolingual web resources to assist backward transliteration. The main ideas are as follows. We assume that for every Chinese en-tity name which needs to be backward transliter-ated to an English original name, the correct transliteration exists somewhere in the web. What we need to do is to find out the answers based on the clues given by statistical transliteration results. Different from the traditional methods which ex-tract transliteration pairs from bilingual pages, we only use monolingual web resources. Our method has two advantages. Firstly, there are much more monolingual web resources available to be used. Secondly, our method can revise the transliteration candidates to the existing words before the subse-quent re-ranking process, so that we can better mine the correct transliteration from the Web. Concretely, there are two phases involved in our approach. In the first phase, we split the result of transliteration into syllables, and then a syllable-based searching processing can be employed to revise the result in a word list generated from web pages, with an expectation of higher recall of trans-literation. In the second phase, we use a revised word as a search query to get its contexts and hit information, which are integrated into the AdaBoost classifier to determine whether the word is a transliteration name or not with a confidence score. This phase can readjust the candidate?s score to a more reasonable point so that precision of transliteration can be improved. Table 1 illustrates how to transliterate the Chinese name ??/a?/jia?/xi? back to ?Agassi?.  Chinese name Transliteration results Revised Candidate Re-rank Results 
??? a  jia xi Agassi 
aggasi agahi agacy agasie ? 
agasi agathi agathe agassi ? 
agassi agasi agache agga ? Table 1. An example of transliteration flow The experimental results show that our approach improves the recall from 41.73% to 59.28% in open test when returning the top-100 results, and the top-5 precision is improved from 19.69% to 52.19%. The remainder of the paper is structured as fol-lows. Section 2 presents the framework of our sys-tem. We discuss the details of our statistical transliteration model in Section 3. In Section 4, we introduce the approach of revising and re-ranking the results of transliteration. The experiments are reported in Section 5. The last section gives the conclusion and the prediction of future work. 2 System Framework  Our system has three main modules. 
  Figure 1. System framework 1) Statistical transliteration: This module re-ceives a Chinese Pinyin sequence as its input, and output the N-best results as the transliteration can-didates.  2) Candidate transliteration revision through syllable-based searching: In the module, a transliteration candidate is transformed into a syllable query. We use a syllable-based searching strategy to select the revised candidate from a huge word list. Each word in the list is indexed by sylla-bles, and the similarity between the word and the query is calculated. The most similar words are returned as the revision results. This module guar-
Monolingual web pages 
Words list 
Chinese name 
Statistical model 
Transliteration candidates Syllable-based search 
Revised candidates Re-ranking phase 
Final results Search engine 
542
 
antees the transliteration candidates are all existing words. 3) Revised candidate re-ranking in web pages: In the module, we search the revised candi-dates to get their contexts and hit information which we can use to score the probability of being a transliteration name. This phase doesn?t generate new candidates, but re-rank the revised candidate set to improve the performance in top-5. Under this framework, we can solve the two problems of statistical model mentioned above.  (1) The silent syllables will be given lower weights in syllable-based search, so the missing syllables will be recovered through selecting the most similar existing words which can contain some silent syllables.  (2) The query expansion technology can recall more potential transliteration candidates by ex-panding syllables to their ?synonymies?. So the mistakes introduced when selecting syllables in statistical transliteration will be corrected through giving suitable weights to synonymies.  Through the revision phase, the results of statis-tical model which may have illegal spelling will be mapped to its most similar existing words. That can improve the recall. In re-ranking phase, the revised candidate set will be re-ranked to put the right answer on the top using hybrid information got from web resources. So the precision of trans-literation will be improved. 3 Statistical Transliteration Model We use syllables as translation units to build a sta-tistical Chinese-English backward transliteration model in our system. 3.1 Traditional Statistical Translation Model [P. Brown et al, 1993] proposed an IBM source-channel model for statistical machine translation (SMT). When the channel output f= f1,f2 ?. fn ob-served, we use formula (1) to seek for the original sentence e=e1,e2 ?. en with the most likely poste-riori. 
' argmax ( | ) argmax ( | ) ( )
e e
e P e f P f e P e= =
      (1) The translation model ( | )P f e  is estimated from a paired corpus of foreign-language sentences and their English translations. The language model ( )P e  is trained from English texts. 
3.2 Our Transliteration Model The alignment method is the base of statistical transliteration model. There are mainly two kinds of alignment methods: phoneme-based alignment [Knight and Graehl, 1998; Virga and Khudanpur, 2003] and grapheme-based alignment [Long Jiang, 2007]. In our system, we adopt the syllable-based alignment from Chinese pinyin to English syllables, where the syllabication rules mentioned in [Long Jiang et al, 2007] are used. For example, Chinese name ??/xi ?/er ?/dun? and its backward transliteration ?Hilton? can be aligned as follows. ?Hilton? is split into syllable sequence as ?hi/l/ton?, and the alignment pairs are ?xi-hi?, ?er-l?, ?dun-ton?.  Based on the above alignment method, we can get our statistical Chinese-English backward trans-literation model as, 
argmax ( | ) ( )
E
E p PY ES p ES=
            (2) Where, PY is a Chinese Pinyin sequence, ES is a English syllables sequence, ( | )p PY ES  is the probability of translating ES into PY, ( )p E S  is the generative probability of a English syllable lan-guage model. 3.3 The Difference between Backward Trans-literation and Traditional Translation Chinese-English backward transliteration has some differences from traditional translation. 1) We don?t need to adjust the order of sylla-bles when transliteration.  2) The language model in backward translitera-tion describes the relationship of syllables in words. It can?t work as well as the language model de-scribing the word relationship in sentences. We think that the crucial problem in backward transliteration is selecting the right syllables at every step. It?s very hard to obtain the exact an-swer only based on the statistical transliteration model. We will try to improve the statistical model performance with the assistance of mining web resources. 4 Mining Monolingual Web Pages to As-sist Backward Transliteration  In order to get assistance from monolingual Web resource to improve statistical transliteration, our 
543
 
method contains two main phases: ?revision? and ?re-ranking?. In the revision phase, transliteration candidates are revised using syllable-based search in the word list, which are generated by collecting the existing words in web pages. Because the proc-ess of named entity recognition may lose some NEs, we will reserve all the words in web corpus without any filtering. The revision process can im-prove the recall through correcting some mistakes in the transliteration results of statistical model. In the re-ranking phase, we search every revised candidate on English pages, score them according to their contexts and hit information so that the right answer will be given a higher rank.  4.1 Using Syllable-based Retrieval to Revise Transliteration Candidates In this section, we will propose two methods re-spectively for the two problems of statistical model mentioned in section 1.  4.1.1  Syllable-based retrieval model When we search a transliteration candidate tci in the word list, we firstly split it into syllables {es1,es2,?..esn}. Then this syllable sequence is used as a query for syllable-based searching.  We define some notions here. ? Term set T={t1,t2?.tk} is an orderly set of all syllables which can be viewed as terms.  ? Pinyin set P={py1,py2?.pyk} is an orderly set of all Pinyin.  ? An input word can be represented by a vec-tor of syllables {es1,es2,?..esn}.  We calculate the similarity between a translitera-tion result and each word in the list to select the most similar words as the revised candidates. The {es1,es2,?..,esn} will be transformed into a vector Vquery={t1,t2?.tk} where ti represents the ith term in T. The value of ti is equal to 0 if the ith term doesn?t appear in query. In the same way, the word in list can also be transformed into vector represen-tation. So the similarity can be calculated as the inner product between these two vectors.  We don?t use tf and idf conceptions as traditional information retrieval (IR) to calculate the terms? weight. We use the weight of ti to express the ex-pectation probability of ith term having pronuncia-tion. If the term has a lower probability of having pronunciation, its weight is low. So when we searching, the missing silent syllables in the results 
of statistical transliteration model can be recovered because such syllables have little impact on simi-larity measurement. The formula we used is as fol-lows. 
( , )
/
query word
word py
V V
Sim query word
L L
?
=
            (3) 
The numerator is the inner product of two vec-tors. The denominator is the length of word Lword divided by the length of Chinese pinyin sequence Lpy. In this formula, the more syllables in one word, the higher score of inner production it may get, but the word will get a loss for its longer length. The word which has the shortest length and the highest syllable hitting ratio will be the best. Another difference from traditional IR is how to deal with the order of the words in a query. Ac-cording to transliteration, the similarity must be calculated under the limitation of keeping order, which can?t be satisfied by current methods. We use the algorithm like calculating the edit distance between two words. The syllables are viewed as the units which construct a word. The edit distance calculation finds the best matching with the least operation cost to change one word to another word by using deletion/addition/insertion operations on syllables. But the complexity will be too high to afford if we calculate the edit distance between a query and each word in the list. So, we just calcu-late the edit distance for the words which get high score without the order limitation. This trade off method can save much time but still keep perform-ance. 4.1.2  Mining the Equivalent through Syllable Expansion In most collections, the same concept may be re-ferred to using different words. This issue, known as synonymy, has an impact on the recall of most information retrieval systems. In this section, we try to use the expansion technology to solve prob-lem II. There are three kinds of expansions to be explained below.  Syllable expansion based on phonetic similar-ity: The syllables which correspond to the same Chinese pinyin can be viewed as synonymies. For example, the English syllables ?din? and ?tin? can be aligned to the same Chinese pinyin ?ding?. Given a Chinese pinyin sequence {py1,py2,?..pyn} as the input of transliteration model, for every pyi, there are a set of syllables 
544
 
{es1, es2 ?.. esk} which can be selected as its translation. The statistical model will select the most probable one, while others containing the right answer are discarded. To solve this problem, we expand the query to take the synonymies of terms into consideration. We create an expansion set for each Chinese pinyin. A syllable esi will be selected into the expansion set of pyj based on the alignment probability P(esi|pyj) which can be ex-tracted from the training corpus. The phonetic similarity expansion is based on the input Chinese Pinyin sequence, so it?s same for all candidates. Syllable expansion based on syllable similar-ity: If two syllables have similar alignment prob-ability with every pinyin, we can view these two syllables as synonymy. Therefore, if a syllable is in the query, its synonymies should be contained too. For example, ?fea? and ?fe? can replace each other. To calculate the similarity, we first obtain the alignment probability P(pyj|esk) of every syllable. Then the distance between any two syllables will be calculated using formula (4). 
1
1
( , ) ( | ) ( | )
N
j k i j i k
i
Sim es es P py es P py es
N
=
=
?
(4) This formula is used to evaluate the similarity of two syllables in alignment. The expansion set of the ith syllable can be generated by selecting the most similar N syllables. This kind of expansion is conducted upon the output of statistical translitera-tion model. Syllable expansion based on syllable edit dis-tance: The disadvantage of last two expansions is that they are entirely dependent on the training set. In other word, if some syllables haven?t appeared in the training corpus, they will not be expanded. To solve the problem, we use the method of expan-sion based on edit distance. We use edit distance to measure the similarity between two syllables, one is in training set and the other is absent. Because the edit distance expansion is not very relevant to pronunciation, we will give this expansion method a low weight in combination. It works when new syllables arise.  Combine the above three strategies: We will combine the three kinds of expansion method to-gether. We use the linear interpolation to integrate them. The formulas are follows.   
(1 )
pre sy ed
S S S S? ? ?= ? + +                (5) 
(1 )
pre py ed
S S S S? ? ?= ? + +                (6) 
where Spre is the score of exact matching, Ssy is the score of expansion based on syllables similarity and Spy based on phonetic similarity. We will ad-just these parameters to get the best performance. The experimental results and analysis will be re-ported in section 5.3. 4.2 Re-Ranking the Revised Candidates Set using the Monolingual Web Resource In the first phase, we have generated the revised candidate set {rc1,rc2,?,rcn} from the word list us-ing the transliteration results as clues. The objec-tive is to improve the overall recall. In the second phase, we try to improve the precision, i.e. we wish to re-rank the candidate set so that the correct an-swer will be put in a higher rank. [Al-Onaizan et al, 2002] has proposed some methods to re-score the transliteration candidates. The limitation of their approach is that some can-didates are propbale not existing words, with which we will not get any information from web. So it can only re-rank the transliteration results to improve the precision of top-5. In our work, we can improve the recall of transliteration through the revising process before re-ranking. In this section, we employ the AdaBoost frame-work which integrates several kinds of features to re-rank the revised candidate set. The function of the AdaBoost classifier is to calculate the probabil-ity of the candidate being a NE. Then we can re-rank the revised candidate set based on the score. The features used in our system are as follows. NE or not: Using rci as query to search for monolingual English Web Pages, we can get the context set {Ti1, Ti2??Tin} of rci. Then for every Tik, we use the named entity recognition (NER) software to determine whether rci is a NE or not. If rci is recognized as a NE in some Tik, rci will get a score. If rci can?t be recognized as NE in any con-texts, it will be pruned. The hit of the revised candidate: We can get the hit information of rci from search engine. It is used to evaluate the importance of rci. Unlike [Al-Onaizan et al, 2002], in which the hit can be used to eliminate the translation results which contain illegal spelling, we just use hit number as a feature. The limitation of compound NEs: When trans-literating a compound NE, we always split them into several parts, and then combine their translit-eration results together. But in this circumstance, 
545
 
every part can add a limitation in the selection of the whole NE. For example: ??/xi?/la?/li ?  ?/ke?/lin?/dun? is a compound name. ??/xi?/la?/li? can be transliterate to ?Hilary? or ?Hilaly? and ??/ke?/lin?/dun? can be transliterate to ?Clinton? or ?Klinton?. But the combination of ?Hilary?Clinton? will be selected for it is the most common combination. So the hit of combination query will be extracted as a feature in classifier. Hint words around the NE: We can take some hint words around the NE into the query, in order to add some limitations to filter out noisy words. For example: ??? (president)? can be used as hint word for ???? (Clinton)?. To find the hint words, we first search the Chinese name in Chi-nese web pages. The frequent words can be ex-tracted as hint words and they will be translated to English using a bilingual dictionary. These hint words are combined with the revised candidates to search English web pages. So, the hit of the query will be extracted as feature. The formula of AdaBoost is as follow. 
1
( ) ( ( ))
T
t t
t
H x sign h x?
=
=
?
                 (7) 
Where 
t
?  is the weight for the ith weak classifier 
( )
t
h x . 
t
?  can be calculated based on the precision of its corresponding classifier. 5 Experiments We carry out experiments to investigate how much the revision process and the re-ranking process can improve the performance compared with the base-line of statistical transliteration model. We will also evaluate to which extents we can solve the two problems mentioned in section 1 with the as-sistance of Web resources. 5.1 Experimental data The training corpus for statistical transliteration model comes from the corpus of Chinese <-> Eng-lish Name Entity Lists v 1.0 (LDC2005T34). It contains 565,935 transliteration pairs. Ruling out those pairs which are not suitable for the research on Chinese-English backward transliteration, such as Chinese-Japanese, we select a training set which contains 14,443 pairs of Chinese-European & American person names. In the training set, 1,344 
pairs are selected randomly as the close test data. 1,294 pairs out of training set are selected as the open test data. To set up the word list, a 2GB-sized collection of web pages is used. Since 7.42% of the names in the test data don?t appear in the list, we use Google to get the web page containing the ab-sent names and add these pages into the collection. The word list contains 672,533 words. 5.2 Revision phase vs. statistical approach Using the results generated from statistical model as baseline, we evaluate the revision module in recall first. The statistical transliteration model works in the following 4 steps: 1) Chinese name are transformed into pinyin representation and the English names are split into syllables. 2) The GIZA++1 tool is invoked to align pinyin to sylla-bles, and the alignment probabilities ( | )P py es are obtained. 3) Those frequent sequences of syllables are combined as phrases. For example, ?be/r/g???berg?, ?s/ky???sky?. 4) Camel 2  de-coder is executed to generate 100-best candidates for every name. We compare the statistical transliteration results with the revised results in Table 2. From Table 2 we can find that the recall of top-100 after revision is improved by 13.26% in close test set and 17.55% in open test set. It proves that the revision module is effective for correcting the mistakes made in statistical transliteration model. Transliteration results Revised results  close open close open Top1 33.64% 9.41% 27.15% 11.04% Top5 40.37% 13.38% 42.83% 19.69% Top10 47.79% 17.56% 56.98% 26.52% Top20 61.88% 25.44% 71.05% 37.81% Top50 66.49% 36.19% 82.16% 46.22% Top100 72.52% 41.73% 85.78% 59.28% Table 2. Statistical model vs. Revision module To show the effects of the revision on the two above-mentioned problems in which the statistical model does not solve well: the losing of silent syl-lables and the selection bias problem, we make a statistics of the improvements with a measurement of ?correction time?. For a Chinese word whose correct transliteration appears in top-100 candidates only if it has been 
                                                           1 http://www.fjoch.com/GIZA++.html 2 http://www.nlp.org.cn 
546
 
revised, we count the ?correction time?. For exam-ple, when ?Argahi? is revised to ?Agassi? the cor-rection time is ?1? for Problem II and ?1? for Problem I, because in ?hi?? ?si? the syllable is expanded, and in ?si? ??ssi? an ?s? is added.   Close test Open test Problem I 0.6931 0.7853 Problem II 0.9264 1.1672 Table 3. Average time of correction This measurement reflects the efficiency of the revision of search strategy, in contrast to those spelling correction techniques in which several operations of ?add? and ?expand? are inevitable. It has proved that the more an average correction time is, the more efficient our strategy is.  
?
???
???
???
???
???
???
???
???
???
?
? ? ? ? ? ? ? ? ?
??????? ?????????  Figure 2. Length influence in recall comparison The recall of the statistical model relies on the length of English name in some degree. It is more difficult to obtain an absolutely correct answer for longer names, because they may contain more si-lent and confused syllables. However, through the revision phase, this tendency can be effectively alleviated. In Figure 2, we make a comparison be-tween the results of the statistical model and the revision module with the changing of syllable?s length in open test. The curves demonstrate that the revision indeed prevents the decrease of recall for longer names. 5.3 Parameter setting in the revision phase We will show the experimental results when set-ting different parameters for query expansion. In the expansion based on phonetic similarity, for every Chinese pinyin, we select at most 20 sylla-bles to create an expansion set. We set 0.1? =  in formula (5). The results are shown in the columns labeled ?exp1? in Table 4. From the results we can conclude that, we get the best performance when 
0.4? = . That means the performance is best when the weight of exact 
matching is a little larger than the weight of fuzzy matching. We can also see that, higher weight of exact matching will lead to low recall, while higher weight of fuzzy matching will bring noise in. The expansion method based on syllable similar-ity is also evaluated. For every syllable, we select at most 15 syllables to create the expansion set. We set 0.1? = . The results are shown in the columns labeled ?exp2? in Table 4. From the results we can conclude that, we get the best performance when 0.5? = . It means that we can?t put emphasis on any matching methods. Comparison with the expansion based on phonetic similarity, the performance is poorer. It means that the expansion based on phonetic similarity is more suitable for revising transliteration candidates. 5.4 Revision phase vs. re-ranking phase After the phase of revising transliteration candi-dates, we re-rank the revised candidate set with the assistance of monolingual web resources. In this section, we will show the improvement in preci-sion after re-ranking. We have selected four kinds of features to inte-grate in the AdaBoost framework. To determine whether the candidate is NE or not in its context, we use the software tool Lingpipe3. The queries are sent to google, so that we can get the hit of queries and the top-10 snippets will be extracted as context. The comparison of revision results and re-ranking results is shown as follows. Revised results Re-ranked results  close open close open Top1 27.15% 11.04% 58.08% 38.63% Top5 42.83% 19.69% 76.35% 52.19% Top10 56.98% 26.52% 83..92% 54.33% Top20 71.05% 37.81% 83.92% 57.61% Top50 82.16% 46.22% 83.92% 57.61% Top100 85.78% 59.28% 85.78% 59.28% Table 5. Revision results vs. Re-ranking results From these results we can conclude that, after re-ranking phase, the noisy words will get a lower 
                                                           3 http://www.alias-i.com/lingpipe/ 
547
 
0.2? =  0.3? =   0.4? =  0.5? =  0.6? =  0.7? =  0.8? =   exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 Top1 13.46 13.32 13.79 13.61 11.04 12.70 11.65 10.93 10.83 11.25 9.62 10.63 8.73 10.18 Top5 21.58 19.59 23.27 20.17 19.69 18.28 21.07 17.25 22.05 16.84 17.90 16.26 17.38 15.34 Top10 27.39 22.71 28.41 24.73 26.52 22.93 26.83 21.81 27.26 20.39 24.38 21.20 25.42 18.20 Top20 35.23 34.88 35.94 29.49 37.81 31.57 38.59 33.04 36.52 31.72 35.25 29.75 34.65 27.62 Top50 43.91 40.63 43.75 40.85 46.22 41.46 48.72 42.79 45.48 40.49 41.57 39.94 42.81 38.07 Top100 53.76 48.47 54.38 52.04 59.28 53.15 57.36 53.46 55.19 51.83 55.63 49.52 53.41 47.15 Table 4.  Parameters Experiment rank. Through the revision module, we get both higher recall and higher precision than statistical transliteration model when at most 5 results are returned. We also use the average rank and average recip-rocal rank (ARR) [Voorhees and Tice, 2000] to evaluate the improvement. ARR is calculated as       
1
1 1
( )
M
i
ARR
M R i
=
=
?
                             (8) where ( )R i  is the rank of the answer of ith test word. M is the size of test set. The higher of ARR, the better the performance is. The results are shown as Table 6. Statistical  model Revision  module Re-rank  Module  close open close open close open Average rank 37.63 70.94 24.52 58.09 16.71 43.87 ARR 0.3815 0.1206 0.3783 0.1648 0.6519 0.4492 Table 6. ARR and AR evaluation The ARR after revision phase is lower than the statistical model. Because the goal of revision module is to improve the recall as possible as we can, some noisy words will be introduced in. The noisy words will be pruned in re-ranking module. That is why we get the highest ARR value at last. So we can conclude that the revision module im-proves recall and re-ranking module improves pre-cision, which help us get a better performance than pure statistical transliteration model 6 Conclusion In this paper, we present a new approach which can revise the results generated from statistical transliteration model with the assistance of mono-lingual web resource. Through the revision process, the recall of transliteration results has been im-proved from 72.52% to 85.78% in the close test set and from 41.73% to 59.28% in open test set, re-spectively. We improve the precision in re-ranking phase, the top-5 precision can be improved to 76.35% in close test and 52.19% in open test. The 
promising results show that our approach works pretty well in the task of backward transliteration. In the future, we will try to improve the similar-ity measurement in the revision phase. And we also wish to develop a new approach using the transliteration candidates to search for their right answer more directly and effectively. Acknowledgments The work is supported by the National High Tech-nology Development 863 Program of China under Grants no. 2006AA01Z144, the National Natural Science Foundation of China under Grants No. 60673042, the Natural Science Foundation of Bei-jing under Grants no. 4073043. References  Yaser Al-Onaizan and Kevin Knight. 2002. Translating named entities using monolingual and bilingual re-sources. In Proc.of ACL-02.  Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration. Computational Linguistics 24(4). Wei-Hao Lin and Hsin-His Chen. 2002 Backward Ma-chine Transliteration by Learning Phonetic Similarity. In Proc. Of the 6th CoNLL Donghui Feng, Yajuan Lv, and Ming Zhou. 2004. A New Approach for English-Chinese Named Entity Alignment. In Proc. of EMNLP-2004. Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng Niu, 2007. Named Entity Translation with Web Min-ing and Transliteration. In Proc. of IJCAI-2007. Wei Gao. 2004. Phoneme-based Statistical Translitera-tion of Foreign Name for OOV Problem. A thesis of Master. The Chinese University of Hong Kong. Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining translations of OOV terms from the web through cross-lingual query expansion. SIGIR 2005. Pu-Jen Cheng, Wen-Hsiang Lu, Jer-Wen Teng, and Lee-Feng Chien. 2004 Creating Multilingual Transla-tion Lexicons with Regional Variations Using Web Corpora. In Proc. of ACL-04 Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 2001. Using the Web as a Bilingual Dictionary. In Proc. of ACL 2001 Workshop on Data-driven Methods in Machine Translation. 
548
 
Paola Virga and Sanjeev Khudanpur. 2003. Translitera-tion of proper names in cross-lingual information re-trieval. In Proc. of the ACL workshop on Multi-lingual Named Entity Recognition. Jenq-Haur Wang, Jei-Wen Teng, Pu-Jen Cheng, Wen-Hsiang Lu, Lee-Feng Chien. 2004. Translating un-known cross-lingual queries in digital libraries using a web-based approach. In Proc. of JCDL 2004. E.M.Voorhees and D.M.Tice. 2000. The trec-8 question answering track report. In Eighth Text Retrieval Con-ference (TREC-8) 
549
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 387?395,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Chinese-English Organization Name Translation System Using 
Heuristic Web Mining and Asymmetric Alignment 
 
 
Fan Yang, Jun Zhao, Kang Liu 
National Laboratory of Pattern Recognition  
 Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China 
{fyang,jzhao,kliu}@nlpr.ia.ac.cn 
  
  
 
Abstract 
In this paper, we propose a novel system for 
translating organization names from Chinese 
to English with the assistance of web 
resources. Firstly, we adopt a chunking-
based segmentation method to improve the 
segmentation of Chinese organization names 
which is plagued by the OOV problem. 
Then a heuristic query construction method 
is employed to construct an efficient query 
which can be used to search the bilingual 
Web pages containing translation 
equivalents. Finally, we align the Chinese 
organization name with English sentences 
using the asymmetric alignment method to 
find the best English fragment as the 
translation equivalent. The experimental 
results show that the proposed method 
outperforms the baseline statistical machine 
translation system by 30.42%. 
1 Introduction 
The task of Named Entity (NE) translation is to 
translate a named entity from the source language 
to the target language, which plays an important 
role in machine translation and cross-language 
information retrieval (CLIR). The organization 
name (ON) translation is the most difficult 
subtask in NE translation. The structure of ON is 
complex and usually nested, including person 
name, location name and sub-ON etc. For 
example, the organization name ???????
????? (Beijing Nokia Communication 
Ltd.)? contains a company name (???/Nokia) 
and a location name (??/Beijing). Therefore, 
the translation of organization names should 
combine transliteration and translation together.  
Many previous researchers have tried to solve 
ON translation problem by building a statistical 
model or with the assistance of web resources. 
The performance of ON translation using web 
knowledge is determined by the solution of the 
following two problems:  
 The efficiency of web page searching: how 
can we find the web pages which contain the 
translation equivalent when the amount of the 
returned web pages is limited? 
 The reliability of the extraction method: how 
reliably can we extract the translation equivalent 
from the web pages that we obtained in the 
searching phase?  
For solving these two problems, we propose a 
Chinese-English organization name translation 
system using heuristic web mining and 
asymmetric alignment, which has three 
innovations.  
1) Chunking-based segmentation: A Chinese 
ON is a character sequences, we need to segment 
it before translation. But the OOV words always 
make the ON segmentation much more difficult. 
We adopt a new two-phase method here. First, 
the Chinese ON is chunked and each chunk is 
classified into four types. Then, different types of 
chunks are segmented separately using different 
strategies. Through chunking the Chinese ON 
first, the OOVs can be partitioned into one chunk 
which will not be segmented in the next phase. In 
this way, the performance of segmentation is 
improved.  
2) Heuristic Query construction: We need to 
obtain the bilingual web pages that contain both 
the input Chinese ON and its translation 
equivalent. But in most cases, if we just send the 
Chinese ON to the search engine, we will always 
get the Chinese monolingual web pages which 
don?t contain any English word sequences, let 
alone the English translation equivalent. So we 
propose a heuristic query construction method to 
generate an efficient bilingual query. Some 
words in the Chinese ON are selected and their 
translations are added into the query. These 
English words will act as clues for searching 
387
bilingual web pages. The selection of the Chinese 
words to be translated will take into 
consideration both the translation confidence of 
the words and the information contents that they 
contain for the whole ON.  
3) Asymmetric alignment: When we extract the 
translation equivalent from the web pages, the 
traditional method should recognize the named 
entities in the target language sentence first, and 
then the extracted NEs will be aligned with the 
source ON. However, the named entity 
recognition (NER) will always introduce some 
mistakes. In order to avoid NER mistakes, we 
propose an asymmetric alignment method which 
align the Chinese ON with an English sentence 
directly and then extract the English fragment 
with the largest alignment score as the equivalent. 
The asymmetric alignment method can avoid the 
influence of improper results of NER and 
generate an explicit matching between the source 
and the target phrases which can guarantee the 
precision of alignment.  
In order to illustrate the above ideas clearly,  
we give an example of translating the Chinese 
ON ??????????? (China Huarong 
Asset Management Corporation)?.  
Step1: We first chunk the ON, where ?LC?, 
?NC?, ?MC? and ?KC? are the four types of 
chunks defined in Section 4.2. 
??(China)/LC  ??(Huarong)/NC  ????
(asset management)/MC  ??(corporation)/KC 
Step2: We segment the ON based on the 
chunking results.  
??(china)  ??(Huarong)  ??(asset)     
??(management)  ??(corporation) 
If we do not chunk the ON first, the OOV 
word ???(Huarong)? may be segmented as ??   
??. This result will certainly lead to translation 
errors. 
Step 3: Query construction:  
We select the words ???? and ???? to 
translate and a bilingual query is constructed as: 
? ? ? ? ? ? ? ? ? ? ? ? + asset + 
management 
If we don?t add some English words into the 
query, we may not obtain the web pages which 
contain the English phrase ?China Huarong Asset 
Management Corporation?. In that case, we can 
not extract the translation equivalent. 
Step 4: Asymmetric Alignment: We extract a 
sentence ??President of China Huarong Asset 
Management Corporation?? from the returned 
snippets. Then the best fragment of the sentence 
?China Huarong Asset Management 
Corporation? will be extracted as the translation 
equivalent. We don?t need to implement English 
NER process which may make mistakes. 
The remainder of the paper is structured as 
follows. Section 2 reviews the related works. In 
Section 3, we present the framework of our 
system. We discuss the details of the ON 
chunking in Section 4. In Section 5, we introduce 
the approach of heuristic query construction. In 
section 6, we will analyze the asymmetric 
alignment method. The experiments are reported 
in Section 7. The last section gives the 
conclusion and future work. 
2 Related Work 
In the past few years, researchers have proposed 
many approaches for organization translation. 
There are three main types of methods. The first 
type of methods translates ONs by building a 
statistical translation model. The model can be 
built on the granularity of word [Stalls et al, 
1998], phrase [Min Zhang et al, 2005] or 
structure [Yufeng Chen et al, 2007]. The second 
type of methods finds the translation equivalent 
based on the results of alignment from the source 
ON to the target ON [Huang et al, 2003; Feng et 
al., 2004; Lee et al, 2006]. The ONs are 
extracted from two corpora. The corpora can be 
parallel corpora [Moore et al, 2003] or content-
aligned corpora [Kumano et al, 2004]. The third 
type of methods introduces the web resources 
into ON translation. [Al-Onaizan et al, 2002] 
uses the web knowledge to assist NE translation 
and [Huang et al, 2004; Zhang et al, 2005; Chen 
et al, 2006] extracts the translation equivalents 
from web pages directly.  
The above three types of methods have their 
advantages and shortcomings. The statistical 
translation model can give an output for any 
input. But the performance is not good enough on 
complex ONs. The method of extracting 
translation equivalents from bilingual corpora 
can obtain high-quality translation equivalents. 
But the quantity of the results depends heavily on 
the amount and coverage of the corpora. So this 
kind of method is fit for building a reliable ON 
dictionary. In the third type of method, with the 
assistance of web pages, the task of ON 
translation can be viewed as a two-stage process. 
Firstly, the web pages that may contain the target 
translation are found through a search engine. 
Then the translation equivalent will be extracted 
from the web pages based on the alignment score 
with the original ON. This method will not 
388
depend on the quantity and quality of the corpora 
and can be used for translating complex ONs. 
3 The Framework of Our System 
The Framework of our ON translation system 
shown in Figure 1 has four modules.  
 
Figure 1. System framework 
1) Chunking-based ON Segmentation Module: 
The input of this module is a Chinese ON. The 
Chunking model will partition the ON into 
chunks, and label each chunk using one of four 
classes. Then, different segmentation strategies 
will be executed for different types of chunks. 
2) Statistical Organization Translation Module: 
The input of the module is a word set in which 
the words are selected from the Chinese ON. The 
module will output the translation of these words.  
3) Web Retrieval Module: When input a 
Chinese ON, this module generates a query 
which contains both the ON and some words? 
translation output from the translation module. 
Then we can obtain the snippets that may contain 
the translation of the ON from the search engine. 
The English sentences will be extracted from 
these snippets.  
4) NE Alignment Module: In this module, the 
asymmetric alignment method is employed to 
align the Chinese ON with these English 
sentences obtained in Web retrieval module. The 
best part of the English sentences will be 
extracted as the translation equivalent. 
4 The Chunking-based Segmentation 
for Chinese ONs  
In this section, we will illustrate a chunking-
based Chinese ON segmentation method, which 
can efficiently deal with the ONs containing 
OOVs. 
4.1 The Problems in ON Segmentation 
The performance of the statistical ON translation 
model is dependent on the precision of the 
Chinese ON segmentation to some extent. When 
Chinese words are aligned with English words, 
the mistakes made in Chinese segmentation may 
result in wrong alignment results. We also need 
correct segmentation results when decoding. But 
Chinese ONs usually contain some OOVs that 
are hard to segment, especially the ONs 
containing names of people or brand names. To 
solve this problem, we try to chunk Chinese ONs 
firstly and the OOVs will be partitioned into one 
chunk. Then the segmentation will be executed 
for every chunk except the chunks containing 
OOVs. 
4.2 Four Types of Chunks  
We define the following four types of chunks for 
Chinese ONs: 
 Location Chunk (LC): LC contains the 
location information of an ON. 
 Name Chunk (NC): NC contains the name   
or brand information of an ON. In most 
cases, Name chunks should be 
transliterated. 
 Modification Chunk (MC): MC contains 
the modification information of an ON. 
 Key word Chunk (KC): KC contains the 
type information of an ON. 
The following is an example of an ON 
containing these four types of chunks. 
??(Beijing)/LC ? ? ? (Peregrine)/NC
????(investment consulting)/MC  ????
(co.)/KC  
In the above example, the OOV ????
(Peregrine)? is partitioned into name chunk. Then 
the name chunk will not be segmented.  
4.3 The CRFs Model for Chunking 
Considered as a discriminative probabilistic 
model for sequence joint labeling and with the 
advantage of flexible feature fusion ability, 
Conditional Random Fields (CRFs) [J.Lafferty et 
al., 2001] is believed to be one of the best 
probabilistic models for sequence labeling tasks. 
So the CRFs model is employed for chunking. 
We select 6 types of features which are proved 
to be efficient for chunking through experiments. 
The templates of features are shown in Table 1,  
389
Description Features 
current/previous/success 
character C0?C-1?C1 
whether the characters is 
a word 
W(C
-2C-1C0)?W(C0C1C2)?
W(C
-1C0C1) 
whether the characters is 
a location name 
L(C
-2C-1C0)?L(C0C1C2)?    
L(C
-1C0C1) 
whether the characters is 
an ON suffix 
SK(C
-2C-1C0)?SK(C0C1C2)? 
SK(C
-1C0C1) 
whether the characters is 
a location suffix 
SL(C
-2C-1C0)?SL(C0C1C2)?
SL(C
-1C0C1) 
relative position in the 
sentence 
POS(C0) 
Table 1. Features used in CRFs model 
where Ci denotes a Chinese character, i denotes 
the position relative to the current character. We 
also use bigram and unigram features but only 
show trigram templates in Table 1. 
5 Heuristic Query Construction 
In order to use the web information to assist 
Chinese-English ON translation, we must firstly 
retrieve the bilingual web pages effectively. So 
we should develop a method to construct 
efficient queries which are used to obtain web 
pages through the search engine. 
5.1 The Limitation of Monolingual Query 
We expect to find the web pages where the 
Chinese ON and its translation equivalent co-
occur. If we just use a Chinese ON as the query, 
we will always obtain the monolingual web 
pages only containing the Chinese ON. In order 
to solve the problem, some words in the Chinese 
ON can be translated into English, and the 
English words will be added into the query as the 
clues to search the bilingual web pages. 
5.2 The Strategy of Query Construction  
We use the metric of precision here to evaluate 
the possibility in which the translation equivalent 
is contained in the snippets returned by the search 
engine. That means, on the condition that we 
obtain a fixed number of snippets, the more the 
snippets which contain the translation equivalent 
are obtained, the higher the precision is. There 
are two factors to be considered. The first is how 
efficient the added English words can improve 
the precision. The second is how to avoid adding 
wrong translations which may bring down the 
precision. The first factor means that we should 
select the most informative words in the Chinese 
ON. The second factor means that we should 
consider the confidence of the SMT model at the 
same time. For example: 
??/LC  ??/NC 
?
?? /MC ????/KC 
(Tianjin   Honda     motor           co. ltd.) 
There are three strategies of constructing 
queries as follows: 
Q1.?????????????  Honda 
Q2.?????????????  Ltd. 
Q3.???????????? ? Motor 
Tianjin 
In the first strategy, we translate the word ??
?(Honda)? which is the most informative word 
in the ON. But its translation confidence is very 
low, which means that the statistical model gives 
wrong results usually. The mistakes in translation 
will mislead the search engine. In the second 
strategy, we translate the word which has the 
largest translation confidence. Unfortunately the 
word is so common that it can?t give any help in 
filtering out useless web pages. In the third 
strategy, the words which have sufficient 
translation confidence and information content 
are selected.  
5.3 Heuristically Selecting the Words to be 
Translated 
The mutual information is used to evaluate the 
importance of the words in a Chinese ON. We 
calculate the mutual information on the 
granularity of words in formula 1 and chunks in 
formula 2. The integration of the two kinds of 
mutual information is in formula 3. 
y Y
p ( x ,y )( , ) = lo g
p ( x ) p ( y )M I W x Y ??
     (1) 
Y
p ( y , c )( , ) = lo g
p ( y ) p ( c )y
M I C c Y
?
?       (2) 
( , )= ( , )+(1- ) ( , )xIC x Y MIW x Y MIC c Y? ?     (3) 
Here, MIW(x,Y) denotes the mutual 
information of word x with ON Y. That is the 
summation of the mutual information of x with 
every word in Y. MIC(c,Y) is similar. cx denotes 
the label of the chunk containing x. 
We should also consider the risk of obtaining 
wrong translation results. We can see that the 
name chunk usually has the largest mutual 
information. However, the name chunk always 
needs to be transliterated, and transliteration is 
often more difficult than translation by lexicon. 
So we set a threshold Tc for translation 
confidence. We only select the words whose 
translation confidences are higher than Tc, with 
their mutual information from high to low. 
390
6 Asymmetric Alignment Method for 
Equivalent Extraction 
After we have obtained the web pages with the 
assistant of search engine, we extract the 
equivalent candidates from the bilingual web 
pages. So we first extract the pure English 
sentences and then an asymmetric alignment 
method is executed to find the best fragment of 
the English sentences as the equivalent candidate. 
6.1 Traditional Alignment Method 
To find the translation candidates, the traditional 
method has three main steps.  
1) The NEs in the source and the target 
language sentences are extracted separately. The 
NE collections are Sne and Tne. 
2) For each NE in Sne, calculate the alignment 
probability with every NE in Tne. 
3) For each NE in Sne, the NE in Tne which has 
the highest alignment probability will be selected 
as its translation equivalent. 
This method has two main shortcomings: 
1) Traditional alignment method needs the 
NER process in both sides, but the NER process 
may often bring in some mistakes. 
2) Traditional alignment method evaluates the 
alignment probability coarsely. In other words, 
we don?t know exactly which target word(s) 
should be aligned to for the source word. A 
coarse alignment method may have negative 
effect on translation equivalent extraction.                                                                                                                                                    
6.2 The Asymmetric Alignment Method 
To solve the above two problems, we propose an 
asymmetric alignment method. The alignment 
method is so called ?asymmetric? for that it 
aligns a phrase with a sentence, in other words, 
the alignment is conducted between two objects 
with different granularities. The NER process is 
not necessary for that we align the Chinese ON 
with English sentences directly.  
[Wai Lam et al, 2007] proposed a method 
which uses the KM algorithm to find the optimal 
explicit matching between a Chinese ON and a 
given English ON. KM algorithm [Kuhn, 1955] 
is a traditional graphic algorithm for finding the 
maximum matching in bipartite weighted graph. 
In this paper, the KM algorithm is extended to be 
an asymmetric alignment method. So we can 
obtain an explicit matching between a Chinese 
ON and a fragment of English sentence. 
A Chinese NE CO={CW1, CW2, ?, CWn} is a 
sequence of Chinese words CWi and the English 
sentence ES={EW1, EW2, ?, EWm} is a sequence 
of English words EWi. Our goal is to find a 
fragment EWi,i+n={EWi, ?, EWi+n} in ES, which 
has the highest alignment score with CO. 
Through executing the extended KM algorithm, 
we can obtain an explicit matching L. For any 
CWi, we can get its corresponding English word 
EWj, written as L(CWi)=EWj and vice versa. We 
find the optimal matching L between two phrases, 
and calculate the alignment score based on L. An 
example of the asymmetric alignment will be 
given in Fig2. 
 
Fig2. An example of asymmetric alignment 
In Fig2, the Chinese ON ???????? is 
aligned to an English sentence ?? the 
Agriculture Bank of China is the four??. The 
stop words in parentheses are deleted for they 
have no meaning in Chinese. In step 1, the 
English fragment contained in the square 
brackets is aligned with the Chinese ON. We can 
obtain an explicit matching L1, shown by arrows, 
and an alignment score. In step 2, the square 
brackets move right by one word, we can obtain a 
new matching L2 and its corresponding alignment 
score, and so on. When we have calculated every 
consequent fragment in English sentence, we can 
find the best fragment ?the Agriculture Bank of 
China? according to the alignment score as the 
translation equivalent.  
The algorithm is shown in Fig3. Where, m is 
the number of words in an English sentence and 
n is the number of words in a Chinese ON. KM 
algorithm will generate an equivalent sub-graph 
by setting a value to each vertex. The edge whose 
weight is equal to the summation of the values of 
its two vertexes will be added into the sub-graph. 
Then the Hungary algorithm will be executed in 
the equivalent sub-graph to find the optimal 
matching. We find the optimal matching between 
CW1,n and EW1,n first. Then we move the window 
right and find the optimal matching between 
CW1,n and EW2,n+1. The process will continue 
until the window arrives at the right most of the 
? [(The) Agriculture Bank (of) China] (is) (the) four 
??    ??      ?? 
 (The) Agriculture [Bank (of) China] (is) (the) four]? 
??    ??      ?? 
Step 1: 
Step 2: 
391
English sentence. When the window moves right, 
we only need to find a new matching for the new 
added English vertex EWend and the Chinese 
vertex Cdrop which has been matched with EWstart 
in the last step. In the Hungary algorithm, the 
matching is added through finding an augmenting 
path. So we only need to find one augmenting 
path each time. The time complexity of finding 
an augmenting path is O(n3). So the whole 
complexity of asymmetric alignment is O(m*n3). 
Algorithm: Asymmetric Alignment Algorithm 
Input: A segmented Chinese ON CO and an 
English sentence ES. 
Output: an English fragment EWk,k+n 
1. Let start=1, end=n, L0=null 
2. Using KM algorithm to find the optimal 
matching between two phrases CW1,n and 
EWstart,end based on the previous matching Lstart-
1. We obtain a matching Lstart and calculate the 
alignment score Sstart based on Lstart. 
3. CWdrop = L(EWstart)  L(CWdrop)=null. 
4. If (end==m) go to 7, else start=start+1, 
end=end+1. 
5. Calculate the feasible vertex labeling for the 
vertexes CWdrop and EWend 
6. Go to 2. 
7. The fragment EWk,k+n-1 which has the highest 
alignment score will be returned. 
Fig3. The asymmetric alignment algorithm 
6.3 Obtain the Translation Equivalent 
For each English sentence, we can obtain a 
fragment ESi,i+n which has the highest alignment 
score. We will also take into consideration the 
frequency information of the fragment and its 
distance away from the Chinese ON. We use 
formula (4) to obtain a final score for each 
translation candidate ETi and select the largest 
one as translation result.  
( )= + log( +1)+ log(1 / +1)i i i iS ET SA C D? ? ?  (4) 
Where Ci denotes the frequency of ETi, and Di 
denotes the nearest distance between ETi and the 
Chinese ON. 
7 Experiments 
We carried out experiments to investigate the 
performance improvement of ON translation 
under the assistance of web knowledge.  
7.1 Experimental Data 
Our experiment data are extracted from 
LDC2005T34. There are two corpora, 
ldc_propernames_org_ce_v1.beta (Indus_corpus 
for short) and ldc_propernames_indu 
stry_ce_v1.beta (Org_corpus for short). Some 
pre-process will be executed to filter out some 
noisy translation pairs. For example, the 
translation pairs involving other languages such 
as Japanese and Korean will be filtered out. 
There are 65,835 translation pairs that we used as 
the training corpus and the chunk labels are 
added manually. 
We randomly select 250 translation pairs from  
the Org_corpus and 253 translation pairs from 
the Indus_corpus. Altogether, there are 503 
translation pairs as the testing set. 
7.2 The Effect of Chunking-based 
Segmentation upon ON Translation  
In order to evaluate the influence of segmentation 
results upon the statistical ON translation system, 
we compare the results of two translation models. 
One model uses chunking-based segmentation 
results as input, while the other uses traditional 
segmentation results. 
To train the CRFs-chunking model, we 
randomly selected 59,200 pairs of equivalent 
translations from Indus_corpus and org_corpus. 
We tested the performance on the set which 
contains 6,635 Chinese ONs and the results are 
shown as Table-2. 
For constructing a statistical ON translation 
model, we use GIZA++1 to align the Chinese NEs 
and the English NEs in the training set. Then the 
phrase-based machine translation system 
MOSES2 is adopted to translate the 503 Chinese 
NEs in testing set into English. 
 Precision Recall F-measure 
LC 0.8083 0.7973 0.8028 
NC 0.8962 0.8747 0.8853 
MC 0.9104 0.9073 0.9088 
KC 0.9844 0.9821 0.9833 
All 0.9437 0.9372 0.9404 
Table 2. The test results of CRFs-chunking model 
We have two metrics to evaluate the 
translation results. The first metric L1 is used to 
evaluate whether the translation result is exactly 
the same as the answer. The second metric L2 is 
used to evaluate whether the translation result 
contains almost the same words as the answer, 
                                                          
1
 http://www.fjoch.com/GIZA++.html 
2
 http://www.statmt.org/moses/ 
392
without considering the order of words. The 
results are shown in Table-3: 
 chunking-based 
segmentation  
traditional 
segmentation 
L1 21.47% 18.29% 
L2 40.76% 36.78% 
Table 3. Comparison of segmentation influence 
From the above experimental data, we can see 
that the chunking-based segmentation improves 
L1 precision from 18.29% to 21.47% and L2 
precision from 36.78% to 40.76% in comparison 
with the traditional segmentation method. 
Because the segmentation results will be used in 
alignment, the errors will affect the computation 
of alignment probability. The chunking based 
segmentation can generate better segmentation 
results; therefore better alignment probabilities 
can be obtained.  
7.3 The Efficiency of Query Construction 
The heuristic query construction method aims to 
improve the efficiency of Web searching. The 
performance of searching for translation 
equivalents mostly depends on how to construct 
the query. To test its validity, we design four 
kinds of queries and evaluate their ability using 
the metric of average precision in formula 5 and 
macro average precision (MAP) in formula 6, 
1
1P r
N
i
i i
HA vera g e ec is io n
N S
=
= ?             (5) 
where Hi is the count of snippets that contain at 
least one equivalent for the ith query. And Si is 
the total number of snippets we got for the ith 
query, 
1= 1
1
( )
1 j
i
HN
j j
i
M A P
R iN H
=
= ??               (6) 
where R(i) is the order of snippet where the ith 
equivalent occurs. We construct four kinds of 
queries for the 503 Chinese ONs in testing set as 
follows: 
Q1: only the Chinese ON.  
Q2: the Chinese ON and the results of the 
statistical translation model.  
Q3: the Chinese ON and some parts? 
translation selected by the heuristic query 
construction method.  
Q4: the Chinese ON and its correct English 
translation equivalent.  
We obtain at most 100 snippets from Google 
for every query. Sometimes there are not enough 
snippets as we expect. We set ? in formula 4 at 
0.7?and the threshold of translation confidence 
at 0.05. The results are shown as Table 4.  
 Average 
precision 
MAP 
Q1 0.031 0.0527 
Q2 0.187 0.2061 
Q3 0.265 0.3129 
Q4 1.000 1.0000 
Table 4. Comparison of four types query 
Here we can see that, the result of Q4 is the 
upper bound of the performance, and the Q1 is 
the lower bound of the performance. We 
concentrate on the comparison between Q2 and 
Q3. Q2 contains the translations of every word in 
a Chinese ON, while Q3 just contains the 
translations of the words we select using the 
heuristic method. Q2 may give more information 
to search engine about which web pages we 
expect to obtain, but it also brings in translation 
mistakes that may mislead the search engine. The 
results show that Q3 is better than Q2, which 
proves that a careful clue selection is needed. 
7.4 The Effect of Asymmetric Alignment 
Algorithm 
The asymmetric alignment method can avoid the 
mistakes made in the NER process and give an 
explicit alignment matching. We will compare 
the asymmetric alignment algorithm with the 
traditional alignment method on performance. 
We adopt two methods to align the Chinese NE 
with the English sentences. The first method has 
two phases, the English ONs are extracted from 
English sentences firstly, and then the English 
ONs are aligned with the Chinese ON. Lastly, the 
English ON with the highest alignment score will 
be selected as the translation equivalent. We use 
the software Lingpipe3 to recognize NEs in the 
English sentences. The alignment probability can 
be calculated as formula 7: 
( , ) ( | )i j
i j
Score C E p e c= ??       (7) 
The second method is our asymmetric 
alignment algorithm. Our method is different 
from the one in [Wai Lam et al, 2007] which 
segmented a Chinese ON using an English ON as 
suggestion. We segment the Chinese ON using 
the chunking-based segmentation method. The 
English sentences extracted from snippets will be 
preprocessed. Some stop words will be deleted, 
such as ?the?, ?of?, ?on? etc. To execute the 
extended KM algorithm for finding the best 
alignment matching, we must assure that the 
vertex number in each side of the bipartite is the 
                                                          
3
 http://www.alias-i.com/lingpipe/ 
393
same. So we will execute a phrase combination 
process before alignment, which combines some 
frequently occurring consequent English words 
into single vertex, such as ?limited company? etc. 
The combination is based on the phrase pair table 
which is generated from phrase-based SMT 
system. The results are shown in Table 5: 
 Asymmetric 
Alignment 
Traditional 
method 
Statistical 
model 
Top1 48.71% 36.18% 18.29% 
Top5 53.68% 46.12% -- 
Table 5. Comparison the precision of alignment 
method 
From the results (column 1 and column 2) we 
can see that, the Asymmetric alignment method 
outperforms the traditional alignment method. 
Our method can overcome the mistakes 
introduced in the NER process. On the other 
hand, in our asymmetric alignment method, there 
are two main reasons which may result in 
mistakes, one is that the correct equivalent 
doesn?t occur in the snippet; the other is that 
some English ONs can?t be aligned to the 
Chinese ON word by word.  
7.5 Comparison between Statistical ON 
Translation Model and Our Method 
Compared with the statistical ON translation 
model, we can see that the performance is 
improved from 18.29% to 48.71% (the bold data 
shown in column 1 and column 3 of Table 5) by 
using our Chinese-English ON translation system. 
Transforming the translation problem into the 
problem of searching for the correct translation 
equivalent in web pages has three advantages. 
First, word order determination is difficult in 
statistical machine translation (SMT), while 
search engines are insensitive to this problem. 
Second, SMT often loses some function word 
such as ?the?, ?a?, ?of?, etc, while our method 
can avoid this problem because such words are 
stop words in search engines. Third, SMT often 
makes mistakes in the selection of synonyms. 
This problem can be solved by the fuzzy 
matching of search engines. In summary, web 
assistant method makes Chinese ON translation 
easier than traditional SMT method.  
8 Conclusion 
In this paper, we present a new approach which 
translates the Chinese ON into English with the 
assistance of web resources. We first adopt the 
chunking-based segmentation method to improve 
the ON segmentation. Then a heuristic query 
construction method is employed to construct a 
query which can search translation equivalent 
more efficiently. At last, the asymmetric 
alignment method aligns the Chinese ON with 
English sentences directly. The performance of 
ON translation is improved from 18.29% to 
48.71%. It proves that our system can work well 
on the Chinese-English ON translation task. In 
the future, we will try to apply this method in 
mining the NE translation equivalents from 
monolingual web pages. In addition, the 
asymmetric alignment algorithm also has some 
space to be improved. 
Acknowledgement 
The work is supported by the National High 
Technology Development 863 Program of China 
under Grants no. 2006AA01Z144, and the 
National Natural Science Foundation of China 
under Grants no. 60673042 and 60875041. 
 
 
References  
Yaser Al-Onaizan and Kevin Knight. 2002. 
Translating named entities using monolingual and 
bilingual resources. In Proc of ACL-2002.  
Yufeng Chen, Chenqing Zong. 2007. A Structure-
Based Model for Chinese Organization Name 
Translation. In Proc. of ACM Transactions on 
Asian Language Information Processing (TALIP) 
Donghui Feng, Yajuan Lv, Ming Zhou. 2004. A new 
approach for English-Chinese named entity 
alignment. In Proc. of  EMNLP 2004. 
Fei Huang, Stephan Vogal. 2002. Improved named 
entity translation and bilingual named entity 
extraction. In Proc. of the 4th IEEE International 
Conference on Multimodal Interface. 
Fei Huang, Stephan Vogal, Alex Waibel. 2003. 
Automatic extraction of named entity translingual 
equivalence based on multi-feature cost 
minimization. In Proc. of the 2003 Annual 
Conference of the ACL, Workshop on Multilingual 
and Mixed-language Named Entity Recognition 
Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 
2001. Using the Web as a Bilingual Dictionary. In 
Proc. of ACL 2001 Workshop on Data-driven 
Methods in Machine Translation. 
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Proc. of 
ACL 2005. 
Conrad Chen, Hsin-His Chen. 2006. A High-Accurate 
Chinese-English NE Backward Translation System 
Combining Both Lexical Information and Web 
Statistics. In Proc. of ACL 2006. 
394
Wai Lam,  Shing-Kit Chan. 2007. Named Entity 
Translation Matching and Learning: With 
Application for Mining Unseen Translations. In 
Proc. of ACM Transactions on Information 
Systems.  
Chun-Jen Lee, Jason S. Chang, Jyh-Shing R. Jang. 
2006. Alignment of bilingual named entities in 
parallel corpora using statistical models and 
multiple knowledge sources. In Proc. of ACM 
Transactions on Asian Language Information 
Processing (TALIP). 
Kuhn, H. 1955. The Hungarian method for the 
assignment problem. Naval Rese. Logist. Quart 
2,83-97. 
Min Zhang., Haizhou Li, Su Jian, Hendra Setiawan. 
2005. A phrase-based context-dependent joint 
probability model for named entity translation. In 
Proc. of the 2nd International Joint Conference on 
Natural Language Processing(IJCNLP)  
Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining 
translations of OOV terms from the web through 
cross-lingual query expansion. In Proc. of the 28th 
ACM SIGIR. 
Bonnie Glover Stalls and Kevin Knight. 1998. 
Translating names and technical terms in Arabic 
text. In Proc. of the COLING/ACL Workshop on 
Computational Approaches to Semitic Language. 
J. Lafferty, A. McCallum, and F. Pereira. 2001.  
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data. In Proc. 
ICML-2001. 
Tadashi Kumano, Hideki Kashioka, Hideki Tanaka 
and Takahiro Fukusima. 2004. Acquiring bilingual 
named entity translations from content-aligned 
corpora. In Proc. IJCNLP-04. 
Robert C. Moore. 2003. Learning translation of 
named-entity phrases from parallel corpora. In Proc. 
of 10th conference of the European chapter of ACL.  
 
 
 
 
395
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1025?1032
Manchester, August 2008
Switching to Real-Time Tasks in Multi-Tasking Dialogue
Fan Yang and Peter A. Heeman
Center for Spoken Language Understanding
OGI School of Science & Engineering
Oregon Health & Science University
fly,heeman@cslu.ogi.edu
Andrew Kun
Electrical and Computer Engineering
University of New Hampshire
andrew.kun@unh.edu
Abstract
In this paper we describe an empirical
study of human-human multi-tasking dia-
logues (MTD), where people perform mul-
tiple verbal tasks overlapped in time. We
examined how conversants switch from the
ongoing task to a real-time task. We found
that 1) conversants use discourse markers
and prosodic cues to signal task switch-
ing, similar to how they signal topic shifts
in single-tasking speech; 2) conversants
strive to switch tasks at a less disruptive
place; and 3) where they cannot, they ex-
ert additional effort (even higher pitch) to
signal the task switching. Our machine
learning experiment also shows that task
switching can be reliably recognized using
discourse context and normalized pitch.
These findings will provide guidelines for
building future speech interfaces to sup-
port multi-tasking dialogue.
1 Introduction
Existing speech interfaces have mostly been used
to perform a single task. However, we envision
that next-generation speech interfaces will be able
to work with the user on multiple tasks at the same
time, which is especially useful for real-time tasks.
For instance, a driver in a car might use a speech
interface to catch up on emails, while occasionally
checking upcoming traffic conditions, and receiv-
ing navigation instructions.
Several speech interfaces that allow multi-
tasking dialogues have been built (Lemon et al,
2002; Kun et al, 2004). However, these interfaces
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
freely switch between different tasks without much
signaling. Thus the user might be confused about
which task the interface is talking about. Multi-
tasking dialogues, even in the best circumstances,
will be difficult for users, as users need to remem-
ber the details of each task and be aware of task
switching.
In order to build a speech interface that supports
multi-tasking dialogue, there needs to be a set of
conventions for the user and the interface to follow
in switching between tasks. To design such a set,
we propose to start with conventions that are ac-
tually used in human-human conversations, which
are natural for users to follow and probably effi-
cient in problem-solving. Multi-tasking dialogues,
where multiple independent topics overlap with
each other in time, regularly arise in human-human
conversation: for example, a driver and a navigator
in a car might be talking about their summer plans,
while occasionally interjecting road directions or
conversation about what music to listen to.
In order to better understand the human con-
ventions on task switching, we have collected the
MTD corpus (Heeman et al, 2005), which consists
of a set of human-human dialogues where pairs of
conversants have multiple overlapping verbal tasks
to perform: an ongoing task that takes a long time
to finish, and a real-time task that can be done in
a couple of turns but has a time constraint. This
paper is focused on how conversants switch from
the ongoing task to a waiting real-time task.
Previous research suggested the correlation be-
tween task switching and certain discourse con-
text; for example, conversants try to avoid task
switching in the middle of an adjacency pair (Shy-
rokov et al, 2007). In a preliminary study (Hee-
man et al, 2005), we examined the timing when
conversants switched from the ongoing task to a
real-time task using some pilot data, and found that
1025
conversants did not always switch to a real-time
task as soon as it arose, but instead waited for dif-
ferent amounts of time depending on its time con-
straint. In this study, we hypothesize that conver-
sants strive to switch at an opportune place in the
ongoing task, and we examine the discourse con-
text where task switching occurs for evidence to
support this hypothesis.
We are also interested in the cues that conver-
sants use to signal task switching. Although there
is a substantial body of research on how people
signal topic shifts in single-tasking speech (mono-
logue and dialogue), such as using discourse mark-
ers and prosodic cues (see Section 2.2), little re-
search work has been done in investigating task
switching in multi-tasking dialogues. In this study,
we examine discourse markers and prosodic cues
for their correlations with task switching. We also
examine combining these cues to recognize task
switching with machine learning techniques.
In Section 2, we review related literature. In
Section 3, we describe the MTD corpus. In Sec-
tion 4, we examine the discourse contexts in which
task switching occurs. In Section 5, we examine
the use of discourse markers and prosody associ-
ated with task switching. In Section 6, we exam-
ine automatic recognizing task switching with ma-
chine learning techniques. We conclude the paper
in Section 7.
2 Related Research
In this section, we first describe two existing
speech interfaces that allow multi-tasking dia-
logues. These speech interfaces, however, freely
switch between tasks as soon as a new task arises,
and without much signaling. We then review lit-
erature on how people signal topic shifts in single-
tasking speech, which sheds light on our research
of signaling task switching in multi-tasking dia-
logues.
2.1 Speech Interfaces for MTD
Kun et al (2004) developed a system called
Project54, which allows a user to interact with
multiple devices in a police cruiser using speech.
The architecture of Project54 allows for handling
multiple tasks overlapped in time. For example,
when pulling over a vehicle, an officer can first is-
sue a spoken command to turn on the lights and
siren, then issue spoken commands to initiate a
data query, go back to interacting with the lights
and siren (perhaps to change the pattern after the
vehicle has been pulled over), and finally receive
the spoken results of the data query. While the
current implementation of Project54 assumes that
the officer initiates the task switching (e.g. the
one about lights and the one about data query), the
system can initiate task switching too. However,
Project54 does not provide infrastructure for sig-
naling to the officer a system-initiated switch. Any
such signaling would have to be hand-coded by de-
velopers.
Lemon et al (2002) also explored multi-tasking
in a dialogue system. They built a multi-tasking
dialogue system for a human operator to direct
a robotic helicopter on executing multiple tasks,
such as searching for a car and flying to a tower.
The system keeps an ordered set of active dialogue
tasks, and interprets the user utterance in terms of
the most active task for which the utterance makes
sense. Conversely, during the system?s turn of
speaking, it can produce an utterance for any of
the dialogue tasks. Thus the system does not take
into account the user?s cost of task switching. The
system switches to a new task as soon as it arises,
instead of at an opportune place to minimize the
user?s effort. Moreover, the system does not sig-
nal when it switches between tasks. As with the
approach of Kun et al (2004) to multiple devices,
it is unclear whether an actual user will be able to
understand such conversations. The user might be-
come confused about which task the system is on.
2.2 Signaling Topic Shifts in STP
Although speech interfaces have not used cues to
signal task switching, researchers have found vari-
ous cues that people naturally use in single-tasking
speech to signal topic shifts. These cues are a good
starting point from which to study how people sig-
nal task switching in multi-tasking dialogue.
Signaling topic shifts in single-tasking speech is
about signaling the boundary of related discourse
segments that contribute to the achievement of a
task. Two types of cues have been identified for
signaling topic shifts. The first type is discourse
markers (Moser and Moore, 1995; Schiffrin, 1987;
Grosz and Sidner, 1986; Passonneau and Litman,
1997; Bangerter and Clark, 2003). Discourse
markers can be used to signal the start of a new dis-
course segment and its relation to other discourse
segments. For example, ?now? might signal mov-
ing on to the next topic, while ?well? might signal
1026
a negative or unexpected response.
The second type of cue is prosody. In read
speech, Grosz and Hirschberg (1992) studied
broadcast news and found that pause length is the
most important factor that indicates a new dis-
course segment. Ayers (1992) found that pitch
range appears to correlate more closely with hi-
erarchical topic structure in read speech than in
spontaneous speech. In spontaneous monologue,
Butterworth (1972) found that the beginning of a
discourse segment exhibited slower speaking rate;
Swerts (1995), and Passonneau and Litman (1997)
found that pause length correlates with discourse
segment boundaries; Hirschberg and Nakatani
(1996) found that the beginning of a discourse
segment correlates with higher pitch. In human-
human dialogue, similar behavior was observed:
the pitch value tends to be higher for starting a new
discourse segment (Nakajima and Allen, 1993). In
human-computer dialogue, Swerts and Ostendorf
(1995) found that the first utterance of a discourse
segment correlates with slower speaking rate and
longer preceding pause. Clearly, prosody is used
to signal topic shifts in single-tasking speech.
3 The MTD Corpus
In order to fully understand multi-tasking human-
human dialogue, we collected the MTD corpus, in
which pairs of subjects perform overlapping verbal
tasks. Details of the corpus collection can be found
in (Heeman et al, 2005).
3.1 Design of Tasks
Conversants work on two types of tasks via conver-
sation: an on-going task that takes a long time to
finish and a real-time task that just takes a couple
turns to complete but has a time constraint.
In the ongoing task, a pair of players work to-
gether to form as many poker hands as possible,
where a poker hand consists of a full house, flush,
straight, or four of a kind. Each player has three
cards in hand, which the other cannot see (players
are separated so that they cannot see each other.)
Players take turns drawing an extra card and then
discarding one, until they find a poker hand, for
which they earn 50 points; they then start over to
form another poker hand. To discourage players
from simply rifling through the cards to look for a
specific card without talking, one point is deducted
for each picked-up card, and 10 points for a missed
poker hand or incorrect poker hand. To complete
Figure 1: The game display for players
this game, players converse to share card informa-
tion, explore and establish strategies based on the
combined cards in their hands (Toh et al, 2006).
The poker game is played on computers. The
game display, which each player sees, is shown in
Figure 1. The player with four cards can click on
a card to discard it. The card disappears from the
screen, and an extra card is automatically dealt to
the other player. The player with four cards clicks
the ?Done Poker Hand? button to start a new game
once they find a poker hand.
From time to time, the computer generates a
prompt for one player to find out whether the other
has a certain picture on the bottom of the display.
The picture game has a time constraint of 10, 25
or 40 seconds, which is (pseudo) randomly deter-
mined. The players get 5 points for the picture
game if the correct answer is given in time. The
overall goal of the players is to earn as many points
as possible from the two games.
To alert the player to the picture game, two
solid bars flash above and below the player?s cards.
Thus the player will know that there is a wait-
ing picture game without taking the attention away
from the poker game. The color of the flash-
ing bars depends on how much time is remaining:
green for 26-40 seconds, yellow for 11-25 seconds
1027
and red for 0-10 seconds. The player can see the
exact amount of time in the heading for the pic-
ture game. In Figure 1, the player needs to find out
whether the other has a blue circle, with 6 seconds
left.
3.2 Corpus Annotations
We transcribed and annotated ten MTD dialogues
totaling about 150 minutes of conversation. The
dialogues were by five pairs of players, all na-
tive American-English speakers. Each pair par-
ticipated in two sessions and each session lasted
about 15 minutes. During each session, 9 picture
games (3 for each time constraint) were prompted
for each player. Of the total 180 picture game
prompted, 8 were never started by players
1
. Thus
the corpus contains 172 picture games.
The ongoing task can naturally be divided into
individual poker games, in which the players suc-
cessfully complete a poker hand. Each poker game
can be further divided into a sequence of card seg-
ments, in which players discuss which card to dis-
card, or a poker hand is found. In total, there are
105 game segments and 690 card segments in the
corpus. As well, we grouped the utterances in-
volved in each picture game into segments. Fig-
ure 2 shows an excerpt from an MTD dialogue
with these annotations. Here b7 is a game segment
in which players got a poker hand of flush; and
b8, b10, b11, b12 and b14, inside of b7, are card
segments. Also embedded in b7 are b9 and b13,
each of which is an segment for a picture game.
As can be seen, players switched from the ongo-
ing poker-playing to a picture game. After the pic-
ture game was completed, the conversation on the
poker-playing resumed.
4 Where to Switch
In a preliminary study (Heeman et al, 2005), we
found that players did not always switch to a real-
time task as soon as it arose, but instead waited for
different amounts of time depending on the time
constraint of the real-time task. We thus hypoth-
esize that players strive to switch at an opportune
place in the ongoing task (poker-playing). There
are three types of places where a player could sus-
pend the poker playing and switch to a waiting
picture game: (G) immediately after completing
a poker game (at the end of a game), (C) immedi-
1
Although in the post-experiment survey all players re-
ported that they never ignored a picture game on purpose
Figure 2: An excerpt of an MTD dialogue
ately after discarding a card (at the end of a card),
and (E) embedded inside a card segment, where
players are deciding which card to discard. In this
section, we examine where task switching occurs.
4.1 Time Constraint and Place of Switching
We first examine the place of switching under dif-
ferent time constraints. As shown in Table 1, for
the time constraint of 10s, 75% of the task switch-
ing was embedded inside a card segment, 23% at
the end of a card, and 2% at the end of a game;
for the time constraints of 25s and 40s
2
, 46% em-
bedded inside a card segment, 33% at the end of a
card, and 21% at the end of a game. The difference
in the places of switching between the time con-
straint of 10s and 25s/40s is statistically significant
(?
2
(2) = 15.92, p < 0.001). The time constraint
of 10s requires players to start a picture game very
quickly in order to complete it in time. On the
other hand, when given 25s or 40s, players are in
a less hurry to switch. Compared with 10s, when
players had 25s or 40s, the percentage of switch-
ing embedded inside a card segment decreases by
29%, while at the end of a card increases by 10%,
and at the end of a game increases by 19%. These
results suggest that when given more time, players
try to switch at the end of a game or a card.
2
We combined the time constraints of 25s and 40s because
25s seemed to be sufficient for most players.
1028
Table 1: Time constraint and place of switching
E C G Total
10s 42 (75%) 13 (23%) 1 (2%) 56 (100%)
25/40s 54 (46%) 38 (33%) 24 (21%) 116 (100%)
Table 2: Waiting time and place of switching
E C G Total
? 3s 47 (69%) 18 (27%) 3 (4%) 68 (100%)
> 3s 49 (47%) 33 (32%) 22 (21%) 104 (100%)
4.2 Waiting Time and Place of Switching
We next examine the place of task switching from
the perspective of waiting time. Waiting time
refers to the time interval between when a pic-
ture game is prompted to a player and when the
player actually starts the picture game. Our ques-
tion is: if players wait at least a certain amount
of time, where would they switch tasks? We arbi-
trary choose a time amount of 3 seconds. We as-
sume that when the waiting time is shorter than 3s,
the player starts the picture game as soon as he or
she notices it without significant waiting; in other
words, based on human reaction time, if players
are going to respond to it right away, they should
be able to do so within 3s. The results are shown
in Table 2. When the waiting time is shorter than
3s, 69% of the task switching is embedded inside
a card segment, 27% at the end of a card, and only
4% at the end of a game; when longer than 3s, 47%
is embedded inside a card segment, 32% at the end
of a card, and 21% at the end of a game. The dif-
ference in the places of switching is statistically
different (?
2
(2) = 11.88, p = 0.003). When the
waiting time is longer than 3s, the percentage of
switching inside a card decreases by 22%, while
switching at the end of a card increases by 5%, and
at the end of a game increases by 17%. These re-
sults suggest that players wait for the end of a game
or a card to switch to a picture game.
4.3 Discussion
We examined the discourse context of task switch-
ing, and found that 1) when given more time, play-
ers intend to switch to a picture game at the end of
a (poker) game or a card; and (2) if players wait,
they are waiting for the end of a (poker) game or a
card to switch to a picture game. These results sug-
gest that players strive to switch to a picture game
at the end of a (poker) game or a card.
In fact, we also observed that after a picture
game that is at the end of a game, players smoothly
start a new poker game as if nothing had hap-
pened; after a picture game that is at the end of a
card, players might sometimes remind each other
what cards they have in hands; while after a pic-
ture game that is in the middle of a card segment,
players might even repeat or clarify the previous
utterances that were said before the interruption.
It is thus reasonable to assume that switching em-
bedded inside a card segment is the most disrup-
tive, followed by at the end of a card, and at the
end of a game is the least. Our experiment results
hence suggest that players strive to switch to a real-
time task at a less disruptive place in the ongoing
task. This is consistent with Clark and Wilkes-
Gibbs (1986), that conversants try to minimize col-
laborative effort.
5 How to Switch
In Section 2.2, we discussed how people use cer-
tain cues, such as discourse markers and prosody,
to signal topic shifts in single-tasking speech. This
suggests that people might also signal task switch-
ing in multi-tasking dialogues. In this section, we
examine how players signal that they are switch-
ing from the ongoing task to a real-time task with
discourse markers and prosody.
5.1 Task Switching and Discourse Markers
Close examination of the MTD corpus found
that ?oh? was the most frequently used discourse
marker when switching to a picture game. An-
other discourse marker, ?wait? (including ?wait a
minute?), was often used together with ?oh? in the
way of ?oh wait?. Thus we examined the use of
?oh? and ?wait? in switching to a picture game.
Players used the discourse markers ?oh? or
?wait? 14.5% (25/172) of the time in switching to
a picture game. In poker playing, 5.7% (238/4192)
of utterances contain the words ?oh? or ?wait?, and
only 4.6% (32/690) of card segments are initiated
with the two discourse markers (i.e. the first ut-
terance of a card segment has ?oh? or ?wait? at
the very beginning). Players have a statistically
higher percentage of using ?oh? or ?wait? at task
switching than in poker playing (?
2
(1) = 22.89,
p < 0.001) or to initiate a card segment (?
2
(1) =
21.84, p < 0.001).
5.2 Task Switching and Prosody
To understand the prosodic cues in initiating
a topic, traditionally researchers compared the
1029
prosody of the first utterance in each segment with
other utterances (e.g. (Nakajima and Allen, 1993;
Hirschberg and Nakatani, 1996)). This approach
encounters two problems here. First, the words in
an utterance might affect the prosody. For exam-
ple, the duration and energy of ?bat? are usually
larger than ?bit?. Thus a large amount of data are
required to balance out these differences. Second,
in the MTD corpus, players typically switch to a
picture game by using a yes-no question, such as
?do you have a blue circle?, while most forward
utterances (c.f. Core and Allen 1997) in the ongo-
ing task are statements or proposals. As questions
have very different prosody than statements or pro-
posals, a direct comparison is further biased.
Examination of the MTD corpus found that 86%
(148/172) of the picture games were initiated by
?do you have ...? with optional discourse markers
at the beginning. While in the poker game, players
used ?do you have ...? 108 times to ask whether
the other had certain cards, such as ?do you have a
queen?? This observation inspired us to compare
the prosody of the phrase ?do you have? in switch-
ing to a picture game and during poker-playing.
3
This avoids comparing prosody of different words
or of different types of utterances.
We measure pitch, energy (local root mean
squared measurement), and duration of each case
of ?do you have?. We aggregate on each player and
calculate the average values. The results are shown
in Table 3. The second and third columns show the
average pitch of the phrase ?do you have? for task-
switching (SWT) and poker-playing (PKR) respec-
tively. When switching to a picture game, play-
ers? average pitch is statistically higher than poker-
playing (t(9) = 4.15, p = 0.001). In fact, for each
of the ten players, the average pitch of ?do you
have? in switching to a picture game is higher than
in poker-playing. These results show a strong cor-
relation between task switching and higher pitch.
We next examine the correlation between energy
and task switching. The fourth and fifth columns in
Table 3 show the average energy of the phrase ?do
you have? for task switching and poker-playing re-
spectively. We do not find a statistically significant
difference (t(9) = 0.80, p = 0.44). We also exam-
ine the duration of ?do you have?. The sixth and
3
Note that most cases of ?do you have? in poker-playing
are not at the beginning of a card segment. It would have
also been interesting to compare the prosody of ?do you have?
of initiating a picture game and of initiating a card segment.
However, we do not have enough data for the latter.
Table 3: Average prosodic values for each player
Player pitch (Hz) energy duration (s)
SWT PKR SWT PKR SWT PKR
4A 136 123 383 266 0.28 0.38
4B 178 156 466 506 0.32 0.30
5A 164 152 357 367 0.37 0.25
5B 214 182 231 153 0.36 0.28
6A 144 126 414 370 0.32 0.21
6B 122 117 564 496 0.25 0.23
8A 238 199 973 1061 0.36 0.21
8B 150 143 246 180 0.33 0.35
9A 109 102 538 465 0.44 0.59
9B 125 122 702 814 0.33 0.24
Table 4: Pitch (Hz) and place of switching
Player E C & G PKR
4A 137 131 123
4B 180 173 156
5A 167 161 152
5B 219 206 182
6A 146 143 126
6B 124 121 117
8A 245 233 199
8B 152 140 143
9A 110 108 102
9B 130 117 122
seventh columns in Table 3 show the results. We
do not find a statistically significant difference ei-
ther (t(9) = 1.03, p = 0.33). These results do not
support that energy or duration (i.e. speaking rate)
is correlated to task switching.
5.3 Intensity of Signal
To better understand how pitch is used in signaling
task switching, we next examine whether it corre-
lates with place of switching, i.e., switching at the
end of a game, at the end of a card, or embedded
inside a card segment. Because there are relatively
less data for switching at the end of a game (see
Table 1 and 2), we combine switching at the end
of a game and at the end of a card (C & G) as a
category.
Table 4 shows the average pitch of ?do you
have? when switching to a picture game embedded
inside a card segment, at the end of a card or game
segment, and during poker-playing. The difference
between these three conditions is statistically sig-
nificant (F (2, 9) = 15.61, p < 0.001). Switching
embedded inside a card segment has a statistically
higher pitch than switching at the end of a card or
game segment (t(9) = 5.54, p < 0.001), which
in turn has a statistically higher pitch than during
poker-playing (t(9) = 2.91, p = 0.01).
1030
5.4 Discussion
Consistent with previous research on topic shifts in
single-tasking speech, our experiments show that
switching to a real-time task correlates with the
use of certain discourse markers and prosodic vari-
ations. It is not surprising that ?oh? and ?wait? cor-
relate with task switching. Task switching involves
a sudden change of the conversation topic, and pre-
vious research found that conversants use ?oh? to
mark a change of state in orientation or awareness
(Heritage, 1984). ?Wait? is used to mark a discon-
tinuity in the ongoing topic, which is also required
by task switching. Thus people may use these dis-
course markers to signal switching to a real-time
task. In terms of prosodic variations, we find that
task switching correlates with higher pitch. This
suggests that pitch is used to signal switching to a
real-time task.
Our experiments have also shown that pitch cor-
relates to place of switching. As discussed in Sec-
tion 4.3, task switching embedded inside a card
segment is the most disruptive, switching at the
end of a card is less, and at the end of a game is the
least. Our results show that switching embedded
in a card segment has a higher pitch than switch-
ing at the end of a card or a game, which in turn has
a higher pitch than non-switching (poker-playing).
This suggests that the degree of disruptiveness cor-
responds to the value of pitch: the more disruptive
place to switch, the higher is the pitch.
From our results we speculate that pitch is used
to divert the hearer from the ongoing task, sig-
naling an unexpected event (c.f. (Sussman et al,
2003)). When task switching is more disruptive,
the speaker uses higher pitch; probably because the
hearer has a stronger expectation of the next utter-
ance to be in the context of poker-playing. The
use of higher pitch servers as a cue that the hearer
should suspend the ongoing context and interpret
the utterance in a new context. According to the
theory of least collaborative effort, the effort of
raising the pitch by the speaker is probably to re-
duce the effort of recognizing and processing the
task switching by the hearer (Clark and Wilkes-
Gibbs, 1986).
6 Machine Learning Experiment
In the previous sections, we showed the correlation
of various cues with task switching. In this sec-
tion, we conduct a machine learning experiment to
determine whether we can reliably recognize task
switching using these cues. For the reasons given
in Section 5.2, we limit our experiment to the 256
cases of ?do you have?, 148 for task switching
and 108 for poker playing. We train a decision
tree classifier (C4.5) to discriminate task switching
from poker playing. We use 5-fold cross validation
to evaluate the performance. We use decision tree
learning because its output is interpretable and we
have found its performance comparable to other
discriminative classifiers for this task.
The feature set includes 1) discourse context:
whether the utterance before ?do you have? is the
end of a poker game, the end of a card segment,
or in the middle of a card segment
4
; 2) cue word:
whether the ?do you have? follows the cue word
?oh? or ?wait?; and 3) normalized pitch: the pitch
of ?do you have? divided by the average pitch of
the speaker during the dialogue.
The decision tree learning obtains an accuracy
of 83% in identifying whether a ?do you have? ini-
tiates a task switching or belongs to poker playing;
and the recall, precision, and F measure for task
switching are 90%, 82%, and 86% respectively. As
a baseline, if we blindly assume that all cases of
?do you have? are for task switching, we have an
accuracy of 58%. Thus decision tree learning with
the three features has 43% relative error reduction
over the baseline.
To examine the structure of the decision tree, we
build a single tree from all 256 cases of ?do you
have?. We find that the decision tree first examines
the normalized pitch; if it is greater than 1.085, it is
a task-switch. Otherwise, if the discourse context
is at the end of a game, then it is for task switch-
ing; if the discourse context is embedded in a card
segment, it is for poker playing; if the discourse
context is at the end of a card: if normalized pitch
is higher than 0.975 then it is for task switching,
otherwise for poker playing. Interestingly, the fea-
ture of cue word is not used in the tree.
The performance and structure of the learned
tree suggest that discourse context and normalized
pitch are useful features for discriminating task
switching.
7 Conclusion
In this paper we have described an empirical study
of human-human multi-tasking dialogues, where
people perform multiple verbal tasks overlapped
4
Card and game segments can be determined fairly accu-
rately from the mouse clicks even without the speech.
1031
in time. We first examined the place of task
switching, i.e. where players suspend the ongoing
task and switch to a real-time task. Our analysis
showed that people strive to switch at a less dis-
ruptive place. We then examined the cues to signal
task switching. We found that task switching cor-
relates with certain discourse markers and prosodic
variations. More interestingly, the more disruptive
the switching is, the higher is the pitch. We thus
speculate that pitch is used by the speaker to help
the listener be aware of task switching and under-
stand the utterance. Finally, our machine learn-
ing experiment showed that discourse context and
pitch are useful features to reliably identify task
switching.
Acknowledgement
This work was funded by the National Science
Foundation under IIS-0326496.
References
Ayers, Gayle M. 1992. Discourse functions of pitch
range in spontaneous and read speech. Presented at
the Linguistic Society of America Annual Meeting.
Bangerter, Adrian and Herbert H. Clark. 2003. Nav-
igating joint projects with dialogue. Cognitive Sci-
ence, 27:195?229.
Butterworth, Brian. 1972. Hesitation and semantic
planning in speech. Journal of Psycholinguistic Re-
search, 4:75?87.
Clark, Herbert H. and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognitive Sci-
ence, 22:1?39.
Core, Mark G. and James F. Allen. 1997. Coding
dialogues with the DAMSL annotation scheme. In
Working Notes: AAAI Fall Symposium on Commu-
nicative Action in Humans and Machines, pages 28?
35, Cambridge.
Grosz, Barbara J. and Julia Hirschberg. 1992. Some
intonational characteristics of discourse structure. In
Proceedings of 2nd ICSLP, pages 429?432.
Grosz, Barbara J. and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Heeman, Peter A., Fan Yang, Andrew L. Kun, and
Alexander Shyrokov. 2005. Conventions in
human-human multithreaded dialogues: A prelimi-
nary study. In Proceedings of IUI (short paper ses-
sion), pages 293?295, San Diego CA.
Heritage, John. 1984. A change-of-state token and as-
pects of its sequential placement. In Atldnson, J. M.
and J. Heritage, editors, Structures of social action:
Studies in conversation analysis, chapter 13, pages
299?345. Cambridge University Press.
Hirschberg, Julia and Christine H. Nakatani. 1996. A
prosodic analysis of discourse segments in direction-
giving monologues. In Proceedings of 34th ACL,
pages 286?293.
Kun, Andrew L., W. Thomas Miller, and William H.
Lenharth. 2004. Computers in police cruisers.
IEEE Pervasive Computing, 3(4):34?41, October-
December.
Lemon, Oliver, Alexander Gruenstein, Alexis Battle,
and Stanley Peters. 2002. Multi-tasking and collab-
orative activities in dialogue systems. In Proceed-
ings of 3rd SIGdial, Philadelphia PA.
Moser, Megan and Johanna D. Moore. 1995. Inves-
tigating cue selection and placement in tutorial dis-
course. In Proceedings of 33rd ACL, pages 130?135.
Nakajima, Shin?ya and James F. Allen. 1993. A study
on prosody and discourse structure in cooperative
dialogues. Technical report, Rochester, NY, USA.
technical report.
Passonneau, Rebecca J. and Diane J. Litman. 1997.
Discourse segmentation by human and automated
means. Computational Linguistics, 23(1):103?139.
Schiffrin, Deborah. 1987. Discourse Markers. Cam-
bridge University Press.
Shyrokov, Alexander, Andrew Kun, and Peter Hee-
man. 2007. Experiments modeling of human-
human multi-threaded dialogues in the presence of
a manual-visual task. In Proceedings of 8th SIGdial,
pages 190?193.
Sussman, E., I. Winkler, and E. Schrg?oer. 2003. Top-
down control over involuntary attention switching in
the auditory modality. Psychonomic Bulletin & Re-
view, 10(3):630?637.
Swerts, Marc and Mari Ostendorf. 1995. Discourse
prosody in human-machine interactions. In Proceed-
ings of ESCA workshop on spoken dialogue systems:
theories and applications, pages 205?208, Visgo
Denmark.
Swerts, Marc. 1995. Combining statistical and pho-
netic analyses of spontaneous discourse segmenta-
tion. In Proceedings of the 12th ICPhS, volume 4,
pages 208?211.
Toh, Siew Leng, Fan Yang, and Peter A. Heeman.
2006. An annotation scheme for agreement analy-
sis. In Proceedings of 9th ICSLP, pages 201?204,
Pittsburgh PA.
1032
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 20?21,
Vancouver, October 2005.
DialogueView: an Annotation Tool for Dialogue
Fan Yang and Peter A. Heeman
Center for Spoken Langauge Understanding
OGI School of Science & Engineering
Oregon Health & Science University
20000 NW Walker Rd., Beaverton OR, U.S.A. 97006
{fly, heeman}@cslu.ogi.edu
1 Introduction
There is growing interest in collecting and annotating cor-
pora of language use. Annotated corpora are useful for
formulating and verifying theories of language interac-
tion, and for building statistical models to allow a com-
puter to naturally interact with people.
A lot of annotation tools have been built or are be-
ing built. CSLU Toolkit (Sutton et al, 1998) and Emu
(Cassidy and Harrington, 2001) are built for words tran-
scription or speech events (such as accent); DAT is built
for coding dialogue acts using the DAMSL scheme (Core
and Allen, 1997); Nb is built for annotating hierarchical
discourse structure (Flammia, 1998); annotation toolkits,
such as Mate (McKelvie et al, 2001), AGTK (Bird et al,
2001), and Nite (Carletta et al, 2003), are built for users
to create their own tools. In this demo, we will present a
novel tool, DialogueView, for annotating speech repairs,
utterance boundaries, utterance tags, and hierarchical dis-
course structure altogether.
The annotation tool, DialogueView, consists of three
views: WordView, UtteranceView, and BlockView.
These three views present different abstractions of a di-
alogue, which helps users better understand what is hap-
pening in the dialogue. WordView shows the words time-
aligned with the audio signal. UtteranceView shows the
dialogue as a sequence of utterances. It abstracts away
from the exact timing of the words and can even skip
words, based on WordView annotations, that do not im-
pact the progression of the dialogue. BlockView shows
the dialogue as a hierarchy of discourse blocks, and ab-
stracts away from the exact utterances that were said. An-
notations are done at the view that is most appropriate for
what is being annotated. The tool allows users to eas-
ily navigate among the three views and it automatically
updates all views when changes are made in one view.
DialogueView makes use of multiple views to present
different abstractions of a dialogue to users. Abstraction
helps users focus on what is important for different an-
notation tasks. For example, for annotating speech re-
pairs, utterance boundaries, and overlapping and aban-
doned utterances, WordView provides the exact timing
information. For coding speech act tags and hierarchi-
cal discourse structure, UtteranceView shows a broader
context and hides such low-level details.
In this presentation, we will show how DialogueView
helps users annotate speech repairs, utterance boundaries,
utterance tags, and hierarchical discourse blocks. Re-
searchers studying dialogue might want to use this tool
for annotating these aspects of their own dialogues. We
will also show how the idea of abstraction in Dialogue-
View helps users understand and annotate a dialogue. Al-
though DialogueView focuses on spoken dialogue, we
feel that abstraction can be used in annotating mono-
logues, multi-party, and multi-modal interaction, with
any type of annotations, such as syntactic structure, se-
mantics and co-reference. Researchers might want to
adopt the use of abstraction in their own annotation tools.
2 WordView
The first view is WordView, which takes as input two au-
dio files (one for each speaker), the words said by each
speaker and the start and stop times of each word (in
XML format), and shows the words time-aligned with the
audio signal. This view is ideal for seeing the exact tim-
ing of speech, especially overlapping speech. Users can
annotate speech repairs, utterance boundaries, and utter-
ance tags in WordView.
WordView gives users the ability to select a region of
the dialogue and to play it. Users can play each speaker
channel individually or both combined. Furthermore, Di-
alogueView allows users to aurally verify their speech re-
pair annotations. WordView supports playing a region
of speech but with the annotated reparanda and editing
terms skipped over. We have found this useful in decid-
ing whether a speech repair is correctly annotated. If one
has annotated the repair correctly, the edited speech will
sound fairly natural.
3 UtteranceView
The annotations in WordView are utilized in building the
next view, UtteranceView. This view shows the utter-
ances of two speakers as if it were a script for a movie.
To derive a single ordering of the utterances of the two
20
speakers, we use the start time of each utterance as anno-
tated in WordView. We refer to this process as linearizing
the dialogue (Heeman and Allen, 1995). The order of the
utterances should show how the speakers are sequentially
adding to the dialogue, and is our motivation for defin-
ing utterances as being small enough so that they are not
affected by subsequent speech of the other speaker.
Users can annotate utterance tags in UtteranceView be-
sides WordView. WordView is more suitable for tags that
depend on the exact timing of the words, or a very lo-
cal context, such as whether an utterance is abandoned
or incomplete, or whether there is overlap speech. Utter-
anceView is more suitable for tags that relate the utter-
ance to other utterances in the dialogue, such as whether
an utterance is an answer, a statement, a question, or an
acknowledgment. Whether an annotation tag can be used
in WordView or UtteranceView (or both) is specified in
the configuration file. Which view a tag is used in does
not affect how it is stored in the annotation files (also in
XML format).
In UtteranceView, users can annotate hierarchical
groupings of utterances. We call each grouping a block,
and blocks can have other blocks embedded inside of
them. Each block is associated with a summary, which
users need to fill in. Blocks can be closed; when a block is
closed, it is replaced by its summary, which is displayed
as if it were said by the speaker who initiated the block.
Just as utterances can be tagged, so can discourse blocks.
The block tags scheme is also specified in the configura-
tion file.
UtteranceView supports two types of playback. The
first playback simply plays both channels mixed, which is
exactly what is recorded. The second playback is slightly
different. It takes the linearization into account and dy-
namically builds an audio file in which each utterance
in turn is concatenated together, and a 0.5 second pause
is inserted between each utterance. This gives the user
an idealized rendition of the utterances, with overlapping
speech separated. By comparing these two types of play-
backs, users can aurally check if their linearization of the
dialogue is correct.
Users can use the configuration file to customize Utter-
anceView. Typically, UtteranceView gives users a clean
display of what is going on in a dialogue. This clean
display removes reparanda and editing terms in speech
repairs, and it also removes abandoned speech, which
has no contributions to the conversation.1 UtteranceView
also supports adding texts or symbols to an utterance
based on the tags, such as adding ??? after a question,
?...? after an incomplete utterance, and ?+? at both the
beginning and end of an overlapping utterance to signal
the overlap. (c.f. Childes scheme (MacWhinney, 2000)).
1Note that these clean processes are optional. Users can
specify them in the configuration file.
4 BlockView
In addition to WordView and UtteranceView, we are ex-
perimenting with a third view, which we call BlockView.
This view shows the hierarchical structure of the dis-
course by displaying the summary and intention (DSP)
for each block, indented appropriately. BlockView gives
a very concise view of the dialogue. It is also convenient
for navigating in the dialogue. By highlighting a line and
then pressing Sync, the user can see the corresponding
part of the dialogue in UtteranceView and WordView.
5 Availability
DialogueView is written in Incr Tcl/Tk. We also use the
snack package for audio support; hence DialogueView
supports audio file formats of WAV, MP3, AU, and oth-
ers (see http://www.speech.kth.se/snack/ for the complete
list). DialogueView has been tested on Microsoft Win-
dows (2000 and XP) and Redhat Enterprise Linux.
DialogueView is freely available for research and
educational use. Users should first install a stan-
dard distribution of Tcl/Tk, such as ActiveTcl from
http://www.tcl.tk, and then download DialogueView from
http://www.cslu.ogi.edu/DialogueView. The distribution
also includes some examples of annotated dialogues.
References
Steven Bird et al 2001. Annotation tools based on the anno-
tation graph API. In Proceedings of ACL/EACL 2001 Work-
shop on Sharing Tools and Resources for Research and Edu-
cation.
Jean Carletta et al 2003. The NITE XML toolkit: flexible an-
notation for multi-modal language data. Behavior Research
Methods, Instruments, and Computers, April. Special Issue
on Measuring Behavior.
Steve Cassidy and Jonathan Harrington. 2001. Multi-level an-
notation in the Emu speech database management system.
Speech Communication, 33:61?77.
Mark G. Core and James F. Allen. 1997. Coding dialogues with
the DAMSL annotation scheme. In Proceedings of AAAI Fall
1997 Symposium.
Giovanni Flammia. 1998. Discourse Segmentation Of Spo-
ken Dialogue: An Empirical Approach. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
Peter A. Heeman and James Allen. 1995. Dialogue transcrip-
tion tools. Trains Technical Note 94-1, URCS, March.
Brian MacWhinney. 2000. The CHILDES Project: Tools for
Analyzing Talk. Mahwah, NJ:Lawrence Erlbaum Associates,
third edition.
D. McKelvie, et al 2001. The MATE Workbench - An anno-
tation tool for XML coded speech corpora. Speech Commu-
nication, 33(1-2):97?112. Special issue, ?speech Annotation
and Corpus Tools?.
Stephen Sutton et al. 1998. Universal speech tools: The CSLU
toolkit. In Proceedings of 5th ICSLP, Australia.
21
Proceedings of NAACL HLT 2007, pages 17?24,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Avoiding and Resolving Initiative Conflicts in Dialogue?
Fan Yang and Peter A. Heeman
Center for Spoken Language Understanding
OGI School of Science & Engineering
Oregon Health & Science University
{fly,heeman}@cslu.ogi.edu
Abstract
In this paper, we report on an empirical study
on initiative conflicts in human-human conver-
sation. We examined these conflicts in two
corpora of task-oriented dialogues. The re-
sults show that conversants try to avoid initia-
tive conflicts, but when these conflicts occur,
they are efficiently resolved by linguistic de-
vices, such as volume.
1 Introduction
Current computer dialogue systems tend to be system-
initiative. Although there are some mixed-initiative sys-
tems that allow the user to make a request or state a goal,
such systems are limited in how they follow natural ini-
tiative behavior. An example is where the system always
releases the turn whenever the user barges in. However,
in a complex domain where the computer system and hu-
man user are collaborating on a task, the computer sys-
tem might need to interrupt the human user, or might
even need to fight with the human user over the turn.
Thus the next generation of computer dialogue systems
need a better model of initiative (Horvitz, 1999). In what
situations can the system try to take initiative from the
user? What devices can the system use to fight for ini-
tiative? We propose examining human-human conversa-
tion to answer these questions. Once we understand the
conventions people adopt in negotiating initiative, we can
implement them in a computer dialogue system to create
natural interactivity.
In this research work, we examined two corpora of
human-human conversation: the Trains corpus (Heeman
and Allen, 1995) and the MTD corpus (Heeman et al,
2005). The research purpose is to understand conver-
sants? behavior with initiative conflicts, which we define
a situation where both conversants try to direct the con-
versation at the same time, but one of them fails. We
?This work was funded by the National Science Foundation
under IIS-0326496.
found that (1) conversants try to avoid initiative con-
flicts; and (2) initiative conflicts, when they occur, are
efficiently resolved by linguistic devices, such as volume.
In Section 2, we review related research work on mod-
eling initiative and turn-taking. Dialogue initiative and
turn-taking are two intertwined research topics. When
conversants fight to show initiative, they are also fighting
for the turn to speak. In Section 3, we describe the two
corpora and their annotations. In Section 4, we define
initiative conflict and give an example. In Section 5, we
present the evidence that conversants try to avoid initia-
tive conflicts. In Section 6, we present evidence that ini-
tiative conflicts are efficiently resolved by linguistic de-
vices. We discuss our findings in Section 7 and future
work in Section 8.
2 Related Research
2.1 Initiative Models
Researchers have been investigating how people man-
age dialogue initiative in their conversation. Whittaker
and Stenton (1988) proposed rules for tracking initiative
based on utterance types; for example, statements, pro-
posals, and questions show initiative, while answers and
acknowledgements do not. Smith (1993) proposed four
different initiative strategies with differing amounts of
control by the system. Chu-Carrol and Brown (1998)
distinguished dialogue initiative from task initiative, and
proposed an evidential model of tracking both of them.
Cohen et al (1998) proposed presenting initiative in dif-
ferent strengths. Some researchers related initiative to
discourse structure. Walker and Whittaker (1990) found
a correlation between initiative switches and discourse
segments. Strayer et al (2003) proposed the restricted
initiative model in which the initiator of a discourse seg-
ment, who introduces the discourse segment purpose, is
in control of the segment and shows most of the initia-
tive. These models allowed the possibility that multiple
conversants will want to show initiative at the same time;
however, none of them addressed initiative conflicts.
Guinn (1998) studied another type of initiative, task
initiative, which is about directing the problem-solving
17
of a domain goal. Guinn proposed that the person who
is more capable of coordinating the current goal is the
person who should be leading the dialogue. Initiative
switches between conversants as goals get pushed and
popped from the problem-solving stack. However, be-
cause conversants only have incomplete information, ini-
tiative conflicts might occur when conversants overesti-
mate their own capability or underestimate the other?s.
Guinn proposed a negotiation model to resolve these con-
flicts of task initiative. Conversants negotiate by inform-
ing each other of positive and negative information of
their plans to achieve the goal. By comparing each other?s
plan, the conversant whose plan has the higher probabil-
ity of success takes initiative. Guinn?s research on con-
flicts of task initiative, however, has little bearing on con-
flicts of dialogue initiative. For dialogue initiative, very
often, one of the conversants just gives up the attempt
very quickly, without giving a justification. As stated by
Haller and Fossum (1999):?... conflicts are often simple
clashes that result from both participants trying to take
the initiative at the same time. Such conflicts do not nec-
essarily require complex negotiation to resolve. Often,
unwritten rules based on factors like social roles, personal
assertiveness, and the current locus of control play a part
in determining who will give away.? However, Haller and
Fossum did not further investigate how conversants effi-
ciently resolve conflicts of dialogue initiative.
2.2 Turn-Taking and Initiative
Turn-taking in conversation is highly related to initiative.
Conversants have to possess the turn in order to show ini-
tiative. When conversants are fighting for initiative, they
are also fighting for the turn to speak. Thus the mech-
anisms of turn-taking might share some similarity with
initiative. On the other hand, turn-taking is different from
initiative; for example, an answer takes a turn, but an-
swering does not show initiative.
Turn-taking in conversation has been discussed in lin-
guistics literature. Duncan (1974) examined cues (ges-
ture, acoustic, and linguistic) that conversants use to sig-
nal turn-taking or turn-releasing. A model based on these
signals was created to account for conversants? turn-
taking behavior. In this model, miscues are the cause of
overlapping speech: for example, the hearer misrecog-
nizes the speaker?s cue to keep the turn, or the speaker
fails to properly signal.
Sacks et al (1974) proposed a set of rules for turn-
taking: the current speaker can select somebody else to
speak; otherwise, hearers can self-select to speak; oth-
erwise, the speaker can self-select to speak. This model
suggested that overlapping speech results from either the
hearer waiting too long to speak, or the speaker not wait-
ing long enough.
Schegloff (2000) examined overlapping speech in de-
tail in human conversation. He concluded that (1) fights
for turn are often accompanied with sudden acoustic al-
teration, such as louder volume, higher pitch, and faster
or slower speaking rate; (2) the vast majority of fights for
turn are resolved very quickly; (3) fights for turn are re-
solved through an interactive procedure, e.g. syllable by
syllable negotiation, using devices such as volume, pitch,
and speaking rate. However, his analysis only consisted
of a few examples; no statistical evidence was given. It
is thus unclear whether his conclusions represent human
conventions of initiative conflict, or are occasional behav-
ior that would only occur under special circumstances.
3 Corpora and Annotations
To understand human behavior in initiative conflicts, we
examined two corpora, the Trains corpus and the MTD
corpus. These two corpora have very different domain se-
tups. The distinct behavior seen in each corpus will help
inform us how domain settings affect initiative, while the
common behavior will help inform us the cross-domain
human conventions.
3.1 The Trains Corpus
The Trains corpus is a collection of human-human task-
oriented dialogues, in which two participants work to-
gether to formulate a plan involving the manufacture and
transportation of goods. One participant, the user, has a
goal to solve; and the other participant, the system, knows
the detailed domain information including how long it
takes to ship and manufacture goods.
We annotated eight Trains dialogues totaling about
45 minutes using the tool DialogueView (Yang et al,
2007). We tagged each utterance with a simplified
DAMSL scheme (Core and Allen, 1997). Utterances
were tagged as forward or backward functions, stalls, or
non-contributions. Forward functions include statements,
questions, checks and suggestions. Backward functions
include agreements, answers, acknowledgments, repeti-
tions and completions. Examples of stalls are ?um? and
?let?s see?, used by a conversant to signal uncertainty of
what to say next or how to say it. Non-contributions in-
clude abandoned and ignored utterances. The flow of
the dialog would not change if non-contributions were
removed.
Hierarchical discourse structure was annotated follow-
ing Strayer et al (2003). To determine whether a group
of utterances form a discourse segment, we took into ac-
count whether there exists a shared goal introduced by
one of the conversants (cf. Grosz and Sidner, 1986).
3.2 The MTD Corpus
The MTD corpus contains dialogues in which a pair of
participants play two games via conversation: an ongoing
18
game that takes a relatively long time to finish and an
interruption game that can be done in a couple turns but
has a time constraint. Both games are done on computers.
Players are separated so that they cannot see each other.
In the ongoing game, the two players work together to
assemble a poker hand of a full house, flush, straight, or
four of a kind. Each player has three cards in hand, which
the other cannot see. Players take turns drawing an extra
card and then discarding one until they find a poker hand,
for which they earn 50 points. To discourage players from
simply rifling through the cards to look for a specific card
without talking, one point is deducted for each picked-up
card, and ten points for a missed or incorrect poker hand.
To complete this game, players converse to share card
information, and explore and establish strategies based
on the combined cards in their hands.
From time to time, the computer generates a prompt
for one player to start an interruption game to find out
whether the other player has a certain picture on the
screen. The interruption game has a time constraint of
10, 25, or 40 seconds, which is (pseudo) randomly deter-
mined. Players get five points for the interruption game
if the correct answer is given in time. Players are told to
earn as many points as possible.
We annotated six MTD dialogues totaling about 90
minutes. Utterances were segmented based on player?s
intention so that each utterance has only one dialogue
act that is to share information, explore strategies, sug-
gest strategies, or maintain an established strategy (Toh
et al, 2006). We applied the same simplified DAMSL
scheme on utterance tag annotations. Figure 1 shows an
annotated excerpt of an MTD dialogue. We grouped ut-
terances into blocks. Block b21 is a game block in which
conversants completed a poker hand. Blocks b22 and b23
are two card blocks in which conversants picked up a
new card, discussed what they had in hand, and chose
a card to discard. Block b24 is an interruption segment
in which conversants switched their conversation to the
interruption game. No claim is made that the game and
card blocks are discourse segments according to Grosz
and Sidner?s definition (1986).
4 Defining Initiative Conflicts
An initiative conflict occurs when a conversant?s attempt
to show initiative fails because someone else is show-
ing initiative at the same time. Following Whittaker
and Stenton (1988), we use utterance tags to determine
whether an utterance shows initiative: forward functions
show initiative while others do not. Non-contributions
are viewed as failed attempt to show initiative. Thus we
identify initiative conflicts as overlapping utterances that
involve either a forward function and a non-contribution
or two non-contributions.
Figure 2 gives an example of an initiative conflict from
Figure 1: An excerpt of an MTD dialogue
the MTD corpus. The top conversant says ?that?s pair of
threes and pair of fours?, which ends at time point A. Af-
ter a short pause, at time B, the bottom conversant asks
?how many threes do you have?, which is overlapped by
the top conversant?s second utterance ?I?ll drop? at time
C. The top conversant then abandons the attempt of show-
ing initiative at time D. Hence the bottom speaker is the
winner of this initiative conflict.
We use the term preceding-pause to refer to the time
interval between the end of the previous utterance and
the first utterance that is involved in the overlap (from A
to B in Figure 2). Offset refers to the interval between
the start times of the two overlapped utterances (from B
to C). Duration refers to the time interval from the begin-
ning of overlap till the end of overlap (from C to D).
In the Trains corpus, there are 142 cases of overlap-
ping speech, 28 of which are initiative conflicts. Of the
remaining, 96 cases involve a backward function (e.g. an
acknowledgment overlapping the end of an inform), and
10 cases involve a stall. The remaining 8 cases are other
types of overlap, such as a collaborative completion, or
conversants talking about the same thing: for example,
one saying ?we are a bit early? and the other saying ?we
are a little better?.
In the MTD corpus, there are 383 cases of overlapping
speech, 103 of which are initiative conflicts. Of the re-
maining, 182 cases involve a backward function, 21 cases
involve a stall, and 77 cases are others. Initiative conflicts
19
Figure 2: An illustration of an initiative conflict
are more frequent in the MTD corpus (103 cases in 90
min) than in the Trains corpus (28 cases in 45 min).
There are three cases in the Trains and thirteen cases in
the MTD corpus where the preceding-pause is negative,
i.e. the first overlapped utterance is started before the
other conversant finishes the previous utterance. Some-
times the hearer starts a little bit early to take the turn. If
the original speaker does not intend to release the turn,
a conflict arises. Because these cases involve three ut-
terances, we exclude them from our current analysis and
save them for future research.1 This leaves 25 cases in
the Trains corpus and 90 cases in the MTD corpus for
analyzing initiative conflicts.
5 Avoiding Initiative Conflicts
In this section, we show that conversants try to avoid ini-
tiative conflicts by examining both the offset of initiative
conflicts and the urgency levels.
5.1 Offset of Initiative Conflicts
The offset of an initiative conflict indicates where the
conflict happens. A short offset indicates that the conflict
happens at the beginning of an utterance, while a long
offset indicates an interruption in the middle.
Figure 3 shows the cumulative distribution function
(CDF) for offsets for both corpora individually. The mean
offset is 138ms for the Trains corpus, and 236ms for
the MTD corpus. In comparison to the average length
of forward utterances (2596ms in the Trains corpus and
1614ms in the MTD corpus), the offset is short. More-
over, in the Trains corpus, 88% of offsets are less than
300ms (and 80% less than 200ms); in the MTD corpus,
75% of offsets are less than 300ms. Thus most initiative
conflicts happen at the beginning of utterances.
1These cases of negative value preceding-pause are in fact
very interesting. They seem to contradict with Sacks et
al. (1974)?s model that the hearer has priority to self select to
speak. If Sacks et al is correct, the speaker should wait a cer-
tain amount of time in order not to overlap with the hearer, but
in these cases we see that the speaker self-selects to speak with-
out taking into account whether the hearer self-selects to speak
or not.
0 500 1000 15000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Offset (ms)
Perc
enta
ge
A: Trains
0 500 1000 15000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Offset (ms)
Perc
enta
ge
B: MTD
Figure 3: CDF plot for offsets of initiative conflicts
Few initiative conflicts have offsets longer than 500ms.
There is one instance in the Trains corpus and eleven in
the MTD corpus. Four cases are because the second con-
versant has something urgent to say. For example, when
an interruption game is timing out, conversants would in-
terrupt, sometimes in the middle of an utterance, which
results in a long offset. Another six cases are due to mis-
cues. Figure 4 shows an example. Conversant B said ?I
have two aces? with end-of-utterance intonation, paused
for about half a second, and then added ?and a seven?.
The ending intonation and the pause probably misled
conversant A to believe that B had finished, and thus A
started a new forward utterance, which overlapped with
B?s extension. A?s utterance was then quickly abandoned.
In these cases, it is ambiguous whether B?s utterance ?I
have two aces ... and a seven? should be further chopped
into two utterances. The final two cases are intrusions,
with an example shown in Figure 5. Conversant A cut in
probably because he was confident with his decision and
wanted to move on to the next card. In such cases, the
intruder might be perceived as being rude.
20
B: I have two aces and a seven
A: I have .
Figure 4: Long offset: miscue
B: well let?s just
A: it?s no help I think it goes away
Figure 5: Long offset: intrusion
The preponderance of short offsets provides evidence
that conversants try to avoid initiative conflicts. When A
detects that B is talking, A should not attempt to show
initiative until the end of B?s utterance in order to avoid
conflicts, unless there is an urgent reason. If conversants
did not take into account whether someone else is speak-
ing before attempting initiative, we would see a lot of in-
trusions in the middle of utterances, which in fact rarely
happen in the two corpora. As we have shown, initiative
conflicts tend to happen at the beginning of utterances.
Thus initiative conflicts occur mainly due to unintentional
collision, i.e. both conversants happen to start speaking
almost at the same time. The fact that the offset of most
initiative conflicts is within 300ms confirms this.2
5.2 Urgency Level and Initiative Conflicts
To further support the hypothesis that conversants avoid
initiative conflicts except for urgent reasons, we exam-
ined the MTD corpus for the correlation between the ur-
gency levels of the interruption game and initiative con-
flicts. For the urgency level of 10 seconds, conversants
started 33 interruption games, 8 of which were intro-
duced via initiative conflicts. For 25 seconds, conversants
started 36 interruption games, 5 introduced via initiative
conflicts. For 40 seconds, conversants started 33 interrup-
tion games, 3 introduced via initiative conflicts. Thus the
percentages of initiative conflicts for the three urgency
levels are 24% for 10 seconds, 14% for 25 seconds, and
9% for 40 seconds. The urgency level of 10 seconds
requires conversants to start the interruption game very
quickly in order to complete it in time. On the other hand,
the urgency level of 40 seconds allows conversants ample
time to wait for the best time to start the game (Heeman
et al, 2005). Thus we see the percentage of initiative
conflicts decreases as it becomes less urgent to the inter-
ruption game. These results suggest that conversants try
to avoid initiative conflicts if they can, unless there is an
urgent reason.
6 Resolving Initiative Conflicts
In this section, we present evidence that initiative con-
flicts, if they occur, are resolved very quickly using sim-
ple devices.
2This 300ms might be related to human reaction time.
0 500 1000 1500 2000 2500 3000 3500 40000
0.2
0.4
0.6
0.8
1
Length(ms)
Perc
enta
ge
A: Trains
Duration of initiative conflictsLength of forward utterances
0 500 1000 1500 2000 2500 3000 3500 40000
0.2
0.4
0.6
0.8
1
Length(ms)
Perc
enta
ge
B: MTD
Duration of initiative conflictsLength of forward utterances
Figure 6: CDF plot for durations of initiative conflicts
together with lengths of forward utterances
6.1 Duration of Initiative Conflicts
The duration of an initiative conflict, as defined in Sec-
tion 4, indicates how quickly the conflict is resolved. Fig-
ure 6 shows the cumulative distribution function of dura-
tions of initiative conflicts and the lengths of forward ut-
terances in the two corpora. The mean duration is 328ms
in the Trains corpus and 427ms in the MTD corpus. From
Figure 6 we see that the duration is much shorter than the
length of forward utterances, which have the mean length
of 2596ms in the Trains corpus and 1614ms in the MTD
corpus. The difference between duration of initiative con-
flicts and length of forward utterances is statistically sig-
nificant (p < 10?5, ttest). On average, the duration of
initiative conflicts is about 1/8 the length of forward ut-
terances in the Trains corpus and about 1/4 in the MTD
corpus. The short durations suggest that initiative con-
flicts are resolved very quickly.
According to Crystal and House (1990), the average
length of CVC syllable is about 250ms. Thus on aver-
age, the length of initiative conflicts is about one to two
syllables.3 In fact, 96% of conflicts in the Trains corpus
and 73% in the MTD corpus are resolved within 500ms.
These observations are consistent with one of Schelogff?s
(2000) claims about turn-taking conflicts, that they usu-
ally last less than two syllables to resolve.
6.2 Resolution of Initiative Conflicts
From our definition of initiative conflict, at least one of
the speakers has to back off. For expository ease, we re-
3It would be interesting to examine the length of initiative
conflicts based on syllable. However currently we do not have
syllable-level alignment for the two corpora. We leave this for
future research.
21
fer to the person who gets the turn to contribute as the
winner, and the other who fails as the yielder. There are
two cases in the Trains corpus and three cases in the MTD
corpus in which both speakers abandoned their incom-
plete utterances, paused for a while, and then one of them
resumed talking. These five cases are treated as ties: no
winners or yielders, and are excluded from our analysis
here.
Given how quickly initiative conflicts are resolved, we
examined whether the resolution process might be depen-
dent on factors presented before the conflict even begins,
namely who was speaker in the previous utterance, and
who was interrupted. If we predict that the conversant
who spoke prior to the conflict (speaker of u262 in Fig-
ure 2) loses, we get 55% accuracy in the Trains corpus
and 61% accuracy in the MTD corpus. If we predict
the conversant who spoke first in the overlap (speaker of
u263 in Figure 2) wins, we get 60% accuracy in the Trains
corpus and 53% accuracy in the MTD corpus. These low
percentages suggest that they are not robust predictors.
We next examined how conversants resolve the con-
flicts using devices such as volume, pitch, and others.
6.2.1 Volume
For a stretch of speech, volume is calculated as the mean
energy of the spoken words. For each initiative conflict,
we calculated each conversant?s volume during the over-
lap, and then normalized it with respect to the conver-
sant?s volume throughout the whole conversation.4 We
refer to this as relative volume. In the Trains corpus, the
average relative volume of the winner is 1.06; the average
relative volume of the yielder is 0.93. The difference is
statistically significant (P < 0.01, anova). In the MTD
corpus, the average relative volume of the winner is 1.12;
the average relative volume of the yielder is 0.98. The dif-
ference is also statistically significant (p < 10?6, anova).
These results show that the winner is the one speaking at
a higher relative volume.
To strengthen our argument, we also calculated volume
ratio as the relative volume of the winner divided by the
yielder. The average volume ratio in the Trains corpus is
1.16 and in the MTD corpus is 1.18. If a classifier always
chooses the speaker with higher relative volume to be the
winner, we achieve about 79% accuracy in both corpora,
which is a 29% absolute improvement over random pre-
diction. These results further confirm that the conversant
who speaks at a higher relative volume wins the initiative
conflicts.
Given the importance of volume in the resolution pro-
cess, we examined whether it has an impact on the du-
ration of initiative conflicts. Figure 7 plots the relation
4Normalization is necessary particularly as conversants
heard each other via headsets, and the microphones were not
calibrated to have exactly the same gains.
0 200 400 600 800 1000 1200 1400 16000.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
Duration of initiative conflicts (ms)
Volu
me 
ratio
 False prediction
Figure 7: Volume ratio and duration of conflicts
between volume ratio and duration of conflicts for all
the cases in the two corpora. For reference, the dot-
ted line divides the data points into two groups: under
the line are what volume ratio fails to predict the win-
ner, and above the line are success. If we look at the
points where volume ratio succeeds, we see that when
duration of initiative conflicts is long, volume ratio tends
to be small: in fact, the average volume ratio for initiative
conflicts shorter than 600ms is 1.27; for long than 600ms
is 1.13; and the difference is statistically significant (ttest,
p < 0.01).
To further understand how volume is used in the reso-
lution procedure, we examined how volume changes dur-
ing the overlap. For initiative conflicts whose duration is
longer than 600ms, we cut the overlapped speech evenly
in half, and calculated the relative volume for each half
individually. For the first half, the average relative vol-
ume of the winner is 1.03, and the yielder is 1.02. The
difference is not statistically significant (p = 0.93, paired
ttest). For the second half, the average relative volume of
the winner is 1.20, and the yielder is 1.02. The difference
is statistically significant (p < 0.001, paired ttest). The
fact that these long initiative conflicts are not resolved in
the first half is probably partially due to the close relative
volume.
We then calculated volume increment as subtracting the
relative volume of the first half from the second half. The
average volume increment of the winner is 0.17; the aver-
age volume increment of the yielder is 0. The difference
is statistically significant (p < 0.001, paired ttest). These
results show that the range of volume increment during
the overlap by the winner is larger than the yielder. The
behavior of increasing volume during overlap to win the
fight suggests that conversants use volume as a device to
resolve initiative conflicts.
22
6.2.2 Pitch
We used the tool WaveSurfer (Sjo?lander and Beskow,
2000) to extract the f0 from the audio files. We calcu-
lated relative pitch similarly as we did for volume.
In the Trains corpus, the average relative pitch of the
winner is 1.02; the average relative pitch of the yielder
is 0.96. The difference is not statistically significant
(P = 0.54, anova). In the MTD corpus, the average
relative pitch of the winner is 1.09; the average relative
pitch of the yielder is 0.98. The difference is statistically
significant (p < 0.001, anova). If we choose the speaker
with higher pitch to be the winner, we achieve about 65%
accuracy in the Trains corpus and 62% in the MTD cor-
pus. These results suggest that pitch alone is not robust
for predicting the winner of initiative conflicts, at least
not as predictive as volume, although we do see the ten-
dency of higher pitch by the winner.
We also examined pitch range in the window of 100ms
and 300ms respectively. We calculated the pitch range
of the overlapping speech and then normalized it with
respect to the conversant?s pitch range throughout the
whole conversation. We did not see a significant corre-
lation between pitch range and the winner of initiative
conflicts. Thus pitch does not seem to be a device for
resolving initiative conflicts.
6.2.3 Role of Conversants
Human-computer dialogues often have a user interact-
ing with a system, in which the two have very different
roles. Hence, we investigated whether the conversant?s
role has an effect in how initiative conflicts are resolved.
We focused on the Trains corpus due to both its rich dis-
course structure and the difference in the roles that the
system and the use have.
In the Trains corpus, if we predict that the initiator of
a discourse segment wins the conflicts, we get 65% ac-
curacy. In system-initiated segments, the system wins all
eight conflicts; however, in user-initiated segments, the
user only wins seven and system wins eight. The user
does not have an advantage during initiative conflicts in
its segments. Moreover, if the initiator had an advantage,
we would expect the system to have fought more strongly
in the user-initiated segments in order to win. However,
we do not see that the relative volume of the system win-
ning in user-initiated segments is statistically higher than
in system-initiated segments in this small sample size
(p = 0.9, ttest). The initiator does not seem to have a
privileged role in the resolution process.
From the above analysis, we see that the system wins
the conflicts 16 out of 23 times. Thus if we predict that
the system always wins the conflicts, we achieve 70%
accuracy. This is not surprising because the system has
all the domain information, and is more experienced in
solving goals. If the system and user want to speak at
the same time, both would know that the system proba-
bly has a more significant contribution. That the system
wins most of the initiative conflicts agrees with Guinn
(1998) that capability plays an important role in deter-
mining who to show initiative next.
7 Discussion
In this paper, we present our empirical study of human
behavior in initiative conflicts. Our first finding is that
conversants try to avoid initiative conflicts. The conse-
quence of initiative conflicts is that at least one of the
conversants would have to back off, which makes their
effort of contributing in vain. Moreover, the effort of
resolving initiative conflicts is overhead to the dialogue.
According to the theory of least collaborative effort by
Clark and Wilkes-Gibbs (1986), it only makes sense for
conversants to interrupt when the loss of not interrupting
is higher than the cost of an initiative conflict. Thus the
theory of least collaborative effort is consistent with our
conclusion that most initiative conflicts are unintentional
collisions, except where conversants interrupt in the mid-
dle of an utterance for urgency reasons.
The second finding of our research is that initiative
conflicts, when they occur, are efficiently resolved. We
found that volume plays an important role: the louder
speaker wins. We also show how conversants change
their volume to resolve initiative conflicts. Conversants
probably identify their eagerness of speaking, confidence
in what they want to say, and capability of achieving the
current goal by means of volume, which resolves the ini-
tiative conflicts very quickly.
Domain settings obviously have an impact on conver-
sants? initiative behavior. There are more frequent initia-
tive conflicts in the MTD corpus than in the Trains cor-
pus. Moreover, the roles of the conversants also affect
their initiative behavior as we found that the system wins
more initiative conflicts in the Trains corpus. In a teacher-
student conversation, one would expect to see that the
teacher interrupts the student more often than vice versa,
but also that the teacher wins more initiative conflicts.
Capability, culture, and social relationship probably are
some underlying elements that influence when and under
what conditions conversants would seek initiative, while
volume is a device for resolving initiative conflicts.
8 Future Work
In this paper we focused on initiative conflicts in dialogue
where two conversants cannot see each other. In face-to-
face conversation, there might be other cues, such as eye-
contact, head-nodding, and hand gesture, that conversants
use in initiative conflicts. Moreover, in a multi-party con-
versation, a conversant might talk to different people on
different topics, and get interrupted from time to time,
23
which leads to an initiative conflict involving multiple
speakers. In our future work, we plan to examine ini-
tiative conflicts in face-to-face multi-party conversation,
such as the ICSI corpus (Shriberg et al, 2004).
Inspired by the findings on human behavior of initia-
tive conflicts, we speculate that conversants might also
have a mechanism to even minimize unintentional ini-
tiative conflicts, which probably includes devices such
as volume, pause, and other prosodic features. The
speaker uses these devices, as opposed to explicitly in-
forming each other of their knowledge to evaluate capa-
bility (Guinn, 1998), to implicitly signal his or her ea-
gerness, confidence and capability. The hearer then com-
pares his or her own eagerness with the speaker?s, and
decides whether to just make an acknowledgement (al-
lowing the speaker to continue the lead) or to take over
the initiative when taking the turn to speak. In our future
work, we plan to build an initiative model to capture this
negotiation process.
References
Jennifer Chu-Carroll and Michael K. Brown. 1998. An
evidential model for tracking initiative in collabora-
tive dialogue interactions. User Modeling and User
Adapted Interaction, 8:215?253.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognitive Science,
22:1?39.
Robin Cohen, C. Allaby, C. Cumbaa, M. Fitzgerald,
K. Ho, B. Hui, C. Latulipe, F. Lu, N. Moussa, D. Poo-
ley, A. Qian, and S. Siddiqi. 1998. What is initiative?
User Modeling and User Adapted Interaction, 8:171?
214.
Mark G. Core and James F. Allen. 1997. Coding dia-
logues with the DAMSL annotation scheme. In Work-
ing Notes: AAAI Fall Symposium on Communicative
Action in Humans and Machines, pages 28?35, Cam-
bridge.
Thomas H. Crystal and Arthur S. House. 1990. Articula-
tion rate and the duration of syllables and stress groups
in connected speech. Journal of Acoustical Society of
America, 88:101?112.
Starkey Duncan. 1974. On the structure of speaker-
auditor interaction during speaking turns. Language
in Society, 2:161?180.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Curry I. Guinn. 1998. An analysis of initiative selection
in collaborative task-oriented discourse. User Model-
ing and User Adapted Interaction, 8:255?314.
Susan Haller and Timothy Fossum. 1999. Using pro-
tocols to model mixed initiative interaction. In Pro-
ceedings of AAAI Workshop on Mixed Initiative Intel-
ligence.
Peter A. Heeman and James F. Allen. 1995. The Trains
spoken dialogue corpus. CD-ROM, Linguistics Data
Consortium.
Peter A. Heeman, Fan Yang, Andrew L. Kun, and
Alexander Shyrokov. 2005. Conventions in human-
human multithreaded dialogues: A preliminary study.
In Proceedings of Intelligent User Interface (short pa-
per session), pages 293?295, San Diego CA.
Eric Horvitz. 1999. Principles of mixed-initiative user
interfaces. In Proceedings of CHI, pages 159?166,
Pittsburgh PA.
Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson.
1974. A simplest systematics for the organization of
turn-taking for conversation. Language, 50(4):696?
735.
Emanuel A. Schegloff. 2000. Overlapping talk and
the organization of turn-taking for conversation. Lan-
guage in Society, 29:1?63.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004. The ICSI meeting recorder dialog act corpus. In
Proceedings of the 5th SIGdial Workshop on Discourse
and Dialogue.
Ka?re Sjo?lander and Jonas Beskow. 2000. WaveSurfer:
An open source speech tool. In Proceedings of ICSLP,
pages 4:464?467, Beijing China.
Ronnie W. Smith. 1993. Effective spoken natural lan-
guage dialogue requires variable initiative behavior:
an empirical study. In AAAI93 Fall Symposium On
Human-Computer Collaboration.
Susan E. Strayer, Peter A. Heeman, and Fan Yang. 2003.
Reconciling control and discourse structure. In J. Van
Kuppevelt and R. W. Smith, editors, Current and New
Directions in Discourse and Dialogue, chapter 14,
pages 305?323. Kluwer Academic Publishers.
Siew Leng Toh, Fan Yang, and Peter A. Heeman. 2006.
An annotation scheme for agreement analysis. In Pro-
ceedings of INTERSPEECH, Pittsburgh PA.
Marilyn Walker and Steve Whittaker. 1990. Mixed ini-
tiative in dialogue: An investigation into discourse seg-
mentation. In Proceedings of 28th ACL.
Steve Whittaker and Phil Stenton. 1988. Cues and con-
trol in expert-client dialogue. In Proceedings of 28th
ACL, pages 123?130.
Fan Yang, Peter A. Heeman, Kristy Hollingshead, and
Susan E. Strayer. 2007. Dialogueview: Annotating
dialogues in multiple views with abstraction. Natural
Language Engineering. To appear.
24
DialogueView: An Annotation Tool for Dialogue
Peter A. Heeman and Fan Yang and Susan E. Strayer
Computer Science and Engineering
OGI School of Science and Engineering
Oregon Health & Science University
20000 NW Walker Rd., Beaverton OR, 97006
heeman@cse.ogi.edu yangf@cse.ogi.edu susan strayer@yahoo.com
Abstract
This paper describes DialogueView, a
tool for annotating dialogues with utter-
ance boundaries, speech repairs, speech
act tags, and discourse segments. The
tool provides several views of the data,
including a word view that is time-
aligned with the audio signal, and an ut-
terance view that shows the dialogue as
if it were a script for a play. The ut-
terance view abstracts away from lower
level details that are coded in the word
view. This allows the annotator to have
a simpler view of the dialogue when cod-
ing speech act tags and discourse struc-
ture, but still have access to the details
when needed.
1 Introduction
There is a growing interest in annotating human-
human dialogue. Annotated dialogues are useful
for formulating and verifying theories of dialogue
and for building statistical models. In addition to
orthographic word transcription, one might want
the following dialogue annotations.
 Annotation of the speech repairs. Speech re-
pairs are a type of disuency where speakers
go back and change or repeat something they
just said.
 Segmentation of the speech of each speaker
into utterance units, with a single ordering of
the utterances. We refer to this as linearizing
the dialogue (Heeman and Allen, 1995a).
 Each utterance tagged with its speech act
function.
 The utterances grouped into hierarchical dis-
course segments.
There are tools that address subsets of the above
tasks. However, we feel that doing dialogue an-
notation is very di?cult. Part of this di?culty
is due to the interactions between the annotation
tasks. An error at a lower level can have a large
impact on the higher level annotations. For in-
stance, there can be ambiguity as to whether an
\okay" is part of a speech repair; this will im-
pact the segmentation of the speech into utter-
ance units and the speech act coding. Sometimes,
it is only by considering the higher level annota-
tions that one can make sense of what is going on
at the lower levels, especially when there is over-
lapping speech. Hence, a tool is needed that lets
users examine and code the dialogue at all lev-
els. The second reason why dialogue annotation
is di?cult is because it is di?cult to follow what
is occurring in the dialogue, especially for coding
discourse structure. A dialogue annotation tool
needs to help the user deal with this overload.
In this paper, we describe our dialogue anno-
tation tool, DialogueView. This tool displays the
dialogue at three dierent levels of abstraction.
The word level shows the words time-aligned with
the audio signal. The utterance level shows the
dialogue as a sequence of utterance, as if it were a
script for a play. It abstracts away from the exact
timing of the words and even skips words that do
not impact the progression of the dialogue. The
block level shows the dialogue as a hierarchy of
discourse segment purposes, and abstracts away
from the exact utterances that were said. Anno-
tations are done at the view that is most appropri-
ate for what is being annotated. The tool allows
the user to easily navigate between the three views
and automatically updates the higher level views
when changes are made in the lower level views.
Because the higher levels abstract away lower level
details, it is easier for the user to understand what
is happening in the dialogue.
1
Yet, the user can
easily refer to the lower level views to see what
1
This approach bears resemblance to the work of
Jonsson and Dahlback (2000) in which they distill
human-human dialogues by removing those parts that
would not occur in human-computer dialogue. They
do this to create training data for their spoken dia-
logue systems.
       Philadelphia, July 2002, pp. 50-59.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
actually occurred when necessary.
In the rest of this paper, we rst discuss other
annotation tools. We then describe the three lev-
els of abstraction in our annotation tool. We then
discuss audio playback. Next, we discuss how the
tool can be customized for dierent annotation
schemes. Finally, we discuss the implementation
of the tool.
2 Existing Tools
There are a number of tools that let users annotate
speech audio les. These include Emu (Cassidy
and Harrington, 2001), SpeechView (Sutton et al,
1998) and Waves+ (Ent, 1993). These tools often
allow multiple text annotation les to be associ-
ated with the waveform and allow users an easy fa-
cility to capture time alignments. For instance, for
the ToBI annotation scheme (Pitrelli et al, 1994),
one can have the word alignment in one text le,
intonation features in a second, break indices in a
third, and miscellaneous features in a fourth. The
audio annotation tools often have powerful signal
analysis packages for displaying such phenomena
as spectrograms and voicing. These tools, how-
ever, do not have any built-in facility to group
words into utterances nor group utterances into
hierarchical discourse segments.
Several tools allow users to annotate higher
level structure in dialogue. The annotation tool
DAT from the University of Rochester (Fergu-
son, 1998) allows users to annotate utterances or
groups of utterances with a set of hard-coded an-
notation tags for the DAMSL annotation scheme
(Allen and Core, 1997; Core and Allen, 1997).
The tool Nb from MIT (Flammia, 1995; Flammia,
1998) allows users to impose a hierarchical group-
ing on a sequence of utterances, and hence anno-
tate discourse structure. Both DAT and Nb take
as input a linearization of the speaker utterances.
Mistakes in this input cannot be xed. Whether
there are errors or not, users cannot see the exact
timing interactions between the speakers' words or
the length of pauses. This simplication can make
it di?cult for the annotator to truly understand
what is happening in the dialogue, especially for
overlapping speech, where speakers ght over the
turn or make back-channels.
The tools Transcriber (Barras et al, 2001) and
Mate (McKelvie et al, 2001) allow multiple views
of the data: a word view with words time-aligned
to the audio signal and an utterance view. How-
ever, Transcriber is geared to single channel data
and has a weak ability to handle overlapping
speech and Mate only allows one view to be shown
at a time. The author of a competing tool has
remarked that \speed and stability of [Mate] are
both still problematic for real annotation. Also,
the highly generic approach increases the initial
eort to set up the tool since you basically have
to write your own tool using the Mate style-sheet
language" (Kipp, 2001). Hence, he developed a
new tool Anvil that he claims is a better trade-o
among generality, functionality, and complexity.
This tool oers multi-modal annotation support,
but like Transcriber, does not allow detailed an-
notation of dialogue phenomena, such as overlap-
ping speech and abandoned speech, and has no
abstraction mechanism.
3 Word View
Our dialogue annotation tool, DialogueView,
gives the user three views of the data. The low-
est level view is called WordView and takes as
input the words said by each speaker and their
start and stop times and shows them time-aligned
with the audio signal. Figure 1 shows an ex-
cerpt from the Trains corpus (Heeman and Allen,
1995b) in WordView. This view is ideal for view-
ing the exact timing of speech, especially overlap-
ping speech. As will be discussed below, we use
it for segmenting the speech into utterances, and
annotating communicative status and speech re-
pairs.
2
These annotations will allow us to build a
simpler representation of what is happening in the
speech for the utterance view, which is discussed
in the next section.
3.1 Utterance Segmentation
As can be seen in Figure 1, WordView shows the
words segmented into utterances, which for our
purpose is simply a grouping of consecutive words
by a single speaker, in which there is minimal
eect from the other speaker. Consider the ex-
change in Figure 1, where the upper speaker says
\and then take the remaining boxcar down to" fol-
lowed by the lower speaker saying \right, to du-",
followed by the upper speaker saying \Corning".
Although one could consider the upper speaker's
speech as one utterance, we preclude that due to
the interaction from the lower speaker. A full def-
inition of `utterance' is beyond the scope of this
paper, and is left to the users to specify in their
annotation scheme.
Utterance boundaries in WordView are only
shown by their start times. The end of the utter-
ance is either the start of the next utterance by the
2
There currently is no support for changing the
word transcription. There are already a number of
tools that do an excellent job at this task. Hence,
adding this capability is a low priority for us.
Figure 1: Utterance Segmentation in WordView
same speaker or the end of the le. With Word-
View, the user can easily move, insert or delete
utterance boundaries. The tool ensures that the
boundaries never fall in the middle of a word by
the speaker.
The start times of the utterances are used to
derive a single ordering of the utterances of the
two speakers. This linearization of the dialogue
captures how the speakers are sequentially adding
to the dialogue. The linearization is used by the
next view to give an abstraction away from the
exact timing of the speech.
3.2 Communicative Status
WordView allows features of the utterances to be
annotated. In practice, however, we only anno-
tate features related to its communicative status,
based on the DAMSL annotation scheme (Allen
and Core, 1997). Below, we give the utterance
tags that we assign in WordView.
Abandoned: The speaker abandoned what
they were saying and it had no impact on the rest
of the dialogue. This mainly happens where one
speaker loses the ght over who speaks next. Fig-
ure 2 gives an example where the upper speaker
tries to take the turn by saying \takes," and than
abandons this eort.
Incomplete: The speaker did not make a com-
plete utterance, either prosodically, syntactically,
or semantically. Unlike the previous category, the
partial utterance did impact the dialogue behav-
ior. Figure 1 gives two examples. The rst is
where the upper speaker says \and than take the
remaining boxcar down to", and the second is
where the lower speaker said \to du-," which was
than completed by the upper speaker.
Overlap: The speech in the utterance overlaps
with the speech from the prior utterance. For
instance, in Figure 1, the lower speaker utters
\okay" during the middle of an utterance, perhaps
to tell the upper speaker that they are understand-
ing everything so far. However, a simple lineariza-
tion would make it seem that the \okay" is an
acknowledgment of the entire utterance, which is
not the case. Hence, we tag the \okay" utterance
with the overlap tag. The next view, Utterance-
View, will take the overlap tag into account in
displaying the utterances, as will be discussed in
Section 4.3.
Not all overlapping speech needs to be anno-
tated with the overlap tag. In Figure 1, the sec-
ond instance of \Corning" overlaps slightly with
the rst instance of \Corning". However, viewing
it sequentially does not alter the analysis of the
exchange.
3.3 Reordering Overlapping Speech
Consider the example in Figure 2. Before the ex-
cerpt shown, the lower speaker had just nished
an utterance and then paused for over a second.
The upper speaker then acknowledged the utter-
ance with \okay", but this happened a fraction of
a second after the bottom speaker started speak-
ing again. A simple linearization of the dialogue
would have the \okay" following the wrong stretch
of speech|\and than th(at)- takes w- what three
hours." A solution to this would be to anno-
Figure 2: Utterance Ordering and Speech Repairs in WordView
tate the \okay" with the overlap tag. However,
this \Okay" is more similar to the overlapping in-
stances of \Corning" in Figure 1. The fact that
\Okay" overlaps with the start of the next utter-
ance is not critical to understanding what is oc-
curring in the dialogue, as long as we linearize
the \Okay" to occur before the other utterance.
WordView allows the user to change the lineariza-
tion by moving the start times of utterances. This
can be done provided that the speaker was silent
in the time interval preceding where the other per-
son started talking.
In summary, overlapping speech can be handled
in three ways. The utterance can be explicitly
tagged as overlapping; the overlap can be ignored
if it is not critical in understanding what is going
on in the dialogue; or the start time of the utter-
ance can be changed so that the overlap does not
need to be tagged.
3.4 Speech Repairs
WordView also allows users to annotate speech
repairs. A speech repair is where a user goes back
and repeats or changes something that was just
said (Heeman and Allen, 1999). Below we give an
example of a speech repair and show its principle
components: reparandum, interruption point, and
editing term.
Example 1
why don't we take
|{z}
reparandum
"
ip
um
|{z}
et
take two boxcars
The reparandum is the speech that is being re-
placed, the interruption point is the end of the
reparandum, and the editing term consists of
words such as \um", \uh", \okay", \let's see" that
help signal the repair.
To annotate a repair, the user highlights a se-
quence of words and then tags it as a reparandum
or an editing term of a repair. The user can also
specify the type of repair. Figure 2 shows how
speech repairs are displayed in WordView. The
words in the reparandum and editing term are un-
derlined and displayed in a special color.
3.5 Speech Repairs and Utterances
Some phenomena can be marked as either a speech
repair, or could be marked using the utterance
tags of incomplete or abandon. This is especially
true for fresh starts (Heeman and Allen, 1999),
where a speaker abandons the current utterance
and starts over. To avoid having multiple ways of
annotating the same phenomena, we impose the
following restrictions in our annotation scheme.
 There cannot be an utterance boundary in-
side of a reparandum, inside of an editing
term, at the interruption point, nor at the
end of the editing term. Hence, something
annotated as a reparandum cannot also be
annotated as an abandoned utterance.
 Abandoned or incomplete utterances cannot
be followed by an utterance by the same
speaker.
 All word fragments must either be the last
word of a reparandum or the last word of an
utterance that is marked as abandoned or in-
complete.
Figure 3: UtteranceView: Segmented Utterances in UtteranceView
 Abandoned or incomplete utterances can end
with an editing term, which would be marked
as the editing term of an abridged repair.
3.6 Summary
There are a number of reasons why we annotate
utterance boundaries, speech repairs, and commu-
nicative status in WordView. Annotating utter-
ance boundaries and overlapping speech requires
the user to take into account the exact timing of
the utterances, which is best done in this view.
Speech repairs also require ne tuned listening to
the speech and have strong interactions with ut-
terance boundaries. Furthermore, all three types
of annotations can be used to build a simpler view
of what is happening in the dialogue, as will be ex-
plained in the next section.
4 Utterance View
The annotations from the word view are used to
build the next view, which we refer to as Utter-
anceView. The dialogue excerpts from Figures 1
and 2 are shown in the utterance view in Figures 3
and 4, respectively. The utterance view abstracts
away from the detailed timing information and in-
dividual words that were spoken. Instead, it fo-
cuses on the sequence of utterances between the
speakers. By removing details that were anno-
tated in the word view, we still preserve the im-
portant details that are needed to annotate speech
act types for the utterances and to annotate dis-
course segments. Of course, if the user wants to
see the exact timing of the words in the utter-
ances, they can examine the word view, as it is
Figure 4: UtteranceView: Reordered and Abandoned Utterances in UtteranceView
displayed alongside the utterance view. There are
also navigation buttons on each view that allow
the user to easily reposition the portion of the di-
alogue in the other view. Furthermore, changes
made in the word view are immediately propa-
gated into the utterance view, and hence the user
will immediately see the impact of their annota-
tions.
4.1 Utterance Ordering
Utterance ordering in the utterance view is deter-
mined by the start times of the utterances as spec-
ied in WordView. As was explained earlier, alter-
ing the start time of an utterance can be used to
simplify some cases of overlapping speech, where
the overlap is not critical to understanding the
role of the utterance in the dialogue. Figure 2
gave the word view of such an example. Rather
than code it as an overlap, we moved the start
time of the \okay" utterance so that it precedes
the overlapping speech by the other speaker. Fig-
ure 4 shows how this looks in the utterance view.
Here, the annotator would view the \okay" as an
acknowledgment that occurred between the two
utterances of the lower speaker.
4.2 Speech Repairs
In the word view, the user annotates the reparan-
dum and editing term of speech repairs. If the
reparandum and editing term are removed, the
resulting utterance reects what the speaker in-
tended to say. Speech repairs do carry informa-
tion.
 Their occurrence can signal a lack of certainty
of the speaker.
 The reparandum of a repair can have an
anaphoric reference, as in \Peter was, well
he was red."
However, removing the reparandum and editing
term of speech repairs from utterances in the ut-
terance view leads to a simpler representation of
what is occurring in the dialogue. Hence, in the
utterance view, we clean up the speech repairs, as
shown in Figures 2 and 4. Figure 2, which shows
the word view, contains the utterance \and then
th(at) that takes w- what three hours"; whereas
Figure 4, which shows the utterance view, con-
tains \and then that takes what three hours." Of
course, a user can always refer to the word view
when annotating in the utterance view if they
want to see the exact speech that was said. In
most cases, we feel that this will not be neces-
sary for annotating speech act tags and discourse
segments.
4.3 Communicative Status
The communicative status coded in the word view
is used in formatting the utterance view. Utter-
ances tagged as overlapping are indented and dis-
played with `+' on either side, as shown in Fig-
ure 3. Utterances tagged as abandoned are not
shown, as can be seen in Figure 4, in which the
abandoned utterance \takes" made by the upper
speaker is not included. Utterances tagged as in-
complete are shown with a trailing \..." as shown
in Figure 3.
4.4 Annotating Utterance Tags
In the utterance view, one can also annotate the
utterances with various tags. For our work, we
use a subset of the DAMSL tags corresponding
to forward and backward functions (Allen and
Core, 1997). Forward functions include state-
ment, request information, and suggestion. Back-
ward functions include answer, acknowledgment,
and agreement. Although these utterance tags
could be annotated in the word view, doing it in
the utterance view allows us to see more context,
which is needed to give the utterance the proper
tags. When necessary, the annotator can easily
refer to the word view to see the exact local con-
text.
4.5 Annotating Blocks of Utterances
In the utterance view, the user can also annotate
hierarchical groupings of utterances.
3
We use the
utterance blocks to annotate discourse structure
(Grosz and Sidner, 1986). This is similar to what
Flammia's tool allows (Flammia, 1995). Rather
than showing it with indentation and color, we
draw boxes around segments. Figure 3 shows a
dialogue excerpt with three utterance blocks in-
side of a larger block. To create a segment, the
user highlights a sequence of utterances and then
presses the \make segment" button. The user can
change the boundaries of the blocks by simply
dragging either the top or bottom edge of the box.
Blocks can also be deleted. The tool ensures that
if two blocks have utterances in common then one
block is entirely contained in the other.
Tags can also be assigned to the blocks. We
have just started using the tool for discourse seg-
mentation, and so we are still rening these tags.
In Grosz and Sidner's theory of discourse struc-
ture (1986), the speaker who initiates the block
does so to accomplish a certain purpose. We have
a tag for annotating this purpose. We also have
tags to categorize the block as a greeting, spec-
ify goal, construct plan, summarize plan, verify
plan, give info, request info, or other (Strayer and
Heeman, 2001).
The utterance view also allows the user to open
or close a block. When a block is open (the de-
fault), all of its utterances and sub-blocks are dis-
3
We do not allow segments to be interleaved. It is
unclear if such phenomena actually occur.
played. When it is closed, its utterances and sub-
blocks are replaced by the single line purpose.
Opening and closing blocks is useful as it allows
the user to control the level of detail that is shown.
Consider the third embedded block shown in Fig-
ure 3, in which the conversants take seven utter-
ances to jointly make a suggestion. After we have
analyzed it, we can close it and just see the pur-
pose. This will make it easier to determine the
segmentation of the blocks that contain it.
We are experimenting with a special type of
dialogue block. Consider the example from the
previous paragraph, in which the conversants
take seven utterances to jointly make a sugges-
tion. This is related to the shared turns of
Schirin (1987), the co-operative completions of
Linell (1998), and the grounding units of Traum
and Nakatani (1999). We are experimenting with
how to support the annotation of such phenom-
ena. We have added a tag to indicate whether the
utterances in the block are being used to build a
single contribution. For these single contributions,
we also supply a concise paraphrase of what was
said. We have found that this paraphrase can be
built from a sequential subset of the words in the
utterances of the block. For instance, the para-
phrase of our example block is \and then take the
remaining boxcar down to Corning."
5 Block View
We are experimenting with a third view of the
dialogue. This view, which we refer to as Block-
View, abstracts away from the individual utter-
ances, and shows the hierarchical structure of the
discourse segments. This gives a very concise view
of the dialogue. The block view is also convenient
for it provides an index to the whole dialogue.
This allows the user to quickly move around the
dialogue.
6 Audio Playback
Each view gives the user the ability to select a re-
gion of the dialogue and to play it. In the word
view, the user can play each speaker channel indi-
vidually or both combined.
4
This ability is espe-
cially useful for overlapping speech, where the an-
notator would want to listen to what each speaker
said individually, as well as hear the timing be-
tween the speaker utterances.
Just as each view provides a visual abstraction
from the previous one, we also do the same with
audio playback. In the word view, which has
4
In order to play each speaker individually, we re-
quire a separate audio channel for each speaker.
wordViewUtt => atmostoneof abandoned incomplete overlap
uttViewUtt => anyof forward backward comment
uttViewUtt.forward => oneof statement question suggestion other
uttViewUtt.backward => oneof agreement understanding answer other
uttViewUtt.comment => other
Figure 5: Sample Specication of Utterance Tags
Figure 6: Sample Utterance Annotation Panel
the speech repairs annotated, the user can play
back the speech cleanly of either speaker, where
the stretches of speech annotated as the reparan-
dum or editing term of a repair are skipped. We
have found this to be of great assistance in verify-
ing if something should be annotated as a repair
or not. It gives us an audio means to verify the
speech repair annotations. If we have annotated
the repair correctly, the edited audio signal should
sound fairly natural.
In formatting the utterance view, we take into
account whether utterances have been marked as
abandoned or overlapped. We provide a special
playback in the utterance view that takes this
into account. We build an audio le in which we
skip over repairs, skip over abandoned speech, and
shorten large silences. If there is overlap that is
not marked as signicant, we linearize it by con-
catenating the utterances together. If the overlap
is marked as signicant, we keep the overlap. We
are nding that this gives us an audio means to
ensure that our markings of abandonment, over-
lap and our linearization is correct.
We are also experimenting with even further
simplifying the audio output. For blocks that have
a paraphrase, and the block is closed, we play the
paraphrase by constructing it from the words said
in the block. For blocks that are closed that do
not have a paraphrase, we use the text-to-speech
engine in the CSLU toolkit (Colton et al, 1996;
Sutton et al, 1997) to say the purpose, as if there
was a narrator.
7 Customizations
Some aspects of the tool are built in, such as the
notion of utterances, speech repairs, and hierar-
chical grouping of utterances into blocks. How-
ever, the annotations of these phenomena and how
they are displayed can be customized through a
conguration le. This allows us to easily exper-
iment as we revise our annotation scheme; to use
domain specic tags; and to make the tool useful
for other researchers who might use dierent tags.
Speech repair tags, utterance tags, and block
tags are specied in the conguration le. Fig-
ure 5 gives a sample of how the annotation
tags for an utterance are specied. The two top
level entries in the gure are \wordViewUtt" and
\uttViewUtt", which specify the utterance anno-
tation tags in WordView and UtteranceView, re-
spectively. The decomposition can be of one of
three types.
atmostoneof: at most one of the attributes can
be specied
oneof: exactly one of the attributes must be spec-
ied
anyof: there is no restriction on which attributes
can be specied
The subcomponents can either be terminals as is
the case for the decomposition of \wordViewUtt",
or can be non-terminals, as is the case for each
of the three subcomponents of \uttViewUtt".
Hence, hierarchical tags are supported. Termi-
nals are assumed to be of binary type, except for
\other", which is assumed to be a string. The
conguration le determines how the annotation
panel is generated. For the annotation scheme
specied in Figure 5, Figure 6 shows the annota-
tion panel that would be automatically generated
for the utterance view.
As we explained earlier, some of the utterance
tags aect how the word view and utterance view
are formatted. Rather than hard code this func-
tionality, it is specied in the conguration le.
We are still experimenting with the best way to
code this functionality. Figure 7 gives an exam-
ple of how we code the utterance tag function-
ality. The rst line indicates that the utterance
wordViewUtt.abandoned do wordView color red
wordViewUtt.abandoned uttView ignore
wordViewUtt.incomplete wordView color yellow
wordViewUtt.incomplete uttView trailsoff
wordViewUtt.overlap wordView color blue
wordViewUtt.overlap uttView overlap
Figure 7: Sample Utterance Display Specication
tag of \abandoned" coded in WordView should
be displayed in red in WordView. The second line
indicates that it should not be displayed in Utter-
anceView.
8 Implementation
DialogueView is written in Tcl/Tk. We also use
utilities from the CSLU Speech Toolkit (Colton
et al, 1996; Sutton et al, 1997), including audio
and wave handling and speech synthesis. We have
rewritten the tool to use an object-oriented exten-
sion of Tcl called Tclpp, designed and developed
by Stefan Simnige. This is allowing us to better
manage the growing complexity of the tool as well
as reuse pieces of the software in our annotation
comparison tool (Yang et al, 2002). It should also
help in expanding the tool so that it can handle
any number of speakers.
9 Conclusion
In this paper, we described a dialogue annotation
tool that we are developing for segmenting dia-
logue into utterances, annotating speech repairs,
tagging speech acts, and segmenting dialogue into
hierarchical discourse segments. The tool presents
the dialogue at dierent levels of abstraction al-
lowing the user to both see in detail what is go-
ing on and see the higher level structure that is
being built. The higher levels not only abstract
away from the exact timing, but also can skip over
words, whole utterances, and even simplify seg-
ments to a single line paraphrase. Along with the
visual presentation, the audio can also be played
at these dierent levels of abstraction. We feel
that these capabilities should help annotators bet-
ter code dialogue.
This tool is still under active development. In
particular, we are currently rening how blocks
are displayed, improving the ability to customize
the tool for dierent tagsets, and improving the
audio playback facilities. As we develop this tool,
we are also doing dialogue annotation, and ren-
ing our scheme for annotating dialogue in order to
better capture the salient features of dialogue and
improve the inter-coder reliability.
10 Acknowledgments
The authors acknowledgment funding from the In-
tel Research Council.
References
James F. Allen and Mark G. Core. 1997. Damsl:
Dialog annotation markup in several layers.
Unpublished Manuscript.
Claude Barras, Edouard Georois, Zhibiao Wu,
and Mark Liberman. 2001. Transcriber: devel-
opment and use of a tool for assisting speech
corpora production. Speech Communications,
33:5{22.
Steve Cassidy and Jonathan Harrington. 2001.
Multi-level annotation in the Emu speech
database management system. Speech Commu-
nications, 33:61{77.
Don Colton, Ron Cole, David G. Novick, and
Stephen Sutton. 1996. A laboratory course
for designing and testing spoken dialogue sys-
tems. In Proceedings of the International Con-
ference on Audio, Speech and Signal Processing
(ICASSP), pages 1129{1132.
Mark G. Core and James F. Allen. 1997. Coding
dialogs with the DAMSL annotation scheme.
In Working notes of the AAAI Fall Symposium
on Communicative Action in Humans and Ma-
chines.
Entropic Research Laboratory, Inc., 1993.
WAVES+ Reference Manual. Version 5.0.
George Ferguson. 1998. DAT: Dialogue annota-
tion tool. Available from www.cs.rochester.edu
in the subdirectory research/speech/damsl.
Giovanni Flammia. 1995. N.b.: A graphical user
interface for annotating spoken dialogue. In
AAAI Spring Symposium on Empirical Meth-
ods in Discourse Interpretation and Generation,
pages pages 40{46, Stanford, CA.
Giovanni Flammia. 1998. Discourse segmenta-
tion of spoken dialogue: an empirical approach.
Doctoral dissertation, Department of Electrical
and Computer Science, Massachusetts Institute
of Technology.
Barbara J. Grosz and Candace L. Sidner. 1986.
Attention, intentions, and the structure of dis-
course. Computational Linguistics, 12(3):175{
204.
Peter A. Heeman and James Allen. 1995a. Dia-
logue transcription tools. Trains Technical Note
94-1, Department of Computer Science, Univer-
sity of Rochester, March. Revised.
Peter A. Heeman and James F. Allen. 1995b. The
Trains spoken dialog corpus. CD-ROM, Lin-
guistics Data Consortium, April.
Peter A. Heeman and James F. Allen. 1999.
Speech repairs, intonational phrases and dis-
course markers: Modeling speakers' utterances
in spoken dialog. Computational Linguistics,
25(4):527{572.
Arne Jonsson and Nils Dahlback. 2000. Distill-
ing dialogues | a method using natural di-
alogue corpora for dialogue systems develop-
ment. In Proceedings of the 6th Applied Natural
Language Processing Conference, pages 44{51,
Seattle.
Michael Kipp. 2001. Anvil: A generic annotation
tool for multimodal dialogue. In Proceedings of
the 7th European Conference on Speech Com-
munication and Technology (Eurospeech).
Per Linell. 1998. Approaching Dialogue: Talk,
Interaction and Contexts in Dialogical Perspec-
tives. John Benjamins Publishing.
David McKelvie, Amy Isard, Andreas Mengel,
Morten Baun Muller, Michael Grosse, and Mar-
ion Klein. 2001. The MATE workbench | an
annotation tool for XML coded speech corpora.
Speech Communications, 33:97{112.
John F. Pitrelli, Mary E. Beckman, and Ju-
lia Hirschberg. 1994. Evaluation of prosodic
transcription labeling reliability in the ToBI
framework. In Proceedings of the 3rd Interna-
tional Conference on Spoken Language Process-
ing (ICSLP-94), Yokohama, September.
Deborah Schirin. 1987. Discourse Markers.
Cambridge University Press, New York.
Susan E. Strayer and Peter A. Heeman. 2001.
Dialogue structure and mixed initiative. In
Second workshop of the Special Interest Group
on Dialogue, pages 153{161, Aalborg Denmark,
September.
Stephen Sutton, Ed Kaiser, Andrew Cronk, and
Ronald Cole. 1997. Bringing spoken language
systems to the classroom. In Proceedings of the
5th European Conference on Speech Commu-
nication and Technology (Eurospeech), Rhodes,
Greece.
S. Sutton, R. Cole, J. de Villiers, J. Schalkwyk,
P. Vermeulen, M. Macon, Y. Yan, E. Kaiser,
R. Rundle, K. Shobaki, P. Hosom, A. Kain,
J. Wouters, M. Massaro, and M. Cohen. 1998.
Universal speech tools: the cslu toolkit. In Pro-
ceedings of the 5th International Conference on
Spoken Language Processing (ICSLP-98), pages
3221{3224, Sydney Australia, November.
David R. Traum and Christine H. Nakatani. 1999.
A two-level approach to coding dialogue for dis-
course structure: Activities of the 1998 working
group on higher-level structures. In Proceed-
ings of the ACL'99 Workshop Towards Stan-
dards and Tools for Discourse Tagging, pages
101{108, June.
Fan Yang, Susan E. Strayer, and Peter A. Hee-
man. 2002. ACT: a graphical dialogue anno-
tation comparison tool. Submitted for publica-
tion.
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 33?40,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
A Data Driven Approach to Relevancy Recognition for
Contextual Question Answering
Fan Yang?
OGI School of Science & Engineering
Oregon Health & Science University
fly@cslu.ogi.edu
Junlan Feng and Giuseppe Di Fabbrizio
AT&T Labs - Research
180 Park Avenue, Florham Park, NJ, 07932 - USA
junlan@research.att.com, pino@research.att.com
Abstract
Contextual question answering (QA), in
which users? information needs are satis-
fied through an interactive QA dialogue,
has recently attracted more research atten-
tion. One challenge of engaging dialogue
into QA systems is to determine whether
a question is relevant to the previous inter-
action context. We refer to this task as rel-
evancy recognition. In this paper we pro-
pose a data driven approach for the task
of relevancy recognition and evaluate it
on two data sets: the TREC data and the
HandQA data. The results show that we
achieve better performance than a previ-
ous rule-based algorithm. A detailed eval-
uation analysis is presented.
1 Introduction
Question Answering (QA) is an interactive
human-machine process that aims to respond
to users? natural language questions with exact
answers rather than a list of documents. In the
last few years, QA has attracted broader research
attention from both the information retrieval
(Voorhees, 2004) and the computational linguistic
fields (http://www.clt.mq.edu.au/Events/
Conferences/acl04qa/). Publicly ac-
cessible web-based QA systems, such as
AskJeeves (http://www.ask.com/) and START
(http://start.csail.mit.edu/), have scaled up
?The work was done when the first author was visiting
AT&T Labs - Research.
this technology to open-domain solutions. More
task-oriented QA systems are deployed as virtual
customer care agents addressing questions about
specific domains. For instance, the AT&T Ask
Allier agent (http://www.allie.att.com/) is
able to answer questions about the AT&T plans
and services; and the Ikea ?Just Ask Anna!? agent
(http://www.ikea.com/ms/en US/) targets ques-
tions pertaining the company?s catalog. Most of
these QA systems, however, are limited to answer
questions in isolation. The reality is that users often
ask questions naturally as part of contextualized
interaction. For instance, a question ?How do I
subscribe to the AT&T CallVantager service?? is
likely to be followed by other related questions
like ?How much will the basic plan cost?? and
so on. Furthermore, many questions that users
frequently want answers for cannot be satisfied with
a simple answer. Some of them are too complicated,
broad, narrow, or vague resulting that there isn?t a
simple good answer or there are many good answer
candidates, which entails a clarification procedure
to constrain or relax the search. In all these cases,
a question answering system that is able to answer
contextual questions is more favored.
Contextual question answering as a research chal-
lenge has been fostered by TREC (Text Retrieval
Conference) since 2001. The TREC 2001 QA track
made the first attempt to evaluate QA systems? abil-
ity of tracking context through a series of questions.
The TREC 2004 re-introduced this task and orga-
nized all questions into 64 series, with each series
focusing on a specific topic. The earlier questions
in a series provide context for the on-going ques-
tion. However, in reality, QA systems will not be
33
informed about the boundaries between series in ad-
vance.
One challenge of engaging dialogue into QA sys-
tems is to determine the boundaries between topics.
For each question, the system would need to deter-
mine whether the question begins a new topic or it
is a follow-up question related to the current exist-
ing topic. We refer to this procedure as relevancy
recognition. If a question is recognized as a follow-
up question, the next step is to make use of context
information to interpret it and retrieve the answer.
We refer to this procedure as context information fu-
sion. Relevancy recognition is similar to text seg-
mentation (Hearst, 1994), but relevancy recognition
focuses on the current question with the previous
text while text segmentation has the full text avail-
able and is allowed to look ahead.
De Boni and Manandhar (2005) developed a rule-
based algorithm for relevancy recognition. Their
rules were manually deduced by carefully analyzing
the TREC 2001 QA data. For example, if a question
has no verbs, it is a follow-up question. This rule-
based algorithm achieves 81% in accuracy when rec-
ognizing the question relevance in the TREC 2001
QA data set. The disadvantage of this approach is
that it involves a good deal of human effort to re-
search on a specific data set and summarize the rules.
For a new corpus from a different domain, it is very
likely that one would have to go over the data set and
modify the rules, which is time and human-effort
consuming. An alternative is to pursue a data driven
approach to automatically learn the rules from a data
set. In this paper, we describe our experiments of
using supervised learning classification techniques
for the task of relevancy recognition. Experiments
show that machine learning approach achieves better
recognition accuracy and can also be easily applied
to a new domain.
The organization of this paper is as follows. In
Section 2, we summarize De Boni and Manandhar?s
rule-based algorithm. We present our learning ap-
proach in Section 3. We ran our experiments on
two data sets, namely, the TREC QA data and the
HandQA data, and give the results in Section 4. In
section 5, we report our preliminary study on con-
text information fusion. We conclude this paper in
Section 6.
2 Rule-Based Approach
De Boni and Manandhar (2005) observed the fol-
lowing cues to recognize follow-up questions:
? Pronouns and possessive adjectives. For exam-
ple, if a question has a pronoun that does not re-
fer to an entity in the same sentence, this ques-
tion could be a follow-up question.
? Cue words, such as ?precisely? and ?exactly?.
? Ellipsis. For example, if a question is not syn-
tactically complete, this question could be a
follow-up question.
? Semantic Similarity. For example, if a ques-
tion bears certain semantic similarity to previ-
ous questions, this question might be a follow-
up question.
De Boni and Manandhar (2005) proposed an
algorithm of calculating the semantic similar-
ity between the current question Q and a pre-
vious question Q?. Supposed Q consists of a
list of words (w1, w2, ..., wn) and Q? consists
of (w?1, w?2, ..., w?m):
SentenceSimilarity(Q, Q?) (1)
=
?
1?j?n
( max
1?i?m
WordSimilarity(wj , w?i))
The value of WordSimilarity(w, w?) is the
similarity between two words, calculated from
WordNet (Fellbaum, 1998). It returns a value
between 0 (w and w? have no semantic rela-
tions) and 1 (w and w? are the same).
Motivated by these observations, De Boni and
Manandhar (2005) proposed the rule-based algo-
rithm for relevancy recognition given in Figure 1.
This approach can be easily mapped into an hand-
crafted decision tree. According to the algorithm,
a question follows the current existing topic if it (1)
contains reference to other questions; or (2) contains
context-related cue words; or (3) contains no verbs;
or (4) bears certain semantic similarity to previous
questions or answer. Evaluated on the TREC 2001
QA context track data, the recall of the algorithm
is 90% for recognizing first questions and 78% for
follow-up questions; the precision is 56% and 76%
respectively. The overall accuracy is 81%.
34
Given the current question Qi and a sequence of history ques-
tions Qi?n, ..., Qi?1:
1. If Qi has a pronoun or possessive adjective which has
no references in the current question, Qi is a follow-up
question.
2. If Qi has cue words such as ?precisely? or ?exactly?, Qi
is a follow-up question.
3. If Qi does not contain any verbs, Qi is a follow-up ques-
tion.
4. Otherwise, calculate the semantic similarity measure of
Qi as
SimilarityMeasure(Qi)
= max
1?j?n
f(j) ? SentenceSimilarity(Qi, Qi?j)
Here f(j) is a decay function. If the similarity measure is
higher than a certain threshold, Qi is a follow-up ques-
tion.
5. Otherwise, if answer is available, calculate the semantic
distance between Qi and the immediately previous an-
swer Ai?1: SentenceSimilarity(Qi, Ai?1). If it is
higher than a certain threshold, Qi is a follow-up ques-
tion that is related to the previous answer.
6. Otherwise, Qi begins a new topic.
Figure 1: Rule-based Algorithm
3 Data Driven Approach
3.1 Decision Tree Learning
As a move away from heuristic rules, in this paper,
we make an attempt towards the task of relevancy
recognition using machine learning techniques. We
formulate it as a binary classification problem: a
question either begins a new topic or follows the
current existing topic. This classification task can
be approached with a number of learning algorithms
such as support vector machines, Adaboost and arti-
ficial neural networks. In this paper, we present our
experiments using Decision Tree. A decision tree
is a tree in which each internal node represents a
choice between a number of alternatives, and each
leaf node represents a decision. Learning a decision
tree is fairly straightforward. It begins from the root
node which consists of all the training data, growing
the tree top-down by recursively splitting each node
based on maximum information gain until certain
criteria is met. Although the idea is simple, decision
tree learning is often able to yield good results.
3.2 Feature Extraction
Inspired by De Boni and Manandhar?s (2005) work,
we selected two categories of features: syntactic fea-
tures and semantic features. Syntactic features cap-
ture whether a question has certain syntactic compo-
nents, such as verbs or pronouns. Semantic features
characterize the semantic similarity between the cur-
rent question and previous questions.
3.2.1 Syntactic Features
As the first step, we tagged each question with
part-of-speech tags using GATE (Cunningham et al,
2002), a software tool set for text engineering. We
then extracted the following binary syntactic fea-
tures:
PRONOUN: whether the question has a pronoun
or not. A more useful feature would be to la-
bel whether a pronoun refers to an entity in the
previous questions or in the current question.
However, the performances of currently avail-
able tools for anaphora resolution are quite lim-
ited for our task. The tools we tried, includ-
ing GATE (Cunningham et al, 2002), Ling-
Pipe (http://www.alias-i.com/lingpipe/)
and JavaRAP (Qiu et al, 2004), tend to use
the nearest noun phrase as the referents for pro-
nouns. While in the TREC questions, pronouns
tend to refer to the topic words (focus). As a
result, unsupervised anaphora resolution intro-
duced more noise than useful information.
ProperNoun: whether the question has a proper
noun or not.
NOUN: whether the question has a noun or not.
VERB: whether the question has a verb or not.
DefiniteNoun: if a question has a definite noun
phrase that refers to an entity in previous ques-
tions, the question is very likely to be a follow-
up question. However, considering the diffi-
culty in automatically identifying definite noun
phrases and their referents, we ended up not us-
ing this feature in our training because it in fact
introduced misleading information.
3.3 Semantic Features
To compute the semantic similarity between two
questions, we modified De Boni and Manandhar?s
formula with a further normalization by the length
of the questions; see formula (2).
35
SentenceSimilarity(Q, Q?) (2)
= 1n
?
1?j?n
( max
1?i?m
WordSimilarity(wj , w?i))
This normalization has pros and cons. It removes
the bias towards long sentences by eliminating the
accumulating effect; but on the other hand, it might
cause the system to miss a related question, for ex-
ample, when two related sentences have only one
key word in common.1
Formula (2) shows that sentence level similarity
depends on word-word similarity. Researchers have
proposed a variety of ways in measuring the seman-
tic similarity or relatedness between two words (to
be exact, word senses) based on WordNet. For ex-
ample, the Path (path) measure is the inverse of
the shortest path length between two word senses
in WordNet; the Wu and Palmer?s (wup) measure
(Wu and Palmer, 1994) is to find the most spe-
cific concept that two word senses share as ances-
tor (least common subsumer), and then scale the
path length of this concept to the root note (sup-
posed that there is a virtual root note in WordNet)
by the sum of the path lengths of the individual
word sense to the root node; the Lin?s (lin) mea-
sure (Lin, 1998) is based on information content,
which is a corpus based measure of the specificity of
a word; the Vector (vector) measure associates each
word with a gloss vector and calculates the similar-
ity of two words as the cosine between their gloss
vectors (Patwardhan, 2003). It was unclear which
measure(s) would contribute the best information to
the task of relevancy recognition, so we just exper-
imented on all four measures, path, wup, lin, and
vector, in our decision tree training. We used Peder-
sen et al?s (2004) tool WordNet::Similarity to com-
pute these four measures. WordNet::Similarity im-
plements nine different measures of word similar-
ity. We here only used the four described above be-
cause they return a value between 0 and 1, which
is suitable for using formula (2) to calculate sen-
tence similarity, and we leave others as future work.
Notice that the WordNet::Similarity implementation
1Another idea is to feed the decision tree training both the
normalized and non-normalized semantic similarity informa-
tion and see what would come out. We tried it on the TREC data
and found out that the normalized features actually have higher
information gain (i.e. appear at the top levels of the learned tree.
can only measure path, wup, and lin between two
nouns or between two verbs, while it uses all the
content words for the vector measure. We thus have
the following semantic features:
path noun: sentence similarity is based on the
nouns2 similarity using the path measure.
path verb: sentence similarity is based on the non-
trivial verbs similarity using the path measure.
Trivial verbs include ?does, been, has, have,
had, was, were, am, will, do, did, would, might,
could, is, are, can, should, shall, being?.
wup noun: sentence similarity is based on the
nouns similarity using the Wu and Palmer?s
measure.
wup verb: sentence similarity is based on the
non-trivial verbs similarity using the Wu and
Palmer?s measure.
lin noun: sentence similarity is based on the nouns
similarity using the Lin?s measure.
lin verb: sentence similarity is based on the non-
trivial verbs similarity using the Lin?s measure.
vector: sentence similarity is based on all content
words (nouns, verbs, and adjectives) similarity
using the vector measure.
4 Results
We ran the experiments on two sets of data: the
TREC QA data and the HandQA data.
4.1 Results on the TREC data
TREC has contextual questions in 2001 context
track and 2004 (Voorhees, 2001; Voorhees, 2004).
Questions about a specific topic are organized into a
session. In reality, the boundaries between sessions
are not given. The QA system would have to rec-
ognize the start of a new session as the first step of
question answering. We used the TREC 2004 data
as training and the TREC 2001 context track data as
testing. The training data contain 286 factoid and list
questions in 65 sessions3; the testing data contain 42
questions in 10 sessions. Averagely each session has
about 4-5 questions. Figure 2 shows some example
questions (the first three sessions) from the TREC
2001 context track data.
2This is to filter out all other words but nouns from a sen-
tence for measuring semantic similarity.
3In the TREC 2004 data, each session of questions is as-
signed a phrase as the topic, and thus the first question in a ses-
sion might have pronouns referring to this topic phrase. In such
cases, we manually replaced the pronouns by the topic phrase.
36
CTX1a Which museum in Florence was damaged by a
major bomb explosion in 1993?
CTX1b On what day did this happen?
CTX1c Which galleries were involved?
CTX1d How many people were killed?
CTX1e Where were these people located?
CTX1f How much explosive was used?
CTX2a Which industrial sector supplies the most
jobs in Toulouse?
CTX2b How many foreign companies were based there
in 1994?
CTX2c Name a company that flies there.
CTX3a What grape variety is used in Chateau Petrus
Bordeaus?
CTX3b How much did the future cost for the 1989
Vintage?
CTX3c Where did the winery?s owner go to college?
CTX3d What California winery does he own?
Figure 2: Example TREC questions
4.1.1 Confusion Matrix
Table 1 shows the confusion matrix of the deci-
sion tree learning results. On the testing data, the
learned model performs with 90% in recall and 82%
in precision for recognizing first questions; for rec-
ognizing follow-up questions, the recall is 94% and
precision is 97%. In contrast, De Boni and Man-
andhar?s rule-based algorithm has 90% in recall and
56% in precision for recognizing first questions; for
follow-up questions, the recall is 78% and precision
is 96%. The recall and precision of our learned
model to recognize first questions and follow-up
questions are all better than or at least the same
as the rule-based algorithm. The accuracy of our
learned model is 93%, about 12% absolute improve-
ment from the rule-based algorithm, which is 81% in
accuracy. Although the size of the data is too small
to draw a more general conclusion, we do see that
the data driven approach has better performance.
Training Data
Predicted Class
True Class First follow-up Total
First 63 2 65
follow-up 1 220 221
Total 64 222 286
Testing Data
Predicted Class
True Class First follow-up Total Recall
First 9 1 10 90%
follow-up 2 30 32 94%
Total 11 31 42
Precision 82% 97%
Table 1: Confusion Matrix for TREC Data
4.1.2 Trained Tree
Figure 3 shows the first top two levels of the tree
learned from the training data. Not surprisingly,
PRONOUN turns out to be the most important fea-
ture which has the highest information gain. In the
TREC data, when there is a pronoun in a question,
the question is very likely to be a follow-up ques-
tion. In fact, in the TREC 2004 data, the referent
of pronouns very often is the topic phrase. The fea-
ture path noun, on the second level of the trained
tree, turns out to contribute most information in this
recognition task among the four different semantic
similarity measures. The similarity measures using
wup, wup noun and wup verb, and the vector mea-
sure do not appear in any node of the trained tree.
Figure 3: Trained Tree on TREC Data
The following are rules generated from the train-
ing data whose confidence is higher than 90%. Con-
fidence is defined as out of the training records for
which the left hand side of the rule is true, the per-
centage of records for which the right hand side is
also true. This measures the accuracy of the rule.
- If PRONOUN=1 then follow-up question
- If path noun?0.31 then follow-up question
- If lin noun?0.43 then follow-up question
- If path noun<0.15 and PRONOUN=0 then first question
De Boni and Manandhar?s algorithm has this
rule:?if a question has no verb, the question is
follow-up question?. However, we did not learn this
rule from the data, nor the feature VERB appears in
any node of the trained tree. One possible reason
is that this rule has too little support in the training
set (support is defined as the percentage of which
the left hand side of the rule is true). Another pos-
sible reason is that this rule is not needed because
the combination of other features is able to provide
enough information for recognizing follow-up ques-
tions. In any case, the decision tree learns a (local)
37
optimized combination of features which captures
most cases, and avoids redundant rules.
4.1.3 Error Analysis
The trained decision tree has 3 errors in the test-
ing data. Two of the errors are mis-recognition of
follow-up questions to be first questions, and one is
the vice versa.
The first error is failure to recognize the ques-
tion ?which galleries were involved?? (CTX1c) as
a follow-up question (see Figure 2 for context). It
is a syntactically complete sentence, and there is no
pronoun or definite noun in the sentence. Seman-
tic features are the most useful information to rec-
ognize it as a follow-up question. However, the se-
mantic relatedness in WordNet between the words
?gallery? in the current question and ?museum? in
the first question of this session (CTX1a in Figure 2)
is not strong enough for the trained decision tree to
relate the two questions together.
The second error is failure to recognize the ques-
tion ?Where did the winery?s owner go to college??
(CTX3c) as a follow-up question. Similarly, part
of the reason for this failure is due to the insuffi-
cient semantic relatedness between the words ?win-
ery? and ?grape? (in CTX3a) to connect the ques-
tions together. However, this question has a definite
noun phrase ?the winery? which refers to ?Chateau
Petrus Bordeaux? in the first question in this session.
We did not make use of the feature DefiniteNoun in
our training, because it is not easy to automatically
identify the referents of a definite noun phrase, or
even whether it has a referent or not. A lot of def-
inite noun phrases, such as ?the sun?, ?the trees in
China?, ?the first movie?, and ?the space shuttles?,
do not refer to any entity in the text. This does not
mean that the feature DefiniteNoun is not important,
but instead that we just leave it as our future work to
better incorporate this feature.
The third error, is failure to recognize the question
?What does transgenic mean?? as the first question
that opens a session. This error is due to the over-
fitting of decision tree training.
4.1.4 Boosting
We tried another machine learning approach, Ad-
aboost (Schapire and Singer, 2000), which is resis-
tant (but not always) to over-fitting. It calls a given
weak learning algorithm repeatedly in a series of
rounds t = 1, ..., T . Each time the weak learning
algorithm generates a rough ?rule of thumb?, and
after many rounds Adaboost combines these weak
rules into a single prediction rule that, hopefully,
will be more accurate than any one of the weak
rules. Figure 2 shows the confusion matrix of Ad-
aboost learning results. It shows that Adaboost is
able to correctly recognize ?What does transgenic
mean?? as beginning a new topic. However, Ad-
aboost has more errors in recognizing follow-up
questions, which results in an overall accuracy of
88%, slightly lower than decision tree learning.
Training Data
Predicted Class
True Class First follow-up Total
First 64 1 65
follow-up 1 220 221
Total 65 221 286
Testing Data
Predicted Class
True Class First follow-up Total Recall
First 10 0 10 100%
follow-up 5 27 32 84%
Total 15 27 42
Precision 67% 100%
Table 2: Confusion Matrix Using Adaboosting
4.2 Results on the HandQA data
We also conducted an experiment using real-world
customer-care related questions. We selected our
test data from the chat logs of a deployed online
QA system. We refer to this system as HandQA.
HandQA is built using a telecommunication ontol-
ogy database and 1600 pre-determined FAQ-answer
pairs. For every submitted customer question,
HandQA chooses one of these 1600 answers as the
response. Each chat session contains about 3 ques-
tions. We assume the questions in a session are
context-related.
The HandQA data are different from the TREC
data in two ways. First, HandQA questions are real
typed questions from motivated users. The HandQA
data contain some noisy information, such as typos
and bad grammars. Some users even treated this
system as a search engine and simply typed in the
keywords. Second, questions in a chat session ba-
sically asked for the same information. Very often,
when the system failed to get the correct answer to
38
the user?s question, the user would repeat or rephrase
the same question, until they gave up or the system
luckily found the answer. As an example, Figure 4
shows two chat sessions. Again, we did not use the
system?s answer in our relevancy recognition.
How to make number non published?
Non published numbers
How to make number non listed?
Is my number switched to Call Vantage yet?
When will my number be switched?
When is number transferred?
Figure 4: Example questions in HandQA
A subset of the HandQA data, 5908 questions in
2184 sessions are used for training and testing the
decision tree. The data were randomly divided into
two sets: 90% for training and 10% for testing.
4.2.1 Confusion Matrix
Table 3 shows the confusion matrix of the deci-
sion tree learning results. For recognizing first ques-
tions, the learned model has 73% in recall and 62%
in precision; for recognizing follow-up questions,
the recall is 75% and precision is 84%. The accuracy
is 74%. A base line model is to have all questions
except the first one as following up questions, which
results in the accuracy of 64% (380/590). Thus the
learned decision tree yields an absolute improve-
ment of 10%. However, the results on this data set
are not as good as those on the TREC data.
Training Data
Predicted Class
True Class First follow-up Total
First 1483 490 1973
follow-up 699 2646 3345
Total 2182 3136 5318
Testing Data
Predicted Class
True Class First follow-up Total Recall
First 153 58 211 73%
follow-up 93 286 379 75%
Total 246 344 590
Precision 62% 84%
Table 3: Confusion Matrix for HandQA Data
4.2.2 Trained Tree
Table 5 shows the top two levels of the tree
learned from the training data, both of which are
on the semantic measure path. This again confirms
that path best fits the task of relevancy recognition
among the four semantic measures.
No syntactical features appear in any node of the
learned tree. This is not surprising because syntac-
tic information is noisy in this data set. Typos, bad
grammars, and mis-capitalization affect automatic
POS tagging. Keywords input also results in incom-
plete sentences, which makes it unreliable to recog-
nize follow-up questions based on whether a ques-
tion is a complete sentence or not. Furthermore,
because questions in a session rarely refer to each
other, but just repeat or rephrase each other, the fea-
ture PRONOUN does not help either. All these make
syntactic features not useful. Semantic features turn
out to be more important for this data set.
Figure 5: Trained Tree on HandQA Data
4.2.3 Error Analysis
There are two reasons for the decreased perfor-
mance in this data set. The first reason, as we ana-
lyzed above, is that syntactical features do not con-
tribute to the recognition task. The second reason is
that consecutive chat sessions might ask for the same
information. In the handQA data set, questions are
basically all about telecommunication service, and
questions in two consecutive chat sessions, although
by different users, could be on very similar topics or
even have same words. Thus, questions, although in
two separate chat sessions, could have high semantic
similarity measure. This would introduce confusing
information to the decision tree learning.
5 Making Use of Context Information
Relevancy recognition is the first step of contextual
question answering. If a question is recognized as
following the current existing topic, the next step is
to make use of the context information to interpret it
39
and retrieve the answers. To explore how context in-
formation helps answer retrieval, we conducted pre-
liminary experiments with the TREC 2004 QA data.
We indexed the TREC documents using the Lucene
search engine (Hatcher and Gospodnetic, 2004) for
document retrieval. The Lucene search engine takes
as input a query (a list of keywords), and returns a
ranked list of relevant documents, of which the first
50 were taken and analyzed in our experiments. We
tried different strategies for query formulation. Sim-
ply using the questions as the query, only 20% of
the follow-up questions find their answers in the first
50 returned documents. This percentage went up
to 85% when we used the topic words, provided in
TREC data for each section, as the query. Because
topic words are usually not available in real world
applications, to be more practical, we tried using the
noun phrases in the first question as the query. In
this case, 81% of the questions are able to find the
answers in the returned documents. When we com-
bined the (follow-up) question with the noun phrases
in the first question as the query, the retrieved rate
increases to 84%. Typically, document retrieval is a
crucial step for QA systems. These results suggest
that context information fusion has a big potential to
improve the performance of answer retrieval. How-
ever, we leave the topic of how to fuse context infor-
mation into the follow-up questions as future work.
6 Conclusion
In this paper, we present a data driven approach, de-
cision tree learning, for the task of relevancy recog-
nition in contextual question answering. Experi-
ments show that this approach achieves 93% accu-
racy on the TREC data, about 12% improvement
from the rule-based algorithm reported by De Boni
and Mananhar (2005). Moreover, this data driven
approach requires much less human effort on inves-
tigating a specific data set and less human exper-
tise to summarize rules from the observation. All
the features we used in the training can be automat-
ically extracted. This makes it straightforward to
train a model in a new domain, such as the HandQA.
Furthermore, decision tree learning is a white-box
model and the trained tree is human interpretable. It
shows that the path measure has the best information
gain among the other semantic similarity measures.
We also report our preliminary experiment results on
context information fusion for question answering.
7 Acknowledgement
The authors thank Srinivas Bangalore and Mazin E.
Gilbert for helpful discussion.
References
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust nlp tools and applications. In Proceedings
of the 40th ACL.
Marco De Boni and Suresh Manandhar. 2005. Imple-
menting clarification dialogues in open domain ques-
tion answering. Natural Language Engineering. Ac-
cepted.
Christiane Fellbaum. 1998. WordNet:An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Erik Hatcher and Otis Gospodnetic. 2004. Lucene in
Action. Manning.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of 32nd ACL, pages
9?16.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the International Con-
ference on Machine Learning.
Siddharth Patwardhan. 2003. Incorporating dictionary
and corpus information into a context vector measure
of semantic relatedness. master?s thesis, University of
Minnesota, Duluth.
Ted Pederson, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - measuring the re-
latedness of concepts. In Proceedings of the 9th AAAI.
Intelligent Systems Demonstration.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A
public reference implementation of the rap anaphora
resolution algorithm. In Proceedings of LREC, pages
291?294.
Robert E. Schapire and Yoram Singer. 2000. BoosTex-
ter: A boosting-based system for text categorization.
Machine Learning, 39:135?168.
Ellen M. Voorhees. 2001. Overview of the TREC 2001
question answering track. In Proceedings of TREC-10.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In Proceedings of TREC-13.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proceedings of 32nd ACL,
pages 133?138.
40
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1191?1200,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
An Empirical Study Of Semi-Supervised Chinese Word Segmentation
Using Co-Training
Fan Yang
Nuance Communications, Inc.
fan.yang@nuance.com
Paul Vozila
Nuance Communications, Inc.
paul.vozila@nuance.com
Abstract
In this paper we report an empirical study
on semi-supervised Chinese word segmenta-
tion using co-training. We utilize two seg-
menters: 1) a word-based segmenter lever-
aging a word-level language model, and 2)
a character-based segmenter using character-
level features within a CRF-based sequence
labeler. These two segmenters are initially
trained with a small amount of segmented
data, and then iteratively improve each other
using the large amount of unlabelled data.
Our experimental results show that co-training
captures 20% and 31% of the performance
improvement achieved by supervised training
with an order of magnitude more data for the
SIGHAN Bakeoff 2005 PKU and CU corpora
respectively.
1 Introduction
In the literature there exist two general models for
supervised Chinese word segmentation, the word-
based approach and the character-based approach.
The word-based approach searches for all possible
segmentations, usually created using a dictionary,
for the optimal one that maximizes a certain util-
ity. The character-based approach treats segmenta-
tion as a character sequence labeling problem, indi-
cating whether a character is located at the bound-
ary of a word. Typically the word-based approach
uses word level features, such as word n-grams and
word length; while the character-based approach
uses character level information, such as character n-
grams. Both approaches have their own advantages
and disadvantages, and there has been some research
in combining the two approaches to improve the per-
formance of supervised word segmentation.
In this research we are trying to take advantage of
the word-based and the character-based approaches
in the semi-supervised setting for Chinese word seg-
mentation, where there is only a limited amount
of human-segmented data available, but there ex-
ists a relatively large amount of in-domain unseg-
mented data. The goal is to make use of the in-
domain unsegmented data to improve the ultimate
performance of word segmentation. According to
Sun et al (2009), ?the two approaches [word-based
and character-based approaches] are either based on
a particular view of segmentation.? This naturally
motivates the use of co-training, which utilizes two
models trained on different views of the input la-
beled data which then iteratively educate each other
with the unlabelled data. At the end of the co-
training iterations, the initially weak models achieve
improved performance. Co-training has been suc-
cessfully applied in many natural language process-
ing tasks. In this paper we describe an empiri-
cal study of applying co-training to semi-supervised
Chinese word segmentation. Our experimental re-
sults show that co-training captures 20% and 31%
of the performance improvement achieved by super-
vised training with an order of magnitude more data
for the SIGHANBakeoff 2005 PKU and CU corpora
respectively.
In section 2 we review the two supervised ap-
proaches and co-training algorithm in more detail.
In section 3 we describe our implementation of the
co-training word segmentation. In section 4 we de-
1191
Figure 1: A search space for word segmenter
scribe our co-training experiments. In section 5 we
conclude the paper.
2 Related Work
In this section, we first review the related research on
the word-based and the character-based approaches
for Chinese word segmentation, and comparatively
analyze these two supervised approaches. We then
review the related research on co-training.
2.1 Supervised Word Segmentation
2.1.1 Word-Based Segmenter
Given a character sequence c1c2...cn, the word-
based approach searches in all possible segmenta-
tions for one that maximizes a pre-defined utility
function, formally represented as in Equation 1. The
search space, GEN(c1c2...cn), can be represented
as a lattice, where each vertex represents a charac-
ter boundary index and each arc represents a word
candidate which is the sequence of characters within
the index range. A dictionary1 can be used to gener-
ate such a lattice. For example, given the character
sequence ??????? and a dictionary that con-
tains the words {????????} and all single
Chinese characters, the search space is illustrated in
Figure 1.
W? = arg max
W?GEN(c1c2...cn)
Util(W ) (1)
Dynamic programming such as Viterbi decoding
is usually used to search for the optimized segmen-
tation. The utility can be as simple as the negation
of number of words (i.e. Util(W ) = ? | W |),
1A dictionary is not a must to create the search space but
it could shrink the search space and also lead to improved seg-
mentation performance.
which gives a reasonable performance if the dictio-
nary used for generating the search space has a good
coverage. Alternatively one can search for the seg-
mentation that maximizes the word sequence prob-
ability P (W ) (i.e. Util(W ) = P (W )). With a
Markov assumption, P (W ) can be calculated using
a language model as in Equation 2.
P (W ) = P (w1w2...wm)
= P (w1).P (w2|w1)...P (wn|w1w2...wn1)
= P (w1)P (w2|w1)...P (wn|wn?1)
(2)
More generally, the utility can be formulated as
a semi-Markov linear model, defined as Equation 3,
in which ? is the feature function vector, and ? is
the parameter vector that can be learned from train-
ing data using different techniques: Liang (2005),
Gao et al (2005), and Zhang and Clark (2007) use
averaged perceptron; Nakagawa (2004) uses general
iterative scaling; Andrew (2006) uses semi-Markov
CRF; and Sun (2010) uses a passive-aggressive
learning algorithm.
Util(W ) = ?T?(c1c2...cn,W ) (3)
2.1.2 Character-Based Segmenter
The character-based approach treats word seg-
mentation as a character sequence labeling problem,
to label each character with its location in a word,
first proposed by Xue (2003).2 The basic label-
ing scheme is to use two tags: ?B? for the begin-
ning character of a word and ?O? for other charac-
ters (Peng et al, 2004). Xue (2003) use a four-tag
scheme based on some linguistic intuitions: ?B? for
the beginning character, ?I? for the internal charac-
ters, ?E? for the ending character, and ?S? for single-
character word. For example, the word sequence ??
?? ? ??? can be labelled as ?\B ?\I ?\E
?\S ?\B ?\E. Zhao et al (2010) further extend
this scheme by using six tags.
Training and decoding of the character labeling
problem is similar to part-of-speech tagging, which
2Teahan et al (2000) use a character language model to de-
termine whether a word boundary should be inserted after each
character, which can also be considered as a character-based
approach as well.
1192
is also generally formulated as a linear model. Many
machine learning techniques have been explored:
Xue (2003) use a maximum entropy model; Peng et
al. (2004) use linear-chain CRF; Liang (2005) uses
averaged perceptron; Sun et al (2009) use a discrim-
inative latent variable approach.
2.1.3 Comparison and Combination
It is more natural to use word-level informa-
tion, such as word n-grams and word length, in a
word-based segmenter; while it is more natural to
use character-level information, such as character n-
grams, in a character-based segmenter. Sun (2010)
gives a detailed comparison of the two approaches
from both the theoretical and empirical perspec-
tives. Word-level information has greater represen-
tational power in terms of contextual dependency,
while character-level information is better at mor-
phological analysis in terms of word internal struc-
tures.
On one hand, features in a character-based model
are usually defined in the neighboring n-character
window; and an order-K CRF can only look at the
labels of the previous K characters. Given that many
words contain more than one character, a word-
based model can examine a wider context. Thus
the contextual dependency information encoded in
a character-based model is generally weaker than in
a word-based model. Andrew (2006) also shows
that semi-Markov CRF makes strictly weaker in-
dependence assumptions than linear CRF and so
a word-based segmenter using an order-K semi-
Markov model is more expressive than a character-
based model using an order-K CRF.
On the other hand, Chinese words have internal
structures. Chinese characters can serve some mor-
phological functions in a word. For example, the
character? usually works as a suffix to signal plu-
ral; the character ? can also be a suffix meaning a
group of people; and ? generally works as a pre-
fix before a person?s nickname that has one charac-
ter. Such morphological information is extremely
useful for identifying unknown words. For exam-
ple, a character-based model can learn that ? is
usually tagged as ?B? and the next character is usu-
ally tagged as ?E?. Thus even when ?? is not an
existing word in the training data, a character-based
model might still be able to correctly label it as?\B
?\E.
Recent advanced Chinese word segmenters, either
word-based or character-based, have been trying to
make use of both word-level and character-level in-
formation. For example, Nakagawa (2004) inte-
grates the search space of a character-based model
into a word-based model; Andrew (2006) converts
CRF-type features into semi-CRF features in his
semi-Markov CRF segmenter; Sun et al (2009) add
word identify information into their character-based
model; and Sun (2010) combine the two approaches
at the system level using bootstrap aggregating.
2.2 Co-Training
The co-training approach was first introduced by
Blum and Mitchell (1998). Theoretical analysis of
its effectiveness is given in (Blum and Mitchell,
1998; Dasgupta et al, 2001; Abney, 2002). Co-
training works by partitioning the feature set into
two conditionally independent views (given the true
output). On each view a statistical model can be
trained. The presence of multiple distinct views of
the data can be used to train separate models, and
then each model?s predictions on the unlabeled data
are used to augment the training set of the other
model.
Figure 2 depicts a general co-training framework.
The inputs are two sets of data, a labelled set S and
an unlabelled set U. Generally S is small and U is
large. Two statistical models M1 and M2 are used,
which are built on two sets of data L1 and L2 initial-
ized as S but then incrementally increased in each
iteration. C is a cache holding a small subset of U
to be labelled by both models (Blum and Mitchell,
1998; Abney, 2002). In some applications, C is
not used and both models label the whole set of U
(i.e. C==U) (Collins and Singer, 1999; Nigam and
Ghani, 2000; Pierce and Cardie, 2001). The stop-
ping criteria can be, for example, when U is empty,
or when a certain number of iterations are executed.
In step 5 and 6 during each iteration, some data
labelled by M1 are selected and added to the train-
ing set L2, and vice versa. Several selection algo-
rithms have been proposed. Dasgupta et al (2001)
and Abney (2002) use a selection algorithm that tries
to maximize the agreement rate between the two
models. The more popular selection algorithm is to
choose the K examples that have the highest con-
1193
Input:
S is the labelled data
U is the unlabelled data
Variables:
L1 is the training data for View One
L2 is the training data for View Two
C is a cache holding a small subset of U
Initialization:
L1 <- S
L2 <- S
C <- randomly sample a subset of U
U <- U - C
REPEAT:
1. Train M1 using L1
2. Train M2 using L2
3. Use M1 to label C
4. Use M2 to label C
5. Select examples labelled by M1, add to L2
6. Select examples labelled by M2, add to L1
7. Randomly move samples from U to C
so that C maintains its size
UNTIL stopping criteria
Figure 2: A generic co-training framework
fidence score (Nigam and Ghani, 2000; Pierce and
Cardie, 2001). In order to balance the class distri-
butions in the training data L1 and L2, Blum and
Mitchell (1998) select P positive examples and Q
negative examples that have the highest confidence
scores respectively. Wang et al (2007) and Guz et
al. (2007) use disagreement-based selection, which
adds to L2, data that is labeled by M1 and M2 with
high and low confidence respectively, with the in-
tuition that such data are more useful and compen-
satory to M2. Finally, instead of adding the selected
data to the training data, Tur (2009) propose the co-
adaptation approach which linearly interpolates the
existing model with the new model built with the
new selected data.
3 Segmentation With Co-Training
3.1 Design of Two Segmenters
The use of co-training needs two statistical models
that satisfy the following three conditions. First, in
theory these two models need to be built on two con-
ditionally independent views. However this is a very
strong assumption and many large-scale NLP prob-
lems do not have a natural split of features to satisfy
this assumption. In practice it has been shown that
co-training can still achieve improved performance
when this assumption is violated, but conforming to
the conditionally independent assumption leads to
a bigger gain (Nigam and Ghani, 2000; Pierce and
Cardie, 2001). Thus we should strive to have the two
models less correlated. Second, the two models both
need to be effective for the task, that is, each of the
models itself can perform the task reasonably well.
Third, the decoding and training of the two models
need to be efficient, as in co-training we need to seg-
ment the unlabelled data and re-train the models in
each iteration. In the following we describe our de-
sign of the two segmenters.
Word-based segmenter In the word-based seg-
menter, we utilize a statistical n-gram lan-
guage model and try to optimize the language
modeling score together with a word insertion
penalty, as show in Equation 4. K is a per-
word penalty that is pre-determined with 10
fold cross-validation on the SIGhan PKU train-
ing set. We train a Kneser-Ney backoff lan-
guage model from the training data, and extract
a dictionary of words from the training data for
generating the search space. Our pilot study
suggested that a bigram language model is suf-
ficient for this task.
Util(W ) = ln(P (W )) ? |W | ? K (4)
Character-based segmenter We use an order-1
linear conditional random field to label a char-
acter sequence. Following Xue (2003), we use
the four-tag scheme ?BIES?. We use the tool
CRF++3. The features that we use are charac-
ter n-grams within the neighboring 5-character
window and tag bigrams. Given a character c0
in the character sequence c?2c?1c0c1c2, we ex-
tract the following features: character unigrams
c?2, c?1, c0, c1, c2, bigrams c?1c0 and c0c1. L2
regularization is applied in learning.
As can be seen, we build a word-based segmenter
that uses only word level features, and a character-
based segmenter that uses only character level fea-
tures. These two segmenters by no means satisfy
the conditionally independence assumption, but we
have the hope that they are not too correlated as
they use different levels of information and these
3http://crfpp.googlecode.com/svn/trunk/
doc/index.html
1194
different levels of information have been shown to
be complementary in literature. Also the effective-
ness of these two segmenters has been demonstrated
in literature and will be shown again in our results
in Section 4. Finally, both segmenters can decode
and be trained pretty quickly. In our implemen-
tation, running on a Xeon 2.93GHz CPU with 4G
of memory, it takes less than 30 seconds to build a
word-based segmenter and less than 1 hour to build
a character-based segmenter with the SIGhan PKU
training data, and it takes less than 20 seconds to ap-
ply the word-based segmenter or less than 5 seconds
to apply the character-based segmenter to the PKU
testing data.
3.2 Co-Training
We follow the framework in Figure 2 for the co-
training setup. We do not use the cache C, but di-
rectly label the whole unlabelled data set U, because
in our experiment setup (see Section 4) U is not
huge and computationally we can afford to label the
whole set. The stopping criteria we use is when U is
empty. Following Wang et al (2007) and Guz et al
(2007), we use disagreement-based data selection.
In every iteration, we pick some sentences that are
segmented by the character-based model with high
confidence but are segmented by the word-based
model with low confidence to add to the training
data of the word-based model, and vice versa. Con-
fidence score is normalized with regard to the length
of the sentence (i.e. number of characters) to avoid
biasing towards short sentences. Confidence scores
between the two segmenters, however, are not di-
rectly comparable. Thus we rank the sentences by
their confidence scores in each segmenter respec-
tively, and calculate the rank difference between the
two segmenters. This rank difference is used as the
indication of the gap of the confidence between the
two segmenters. The sentences of highest rank dif-
ference are assigned to the training data of the word-
based segmenter, with the segmentations from the
character-based model; and the sentences of lowest
rank difference are assigned to the training data of
the character-based model, with segmentations from
the word-based model.
4 Experiments
4.1 Data and Experiment Setup
We conduct a set of experiments to evaluate the per-
formance of our co-training on semi-supervised Chi-
nese word segmentation. Two corpora, the PKU cor-
pus and the CU corpus, from the SIGhan Bakeoff
2005 are used. The PKU corpus contains texts of
simplified Chinese characters, which include 19056
sentences in the training data and 1945 sentences in
the testing data. The CU corpus contains texts of
traditional Chinese characters, which include 53019
sentences in the training data and 1493 sentences in
the testing data. The training data in each corpus is
randomly split into 10 subsets. In each run one set
is used as the labelled data S, and the other nine sets
are combined and used as the unlabelled data U with
segmentations removed. That is, 10% of the training
data is used as segmented data, and 90% are used
as unsegmented data in our semi-supervised train-
ing. This setup resembles our semi-supervised ap-
plication, where there is only a small limited amount
of segmented data but a relatively large amount of
in-domain unsegmented data available. The final
trained character-based and word-based segmenters
from co-training are then evaluated on the testing
data. Results we report in this paper are the aver-
age of the 10 runs. F-measure is used as the per-
formance measurement. A 99% confidence interval
is calculated as ?2.56
?
p(1? F )/N for statistical
significance evaluation, where F is the F-measure
and N is the number of words. Subsequent asser-
tions in this paper about statistical significance indi-
cate whether or not the p-value in question exceeds
1%.
4.2 Co-Training Results
For comparison, we measure the baseline as the
performance of a model trained with the 10%
segmented data only (referred to as BASIC base-
lines). The BASIC baselines, both for the word-
based model and the character-based model, how-
ever, use only the segmented data but leave out the
large amount of available unsegmented data. We
thus measure another baseline (referred to as FOLD-
IN), which naively uses the unsegmented data. In the
FOLD-IN baseline, a model is first trained with the
10% segmented data, and then this model is used
1195
Table 1: Co-training results
PKU CU
char word char word
BASIC 90.4 84.2 89.2 78.4
FOLD-IN 90.5 84.2 89.3 78.5
CEILING 94.5 93.0 94.2 88.9
CO-TRAINING 91.2 90.3 90.2 86.2
Figure 3: Gap filling with different split ratio
to label the unsegmented data. The automatic seg-
mentation is then combined with the segmented data
to build a new model. We also measure the CEIL-
ING as the performance of a model trained with all
the training data available, i.e. we use the true seg-
mentations of the 90% unsegmented data together
with the 10% segmented data to train a model. The
CEILING tells us the oracle performance when we
have all segmented data for training, while the BA-
SIC shows how much performance is dropped when
we only have 10% of the segmented data. The per-
formance of co-training will tell us how much we
can fill the gap by taking advantage of the other 90%
as unsegmented data in the semi-supervised training.
The FOLD-IN baseline further verifies the effective-
ness of co-training, i.e. co-training should perform
better than naively folding in the unsegmented data.
Table 1 presents the results. First, we see that
both the word-based and character-based models
are doing a decent job under the CEILING condi-
tion. This confirms the effectiveness of each in-
dividual model, which is generally a requirement
for running co-training. The character-based seg-
menter, although simple and with character-level
features only, achieves the performance that is close
to the state-of-the-art technologies that are much
more complicated (The best performance is 95.2%
for the PKU corpus and 95.1% for the CU corpus,
see (Sun et al, 2009)). Second, we see that under
all four conditions, the character-based segmenter
performs better than the word-based model. This
is not too surprising as these results are consistent
with those reported in the literature. The word-based
segmenter implemented in this work is less power-
ful, and it needs a good dictionary to achieve good
performance. In our implementation, a dictionary
is extracted from the segmented training set. Thus
the word-based model suffers a lot when the train-
ing data is small. Third, we see that both the word-
based model and the character-based model are im-
proved by co-training, and the improvements are all
statistically significant. It is not surprising for the
word-based model to learn from the more accurate
character-based model, which can also identify new
words to add to the dictionary. More interestingly,
the character-based segmenter is able to benefit from
the less powerful word-based segmenter. For the
character-based model, about 20% of the gap be-
tween BASIC and CEILING is filled by co-training,
consistently in both the PKU and CU corpora. Fi-
nally, comparing FOLD-IN and BASIC, we see that
naively using the unsegmented data does not lead to
a significant improvement. This suggests that co-
training provides a process that effectively makes
use of the unsegmented data.
For completeness, in Figure 3 we also show the
relative gap filling with different splits of the seg-
mented vs unsegmented data. With more data mov-
ing to the segmented set, the absolute improvement
of co-training over BASIC gets smaller, while the
gap between the BASIC and CEILING also becomes
smaller. The relative gap filled, i.e. the improve-
ment relative to the difference between BASIC and
CEILING, as can be seen, consistently falls inside
the section of 15% and 25%.
1196
4.3 Further Analysis
It is not surprising that the word-based segmenter
benefits from co-training since it learns from the
more accurate character-based segmenter. Our fo-
cus, however, is to better understand what benefit
the character-based segmenter gains from the co-
training procedure. The character-based segmenter
treats word segmentation as a character sequence
labelling problem with four tags ?B I E S?. As-
suming that segmentation accuracy is proportional
to tag accuracy, we examine the tag accuracy of
the character-based segmenter before and after co-
training.
If a character is labelled with tag T0 initially be-
fore co-training and with tag T1 after co-training,
with the tag T1 different from T0, there can be one
of three cases: 1) T0 is correct; 2) T1 is correct; or
3) neither is correct. The absolute gain from co-
training of switching from tag T0 to T1 is defined
as the number of case 2 instances less case 1 in-
stances. Absolute gain indicates the gain of tag ac-
curacy where co-training learns to switch from T0 to
T1, and it contributes to the overall tag accuracy im-
provement. We also define relative gain of switch-
ing from tag T0 to T1 as the absolute gain divided
by the total number of cases switching from tag T0
to T1. Relative gain indicates how well co-training
learns to switch from T0 to T1.
Results are shown in Table 2. For both absolute
gain and relative gain, 12 ordered switching pairs
can be divided into two pools, a positive pool that
has higher gain includingB ? E, E ? B, S ? B,
S ? E, E ? I , B ? I , B ? S, and a neutral
pool that has lower or even negative gain including
I ? E, I ? S, I ? B, E ? S, S ? I . The
S ? B, S ? E, B ? I , E ? I in the positive
pool actually suggest that the character-based seg-
menter learns from co-training to combine a single-
character word with it?s neighbour to create a new
longer word; whereas the I ? E, I ? S, I ? B in
the neutral pool suggest that it does not really learn
how to separate a longer words into smaller units.
4.4 Feature Combination
We split the features into two sets, a character-level
feature set used by the character-based segmenter
and a word-level feature set used by the word-based
Table 2: Absolute Gain and Relative Gain
Absolute Gain Relative Gain
T0 T1 PKU CU PKU CU
B I 678 681 0.28 0.59
B E 2331 1727 0.41 0.46
B S 1025 686 0.07 0.08
I B 458 -283 0.07 -0.08
I E 61 -1117 0.01 -0.23
I S 323 -338 0.09 -0.34
E B 2163 1601 0.41 0.46
E I 963 819 0.36 0.62
E S 520 -13 0.03 0.00
S B 1847 892 0.27 0.30
S I 104 47 0.22 0.28
S E 1438 846 0.26 0.55
segmenter. We have shown that these two seg-
menters improve each other via co-training. How-
ever, as reviewed in Section 2.1, there is active re-
search in combining the character-level and word-
level features in a segmenter. When training with
the whole set of data (i.e. under the CEILING con-
dition), a segmenter with combined features tends to
perform better than only using one set of features.
Thus we need to address two problems. First, we
want to understand whether co-training, which splits
the features, can actually beat the BASIC and FOLD-
IN baselines of a segmenter with combined features.
Second, we want to explore whether we can further
improve the final co-training performance by feature
combination.
To address these two problems, we adopt Weiwei
Sun?s character-based segmenter4 in (Sun, 2010).
We use this segmenter because it is publicly avail-
able and it performs well on both the PKU corpus
and CU corpus. It models word segmentation as
a character labelling problem, and solves it with a
passive-aggressive optimization algorithm. It uses
the same feature set as in (Sun et al, 2009), in-
cluding both character-level features and word-level
features. Character-level features include character
uni-grams and bi-grams in the five character win-
dow, and whether the current character is the same
as the next or the one after the next character. Word-
4Available at http://www.coli.uni-
saarland.de/ wsun/ccws.tgz
1197
Table 3: Sun-Segmenter?s performance
PKU CU
BASIC 90.3 89.2
FOLD-IN 90.6 89.7
CEILING 94.8 95.0
Table 4: Results of feature combination
PKU CU
data combination 91.2 90.9
relabelling 91.2 91.0
level features include what word uni-grams or bi-
grams are anchored at the current character. Word
uni-grams and bi-grams are extracted from the la-
beled training data. For more details, please refer
to (Sun et al, 2009) and (Sun, 2010). For ease
of description, we will refer to Weiwei Sun?s seg-
menter with combined features as Sun-Segmenter,
and the character-based segmenter used in our co-
training which uses character-level features as Char-
Segmenter.
Table 3 shows the performance of the Sun-
Segmenter under the three conditions: BASIC,
FOLD-IN, and CEILING. We see that under
the CEILING condition, the Sun-Segmenter out-
performs the Char-Segmenter by 0.3% in the PKU
corpus and 0.8% in the CU corpus. However, un-
der the BASIC condition when there is only 10%
of training data available, the Sun-Segmenter gives
no gain. This probably is due to the fact that the
Sun-Segmenter uses a much larger feature set and
thus correspondingly a larger training set is needed
to avoid under-fitting. The Sun-Segmenter has more
gain when folding in the unsegmented data than the
Char-Segmenter, further suggesting that the Sun-
Segmenter is benefiting from the size of data. For
both corpora, however, the Char-Segmenter after co-
training beats the FOLD-IN baseline of the Sun-
Segmenter by at least 0.5%, and the improvement
is statistically significant. When there is only a
small amount of segmented data available, using a
more advanced segmenter with combined features
still under-performs compared to co-training. These
results justify the split of features for running co-
training.
Next we would like to explore whether we could
further improve the co-training performance, given
that we have a more advanced segmenter using com-
bined features. We try two approaches. In the first
approach, after all the iterations of co-training, the
data are split into two sets, one set for training the
word-based segmenter L1 and the other set for train-
ing the character-based segmenter L2. The segmen-
tations of these two sets of data are probably bet-
ter than the segmentations under the FOLD-IN con-
dition. We thus combine the two sets of data, and
use the combined data to train a new model with the
Sun-Segmenter. In the second approach, we use the
character-based segmenter after co-training, which
has an improved performance, to relabel the set of
unsegmented data U, and then combine it with the
segmented data set S.We then use the combined data
to train a new model with the Sun-Segmenter.
Results are shown in Table 4. In the PKU corpus,
we do not see a gain using either the data combina-
tion approach or the relabelling approach compared
to the performance of the Char-Segmenter after co-
training, probably because the Sun-Segmenter just
modestly improves over the Char-Segmenter under
the CEILING condition. However, in the CU cor-
pus, where under the CEILING condition the Sun-
Segmenter has a much bigger gain over the Char-
Segmenter, there is 0.7% improvement by using the
data combination approach and 0.8% by using the
relabelling approach, and the improvement is statis-
tically significant. Overall, using co-training with
feature combination we are able to cut the gap be-
tween the BASIC baseline and CEILING of the Sun-
Segmenter by 20% in the PKU corpus and 31% in
the CU corpus.
5 Discussion
There has been some research on semi-supervised
Chinese word segmentation. For example, Liang
(2005) derive word cluster features and mutual in-
formation features from unlabelled data, and add
them to supervised discriminative training; Li and
Sun (2009) use punctuation as implicit annotations
of a character starting a word (the character after a
punctation) or ending a word (the character before
a punctuation) in a large unlabelled data set to aug-
ment supervised data; Sun and Xu (2011) derive a
large set of features from unlabelled data, includ-
1198
ing mutual information, accessor variety and punc-
tuation variety to augment the character and word
features derived from labelled data. These research
works aim to use huge amount of unsegmented data
to further improve the performance of an already
well-trained supervised model.
In this paper, we assume a much limited amount
of segmented data available, and try to boost up the
performance by using in-domain unsegmented data.
Chinese word segmentation is domain-sensitive or
application sensitive. For example, a CRF seg-
menter trained on the SIGhan MSR training data,
which achieves an F-measure of 96.5% in the MSR
testing data, only has 83.8% when applied to the
PKU testing data; and the same CRF segmenter
trained on the PKU training data achieves 94.5% on
the PKU testing data. When one starts a new ap-
plication that requires word segmentation in a new
domain, it is likely that there is only a very small
amount of segmented data available.
We propose the approach of co-training for Chi-
nese word segmentation for the semi-supervised set-
ting where there is only a limited amount of human-
segmented data available, but there exists a relatively
large amount of in-domain unsegmented data. We
split the feature set into character-level features and
word-level features, and then build a character-based
segmenter with character-level features and a word-
based segmenter with word-level features, using the
limited amount of available segmented data. These
two segmenters then iteratively educate and improve
each other by making use of the large amount of
unsegmented data. Finally we combine the word-
level and character-level features with an advanced
segmenter to further improve the co-training perfor-
mance. Our experiments show that using 10% data
as segmented data and the other 90% data as unseg-
mented data, co-training reaches 20% performance
improvement achieved by supervised training with
all data in the SIGHAN 2005 PKU corpus and 31%
in the CU corpus.
Acknowledgments
The authors thank Weiwei Sun for helping with
data setup and technical consultation of the Sun-
Segmenter. The authors also thank Christian Mon-
son and Nicola Ueffing for helpful discussions.
References
Steven Abney. 2002. Bootstrapping. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 360?367.
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 465?
472.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the eleventh annual conference on Computa-
tional learning theory, pages 92?100.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
100?110.
Sanjoy Dasgupta, Michael L. Littman, and David
Mcallester. 2001. Pac generalization bounds for co-
training. In Proceedings of Advances in Neural Infor-
mation Processing Systems, pages 375?382.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang.
2005. Chinese word segmentation and named entity
recognition: A pragmatic approach. Computationl
Linguistics, 31(4):574.
Umit Guz, Sebastien Cuendet, Dilek Hakkani-Tur, and
Gokhan Tur. 2007. Co-training using prosodic and
lexical information for sentence segmentation. In pro-
ceedings of INTERSPEECH, pages 2597?2600.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Comput. Linguist., 35:505?512, December.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, MASSACHUSETTS
INSTITUTE OF TECHNOLOGY, May.
Tetsuji Nakagawa. 2004. Chinese and japanese word
segmentation using word-level and character-level in-
formation. In Proceedings of the 20th international
conference on Computational Linguistics.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the
effectiveness and applicability of co-training. In Pro-
ceedings of the ninth international conference on In-
formation and knowledge management, pages 86?93.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of the
20th international conference on Computational Lin-
guistics.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In In Proceedings of the 2001 Conference on
1199
Empirical Methods in Natural Language Processing,
pages 1?9.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese word
segmentation using unlabeled data. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 970?979, Edinburgh,
Scotland, UK., July.
Xu Sun, Yaozhong Zhang, TakuyaMatsuzaki, Yoshimasa
Tsuruoka, and Jun?ichi Tsujii. 2009. A discrimi-
native latent variable chinese segmenter with hybrid
word/character information. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 56?64.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: comparison and combi-
nation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics: Posters, pages
1211?1219.
W. J. Teahan, Rodger McNab, Yingying Wen, and Ian H.
Witten. 2000. A compression-based algorithm for
chinese word segmentation. Computational Linguis-
tics, 26(3):375?393, September.
Gokhan Tur. 2009. Co-adaptation: Adaptive co-
training for semi-supervised learning. In proceedings
of ICASSP, pages 3721?3724.
Wen Wang, Zhongqiang Huang, and Mary Harper. 2007.
Semi-supervised learning for part-of-speech tagging of
mandarin transcribed speech. In In ICASSP.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, pages 29?48.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 840?847, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. ACM Trans-
actions on Asian Language Information Processing,
9(2):5:1?5:32, June.
1200
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90?98,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Semi-Supervised Chinese Word Segmentation Using Partial-Label
Learning With Conditional Random Fields
Fan Yang
Nuance Communications Inc.
fan.yang@nuance.com
Paul Vozila
Nuance Communications Inc.
paul.vozila@nuance.com
Abstract
There is rich knowledge encoded in on-
line web data. For example, punctua-
tion and entity tags in Wikipedia data
define some word boundaries in a sen-
tence. In this paper we adopt partial-label
learning with conditional random fields to
make use of this valuable knowledge for
semi-supervised Chinese word segmenta-
tion. The basic idea of partial-label learn-
ing is to optimize a cost function that
marginalizes the probability mass in the
constrained space that encodes this knowl-
edge. By integrating some domain adap-
tation techniques, such as EasyAdapt, our
result reaches an F-measure of 95.98% on
the CTB-6 corpus, a significant improve-
ment from both the supervised baseline
and a previous proposed approach, namely
constrained decode.
1 Introduction
A general approach for supervised Chinese word
segmentation is to formulate it as a character se-
quence labeling problem, to label each charac-
ter with its location in a word. For example,
Xue (2003) proposes a four-label scheme based on
some linguistic intuitions: ?B? for the beginning
character of a word, ?I? for the internal characters,
?E? for the ending character, and ?S? for single-
character word. Thus the word sequence ????
???? can be turned into a character sequence
with labels as?\B?\I?\E?\S?\B?\E.
A machine learning algorithm for sequence label-
ing, such as conditional random fields (CRF) (Laf-
ferty et al., 2001), can be applied to the labelled
training data to learn a model.
Labelled data for supervised learning of Chi-
nese word segmentation, however, is usually ex-
pensive and tends to be of a limited amount. Re-
searchers are thus interested in semi-supervised
learning, which is to make use of unlabelled data
to further improve the performance of supervised
learning. There is a large amount of unlabelled
data available, for example, the Gigaword corpus
in the LDC catalog or the Chinese Wikipedia on
the web.
Faced with the large amount of unlabelled data,
an intuitive idea is to use self-training or EM, by
first training a baseline model (from the supervised
data) and then iteratively decoding the unlabelled
data and updating the baseline model. Jiao et al.
(2006) and Mann and McCallum (2007) further
propose to minimize the entropy of the predicted
label distribution on unlabeled data and use it as
a regularization term in CRF (i.e. entropy reg-
ularization). Beyond these ideas, Liang (2005)
and Sun and Xu (2011) experiment with deriv-
ing a large set of statistical features such as mu-
tual information and accessor variety from un-
labelled data, and add them to supervised dis-
criminative training. Zeng et al. (2013b) experi-
ment with graph propagation to extract informa-
tion from unlabelled data to regularize the CRF
training. Yang and Vozila (2013), Zhang et al.
(2013), and Zeng et al. (2013a) experiment with
co-training for semi-supervised Chinese word seg-
mentation. All these approaches only leverage
the distribution of the unlabelled data, yet do not
make use of the knowledge that the unlabelled data
might have integrated in.
There could be valuable information encoded
within the unlabelled data that researchers can take
advantage of. For example, punctuation creates
natural word boundaries (Li and Sun, 2009): the
character before a comma can only be labelled
as either ?S? or ?E?, while the character after a
comma can only be labelled as ?S? or ?B?. Fur-
thermore, entity tags (HTML tags or Wikipedia
tags) on the web, such as emphasis and cross refer-
ence, also provide rich information for word seg-
mentation: they might define a word or at least
90
Figure 1: Sausage constraint (partial labels) from natural annotations and punctuation
give word boundary information similar to punc-
tuation. Jiang et al. (2013) refer to such structural
information on the web as natural annotations, and
propose that they encode knowledge for NLP. For
Chinese word segmentation, natural annotations
and punctuation create a sausage
1
constraint for
the possible labels, as illustrated in Figure 1. In
the sentence ???????????????
?????, the first character ? can only be la-
belled with ?S? or ?B?; and the characters? before
the comma and ? before the Chinese period can
only be labelled as ?S? or ?E?. ?????? and ??
???? are two Wikipedia entities, and so they
define the word boundaries before the first char-
acter and after the last character of the entities as
well. The single character ? between these two
entities has only one label ?S?. This sausage con-
straint thus encodes rich information for word seg-
mentation.
To make use of the knowledge encoded in the
sausage constraint, Jiang et al. (2013) adopt a con-
strained decode approach. They first train a base-
line model with labelled data, and then run con-
strained decode on the unlabelled data by binding
the search space with the sausage; and so the de-
coded labels are consistent with the sausage con-
straint. The unlabelled data, together with the
labels from constrained decode, are then selec-
tively added to the labelled data for training the
final model. This approach, using constrained de-
code as a middle step, provides an indirect way
of leaning the knowledge. However, the middle
step, constrained decode, has the risk of reinforc-
ing the errors in the baseline model: the decoded
labels added to the training data for building the
final model might contain errors introduced from
the baseline model. The knowledge encoded in
1
Also referred to as confusion network.
the data carrying the information from punctuation
and natural annotations is thus polluted by the er-
rorful re-decoded labels.
A sentence where each character has exactly
one label is fully-labelled; and a sentence where
each character receives all possible labels is zero-
labelled. A sentence with sausage-constrained la-
bels can be viewed as partially-labelled. These
partial labels carry valuable information that re-
searchers would like to learn in a model, yet the
normal CRF training typically uses fully-labelled
sentences. Recently, T?ackstr?om et al. (2013) pro-
pose an approach to train a CRF model directly
from partial labels. The basic idea is to marginal-
ize the probability mass of the constrained sausage
in the cost function. The normal CRF training us-
ing fully-labelled sentences is a special case where
the sausage constraint is a linear line; while on
the other hand a zero-labelled sentence, where the
sausage constraint is the full lattice, makes no con-
tribution in the learning since the sum of proba-
bilities is deemed to be one. This new approach,
without the need of using constrained re-decoding
as a middle step, provides a direct means to learn
the knowledge in the partial labels.
In this research we explore using the partial-
label learning for semi-supervised Chinese word
segmentation. We use the CTB-6 corpus as the
labelled training, development and test data, and
use the Chinese Wikipedia as the unlabelled data.
We first train a baseline model with labelled data
only, and then selectively add Wikipedia data with
partial labels to build a second model. Because
the Wikipedia data is out of domain and has dis-
tribution bias, we also experiment with two do-
main adaptation techniques: model interpolation
and EasyAdapt (Daum?e III, 2007). Our result
reaches an F-measure of 95.98%, an absolute im-
provement of 0.72% over the very strong base-
91
line (corresponding to 15.19% relative error re-
duction), and 0.33% over the constrained decode
approach (corresponding to 7.59% relative error
reduction). We conduct a detailed error analy-
sis, illustrating how partial-label learning excels
constrained decode in learning the knowledge en-
coded in the Wikipedia data. As a note, our result
also out-performs (Wang et al., 2011) and (Sun
and Xu, 2011).
2 Partial-Label Learning with CRF
In this section, we review in more detail the
partial-label learning algorithm with CRF pro-
posed by (T?ackstr?om et al., 2013). CRF is an
exponential model that expresses the conditional
probability of the labels given a sequence, as
Equation 1, where y denotes the labels, x denotes
the sequence, ?(x, y) denotes the feature func-
tions, and ? is the parameter vector. Z(x) =
?
y
exp(?
T
?(x, y)) is the normalization term.
p
?
(y|x) =
exp(?
T
?(x, y))
Z(x)
(1)
In full-label training, where each item in the se-
quence is labelled with exactly one tag, maximum
likelihood is typically used as the optimization tar-
get. We simply sum up the log-likelihood of the n
labelled sequences in the training set, as shown in
Equation 2.
L(?) =
n
?
i=1
log p
?
(y|x)
=
n
?
i=1
(?
T
?(x
i
, y
i
)? log Z(x
i
))
(2)
The gradient is calculated as Equation 3, in
which the first term
1
n
?
n
i=1
?
j
is the empirical
expectation of feature function ?
j
, and the second
term E[?
j
] is the model expectation. Typically a
forward-backward process is adopted for calculat-
ing the latter.
?
??
j
L(?) =
1
n
n
?
i=1
?
j
? E[?
j
] (3)
In partial-label training, each item in the se-
quence receives multiple labels, and so for each
sequence we have a sausage constraint, denoted as
?
Y (x, y?). The marginal probability of the sausage
is defined as Equation 4.
p
?
(
?
Y (x, y?)|x) =
?
y?
?
Y (x,y?)
p
?
(y|x) (4)
The optimization target thus is to maximize the
probability mass of the sausage, as shown in Equa-
tion 5.
L(?) =
n
?
i=1
logp
?
(
?
Y (x
i
, y?
i
)|x
i
) (5)
A gradient-based approach such as L-BFGS
(Liu and Nocedal, 1989) can be employed to op-
timize Equation 5. The gradient is calculated as
Equation 6, where E
?
Y (x,y?)
[?
j
] is the empirical ex-
pectation of feature function ?
j
constrained by the
sausage, and E[?
j
] is the same model expectation
as in standard CRF. E
?
Y (x,y?)
[?
j
] can be calculated
via a forward-backward process in the constrained
sausage.
?
??
j
L(?) = E
?
Y (x,y?)
[?
j
]? E[?
j
] (6)
For fully-labelled sentences, E
?
Y (x,y?)
[?
j
] =
1
n
?
n
i=1
?
j
, and so the standard CRF is actually
a special case of the partial-label learning.
3 Experiment setup
In this section we describe the basic setup for
our experiments of semi-supervised Chinese word
segmentation.
3.1 Data
We use the CTB-6 corpus as the labelled data. We
follow the official CTB-6 guideline in splitting the
corpus into a training set, a development set, and a
test set. The training set has 23420 sentences; the
development set has 2079 sentences; and the test
set has 2796 sentences. These are fully-labelled
data.
For unlabelled data we use the Chinese
Wikipedia. The Wikipedia data is quite noisy
and asks for a lot of cleaning. We first filter out
references and lists etc., and sentences with ob-
viously bad segmentations, for example, where
every character is separated by a space. We
also remove sentences that contain mostly En-
glish words. We then convert all characters into
full-width. We also convert traditional Chinese
characters into simplified characters using the tool
92
mediawiki-zhconverter
2
. We then randomly select
7737 sentences and reserve them as the test set.
To create the partial labels in the Wikipedia
data, we use the information from cross-reference,
emphasis, and punctuation. In our pilot study we
found that it?s beneficial to force a cross-reference
or emphasis entity as a word when the item has
2 or 3 characters. That is, if an entity in the
Wikipedia has three characters it receives the la-
bels of ?BIE?; and if it has two characters it is la-
belled as ?BE?.
3
3.2 Supervised baseline model
We create the baseline supervised model by using
an order-1 linear CRF with L2 regularization, to
label a character sequence with the four candidate
labels ?BIES?. We use the tool wapiti (Lavergne
et al., 2010).
Following Sun et al. (2009), Sun (2010), and
Low et al. (2005), we extract two types of fea-
tures: character-level features and word-level fea-
tures. Given a character c
0
in the character se-
quence ...c
?2
c
?1
c
0
c
1
c
2
...:
Character-level features :
? Character unigrams: c
?2
, c
?1
, c
0
, c
1
, c
2
? Character bigrams: c
?2
c
?1
, c
?1
c
?0
,
c
0
c
1
, c
1
c
2
? Consecutive character equivalence:
?c
?2
= c
?1
, ?c
?1
= c
?0
, ?c
0
= c
1
,
?c
1
= c
2
? Separated character equivalence:
?c
?3
= c
?1
, ?c
?2
= c
0
, ?c
?1
= c
1
,
?c
0
= c
2
, ?c
1
= c
3
? Whether the current character is a punc-
tuation: ?Punct(c
0
)
? Character sequence pattern:
T (C
?2
)T (C
?1
)T (C
0
)T (C
1
)T (C
2
).
We classify all characters into four
types. Type one has three characters
??? (year) ??? (month) ??? (date).
Type two includes number characters.
Type three includes English characters.
All others are Type four characters.
Thus ?????S? would generate the
character sequence pattern ?41213?.
2
https://github.com/tszming/mediawiki-zhconverter
3
Another possibility is to label it as ?SS? but we find that
it?s very rare the case.
Word-level features :
? The identity of the string c[s : i] (i?6 <
s < i), if it matches a word from the
list of word unigrams; multiple features
could be generated.
? The identity of the string c[i : e] (i <
e < i+6), if it matches a word; multiple
features could be generated.
? The identity of the bi-gram c[s : i ?
1]c[i : e] (i ? 6 < s, e < i + 6), if
it matches a word bigram; multiple fea-
tures could be generated.
? The identity of the bi-gram c[s : i]c[i +
1 : e] (i?6 < s, e < i+6), if it matches
a word bigram; multiple features could
be generated.
? Idiom. We use the idiom list from (Sun
and Xu, 2011). If the current character
c
0
and its surrounding context compose
an idiom, we generate a feature for c
0
of
its position in the idiom. For example, if
c
?1
c
0
c
1
c
2
is an idiom, we generate fea-
ture ?Idiom-2? for c
0
.
The above features together with label bigrams
are fed to wapiti for training. The supervised base-
line model is created with the CTB-6 corpus with-
out the use of Wikipedia data.
3.3 Partial-label learning
The overall process of applying partial-label learn-
ing to Wikipedia data is shown in Algorithm 1.
Following (Jiang et al., 2013), we first train the
supervised baseline model, and use it to estimate
the potential contribution for each sentence in the
Wikipedia training data. We label the sentence
with the baseline model, and then compare the
labels with the constrained sausage. For each
character, a consistent label is defined as an ele-
ment in the constrained labels. For example, if
the constrained labels for a character are ?SB?,
the label ?S? or ?B? is consistent but ?I? or ?E? is
not. The number of inconsistent labels for each
sentence is then used as its potential contribution
to the partial-label learning: higher number indi-
cates that the partial-labels for the sentence con-
tain more knowledge that the baseline system does
not integrate, and so have higher potential contri-
bution. The Wikipedia training sentences are then
ranked by their potential contribution, and the top
93
Figure 2: Encoded knowledge: inconsistency ratio
and label reduction
K sentences together with their partial labels are
then added to the CTB-6 training data to build a
new model, using partial-label learning.
4
In our
experiments, we try six data points with K =
100k, 200k, 300k, 400k, 500k, 600k. Figure 2
gives a rough idea of the knowledge encoded in
Wikipedia for these data points with inconsistency
ratio and label reduction. Inconsistency ratio is the
percentage of characters that have inconsistent la-
bels; and label reduction is the percentage of the
labels reduced in the full lattice.
We modify wapiti to implement the partial-label
learning as described in Section 2. Same as base-
line, L2 regularization is adopted.
Algorithm 1 Partial-label learning
1. Train supervised baseline model M
0
2. For each sentence x in Wiki-Train:
3. y? Decode(x, M
0
)
4. diff? Inconsistent(y,
?
Y (x, ?y))
5. if diff > 0:
6. C? C ? (
?
Y (x, y?), diff)
7. Sort(C, diff, reverse)
8. Train model M
pl
with CTB-6 and top K sen-
tences in C using partial-label learning
3.4 Constrained decode
Jiang et al. (2013) implement the constrained de-
code algorithm with perceptron. However, CRF
is generally believed to out-perform perceptron,
yet the comparison of CRF vs perceptron is out
4
Knowledge is sparsely distributed in the Wikipedia data.
Using the Wikipedia data without the CTB-6 data for partial-
label learning does not necessarily guarantee convergence.
Also the CTB-6 training data helps to learn that certain la-
bel transitions, such as ?B B? or ?E E?, are not legal.
of the scope of this paper. Thus for fair compar-
ison, we re-implement the constrained decode al-
gorithm with CRF.
Algorithm 2 shows the constrained decode im-
plementation. We first train the baseline model
with the CTB-6 data. We then use this baseline
model to run normal decode and constrained de-
code for each sentence in the Wikipedia training
set. If the normal decode and constrained decode
have different labels, we add the constrained de-
code together with the number of different labels
to the filtered Wikipedia training corpus. The fil-
tered Wikipedia training corpus is then sorted us-
ing the number of different labels, and the top K
sentences with constrained decoded labels are then
added to the CTB-6 training data for building a
new model using normal CRF.
Algorithm 2 Constrained decode
1. Train supervised baseline model M
0
2. For each sentence x in Wiki-Train:
3. y? Decode(x, M
0
)
4. y?? ConstrainedDecode(x, M
0
)
5. diff? Difference(y, y?)
6. if diff > 0:
7. C? C ? (y?, diff)
8. Sort(C, diff, reverse)
9. Train model M
cd
with CTB-6 and top K sen-
tences in C using normal CRF
4 Evaluation on Wikipedia test set
In order to determine how well the models learn
the encoded knowledge (i.e. partial labels) from
the Wikipedia data, we first evaluate the mod-
els against the Wikipedia test set. The Wikipedia
test set, however, is only partially-labelled. Thus
the metric we use here is consistent label accu-
racy, similar to how we rank the sentences in Sec-
tion 3.3, defined as whether a predicted label for
a character is an element in the constrained la-
bels. Because partial labels are only sparsely dis-
tributed in the test data, a lot of characters receive
all four labels in the constrained sausage. Eval-
uating against characters with all four labels do
not really represent the models? difference as it is
deemed to be consistent. Thus beyond evaluating
against all characters in the Wikipedia test set (re-
ferred to as Full measurement), we also evaluate
against characters that are only constrained with
less than four labels (referred to as Label mea-
surement). The Label measurement focuses on en-
94
coded knowledge in the test set and so can better
represent the model?s capability of learning from
the partial labels.
Results are shown in Figure 3 with the Full
measurement and in Figure 4 with the Label mea-
surement. The x axes are the size of Wikipedia
training data, as explained in Section 3.3. As
can be seen, both constrained decode and partial-
label learning perform much better than the base-
line supervised model that is trained from CTB-6
data only, indicating that both of them are learning
the encoded knowledge from the Wikipedia train-
ing data. Also we see the trend that the perfor-
mance improves with more data in training, also
suggesting the learning of encoded knowledge.
Most importantly, we see that partial-label learn-
ing consistently out-performs constrained decode
in all data points. With the Label measurement,
partial-label learning gives 1.7% or higher abso-
lute improvement over constrained decode across
all data points. At the data point of 600k, con-
strained decode gives an accuracy of 97.14%,
while partial-label learning gives 98.93% (base-
line model gives 87.08%). The relative gain (from
learning the knowledge) of partial-label learning
over constrained decode is thus 18% ((98.93 ?
97.14)/(97.14 ? 87.08)). These results suggest
that partial-label learning is more effective in
learning the encoded knowledge in the Wikipedia
data than constrained decode.
5 CTB evaluation
5.1 Model adaptation
Our ultimate goal, however, is to determine
whether we can leverage the encoded knowledge
in the Wikipedia data to improve the word seg-
mentation in CTB-6. We run our models against
the CTB-6 test set, with results shown in Fig-
ure 5. Because we have fully-labelled sentences
in the CTB-6 data, we adopt the F-measure as
our evaluation metric here. The baseline model
achieves 95.26% in F-measure, providing a state-
of-the-art supervised performance. Constrained
decode is able to improve on this already very
strong baseline performance, and we see the nice
trend of higher performance with more unlabeled
data for training, indicating that constrained de-
code is making use of the encoded knowledge in
the Wikipedia data to help CTB-6 segmentation.
When we look at the partial-label model, how-
ever, the results tell a totally different story.
Figure 3: Wiki label evaluation results: Full
Figure 4: Wiki label evaluation results: Label
Figure 5: CTB evaluation results
95
First, it actually performs worse than the base-
line model, and the more data added to train-
ing, the worse the performance is. In the previ-
ous section we show that partial-label learning is
more effective in learning the encoded knowledge
in Wikipedia data than constrained decode. So,
what goes wrong? We hypothesize that there is
an out-of-domain distribution bias in the partial la-
bels, and so the more data we add, the worse the
in-domain performance is. Constrained decode
actually helps to smooth out the out-of-domain
distribution bias by using the re-decoded labels
with the in-domain supervised baseline model.
For example, both the baseline model and con-
strained decode correctly give the segmentation
???/?/??/?/???/?/??, while partial-
label learning gives incorrect segmentation ??
?/?/??/?/?/??/?/??. Looking at the
Wikipedia training data, ?? is tagged as an en-
tity 13 times; and ???, although occurs 13
times in the data, is never tagged as an entity.
Partial-label learning, which focuses on the tagged
entities, thus overrules the segmentation of ??
?. Constrained decode, on the other hand, by us-
ing the correctly re-decoded labels from the base-
line model, observes enough evidence to correctly
segment??? as a word.
To smooth out the out-of-domain distribution
bias, we experiment with two approaches: model
interpolation and EasyAdapt (Daum?e III, 2007).
5.1.1 Model interpolation
We linearly interpolate the model of partial-label
learningM
pl
with the baseline modelM
0
to create
the final model M
pl
+
, as shown in Equation 7. The
interpolation weight is optimized via a grid search
between 0.0 and 1.0 with a step of 0.1, tuned on
the CTB-6 development set. Again we modify
wapiti so that it takes two models and an interpo-
lation weight as input. For each model it creates
a search lattice with posteriors, and then linearly
combines the two lattices using the interpolation
weight to create the final search space for decod-
ing. As shown in Figure 5, model M
pl
+
consis-
tently out-performs constrained decode in all data
points. We also see the trend of better performance
with more training data.
M
pl
+
= ? ?M
0
+ (1? ?) ?M
pl
(7)
5.1.2 EasyAdapt
EasyAdapt is a straightforward technique but has
been shown effective in many domain adaptation
tasks (Daum?e III, 2007). We train the model
M
pl
ea
with feature augmentation. For each out-of-
domain training instance < x
o
, y
o
>, where x
o
is the input features and y
o
is the (partial) labels,
we copy the features and file them as an additional
feature set, and so the training instance becomes<
x
o
, x
o
, y
o
>. The in-domain training data remains
the same. Consistent with (Daum?e III, 2007),
EasyAdapt gives us the best performance, as show
in Figure 5. Furthermore, unlike in (Jiang et al.,
2013) where they find a plateau, our results show
no harm adding more training data for partial-label
learning when integrated with domain adaptation,
although the performance seems to saturate after
400k sentences.
Finally, we search for the parameter setting of
best performance on the CTB-6 development set,
which is to use EasyAdapt with K = 600k sen-
tences of Wikipedia data. With this setting, the
performance on the CTB-6 test set is 95.98%
in F-measure. This is 0.72% absolute improve-
ment over supervised baseline (corresponding to
15.19% relative error reduction), and 0.33% ab-
solute improvement over constrained decode (cor-
responding to 7.59% relative error reduction); the
differences are both statistically significant (p <
0.001).
5
As a note, this result out-performs (Sun
and Xu, 2011) (95.44%) and (Wang et al., 2011)
(95.79%), and the differences are also statistically
significant (p < 0.001).
5.2 Analysis with examples
To better understand why partial-label learning is
more effective in learning the encoded knowledge,
we look at cases where M
0
and M
cd
have the in-
correct segmentation while M
pl
(and its domain
adaptation variance M
pl
+
and M
pl
ea
) have the cor-
rect segmentation. We find that the majority is
due to the error in re-decoded labels outside of en-
coded knowledge. For example, M
0
and M
cd
give
the segmentation ???/?/?/?/6.9/??, yet the
correct segmentation given by partial-label learn-
ing is ???/?/??/6.9/ ??. Looking at the
Wikipedia training data, there are 38 tagged enti-
ties of??, but there are another 190 mentions of
5
Statistical significance is evaluated with z-test using the
standard deviation of
?
F ? (1 ? F )/N , where F is the F-
measure and N is the number of words.
96
?? that are not tagged as an entity. Thus for con-
strained decode it sees 38 cases of ??\B ?\E?
and 190 cases of ??\S ?\S? in the Wikipedia
training data. The former comes from the encoded
knowledge while the latter comes from re-decoded
labels by the baseline model. The much bigger
number of incorrect labels from the baseline re-
decoding badly pollute the encoded knowledge.
This example illustrates that constrained decode
reinforces the errors from the baseline. On the
other hand, the training materials for partial-label
learning are purely the encoded knowledge, which
is not impacted by the baseline model error. In this
example, partial-label learning focuses only on the
38 cases of ??\B ?\E? and so is able to learn
that?? is a word.
As a final remark, we want to make a point that,
although the re-decoded labels serve to smooth out
the distribution bias, the Wikipedia data is indeed
not the ideal data set for such a purpose, because
it itself is out of domain. The performance tends
to degrade when we apply the baseline model to
re-decode the out-of-domain Wikipedia data. The
errorful re-decoded labels, when being used to
train the model M
cd
, could lead to further er-
rors. For example, the baseline model M
0
is able
to give the correct segmentation ???/????
in the CTB-6 test set. However, when it is ap-
plied to the Wikipedia data for constrained de-
code, for the seven occurrences of???, three of
which are correctly labelled as ??\B?\I?\E?,
but the other four have incorrect labels. The fi-
nal model M
cd
trained from these labels then
gives incorrect segmentation ??/?/??/?/?
?/?/??/??/??/??/??? in the CTB-
6 test set. On the other hand, model interpolation
or EasyAdapt with partial-label learning, focusing
only on the encoded knowledge and not being im-
pacted by the errorful re-decoded labels, performs
correctly in this case. For a more fair comparison
between partial-label learning and constrained de-
code, we have also plotted the results of model in-
terpolation and EasyAdapt for constrained decode
in Figure 5. As can be seen, they improve on con-
strained decode a bit but still fall behind the cor-
respondent domain adaptation approach of partial-
label learning.
6 Conclusion and future work
There is rich information encoded in online web
data. For example, punctuation and entity tags de-
fine some word boundaries. In this paper we show
the effectiveness of partial-label learning in digest-
ing the encoded knowledge from Wikipedia data
for the task of Chinese word segmentation. Unlike
approaches such as constrained decode that use
the errorful re-decoded labels, partial-label learn-
ing provides a direct means to learn the encoded
knowledge. By integrating some domain adap-
tation techniques such as EasyAdapt, we achieve
an F-measure of 95.98% in the CTB-6 corpus, a
significant improvement from both the supervised
baseline and constrained decode. Our results also
beat (Wang et al., 2011) and (Sun and Xu, 2011).
In this research we employ a sausage constraint
to encode the knowledge for Chinese word seg-
mentation. However, a sausage constraint does
not reflect the legal label sequence. For exam-
ple, in Figure 1 the links between label ?B? and
label ?S?, between ?S? and ?E?, and between ?E?
and ?I? are illegal, and can confuse the machine
learning. In our current work we solve this issue
by adding some fully-labelled data into training.
Instead we can easily extend our work to use a lat-
tice constraint by removing the illegal transitions
from the sausage. The partial-label learning stands
the same, by executing the forward-backward pro-
cess in the constrained lattice. In future work we
will examine partial-label learning with this more
enforced lattice constraint in depth.
Acknowledgments
The authors would like to thank Wenbin Jiang, Xi-
aodong Zeng, and Weiwei Sun for helpful discus-
sions, and the anonymous reviewers for insightful
comments.
References
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Annual meetingassociation for computa-
tional linguistics, pages 256?263. Association for
Computational Linguistics.
Wenbin Jiang, Meng Sun, Yajuan Lv, Yating Yang,
and Qun Liu. 2013. Discriminative learning with
natural annotations: Word segmentation as a case
study. In Proceedings of The 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 761?769.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proceed-
ings of the 21st International Conference on Com-
97
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL-44, pages 209?216.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289, San Francisco, CA, USA.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for Chinese word segmentation.
Computational Linguistics, 35:505?512.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. Master?s thesis, MASSACHUSETTS
INSTITUTE OF TECHNOLOGY, May.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math-
ematical Programming, 45(3):503?528, December.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161?164, San Francisco, CA, USA.
Gideon S. Mann and Andrew McCallum. 2007. Ef-
ficient computation of entropy gradient for semi-
supervised conditional random fields. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association
for Computational Linguistics; Companion Volume,
Short Papers, NAACL-Short ?07, pages 109?112.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 970?
979, Edinburgh, Scotland, UK., July.
Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun?ichi Tsujii. 2009. A dis-
criminative latent variable Chinese segmenter with
hybrid word/character information. In Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 56?64.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: comparison and com-
bination. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 1211?1219.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing, pages 309?317.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, pages 29?48.
Fan Yang and Paul Vozila. 2013. An empirical study
of semi-supervised Chinese word segmentation us-
ing co-training. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1191?1200, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and
Isabel Trancoso. 2013a. Co-regularizing character-
based and word-based models for semi-supervised
chinese word segmentation. In Proceedings of The
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 171?176.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao,
and Isabel Trancoso. 2013b. Graph-based semi-
supervised model for joint chinese word segmen-
tation and part-of-speech tagging. In Proceedings
of The 51st Annual Meeting of the Association for
Computational Linguistics, pages 770?779.
Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for Chinese word seg-
mentation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 311?321, Seattle, Washington, USA,
October. Association for Computational Linguistics.
98
An Investigation of Interruptions and
Resumptions in Multi-Tasking Dialogues
Fan Yang?
Nuance Communications, Inc.
Peter A. Heeman
Oregon Health & Science University
Andrew L. Kun
University of New Hampshire
In this article we focus on human?human multi-tasking dialogues, in which pairs of con-
versants, using speech, work on an ongoing task while occasionally completing real-time tasks.
The ongoing task is a poker game in which conversants need to assemble a poker hand, and the
real-time task is a picture game in which conversants need to find out whether they have a certain
picture on their displays. We employ empirical corpus studies and machine learning experiments
to understand the mechanisms that people use in managing these complex interactions. First,
we examine task interruptions: switching from the ongoing task to a real-time task. We find
that generally conversants tend to interrupt at a less disruptive context in the ongoing task
when possible. We also find that the discourse markers oh and wait occur in initiating a task
interruption twice as often as in the conversation of the ongoing task. Pitch is also found to be
statistically correlated with task interruptions; in fact, the more disruptive the task interruption,
the higher the pitch. Second, we examine task resumptions: returning to the ongoing task after
completing an interrupting real-time task. We find that conversants might simply resume the
conversation where they left off, but sometimes they repeat the last utterance or summarize the
critical information that was exchanged before the interruption. Third, we apply machine learn-
ing to determine how well task interruptions can be recognized automatically and to investigate
the usefulness of the cues that we find in the corpus studies. We find that discourse context, pitch,
and the discourse markers oh and wait are important features to reliably recognize task interrup-
tions; and with non-lexical features one can improve the performance of recognizing task inter-
ruptions with more than a 50% relative error reduction over a baseline. Finally, we discuss the
implication of our findings for building a speech interface that supports multi-tasking dialogue.
1. Introduction
Existing speech interfaces have mostly been used to perform a single task, where
the user finishes with one task before moving on to the next. We envision that
? Nuance Communications, Inc., 505 First Ave. South, Suite 700, Seattle, WA 98104.
E-mail: fan.yang@nuance.com.
Submission received: 26 July 2009; revised submission received: 22 July 2010; accepted for publication:
13 October 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
next-generation speech interfaces will be able to work with the user on multiple tasks
at the same time, which is especially useful for real-time tasks. For instance, a driver in
a car might use a speech interface to catch up on e-mails, while occasionally checking
upcoming traffic conditions, and receiving navigation instructions; or a police officer
might need to be alerted to a nearby accident while accessing a database during a
routine traffic stop.
Several speech interfaces that allow multi-tasking dialogues have been built (e.g.,
Traum and Rickel 2002; Kun, Miller, and Lenharth 2004; Lemon and Gruenstein 2004;
Larsson 2003). However, it is unclear that the mechanisms of managing multiple ver-
bal tasks in these systems resemble human conventions or do the best to help users
with task switching. For complex domains, the user might be confused about which
task the interface is talking about, or might be confused about where they left off in
a task.
In order to build a speech interface that supports multi-tasking dialogue, we need to
determine a set of conventions that the user and interface can follow in task switching.
We propose to start with conventions that are actually used in human?human speech
conversations, which are natural for users to follow and probably efficient in problem-
solving. Once we understand the human conventions, we can try to implement them in
a dialogue manager and run user studies to verify the effectiveness of such conventions
in human?computer dialogue.
In this article we focus on understanding the human conventions of managing
multiple tasks. Multi-tasking dialogues, where multiple independent topics overlap
with each other in time, regularly arise in human?human conversation: For example,
a driver and a passenger in a car might be talking about their summer plans, while
occasionally interjecting road directions or conversation about what music to listen to.
However, little is known about how people manage multi-tasking dialogues. Given the
scenario where a real-time task with a time constraint arises during the course of an
ongoing task, we are specially interested in two switching behaviors: task interruption,
which is to suspend the ongoing task and switch to a waiting real-time task, and task
resumption, which is to return to the interrupted ongoing task after completing a real-
time task.
The first question we ask is how quickly conversants respond to a real-time task.
Intuitively if the real-time task is very urgent (e.g., the driver is about to miss a turn),
the passenger might want to immediately cut off the ongoing conversation, and notify
the driver of the turn. However, if the real-time task is less urgent, for example, the
driver does not like the music and wants the passenger to load another CD, do conver-
sants still immediately interrupt the ongoing conversation? If conversants do vary how
quickly they interrupt, are there any regularities of where conversants switch from the
ongoing task to the real-time task? We hypothesize that, given the choice, conversants
interrupt the ongoing task where the interruption is less disruptive to the ongoing
task.
The second question we ask is how conversants signal task interruptions. Previous
research showed that conversants signal the start of a new topic in single-tasking speech
(monologue and dialogue) with discourse markers and prosodic cues.We thus hypothe-
size that conversants also use these cues to signal task interruptions. We also investigate
whether conversants vary the intensity of the cues, and under what circumstances.
The third question we ask is what conversants do immediately upon resuming the
ongoing task. Switching to a real-time task causes the ongoing task to be temporarily
suspended. On completing the real-time task and returning to the ongoing task, do
conversants simply continue on fromwhere theywere interrupted?We hypothesize that
76
Yang, Heeman, and Kun Multi-Tasking Dialogues
conversants might sometimes perform certain actions to recover from the interruption.
For example, it is imaginable that conversants might ask where were we at for summer
plans, and then review what was discussed before the interruption.
To answer these questions, we collect the MTD corpus, which consists of a set
of human?human dialogues where pairs of conversants have multiple overlapping
verbal tasks to perform. In our research, we keep things relatively simple by having
conversants talk to each other to play two games on computers. The first game, the
ongoing task, is a poker game in which conversants need to assemble a poker hand,
which usually takes a relatively long time to complete. The second game, the real-time
task, is a picture game in which conversants need to find out whether they have a
certain picture on their displays, which can be done in a couple of turns but has a time
constraint. In Section 3, we describe the task setup and corpus collection. In Section 4,
we examine when and where conversants suspend the ongoing task and switch to the
real-time task. In Section 5, we examine how conversants signal task interruptions. In
Section 6, we examine the behavior of context restoration in task resumptions.
In addition to the three questions we have asked, in Section 7, we use machine
learning to automatically recognize task interruptions. Recognizing task interruptions
is an important component in building speech interfaces that support multi-tasking
dialogue. For example, the speech interface can accordingly switch the language model
when it detects that the user has switched to another task, which should improve
speech recognition performance (Iyer and Ostendorf 1999) and utterance understand-
ing, leading to higher user satisfaction (Walker, Passonneau, and Boland 2001). We run
machine learning experiments to determine how well we can automatically recognize
task interruptions and to understand the utility of the features that we found in our
corpus studies. Finally, we conclude the paper in Section 8. This paper includes and
extends Heeman et al (2005), Yang, Heeman, and Kun (2008), and Yang and Heeman
(2009) with more corpus data, more robust statistical analysis, more machine learning
experiments, and more comprehensive discussions.
2. Related Research
2.1 Existing Systems for Multi-Tasking Dialogues
There is some initial research effort in building speech interfaces to support multi-
tasking dialogue. Kun, Miller, and Lenharth (2004) developed a system called Project54,
which allowed a user to interact with multiple devices in a police cruiser using speech.
The architecture of Project54 allowed for handling multiple tasks overlapped in time.
For example, when pulling over a vehicle, an officer could first issue a spoken command
to turn on the lights and siren, then issue spoken commands to initiate a data query,
go back to interacting with the lights and siren (perhaps to change the pattern after
the vehicle has been pulled over), and finally receive the spoken results of the data
query. This example shows that system responses related to different tasks could be
interleaved: The system responded to the data query after the user had already switched
back to interacting with the lights and siren.
Lemon and Gruenstein (2004) also explored multi-tasking in a speech interface.
They built a speech interface for a human operator to direct a robotic helicopter on
executing multiple tasks, such as searching for a car and flying to a tower. The interface
kept an ordered set of active dialogue tasks, and interpreted the user utterance in terms
of the most active task for which the utterance made sense. Conversely, during the
77
Computational Linguistics Volume 37, Number 1
interface?s turn of speaking, it could produce an utterance for any of the dialogue tasks
and thus intermixed utterances from different tasks.
In Kun, Miller, and Lenharth (2004) or Lemon and Gruenstein (2004), the systems
did not explicitly signal tasks switching, either for task interruptions or for task re-
sumptions, but instead relied on semantic interpretation to determine which task an
utterance belonged to. Larsson (2003) built the GoDis system which hard-coded two
types of signals when resuming an interrupted conversation. The first type of signal was
to use the discoursemarker so to implicitly signal a topic resumption. The second type of
signal was to use the phrase returning to the issue of to explicitly resume an interrupted
topic. For example, when searching for the price of an air ticket with GoDis, the user
could suspend the system?s questionwhen do you want to travel by interjecting a question
do I need a visa. The system, after a short dialogue answering the user?s question about
a visa, would resume the ticket booking by returning to the issue of price.
Traum and his colleagues (Rickel et al 2002; Traum and Rickel 2002) developed the
Mission Rehearsal Exercise system in which the user and virtual humans collaborated
on multiple tasks that could interrupt each other. They created a scenario in which a
lieutenant (the user) was sent to a village for an Army peacekeeping task. However,
on his way, he encountered an auto accident in which his platoon?s vehicle crashed
into a civilian vehicle, injuring a local boy. The boy?s mother and an Army medic were
hunched over him, and a sergeant approached the lieutenant to brief him on the situ-
ation. These multiple virtual humans could interrupt or be involved in conversations
with the lieutenant. The authors proposed and partially implemented a multi-level
dialogue manager, with levels for turn-taking, initiative, grounding, topic management,
negotiation, and rhetorical structure. In their view, topic management included where
one topic is started before an old one is completed. They described how topic shifts
in general can be signaled with cue phrases, such as now and anyways, and with non-
verbal cues.
These researchworks show the usefulness of a spoken dialogue system being able to
handle multiple tasks, and promote a thorough examination of multi-tasking dialogue.
In this article we examine the conventions of task switching in human?human dialogue
as the first step towards understanding the practice of managing tasking switching in a
computer dialogue system.
2.2 Insights from Non-Verbal Task Switching
Research in cognitive science suggests that task interruptions and resumptions are
complicated behavior and warrant investigation. There is extensive research on the
disruptiveness of interruptions, in which individuals switch between multiple manual-
visual tasks. For example, Gillie and Broadbent (1989) found that the length (in time)
of an interruption is not an important factor, but that the real-time task?s complexity
and similarity to the ongoing task contribute to the disruptiveness. On the other hand,
in their study of checklists, Linde and Goguen (1987) found that it is not the number
of interruptions but the length of interruptions that affects the disruptiveness. Cutrell,
Czerwinski, and Hovitz (2001) examined the influence of instant messaging on users
performing ongoing computing tasks, and found that interruptions unrelated to the
ongoing task resulted in longer task resumptions. Although these results do not appear
to always converge on the same conclusions, they suggest that task switching can be
disruptive to users.
Researchers have been trying to minimize the disruptive effect of task switching
in human?computer interaction. McFarlane (1999) explored four alternatives for when
78
Yang, Heeman, and Kun Multi-Tasking Dialogues
to suspend the ongoing task and switch to the interruption, namely, immediate, negoti-
ated, mediated, and scheduled, and found mixed results. Renaud (2000) argued for, and
built, a prototype of a visualization tool to help users restore the context of the ongoing
task when returning from an interruption. Hess and Detweiler (1994) and Gopher,
Greenshpan, and Armony (1996) found that the disruptive effects are reduced as people
gain more experience with interruptions. These studies suggest that it is worthwhile to
investigate how a computer dialogue system should manage task switching.
2.3 Insights from Discourse Structure Research
Research in discourse structure also sheds light on task switching. It is important to
understand the conventions that people use to manage discourse structure as these
might also be used for managing multiple tasks. According to Grosz and Sidner (1986),
the structure of a discourse is a combination of linguistic structure, intentional structure,
and attentional state. The linguistic structure is a hierarchical segmentation of the
dialogue. Each segment has a purpose, which is established by the conversant who
initiates the segment. The purposes come together to form the intentional structure.
The attentional state contains the objects, properties, and relations that are most salient
at any point in the dialogue. The attentional state is claimed to work like a stack. When
a new segment is started, a new focus space is created on top of the attentional stack.
When the segment completes, the focus space is popped off.1
Signaling discourse structure in single-tasking speech is about signaling the bound-
ary of related discourse segments that contribute to the achievement of a discourse
purpose. Two types of cues have been identified. The first type is discourse markers
(Grosz and Sidner 1986; Schiffrin 1987; Moser and Moore 1995; Passonneau and Litman
1997; Bangerter and Clark 2003). Discourse markers can be used to signal the start of a
new discourse segment and its relation to other discourse segments. For example, now
might signal moving on to the next topic, andwellmight signal a negative or unexpected
response.
The second type of cue is prosody. In read speech, Grosz and Hirschberg (1992)
studied broadcast news and found that pause length is the most important factor that
indicates a new discourse segment. Ayers (1992) found that pitch range appears to cor-
relate more closely with hierarchical topic structure in read speech than in spontaneous
speech. In spontaneous monologue, Butterworth (1972) found that the beginning of a
discourse segment exhibits slower speaking rate; Swerts (1995) and Passonneau and
Litman (1997) found that pause length correlates with discourse segment boundaries;
Hirschberg and Nakatani (1996) found that the beginning of a discourse segment corre-
lates with higher pitch. In human?human dialogue, similar behavior has been observed:
The pitch value tends to be higher for starting a new discourse segment (Nakajima and
Allen 1993). In human?computer dialogue, Swerts and Ostendorf (1995) found that the
first utterance of a discourse segment correlates with slower speaking rate and longer
preceding pause. Thus, we are interested in whether discourse markers and prosodic
cues are also used in signaling task interruptions in multi-tasking dialogue.
1 Grosz and Sidner (1986) also briefly talked about interruptions. In their discourse structure theory,
interruptions are modeled as special discourse segments. When a task interruption happens, an
attentional state is created for the real-time task and pushed on top of the discourse stack. There is
an impenetrable separation between the attentional state of the real-time task and the interrupted
ongoing task, so that the real-time task cannot access the ongoing task. When the real-time task is
completed, its attentional state is popped off and the ongoing task becomes salient.
79
Computational Linguistics Volume 37, Number 1
3. The MTD Corpus
In order to better understand multi-tasking human?human dialogue, we collected the
MTD corpus, in which pairs of players perform overlapping verbal tasks.
3.1 Design of Tasks
For the MTD corpus, we decided to have players complete two types of tasks via
conversation: an ongoing task and real-time tasks. The ongoing task needs to build up
significant context that players have to keep in mind. On task resumption, this context
is needed to finish the task, and so might need to be re-established. The task should
also encourage both players to equally participate as we believe that mixed-initiative
will be the conversational mode in future speech interfaces. The real-time task can be
kept simple: It does not build up much context and can be finished in a couple turns.
However, we vary the urgency of this task.
For the ongoing task, a pair of players collaborate to assemble as many poker
hands as possible, where a poker hand consists of a full house, flush, straight, or four
of a kind. Each player initially has three cards in hand, which the other cannot see.
Players take turns drawing an extra card and then discarding one, until they find a
valid poker hand, for which they earn 50 points; they then start over to form another
poker hand. To discourage players from rifling through the cards to look for a specific
one without talking, one point is deducted for each picked-up card, and ten points for a
missed or incorrect poker hand. To complete this game, players converse to share card
information, and explore and establish strategies based on the combined cards in their
hands (Toh, Yang, and Heeman 2006). The poker game is played on computers. The
game display, which each player sees, is shown in Figure 1. The player with four cards
can click on a card to discard it. The card disappears from the screen, and a new card
is automatically dealt to the other player. Once they find a poker hand the player with
four cards clicks the Done Poker Hand button to start a new game.
The real-time task is a picture game. From time to time, the computer prompts one
of the players to determine whether the other has a certain picture on the bottom of the
display. The picture task has a time constraint of 10, 25, or 40 seconds, which is (pseudo)
randomly determined. Two solid bars above and below the player?s cards flash when
there is a pending picture game. This should alert the player to a pending picture game
without taking the attention away from the poker game. The color of the flashing bars
depends on howmuch time remains: green for 26?40 seconds, yellow for 11?25 seconds,
and red for 0?10 seconds. The player can see the exact amount of time left in the heading
of the picture game. In Figure 1, the player needs to find out whether the other player
has a blue circle, with 6 seconds left. The players get 5 points if the correct answer is
given in time. The overall goal of the players is to earn as many points as possible from
the two tasks.
3.2 Corpus Collection
We recruited six pairs of players, who each received US $10 for completing the data
collection. All players were native American English speakers, and had a bachelor?s
degree or higher in computer science or electrical engineering. None of the players were
in our research lab, and there was no evidence that any player knew about our research
program before they participated.
80
Yang, Heeman, and Kun Multi-Tasking Dialogues
Figure 1
The game display for players.
The data collection for each pair of players lasted about one hour. Players were
separated so that they could not see each other and they talked to each other through
headsets. After a short orientation, the players played the poker game for about 5
minutes to become familiar with the rules. They then had a practice conversation with
both the poker game and the picture game for about 15 minutes, so that they got used to
managing both tasks. Finally, they had two more conversations, each lasting for about
15 minutes. In each conversation, nine picture games, three for each urgency level, were
prompted for each player. In this research, we analyze the last two conversations, but
not the practice one. Thus we have a total of about 180 minutes of conversation from
the six pairs of players.
For each dialogue, we recorded both channels of speech (each in an audio file) and
created a log file. The log file contains all the events of the computer dealer and the
GUI actions of the two players for each task with time-stamps. For the poker game, it
contains information of when a card is dealt or discarded, and information of when a
poker hand is achieved or missed; for the real-time task, it contains each question, the
time it is generated, the answer, and the time it is answered.
A post-experiment survey was conducted in which players were given the follow-
ing questions: (1) Did you ever play poker before you participated in this experiment?
(2) Did you always immediately notice the flashing that signaled a new picture task?
81
Computational Linguistics Volume 37, Number 1
Table 1
Summary statistics of game, card, and picture segments for each pair of players.
R1 R2 R3 R4 R5 R6 Total
Game segments 7 13 39 35 11 15 120
Card segments 40 118 227 225 82 89 781
Picture segments 30 36 36 35 35 36 208
(3) Did you ever purposefully ignore a picture task? (4) How did you make use of the
different urgency levels (40, 25, or 10 seconds)? (5) How did the picture task affect the
poker game? (6) Do you have any other comments? All players had at least some poker
experience. All players reported that they always noticed the bars immediately when
they started to flash, and that they never ignored a real-time task on purpose. Some
players also mentioned that they enjoyed the games.
3.3 Dialogue Segmentation
We segmented each dialogue into utterances using consensus annotations (see Yang and
Heeman [2010] for more details), following the guidelines of the Trains corpus (Heeman
and Allen 1995). We also annotated each utterance as to whether or not it is a trivial
utterance. We define trivial utterances as those that are just a stall (such as uh and um)
or a simple acknowledgement (such as okay, uh-huh, and alright). According to Strayer,
Heeman, and Yang (2003), annotators reached high inter-coder agreement on a similar
annotation scheme.2 There are in total about 4,300 non-trivial utterances in playing the
poker game.
The ongoing task can be naturally divided into individual poker games, in which
the players successfully complete a poker hand. Each poker game can be further divided
into a sequence of card segments, in which players discuss which card to discard, or
players identify a poker hand. In total, there are 120 game segments and 781 card
segments in the corpus. We also group the utterances involved in each picture game
into a segment. Of the 216 prompted picture games, 8 were never started, although
players reported that they never ignored a picture game. Hence we have 208 picture
games. Table 1 shows the statistics for each pair of players (R1, R2, ..., R6).
Figure 2 shows an excerpt from an MTD dialogue with the segmentations. Here b7
is a game segment in which players get a poker hand of a flush; and b8, b10, b11, b12,
and b14, inside of b7, are card segments. Also embedded in b7 are b9 and b13, each of
which is a segment for a picture game. As can be seen, players switch from the ongoing
poker-playing to a picture game. After the picture game is completed, the conversation
on the poker-playing resumes.
Most of the segments can be automatically derived from the log file. For example,
the time a new hand is dealt is usually the start of a new game segment; the time a
new card is dealt is usually the start of a new card segment. We then manually fixed
any mistakes. For example, a mis-generated segment is removed where a player simply
discarded a card without any discussion; and a segment boundary is moved if an
utterance about the card being discarded, typically an acknowledgment, is said after
the new card is dealt.
2 They reported an inter-annotator agreement of 92%, which corresponded to ? = 0.83.
82
Yang, Heeman, and Kun Multi-Tasking Dialogues
Figure 2
An excerpt of an MTD dialogue.
The game, card, and picture segments are cohesive units of discourse in which the
conversants attempt to complete a domain task, that of winning the card game, deciding
what card to discard, or identifying a picture. Thus they follow Grosz and Sidner?s
(1986) definition of discourse segments.
3.4 Discourse Context
We define discourse context on the task level. We distinguish three types of discourse
context where a player suspends the poker playing and switches to a pending picture
game: (G) immediately after completing a poker game (at the end of a game), (C)
immediately after discarding a card (at the end of a card discussion), and (E) embedded
in a card discussion, where players are deciding which card to discard. Corresponding
to our dialogue segmentations, an interruption at the end of a game is thus a picture
game segment between two poker game segments; an interruption at the end of a card
is a picture segment between two card segments; and an interruption embedded in a
83
Computational Linguistics Volume 37, Number 1
card discussion is a picture segment embedded in a card segment. As shown in Figure 2,
both b9 and b13 are interruptions at the end of a card discussion.
4. Where to Interrupt
In this section, we examine whether players wait for certain discourse contexts in the
poker playing to interrupt with a picture game.
4.1 Response Delay
During poker playing, if a picture game is prompted, the bars around the cards flash in
different colors depending on the amount of time left. It is up to the player to decide
when to start the picture game (by asking the other player whether there is a certain
picture at the bottom of the display). The players can start the picture game as soon
as they notice it, for example, within one second; or they can delay the picture game,
for example, for 35 seconds, if the time constraint allows. We thus examine the response
delay, defined as the time interval between when a picture game is prompted and when
the player starts it, to understand how soon a player responds to a picture game. We are
particularly interested in how players respond to different urgency levels, i.e., whether
players wait longer when they are given more time.
Figure 3 shows the average response delay for each player for the urgency levels of
10 sec (black), 25 sec (gray), and 40 sec (white), with the actual values displayed in the
columns below. There are certainly individual differences. Player 5A seems to respond
to a real-time task as soon as the bars start flashing, regardless of the urgency levels.
In fact, in 17 out of the 18 picture games, 5A has less than three seconds of response
delay; and the longest response delay is only 3.22 seconds. Player 4B also has interesting
behavior: He waits a significant amount of time under the urgency level of 25 sec, but
promptly responds under the urgency level of 40 sec. However, overall the response
delay under the urgency levels of 40 sec (M = 12.5 sec) or 25 sec (M = 9.7 sec) is much
higher than under the urgency level of 10 sec (M = 2.8 sec). The response delay for 40 sec
is significantly higher than for 10 sec, t(11) = 4.2, p < 0.001; as is for 25 sec versus 10 sec,
t(11) = 6.36, p < 0.001. In fact, for question (4) how did you make use of the different urgency
levels (40, 25, or 10 seconds) in the post-experiment survey, all players but 5A answered
that they waited to initiate the picture game when they were given 25 sec or 40 sec (5A
answered ?not really.?) The 10 sec urgency level requires players to start a picture game
very quickly in order to complete it in time. On the other hand, when given 25 sec or
40 sec, players are in less of a hurry to switch.
4.2 Urgency Level and Discourse Context
The results on response delay show that players do not always start the real-time
picture game as soon as the bars start flashing, especially when players are given 25 sec
or 40 sec. Of course there are individual differences: Some players wait longer, some
players wait less time, and one does not even wait. The more interesting question,
however, is if players do not immediately start the picture game, what is the purpose of
delaying the switch to this real-time task? Are players delaying the switch just because
they feel that they have time and thus do not need to rush, or because they want to
interrupt at a certain point in the ongoing task?
84
Yang, Heeman, and Kun Multi-Tasking Dialogues
Figure 3
Response delay for different urgency levels.
Figure 4
Distribution of discourse contexts for task interruptions under different urgency levels.
We now examine how the urgency level affects where in the discourse context
players interrupt the ongoing task and switch to the real-time task. Because we do not
find a statistically significant difference of response delay under the urgency levels of
25 sec and 40 sec, t(11) = 1.51, p = 0.16, we combine these two urgency levels in this
analysis.3
Figure 4 shows the distribution of the discourse contexts of task interruptions for the
urgency levels. Overall the percentage of embedded interruptions for the 10 sec urgency
level (M = 76%) is significantly higher than for 25/40 sec (M = 47%), t(11) = 4.46, p <
0.001. In fact, all players except 4A have a higher percentage of embedded interruptions
for 10 sec than for 25/40 sec. The percentage of interruptions at the end of a game for
10 sec (M = 3%) is significantly lower than for 25/40 sec (M = 20%), t(11) = 4.16, p <
0.001. In fact, all players have a higher or equal percentage of interruptions at the end
of a game for 25/40 sec than for 10 sec. These results suggest an answer to our question
about why players delay switching to the ongoing task. When players are given more
time, that is, when the picture game is less urgent, players often utilize the additional
3 In fact, we find that under the urgency levels of 25 sec and 40 sec, players behave similarly in terms of
the discourse context of task interruptions. The reason for the lack of difference might be that it takes on
average 90 seconds to complete a poker hand and 14 seconds to complete a card segments. Hence, there
is little to be gained from separately reasoning about the 25 sec versus 40 sec urgency levels. In hindsight,
we should have used a longer time for the lowest urgency level.
85
Computational Linguistics Volume 37, Number 1
time to delay the switch to the real-time task such that this switch would happen at the
end of a game or a card rather than in the middle of a card discussion.
4.3 Response Delay and Discourse Context
In Section 4.2, we find that players tend to interrupt more often at the end of a card or a
poker game when they are given more time. However, players do not necessarily wait
for more time when the picture game is less urgent. For example, player 5A seems to
always start a picture game as soon as the bars start flashing, regardless of how urgent
the picture game is. To better understand the rationale of delaying a prompted picture
game, we next examine the correlation between response delay and the discourse
context where the switch to the real-time task occurs.
We assume that if the response delay is shorter than some amount of time, say t1,
players intend to start the picture game as soon as possible; we also assume that if the
response delay is longer than some other time, say t2, players intend to delay the picture
game. For the window between t1 and t2, it is unclear as to what players are doing due
to individual differences. In this article, we set t1 to 3 seconds and t2 to 6 seconds. From
listening to the dialogues, it seems to us that when players interrupt within 3 seconds,
they intend to do so right away, and when players wait at least 6 seconds, they do not.
These two time points are also consistent with human performance in task switching
(Meiran, Chorev, and Sapir 2000). This gives us 77 cases of interruptions with a response
delay of less than 3 seconds, 88 cases greater than 6 seconds, and 43 cases in between.
We have also examined other time thresholds, and find similar results.
Figure 5 shows the distribution of the discourse contexts of task interruptions
regarding the response delay. Because 5A always starts a picture game as soon as the
bars start flashing, we do not have data for when he waits for more than 6 seconds. We
thus exclude 5A from this analysis. The percentage of embedded interruptions for less
than 3 sec response delay (M = 71%) is significantly higher than for more than 6 sec
response delay (M = 41%), t(10) = 3.54, p = 0.002. The percentage of interruptions at the
end of a game for less than 3 sec response delay (M = 5%) is significantly lower than
for more than 6 sec response delay (M = 23%), t(11) = 3.49, p = 0.003. Compared with
immediately starting a picture game, if players wait for a certain amount of time, they
are more likely to suspend the ongoing task at the end of a poker game or a card than
to suspend the ongoing task in the middle of a card discussion.
Figure 5
Distribution of discourse contexts for task interruptions under different response delays.
86
Yang, Heeman, and Kun Multi-Tasking Dialogues
4.4 Discussion
In our research, we define task-level discourse contexts, and investigate the discourse
contexts where task interruptions of different urgency occur. We first examine the
response delay, and find that players do not always interrupt the poker playing as soon
as a picture game starts flashing, but instead they tend to wait longer for less urgent
picture games. We then examine the correlation between discourse context and urgency
level, and find that when given more time players tend to switch more often to a picture
game at the end of a (poker) game or a card. We finally examine the correlation between
discourse context and response delay, and find that if players wait for at least a certain
amount of time, they tend to switch more often to a picture game at the end of a (poker)
game or a card. These results suggest that players prefer to interrupt at the end of a
game or a card rather than interrupt in the middle of a card discussion. In fact, after the
practice session, player pair R3 explicitly decided that they should try to delay a picture
game until the end of a poker game. In other work, Shyrokov, Kun, and Heeman (2007)
examined the correlation between task interruption and conversational-level discourse
context. Similarly, they found that conversants try to avoid interrupting adjacency
pairs.
Discourse context is probably not the only factor that determines when players
switch tasks. We observed that sometimes players had time but still chose to interrupt
inside a card discussion; or that sometimes players waited past a card segment and then
interrupted inside the new card discussion. One guess is that at certain points in a card
discussion, players have less cognitive load and so switch tasks. Another guess is that
at certain points during poker playing, players get frustrated and decide to switch to a
pending picture game. However, these analyses are beyond the scope of this article.
5. Signaling Task Interruption
In this section, we examine how players signal that they are switching from the ongoing
task to a real-time task. In Section 2.3, we discussed how people use certain cues, such as
discourse markers and prosody, to signal discourse structure in single-tasking speech.
This suggests that peoplemight also signal task interruptions inmulti-tasking dialogues
and might even use similar cues.
5.1 Discourse Markers
First, we examine whether discourse markers co-occur with task interruptions. For this
exploratory study, we treat any word that can serve as a discourse maker and that
precedes a task interruption as a discourse marker, even though their roles in dialogue
are sometimes ambiguous, such as and, now, and okay (Gravano et al 2007). We also
include the fillers uh and um, which were shown to sometimes have a discourse function
(Swerts 1998).
Of the total 208 task interruptions, 76 are initiated with a discourse marker, which
accounts for 36.5%.We list these discoursemarkers in Table 2 grouped by their discourse
function. For their use in task interruptions, column 2 shows the number of occurrences
of each group and column 3 shows the number of players who use them. The first
group consists of oh and wait, which are usually used to signal a sudden or urgent
event (Heritage 1984; Schiffrin 1987; Byron and Heeman 1997). This group has the
most frequently uttered discourse markers in task interruption with 27 occurrences, and
seven players utter them at least once. The second group consists of the fillers uh and
87
Computational Linguistics Volume 37, Number 1
Table 2
List of discourse markers used in task interruptions.
Discourse Markers Total Occurrences Number of Players
oh wait 27 7
um uh 23 10
now okay alright 13 8
and 10 5
OTHERS (so but hey) 3 3
um. This group is uttered by the most players with 23 occurrences. The third group
consists of now, okay, and alright, which can signal the end of the current topic and
moving on to the next (Hirschberg and Litman 1987; Gravano et al 2007). This group
has 13 occurrences by eight players. The word and is uttered 10 times by five players.
Finally there is one occurrence of so, one of but, and one of hey. Interestingly, there are
also two cases of calling the name of the other player, such as Gary do you have a blue
triangle?
We next examine the discourse markers oh and wait in more depth. We choose them
because this group co-occurs most frequently with task interruptions, and because task
interruptions involve starting a new and urgent task, which fits their discourse function.
Verifying whether oh and wait are being used as discourse markers is straightforward.
We manually verified that all 27 instances of oh and wait that initiated a picture game
are discourse markers, and we also identified all usages of oh and wait in poker playing
that are discourse markers. For each player, we calculated the rate of task interruptions
initiated with an oh or wait, and compared it with two baselines: (1) the rate of non-
trivial utterances in poker playing that are initiated with an oh or wait, and (2) the rate
of card segments that are initiated with an oh or wait. The rate of task interruptions
initiatedwith an oh orwait (M = 12.7%) is significantly higher than the rate of utterances
initiatedwith an oh orwait (M = 5.7%), t(11) = 1.80, p = 0.05. It is also higher than the rate
of card segments initiated with an oh orwait (M = 7.1%), which is marginally significant
t(11) = 1.66, p = 0.06. These results suggest that the discourse markers oh and wait are
sometimes used in signaling task interruptions.
5.2 Prosody
To understand the prosodic cues in initiating a topic, traditionally researchers compared
the prosody of the first utterance in each topic with other utterances (e.g., Nakajima and
Allen 1993; Hirschberg and Nakatani 1996). For example, they calculated the average
pitch in the utterance or the first part of the utterance that initiates a topic and found
that it is higher than the other utterances in the topic. This approach encounters two
problems here. First, the words in an utterance might affect the prosody. For example,
the duration and energy of bat are usually larger than bit. Thus a large amount of data are
required to balance out these differences. Second, in the MTD corpus, players typically
switch to a picture game by using a yes?no question, such as do you have a blue circle,
whereas most non-trivial utterances in the ongoing task are statements or proposals. As
questions have very different prosody than statements or proposals, a direct comparison
is further biased.
Examination of the MTD corpus finds that 82% (170/208) of the picture games are
initiated by do you have ... with optional discourse markers at the beginning. While in
88
Yang, Heeman, and Kun Multi-Tasking Dialogues
Figure 6
Average pitch of do you have for task interruptions and poker playing.
the poker game, players use do you have ... 115 times to ask whether the other has certain
cards, such as do you have a queen? This abundance of utterances with identical initial-
wording and speech-act-type inspired us to compare the prosody of the phrase do you
have in switching to a picture game and during poker-playing.4 This avoids comparing
prosody of different words or of different types of utterances.
We measure pitch, energy (local root mean squared measurement), and duration
of each case of do you have. We aggregate on each individual player and calculate the
average values. Figure 6 shows the average pitch of the phrase do you have in task inter-
ruption (INT) and poker-playing (PKR) of each player, with the actual values displayed
in the columns below. For task interruption, players? average pitch is significantly
higher than poker-playing, t(11) = 4.82, p < 0.001. In fact, for each of the 12 players,
the average pitch of do you have in task interruption is higher than in poker-playing.
These results show a strong correlation between task interruption and higher pitch.
We also examine energy and duration (speaking rate) for the phrase do you have in
task interruption and poker-playing. However, we do not find a statistically significant
difference in energy, t(11) = 1.53, p = 0.16, or in duration t(11) = 1.67, p = 0.12.
5.3 Intensity of Cues
To better understand how pitch is used in signaling task interruptions, we next examine
whether it correlates with the discourse context of interruptions, namely, interrupting
at the end of a game, at the end of a card discussion, or embedded in a card discussion.
Because there are relatively fewer data for interrupting at the end of a game, we combine
interruptions at the end of a game and at the end of a card discussion (G/C).
Figure 7 shows the average pitch of do you have when switching to a picture game
embedded in a card discussion, at the end of a game or card discussion, and during
poker-playing (i.e., no task switching involved), with the actual values displayed in the
columns below. The difference between these three conditions is statistically significant,
F(2, 11) = 21.60, p < 0.001. Interruptions embedded in a card discussion has a signifi-
cantly higher pitch than at the end of a game or card discussion, t(11) = 5.74, p < 0.001,
4 It would have been interesting to compare the prosody of utterances that initiate a picture game and
those that initiate a card segment. However, we do not have enough utterances that initiate a card
segment that begin with do you have.
89
Computational Linguistics Volume 37, Number 1
Figure 7
Average pitch of do you have for different discourse contexts.
which in turn has a significantly higher pitch than during poker-playing, t(11) = 3.56,
p = 0.002. These results suggest a statistical correlation between discourse context of
task interruption and intensity of cues.5
5.4 Discussion
We find that discourse markers are sometimes used to mark task interruptions, but for
less than 40%. For the discourse markers oh and wait, we find a statistical correlation
between their use and task interruptions. This result should not be surprising as task
interruptions involve a sudden change of the conversation topic, and previous research
found that conversants use oh to mark a change of state in orientation or awareness.
wait is used to mark a discontinuity in the ongoing topic, which is also required by
task switching. Thus, it seems natural for people to use these discourse markers to
signal switching to a real-time task. The use of the other discourse markers is less clear,
but we have some speculations about their use with task interruptions. The discourse
markers now, okay, and alright tend to start a new topic in single-tasking speech, which
is consistent with initiating a task interruption. The fillers um and uh might be used to
hold the floor giving the player who initiates the picture game extra time to mentally
switch tasks; or they might be used to help mark the switch itself, similar to how they
sometimes mark topic shifts (Swerts 1998). Calling the name of the other player or
saying heymight be used to alert the other player of the task switching.
We also find that players signal task interruptions with prosodic cues. Pitch turns
out to be the most prominent feature. Not only do we find a strong correlation between
higher pitch and task interruption, but we also find a correlation between pitch and
the discourse context of the interruption. Switching embedded in a card segment has a
higher pitch than switching at the end of a card segment or a game, which in turn has
a higher pitch than non-switching (poker-playing). We speculate that pitch, as well as
discourse markers and calling the name of the other player, is used to disengage the
hearer from the ongoing task, signaling an unexpected event (see Section 8.1 for more
discussion).
5 We are also interested in whether players alter their use of discourse markers depending on the place of
interruption. However, perhaps due to a lack of data, we do not find a statistical difference.
90
Yang, Heeman, and Kun Multi-Tasking Dialogues
On the other hand, we do not find a statistically significant correlation between
energy and task interruption, or between speaking rate and task interruption. It would
be interesting to understand why pitch is used yet not other prosodic cues. In another
study, in which we examined initiative conflicts, where both conversants speak at the
same time trying to steer the conversation in different directions, we found that energy
is the dominant device for resolving who wins the conflict (Yang and Heeman 2010).
Probably conversants use different prosodic devices, such as pitch, energy, and speaking
rate, for different conversational functions. Further research is needed to explore this
hypothesis.
Finally, it is also interesting to investigate whether players signal the urgency of the
real-time task. In our task setup, besides the urgency level, which is the time (10 sec,
25 sec, or 40 sec) initially given to the players to complete a picture game, a more
important factor that defines urgency is the remaining time, which is the time left
to complete a picture game when players switch to it. Intuitively, when players start
a picture game, the more time that is remaining to finish the task, the less hurried
they need to be. However, we were not able to find a statistical correlation between
urgency level and pitch, or between remaining time and pitch. Norwas there a statistical
correlation with volume or speaking rate. Our explanation is that our task setup might
not be complicated enough. It only takes a couple utterances to finish a picture game,
and players were able to start the picture task far enough ahead that remaining time
was rarely a factor.
6. Context Restoration
On completing an interrupting picture game, players resume poker playing. Due to
our task setup, players tend to mutually know when a picture game ends. Thus we do
not examine how players signal task resumption, but instead we focus on how players
restore the context of the ongoing task, that is, how players re-establish the conversation
on poker playing after being interrupted by a picture game. We use the same distinction
of discourse contexts as we use in examining task interruptions: (1) restoration in the
middle of a card discussion, which corresponds to the players interrupting embedded
in a card discussion; (2) restoration at the beginning of a card, which corresponds to the
players interrupting after a card discussion, and then resuming to poker playing with
one of the players having a new card; and (3) restoration at the beginning of a game,
which corresponds to the players interrupting at the end of a poker game, and then
resuming to poker playing with the beginning of another poker game.
6.1 Restoration in the Middle of a Card Discussion
We start by investigating context restoration in the middle of a card discussion, because
these have the most context. We explored the corpus to look for signs of context restora-
tion behavior after an embedded interruption, by examining informational redundancy
(Walker 1996) of the first non-trivial utterance after completing a picture game.
Probably due to the simplicity of the picture game, especially that it can be com-
pleted in a couple turns, we find that after completing an embedded picture game
players usually continue poker playing without a clear indication of context restoration.
As shown in Example (1), B suspends his own question in poker playing and interrupts
with a picture game. After the completion of the picture game, A gives the answer to
B?s original question right away: The dialogue on poker playing continues as if the
interruption never happened.
91
Computational Linguistics Volume 37, Number 1
Example 1 (Continuation)
B: what do you have to make a high straight with?
B: you got a red circle?
A: no
A: I have a ten of diamonds and an ace of clubs
We do, however, find two types of utterances at the beginning of a resumption that
are informationally redundant (Walker 1996), as listed here.
Utterance Restatement: The first non-trivial utterance after the interruption is a re-
statement of the last non-trivial utterance before the interruption. This can be
further divided into three sub-categories: A) self-repetition: the player repeats
(part of) his or her own utterance, as shown in Example (2); B) other-repetition:
the player repeats (part of) the other?s utterance, as shown in Example (3); and C)
clarification: the player asks for a repetition with a clarification question, as shown
in Example (4).
Example 2 (Self-Repetition)
B: I have three clubs right now
B: do you have a yellow square?
A: yes
B: I have three clubs
B: do you have any clubs?
Example 3 (Other-Repetition)
B: I have jack and two queens
B: um do you have a yellow plus sign?
A: yes
A: a jack and two queens
A: I have a ten
Example 4 (Clarification)
A: I have a six of clubs a nine of spades and a four of diamonds
B: okay
B: okay how about a uh red cross
A: no
B: okay
B: four diamonds six something?
A: clubs
Card Review: The player re-communicates what cards are in hand, as shown in Exam-
ple (5). We define card review as utterances that inform of all of the cards in the
player?s hand, and where this information has already been communicated.
Example 5 (Card Review)
A: so I got a ten of spades
B: alright
B: and do you have a red circle?
A: um yes
B: I mean no no a blue circle
A: oh yes
A: and okay I have a queen of spades a ten of I mean a queen
of diamonds a ten of spades a king of clubs and a two of clubs
92
Yang, Heeman, and Kun Multi-Tasking Dialogues
For the 115 embedded interruptions, we find 34 cases of utterance restatement (20
self-repetitions, 4 other-repetitions, and 10 clarifications) and 9 cases of card review.
Figure 8 shows the rate of each category, aggregated on each pair of players (R1-R6).
The rate of utterance restatements, calculated as the number of embedded inter-
ruptions that are followed by an utterance restatement divided by the total number
of embedded interruptions, ranges from 22% to 37% among the player pairs. To make
sense of these numbers, we annotate each non-trivial utterance in the poker games to
mark whether it is a restatement of the immediate previous one within a card segment,
and calculate the baseline as the rate of performing utterance restatement without being
interrupted by a picture game. From Figure 8, we see that for all six pairs of players, the
rate of utterance restatement after an embedded interruption is higher than the baseline,
and it is statistically significant, t(5) = 13.52, p < 0.001. This suggests that utterance
restatement after an embedded interruption is not a random behavior, but it is part of
the resumption to the ongoing task.
We next examine card review, which does not seem to be a common behavior in all
player pairs. The pairs R1, R4, and R6 never performed it in resuming poker playing,
and R2 only performed it once. The pairs with the highest rates are R5 and R3, with
26% (6/23) and 17% (2/12), respectively. Interestingly, these two pairs have the lowest
rates of performing utterance restatement, with 22% and 27%, respectively. This might
suggest that card review might be complementary to utterance restatement for context
restoration, although more data are needed to validate this hypothesis.
6.2 Restoration at the Beginning of a Card Segment
For restoration at the beginning of a card segment, we find that players mostly just
continue poker playing without a clear indication of being affected by the interruption,
as illustrated by card segments b10 and b14 in Figure 2. Some players might perform an
act similar to card review, in that they communicate all of the cards in his or her hand.
However, the version here differs as it includes the new card just picked up, which has
not been communicated before. Thus we refer to this act as card review + new card.
Table 3 shows the rate of performing card review + new card after an interruption for
each pair, respectively. The baseline is the rate of performing this action at the beginning
Figure 8
Restoration in the middle of a card discussion.
93
Computational Linguistics Volume 37, Number 1
Table 3
Restoration at the beginning of a card segment.
R1 R2 R3 R4 R5 R6
Card review + new card 0% 19% 9% 8% 0% 42%
(0/2) (3/16) (1/11) (1/13) (0/9) (5/12)
Baseline 0% 6% 1% 3% 0% 5%
(0/31) (5/89) (2/177) (5/177) (0/62) (3/62)
of a card segment (excluding the first card segment of a poker game) not following an
interruption of a picture game. Player pairs R1 and R5 never performed this action at
all. R3 and R4 performed this action only once after interruptions at the end of a card
discussion and have a very low overall rate of performing this action during poker
playing. However, for player pairs that have a high overall rate of using this action (R2
and R6), they have an even higher rate of using this action after an interruption. For R6
in particular, the rate of performing this action after an interruption at the end of a card
segment is significantly higher than the baseline, ?2(1) = 6.81, p = 0.01. This suggests
that if players use card review + new card in conversation, they tend to use it more often
after an interruption at the end of a card segment probably for context restoration.
6.3 Restoration at the Beginning of a Game
For interruptions at the beginning of a poker game, we do not find any behavior
associated with context restoration. This is not surprising because there is really no
context that needs to be carried over to the next game.
6.4 Discussion
In this section, we examine the behavior of context restoration when players complete
an interrupting picture game and resume poker playing. Probably due to the simplicity
of the picture game, we find that players mostly just have a smooth continuation as if
the interruption did not happen. However, we do find that players sometimes make
two types of context restorations?utterance restatements and card reviews?and we
find that players have a higher rate of performing these when returning to the ongoing
task.
Card review seems to be refreshing the critical information needed to complete a
task, while utterance restatement is refreshing the last utterance. Both types of restora-
tion behavior are similar to the informationally redundant units that Walker (1996)
studied. In Walker?s work, she posited a limited memory model in which information
will eventually fade away. This might be the explanation here as well. On resuming to
a task that was discussed several utterances ago, the conversant might feel that some
of the critical information might have been forgotten, and so might use card review to
refresh the information. Conversely, the conversant might feel that just the last utterance
needs to be refreshed. Depending onwhether it is the same conversant who resumes the
ongoing task and who says the last utterance before the interruption, it takes the form
of a self-repetition, other-repetition, or request-repetition if clarification is needed. This
explanation for card review and utterance restatement is consistent with the results of
our post-experiment survey, in which some players reported that they had difficulties
94
Yang, Heeman, and Kun Multi-Tasking Dialogues
remembering the context of poker playing when they were interrupted by a picture
game.
In a more complex domain, conversants will probably perform context restoration
more frequently when returning to an interrupted task (Gillie and Broadbent 1989;
Villing 2010), and might use even higher-level summarization beyond utterance restate-
ment and information review, such as reviewing the agreements or decisions that have
been made so far in the conversation.
7. Recognizing Task Interruption: A Machine Learning Approach
Recognizing task switching is important for a speech interface; for example, the speech
interface can accordingly switch the language model when it detects that the user has
switched to another task. In this section, we describe twomachine learning experiments
of recognizing task interruptions using prosody, discourse context, and discourse mark-
ers. The purpose of the first experiment is to understand how these features contribute
to the automatic identification of task interruptions; here, we only include utterances
that start with do you have for better extracting prosodic features. The purpose of the
second experiment is to investigate how well interruptions can be identified without
using lexical features, as could be used in an actual system.
7.1 Recognizing Task Interruptions on Do You Have Utterances
In the previous sections, we examined players? behavior of task switching in the MTD
corpus. We found that players favor certain discourse contexts in the ongoing task for
task interruptions, and that they signal task interruptions with prosodic cues and some-
times with certain discourse markers (oh and wait). We thus conduct a machine learning
experiment to understand how these features contribute to the automatic identification
of task interruptions. In this experiment, we focus on the 285 cases of do you have, 170
for task interruption and 115 for poker playing. As we argued in Section 5.2, this allows
us to better extract and understand prosodic features of task interruptions.
We extract the following features: 1) discourse context: whether the utterance before
do you have is the end of a poker game, the end of a card segment, or in the middle of a
card segment; 2) oh/wait: whether the discourse marker oh/wait precedes do you have;
3) normalized pitch: the pitch of do you have divided by the average pitch of the speaker
during the dialogue. We refer to these features as the core feature set, which we found
to be correlated with task interruptions (Section 4 and 5). We also include the following
additional features: 4) discourse markers: whether a discourse marker precedes do you
have; 5) normalized energy: the energy of do you have divided by the average energy of
the speaker during the dialogue; and 6) duration: the duration of do you have.
We use a decision tree classifier (C4.5) to discriminate task interruption from poker
playing (Quinlan 1986). C4.5 builds a decision tree by using a top?down, greedy pro-
cedure to (locally) optimize mutual information, and prunes the tree with a confidence
level (of 25%). We use C4.5 because its output is interpretable and we have found its
performance comparable to other discriminative classifiers for this task.
We use three re-sampling methods in training and testing the decision tree learn-
ing, which we refer to as general-leave-one-out, speaker-leave-one-out, and leave-one-
speaker-out. In the general leave-one-out method, each data point is tested with the
decision tree trained on all other data points. This approach allows decision trees to
be built with as much training data as possible, which in our case is 284 data points.
95
Computational Linguistics Volume 37, Number 1
Table 4
Performance for general-leave-one-out.
Accuracy Recall Precision F
Baseline 59.6% 100.0% 59.6% 74.7%
Core features 81.4% 89.4% 81.3% 85.2%
Core + discourse markers 80.7% 88.2% 81.1% 84.5%
Core + energy + duration 80.7% 85.3% 82.9% 84.6%
All features 80.4% 84.7% 82.8% 83.7%
In the speaker-leave-one-out method, each data point is tested with the decision tree
trained on the other data points of the same player. This approach is a speaker-specific
model that evaluates the performance of training a decision tree and testing on the same
speaker. In the leave-one-speaker-out method, each player?s data are tested with the
decision tree trained on the other 11 players. This approach is a speaker-independent
model that evaluates the performance of a learned decision tree on a new speaker.
Table 4 shows the results with the general-leave-one-out method. The decision tree
learning with the core feature set obtains an accuracy of 81.4% in recognizing whether
a do you have initiates a task interruption or belongs to poker playing; and the recall,
precision, and F-score for task interruption are 89.4%, 81.3%, and 85.2%, respectively.
For comparison, we use a naive baseline that assumes that all cases of do you have are
task interruptions, which has an accuracy of 59.6%. Thus we achieve 54.0% relative error
reduction in comparison to the baseline. These results show that our machine learning
approach substantially improves the recognition of task interruptions.
Also from Table 4 we see that there is no improvement by adding more features,
namely, discourse markers, energy and duration, or all of them. This suggests that
these features are not adding more information to this discrimination task, which is
not surprising as we did not find them strongly correlated with task interruption in our
corpus study.
Table 5 shows the results for each player with the general-leave-one-out, the
speaker-leave-one-out, and the leave-one-speaker-out, using the core feature set.
Table 5
Accuracy for the three re-sampling methods.
Player General-leave-one-out Speaker-leave-one-out Leave-one-speaker-out
1A 75.0% 66.7% 75.0%
1B 84.6% 69.2% 73.1%
2A 77.8% 74.1% 77.8%
2B 100.0% 90.0% 95.0%
3A 88.0% 88.0% 88.0%
3B 76.7% 76.7% 72.6%
4A 94.4% 94.4% 94.4%
4B 64.7% 64.7% 64.7%
5A 73.9% 69.6% 73.9%
5B 92.9% 71.4% 92.9%
6A 89.5% 84.2% 78.9%
6B 63.6% 90.9% 63.6%
Mean 81.8% 78.3% 79.2%
96
Yang, Heeman, and Kun Multi-Tasking Dialogues
Overall, all the three reach an accuracy of about 80%, which is much higher than the
baseline performance. The performance with the leave-one-speaker-out (M = 79.2%),
which is a speaker-independent model, is particulary encouraging, because in building
a speech interface, it is not always possible to collect speaker-specific data. On the
other hand, we see that the performance with the speaker-leave-one-out (M = 78.3%)
is slightly lower than the leave-one-speaker-out (M = 79.2%). Although this could
be interpreted as that interruption recognition is a speaker-independent task, we
think that a more viable explanation is that for some players, we do not have enough
data to build speaker-specific decision trees. The general-leave-one-out (M = 81.8%),
which uses the most data for training, out-performs the leave-one-speaker-out and
the speaker-leave-one-out. In fact, the general-leave-one-out can also be viewed as a
naive speaker-adaptive model by simply combining speaker-independent data and
speaker-specific data together for training. We speculate that more improvement can be
achieved by interpolating a speaker-independent model with a speaker-specific model,
which we leave for future work.
Finally, we examine the structure of the decision trees learned. Here, we build a
single tree from all 285 cases of do you have with the core feature set, shown in Figure 9.
In the decision tree, the first query is about pitch. If pitch is low it is for poker playing,
otherwise it queries about oh/wait. If the utterance starts with a oh or wait, it is for task
interruptions, otherwise it queries about discourse context. If the discourse context is at
the end of a game or a card discussion, it is for task interruption, otherwise it queries
pitch again. If pitch is lower than a threshold it is for poker playing, otherwise it is for
task interruptions. The structure of the learned tree and its performance confirm that
discourse context, the discourse markers oh and wait, and normalized pitch are useful
features for recognizing task interruptions.
Figure 9
The learned decision tree.
97
Computational Linguistics Volume 37, Number 1
7.2 Recognizing Task Interruptions on All Utterances
The previous experiment helped us determine which features are useful for recognizing
task interruptions. However, the experiment was based only on utterances that start
with do you have, yet not all task interruptions are initiated with do you have. We
thus conduct a further machine learning experiment on recognizing task interruptions
involving any utterances. We extend our feature set to help make up for not limiting
ourselves to do you have utterances. We purposely do not use any lexical features of
the current utterance so that our approach can be applied before speech recognition is
performed.
We extract the following features for all non-trivial utterances: 1) discourse context:
whether the previous utterance is the end of a poker game, the end of a card segment,
or in the middle of a card segment;6 2) overlap: whether the utterance overlaps with the
previous non-trivial utterances; 3) duration: the length in time of the utterance; 4) nor-
malized pitch: the average normalized pitch of the first 100 msec/200 msec/500 msec
and the whole utterance (four features); 5) normalized energy: the average normalized
energy of the first 100 msec/200 msec/500 msec and the whole utterance (four features);
and 6) pitch range: the pitch range of the first 100 msec/200 msec/500 msec and the
whole utterance (four features). In total we have 15 features.
The data that we have are highly skewed. We have 208 cases of task interruptions
but more than 4,000 non-interrupting utterances. We thus perform down-sampling so
that both classes have the same number of data points. In the first down-sampling,
which we refer to as general down-sampling, we use all 208 cases of task interruptions,
and we randomly select 208 non-interrupting utterances. A concern with the general
down-sampling is that 82% of the task interruptions are do you have questions, and do
you have questions are only about 2.5% of the non-interrupting utterances. It is unclear
whether a classifier trained from such a data set discriminates task interruptions or
discriminates do you have utterances. Thus in the second down-sampling, which we
refer to asDYH down-sampling, we use all 208 cases of task interruptions, and we also
use all 105 cases of non-interrupting do you have utterances, then finally we randomly
select 103 other non-interrupting utterances. The DYH down-sampling, however, still
has imbalanced do you have utterances in the two classes. Thus we further introduce
the Balanced-DYH down-sampling, in which we use all 105 do you have utterances
in the poker playing and 38 other (i.e., non do you have) utterances in task interrup-
tions, and randomly select 105 do you have utterances from task interruptions and
38 other utterances from poker playing. We run the experiments with decision tree
learning (C4.5) (Quinlan 1986) and support vector machine (SVM) (Chang and Lin
2001).
We evaluate the performance using general-leave-one-out. The procedure of
down-sampling and general-leave-one-out is repeated 10 times, and then we calculate
the average performance. Note that in our evaluation, the distribution of task inter-
ruption (which is 50%) is different from the true distribution in the corpus (which is less
than 5%). We adopt some metrics from medical diagnostic tests that do not involve
prior distributions. Sensitivity is defined as TruePositive/(TruePositive+ FalseNegative),
which, in our case, is the recall of task interruptions. It measures the percentage
of task interruptions that the classifier correctly identifies as such. Specificity is
6 Card and game segments could be determined fairly accurately from the mouse clicks even without
the speech.
98
Yang, Heeman, and Kun Multi-Tasking Dialogues
defined as TrueNegative/(TrueNegative+ FalsePositive), which, in our case, is the re-
call of non-interruptions. It measures the percentage of non-interruptions that the
classifier correctly identifies as such. These two metrics can then be combined
using the likelihood ratio, which provides a direct estimate of how much a prediction
will change the odds. The likelihood ratio for a positive result (LR+) is defined as
LR+ = sensitivity/(1? specificity). It tells us how much the odds of a task interruption
increase when the classifier predicts positive (task interruption). The likelihood ratio
for a negative result (LR?) is defined as LR? = specificity/(1? sensitivity). It tells us
howmuch the odds of a task interruption decrease when the classifier predicts negative
(non-interruption).
Table 6 shows the results. If we assume a naive baseline with no knowledge, its
sensitivity and specificity are both 50%, and LR+ and LR? are both 1.0. For all three
down-sampling settings, SVM performs slightly better than C4.5, and both are much
better than the baseline. The result for SVM with general down-sampling shows how
well we can recognize task interruptions for our MTD domain, for which we achieve a
sensitivity of 78.6% and a specificity of 76.9%. For the Balanced-DYH down-sampling,
in which we have the same number of do you have utterances in both the classes, SVM
cannot make use of the features that distinguish do you have from other utterances.
Hence, its result might be more indicative of performance in other domains, where
task interruptions might not be marked by the same introductory words. Even here,
we obtain a sensitivity of 75.3%, a specificity of 75.8%, and 3.11 in LR+ and 3.07 in LR?,
which is more than a 50% relative error reduction over the baseline.
Overall, our results show that non-lexical features are useful for the recognition of
task interruptions. Because the features used in our machine learning experiments do
not require the lexical information of the current utterance, we can make use of the
identification of task interruptions to benefit automatic speech recognition (ASR). For
example, we can build two language models, one for the ongoing task, and one for the
real-time task. For each utterance, we can calculate the likelihood of the utterance being
a task interruption, using the decision tree classifier or the SVM classifier. We can then
use this likelihood to dynamically interpolate the two language models in the speech
decoding. This should be able to improve the accuracy of ASR, which we leave for
future work.
8. Conclusion
In this article we describe a series of empirical studies of human?human multi-tasking
dialogues, where people perform multiple verbal tasks overlapped in time. We first
Table 6
Performance for non-lexical features.
Sensitivity Specificity LR+ LR?
Baseline 50.0% 50.0% 1.0 1.0
C4.5 + general down-sampling 77.5% 75.3% 3.14 3.35
C4.5 + DYH down-sampling 72.9% 73.2% 2.72 2.70
C4.5 + B-DYH down-sampling 69.4% 71.8% 2.46 2.35
SVM + general down-sampling 78.6% 76.9% 3.40 3.59
SVM + DYH down-sampling 78.6% 78.4% 3.64 3.66
SVM + B-DYH down-sampling 75.3% 75.8% 3.11 3.07
99
Computational Linguistics Volume 37, Number 1
examined the discourse context of task interruptions, that is, where conversants sus-
pend the ongoing task and switch to a real-time task. Our analysis shows that people
are more likely to wait until the end of a card or game segment for task switching.
We then examined the cues that people use to signal task interruptions. We find that
task interruptions correlate with certain discourse markers and prosodic variations.
More interestingly, the intensity of pitch depends on the discourse context of the task
interruption. We next conducted an exploratory study on context restoration in task
resumption. We find that when returning to an interrupted task, conversants sometimes
re-synchronize the interrupted ongoing conversation by either restating a previous
utterance or summarizing the critical information. Finally, our machine learning ex-
periments show that discourse context, pitch, and the discourse markers oh and wait
are useful features to reliably recognize task interruptions; and, more importantly, with
non-lexical features one can improve the performance of recognizing task interruptions
with more than a 50% relative error reduction over the baseline.
8.1 Disruptiveness of Task Interruption
In our study on multi-tasking dialogues, we distinguish three types of discourse con-
texts where players suspend the poker player and switch to a picture game. We claim
that these discourse contexts differ in terms of players? engagement andmemory load in
the ongoing task. First, we feel that players are more engaged in the ongoing task during
card discussion. In the middle of a card discussion, players actively share information,
explore different (potential) poker hands, and decide what to discard if no poker hand
is found. Second, we feel that players also have a higher memory load in the middle
of a card discussion. Across poker games, players do not have to remember anything;
across card segments, players need to remember what cards each other has; while inside
of a card discussion, players need to also remember what card is being discussed, and
how far they are into deciding which card to discard.
Engagement can be used to explain the intensity of cues in task interruptions. As we
found in Section 5, when players interrupt in the middle of a card discussion, they use a
higher pitch than in the case when they interrupt at the end of a game or a card, which
is also marked with a higher pitch than non-task-switching (during poker playing).
According to Miyata and Norman (1986), a more intrusive signal is needed to attract
the attention of people heavily engaged in an ongoing task. Sussman, Winkler, and
Schro?ger (2003) found that higher pitch can serve as a more intrusive signal. Thus when
interrupting in the middle of a card discussion, the speaker uses higher pitch probably
because the hearer is more engaged in the ongoing task.
Memory load can explain the context restoration behavior in task resumptions. As
we found in Section 6, after a picture game that is at the end of a game, players smoothly
start a new poker game as if nothing happened; after a picture game that is at the end
of a card segment, players might sometimes use information summary to remind each
other of what cards they have in hand; and after a picture game that is in the middle
of a card segment, players might even repeat or clarify the previous utterance that has
been said before the interruption. These observations are consistent with the memory
load of discourse contexts. If players are interrupted in a discourse context where the
memory load is high, because of the limited working memory, players would need to
spend extra effort to recover the memory after completing the interruptions.
Engagement and memory can also explain our finding on the discourse context
of task interruptions. According to Miyata and Norman (1986), interruptions where
100
Yang, Heeman, and Kun Multi-Tasking Dialogues
people are deeply engaged in the ongoing task, or where people have a high memory
load, should be disruptive. Thus interruptions at the end of a card game are the least
disruptive, with those at the end of a card discussion being more disruptive, and those
embedded inside of a card discussion being the most disruptive. A more disruptive
interruption tends to have a higher cost to the ongoing task. The disruptiveness of inter-
ruptions thus explains players? behavior of delaying the picture game. For task inter-
ruptions, players do not always switch to a real-time task when it is prompted, but
instead they take into account the discourse context of the ongoing task. They strive to
switch to a picture game at the end of a (poker) game or a cardwhen possible. According
to Clark and Wilkes-Gibbs (1986), players would try to minimize their collaborative
effort in dialogue. The reason that players try to avoid interrupting in the middle of a
card discussion probably is because such interruptions have a higher cost to the ongoing
task, i.e. these interruptions are more disruptive. Delaying the switch to the real-time
task is thus used as a tool to reduce the disruptiveness of the switch.
Our studies thus suggest that conversants strive to interrupt at a discourse context
where the cost of interruption is low, but if they interrupt in a more intensive context
they use stronger cues to mark the more disruptive interruption.
8.2 Implication for Speech Interface Design
By understanding people?s conventions in task interruptions and context restoration,
we can implement these conventions into a speech interface to allow natural and
smooth task switching in human-computer dialogue. Based on our findings, we propose
the following principles for building a speech interface that supports multi-tasking
dialogue:
 Minimize the disruptiveness of task switching. Delay task switching till
the user?s engagement and memory load in the ongoing task are low so
that the interruption is less disruptive, while still accomplishing the
interruption task in a timely matter. Minimizing the disruptiveness
reduces the cost of interruptions to the ongoing task.
 Signal task switching. Discourse markers, such as oh and wait, and
prosodic variations, especially high pitch, can be used to signal task
switching. These devices help to disengage the user?s attention from the
ongoing task so that the user is aware of the task switching. Use stronger
cues (e.g., higher pitch) when the task switching is more disruptive
(i.e., when the user is more engaged in the ongoing task).
 Recognize task switching. The speech interface can make use of non-lexical
features, such as contextual information and the user?s pitch, together with
discourse markers if available, to help recognize the user?s initiation of
task interruptions. Recognizing task switching helps the speech interface
to interpret the user?s utterance in the correct context, which should lead
to higher speech recognition accuracy and better language understanding.
 Restore context after an interruption. Utterance restatement and
information summary are two effective devices. Context restoration is
needed, especially after a disruptive interruption where the memory
load in the ongoing task is high, in order to help resolve or prevent
misunderstandings and forgetting.
101
Computational Linguistics Volume 37, Number 1
8.3 Future Work
There are obviously a lot of open questions regarding multi-tasking dialogue that are
not solved in this article. In this research, we only examined a domain where an ongoing
task, rich in context, is interrupted by real-time tasks, which are short and simple in
nature. Psychological research showed that the complexity of the real-time task and its
similarity to the ongoing task play an important role in the disruptiveness of interrup-
tions (Gillie and Broadbent 1989); thus we can vary these factors in future research. First,
we can vary the complexity of the real-time tasks; for example, for some interruptions,
the player needs to find out whether the other player has a combination of pictures, such
as a black square but not a white triangle ( ? ?). This will allow us to examine the
correlation between the length of interruptions and context restoration. Second, we can
use real-time tasks that are less structured, so that people do not mutually know when
it ends. This will allow us to examine whether and how people signal task resumptions.
Third, we can introduce ambiguity between the ongoing task and the real-time task: for
example, to put the card suits (????) into the picture game, where an utterance such
as do you have a heart? can belong to either task. This will allow us to see a wider range
of task switching behavior.
Furthermore, in this research we do not investigate how multi-tasking dialogue
would be affected by a manual-visual task, such as driving. This is an important ques-
tion, because for hands-busy, eyes-busy situations such as driving, speech interfaces
may provide a human?computer interaction modality that interferes the least with the
execution of the manual?visual task (Weng et al 2006; Villing et al 2008). We expect
that the presence of the manual?visual task will even further necessitate a good under-
standing of the natural and efficient human conventions for managing multi-tasking so
as not to adversely affect the manual?visual task.
Finally, it was pointed out that human?computer dialogue is not exactly the same as
human?human dialogue?that is, people might change their behavior when talking to
a computer (Doran et al 2001). It will thus be useful to build an actual speech interface
for multi-tasking dialogue, or perhaps first simulate such a system with Wizard of Oz
experiments, and to examine whether following the principles that we derived from
human?human dialogue does lead to improvements.
Acknowledgments
This work was funded by the National
Science Foundation under grant IIS-0326496.
The authors thank Alex Shyrokov, David
Traum, Elizabeth Shriberg, and members of
CSLU for helpful discussions. The authors
also wish to thank the reviewers for their
constructive comments.
References
Ayers, Gayle M. 1992. Discourse functions
of pitch range in spontaneous and read
speech. Presented at the Linguistic Society
of America Annual Meeting. 9?12 January,
Philadelphia, PA.
Bangerter, Adrian and Herbert H. Clark.
2003. Navigating joint projects with
dialogue. Cognitive Science, 27:195?229.
Butterworth, Brian. 1972. Hesitation and
semantic planning in speech. Journal of
Psycholinguistic Research, 4:75?87.
Byron, Donna K. and P. Heeman. 1997.
Discourse marker use in task-oriented
spoken dialog. In Proceedings of the 5th
EUROSPEECH, pages 2223?2226, Rhodes.
Chang, Chih-Chung and Chih-Jen Lin.
2001. LIBSVM: a library for support
vector machines. Software available at
www.csie.ntu.edu.tw/?cjlin/libsvm.
Clark, Herbert H. and Deanna Wilkes-Gibbs.
1986. Referring as a collaborative process.
Cognitive Science, 22:1?39.
Cutrell, Edward, Mary Czerwinski,
Eric Horvitz. 2001. Notification,
disruption, and memory: Effects of
messaging interruptions on memory
and performance. In Proceedings of
INTERACT, pages 263?269, Tokyo.
102
Yang, Heeman, and Kun Multi-Tasking Dialogues
Doran, Christine, John Aberdeen, Laurie
Damianos, and Lynette Hirschman.
2001. Comparing several aspects of
human?computer and human?human
dialogues. In 2nd SigDial Workshop on
Discourse and Dialogue, pages 1?10,
Aalborg, Denmark.
Gillie, Tony and Donald Broadbent. 1989.
What makes interruptions disruptive? A
study of length, similarity, and complexity.
Psychological Research, 50(4):243?250.
Gopher, Daniel, Yaakov Greenshpan, and
Lilach Armony. 1996. Switching attention
between tasks: Exploration of the
components of executive control and their
development with training. In Proceedings
of the Human Factors and Ergonomics Society
40th Annual Meeting, pages 1060?1064,
Santa Monica, CA.
Gravano, Agustin, Stefan Benus, Julia
Hirschberg, Shira Mitchell, and Illa
Vovsha. 2007. Classification of discourse
functions of affirmative words in spoken
dialogue. In Proceedings of INTERSPEECH,
pages 1613?1616, Antwerp, Belgium.
Grosz, Barbara J. and Julia Hirschberg.
1992. Some intonational characteristics
of discourse structure. In Proceedings
of 2nd International Conference on Spoken
Language Processing, pages 429?432, Banff.
Grosz, Barbara J. and Candace L. Sidner.
1986. Attention, intentions, and the
structure of discourse. Computational
Linguistics, 12(3):175?204.
Heeman, Peter A. and James F. Allen. 1995.
The Trains 93 dialogues. Trains Technical
Note 94-2, Department of Computer
Science, University of Rochester.
Heeman, Peter A., Fan Yang, Andrew L.
Kun, and Alexander Shyrokov. 2005.
Conventions in human?human
multithreaded dialogues: A preliminary
study. In Proceedings of Intelligent User
Interface, pages 293?295, San Diego, CA.
Heritage, John. 1984. A change-of-state
token and aspects of its sequential
placement. In J. Maxwell Atkinson and
John Heritage, editors, Structures of Social
Action: Studies in Conversation Analysis.
Cambridge University Press, chapter 13,
pages 299?345.
Hess, Stephen M. and Mark C. Detweiler.
1994. Training to reduce the disruptive
effects of interruptions. In Proceedings of
the Human Factors and Ergonomics Society
38th Annual Meeting, pages 1173?1177,
Nashville, TN.
Hirschberg, Julia and Diane Litman. 1987.
Now let?s talk about now: Identifying cue
phrases intonationally. In Proceedings of the
25th Annual Meeting of the Association for
Computational Linguistics, pages 163?171,
Stanford, California.
Hirschberg, Julia and Christine H. Nakatani.
1996. A prosodic analysis of discourse
segments in direction-giving monologues.
In Proceedings of 34th Annual Meeting of the
Association for Computational Linguistics,
pages 286?293, Santa Cruz, CA.
Iyer, Rukmini M. and Mari Ostendorf. 1999.
Modeling long distance dependence in
language: Topic mixtures versus dynamic
cache models. IEEE Transactions on Speech
and Audio Process, 7(1):30?39.
Kun, Andrew L., W. Thomas Miller, and
William H. Lenharth. 2004. Computers in
police cruisers. IEEE Pervasive Computing,
3(4):34?41.
Larsson, Staffan. 2003. Interactive
communication management in an
issue-based dialogue system. In
Proceedings 7th Workshop on the Semantics
and Pragmatics of Dialogue, pages 75?83,
Saarbru?cken.
Lemon, Oliver and Alexander Gruenstein.
2004. Multithreaded context for
robust conversational interfaces:
Context-sensitive speech recognition and
interpretation of corrective fragments.
ACM Transactions on Computer-Human
Interaction, 11(3):241?267.
Linde, Charlotte and Joseph Goguen. 1987.
Checklist interruption and resumption:
A linguistic study. Technical Report
CR-177460, National Aeronautics and
Space Administration.
McFarlane, Daniel C. 1999. Coordinating
the interruption of people in
human?computer interaction. In
Proceedings of INTERACT, pages 295?303,
Edinburgh, Scotland.
Meiran, Nacshon, Ziv Chorev, and Ayelet
Sapir. 2000. Component processes in
task switching. Cognitive Psychology,
41:211?253.
Miyata, Yoshiro and Donald A. Norman.
1986. Psychological issues in support of
multiple activities. In D. A. Norman and
S. W. Draper, editors, Participant Centered
Design: New Perspectives on Human
Computer Interaction. Lawrence Erlbaum,
Hillsdale, NJ, chapter 13, pages 265?284.
Moser, Megan and Johanna D. Moore. 1995.
Investigating cue selection and placement
in tutorial discourse. In Proceedings of
33rd Annual Meeting of the Association for
Computational Linguistics, pages 130?135,
Cambridge, MA.
103
Computational Linguistics Volume 37, Number 1
Nakajima, Shin?ya and James F. Allen.
1993. A study on prosody and discourse
structure in cooperative dialogues.
TRAINS Technical Note 93-2, University
of Rochester, Rochester, NY.
Passonneau, Rebecca J. and Diane J. Litman.
1997. Discourse segmentation by human
and automated means. Computational
Linguistics, 23(1):103?139.
Quinlan, J. R. 1986. Induction of decision
trees.Machine Learning, 1(1):81?106.
Renaud, Karen. 2000. Expediting rapid
recovery from interruptions by providing
a visualisation of application activity. In
Proceedings of OzCHI, pages 348?355,
Sydney.
Rickel, Jeff, Stacy Marsella, Jonathan Gratch,
Randall Hill, David Traum, and William
Swartout. 2002. Towards a new generation
of virtual humans for interactive
experiences. IEEE Intelligent Systems,
17(4):32?38.
Schiffrin, Deborah. 1987. Discourse Markers.
Cambridge University Press.
Shyrokov, Alexander, Andrew Kun, and
Peter Heeman. 2007. Experiments
modeling of human?human
multi-threaded dialogues in the presence
of a manual?visual task. In Proceedings of
8th SIGdial Workshop on Discourse and
Dialogue, pages 190?193, Antwerp.
Strayer, Susan E., Peter A. Heeman, and
Fan Yang. 2003. Reconciling control and
discourse structure. In J. van Kuppevelt
and R. W. Smith, editors, Current and
New Directions in Discourse and Dialogue.
Kluwer Academic Publishers, Dordrecht,
chapter 14, pages 305?323.
Sussman, E., I. Winkler, and E. Schro?ger.
2003. Top?down control over involuntary
attention switching in the auditory
modality. Psychonomic Bulletin & Review,
10(3):630?637.
Swerts, Marc. 1995. Combining statistical
and phonetic analyses of spontaneous
discourse segmentation. In Proceedings of
the 12th ICPhS, volume 4, pages 208?211,
Stockholm.
Swerts, Marc. 1998. Filled pauses as markers
of discourse structure. Journal of
Pragmatics, 30:485?496.
Swerts, Marc and Mari Ostendorf. 1995.
Discourse prosody in human?machine
interactions. In Proceedings of ESCA
Workshop on Spoken Dialogue Systems:
Theories and Applications, pages 205?208,
Visgo.
Toh, Siew Leng, Fan Yang, and Peter A.
Heeman. 2006. An annotation scheme for
agreement analysis. In Proceedings of 9th
International Conference on Spoken Language
Processing, pages 201?204, Pittsburgh, PA.
Traum, David and Jeff Rickel. 2002.
Embodied agents for multi-party dialogue
in immersive virtual world. In Proceedings
of the First International Joint Conference on
Autonomous Agents and Multi-agent
Systems, pages 766?773, Bologna.
Villing, Jessica. 2010. Now, where was I?
Resumption strategies for an in-vehicle
dialogue system. In Proceedings of the
48th Annual Meeting of the Association for
Computational Linguistics, pages 798?805,
Uppsala.
Villing, Jessica, Cecilia Holtelius, Staffan
Larsson, Anders Lindstro?m, Alexander
Seward, and Nina A?berg. 2008.
Interruption, resumption and domain
switching in in-vehicle dialogue. In
Proceedings of the 6th International
Conference on Advances in Natural
Language Processing, pages 488?499,
Berlin.
Walker, Marilyn A. 1996. The effect of
resource limits and task complexity on
collaborative planning in dialogue.
Artificial Intelligence Journal, 85:181?243.
Walker, Marilyn A., Rebecca Passonneau,
and Julie E. Boland. 2001. Quantitative
and qualitative evaluation of DARPA
communicator spoken dialogue systems.
In Proceedings of the Association of
Computational Linguistics, pages 515?522,
Toulouse, France.
Weng, Fuliang, Sebastian Varges, Badri
Raghunathan, Florin Ratiu, Heather
Pon-barry, Brian Lathrop, Qi Zhang,
Harry Bratt, Tobias Scheideck, Kui Xu,
Matthew Purver, and Rohit Mishra.
2006. CHAT: A conversational helper
for automotive tasks. In Proceedings of
9th International Conference on Spoken
Language Processing, pages 1061?1064,
Pittsburgh, PA.
Yang, Fan and Peter A. Heeman. 2009.
Context restoration in multi-tasking
dialogue. In Proceedings of 13th International
Conference on Intelligent User Interfaces,
pages 373?377, Sanibel, FL.
Yang, Fan and Peter A. Heeman. 2010.
Initiative conflicts in task-oriented
dialogue. Computer Speech and Language,
24:175?189.
Yang, Fan, Peter A. Heeman, and Andrew
Kun. 2008. Switching to real-time tasks in
multi-tasking dialogue. In Proceedings of
International Conference on Computational
Linguistics, pages 1025?1032, Manchester.
104

