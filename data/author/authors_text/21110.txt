Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 747?757,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A discourse-driven content model for summarising scientific articles
evaluated in a complex question answering task
Maria Liakata
University of Warwick/
EMBL-EBI, UK
M.Liakata@warwick.ac.uk
Simon Dobnik
University of Gothenburg, Sweden
simon.dobnik@gu.se
Shyamasree Saha
EMBL-EBI, UK
saha@ebi.ac.uk
Colin Batchelor
Royal Society of Chemistry, UK
batchelorc@rsc.org
Dietrich Rebholz-Schuhmann
University of Zurich, Switzerland/
EMBL-EBI, UK
rebholz@ebi.ac.uk
Abstract
We present a method which exploits auto-
matically generated scientific discourse an-
notations to create a content model for the
summarisation of scientific articles. Full pa-
pers are first automatically annotated using the
CoreSC scheme, which captures 11 content-
based concepts such as Hypothesis, Result,
Conclusion etc at the sentence level. A content
model which follows the sequence of CoreSC
categories observed in abstracts is used to pro-
vide the skeleton of the summary, making a
distinction between dependent and indepen-
dent categories. Summary creation is also
guided by the distribution of CoreSC cate-
gories found in the full articles, in order to
adequately represent the article content. Fi-
nally, we demonstrate the usefulness of the
summaries by evaluating them in a complex
question answering task. Results are very en-
couraging as summaries of papers from auto-
matically obtained CoreSCs enable experts to
answer 66% of complex content-related ques-
tions designed on the basis of paper abstracts.
The questions were answered with a precision
of 75%, where the upper bound for human
summaries (abstracts) was 95%.
1 Introduction
The publication boom of the last few years, espe-
cially in the life sciences, has highlighted the need
to facilitate automatic access to the information con-
tent of articles. Researchers, curators, reviewers all
need to process a continuously expanding flow of
articles whether the purpose is to follow the state of
the art, curate large knowledge bases or have a good
working knowledge of their own and related disci-
plines to assess progress in research. While a lot
of effort has concentrated on information extraction
of particular types of entities and relations from the
scientific literature (Cohen and Hersh, 2005; Kim
et al, 2009; Ananiadou et al, 2010; Kim et al,
2011), with a view to support scientists in obtain-
ing relevant information from scientific articles and
abstracts, less work has focussed on automatically
combining such information in the form of a co-
hesive summary which preserves the context. Re-
searchers rely to a great extent on author-written ab-
stracts, but the latter suffer from a number of prob-
lems; they are less structured, vary significantly in
terms of length, are often not self-contained and
have been written independently of the main doc-
ument (Teufel, 2010, p.83).
Teufel (2001; 2010), (Teufel and Moens, 2002)
identify argumentative zones within scientific arti-
cles and use them to create use-targeted extractive
summaries. Argumentative zones are annotations
which designate the type of knowledge claim and
rhetorical status for a sentence and how these relate
to the communicative function of the entire paper.
A selection of various combinations of argumenta-
tive zones are chosen for the use-targeted extractive
summaries (rhetorical extracts), each of which ful-
fills a different role. For instance, purpose-oriented
extracts less than 10 sentences long are generated
containing a predetermined number of AIM, SOLU-
TION and BACKGROUND zones. As the emphasis
of this approach was the identification of the argu-
mentative zones, less attention was given to the sen-
tence selection criteria for the extractive summaries.
747
The sentences chosen for the rhetorical extracts were
either all sentences of a particular category (in the
case of rare categories) (Teufel and Moens, 2002),
selected according to a classifier trained on a rele-
vance gold standard (Teufel and Moens, 2002), man-
ually or randomly selected (Teufel, 2010, p.60).
More recently Contractor et al (2012) have used
automatically annotated argumentative zones (Guo
et al, 2011) to guide the creation of extractive sum-
maries of scientific articles. Here argumentative
zones are used as features for the summariser, along
with verbs, tf-idf values and sentence location. They
use a standard approach to summarisation, with a bi-
nary classification recognising candidate sentences
which are then fed into a clustering mechanism. Ex-
tracts can be created to summarise the entire paper or
focus on specific user-specified aspects. The num-
ber of sentences to include in the summary is pre-
specified (either directly or using a compression ra-
tio).
Our approach also makes use of the scientific dis-
course for summarisation purposes. We use the sci-
entific discourse to create a content model for ex-
tractive summarisation, with a focus on represent-
ing the content of the full paper, while keeping the
cohesion of the narrative. We first automatically
annotate the articles with a scheme which captures
fine-grained aspects of the content and conceptual
structure of the papers, namely the Core Scientific
Concepts (CoreSC) scheme (Liakata et al, 2010; Li-
akata et al, 2012). The CoreSC scheme is ?uniquely
suited to recovering common types of scientific ar-
guments about hypotheses, explanations, and evi-
dence? (White et al, 2011), which are not read-
ily identifiable by other annotation schemes. Also,
when compared to argumentative zoning and more
specifically its extension for chemistry papers, AZ-
II (Teufel et al, 2009), it was shown to provide a
greater level of detail in terms of categories denot-
ing objectives, methods and outcomes whereas AZ-
II focusses on the attribution of knowledge claims
and the relation with previous work (Liakata et al,
2010).
We then use the distribution of CoreSC categories
observed in abstracts to create a content model
which provides a skeleton for extractive summaries.
The reasoning behind this is to try to preserve co-
hesion within the summaries and we hypothesise
that the sequence of CoreSC categories is a good
proxy for cohesion (see section 3.1). In creating
the summary, instantiating the content model, we
identify independent categories and dependent cate-
gories, and we argue that in order to preserve the co-
hesion of the text the independent categories should
be determined first (see section 3.2). We also pre-
serve in the summary the distribution of CoreSC cat-
egories found in the corresponding full paper.
Finally, we evaluate the extractive summaries in
a complex real world question-answering task, in
which we assess the usefulness of the summaries as
well as to what extent the generated CoreSC sum-
maries represent the content of the original arti-
cle. Experts are presented with different types of
summaries and are asked to answer article-specific
questions on the basis of the summaries (see sec-
tion 4.1). Our results show that automatically gen-
erated CoreSC summaries can answer 66% of com-
plex questions with 75% precision, outperforming
a baseline of microsoft autosummarise summaries
(See section 4.2).
We have also peformed an intrinsic evaluation of
the summaries using ROUGE and automatic mea-
sures for summary informativeness, such as the
Jensen-Shannon divergence, yielding positive re-
sults (See section 4.2). However, as such measures
have not yet reached maturity and are harder to in-
terpret, we consider the user-based evaluation to be
a more reliable measure of summary quality.
Code for generating the summaries can be ob-
tained by contacting the first author and/or visiting
http://www.sapientaproject.com/software.
2 Related work
The Core Scientific Concepts (CoreSC) Scheme:
The CoreSC scheme consists of three layers; the first
layer corresponds to eleven concepts (Background
(BAC), Hypothesis (HYP), Motivation (MOT), Goal
(GOA), Object (OBJ), Method (MET), Model
(MOD), Experiment (EXP), Observation (OBS),
Result (RES) and Conclusion (CON)); the second
layer corresponds to properties of the concepts (e.g.
New/Old) and the third layer provides identifiers
which link instances of the same category. Liakata
et al (2010) created a corpus of 265 full scientific
articles from chemistry and biochemistry annotated
748
with this scheme and trained classifiers using SVMs
and CRFs in (Liakata et al, 2012), with an accu-
racy of >51% across the 11 concepts. Their data
and CoreSC classification system are available on-
line and can provide a good benchmark for com-
parison. Louis & Nenkova (2012) have successfully
used the CoreSC corpus for evaluating syntax-based
coherence models, which indicates the strong con-
nection between coherence and discourse structure.
Summarisation for scientific articles: A lot of
the work on summarising scientific articles has fo-
cussed on citation-based summaries. Qazvinian &
Radev (2008) use sentences from papers citing the
article to be summarised. Sentences are clustered to-
gether creating a topic, with the combination of clus-
ters forming a citation summary network. Qazvinian
& Radev (2010), (Qazvinian et al, 2010) also make
use of citation sentences in other scientific papers to
summarize the contributions of a paper. The draw-
back of citation summaries is that a paper must be
already cited, so this type of summary will not be
useful to a paper reviewer. Also, citations of articles
will have been influenced by other citations rather
than the paper itself.
Document models for summarisation: Our con-
tent model has some similarities with content mod-
elling using global sentence ordering (Barzilay and
Lee, 2004; Chen et al, 2009). In (Barzilay and
Lee, 2004) unsupervised methods are used to cre-
ate HMM topic sequence models for newswire text
articles. Topics are assigned to texts according to
the content model and extracts of fixed length are
created by selecting the topics most likely to occur
in summaries. While we use supervised methods to
annotate papers with a fixed set of topics (CoreSCs)
in scientific papers, our summary content model for
extracts shares similar principles such as global or-
dering of sentences and non-recurrence. However,
their evaluation involved newspaper articles and ex-
tracts which are a lot shorter (15 and 6 sentences,
respectively).
It is not clear whether unsupervised topic mod-
elling such as (Chen et al, 2009) can be applied to
scientific articles (over 100 sentences long), which
by nature include repetition of topics. It would be
interesting to make comparisons with summaries us-
ing content models learnt from our data automati-
cally, following a similar approach to (Sauper et al,
2010) which learns a content model jointly with a
particular supervised task in web-based documents.
3 Extractive Summarisation using
CoreSCs
In this section we describe how we use CoreSC dis-
course categories annotated at the sentence level to
create extractive summaries of full papers, which we
subsequently evaluate in a question answering task
in section 4.
To generate summaries we follow classic text ex-
traction techniques while making use of a document
content model based on CoreSCs. Our aim is for
the content model to reflect both the distribution of
CoreSCs in the paper as well as the discourse model
of human summaries, as the latter is indicated by the
generic ordering of CoreSC categories in abstracts
encountered in a corpus of 265 annotated full pa-
pers (Liakata and Soldatova, 2009; Liakata et al,
2012). While we do not consider abstracts to be ade-
quate summaries, we at least consider them to be co-
herent summaries, which is why the content model
reflects the distribution of CoreSCs in the abstracts.
To create our summaries, we employed automat-
ically generated CoreSC annotations, which are the
output of the classifiers described in (Liakata et al,
2012). These classifiers assign CoreSC categories
to sentences on the basis of features local to a sen-
tence, such as significant n-grams, verbs and word
triples, as well as global features such as the posi-
tion of the sentence within the document and within
a paragraph and section headers. The following sub-
sections give details about the creation of extractive
summaries from CoreSC categories.
3.1 A content model for CoreSC extractive
summaries
Building an extractive summary using a computa-
tional model of document structure is an idea shared
by many previous approaches, whether the model is
hand-crafted, based on rhetorical elements (McKe-
own, 1985; Teufel and Moens, 2002) or rhetorical
relations (Marcu, 1998b; Marcu, 1998a) or whether
it is a content model, learnt automatically from text
as in (Barzilay and Lee, 2004), focussing on the lo-
cal content or a combination of the local content and
global structure (Sauper et al, 2010).
749
Our document content model is primarily based
on the global discourse of the article as provided by
the type and number of CoreSC categories. How-
ever, unlike (Teufel and Moens, 2002), who take a
fixed number of AZ categories of specific type to
create rhetorical extracts, the number of categories
used from each CoreSC category depends on their
distribution in the original article. Any and all types
of CoreSC category could potentially appear in a
summary, as our summaries are meant to be repre-
sentative of the entire content of the paper. Also, the
ordering of the categories in the summary is learnt
to reflect the ordering of categories observed in ab-
stracts of papers from the same domain.
Our model also caters for local discourse depen-
dencies. For example, the selection of a particu-
lar ?Method? sentence for inclusion in the summary
should influence the choice of ?Experiment? sen-
tences, which refers to particular experimental pro-
cedures performed. This is not an issue of concern
to (Teufel and Moens, 2002), but relates to the no-
tion of NUCLEUS and SATELLITE clauses, which
form the foundation of Rhetorical Structure The-
ory (Mann and Thompson, 1998), and guides the
summarisation paradigm of (Marcu, 1998a; Marcu,
1998b). However, the difference here is that we
define a-priori certain categories to be independent
(have the property of playing the role of nucleus in
the discourse) and specify their relation with partic-
ular types of dependent categories. Thus, nuclearity
becomes a property of the CoreSC category, which
is indirectly inherited by the sentence.
Therefore, when creating the CoreSC content
model for summaries we addressed the following is-
sues: (i) summary length; (ii) number of sentences
from each CoreSC, (iii) the ordering in which sen-
tences from each CoreSC category should appear
and (iv) the extraction of sentences according to in-
dependent and dependent categories.
? Summary length: While the literature (Teufel,
2010, p.45) suggests that 20?30% of the original
document is required for an adequately informa-
tive summary, (Teufel, 2010, p.55) assumes this
is too long for scientific papers. For this reason
and to allow better comparison between papers
of varying lengths, we fixed our summary length
to 20 sentences. This is reasonable considering
we have 11 CoreSCs, any and all of which can
appear in both abstracts and full papers.
? Number of sentences from each category: To
reflect the content of the paper, the distribution
of the CoreSC categories in the extract follows
the distribution of CoreSCs in the full paper.
For each CoreSC we determine the number of
sentences to be selected (n(selected(C))) by
multiplying the ratio of that category in the paper
by 20. A difficulty arises if the ratio of a partic-
ular concept in the paper is very low (? 0.05)
in which case we prefer to include one sentence.
If a particular concept is not at all present in the
paper, the number of selected sentences for that
category will be 0.
? Ordering of CoreSC categories in the sum-
mary: According to a study of empirical sum-
maries (Liddy, 1991), sentences of a particular
textual type appear in a particular order. Since
paper abstracts were the closest approximation
of human summaries available to us, CoreSC
category transitions found in abstracts have been
adopted in our content model for extracts. The
transitions were derived semi-empirically. First,
we extracted initial, medium and final bi-grams
of categories from paper abstracts together with
transition probabilities.
Using this information we manually constructed
transitions of the CoreSC categories that best fit
the observed frequencies and our own intuitions.
This gave us the following sequence: MOT >
(HYP) > OBJ > GOA > BAC > MOD > MET
> EXP > OBS > (HYP) > RES > CON. HYP
appears twice in the sequence as annotators had
distinguished two types of hypotheses, global
hypotheses (stated together with other objec-
tives) and hypotheses about particular observa-
tions. The model provides an amalgamated rep-
resentation of CoreSC concepts in abstracts. In-
terestingly, our semi-empirically derived model
closely follows the content model for abstracts
described in (Liddy, 1991). It would be interest-
ing to see how this compares to a Markov model
of CoreSC categories learnt from the annotated
abstracts.
750
3.2 Sentence extraction based on independent
and dependent categories
Sentence extraction involves selecting the most rel-
evant sentences to include in a summary. Typically,
this entails ranking the sentences according to some
measure of salience and selecting the top n-best
sentences. For example, a sentence will be repre-
sented by a number of features associated with it,
such as whether it contains certain high frequency
words or cue phrases, its location in the document,
location in a paragraph (Brandow et al, 1995; Ku-
piec et al, 1995). Other methods include clustering
based on sentence similarity and choosing the cen-
troids (Erkan and Radev, 2004) or choosing the best
connected sentences (Mihalcea and Tarau, 2004).
When sentences are classified according to
CoreSC categories features such as the ones de-
scribed above for text extraction are taken into
account. Liakata et al (2012) report that the
most salient features for classifying CoreSC cat-
egories are overall n-grams, verbs and direct ob-
jects whereas other features such as the location of
the sentence, the neighbouring section headings and
whether a sentence contains citations play an impor-
tant role for some of the categories. Thus, classi-
fication into CoreSC categories already provides a
selection bias for sentence extraction.
As explained in section 3.1, the number of
CoreSC categories in the summaries is determined
according to their distribution in the paper and the
order of the categories is specified in the content
model. Salience for sentence extraction in this case
is determined by the need to select the most repre-
sentative sentences for a category. There isn?t much
point, for example, in identifying that we need to in-
clude a Method sentence (MET) and that this should
be followed by an Experiment sentence (EXP), if we
are not sure that those are indeed the categories of
the sentences we are about to select.
We therefore rank sentences according to the clas-
sifier confidence score (probability) with which they
were assigned a CoreSC category in (Liakata et al,
2012). The intuition behind this is that sentences
with high classifier confidence will be less noisy,
high precision cases and more representative of a
particular category. Indeed, (Liakata et al, 2012)
report statistical significance for the correlation be-
tween high classifier confidence and agreement be-
tween manual and automatic classification
However, as mentioned in section 3.1, there is
inter-dependence between sentences in the text,
which is in turn inherited by the categories assigned
to them. For example, the highest ranking MET
sentence will be related to an Experiment (EXP) or
Background (BAC) sentence, which may not be the
ones with the highest confidence score in their cate-
gory.
In order to preserve discourse cohesion it is im-
portant to select related sentences from different
categories. We resolve this by distinguising the
CoreSCs into independent categories, which by def-
inition are expected to show nucleus behaviour, and
dependent categories. We also specify the rela-
tion between independent and dependent categories.
The independent categories include the categories
with the lowest percentage of sentences in scien-
tific articles as reported in (Liakata et al, 2012),
namely: Motivation (MOT) (1%), Goal (GOA)(1%),
Hypothesis (HYP)(2%), Object (OBJ)(3%), Model
(MOD)(9%), Conclusion (CON)(9%) and Method
(MET)(11%). Categories whose sentence selec-
tion semantically depends on the former are Exper-
iment (EXP)(10%), Background (BAC)(19%), Re-
sult (RES)(21%) and Observation (OBS)(14%). The
independent categories also have higher precision
than recall, in contrast to the dependent categories.
While MET and EXP are almost equally represented
in the CoreSC corpus, EXP by definition provides
the detailed steps of an experimental method and
thus it is semantically dependent on some MET cat-
egory. More specifically, the dependencies are con-
sidered to be as follows: EXP, BAC depend on MET,
RES depends on CON and OBS depends on RES
(OBS is double-dependent).
Sentence extraction is driven by first identifying
the independent categories based on classifier con-
fidence scores and then choosing the corresponding
dependent categories on the basis of both related-
ness to the independent categories and classifier con-
fidence. We use sentence proximity (defined below)
as a measure for relatedness and combine it with
classifier confidence during sentence extraction.
The mechanism to select sentences for inclusion
in the summary, which considers category depen-
dencies, proceeds as follows:
751
? For an independent category CatI, order sentences by
decreasing order of confidence score. The confidence
score is the average confidence score of the SVM and
CRF classifiers reported in (Liakata et al, 2012) for
a sentence.
? For a dependent category Cat, for which we need n
sentences, given the selected sentences m from the
corresponding independent category CatI we do the
following:
? If m = 0, then treat Cat as independent category
for this case.
? Otherwise, for each selected sentence ti in CatI,
calculate its proximity score to every sentence cj
of the dependent category Cat. Proximity is de-
fined as 1?Distance where Distance is an ab-
solute difference in sentence ids between cj and
ti normalised by the maximum absolute distance
found between all cj and ti pairs.
? The classifier prediction score for each cj is mul-
tiplied by the Proximity(cj , ti) score and the
sentences are re-ranked according to the new
scores, where only the n highest ranking cjs are
kept. The last two steps result in an m?n matrix.
? If m = 1, then the choice for the n sentences for
Cat is straightforward.
? Otherwise, we pick the n highest ranking cjs,
proceeding row-wise. Thus, the highest ranking
cjs for the highest ranking independent sentences
ti are given priority and any cj is chosen at most
once.
Once the sentence ids are selected for each inde-
pendent and each dependent category we plug them
into the content model. Sentence order is preserved
within each CoreSC category. For example, if two
Result sentences are selected, the order in which
they appear in the paper will be preserved in the
summary.
4 Summary evaluation via question
answering
4.1 Task Description and experimental setup
We evaluate the extractive CoreSC summaries in
terms of how well they enable 12 chemistry ex-
perts/evaluators (with at least a Masters degree in
chemistry) to answer complex questions about the
papers. Our test corpus consists of 28 papers held
out from the ART/CoreSC corpus, roughly 1/9,
which were annotated automatically with the SVM
and CRF classifiers described in (Liakata et al,
2012) trained on the remaining 8/9 of the corpus.
For each of the 28 papers in the test corpus, we gen-
erated CoreSC summaries automatically using the
method described in section 3. We compare the
performance of the experts on a question answer-
ing (Q-A) task when given the CoreSC summaries
and two other types of summary, amounting to a
total of three experimental conditions (A,B,C). The
other two types of summary are the original paper
abstracts (summaries A), in the absence of human
summaries, and summaries generated by Microsoft
Office Word 2007 AutoSummarize (summaries B).
Microsoft Office Word 2007 AutoSummarize
(MA) is a widely available commercial system with
reportedly good results (Garcia-Hernandez et al,
2009) and performance equivalent to TextRank (Mi-
halcea and Tarau, 2004). MA works by assigning a
score to each word in a sentence depending on its
frequency in the document and sentences are ranked
and extracted according to the combination of scores
of the words they contain. MA therefore follows
classic lexicalised text extraction techniques, is do-
main independent and is completely agnostic of the
discourse. For the latter reason, we considered MA
to be a suitable baseline the comparison with which
would illustrate the effect of using CoreSC cate-
gories on the summary and the merits of having a
discourse based model for summarisation.
Neither the paper title nor section headings were
available to any of the summarising systems as our
extractive system does not make direct use of them
and we were not sure how they would influence MA.
To ensure that each evaluator considered only one
type of summary per paper, so as to avoid bias from
previous stimuli, and to make sure all experts were
exposed to all papers and all types of summary, the
12 experts were assigned to four groups (G1-G4)
and were allocated 28 summaries each according to
the Latin Square design in Table 1.1.
The experimental setup follows the paradigm
of (Teufel, 2001). However, while (Teufel, 2001) de-
veloped a Q-A task to evaluate summaries showing
the contribution of a scientific article in relation to
previous work, the purpose of the Q-A task at hand
1Initially we had four experimental conditions but one was
dropped, so is not presented in this context
752
is to show the usefulness of the extracted summaries
in answering questions on the paper, and how they
compare to a discourse-agnostic baseline. In the
case of (Teufel, 2001) the task consists of a fixed set
of five questions, the same for all articles tuned par-
ticularly to the relation of current and previous work.
By contrast, the current Q-A task aims to show how
well the summaries represent the content of the en-
tire paper, which means that questions are individ-
ual to each paper and required domain knowledge to
create.
Each of the 12 experts answered three content-
based questions per summary, where the questions
were individual to each paper. An example of the
questions and the corresponding answers for a given
paper can be found below.
Example 4.1.1
? Q:What do DNJ imino sugars inhibit the action of?
A: They inhibit glycosidases and ceramide glucosyl-
transferases.
? Q:What methods do the authors use to study the confor-
mation of N-benzyl-DNJ?
A: They use resonant two-photon ionization (R2PI),
ultraviolet?ultraviolet (UV?UV) hole burning, and in-
frared (IR) ion-dip spectroscopies in conjunction with
electronic structure theory calculations.
? Q:What is the conformation of the exocyclic hydrox-
ymethyl group?
A: The exocyclic hydroxymethyl group is axial to the
piperidine ring (gauche- to the ring nitrogen).
As one can see, the questions are complex wh-
questions and correspond to answers with multiple
components. Questions were complex, to minimise
the likelihood of correct random answers. They
were designed by a senior chemistry expert with
knowledge of linguistics, so that they could be an-
swered based on the abstracts (A). For this purpose,
the senior expert chose abstracts that were at least
three sentences long. Ideally, the questions and an-
swers should have been set on the basis of the en-
tire paper, but this was not possible given our time-
frame for the experiment.The underlying assump-
tion is that a good summary should cover most of the
main points of the paper. One of the merits of set-
ting the questions on the basis of the abstracts was
that the answers to be identified were deemed suf-
ficiently important to be expressed in the humanly
created abstract. However, automatic summaries
created in the way proposed here could potentially
answer questions beyond the scope of the abstract
and in cases of very short abstracts be much more
informative.
Experts were told that summaries were automati-
cally generated with no details about different types
of summary; it is assumed that none of them is com-
pletely familiar with the work mentioned in the 28
papers.
On average, it took experts less than 10 minutes to
read a summary and answer the three content-based
questions.
Papers (28)
Evaluator groups 1?7 8?14 15?21 22-28
G1 A B - C
G2 C A B -
G3 - C A B
G4 B - C A
Table 1: Distribution of summaries to evaluators
4.2 Results and Discussion
We compared each evaluator?s answers obtained af-
ter reading a summary against the model answers
set by the senior expert, the author of the questions,
based on the abstract (A) of the corresponding pa-
per. If an evaluator?s answer is identical to a model
answer, then this counts as ?matched?.
For instance in example 4.1.1 above, ?axial to
the piperidine ring?, ?gauche- to the ring nitrogen?
and ?The OH6 group is axial (Gauche) to the ring
nitrogen? were all considered correct, fully matched
answers to the question ?What is the conformation
of the exocyclic hydroxymethyl group??. In the
case of the second question in the same example all
of the following were considered correct and fully
matched: ?Resonant two-photon ionization (R2PI),
UV/UV hole-burn, and IR ion-dip spectroscopies in
conjunction with electronic structure theory calcula-
tions?, ?R2PI UV/UV hole-burn IR ion-dip e- struc-
ture theory calculations? and ?a combination of res-
onant two-photon ionization (R2PI), UV/UV hole-
burn, and IR ion-dip spectroscopies in conjunction
with electronic structure theory calculations?.
If the answer requires listing more than one item
(as is the case with questions one and two of ex-
ample 4.1.1), all of the items have to be matched.
Partially matched answers are counted as ?partially
matched?. Non-matching answers can be of two
753
types. If an un-matched answer coincided with the
answer the senior expert would have given after
reading that particular summary, then it was marked
as ?un-matched:justified?: Such answers were cor-
rect given the particular summary, but are not nec-
essarily correct with respect to the paper and do
not count as alternative answers. If the answer
was un-matched and also unjustified given the con-
tent of the summary, then it was marked as ?un-
matched:unjustified? . These are cases of evalua-
tor error. Similarly, cases where the evaluator gave
?N/A? as an answer were marked as ?justified? or
?unjustified? according to whether the senior expert
could find the answer in the summary or not. The
results from marking answers are shown in Table 2.
Number of A B C
Matched 240 126 135
Partially matched 0 4 3
Un-matched:justified 0 25 15
N/A:justified 0 71 71
Un-matched:unjustified 5 11 17
N/A:unjustified 7 15 11
All answers 252 252 252
Table 2: Matches between summary-based answers and
model answers
Micro-AVG Macro-AVG
S. types R P F R P F
A 1 0.95 0.98 1 0.95 0.97
B 0.64 0.70 0.67 0.64 0.64 0.60
C 0.66 0.75 0.70 0.64 0.70 0.65
Table 3: Precision, Recall and F-score for answering
questions using the four types of summary. A: abstracts,
B: autosummarize, C:automatic CoreSC summaries.
We report Precision, Recall and F-score (P-R-F)
for answering questions given each type of sum-
mary (Table 3). To calculate these we define TP as
matched answers, FN as N/A:justified and FP every-
thing else (partially matched + un-matched:justified
+ un-matched:unjustified + N/A:unjustified). Here,
the standard definition of recall (TP/(TP+FN))
demonstrates how many questions can be answered
using the summary (summary coverage) and Preci-
sion (TP/(TP+FP)) how well the questions are an-
swered (summary clarity).
We consider the F-measure to be an overall indi-
cator of the summary usefulness. Micro-averaging
is obtained by adding all answers from all papers to
calculate TP, FN and FP whereas macro-averaging
calculates P-R-F first per paper and then averages
over all papers.
The rankings remain consistent regardless of the
averaging method. Condition A (abstracts) shows
perfect Recall (the evaluators are able to answer all
the questions) whereas Precision is affected by un-
justified failed matches (Table 2). The perfect recall
is hardly surprising as the questions are designed
on the basis of the abstract but provides a sanity
check for the experiment. The precision sets an up-
per bound for precision with automatic summaries.
Summaries of condition C provide answers to more
questions (Recall) and with greater accuracy (Pre-
cision) than summaries B. When macro-averaging,
the Recall score of summaries C is tied with that for
summaries B but Precision is 6% higher.
To verify the statistical significance for the dif-
ference in precision and recall for summaries B and
C respectively, we performed Monte Carlo sampling
10000 times, for the populations of answers for sum-
maries B and C. During each iteration of sampling,
precision and recall were calculated, creating popu-
lations of 10000 recalls and 10000 precisions propa-
gated to be representative of the original population
of answers. A t-test performed on the population
of precision and the population of recalls showed
statistical significance at 95% in both cases, with
summaries C having a precision of 5% higher and
a recall of 1.4-1.6% higher than summaries B (see
Table 4). Therefore, we can say that CoreSC sum-
maries C are overall better for answering questions
than summaries B.
Comparison between B and C (B-C)
precision recall
t = -105.90 t = -32.52
df = 19959.79 df = 19994.40
p-value < 2.2e-16 p-value < 2.2e-16
alternative hypothesis: true difference in means 6= 0
95% confidence interval: 95% confidence interval:
-0.051 -0.049 -0.016 -0.014
sample estimates: sample estimates:
mean of x mean of y mean of x mean of y
0.696 0.746 0.639 0.655
Table 4: Test for statistical significance betwen sum-
maries B (microsoft) and C (CoreSC)
The difference in precision between summaries
B and C shows the advantage of having a con-
754
tent model: summaries C are significantly clearer.
We had also expected CoreSC summaries to have a
much higher coverage than summaries B, and there-
fore significantly higher recall. However, this dif-
ference was less pronounced perhaps because au-
tosummarize favours shorter sentences, which are
more likely to be found in the abstracts. We expect
that a refinement in the sentence selection criterion,
which would also take sentence length into account,
will help to showcase further the benefits of using a
CoreSC-based content model.
Analysis using ROUGE showed that while sum-
maries C had a slightly higher ROUGE-1 measure
than summaries B (0.75 vs 0.73), with respect to ab-
stracts, ROUGE-L was the same for the two (0.70).
In table 5 we also report measurements on sum-
mary informativeness based on divergence (Kull-
back Leibler (KL) divergence and Jensen Shannon
(JS) divergence), as in (Louis and Nenkova, 2013).
KL divergence is asymmetric and reflects the aver-
age number of bits wasted by coding samples of a
distribution P using another distribution Q. JS diver-
gence is an information-theoretic measure, reflect-
ing the average distance of the KL divergence be-
tween summary and input (the full paper in our case)
from the mean vocabulary distributions. Compared
to other measures, JS divergence has been found
to produce the best predictions of summary qual-
ity (Louis and Nenkova, 2013). In practice, what JS
divergence tells us is how ?different?/divergent the
summary is from the original paper. Low divergence
scores are indicative of greater overlap between the
summaries and the original paper and are considered
positive in terms of the summary information con-
tent.
type KLI-S KLS-I UnJSD SJSD
B 1.66 0.70 0.21 0.19
C 1.40 0.62 0.18 0.17
random 1.61 0.79 0.21 0.19
Table 5: Macro-averaged divergence scores for the 28
test summaries. B: Autosummarize, C: CoreSC, random:
random summaries each 20 sentences long for each paper.
KLI-S: Average Kullback Leibler divergence between in-
put and summary. KLS-I: Kullback Leibler divergence
between summary and input, since KL divergence is not
symmetric. UnJSD: Jensen Shannon divergence between
input and summary. No smoothing. SJSD:A version with
smoothing.
One can see the that CoreSC summaries have con-
sistently lower divergence (both KL and JS) than mi-
crosoft autosummarise summaries and random sum-
maries of the same length. This is a positive out-
come but since such automatic measures of sum-
mary quality have not yet reached maturity and are
harder to interpret, we consider the manual evalua-
tion a more reliable indicator of summary informa-
tiveness and usefulness. Note that it is not appropri-
ate to use divergence to assess the abstracts as this
measure is influenced by the length of a text, which
varies dramatically in the case of abstracts.
5 Conclusions and future work
We have shown how a content model based on
the scientific discourse as annotated by the CoreSC
scheme can be used to produce extractive sum-
maries. These summaries can be generated as al-
ternatives to abstracts. Since they preserve the dis-
tribution of CoreSCs in the paper and are not pro-
duced independently of it, as is the case with many
abstracts, they are potentially more representative of
abstracts than the full article. We have tested the use-
fulness CoreSC based summaries in answering com-
plex questions relating to the content of scientific
papers. Extracts from automated CoreSCs are infor-
mative, outperform microsoft autosummarise sum-
maries, in both intrinsic and extrinsic evaluation, and
enable experts to answer 66% of complex questions
with a precision of 75%.
In the future we would like to experiment further
with refining the sentence selection method so as
to consider criteria for local cohesion, such as lex-
ical chains. We would also like to perform com-
parisons with automatically induced content mod-
els and check their viability for scientific articles.
We also would like to perform a human based eval-
uation of coherence and explore the full potential
of these summaries as alternatives to author-written
abstracts. This work constitutes a very important
step in producing automatic summaries of scientific
papers and enabling experts to extract information
from the papers, a major requirement for resource
curation, which is dependent on constant reviewing
of the literature.
755
Acknowledgements
This work has been funded by an Early Career
Leverhulme Trust Fellowship to Dr Liakata and by
EMBL-EBI, UK. The authors would like to thank
Annie Louis, Yufan Guo, Simone Teufel, Stephen
Clark and the anonymous reviewers for their valu-
able comments. We would also like to thank Mo
Abrahams for the python version of the summarisa-
tion code and the cafe summary toolkit.
References
S. Ananiadou, Pyysalo S., and J. Tsujii. 2010. Event
extraction for systems biology by text mining the liter-
ature. Trends in Biotechnology, 28(7):381?390.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120. Best paper award.
R. Brandow, K. Mitze, and L. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection. Information Processing and Management,
31:675?685.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Content modeling using latent
permutations. J. Artif. Int. Res., 36:129?163, Septem-
ber.
A. M. Cohen and W. R. Hersh. 2005. A survey of current
work in biomedical text a survey of current work in
biomedical text mining. Briefings in Bioinformatics,
6:57?71.
Danish Contractor, Yufan Guo, and Anna Korhonen.
2012. Using argumentative zones for extractive sum-
marization of scientific articles. In COLING, pages
663?678.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search, 22:2004.
R.A. Garcia-Hernandez, Y. Ledeneva, G.M. Mendoza,
A.H. Dominguez, J. Chavez, A. Gelbukh, and J.L.T.
Fabela. 2009. Comparing commercial tools and state-
of-the-art methods for generating text summaries.
In Artificial Intelligence, 2009. MICAI 2009. Eighth
Mexican International Conference on, pages 92?96,
November.
Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011.
A weakly-supervised approach to argumentative zon-
ing of scientific documents. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 273?283. Association for
Computational Linguistics.
T. Kim, J.and Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of bionlp?09 shared task on event ex-
traction. In Proceedings of the Workshop on BioNLP:
Shared Task, pages 1?9, Boulder, Colorado.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 1?6,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
756
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings of
the 18th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?95, pages 68?73, New York, NY, USA. ACM.
M. Liakata and L.N. Soldatova. 2009. The ART Corpus.
Technical report, Aberystwyth University.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batchelor.
2010. Corpora for the conceptualisation and zoning of
scientific papers. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Evalu-
ation, Valetta,Malta.
M. Liakata, S. Saha, S. Dobnik, C. Batchelor, and
Rebholz-Schuhmann D. 2012. Automatic recognition
of conceptualisation zones in scientific articles and
two life science applications. Bioinformatics, 28:991?
1000.
Elizabeth DuRoss Liddy. 1991. The discourse-level
structure of empirical abstracts: an exploratory study.
Inf. Process. Manage., 27:55?81, February.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1157?1168. Asso-
ciation for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically as-
sessing machine summary content without a gold stan-
dard. Computational Linguistics, 39(2):267?300.
W. C. Mann and S. A. Thompson. 1998. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 1998a. Improving summarization
through rhetorical parsing tuning. In Proceedings of
The Sixth Workshop on Very Large Corpora, pages
206?215, Montreal,Canada.
Daniel C. Marcu. 1998b. The rhetorical parsing,
summarization, and generation of natural language
texts. Ph.D. thesis, Toronto, Ont., Canada, Canada.
AAINQ35238.
Kathleen R. McKeown. 1985. Text generation: using
discourse strategies and focus constraints to generate
natural language text. Cambridge University Press,
New York, NY, USA.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Texts. In Conference on Empirical
Methods in Natural Language Processing, Barcelona,
Spain.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proceedings of the 22nd International
Conference on Computational Linguistics - Volume 1,
COLING ?08, pages 689?696, Morristown, NJ, USA.
Association for Computational Linguistics.
Vahed Qazvinian and Dragomir R Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th annual
meeting of the association for computational linguis-
tics, pages 555?564. Association for Computational
Linguistics.
Vahed Qazvinian, Dragomir R Radev, and Arzucan
O?zgu?r. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
pages 895?903. Association for Computational Lin-
guistics.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text anal-
ysis applications. In EMNLP?10, pages 377?387.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28:409?445, De-
cember.
Simone Teufel, Advaith Siddharthan, and Colin Batche-
lor. 2009. Towards discipline-independent argumen-
tative zoning: Evidence from chemistry and computa-
tional linguistics. In Proceedings of EMNLP-09, Sin-
gapore.
Simone Teufel. 2001. Task-based evaluation of sum-
mary quality: Describing relationships between sci-
entific papersworkshop ?automatic summarization?,
naacl-2001. In NAACL-01 Workshop ?Automatic Text
Summarisation?, Pittsburgh, PA.
Simone Teufel. 2010. The Structure of Scientific Arti-
cles: Applications to Citation Indexing and Summa-
rization. CSLI Studies in Computational Linguistics.
Center for the Study of Language and Information,
Stanford, California.
Elizabeth White, K. Bretonnel Cohen, and Larry Hunter.
2011. Hypothesis and evidence extraction from full-
text scientific journal articles. In Proceedings of
BioNLP 2011 Workshop, pages 134?135, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
757
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 72?79,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Probabilistic Rich Type Theory for Semantic Interpretation
Robin Cooper
1
, Simon Dobnik
1
, Shalom Lappin
2
, and Staffan Larsson
1
1
University of Gothenburg,
2
King?s College London
{cooper,sl}@ling.gu.se, simon.dobnik@gu.se, shalom.lappin@kcl.ac.uk
Abstract
We propose a probabilistic type theory in which a
situation s is judged to be of a type T with probabil-
ity p. In addition to basic and functional types it in-
cludes, inter alia, record types and a notion of typ-
ing based on them. The type system is intensional
in that types of situations are not reduced to sets
of situations. We specify the fragment of a com-
positional semantics in which truth conditions are
replaced by probability conditions. The type sys-
tem is the interface between classifying situations
in perception and computing the semantic interpre-
tations of phrases in natural language.
1 Introduction
Classical semantic theories (Montague, 1974), as
well as dynamic (Kamp and Reyle, 1993) and un-
derspecified (Fox and Lappin, 2010) frameworks
use categorical type systems. A type T identifies
a set of possible denotations for expressions in T ,
and the system specifies combinatorial operations
for deriving the denotation of an expression from
the values of its constituents.
These theories cannot represent the gradience
of semantic properties that is pervasive in speak-
ers? judgements concerning truth, predication, and
meaning relations. In general, predicates do not
have determinate extensions (or intensions), and
so, in many cases, speakers do not make categor-
ical judgements about the interpretation of an ex-
pression. Attributing gradience effects to perfor-
mance mechanisms offers no help, unless one can
show precisely how these mechanisms produce the
observed effects.
Moreover, there is a fair amount of evidence in-
dicating that language acquisition in general cru-
cially relies on probabilistic learning (Clark and
Lappin, 2011). It is not clear how a reasonable
account of semantic learning could be constructed
on the basis of the categorical type systems that ei-
ther classical or revised semantic theories assume.
Such systems do not appear to be efficiently learn-
able from the primary linguistic data (with weak
learning biases), nor is there much psychological
data to suggest that they provide biologically de-
termined constraints on semantic learning.
A semantic theory that assigns probability
rather than truth conditions to sentences is in a
better position to deal with both of these issues.
Gradience is intrinsic to the theory by virtue of
the fact that speakers assign values to declarative
sentences in the continuum of real numbers [0,1],
rather than Boolean values in {0,1}. In addition,
a probabilistic account of semantic learning is fa-
cilitated if the target of learning is a probabilistic
representation of meaning. Both semantic repre-
sentation and learning are instances of reasoning
under uncertainty.
Probability theorists working in AI often de-
scribe probability judgements as involving distri-
butions over worlds. In fact, they tend to limit
such judgements to a restricted set of outcomes
or events, each of which corresponds to a par-
tial world which is, effectively, a type of situa-
tion (Halpern, 2003). A classic example of the re-
duction of worlds to situation types in probability
theory is the estimation of the likelihood of heads
vs tails in a series of coin tosses. Here the world
is held constant except along the dimension of a
binary choice between a particular set of possi-
ble outcomes. A slightly more complex case is
the probability distribution for possible results of
throwing a single die, which allows for six pos-
sibilities corresponding to each of its numbered
faces. This restricted range of outcomes consti-
tutes the sample space.
We are making explicit the assumption, com-
mon to most probability theories used in AI, with
clearly defined sample spaces, that probability
is distributed over situation types (Barwise and
Perry, 1983), rather than over sets of entire worlds.
An Austinian proposition is a judgement that a
72
situation is of a particular type, and we treat it
as probabilistic. In fact, it expresses a subjec-
tive probability in that it encodes the belief of an
agent concerning the likelihood that a situation is
of that type. The core of an Austinian proposi-
tion is a type judgement of the form s : T , which
states that a situation s is of type T . On our ac-
count this judgement is expressed probabilistically
as p(s : T ) = r, where r ? [0,1].
1
On the probabilistic type system that we pro-
pose situation types are intensional objects over
which probability distributions are specified. This
allows us to reason about the likelihood of alter-
native states of affairs without invoking possible
worlds.
Complete worlds are not tractably repre-
sentable. Assume that worlds are maximal con-
sistent sets of propositions (Carnap, 1947). If
the logic of propositions is higher-order, then the
problem of determining membership in such a set
is not complete. If the logic is classically first-
order, then the membership problem is complete,
but undecidable.
Alternatively, we could limit ourselves to
propositional logic, and try to generate a maxi-
mally consistent set of propositions from a single
finite proposition P in Conjunctive Normal Form
(CNF, a conjunction of disjunctions), by simply
adding conjuncts to P . But it is not clear what
(finite) set of rules or procedures we could use to
decide which propositions to add in order to gen-
erate a full description of a world in a systematic
way. Nor is it obvious at what point the conjunc-
tion will constitute a complete description of the
world.
Moreover, all the propositions that P entails
must be added to it, and all the propositions with
which P is inconsistent must be excluded, in or-
der to obtain the maximal consistent set of propo-
sitions that describe a world. But then testing the
satisfiability of P is an instance of the ksat prob-
lem, which, in the general case, is NP-complete.
2
1
Beltagy et al. (2013) propose an approach on which clas-
sical logic-based representations are combined with distribu-
tional lexical semantics and a probabilistic Markov logic, in
order to select among the set of possible inferences from a
sentence. Our concern here is more foundational. We seek to
replace classical semantic representations with a rich proba-
bilistic type theory as the basis of both lexical and composi-
tional interpretation.
2
The ksat problem is to determine whether a formula in
propositional logic has a satisfying set of truth-value assign-
ments. For the complexity results of different types of ksat
problem see Papadimitriou (1995).
By contrast situation types can be as large or as
small as we need them to be. They are not max-
imal in the way that worlds are, and so the issue
of completeness of specification does not arise.
Therefore, they can, in principle, be tractably rep-
resented.
2 Rich Type Theory and Probability
Central to standard formulations of rich type the-
ories (for example, (Martin-L?of, 1984)) is the no-
tion of a judgement a : T , that object a is of type
T . We represent the probability of this judgement
as p(a : T ). Our system (based on Cooper (2012))
includes the following types.
Basic Types are not constructed out of other ob-
jects introduced in the theory. If T is a basic type,
p(a : T ) for any object a is provided by a probabil-
ity model, an assignment of probabilities to judge-
ments involving basic types.
PTypes are constructed from a predicate and
an appropriate sequence of arguments. An exam-
ple is the predicate ?man? with arity ?Ind ,Time?
where the types Ind and Time are the basic type
of individuals and of time points respectively.
Thus man(john,18:10) is the type of situation (or
eventuality) where John is a man at time 18:10.
A probability model provides probabilities p(e :
r(a
1
, . . . , a
n
)) for ptypes r(a
1
, . . . , a
n
). We take
both common nouns and verbs to provide the com-
ponents out of which PTypes are constructed.
Meets and Joins give, for T
1
and T
2
, the meet,
T
1
? T
2
and the join T
1
? T
2
, respectively. a :
T
1
? T
2
just in case a : T
1
and a : T
2
. a : T
1
?
T
2
just in case either a : T
1
or a : T
2
(possibly
both).
3
The probabilities for meet and joint types
are defined by the classical (Kolmogorov, 1950)
equations p(a : T
1
? T
2
) = p(a : T
1
)p(a : T
2
| a : T
1
)
(equivalently, p(a : T
1
? T
2
) = p(a : T
1
, a : T
2
)), and
p(a : T
1
? T
2
) = p(a : T
1
) + p(a : T
2
) ? p(a : T
1
? T
2
),
respectively.
Subtypes A type T
1
is a subtype of type T
2
,
T
1
v T
2
, just in case a : T
1
implies a : T
2
no mat-
ter what we assign to the basic types. If T
1
v T
2
then a : T
1
?T
2
iff a : T
1
and a : T
1
?T
2
iff a : T
2
.
Similarly, if T
2
v T
1
then a : T
1
? T
2
iff a : T
2
and a : T
1
? T
2
iff a : T
1
.
If T
2
v T
1
, then p(a : T
1
? T
2
) = p(a : T
2
),
and p(a : T
1
? T
2
) = p(a : T
1
). If T
1
v T
2
,
3
This use of intersection and union types is not standard in
rich type theories, where product and disjoint union are pre-
ferred following the Curry-Howard correspondence for con-
junction and disjunction.
73
then p(a : T
1
) ? p(a : T
2
). These definitions
also entail that p(a : T
1
? T
2
) ? p(a : T
1
), and
p(a : T
1
) ? p(a : T
1
? T
2
).
We generalize probabilistic meet and join types
to probabilities for unbounded conjunctive and
disjunctive type judgements, again using the clas-
sical equations.
Let
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) be the conjunctive
probability of judgements a
0
: T
0
, . . . , a
n
: T
n
.
Then
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) =
?
p
(a
0
: T
0
, . . . , a
n?1
:
T
n?1
)p(a
n
: T
n
| a
0
: T
0
, . . . , a
n?1
: T
n?1
). If n = 0,
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) = 1.
We interpret universal quantification as an un-
bounded conjunctive probability, which is true if
it is vacuously satisfied (n = 0) (Paris, 2010).
Let
?
p
(a
0
: T
0
, a
1
: T
1
, . . .) be the disjunctive
probability of judgements a
0
: T
0
, a
1
: T
1
, . . ..
It is computed by
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) =
?
p
(a
0
: T
0
, . . . , a
n?1
: T
n?1
) + p(a
n
: T
n
) ?
?
p
(a
0
:
T
0
, . . . , a
n?1
: T
n?1
)p(a
n
: T
n
| a
0
: T
0
, . . . , a
n?1
:
T
n?1
). If n = 0,
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) = 0.
We take existential quantification to be an un-
bounded disjunctive probability, which is false if it
lacks a single non-nil probability instance (n = 0).
Conditional Conjunctive Probabilities are
computed by
?
p
(a
0
: T
0
, . . . , a
n
: T
n
| a : T ) =
?
p
(a
0
: T
0
, . . . , a
n?1
: T
n?1
| a : T )p(a
n
: T
n
|
a
0
: T
0
, . . . , a
n?1
: T
n?1
, a : T )). If n = 0,
?
p
(a
0
:
T
0
, . . . , a
n
: T
n
| a : T ) = 1.
Function Types give, for any types T
1
and T
2
,
the type (T
1
? T
2
). This is the type of total func-
tions with domain the set of all objects of type
T
1
and range included in objects of type T
2
. The
probability that a function f is of type (T
1
? T
2
)
is the probability that everything in its domain is of
type T
1
and that everything in its range is of type
T
2
, and furthermore that everything not in its do-
main which has some probability of being of type
T
1
is not in fact of type T
1
. p(f : (T
1
? T
2
)) =
?
a?dom(f)
p
(a : T
1
, f(a) : T
2
)(1?
?
a6?dom(f)
p
(a : T
1
))
Suppose that T
1
is the type of event where there
is a flash of lightning and T
2
is the type of event
where there is a clap of thunder. Suppose that f
maps lightning events to thunder events, and that
it has as its domain all events which have been
judged to have probability greater than 0 of being
lightning events. Let us consider that all the puta-
tive lightning events were clear examples of light-
ning (i.e. judged with probability 1 to be of type
T
1
) and are furthermore associated by f with clear
events of thunder (i.e. judged with probability 1 to
be of type T
2
). Suppose there were four such pairs
of events. Then the probability of f being of type
(T
1
? T
2
) is (1? 1)
4
, that is, 1.
Suppose, alternatively, that for one of the four
events f associates the lightning event with a silent
event, that is, one whose probability of being of
T
2
is 0. Then the probability of f being of type
(T
1
? T
2
) is (1 ? 1)
3
? (1 ? 0) = 0. One clear
counterexample is sufficient to show that the func-
tion is definitely not of the type.
In cases where the probabilities of the an-
tecedent and the consequent type judgements are
higher than 0, the probability of the entire judge-
ment on the existence of a functional type f will
decline in proportion to the size of dom(f). As-
sume, for example that there are k elements a ?
dom(f), where for each such a p(a : T
1
) =
p(f(a) : T
2
) ? .5. Every a
i
that is added to
dom(f) will reduce the value of p(f : (T
1
?
T
2
)), even if it yields higher values for p(a : T
1
)
and p(f(a) : T
2
). This is due to the fact that we
are treating the probability of p(f : (T
1
? T
2
))
as the likelihood of there being a function that is
satisfied by all objects in its domain. The larger
the domain, the less probable that all elements in
it fulfill the functional relation.
We are, then, interpreting a functional type
judgement of this kind as a universally quantified
assertion over the pairing of objects in dom(f)
and range(f). The probability of such an asser-
tion is given by the conjunction of assertions cor-
responding to the co-occurrence of each element a
in f ?s domain as an instance of T
1
with f(a) as an
instance of T
2
. This probability is the product of
the probabilities of these individual assertions.
This seems reasonable, but it only deals with
functions whose domain is all objects which have
been judged to have some probability, however
low, of being of type T
1
. Intuitively, functions
which leave out some of the objects with lower
likelihood of being of type T
1
should also have a
probability of being of type (T
1
? T
2
). This fac-
tor in the probability is represented by the second
element of the product in the formula.
74
Negation ?T , of type T , is the function type
(T ? ?), where ? is a necessarily empty type
and p(?) = 0. It follows from our rules for func-
tion types that p(f : ?T ) = 1 if dom(f) = ?, that
is T is empty, and 0 otherwise.
We also assign probabilities to judgements con-
cerning the (non-)emptiness of a type, p(T ). we
pass over the details of how we compute the prob-
abilities of such judgements, but we note that our
account of negation entails that p(T ? ?T ) = 1,
and (ii) p(??T ) = p(T ). Therefore, we sustain
classical Boolean negation and disjunction, in con-
trast to Martin-L?of?s (1984) intuitionistic type the-
ory.
Dependent Types are functions from objects to
types. Given appropriate arguments as functions
they will return a type. Therefore, the account of
probabilities associated with functions above ap-
plies to dependent types.
Record Types A record in a type system asso-
ciated with a set of labels is a set of ordered pairs
(fields) whose first member is a label and whose
second member is an object of some type (possibly
a record). Records are required to be functional on
labels (each label in a record can only occur once
in the record?s left projection).
A dependent record type is a set of fields (or-
dered pairs) consisting of a label ` followed by T
as above. The set of record types is defined by:
1. [], that is the empty set or Rec, is a record type. r : Rec
just in case r is a record.
2. If T
1
is a record type, ` is a label not occurring in T
1
,
and T
2
is a type, then T
1
? {?`, T
2
?} is a record type.
r : T
1
? {?`, T
2
?} just in case r : T
1
, r.` is defined (`
occurs as a label in r) and r.` : T
2
.
3. If T is a record type, ` is a label not occuring in
T , T is a dependent type requiring n arguments, and
?pi
1
, . . . , pi
n
? is an n-place sequence of paths in T ,
4
then T ? {?`, ?T , ?pi
1
, . . . , pi
n
???} is a record type.
r : T ? {?`, ?T , ?pi
1
, . . . , pi
n
???} just in case r : T ,
r.` is defined and r.` : T (r.pi
1
, . . . , r.pi
n
).
The probability that an object r is of a record
type T is given by the following clauses:
1. p(r : Rec) = 1 if r is a record, 0 otherwise
2. p(r : T
1
? {?`, T
2
?}) =
?
p
(r : T
1
, r.` : T
2
)
3. If T : (T
1
? (. . . ? (T
n
? Type) . . .)), then
p(r : T ? {?`, ?T , ?pi
1
, . . . , pi
n
???}) =
?
p
(r : T, r.` :
T (r.pi
1
, . . . , r.pi
n
) | r.pi
1
: T
1
, . . . , r.pi
n
: T
n
)
4
In the full version of TTR we also allow absolute paths
which point to particular records, but we will not include
them here.
3 Compositional Semantics
Montague (1974) determines the denotation of a
complex expression by applying a function to an
intensional argument (as in [[ NP ]]([[
?
VP ]])). We
employ a variant of this general strategy by ap-
plying a probabilistic evaluation function [[ ? ]]
p
to
a categorical (non-probabilistic) semantic value.
For semantic categories that are interpreted as
functions, [[ ? ]]
p
yields functions from categorical
values to probabilities. For sentences it produces
probability values.
The probabilistic evaluation function [[ ? ]]
p
pro-
duces a probabilistic interpretation based on a
classical compositional semantics. For sentences
it will return the probability that the sentence is
true. For categories that are interpreted as func-
tions it will return functions from (categorical) in-
terpretations to probabilities. We are not propos-
ing strict compositionality in terms of probabili-
ties. Probabilities are like truth-values (or rather,
truth-values are the limit cases of probabilities).
We would not expect to be able to compute the
probability associated with a complex constituent
on the basis of the probabilities associated with its
immediate constituents, any more than we would
expect to be able to compute a categorical inter-
pretation entirely in terms of truth-functions and
extensions. However, the simultaneous computa-
tion of categorical and probabilistic interpretations
provides us with a compositional semantic system
that is closely related to the simultaneous com-
putation of intensions and extensions in classical
Montague semantics.
The following definition of [[ ? ]]
p
for a fragment
of English is specified on the basis of our proba-
bilistic type system and a non-probabilistic inter-
pretation function [[ ? ]], which we do not give in
this version of the paper. (It?s definition is given
by removing the probability p from the definition
below.)
[[ [
S
S
1
and S
2
] ]]
p
= p(
[
e
1
:[[ S
1
]]
e
2
:[[ S
2
]]
]
)
[[ [
S
S
1
or S
2
] ]]
p
= p(
[
e:[[ S
1
]]?[[ S
2
]]
]
)
[[ [
S
Neg S] ]]
p
= [[ Neg ]]
p
([[ S ]])
[[ [
S
NP VP] ]]
p
= [[ NP ]]
p
([[ VP ]])
[[ [
NP
Det N] ]]
p
= [[ Det ]]
p
([[ N ]])
[[ [
NP
N
prop
] ]]
p
= [[ N
prop
]]
p
[[ [
VP
V
t
NP] ]]
p
= [[ V
t
]]
p
([[ NP ]])
[[ [
VP
V
i
] ]]
p
= [[ V
i
]]
p
[[ [
Neg
?it?s not true that?] ]]
p
= ?T :RecType(p(
[
e:?T
]
))
[[ [
Det
?some?] ]]
p
= ?Q:Ppty(?P :Ppty(p(
[
e:some(Q, P )
]
)))
[[ [
Det
?every?] ]]
p
= ?Q:Ppty(?P :Ppty(p(
[
e:every(Q, P )
]
)))
[[ [
Det
?most?] ]]
p
= ?Q:Ppty(?P :Ppty(p(
[
e:most(Q, P )
]
)))
75
[[ [
N
?boy?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:boy(r.x)
]
))
[[ [
N
?girl?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:girl(r.x)
]
))
[[ [
Adj
?green?] ]]
p
=
?P :Ppty(?r:
[
x:Ind
]
(p((
[
e:green(r.x,P )
]
)))))
[[ [
Adj
?imaginary?] ]]
p
=
?P :Ppty(?r:
[
x:Ind
]
(p((
[
e:imaginary(r.x,P )
]
)))))
5
[[ [
N
prop
?Kim?] ]]
p
= ?P :Ppty(p(P (
[
x=kim
]
)))
[[ [
N
prop
?Sandy?] ]]
p
= ?P :Ppty(p(P (
[
x=sandy
]
)))
[[ [
V
t
?knows?] ]]
p
=
?P:Quant(?r
1
:
[
x:Ind
]
(p(P(?r
2
:(
[
e:know(r
1
.x,r
2
.x)
]
)))))
[[ [
V
t
?sees?] ]]
p
=
?P:Quant(?r
1
:
[
x:Ind
]
(p(P(?r
2
:(
[
e:see(r
1
.x,r
2
.x)
]
)))))
[[ [
V
i
?smiles?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:smile(r.x)
]
))
[[ [
V
i
?laughs?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:laugh(r.x)
]
))
A probability distribution d for this fragment,
based on a set of situations S, is such that:
p
d
(a : Ind) = 1 if a is kim or sandy
6
p
d
(s : T ) ? [0, 1] if s ? S and T is a ptype
p
d
(s : T ) = 0 if s 6? S and T is a ptype
7
p
d
(a : [
?
P ]) = p
d
(P (
[
x=a
]
))
p
d
(some(P,Q)) = p
d
([
?
P ] ? [
?
Q])
p
d
(every(P,Q)) = p
d
([
?
P ]? [
?
Q])
p
d
(most(P,Q)) = min(1,
p
d
([
?
P ]?[
?
Q]
?
most
p
d
([
?
P ])
)
The probability that an event e is of the type in
which the relation some holds of the properties P
andQ is the probability that e is of the conjunctive
type P ?Q. The probability that e is of the every
type for P and Q is the likelihood that it instanti-
ates the functional type P ? Q. As we have de-
fined the probabilities associated with functional
types in terms of universal quantification (an un-
bounded conjunction of the pairings between the
elements of the domain P of the function and its
range Q), this definition sustains the desired read-
ing of every. The likelihood that e is of the type
most for P and Q is the likelihood that e is of
type P ?Q, factored by the product of the contex-
tually determined parameter ?
most
and the likeli-
hood that e is of type P , where this fraction is less
than 1, and 1 otherwise.
Consider a simple example.
[[ [
S
[
NP
[
N
prop
Kim]] [
VP
[
V
i
smiles]]] ]]
p
=
?P :Ppty(p(P (
[
x=kim
]
)))(?r:
[
x:Ind
]
(
[
e:smile(r.x)
]
)) =
p(?r:
[
x:Ind
]
(
[
e:smile(r.x)
]
)(
[
x=kim
]
)) =
p(
[
e:smile(kim)
]
)
5
Notice that we characterize adjectival modifiers as rela-
tions between records of individuals and properties. We can
then invoke subtyping to capture the distinction between in-
tersective and non-intersective modifier relations.
6
This seems an intuitive assumption, though not a neces-
sary one.
7
Again this seems an intuitive, though not a necessary as-
sumption.
Suppose that p
d
(s
1
:smile(kim)) = .7,
p
d
(s
2
:smile(kim)) = .3, p
d
(s
3
:smile(kim)) =
.4, and there are no other situations s
i
such
that p
d
(s
i
:smile(kim)) > 0. Furthermore, let
us assume that these probabilities are indepen-
dent of each other, that is, p
d
(s
3
:smile(kim)) =
p
d
(s
3
:smile(kim) | s
1
:smile(kim), s
2
:smile(kim))
and so on. Then
p
d
(smile(kim))=
?
p
d
(s
1
: smile(kim), s
2
: smile(kim), s
3
: smile(kim)) =
?
p
d
(s
1
: smile(kim), s
2
: smile(kim)) + .4 ? .4
?
p
d
(s
1
:
smile(kim), s
2
: smile(kim)) =
(.7 + .3? .7? .3) + .4? .4(.7 + .3? .7? .3) =
.874
This means that p
d
(
[
e:smile(kim)
]
) = .874.
Hence [[ [
S
[
NP
[
N
prop
Kim]] [
VP
[
V
i
smiles]]] ]]
p
d
= .874
(where [[ ? ]]
p
d
is the result of computing [[ ? ]]
p
with respect to the probability distribution d).
Just as for categorical semantics, we can con-
struct type theoretic objects corresponding to
probabilistic judgements. We call these proba-
bilistic Austinian propositions. These are records
of type?
?
sit : Sit
sit-type : Type
prob : [0,1]
?
?
where [0,1] is used to represent the type of real
numbers between 0 and 1. They assert that the
probability that a situation s is of type Type is the
value of prob.
The definition of [[ ? ]]
p
specifies a compositional
procedure for generating an Austinian proposition
(record) of this type from the meanings of the syn-
tactic constituents of a sentence.
4 An Outline of Semantic Learning
We outline a schematic theory of semantic learn-
ing on which agents acquire classifiers that form
the basis for our probabilistic type system. For
simplicity and ease of presentation we take these
to be Naive Bayes classifiers, which an agent ac-
quires from observation. In future developments
of this theory we will seek to extend the approach
to Bayesian networks (Pearl, 1990).
We assume that agents keep records of observed
situations and their types, modelled as probabilis-
tic Austinian propositions. For example, an obser-
vation of a man running might yield the following
Austinian proposition for some a:Ind, s
1
:man(a),
s
2
:run(a):
76
??
?
?
?
?
?
?
?
sit =
?
?
ref = a
c
man
= s
1
c
run
= s
2
?
?
sit-type =
?
?
ref : Ind
c
man
: man(ref)
c
run
: run(ref)
?
?
prob = 0.7
?
?
?
?
?
?
?
?
?
An agent, A, makes judgements based on a
finite string of probabilistic Austinian proposi-
tions, J, corresponding to prior judgements held
in memory. For a type, T , J
T
represents that set of
Austinian propositions j such that j.sit-type v T .
If T is a type and J a finite string of probabilis-
tic Austinian propositions, then || T ||
J
represents
the sum of all probabilities associated with T in J
(
?
j?J
T
j.prob). P(J) is the sum of all probabilities
in J (
?
j?J
j.prob).
We use prior
J
(T ) to represent the prior proba-
bility that anything is of type T given J, that is
||T ||
J
P(J)
if P(J) > 0, and 0 otherwise.
p
A,J
(s : T ) denotes the probability that agent A
assigns with respect to prior judgements J to s be-
ing of type T . Similarly, p
A,J
(s : T
1
| s : T
2
) is
the probability that agent A assigns with respect
to prior judgements J to s being of type T
1
, given
that A judges s to be of type T
2
.
When an agent A encounters a new situation
s and considers whether it is of type T , he/she
uses probabilistic reasoning to determine the value
of p
A,J
(s : T ). A uses conditional probabilities
to calculate this value, where A computes these
conditional probabilities with the equation p
A,J
(s :
T
1
| s : T
2
) =
||T
1
?T
2
||
J
||T
2
||
J
, if || T
2
||
J
6= 0. Otherwise,
p
A,J
(s : T
1
| s : T
2
) = 0.
This is our type theoretic variant of the stan-
dard Bayesian formula for conditional probabili-
ties: p(A | B) =
|A&B|
|B|
. But instead of counting
categorical instances, we sum the probabilities of
judgements. This is because our ?training data? is
not limited to categorical observations. Instead it
consists of probabilistic observational judgements
that situations are of particular types.
8
Assume that we have the following types:
T
man
=
[
ref : Ind
c
man
: man(ref)
]
and
T
run
=
[
ref : Ind
c
run
: run(ref)
]
8
As a reviewer observes, by using an observer?s previous
judgements for the probability of an event being of a partic-
ular type, as the prior for the rule that computes the proba-
bility of a new event being of that type, we have, in effect,
compressed information that properly belongs in a Bayesian
network into our specification of a naive Bayesian classifier.
This is a simplification that we adopt here for ease of expo-
sition. In future work, we will characterise classifier learning
through full Bayesian networks.
Assume also that J
T
man
?T
run
has three members,
corresponding to judgements by A that a man was
running in three observed situations s
1
, s
3
, and
s
4
, and that these Austinian propositions have the
probabilities 0.6, 0.6. and 0.5 respectively.
Take J
T
man
to have five members correspond-
ing to judgements by A that there was a man in
s
1
, . . . , s
5
, and that the Austinian propositions as-
signing T
man
to s
1
, . . . , s
5
all have probability 0.7.
Given these assumptions, the conditional probabil-
ity that A will assign on the basis of J to someone
runs, given that he is a man is p
A,J
(r : T
run
| r :
T
man
) =
||T
man
?T
run
||
J
||T
man
||
J
=
0.6+0.6+0.5
0.7+0.7+0.7+0.7+0.7
= .486
We use conditional probabilities to construct a
Naive Bayes classifier. A classifies a new situa-
tion s based on the prior judgements J, and what-
ever evidence A can acquire about s. This evi-
dence has the form p
A,J
(s : T
e
1
), . . ., p
A,J
(s : T
e
n
),
where T
e
1
, . . . , T
e
n
are the evidence types. The
Naive Bayes classifier assumes that the evidence is
independent, in that the probability of each piece
of evidence is independent of every other piece of
evidence.
We first formulate Bayes? rule of conditional
probability. This rule defines the conditional prob-
ability of a conclusion r : T
c
, given evidence r :
T
e
1
, r : T
e
2
, . . . , r : T
e
n
, in terms of conditional prob-
abilities of the form p(s
i
: T
e
i
| s
i
: T
c
), 1 ? i ? n,
and priors for conclusion and evidence:
p
A,J
(r : T
c
| r : T
e
1
, . . . , r : T
e
n
) =
prior
J
(T
c
)
||T
e
1
?T
c
||
J
||T
c
||
J
...
||T
e
n
?T
c
||
J
||T
c
||
J
prior
J
(T
e
1
)...prior
J
(T
e
n
)
The conditional probabilities are computed
from observations as indicated above. The rule of
conditional probability allows the combination of
several pieces of evidence, without requiring pre-
vious observation of a situation involving all the
evidence types.
We formulate a Naive Bayes classifier as a func-
tion from evidence types T
e
1
, T
e
2
, . . . , T
e
n
(i.e. from
a record of type T
e
1
? T
e
2
? . . . ? T
e
n
) to conclusion
types T
c
1
, T
c
2
, . . . , T
c
m
. The conclusion is a disjunc-
tion of one or more T ? {T
c
1
, T
c
2
, . . . , T
c
m
}, where
m ranges over all possible non-disjunctive conclu-
sions distinguished by the classifier. This function
is specified as follows.
? : (T
e
1
? . . .?T
e
n
)? (T
c
1
? . . .?T
c
m
) such that ?(r) =
(
?
argmax
T??T
c
1
,...,T
c
m
?
p
A,J
(r : T | r : T
e
1
, . . . , r : T
e
n
)
The classifier returns the type T which max-
imises the conditional probability of r : T given
77
the evidence provided by r. The argmax operator
here takes a sequence of arguments and a func-
tion and yields a sequence containing the argu-
ments which maximise the function (if there are
more than one).
The classifier will output a disjunction in case
both possibilities have the same probability. The
?
operator takes a sequence and returns the dis-
junction of all elements of the sequence.
In addition to computing the conclusion which
receives the highest probability given the evi-
dence, we also want the posterior probability of
the judgement above, i.e. the probability of the
judgement in light of the evidence. We obtain the
non-normalised probabilities (p
nn
A,J
) of the different
possible conclusions by factoring in the probabili-
ties of the evidence:
p
nn
A,J
(r : ?(r)) =
?
T?
?
?1
?(r)
p
A,J
(r : T | r : T
e
1
, . . . , r : T
e
n
)p
A,J
(r :
T
e
1
) . . . p
A,J
(r : T
e
n
)
where
?
?1
is the inverse of
?
, i.e. a function that
takes a disjunction and returns the set of disjuncts.
We then take the probability of r : ?(r) and
normalise over the sum of the probabilities of
all the possible conclusions. This gives us the
normalised probability of the judgement resulting
from classification p(r : ?(r)) =
p
nn
A,J
(r:?(r))
?
1?i?m
p
nn
A,J
(r:T
c
i
)
.
However, since the probabilities of the evidence
are identical for all possible conclusions, we can
ignore them and instead compute the normalised
probability with the following equation (where m
ranges over all possible non-disjunctive conclu-
sions distinguished by the classifier, as above).
p
A,J
(r : ?(r)) =
?
T?
?
?1
?(r)
p
A,J
(r:T |r:T
e
1
,...,r:T
e
n
)
?
1?i?m
p
A,J
(r:T
c
i
|r:T
e
1
,...,r:T
e
n
)
The result of classification can be represented as
an Austinian proposition
?
?
sit = s
sit-type = ?(s)
prob = p
A,J
(s : ?(s))
?
?
which A adds to J as a result of observing and
classifying s, and is thus made available for sub-
sequent probabilistic reasoning.
5 Conclusions and Future Work
We have presented a probabilistic version of a rich
type theory with records, relying heavily on classi-
cal equations for types formed with meet, join, and
negation. This has permitted us to sustain classi-
cal equivalences and Boolean negation for com-
plex types within an intensional type theory. We
have replaced the truth of a type judgement with
the probability of it being the case, and we have
applied this approach to judgements that a situa-
tion if of type T .
Our probabilistic formulation of a rich type the-
ory with records provides the basis for a compo-
sitional semantics in which functions apply to cat-
egorical semantic objects in order to return either
functions from categorical interpretations to prob-
abilistic judgements, or, for sentences, to proba-
bilistic Austinian propositions. One of the inter-
esting ways in which this framework differs from
classical model theoretic semantics is that the ba-
sic types and type judgements at the foundation of
the type system correspond to perceptual judge-
ments concerning objects and events in the world,
rather than to entities in a model and set theoretic
constructions defined on them.
We have offered a schematic view of semantic
learning. On this account observations of situa-
tions in the world support the acquisition of naive
Bayesian classifiers from which the basic proba-
bilistic types of our type theoretical semantics are
extracted. Our type theory is, then, the interface
between observation-based learning of classifiers
for objects and the situations in which they figure
on one hand, and the computation of complex se-
mantic values for the expressions of a natural lan-
guage from these simple probabilistic types and
type judgements on the other. Therefore our gen-
eral model of interpretation achieves a highly in-
tegrated bottom-up treatment of linguistic mean-
ing and perceptually-based cognition that situates
meaning in learning how to make observational
judgements concerning the likelihood of situations
obtaining in the world.
The types of our semantic theory are inten-
sional. They constitute ways of classifying situ-
ations, and they cannot be reduced to set of situa-
tions. The theory achieves fine-grained intension-
ality through a rich and articulated type system,
where the foundation of this system is anchored in
perceptual observation.
The meanings of expressions are acquired on
the basis of speakers? experience in the applica-
tion of classifiers to objects and events that they
encounter. Meanings are dynamic and updated in
light of subsequent experience.
78
Probability is distributed over alternative situ-
ation types. Possible worlds, construed as maxi-
mal consistent sets of propositions (ultrafilters in a
proof theoretic lattice of propositions) play no role
in this framework.
Bayesian reasoning from observation provides
the incremental basis for learning and refining
predicative types. These types feed the combina-
torial semantic procedures for interpreting the sen-
tences of a natural language.
In future work we will explore implementations
of our learning theory in order to study the viabil-
ity of our probabilistic type theory as an interface
between perceptual judgement and compositional
semantics. We hope to show that, in addition to
its cognitive and theoretical interest, our proposed
framework will yield results in robotic language
learning, and dialogue modelling.
Acknowledgments
We are grateful to two anonymous reviewers for
very helpful comments on an earlier draft of this
paper. We also thank Alex Clark, Jekaterina
Denissova, Raquel Fern?andez, Jonathan Ginzburg,
Noah Goodman, Dan Lassiter, Michiel van Lam-
balgen, Poppy Mankowitz, Aarne Ranta, and Pe-
ter Sutton for useful discussion of ideas presented
in this paper. Shalom Lappin?s participation in
the research reported here was funded by grant
ES/J022969/1 from the Economic and Social Re-
search Council of the UK, and a grant from the
Wenner-Gren Foundations. We also gratefully ac-
knowledge the support of Vetenskapsr?adet, project
2009-1569, Semantic analysis of interaction and
coordination in dialogue (SAICD); the Depart-
ment of Philosophy, Linguistics, and Theory of
Science; and the Centre for Language Technology
at the University of Gothenburg.
References
Jon Barwise and John Perry. 1983. Situations and
Attitudes. Bradford Books. MIT Press, Cambridge,
Mass.
I. Beltagy, C. Chau, G. Boleda, D. Garrette, K. Erk,
and R. Mooney. 2013. Montague meets markov:
Deep semantics with probabilistic logical form. In
Second Joint Conference on Lexical and Computa-
tional Semantics, Vol. 1, pages 11?21. Association
of Computational Linguistics, Atlanta, GA.
R. Carnap. 1947. Meaning and Necessity. University
of Chicago Press, Chicago.
A. Clark and S. Lappin. 2011. Linguistic Nativism
and the Poverty of the Stimulus. Wiley-Blackwell,
Chichester, West Sussex, and Malden, MA.
Robin Cooper. 2012. Type theory and semantics in
flux. In Ruth Kempson, Nicholas Asher, and Tim
Fernando, editors, Handbook of the Philosophy of
Science, volume 14: Philosophy of Linguistics. El-
sevier BV, 271?323. General editors: Dov M. Gab-
bay, Paul Thagard and John Woods.
C. Fox and S. Lappin. 2010. Expressiveness and
complexity in underspecified semantics. Linguistic
Analysis, Festschrift for Joachim Lambek, 36:385?
417.
J. Halpern. 2003. Reasoning About Uncertainty. MIT
Press, Cambridge MA.
H. Kamp and U. Reyle. 1993. From Discourse to
Logic: Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Kluwer, Dordrecht.
A.N. Kolmogorov. 1950. Foundations of Probability.
Chelsea Publishing, New York.
Per Martin-L?of. 1984. Intuitionistic Type Theory. Bib-
liopolis, Naples.
Richard Montague. 1974. Formal Philosophy: Se-
lected Papers of Richard Montague. Yale University
Press, New Haven. ed. and with an introduction by
Richmond H. Thomason.
C. Papadimitriou. 1995. Computational Complexity.
Addison-Wesley Publishing Co., Readin, MA.
J. Paris. 2010. Pure inductive logic. Winter School in
Logic, Guangzhou, China.
J. Pearl. 1990. Bayesian decision methods. In
G. Shafer and J. Pearl, editors, Readings in Uncer-
tain Reasoning, pages 345?352. Morgan Kaufmann.
79
Proceedings of the 25th International Conference on Computational Linguistics, pages 33?37,
Dublin, Ireland, August 23-29 2014.
Exploration of functional semantics of prepositions from corpora of
descriptions of visual scenes
Simon Dobnik
1
and John D. Kelleher
2?
1
University of Gothenburg, Centre for Language Technology,
Dept. of Philosophy, Linguistics & Theory of Science, Gothenburg, Sweden
2
Dublin Institute of Technology, Applied Intelligence Research Centre,
School of Computing, Dublin, Ireland
simon.dobnik@gu.se, john.d.kelleher@dit.ie
Abstract
We present a method of extracting functional semantic knowledge from corpora of descriptions
of visual scenes. Such knowledge is required for interpretation and generation of spatial descrip-
tions in tasks such as visual search. We identify semantic classes of target and landmark objects
related by each preposition by abstracting over WordNet taxonomy. The inclusion of such knowl-
edge in visual search should equip robots with a better, more human-like spatial cognition.
1 Introduction
Visual search is an area of growing research importance in mobile robotics; see (Sj?o?o, 2011; Kunze et
al., 2014) among others. Visual search involves directing the visual sensors of a robot with the goal
of locating a specific object. Several recent approaches have integrated non-visual (often linguistically
motivated information) into the visual search process. The intuition behind this is that if the robot knows
that object X is often located near/on/. . . object Y then in situations where Y is visually salient it may be
easier for the system to search by first locating Y and then use relational information to direct the search
for X. A key component of these approaches to visual search is the definition of spatial semantics of the
relational information. Appropriately modelling these semantics is crucial because fundamentally it is
these models that define the scope of the visual search in relation to Y.
In language spatial relations between objects are often expressed using locative expressions such as
?the apple above a bowl?, ?the computer is on the shelf? and ?the plane is over the house?. In these
expressions a target object is located relative to a landmark object using a preposition to describe the
spatial relation. Crucially, there are differences between prepositions with respect to how their spatial
relations are defined. The semantics of some prepositions can be modelled in terms of geometric prim-
itives whereas the semantics of other prepositions are sensitive to the functional relations between the
target and the landmark (Coventry and Garrod, 2004). Consider the example ?Alex is at her desk?. This
description refers to a situation where Alex is not only geometrically proximate to her desk but also
where she is sitting down and working. The extra constraints are coming from the functional relations
that normally exist between an individual and a desk.
Returning to visual search, being able to identify whether a given preposition is primarily geometric
of functional is important because this classification informs the design of the spatial semantics for the
preposition and hence the appropriate definition of the search relative to the landmark object. In this paper
we present some ongoing experiments which attempt to develop techniques that can classify prepositions
as geometric or functional.
2 Spatial descriptions
(Coventry and Garrod, 2004) show in the experiments with human observers of images of a man in
the rain holding an umbrella where the umbrella is providing a varying degree of protection from the
rain that ?above? is more sensitive to the geometrical component than ?over? and that ?over? is more
?
Both authors are equal contributors.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
33
sensitive to the object function component than ?above?. Descriptions of ?the umbrella is over a man?
were considered acceptable even in cases where the umbrella was held horizontally but was providing
protection from the rain.
Modelling of functional knowledge in the computational models of meaning of spatial prepositions is
not straightforward. Humans (and even expert linguists) do not seem to have clear intuitions what the
functional component of each preposition sense may be and hence they must be confirmed experimentally
(Garrod et al., 1999). This may take significant time for various combinations of prepositions and target
and landmark objects. One needs to develop a complex ontology of object properties and then associate
spatial prepositions with rules that pick out certain properties of objects for a particular preposition sense
(for examples of such rules see (Garrod et al., 1999, p.170)).
In this paper we describe a method of extraction of these meaning components from a corpus of de-
scriptions of visual scenes automatically. Unlike in the psycho-linguistic experiments described above
we examine general corpora that obtain a wide and unrestricted set of images that humans described
freely. The purpose of the experiment is to investigate whether functional knowledge can be extracted
from contextual language use. For example, can we make generalisations about the semantics of the
arguments that a particular prepositional sense takes automatically. Furthermore, we are also interested
if the distinctions between geometric and functional sensitivity of prepositions reported experimentally
could be determined this way. This information would allow us to weight the contributions of the ge-
ometric and functional knowledge when generating and interpreting spatial descriptions of scenes. We
hypothesise, that if a preposition is sensitive to functional meaning then there will be functional relations
between target and landmark objects that it is used with and consequently the preposition will will be
much more restrictive or specific in the semantic type of targets and landmarks that it requires. Other
prepositions may be more sensitive to the satisfaction of the geometric constraint and hence we expect
that they will co-occur with objects of more general semantic types.
3 Datasets and extraction of spatial descriptions
The goal of this work is to analyse the semantics of linguistic expressions that are used to describe the
relative location of objects in visual contexts. We base our analysis on two corpora of image descrip-
tions: specifically, the IAPR TC-12 Benchmark corpus (Grubinger et al., 2006)
1
which contains 20,000
images and multi-sentence descriptions and the 8K ImageFlickr dataset (Rashtchian et al., 2010)
2
which
contains 8108 images. In both corpora the situations and events represented by images are described by
several sentences which contain spatial descriptions with prepositions: in the first case all sentences are
by a single annotator and in the second case each sentence is by a different annotator. The descriptions
are geometrically constrained by the visual context. On the other hand, the describers? choice of the tar-
get and the landmarks objects and the preposition in these descriptions will tell us about their functional
semantics. The main pre-processing step was to extract parts of spatial expressions used in the image
descriptions. Once extracted each spatial expression was stored in a type with the following structure:
?preposition, target, landmark?. To do this extraction we first parsed both corpora of linguistic descrip-
tions for dependencies (Marneffe et al., 2006) using Stanford CoreNLP tools
3
. Then we wrote several
extraction rules that matched dependency parses and extracted all three parts of spatial expressions that
we are looking for. All words were lemmatized and converted to lower case, compound prepositions
such as ?on the left side of? were rewritten as single words and names, etc. were converted to their
named entity categories such as ?person?. The extracted patterns from both corpora were combined into
a single dataset from which we can determine their frequency counts.
4 Determining conceptual categories of objects
The intuition behind our experiment is that functionally defined prepositions can be distinguished from
geometrically defined prepositions by virtue of the fact that the functional relations between the target
1
http://imageclef.org/photodata
2
http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html
3
http://nlp.stanford.edu/software/corenlp.shtml
34
and landmark objects that are encoded in the semantics of functional prepositions result in less variance
across the object types that occur with geometric prepositions. In other words, the target objects that
occur with a functional preposition will be more semantically similar to each other than the target ob-
jects that occur with a geometric preposition. Likewise the landmark objects that occur with a functional
preposition will be more semantically similar than the landmark objects that occur with a geometric
preposition. In order to test this intuition we need to be able to cluster the target and landmark objects
that occur with a given preposition into conceptual categories. We can then define patterns that describe
the pairs of conceptual categories that a preposition occurs with. Based on the intuition that functional
prepositions occur with more specific object classes we would expect that functional prepositions gener-
ate more patterns of use and that the patterns include more specific classes of objects.
To determine conceptual categories that objects of prepositions belong to, WordNet (Fellbaum, 1998)
appears to be an ideal tool. It contains taxonomies of words that were constructed by humans using
their intuitions. While certain classification of words in the ontology are not entirely unproblematic it is
nonetheless considered a gold-standard resource for lexical semantics. In particular, we are interested in
finding out given a certain preposition what are the possible semantic classes of its target and landmark
objects. To determine the class synset (a sense in WordNet terminology) that covers a bag of words
best we use the class-labelling algorithm of Widdows (2003). Given a list of words, this algorithm finds
hypernyms which subsume as many as possible words in the list, as closely as possible. The algorithm
works by first defining the set of all possible hypernyms for the words in the list. In then computes a
score for each of the hypernyms: a hypernym score is incremented by a small positive value for each
word it subsumes (this positive value is defined as 1 divided by the square of the vertical distance in the
hierarchy between the hypernym and the word) and decremented by a small negative value g for each
word it does not subsume. The algorithm returns the hypernym with the highest score. Widdows (2003)
sets g to 0.25 for lists that contain 5 words on average. The lists in our experiments are much longer
so we scale g to the length of the list input: g = 0.25 ?
5
list length
. Given a bag of nouns we use the
class-labelling algorithm to determine the bets matching hypernym. Words that are subsumed by this
hypernym are labelled as belonging to this class and are removed from the bag of words. The algorithm
is repeated on the remaining words recursively until all words from the bag of words are exhausted. The
procedure allows us to greedily create classes of words that are most general categories representing
these words, the level of generality can be tweaked by the parameter g. The bag of words is allowed
to contain duplicates as these are indicators of coherent classes. Duplicate words are all covered by
a common hypernym and hence this hypernym will be given more weight in the overall scoring. The
hypernyms introduced by infrequent and non-similar words are given less weight. This filters words that
may be included due to an error.
5 Patterns of prepositional use
The algorithm for class-labelling of words backed by the WordNet ontology allows us to predict the
typical classes or semantic types of the landmark and the target objects related by a preposition. From
these several patterns can be generated. Such patterns can be used both in the interpretation of visual
scenes (visual search) or generation of spatial referring expressions that optimally constrain the set of
intended objects. We create the patterns by collecting all targets and all landmarks that occur with a
particular preposition. We apply the class labelling on the bags of words representing the targets and
the landmarks to obtain a set of classes representing targets and landmarks. Finally, for every tuple
?target, preposition, landmark? we replace target and landmark with target class and landmark class and
collect a set of ?target class, preposition, landmark class? patterns. This method assumes that targets
and landmarks are semantically independent of each other. Below are some examples of patterns that
our algorithm has found (the notation for the names of the objects corresponds to the names of synsets
in the WordNet taxonomy, the numbers in brackets indicate the number of examples out of total ex-
amples covered by this pattern): (i) travel.v.01 over object.n.01 (9/713), sunlight.n.01 over object.n.01
(13/713), bridge.n.01 over object.n.01 (23/713), bridge.n.01 over body of water.n.01 (42/713), air.v.03
over object.n.01 (36/713), artifact.n.01 over body of water.n.01 (42/713), artifact.n.01 over object.n.01
35
(175/713),. . . (ii) breeze.n.01 above body of water.n.01 (8/183), person.n.01 above artifact.n.01 (9/183),
artifact.n.01 above steer.v.01 (14/183), artifact.n.01 above entrance.n.01 (16/183) artifact.n.01 above ar-
tifact.n.01 (27/183),. . . (iii) person.n.01 under tree.n.01 (7/213), shirt.n.01 under sweater.n.01 (8/213),
person.n.01 under body of water.n.01 (11/213), person.n.01 under artifact.n.01 (13/213) artifact.n.01
under artifact.n.01 (16/213), person.n.01 under structure.n.01 (17/213), artifact.n.01 under structure.n.01
(21/213),. . . (iv) box.n.05 below window.n.08 (1/14), crown.n.04 below script.n.01 (2/14),. . .
4
The
patterns show that different prepositions which are seemingly synonyms when considering geometry
(?over?/?above? and ?below?/?under?) do relate different types of objects and from the labels of the
semantic classes we may speculate what kind of semantic knowledge the relations are sensitive to. Im-
portantly, the labels of the classes and different patterns extracted show that there may be several distinct
and even unrelated situations that a preposition is referring to. Consider for example, person.n.01 under
tree.n.01, shirt.n.01 under sweater.n.01 (8/213), person.n.01 under body of water.n.01 are denoting three
different kinds of situations which require distinct and unrelated geometric arrangements. Overall, the
results indicate that the method is able to extract functional knowledge which is a reflection of the way
humans conceptualise objects and relations between them and which may be useful for improving visual
processing of scenes.
Another question we set off to answer is whether from the patterns one can determine the functional
and geometric bias of a preposition. As noted previously, our application of the class labelling algorithm
is greedy and attempts to cover a large number of words. The more words are generalised over the
more generic classes are created. To counter this confounding factor we down-sampled the dataset by
creating, for the results we report here, 50 samples of 20 randomly chosen words. The same procedure
for creating patterns of prepositional use was applied as before. On each sampling iteration we estimate
for each preposition (i) the average depth of the target and landmark hypernyms in the WordNet
taxonomy, (ii) the number of patterns created, and (iii) the entropy of the patterns over the examples
in the dataset. Finally, we average all values obtained from iterations and we rank the prepositions by the
ascending values of the these measures as shown below:
(i) on (3.17), near (3.55), with (3.66), next to (3.83), of (3.95), between (4.17), in front of (4.26), above
(4.27), over (4.48), around (4.52), behind (4.65), from (4.74), at (4.89), under (4.93), for (4.97),
through (5.27), in (5.45)
(ii) on (10.5), with (11.5), near (12), next to (12.1), between (12.6), of (12.6), above (12.7), around (13),
in front of (13.1), over (13.7), from (13.8), behind (13.9), for (14.2), under (14.3), in (14.5), through
(15.1), at (15.2)
(iii) on (2.74), next to (3.05), with (3.07), near (3.1), between (3.2), of (3.29), above (3.33), around
(3.36), in front of (3.39), over (3.48), from (3.5), behind (3.51), for (3.59), under (3.62), in (3.62),
through (3.75), at (3.76)
With small variations all measures (i), (ii) and (iii) rank the prepositions very similarly. Items at the
beginning of the list are used with target and landmark objects that belong to more general classes (i),
they are covered by a lower number of preposition patterns (ii), and the entropy of these patterns is low
(iii)
5
. Hence, we expect that items at the top of the lists are less affected by the type of the target and
landmark objects than items at the bottom of the lists. They are the prepositions where the geometric
component is stronger to determine the spatial relation. On the other hand, prepositions at the bottom
of the list rely more on the functional component of meaning. The results predict the observations from
the literature. Although, being sometimes quite close ?above? precedes ?over? in respect to to all three
measures. ?Below? was not processed in this study as there were too few examples but ?under? is found
at the tail of the list. The ranking of other prepositions also aligns with our intuitions. For example,
?on?, appearing as the head of the list, requires a contact between objects (a geometric notion), whereas
?in?, appearing in the tail of the list requires that the target is constrained by the landmark (a functional
notion).
4
There are only 14 examples of ?below? in the dataset for which nearly always unique patterns were created.
5
Entropy balances between the number of classes and the frequency of items in them. Low entropy indicates that there is a
tendency of items clustering in small number of specific classes rather rather than being equally dispersed over classes.
36
6 Conclusions and further work
In the preceding discussion we have demonstrated a method of extracting (i) functional semantic knowl-
edge ? required for the generation and interpretation of spatial descriptions ? from the corpora of de-
scriptions referring to visual scenes and (ii) made predictions about the bias of spatial descriptions to
functional knowledge. We hope that this information can facilitate visual search as it further restricts the
set of possible situations and objects involved. We have constructed several patterns of preposition use
and have shown that a preposition such as ?under? may refer to several distinct situations constrained
by the knowledge of object function and that these situations would require geometric representations
that are likely to be quite different. This knowledge should allow us to create different routines for
visual search that could be applied over a wide set of domains and would better approximate the way
humans perceive and reason about space. The applicability of this information for visual search must be
properly evaluated in a visual application. Estimating the functional or geometric bias of prepositions
informs us for their modelling but more importantly confirms that the patterns extracted here follow the
experimental observations reported in the literature.
In the procedure we have made several design choices: the choice of the corpora and the way the
corpora is processed and information extracted, the algorithm with which words are labelled for semantic
classes and finally the method with which patterns are created. For example, before class labelling we
could use an algorithm that clusters words of similar hypernym depth into discrete classes over which
hypernyms are generalised. This would allow us to distinguish better between different situations that
a preposition is referring to. When creating patterns we assume that target and landmark objects are
independent of each other. However, this may not necessarily be the case. For example, the category of
the target object may constrain the category of the landmark which means that the latter category should
only be generalised over landmark words that occur with some target category.
References
Kenny R Coventry and Simon C Garrod. 2004. Saying, seeing, and acting: the psychological semantics of spatial
prepositions. Psychology Press, Hove, East Sussex.
Christiane Fellbaum. 1998. WordNet: an electronic lexical database. MIT Press, Cambridge, Mass.
Simon Garrod, Gillian Ferrier, and Siobhan Campbell. 1999. In and on: investigating the functional geometry of
spatial prepositions. Cognition, 72(2):167?189.
Michael Grubinger, Paul D. Clough, Henning M?uller, and Thomas Deselaers. 2006. The IAPR benchmark: A new
evaluation resource for visual information systems. In Proceedings of OntoImage 2006: Workshop on language
resources for content-based mage retrieval during LREC 2006, Genoa, Italy, 22 May. European Language
Resources Association.
Lars Kunze, Chris Burbridge, and Nick Hawes. 2014. Bootstrapping probabilistic models of qualitative spatial
relations for active visual object search. In AAAI Spring Symposium 2014 on Qualitative Representations for
Robots, Stanford University in Palo Alto, California, US, March, 24?26.
Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Proceedings of Int?l Conf. on Language Resources and Evaluation
(LREC), pages 449?454, Genoa, Italy. European Language Resources Association.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annotations
using Amazon?s Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on creating speech
and language data with Amazon?s Mechanical Turk, Los Angeles, CA, 6 June. North American Chapter of the
Association for Computational Linguistics (NAACL).
Kristoffer Sj?o?o. 2011. Functional understanding of space: Representing spatial knowledge using concepts
grounded in an agent?s purpose. Ph.D. thesis, KTH, Computer Vision and Active Perception (CVAP), Cen-
tre for Autonomous Systems (CAS), Stockholm, Sweden.
Dominic Widdows. 2003. Unsupervised methods for developing taxonomies by combining syntactic and statis-
tical information. In Proceedings of the 2003 Conference of the North American Chapter of the Association
for Computational Linguistics on Human Language Technology, volume 1, pages 197?204. Association for
Computational Linguistics.
37
