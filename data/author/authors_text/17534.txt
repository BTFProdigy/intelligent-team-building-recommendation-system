Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1366?1371,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Connecting Language and Knowledge Bases with Embedding Models
for Relation Extraction
Jason Weston
Google
111 8th avenue
New York, NY, USA
jweston@google.com
Antoine Bordes
Heudiasyc
UT de Compi?gne
& CNRS
Compi?gne, France
bordesan@utc.fr
Oksana Yakhnenko
Google
111 8th avenue
New York, NY, USA
oksana@google.com
Nicolas Usunier
Heudiasyc
UT de Compi?gne
& CNRS
Compi?gne, France
nusunier@utc.fr
Abstract
This paper proposes a novel approach for rela-
tion extraction from free text which is trained
to jointly use information from the text and
from existing knowledge. Our model is based
on scoring functions that operate by learning
low-dimensional embeddings of words, enti-
ties and relationships from a knowledge base.
We empirically show on New York Times ar-
ticles aligned with Freebase relations that our
approach is able to efficiently use the extra in-
formation provided by a large subset of Free-
base data (4M entities, 23k relationships) to
improve over methods that rely on text fea-
tures alone.
1 Introduction
Information extraction (IE) aims at generating struc-
tured data from free text in order to populate Knowl-
edge Bases (KBs). Hence, one is given an incom-
plete KB composed of a set of triples of the form
(h, r , t); h is the left-hand side entity (or head), t
the right-hand side entity (or tail) and r the relation-
ship linking them. An example from the Freebase
KB1 is (/m/2d3rf ,<director-of>, /m/3/324), where
/m/2d3rf refers to the director ?Alfred Hitchcock"
and /m/3/324 to the movie ?The Birds".
This paper focuses on the problem of learning to
perform relation extraction (RE) under weak super-
vision from a KB. RE is sub-task of IE that consid-
ers that entities have already been detected by a dif-
ferent process, such as a named-entity recognizer.
RE then aims at assigning to a relation mention m
1www.freebase.com
(i.e. a sequence of text which states that some rela-
tion is true) the corresponding relationship from the
KB, given a pair of extracted entities (h, t) as con-
text. For example, given the triple (/m/2d3rf ,?wrote
and directed", /m/3/324), a system should predict
<director-of>. The task is said to be weakly super-
vised because for each pair of entities (h, t) detected
in the text, all relation mentions m associated with
them are labeled with all the relationships connect-
ing h and t in the KB, whether they are actually ex-
pressed by m or not.
Our key contribution is a novel model that em-
ploys not only weakly labeled text mention data, as
most approaches do, but also leverages triples from
the known KB. The model thus learns the plausi-
bility of new (h, r , t) triples by generalizing from
the KB, even though this triple is not present. A
ranking-based embedding framework is used to train
our model. Thereby, relation mentions, entities and
relationships are all embedded into a common low-
dimensional vector space, where scores are com-
puted. We show that our method can successfully
take into account information from a large-scale KB
(Freebase: 4M entities, 23k relationships) to im-
prove over systems that are only using text features.
This paper is organized as follows: Section 2
presents related work, Section 3 introduces our
model and its main influences, and experimental re-
sults are displayed in Section 4.
2 Previous Work
Learning under weak supervision is common in nat-
ural language processing, especially for tasks where
the annotation costs are significant such as in se-
1366
mantic parsing (Kate and Mooney, 2007; Liang et
al., 2009; Bordes et al, 2010; Matuszek et al,
2012). This is also naturally used in IE, since it
allows to train large-scale systems without requir-
ing to label numerous texts. The idea was intro-
duced by (Craven et al, 1999), which matched the
Yeast Protein Database with PubMed abstracts. It
was also used to train open extractors based on
Wikipedia infoboxes and corresponding sentences
(Wu and Weld, 2007; Wu and Weld, 2010). Large-
scale open IE projects (e.g. (Banko et al, 2007))
also rely on weak supervision, since they learn mod-
els from a seed KB in order to extend it.
Weak supervision is also a popular option for RE:
Mintz et al (2009) used Freebase to train weakly su-
pervised relational extractors on Wikipedia, an ap-
proach generalized by the multi-instance learning
frameworks (Riedel et al, 2010; Hoffmann et al,
2011; Surdeanu et al, 2012). All these works only
use textual information to perform extraction.
Lao et al (2012) proposed the first work aiming to
perform RE employing both KB data and text, using
a rule-based random walk method. Recently, Riedel
et al (2013) proposed another joint approach based
on collaborative filtering for learning entity embed-
dings. This approach connects text with Freebase
by learning shared embeddings of entities through
weak supervision, in contrast to our method where
no joint learning is performed. We do not compare
to these two approaches since they use two different
evaluation protocols that greatly differ from those
used in all aforementioned previous works. Never-
theless, our method is easier to integrate into exist-
ing systems than those, since KB data is used via
the addition of a scoring term, which is trained sepa-
rately beforehand (with no shared embeddings). Be-
sides, we demonstrate in our experimental section
that our system can handle a large number of rela-
tionships, significantly larger than that presented in
(Lao et al, 2012; Riedel et al, 2013).
3 Embedding-based Framework
Our work concerns energy-based methods, which
learn low-dimensional vector representations (em-
beddings) of atomic symbols (words, entities, re-
lationships, etc.). In this framework, we learn two
models: one for predicting relationships given re-
lation mentions and another one to encode the in-
teractions among entities and relationships from the
KB. The joint action of both models in prediction
allows us to use the connection between the KB and
text to perform relation extraction. One could also
share parameters between models (via shared em-
beddings), but this is not implemented in this work.
This approach is inspired by previous work designed
to connect words and Wordnet (Bordes et al, 2012).
Both submodels end up learning vector embed-
dings of symbols, either for entities or relationships
in the KB, or for each word/feature of the vocabulary
(denoted V). The set of entities and relationships in
the KB are denoted by E and R, and nv , ne and nr
denote the size of V , E and R respectively. Given
a triple (h, r , t) the embeddings of the entities and
the relationship (vectors in Rk ) are denoted with the
same letter, in boldface characters (i.e. h, r, t).
3.1 Connecting Text and Relationships
The first part of the framework concerns the learn-
ing of a function Sm2r (m, r), based on embeddings,
that is designed to score the similarity of a relation
mention m and a relationship r .
Our scoring approach is inspired by previous
work for connecting word labels and images (We-
ston et al, 2010), which we adapted, replacing im-
ages by mentions and word labels by relationships.
Intuitively, it consists of first projecting words and
features into the embedding space and then comput-
ing a similarity measure (the dot product in this pa-
per) between this projection and a relationship em-
bedding. The scoring function is then:
Sm2r (m, r) = f(m)>r
with f a function mapping words and features into
Rk , f(m) = W>?(m). W is the matrix of Rnv?k
containing all word embeddings w, ?(m) is the
(sparse) binary representation of m (? Rnv ) indi-
cating absence or presence of words/features, and
r ? Rk is the embedding of the relationship r .
This approach can be easily applied at test time to
score (mention, relationship) pairs. Since this type
of learning problem is weakly supervised, Bordes et
al. (2010) showed that a convenient way to train it
is by using a ranking loss. Hence, given a data set
D = {(mi , ri ), i = 1, ... , |D|} consisting of (men-
tion, relationship) training pairs, one could learn the
1367
embeddings using constraints of the form:
?i , ?r ? 6= ri , f(mi )>ri > 1 + f(mi )>r? , (1)
where 1 is the margin. That is, we want the re-
lation that (weakly) labels a given mention to be
scored higher than other relation by a margin of 1.
Then, given any mention m one can predict the cor-
responding relationship r?(m) with:
r?(m) = arg max
r ??R
Sm2r (m, r ?) = arg max
r ??R
(
f(m)>r?
)
.
Learning Sm2r (?) under constraints (1) is well
suited when one is interested in building a per-
mention prediction system. However, performance
metrics of relation extraction are sometimes mea-
sured using precision recall curves aggregated for
all mentions concerning the same pair of entities,
as in (Riedel et al, 2010). In that case the scores
across predictions for different mentions need to be
calibrated so that the most confident ones have the
higher scores. This can be better encoded with con-
straints of the following form:
?i , j , ?r ? 6= ri , rj , f(mi )>ri > 1 + f(mj)>r? .
In this setup, scores of pairs observed in the training
set should be larger than that of any other prediction
across all mentions. In practice, we use ?soft? rank-
ing constraints (optimizing the hinge loss), i.e. we
minimize:
?i , j , ?r ? 6= ri , rj , max(0, 1?f(mi )>ri +f(mj)>r?).
Finally, we also enforce a (hard) constraint on the
norms of the columns of W and r, i.e. ?i , ||Wi ||2 ?
1 and ?j , ||rj ||2 ? 1. Training is carried out by
Stochastic Gradient Descent (SGD), updating W
and r at each step, following (Weston et al, 2010;
Bordes et al, 2013). That is, at the start of training
the parameters to be learnt (the nv ? k word/feature
embeddings in W and the nr ? k relation embed-
dings r ) are initialized to random weights. We ini-
tialize each k-dimensional embedding vector ran-
domly with mean 0, standard deviation 1k . Then, we
iterate the following steps to train them:
1. Select at random a positive training pair
(mi , ri ).
2. Select at random a secondary training pair
(mj , rj), used to calibrate the scores.
3. Select at random a negative relation r ? such that
r ? 6= ri and r ? 6= rj .
4. Make a stochastic gradient step to minimize
max(0, 1? f(mi )>ri + f(mj)>r?).
5. Enforce the constraint that each embedding
vector is normalized, i.e. if ||Wi ||2 > 1 then
Wi ?Wi/||Wi ||2.
3.2 Encoding Structured Data of KBs
Using only weakly labeled text mentions for train-
ing ignores much of the prior knowledge we can
leverage from a large KB such as Freebase. In or-
der to connect this relational data with our model,
we propose to encode its information into entity and
relationship embeddings. This allows us to build a
model which can score the plausibility of new en-
tity relationship triples which are missing from Free-
base. Several models have been recently developed
for that purpose (e.g. in (Nickel et al, 2011; Bor-
des et al, 2011; Bordes et al, 2012)): we chose in
this work to follow the approach of (Bordes et al,
2013), which is simple, flexible and has shown very
promising results on Freebase data.
Given a training set S = {(hi , ri , ti ), i =
1, ... , |S|} of relations extracted from the KB, this
model learns vector embeddings of the entities and
of the relationships using the idea that the func-
tional relation induced by the r -labeled arcs of the
KB should correspond to a translation of the em-
beddings. That is, given a k-dimensional embed-
ding of the left-hand side (head) entity, adding the
k-dimensional embedding of a given relation should
yield the point (or close to the point) of the k-
dimensional embedding of the right-hand side (tail)
entity. Hence, this method enforces that h + r ? t
when (h, r , t) holds, while h + r should be far away
from t otherwise. The model thus gives the follow-
ing score for the plausibility of a relation:
Skb(h, r , t) = ?||h + r ? t||22 .
A ranking loss is also used for training Skb. The
ranking objective is designed to assign higher scores
1368
to existing relations versus any other possibility:
?i ,?h? 6= hi , Skb(hi , ri , ti ) ? 1 + Skb(h?, ri , ti ),
?i ,?r ? 6= ri , Skb(hi , ri , ti ) ? 1 + Skb(hi , r ?, ti ),
?i ,?t ? 6= ti , Skb(hi , ri , ti ) ? 1 + Skb(hi , ri , t ?).
That is, for each known triple (h, r , t), if we re-
placed the (i) head, (ii) relation or (iii) tail with some
other possibility, the modified triple should have a
lower score (i.e. be less plausible) than the original
triple. The three sets of constraints defined above
encode the three types of modification. As in Sec-
tion 3.1 we use soft constraints via the hinge loss,
enforce constraints on the norm of embeddings, i.e.
?h,r ,t , ||h||2 ? 1, ||r ||2 ? 1, ||t||2 ? 1, and training
is performed using SGD, as in (Bordes et al, 2013).
At test time, one may again need to calibrate the
scores Skb across entity pairs. We propose a sim-
ple approach: we convert the scores by ranking all
relationshipsR by Skb and instead output:
S?kb(h, r , t)=?
(?
r ? 6=r
?(Skb(h, r , t)>Skb(h, r
?, t))
)
,
i.e. a function of the rank of r . We chose the simpli-
fied model ?(x) = 1 if x < ? and 0 otherwise; ?(?)
is the Kronecker function.
3.3 Implementation for Relation Extraction
Our framework can be used for relation extraction
in the following way. First, for each pair of entities
(h, t) that appear in the test set, all the correspond-
ing mentionsMh,t in the test set are collected and a
prediction is performed with:
r?h,t = argmax
r?R
?
m?Mh,t
Sm2r (m, r) .
The predicted relationship can either be a valid re-
lationship or NA ? a marker that means that there is
no relation between h and t (NA is added to R dur-
ing training and is treated like other relationships).
If r?h,t is a relationship, a composite score is defined:
Sm2r+kb(h, r?h,t , t)=
?
m?Mh,t
Sm2r (m, r?h,t)+S?kb(h, r?h,t , t)
That is, only the top scoring non-NA predictions are
re-scored. Hence, our final composite model favors
predictions that agree with both the mentions and the
KB. If r?h,t is NA, the score is unchanged.
4 Experiments
We use the training and test data, evaluation frame-
work and baselines from (Riedel et al, 2010; Hoff-
mann et al, 2011; Surdeanu et al, 2012).
NYT+FB This dataset, developed by (Riedel et
al., 2010), aligns Freebase relations with the New
York Times corpus. Entities were found using the
Stanford named entity tagger (Finkel et al, 2005),
and were matched to their name in Freebase. For
each mention, sentence level features are extracted
which include part of speech, named entity and de-
pendency tree path properties. Unlike some of the
previous methods, we do not use features that aggre-
gate properties across multiple mentions. We kept
the 100,000 most frequent features.There are 52 pos-
sible relationships and 121,034 training mentions of
which most are labeled as no relation (labeled ?NA?)
? there are 4700 Freebase relations mentioned in the
training set, and 1950 in the test set.
Freebase Freebase is a large-scale KB that has
around 80M entities, 23k relationships and 1.2B re-
lations. We used a subset restricted to the top 4M
entities for scalability reasons ? where top is defined
as the ones with the largest number of relations to
other entities. We used all the 23k possible relation-
ships in Freebase. To make a realistic setting, we
did not choose the entity set using the NYT+FB data
set, so it may not overlap completely. For that rea-
son, we needed to keep the set rather large. Keeping
the top 4M entities gives an overlap of 80% with the
entities in the NYT+FB test set. Most importantly,
we then removed all the entity pairs present in the
NYT+FB test set from Freebase, i.e. all relations
they are involved in independent of the relationship.
This ensures that we cannot just memorize the true
relations for an entity pair ? we have to learn to gen-
eralize from other entities and relations.
As the NYT+FB dataset was built on an earlier
version of Freebase we also had to translate the dep-
recated relationships into their new variants (e.g.
?/p/business/company/place_founded ? ? ?/orga-
nization/organization/place_founded?) to make the
two datasets link (then, the 52 relationships in
NYT+FB are now a subset of the 23k from Free-
base). We then trained the Skb model on the remain-
ing triples.
1369
recall
prec
ision
0 0.1 0.20
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Wsabie M2R+FBMIMLREHoffmannWsabie M2RRiedelMintz
recall
prec
ision
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10.4
0.5
0.6
0.7
0.8
0.9 Wsabie M2R+FBMIMLREHoffmannWsabie M2RRiedelMintz
Figure 1: Top: Aggregate extraction precision/recall
curves for a variety of methods. Bottom: the
same plot zoomed to the recall [0-0.1] region.
WsabieM2R is our method trained only on mentions,
WsabieM2R+FB uses Freebase annotations as well.
Modeling Following (Bordes et al, 2013) we set
the embedding dimension k to 50. The learning rate
for SGD was selected using a validation set: we ob-
tained 0.001 for Sm2r , and 0.1 for Skb. For the cal-
ibration of S?kb, ? = 10 (note, here we are ranking
all 23k Freebase relationships). Training Sm2r took
5 minutes, whilst training Skb took 2 days due to the
large scale of the data set.
Results Figure 1 displays the aggregate precision
/ recall curves of our approach WSABIEM2R+FB
which uses the combination of Sm2r + Skb, as well
as WSABIEM2R , which only uses Sm2r , and existing
state-of-the-art approaches: HOFFMANN (Hoffmann
et al, 2011)2, MIMLRE (Surdeanu et al, 2012).
RIEDEL (Riedel et al, 2010) and MINTZ (Mintz et
al., 2009).
WSABIEM2R is comparable to, but slightly worse
than, the MIMLRE and HOFFMANN methods, possi-
bly due to its simplified assumptions (e.g. predict-
ing a single relationship per entity pair). However,
the addition of extra knowledge from other Freebase
entities in WSABIEM2R+FB provides superior per-
formance to all other methods, by a wide margin, at
least between 0 and 0.1 recall (see bottom plot).
Performance of WSABIEM2R and
WSABIEM2R+FB for recall > 0.1 degrades rapidly,
faster than that of other methods. This is also
caused by the simplifications of WSABIEM2R that
prevent it from reaching high precision when the
recall is greater than 0.1. We recall that Freebase
data is not used to detect relationships i.e. to
discriminate between NA and the rest, but only to
select the best relationship in case of detection.
That is WSABIEM2R+FB only improves precision,
not recall, so both versions of Wsabie are similar
w.r.t. recall. This could be improved by borrowing
ideas from HOFFMANN (Hoffmann et al, 2011) or
MIMLRE (Surdeanu et al, 2012) for dealing with
the multi-label case. Our approach, which uses
Freebase to increase precision, is general and could
improve any other method.
5 Conclusion
In this paper we described a framework for leverag-
ing large scale knowledge bases to improve relation
extraction by training not only on (mention, relation-
ship) pairs but using all other KB triples as well. We
empirically showed that it allows to significantly im-
prove precision on extracted relations. Our model-
ing approach is general and should apply to other
settings, e.g. for the task of entity linking.
Acknowledgments
This work was carried out in the framework of
the Labex MS2T (ANR-11-IDEX-0004-02), and
funded by the French National Agency for Research
(EVEREST-12-JS02-005-01).
2There is an error in the plot from (Hoffmann et al, 2011),
which we have corrected. The authors acknowledged this issue.
1370
References
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In IJCAI, vol-
ume 7, pages 2670?2676.
Antoine Bordes, Nicolas Usunier, and Jason Weston.
2010. Label ranking under ambiguous supervision for
learning semantic correspondences. In Proceedings of
the 27th International Conference on Machine Learn-
ing (ICML-10), pages 103?110.
Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Proc. of the 25th Conf.
on Artif. Intel. (AAAI).
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words and
meaning representations for open-text semantic pars-
ing. In Proc. of the 15th Intern. Conf. on Artif. Intel.
and Stat., volume 22, pages 127?135. JMLR.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,
Jason Weston, and Oksana Yakhnenko. 2013. Trans-
lating embeddings for modeling multi-relational data.
In Advances in Neural Information Processing Sys-
tems (NIPS 26).
Mark Craven, Johan Kumlien, et al 1999. Constructing
biological knowledge bases by extracting information
from text sources. In ISMB, volume 1999, pages 77?
86.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363?370.
Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, vol-
ume 1, pages 541?550.
Rohit J Kate and Raymond J Mooney. 2007. Learning
language semantics from ambiguous supervision. In
AAAI, volume 7, pages 895?900.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1017?
1026. Association for Computational Linguistics.
Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1-Volume 1, pages 91?99.
Association for Computational Linguistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume 2,
pages 1003?1011. Association for Computational Lin-
guistics.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings of
the 28th International Conference on Machine Learn-
ing (ICML-11), pages 809?816.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowledge
Discovery in Databases, pages 148?163. Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT, pages 74?84.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 455?465. Associa-
tion for Computational Linguistics.
Jason Weston, Samy Bengio, and Nicolas Usunier. 2010.
Large scale image annotation: learning to rank with
joint word-image embeddings. Machine learning,
81(1):21?35.
Fei Wu and Daniel S Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the sixteenth
ACM conference on Conference on information and
knowledge management, pages 41?50. ACM.
Fei Wu and Daniel S Weld. 2010. Open information ex-
traction using wikipedia. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 118?127. Association for Computa-
tional Linguistics.
1371
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615?620,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Question Answering with Subgraph Embeddings
Antoine Bordes
Facebook AI Research
112 avenue de Wagram,
Paris, France
abordes@fb.com
Sumit Chopra
Facebook AI Research
770 Broadway,
New York, USA
spchopra@fb.com
Jason Weston
Facebook AI Research
770 Broadway,
New York, USA
jase@fb.com
Abstract
This paper presents a system which learns
to answer questions on a broad range of
topics from a knowledge base using few
hand-crafted features. Our model learns
low-dimensional embeddings of words
and knowledge base constituents; these
representations are used to score natural
language questions against candidate an-
swers. Training our system using pairs of
questions and structured representations of
their answers, and pairs of question para-
phrases, yields competitive results on a re-
cent benchmark of the literature.
1 Introduction
Teaching machines how to automatically answer
questions asked in natural language on any topic
or in any domain has always been a long stand-
ing goal in Artificial Intelligence. With the rise
of large scale structured knowledge bases (KBs),
this problem, known as open-domain question an-
swering (or open QA), boils down to being able
to query efficiently such databases with natural
language. These KBs, such as FREEBASE (Bol-
lacker et al., 2008) encompass huge ever growing
amounts of information and ease open QA by or-
ganizing a great variety of answers in a structured
format. However, the scale and the difficulty for
machines to interpret natural language still makes
this task a challenging problem.
The state-of-the-art techniques in open QA can
be classified into two main classes, namely, infor-
mation retrieval based and semantic parsing based.
Information retrieval systems first retrieve a broad
set of candidate answers by querying the search
API of KBs with a transformation of the ques-
tion into a valid query and then use fine-grained
detection heuristics to identify the exact answer
(Kolomiyets and Moens, 2011; Unger et al., 2012;
Yao and Van Durme, 2014). On the other hand,
semantic parsing methods focus on the correct in-
terpretation of the meaning of a question by a se-
mantic parsing system. A correct interpretation
converts a question into the exact database query
that returns the correct answer. Interestingly, re-
cent works (Berant et al., 2013; Kwiatkowski et
al., 2013; Berant and Liang, 2014; Fader et al.,
2014) have shown that such systems can be ef-
ficiently trained under indirect and imperfect su-
pervision and hence scale to large-scale regimes,
while bypassing most of the annotation costs.
Yet, even if both kinds of system have shown the
ability to handle large-scale KBs, they still require
experts to hand-craft lexicons, grammars, and KB
schema to be effective. This non-negligible hu-
man intervention might not be generic enough to
conveniently scale up to new databases with other
schema, broader vocabularies or languages other
than English. In contrast, (Fader et al., 2013) pro-
posed a framework for open QA requiring almost
no human annotation. Despite being an interesting
approach, this method is outperformed by other
competing methods. (Bordes et al., 2014b) in-
troduced an embedding model, which learns low-
dimensional vector representations of words and
symbols (such as KBs constituents) and can be
trained with even less supervision than the system
of (Fader et al., 2013) while being able to achieve
better prediction performance. However, this ap-
proach is only compared with (Fader et al., 2013)
which operates in a simplified setting and has not
been applied in more realistic conditions nor eval-
uated against the best performing methods.
In this paper, we improve the model of (Bor-
des et al., 2014b) by providing the ability to an-
swer more complicated questions. The main con-
tributions of the paper are: (1) a more sophisti-
cated inference procedure that is both efficient and
can consider longer paths ((Bordes et al., 2014b)
considered only answers directly connected to the
615
question in the graph); and (2) a richer represen-
tation of the answers which encodes the question-
answer path and surrounding subgraph of the KB.
Our approach is competitive with the current state-
of-the-art on the recent benchmark WEBQUES-
TIONS (Berant et al., 2013) without using any lex-
icon, rules or additional system for part-of-speech
tagging, syntactic or dependency parsing during
training as most other systems do.
2 Task Definition
Our main motivation is to provide a system for
open QA able to be trained as long as it has ac-
cess to: (1) a training set of questions paired with
answers and (2) a KB providing a structure among
answers. We suppose that all potential answers are
entities in the KB and that questions are sequences
of words that include one identified KB entity.
When this entity is not given, plain string match-
ing is used to perform entity resolution. Smarter
methods could be used but this is not our focus.
We use WEBQUESTIONS (Berant et al., 2013)
as our evaluation bemchmark. Since it contains
few training samples, it is impossible to learn on
it alone, and this section describes the various data
sources that were used for training. These are sim-
ilar to those used in (Berant and Liang, 2014).
WebQuestions This dataset is built using FREE-
BASE as the KB and contains 5,810 question-
answer pairs. It was created by crawling questions
through the Google Suggest API, and then obtain-
ing answers using Amazon Mechanical Turk. We
used the original split (3,778 examples for train-
ing and 2,032 for testing), and isolated 1k ques-
tions from the training set for validation. WE-
BQUESTIONS is built on FREEBASE since all an-
swers are defined as FREEBASE entities. In each
question, we identified one FREEBASE entity us-
ing string matching between words of the ques-
tion and entity names in FREEBASE. When the
same string matches multiple entities, only the en-
tity appearing in most triples, i.e. the most popular
in FREEBASE, was kept. Example questions (an-
swers) in the dataset include ?Where did Edgar
Allan Poe died?? (baltimore) or ?What degrees
did Barack Obama get?? (bachelor of arts,
juris doctor).
Freebase FREEBASE (Bollacker et al., 2008)
is a huge and freely available database of
general facts; data is organized as triplets
(subject, type1.type2.predicate, object),
where two entities subject and object (identi-
fied by mids) are connected by the relation type
type1.type2.predicate. We used a subset, cre-
ated by only keeping triples where one of the
entities was appearing in either the WEBQUES-
TIONS training/validation set or in CLUEWEB ex-
tractions. We also removed all entities appearing
less than 5 times and finally obtained a FREEBASE
set containing 14M triples made of 2.2M entities
and 7k relation types.
1
Since the format of triples
does not correspond to any structure one could
find in language, we decided to transform them
into automatically generated questions. Hence, all
triples were converted into questions ?What is the
predicate of the type2 subject?? (using the
mid of the subject) with the answer being object.
An example is ?What is the nationality of the
person barack obama?? (united states). More
examples and details are given in a longer version
of this paper (Bordes et al., 2014a).
ClueWeb Extractions FREEBASE data allows
to train our model on 14M questions but these have
a fixed lexicon and vocabulary, which is not real-
istic. Following (Berant et al., 2013), we also cre-
ated questions using CLUEWEB extractions pro-
vided by (Lin et al., 2012). Using string match-
ing, we ended up with 2M extractions structured
as (subject, ?text string?, object) with both
subject and object linked to FREEBASE. We
also converted these triples into questions by using
simple patterns and FREEBASE types. An exam-
ple of generated question is ?Where barack obama
was allegedly bear in?? (hawaii).
Paraphrases The automatically generated ques-
tions that are useful to connect FREEBASE triples
and natural language, do not provide a satisfac-
tory modeling of natural language because of their
semi-automatic wording and rigid syntax. To
overcome this issue, we follow (Fader et al., 2013)
and supplement our training data with an indirect
supervision signal made of pairs of question para-
phrases collected from the WIKIANSWERS web-
site. On WIKIANSWERS, users can tag pairs of
questions as rephrasings of each other: (Fader et
al., 2013) harvested a set of 2M distinct questions
from WIKIANSWERS, which were grouped into
350k paraphrase clusters.
1
WEBQUESTIONS contains ?2k entities, hence restrict-
ing FREEBASE to 2.2M entities does not ease the task for us.
616
3 Embedding Questions and Answers
Inspired by (Bordes et al., 2014b), our model
works by learning low-dimensional vector embed-
dings of words appearing in questions and of enti-
ties and relation types of FREEBASE, so that repre-
sentations of questions and of their corresponding
answers are close to each other in the joint embed-
ding space. Let q denote a question and a a can-
didate answer. Learning embeddings is achieved
by learning a scoring function S(q, a), so that S
generates a high score if a is the correct answer to
the question q, and a low score otherwise. Note
that both q and a are represented as a combina-
tion of the embeddings of their individual words
and/or symbols; hence, learning S essentially in-
volves learning these embeddings. In our model,
the form of the scoring function is:
S(q, a) = f(q)
>
g(a). (1)
Let W be a matrix of R
k?N
, where k is the di-
mension of the embedding space which is fixed a-
priori, andN is the dictionary of embeddings to be
learned. LetN
W
denote the total number of words
and N
S
the total number of entities and relation
types. WithN = N
W
+N
S
, the i-th column ofW
is the embedding of the i-th element (word, entity
or relation type) in the dictionary. The function
f(.), which maps the questions into the embed-
ding spaceR
k
is defined as f(q) =W?(q), where
?(q) ? N
N
, is a sparse vector indicating the num-
ber of times each word appears in the question q
(usually 0 or 1). Likewise the function g(.) which
maps the answer into the same embedding space
R
k
as the questions, is given by g(a) = W?(a).
Here ?(a) ? N
N
is a sparse vector representation
of the answer a, which we now detail.
3.1 Representing Candidate Answers
We now describe possible feature representations
for a single candidate answer. (When there are
multiple correct answers, we average these rep-
resentations, see Section 3.4.) We consider three
different types of representation, corresponding to
different subgraphs of FREEBASE around it.
(i) Single Entity. The answer is represented as
a single entity from FREEBASE: ?(a) is a 1-
of-N
S
coded vector with 1 corresponding to
the entity of the answer, and 0 elsewhere.
(ii) Path Representation. The answer is
represented as a path from the entity
mentioned in the question to the answer
entity. In our experiments, we consid-
ered 1- or 2-hops paths (i.e. with either
1 or 2 edges to traverse): (barack obama,
people.person.place of birth, honolulu)
is a 1-hop path and (barack obama,
people.person.place of birth, location.
location.containedby, hawaii) a 2-hops
path. This results in a ?(a) which is a
3-of-N
S
or 4-of-N
S
coded vector, expressing
the start and end entities of the path and the
relation types (but not entities) in-between.
(iii) Subgraph Representation. We encode both
the path representation from (ii), and the en-
tire subgraph of entities connected to the can-
didate answer entity. That is, for each entity
connected to the answer we include both the
relation type and the entity itself in the repre-
sentation ?(a). In order to represent the an-
swer path differently to the surrounding sub-
graph (so the model can differentiate them),
we double the dictionary size for entities, and
use one embedding representation if they are
in the path and another if they are in the sub-
graph. Thus we now learn a parameter matrix
R
k?N
where N = N
W
+ 2N
S
(N
S
is the to-
tal number of entities and relation types). If
there areC connected entities withD relation
types to the candidate answer, its representa-
tion is a 3+C+D or 4+C+D-of-N
S
coded
vector, depending on the path length.
Our hypothesis is that including more informa-
tion about the answer in its representation will lead
to improved results. While it is possible that all
required information could be encoded in the k di-
mensional embedding of the single entity (i), it is
unclear what dimension k should be to make this
possible. For example the embedding of a country
entity encoding all of its citizens seems unrealis-
tic. Similarly, only having access to the path ig-
nores all the other information we have about the
answer entity, unless it is encoded in the embed-
dings of either the entity of the question, the an-
swer or the relations linking them, which might be
quite complicated as well. We thus adopt the sub-
graph approach. Figure 1 illustrates our model.
3.2 Training and Loss Function
As in (Weston et al., 2010), we train our model
using a margin-based ranking loss function. Let
D = {(q
i
, a
i
) : i = 1, . . . , |D|} be the training set
617
?Who did Clooney marry in 1987?? 
Embedding	 ?matrix	 ?W	 ?
G. Clooney K. Preston 
1987 
J. Travolta 
Model 
Honolulu 
Detec?on	 ?of	 ?Freebase	 ?en?ty	 ?in	 ?the	 ?ques?on	 ?
Embedding model 
Freebase subgraph 
Binary	 ?encoding	 ?of	 ?the	 ?subgraph	 ??(a)	 ?
Embedding	 ?of	 ?the	 ?subgraph	 ?g(a)	 ?
Binary	 ?encoding	 ?of	 ?the	 ?ques?on	 ??(q)	 ?
Embedding	 ?of	 ?the	 ?ques?n	 ? f(q)	 ?
Ques?n 	 ?q	 ?
Subgraph	 ?of	 ?a	 ?candidate	 ?answer	 ?a	 ?(here	 ?K.	 ?Preston)	 ?
Score S(q,a) How	 ?the	 ?candidate	 ?answer	 ?fits	 ?the	 ?ques?on	 ?
Dot	 ?product	 ? Embedding	 ?matrix	 ?W	 ?
Figure 1: Illustration of the subgraph embedding model scoring a candidate answer: (i) locate entity in
the question; (ii) compute path from entity to answer; (iii) represent answer as path plus all connected
entities to the answer (the subgraph); (iv) embed both the question and the answer subgraph separately
using the learnt embedding vectors, and score the match via their dot product.
of questions q
i
paired with their correct answer a
i
.
The loss function we minimize is
|D|
?
i=1
?
a??
?
A(a
i
)
max{0,m?S(q
i
, a
i
)+S(q
i
, a?)}, (2)
where m is the margin (fixed to 0.1). Minimizing
Eq. (2) learns the embedding matrix W so that
the score of a question paired with a correct an-
swer is greater than with any incorrect answer a?
by at least m. a? is sampled from a set of incor-
rect candidates
?
A. This is achieved by sampling
50% of the time from the set of entities connected
to the entity of the question (i.e. other candidate
paths), and by replacing the answer entity by a ran-
dom one otherwise. Optimization is accomplished
using stochastic gradient descent, multi-threaded
with Hogwild! (Recht et al., 2011), with the con-
straint that the columns w
i
of W remain within
the unit-ball, i.e., ?
i
, ||w
i
||
2
? 1.
3.3 Multitask Training of Embeddings
Since a large number of questions in our training
datasets are synthetically generated, they do not
adequately cover the range of syntax used in natu-
ral language. Hence, we also multi-task the train-
ing of our model with the task of paraphrase pre-
diction. We do so by alternating the training of
S with that of a scoring function S
prp
(q
1
, q
2
) =
f(q
1
)
>
f(q
2
), which uses the same embedding ma-
trix W and makes the embeddings of a pair of
questions (q
1
, q
2
) similar to each other if they are
paraphrases (i.e. if they belong to the same para-
phrase cluster), and make them different other-
wise. Training S
prp
is similar to that of S except
that negative samples are obtained by sampling a
question from another paraphrase cluster.
We also multitask the training of the embed-
dings with the mapping of the mids of FREEBASE
entities to the actual words of their names, so that
the model learns that the embedding of the mid of
an entity should be similar to the embedding of the
word(s) that compose its name(s).
3.4 Inference
OnceW is trained, at test time, for a given ques-
tion q the model predicts the answer with:
a? = argmax
a
?
?A(q)
S(q, a
?
) (3)
where A(q) is the candidate answer set. This can-
didate set could be the whole KB but this has both
speed and potentially precision issues. Instead, we
create a candidate set A(q) for each question.
We recall that each question contains one identi-
fied FREEBASE entity. A(q) is first populated with
all triples from FREEBASE involving this entity.
This allows to answer simple factual questions
whose answers are directly connected to them (i.e.
1-hop paths). This strategy is denoted C
1
.
Since a system able to answer only such ques-
tions would be limited, we supplement A(q) with
examples situated in the KB graph at 2-hops from
the entity of the question. We do not add all such
quadruplets since this would lead to very large
candidate sets. Instead, we consider the follow-
ing general approach: given that we are predicting
a path, we can predict its elements in turn using
618
Method P@1 F1 F1
(%) (Berant) (Yao)
Baselines
(Berant et al., 2013) ? 31.4 ?
(Bordes et al., 2014b) 31.3 29.7 31.8
(Yao and Van Durme, 2014) ? 33.0 42.0
(Berant and Liang, 2014) ? 39.9 43.0
Our approach
Subgraph & A(q) = C
2
40.4 39.2 43.2
Ensemble with (Berant & Liang, 14) ? 41.8 45.7
Variants
Without multiple predictions 40.4 31.3 34.2
Subgraph & A(q) = All 2-hops 38.0 37.1 41.4
Subgraph & A(q) = C
1
34.0 32.6 35.1
Path & A(q) = C
2
36.2 35.3 38.5
Single Entity & A(q) = C
1
25.8 16.0 17.8
Table 1: Results on the WEBQUESTIONS test set.
a beam search, and hence avoid scoring all can-
didates. Specifically, our model first ranks rela-
tion types using Eq. (1), i.e. selects which rela-
tion types are the most likely to be expressed in
q. We keep the top 10 types (10 was selected on
the validation set) and only add 2-hops candidates
to A(q) when these relations appear in their path.
Scores of 1-hop triples are weighted by 1.5 since
they have one less element than 2-hops quadru-
plets. This strategy, denotedC
2
, is used by default.
A prediction a
?
can commonly actually be
a set of candidate answers, not just one an-
swer, for example for questions like ?Who are
David Beckham?s children??. This is achieved
by considering a prediction to be all the en-
tities that lie on the same 1-hop or 2-hops
path from the entity found in the question.
Hence, all answers to the above question are
connected to david beckham via the same path
(david beckham, people.person.children,
*
).
The feature representation of the prediction is then
the average over each candidate entity?s features
(see Section 3.1), i.e. ?
all
(a
?
) =
1
|a
?
|
?
a
?
j
:a
?
?(a
?
j
)
where a
?
j
are the individual entities in the over-
all prediction a
?
. In the results, we compare to a
baseline method that can only predict single can-
didates, which understandly performs poorly.
4 Experiments
We compare our system in terms of F1 score as
computed by the official evaluation script
2
(F1
(Berant)) but also with a slightly different F1 def-
inition, termed F1 (Yao) which was used in (Yao
and Van Durme, 2014) (the difference being the
way that questions with no answers are dealt with),
2
Available from www-nlp.stanford.edu/software/sempre/
and precision @ 1 (p@1) of the first candidate en-
tity (even when there are a set of correct answers),
comparing to recently published systems.
3
The
upper part of Table 1 indicates that our approach
outperforms (Yao and Van Durme, 2014), (Berant
et al., 2013) and (Bordes et al., 2014b), and per-
forms similarly as (Berant and Liang, 2014).
The lower part of Table 1 compares various ver-
sions of our model. Our default approach uses
the Subgraph representation for answers and C
2
as the candidate answers set. Replacing C
2
by
C
1
induces a large drop in performance because
many questions do not have answers thatare di-
rectly connected to their inluded entity (not in
C
1
). However, using all 2-hops connections as
a candidate set is also detrimental, because the
larger number of candidates confuses (and slows
a lot) our ranking based inference. Our results
also verify our hypothesis of Section 3.1, that a
richer representation for answers (using the local
subgraph) can store more pertinent information.
Finally, we demonstrate that we greatly improve
upon the model of (Bordes et al., 2014b), which
actually corresponds to a setting with the Path rep-
resentation and C
1
as candidate set.
We also considered an ensemble of our ap-
proach and that of (Berant and Liang, 2014). As
we only had access to their test predictions we
used the following combination method. Our ap-
proach gives a score S(q, a) for the answer it pre-
dicts. We chose a threshold such that our approach
predicts 50% of the time (when S(q, a) is above
its value), and the other 50% of the time we use
the prediction of (Berant and Liang, 2014) instead.
We aimed for a 50/50 ratio because both meth-
ods perform similarly. The ensemble improves the
state-of-the-art, and indicates that our models are
significantly different in their design.
5 Conclusion
This paper presented an embedding model that
learns to perform open QA using training data
made of questions paired with their answers and
of a KB to provide a structure among answers, and
can achieve promising performance on the com-
petitive benchmark WEBQUESTIONS.
3
Results of baselines except (Bordes et al., 2014b) have
been extracted from the original papers. For our experiments,
all hyperparameters have been selected on the WEBQUES-
TIONS validation set: k was chosen among {64, 128, 256},
the learning rate on a log. scale between 10
?4
and 10
?1
and
we used at most 100 paths in the subgraph representation.
619
References
Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (ACL?14), Baltimore, USA.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?13), Seattle, USA.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, Vancouver, Canada. ACM.
Antoine Bordes, Sumit Chopra, and Jason Weston.
2014a. Question answering with subgraph embed-
dings. CoRR, abs/1406.3676.
Antoine Bordes, Jason Weston, and Nicolas Usunier.
2014b. Open question answering with weakly su-
pervised embedding models. In Proceedings of the
7th European Conference on Machine Learning and
Principles and Practice of Knowledge Discovery
in Databases (ECML-PKDD?14), Nancy, France.
Springer.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL?13), Sofia, Bulgaria.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In Proceedings of 20th
SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD?14), New York City, USA.
ACM.
Oleksandr Kolomiyets and Marie-Francine Moens.
2011. A survey on question answering technology
from an information retrieval perspective. Informa-
tion Sciences, 181(24):5412?5434.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP?13), Seattle, USA,
October.
Thomas Lin, Mausam, and Oren Etzioni. 2012. En-
tity linking at web scale. In Proceedings of the Joint
Workshop on Automatic Knowledge Base Construc-
tion and Web-scale Knowledge Extraction (AKBC-
WEKEX?12), Montreal, Canada.
Benjamin Recht, Christopher R?e, Stephen J Wright,
and Feng Niu. 2011. Hogwild!: A lock-free ap-
proach to parallelizing stochastic gradient descent.
In Advances in Neural Information Processing Sys-
tems (NIPS 24)., Vancouver, Canada.
Christina Unger, Lorenz B?uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over RDF data. In Proceedings of the
21st international conference on World Wide Web
(WWW?12), Lyon, France. ACM.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2010. Large scale image annotation: learning to
rank with joint word-image embeddings. Machine
learning, 81(1).
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?14), Baltimore, USA.
620
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 692?701,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Fast Recursive Multi-class Classification of Pairs of Text Entities
for Biomedical Event Extraction
Xiao Liu, Antoine Bordes, Yves Grandvalet
Universit? de Technologie de Compi?gne & CNRS
Heudiasyc UMR 7253
60200 Compi?gne Cedex, France
firstname.lastname@hds.utc.fr
Abstract
Biomedical event extraction from arti-
cles has become a popular research topic
driven by important applications, such as
the automatic update of dedicated knowl-
edge base. Most existing approaches are
either pipeline models of specific classi-
fiers, usually subject to cascading errors,
or joint structured models, more efficient
but also more costly and more involved to
train. We propose here a system based on
a pairwise model that transforms event ex-
traction into a simple multi-class problem
of classifying pairs of text entities. Such
pairs are recursively provided to the classi-
fier, so as to extract events involving other
events as arguments. Our model is more
direct than the usual pipeline approaches,
and speeds up inference compared to joint
models. We report here the best results
reported so far on the BioNLP 2011 and
2013 Genia tasks.
1 Introduction
Huge amounts of biomedical documents, such
as molecular biology reports or genomic papers
are generated daily. Automatically organizing
their content in dedicated databases enables ad-
vanced search and eases information retrieval for
researchers in biology, medicine or other related
fields. Nowadays, these data sources are mostly
in the form of unstructured free text, which is
complex to incorporate into databases. Hence,
many text-mining research initiatives are orga-
nized around the issue of automatically extract-
ing information from biomedical text. Efforts
specifically dedicated to biomedical text are nec-
essary because standard Natural Language Pro-
cessing tools cannot be readily applied to extract
biomedical events since such texts, articles or re-
ports involve highly domain-specific jargon, syn-
tax and dependencies (Kim et al., 2011a).
This paper tackles the problem of event extrac-
tion from biomedical documents. Building on pre-
vious advances in named entity recognition (for
detecting gene or protein mentions for instance),
this task consists in associating to these entities the
related events expressed in the text. Such events
are of multiple types and involve at least one text
entity as argument and another one as trigger; they
can be quite complex since some events have sev-
eral arguments, and recursive in the sense that ar-
guments can themselves be events. An example of
event is given in Figure 1.
Biomedical event extraction is attracting more
and more attention, especially thanks to the or-
ganization of recurrent dedicated BioNLP chal-
lenges (Kim et al., 2009; Kim et al., 2011b; Kim
et al., 2013). We propose here a new approach
which relies on a single multi-class classifier for
recursively detecting events from (trigger, argu-
ment) pairs. Compared to standard pipeline ap-
proaches based on sequences of classifiers (Bj?rne
and Salakoski, 2013; Hakala et al., 2013), we
avoid the intermediate problem of associating iso-
lated triggers to event types, relying on a tricky
multi-label classification problem. Instead, we di-
rectly extract compounds of events in the form
of (trigger, argument) pairs, simply relying on
a multi-class problem, whereby (trigger, argu-
ment) pairs are associated to event types. Con-
sidering pairs of words also allows us to char-
acterize examples by sophisticated joint features
such as shortest path in the dependency parse tree,
and hence to achieve much accurate trigger de-
tection than pipeline models. Besides, compared
to Markov random fields (Riedel and McCallum,
2011a), our discriminant model does not repre-
sent the full joint distribution of words and events.
We thus have a simpler inference process, which
results into drastically reduced training times (15
692
!!!!!!allows!Tax!to!recruit!coac/vator!proteins!CBP/p300!to!!
!
Binding!
Binding! Theme!2Theme!2ThemeTheme
Figure 1: Part of a sentence and corresponding
extracted events for the BioNLP 2013 Genia task.
times faster for processing about 800 training doc-
uments). In short, we propose in this work a
happy medium between pipeline and joint mod-
els. Our approach builds on our previous proposal
(Liu et al., 2013), where we detected triggers di-
rectly from (trigger, argument) pairs. Here, we
upgrade our scheme by adding a recursive clas-
sification process that considerably improves the
detection of complex events.
This paper is organized as follows: Section 2 in-
troduces the problem of biomedical event extrac-
tion and discusses related works. Section 3 de-
scribes our recursive model and its training pro-
cess. The post-processing procedures and the fea-
tures used are detailed in Sections 4 and 5. Sec-
tion 6 shows that our method achieves excellent
empirical results, with the best performance re-
ported so far on the BioNLP 2011 and 2013 Genia
tasks, and a reduced training duration compared to
the previously state-of-the-art models.
2 Context and Related Works
Biomedical event extraction aims at extracting
event formulas from sentences, defined as se-
quences of tokens (words, numbers, or symbols).
2.1 Task Definition
Terminology regarding biomedical events, trig-
gers, etc. varies from one task or data set to an-
other; in the following, we use the definitions used
by the Genia (GE) task 1 of the BioNLP chal-
lenges. An event is constituted of two kinds of
elements: an event trigger and one or several ar-
guments. The event trigger is an entity, that is,
a sequence of consecutive tokens which indicates
that an event is mentioned in the text. The argu-
ments of an event are participants, which can be
proteins, genes or other biomedical events.
In the data settings of the GE task, protein
mentions are already annotated in the text. Fig-
ure 1 illustrates biomedical event extraction in
the GE task framework: given 3 proteins ?Tax?,
?CBP? and ?p300?, one must detect ?recruit?
as an event trigger for two events of the Bind-
ing category , encoded by the formulas: (?re-
Class Type Principal arg Optional arg
Gene_expression theme (P)
S Transcription theme (P)
V Protein_catabolism theme (P)
T Phosphorylation theme (P)
Localization theme (P)
B
I Binding theme (P) theme_2 (P)
N
R Regulation theme (P/E) cause (P/E)
E Positive_regulation theme (P/E) cause (P/E)
G Negative_regulation theme (P/E) cause (P/E)
Table 1: Classes and types of events with their
arguments (P stands for Protein, E for Event).
cruit?, theme:?Tax?, theme_2:?CBP?) and (?re-
cruit?, theme:?Tax?, theme_2:?p300?).
A key part of the task is to detect the trigger en-
tities among the candidates sequences of tokens.
The BioNLP GE task considers 9 types of events.
1
Table 1 lists these events. The 9 event types may
be merged into three broader categories: the first
5 (termed SVT) have a single theme argument; the
Binding event (or BIN) can accept up to two theme
arguments; the last 3 types (termed REG) also ac-
cept up to two arguments, a theme and an optional
cause. REG events can be recursive because their
arguments can be proteins or events.
2.2 Related Works
Current approaches fall into two main cate-
gories: pipeline incremental models and global
joint methods. Pipeline approaches (S?tre et al.,
2009; Cohen et al., 2009; Quirk et al., 2011)
are the simplest way to tackle the problem of
event extraction. A sequence of specific classi-
fiers are ran on the text to successively (P
1
) de-
tect event triggers, (P
2
) assign them arguments,
(P
3
) detect event triggers whose arguments can
be events, and (P
4
) assign them arguments (steps
(P
3
) and (P
4
) can be ran multiple times). Such
systems are relatively easy to set up and experi-
enced many successes: the TEES system (Bj?rne
et al., 2009; Bj[Pleaseinsertintopreamble]rne et
al., 2012; Bj?rne and Salakoski, 2013) won the
BioNLP GE task in 2009 and ranked 2
nd
in 2013,
whereas the EVEX system won in 2013 (Van Lan-
deghem et al., 2011; Hakala et al., 2013). How-
ever, all these methods suffer from error cascad-
ing. Besides, prediction must be formalized as
1
The BioNLP 2013 challenge considered 13 types of
events, but we only dealt with the 9 types defined in the pre-
vious challenges, because there was not enough data on the
newly defined types for proper training or model selection.
693
a multi-label classification problem because some
words can participate in the definition of several
events of different types. Detecting triggers in iso-
lation of their arguments in steps (P
1
) and (P
3
) are
ill-posed intermediate problems, since the notion
of trigger is intrinsically tied to its argument. The
latter brings contextual information that is indis-
putably relevant for detection. Besides, rich fea-
tures coding for (trigger, argument) pairs (Miwa et
al., 2010) are only used by pipeline models for as-
signing arguments, whereas they could be useful
for trigger detection as well.
Global joint approaches (Riedel et al., 2009;
McClosky et al., 2011) aim at solving the event
extraction task at once, so as to resolve the
drawbacks of pipeline models. In (McClosky et
al., 2011), event annotations are converted into
pseudo-syntactic representations and the task is
solved as a syntactic extraction problem by tradi-
tional parsing methods. In (Riedel et al., 2009;
Riedel and McCallum, 2011a; Riedel et al., 2011;
Riedel and McCallum, 2011b), some models are
proposed based on the maximization of a global
score taking into account the annotations of nodes
and edges in a graph representing each sentence.
This maximization problem is formalized as an in-
teger linear program with consistency constraints,
and solved via dual decomposition. Such joint
models perform very well (winner of the BioNLP
2011 GE task), but suffer from consequential com-
puting costs, as all possible combinations of words
are considered as potential events. In the follow-
ing, we show that our model is able to reach better
accuracies than joint models while being compu-
tationally much cheaper.
A method based on the search-based structured
prediction paradigm (Vlachos and Craven, 2012)
has been proposed as an intermediate step between
joint and pipeline approaches, by turning the struc-
tured prediction problem into a sequence of multi-
class classification tasks. Our experiments demon-
strate that, despite being conceptually simpler, our
recursive pairwise model outperforms it.
3 Recursive Pairwise Model
In this section, we present our recursive pair-
wise model. It directly extracts pairwise inter-
actions between entities, thereby contrasting with
the usual pipeline approaches, which require de-
tecting triggers as an intermediate problem. Our
approach proceeds in two steps:
1. Main (trigger, theme) pair extraction:
main event extraction step that detects the
triggers with one of their arguments;
2. Post-processing: step that adds extra-
arguments to BIN and REG events.
Step 1 is the main innovative part of our sys-
tem, and is detailed in the remainder of this sec-
tion. Step 2, which relies on more established
techniques, is described in Section 4.
3.1 Direct Extraction of Simple Events
We process entities differently depending on
whether they are marked as proteins in the anno-
tation or not; the latter are termed candidate en-
tities. We denote C
S
= {c
i
}
i
the set of candi-
date entities, which is built from the sentence to-
kens (see Section 5 for details on its construction),
A
S
= {a
j
}
j
the set of candidate arguments (that
is, the proteins identified by a named-entity recog-
nizer beforehand) in a given sentence S, and the
set of event types (augmented by None) is Y .
The first steps of a pipeline model consist in pre-
dicting whether candidate entities c
i
? C
S
are trig-
gers or not and then, whether arguments a
j
? A
S
can participate to a subset of events from Y . In-
stead, our pairwise model directly addresses the
problem of classifying the (candidate, argument)
pairs p
ij
= (c
i
, a
j
) as events of type from Y .
This classification is based on Support Vector
Machines (SVMs), where the multi-class problem
is broken down in a series of one-vs-rest binary
problems, one for each event type. The final de-
cision associated to each pair p
ij
is simply taken
as the event (including None) whose score is max-
imal. Classifying a pair p
ij
as not-None jointly
detects the event trigger c
i
and its argument a
j
.
3.2 Recursive Extraction of Complex Events
For simple SVT and BIN events, the set A
S
of
possible arguments is restricted to proteins, but the
events of class REG may have other events as ar-
guments, thus A
S
has to be enriched. Consider-
ing all possible events would be intractable, so that
the set of possible arguments is updated dynami-
cally in the process of extracting events. As here
possible arguments are exclusive to event types, in
practice it is simpler to update the set of pairs that
remain to be assessed.
Assume that an event has been actually pre-
dicted, that is, that p
??
= (c
?
, a
?
) has been clas-
694
Algorithm 1 Recursive pairwise event extraction
input sentence S, candidate entities C
S
= {c
i
}
i
and labeled proteins A
S
= {a
j
}
j
1: initialize candidate pairs
P
S
= {(c
i
, a
j
), c
i
? C
S
, a
j
? A
S
}
2: initialize extracted events E
S
= ?
3: score and label the pairs in P
S
4: while P
S
6= ? do
5: select the pair p
??
? P
S
with highest score
6: update P
S
? P
S
? {p
??
}
7: if y?
??
6= None then
8: create event e?
??
= (c
?
, a
?
, y?
??
)
9: update E
S
? E
S
? {e?
??
}
10: update P
S
? P
S
? {(c
i
, e?
??
)|c
i
? C
S
}
11: censor pairs P
S
to avoid cycles
12: score and label the new {(c
i
, e?
??
)} pairs
13: end if
14: end while
15: return extracted events E
S
sified as y?
??
6= None; the predicted event is de-
noted e?
??
= (c
?
, a
?
, y?
??
). We create all pairs
with it as argument, {(c
i
, e?
??
)|c
i
? C
S
}, and add
them to P
S
, so as to allow for the detection of re-
cursive events. We assume that recursive events
constitute a directed acyclic graph, where the an-
cestor of a candidate entity cannot be used as its
argument. The dynamic updating process is thus
constrained to prevent the creation of cycles.
Algorithm 1 summaries our event extraction
algorithm. For all events with a single argu-
ment, predicting y? variables directly responds to
the event extraction problem. When appropriate,
additional optional arguments are added after all
pairwise events have been extracted, by the post-
processing described in Section 4.
3.3 Fitting the Pairwise Model
The prediction process described above relies on
a multi-class classifier. We stress again that, since
pairs are assigned to a single class, there is no need
to address the more difficult multi-label problem
encountered in standard pipeline approaches. An
entity may still be assigned to several events, pos-
sibly of different types, through the allocation of
labels to several pairs comprising this entity.
Training SVMs For each event type, a series
of binary linear SVMs is fitted to the avail-
able training data, using the implementation from
scikit-learn.org. As events are rare, each
binary classification problem is highly unbal-
anced. We thus use different losses for posi-
tive and negative examples (Morik et al., 1999;
Veropoulos et al., 1999), resulting in two hyper-
parameters that are set by cross-validation, so as to
maximize the F-score of the corresponding event
type taken in isolation.
For the SVT and BIN events, the training sets
are all composed of the possible (candidate, argu-
ment) pairs P
S
= {p
ij
= (c
i
, a
j
)|c
i
? C
S
, a
j
?
A
S
} readily extracted from all training sentences,
and they only differ in the definition of the posi-
tive and negative class, according to the true label
associated to each pair.
Creating the training sets for REG events is
more complicated: since they can take events as
arguments, new pairs are added to P
S
by consid-
ering all the events already detected, as sketched
in Algorithm 1. Hence, the sets of training exam-
ples are not deterministically known before train-
ing, but depend on predictions of all other clas-
sifiers. Training directly on them requires to use
either online algorithms or complex search-based
structured prediction procedures as in (Vlachos
and Craven, 2012). In this paper, we prefer to
use instead the true labels y
??
during the training
phase of REG and None classifiers: the training
sets are then the enriched sets of possible (candi-
date, argument) pairs P
S
= {p
ij
= (c
i
, a
j
)|c
i
?
C
S
, a
j
? A
S
} ? {p
i?
= (c
i
, e
??
)|c
i
? C
S
, ?? :
y
??
6= None}. This allows to know all train-
ing examples beforehand and hence to use stan-
dard batch SVM algorithms. The drawback is
that, since extracted events in test are imperfect,
this creates a divergence between training and test-
ing scenarios, which can lead to degraded perfor-
mance. However, as our experiments show, this
effect is marginal compared to the advantages of
using fast reliable batch training algorithms.
Score Combination As said earlier, the decision
rule simply consists in predicting the class corre-
sponding to the highest SVM score. This simple
scheme could be improved, either by using multi-
class classifiers or by using more refined combi-
nations optimizing a global criterion as proposed
in (Liu et al., 2013). Though this route deserves
to be thoroughly tested, we conjecture that only
marginal gains should be expected since the vast
majority of errors are due to the detection of an
event when there is none or to the absence of de-
tection of an existing event: when an event is de-
695
tected, its correct type is predominantly predicted.
3.4 Computational Considerations
The pairwise structure leads to a simple inference
procedure, with a slight increase in computational
complexity compared to pipeline models. We de-
note m = card(C
S
), the number of candidate enti-
ties, n = card(A
S
), the number of annotated pro-
teins and m
?
the number of detected triggers. The
complexity of a pipeline model isO(m
?
(n+m
?
)),
whereas that of our approach is O(m(n + m
?
)).
This implies more calls to the classifying mech-
anism, but this is not too penalizing, since SVM-
based classification scales well with the number of
examples. Besides, this is still cheaper than joint
models such as (Riedel and McCallum, 2011a),
whose complexity is O(m(n
2
+m)).
4 Post-Processing
This section describes the post-processing car-
ried out once the (trigger, theme) pairs are de-
tected and labeled as events. The goal is to look
whether extra-arguments should be added to these
extracted events.
4.1 Binding Theme Fusion
This step attempts to merge several pairs la-
beled as Binding to create multiple arguments
events. We take the set of extracted Binding events
{(c
?
, a
?
)} that share the same trigger c
?
, and all
combinations {(c
?
, a
?
, a
?
)|? 6= ?} are classified
by a binary SVM. Once a combination (c
?
, a
?
, a
?
)
is predicted as a correct merge, it is added to pre-
dicted events while both pairs (c
?
, a
?
) and (c
?
, a
?
)
are removed.
4.2 Regulation Cause Assignment
This step looks for optional cause arguments that
may be added to the extracted REG events. Given
an extracted event (c
?
, a
?
) and a candidate argu-
ment set A
S
= {a
?
} containing all the proteins of
the sentence S as well as all events extracted by
the classifier, all combinations {(c
?
, a
?
, a
?
)|? 6=
?} are classified by a binary SVM. Since cause
argument could be another event, we extract them
incrementally in a dynamic process alike (trigger,
theme) pair extraction, also with constraints avoid-
ing the creation of cycles.
5 Features
This section details our features as well as the data
preprocessing used by our system.
Pre-processing Tokenization and sentence split-
ting have an important impact on the quality of the
dependency parse trees as well as the way we han-
dle compound words that contain protein names.
Data is split in sentences using both the nltk
toolkit (nltk.org) and the sentence splitting
provided for the BioNLP GE task. High quality
dependency parse trees require a fine grained tok-
enization, whereas coarse tokenization conserves
some biomedical jargon that could also provide
essential information. Hence, two tokenizations
are used for different features. Tokenization1, pro-
vided by the organizers of the BioNLP GE task,
is a coarse tokenization that is used to character-
ize when a candidate entity and a protein are in
the same token. Tokenization2 is fined grained,
based on the Stanford parser (McClosky et al.,
2011) that is slightly modified for primary tok-
enization. It supplies the dependency parse, can-
didate entity match and most of our features. The
dependency parse trees are finally obtained us-
ing a phrase structure parser (McClosky et al.,
2010), using the post-processing of the Stanford
corenlp package (De Marneffe et al., 2006). We
used stems (obtained by Snowball stemmer pro-
vided in nltk) as base forms for the tokens.
Candidate set For each sentence S, the set C
S
is built with a gazetteer: candidate entities are
recursively added by searching first the longest
token sequences (from Tokenization2) from the
gazetteer. For entities with several tokens, a rep-
resentative head token is selected by a heuristic
based on the dependency parse.
Candidate entities Three types of tokens are
considered: the head token, its parent and child
nodes in the dependency tree, and the tokens be-
longing to a neighboring window of the entity. The
size k of the word window is a hyper-parameter of
our model. Table 2 lists all features which include
stems, part-of-speech (POS) tags, etc. Special care
was taken to design the feature for head token
since it plays an extremely important role in can-
didate entities. We hence employed features and
heuristics to deal with compound-words, hyphens
and prefixes, inspired by such tools developed in
the code of the UCLEED system (based on Tok-
696
Base form (stem) of the head token.
Base form of the head token without
?-? or ?/? before of after.
Sub-string after ?-? in the head token.
POS of the head token.
First token of the entity is after ?-? or ?/?.
Last token of the entity is before ?-? or ?/?.
Head token has a special prefix: "over",
"up", "down", "co"
Candidate Concat. of base form and POS of parents
entity of the head token in dependency parse.
features Concat. of base form and POS of children
of the head token in dependency parse.
Base forms of k neighboring tokens
around the entity.
POS of k neighboring tokens around the
entity.
Neighborhood of the entity has ?-? or ?/?.
Sentence has "mRNA".
Entity is connected with another string
using Tokenization1.
Argument is a protein.
POS of the head token.
Features extracted from IntAct when the
argument is a protein.
Argument Base forms of k neighboring tokens
features around the argument.
POS of k neighboring tokens around the
argument.
Concat. of base form and POS of children
of the head token in dependency parse.
Token sequence between candidate and
argument has proteins.
V-walk features between candidate and
argument with base forms.
Pairwise E-walk features between candidate and
features argument with base forms.
V-walk features between candidate and
argument with POS.
E-walk features between candidate and
argument with POS.
Candidate and the argument share a token
using Tokenization1.
Table 2: Features used by our system. Most are
based on Tokenization2 except when specified.
enization2).
2
Protein names and POS in tokens are
substituted by the token PROT, e.g. transforming
"LPS-activated" into "PROT-activated". There is
total of a 35,365 candidate features.
Arguments Table 2 also lists the argument fea-
tures, which are a subset of those for candidate en-
tities. Most head word features are removed, but
base forms and POS of the neighboring tokens and
of the parent node in the dependency tree are still
included. Assigning label from SVT or BIN event
classes to a (c
i
, e
??
) pair should never occur, be-
cause only regulation events could have another
event as argument. Therefore, we add a feature
that indicates whether the argument is a protein
2
See github.com/riedelcastro/ucleed.
Figure 2: Example of E-walks and V-walks
features for encoding a dependency parse tree.
or a trigger entity. Proteins are also described us-
ing features extracted from the Uniprot knowledge
base (uniprot.org). There is total of 4,349 ar-
gument features.
Pairwise relations Our pairwise approach is
able to take advantage of features that code inter-
actions between candidate triggers and arguments,
such as those listed in Table 2. Hence, we have a
feature indicating if both elements of a pair belong
to the same token (based on Tokenization1).
But the most important pairwise features come
from the shortest path linking two candidate and
arguments in the dependency parse tree of the sen-
tence. Incorporating such dependency information
into the pairwise model relies on the process en-
coding the path into feature vectors. Many for-
matting methods have been proposed in previous
works. Following (Miwa et al., 2010), our sys-
tem use a combination of E-walks, that encode the
path into triplets (dep-tag, token, dep-tag), and V-
walks that encode it into triplets (token, dep-tag,
token), where tokens are encoded via stem and
POS tags, and dep-tags are the dependency labels.
Figure 2 illustrates this formatting: from the de-
pendency parse given on top, three V-walk and two
E-walk features are defined. These are inserted
in the feature vector using a bag-of-words pro-
cess, thus losing any relative ordering information.
These imperfect representations lose a lot of infor-
mation and can even add noise, especially when
the path is long. Therefore, we applied heuris-
tics from the UCLEED system to remove some
uninformative edges from the dependency parse.
Moreover, dependency parse features are added
only for pairs for which the (candidate, argument)
path length is below a threshold whose value is a
hyper-parameter. There is a total of 176,106 pair-
wise features.
697
Event Type TEES 2.1 EVEX Pipeline Our approach
or Class counterpart
Gene_expression 82.7 82.7 83.9 85.1
Transcription 55.0 55.0 61.7 62.8
Protein_catabol 56.3 56.3 66.7 68.8
Phosphorylation 72.6 71.5 81.8 81.8
Localization 63.3 60.7 56.9 57.7
SVT TOTAL 74.9 74.5 79.0 79.6
BIN TOTAL 43.3 42.9 41.6 42.4
Regulation 23.0 23.4 23.1 31.8
Positive_regul 38.7 39.2 36.5 46.3
Negative_regul 43.7 43.9 38.1 43.6
REG TOTAL 38.1 38.4 35.1 43.2
ALL TOTAL 50.7 51.0 50.8 54.4
Table 3: F-scores on the test set of the BioNLP
2013 GE task.
6 Experiments
In this section, we evaluate empirically our sys-
tem in the framework (data, annotations and eval-
uation) of biomedical event extraction defined in
the GE tasks of the BioNLP challenges. More pre-
cisely, we present results on the test sets of the
fresh 2013 GE task, and of the 2011 edition to
compare to joint methods.
In order to assess the efficiency of our mod-
eling choices, we also implemented a pipeline
counterpart system, following the structure of
the TEES approach (Bj?rne et al., 2009;
Bj[Pleaseinsertintopreamble]rne et al., 2012;
Bj?rne and Salakoski, 2013) but using the same
feature set, pre-processing and a similar post-
processing as our system. This pipeline system
comprises four steps: (1) trigger classification,
which assigns event types from Y to candidate
entities c
i
? C
S
using a multi-class SVM classi-
fier; (2) edge detection, which identifies the edges
between extracted triggers and proteins and be-
tween REG triggers and all the triggers; labels
from Y
edge
= {theme, cause,None} are as-
signed to those pairs; (3) binding theme fusion,
identical to as in Section 4.1; (4) theme-cause fu-
sion, as in Section 4.2, given two predicted pairs
(c
i
, theme : a
?
), (c
i
, cause : a
?
), this step de-
cides whether they should be merged into a single
event (c
i
, theme : a
?
, cause : a
?
).
6.1 Genia Shared Task 2013
For the BioNLP 2013 GE task, the hyper-
parameters of our system have been optimized on
the GE task development set (except for the regu-
larization parameters of the SVMs, which are se-
lected by cross-validation), after training on the
corresponding training sets: token window size is
2 for candidate entities and 1 for arguments, the
threshold for dependency path is 4. Using these
hyper-parameter values, the final model submit-
ted for test evaluation on the GE task server has
been trained on all documents from training and
development sets of BioNLP 2011 and 2013 GE
tasks. Detailed descriptions of the BioNLP 2011
and 2013 GE data are respectively given in (Kim
et al., 2011b) and (Kim et al., 2013).
Table 3 lists the detailed test F-scores, as
returned by the official challenge test server
(using the default approximate span & recur-
sive matching evaluation setting). We com-
pare our model to the winner of the challenge,
EVEX (Hakala et al., 2013), and of the best
runner-up, TEES 2.1 (Bj?rne and Salakoski,
2013), which are both pipeline approaches.
Our approach is slightly below TEES 2.1 on
BIN events, but overall, it outperforms all com-
petitors significantly (by more than 3%), with
a wide margin on REG events. Our pipeline
counterpart has an overall performance similar to
EVEX and TEES 2.1, while being better for SVT
and worse for BIN and REG events. These dis-
parities are due to the differences in features and
in processing details. The benefits of the pairwise
structure and the recursive process are demon-
strated by the considerable improvement upon the
pipeline counterpart (using the same features, pre-
and post-processing). In particular, the recursive
prediction process run on REG events brings about
a very substantial improvement (more than 8%).
6.2 Genia Shared Task 2011
The best performing methods on the BioNLP
2013 GE task were pipeline approaches, but the
joint models that were performing better in the
previous challenge were not competing in 2013.
As these joint models are quite tricky to train,
we compare our system with joint models on
the BioNLP 2011 GE task, where trustworthy
performances have been publicly released. We
train our model using the training and develop-
ment sets available at the time of the challenge
and we then get an evaluation on the same test
data using the official test server maintained on-
line by BioNLP organizers. Table 4 lists the re-
sults of our approach, its pipeline counterpart, and
those of UCLEED (Riedel and McCallum, 2011a)
and TEES (Bj[Pleaseinsertintopreamble]rne et al.,
698
Event UCLEED SEARN TEES Pipeline Our approach
Class counterpart
SVT 73.5 71.8 n/a 71.8 74.0
BIN 48.8 45.8 n/a 40.0 50.5
REG 43.8 43.0 n/a 35.7 45.1
ALL 55.2 53.5 53.3 50.0 55.6
Table 4: F-scores on the test set of the BioNLP
2011 GE task.
2012), which are respectively the best performing
joint model and best pipeline on this task. We
also added SEARN (Vlachos and Craven, 2012),
which is a hybrid between them.
3
As for 2013 data, our system achieves a higher
F-score on all event classes compared to its
pipeline counterpart. The benefits of the pair-
wise structure and the recursive process are larger
here, thereby outperforming the overall F-score of
TEES (no detailed results available), which itself
performs better than our pipeline counterpart. Sys-
tematic improvements on all event classes are also
observed compared to the joint model UCLEED
and to the search-based structured prediction ap-
proach of SEARN. To our knowledge, our model
thus reaches the best overall performance reported
so far on this data set for a single model.
4
By combining the use of the simple pair struc-
ture between triggers and arguments with a recur-
sive prediction process, our approach is able to
outperform pipeline models and to be at least at
par with models relying on much more sophisti-
cated structures. For this task, it is thus highly
beneficial to consider pairwise interactions from
beginning to end, but more complex dependen-
cies seem not to be essential, especially since they
come at a higher computational cost.
6.3 Training Durations
In this last section, we propose to illustrate the
lower complexity of our approach compared to
UCLEED by providing durations for training both
systems on BioNLP 2011 GE. These timings do
not involve preprocessing but only running cross-
validation on the training set and evaluation on
the development and test sets. For UCLEED,
3
The results for UCLEED, TEES and SEARN mod-
els are reproduced from (Riedel and McCallum, 2011a;
Bj[Pleaseinsertintopreamble]rne et al., 2012; Vlachos and
Craven, 2012) respectively.
4
We do not compare with the results of FAUST (Riedel et
al., 2011), which achieved the best F-score on this task (56.0)
because this is an ensemble of various models of UCLEED
and of the Stanford system (McClosky et al., 2011), which
makes it an unfair comparison.
we used the code (in java & scala) provided by
the authors
5
and we chose BioNLP 2011 GE be-
cause this code was primarily designed to run
on it. Our code, in python, is publicly available
from github.com/XiaoLiuAI/RUPEE. Ex-
periments were conducted on the same computer,
with a quad-core Intel Xeon CPU and 16GB of
RAM. Both codes are multi-threaded and used
all 4 threads simultaneously. Under these condi-
tions, UCLEED requires around 8h30min to run
its 10 epochs,
6
while our code completes training
in about 30min. Some of these differences may be
due to implementation choices, but we believe that
the 15 fold speed increase (for around 800 training
documents) is at least partially due to the lower
complexity of our approach.
7 Conclusion
We introduced a recursive pairwise model de-
signed for biomedical event extraction. This pair-
wise model improves on the best current ap-
proaches of the BioNLP 2013 and 2011 GE tasks.
Our system breaks down the overall event extrac-
tion task into the classification of (trigger, theme)
pairs, assigned to event types. These (trigger,
theme) pairs enable to use joint features in off-the-
shelf classifiers, without resorting to costly global
inference models. We also implemented a recur-
sive procedure that deals with regulation events,
which may include other events in their definition.
All operations are run in a unified framework, us-
ing a single event classifier.
Our system is fast and more accurate than the
available pipeline models or joint models. Given
its simplicity and scalability, we believe that our
model is a strong basis for large-scale event extrac-
tion projects. Several refinements are possible, for
example by exploring other types features, or by
enabling the direct processing of triplets that may
be encountered in binding or regulation events.
Acknowledgments
This work was carried out in the framework of
the Labex MS2T funded by the French National
Agency for Research through the program ?Invest-
ments for the future? (ANR-11-IDEX-0004-02),
and supported by the ?young researchers? pro-
gram (EVEREST-12-JS02-005-01).
5
See github.com/riedelcastro/ucleed .
6
UCLEED might be faster by using feature caching, but
we had to disable it because it was taking up too much RAM.
699
References
J. Bj?rne and T. Salakoski. 2013. TEES 2.1: Auto-
mated annotation scheme learning in the BioNLP
2013 shared task. In Proceedings of BioNLP Shared
Task 2013 Workshop, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
J. Bj?rne, J. Heimonen, F. Ginter, A. Airola,
T. Pahikkala, and T. Salakoski. 2009. Extract-
ing complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado, June. Association
for Computational Linguistics.
J. Bj?rne, F. Ginter, and T. Salakoski. 2012. University
of turku in the bionlp?11 shared task. BMC Bioinfor-
matics, 13(Suppl 11):S4.
K. B. Cohen, K. Verspoor, H. Johnson, C. Roeder,
P. Ogren, W. Baumgartner, E. White, and L. Hunter.
2009. High-precision biological event extraction
with a concept recognizer. In Proceedings of the
BioNLP 2009 Workshop Companion Volume for
Shared Task, pages 50?58, Boulder, Colorado, June.
Association for Computational Linguistics.
M.-C. De Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In Proceedings of
LREC, volume 6, pages 449?454.
K. Hakala, S. Van Landeghem, T. Salakoski, Y. Van de
Peer, and F. Ginter. 2013. EVEX in ST?13: Appli-
cation of a large-scale text mining resource to event
extraction and network construction. In Proceedings
of BioNLP Shared Task 2013 Workshop, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
J.-D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-
jii. 2009. Overview of BioNLP?09 shared task
on event extraction. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 1?9, Boulder, Colorado, June. Association for
Computational Linguistics.
J.-D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2011a. Extracting bio-molecular events from litera-
ture. Computational Intelligence, 27(4):513?540.
J.-D. Kim, Y. Wang, T. Takagi, and A. Yonezawa.
2011b. Overview of genia event task in bionlp
shared task 2011. In Proceedings of BioNLP Shared
Task 2011 Workshop, pages 7?15, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
J.-D. Kim, Y. Wang, and Y. Yasunori. 2013. The
genia event extraction shared task, 2013 edition -
overview. In Proceedings of the BioNLP Shared
Task 2013 Workshop, pages 8?15, Sofia, Bulgaria,
August. Association for Computational Linguistics.
X. Liu, A. Bordes, and Y. Grandvalet. 2013. Biomed-
ical event extraction by multi-class classification of
pairs of text entities. In Proceedings of the BioNLP
Shared Task 2013 Workshop, pages 45?49, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic domain adaptation for parsing. In Human
Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, HLT ?10,
pages 28?36, Stroudsburg, PA, USA. Association
for Computational Linguistics.
D. McClosky, M. Surdeanu, and C. D. Manning. 2011.
Event extraction as dependency parsing. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages
1626?1635, Stroudsburg, PA, USA. Association for
Computational Linguistics.
M. Miwa, R. S?tre, J.-D. Kim, and J. Tsujii. 2010.
Event extraction with complex event classification
using rich features. J. Bioinformatics and Computa-
tional Biology, 8(1):131?146.
K. Morik, P. Brockhausen, and T. Joachims. 1999.
Combining statistical learning with a knowledge-
based approach - a case study in intensive care moni-
toring. In Proceedings of the Sixteenth International
Conference on Machine Learning (ICML 1999).
C. Quirk, P. Choudhury, M. Gamon, and L. Vander-
wende. 2011. MSR-NLP entry in BioNLP shared
task 2011. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 155?163, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
S. Riedel and A. McCallum. 2011a. Fast and robust
joint models for biomedical event extraction. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 1?
12, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
S. Riedel and A. McCallum. 2011b. Robust biomed-
ical event extraction with dual decomposition and
minimal domain adaptation. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 46?50,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
S. Riedel, H.-W. Chun, T. Takagi, and J. Tsujii. 2009.
A Markov logic approach to bio-molecular event ex-
traction. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 41?
49, Boulder, Colorado, June. Association for Com-
putational Linguistics.
S. Riedel, D. McClosky, M. Surdeanu, A. McCallum,
and C. D. Manning. 2011. Model combination for
event extraction in BioNLP 2011. In Proceedings
700
of BioNLP Shared Task 2011 Workshop, pages 51?
55, Portland, Oregon, USA, June. Association for
Computational Linguistics.
R. S?tre, M. Miwa, K. Yoshida, and J. Tsujii. 2009.
From protein-protein interaction to molecular event
extraction. In Proceedings of the BioNLP 2009
Workshop Companion Volume for Shared Task,
pages 103?106, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
S. Van Landeghem, F. Ginter, Y. Van de Peer, and
T. Salakoski. 2011. Evex: A pubmed-scale resource
for homology-based generalization of text mining
predictions. In Proceedings of BioNLP 2011 Work-
shop, pages 28?37, Portland, Oregon, USA, June.
Association for Computational Linguistics.
K. Veropoulos, C. Campbell, and N. Cristianini. 1999.
Controlling the sensitivity of support vector ma-
chines. In T. Dean, editor, Proceedings of the Inter-
national Joint Conference on Artificial Intelligence,
pages 55?60.
A. Vlachos and M. Craven. 2012. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. BMC bioinfor-
matics, 13(Suppl 11):S5.
701
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 45?49,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Biomedical Event Extraction
by Multi-class Classification of Pairs of Text Entities
Xiao Liu Antoine Bordes Yves Grandvalet
Universit? de Technologie de Compi?gne & CNRS
Heudiasyc UMR 7253, Rue Roger Couttolenc, CS 60319
60203 Compi?gne Cedex, FRANCE
firstname.lastname@hds.utc.fr
Abstract
This paper describes the HDS4NLP en-
try to the BioNLP 2013 shared task on
biomedical event extraction. This system
is based on a pairwise model that trans-
forms trigger classification in a simple
multi-class problem in place of the usual
multi-label problem. This model facili-
tates inference compared to global models
while relying on richer information com-
pared to usual pipeline approaches. The
HDS4NLP system ranked 6th on the Ge-
nia task (43.03% f-score), and after fix-
ing a bug discovered after the final submis-
sion, it outperforms the winner of this task
(with a f-score of 51.15%).
1 Introduction
Huge amounts of electronic biomedical docu-
ments, such as molecular biology reports or ge-
nomic papers are generated daily. Automatically
organizing their content in dedicated databases en-
ables advanced search and ease information re-
trieval for practitioners and researchers in biology,
medicine or other related fields. Nowadays, these
data sources are mostly in the form of unstruc-
tured free text, which is complex to incorporate
into databases. Hence, many research events are
organized around the issue of automatically ex-
tracting information from biomedical text. Efforts
dedicated to biomedical text are necessary because
standard Natural Language Processing tools can-
not be readily applied to extract biomedical events
since they involve highly domain-specific jargon
and dependencies (Kim et al, 2011).
This paper describes the HDS4NLP entry to one
of these challenges, the Genia task (GE) of the
BioNLP 2013 shared task. The HDS4NLP sys-
tem is based on a novel model designed to directly
extract events having a pairwise structure (trigger,
Figure 1: Part of a sentence and corresponding events for the
BioNLP 2013 GE task.
argument), in contrast to standard pipeline models
which first extract the trigger and then search for
the argument. Combining these two steps enables
to use more sophisticated event features while
largely avoiding error propagation. The model us-
age is also simple, in the sense that it does not rely
on any complex and costly inference process as
required by joint global systems based on Integer
Linear Programming.
The official HDS4NLP entry was only ranked
6th on the GE task (with 43.03% f-score). How-
ever, after fixing a bug discovered after the final
submission, the HDS4NLP system outperformed
the winner of the GE task, with a f-score 51.15%
to be compared to the 50.97% of EVEX.
2 BioNLP Genia Task
BioNLP Genia task aims at extracting event for-
mulas from text sentences, which are defined as
sequences of tokens (words, numbers, or sym-
bols). Events are constituted of two elements: an
event trigger and one or several arguments. The
event trigger is a sequence of tokens that indicates
an event is mentioned in the text. The arguments
of an event are participants, which can be pro-
teins, genes or other biomedical events. Figure 1
illustrates the GE task: given 3 proteins ?Tax?,
?CBP? and ?p300?, one must detect ?recruit? as
an event trigger and then extract two formulas:
( ?recruit?, Theme:?Tax?, Theme2:?CBP?) and
(?recruit?, Theme:?Tax?, Theme2:?p300?), both
with event type Binding.
In our work, we process tokens differently de-
pending on whether they are marked as proteins in
the annotation or not; the latter are termed candi-
date tokens. A key part of the task is to detect the
45
trigger tokens among the candidates. The BioNLP
2013 GE task considers 13 types of events, but we
only dealt with the 9 types already existing in the
2011 GE task, because there was not enough data
on the newly defined event types for proper train-
ing or model selection.
Table 1 lists these events and their properties.
The 9 event types may be merged into three main
groups: the first 5 have a single argument, a
Theme; the Binding event can accept up to two
arguments (2 Themes); the last 3 types also ac-
cept up to two arguments, a Theme and an optional
Cause. In the following, we refer to the first 6
types as non-regulation events and to the remain-
ing 3 as regulation ones.
Event type Principal arg Optional arg
Gene_expression Theme (P)
Transcription Theme (P)
Protein_catabolism Theme (P)
Phosphorylation Theme (P)
Localization Theme (P)
Binding Theme (P) Theme2 (P)
Regulation Theme (E/P) Cause (E/P)
Positive_regulation Theme (E/P) Cause (E/P)
Negative_regulation Theme (E/P) Cause (E/P)
Table 1: Main types of events with their arguments (P stands
for Protein, E for Event).
3 Previous Work
The preceding approaches falls into two main cat-
egories: pipeline incremental models and joint
global methods.
Pipeline approaches (S?tre et al, 2009; Co-
hen et al, 2009; Bj?rne et al, 2009) are the sim-
plest way to tackle the problem of event extrac-
tion. A sequence of classifiers are ran on the text to
successively (1) detect non-regulation event trig-
gers, (2) assign them arguments, (3) detect regula-
tion event triggers and (4) assign them arguments.
Such systems are relatively easy to set up but suf-
fer from error cascading. Besides, they detect trig-
gers using classifiers solely taking tokens as in-
put, or involve dependency parse information by
tree depth other than a concrete potential argument
(Bj?rne et al, 2009).
In the corpuses used in 2009 and 2011 for the
GE task, some tokens belong to several events of
different types; their classification thus requires to
solve a multi-label problem. We believe that de-
tecting triggers in isolation breaks the structured
problem down to excessively fine-grained sub-
tasks, with contextual information loss that leads
to ill-posed problems.
Global approaches (Riedel et al, 2009; Mc-
Closky et al, 2011) aim at solving the whole task
at once, so as to resolve the drawbacks of pipeline
models. In (McClosky et al, 2011), event an-
notations are converted into pseudo-syntactic rep-
resentations and the task is solved as a syntactic
extraction problem by traditional parsing meth-
ods. (Riedel et al, 2009; Riedel and McCallum,
2011a; Riedel et al, 2011; Riedel and McCallum,
2011b) encode the event annotations as latent bi-
nary variables indicating the type of each token
and the relation between each pair of them (pro-
tein or candidate) in a sentence. The state of these
variables is predicted by maximizing the global
likelihood of an Integer Linear Program.This joint
model achieves good performance (winner of the
2011 GE task), but might be overly complicated,
as it considers all possible combinations of tokens,
even unlikely ones, as potential events together.
4 Pairwise Model
Our new pairwise approach operates at the sen-
tence level. We denote CS = {ei}i the set of can-
didate tokens, AS = {aj}j the set of candidate
arguments in a given sentence S, and the set of
event types (augmented by None) is denoted Y .
The first step of a pipeline model assigns labels
to candidate tokens ei ? CS . Instead, our pair-
wise model addresses the problem of classifying
candidate-argument pairs (ei, aj) ? CS?AS . De-
noting fk the binary classifier predicting the event
type k ? Y , event extraction is performed by:
?(ei, aj) ? CS ?AS , y?ij = argmax
k?Y
fk(ei, aj) .
Variable y?ij encodes the event type of the pair
made of the candidate token ei and the argument
aj , an event being actually extracted when y?ij 6=
None. For the fk classifiers, we use Support Vec-
tor Machines (SVMs) (using implementation from
scikit-learn.org) in a one-vs-rest setting.
We used procedures from (Duan et al, 2003; Platt,
1999; Tax and Duin, 2002) to combine the outputs
of these binary classifiers in order to predict a sin-
gle class from Y for each pair (ei, aj).
This simple formulation is powerful because
classifying a pair (ei, aj) as not-None jointly de-
tects the event trigger ei and its argument aj . For
all event types with a single argument, predicting
y? variables directly solves the task. Working on
pairs (ei, aj) also allows to take into account in-
teractions, in particular through dedicated features
46
describing the connection between the trigger and
its argument (see Section 6). Finally, classifying
pairs (ei, aj) is conceptually simpler than classi-
fying ei: the task is a standard classification prob-
lem instead of a multi-label problem. Note that
entity ei may still be assigned to several categories
through the allocation of different labels to pairs
(ei, aj) and (ei, ak).
Though being rather minimalist, the pairwise
structure captures a great deal of trigger-argument
interactions, and the simplicity of the structure
leads to a straightforward inference procedure.
Compared to pipeline models, the main drawback
of the pairwise model is to multiply the number of
examples to classify by a card(AS) factor. How-
ever, SVMs can scale to large numbers of exam-
ples and card(AS) is usually low (less than 10).
5 Application to BioNLP Genia Task
For any given sentence, our system sequentially
solves a set of 4 pairwise relation extraction prob-
lems in the following order:
1. Trigger-Theme pair extraction (T-T),
2. Binding-Theme fusion (B-T),
3. Regulation-Theme pair extraction (R-T),
4. Regulation-Cause assignment (R-C).
Steps T-T and R-T are the main event extrac-
tion steps because they detect the triggers and one
argument. Since some events can accept multi-
ple arguments, we supplement T-T and R-T with
steps B-T and R-C, designed to potentially add ar-
guments to events. All steps are detailed below.
Steps T-T & R-T Both steps rely on our pair-
wise model to jointly extract event triggers, deter-
mine their types and corresponding themes. How-
ever, they detect different triggers with different
potential argument sets: for step T-T, AS con-
tains only proteins and Y = {Gene_expression,
Transcription, Protein_catabolism, Phosphoryla-
tion, Localization, Binding, None}. For step R-
T, AS contains proteins and all predicted trig-
gers, Y = {Regulation, Positive_regulation, Neg-
ative_regulation, None}.
Steps B-T & R-C These steps attempt to assign
optional arguments to Binding or regulation events
detected by T-T or R-T respectively. They proceed
similarly. Given an extracted event (ei, aj) and a
candidate argument set AS = {ak}, all combina-
tions {(ei, aj , ak)|k 6= j} are classified by a bi-
nary SVM. For B-T, AS contains all the proteins
Type Features
Surface features Stem
String after ?-?
String while pruning ?-? and/or ?/?
Prefix of token
Semantic features Lemma from WordNet
Part-of-speech (POS) tag
Token annotated as protein
Table 2: Word features.
of the sentence S that were extracted as argument
of a Binding event by T-T. For R-C, AS contains
all proteins and triggers detected by T-T. In both
cases, a post-processing step is used to select the
longest combination.
6 Features
We present here our features and preprocessing.
Candidate set For each sentence S, the set CS
is built using a trigger gazetteer: candidates are
recursively added by searching first the longest to-
kens sequences from the gazetteer. For candidates
with several tokens, a head token is selected using
an heuristic based on the dependency parse.
Candidate tokens Three types of features are
used, either related to the head token, a word win-
dow around it, or its parent and child nodes in the
dependency tree. Table 2 lists word features.
Proteins The protein name is a useless feature,
so the word features of the head token were re-
moved for proteins. Word features of the neigh-
boring tokens and of the parent node in the de-
pendency tree were still included. Proteins are
also described using features extracted from the
Uniprot knowledge base (uniprot.org).
Pairwise relations Our pairwise method is apt
to make use of features that code interactions be-
tween candidate triggers and arguments. These
patterns are defined from the path linking two to-
kens in the dependency parse tree of the sentence.
Special care was taken to perform tokenization
and sentence splitting because this has an impor-
tant impact on the quality of the dependency parse
trees. Data was split in sentences using both the
nltk toolkit (nltk.org) and the support anal-
ysis provided for the BioNLP 2013 GE task. To-
kenization was carried out using a slightly mod-
ified version of the tokenizer from the Stanford
event parser (McClosky et al, 2011). The de-
pendency parse trees were finally obtained using
phrase structure parser (McClosky et al, 2010)
47
combined with post-processing using the Stanford
corenlp package (De Marneffe et al, 2006).
Incorporating dependency information into the
pairwise model relies on the process encoding the
path into feature vectors. Many formatting meth-
ods have been proposed in previous works, such
as E-walks, that format the path into triplets (dep,
word, dep), V-walks that use triplets (word, dep,
word) or simply N-grams of words, following the
dependency parse: words are usually encoded via
stem and POS tags, and dep by the dependency
labels (Miwa et al, 2010). All these imperfect
representations lose a lot of information and can
even add noise, especially when the path is long.
Hence dependency parse features are processed
only for pairs for which the candidate-argument
path length is below a threshold whose value is a
hyper-parameter.
7 Experimental Results
The hyper-parameters of our system have been op-
timized on the BioNLP 2013 GE task development
set, after training on the corresponding training
set. Using these hyper-parameter values, the fi-
nal model submitted for test evaluation on the GE
task server has been trained on all documents from
training and development sets of BioNLP 2011
and 2013 GE tasks.
Table 3 lists the test results of our official sub-
mission. Our system achieved the best score for
SIMPLE ALL and second best for PROT-MOD
ALL, but it suffered from a rather poor perfor-
mance on REGULATION ALL, causing a low
overall score relegating the submission to the 6th
place in the competition.
Event Class recall prec. f-score
SIMPLE ALL 75.27 83.27 79.07
Binding 41.74 33.74 37.32
PROT-MOD ALL 70.68 75.84 73.17
REGULATION ALL 16.67 30.86 21.64
EVENT ALL 37.11 51.19 43.03
Table 3: Official test evaluation results.
After the test results were disclosed, we sus-
pected that our poor results on REGULATION
ALL were due to a bug, which was eventually dis-
covered in the post-processing step of R-C. We
re-trained our system after having fixed the bug
on the latest revision of the training set (our of-
ficial entry used revision 2 of the training set in-
stead of revision 3, which resulted in slightly dif-
ferent annotations for Binding events). This led
Event Class recall prec. f-score
Binding 43.24 34.37 38.30
REGULATION ALL 31.43 47.70 37.89
EVENT ALL 45.96 57.66 51.15
Table 4: Test evaluation results after bug fix.
to the results displayed in Table 4 (we only show
results that differ from Table 3). Our system ac-
tually achieves a EVENT ALL f-score of 51.15%,
instead of 43.03%: this rating is higher than the
best score of the BioNLP 2013 GE task (50.97%).
To compare to previous models, we also trained
our system on BioNLP2011 GE task training set
and evaluated it on development set. Our ap-
proach reaches a EVENT ALL f-score of 51.28%,
which is lower than that of this challenge?s winner,
the FAUST system (Riedel et al, 2011) (55.9%).
However, FAUST is a combination of several dif-
ferent models, compared to the UMass model
(Riedel and McCallum, 2011a), which is the main
constituent of FAUST, we achieve a higher EVT
score (74.93% vs 74.7%) but a lower overall score
(51.28% vs 54.8%). Our system is outperformed
on Binding and Regulation events; this indicates
the directions in which it should be improved.
8 Conclusion
This paper introduced a pairwise model designed
for biomedical event extraction, which, after bug
fix, outperforms the best performance of the
BioNLP 2013 GE task. This system decomposes
the overall task into the multi-class problem of
classifying (trigger, argument) pairs. Relying of
this pairwise structure for input examples allows
to use joint (trigger, argument) features, to avoid
costly global inference procedures over sentences
and to solve a simple multi-class problem instead
of a multi-label multi-class one.
Still, some issues remain. We currently cannot
extract regulation events whose arguments are an-
other regulation event. We are also subject to some
cascading error between steps T-T and R-T. In fu-
ture works, we intend to improve our system by
turning it into a dynamic online process that will
perform recursive event extraction.
Acknowledgments
This work was carried out in the framework of
the Labex MS2T (ANR-11-IDEX-0004-02), and
funded by the French National Agency for Re-
search (EVEREST-12-JS02-005-01).
48
References
Jari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado, June. Association
for Computational Linguistics.
K. Bretonnel Cohen, Karin Verspoor, Helen John-
son, Chris Roeder, Philip Ogren, William Baumgart-
ner, Elizabeth White, and Lawrence Hunter. 2009.
High-precision biological event extraction with a
concept recognizer. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 50?58, Boulder, Colorado, June. Association
for Computational Linguistics.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Kaibo Duan, S. Sathiya Keerthi, Wei Chu, Shirish Kr-
ishnaj Shevade, and Aun Neow Poo. 2003. Multi-
category classification by soft-max combination of
binary classifiers. In In 4th International Workshop
on Multiple Classifier Systems.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP shared task 2011. In Proceed-
ings of BioNLP Shared Task 2011 Workshop, pages
1?6, Portland, Oregon, USA, June. Association for
Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 28?36, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ?11, pages 1626?1635, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010. Event extraction with
complex event classification using rich features.
J. Bioinformatics and Computational Biology,
8(1):131?146.
John C. Platt. 1999. Probabilistic outputs for sup-
port vector machines and comparisons to regularized
likelihood methods. In Advances in Large Margin
Classifiers, pages 61?74. MIT Press.
Sebastian Riedel and Andrew McCallum. 2011a. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1?12, Edinburgh, Scotland, UK., July. Asso-
ciation for Computational Linguistics.
Sebastian Riedel and Andrew McCallum. 2011b. Ro-
bust biomedical event extraction with dual decom-
position and minimal domain adaptation. In Pro-
ceedings of BioNLP Shared Task 2011 Workshop,
pages 46?50, Portland, Oregon, USA, June. Asso-
ciation for Computational Linguistics.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A Markov logic approach
to bio-molecular event extraction. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
Shared Task, pages 41?49, Boulder, Colorado, June.
Association for Computational Linguistics.
Sebastian Riedel, David McClosky, Mihai Surdeanu,
Andrew McCallum, and Christopher D. Manning.
2011. Model combination for event extraction in
BioNLP 2011. In Proceedings of BioNLP Shared
Task 2011 Workshop, pages 51?55, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Rune S?tre, Makoto Miwa, Kazuhiro Yoshida, and
Jun?ichi Tsujii. 2009. From protein-protein interac-
tion to molecular event extraction. In Proceedings
of the BioNLP 2009 Workshop Companion Volume
for Shared Task, pages 103?106, Boulder, Colorado,
June. Association for Computational Linguistics.
D. M. J. Tax and R. P. W. Duin. 2002. Using two-class
classifiers for multiclass classification. In Proceed-
ings of the 16th International Conference on Pattern
Recognition, volume 2, pages 124?127 vol.2.
49
